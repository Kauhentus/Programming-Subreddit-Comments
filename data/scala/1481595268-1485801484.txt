Here are a couple solutions for you. The first gets the Futures alone, gets their results, and then recombines them with the first values. The second combines each pair into a single Future, and gets the results. This is probably my preference. import scala.concurrent.{Await, Future} import scala.concurrent.duration._ class Main0 { type A type B val list: List[(A, Future[B])] = ??? val results = Await.result(Future.sequence(list.map(_._2)), 3.seconds) val combinedResults: List[(A, B)] = list.map(_._1).zip(results) } class Main1 { type A type B val list: List[(A, Future[B])] = ??? val combinedFutures: List[Future[(A, B)]] = for ((a, b) &lt;- list) yield { for (bResult &lt;- b) yield { (a, bResult) } } val results: List[(A, B)] = Await.result(Future.sequence(combinedFutures), 3.seconds) } 
Await.result should be avoided at all costs because it is blocking
Await.result should be avoided at all costs because it is blocking
IIRC—and I may very well not—the issue is one of conflict between _path_-dependent types and the level of type machinery needed to express, e.g. the monad laws universally. The specifics have something to do with Scala's type system being essentially System F-sub-omega, whereas, e.g. Coq's type system is at the far right-back-top corner of the lambda cube. I could have sworn I'd seen this all discussed before, but I can't seem to find the references right now. I'll try to follow up tomorrow.
Plus you could end up waiting 3 * N seconds for each operation, which could become a very long time.
I was merely demonstrating how to get the final results. You can stop at "combinedFutures" and use Future.sequence(combinedFutures) for more computations.
You should use it at the end of your program, so that it doesn't exit before your Futures have completed.
Well, I'd say "never use threading with locks and mutexes," but that doesn't necessarily imply actors instead. If you have to stick with the standard library for some reason, certainly look at [`Future`](http://www.scala-lang.org/api/2.12.x/scala/concurrent/Future.html). If you don't mind adding a third-party library and learning a bit about how to use it, then I recommend scalaz's [`Task`](http://timperrett.com/2014/07/20/scalaz-task-the-missing-documentation/). If you want to look into streaming things to and fro, synchronously or asynchronously, then I'd look at [scalaz-stream](https://gist.github.com/djspiewak/d93a9c4983f63721c41c).
I was looking for something like this, and couldn't not share. [Here's a screenshot of it running in my terminal](http://imgur.com/a/APr0L)
Kind of the other way around (you define your Slick entities in terms of case classes), but: with [Slickless](https://github.com/underscoreio/slickless). **Update:** Upon reflection, this isn't really an answer, since the `Table` _per se_ isn't really derived from the case class. That would be more work, not to mention all the points that m50d [rightly enumerated](https://www.reddit.com/r/scala/comments/5hpssq/biweekly_scala_ask_anything_and_discussion_thread/db3ek5g/).
I can completely relate.
RemindMe! 2 days
I will be messaging you on [**2016-12-15 10:57:09 UTC**](http://www.wolframalpha.com/input/?i=2016-12-15 10:57:09 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/scala/comments/5i1e3y/found_this_on_github_a_little_dusty_but_looks/db4y3t2) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/scala/comments/5i1e3y/found_this_on_github_a_little_dusty_but_looks/db4y3t2]%0A%0ARemindMe! 2 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! db4y3xt) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I haven't used Monix directly but am in love with scalaz `Task`. How do they compare in your experience?
&gt; I will say, it looks like the community has spoken, and monad transformers are dead (or at least dying). I'm not sure yet. I think everyone's excited about the various effect models and trying to understand them, and the various implementations are part of that (I recently made the first release of my own, paperdoll, though it's not quite at the point where I'd advertise it yet). That doesn't necessarily mean we've explored all the advantages and disadvantages yet.
Yep. I absolutely should emphasize "looks like." Daniel Spiewak has some criticisms of Eff that I need to pay closer attention to.
Scala uses the new Java lambda expressions and stream functions native vm code to compile producing less code. 
I noticed monix's task has better performance. As for a detailed difference, check out monix documentation on its task. There's a section comparing with scalaz task. Monix also provides task typeclasses for both scalaz and cats. In other words it can be a drop in replacement for scalaz task. And you dun have to use observables. Tasks and the relevant typeclasses are separate module. API are mostly the same. No reason not to give it a try ;) Edit: the downside is that libraries like http4s and doobie return scalaz task only. Not sure how to convert between the two implementations. But converting from future is dead simple, `Task.fromFuture`
Open an issue or ping me if you run into trouble.
Are you able to provide an example for json format?
There are many ways of solving this. I would explore how to do it with types and functions: case class Product(id: Option[Long], sku: String, name: String, price: Double) case class Discount(value: Float) extends AnyVal case class Total(amount: Double) def charge(product: Product, discount: Discount): Total = Total(product.price - (product.price * discount))
I've written a utility method like this in the last couple of projects I've worked on: def toMap[A,B](pairs: List[(A, Future[B])]): Future[Map[A,B]] = { val z: Future[Map[A,B]] = Future.successful(Map.empty) pairs.foldLeft(z) { (futureAcc, tuple) =&gt; val (a, futureB) = tuple for { acc &lt;- futureAcc b &lt;- futureB } yield acc + (a -&gt; b) } } Once you have a `Future[Map[A,B]]` (in your case, `Future[Map[Enum.Value, File]]` you can block on it to get the `Map` you're looking for, or map/flatMap over the Future if you don't want to block.
The "advanced" functional way of doing such things is using lenses. There are several lense libraries available for Scala (e.g. http://julien-truffaut.github.io/Monocle/).
Thanks for the reply, so would a method in the case class be the right play for that? That is certainly the way I would approach it in Java since I'm mutating the state of that object.
Yeah, I think so. You've definitely got the right idea about class immutability. Either a method in the case class, or another class which is constructed with a Product and simply applies a discound 'filter'.
a coherent news article is a treat.
No, this guy is awesome.
There are a lot of definitions of functional programming. One of my favorite (if slightly tongue-in-cheek) is that it's programming without assignment. You should never need a `var`. Technically you don't really need a `val` either since you could use a `def` instead, but it prevents re-running code in a language that doesn't have compiler level referential transparency. case class Product(id: Option[Int], sku: String, name: String, price: BigDecimal) { def applyDiscount(amount: BigDecimal) = { val newPrice = if (price - amount &lt; 0) 0 else price - amount this.copy(price=newPrice) } } 
To hell with the JVM. 
 case class Product(id: Option[Int], sku: String, name: String, price: BigDecimal) { def applyDiscount(amount: BigDecimal) = copy(price = (price - amount).max(0)) } Where you put this depends much on the larger context of your codebase, but I will say that in more functional projects I see less and tend more to avoid methods defined on case classes, usually in favor of something like a typeclass. I find that a "functional approach" has more meaning in larger contexts of code and so can be challenging to illustrate or analyze in such focused examples, but I would say that if you focus in these kinds of contexts on brevity, and chaining computations, you will get a kind of mental exercise that is useful in a larger functional codebase. Namely that is, you will be thinking in expressions, as opposed to imperatively or procedurally. Try to get by without statements or control structures for a bit and see how it feels past the initial awkwardness.
&gt; did they get rid of invoke dynamic usage entirely? I know that's a blocker for java 9 ahead of time compilation Er no. On the contrary, Scala 2.12 *starts* using `invokedynamic`.
I use Akka quite successfully for distribution, but relying on Akka's higher-level constructs, like Akka Persistence, Streams, Clustering, Cluster Singleton, Distributed Pub/Sub and Distributed Data. Our only direct use of actors is a small layer on Akka Persistence that provides for atomic updates: https://github.com/artsy/atomic-store.
Looks great! I just wish it uses `monix` instead of `Rx.js`
Replacing Rx with Monix would probably be trivially possible, always open to PRs. :)
I am in the same situation as you, maybe a step ahead of you. These two books are really helping me grasping the fp concepts and also show me how to apply them. https://www.manning.com/books/functional-programming-in-scala https://www.manning.com/books/functional-and-reactive-domain-modeling You can get discount codes from these sites: https://www.lightbend.com/resources/e-book/functional-programming-in-scala https://www.lightbend.com/resources/e-book/functional-and-reactive-domain-modeling
Hmm...looking at github again, I think the issue there is that I couldn't see any docs on how to generate classes from schemas. 
On back-pressure, there are indeed data-sources that cannot be stopped - if your data-source is signaling mouse movement, you obviously can't tell the user to stop for a moment. However back-pressure doesn't necessarily mean pausing the data-source. For data-sources that cannot be paused, you can apply *buffering*, in which case having back-pressure in the protocol means that the producer knows when the consumer is slow. Think of it like this: without back-pressure you end up with a lot of instances in which the buffering done is unbounded, thus endangering your application, whereas with back-pressure in your communication protocol you can fine-tune the buffering being done. In Monix, besides Unbounded, you also have DropNew (for dropping incoming events on overflow), DropOld (for dropping older events on overflow, less efficient, but gets the job done), ClearBuffer (to drop the whole buffer on overflow) and TriggerOverflow (to end with an error). And in case events are being dropped on overflow, you also have the option to log this event somewhere, or even to signal an overflow event to your client - such that the client can then display a warning to the user that the connection is too slow or something. Back-pressure is in many cases also about correctness. For example lets say that your consumer (Observer) does HTTP requests based on the onNext events received. But you only want a single HTTP request to be done or lets say at most 4 requests to be done in parallel, but no more. With back-pressure this is easy and if the data-source can be paused then no buffering is needed. Also fun story: in RxJava (and I think RxJS too, but not sure) the "*flatMap*" operation is an alias for "*mergeMap*", but in Monix "*flatMap*" is an alias for "*concatMap*". The reason is that "*concatMap*" needs to do buffering in case you don't have back-pressure in the protocol, making it unsafe because the buffer ends up being unbounded, also because of not having back-pressure. But in Monix the "*concatMap*" operation doesn't need to do buffering, the "*mergeMap*" operation being the one that needs buffering - but even then, you can fine tune that buffer's behavior if you want. So "*flatMap*" in Monix is "*concatMap*" because it is safe and because "*concatMap*" is in fact the monadic bind that we want and that Scala developers expect. Your comparison with Iterator is very apt. Rx.NET also has AsyncIterator (or something along those lines). Having back-pressure in what is otherwise a push-based protocol feels like a pull-based approach, but there are differences. I cannot explain the nuances right now, because it would be the subject of a long blog post, but btw, I also plan to introduce a pure pull-based stream type in Monix, as our own alternative for AsyncIterator, but one that is purely functional and powered by Task - it's going to be interesting. For example it won't handle time based ops, timeouts, or other reactive stuff, it will have worse performance, but in some cases it will be easier to reason about. And it will support foldRight, which can't happen for Observable :-) On Rx, you can think of Monix as the Scala version of Rx. Knowledge about Rx is transferable from/to Monix as much as it is transferable between RxJava, RxJS and Rx.NET. Note that these versions have differences in their protocol and all of them have opinions about how things should be, very much like Monix. I did not give this library the "*Rx*" name, because "*Rx*" is for stuff coming from Microsoft mostly, plus I do not want Monix to repeat mistakes or to be restricted to the Observable pattern. As a final note, I've got nothing against RxJS usage. In case you can't tell, I love Rx and all its implementations :-)
1st rule of Scala, don't actually build anything. 2nd rule of Scala, deliberate.
There are many ways to slice this. You're asking good questions. The best way forward is to try something, and see how it works. You're interested in functional programming, so I would keep in mind the following: * Avoid vars whenever possible. * Don't mutate (you're already on to this). * Avoid unnecessary side-effects (another way to state the two points above). In my experience using scala and fp principals to build applications, the best way to think about programming is to build from a solid foundation of truth. This truth is told by the type system. You should avoid business logic by leveraging the language. E.g. if you find yourself writing a lot of if / else statements, you may be doing it wrong. I realize the above may be vague. With the above principals, I would consider something like this: // A Product shouldn't have an optional ID. If it's optional, every time you want to access // it, you'll have to check for it. That would become tedious without payoff (just because // it's tedious doesn't always mean it's a bad idea). I give an example // below that should remove the need for this optional ID. // Some may think it overkill to make Sku a type instead of using a String, but the // cost is low, and the readability and type-safety it gives is great. // Not sure what you'll need to represent a price, but BigInt is probably not the best // representation. At the very least wrap a BigInt in a Money type. This also improves // type-safety and readability. case class Product(id: ProductId, sku: Sku, price: Money) case class ProductId(value: Int) case class Sku(value: String) case class Money(value: BigInt) { def minus(v: Money): Money = Money(value - v) } object Money { def zeroIfNegative(v: Money): Money = if (v.value &lt; 0) Money(0) else v } // For the apply discount logic, you may be overthinking it. val p = Product(ProductId(1), Sku("abc"), Money(10.00)) // This will "apply a discount" val discountedP = p.copy(price = Money.zeroIfNegative(p.price.minus(Money(2))) // But, after I type this out, it may be nicer to put this in your Product class. At this point // it doesn't matter either way. You could take this even further... trait Id[T] { def value: T } trait DomainId extends Id[Int] case class ProductId(value: Int) extends DomainId trait DomainObject { id: DomainId } object DomainObject { // A "free" domain object is an object that has not yet been persisted and, therefore, // does not have an ID. trait Free trait Update { // Gotta know what you're updating. def id: DomainId } } case class Product(id: ProductId, sku: Sku, price: Money) extends DomainObject object Product { case class Free(sku: Sku, price: Money) extends DomainObject.Free // SKU is not updatable. This is my own arbitrary decision. case class Update(id: ProductId, price: Money) extends DomainObject.Update } I typed out the above just to give you a very short / abbreviated example of a technique that I used in the past. It's overkill for your simple example. But it might help show you things that you can do with the scala type system to help you become more efficient in your work.
I'm not very familiar with either of these, my impression is: * OutWatch uses a virtual DOM (snabbdom.js), whereas Binding.scala directly calculates which DOM updates are needed * OutWatch does not use macros so I'm guessing the JS code size should be smaller * OutWatch API is more focused on streams /FRP whereas Binding.scala API seems to be more traditional (compare how they handle click events for example).
Also, how does it compare to https://github.com/OlivierBlanvillain/monadic-html ?
What exactly would you consider the big difference?
As I explained, with `Reader`, you have to pass the dependencies explicitly to the function, which can include things such as a logger or a database connection. With DI, these are injected and are available to the function without having to appear in the signature of the function (which would break encapsulation). 
So they are still working on it? I was under impression that they are switching over to developing a scala plugin for VSCode.
The community edition is open source, as well as the scala plugin. https://github.com/JetBrains/intellij-community https://github.com/JetBrains/intellij-scala
Weird, could you share the output?
Well, look at the [changelog](http://scala-ide.org/docs/changelog.html) then.
And still far behind the IntelliJ IDEA Scala plugin, that is superior. /sigh/
And there's *absolutely* no difference at all between CE and Ultimate for Scala development. Not a single bit.
You do lose referential transparency in practice, since implicit resolution is context-dependent. Same as throwing an exception breaks referential transparency.
I'm not using any proxy. Here's the update.log [file.](http://pastebin.com/5TgCN2LU) I'm guessing this is what you mean? In this, I've tried to create a new Play 2.x project from IDEA and it was taking a long time, and I uploaded this file. 
Here's the update.log [file.](http://pastebin.com/5TgCN2LU) I'm guessing this is what you mean? In this, I've tried to create a new Play 2.x project from IDEA and it was taking a long time, and I uploaded this file. 
But that is applicable to both approaches, no? Only with dependency injection you can't see in the types what's going on. Edit: okay so I guess you made me refute my own point. But isn't this breaking of encapsulation then unavoidable?
Good article. I have played with `scala-async` as well and its promises are great (pun intended) but its current limitations are very frustrating. I hope somebody will pick up development again. Based on the [github commits](https://github.com/scala/async/commits/master), nothing has been done over the last year except adding Scala 2.12 support.
I just found [this article](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html) which tries to explain functors, applicatives and monads in an intuitive way, based on Haskell's syntax. Two questions: * In the section about functors at the very end, it says that functions are also functors. Is that also true for Scala? I assume `fmap`is `map` in Scala, but I wasn't able to combine two functions (e.g. both of type `Int =&gt; Int`) with `map`, however, `andThen` seems to do exactly that. * In the section about applicatives, Haskell's `&lt;*&gt;` operator is introduced. Is there an aquivalent operation in Scala? I figured that `fmap` is `map` and `&gt;&gt;=` is `flatMap`, so what about `&lt;*&gt;`? It seems to apply the function contained in `A[B =&gt; B]` to the value contained in `A[B]`, producing another `A[B]`.
Does it ever succeed and then fail, or essentially the problem is it can't download sbt? Anything unique about your setup? What OS, what Java version?
Huh, nice! How did I not know that? Hmm. If it's *all* available what technically stops anyone from just compiling the source themselves and skip the payment part? I mean it's a quite expensive piece of software (even though it's definitely worth every penny imo!)
Does this do some sort of virtual DOM diff like React? For example, what happens if I have a list of values (each rendering to one div) and I add another element to the list?
The problem is not that it fails, though it does sometimes. But other projects that I've created use SBT right? So wouldn't SBT be in the local cache? Why would SBT be fetched every time I create a new project ? I have a pretty bland setup with U until 16.04 and IDEA Ultimate the latest version that is. I'm also using OpenJDK 1.8 or so. 
The Community Edition and Scala Plugin are both Open Source. But the Ultimate Edition contains proprietary components, FWIW.
As for "when to use it", I personally think it's strictly better than for-comprehensions, because it composes with a greater subset of other Scala syntax. Much like JS generators or its own `async` functions, `async`/`await` really shines in flows of async operations with branching, especially when the number of operations may differ between pathways. Also performance can be better. The project's [readme](https://github.com/scala/async#comparison-with-direct-use-of-future-api) makes a pretty good case for it! For a quick alteration of a `Future`'s result, it's possible that `map` is the better way to go. As mentioned in other comments, I'm also a little disappointed that we haven't seen any expansions of the power of the library in the recent past. In particular, being able to traverse containers would be really nice, some ability to work inside closures, and compatibility with `try`/`catch`.
&gt; if jetbrains shuts down or screws up their UI, I'm doomed. Ding ding ding! This is why IDE monocultures (and monocultures generally) are bad. Choice and competition are better for users.
Shrug. I use Eclipse, my two teammates use IntelliJ. We both have about the same number of problems these days (ie, not too many), just different ones.
&gt; In the section about functors at the very end, it says that functions are also functors. Is that also true for Scala? I assume fmapis map in Scala, but I wasn't able to combine two functions (e.g. both of type Int =&gt; Int) with map, however, andThen seems to do exactly that. It's also true in Scala; unfortunately the standard library only defines `andThen` directly, but you should be able to use `map` from ScalaZ (hopefully also Cats, but I can't find the Cats scaladoc online as easily) on functions. &gt; In the section about applicatives, Haskell's &lt;*&gt; operator is introduced. Is there an aquivalent operation in Scala? I figured that fmap is map and &gt;&gt;= is flatMap, so what about &lt;*&gt;? It seems to apply the function contained in A[B =&gt; B] to the value contained in A[B], producing another A[B]. ScalaZ defines `&lt;*&gt;`. In vanilla Scala one would have to emulate it via `flatMap` (and it wouldn't work for non-monads, but most applicatives are also monads anyway)
Thank you. :)
It basically means "something happened, but we can't give you a useful error message". It's unfortunate, and hopefully something we will see less of with dotty. When it happens just add type annotations until it goes away or an error is revealed
Thanks. Judging from your and jtcwang's answer, libraries such as ScalaZ and Cats are really more than just a nice-to-have when someone wants to dig deeper into FP with Scala.
Thanks for the detailed explanation. Functional programming seems to have some real advantages, but hard to think in a functional way after years of doing OOP. 
I'm confused - why do you think Cats is hilariously overengineered? Both it and Scalaz have certain typeclasses that only exist to allow others to exist that are useful. Also why mistrust Sabin's work? HLists are particularly useful, as are generic tuples and extensible records. Functor, Applicative, Apply, Monad, Traversible, and Foldable simply unify principles that are already present in the scala std library, allowing you to raise the level of abstraction a notch above the specific std library class. Other things, like Free, and State allow recursion over Monads that would otherwise cause stack overflows. Task allows you to have lazily evaluated Futures, which lets you control the execution order of your program more tightly. Xor is a Try you can rely on. Typeclasses themselves are superior to inheritance or extension methods for adapters, as they must be declared at their site of use and do not require extending a heirarchy, and allow you to override final public methods at need. Raising the level of abstraction using these libraries and techniques allows you to have code that is cohesive, flexible, and decoupled and composable. It even allows you to decouple execution from description which can eliminate much boilerplate in client classes. The silly thing is that you are already using these abstractions, but in the standard library they are distributed to traits that are difficult to extend because they do too much. Now, whether or not they have an encoding that makes sense (the typeclass subclassing makes things difficult and the single character variable naming scheme for type and function arguments makes them difficult to teach), is a different matter -- is that what you mean? I also agree that much of the time your choice of Functor/Monad has semantic meaning. But a lot of code really is glue code (just uses map,flatMap,filter,foldLeft,and, sequence inside a named function), and thus shouldn't really require a specific Monad/Functor in its signature to decouple the glue from the logic. I find those two libraries to be less overengineered than the std library, certainly. And I have a distaste for symbolic operators. But I wouldn't throw out the baby with the bathwater just yet. The useful pieces are easy enough that you could write them all yourself, sure. I don't want to spend that much time. They are buried as transitive dependencies of lots of scala libraries, so you often have cats or scalaz and shapeless anyway. Now, if you want to be fully OO with scala, you can do that too. If so I might suggest using akka so that you aren't tempted to use structs with methods and subclassing instead of message passing and composition (enforces decoupling), which leads to the spaghetti universe we all live in day to day. 
What's the state of the scala reflection api in 2.11+? Any good examples? I am stuck trying to get an annotation off a case class member. I know it's there because I can see it on the constructor param using the Java API, but reflecting on the type and getting annotations in case accessors shows nothing. Specifically this is my question: http://stackoverflow.com/questions/41176273/get-case-class-annotations-with-reflection Every time I try the scala reflection stuff I get frustrated
It'd live in another class, basically just forcing you to separate data and actions. Don't mutate case classes, use the copy function to create copies of them. You can use var if it makes sense, especially high perf scenarios. If you only use it when it's declared in a function it's still thread safe since it's stack allocated and not heap allocated. In general vars are frowned on, and you very very rarely need to use them
It isn't really personal, I just thought /r/scala would be a good place to talk about it.
&gt; Morris behaves very unpleasantly, at least over IRC. He upsets people, including newcomers seeking help, consistently enough that it can't not be deliberate (e.g. calling anyone who disagrees with you "disgusting" is not a good-faith, accidental choice of words). I firmly believe a community with him in it cannot be successful. I think this vilification of Tony side steps the real issue. The concerted and targeted troll campaign in the Scala community. Tony by himself doesn't make the community hostile. Tony isn't targeting women who speak at Scala conferences. Last, but not least, Tony is capable of change, and has changed in demeanor in the last year. But the hostility hasn't changed, in fact you could argue it's gotten worst.
Well yeah I agree. I guess. But this is on /r/scala so I suppose most of us think it's at least fine. I like the language as a pl geek and a concurrency geek..I would start a project for a business in java though since it has fewer build breaking changes between versions and a larger population of experienced programmers though.
Thank you for the unbiased comments. I better understand the issue, but am as unsatisfied as you are with the result. Ideally, one of the libraries will be decisively victorious, and my guess is that it'll be Cats, given the group's superior community outreach. But it's a shame that they felt fragmenting the community and adopting a radically liberal CoC was necessary in achieving this. Sigh.
AFAIK it was a question, not something that happened already: https://www.reddit.com/r/scala/comments/52w7n8/future_of_scala_ide_would_you_like_to_see_an/ Anyway, biased against VisualStudio user here.
It may be a bit non-straightforward, but well, why criticise a free software / open source project? I think everyone would be happy if you put a PR for things bothering you the most. Also, I don't think contributors are in any way forced to follow a roadmap, they might as well fix what's most important for them personally. It's actually OK (for everything except maybe UI), as far as my observations of FreeSoftware goes.
Thanks for the heads up! I thought scala was not fine
&gt; So, continue to experiment and strive for organic growth. Resist early standardization, and simplify instead of piling up language extensions. Clarify and edit, as understanding grows, and iterate toward a closer approximation of the mutable and unattainable optimum. Couldn't agree more
Thanks, very insightful :)
I do think it's important to get past the mental block against the word "monad." Note that I didn't say "the mental block against monads," because no one who's been using Scala for more than about a week has such a mental block: they use and understand monads just fine, every time they use a for-comprehension. What's interesting—and frustrating for everyone concerned—is taking this intuition and making explicit what's behind it, then providing some machinery to support explicit development and use of monads. Even when that's done, if you don't already know why you want to use monads explicitly, and don't already understand this abstraction that's been concretized and dropped in front of you, it can be pretty off-putting—especially when you know you're smart and capable, but nevertheless feel at sea with all of this stuff. I think Brian Beckman explains the mathematics of monads as well as anyone else ever has. But I don't think he does any better at giving the motivation for them in programming than most functional programmers do—myself unfortunately included.
I think /u/m50d is a bit biased, as I've said here in the past. I think both libraries have their strengths/weaknesses, but by no means does Scalaz not 'welcome newcomers' or speak to the community or whatnot. Last year at LambdaConf we had maybe 6 or so various Scalaz committers giving talks on current and new ideas for Scala and FP, along with informal 'State of Scalaz' meetings afterwords that had open invitations to anyone in the Scala community. A couple non-Scalaz folk showed up to observe and learn even. I've found a handful of the Cats committers to be some of the kindest and most knowledgable people in the Scala community, and I've found some of the Scalaz committers to also be some of the most knowledgable and kindest in the community. Cats has some bad apples that really ruin it for me personally, and clearly /u/m50d thinks one committer in the Scalaz project ruins it for him so... *shrug*. 
This basically relies on reflection. You could have written a version that's based on macros. Pretty sure that's what this one does https://github.com/melrief/pureconfig
I did the same thing about 8 months ago–I had never used Scala before, but was starting a job w/Scala &amp; Spark. I recommend focusing primarily on learning functional Scala–writing Spark code looks a lot like doing operations on lists, but with a distributed backend. For this, I highly recommend the book *Functional Programming in Scala* In terms of getting set up quickly, here's what I'd do: 1. Install giter8 and use it to set up a simple sbt project (https://github.com/ferhtaydn/sbt-skeleton.g8) 2. Install IntelliJ IDEA, and Scala worksheets as your main way of writing exercises and trying code. Use this to do the exercises from the book. 3. After you do the exercises, check the answers from the Github repo: https://github.com/fpinscala/fpinscala Initially I think you can ignore almost all other setup. Before starting your new role, you don't really need to learn how to use SBT, or much about Spark–this tends to be highly context-dependent, so you'll pick it up faster on the job itself.
You sure about that? I'd have to imagine that implicit search for the typical DI usage of implicits would be pretty trivial.
I was in the exact same situation 6 months ago. There's an excellent Coursera course on Scala by the language creator. After that, the language documentation itself. Collections are most useful. Become familiar with the Dataset (formerly DataFrame) API. Know how to use map and filter. Know how to perform SQL queries natively on data sets.
So, who creates that `UserRepository` in your example? And what if it needs three more classes? And these classes need four more classes to be created? This is a typical topological sorting of what can be a graph of a hundred nodes. Are you expecting me to figure out what that topological order is and write all this boiler plate myself in my code? And then one day, one of these classes needs one more parameter in its constructor, and not only do I need to update all the call sites, but I need to re-sort the whole graph manually? That's insanity. No thanks. I'll pick a DI container, tell it "I want a `UserRepository`, I don't care how you do it" and enjoy the productivity gain while you slave away on a piece of paper trying to topologically sort your graph of classes. 
I did that Scala course in summer and they also announced a Spark course back then (as part of the "Scala specialization"). Unfortunately, it still seems to be unavailable ("starts soon"). I emailed the author but didn't get a response :(
 I agree just like pretty much everyone in the Scala community since implicit abuse is real and dangerous. Another trap is implicit conversion which shouldn't be used outside special DSLs. These things should be limited somehow...
thank you very nice methodical answer I really like the github repo with questions and answers
Have you taken a look at scanamo? They use the Free Monad to great effect. It seems similar to what you are doing 
Thanks for pointing to this resource. I did not know such library exists and will look into their source code. From what I quickly checked, it is quite different from what I am trying to achieve as I am trying to recode the existing DynamoDB library from the ground up whereas most of the libraries are "just" wrappers for the Java client. It also seems they don't provide any type class for writing / reading from DynamoDB format.
You're wrong about the reflection part. But pureconfig looks like a pretty interesting library! 
I'd say that's actually the right move–Spark notebooks are decent, but nowhere near as good for exploratory data analysis as the Python or R tools. Python's fastparquet library looks pretty promising, as a way to use pandas &amp; matplotlib style libraries on Spark-sized datasets.
I’m learning macros and I decided to make macro based serializer/deserializer (I know those already exist, I do it for fun and to learn it) Now I want my deserializer to be able to handle various types of content. For example { "Name": "Tiger" "Type": "Cat" "Lives": 9 } vs { "Name": "Tiger" "Type": "Dog" "CarsChased": 97 } And I want to json deserialize into 2 different classes dependant on Type. I was just wondering if I could just make val Type = "Cat" in cat class and use macro to generate if(json.Type == "Cat") //Or dog or whatever I put in the vale initialization new Cat() I know I can use annotations, but i was wondering if I could do this way. 
&gt; And then one day, one of these classes needs one more parameter in its constructor, and not only do I need to update all the call sites, but I need to re-sort the whole graph manually? You only need to update all places where the object in question is created. E.g. if your `UserRepository` needs another paramater and you only create one of it for the whole lifecycle of your app, then there is only one place (more or less) in the code that you have to change. If you are passing that additional parameter down through multiple layers then you are probably doing something wrong.
This looks good, except I would prefer the lightweight method call syntax: `copy(price = price - amount max 0)`. I prefer to think of the method as just another operator.
You've gotten some good answers here. I like https://www.reddit.com/r/scala/comments/5i5y29/coming_from_java_to_scala/db6boxa/ as I feel it to be the most idiomatic. To me that's an important thing to keep in mind: what is idiomatic Scala? That's something you'll need to learn, because it will often be the 'path of least resistance' when you're programming in Scala. Imho idiomatic Scala is somewhere between OOP, ML-style FP, and Haskell-style statically-typed programming. One last advice I'd offer is to step back a bit and really look at the type signatures. They're often slightly hidden by the implementation bodies, but they're worth looking at. In your case: case class Product( id: Option[Int], sku: String, name: String, price: BigDecimal) { def applyDiscount(amount: BigDecimal): Product } object Product { implicit val ProductReads: JsonReader implicit val ProductWrites: JsonWriter } Read out the signature of `applyDiscount`: 'takes a `BigDecimal` and returns a `Product`'. Does that make sense? If it does, you probably have a good design. As someone pointed out here, does it make sense for the `id` to be optional? Think about it from the perspective of if you're storing these products in a database table.
While you can prevent it with the right compiler flags (Y-no-adapted-args) skipping dots and parens out of the box can lead to runtime bugs due to native behaviors and implicit conversions included in Predef. It also makes operator precedence opaque in long chained expressions, which in real code can easily be the majority case. For a purely visual syntax hack, I find the benefit absolutely not worth the potential cost.
In addition, does anyone have links to good resources on designing programs with Akka? 
I'd rather wire things manually then rely on any magic I guess. Each to their own
It's called a library. We use them because they save countless hours of manual boiler plate. Pretty sure you already use a bunch in your build file. 
We use Akka and Eventuate on our newer web services. I just released to QA an Akka/Eventuate service to take in events from our other servers and asynchronously deliver emails using Amazons SES platform as well as process the feedback loop from Amazon to match events and notify the requesting service of status. For my next project I'll be applying the same workflow to deliver SMS via Amazon SNS. That was my first experience writing any production code in Akka or Eventuate. And other than the amount of bullshit you sometimes have to put up with because of eventuates youth, I gotta say I'm feeling like a convert. Suddenly everything's feels like it should be mapped to an event feed and asynchronously dealt with. 
I have only used Akka in one simple case, but I think its a nice example. In particular because, imho, Akka should not be exposed as the programming model. I think it is best suited as an internal engine for an application/framework, which can then provide a more familiar and domain-specific abstraction. I used Akka to implement a [cache simulator](https://github.com/ben-manes/caffeine/wiki/Simulator), which broadcasts events to many consumers and aggregates the results. This is a classic master-worker problem with bounded queues to throttle the master. Akka lets the policies run in parallel with very little extra code compared to doing it sequentially. This could probably be improved upon by switching to streams with backpressure instead.
I think this misses the entire point of an event and what it means. An event is often a result of a command, therefore it is reflective of a state change that has actually happened. A command that isn't accepted by the domain, shouldn't be suddenly accepted when the business logic changes, and the whole point of having a domain is to validate commands. If you want to have a record of the commands people send, I would either just log them, or save them seperately, but merging them would not be a good solution to the problem imo.
The compiler can figure it out if you put it as a member of a type, bizarrely. See https://github.com/m50d/scalaz-transfigure/blob/master/src/main/scala/scalaz/transfigure . You still want an equivalent of scalaz `Leibniz` though really, so it's worth writing your own. I would try to pull the above out as a library if there's interest, but I think scala 2.12 partial unification of higher kinded type construction makes it a lot simpler (i.e. it would remove the need for `UnapplyC` entirely).
&gt; I'd also strongly recommend against implicit conversions from String to anything, as they rarely make any sense (you don't really mean "all Strings are Persons," do you)? A useful alternative is pimping a method onto `StringContext`, which allows you to use e.g. `p"m50d"` as a `Person` literal. 
I think I'd endorse that statement. ScalaZ welcomes newcomers a lot of the time, but it is (or at least was a year or two ago, and I am not aware of any changes that would have addressed this since) a reasonably common occurrence that a newcomer arrives on IRC, asks a few questions (that embody assumptions that are contrary to the functional-scala consensus - for the sake of the argument let's even just say these assumptions are wrong), and receives a response (almost always from Morris) that not only tells them they're wrong but seems carefully phrased to upset them as much as possible. Such newcomers tend to leave and never come back, and I don't blame them at all.
I also like [ssc](https://github.com/ElderResearch/ssc)
But Morris is gone now. Do you think the situation is still the same? There are people in the IRC channel abusing newcomers?
This is currently under heavy development, but we are using akka for a simple payment verification node on the Bitcoin network: https://github.com/christewart/bitcoin-s-spv-node/tree/address_manager
I think from my last explanation you should be able to understand the kind of bugs that can arise from this, and if you do you would then see why my original code is not susceptible. Further, my point about opacity of precedence is not relevant to mathematical operations (everyone knows it already.) On top of that, and IMO worst of all, we are now 4 comments deep on a discussion that has no material impact on the performance or correctness of code, which is as wasteful a discussion for people in our field as tabs vs spaces. I have a very strict radar in code review, which is that unless it can impact performance or correctness, I don't want to talk about it. I know that's not how everybody works, and I'm fortunate to have steered my career through places where this is a valued approach.
I didn't know he was gone. For good? Banned? If so I'll take another look at IRC.
And to slightly correct myself "no material impact on the performance or correctness of code" in this case really should be "no material _benefit_ on the performance or correctness of code to remove parens and dots, but a real risk to correctness." Hence my original argument, whether or not you think that mathematical operations with correct grouping parens should be exempt. BTW, in your example "price - amount max 0" - are you expecting all your programmers to have confidence that this means "(price - amount) max 0" and not "price - (amount max 0)" ? Do you have that confidence? I do not, and I don't want to have to retain that information when there's way more important things for me to focus on.
[Reactive Messaging Patterns with the Actor Model](https://smile.amazon.com/Reactive-Messaging-Patterns-Actor-Model/dp/0133846830?sa-no-redirect=1)
[removed]
I'll generalize slightly to: always take a good look at [Martin Krasser](https://github.com/krasserm)'s work. [streamz](https://github.com/krasserm/streamz) is another gem.
Yeah this is exactly what command sourcing is
Typo line 14
Thanks! Should be fixed now.
what'd you use for the illustrations?
Java is an easier language to learn. It makes sense when you want something more imperative ([same motivation as for using Go](http://nomad.so/2015/03/why-gos-design-is-a-disservice-to-intelligent-programmers/)) and arguably more mature. Java's lack of expressiveness can lead to more needless boilerplate (incidental complexity) that makes code less type safe, harder to understand, extend, and maintain, but it also means language features aren't the bottleneck for understanding what a piece of code does, and it gives you fewer opportunities to [violate](http://jimplush.com/talk/2015/12/19/moving-a-team-from-scala-to-golang/) the [principle of least power](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html). People encourage using Scala as "Java without semicolons," but there are a lot of nuances to pick up (scratching the surface: for expressions, inability to use break and return as in Java, Scala IDE not being able to consistently infer types for you) so this can be poorly-received. 
Something tells me you're a fan of Spring.
I agree with the principle of least power, and there's indeed some pretty insane Scala code out there (scalaz &amp; monad transformers comes to mind), but it's just as easy to get lost in all the patterns &amp; reflection that Java has to resort to to compensate for a needlessly "stupid" language. Scala doesn't have to be hard to learn unless you start by giving them the category theory crap lecture before they've mastered the basics. I actually don't see why it would have to be harder than Java - ie in java you have to know what are "primitive types" and what's reference types, you also need to fight with getter and setters and implement your own hashCode and equals methods - in Scala you just write case class and you're done for the evening. 
You forgot to hide `None` and `Some`. scala&gt; :paste // Entering paste mode (ctrl-D to finish) import scala.{Option =&gt; _, None =&gt; _, Some =&gt; _} sealed trait Option[+A] { def map[B](f: A =&gt; B): Option[B] = this match { case None =&gt; None case Some(a) =&gt; Some(f(a)) } } case class Some[+A](get: A) extends Option[A] case object None extends Option[Nothing] // Exiting paste mode, now interpreting. import scala.{Option=&gt;_, None=&gt;_, Some=&gt;_} defined trait Option defined class Some defined object None scala&gt; Option(42).map(_ + 42 ) res1: Option[Int] = Some(84) 
It makes no difference. When I use paste mode there are no compilation errors or when I use scalac Option.scala. When I use :load then I get the errors. &gt; scala&gt; :load Option.scala Loading Option.scala... import scala.{Option=&gt;_, None=&gt;_, Some=&gt;_} defined trait Option &lt;console&gt;:15: error: not found: type A def map[B](f: A =&gt; B): Option[B] = this match { ^ &lt;console&gt;:16: error: pattern type is incompatible with expected type; found : None.type required: type case None =&gt; None ^ &lt;console&gt;:16: error: type mismatch; found : None.type required: Option[B] case None =&gt; None ^ &lt;console&gt;:17: error: constructor cannot be instantiated to expected type; found : Some[A] required: type case Some(a) =&gt; Some(f(a)) ^ &lt;console&gt;:17: error: type mismatch; found : Some[B] required: Option[B] case Some(a) =&gt; Some(f(a)) ^ defined class Some defined object None scala&gt; &lt; 
Isn't OP's approach is better though because it doesn't incur any runtime cost? 
I think [Funnel](http://verizon.github.io/funnel/) is a well-architected scalaz-stream-based project, if I do say so myself. :-)
Doobie. Seriously, see source code at github: https://github.com/tpolecat/doobie
Seconded. Rob isn't just a great coder, he's a nice guy too.
Using Akka-http with [Sangria](https://github.com/sangria-graphql/sangria) in our GraphQL api-service. 
I've read a few of de Goes's blogs on free monads + applicatives, and found them quite interesting. It seems to be the such a new development, though, that I wonder if it could be considered a best practice.
wow. props to him for using tempting to support both scalaz and cats. 
In what cases would you use repl? I also really like the worksheets, but I never found repls useful (haven't tried much though) 
As the reply on stack overflow suggested it, I ended up implementing cats in my lib, using applicative typeclass for the Reader and ContravariantCartesian for the Writer. Therefore it is possible to compose easily Reader - Writer. 
You've said what you want but not what you're offering. I might theoretically be interested in this if the money was there, but I doubt it would be (my present range is probably beyond what there is in web work, and I'd want to stay in my current high-cost-of-living city), and it's not worth my while pursuing if our expectations are completely out of line with each other. (My point isn't about me specifically; my point is that as the one posting it's incumbent on you to give at least some broad guidance about what kind of compensation you're offering).
Surround the sealed trait and it's subtypes with curly-brackets. { sealed trait Option[+A] { def map[B](f: A =&gt; B): Option[B] = this match { case None =&gt; None case Some(a) =&gt; Some(f(a)) } } case class Some[+A](get: A) extends Option[A] case object None extends Option[Nothing] } *Per @kevin_meredith you will also need to hide None and Some.
changes: https://www.scala-js.org/news/2016/12/21/announcing-scalajs-0.6.14/
I've found that you still need to have a `:paste` line at the start of your `:load`ed file, at least with 2.11.8. Have you tried that?
Yes, the difference is now @ScalaJSDefined traits can have concrete members equal to `js.undefined` so scala doesn't force you to specify all the empty fields when you create an instance of that trait.
I think it just desugars to sth similar to Either, the union types in scala dot are way more powerfull
Yes, see [Union.scala](https://github.com/scala-js/scala-js/blob/7640e858efbf8dbf52c99ca8a82cfb2350836e02/library/src/main/scala/scala/scalajs/js/Union.scala) for it's encoding. It's simply casting using implicit evidence which works well enough on javascript, but this encoding wouldn't work on the jvm.
It's not impossible to have unboxed unions now. Miles Sabin demonstrated [this](https://milessabin.com/blog/2011/06/09/scala-union-types-curry-howard/) almost half a decade ago. Mile's encoding comes with the disadvantage of having to pattern match on the union values, and all the runtime pitfalls of performing type tests while pattern matching(though if you were stubborn about it, you could attempt to address that with shapeless Typeable constraints). The scala.js encoding of union types is even more naive than Miles', it just attempts to cast with implicit evidence to one of members of the union. This works on javascript, because if two types implement a method called `def bar(): Baz`, if the compiler resolves the the left or right side first, the encoding in javascript is the same, it get's turned into a call `obj.bar()`either way.
Quasar is the most advanced Scala codebase I know of. It's very interesting and the spinoff project [Matryoshka](https://github.com/slamdata/matryoshka) is amazing.
Actually the encoding can also work on the JVM if you replace `sealed trait |` (+ some Scala.js compiler magic) by a `package object union { type |[A, B] }`, i.e., an uninterpreted higher-kinded abstract type member.
Miles Sabin's idea was a neat trick. However, for the purposes of Scala.js it is not acceptable, because it requires methods taking union types to have an additional `implicit` evidence. That would not work if said method is actually a JavaScript method (which is usually the whole point of union types of Scala.js, for JS interop), because that would give an incomprehensible argument to JavaScript. My implementation of `|` does not suffer from this issue. It relies on an implicit *conversion* from `Foo` to `Foo | A`, which happens at call site, in Scala code. See my longer answer on the encoding for details. Also, my implementation is 100% free at run-time, because the optimizer is able to completely inline and dce away the implicit conversion and its evidences.
I like Finagle's architecture. They took the basic idea of an asynchronous function (`A =&gt; Future[B]`) and built some really cool web service/communications machinery out of that. Oh, and it's the stack that runs Twitter.
I built something like [this](https://gist.github.com/dispalt/9c7449bfdf4bc2d41c31248b722daf68) which is a more fluent style, which helps with some of the boilerplate that jsoptionbuilder has.
Java forces you most of the time to violate the [principle of least power](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html). In Java mutability is the norm. Of course you can avoid it but immutable programming is so tedious there (lots of boilerplate, standard library with mutation as default, etc) that it does not worth it. Besides, a poorer language does not make its code easier to read. Scala has indeed more advanced concepts than Java but it enables to design APIs and abstractions that are closer to the domain language. Saying *what* you do, in domain terms, is more meaningful than saying *how* you do it. Let's take an example: Java also has more advanced concepts than C: OOP, generics, the VM, etc. Would you say that a (large) piece of C code is easier to understand than some Java one? It's no luck if OOP had such a large success. It often makes development easier, at the cost of learning a few things. This is also true for Scala. Teams moving away from Scala are often ones not willing to learn the language. Moving to Scala but coding like in Java/Go makes no sense. So i'm not surprised such teams get disappointed. They approach was flawed at its roots.
Maybe not strictly on-topic, but thought this was a nice write-up giving a charitable explanation of why someone might want to use a dynamically typed language, from the perspective of a Scala developer.
I agree, and as a long-time Lisper/Schemer, I can also try to answer any questions anyone might have about that. (No, really! My name is in the acknowledgements of "Paradigms of Artificial Intellgence Programming: Case Studies in Common Lisp" and "The Little Prover"! I wasn't always like this!)
"Come to the dark side, we have types" kind of thing?
never professionaly
Yeah. I'd say the closest I've found in spirit is probably [ML for the Working Programmer](http://amzn.to/2i089FH), which is a bit mis-titled, because I don't know how many "working programmers" want to end up developing an automated proof assistant. So I tend to think of the book as kind of Paulson's recapitulation of the history of ML's development, which took place in the context of developing the LCF family of proof assistants, kind of like PAIP is Norvig's reconstruction of "classical" AI development, which ends up teaching us quite a bit about how to use Common Lisp effectively in the process.
very vague..... Just like his previous job ads on other forums.
"You know the type" is a different bar than "You have proven the type to the compiler within the constraints of the language's type system". You can know the type by type annotations like JSDoc, simply remembering it or most commonly – being able to quickly recognize the expected or resulting type from the code in the immediate vicinity. As someone with a lot of experience with dynamic languages, the tradeoffs the author mentions make total sense to me. It's sometimes easier to prove the type to yourself than to the compiler since your brain supports more flexible type definitions than the compiler, but on the flip side you lose when you're dealing with other people's ideas of types. And refactors are a pain, yes. I personally am tired of dynamic languages for standard reasons so I've started on Scala. So far I think the effort wasted on proving myself to the compiler is mostly my inexperience with it, but I don't think it'll ever go away completely. Still worth it, but I can definitely see some people preferring dynamic typing for some projects.
Can you give an example? I personally, have never come across a scenario where I have felt i was proving something to the compiler for the sake of it.
Why would a welder want to spend time explaining how rivets are a decent fastening system? I'll grand welding is probably the future for most large scale construction projects, but rivets have a place too. Beyond that, many smaller construction projects, especially maritime applications, still chose rivets, and welders can benefit (even if only in how better to evangelize welding) from understanding why the people building those projects make the choice they do. 
Advertising is about showing exactly what you want to show and nothing more. If nothing here appeals to you, move along. This is probably not for you.
If you need to make a living, the only languages that "pan out" are Java, C, C++ C#, Javascript and PHP. 
This just means you would make a lot more money by knowing some of these mainstream languages. 
Yeah, the profusion of ScalaTest styles is a bit of a pain. Personally I like http://doc.scalatest.org/3.0.1/#org.scalatest.refspec.RefSpec the best because of its simplicity--no new methods to learn for organising tests, just objects and defs. The downside is it's JVM-only.
I haven't used a single one of these in my professional career. Languages I have been paid to write are SAS, SQL (seriously–a 50kloc codebase entirely in SQL; terrible idea), Python, and Scala. 
You can map your eclipse keys to IntelliJ if you want. I would strongly advice against it though. C = Ctrl/Cmd C-Shift-O =&gt; search file names C-Shift-F =&gt; text search Ctrl-Alt-O =&gt; fix imports Alt-Return =&gt; import suggestions.
Right okay. Yeah, that happens a lot to me. But that's not reslly an issue in my eyes. I thought you had valid programs that you could not represent to the type system correctly.
it's not really a huge issue imo, but it seems like one area where that complaint holds up
I think a lens is the least powerful tool to generalize this function and I deliberately chose the way that is closer to structural typing, giving you less boilerplate and preventing passing well-typed but unexpected things like `lens[User] &gt;&gt; 'email`. My impression was that `LabelledGeneric` is a fit for operations that require all (or at least a significant or unknown subset of) fields with their names, and I didn't find a way to express "a record with a string named `external_id`" that would be easier than this lens requirement.
Show type (Ctrl-Shift-P) Show implicit parameters (I think Cmd-Shift-P?) Show implicit conversions (Ctrl-Q?) Find symbol (Cmd-Option-O) Uncertain on some of these because I'm not in front of my computer and muscle memory knows better than I do. These are all invaluable for Scala development, though. 
You do sound like an ass. Scala is just another language, you haven't accomplished something grand because you learnt it.
I mapped show implicits to a hot key and that coupled with tweaking the syntax highlighting to differentiate objects, traits, method invocations, local variables, mutated locals, statice, and underlining implicitly converted variables, really made it easier to see when scala implicit magic was goin on 
It was beneficial for both of us :) Didn't know `LabelledGeneric` works for non-case classes. There's still a lot for me to explore.
Extract method is really cool!
I thought we were talking about design intent? If you're asking what's used in practice I've seen very successful large systems in Python, and one in TCL.
It's worth mentioning that you don't necessarily need to implement the shapeless portion of this example yourself. Existing Lens libraries like Monocle already have functionality to generate lenses with with shapeless/macros.
yeah thanks for the advice gonna learn the intellij shortcuts 
&gt; Ctrl + Shift + P ==&gt; show type of the selection `alt + =` on linux. wonderful feature to have https://www.jetbrains.com/help/idea/2016.2/working-with-scala-show-type-info-action.html
Interesting! So I could have an implicit TaskHelper class that actually uses the Repository, takes in a task, and define an isComplete method for it?
Do you have any suggestions for learning to think in terms of Scala's type system, rather than trying to shoehorn in Java's approach everywhere? In other words, how were you able to make this leap? 
What does it do for functions not returning ()?
I find it useful to think in terms of "inherent" behavior and "associated" behavior. Inherent behavior is something you ask the object to do for you. Associated behavior is something you do to the object. For instance, using member methods of an object correlates with inherent behavior. Using a non-member function and passing an object as a parameter correlates with associated behavior. Many things you can express with inherent behavior can also be expressed with associated behavior and vice versa, to the point where some languages have a "universal function call syntax" where calling `a.b()` is precisely the same as `b(a)`. With this definition, inherent behavior is very much like how Java/C# want you to do object-oriented programming. Associated behavior is much more like how Haskell/Rust want you to do functional programming. Naturally each approach has their trade-offs. Scala is somewhat unique in that it supports both models and that has its own set of trade-offs. So I view the leap that Mimshot made as first identifying the behaviors that needed abstraction, then expressing those behaviors in an associated way rather than an inherent way. Maybe it'll be clearer with code and going in steps from an inherent approach to an associated one. To start, the behaviors that need abstracting are 1) get id to hash (so grouping can occur), and 2) replace the original id with the hashed version. Let's have a trait with those behaviors trait IdentifiedReplacement[T &lt;: IdentifiedReplacement[T]] { def id: Int def withId(id: Int): T } and make the case class implement them case class MyCaseClass(externalId: Int) extends IdentifiedReplacement[MyCaseClass] { override def id: Int = externalId override def withId(id: Int): MyCaseClass = copy(externalId = id) } Then you can implement the hashing function in terms of this trait def hashExternalIds[A &lt;: IdentifiedReplacement[A]](rdd: RDD[A]) = { rdd.groupBy(_.id).flatMap { case (id, obs) =&gt; val h = hash(id) obs.map(_ withId h) } } This works. It's very Java-like and inheritance based. It asks the objects to give you their id and it asks the objects to replace the id with a hash. But what if you had a lot of classes you had to change to inherit from this new interface? Or what if the classes were in a library you couldn't or just plain didn't want change? Instead of asking the object, "give me your id", you can instead say "here's how I get an id from this object". Instead of asking the object, "update your hash", you can instead say "here's how I update this object's hash". Instead of giving the type its own inherent behavior, you can associate the behavior to the type. In code, it looks pretty similar, except objects are passed explicitly. trait IdentifiedReplacement[T] { def id(t: T): Int def withId(t: T, id: Int): T } and you don't inherit, you define the behavior as a value for that type. val ccir = new IdentifiedReplacement[MyCaseClass] { override def id(cc: MyCaseClass): Int = cc.externalId override def withId(cc: MyCaseClass, id: Int): MyCaseClass = cc.copy(externalId = h) } Then the hashing function uses that value, passing in the objects it expects, gaining the desired behavior through the association def hashExternalIds[A](rdd: RDD[A], ir: IdentifiedReplacement[A]) = { rdd.groupBy(ir.id).flatMap { case (id, obs) =&gt; val h = hash(id) obs.map(a =&gt; ir.withHash(a, h)) } } But now the signature of the method is more complicated. In Scala we can clean that up by letting the associated behavior be implicitly discovered by the compiler. So we make the behavior implicit implicit val ccir = ... and make the hash function expect it implicitly def hashExternalIds[A](rdd: RDD[A])(implicit ir: IdentifiedReplacement[A]) = ... and we can use some syntax sugar and `implicitly` to express the same thing def hashExternalIds[A: IdentifiedReplacement](rdd: RDD[A])) = { val ir = implicitly[IdentifiedReplacement[A]] // rest as before } The location of the implicit value doesn't matter too much so long as the compiler can find it. Mimshot put it in the companion object and I would suggest doing the same. Anyway, now this is pretty close to what Mimshot had. To fully get to Mimshot's version requires only two more ideas. The first idea is that "getting an id" and "replacing a hash" are two separate, unrelated behaviors, so they should at least be split into separate traits. trait Identifiable[T] { def id(t: T): Int } trait IdentifiedReplacement[T] { def withId(t: T, id: Int): T } The second idea is that you do not actually need to replace anything. You don't have to update the objects inherent data. You can associate the hash data to the original object just as we associated the behavior. So it turns out there's no need for a second trait at all, you just use a case class. case class Identified[A](id: Int, a: A) // id now associated with an instance of A def hashExternalIds[A: Identifiable](rdd: RDD[A])) = { val identified = implicitly[Identifiable[A]] rdd.groupBy(identified.id).flatMap { case (id, obs) =&gt; val h = hash(id) obs.map(Identified(h, _)) } } And now we're identical to Mimshot's implementation! Everything is cleanly separated. How you get an id for a type is defined in exactly one place. You didn't have to change any classes to get new behavior. You don't have to lose any of the original data. And it's very extensible; you can even write another `Identifiable` instance for the same type to get a totally different set of groupings while still using the same hash method. Of course you can do the same treatment to the hashing function, or add a function which creates `Identified` to create something else. In Scala, this method of defining associated behavior and finding it implicitly is called the "typeclass pattern", and you can learn a lot more by Googling that :P Edit: typos, clarity, and corrections
Looks like the problem affects more stuff (see my edit). :( I mostly wanted to know if this is expected behavior. At this point, it doesn't seem like it is. scala&gt; (0 until 2).par.foreach(Nil) java.lang.IndexOutOfBoundsException: 0 scala&gt; (0 until 2).par.foreach(x =&gt; List.apply(x)) (hangs) scala&gt; (0 until 2).par.foreach(_ * 2) (hangs) 
Looks like this is a known problem, with a workaround. https://issues.scala-lang.org/browse/SI-8119 Possibly related: https://issues.scala-lang.org/browse/SI-8955
&gt; People do bother, even in languages with deficient type systems. Do they e.g. have a "no `String` rule"? (all uses of exposed `String`s rather than a business-specific type must be justified). That's the sort of thing you need to make the type system actually work for you, and in C++ or Java it's a lot of overhead. &gt; Scala in particular would be a nightmare because it encourages "cleverness", which is a liability when you employ 30000 engineers many of whom do want to appear clever at the expense of everything else. Shrug. I work on a scala codebase with around 500 people. We make it work, and the benefits outweigh the costs. It requires careful standardisation and code review, but I think a codebase that size with that many people always would, in any language.
"Alt + Equals" is extremely helpful for me to see the type of whatever is under your cursor or selected.
Don't really understand your question. The Task is a model with business logic. Shouldn't have any relation to Futures
I'm not referring nor implying anything about scalaz. Never used it.
This is great!! I really appreciate being able to read the thought process here. I wish there were more of these step-by-step thought exercises that "arrive" at the Scala features and how they solve the problem at hand. Thank you so much for taking the time to write this! 
It seems in this context, a task is a literal task - wash the dishes, buy eggs and milk - not a future. 
Man, I'd really like to develop some back-end services for this application, though I'm not really a math/ML person, more like a well-rounded web dev.
Really sad to see the reformat on compile functionality getting dropped from the SBT plugin, it was a nice step to have "for free" in our workflow.
I copy pasted the gist into my project folder (changed the package name and added the plugin import in the process) and it seemed to hang infinitely and never compile. Our project is a multi module project which may or may not have something to do with it. I'll did a little deeper next week sometime, but I'm no SBT expert myself.
This handles the empty files case and doesn't issue the deprecation warnings: https://gist.github.com/pettyjamesm/ed6351ac2054a76ee6c223e216804298 The output is really ugly though. The calls to runMain pollute the SBT output with all the file paths and then the percentage formatting hijacks the SBT stream. edit: Spoke too soon, it doesn't seem to be working across all my projects uniformly. It might be my build.sbt though. I'll spend real time on this next week, sorry for the noise on reddit instead of Github!
Cool! Thanks for investigating this. I'm happy to accommodate changes in the cli, for example adding a --quiet flag to shut down the console output. Yes, please follow up with an issue on Github and bring it up on Gitter. I'm sure other users want to pitch in.
This is a great question. I had trouble with how to think about eager loading and collections for years, and it took a while to realize that the ORM / DRM persistence model really doesn't give you good guidance about how to model your domain effectively. What follows is IMHO, but I think talking about eager loading / lazy loading really gets down to where control lies in domain objects conceptually, and how those map to the implementation in persistence frameworks. So, you only use collections in persistence frameworks when you know you have total control over all the domain objects. In DDD terms, this means the persistence object is an entity that is an "aggregate root", so that any changes to the collection are entirely bounded and defined by their relation to the aggregate root, and the transaction boundary is around the aggregate root. So this means if you've got a reference to an entity (i.e. `Issue` or `Version`) that can be outside of the aggregate root, you're not safe -- updating a project doesn't mean that you're updating all of the issues inside of it. Instead, you've got a project which has a set of issue ids. If you call `project.issues` then that runs a query against the repository for those issues, but the project's list of issue ids doesn't pull in the issues immediately. If the aggregate root contains value objects or entirely bounded entities then you can / should use eager loading, because any transactional change to that aggregate root will also be a change to its child entities -- it's all or nothing every time. Vaughn Vernon is great on this, especially his "Effective Aggregate Design" series: * https://vaughnvernon.co/?p=879 * https://vaughnvernon.co/?p=838
Thanks. Removing N+1 at compile time, now that's something.
@korsupporter @fromscalatohaskell I'm going through the same exercise now! Besides the scala.js resources mentioned above, did you come across any helpful, say, javascript resources (eg ones that avoid having to comprehend and then discard javascript syntax; or has only the syntax you need for scala.js) ? I'm having trouble understanding all the new terms : html, dom, and 'other' namespaces, libraryDependencies, document.getElementById javascript API ...
Thank you, it worked! And thanks for the links!
Reading through the source for Spray and Shapeless until I perfectly understood 90% of spray and 70% of shapeless.
Yep, and the scala/scala gitter channel is also very useful. Watch conversations go by and ask questions.
I finally understand monad, functors stuff watching this great video: https://www.youtube.com/watch?v=3Ycp55QEbGM&amp;t=898s&amp;list=LLXNPKwlcFqzKr9GK9X7GBgg&amp;index=37
Pretty badly I a 16 gb macbook pro and Intellij + SBT + chrome (maybe a dozen tabs) is enough to bring me to borderline swapping; sometimes swapping small amounts, sometimes not. It's not super overloaded, but there certainly isn't space to put much else (windows VM, android emulator, ...) on that machine without going into full swap. Haven't exactly profiled where all the memory is going, but IntelliJ easily takes 2-3 gb (that's after assigning it 1.5gb of heap), SBT another 1-2gb, whatever process I'm running a few more gigs, and chrome similarly. I know that doesn't add up to 16gb but somehow it still ends up with a non-zero "Swap Used" in the activity monitor, and when that number rises the machine gets noticeably more sluggish. Not sure if there's a solution for this that I just haven't seen
Ubuntu acer e5 laptop with i7/16GB: My system is running fore more than a week, I've two projects open in Intellij(~800MB) with two SBT instances(x2=~1GB), 1 plugin-bloated firefox(~800MB) with a few tabs and a few other apps. The battery life is OK: intellij can compete in energy consumption with activities like watching a movie. Listening to music or watching a series used to drain more energy than the development. The temperature used to be ~45C. The CPU(i7, 2.3-3.3GHZ/core) usage is ~7% while editing, 15-30% on search and similar stuff. Usually, SBT's memory consumption is based on how large is your project and the CPU usage depends on how long is the compilation. Better modularization = shorter compilation and longer battery life ;) Common Tips: Use [low-voltage memory(DDR3L)](http://www.dell.com/support/article/hu/hu/hubsdt1/SLN153768/en). Avoid the usage of swap(pagefile) memory. Use integrated GPU instead of the dedicated one(intel energy consumption &lt;&lt;&lt; nvidia energy consumption). Btw, why do you need 6 hours long battery-only-work? Do you travel that much or there is no socket for your laptop?
Do you have all the fancy autocomplete/inspections features turned on? 7% seems very low. Type-Checking and autocomplete drives the CPU meter up to 80% on all cores every minute or so. In between it is at 3-10%. &gt; Btw, why do you need 6 hours long battery-only-work? Do you travel that much or there is no socket for your laptop? I have to carry a couple of things with me, so I have to minimize on weight where possible.
Yeah I really don't know what's causing it to eat up so much and start eating into swap. One of these days I'll have to dig into it properly to see if I can make it stop doing that, but for now that's just the way it is for me
&gt; Type-Checking and autocomplete drives the CPU meter up to 80% on all cores every minute or so. Wow! You should try "File -&gt; Invalidate Caches and Restart". If this doesn't work then search for another solution or file a bug report because 80% is way too much. I've tried the auto-completion in my two open projects but the max CPU was 25%. The average is below 10%. If you want to minimize the weight and this issue won't be resolved then you can find lightweight chargers with ~70% less weight/size which fit in your pockets.
Dell Precision M4700 with i7 Extreme, 2 X SSD, and 32GB. I have a ramdisk setup for all Scala development (i.e. both Scala IDE and SBT share same `/tmp/sbt` compile target). Battery life is already terrible (2 hours at best) with this laptop so I don't bother doing Scala compilation without AC power source. All CPU cores are enagaged during builds, and overall compile times are vastly improved (compared to 2.9/2.10 days). Incremental builds seem to have taken a step backward in sbt 0.13.13 (I think) as even a `println` within a method without affecting return type can incur a 4-6 second recompile. Anyway, the Java server process itself is a hungry demanding beast, so firing up several sbt sessions in separate terminals and expecting great battery life is...very optimistic. Also, even when sitting idle GC will periodically kick in, which generates heat, which clicks on the laptop fans, which in turn fires up the annoyance neurons in the grey matter. The end result: furrowed-brow development syndrome. Good luck and god speed to the future
Yes I tried it. it loads without errors, but its not loading the file. For example, if I change the function name to mapi then that function name is not visible in the REPL. also I tried keeping the function name as map and changing the output of case Some(x) =&gt; None but still its uses the default REPL map. (Some(3).map(_*2) gives 6 instead of None) by the way I'm using 2.12.1
Not available for the position (Living in Australia), just curious if you've played around at all yet with Scala on android? I've only ever heard people have a hard time to get it to run so might be worth looking into whether it's worth it for a single language code base?
The reason I said (I think) is because it could also be the switch from Play 2.4 to 2.5 and/or refactoring of monolithic code base into separate libraries. I tend to only follow class per source file for the model (i.e. database tables to case class mappings) and other standalone code. You could be right, splitting out the old monolitchic Play app into separate libraries has led to, for example, controllers depending on several layers of helper traits. IOW, the compiler probably has a lot more work to do now (though overall build times are shorter due to client app being much leaner).
So with the brace on the same line it gives exactly the same error? Or a different error?
A friend of mine has done shapeless workshops and so he prepared exercises for many of concepts mentioned in the article: https://github.com/krzemin/scalawave-typelevel-workshop My workshops on monads were less popular on GitHub/Twitter, eh...
Honestly, the Coursera Course [Functional Programming Principles](https://www.coursera.org/learn/progfun1) in Scala" (progfun). Watching the lectures and doing the exercises got some things to click for me.
When Martin odersky admitted they got carried away with all the symbolic operators i finally understood it was a mistake 
Incidentally, I've realised that you're correct that scala.meta provides better macros, which I'm going to try now: http://scalameta.org/tutorial/#Part4-Macroannotations
Ctrl-Alt-P -- Extract Parameter. E.g. given `def m(x: Int) = x + 2`, you could select the `2` and get a dialog box that lets you turn it into `def m(x: Int, y: Int) = x + y` (simplistic example for illustration purposes). Ctrl-F6 -- Change signature, opens a dialog box that lets you modify the parameters etc. Both support multiple parameter lists. Ctrl-shift-F12 - maximize editor I made my own shortcuts for splitting tabs -- alt-[.] move right, alt-[,] move down, alt-shift-[,] move to opposite group (for unsplitting)
Apparently it forgets implicitness though :/
2014 MBP with 16 Gb ram and an SSD (encrypted). I use EAP Intellij and EAP scala plugin. I give Intellij 1 Gb, and the scala compiler 1.5Gb. Current project is around 140,000 LOC over a dozen SBT sub-modules. Sub-modules help to improve build times since independent modules can compile in parallel. Depending on how much of the project gets recompiled on changes, I get anything from 2 to 4 hours of battery life. During compiles all 8 cores are busy and the fans will spin up but stop a minute after the build stops. CPU usage during editing is minimal, a few %. Intellij's type-aware highlighting is getting pretty good now, it still shows some errors for invalid code, almost always for scalaz or cats syntax, never for simpler code. The intellij scala team are responsive to bugs and if you can give small examples they will address highlighting issues.
Oh, I wish I could do scala w/o an IDE... I'm too reliant on those red squigglies
After trying to slog my way through a lot of the reference material out there, I found [twitter's scala school](https://twitter.github.io/scala_school/) refreshingly to the point
IntelliJ's Low Power Mode might help you.
That's because an IDE wasn't used to type that comment.
That was like awakening http://danielwestheide.com/scala/neophytes.html
I can't handle snowflakes in front of the text of an article I'm trying to read.
Actors can be distributed on multiple applications/servers, futures by itself can't. 
Got it. But is there any advantage if I have low load and don't need a cluster?
I agree with both of your points (I could add a few more though, but I'm dont want to do that here). I believe akka is a very specific tool for limited amount of problems. On contrary, it is marketed, and perceived, as some general thing and programming pattern that solves everything. 1) That is absolutely right way to go about it, thread pool + futures. 2) Also you're supposed to create actor hierarchies, therefor your service should not receive actor as ctor argument, but they should create and handle their children, etc., which only increase the pain poins of testability. I have not found a good way to test actors. Generally I'd suggest you avoid using akka until you absolutely must (that will be rare), and then isolate it from application as much as possible, creating only thin akka layer (both vertically and horizontally thin ideally). Also for scenarios such: &gt; "get many events from kafka, aggregate and store". I suggest you check https://github.com/functional-streams-for-scala/fs2 and https://github.com/krasserm/streamz. I've found those to be much saner/resonable model for such machines.
Before I respond to your post, I'd like to say the following: Don't use Spray. Use Akka-HTTP. Spray is effectively obsolete. You want Akka-HTTP 10.x, which is just as fast and has Akka Streams built in. Streams uses Actors under the hood, but gives you an abstraction that lets you focus on the elements. If you want to use Akka testing, you need akka-testkit, which is fairly low level. For streams, you want akka-stream-testkit, which is easier. http://doc.akka.io/docs/akka/2.4.9/scala/stream/stream-testkit.html The example you're using is quite old as well -- for an example of an akka-http project using Guice and slick, see: https://github.com/kgoralski/akka-http-slick-guice Now to your points: You want to use Actors when you want to hold state, and you want to use them when you need to handle failure -- that is, an exception from several child actors that manage database operations can propagate up to a parent actor which acts as a supervisor and can determine more complex failures and define a failure hierarchy. This is more difficult to manage than Future, which does have recover / Try etc but doesn't have the same supervisor capabilities. Read Jamie Allen's book for more details. For a REST service with DB persistence layer, you should be using Slick with HikariDB, and size the thread pool according to http://slick.lightbend.com/doc/3.1.0/database.html#database-thread-pool -- you don't want to just throw 10 threads at a DB and hope for the best, you have to actually size it according to performance tests and work out the queueSize. If you want HTTP inside a dependency injection framework using Akka with a testing framework and Guice DI and no complicated bits unless you want them, then you want Play. Play is all of those things put together. * https://www.playframework.com/documentation/2.5.x/ScalaAkka * https://www.playframework.com/documentation/2.5.x/ScalaTestingYourApplication * https://www.playframework.com/documentation/2.5.x/ScalaTestingWithGuice Here are the example projects: * https://github.com/playframework/play-isolated-slick / https://example.lightbend.com/v1/download/play-isolated-slick * https://github.com/playframework/play-scala-intro / https://example.lightbend.com/v1/download/play-scala-intro * https://github.com/playframework/play-rest-api And there are more projects here: https://playframework.com/download We're aiming to integrate Akka-HTTP as the default HTTP engine for Play 2.6.x, and there's an experimental version of the engine in 2.5.x, but for now the core engine is handled by Netty, with Akka Streams are used internally to communicate with Netty. 
Oh yeah, I'm entirely aware that this is entirely different traits system. I wasn't particularly clear on this, but that's how a trait system would look in my ideal programming language. My complaint with Scala is that there are a lot of brilliant ideas in the language, but the syntactic machinery only brings us 80% of the way there. I'm hoping that Scala.Meta can get us a bit close to 100%.
Personal Comment: I actually think that stuff shouldn't be mocked unless it's really really needed. My Personal Guidance is: "mock until it works, test until it hurts." Actually I know there are groups that won't agree with me and would prefer a BDD style or more unit approach, however in a fast changing world I actually had the best luck by testing at the highest possible level and only test lower levels for specific stuff that barly changes. (P.S.: My comment would be equal to amazedballer)
&gt; Actors can be distributed on multiple applications/servers, futures by itself can't. Can you please explain why that is? 
Funny, to me the 2nd point is just the other way around. I find testing much easier with Akka than with Spring, and calling testing with Spring a 'pleasure' is honestly a much too bright view. The reason I love Akka, is not necessarily because of concurrency, but the way of thinking. Thinking in Actors (or Microservices in general) usually helps me much more than thinking in plain objects. The concurrency is just a nice addition in my opinion.
I encountered similar problems when starting out with akka, from my experience: 1) Using actors for everything instead of using them for what they are good for: a) maintaining a shared state b) remoting c) sophisticated failure handling. A DB connection pool is an use case that needs A (open connections) &amp; C (reconnections) if I were to implement my own I would probably use akka but given that this already exists (see hikariCP) then you don't need to reinvent the wheel. 2) Unnecessarily fancy dependency injection in actors: testing doesn't have to be a pain, forget about framework based DI, that's just baggage from Java world, put all your dependencies as constructor params and pass them through the props (see http://di-in-scala.github.io/) its that simple and straightforward, now you can test your singleton stateless services with plain old unit tests and test the stateful actors by injecting mocks, (also learn how to use the test kit cause it is very straight forward to set a sender when you tell a message actorRef.tell(senderActorRef, message) I think)
I agree with what a lot of was put here: https://www.reddit.com/r/scala/comments/5jdtvn/what_projects_do_you_think_represent_good_fp/
It sounds like scalac just followed a more traditional, simplistic approach. Java's fine with this approach because its type system is far more basic than scalas. The lazy completer is elegant in that it can push the concerns off until they're needed, letting the runtime solve "tieing the knot" for you -- in Haskell this works well, but it seems this doesn't optimize as well on the jvm. The other problem is that scala's type system is very complex, which, as noted in the article, creates greater opportunities for manual optimizations (like bundling io ops). 
Thank you, I will make a sacrifice to the JVM tuning gods in your honor. 
Thanks! I need to pay better attention :-) 
Very interesting, thank you. It means completely change the way I think about actors. Do you think that it is more readable that "old java DI way"?
I doubt SBT will have such functionality built in. I would look at an OS-specific way to limit or deprioritize it instead (e.g. CPU quotas) - you might have more luck asking about that in the subreddit for your OS.
This book was excellent at introducing architectural patterns in FP. https://www.manning.com/books/functional-and-reactive-domain-modeling It shows some examples with Kleisli and dabbles with the free monad. I personally find the free monad the cleanest pattern for non trivial FP programs. There are a bunch of awesome talks on it by runar and a few others from some of the recent conferences (it's all on YouTube).
Thanks. I got this book and will explore. 
Yes I think its more readable. Using DI normally means you are adding a layer of indirection in your code, and you only know what really is being used if you look at the bindings you've in your DI framework. Whereas with that approach the dependencies are always explicitly declared and your code is easy to read. I challenge you to read some code you have written a long time ago that uses DI and see how easy it is to understand. Better yet go read someone else's code that uses DI and see how easy it is for you to understand it.
What application does this post have to https://www.reddit.com/r/scala/comments/5gmwgh/fast_scala_compilation_for_everyone/? 
The main value is controlled concurrent access to a state machine, which handles operations in a linear thread safe manor. The life cycle management gives you a simple way to decide what to do with that state should an error happen. If something has state and needs to handle concurrent operations from multiple sources, it's a win. This also lends itself well to working with devices and sockets, which naturally have their own state that generally fits the actor life cycle well. For simple asynchronous map/reduce/gather type of operations, use futures.
I'd recommend grouping by functionality rather than by layer, though I think this is good practice in Java too. Typeclasses make it more practical to put the parts that relate to a particular business type with that type (e.g. the database-related part can go in a typeclass instance), or if you really want provable decoupling you can detach the typeclass instances from both. http://stackoverflow.com/a/26488576 sums up how I set up my project structure and dependency management.
Nothing, the linked thread's project seems to be about speeding up existing scalac/dotc via distributed compilation over a number of servers. Kentucky Mule, if fully implemented, would be a from-the-ground-up high performance Scala and/or Dotty compiler. Ship has sailed methinks, Dotty's already evolved to the point where rewriting it would set the project back a couple of years, and forget about rewriting the Scala compiler since Dotty is its presumed sucessor. Maybe Lightbend and EPFL teams working on respective compilers can benefit from the Kentucky Mule approach in some way. Cool project regardless...
Want to share a small observation. I was trying out some *shapeless* and IDEA didn't seem to like it until I [found a way](https://confluence.jetbrains.com/display/IntelliJIDEA/Working+with+Type-Aware+Highlighting) to make it [shut up](http://i.imgur.com/rDu2RqL.png) about that particular piece of code only. Which is nice since I use IDE features like suggestions &amp; go-to-library-sources very often while I'm still learning, but it's irritating that I have to bear not using `sequenceU` and the likes to avoid IDE squiggly-lining my code.
I think you could try a different dev workflow and have things speed up a bit. I'm strictly using Vim with the Conque plugin and I use this thing called Ammonite for REPL/autocompletion. ;) No lag, things go very fast. 
There are a ton of abstract things in your post that don't nessecary have a scala counterpart. What do you mean exactly by * Resource * client * repository layer * persitance layer
&gt; like bundling io ops I think Haskell is actually able to bundle IO types by default, and it does this because it tracks side effects through its type system. They call it IO fusion iirc
Ya, being a lazy language allows them to defer the execution until the action is merged into the main IO monad. 
If it's legit then I wouldn't mind throwing them a couple bucks. Good programming books are **very** hard to come by IMHO.
The intellij devs are pretty responsive about fixing these issues. The latest EAP is getting much better with fewer false errors, still more to do though but its getting there.
On linux or osx you can `nice` or `renice` the process to reduce its CPU usage.
Have you looked at Lagom?
Not really no. What does Lagom do really? 
Wow ok so that's actually spot on to what I want / need. There's an unfortunate detail here though. I need to support a few Databases that actually aren't on this list. So I guess my first thought would be to see if I can extract what they've done here and adapt it. 
So actually the more I think about this the more it applies to anything that "writes" or anything that "reads". In this scenario I could write to Kafka using the same interfaces. Or I could write to a logger... That's not necessarily the focus but it's just an idea that might apply here.
Do you have any sense of how fast a complete scala compiler written with this architecture would be?
It's open source. Instead of extracting what they've done, why not become a contributor?
I tend to replace tests with use of the type system where possible - I keep coming back to https://spin.atomicobject.com/2014/12/09/typed-language-tdd-part1/ , and the techniques described there are much more lightweight to apply in Scala where you have `case class`es, `apply` methods, and the very lightweight `require` syntax when you have to fall back to a runtime check. It's correct to want all your logic to be thoroughly tested. But something like `Free` makes that easier, and lets you avoid tests that don't test anything, by having a clear separation between logic and values: your values are just values, no need to test those because they don't do anything. You don't need to test `runFC` because it's a library function that's tested in the library. What you do have to test is your own `~&gt;` interpreter - and it's easy to test that because it's just a function, so you can just apply it to a value and check that the result is as it should be.
This sounds like the kind of thing the Free monad is great at. You create a command language, and then your expression of commands is completely decoupled from their execution - you can have multiple interpreters for the same commands and even swap them out at runtime. http://michaelxavier.net/posts/2014-04-27-Cool-Idea-Free-Monads-for-Testing-Redis-Calls.html is a simple example if you can read Haskell.
[State monad](http://typelevel.org/cats/datatypes/state.html)?
I have yet to find a useful version of this kind of diagram, and I have found places that used/expected them to be terrible places to work because the diagrams are simply the wrong abstraction and lead to suggesting things that don't make any sense. This probably isn't very constructive but it's the only answer I can give.
I used to think unit tests were a waste of time and believed integration tests were the most efficient way of proving code worked. I found my unit tests were always fragile and felt like they were just extra work that didn't provide a lot of value. I've changed my mind a bit recently. I think the biggest cause of fragile unit tests has to do with mixing two types of code: glue code and domain logic. - I call any code that is wiring together different parts of the application "glue" code. Like call this service, combine the result with the result of another service, and return to caller. - I call domain logic something like, given a list of objects, lets say accounts, remove all of the accounts that don't have admin privileges and return to caller (as an example). The difference being domain logic does not have any dependency on any other part of the application. I think where a lot of unit tests break down and become fagile is when you test something that combines these two, e.g.: This service retrieves accounts from the database *and* filters non-admin users. So, you end up mocking the thing that retrieves the accounts so you can test the logic bit. Then, when you change the class that retrieves the accounts, you may end up with breaking tests on something that is just asserting that only non-admin accounts are getting filtered out. That unit test is doing too much and at that point I'd almost rather just delete it. I'm trying my best these days to separate these types of code instead, because when you're not setting up the glue in order to test the logic, the test has a smaller scope and is much easier to maintain. With **Scala**, the idea of a "pure" function has helped me out a bit in this regard, because by following the "pure" methodology you end up with code that separates glue code and domain logic. To compare, before hand maybe my method retrieved and filtered, but now, I have one method that retrieves and a pure function that filters. Testing the pure function is usually a piece of cake, and I only have to change it if I change the filtering rules. So, I like to usually separate these types of code into their own classes. Have a bunch of pure functions on value/domain objects etc, and then put all the glue code into services. I hope this example made sense. It's not unique to Scala but the functional style that I was introduced to via Scala hammered this home for me. You can apply this to all sorts of applications. I guess trying to learn idiomatic Scala is what helped me. Edit: And lemme say, after I had tests like this I fucking loved having them and really started to see their value, because I can refactor and reorganize the glue code bits at will without worrying about the logic bits, which usually leads to an application structure I'm much happier with that is easier to maintain and extend. (I just don't test the glue code, if that is broken you usually end up with an application that doesn't start, not some crazy bug that's hard to find like if the domain logic is broken). 
I know, and it makes sense. However, we cannot afford having MapReduce framework now. There is a high cost associated to with Hadoop maintenance. On the other hand, we are actually no processing files, it just an example of our real problem, we are actually streaming the data in using akka from different sources including Kafka. 
UML is a language to describe an object-oriented system. Most of the diagrams are around the behaviour of collaborating objects. If you would be using Scala for an imperative oo system than you can just do the same as you did with java. However I think you are looking for a more fp or hybrid approach and then UML being an oo-modeling language won't fit. 
Consider recommending [sequence diagrams](https://en.wikipedia.org/wiki/Sequence_diagram) and/or [communication diagrams](https://en.wikipedia.org/wiki/Communication_diagram) instead, and be prepared to explain why class diagrams aren't very useful in Scala. If you want to go farther and explain why UML isn't very useful, that's up to you (and recommended by me).
I don't consider those worth it. If I can go to the trouble of thinking up a general property, I can probably also encode that property in the type system and then I won't bother with a unit test at all. The cases where I test are precisely the cases where I can't figure out or be bothered with the generalization and just want to sanity-check with a couple of examples.
Thanks! This is almost exactly what I built for my use case.
&gt; be prepared to explain why class diagrams aren't very useful in Scala. What is your explanation, please, Paul, as to why class diagrams aren't useful in Scala? Perhaps because Functional Programming frowns upon inheritance?
&gt; what is the best way to visualize function invocations, monads and so on types.
Wow a lot here. Thanks for the info. Cheers.
I'd argue they aren't useful in Java, either, actually. What do they capture? A static inheritance relationship. I can't remember any occasion on which there was useful, actionable information I got from staring at that aspect of a relationship. I suppose it's possible, in code that religiously adhered to Liskov substitutability and was heavily polymorphic, it might have been of some value. But then, the irony would be that we'd just be talking about [contravariance](https://apocalisp.wordpress.com/2010/10/06/liskov-substitution-principle-is-contravariance/) anyway, and we wouldn't need the diagrams. In other words, for those diagrams to have any "value," they'd have to express that anywhere you can use an `A` you can use a `B`, which is trivial. But if they don't express that, they only express the static relationship, and not the "situation on the ground," where `B` may behave completely differently and unexpectedly from `A`, which can at least be captured by sequence diagrams (but almost certainly won't be). The whole point of diagramming is to somehow capture and/or summarize information that isn't in the code itself. The fact that the diagrams have to evolve with the code is a clear signal there's something wrong, and the attempts to develop tools that extract code from diagrams or diagrams from code underline that observation in red ink. Their value is highly questionable even in imperative OO contexts; in pure FP ones (as with so much else in programming pop culture), it's negative.
In a big project my company shipped, we use Akka heavily (Cluster, Distributed Pub/Sub, Persistence, HTTP, Streams, Distributed Data, etc.), but only directly implement our own actors in very limited situations. We have only 2 custom actors, and they're factored out into a library you might find worthwhile to look at: https://github.com/artsy/atomic-store. Testing is a little tricky to begin with, but not too bad. You can see examples in that repo. The key for me was to design my actors to be created on their own, with injectable child `Props`. That let me test the behavior of each actor individually. The simpler you keep your actors, the easier this will be. This is of course much like any other unit of modularity in programming. Then I wrote some integration tests for the whole system.
Well, I mostly agree with you - but if the company pays $$$ to get this stuff done - you have to jump over the bar.
Okay, any suggestions what to use instead? Aside of "the code is self-documented enough"?
They pay people for solving tasks - not assuming them to watch some talks, unfortunately.
[removed]
Optional parenthesis, dot notation, and inferred return types. Not really "a mess" but I guess coming from the strict world of java I can see how you might look at it crooked eyed. Shops I know just agree on a style guide and stick to it
Yes. Jetbrains has a good post on the subject matter: https://blog.jetbrains.com/scala/2016/10/05/beyond-code-style/ Also see http://www.scalastyle.org/ for an automated style guide. 
Yes this is a mess http://stackoverflow.com/questions/8303817/nine-ways-to-define-a-method-in-scala There's a lot of other nice things about Scala, and other languages have their own messy parts (how many ways can you define a class in Javascript???) but I don't think there's much ambiguity that this part of Scala really sucks for beginners. On the bright side, line 5 and 6 are going away. I wish they got rid of lines 9 and 11, but apparently nobody other than me seems to dislike them. After that, I think the last main source of confusion is the whole "parens are optional, at each callsite" rule. Without these three things, it actually becomes pretty consistent not unlike other programming languages 
Java has type inference, especially with lambdas, so that should not be foreign. And http://stackoverflow.com/questions/6939908/scala-methods-with-no-arguments provides some insight as to why you may or may not want an empty argument list. Don't just assume things don't mean anything because you haven't discovered their uses yet.
Deciding on "the best" way to write Scala has been a challenge, speaking as a newer adopter. For example, "SBT style" is a lot different than "Spark style". I much prefer Spark style, but I'm torn between hating SBT style and wanting our build scripts to look like examples and documentation in the wild. It's kind of weird having two different formats for our build script vs application code, and even the various possible syntaxes within "Spark style" (`rdd.map { r =&gt; ... }` vs `rdd.map(r =&gt; { ... })`) can be jarring to switch between while reading. I know it's all pretty trivial, and we will likely end up adopting the official style on scala-lang.org, but thought it worthwhile to share my experience.
Conventionally I use parens if the map function is a one-liner; otherwise use curly braces.
I can and i did. Why? I picked an extreme example to give you a chance to spot the issue. This sort of flexibility means that you as a developer need to master all possibilities and not just the possibilities used in your style guide. Because on your next job the company might use a completely different style. Heck usually styles even differ between departments. Think about short term contractors trying to help on your project. What happens when you are trying to narrow down a bug inside a 3rd party library? Are you good enough to instantly adopt to a completely different style? Additionally it is also a huge burden for the scalac team because they need to ensure that all possibilities work without an issue. Paul philips mentions that in his talk "we're doing it all wrong" (easy to find on youtube). I wish that i could remember a quotation from a well known java guy. It was something like "a programming language is not defined by its features. It is defined by the things we avoided to add to it". I i think it was from Joshua Bloch. Additionally even the small detail of optional semi-colons has implications. It scares developers away (hopefully not too many).
Here's how I view them when I see them, because I see these regularly working on a Scala + Java codebase, and these are the conventions my team has adopted: &gt;`def doSomething():Unit = {...}` This is a public method that mutates _something_ within it. The parentheses mean "I mutate something". &gt;`def doSomething:Unit = {...}` This is a public accessor, poorly named as-is. It ~~wouldn't~~ shouldn't pass code review because its name indicates that it mutates something inside it. Simply calling it `something` would be more appropriate: `def something: Any` &gt;`def doSomething() = {...}` This is a private-ish mutator. It's private-ish because it's not marked private as-is, and our style checker would flag it as needing a return type if it's public. &gt;`def doSomething = {...}` This is private-ish accessor, poorly named as-is. &gt;`def doSomething() {...}` &gt;`def doSomething {...}` Someone copied from Java poorly and the methods are private-ish. Assuming that `MyClass` here is an instance of a class and not an object, but it doesn't matter whole lot, really: &gt;`MyClass.doSomething` Accessing something on `MyClass` without mutating anything in the process. &gt;`MyClass.doSomething()` Mutating, or accessing with the potential to mutate as a side effect. That's ideally not true, but when interfacing with older Java code, it's a possibility. &gt;`MyClass doSomething()` &gt;`MyClass doSomething` Added this one. We don't use this one, really. It's probably the rarest of these. &gt;`MyClass.doSomething(param)` &gt;`MyClass doSomething(param)` &gt;`MyClass doSomething param` We use these only in ScalaTest specs or when we've successfully built an API that _really_ lends itself to having a DSL: retriever retrieving(files) should haveSize 4 It's kinda up to the author. 
I think the Scala plugin for IntelliJ made this part of the default configuration with the new version recently released ( at least for ultimate). Made a huge difference in my code 
Keep in mind, though, that Scala is deliberately designed to offer robust support for developing DSLs, and its syntactic flexibility, which remains principled, is an important component of that. Paul Phillips' complaint has more—much more—to do with the standard library and how it violates various laws and expectations. So much so that he's written [psp-std](https://github.com/paulp/psp-std) in reaction to it. Not that he doesn't also have criticisms of the language, of course... Anyway, it's important to understand that in Scala, "a programming language is not defined by its features. It is defined by the things we avoided to add to it" is an _antigoal_, and the language will continue to frustrate anyone who expects a programming language to be prescriptive.
No we cant compare the use of optional styles of Scala syntax to that of Perl There is just too much syntactic bullshit and tomfoolery happening to every make a sane argument for comparison though I know youre trying Ok that was just to make sure youre still going to read this I havent used any punctuation yet You might have noticed You might not But youre still reading this and understanding what Im trying to say You might even be inserting periods in your head You might not The point is this isnt normal and it feels weird But like most things that would change if every post on reddit was sans puncutation Code is so personal and yet professional share their inner most critical thinking skills with their teams so openly. It's a very vulnerable feeling for me. Others might feel the same. There's temptations to push away things that we don't understand so we can feel comfortable and validated again. The demonstration above wasn't me trying to be an ass but rather to show that even without comfort or normalcy we (humans) have the insane ability to figure things out. I like the syntax. It makes me feel like an artist rather than a robot.
&gt; The parentheses mean "I mutate something". That's by convention only, right? 
Yea I do the same. It seems to work really well for how I visually parse the function body.
I went from Ruby to Java and I felt like someone chained my brain to the desk. I share the sentiment that Scala allows an awesome syntax. Especially for particular people like me. 
It would have been better if your closing sentence was, "My fault, i mixed wrong the letters in the last word."
I remembered the 2nd rule wrong: The first and the last letter needs to stay in place. The letters in between can be scrambled randomly: https://en.wikipedia.org/wiki/Typoglycemia
Do you want to just hug it out? o//
I don't dislike this case: MyClass doSomething(param) because I see it as not that parens are optional, but whitespace before them: MyClass doSomething (param) Obviously you shouldn't ban parens, since they are useful for grouping both in the case of operators and methods: 5 * (x + 17) list map (square andThen intToString) // List of Strings list map square andThen intToString // Partial function Case 11 is like writing `2 *p` instead of `2 * p` or `2*p`. It's a weird way of putting a whitespace, but it is allowed for operators in most mainstream languages, so it makes sense to me due to "operator is a method" rule and I don't really want compiler to complain about whitespace *~gough~* (although I have no objections against any opt-in with regard to that)
Yes, recomended by scala-lang.org http://docs.scala-lang.org/style/method-invocation.html#arity-0
Scala is a mess, but for other reasons. The syntactic flexibility you're talking about is very superficial, and you get used to it very quickly. Even in Java you have variations: MyClass.doSomething(param) (MyClass).doSomething(param) MyClass.doSomething((param)) (MyClass).doSomething((param)) ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); ArrayList&lt;&gt; list = new ArrayList&lt;String&gt;(); Now a few specifics: def doSomething():Unit = {...} def doSomething() = {...} The return type is just optional. If you ever don't know what the return type of a given function is, your IDE should be able to tell you. def doSomething() {...} def doSomething {...} This style is deprecated, and will produce warnings on modern versions of Scala. MyClass doSomething() MyClass doSomething(param) I wouldn't recommend this style and don't see it used. MyClass doSomething param Yes, this is an extra bit of flexibility that you just have to learn. IMO it's worth it in terms of the DSLs it enables (a lot of things that would be a separate configuration file in Java can be just code in Scala), but the cost is real.
 def doSomething(): Unit = ??? Indicate side effect def something: X = ??? Indicate referential transparency / no side-effects. def doSomething() = ??? Use type inference. In public API, do this only when return type is immediately apparent (e.g. RHS is a literal). def doSomething() {...} "Procedure-syntax" is deprecated and should not be used any longer. myValue doSomething param Infix syntax is useful for symbolic or symmetric operators and DSLs. E.g. `x min y` or `x + y`. myClass doSomething(param) Redundant parentheses, don't use. Just as `4 + ((((5))))`. All of this is part of the reason why we prefer Scala over Java ;)
On IDEA, there is a linter to say you have to add parentheses for function which return Unit and doesn't have parameters.
Yes. I like the `_!` and `_?` conventions in the same manner as Ruby ("do it and I don't care about the return value so raise an exception on failure," and "returns a boolean") but Scala's static typing handles it enough that those conventions are cosmetic only to appease the Ruby part of my brain. We don't use them in my primary product's code, but I've used them elsewhere.
A follow up question is, what is the value of unit tests beyond test coverage? What if I can achieve tight feedback loops and code coverage via integration tests that test the edges? The question isn't being argumentative. Could be just my style of programming, I am genuinely interested in other people's perspectives.
Good to know others are like me :) Seems like people are for using the type system to eliminate tests where possible. I am definitely going to explore that idea.
This can be maddening at first, but convention / idiomatic scala make this simpler. I would also say that your example is simpler to overcome than the underscore in scala. I definitely understand where you are coming from, as I came to Scala from Java myself many years ago, and had the same reactions. def doSomething(): Unit - this is proper if the function is public and the function is side effecting. Always make explicit the return type on public functions. def doSomething: Unit is wrong by convention. Unit implies side effecting, so you would always use parens. The only times you can omit parens is for "getters", some would argue pure functions as well (a function that has no side effects that is only works with its inputs) def doSomething() - is wrong by convention. Public methods should always adorn the type def doSomething - is wrong by convention for the above reasons def doSomething() is wrong by convention for the above reasons ----- My point is, there are lots of examples of "wrong by convention" in any language. In Java, I can do something like: public class MyClass { public int myField; And that would be valid syntax, by typically incorrect by convention.
What?
Well put! There's one thing about this convention that bothers me though: If your method takes zero arguments, `()` at the end is a nice indicator of side-effects. But if your side-effecting method takes parameters, it is no longer standing out! I wish there was a better way of signalling when a function has side effects and not. Can't wait for the Effect System that Martin described in one of his latest talks is ready! :D (In the meantime, I know there's monads (like Free) and monad transformers, but those concepts always seem to get way out of hand in terms of complexity. Suddenly it's no longer a tiny beautiful easily-understood piece of code that you can greet newcomers with :(
Have you ever worked in a team of 8 on a legacy codebase in CD environment? You'll quickly discover that you have no time, energy nor sufficient knowledge to go through code to find out if your change broke anything anywhere, team will be making changes at a rapid pace and every commit goes straight to production. Integration tests will never be as rapid and robust as unit tests, especially in local dev loop, where you have two options - either to spin up the whole world or mock out the dependencies which makes them prone to becoming outdated. There's a reason [test pyramid](https://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html) exists.
This is an educated guess based on behavior, but a guess nonetheless. Since Scala's type inference is not documented anywhere I know of, I'd have to go through the source to figure it out completely. Maybe someone else can fill in the details I don't know. I'm pretty sure this is brought on by the fact that both [`++`](http://www.scala-lang.org/api/current/scala/Array.html#++[B](that:scala.collection.GenTraversableOnce[B]\):Array[B]) and [`transpose`](http://www.scala-lang.org/api/current/scala/Array.html#transpose[U](implicitasArray:T=&gt;Array[U]\):Array[Array[U]]) allow you to build to different types, coupled with delayed/missing implicit resolution. So in the failing case you have `m ++ (gt: GenTraversableOnce[B])` where you need to figure out what `B` is. An attempt is made to fill in `B` from the right hand side, which is the result of `m.transpose`. For an `Array[T]`, the `transpose` result depends on which implicit conversion is used for `T =&gt; Array[U]`. Even though you have `T = Array[String]` you could theoretically have a conversion that results in `Array[CharSequence]` or `Array[Int]` or anything else that converts `Array[String]` to `Array[U]`. At this point because `Array[U]` is a `GenTraversableOnce[U]`, we know `B = U`, but we still don't know what `U` is. The most we can say is that `U` has the same bounds as `B`, namely that it must be a supertype of `String`, which is a restriction imposed by the `++` signature. If an implicit were resolved at this point to supply `U`, everything would be fine, but it's apparently not done until later so you get an error. When extracting `m.transpose` to a variable `n`, it forces the implicit resolution earlier so that the type of `n` is known. The resolved implicit is the identity function (`$conforms`, in `Predef`), so `U = String`, which makes `B = String`, which makes everything known and happy. When putting `m.transpose` first, the identity implicit is also resolved, presumably so the can determine the type parameter bound needed for the signature of `++`. Another workaround is to supply the parameter yourself: `m ++ m.transpose(identity)` Honestly this feels like a bug to me, especially since the IntelliJ Scala plugin can resolve the implicit and infer the resulting type in this case.
Right. The convention with side-effects through empty parentheses is, one must say, also something that was realized later. I think. Because technically, it is a consequence of Scala being very simple and regular, paradoxically: A method is `def &lt;name&gt;{[&lt;tparams...&gt;]}&lt;vparamss&gt;{: &lt;Res&gt;} = &lt;body&gt;` where `vparamss: List[List[_]]`. The fact that Scala has multiple parameter lists introduces the distinction between `Nil` and `List(Nil)`.
Can you explain why isInstanceOf() works this way: class Animal val a = new Animal val b = new Animal a.isInstanceOf[b.type] //false here - why?
That's logical and definitely fits the evidence. Thanks for your help. It makes sense to me that resolving the implicits on parameters would be part of typechecking the call to `++`, but there may be a subtle reason why it can't happen in time. Or, as you say, it may simply be a bug. Thanks again for the insight. 
&gt; This sort of flexibility means that you as a developer need to master all possibilities and not just the possibilities used in your style guide. &gt;Because on your next job the company might use a completely different style. Heck usually styles even differ between departments. Think about short term contractors trying to help on your project. Some of the OP's criticism is legit, hence why they are removing some of the options (such as procedure syntax) or whatnot. However, as someone who works on a *very* large Scala project with various smaller teams all using their own style in each microservice, this really hasn't been as much of a problem as you might think. The type system along with the REPL make it pretty easy to get unconfused about the various ways of doing something, and after enough time you become fluent in the various 'styles'/dialects of Scala. Complexity of the application, understanding FP concepts, third party libraries and how they work all take orders of magnitude more time than it takes to parse a function written in an 'obscure' scala style. Now, granted, I have a lot of experience and others don't, but we have 'scala' chat rooms for help, and then you have the IRC/Twitterverse as backup and the new folks seem to do fairly well. They just ask more questions. 
Ah. Ok, sure. That style guide seems totally reasonable. I thought you maybe meant this *much* more opinionated style guide by DataBricks that uh, reveals some huge misunderstandings of the language: https://github.com/databricks/scala-style-guide#chaining
I even seen Java developers being slowed down by different styles. Maybe i should rephrase it to a question: how many seconds a day will be lost? (multi style codebase vs coherent style) We both know that a honest answer would exclude 0. But as many things in our developers world it is nearly impossible to measure its impact. Who knows, maybe it is even a good thing. It sets a higher base level. Companies ~~will~~ can not longer hire the cheapest available dev. 
Thank you for the link. This is very interesting!
Very true. Good point. Really a non-issue. 😁
Code looks nice at a glance, it seems you've mapped most of the domain out as simple case classes which in my experience is like 90 percent of producing correct and safe code. One thing I noticed and wondered about was the Map[String, Any] in the Document type. Is this an internal type for the lib? Is it possible to replace with a parameterized type and an implicit codec? At work we run a fairly large business application entirely out of a Free over Coproduct of DSLs, as in Runar's reasonably priced monads talk - http://functionaltalks.org/2014/11/23/runar-oli-bjarnason-free-monad/ - In implementing and building on this app I've gained some insights over the past several months on the use of Free at scale, so I wanted to share a few points that may or may not become relevant to you at some time. 1- the type-plumbing boilerplate is actually not necessary. Most examples spell it all out, similarly to how you have it here - https://github.com/trbngr/dutchman/blob/master/core/src/main/scala/dutchman/Operations.scala. You can just as well name your case classes lower-camel and use an implicit conversion or syntax enhancement on your base operation type to get from the case class to your Free. It seems trivial but it really adds up, and the boilerplate needlessly adds surface area for silly logic bugs. Also, the fact that you have defined individual methods for immediately interpreting single-command Free programs here - https://github.com/trbngr/dutchman/blob/master/core/src/main/scala/dutchman/package.scala adds another layer of this boilerplate. I suppose the goal is to give the end user a convenient way to run the effects of individual operations, but you could also achieve this with a syntax enhancement on your base operation type. I'd also note that if this is the primary means of interacting with the lib, it sort of cancels out the primary gains of using Free (I did exactly this with my first real implementation based on a Free monad.) 2- Larger DSLs give greater control to interpreters at the cost of easily doing alternate interpretations. I remember reading somewhere that /u/tpolecat has grown the DSLs in doobie to the point where providing your own interpreter is a prohibitively large task. In the application we support based off Free at work, I made a choice when we first needed an alternate interpreter to move from doing most work in the interpreters to doing most work in higher level Free-based programs. The result is a large application built on less than 20 or so interpreted commands, so adding new interpreters (rare, but it happens) is relatively simple now. 3 - One base operation type might limit you in the long run. For organizational purposes, as well as additional flexibility, you might eventually want to consider the Coproduct approach linked above in the reasonably priced monads talk. Then again you have a fairly focused lib here so you might never need it. 4 - Free monad based programs have implications on concurrency. Namely that switching to applicative is something of a process, and I've been told, not completely solved. Even if you interpret to a Future, a Free based program will execute sequentially. I banged my head on this for some time, and eventually arrived at a solution here - https://github.com/mblink/composefree/blob/master/example/src/main/scala/Example.scala#L33 - using a Free[Coproduct[FreeAp[Free[DSL, ?], ?], Free[DSL, ?], ?], A] Here are the primary resources that got me in that direction: https://www.reddit.com/r/scala/comments/55s01p/concurrency_in_a_free_monad_based_program/ https://github.com/typelevel/cats/issues/983 https://vimeo.com/channels/flatmap2016/165928373 (this one more or less outlined the solution I ended up with.) I think these points get less and less specific to your project as they go on, but it's cool to see how you used Free, and if the lib grows or you continue to use this approach in larger contexts I hope that some of it is helpful to you. 
&gt; we're using scalaz and argonaut instead of cats/circe, so I'll gladly take a look. The idea here is to support any json and/or http lib. However, I'm currently stuck with cats. Not a bad thing as far as I can tell. &gt; My approach was generally different from yours, I started with the the Java based Transport client. Sticking with HTTP because of AWS ES. &gt; Is there a particular reason you don't use Inject to compose multiple different APIs together? No reason at all. This is just my first foray into `cats` and now that `Free` isn't any longer some magical voodoo to be afraid of, I'll happy to research more into that. EDIT: Now that I have a domain that makes sense using `Free`, I went back and re-read the docs on composing using `Inject`. It makes SO much sense now. I will be reworking this thing next week. :) &gt; It seems to me that there can be great simplifications if you consider designing multipass compilers. Natural Transformations from Free[Api, A] to Free[Api, A] are perfectly valid, or even transformations Free[IndexApi, A] to Free[SingleDocumentIndexApi, A] or Free[BulkDocumentIndexApi, A]. This way general operations like this don't have to try and belong to mutiple algebras at the same time. This was indeed a pain point. I *really* hate ending up with that giant algebra. Thanks for the discussion. Writing this library has been a pleasurable learning experience. I can't wait to enhance it further.
&gt; and design but I am not convinced that JVM is appropriate Is this handwaving, or do you have a concrete reason this is the case? The only reason I might think this might be the case would be that you're reliant on SIMD based optimizations to be reasonably performant, or perhaps need to work with hardware sensors or GPGPUs. &gt; engineering applications that require a lot of memory and multiple cores to produce results in a few days of running on a supercomputer What you describe is actually what the JVM is very good at naturally. &gt; I am not aware of any JVM-based software in use or under development for that purpose. Which space exactly? Computational Fluid Dynamics or supercomputing in general? I don't know if could name examples in the former, but there are definitely examples of the later particularly when working with distributed supercomputers. Just look anything Hadoop or Spark based, those two libraries are de facto choices for that market.
The applicative/monad structure I've settled on is Free[Coproduct[FreeAp[Free[DSL, ?], ?], Free[DSL, ?], ?], A] Which then allows you to interpret applicative operations nested within monadic programs. It's laid out very nicely in this video - https://vimeo.com/channels/flatmap2016/165928373
&gt; It's not a "huge undertaking" You are missing the point. Don't narrow yourself down to "numeric tasks". The compiler needs be on par on normal day applications while being fully scala compliant. 
**disapproval**: Your wishful thinking != reality.
Read and understand my post before responding.
Define "reactive"
http://www.reactivemanifesto.org/
FWIW the doobie algebras are large because they map 1:1 to the JDBC API. This was a design goal: any JDBC program can be written using doobie's Free API. But as you say, it's impractical to write an alternative interpreter because there are one thousand constructors. (The provided interpreter is machine-generated in fact). I'm working on a different representation that should make this somewhat easier.
That's different, native is much more difficult. You can try scala-native and see how class is compiled into llvm bitcode/IR and how the standard library is interoperated.
scala-native is really hard work; there are really a big gap between JVM and LLVM. Many PhD projects terminated finally and Scala.js is perhaps one of the very few that have survived. Of course I hope scala would go native, but almost for sure it is a long way...
In a system I am working on now, the datastore offers Rx Observables of items being stored - e.g Observable[Trade]. When something is written to the database successfully, it is pushed to the relevant observables. The rest of the system is built on top of those observables, filtering / mapping etc as they need. In a clustered environment, when something is persisted the rest of the cluster is notified (via REST in our case but it could be anything), causing other nodes to reload the data and push the updated data through the same observables.
I am similar regarding video but I made the sacrifice for this one because there's so little on this subject and the summary promised a solution. The relevant part is very brief and starts around 18min. Aside from that I've been planning a write-up on this for a while, once I have it published I will drop a link right here.
I just bumped into this; Slick3 provides a ferociously complicated API compared to Slick2, apparently in the interests of making things Reactive™. It seems if you're willing to block on your DB requests (like much of the rest of the world does) you end up with a dramatically simpler API that more or less matches what I'd expect a ORM to look like
I've touched on this before (too strongly, as I was reminded at the time), but let me try to break my thoughts down into the two categories I now think are relevant: 1. If there's some context in which you're using ScalaJVM and/or ScalaJS today, would benefit from lacking the JVM's startup time or having a statically-linked binary to ease installation etc. _a là_ Go, and don't have the bandwidth to or find yourself struggling to learn another language, then Scala-native _might_ make sense. 2. For everything else, there's [OCaml|Haskell|Rust|Swift|Idris|...] I tend to think that, if you're already using a marginal language like Scala, you've probably got the temperament to learn one of the above languages, and probably work in a context where it would be permissible (possibly with the kind of negotiation that an eager Scala learner in a Java shop would have to undertake). What I think you'll find, speaking as an enthusiastic OCaml fan, is that all of them are better languages than Scala along many important dimensions, because they haven't had to undertake the balancing act that Scala has pulled off (and let me reiterate, pulled off _well_): being OO/FP and running on the JVM. If you want strict impure FP (no one uses the "O"), you'll probably love OCaml. If you're a scalaz/Cats user, you probably already know Haskell makes you jump through _far fewer_ hoops and type inference works reliably. If you are working on or at least looking at iOS, you've probably at least glanced at Swift, which is now cross-platform. And Rust is [underappreciated as a functional language](http://science.raphael.poss.name/rust-for-functional-programmers.html). I've been a professional Scala developer now for about six years, and am delighted by that. But I have to be honest: if it weren't for the need to be on the JVM on the server, and the desire to use the same language on the front and back ends on my personal project, I'd be using a different language. (I ultimately plan to sell my personal project and let someone else run with it, otherwise it _would_ be in a different language.)
Mmm. You do realize that those calls are blocking under the hood right? AFAIK no db driver is async. Typically libraries layer an async API by executing your DB code in a threadpool and returning a future. Apologies if you did know that and I misunderstood you. To the OP however, I sympathize. I've worked with a lot of DB libraries and slick is by far the most complicated I've ever used. It is...not a pleasant experience.
Slick 3.0.0-M1 had a blocking API based on the new design, which was later removed for political reasons ("making things Reactive™"). Simply replace `run` with `runBlocking` and `stream` with `iterator` (or whatever the methods were called) and that's it. The only impediment for adding it back (now that the political motivation is gone) is that the database connection pool needs to be tuned differently for blocking calls, which requires either a new API / configuration, or good documentation. Pull requests for restoring the 3.0.0-M1 API with appropriate changes to the documentation for database configuration are welcome. However, going asynchronous was not the main reason for replacing the old API (but it was the reason for breaking with the old API at that point in time because the old API design would not have worked for asynchronous calls). Resource management is hard in practice. People constantly get it wrong. A session that you create, pass around, and eventually close, is mutable state. And when you have a mostly asynchronous program full of Futures (which was the case because Slick is frequently used with Play or Akka), that makes it especially hard to deal with mutable state. In addition to that, it does not constrain the API to make incorrect use impossible (or at least much harder) even in the single-threaded case. We started with `withSession[R](f: =&gt; R)` and a global implicit `Session` import. This looks very convenient in toy programs and sample code but is error-prone in practice, so we abandoned it in favor of `withSession[R](f: Session =&gt; R)`, which looks slightly less nice in sample code but is harder to get wrong in practice. The new execution API in Slick 3.0 is the logical continuation of this change: `withSession(Session =&gt; R)` only takes care of the top level, you still need to pass a mutable `Session` around when you compose database calls. `DBIO[R]` in Slick 3.1 is essentially `Session =&gt; R` (or, if you want to view `Session` as immutable: `Session =&gt; (R, Session)), i.e. an IO monad with a database connection as the mutable world state. DBIO composition is blocking-agnostic (and was designed to be from the ground up). You can write an interpreter for DBIO that does everything with blocking calls on a single thread, or one that uses Futures everywhere. (As it turns out, if you have the asynchronous interpreter, you don't need to write a synchronous one anymore because you can just run everything on a SameThreadExecutionContext, which is exactly what Slick 3.0.0-M1 did.) tl;dr: Even for synchronous calls I think the new API is better. Slick 3.0.0-M1 had a synchronous interpreter for DBIO.
Thanks! I understand the getting-started bit, but since the tutorial does not seem to be available yet, I'm asking myself what your library is actually doing on top of DeepLearning4J (which I gather is the underlying ML library)? The DSL looks a bit superfluous to me so far, why don't you provide a simple API with regular methods instead of asking people to use weird Shapeless operators sugared with a compiler plugin for "special implicit dependent type syntax"? I'm not trying to provoke here, just trying to understand why you made the choice you made. And I'm eager to read the tutorial, do you have any release date planned?
Awesome, continue to enjoy ScalaQuery (the predecessor to Slick) to this day; has the same withSession/withTransaction syntax as the referenced blocking API for Slick 3. I use Slick's excellent code generator, but other than that Slick 2.x/3.x isn't all that compelling when blocking is the norm in jdbc land (Quill, on the other hand, can be async from the ground up via postgres/mysql async drivers). Will be fun to see how query DSLs evolve with Dotty and Scala Meta on the horizon. At minimum we'll have lighter libraries and faster build times to look forward to.
&gt; Mmm. You do realize that those calls are blocking under the hood right? AFAIK no db driver is async. Typically libraries layer an async API by executing your DB code in a threadpool and returning a future. That's not true. [Quill](http://getquill.io) provides fully non-blocking IO db access for postgres and mysql.
In brief, DeepLearning.scala aims to be a general-purpose language along with the ability of machine learning. The goal will be complete after it supports `for` and `while`.
I upvoted you to thank for your wonderful Scala.js. However I still suspect that scala-native will come to production level shortly if no significant changes have been made. You guys in LAMP are really smart, and I hope some more people would join you (specially @densh) to boost the scala-native.
I just wanted to thank you again for your thoughtful response. That talk you linked was possibly the most eye opening video I've ever watched. My mind is racing with what is possible with any project I'm involved with. What have you done? 😳
Ok thanks for clarification. So your project is aimed at researchers who want to understand how to develop ML algorithms, is that correct?
For a truly reactive system you shouldn't use a relational database in the first place. We're pushing people towards different solutions for that, JDBC/relational is seen as a legacy integration tool, so there's less need of it being "pure". (This is my interpretation, not an official company statement)
Awesome to hear. That video was similar for me, removed a huge block and opened up some really important things.
The Coursera stuff should get you well started; but the next level with Scala that helps you understand a lot of concepts, is typeclasses. Ignore the higher-kinded stuff like monads for now, just try implementing a toy typeclass like `ToString`. There is a fairly explanatory paper by Prof. Odersky and a couple of others that show how to create and use them properly: https://ropas.snu.ac.kr/~bruno/papers/TypeClasses.pdf
You can probably cannibalise some parts of UML to come up with something. Case classes can be class diagrams, functions can be boxes with input and output, typeclasses will probably also be class diagrams. Just beware that most formalised diagrams for code are a big waste of time, people don't look at them, don't update them, and soon forget about them.
* A.What are some of the best examples of wrapper/enhancement libraries over Java libraries for someone relatively new to the language to learn from? (To learn to improve/write facades) * B. What are some of the worst practices you've encountered in facade libraries? * C. What sorts of enhancements (if any) make you more productive when using facade/enhancement libraries? (eg. stuff like foo.onClick = (a:MouseEvent) =&gt; {...} instead of foo.setOnMouseClick(new MouseClickHandler...)) (D. Bit of background/context/discussion about the types of libraries I'm addressing: Having tried a couple android libraries and Scalafx, I have been frustrated at times with the sort of edge-casey hacks necessary to get something done and stay within the bounds of the facade that the author might not have anticipated. Often, it's easier to step down to the core library and do things the 'idiomatic java way' in Scala. Sometimes this doesn't work with the way the facade is set up. *It seems like open source facade libraries are more popular/widely used in Scala.js (for obvious reasons), although, these questions are more geared towards the Java-wrapper variant of facade.) 
I'm also an enthusiastic ocaml fan, and would probably use it everywhere, but IMO it has two fatal flaws that always push me back to scala: no typeclasses, and no multi-core runtime. They seem to be working on the multi-core runtime, but it is coming along slowly to say the least. I'm pretty confident that scala-native will be production ready (at least for my use cases) long before OCaml's multi-core runtime does. What I really want is dotty-native. That would be a dream come true. 
May I ask what a Dotty-native would bring to the table that a Scala-native wouldn't?
Yes. Basically, DeepLearning.scala is like a strong typed version of Theano, with some additional features for ADTs and control flows.
I mean there is a lot of things to like about Dotty, but whole program optimization especially benefits the types of programs that you would lean towards native compilation for. You would be able to cut out huge amounts of dead code that ends up packaged in jars, and it would allow for a larger amount of AOT optimizations that would usually only happen on the JVM after some heavy-overhead pre-JIT profiling. 
Keep in mind that not all Scala programmers find great benefit from strongly emphasizing the FP side of Scala's gorgeous OO/FP fusion in their code. Some domains like GUI, Game Dev, Mobile-front-end reap massive benefits from Scala's succinctness without even having to dip it's toes in the monad waters (although, surely there's lots of exciting, unexplored potential here!). Swift is a breath of fresh air compared to Objective-C, but it's far from as pleasant an experience coding as Scala. Scala/Dotty is picking up steam and it's a matter of time before we start seeing a huge surge in adoption on all fronts. Scala Native, the third pillar, could play a monumental role in that growth! Exciting times to be a Scala developer to be sure!
I dunno. I was exposed to OCaml's modules earlier than the scads of Scala encodings of typeclasses, so it's hard for me to say one has a clear advantage over the other, especially since the introduction of first-class modules in OCaml. And I keep hearing that the multi-core GC is important but am no closer to understanding why, because I don't do the kind of large-scale data-parallel work where it's apparent that it would make a difference (i.e. [netmulticore](http://projects.camlcity.org/projects/dl/ocamlnet-4.1.2/doc/html-main/Intro.html#netmulticore) is plenty good enough for me when I feel I need that).
I think it will be very hard to write a good idiomatic Scala library as a "facade" - really one needs to design a scala-first API and then an implementation in terms of the Java library, which of course is a much bigger task. I guess doobie is the example I'd point to.
Out of curiosity, what were your problems with ScalaFX?
You forgot to link to your project. I'm guessing it's https://github.com/denisrosset/valueopt.
Main post edited! thanks.
For the record I found the slides at https://speakerdeck.com/markus1189/free-monads-and-free-applicatives . Hope I didn't miss too much by just reading those. Defining the structure like this seems to mean only certain types of nesting are allowed? It seems to me that what you really want is a pair of mutually recursive types, which we can define with a mu combinator if need be: sealed trait NodeU[F[_], DSL[_], A] //possibly representable as a coproduct case class ApplicativeNode[F[_], DSL[_], A]( value: FreeAp[Coproduct[F, DSL, ?], A]) extends NodeU[F, DSL, A] case class MonadicNode[F[_], DSL[_], A](value: Free[F, A]) extends NodeU[F, DSL, A] type Program[DSL, A] = Fix[NodeU[?, DSL, A]] def combinedInterpreter[DSL[_], G[_]](interpret: DSL ~&gt; G, interpretOpt: FreeAp[Coproduct[G, DSL, ?], ?] ~&gt; G): (Program[DSL, ?] ~&gt; G) = new (Program[DSL, ?] ~&gt; G) { override def apply[A](program: Program[DSL, A]) = program.cata[G] { case MonadicNode(value) =&gt; value.run(interpret) case ApplicativeNode(value) =&gt; value.run(interpretOpt) } } (I'm not sure whether there's a public implementation of `Fix` and `cata` for Scala around, but they're easy to write if not - I'll do it in a few days)
Yeah, but why not `==` instead of `eq`?
Related: https://github.com/sjrd/scala-unboxed-option 
Good one. Too bad 2.10 does not support type aliases in package objects... because otherwise the type alias approach removes a limitation of 2.10 (value classes enclosing an Any type). Edit: I found a workaround, which is to have the package object extend a class with the required type members.
So does the extra type bound cause performance issues, or is it just to reduce notation? 
The extra type bound `A &gt;: Null` should be free at runtime. The problem is then that `Opt[A]` is not a drop-in replacement for `Option[A]` -- all your code needs to obey the `A &gt;: Null` constraint.
&gt; This allows Opt[A] to be used in generic code without propagating A &gt;: Null bounds everywhere But, `null` can still be be passed as an `A` whose type constraint is `A &gt;: Null`: $scala Welcome to Scala 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101). scala&gt; :t null Null scala&gt; def f[A &gt;: Null](x: A):A = x f: [A &gt;: Null](x: A)A scala&gt; f( null ) res1: Null = null http://stackoverflow.com/q/41445801/409976. What's the purpose, then, of using that type constraint? What does it buy us?
I understand the difference between the two. It just seems somehow odd to use `eq` because it relies on interning for common cases of `String` and `Symbol` and makes it fairly difficult to make your own classes with same behavior: val s1, s2 = "Hello" s1.isInstanceOf[s2.type] // true - interning magic s1.map(identity).isInstanceOf[s2.type] // false s1.map(identity) == s2 // true case class Immutable() val i1, i2 = Immutable() i1.isInstanceOf[i2.type] // false - different references I've rarely seen Scala code that needs `eq` for something else than low-level optimizations or Java interop.
If `Opt[A]` uses the bound `A &gt;: Null`, it means that I have to change: `def test[A](a: A): Option[A] = ...` into `def test[A &gt;: Null](a: A): Opt[A] = ...` and now every function in my code that depends on `test` has to satisfy the type bound. It's viral. In any case, I like the design of `scala-unboxed-option` much better, and it is unboxed on 2.10 too.
[removed]
I know that you're referring to the Functor Laws, i.e. why `res5` should equal `res6`, but I think posting a link to them, please, will be useful to folks who aren't familiar with them.
slighty different error depending where the brace is placed : Loading Option.scala... import scala.{Option=&gt;_, Either=&gt;_, _} Option.scala:16: error: constructor cannot be instantiated to expected type; found : Some[A(in class Some)] required: Option[A(in trait Option)] case Some(b) =&gt; Some(f(b)) ^ Option.scala:16: error: type mismatch; found : Some[B] required: Option[B] case Some(b) =&gt; Some(f(b)) ^ Option.scala:17: error: pattern type is incompatible with expected type; found : None.type required: Option[A] case None =&gt; None ^ Option.scala:17: error: type mismatch; found : None.type required: Option[B] case None =&gt; None ^ Option.scala:14: error: illegal inheritance from sealed class Option case object None extends Option[Nothing] ^ Option.scala:14: error: illegal inheritance from sealed class Option case class Some[+A](get:A) extends Option[A] ^ scala&gt;
Ok, that looks like it's not doing the equivalent of `:paste` - I suspect it's still finding stdlib `Some`. Try passing `-Yno-imports` as a command line argument so that you definitely don't get them imported. I think that will get you a different error (where it fails to find `Some`) which will be a sign it's on the right track - you'll need to also do the `:paste` trick as other people have suggested.
This work is really impressive. But it feels to me that Scala hasn't quite made it when it comes to providing zero-cost abstractions. I wonder, what language-level features could solve this problem on a more generic level, without so much hackery?
Weekend done good :)
How does ocaml_of_js compare to Scala.js? For me writing the whole app in one strongly typed functional language is a great advantage. Scala.js has been great in my admittedly limited experience with it.
`isInstanceOf` is already a crime against OOP and anyone who will maintain the code, let alone `isInstanceOf[.type]` :). So, maybe, it's good that it's difficult.
I disagree with the example ``` def apply(name: String, startDate: LocalDate, endDate: LocalDate): Option[Conference] ``` What you should ask for is a `LocalDateRange`, that contains `startDate` and `endDate` with the contract that `startDate &lt; endDate`. In turn, `LocalDateRange` can be constructed as close as possible to e.g. the user input. Otherwise, there will be plenty of situations where your code *knows* that `startDate &lt; endDate`, and you will have to clutter your code with `Option.get`.
That `Map` is painful 
i think it is not just scala experience it is total work experience 
I think its doable, but the result wont look nice, since `check` is an isolated function call. 
Yes, in a pedantic sense. You can unwrap the Scala wrapper to get to the underlying Swing doodads, and then you can use the DnD support in AWT/Swing which exists but is horrible. So yes it's possible but you may set yourself on fire and jump out the window before you get it working.
I think this is right. Facades and implicit bedazzlement are very limiting as a general solution (but are sometimes good enough for a *particular* use case). The free monad approach (like doobie) works well for "flat" low-level APIs like JDBC, which has a small number of data types and not much in the way of abstraction. I think it's the best you can do but it requires a compliant underlying API. For something like Swing the issue is that (a) the API has a huge surface area and everything is connected, so you either have to limit your coverage and make some programs inexpressible or you have a gigantic engineering task ahead of you; and (b) it's an awful API with a lot of bad abstractions, and it's very hard to paper over a bad API with a good one … you'll be wrapping *and* redesigning Swing at the same time which is a tall order. This is why scala-swing is kind of hopeless.
As I mentioned on the blog, [refined](https://github.com/fthomas/refined) might be very useful for this problem too.
Yeah, refined is awesome! :)
I have sort of love/hate relationship with ScalaFX. It fits my goal to make small-scale UI applications, but I hit its facadeness often enough to be annoyed: - No OOP like in JavaFX. To build a custom component in JavaFX I can extend provided classes, override some methods and be done, but for ScalaFX I cannot do that because overriden methods won't be called by JavaFX. You need to make a Frankenstein of Java class with convenient API OR make two classes, one for actual work, and a wrapper. I don't like having to wrap my own classes! - Property has too few operators... Yes, it makes a nice DSL for quick examples, but it is a huge pain to do anything specifically, *even* if you add properties to your model (which you wouldn't probably do in idiomatic Scala), because you need `map` and `flatMap` fairly often. - ...and extending it is a pain because of abstraction leaks Since I wanted more operators, I wrote thin sugar for Monix observables and, alternatively, cats.Monad instance with some conversions. And I bump into abstraction leaks all the time doing this: - Properties have two type parameters, one for backed Java type which I don't care about in Scala. But I have to, because of primitive wrappers like IntegerProperty. Hit it with `cats.Monad`, because typeclasses don't play nicely with existentials. - The conversion is shallow. Some objects expose `javafx.collections.ObservableList[SomeJavaFXType]` or `javafx.beans.ObjectProperty[javafx.collections.ObservableList[SomeJavaFXType]]` instead of ScalaFX list of ScalaFX objects. So I end up using weird tricks like implicit conversion chaining I don't usually do. Maybe that's all because I don't want to completely embrace JavaFX, but I totally got used to FP in my model. A good example of "wrapper"-ish library IMO is [better-files](https://github.com/pathikrit/better-files), if you're not completely sold on FP. It gives expressive and clear API on top of `java.nio` with not so much code, but it's not mechanical translation either. A good ScalaJS facade is something like [RxScala.js](https://github.com/LukaJCB/rxscala-js) - not only adding types, but also aligning API to that of standard library.
With scalaz or cats you can turn each element into single-element map, then add the maps together. I think this is a nice way to express things. scala&gt; "banana".toList.foldMap(k =&gt; Map(k -&gt; 1)) res1: scala.collection.immutable.Map[Char,Int] = Map(a -&gt; 3, b -&gt; 1, n -&gt; 2) 
ScalaFiddle works well for this: https://scalafiddle.io/sf/gKgxQY0/1014
Hey there, We use sharded and persistent actors (Akka Cluster Sharding + Akka Persistence), we do an upgrade by bringing up the new version of the application along with the old version of the application and then slowly cut down the older version of the application. Since the persistent actors are sharded, they will automatically migrate from the old machines to the new machines thanks to Akka Cluster when the old machines are removed. Your application really needs to be cluster aware to do this seamlessly (without downtime) otherwise you will end up with 2 of the same persistent actors coming up at once which can definitely cause issues when it comes to writing to the journal and discrepancies in in-memory state. When the actors are cluster aware (sharded or singleton) the Shard Coordinator will take care of buffering messages that happen to be coming in during the migration and will deliver to the actor once the migration is complete. 
Thinking again... the `A &gt;: Null` bound in `noption` is misleading, because the implementation actually cannot store a `null` value, while the type constraint hints at the opposite conclusion.
It is not the job of `Conference.apply` to validate a time frame. Let's say you have a `class RadioBroadcast(name: String, url: URL, startDate: LocalDate, endDate: LocalDate)`, with the same `apply` method returning `Option[RadioBroadcast]`. You are then writing def getRadioBroadcast(conf: Conference, reservedURL: URL) = RadioBroadcast.apply(conf.name, reservedURL, conf.startDate, conf.endDate) .getOrElse(sys.error("should never happen")) because you know that `conf.startDate` and `conf.endDate` define a consistent time frame, but you have no way to pass this knowledge to `RadioBroadcast.apply`. Then, `.getOrElse(sys.error(""))` becomes a pattern. I know two alternatives: - Encoding invariants in new data classes, such as `TimeFrame(startDate, endDate)`, where the validation occurs at construction. This has the disadvantage of additional allocations and indirections (a price I cannot pay in my work, but which is OK in the majority of cases), - Encoding knowledge in implicits such as `IsLess[startDate.type, endDate.type]`. Then `Conference` can provide an implicit about its members. Disadvantages: the method definitions become overly verbose, and the encoding only applies to stable values (for example, if `Conference` has `def startDateGMT: GMTDate` and `def endDateGMT: GMTDate`, you cannot construct the required implicit about `GMTDate` pairs without allocating an additional object to provide stable instances of the `GMTDate` objects. - Other tricks ? Edit: formatting Edit2: BTW, thanks for the blog post. I think a discussion on how to encode and pass invariants is badly needed. I resigned myself to the impossibility of encoding some invariants in an efficient and practical way. Ffor example, how do you write a safe `BigInt` type? You implement `def divide(lhs: BigInt, rhs: NonZeroBigInt)`, and now you have to encode the ring axioms using the type system. This comes up in many contexts: a typesafe linear algebra library needs to know about a matrix being square, being invertible, and to pass the info through operations such as vertical concatenation or addition.
Use foldLeft. fooSeq.foldLeft(Map[Int, Double]()) { (acc, i) =&gt; i.id match { case Some(v) =&gt; acc + (v -&gt; i.count) case None =&gt; acc } }
Do you mean changing the function inside filter()? The result would still be a Seq[Foo] which wouldn't really change anything
Thanks. If I were to change `mySeq.flatMap(x =&gt; x._1.map(y =&gt; (y, x._2)))(breakOut)` to `mySeq.flatMap(x =&gt; x._1.map(y =&gt; (y -&gt; x._2)))` would the breakout still be needed?
Because you want the optionality (so to speak) of your `flatMap` to be controlled by the optionality of the `id` field.
This is probably a good way to do it but unfortunately in the real life case, my Foo has about 13 more fields :(
To me this suggests a fold since you are iterating over a Seq[Int] and reducing it into something other than Int. I think it is about the same time complexity as the groupBy method, which I like from a readability perspective, but prefer this way from a purely functional perspective. Seq(1, 1, 1, 2).foldLeft(Map[Int, Int]()){ case (m, n) =&gt; m + (n -&gt; m.getOrElse(n, 0) + 1) }
`a -&gt; b` and `(a, b)` are both syntactic sugar for `Tuple2(a, b)`. The point of `breakOut` is to collect the result of that `flatMap` directly into a Map. Try it in a REPL. scala&gt; import scala.collection.breakOut import scala.collection.breakOut scala&gt; val mySeq = Seq(Some(2) -&gt; 8.0, None -&gt; 4.4) mySeq: Seq[(Option[Int], Double)] = List((Some(2),8.0), (None,4.4)) scala&gt; val m: Map[Int, Double] = mySeq.flatMap(x =&gt; x._1.map(y =&gt; (y, x._2)))(breakOut) m: Map[Int,Double] = Map(2 -&gt; 8.0) Alternatively (instead of using `breakOut`), you could use `.toMap`, but that takes an extra step.
You can pick relevant fields first, e.g. fooSeq .map(foo =&gt; (foo.id, foo.count)) .collect { case (Some(id @ "myid"), count) =&gt; id -&gt; count } .toMap
Yep, sadly, Scala lacks a syntax for extracting only some fields, by name. I find this rather ugly, but you could do: `fooSeq.collect { case f: Foo if f.id.nonEmpty =&gt; f.id -&gt; f.count }.toMap` Also, f you find you're using a particular projection to only a couple fields of interest pretty often, you can make your own extractor: ``` object FooCount { def unapply(arg: Foo): Option[(Option[Int], Double)] = Some((arg.id, arg.count)) } ``` Then you can do: `fooSeq.collect { case FooCount(Some(id), count) =&gt; id -&gt; count }.toMap` I've done this in cases where I have a data type that has a whole bunch of metadata for external purposes, but my internal logic tends to key off of just one field.
Where are the SBT nerds now? You've got nothing to defend your Stupid Bullshit Technology. FP is joke, SBT is a joke, how about you take your failed experiment and go a ruin another language and let us REAL programmers go back do doing REAL work. #\#MAKESCALAGREATAGAIN
See the [adjacent topic](https://www.reddit.com/r/scala/comments/5m1igc/how_can_i_create_a_map_of_counts_from_a_seq_ie/) for some options. 
You forgot to mention about forward compatibility issues with protocol classes. Can be PITA if not implemented correctly.
Note that you can express that kind of pattern as a `for`/`yield` if you find that clearer: (for { x &lt;- fooSeq i &lt;- x.id } yield (x.id -&gt; x.count)) toMap
This is the probably the simplest solution and probably what should be used in most cases, but it's worth pointing out that [the solution](https://www.reddit.com/r/scala/comments/5m1igc/_/dc0q7md) given by u/deportes can be much more effecient in memory if the original Seq is very large with a relatively small number of unique values.
Right, but until it's suitably "improved" can people stop recommending it to newbies? I'm happy to work on cross-build support for maven *and it seems like this person already added cross-build support to their gradle build*, which is the one thing that people absolutely had to use SBT for (not that newbies should be worrying about cross-building in the first place). And can we build our other pieces to work with other build systems? There's a huge list of things I'd like to try out - MiMA, the "scala platform", scala.js, typelevel scala - that I'm not going to touch it if the only way to use them is with SBT. If I'm forced to pick between participating in the wider scala community/ecosystem and being able to use maven, I'm going with the latter.
Are you referring to the Map in `DataParserActor`? It's definitely not the most efficient way to get counts. I chose the method I used for clarity's sake. if this were running in production something like groupBy would be a good alternative
Thanks for expanding on why having a separate type for the time frame is useful. Again, I'm not disagreeing with you. The `getOrElse` is indeed undesirable, and the optional value should instead be used as the return value for `getBroadCast`. However, as you pointed out, by modelling a time frame as its own thing you can avoid the extra invariant validation check, thus the construction of the `RadioBroadcast` doesn't require an optional type (provided that there's no other validation checks necessary). There's always a combination of fields that make sense in multiple contexts. The combination of a start date and an end date as a time frame is especially a reusable concept, thus very useful to model as a separate entity. However, sometimes you only need an invariant in one datatype, so you can be pragmatic and only implement it locally and refactor later. In any case, I think it's more important to first make sure the invalid instances cannot be created in the first place and that the validation errors are represented in the type level. This is something I've seen a lot of devs struggle with. IIRC, even the official Scala documentation recommends using `require`. With this detail covered, we can focus on techniques for modelling datatypes and implementing validation mechanisms. I've seen quite a few techniques and tools recommended already (dependent types, the refined library etc.), and I think the choices largely depend on the situation. Thanks for your feedback! There are a lot of ideas for a follow-up blog post. ;)
I have to agree with the thrust of this article. In our company we've abandoned SBT and switched to pants (personally I'd have preferred gradle, but...anything is better than SBT). SBT's deficiencies become more stark when you compare it with other tools like npm, cargo, gradle and, heck, even maven. 
&gt; It is true that SBT lacks simple way to exclude tests based on pattern. Easiest way probably would be to put your unit tests to separate packages too At that point you might as well just put those sources into `src/it/scala` and use the built in integration test config.
there is no god
&gt; testing at the highest possible level and only test lower levels for specific stuff that barly changes This is how I've been doing it, I think. I mostly write high-level tests. Since I know what's going on in the implementation, I know what high-level things to test that may uncover corner cases in the lower levels. Similarly, when I find a bug I add high-level tests that exhibit it. The advantage is that the underlying implementation can change quite freely and all the high-level tests are still valid and do not need to be adapted. Besides, when coding in a type-safe FP way, low-level building blocks tend to be correct by construction. Errors are more likely to be found in the combination of functionalities. Does this approach have a name? 
I don't consider 2.10 support important enough to undermine the quality of the API for newer versions anymore. But I'm open to suggestions to support 2.10 as long as it doesn't compromise the API quality, IDE-friendliness and performance of newer versions. &gt; without the conversion for the operations. So far I haven't succeeded (trying all sorts of hacks to direct the implicit search). I have tried before, and I haven't succeeded. However, scala-unboxed-option has been designed as a potential *replacement* for `Option` in the standard library. If that works out, the necessary implicits can be put in `Predef`.
[removed]
Thank you for the comprehensive reply. **Releasing** It seems I missed this plugin. I have to check it out. **Publishing to maven central** The approach with .gitignore is a bit inconvenient. As for per-user config, that's may work I didn't expect that a part of my building code can be located somewhere else - and this feature scares :) **Test exclusion** The reason I gave up on that step is because when I opened that documentation they're already referring an alternative way to configure the project (via `(project in file("."))`... ) that leads: 1) I needed to break what I had already configured 2) I needed to go back to the beginning of the manual and start learning this DSL. **Project properties** I wanted to have a per-user properties file that contains all sensitive information in one place. This solution with an override-able option and per-user config may help. Thanks. 
Yeah, `Option` is not the type I would use either. The Scala ecosystem provides a lot of other options for representing validation results: Scala `Either`, ScalaZ disjunction, ScalaZ validation etc. Personally, I would expose a "safe" API (not use exceptions) even for functionality where you can get undefined behaviour based on just invalid input. Even if the user just does a `.get` or something on the result, they at least have to do it explicitly.
You may find the following of use: http://aperiodic.net/phil/scala/s-99/
&gt; SBT's deficiencies become more stark.. Do you have specific examples he didn't outline here?
&gt; Now if there was just a way to flip the tag from Checked to Unchecked instead. Otherwise you'll have to remember to use the Checked tag everywhere in the code base. In shapeless, `A @@ T &lt;: A`, so every `Conference @@ Unchecked` would be a `Conference`, which sort of defeats the point. In Scalaz they are unrelated and require explicit unwrapping, but it sort of defeats the syntactic ease ("use anything but it might drop the tag") you get with the latter. It might work if you require that all actions / DB writes require valid data, so you will be forced to either change parameter from `A` to `A @@ Checked` or return value from `F[B]` to `F[ValidatedNel[String, B]]` in your domain methods, probably pushing the former as far as you can get away with. &gt; I usually have a different model for representing the form input data. Sure, there's a bit of repetition there, but then again the form input rarely matches the data structure I'm after either. I guess it depends on a domain or specifics of your forms. I'd avoid repetition if it's the same domain type and it's possible to. &gt; I think it's worth sacrificing a little bit of developer convenience for safer code. &gt; What I've found is that making sure all the exceptions produced by the require statements are captured is a humongous task in large code bases involving multiple developers of various skill levels To me it's a choice to be made between (cost of a bug, frequency of a bug) and (cost of reading / writing ceremonial code, difficulty of getting it right) that you can decide based only on your situation. But it's interesting to try to find a way that is *both* safe and convenient! (e.g. if risky use-cases are needed often enough for debugging or tests with fixed data, there can be implicit allowing `import domain.validation.enableUnsafeYolo` ... `val checkedConference = Conference(...).yolo_!` that is easier to spot during review)
You're welcome. I might be wrong at some points, being still a noob. BTW you can just use java system properties for your own config like [shown here](http://stackoverflow.com/a/37597406/2758343) or wrap your property-reading in `lazy val` instead of setting, so it can be used not only in settings or tasks. Your use-case is probably nonstandard enough from SBT design point of view to justify few lines of Scala. You also can mix `(project in file("."))` with flat-style config, e.g. // these three will be added to root settings scalaVersion := "2.11.8" enablePlugins(ScalaJSPlugin, WorkbenchPlugin) libraryDependencies += "com.thoughtworks.binding" %%% "dom" % "latest.release" lazy val root = (project in file(".")). configs(IntegrationTest). settings(Defaults.itSettings: _*) But that's why I gave most simple option requiring no build changes in the first place :) 
OP's question is not good interview preparation.
&gt; if you are preparing to interview, my advice is: please, don't. Well, you did. But I understand what you meant now.
I was wondering whether you could expand on the relative merits of using `sealed abstract case class` versus scalameta's `@data` annotation macro? Using `sealed abstract case class` seems very lightweight but has the disadvantage of being able to produce illegal values within the same file the datatype is defined. Scalameta's `@data` annotation macro would not have this problem (and could therefore be viewed as safer) but requires an extra library dependency and compiler plugin. I'm not sure whether there could also be issues with IDE support. 
I actually just wanted a list because I was curious lol . I am currently working with scala . The reason I wanted the list was to kinda test my understanding of the language a basic to intermediate level. Like when Im with friends sometimes we just do ctci problems or other coding problems.
I hate not being able to easily script in maven. I really like the moduleid dsl. I really like incremental compilation. I like typed keys. Gradle isn't typed. Maven isn't flexible. Sbt has weird dsls. 90% of what you do in most builds is trivial copypasta anyway. But I've been using sbt for 5+ years, so I'm pretty well-versed by now. It took about two weeks to learn enough sbt to read any build out there. It takes me ten minutes to set up a complex multi-project build. Just go read the manual, hit the gitter room. I really need to write another explanation of sbt and contribute to the project, because the hand-wringing over sbt has reached ridiculous proportions.
I think the number one reason would be because they want to write their builds in the same language as their project. I mean you've decided Scala is the right language for your project -- why would someone writing a compiler plugin or build tool extension not come to the same conclusion? I mean Scala is fairly well respected for making DSLs, it's easy to compile on the fly, etc. I can see why some of the design choices in SBT throw you off, but all in all I still want to code in Scala, not Groovy.
The Scala community is toxic!
https://github.com/Jarlakxen/Scala-Interview-Questions might be helpful.
&gt; And having to do customizations in Maven left me frustrating much more than SBT ever did, Yeah it's very non-customizable. When working with a lot of projects I've found that's much better - every project's source is in the same place, every project builds in the same way. If you really do need a custom build step it's not a lot of work to encapsulate it as a plugin, with the advantage that (unlike far too many complex-logic "build scripts") it will naturally be first-class code that goes through your normal testing/release process. &gt; I'd like to see a pom.xml that is not "a lot of boilerplate". You use inheritance (or, in the worst case, profiles) to pull out the boilerplate. There's a bunch of boilerplate once, but only once.
It seems to be cultural/social more than anything to do with actual functionality. That said it's worth saying that SBT has relatively good support for cross-building for multiple Scala versions, and the high-profile projects/people are disproportionately libraries that need that.
Type members are intended to already do what you want - the point is that each subclass of `Geometry` will contain its own `Circle`, `Line` and `Point`. So rather than having `AbstractCircle` and `Bar` be subtypes of `Geometry`, put them inside `Geometry` or inside traits that can be mixed into `Geometry`. (The problem you have is that a `Bar.Circle` is a different type from a `c.Circle` because they're different subtypes of `Geometry`) Edit: look at the "cake pattern" for the general case of this.
The ones outlined here hit all our major issues. I think the biggest frustration we have is that the SBT mental model is opaque and results in people cargo-culting little snippets of "how to get things done". 
This is just as common in Gradle in my experience. Especially as you move away from their core competencies where the docs aren't as solid or complete. I do admit to reading sbt source to answer questions but only a few times over the years. Not sure I agree it's quite as opaque as you let on if you've read the docs. 
Fix and Mu are new to me, and I think it will take me some time to fully wrap my head around them. I found this somewhat helpful ( http://typelevel.org/blog/2014/04/14/fix.html ) but it is still fuzzy. I was able to get this working just using Coproducts and a really strange natural transformation (which in writing had me kind of appreciating the idea of Fix, in that I was expecting I'd hit a wall without it,) but I'm curious if you see drawbacks to this approach vs one based on your sample. https://gist.github.com/beezee/a53c52d537ca38d25da94f0e7b9a8fdd Also if you have any recommendations for coming to terms with Fix and Mu, examples, resources, etc, I'd love to hear.
It's not undefined behaviour. In cases where failures indicate a programming error, it's best to fail-fast, let it crash, rather than allowing the error to propagate. The point of option/either/etc. is to allow you to handle a bunch of failures in a uniform way, but when there is no reasonable way to proceed, failing is already that uniform way.
FWIW I've started fiddling with an implementation. I'd forgotten that Scala lacks kind polymorphism which makes doing this kind of thing at the natural-transformation level pretty tedious - e.g. https://github.com/m50d/tierney/blob/master/core/src/main/scala/tierney/core/package.scala is an implementation of `Fix` and `cata` for `*` and `* -&gt; *` types, but one would have to copy/paste the implementation for every other kind one wanted to support, and even supporting for `* -&gt; *` I had to write my own `Functor1` type (which should probably be called `FunctorK` - my IDE is struggling with renames at the moment), which in turn meant that in testing I had to write the instance by hand rather than deriving it via kittens. So I'm not sure this strategy is viable, but I guess I'll write what I can and release it and see if it's useful to anyone, and I'll see if I can make the `Free` structure in this approach if only for my own interest. (Also the `Partial*` stuff is nasty enough that I should probably bite the bullet and start using kind-projector). On a quick look your gist seems like the right way to do this, and probably easier to understand than my approach - `NodeF` and `RecProg` are mutually recursive as they should be, you've just done it directly. I would define `Prog` as `Free[NodeF, A]` - or, thinking about it, even `Free[ApProg, A]`, because we can't destructure the free monad structure at all anyway. Doing it this way makes the representation a little more canonical (i.e. your structure allows more ways of expressing semantically the same thing), and should be equally usable if one provides suitable helpers. I think. The advantage of my approach would be being able to use a library of existing traversals, but that library doesn't actually exist in Scala yet, so meh. Everything I know about the fix-point stuff is based on the haskell "recursion-schemes" library. There are a few useful tutorials, though mostly in Haskell; http://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/ was one that made sense to me.
I don't find it constructive to punish programmers of their own errors with easy to miss runtime errors. :-) Sometimes these programming errors aren't. Maybe the input to the function comes from somewhere else, and the programmer forgets to filter the invalid inputs. That can be pretty hard to catch during code review. Rather than shift the burden of remembering to filter out the bad inputs, the function should instead return a value to all given inputs even if it just says "bad input" for invalid parameters. The programmers themselves can then decide whether to just throw an exception or someway handle the situation. There is however a class of inputs in Scala that I would generally not consider productive to return a special return value for or even validate the input: nulls.
Erroring out is a lot harder to miss than returning `None`. Making all your functions return `Option` is just repeating the mistake of `null` all over again and bad for the same reason that `null` is - it allows errors to propagate to the point where they're much harder to diagnose.
Hi. God here. WTF man! I DON'T CARE about programming languages. The last code I wrote was in SNOBOL and I fucked it up and caused a massive flood. I gave up programming after that. Seeing all them dead bodies floating in the muck really shook me up. But seriously if I HAD to write something up I'd probably go with Lua. My kid seems really into it, but he's such a damn hippie he's always obsessing about something. Most likely I'd just trick you suckers into writing it for me.
I disagree. Nulls are invisible to the type system, which is why they propagate to wrong places easily. `None` isn't invisible to the type system, thus you can't accidentally pass it as a parameter to a function that doesn't expect an optional value. Also, there's no reason you couldn't use another type besides `Option`. For example, you could use an `Either` or a custom ADT to better encode failure cases. It's true that `None.get` errors are harder to decipher than thrown errors, but those errors are still only captured because you detect them at runtime. Wouldn't it be nicer if you could eliminate those programming errors during compilation phase or code review? During runtime, you can also use descriptive error messages when you unsafely unwrap results.
Why would you not prepare at all? Personally I find the process of going through Scala resources/questions in my head really helpful before going to an interview, it helps me articulate the answers better in an interview environment if I have already thought about them beforehand (it's usually telephone interviews for the low level technical screening exercises). I don't get why you would not prepare, sure preparing for specific questions might not be great but if the questions expose concepts that are common then articulating them under pressure is much easier. Failing to prepare == preparing to fail comes to mind :) 
&gt; None isn't invisible to the type system, thus you can't accidentally pass it as a parameter to a function that doesn't expect an optional value. Sure, but what are you going to do with it, realistically? If you have lots of functions that return options it's all too easy to toss them all into a big `for`/`yield` block, and then at the end of that you get `None` and have no idea why. &gt; Also, there's no reason you couldn't use another type besides Option. For example, you could use an Either or a custom ADT to better encode failure cases. Or you could fail immediately, which gives you even more information (a full stack trace, usually). &gt; It's true that None.get errors are harder to decipher than thrown errors, but those errors are still only captured because you detect them at runtime. Wouldn't it be nicer if you could eliminate those programming errors during compilation phase or code review? But if you're using `Option` you're not eliminating them, you're just allowing them to propagate - you detect them even *later* than if you'd errored during the function that returned `None`.
I've had a good experience using Protobuf for my communication as well. And then you use all the same tools as for managing compatibility as with your stored data.
I don't follow. What do you mean "put them inside `Geometry`"? How can I force all particular implementations of `AbstractCircle` to only accept and return `AbstractCircle`s of the same type in the `foo` method? None of the "cake pattern" examples I have found do anything like this.
&gt; What do you mean "put them inside Geometry"? As in: trait Geometry { trait Circle { def foo(c: Circle): Circle } } object Planar extends Geometry { class PlanarCircle extends Circle { // may not even be necessary override def foo(c: Circle): Circle = ... } } object Spherical extends Geometry { ... } You can have types and subclasses and mixins and so on if you need them, but if you just want to keep the distinction then you don't need any of that - by putting `Circle` inside `Geometry`, `Planar.Circle` and `Spherical.Circle` are different types and the compiler will enforce that you don't pass one where the other was expected.
Ahh, yes. Marking it `private` does indeed work. I suppose it also makes the `sealed` modifier redundant.
When I first tried SBT about 5 years ago, it was a disaster; most of the examples on the net were for SBT 0.7 and the build files were incompatible with the current version, which was 0.10. WTF? After a few days, I gave up and used maven. I joined my previous employer about 3.5 years ago, the build guy tried to get everything working with gradle; gave up and used maven. SBT was ruled out from day one. I believe the issue was we needed to mix scala and java in the same project. My current employer uses scala + gradle. None of our projects use SBT.
If the return is a return - a reasonable value for the function - then using Option/Either/etc. makes sense. But if it's just "you called this wrong" - if the only meaningful way to handle it is for the programmer to fix their code - then better to fail fast and give them as much information as possible.
I feel like Swift is about 5 years behind Scala. It's going through some pretty bad growing pains. It's probably going to close the gap more quickly than 5 years, but by that time, Scala will have Dotty and scala-native will probably be much further along. And much in the way today's Scala is a bit encumbered by decisions meant to accommodate the JVM, Swift is similarly complicated by Obj-C compatibility.
I would presume it will be easy to make dotty-native once Dotty itself and scala-native are mature. Is there reason to think this isn't so?
&gt; the SBT mental model This is my biggest complaint. That I don't have a clear idea of what it is doing or how it does it or why it is structured the way it is. And I'm amazed that the community hasn't produced a solution/complaint large enough to address this.
If you have some complicated processes that already have solutions in Maven, I can see why you would lean that way. No one wants to remake build tool plugins, let alone justify rebuilding them to managers. 
It hurts to be proven correct on a straw man joke.
Just drop the `PlanarCircle` altogether and [define everything inside](https://scalafiddle.io/sf/K1KYv9p/0).
You can still define type aliases and subclasses as you were doing. Just don't try to have classes that extend `Geometry` directly - rather, put them inside traits that do.
Perfect! This is exactly what I was looking for. I didn't realize that one could override type members by just specifying a class with the same name.
only Scala
Yes, because you move elements past the end of the sequence. In some circumstances, it could be worth representing the seq-and-group pair with more structure. Sometimes not. It really depends on context (which is the nuance that I was missing in the blog post).
Simply use collect: `tupSeq.collect{ case s @ (Some(k),v) =&gt; (k,v) }.toMap` [live Scala Fiddle](https://scalafiddle.io/sf/p47itYt/1) 
I like images 
 fooSeq.map(f =&gt; (f.id -&gt; f)).collect { case (Some(id), f) =&gt; id -&gt; f.count }.toMap or val counts = for { foo &lt;- fooSeq id &lt;- foo.id } yield id -&gt; foo.count counts.toMap
I know it's kinda offbeat, but I've used chisel which runs on top of scala to program FPGAs. At my university we're running cellular automata on an FPGA based supercomputer and it's currently being redone with chisel. It's worth checking out at least.
You're gonna want `BigInt`.
[removed]
[removed]
I can say the same... 
 val one1 = 1 &gt; The same function, using val and type inference". Good god.
Yeah I cant stress this enough, the original sbt was one of the first build tools that properly dealt with cross compiling. Also earlier versions of sbt were somewhat simpler in design plus some extra needed functionality (albeit a lot less than what we have now, hence why its design was simpler). These two things are probably the reason why it got so much traction in the early days. However since then, SBT has kinda morphed into some sought of monster
That's the type signature of the function. In Scala, type arguments are passed in []'s, as opposed to Java's &lt;&gt;'s. That's just saying that "one" is a method which takes no type parameters, no actual parameters, and returns an Int. 
 scala&gt; def z(one: one[] =&gt; Int): Unit = () &lt;console&gt;:1: error: identifier expected but ']' found. def z(one: one[] =&gt; Int): Unit = () ^ Thus the question remains...
I really don't get this. I love vim as a simple editor, but this is just too much for nothing, Intellij has a perfectly good Vim mode, if you actually like the input mode.
The world shall never know 
I was a vi/vim guy for over 20 years until I switched over to spacemacs about 18 months ago. For the most part it just works, although a few tweaks were necessary to get ensime into a state I wanted. The `hybrid-mode` is awesome because it allows you to use both vim and emacs keybindings. I tried repeatedly to get vim to play nicely with scala and finally just gave up and made the switch.
The author kind of explains the why in the intro paragraphs. But I'm with you. For me, the advantage of an IDE is that it understands my code and allows me to ask it questions about my code. Where is this function used? What classes derive from this class? For me, for anything but the most simple codebases, these sorts of things are essential.
That's what the REPL interpreter outputs after you define a function, not what you should be typing... 
So where should see this in Scala?
Just to counter the other comments, I use neovim for all Java and scala, and i'm quite happy. 
IdeaVim is ok but far from perfect. The editor (that wasn't designed for vim) has quite a few quirks regarding selection and modes.
Nope
I'm currently learning Scala and Akka (having previously programmed in various other functional languages) and I'm looking for some fun project to contribute to and cut my teeth on. Any suggestion will be welcome.
Also see http://www.cakesolutions.net/teamblogs/2011/12/22/merry-christmas and http://www.cakesolutions.net/teamblogs/2012/02/06/scala-is-not-ascii-art
https://www.meetup.com/Seattle-Scala-User-Group/events/220102798/ You're not the only one.
what exactly are you missing? 
Just a data point, but I'm getting kind of tired of these.
If you don't want to use the DSL you can write plan SQL with Slick. You can also ask questions on https://gitter.im/slick/slick. Or you can look at one of the many reasonable alternatives like Anorm, ScalikeJDBC, Quill, or doobie. You can probably find something that matches your desired style. Also, if you don't mind it would be helpful to take it down a notch. I know you're frustrated but the embellishments don't help your odds of getting a good answer.
Hi. I'm not actually looking for an answer because I know there is none. The library sucks and it's not going to change. It seems that everyone is using Slick nowadays so I'm stuck with it. I admire your cordiality, though. +1
For me, it was always http://naildrivin5.com/scalatour/
can't wait for your blog. I am on IntelliJ but looking for a way to switch to Vim
The slowdown and lag is more to do with sbt and scala than intellij anyways. Not like going VIM or any other editor really avoids that. Intellij does do some stupid stuff with SBT occasionally though that makes it suck more than normal.
Wow, at some point, this whole discussion got brigaded. Man, there are some fragile people out there. I only realized this after exploring Reddit and looking at my controversial comments :) I don't mind a few downvotes. But I feel for people who may already not feel particularly embraced by the Scala community, and the message they might see. Sadly, this a system in which a small number of crusty people willing to put in a little effort can dominate a much larger number of totally tolerant people who don't feel particularly pressed.
There are minor annoyances that you just need to learn to work around such as refactoring not putting the editor in input mode. Also the dialogues doesn't work consistently . e.g. Any input box is neither a normal input box nor a vim-enabled input box - you can't put it into normal mode for pasting anything, nor can you use ctrl-v. Maybe these have a fix if any of you know how. The other problem is a lack of plugin (architecture). Must-have plugins like vim-surround has to be added to the official ideavim codebase instead of being loaded separately
Eh? Those settings aren't configured in IdeaVim for a reason - you configure them from IntelliJ itself.
I'll agree with ya. There are so many things I want / need to learn more about in the software world, I'm starting to become more weary of using Scala again due to some of these libs and APIs. I just don't have the time to deal with it. Functional programming seems great, but next time I decide to write something in it, I'm considering alternatives like Kotlin or F#. Scala has had time for improvement, but all the little problems combine to make it a big no-no going forward.
I actually don't know anyone using Slick. Selection bias is a funny thing. As tpolecat pointed out, there's no shortage of good alternatives, although I think it's worth pointing out that there's nothing particularly hard to understand about either of your examples, given a bit of Scala experience and a bit of reading of Slick's documentation.
Despite that Intellij does not have a good Vim mode. I mean non of the IDEs have the good Vim mode; and if there is one, it's *the other editor*'s evil-mode. And that's probably I use evil-mode in Emacs and use Emacs keybings in ALL the IDEs.
Well, in my company everyone's using it (other than my team, that has legacy squeryl code). I was under the impression Slick was Typesafe's so that most people nowadays would be using it? If not, what's in your opinion the most widely used one? Or is there none? We're already quite pissed that squeryl pretty much died, the last thing we would want to do is to depend on something that sooner or later gets abandoned, again.. "and a bit of reading of Slick's documentation." -&gt; Some reading of documentation is always unavoidable, but what troubles me is that the guy in charge of slick just went through the process of creating yet-another-so-cool-and-smart-and-slick-dsl just because, when a simple API, with actual method names and without all that smartness would have made things easier for everyone.
Kotlin doesn't have any traction (and I doubt it will ever get) and F# is a complete dead end (plus, it's largely maintained and used by the MS ecosystem, from which nothing of great relevance has come from in the last years). But yeah, Scala has this thing where if there is one easy and one complex way of accomplishing something, the _complicated_ way will be chosen..you know.. they have to always be so smart about it..
That code is from like 3 versions ago
Why not? It has all the facilities that VIM has for text editing, including macros.
It has all the EDITOR features, it would be stupid to open files that way in Idea, why would you want to do that, same with execute and command. Nobody uses VIM because of the execute, command, autocmd and open features, because every editor can do that.
please write it. I use intelliJ+vim but intelliJ isn't that much helpful
To be fair, vim can (actually processes that vim plugs into) can understand your code in some languages. If there was anything as good as youcompleteme with clang++ support for vim, then I would use that instead of intellij.
Yeah, I wasn't trying to put Vim down. I used it for a while on a Ruby project, and I think I understand why people like it. I'm just saying that code inspection is a huge advantage that a lot of IDEs have. And for Scala especially, it seems really important to be able to ask questions like "what is the type of this expression?" and "where is this thing used?". If Vim plugins existed to answer these questions, it would be a contender in my eyes. 
You can do everything in Intellij IDEA with the keyboard... so that's not an actual reason.
You could build it up recursively. There's probably a `Poly` trick or maybe even something built into shapeless, but I can never remember that stuff and usually find it easiest to write the type-level function directly: sealed trait AsCoproduct[H] { type R &lt;: Coproduct } object AsCoproduct { type Aux[H, R0] = AsCoproduct { type R = R0 } implicit def nil: Aux[HNil, CNil] = new AsCoproduct[HNil]{type R = CNil} implicit def cons[H, T &lt;: HList](implicit rest: AsCoproduct[T]): Aux[H :: T, H :+: rest.R] = new AsCoproduct[H :: T] { type R = H :+: rest.R } def apply[H](implicit acp: AsCoproduct[H]): Aux[H, acp.R] = acp } val witness = AsCoproduct[String :: Int :: Boolean :: HNil] type t = witness.R (untested but hopefully it gives the general idea) That said, can I ask why? What's the use case for this kind of "dual type"? Generally you'd want to also invert some other aspect of the type, e.g. a producer for `String :: Int :: Boolean :: HNil` could be connected up with a consumer of `String :+: Int :+: Boolean :+: CNil` or something like that.
it's not intuitive IMO. For example, using emacs keybindings in Intellij C+x C+f opens a file easily; or in other cases I would rather not press ESC to do some lightweight moves. Also ex-mode is disgusting.
First-class sum types should theoretically help a lot. If the compiler knows it's representing `A+B` and never has to actually represent `A` and `B` individually, it should be able to choose some kind of efficient representation involving tagging (which using `null` for `None` is a special case of). Compare the OCaml runtime representation (complete with funky 31-/63-bit integers), which also involves giving up "everything is an object" (which Scala already does to a limited extent with `AnyVal`). Fundamentally there are always hard tradeoffs to be made - in the case of `null` limiting ourselves to a 31GiB, 1023 MiB, 1023 KiB and 1016B heap and representing an optional value as a single reference is clearly better than using two references for that representation allowing up to a 32GiB heap, but with more complex sum types the decision becomes less clear. But this kind of optimization decision is the compiler's job, and one can hope the compiler would be ok at it. Whether this stuff is even possible on the JVM (yet alone with Java compatibility at the level of e.g. giving Java reflection access to all Scala types) is another question.
what's the problem with their custom classloader? (just asking, I've never had problems with it)
The point of vim is to use your keyboard (effectively) and avoid the mouse. 
If you have a more complex startup prodedure (e.g. create sockets to some APIs, start an actor tree, schedulers, or what have you) then Play will not really shut down everything when you press Ctrl+D. A lot of garbage gets left behind, and when you modify classes and re-start the application, you will run out of heapspace or metaspace after doing the same a few time.
Wow...I thought it was me doing something wrong...I've been running into huge memory leaks during development, and this could explain a lot. Also, I seem to leak connections when my Postgres database throws exceptions that are uncaught during development. Every once in a while I have to fresh restart sbt to continue. Unfortunately, I don't have the time nor the patience to track down leaks, so I haven't been able to report them. 
You won't find much when looking into it, moslty stale/closed bug reports and people blaming other frameworks/libs. This only affects development, though. Play is pretty good otherwise.
[This](https://playframework.com/documentation/2.5.x/ScalaDependencyInjection#Stopping/cleaning-up) doesn't address the problem?
Ops. At then end of the article, you have a big Alluxio logo with a Poll that goes "Have you heard of X". I'm sorry, this article looks good and made me believe it's an ad. I retract the above statement.
No, I never said that "IdeaVim is perfect". You are putting words in my mouth. IdeaVim is an imitation of Vim, because it is an imitation, or *emulation*, it will be missing some things that cannot be replicated in IDEA. When it comes to basic editing commands, I'd say most of it is there. Out of all vi emulation layers I've grown to like IdeaVim and evil the most.
In the interest of watertight abstraction, it seems like the type-level encoding (sum versus tagged union) should be orthogonal to how it's represented in memory. I wonder if in some of these cases, the underlying representation should just be a sensible default, but with ways to override if you need to optimize. I think my concern is that in Scala, we tend to resort to incredible lengths of hackery to get a result through the existing language features (e.g. Shapeless), rather than modifying the underlying machinery. I totally get the argument that it often makes sense to experiment on this level, but it still feels a bit suboptimal, somehow.
FP is a discipline in Scala; the compiler can't help you. So the way to ensure purity is to always be pure in your own code, and only use pure libraries. Check out typelevel.org if you're interested in this kind of programming.
Sorry, I associated to the top commenter's words: ["Intellij has a perfectly good Vim mode"](https://www.reddit.com/r/scala/comments/5mqbth/scala_development_in_vim/dc5kziz/?st=ixqn3utc&amp;sh=24c9c653). Btw, everything can be emulated in intellij, it's just a matter of effort. 
Meh. I mean, it feels like Scala should break compatibility with Java, but the only reason I'm working in Scala rather than OCaml in the first place is the compatibility with Java. The language has always been kind of a messy compromise. 
A quick way to check the purity of a function is to ask yourself if you can memoize the outputs of the function. Since a pure function always outputs the same value for the same input, you could just cache it. 
Maybe Scala will break compatibility. I feel like anything is in play now that the language is increasingly multi-platform, and Dotty is coming out. I'm complaining, but I think Scala is pretty great now, and has awesome potential. My hope is to make sure we don't accept that the way things have been done by routine is the way they always will be done. It feels like Dotty is going to provide an opportunity to take some risks and clean out some cruft, and I think it also provides the opportunity to raise the bar on some of the standards, such as zero-cost abstractions.
Heh, I don't share your optimism. The whole 2.12 cycle was focused on Java interop (irrelevant to my use cases); even if Dotty came out tomorrow I'd feel that Scala had stalled at this point and lost a lot of momentum. I hope it can bring it back, but at the moment I'm looking to alternative languages - Idris in particular is coming along very nicely.
That's exactly why to use [scalaz](https://github.com/scalaz/scalaz) or [Cats](http://typelevel.org/cats/). Rather, it's one of the reasons. A few I can think of are: 1. The functional programming community has identified many useful constructs decades ago and determined which ones are so fundamental that everything else should be composed from them. scalaz and Cats reflect these decades of learning. 2. Their developers deeply grok pure functional programming and have done the heavy lifting, so I don't have to. 3. In scalaz's case, the dangers inherent in Scala not enforcing purity that can creep into even the most carefully written code have largely been shaken out over years of development (and Cats benefits from this and adds its own approaches to safety—I'm confident it won't take Cats as long to shake out whatever issues it may have). 4. There are high-quality related libraries in use today: http4s, Doobie, Argonaut, Circe... These basically solve the problem: write pure functions, use the ones in scalaz/Cats, or use applicatives or monads to handle effects. When faced with a gnarly impure third-party library, develop your own free monad(s) for it, or at least wrap the darned thing in `Task`. It's not actually hard to develop referentially-transparent software in Scala in practice. The hardest part is really just understanding what that means and deciding to be uncompromising about it (uncompromising because referential transparency is a transitive property—any callee at any level violating it means your function violates it). So, to your point I think, assume non-scalaz/Cats third-party code you call needs to be made monadic.
Play contributor here: it's not the custom classloader. Play cleans up after itself, but does not know what resources you've opened up which should be closed when the application shuts down. You need to use `ApplicationLifecycle.addStopHook` if you have resources you want to clean up on application reload. Please see: https://www.playframework.com/documentation/2.5.x/ScalaDependencyInjection#Stopping/cleaning-up
You need to use `ApplicationLifecycle.addStopHook` if you have resources you want to clean up on application reload. Please see: https://www.playframework.com/documentation/2.5.x/ScalaDependencyInjection#Stopping/cleaning-up Regarding the postgres database exceptions, if you have a catch-all http error handler that can see uncaught db exceptions and return the connection to the pool, that should cover it. HikariCP has leak detection, but I don't think it covers this.
not to sound ignorant, but how exactly is neovim better than vim? afaik neovim doesn't give you any features at all (as the end user)
I ask out of curiosity for whether Scala's type system today can model it. From a practical standpoint, it also seems relevant for Scala Native.
Do you have any examples of an http error handler that would clean up after database exceptions?
Ownership solves a lot more than garbage collection. 
Your service should be cleaning up the DB exception rather than sending it back up the pike.
Eh. I find it quite simple to use. I use the code generator. It's really nice. There's no need to touch the output, or really even comprehend it. My application code has no operators, other than the `===`. It's just a bunch of for-comprehensions. The pattern is simple: 1) Take your parameters 2) Define your query &amp; projection or mainpulation 3a) Specify Scala code to run on the result and return to step 2 or 3b) Specify Scala code to produce your final result, often mapping rows to instances of your vanilla Scala class hierarchy 4) `run` your `DBIOAction` and get a `Future` back. 5) Proceed with your non-DB code It took a few days for it to click for me, but it's not rocket science if you approach it with an open mind.
I don't, sorry. I generally wrap database access behind a DAO, and then all DAO methods are wrapped in a block that handles the try / catch / finally for any resources there.
Take a look at [this ScalaWorld talk](https://www.youtube.com/watch?v=nwWvPeX6U9w)
who said anything about mouse?
There are a few [monadic region](https://github.com/retronym/scalaz7-experimental/blob/master/effect/src/main/scala/scalaz/effect/RegionT.scala) implementations around, which offer the same kind of functionality in a generic, userspace way rather than as a language-level builtin. To do it properly one would want to combine with [rank-2 types](https://apocalisp.wordpress.com/2010/07/02/higher-rank-polymorphism-in-scala/) to ensure that manager resources can't be carried out of their regions. An alternative approach would be [second-class values](https://www.cs.purdue.edu/homes/rompf/papers/osvald-oopsla16.pdf) which would be a language-level change AIUI. As far as I know neither of these has a full implementation, much less production-quality. All the pieces are in place, but no-one's quite put them together. (I might get around to it eventually, but at the moment I'm focusing on a recursion-schemes-like project). Edit: I should have mentioned the resource management functionality in fs2, which is somewhat similar to regions.
See [Scala-Offheap](https://github.com/densh/scala-offheap).
&gt; As far as I know neither of these has a full implementation, much less production-quality. [Scala-Offheap](https://github.com/densh/scala-offheap) is a full production quality library from the author of scala-native. Currently there is no scala-native port of this library which where I think it would find the most use. That said, I'm curious to the merits of a regional memory management versus a borrow checker. 
If you've got huge derivations, it can be sluggish, but we are working on a fix for that https://github.com/ensime/ensime-plugin-implicits Turning off the typechecker would be equivalent to turning off ensime. You can't complete a variable contextually unless you know its type.
I meant neither rank-2 types or second-class values is fully implemented AIUI. Scala-offheap sounds like it only offers simple blocks rather than regions that can be threaded across e.g. async computations? Though having the ability to manage memory at all is progress I guess. (FWIW the project describes itself as "experimental"). The concept of a borrow checker is broader - one could consider regions one particular implementation of a borrow checker. Monadic regions have the advantage of being monads that can work with familiar monadic control flow operators etc. A language-integrated borrow checker like Rust's can be integrated with the language-level control flow structures, compilation error reporting etc., but language-level functionality tends to be a lot more brittle and harder to reason about (since the user has to understand its semantics directly) than userspace functionality written as a normal library.
Oh cool, by the same person as scala-native. Probably not by coincidence :) I wonder if it retains the same aspects of safety as the ownership checker. I should say that I ask these things not really as an advocate for ownership. I've never tried it myself, and it looks a little confusing. But mostly just out of curiosity of what Scala's current type system is capable of.
I hear you. I share that concern somewhat, and I'm certainly not really doing much about it myself, other than chiming in with opinions. I think it largely comes down to the community attitude behind Dotty. Not really knowing much about Idris, would you care to give maybe like 3 sentences on how you see it being a better direction than Scala? I hate when people ask questions whose answers are much longer than the question, so no worries if you don't want to take the time :)
I do some of that in [this talk](https://www.youtube.com/watch?v=rBmx9NsNSXM) but the types are more fancy than what you describe above.
added into my playlist, for sure.
BTW this videos could be usefull too [Functor, Apply, Applicative, Bind and Monad by Oliver Daff](https://www.youtube.com/watch?v=3Ycp55QEbGM) and series [Functional Structures in Scala](https://www.youtube.com/playlist?list=PLFrwDVdSrYE6dy14XCmUtRAJuhCxuzJp0)
Thanks for the tips!
I haven't seen a video with the thing you've described, but I've read an article from... John de Goes, which shows something very similar to what you're looking for: http://degoes.net/articles/insufficiently-polymorphic
Ah silly me I need to run this : scalac -classpath "C:\Program Files (x86)\scala\lib Test.scala then run like this scala Test
I agree. Neovim uses a network protocol to handle plugins (as well as ui), which makes it really flexible. Lots of cool things! 
If you want to create "executable programs", i.e. a big JAR that includes all the libraries, look at `sbt-assembly`.
&gt; Thanks for your praise and valuable input! Thanks for responding! I dig the project, and am going to read on. If you're looking for help, I can't promise much, but I'm intrigued enough to at least be interested in helping in theory :) &gt; I like the idea, to be able to obtain the page tree configuration via an external service, but I guess that then you'd have to resort to reflection to build a page. I prefer to build the page tree in a (relatively) typesafe manner and to construct the actual page without reflection, but it would be a cool option to also be able to consume a configuration and build a page from it. We didn't use reflection; we did the mapping in deserialization of the responses from the page. Although, I never was completely satisfied with our approach. Basically, the configuration parameters of a component were bundled up in a class that extended a ComponentDefinition market trait. Every Component had a free-form constructor, and then its companion object implemented a ComponentCompanion interface, that specified (among other things) a `renderComponent` method, which took a ComponentDefinition and was responsible for instantiating the Component and rendering it to something roughly of the shape `Future[(Html, Seq[Resource])]`. In a rather sneaky way, Components themselves were injected with the ComponentBuilder, so that they could delegate some of their rendering to child Components. And so Components could be called for by the tree structure but also by each other. The ComponentBuilder also fed some overall page context data to all components. &gt; I also wanted to bind a resource directly to a pagelet, because then you can use a pagelet anywhere. That is, in any page tree or standalone, because a pagelet is truly self-contained. I take this to mean that the Pagelet could hypothetically work as an ordinary controller? If so, I'm not sure I see the benefit. Out of curiosity, what advantage is there to having a Pagelet defined by an Action? I'm ashamed to say I worked with Play over a year but never did bother to thoroughly learn Actions. I'm just wondering if it might be advantageous to only use Actions at the full page controller level, if they don't bring something irreplaceable to the Pagelet interface. &gt; I think to offer the option to stream a pagelet when it's ready regardless of the order is relatively simple on the server-side, but to offer a convenient set of client side scripts to deal with the complexity of any pagelet arriving at any time will require some thought. And after all, the biggest impact of streaming a page is that the browser can begin to load all resources which are referenced in the Html header while the server is still busy rendering the body. Ah, I see. I still think it's an admirable eventual goal, with the simple case maybe being an optimization. Interesting problem, though!
Uh, that doesn't seem right. You shouldn't have to explicitly tell the compiler where to find the standard lib. Are you sure you set up Scala properly?
[removed]
[removed]
Yes, please! better-files is great.
`scalac.bat` adds every folder and .jar file in the `lib` folder to `_TOOL_CLASSPATH` and then uses that as java class path. It expects the `lib` folder to be in its parent directory.
I've migrated my utilities from shell/viml/scala into a [single vim plugin](https://gitlab.com/stevendobay/vist). This makes working with scala and (neo)vim smooth for me. It helps to create sbt projects, build, jump between errors and sources, rename stuff by analyzing sbt compilation output and such.
which line in the bat file assigns the _TOOL_CLASSPATH to CLASSPATH ?. I manually did this (in the bat file) and it works, but could not see it being assigned in the bat file. maybe you could try running set command in your terminal, see what java or scala environment vars are set. upon opening a terminal I have JAVA_HOME , SCALA_HOME set. My scalac -version is 2.12.1
everything looks ok from echo that command C:\Users\tom\workspace_scala\fp_scala&gt;"C:\Program Files\Java\jdk1.8.0_102\bin\java.exe" -Xmx256M -Xms32M -Dscala.home="C:\PROGRA~2\scala\bin\.." -Denv.emacs="" "-Dscala.usejavacp=true" -cp "C:\PROGRA~2\scala\bin\..\lib\jline-2.14.1.jar;C:\PROGRA~2\scala\bin\..\lib\scala-compiler.jar;C:\PROGRA~2\scala\bin\..\lib\scala-library.jar;C:\PROGRA~2\scala\bin\..\lib\scala-parser-combinators_2.12-1.0.4.jar;C:\PROGRA~2\scala\bin\..\lib\scala-reflect.jar;C:\PROGRA~2\scala\bin\..\lib\scala-swing_2.12-2.0.0-M2.jar;C:\PROGRA~2\scala\bin\..\lib\scala-xml_2.12-1.0.6.jar;C:\PROGRA~2\scala\bin\..\lib\scalap-2.12.1.jar" scala.tools.nsc.Main Test.scala Not sure what this scala.tools.nsc.Main is though Here is the version that works when I explicitly place -classpath on the command line C:\Users\tom\workspace_scala\fp_scala&gt;"C:\Program Files\Java\jdk1.8.0_102\bin\java.exe" -Xmx256M -Xms32M -Dscala.home="C:\PROGRA~2\scala\bin\.." -Denv.emacs="" "-Dscala.usejavacp=true" -cp "C:\PROGRA~2\scala\bin\..\lib\jline-2.14.1.jar;C:\PROGRA~2\scala\bin\..\lib\scala-compiler.jar;C:\PROGRA~2\scala\bin\..\lib\scala-library.jar;C:\PROGRA~2\scala\bin\..\lib\scala-parser-combinators_2.12-1.0.4.jar;C:\PROGRA~2\scala\bin\..\lib\scala-reflect.jar;C:\PROGRA~2\scala\bin\..\lib\scala-swing_2.12-2.0.0-M2.jar;C:\PROGRA~2\scala\bin\..\lib\scala-xml_2.12-1.0.6.jar;C:\PROGRA~2\scala\bin\..\lib\scalap-2.12.1.jar" scala.tools.nsc.Main -classpath "C:\Program Files (x86)\scala\lib" Test.scala 
We are using better-files at work so this would be greats Beats the shit out of working with plain java.nio for sure.
We can start with anything under the hood. If the interface is idiomatic, easy to use, extensible, scalable - yes why not should check out. But it says that if forwards NIO exceptions. Not abstracted enough
I'm certainly not trying to start any Scala vs. Haskell drama, but just thought I'd share.
You're very welcome to help! I have a vague roadmap, but I'm also open to new ideas. So if you have something concrete in mind, let's talk it over. Of course you can also help to work on the tasks which I want to do next. &gt; I take this to mean that the Pagelet could hypothetically work as an ordinary controller? Yes. you can serve any part of the pagelet tree. You can render just a single paglet or any part of the pagelet tree. For instance, you could render the header paglet which consists of a navigation, metanavigation and searchbox pagelet. Even all the the resources these pagelets depend on are included. You get a fully working web page. In fact, each paglet can act as a complete page. If you build a page in a team, different members of the team can build their pagelets in complete isolation and later combine the different parts into the final page. This also narrows the scope a developer has to consider and simplifies development and debugging. &gt; If so, I'm not sure I see the benefit. I think there are multiple benefits. First of all, most of code you write is completely independent from Play Pagelets. A pagelet is just a Play Action. Anyone knows Play Actions and can immediately start to write one. Then, if sometime in the future, a better library comes along, it's easy to migrate, because almost no code you write is tied to the pagelets library. In fact, the only code which depends on the library is the the page tree declaration and the main Action of a page. If you clone https://github.com/splink/pagelets-seed/ you can see how a simple pagelets based app looks like and try it out. I think this is the best option to get a thorough impression.
Not strictly Scala-related, but it's obviously right up our alley.
oh, the famous quicksort that isn't! it may, on the surface, look like a credible qsort implementation, but isn't nowhere near as efficient. edit: to whoever is downvoting, this is what i'm referring to: where left = [ y | y &lt;- xs, y &lt; x ] right = [ y | y &lt;- xs, y &gt;= x ] that is not quicksort edit 2: http://www.informit.com/articles/article.aspx?p=1407357&amp;seqNum=3
TypeLead looks like a shell company set up to promote Eta. I'm not saying this is necessarily a bad thing but i'm not convinced this constitutes "commercial support".
When running on the JVM, yes. When running on node you'd use the node way, and when running naively you'd use a native way. So in crossplatform Scala you want a way to specify file operations that's decoupled from the details of doing them on the JVM.
That PR is to merge to master which I am not comfortable with. I would rather merge to a 2.12 branch. And, don't worry, better-files is not abandoned. I will be releasing new version this weekend.
Maybe it was this one from Graham Hutton? Can't check the video right now, but I remember him refining the program step by step throughout the lecture: https://www.youtube.com/watch?v=rlwSBNI9bXE Edit: oh, overlooked the scalaz/cats part. But the video is a nice lecture about pure functional programming anyway, albeit in Haskell.
It *is* quicksort, just not a very sophisticated one since it merely pivots in the center.
I build this, because I rely on it for my masters thesis and did not find any scala implementation. I'd be really happy if you give me some feedback to possible faults or bad style.
Cool thx for posting!
[The difference between this quicksort and what u/kpws would call "true quicksort" is that this isn't swapping array elements in-place but is instead allocating new lists.](http://stackoverflow.com/questions/7717691/why-is-the-minimalist-example-haskell-quicksort-not-a-true-quicksort)
Technically, using Action.async will return a Future[Result], which makes it easier to swap out the thread it's using i.e. you can use Future's map / recover etc. So it's more of a "thread boundry demarcation thingie" to use Action.async. You're correct that in practice you have to use a custom execution context if you want to do anything useful -- 2.6 will have a `CustomExecutionContext` class which should make this much more directly useful. The big deal is that even if you're using a blocking API, you can at least get that execution off the main rendering thread pool and onto a thread pool that's sized appropriately for blocking. Edit: oh hai @schmitch
Ah I see. Thanks.
&gt; So I'm with you there, but I'm a little bit dubious about the utility, since you'd still need to render an HTML document skeleton for most applications I can think of. The Html skeleton is automatically wrapped around the chosen part of the tree when PageletAction.async is used: def pagelet(id: Symbol) = PageletAction.async(onError)(tree, id) { (_, page) =&gt; template(page) } So a sole pagelet or any part of the tree can be served as a fully working html page. About your reconceptualization: It actually looks like a major architectural change which requires to rewrite large parts of the library. So please don't be disappointed, but at the moment that's not where I want to go. Currently I see Pagelets as an extension for the Playframework which is as non invasive as possible. Users can just write their code like they are used to, e.g. they write simple Actions. Then they can later glue it all together with the Pagelets Tree API. I always thought it to be one of the great strengths of the Pagelets project to rely on the existing mechanisms of the Playframework. Nevertheless I'm intrigued, but I think that I don't fully grok your idea. Can you maybe sketch it from the perspective of an API user. For example, how would the client code roughly look which renders a page consisting of some pagelets? 
&gt; Imagine you do not want to leave the password for launching all your countries nukes in the hand of the president
If the wrong error highlighting is easily reproducible, file a ticket on https://youtrack.jetbrains.com/ - then hopefully it will be fixed in the next update.
Easily reproducible? The basic stage code used to work without error. Now it does not. And your explanation is to file a ticket. No more.
If you can type up an easy example that shows the wrong highlight, filing a bug is the best way to get it fixed. Complaining alone doesn't help that much, unfortunately, even if I share your frustration.
If you're writing scala, you're presumably a software engineer. If you're a software engineer, you probably ship bugs occasionally. If you ship bugs occasionally, your users probably find them occasionally. Let them know, they're nice people. They're very responsive, and will probably help you rollback to the last dot version. 
Hi 2.12 support is now on Maven: https://oss.sonatype.org/#nexus-search;quick~better-files
Ah! I did not know that. Thanks! Anyway 2.12 support is released
Yeah it's pretty easy to get into it in a month like I learned it without even writing anything nontrivial in java I mostly used jruby for stuff most people used groovy for at that time. I learned scala as well as akka, spark, spray, netty, and a bunch of java libs, in under a month. I didn't get good at it for a while though. For FP well don't jump to scalaz or cats right away, focus on stuff like Partials, Currying, andThen, compose, higher order functions, lazy evaluation. Those are like the things that you can learn quickly and give you the big advantages FP promises.
Just file the ticket. It's the professional thing to do. Everything else isn't.
[OP's usual bug strategy](http://i.imgur.com/1rTBCth.gif)
another excellent language on jvm [idris](https://www.reddit.com/r/Idris/comments/5naa4i/idris_jvm_now_guards_against_java_nulls_using/)
I have no idea what winrar is using :)
Looks good, pretty clear. Is the prime a `BigInt` or a `String`? You say both in parts of the readme. Why MD5? Is it specified? Seems an odd choice for a new project. You might like to use a `StringContext` pimp for `BigInt` literals. Why is `BigPrimes#primes` a `List`? Wouldn't some kind of map be better? `LaGrangeInterpolation#L19` the `-&gt;` is misleading. `gcdD` should probably return a `case class` rather than a list? Or I may be misunderstanding. Consider parameterizing `SSSSOps` by `F[_]: Monad` (and the implementation uses `Either[ShareError, ?]` (kind-projector syntax)) rather than parameterizing by `Error` - that's more in line with what people would expect. There's a lot of cases when you're `foldLeft`ing with something that's literally a group (so certainly a monoid). Scala has an awkward relationship with multiple groups using the same base set, but you might consider representing multiplicative integers mod `p` (or the kind of linear-congruence stuff you're doing) with a wrapper type that has a monoid instance, and then you can replace a lot of the `foldLeft`s with just `sum` operations. Have a look at spire which may have the algebraic constructs you're using already.
WinRAR etc. use Reed-Solomon. [It's possible to view Reed-Solomon as based on polynomials over finite fields](https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction#The_BCH_view:_The_codeword_as_a_sequence_of_coefficients), so there may be some level of commonality, but it's certainly not the exact same algorithm - Reed-Solomon uses the plain data as the first n points for simplicity (which also means that if you only have one part of a shared archive with recovery you can still extract the first part of the data), whereas SSSS by design picks points that have no special relation to the secret so that you can't learn anything about it unless you have enough pieces.
I wasn't completely on-board until you pointed out the lack of completion signaling in `Traversable`, then I started nodding in furious agreement. 
At work right now, thanks a lot for your input. Definitely going to look into all of this later. Especially your suggestions to look for a monoid instead of the folds!
I actually didn't get that part. If the method returns it has completed, right? And presumably it throws an exception if it doesn't complete. So where's the problem?
[Strategic Scala Style: Principle of Least Power](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html) Having a *less* powerful `Traversable` gives *more* freedom to people who *use* a `Traversable`. In practice, I do not often implement new collections, but I definitely *use* them often. It is good for my freedom as a user to know that `Traversable` is constrained: that it is finite, and that calls to its methods will synchronously complete. There are use case for `Subscribable`-like things. But the fact that it is more powerful is not always a good thing.
What's the use case? Either I want my entire project to be binary compatible or not (then I use the sbt-mima plugin), but a single annotated class?
The distinction `TraversableOnce &gt;: Traversable` purely based on the trait inheritance is very odd, though.
Psst, he gave a talk on it at ScalaIO, and [it's on youtube](https://www.youtube.com/watch?v=xjNlyvdyAww&amp;feature=youtu.be&amp;t=38m16s). It sounds like the fix is going to be (or has already been) merged into Typelevel Scala, but not Lightbend Scala (yet).
&gt; I don't think I understand I don't have to have a separate package process or subproject to do this in sbt. I don't have to do that in Gradle, either, but untyped code bothers me. Whether it is an executable jar or a plugin, I still have to package up the code separately first to run it as a jar. As for multiple inheritance, I definitely get why they didn't do it. I haven't tried a separate profile for the spring-boot projects I've worked on, that sounds like a really nice trick. I could, of course, crib the spring boot stuff verbatim, which is what I've done before, but then upgrading becomes a pain. SBT can only have one root, as well, but you can provide dependencies via plugins that have auto-settings. And I think that is a really handy tool. I think SBT is a pain, especially with all the changes that have occurred with the api (dependent tasks change from version to version), but I don't think Maven is the correct solution to the problem, or Pants, etc. I don't particularly like declarative-only builds -- I am the type of person who likes Make and Rake, but I want typed settings too. I want chocolate in my peanut butter. And I want to script my build with Scala, as well. A Free-applicative/Free-monad based DSL isn't my ideal solution either, because then the DSLs are closed for modification (that would be a declaritive build). Someone will invent something to make a scala build tool that is perfect someday. Till then, I don't find SBT as painful as Maven.
Having to be able to publish a custom maven plugin to run custom build code is not as flexible as SBT. I don't have to publish a plugin to work. It's one extra step. Gradle allows this though, but the declaration is untyped. As for copying XML files everywhere, I though maven archetypes were for this purpose? That is something that sbt new and giter8 is addressing.
You're introducing a new problem in the discussion: finiteness. The article, and I, haven't talked about the potential infinite size of `Traversable` and `Iterable`. The discussion has so far been about synchronicity versus asynchronicity. I have only been saying that I have freedom gained by the constraint of synchronicity. With respect to synchronicity, the freedom I receive from `Traversable` is definitely not an illusion. Unless of course another entity X breaks the contract of `Traversable`, but that makes X at fault for not respecting the contract. As I said in another comment, types are not the only thing that defines the contract of abstractions. Arguments such as "if it's not in the type, it ain't true" are not valid, for the simple reason that accepting that argument also means accepting `def add(a: Int, b: Int): Int = 0` is a valid implementation of `add`. With regard to finiteness, note that `Traversable` does not promise finiteness any more than `Iterable`. And for example, `Stream` is an `Iterable` and `Traversable`, but is potentially infinite. *This* IMO is a mistake. If it were up to me, `Stream` would not be an `Iterable` nor a `Traversable`. I would encode in the contract of `Traversable` *and* `Iterable` that they have to be finite. `Stream` (and other stuff such as `Observable`) would be part of a different hierarchy of potentially infinite collections.
I would say that it's the name `add` that forms the contract there more than any external documentation.
[removed]
All I see is a white page with the text 'B-Fil.com'.
&gt; I started looking into the F[_]: Monad part, because I was not quite sure how to do that. Did you meant it in this way: git pull? Yeah - sorry, I forgot about how traits can't use context bounds, you're right to make it an abstract class. You shouldn't have to declare your own monad instance for `Either`, cats should come with one that you can import. &gt; So keeping the Monad generic and making the Share an explicit return type. On the other hand I did not need any library with the earlier version, but now rely on cats. Yeah. IMO Monad is important and common enough that it really ought to be part of the language (`for`/`yield` ought to require it really), but it isn't. TBH I suspect the whole thing may be overengineering compared to just using `ShareError` in the interface - or even not separating an interface in the first place. But doing it this way allows you to use `Id` for testing, and opens the door to implementations that use e.g. `Future` as their carrier type, or [use a monadic representation of random](https://github.com/NICTA/rng), or something like that. But arguably not worth it until you actually come to make another implementation of the interface.
&gt; Whether it is an executable jar or a plugin, I still have to package up the code separately first to run it as a jar. You don't, you just do `mvn -pl :databaseMigrations exec:java` or similar. You do have to put each executable into its own module, which I've come to appreciate (I end up with a structure where each module is either pure library or single executable, so all the entry points to the codebase are very clear), but does have some overhead to it. &gt; I want typed settings too. Your settings are typed in Maven, just via XML schema rather than Scala. You get e.g. IDE autocompletion of settings. &gt; And I want to script my build with Scala, as well. It's surprisingly simple to write a custom maven plugin, even using Scala. It forces you to put the build step into its own module with its own release cycle, but if the step contains nontrivial logic (and if it doesn't, why is it a custom step at all?) you probably want to have that anyway, and having it as first-class code naturally nudges you towards unit tests etc.
What is the difference between () and {} in the following context? Why do I need () for 2. to work? and why 3. doesn't work? I tried to play around with the pipe operator. I found an implementation: implicit class Pipe[A](a: A){ def |&gt;[B] (f: A =&gt; B) = f(a) } It works in one line as it is expected 1. 5 |&gt; (x =&gt; x + 1) |&gt; (x =&gt; x * 8) But it only works with more than 1 lines if I put the whole thing inside ()-s 2. (5 |&gt; (x =&gt; x + 1) |&gt; (x =&gt; x * 8)) It doesn't work with {}-s: 3. {5 |&gt; (x =&gt; x + 1) |&gt; (x =&gt; x * 8)} 
Yes, the differences between set and type theory are interesting. Probably more related to a language like Idris than Scala though.
Weird, here's the repo link: https://github.com/bfil/scala-automapper
&gt; My point was the opposite: that there are natural Traversables which aren't Iterables =P Oops, that was in fact what I meant :-) About laziness: I don't see it. If Traversable is defined by `foreach`, every operation on it needs to consume the collection as a whole or not at all. You can use tricks like break (backed by exceptions) to stop before the end, but then you'd need something like coroutines to resume. In fact, if you have the means to implement element-wise laziness on a Traversable then it would be equally easy to implement an Iterable from a Traversable. 
[geny.Generator](https://github.com/lihaoyi/geny)s are defined by `foreach`, but behave more like `Iterable`s than `Iterator`s: they are "immutable" and calling a terminal operation like `sum` multiple times performs the entire operation each time. In a sense, they are non-resumable, but instead restartable. As a result, every transformation like `map` `filter` `drop` `take` `slice` etc. simply "build up" a chain of operations, and the whole chain gets evaluated when someone calls a terminal operation like `foreach` `mkString` `toArray` etc.. If someone calls a terminal operation multiple times on the same chain of transformations, the whole chain gets evaluated repeatedly (just like how Iterable can have `foreach` called multiple times without exhaustion). And since the transformations simply instantiate a new operation in the chain (defined [here](https://github.com/lihaoyi/geny/blob/master/geny/shared/src/main/scala/geny/Generator.scala#L182-L293)) rather than executing immediately, they are "lazy" in the sense that no action is performed until a terminal operation (though not "lazy" in the sense of caching the result after it's computed). I guess calling them the push-based equivalent of pull-based `Iterator`s isn't quite accurate; they are rather the push-based equivalent of pull-based `Iterable`s (Calling it "foreach-based" is a bit of a simplification in the case of `geny.Generator`; the actual abstract method is [generate](https://github.com/lihaoyi/geny/blob/master/geny/shared/src/main/scala/geny/Generator.scala#L45), to allow it to terminate early, which is a `foreach` that lets you signal you're done, but that's mostly a performance tweak to avoid continuing to stream e.g. lines of a file you don't want just to throw them away)
&gt; I meant it the way LazyList or Stream is a lazy, whereas List or Array is strict. What is that laziness that you mean? It is not clear to me what is the "core" of your definition and what is peripheral
This talk I gave https://vimeo.com/194959852 goes into some detail about the differences between Ammonite scripts and "plain Scala" scripts
To be honest, I still prefer shapeless's generic programming way though its learning curve is far steep.
Yes, I realize that the term lazuness is a bit ambiguous here. It can mean both (1) "look at only as many elements as are needed at this point" and (2) "delay operations until the result is needed". I meant both of them together. It's true that Traversables can easily do (2), in fact (2) is completely independent from the choice of underlying traversal since you can always delay operations.
&gt; All of them can be implemented as Iterators, because there's nothing in the protocol that prevents it. We aren't talking about convenience. Of course. If we don't care about convenience, everything can be implemented as `Boolean`s and NAND. That means we should probably get rid of `Observable`, `Int`, `Float`, `String`s, ... What's the point of using Scala at all anyway? There's nothing in the `Boolean`/`NAND` protocol that makes it unable to do something that Scala can do... It's easy to say *anything* is unnecessary when you say you don't care about the core feature that thing provides
Well, the "semicolon inference" of Scala behaves that way: 5 |&gt; (x =&gt; x + 1) |&gt; (x =&gt; x * 8) Will be parsed inside `{}` as 5 |&gt; (x =&gt; x + 1); |&gt; (x =&gt; x * 8); Note that the last line is syntactically valid Scala - you can have something like `def |&gt; [A] (a: A) = a` in the scope, it's just that you happen to not. Notice that writing second operand on the next line would work as expected: 5 |&gt; (x =&gt; x + 1) |&gt; (x =&gt; x * 8) And so will the method syntax: 5 .|&gt; (x =&gt; x + 1) .|&gt; (x =&gt; x * 8) On the other hand, stuff inside `()` parens is parsed as a single expression, unless you manually insert some semicolons.
Awesome, Thanks! I felt it wasn't going to be something very complicated, but I didn't think of semicolons.
My point is that `Traversable` is not useful enough to be in the *standard collections library*, a library that has had a very complex inheritance hierarchy, the number one reason for complaint actually and could use some simplifications; don't you agree? Just to be clear about what that means, here's the inheritance hierarchy for List: https://tpolecat.github.io/assets/list.png Plus, you should be glad IMO, I mean don't you want people to head towards better abstractions, like your own Geny, when feeling the need for a push-based protocol? Not everything needs to be in the standard library. Batteries included are nice, but not when they are slightly broken, because it then keeps people from moving to better abstractions.
This has been reported in https://youtrack.jetbrains.com/issue/SCL-11224 and has already been fixed.
I agree that`Traversable` is relatively useless; but `TraversableOnce` and `GenTraversableOnce` are worth having. The major convenience of `TraversableOnce` is that `Iterator`, `Iterable` both implement it; so you can write: `def method[A](gtrav:GenTraversableOnce[A])` which can use foreach/fold/etc and call it like: `method(list)`, `method(set)`, `method(iterator)`, `method(parallel)` `def method[A](trav:TraversableOnce[A])` which can use flatMap/etc: `method(list)`, `method(set)`, `method(iterator)` Sometimes an API will only provide iterators, and having to do `method(iterator.toList)` is inefficient. And having to write `method(collection.iterator)` every time is inconvenient, and inefficient for parallel collections. And overloading the method just leads to bugs and sadness. ~~What does bother me is that `foreach` in is rarely provided asynchronously; because it gets used by default methods to implement `flatMap` via side-effects.~~ okay, what was incorrect, `GenTraversableOnce` can be asynchronous, which is why it does not provide `flatMap` and such.
Something like that: import shapeless._ case class automap[S](source: S) { def to[T] = new { def get[SRepr, TRepr](implicit gs: Generic.Aux[S, SRepr], gt: Generic.Aux[T, TRepr], ev: SRepr =:= TRepr): T = gt.from(gs.to(source)) } } case class SourceClass(label: String, value: Int) case class TargetClass(label: String, value: Int) val source = SourceClass("label", 10) val target = automap(source).to[TargetClass].get
IIRC play relies on some special build plugin, though it looks like there's a plugin available for maven as well. It's certainly possible to mix angular and play. That said, why do you want to combine the two? IME if you're using angular it's better to build a full client-side application with it (which you can use scala.js for if you like), in which case you just need static (or almost-static) pages that talk to a REST interface. And if you're only writing a REST backend then I find other frameworks (e.g. akka-http) have much better support for that use case than Play, which is oriented towards writing HTML UIs (you *can* use Play for a REST API, but it's not what it's primarily designed for). So while you can combine the two, I don't think you'd be playing to either's strengths by doing so.
So, from a quick overview of the code: * The sooner you can organize your code by subproject, the better. If you can take your search interface and anything that isn't directly related to processing HTTP requests or rendering HTTP requests, you can move that into a subproject with an API and expose only the details you need -- i.e. the Repository related classes can go into a subproject that is not Play related https://www.playframework.com/documentation/2.5.x/SBTSubProjects * Use custom action builders, and leverage dependency injection there. See https://developer.lightbend.com/guides/play-rest-api/part-1/index.html for details. The controller is a bunch of HTTP traits, and all of the smarts are actually provided by the action and the request. If you can make the controller only about rendering and processing and have the action / request do everything else, you can minimize the amount of refactoring needed. * The more tests you have, the better. Leverage Travis CI and set up a build file, and you can get tests and CI for free. You can do unit testing and integration testing very easily in Play and it's a great way to check your refactoring works. * Check out the sample projects at https://playframework.com/download and poke at them. Look at Silhouette and Deadbolt 2 for authentication and authorization, and check out the [Module Directory](https://www.playframework.com/documentation/2.5.x/ModuleDirectory).
It can work, but boxing values in `Option` is terrible for performance. Think of `Iterator` as an abstraction that can iterate over arrays with near-zero overhead. If you can't iterate over arrays with near-zero overhead, then you might as well use something that doesn't mutate stuff, since it would be much safer. If performance doesn't matter, then we might as well use something like this: https://github.com/clojure/clojure/blob/master/src/jvm/clojure/lang/ISeq.java But as it happens, performance does matter and boxing sucks.
Don't forget to use Scalas advanced typesystem to your advantage. E.g.: * Never use primitive types except when you wrap them (or if you need raw performance). Strings are no exception! * Avoid macros whenever you can and try to use e.g. shapeless if possible * Use the type system to abstract over things like `entity with ID`. Don't put your IDs of your entities into your entity-caseclass but rather abstract over it with a generic case class. * Avoid the usage of implicits unless for well known patterns like typeclasses unless you really know what you're doing. * Use typeclasses instead of inheritance * ... Oh and don't forget to turn on important compiler flags: http://blog.threatstack.com/useful-scalac-options-for-better-scala-development-part-1
I responded over at /r/play. &gt;Just develop the Angular2 frontend seperately from the Play app, that will be the simplest solution. &gt; &gt;As for Maven, you can check [play2-maven-plugin](https://github.com/play2-maven-plugin/play2-maven-plugin), but I would just use sbt if at all possible.
This subreddit in particular probably doesn't need any convincing. But I wanted to start an article series. Better late than never! Open to feedback or content requests. Specifically content geared towards people who are not already functional enthusiasts or feel like any existing material isn't as clear as it could be.
[removed]
[removed]
Hello, Other than knowledge of scala, what are the prerequisites to understand the project and contribute to? Do you have the a list of features you wished to be implemented? 
Pants is definitely production ready, but it is not for everyone. Comparing it to SBT is almost apples to oranges, though. They have completely different philosophies with regard to explicitness vs magic. Pants is also really purpose built for large projects and monorepos. If you're working on a huge project or many smaller projects in the same repo, than you should definitely check out pants. If your project is smaller and self contained, then Gradle is the best option, IMO. SBT is purpose built for creating inscrutable hieroglyphics and invoking dark voodoo magic. I don't personally recommend it for any other than those purposes.
Interestingly enough, I had to solve the same problem recently. So far as I can tell, it works exactly as needed - it has tests and all! - but the code got... kind of nasty: https://github.com/nrinaudo/kantan.codecs/blob/master/core/src/main/scala/kantan/codecs/resource/ResourceIterator.scala
I've been doing Scala for 7 years now and I'm still figuring out new things. I think it's important to realise that a codebase is never "done", and that this year's code will always be better than last year's code - rather than trying to do a bunch of up-front refactorings it's better to adopt an idea of continuous improvement where as and when you work in a given area you bring it up to the current standards. Your list is pretty good, and I think the best practices you link is good. Some next steps I'd say are: * reduce is often a sign that you should define a monoid and use monoid sum. * `map` followed by sum is `foldMap` * folds that use `flatMap`s can usually be replaced with `traverse`/`sequence` * other folds can sometimes be replaced by specific methods e.g. `find`, `exists`, `groupBy`. Basically `foldLeft` is a very general/powerful method which makes it hard to reason about, it's best to use more specific things whenever you can. * `match` constructs are easy to write unsafely and can often be replaced with `fold` (e.g. never `match` an `Option` or an `Either` (unless you need to for `@tailrec`)) * `if`/`else` is generally a sign that you want an ADT (`sealed trait`). So is a datastructure full of `Option`s or `Either`s, especially if there are invariants that relate them (e.g. "if `a` is `Some` then `b` is `Left`"). Define a `fold` method on your ADT * if there are two different states, make them two different types (e.g. a few months ago I had a bug where I passed a graph to a function that expected that graph to have been filtered by another function first. Solution: make the filtered graph and the unfiltered graph different types). * folds (or just regular code flow) that produce a value and accumulate a secondary value (e.g. a list) should be represented as `Writer` * Any construction that threads a secondary "state" value through should be represented as `State` * Cheat sheet for validation-like code: * Want fail-fast? Use `Either` (or in pre-2.12 Scala, `\/` or `Xor`) * Want to accumulate all failures? Use `Validation` and accept that you won't be able to use `for`/`yield` * Want to accumulate failures but still return a value in the failure case? Use `Writer` * Avoid reflection. Things that use reflection to walk the object graph (e.g. serialization) are usually best replaced with typeclasses; use shapeless-based typeclass derivation to avoid the overhead of writing them by hand. * Proxies/interceptors should be avoided. Any kind of "block" or "context" construct (e.g. a database transaction) should probably be represented as a value that you pass into a single method that does the open/close, so that you can't have a path where you forget to match them up. The free monad can give you a more lightweight way to represent your commands * `foreach` should usually be replaced by `traverse`/`sequence` using a value to represent the effect (e.g. `Task`) * double-`flatMap` (`flatMap { _.flatMap {... }}` or `flatMap { _.map { ... } }`) is sometimes a sign you should be using a monad transformer. Alternatively, if you're struggling to combine stacks of effects and nest `flatMap`s correctly, consider using a free coproduct approach instead. * if you're defining a lot of tree-like structures and find yourself repeating a lot of traversal code boilerplate, consider recursion-schemes style with fixed-point types.
The basic example is something like: trait Animal { def eat(f: Food) } class Dog extends Animal { def eat(f: Food) = ... } class Cat extends Animal { def eat(f: Food) = ... } def doSomething(a: Animal) = { ...; a.eat(f); ... } vs trait CanEat[A] { def eat(a: A, f: Food) } class Dog() object Dog { implicit object DogCanEat extends CanEat[Dog]{ def eat(d: Dog, f: Food) = ... } } class Cat() object Cat { ... } def doSomething[A](a: A)(implicit canEat: CanEat[A]) = { ...; canEat.eat(a, f); ... } You can see that the typeclass style is more verbose but also more flexible; `Dog` and `Cat` are no longer related to each other and you could even define `Dog` and `Food` in separate modules that have no visibility of each other (you just need to define `DogCanEat` and have it in implicit scope where you need it - but that can be in a third module that depends on both `Dog` and `Food`). I would keep inheritance for "core" functions of an object, but use typeclasses for "secondary" functionality e.g. JSON seralization, database mappers, log pretty-printing. It means that you can keep the business-level type clean and decoupled from those concerns without having to completely duplicate for a DTO or wrapper.
You are awefully close to the sig of an iteratee there...
Thanks, as someone like me who's familiar with Scala but not so much with functional Scala, this post is really helpful. This should be put up somewhere other than in a reddit comment.
awesome. I actually have exactly the use case for this in my current project. I'll try to apply it! 
Is stack overflow documentation really used by people ?
&gt; However, no testing frameworks in the Scala world are capable of mocking the results of static functions. PowerMock can. You might be able to combine it with ScalaTest etc. (which already support EasyMock/Mockito which PowerMock integrates with) - I wouldn't know since I don't use ScalaTest. &gt; This method is written in a functional programming flavour, which consists of many higher ordered functions flatMap, map and traverseM. The `map` and `flatMap` can be replaced with a `for`/`yield`.
Yeah thanks a lot unfortunately its not free lol but it has a github repo too attached to the book which is nice.
You can use a chunk size of 1, then `map(_.mkString)` the result. Yeah, I know, but it does work. Seems to me that the replacement for `statefulMapConcat` is [scan](https://github.com/functional-streams-for-scala/fs2/blob/v0.9.2/core/shared/src/main/scala/fs2/pipe.scala#L309-L320).
No worries! You can actually see the latest status from the meeting (as well as a brief look at this history of the proposal) here https://youtu.be/eqSSXg7Up2I
Yep, definitely planning on doing that. Thanks for the support!
well there is also the async/await approach with a macro: https://github.com/scala/async
The purpose of this example is creating a XHTML template with the same imperative style like a normal PHP program, avoiding complicated combination of higher-kinded functions. IMHO, your example is a little far from a "template".
Yes, ThoughtWorks Each library can be seen as a general purpose replacement of scala.async. You can have a look https://www.reddit.com/r/scala/comments/5oinm2/businessfriendly_functional_programming_part_2/ for other usage.
No need for a macro: (f1 |@| f2 |@| f3 |@| f4) (_ + _ + _ + _) foreach println Applicative effects have the be benefit of being parallelizable as well.
Honestly this library is a learning experience for me myself. There's some good documentation around for the Haskell version but I don't know a whole lot of Haskell. 
No, it's an applicative builder (`|@|`) with an `apply` call
In cats, you do. In scalaz, you don't. I believe cats will be phasing out applicative builder in favor of tuples (e.g. `(f1, f2, f3, f4) mapN (_ + _ + _ + _)`)
Ehm, I don't get your point, the library isn't asking you to write macros, it does it for you. Shapeless uses macros pretty much everywhere: https://github.com/milessabin/shapeless/search?utf8=%E2%9C%93&amp;q=macro
&gt; The idea is that using akka persistance implementing something that functions as a simple SQL database in Scala is a few hundred lines of code. Well, not really. I imagine what you mean is "I only care about persisting Akka messages," which is quite different from "I want to implement SQL." &gt; I mean storing objects in memory, and make sure that they are persistent and can be restored when power goes out. Right, but that's very different from what, e.g. acid-state does. &gt; So the idea would be to get rid of SQL databases and just use the memory and restore the application state using akka persistence after a power outage. So basically cutting out all the SQL baggage and simply using Scala to describe and query the data model. There really is no "data model" here, other than as a description of what your application manipulates that you want to survive across reboots. On a fundamental level, then, you've answered your own question: use akka-persistence. If you mean "is there an event-and-command-sourcing framework built on top of akka-persistence," the answer is again yes: [Eventuate](http://rbmhtechnology.github.io/eventuate/).
As a general note you might enjoy [Functional and Reactive Domain Modeling](https://www.manning.com/books/functional-and-reactive-domain-modeling) by Debasish Ghosh, which may help you re-cast your Java ideas in Scala terms. In Scala as a first pass I would write an immutable data type, something like this. case class Order( id: UUID, customer: Customer, orderPlacedAt: Date, confirmationNumber: String, lineItems: List[LineItem], shipments: List[Shipment] ) { def total: BigDecimal = ... def taxAmount: BigDecimal = ... def +(s: Shipment): Order = copy(shipments = s :: shipments) def unshippedLineItems: List[LineItem] = lineItems.filterNot(_.shipped) etc. } You mentioned Anorm so you may later need to deal with partial loading, which you can do by introducing type parameters for the child values as I sketch out [here](https://www.reddit.com/r/scala/comments/5k846g/biweekly_scala_ask_anything_and_discussion_thread/dbmbh5a/). 
The Scala approach to database access tends to be lower-level, mapping rows as tuples rather than trying to map graphs and manage relationships and traversals, with immutable data rather than trying to track "dirty" status of mutable data. So it's different and may seem primitive in some ways, but I think you end up with programs that are much easier to reason about. Slick has a bit of learning curve, but it's mature and works well for many people. A newer project called Quill takes a different approach for a high-level API and is definitely worth a look if you're shopping around. You have looked at Anorm so you have an idea of what it can do. ScalikeJDBC is another you might consider. I work on one called doobie which is a good fit if you're doing pure functional programming, but it will probably seem odd coming straight from Java. Anyway, good luck!
Miles Sabin is a wizard!
&gt; There are Scala bindings/mapping to SQL databases, but why bother creating such bindings/mappings to SQL databases when Akka persistance takes care of persistence anyway ? Well, I can think of a few things: 1. "Persistence" and "a database" really are different things. Whether you like SQL or not, you often do want some sort of structured data store that supports ad-hoc queries. 2. Not everyone uses Akka. &gt; I mean the relational model is a useful abstraction but why not use Scala to manipulate it instead of SQL or a Scala mapping to SQL ? That's a different question, and maybe [Quill](https://getquill.io/) is a good running start at answering it for Scala and SQL. But SQL is already a domain-specific language for relational algebra, so it's good to be able to use it directly, too. FWIW, I'd like to have a decent DataLog implementation, but that doesn't seem to exist, either.
Wonderful! 🎉
here comes the bad infograph guy again
As a small feedback, a "*side effect*" is in no way an "*unintended consequence*" as you are describing it. Most described side effects in programs are very much intended, many being encapsulated in well grown APIs. That the mutability of your `jewel` variable makes it untrustworthy, as per that example due to it being nullable, that can be seen as an encapsulation problem. Also `null` usage doesn't have much to do with immutability or functional programming, being a (mis)feature of the type-system / runtime. A nullable value definition is just as untrustworthy as a nullable variable. And you can't just place null checks everywhere, because you end up going insane over it. Best approach is to simply ignore the possibility, wrap null-returning Java APIs with Option and ban usage of nulls within your team, enforced by code reviews and public shaming. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/javafx] [Templating FXML with data-binding in Scala](https://np.reddit.com/r/JavaFX/comments/5ovgw1/templating_fxml_with_databinding_in_scala/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Coursera Learn functional programming with Scala
Maybe. Thats how i understand this question This course have one week on tree data structures. And an assigment with the functionality like you said. 
If tag.name is inmutable and you want to edit it for all documents you need to make a new Tag with the new name and make all Documents to point to the new Tag. If you want inmutability + not duplicated tags you can do something like: class Document(var tags: List[Int]) class Tag(val name: String, val id: Int) and save tags in a list or map or whatever you need, so if you want to change a tag name, you edit the tag with the correct id and there is no need to edit Documents.
I'm well aware Shapeless uses macros. You were making the point that Miles' solution is "way more complex". IMO the comparaison is only fair if you compare his example to your library and NOT his example to your library usage. One could rewrite a similar lib based on shapeless and end up with something easy to use too. Quite frankly I do not know if [this](https://github.com/bfil/scala-automapper/blob/master/automapper-macros/src/main/scala/com/bfil/automapper/Mapping.scala) is simpler than Miles' example. I think both are complex. Anyway, I'm not trying to bash your work or anything. It's cool that you're sharing your work with us :)
In pure style you can't use clock time as an implicit time and object identity as implicit identity. You have to model the concepts you care about explicitly. Is the idea that each document refers to a list of tag identities, and the tag associated with an identity can be different at different times? Then model it that way - look at the way log-structured datastores work (with CRDTs etc.) and structure your data in append-only style. I don't have any specific guidelines for this, but there might be useful stuff in what's been written about working with Cassandra etc. (any "big data" datastores where you can't afford global transactions really) Or is the idea that a "tag name update" modifies all the documents in a single transaction? Then model it that way - lenses, state monads, and Clojure-style STM are tools that can help. Sorry I'm not aware of any tutorials/documentation - I've generally picked this stuff up as I went along (it helped that I was used to Cassandra style already). But hopefully that gives you some of the direction?
I've looked into this kind of thing briefly. The trouble is that the interesting refactoring usually requires human intervention. Often that's the whole point. E.g. look at the Python 2 -&gt; 3 issues, where the whole point is to make an explicit split between strings and bytes and to ensure everything is encoding-aware - you can't automate that. For the parts that are automatable, `2to3` exists and that's fine, but it really doesn't help that much. If you want to do this I think the most useful thing would be to have a separate maven artifact containing the refactoring information - you could even "compile" the refactor to IDE-specific formats that already exist (certainly eclipse has an XML format you can express refactorings in) and upload them all as separate artifacts, the same way we do with source/doc/... jars at present. By all means try it. But I'm doubtful that it will end up actually saving much time.
I don't know why you need this, maybe you could solve it with some existing library for relational databases and using h2 database if data's size is small... If this not work for you, writting a small library to handle it in a generic way seems like a good idea to me.
&gt; without the need to update all Documents which refer to the given tag ? It's not possible for this exact example. Immutable structure are great when you can share value. For example with a List: trait IntList { def tail: IntList } case class IntCons(head: Int, override val tail: IntList) extends IntList case object IntNil extends IntList { def tail = sys.error("tail on empty list") } val a = IntCons( 1, IntCons( 2, IntNil ) ) val b = IntCons( 3, a ) b.tail eq a // eq is a referencial equality def updateHead(xs: IntList, f: Int =&gt; Int): IntList = { xs match { case IntCons(h, t) =&gt; IntCons(f(h), t) case IntNil =&gt; IntNil } } val b0 = updateHead(b, x =&gt; x + 1) b0.tail eq b0.tail
My RSS subscription to this reddit showed me the Twitter link to the SIP. Has science gone too far?
DeepLearning.scala will benefit from this PR, too. Thank you, Miles Sabin!
I've been working on a project that will reserve my Dad's timeshare at a very specific point in time. The timeshares become available to book 1 year ahead of time - at exactly 8pm on Thursdays - but if you try to book it yourself, you don't usually get it. It's like a 2 second window. So I'm using ScalaTest's Selenium wrapper, which has been pretty nice. I also made good use of [Scallop](https://github.com/scallop/scallop) for CLI parsing, it's very nice and fairly easy to use. 
No, you need the period at the `.is`. I think that's probably your best option, but if you really really want to get that syntax, you can kind of fake it with something like trait ASyntax { def PrimaryKey: Boolean } case object a implicit class StringSyntax(str: String) { def is(dummy: a.type): ASyntax = new ASyntax { def PrimaryKey = { /* your logic goes here */ true } } } val sqlStatement = new StringSyntax("CREATE PRIMARY KEY") sqlStatement is a PrimaryKey
Kind of reminds me of Eclipse's [refactoring scripts](http://help.eclipse.org/neon/index.jsp?topic=%2Forg.eclipse.jdt.doc.user%2Fconcepts%2Fconcept-refactoring.htm). &gt; Refactorings can not only be performed interactively, but also from refactoring scripts. Most refactorings available in the Refactor menu are stored in the workspace refactoring history in order to be used in refactoring scripts afterwards. The refactoring tools support the creation of refactoring scripts based on refactorings in the workspace refactoring history. Refactoring scripts can then be applied to an arbitrary workspace. Applying a refactoring script launches a refactoring wizard which is able to replay the refactorings as if they had been initiated by the user which originally had created them.
Yeah, I am thinking the same, I plan to write a lib for this, I just wonder why no-one has done it before. Why do I need it ? Because : 0) having mutable data model is a non-option 1) normalising the data is useful (simplifies updates and simplifies my code), if I have ten places where I refer to an Entity then I don't need update it manually at 10 different locations. 2) SQL is old, Scala is better, as a language, I want to get rid of all SQL, not even bind to it (as in slick etc...), also having to sync a database with in-memory data model is an unneccessary cost if I can solve persistence using akka-persistence / event sourcing Because of these advantages I am surprised that there is no (public) library written for this. 
Really because scala kind of enables me too much when it comes to bad habits, an example being when I wrote a raft implementation I used way more traits than I needed to, and I used akka fsm with a mutable var to change states. When using become + val would have been the smart thing to do. I suppose spending the time using cats or scalaz would help fix the problem. 
ahhh, swizzling-- I never fail to make _some_ edge case error doing this
There are a few of us around working on these things. Olafur Geirson probably being one of the main people with scalafix but there are a few others. IntelliJ is working on adding a migrations feature to IntelliJ that lib authors will write automated refactoring for between versions of their lib. I have been working on this in general too, writing a compiler plugin for it right now and am starting small with a cats Xor to Either rewriter just to feel out the right API. Already created and abandoned a couple of versions but I think the next one might stick. 
And if he wants to stay on the JVM he could use [Frege](https://github.com/Frege/frege) instead. Dierk König said last spring at the JUG HH that Frege is almost 100% compatible to the [haskellbook.com](http://haskellbook.com/). So for a beginner it reduces incompabilities down to zero.
If anyone's interested, here is the initial post: http://www.scala-lang.org/old/node/8610
Why did you need EitherT transformer? Isn't the whole point of Free monads is to add composability of various monads?
You can interpret `Free` *into* any monad. And that's basically what it buys you. If, on the other hand, you want it to behave as `Either` monad *and* `Free` monad - you'll need a transformer. For example: sealed trait Action[R] case class OpA(param: Int) extends Action[Either[String, Int]] case class OpB(param: Int) extends Action[Either[String, Int]] type Program[R] = Free[Action, R] val program: Program[Either[String, Int]] = for { i1 &lt;- Free.liftF(OpA(10)) i2 &lt;- Free.liftF(OpB(i1)) } yield i1 + i2 Won't compile because i1 is `Either[String, Int]`. You'd have to unwrap it yourself: i1.fold(_ =&gt; Free.pure(Right(0)), realI =&gt; Free.liftF(OpB(realI))) As you can imagine this gets unwieldy really soon. So instead you wrap it in `EitherT` type Program[R] = Free[Action, R] val program: EitherT[Program, String, Int] = for { i1 &lt;- EitherT(Free.liftF(OpA(10))) i2 &lt;- EitherT(Free.liftF(OpB(i1))) } yield i1 + i2 Now everything is peachy. 
Sentence-like DSLs aren't trivial in Scala. sqlStatement is a PrimaryKey is equivalent to sqlStatement.is(a).PrimaryKey So PrimaryKey has to be a method (and using postfix ops) If you need to pass your own classes-something, it has to be an odd word (1st, 3rd, 5th, etc). Alternatively, you can use a parenthesis like sqlStatement is a (PrimaryKey) Or a type parameter: sqlStatement is a [PrimaryKey] Or combine `is a` to a single name (`isA`, `is_a` or \`is a\` in backticks) Check out [ScalaTest matchers examples](http://www.scalatest.org/at_a_glance/FlatSpec).
Yes, you got it all right for the most part. You don't need parens if you make `PrimaryKey` a method. BUT, if it needs to be an object that a user will provide instead of method, you can make it be written as `sqlStatement is a (PrimaryKey)`, and it will be parsed as `sqlStatement.is(a(PrimaryKey))` For example, in ScalaTest I can write `x shouldBe Some(42)` or `x should be (Some(42))`, but it's not possible to make `x should be Some(42)` work for any argument, because it is parsed as `x.should(be).Some(42)` - `Some` is a method, not an stdlib object. Also, `map should contain value "FOO"` is valid in ScalaTest - there, user-provided object go as 1st word (`map`) and 5th (`"FOO"`). In `x shouldBe Some(42)`, user-provided objects go in 1st (`x`) and 3rd (`Some(42)`) slots respectively.
This has nothing to do with skin thickness or validity of criticism, it has to do with decency and tone, as well as the reputation of the community. 
&gt; They help deal with the normalization by turning specific updates into updates that can be applied at the root. &gt; Hmm... right but they don't help to solve the "update the Tags in **all** document" problem, they just make possible to do `a.b.c.d=1` in a pure language.
No, this was not the question. But thanks.
"In integration tests I sometimes use an actual db and rely on composition of underlying actions to write small programs that hit db - there I can check these things." Unfortunately I don't know if I follow what you're trying to say here. Do you mean that you compose Free programs to use in your tests to do assertions in them? This is something I have done, they can then be used in both the integration and unit tests, and it relates to the fact that you are expecting your interpreters to behave in the same way. But maybe I misunderstood you? Also I sometimes can be very pedantic in testing, for example in the case where you test invalid PIN's I would usually use a property-based method to verify that only 1 PIN (the one specified in the application) will be allowed.
How can you do that ? By finding all documents and then creating a lens for each ? That is the same as manually updating all documents, which is ok if you know where the documents are, but what if the documents are in several places (and you forget to update some of them) ? Lenses still not solve the problem to consistently update all the documents because you still need to specify manually which documents you need to update (in which collections you want to update the documents). Right ? The problem is finding all documents (or anything that refers to a Tag) that need to be updated, I don't see how lenses can help with that. 
Interpreters that I've written have, so far, been natural transformations (`~&gt;`). A natural transformation is a [universally-quantified function](https://github.com/scalaz/scalaz/blob/v7.2.8/core/src/main/scala/scalaz/NaturalTransformation.scala#L5). So if I'm using [`forAll`](https://www.scalacheck.org/) (universal quantification, "∀") in my property-based test to send sample inputs to my `~&gt;` and don't get the expected result, that's a bug, pure and simple. And I'd say this is pretty critical to understand: your interpreter _must_ transform any value of its input type to a value of its output type, period. That doesn't mean, e.g. that it can't handle failure—for example, interpreting your free monad into `Task`, it's totally acceptable for some input to be interpreted into `Task.fail(...)`, which is a perfectly good value of type `Task[_]` for a `Foo[_] ~&gt; Task[_]`. It just means you're using `Task` as a failure monad, which it most definitely is.
I get a "page not found" for one of the links (`http://typelevel.org/cats/tut/freemonad.html`). I assume it's meant to be `http://typelevel.org/cats/datatypes/freemonad.html`..... 
Thanks a lot for spotting this. Fixed now. 
Agree.
&gt; if I have database and I do a Put action, I then probably want my Get action to return the same thing I put in there. I disagree slightly :-) "If I have a database" is a specific interpretation of your language. So, at some level - yes, but the whole point is that you can interpret your program differently and this assertion may not hold. 
Well for the PIN thing, I don't know the specifications of what your application needs to do, but that is really a detail in the point I was trying to make, I assumed that Email -&gt; ActualPin would require IO and (ActualPin, SuppliedPin) -&gt; Boolean would not. I just wanted to illustrate that I might grasp to property checking faster than other people would.
Okay, agreed. Maybe I worded it badly. So what I meant to say is that when designing the DSL you probably have some kind of idea in mind of what the operations are supposed to do when interpreted for your actual application. So if we design a DSL with a Put(key, obj) and a Get(key) operation, we might have in mind that things we are going to Put with a certain key will be the objects we are then able to Get with that key later on. We are of course able to define an interpreter which does not abide by these rules, it could just trash everything it receives as Put and always return the same object when we try to Get something. And this might be useful for certain tests. But when we write our actual production interpreter we are expecting the related behaviour. Because we designed the DSL with this behaviour in mind. And if we have a mocking interpreter with the same behaviour. This allows us to go very far in testing complicated code with the DSL as a unit test, where only the mocking interpreters are needed. But maybe this is just me, if you think otherwise I would like to hear what you think of it.
I'll try finding haskellbook for free I am too cheap to buy that ish
Yeah I am learning haskell RIP me. Lol it wasn't as bad as I thought it would be but it was certainly more of a bitch than ocaml, scala, or lisp. 
&gt; Do I understand you correctly by saying that you create tests to test the implementation of the interpreter by checking if its apply function responds to all possible inputs correctly? I'm saying more that that's inherent in the definition of `F[_] ~&gt; G[_]`. If you write your interpreter as essentially everyone does—pattern-matching on your `F`—it's probably sufficient just to have the compiler treat warnings as errors, so if your pattern-matching isn't exhaustive, your code won't compile. &gt; In my experience some interpreters have more advanced machinery behind it that requires some more checks, such as linking certain actions together and verify if they interact correctly. For example if I have database and I do a Put action, I then probably want my Get action to return the same thing I put in there. That doesn't sound like it's up to the interpreter (in fact, _is_ not, since you've just now described the constraint without reference to a particular interpreter). So now the question is "how would I express that constraint in my algebra?" I suspect there's an answer, probably involving a higher-kinded version of Leibniz, as described [here](http://typelevel.org/blog/2014/09/20/higher_leibniz.html). I have that question out to Rúnar Bjarnason, Vincent Marquez, and Brian McKenna, all of whom I know and all of whom are likely to either know the answer, create an answer, or know of someone else's answer. :-)
I have somewhat different approach. As I tried to describe, I divide tests into two groups: "under any interpretation, if CheckPIN is interpreted to false, the program won't execute Delete" - these are essentially unit tests in my parlance, they test programs - interpreter is, by definition, arbitrary. Second, "DBIO interpretation removes the entity from database if Delete is executed" - there I test production interpreter validity, so it is integration test (since from 1 I know that programs are correct under *any* interpretation). Therefore I do not see any need for complicated "mock" interpreters. Had I written property tests, maybe I would have changed my mind, but in all honesty I only write these for algorithmic things which have clearly defined input-output correspondence. And again, maybe I'm wrong about this but I've never felt that my tests are not adequate so far.
So you would encode the constraints on the DSL (which is what you call algebra ? Or are you referring to something else when you say that ?) in the type system? I don't follow how by using Leibniz you would be able to encode that. To me it looks like it would need some kind of dependent types. But I'm not too advanced in either topic so...
Yeah, you're right. I've been thinking about it some more, and it would basically entail some sort of universal quantification over the [singleton types](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#singleton-typed-literals) of the values of the argument type(s). Probably not _impossible_, but also probably _insanely weird to express and slow to compile_.
You need to make secure a Directive, look at Directive1 in the the docs it should give you what you want.
You're still not able (?) express (1+1).type AFAIK. This limitation will be present even in dotty. Given that, apart from toy examples, you will probably want to pass an _expression_ to Put, your solution is bound to be extremely awkward to use in real life (if I'm not mistaken)
Intellij basically had to implement their own typechecker to support thir own PsiTree (basically Intellij's own internal source code AST which supports all of Intellij's autocompletion/inspection/refactoring abilities). Due to Scala having a really complex type system, only the actual scalac compiler can be said to have a valid type checker. Its basically a choice between using scalac but having inferior refactoring/inspection capabilities (as seen by ensime/Eclipse) or having Intellij's own internal typechecker which has superior refactoring/inspection capabilities however occasionally highlights correct code as incorrect
I've reported a few of these highlighting issues and found the Intellij devs to be quite responsive in fixing them. So please submit bug reports with examples if you can.
New akka-http docs on directives are [here](http://doc.akka.io/docs/akka-http/current/scala/http/routing-dsl/directives/index.html). You may want the [basic directives](http://doc.akka.io/docs/akka-http/current/scala/http/routing-dsl/directives/basic-directives/index.html) which discusses passing values to inner routes. There are also some built in [security directives](http://doc.akka.io/docs/akka-http/current/scala/http/routing-dsl/directives/security-directives/index.html#securitydirectives). You _might_ need to secure the individual routes before composing them. val route1 = secure { value =&gt; /* logic */ } val route2 = secure { value =&gt; /* logic */ } val route3 = secure { value =&gt; /* logic */ } val routes = route1 ~ route2 ~ route3
Thank you for this example. The login route provided me some great insight.
No existing IDE or editor plugin can accurately typecheck Scala programs. If you want accurate reporting run sbt in a shell. That's the best you can do.
I've only been working with akka-http 10 for about two weeks so there might be a configuration I don't know about, but I'm not sure how to use the result from an extraction once the route is already defined. That is, I believe creating the route needs to have access to the extracted value already. So you could do val routes = secure { token =&gt; route1 { // full definition here can access token } ~ route2 { // full definition here can access token } } But I don't think you can do // how do these get access to the token? val route1 = ??? val route2 = ??? val routes = secure { token =&gt; route1 ~ route2 } However you might be able to modify the request or context in `secure` with one of the `map` directives so that the routes can pull the value from the request/context when it finally gets passed to them.
1. yes, https://github.com/scala/scala 2. yes, at least in a way, for example for (n &lt;- 1 to 10) println(n) for { n &lt;- 1 to 10 } println(n) 3. it's language which can cause headaches, but imo these are worth it 4. http://www.lihaoyi.com/Ammonite/#ScalaScripts
I think you'll find that Scala is extremely taxing on all IDEs, but - best of luck! And, FWIW, I've found IDEA in general to be great. The Scala plugin struggles from time to time, but it's solid for the limited stuff I throw at it (the company I work for is consciously conservative about the Scala features we use in our codebase)
Hmmm, how about something like this? def route1(token: String) = { pathPrefix("resources") { // do stuff } } def route2(token: String) = { pathPrefix("confidential") { // do stuff } } then with a value returning route: val routes = secure { token =&gt; route1(token) ~ route2(token) } 
Many things are source-compatible (so that same code can be compiled with no change by different compiler versions), but not binary-compatible (you cannot reuse compiled libraries). For instance, 2.12 lambda functions have completely different bytecode representation that requires Java 8 now, so it cannot be used with 2.11 with Java 6. Sometimes there are bugs in compiler that get resolved in next version, but require workaround if the library author want it to still work in the older version - so for that you can have different source dirs, but it's sort of a last resort, often there's no need for it.
IDE for scala: https://www.jetbrains.com/idea/
Hey! Can you tell me what are you using for writing code? (Atom/Sublime/Vim/Eclipse/Idea)
Well, since it is a framework for (mostly audio) signal processing, having a basic understanding of DSP would certainly help; after all there should be some motivation in the domain I guess. The UGen system is modelled after ScalaCollider, which is a realtime variant talking to SuperCollider. For example, it also uses multi-channel-expansion, and the signals are only loosely typed (expanding to vectors of doubles, ints, longs etc. at a very late stage). Some of the research questions are: - how to automatically determine delays in the network, so one doesn't have to think hard and use trial-and-error to determine the buffering one needs to insert to prevent diamond-shaped sub-graphs to deadlock - how to integrate meta-data about the signal streams. For instance, I have added image and matrix processing operations, but carrying around necessary additional data such as number-of-rows, color-model, etc. is not solved (or badly solved). - how to integrate this with a GUI front-end which was the traditional FScape (1.x) series? So I guess a potential contributor has some intrinsic interest in audio or image processing.
I personally hate IDEs... I use a text editor called atom. I didn't like vim, emacs was a little hard cause I don't know lisp. Atom is like a free version of sublime. I'll try neovim though
I don't like IDEs either, but if you don't like vim, then you won't like neovim -- it's the same editor, just the code base has been refactored to clean things up and support new features more easily.
Oh... Last question. R u running Linux, BSD or osx
I use FreeBSD, but any *nix (including osx) should work the same. Windows probably also works fine, I just never use the OS. 
Any time I've seen problems with intellij it's been do to loading the wrong version of some library. Can you provide a short standalone example that runs but shows an error in IntelliJ?
I tried freebsd but it didn't have some essintial apps that I use
I use Atom with syntax highlighting, Fira Code as my font, and I run sbt in a terminal window. Very basic setup.
I see, so i would be better of doing: val routeOne = secure { token =&gt; // do stuff } val routeTwo = secure { token =&gt; // do stuff } and then combine val route = routeOne ~ routeTwo Routes are quite big so i have to seperate them. But in each route i add secure seperately. I think i understand if this thinking is correct. Thank you for all your help.
Yep! Your summary matches my thinking anyway :)
where does monads factor into this?
There is also the `while` loop for raw iteration.
The biggest missteps for me have been techniques that strongly tie components together. Example: the Cake pattern. Or to put it more generally, using inheritance to add behaviour to a type at the point of its definition. Instead of that, try to rely more heavily on typeclasses. They are more like building blocks that you add and remove but with added type-level guarantees. Reading through the steps you went through, it looks like you've got a good grip of functional programming at the value level. The next step after that is FP at the type level. This entails trying to turn runtime errors into compile errors. I see a few good suggestions for that in the other comments here. Here's what I know: * Use phantom types to safely distinguish between types that have the same implementation but different _meanings,_ e.g. my comment https://www.reddit.com/r/scala/comments/5oc93x/scala_code_refactoring/dcryrp3/?utm_content=permalink&amp;utm_medium=api&amp;utm_source=reddit&amp;utm_name=scala * Use phantom types to encode valid state transitions and ensure invalid transitions do not compile. See my blog post http://yawar.blogspot.ca/2016/02/the-essence-of-phantom-types-in-scala.html * Use typeclasses to add behaviour to types in an ad-hoc but statically typechecked manner. * Use smart constructors or `require`s to enforce invariants on creating public types. E.g. if a `Person` type's age should be non-negative, enforce that so that nonsensical values can't be created. * Use lightweight type wrappers (e.g. `case class WeightKg(unwrap: Long) extends AnyVal { require(unwrap &gt; 0) }`) to encode meaning and accept those (and not the raw types) as arguments in your API methods so users know what is required of them. * Use lightweight type wrappers to encode that an operation (e.g. validation of an email) has been done on some data and make your methods accept those and not raw types * Don't give case class constructor parameters default values. It's unsafe because adding or removing a field in future will change the meaning of the value returned by the constructor. If you need a default, put one in the companion object. Some non-type-level things: * Use companion objects as much as possible, remember that they're the equivalent of a class's static area. Anything, including methods, that doesn't need access to your class's instance members should go in the companion. Makes them much easier to test (including in the REPL). * Don't go all the way into 'functional programming'. Remember Scala is designed to be object-functional and carries its own idioms. Neither Haskell nor ML idioms are a 100% perfect fit. E.g., don't hesitate to put operations on your types in the types themselves as methods, where they implicitly have access to the instance value. That's all I can think of for now 😊
It is a shorthand for first function argument. The second one would be second func arg, etc. It allows you to write that instead of `a =&gt; a map f` 
It's placeholder syntax. `{ _ map f }` is the same as `{ x =&gt; x map f }` where `x` is a fresh variable.
Thanks dude!
Awesome. Also thanks for making it print-friendly.
Heh, that wasn't a deliberate choice, just like to keep the design overhead to a minimum.
Here's zero-library case for you: sealed trait ADT[A] case class Foo(i: Int) extends ADT[Int] def extract[A](adt: ADT[A]): A = adt match { case Foo(i) =&gt; i } IDEA says "Expression of type Int doesn't conform to expected type A", but scalac compiles that without any problem. You bump into this quickly when writing interpreter for a Free monad.
This is precisely an indication that your DSL is *not* Free - rather it's an algebra, usually obtainable by quotienting out certain equivalences. E.g. we might have `Put(key, obj1) *&gt; Put(key, obj2) === Put(key, obj2)`. The Right Thing here is to find a representation under which those really are equal as values. Free is a sort of semi-operational bodge when you can't or won't model the algebra in full - it's a starting point rather than an end point (though it is often the most practical model).
I suspect you're confusing the free monad in general with free *coproducts* (which is a common source of confusion given the recent fuss about the latter). (An instance of) the traditional Free monad is a more-or-less ordinary monad and you have to use monad transformers when combining it with any other monad just like any other monad. Some people have been combining the Free monad with coproducts as an alternative to monad transformers, using the idea that you can express any monad as a Free monad and that you can combine two Free monads without needing a transformer by coproducting the "contents" instead.
Not just a DAG but a forest. Every object has at most one owner/parent, and if you want to be able to make synchronized updates to multiple values then they need a common (possibly notional - e.g. an imaginary "root" that doesn't exist in the database but you still have a corresponding type/value in code to synchronize on) ancestor that acts as a kind of locking/synchronization point. I view it as analogous to the write anywhere filesystem style, where every change ripples up to the root (i.e. if you want to change a file you effectively make a copy of the file with your change, then a copy of its directory that has the new copy of your file, then a copy of the parent directory that has the new copy of the directory and so on) - of course all managed by the infrastructure. I also find the filesystem analogy helpful for thinking about "symlinks" - i.e. an object can refer to other objects it doesn't own, but this is explicitly a different kind of relationship from owning another object. So e.g. if users are members of groups, then maybe we decide that the group owns the relationship and the relationship has only a weak reference to the user (we can't have users own their groups or groups own their users) - concretely maybe the relationship just contains the `UserId`. And we define traversal tools so that it's very easy to e.g. combine an update that can be applied to a user, and a group, and get a collection of updates to be applied to all the users - and we apply them explicitly, sequenced via the users or their owners, so we definitely don't lose any updates (unless the updates themselves stomp previous updates - but that's a matter of modeling your updates in a fine-grained enough way that you're happy to compose them - e.g. "set username to foo-admin" vs "append -admin to username"). But we can explicitly see that these are independent updates that won't be applied transactionally all at once. Or maybe we don't want the relationship to be part of the user or of the group - but in that case the "membership" needs to be a first-class entity with its own id and we need to take care of sequencing updates to memberships. At the start this feels like a kind of low-level way of doing things, but it felt like I was able to recover/implement the important high-level constructs using the functional tools, and the constructs that were hard to implement were the ones that were maybe a bit too magic in the first place. You also have very direct insight into e.g. where data can be sharded (different roots can go to separate places). At least that was how it felt for me.
&gt; About the ID, I would disagree slightly, it usually makes more sense to keep IDs tied together with entities For me, having a (case) class means: there are some things (the class' fields) that can *only ever* exist together. But with most entities, they can exist even if there is no repository at all. So it is - at least from a theoretical point of view - not a good idea to model it as if each entity *must* be stored/referenced by something. Therefore, I would rather created some case class `WithId[Entity, Id[Entity]](entity: Entity, id: Id[Entity])` that represents an entity, that is referenced by an Id for this kind of entities. The benefit is, that e.g. in a test, I can just create a `Person(name = ???, age = ???)` and I can test properties of my person without having to care about an Id that I don't even use in my test. On the other side, in my `PersonRepository` I will have something like `def get(id: Id[Person]): Person` and `def save(personForId: WithId[Person, Id[Person]])`.
&gt; your DSL is not Free - rather it's an algebra Do you have any resource on where I can read up more about this? I don't think I understand what you mean by this. Also the Free monad example from cats ( https://encrypted.google.com/search?hl=en&amp;q=cats%20free%20monad ) uses a get/put/delete dsl as example for their use of Free. &gt; Put(key, obj1) *&gt; Put(key, obj2) === Put(key, obj2) Why do you use the applicative *&gt; here? I would think that we need &gt;&gt;= because the order of the puts matter. &gt; it's a starting point rather than an end point (though it is often the most practical model). Again, I am very interested to read more about this. So if you would be able to give me some links (or terms I can google) I would be very grateful.
Spurious warnings about unused imports is the most obvious one, but other things get painted red too. I haven't tried to minimize it. A couple times a year I download the new version and open a project and see what it looks like and it's always a sea of red, so I quit and forget about it. Same with ENSIME. The presentation compiler just stops at a certain point and doesn't do the later stages of typechecking so even in principle it can't possibly work. Martin calls it "best effort" which isn't good enough for my neeeds. Same with IntelliJ reverse-engineering the world's most complicated typechecker. I just don't have time to chase my tail.
I could probably manage that, I am by no means a guru though. Also the guys on the eff gitter channel might be able to help you if you have a specific question. 
The general strategy is to talk about the *intent* to interact with an external service, which allows you to do much of your testing in these terms; i.e., you usually don't need to test that the email was actually sent, you need to test that the *request* to send an email contains the correct information, which shrinks the surface area of your integration testing. [This talk](https://www.youtube.com/watch?v=EaxDl5NPuCA) by Ken Scambler explains this strategy very well I think. A generalization of this idea is to abstract the meaning out of your program using `Free`, and providing multiple interpreters. This allows you to swap out production logic for test logic to simulate unusual failures and so on. It's kind of like mocking in spirit, but in a principled way without the spooky machinery. There are many talks and blog posts out there on this topic.
Thank you! Yes, I've found the command works.
Shrug. I mean the UX of "red underline with one kind of icon is normal, red underline with a subtly different icon is a genuine compiler error" is not great, but it is reliable, at which point the IDE becomes worthwhile (I mean, obviously I think that given that I'm using it).
You can express EitherT in Free form certainly - e.g. in Paperdoll I have a `sendEither` alias defined (or `sendU` will work). I assume FreeK (which is probably the mainstream free coproduct library you should be using - I just know Paperdoll better because it's my own one) will have something similar. I don't know how low-boilerplate the current state of the art is - I found some things I struggled with and some things that needed a nonintuitive encoding to make the type inference work right.
It isn't. I'm waiting for a 2.12-compatible release of scala-ide before I make the move to 2.12.
Thanks for the links I'll check them out.
Why doesn't scala seem to have built-in complex or rational numbers and is there a reasonably-common library people use? (yes I could write my own but I'd rather not have to bother...)
You can use spire, which is chock-full of math types. Specifically, check out spire.math.{ Complex, Rational }
The functional way is ... you guessed it ... functions. Specifically, pass in functions that do the effectful operations you need, then in tests pass in dummy functions with the same type signatures. E.g., object Main { def getUsers(implicit bigquery: String =&gt; Future[QueryResult]): Future[Iterable[User]] = bigquery("select * from users") map { queryResult =&gt; ... } } object Test { def getUsers() = { implicit val fakeBigquery = ... val expected = ... assert(Main.getUsers == expected) } } you can also wrap up these raw functions in typeclasses for more static type-y goodness.
[removed]
Worth a shot! thanks!
I currently use spacemacs for learning Scala (with ensime). And I love it. If I were involved in a bigger project, I would use intellij idea.
Free monads. Talk to me. Given some API defined in terms of data types, and then lifted into Free, how does the composition work? I know I can define a coproduct interpreter of 2 separate API's in order to get them working together (nicer than transformers it seems). That seems to be about combining two very disparate APIS (say talking to github and talking to twitter). But can I define multiple sort of interpreters for a single API, and then compose them together. The use case I am thinking is to add logging and metrics around this API. Most examples embed println's etc inside the core interpreter. I want to define my architecture in layers, and then structure them vertically. Does that make sense?
Ah, OK, makes sense
Okay I guess that makes sense. So the idea would be I build separate DSL's/API's and write my programs in that style, and leverage the fact I can compose interpreters via natural transformation? An example program: val eff: LoggingGithubF[List[Repository]] = for { _ &lt;- Logging.info("beginning program") user &lt;- Github.getUser(id) _ &lt;- Logging.info("get users") repos &lt;- Github.getRepos(user) _ &lt;- Logging.info("get repos") } yield repos Any thoughts? How would you solve the idea of adding logging to an API
&gt; Does anyone have any suggestions for things I should read? Anything on anger management, really.
10/10 shitpost! Upvoted!
Never change tpolecat. Never change.
I'd offer to do internal Scala training instead and not take the job if they refuse. Half because they aren't using Scala already, half because refusing to take advantage of an offer to share knowledge in the company would say something disastrous about the company.
The hard part is that it's one of the big 3 (google, apple, facebook). So trying to come in and change things would pretty hard. I agree that the lack of scala is a point of concern for me, so I'm also applying to more scala-based companies. Still a tough job not to take up.
Well, Apple uses Scala &amp; Facebook uses a fair amount of OCaml and Haskell, and hey, even Hack is a step in the right direction... My comment was mostly teasing, but not entirely. It's important to distinguish between "it'd be so cool to work here" and "but I'd be miserable each and every day." Using badly regressive technology every day would keep me away from any opportunity at this point.
That's exactly my issue (it's not special).
What's the issue exactly ?
I recently made the jump from C++ to Java and did some book research. I got Core Java for the Impatient by Cay S. Horstmann (also wrote Scala for the impatient). Pretty good reviews on both Amazon and Goodreads. I've so far read about 1/3 of the book and very happy with it, very concise. Both him and Herbert Schildt each have two thicker volumes covering the whole Java language, I'll consider getting them when I need them, the impatient-series is a nice concise overview that you can get through in a few days to a week. There are other books covering just the JVM I reckon. This is a video of Cay talking about the first release of his Scala for the impatient -book, I've got the second release but haven't started reading it yet. https://www.youtube.com/watch?v=L4c_TFAALBQ Cay S. Horstmann Cay is a professor of computer science at San Jose State University, a Java Champion, and a frequent speaker at computer industry conferences.
Based on the philosophy of the language. i.e. why Java is Array and why Haskell is List.
It's typical to have a special initializer. My question is why scala wished to be an exception.
fico
I did the same thing about year ago. One evening was enough time to read about the important bits and write some example code to see how it all fit together. [What's New in JDK 8](http://www.oracle.com/technetwork/java/javase/8-whats-new-2157071.html) is a good starting point. Streams and lambdas are the main topics of interest. Look at the java.util.function and java.util.stream packages. There's no concepts that you wouldn't know from Scala. Java 8 just has different syntax and less features when it comes to the functional parts.
 result[row][col] = A[row][col] + B[row][col]; in Java translates to result(row)(col) = A(row)(col) + B(row)(col) in Scala. Like Java, Scala does not have built in element-wise operators for two- or multi-dimensional matrices, so you would have to iterate indeed using `for` or `while`. Therefore, the best option is probably to use a dedicated matrix library for Scala if you intend to do a lot of matrix operations (e.g. nd4s). Obviously, for a tic-tac-toe that's overkill. Here I would use a `for` comprehension: for (row &lt;- rowIndices; col &lt;- colIndices) { drawGrid(row * squareSide, col * squareSide, squareSide, squareSide) if (table(row)(col).selected) ??? } 
Thank you, that's the way I've been doing that, but it seems quite un-scala to me.
Yes, you _could_ choose a default, but to what end? Occasionally saving a few keystrokes? It's not like `val a = [...]` is more readable or significantly easier/nicer than `val a = Array(...)`. I think the fact that `[...]` could reasonably represent so many different things (`Array`, `Vector`, `Set`, etc) is a good argument against having it in a language like Scala. The fact that all reference types are constructed in a consistent and explicit manner is one of the things I appreciate about Scala. Scala has plenty of things in it of dubious value (looking at you, `CanBuildFrom`) but I think omitting this particular bit of syntax sugar and opting for a more unified approach is one of the things Scala got right. Now, if there were value types and stack-allocated arrays, I'd totally agree with the `[...]` syntax for those since it's well understood from other languages with that feature, like C.
But why add that special syntax when it provides no benefit. Just because other languages have out doesn't mean that it adds value. Scala's version is more expressive because it allows you to create any type of collection instead of being forced into one. 
Part of Scala's philosophy is that it's very unopinionated and doesn't push you into defaulting to one style or another one unlike java or Haskell. 
[1, 2, 3] Wouldn't provide any real benefit over List(1, 2, 3). The only argument I can see is that other languages have a [...] construct which is a pretty weak argument and evidence that it's not really an important feature. The question isn't why not its why?
val a = 1 :: 2 :: 3 :: 4 :: Nil You could also do something with [string interpolation](http://docs.scala-lang.org/overviews/core/string-interpolation.html) if you were so inclined, i.e. val = arr"1 2 3 4" if you defined an `arr` StringContext. 
Because it's our best kept secret weaponM_.
So I was curious whether it would be possible to implement something similar if you really wanted it. It turns out "[", "]" and "," are not valid characters for identifiers, which is too bad, but beyond that you pretty much can. import scala.collection.mutable.ArrayBuffer import scala.reflect.ClassTag import scala.language.postfixOps class arrayLit[A](implicit ct: ClassTag[A]) { var buf = new ArrayBuffer[A]() def | (a: A): arrayLit[A] = {buf.append(a); this } def &gt; : Array[A] = buf.toArray override def toString: String = buf.mkString("[ ",", "," ]") } object &lt; { def | [A: ClassTag](a:A): arrayLit[A] = new arrayLit[A]().|(a) } val arr: Array[Int] = &lt; | 1 | 2 | 3 | 4 &gt; I had to make do with some valid ascii symbols. You can do unicode to get slightly better, but then you're dealing with unicode symbols and in my opinion none of the options are really *that* much better anyway. If anyone knows a way to remove the first delimiter I'd love to hear it. 
&gt; Based on the philosophy of the language Given that Scala's philosophy is "whatever you want", you can't choose one default.
Sequences are common, so a special and short initializer is nice. Why does Scala have `+=`?
Honestly - I'd just email the recruiter and see if you can interview with someone who has background in a functional language. If you're interviewing at a big tech company, there are going to be dozens of interviewers who can read/write functionally, and who probably be excited to interview talented candidates. I'd be much less concerned with the specific language and more concerned with your ability to build data structures/algorithms from scratch/limited use of the standard library.
Personally I'm partial to [LiftWeb](https://liftweb.net) .
While we're at it, doesn't [http4s](https://github.com/http4s/http4s) under web frameworks deserve a mention too?
and Seq is only two longer than that. If any collection "type" is special in the compiler it's Seq, and Seq(a, b, c) is short enough for me
A good explanation of the scala philosophy that this decision is based on is in the book Programming in Scala, authored by Odersky (the language's creator) with others. The first edition is online. The explanation is here: http://www.artima.com/pins1ed/a-scalable-language.html . Read the whole thing, but here's one representative quote: &gt; Eric Raymond introduced the cathedral and bazaar as two metaphors of software development.[3] The cathedral is a near-perfect building that takes a long time to build. Once built, it stays unchanged for a long time. The bazaar, by contrast, is adapted and extended each day by the people working in it. In Raymond's work the bazaar is a metaphor for open-source software development. Guy Steele noted in a talk on "growing a language" that the same distinction can be applied to language design.[4] Scala is much more like a bazaar than a cathedral, in the sense that it is designed to be extended and adapted by the people programming in it. Instead of providing all constructs you might ever need in one "perfectly complete" language, Scala puts the tools for building such constructs into your hands. 
Finch
Java 8 for the impatient -&gt; https://www.amazon.com/Java-SE8-Really-Impatient-Course/dp/0321927761
Sorry to bump in, I am also new in Scala and I understand the syntax of the for comprehension above, but I don't see where `rowIndices` and `colIndices` are coming from? Could someone explain me?
Done!
Here's Play with a Postgres DB and a CRUD interface against Slick, a functional ORM: * https://github.com/playframework/play-isolated-slick
Should have specified a "mutable" sequence, but I suppose this is true. In which case, I wonder how the answers about "not baked into the compiler" stack up against this apparent contradiction. 
How would you explain the existence of tuple initialization?
Is Play a "mostly functional framework"? 
`a = a + 4` works for the mutable set too.
If you want to practice some of the basics just to get the hang of it, Derek Banas has some kick-ass tutorials. https://www.youtube.com/watch?v=WPvGqX-TXP0
I think it's important for tuples to have shorthand syntax, because the whole point of tuples is for writing shorthand. If you use a particular type type often, you should turn it into a case class. Another factor: the original plan was to unify function arity with tuples. So just like functions have shorthand (function literals are just syntactic sugar after all), it makes sense that tuples would too. That may also be why they both have the same top arity.
Yes
I'd actually say `+=` is a wart that ought to be removed (the readability penalty is not worth the conciseness advantage)
I don't know about "mostly functional". If you don't need the performance of client-side UI I'm a big fan of Wicket; it uses OO in a really effective way, and you can use it very nicely with immutable data by treating your `Model`s and `Panel`s kind of like lenses. (Indeed one could probably make a lens-based `Model`, though the natural style kind of inverts what lenses do - rather than expanding modifications of a small thing into modifications of a large thing, you expand the large thing into small things, modify them, and then reassemble the results into a new large thing).
Exactly, for tic-tac-toe, `val rowIndices = 0 until 3` or something along these lines
Most of the changes you describe aren't a result of deliberately making the API obtuse, it has to do with the fact that akka-http uses akka-streams (and hence, streaming) basically almost everywhere it can, where as Spray only did streaming in certain places Because of this the API isn't exactly the same, and its also the reason why we have to deal with stuff like `.toStrict` and hence why its different to stuff like `response.body.data` in Spray. Streaming happens to have a very different mental model compared to non streaming
Finch is sweet and is 2-12 compatible. Biggest issue is twitter ecosystem (futures etc) 
I was about to say: boilerplate management :D
Many standard `sealed trait`s e.g. `Option`, `Either` `Try` define a `fold` method where you pass in handlers for all the possibilities and get back the result (kind of a more concise visitor pattern). I'm suggesting if you write a custom `sealed trait` you should also give it a `fold` method e.g.: sealed trait UserAuth { def fold[A](fa: PasswordHash =&gt; A, fb: PublicKey =&gt; A): A } case class PasswordAuth(hash: PasswordHash) extends UserAuth { override def fold[A](fa: PasswordHash =&gt; A, fb: PublicKey =&gt; A) = fa(hash) } case class KeyAuth(key: PublicKey) extends UserAuth { override def fold[A](fa: PasswordHash =&gt; A, fb: PublicKey =&gt; A) = fb(key) }
Http4s plus doobie. It doesnt really get any simpler
/u/bdavisx Scaladex is always up to date :-) https://scaladex.scala-lang.org/search?q=database 
You can also write some boilerplate: implicit class T2Ops[T](v: (T, T)) { def toList: List[T] = List(v._1, v._2) } implicit class T3Ops[T](v: (T, T, T)) { def toList: List[T] = List(v._1, v._2, v._3) } (1, 2).toList (1, 2, 3).toList
Nice. Such a hater. I love it.
Oh dear, I haven't done this for years (no-one's buying server-side web UI), I don't remember it well enough to write in detail. So not happening unless I end up actually wanting to write a web UI for something I think.
How about a `Map[(Int, Int), Int]` ?
And this is really good philosophy :)
To be clear, I never said they deliberately made the API obtuse, I felt they made the API drastically different and offered not enough guidance to make the move. I'll figure it out, every user of spray will have to figure it out, but I won't go looking for excuses to use more of their software. 
I don't like that people use libraries to argue about the language, the mariadb scala/go example is (in my opinion) a library problem
Possibly [Finch](https://github.com/finagle/finch/) as well? :)
Done! :-)
Here is probably the easiest way of doing what you originally asked (though not the best, see below) object Answer { import scalaz._ import Scalaz._ import scalaz.effect.IO // interesting code trait Halt[F[_],G[_]] { def apply[A](f: F[A]): G[Unit] } type ~&gt;|[F[_],G[_]] = Halt[F,G] implicit class AddLogging[F[_],G[_]: Monad](interpreter: F ~&gt; G) { def withLogging(logger: F ~&gt;| G): F ~&gt; G = new (F ~&gt; G) { def apply[A](program: F[A]): G[A] = interpreter(program) &lt;* logger(program) } } // your algebra case class User(name: String) case class Repo() case class Id(value: String) sealed trait Github[A] case class GetUser(id: Id) extends Github[User] case class GetRepos(user: User) extends Github[List[Repo]] object Github { type Program[A] = Free[Github,A] def getUser(id: Id): Free[Github,User] = Free.liftF(GetUser(id)) def getRepos(user: User): Free[Github,List[Repo]] = Free.liftF(GetRepos(user)) } // your program def prog(id: Id): Github.Program[List[Repo]] = for { user &lt;- Github.getUser(id) repos &lt;- Github.getRepos(user) } yield repos // your intepreters val githubEffects = new (Github ~&gt; IO) { def apply[A](program: Github[A]): IO[A] = program match { case GetUser(id: Id) =&gt; IO { // user = fetch through http User(id.value) } case GetRepos(user: User) =&gt; IO { //repos = fetch through http List.fill(user.name.length)(Repo()) } } } val githubLogging = new (Github ~&gt;| IO) { def apply[A](program: Github[A]): IO[Unit] = program match { case GetUser(id: Id) =&gt; IO.putStrLn(s"Get user for id ${id.value}") case GetRepos(user: User) =&gt; IO.putStrLn(s"Get repos for user ${user.name}") } } val compositeInterpreter: Github ~&gt; IO = githubEffects.withLogging(githubLogging) // at the end of the world val myId = Id("yo") val runMe = prog(myId).foldMap(compositeInterpreter) // scala&gt; runMe.unsafePerformIO() // Get user for id yo // Get repos for user yo // res0: List[Answer.Repo] = List(Repo(), Repo()) } Notice however that this approach has various drawbacks: * I can log *arguments* to my commands, but not their *results* * Composition works because both Logging and Github target the same final monad (in this case IO) if you want to use a combination of monads (say, State over IO), you still need monad tranformers. In fact, imho Free monads vs Monad transformers is somewhat of a false dichotomy. * It feels a bit too ad-hoc In general though, I think your goal (vertical composition, rather than horizontal) is worth pursuing. As a matter of fact, I'm writing a logging library to allow you to do just that, albeit with a different implementation strategy than the one outlined above to overcome some limitations/make it more featureful 
 I can't take people seriously after they choose a powerful language with a powerful typesystem and when they realize what they've done and how incompetent they're in PLT they switch to another dummy language which is designed for beginners or created by corporation x. &gt; The key point here is our programmers are Googlers, they’re not researchers. They’re typically, fairly young, fresh out of school, probably learned Java, maybe learned C or C++, probably learned Python. They’re not capable of understanding a brilliant language but we want to use them to build good software. So, the language that we give them has to be easy for them to understand and easy to adopt. - Rob Pike vs. &gt; Scala is an acronym for “Scalable Language”. This means that Scala grows with you. You can play with it by typing one-line expressions and observing the results. But you can also rely on it for large mission critical systems, as many companies, including Twitter, LinkedIn, or Intel do ... At the same time, Scala is the preferred workhorse language for many mission critical server systems. The generated code is on a par with Java’s and its precise typing means that many problems are caught at compile-time rather than after deployment. - Martin Odersky . &gt; bringing the overall CI times from more than an hour to as little as 20 minutes The [scala project](https://github.com/scala/scala) compiles in less than 8 minutes. I'm curious what they've done with their code... &gt; the following is a quote from Raffi Krikorian on Twitter: “What I would have done differently four years ago is use Java and not used Scala as part of this rewrite. [...] it would take an engineer two months before they're fully productive and writing Scala code.” Oh please, I've worked with people who spent years with java and they weren't productive at all. Btw, If you need two months to be productive with Scala then I'm afraid you won't be really productive in anything else. &gt; On the lighter side of things, we were able to identify exactly whether a programmer was using IDEA or sbt based purely on the loudness of their laptop fans. On a MacBook Pro, this is a real problem for anyone hoping to embark on an extended programming session away from a power outlet. It's hard to imagine a proper IT company without good computers and enough power outlets... Btw, they'll need 4-5x as many time to code and read the same in golang since it's designed in the fear expressiveness.
Library support is one of the most important factors. A language can be amazing, but without good, easily accessible libraries is not really worth using. 
Right on about the power consumption. Like, maybe their workforce is primarily made up of digital nomads who aren't always near an outlet. But I don't find that scenario too likely if they are full-time developers.
The core of Play is the `Action`. An `Action` extends from `EssentialAction`, which is a function that takes in a `RequestHeader` and returns an `Accumulator[ByteString, Result]`, where the `Accumulator` is a type of arrow that will take its input in chunks of type `ByteString` and eventually return `Result`. https://www.playframework.com/documentation/2.5.x/ScalaEssentialAction When you create an Action, you pass it a function as input to the apply method. And the way you create more complex actions is through composition: https://www.playframework.com/documentation/2.5.x/ScalaActionsComposition The API is set up to work with `Future` composition so that slow operations can be asynchronously run in a different thread pool while the core set of CPU bound rendering threads handle other work. Everything is based around data flow, and you can run Play with 384 MB of resident memory, because it doesn't keep state around. Working with the HTTP API in Play is done through functions and immutable case classes, so unless you have some other definition I'm not aware of, it's functional. One thing that people do ask about is how to deal with flow of data in validation -- the classic answer is monad transformers if you're okay with the conceptual overhead. https://speakerdeck.com/eamelink/flatten-your-code BTW, Li Haoyi just came out with a great article about functional programming: http://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html
&gt; It sounds like they were mildly bad at choosing Scala concurrency and IO libraries I think this is a real, addressable weakness of the Scala ecosystem. I write Scala every day, but when I'm trying to figure out which library to use for a new use case there are often 10+ choices and it's hard to figure out which one is best. Then I pick one and 6 months later I hear people saying "Oh, obviously library 5 was the best one." Compare this to Ruby or even Python, where there are usually only a few highly recommended libraries, and it's usually easy to find side-by-side comparisons. 
&gt; *Scala's built in futures are dogshit; for example I was able to speed up my spark code by 10 minutes just by moving from built in futures to scalaz tasks* I've worked on the [Monix Task](https://monix.io/docs/2x/eval/task.html), a Typelevel implementation that was inspired by the great work that went into the Scalaz Task and that currently has the best throughput, beating Scalaz by [a factor of 3](https://github.com/rossabaker/benchmarks/pull/4) and I cannot agree with this assessment. For one, as developers, we tend to suffer emotionally when realizing that we've chosen the wrong tool for the job and then we end up blaming the tool. But comparing Scala's `Future/Promise` types with `Task` is comparing apples and oranges. It's literally like comparing `A` with `() =&gt; A`. It's really not the same thing and there are cases in which `Future` is better than `Task` and vice-versa. In fact `Future` can be seen as a complementary for `Task`. Because when you evaluate a `Task` (by means of `runAsync` in Monix, or `unsafePerformAsyncIO` in Scalaz), you either need to give it a callback or you need to return some type that represents `(Try[T] =&gt; Unit) =&gt; Unit`, which you may recognize as the *observer pattern* and which might as well be a `Future[T]`. And yes, I know about `unsafePerformIO` from Scalaz, but that method is blocking threads and thus cheating and this matters because it can't be implemented on top of Javascript.
of course, but I bet you could write the go example nearly the same way in scala, that's the problem I'm having with it. They just chose the wrong tools in scala and found the right ones in go. I'm not saying this isn't a problem about scala, but I think the article made a wrong point there 
I disagree, they make a good point in their article. It is quite fair. Scala was not the right pick for their team in the end so they moved away from it. However I am not quite sure to understand why they felt like dedicating 5 hours to write a nice article about it. They might be trying to buzz to get visibility in the golang community
&gt; I disagree, they make a good point in their article. It is quite fair. They chose Scala because of its hype back then. They chose golang because of its current hype. They wrote the blog post to increase the hype. Their architects changed their mind based on some quotes and an article written 6 years ago. Everything they've mentioned against Scala has been improved through the years and with dotty it'll be far better. They've rewritten their product(s) - which took 20(!) minutes to compile - in another language instead of consulting with Scala experts. 
With full respect to you and your work, the fact remains that built in futures have non-obvious performance issues, and avoiding them in favor of alternatives like Monix or Scalaz yields significant speed and throughput improvements. 
Thiis is really controversial topic. Did they take the right tool for their job? Without context one can find any arguments. All languages have rough edges, else we would have only one ultimate one Lack of scqla developers and learning curve makes me nervous too 
Yes, they use Scala visibility to get some attention. As isaid a lot of people are jumping off ruby or java but do not inform anyone
Wrong tool in wrong hands witu wrong results
Since Perl i love this mess :) if there only one way to do things - it leads to stagnation. I love to have options ;)
Movio guy here, my opinions are my own and not my company's, etc. I also don't want to speak for Mariano and the other guys in his team, since we have different requirements and work on different systems. Scala was certainly a reasonable choice at the time, as the team had to build something robust to interface with an existing PHP application. I wasn't there at the time, but Scala was a brave choice, considering Auckland really is a .NET town. I'm in a different squad, and we still use Scala, but the systems Movio is building are now very different from the monolithic place we used to be. :)
Who said they wrote it because of "bad intent"? I've only said that they're following the golang hype and they weren't using Scala the right way.
Well, our marketing team wants to promote the company and blogging about our tech experiences and talking about what we do is part that. There's no sinister intent there. :)
&gt; Intellij Scala Plugin, with jump-to-definition taking us to outdated JARs rather than the modified files This usually happens because you have a dependency on a particular version of a library, so it takes you to the code for that version (via a sources jar, if possible, or regular jar if not) instead of whatever version you're modifying in the project. If you change the dependency version to match the in-progress version, IntelliJ is pretty good about recognizing that and creating a module dependency, which will then open that module's files in the editor. In short, it takes you to the definition of the code that you're compiling and (presumably) running against, which seems like pretty sane behavior to me. &gt; Coupled with a big monolith and a complex dependency tree with a complicated resolving mechanism - and after years of great engineers babysitting it - adding a property on a model class in one of our core modules would still mean a coffee break Build a monolith, get monolithic problems. Although later they claim they're using microservices, so which is it? I'll acknowledge they touch on a couple good points like the concurrency and deployment models (even though deployment isn't necessarily a language problem), and maybe I'm too cynical and unfair, but the majority of the post reads to me as: "we badly architected a system using a hyped language we didn't understand without any consistent team guidelines and didn't like the results so rather than apply our learnings and refactor using the shared knowledge we already have we're all going to jump ship and rewrite the world in the next hyped language because clearly our problems are the old language' fault and that's also what everyone else is doing". --- As as aside, what do these systems that take 10s of minutes to compile look like? Yeah, Scala takes longer to compile than many other languages, but I've never found it so slow as to be inhibiting. I've worked on systems written by myself and others and never had to wait more than a maybe 20 seconds for a compile. Have I just been lucky? Also, what do people _do_ that a 3 second turnaround is key to your productivity?
&gt; Build a monolith, get monolithic problems. Although later they claim they're using microservices, so which is it? &gt; This is a real business using Scala that doesn't have the luxury of total rewrites. ;) Like a lot of orgs, we've got legacy code. One piece is a Scala monolith. Practices change over time, so new functionality is implemented in microservices.
Can you explain how shapeless is more sane than scalaz? (Hint: it's not). Scalaz had a lot of FUD surrounding it, but deciding that because some *syntax* file has a lot of extra sugar is not a good reason to use Cats. 
This looks neat I was getting into parser combinators last week, and the learning curve was fairly steep. 
It's easy interpret things poorly when you're not talking to a real person, so hey there! :D So, a couple of things: &gt; They chose Scala because of its hype back then &lt;snip&gt; I wasn't there at the time, but there was a lot of interest in FP at Movio as a way to help produce quality code. Scala was a practical way to bring that into our day-to-day life. The enthusiasm for FP itself has waned lately (Not in my squad though. You'll pry functors out of my cold, dead hands), but it's been a good experience. &gt; &lt;snip&gt; They chose golang because of its current hype. Some of our engineers really like Go, and they made a good case for it. I think it's a good attribute of our company that you have the freedom to put the tech you like into production if you can justify it. &gt; with dotty it'll be far better No-one at Movio is holding their breath for Dotty, and we have development to do today. Besides, the JVM may not be the best fit for us going forward.
I write a lot of code in both due to my interest in distributed systems and go, the jvm, and erlang are the big 3 platforms. They really neglected to tell you the fact that Go is a very bare language, this is good and bad, but when I have to write code with a lot of logic, I have to do a lot of data transformations. I would never chose go unless it's a necessity. CSP is great in the sense that it isn't too abstract but it isn't terribly low level, making you feel like you have more control with your channel pipelines and select loops. But it's just a concurrency model so you can't compare it with Akka or OTP. Go's ideal use case would be something to use instead of netty, writing web apis, and command line tools. An example of something I wrote in go but shouldn't have is an in memory document store I wrote. The DB was relying on one struct and I had to constantly lock on write because really it was the only option Go gave me, in scala or clojure I could have used STM, I could have written the query API in half of the code it took me in Go in scala, even less in clojure. TLDR Scala's problems and strengths come from it's over engineering, go with it's under engineering. Go is nothing special as a programming language but is a great programming tool.
Sweet! One of the devs I work with wrote this. He turns out blog posts maybe every other month.
My opinion coming from both sides is the way Gophers use processes is much better practice than the way many Scala devs do. Too many people in the scala camp treat actors like normal objects, and encapsulate too much business logic inside and have too many methods within the actor. They also jump to using akka FSM when just using context.become is all you need for most cases. That or they just have a mutable var outside of the receive. In my experience go devs really don't fuck up with all of those java ish habits. Really just because of the fact that they have defined best practices (rob pikes word is law), also many of those people are still new to FP and akka is one of the first things you learn when you're learning scala, and Akka kind of enables you to do this shit. Actually until I learned Erlang and the OTP I was using Akka that way too, I see a lot of Elixir users making similar fuck ups, honestly I kind of think though the people who are serious about learning how to do things right will read some best practice guides by more experienced scala devs, look at erlang code, look at the code of akka contributors, and prominent software using akka, and start molding their own style. As well as working with CSP, STM, Futures, Streams. As for go not being what a programming language should be well go never aimed to be that it's pretty much C with GC and CSP and a good package management story. Go is a mediocre language it's a very useful programming tool. 
Blame Java, by the way …`split` is a method on `java.lang.String`, not something provided by Scala.
Here's an sbt template showing Play with Postgres, slick, scalajs, circe, diode, and a whole bunch of other stuff: * https://github.com/Daxten/bay-scalajs.g8
When I interview people I expect to see functional programming. I appreciate if people ask what I expect though instead of just assuming.
Sounds like microservices :) 
Even picking a DB library is hard. I'm not sure why this is; I suspect that it's due to having one big enterprisey org (Typesafe/Lightbend), one small type hackerish org (Typelevel), some big individual projects that sit in mostly walled gardens (Spark, all of Twitter's projects - somehow they don't have a strong Scala brand), and then a long tail of more or less individual projects. Each of these organizations seems to have quite different philosophies and tooling that doesn't necessarily work super-slickly together. I suspect this is also a big factor. I'm aware that Typelevel and scalaz community have done a lot to bring some underlying unity to the ecosystem. As that rolls on, I expect those to become the defacto standard that helps organise the ecosystem.
Yeah an in person interview that's what I expect but these type of things are basically unit tests with time limits 
I have reviewed thee article and this is more like "My excuses why i did not sucess in Scala" If you want to write better article the questions will be: What can be better in Scala? How to fix this? How do i know that Scala is the right tool for my job? Is there enough people to hire for our growing project? What and whom do i need to make a competent and informed decision? I sould rename the article to "I did bad research but cannot admit it"
This is not Scala's fault. Management did not do proper research. It happens too often. Management makes desicions not on real metrics(like you said earlier), but on minute preferences and trends. I did this a lot of times too :) overengeneering, trends, fads, choosing framework before writing software requierments. Coupling with bad agile practices that makes explosive mix 
I think you're attributing incompetence here, which I find unfair. We don't have pointy-haired architects calling the shots and choosing the tech stack we'll use 20 years into the future--as developers, we figure that out as we go. To digress a bit, Movio doesn't have a technical management layer; we do Spotify-style squads of ~4-7 devs, with tech leads who answer directly to the CTO. To make that concrete, I'm a tech lead for my squad. I represent my squad and make sure we're all making responsible tech choices, but we're relatively independent. This is why one squad of 6 people can use Go, without freaking out the other 30 devs ;) 
&gt; It's easy interpret things poorly when you're not talking to a real person, so hey there! ... I wasn't there at the time So, you wasn't there but you know it well. I'm really curious, no judgments. &gt; but there was a lot of interest in FP at Movio as a way to help produce quality code. Scala was a practical way to bring that into our day-to-day life...The enthusiasm for FP itself has waned lately It sounds like pure hype for me... &gt; Some of our engineers really like Go, If you *like and know* FP then you don't like golang. Except if you like the hype of a paradigm and switch to it and then some of your engineers start to like something else then you switch to that. Rewriting code? Easy breezy! Seeking for experts with our problems? Please... &gt; I think it's a good attribute of our company that you have the freedom to put the tech you like into production if you can justify it. Those "justifications" are pretty poor(smells like hype), the slow Scala compilation was mostly likely a serious issue with your modularization. The tooling argument is ridiculous - golang doesn't have an IDE as good as IntelliJ(it's not like you can do anything in golang). Also, the lack of expressive power in golang will result in 5-10x as many code, so the IDE support and the "easyness" won't help you in the long run(except if the product is simple and dumb). Also, more code = more thing to read. &gt; No-one at Movio is holding their breath for Dotty, and we have development to do today. You've work today, but you also have time to rewrite a huge(?) code base(which takes 20 minutes to compile!!!) and hype-based blog posts. And you not just replace the language - you change the platform too, throwing away all the stuff you've wrote or used. Wow! When your company chose Scala they knew they need to make tradeoffs and all the issues were *far worse years ago*. But if your compilation takes 20 minutes then you know you've done something wrong - that's where you need to consult someone because the entire [scala project](https://github.com/scala/scala) takes only 7 minutes to compile. Not even mentioning that if you're coding and you've already built the project once the compilation used to take only 2-3 seconds when you code.
Aside from the derisive treatment, I agree that these would be great questions to ask for anyone evaluating new development in Scala. It's difficult to talk to someone who assumes you're incompetent, so I wouldn't know where to go from here. :p
Lol :) You are right. We cannot control future, but we learn to steer off the iceberg. With proper work we do it earlier and more efficiently. 
Thank you! Now works
I feel that actually the article is the way to shift the blame from yourself on Scala. This is not right, because it is the way to protect own identity from the mistake. First thing - you can and should be allowed to do mistakes. Scala founders/devs does mistakes, library developers, end users. If we will try to find the single constant way to achieve future - we will fail. Future is non-deterministic. Trying not to fail is just the way to procrastinate. You had/have working solution is what matters. Ghere are many good points in article. I watching how Scala development is structured. Once i bet on Perl 6, but they failed to deliver. Because they wanted to make ultimate language of the future what is not possible. Actually if they would make minimal future set that users asked for they would have own golang in 2005. Anyway TIMTOWTDI is not for everyone and will not work for young teams. Python and Go are much better there.
Cool, I think we're on the same page about needing to be able to make mistakes in our work, and having the support we need to recover from them. :) 
&gt; It's reductive and sloppy to just say we're on the hype train. As you wish but it's my impression and my opinion. Nothing else. &gt; Yup, but real systems sometimes end up like that. Our current software is written in C mostly and the deploy takes 1 hour :| &gt; Join usssssss Sorry, [but i'm with the new editor of the beast](https://neovim.io/). &gt; To be cute, LOC isn't a good measure of code complexity. Go certainly seems to require more boilerplate, but these guys don't seem to mind, and in a 100 LOC microservice who really cares, right? As I've said more code = more writing = more reading = more time wasted. &gt; It's not being rewritten, that would be just plain irresponsible. Some parts, where it makes sense, are extracted into new services as required for new development. So, they haven't replaced Scala just started to use another language too.
Actually the best post to r/scala ever. I can feel my free monads tingling. 
I forgot to say that people take the post too personal. Actually it have a lot of valid points and things to reason about. I wrote about decoupling- that means if you draw the distinct lines - people will agree, there is a lot of things that could be done better. In Scala, project desicions, tools, teamwork. Also concrete examples helps th understandd the skurce of bad desicions. We all learning :) Actually it was interesting read
The first time I did this stuff was actually in Java, but yeah Scala more recently. Models not that complex tbh - I've mostly worked on teams of about 10 people, so the codebases have been corresponding size. Millions of rows - billions in one case - but that's presumably not what you were asking; in terms of how many distinct types of entity probably high tens / low hundreds.
If that's an important metric, have you looked at using a Java application server instead of docker images? That way you can share common libraries and memory between multiple microservices, and still enjoy a similar level of isolation to what docker would give you.
but is it pure?
The "scala platform" effort exists. I haven't touched it myself because I can't stand SBT, but AIUI it's designed to address exactly this issue. (I've found it's even worse for Python FWIW)
Good to know. I'll ping the dev who wrote it a little later.
Нeу guуs. Recentlу, I undеrstoоd monads in programming - it’s thе mоst еffеctive way to express a sequence of dependent operations. I rеgistered оn the most poрular programming language sitеs, but with thesе languages hаvе a mutable state, theу arе reluсtant tо treat functions as values and оnlу sometimеs agreе on the purely functional style. Рersоnаlly, I would likе tо find а programming language just for expressing mathematical functions, without state. Оne mу friend аdvisеd to lооk fоr scala programming language for immutability. At whiсh thе values аrе registеred with the samе purрosеs - to act as an immutable value in its scope. It turned out thаt languages registerеd morе mutable state thаn immutable. "Whiсh programming language is immutable аnd expressive" - I asked thе quеstion in thе programming language forums I’ve rеgistеred аt mоst оf thеm, but in thе end I found оne goоd programming language. I note that it's immutable by default and аlwаys many abstractions for composing programs from different repositories are оnline. If someоne is intеrestеd, herе it’s this site: https://www.scala-lang.org/ Whаt abоut mе - I hаd composed 3 immutable functional programs in the рast mоnth with 3 frameworks from this wеbsite.
Interesting. People who have read this: what do you think?
I may bite the bullet and go for it, then report back. E: I have bought it, will get started on it soon. I'm currently being thrown into a Scala project at work, so am looking forward to this! The one at work is Akka however.
Is this the same one done by Martinson? 
That'd be appreciated. It looks decent from the table of contents.
He has a more advanced Scala book and I can definitely vouch for the quality of the advanced book. I took a quick glance at this one and it seems to be written quite well. 
As an interviewer, I'm way more interested in your ability to reason about a problem than any particular solution. That said, for the past 5+ years, I've worked in pure-FP Scala organizations, so I've tried to be explicit about framing questions as "How would you do this in a functional way," and sometimes that necessitates clarifying that even mutation, I/O, etc. need to be done "purely," which in Scala comes down to specifics about scalaz/Cats, scalaz-stream/fs2, etc. Also, I never reject a candidate just for not knowing those specifics; those are things an appropriate candidate can learn on the job, just as I did. It really does always come back to: how do you tackle problems; is your thought-process methodical; are you good at eliminating variables in an investigation; do you have a strong inclination toward local reasoning, however expressed; etc.
&gt; I personally would like abstract value types, i.e. I don't want the user to know what the real type is under the hood, but I still want it to get compiled away to nothing at runtime. How does an `AnyVal` not satisfy these criteria?
Can you please give a pseudo-code example of what you're seeking? 
I like this quote from Paul Phillips' talk: "Those who would give up correctness for a little temporary performance deserve neither correctness nor performance." - slide 14 from Paul Phillips' [presentation](http://www.slideshare.net/extempore/keynote-pnw-scala-2013)
 class Id(unwrap: Long) extends AnyVal { def toLong: Long = unwrap }
Why not just use a typeclass and resolve the underlying id with a fundep? trait Id[T, I] { def fromId(i: I): T def toId(t: T): I } def printLongId[T](t: T)(implicit id: Id[T, Long]) = println(id.toId(t)) case class CommentId(id: Long) extends AnyVal case class GitCommitId(hash: String) extendsAnyVal printLongId(CommentId(1L)) printLongId(GitCommitId("deb98304611c")) //Compilation Error, Id[GitCommitId, Long] not found. EDIT: If using cats or scalaz, you can generalize this by asking for any T which is isomorphic to Long.
Thanks really nice answers and I agree fully. My concern was more about the fact that those sites don't have anyone to asks specific or guide it what they are interested in. It's purely a text specification and you submit a file as an answer. Also out of curiosity how far did you manage to get in 5 years with scalaz /cats /streams etc? 
Sorry, I was just plain wrong with the earlier version of my comment. Revised a few minutes ago but I guess you were replying to the earlier version by then.
in the case statements you would return the the val +acc so the first case would look like: case 1 =&gt; "A" + acc Side note: I hope I'm looking at that correctly. I could be wrong and it might be acc + "A"
I don't think it's really a problem with iteration, but I'd probably write this as a recursive algorithm regardless. My advice would be to take several tread dumps, usually `kill -3 $PID` on unix systems. That will show you where the program is currently executing. With a few samples you will probably see program is executing in one particular spot. It might also be a good idea to log GC. My guess would be &gt;new Mp3File(file).hasId3v2Tag is where you are spending a lot of time. Depending on how it works that could take a ton of memory and a lot of IO. It better to measure than guess though, so definitely log GC and take some thread dumps.
We work on a separate greenfield product started 2 years ago, so we were able to move to microservices once the core service expanded too much. We've still got a one or two large services (~1-2K LOC), but most are small. Most of what we do is in Play microservices. My squad has Haskell experience and we use Scalaz. We've stayed away from Akka entirely. Tooling, and Ensime in particular, is great for these smaller codebases, so no complaints from me there. We have maybe 20 microservices in prod at the mo, with a replication factor of 2 for most of them in Kubernetes. All those JVMs are starting to put pressure on our architecture to do sub-optimal things, like running multiple services in the same VM. We would really rather have them separate, so Kube can shuffle them around and monitor their state independently. This is really pressuring us to move somewhere else.
Not the brightest solution but it might help: import scala.concurrent.Future import scala.concurrent.ExecutionContext.Implicits.global def walkDir(dir: File): Future[Vector[Song]] = { print("Beginning music load process") def walk(path: File): Array[Future[Option[Song]]] = if(path.isDirectory()) path.listFiles.flatMap(walk) else Array(Future { // it's better to divide the task if(path.getName.endsWith("mp3") &amp;&amp; new Mp3File(path).hasId3v2Tag) Some(new Song(path)) // because of this else None }) walk(dir).foldLeft(Future.successful(Vector[Song]()))((buffer, future) =&gt; // fold the results into a Vector buffer.zipWith(future)((vec, option) =&gt; option match { case Some(song) =&gt; vec :+ song case _ =&gt; vec }) ) } and the you can either walkDir(myDir).onComplete(doSomethingWithTheSongs) or import scala.concurrent.Await import scala.concurrent.duration._ val songs = Await.result(walkDir(myDir), 30.seconds) I'd go with the first. Edit: minor enhancement
A typical service just exposes some routes, checks credentials, validates some JSON, and does some queries. Maybe 300 LOC, excluding tests and code generated from the API spec. These days, we express a lot of operations in our domain with CRUD APIs, feeding into ES or Kafka as appropriate. Then, we have downstream processors (Samza now, maybe Spark down the track) that do heavy lifting and output something different. A basic CRUD service doesn't do much at all and won't be that big--most of what we do in my squad is shuffle data around and analyse it in aggregate.
This was my first ever Scala project, made months ago. At the time I was really just fumbling on through the language and careless shit like that just kinda... happened.
Fwiw TypeLevel has no relation to Typesafe/lightbend. 
if you were to call getDateString2(1, "") it will return "A" because the accumulator at the time will be a "" and case one 1 returns "A" + acc or "A" + "". I'm still kind of lost on what this actually does, and I think your case _ isn't doing what you want, because I don't see how your accumulator ever actually grows. If it starts as "", then the _ step will cause it to just become "" + trans("") every time. And I don't know what 'case o@_ =&gt; o' does but I'm going to guess for empty string it will return empty string. 
You might be better off creating a tailrec helper method within getDateString2 that is only executed for values greater than 3, in which case your accumulator would start at "AYYA". Execute a match then on day. If 1, then "A". If 2, then "AY", If 3, then "AYYA". Any other case helper(days - 3, "AYYA"). Does that make sense?
Yeah that's actually what I'm aiming for. I realize it was a stupid question, just one more in the helper when would you return the accumulator? 
Basically it does this : Day 1 - A Day 2 - AY Day 3 - AYYA Day 4 - AYYAYAAY Day 5 - AYYAYAAYYAAYAYYA ... 
You can return your acc when you have landed on your base case and need to terminate the recursion. So if you go with iuhoosierkyle's suggestion I think this will be when the first argument == 0 in the helper.
Thanks for the reply :) and wow Funnel seems like really nice codebase to check out. By the way is this you https://www.youtube.com/watch?v=uiJycy6dFSQ or just a name collision? If so I really liked the talk :)
hey, there are already a bunch of good explanations here why that is slow. Here is my solution for your problem: use better-files! https://github.com/pathikrit/better-files it has a fast implementation of listRecursively, also you should use par since you have a bunch of file IO I guess def walkDir(dir: File): Array[Song] = { print("Beginning music load process") dir.listRecursively .par .filter(f =&gt; f.extension == Some(".mp3") &amp;&amp; new Mp3File(file).hasId3v2Tag) .map(file =&gt; new Song(file)) .toArray }
parallelising the reading from disk is not going to speed up much, is it? as access to disk will be sequential anyway.
`.par` may not be worth it - certainly benchmark with and without before adding it.
Haha you got me
Use a type parameter. trait ItemType case object Pen extends ItemType case object Paper extends ItemType case object Staple extends ItemType case class Order(customerId: String, itemType: ItemType, items: List[ItemData]) case class PenData(color: String, weight: Int) extends ItemData[Pen] case class PaperData(color: String, length: Int, width: Int, lines: Boolean) extends ItemData[Paper] case class StapleData(size: Int, thickness: Int) extends ItemData[Staple] case class Order[Data[_], I](customerId: String, itemType: I, items: List[Data[I]]) val order1: Order[PenData, Pen] = Order("123", Pen, List(PenData(...))) val order2: Order[PaperData, Pen] = Order("124", Pen, List(PaperData(...))) //Compilation Error. 
I'd imagine with this, there's no reason to for you to do any pattern matching, you already know what the type of itemType is.
true
Yes i can, i'm working on articles to back the experience with Scalameta and my library
I think having the itemType as a typeclass is interesting too: https://scalafiddle.io/sf/LGl39i4/0 . You don't have to annotate the type of your order if your collection is filled with a certain type of ItemData.
&gt; https://scalafiddle.io/sf/LGl39i4/0 I posted some examples above, hopefully that helps. This is a ficticious domain analog to what I am actually working on as I cannot safely share my code as of yet. The business rules are really sort of validation checks across a sequence of orders. Where each order type will invoke different business rules.
I'd make an extension method with implicit parameter and be done. implicit class StringDistanceCalculator (string: String) extends AnyVal { def distanceTo (string2: String)(implicit stringDistance: StringDistance) = stringDistance.distance(string,string2) } If you need StringDistance as a Function type, you can do it with a wrapper as well: implicit class StringDistanceWrapper (stringDistance: StringDistance) extends Function2[String,String,Double] { def apply(string1: String, string2: String) = stringDistance.distance(string1, string2) }
Nah, I don't think it's possible to easily port it to Maven.
lol
I'm working on a DSL for describing calendar systems in my free time. For when you like need to calculate in the polticial calendar of Athens 400 BCE (and converting it to the religious calendar) or when you need to keep track of the three moons circling your fantasy world. It's my first Scala project. I'm a latecomer to programming, having worked in philology for quite some time. 
I would probably start off with something like this: sealed trait Tree[+T] { def headOption: Option[T] def isEmpty: Boolean } object Tree { def apply[T](): Tree[T] = EmptyTree def apply[T](head: T, children: Vector[Tree[T]]): Tree[T] = NonEmptyTree(head, children) def apply[T](head: T, children: Tree[T]*): Tree[T] = NonEmptyTree(head, children.toVector) def unapply[T](tree: Tree[T]): Option[(T, Vector[Tree[T]])] = tree match { case NonEmptyTree(head, children) =&gt; Some((head, children)) case EmptyTree =&gt; None } } case class NonEmptyTree[+T](head: T, children: Vector[Tree[T]]) extends Tree[T] { def headOption: Some[T] = Some(head) def isEmpty = false } object NonEmptyTree { def apply[T](head: T, children: Tree[T]*): NonEmptyTree[T] = NonEmptyTree(head, children.toVector) } case object EmptyTree extends Tree[Nothing] { def headOption = None def isEmpty = true } val tree1 = NonEmptyTree(1, Tree(1), NonEmptyTree(3), EmptyTree) val tree2 = EmptyTree val tree3 = Tree[Int]() val tree4 = Tree(1, Tree(2), Tree(4)) tree4 match { case NonEmptyTree(head, children) =&gt; println("non empty!") case EmptyTree =&gt; println("empty!") } tree2 match { case t @ Tree(h, c) =&gt; println(t) case EmptyTree =&gt; println("empty!") } 
I wouldn't worry about methods. Define an ADT and use pattern-matching. sealed trait Tree[+A] case object Leaf extends Node[Nothing] case class Node[A](data: A, left: Tree[A], right: Tree[A]) extends Tree[A] If pattern-matching is a hassle you can write a fold to generalize traversals: sealed trait Tree[+A] { def fold[B](f: =&gt; B)(g: (A, B, B) =&gt; B): B = this match { case Leaf =&gt; f case Node(a, l, r) =&gt; g(a, l.fold(f)(g), r.fold(f)(g)) } } which lets you do things like scala&gt; val t: Tree[Int] = Node(1, Node(2, Leaf, Leaf), Node(3, Node(4, Leaf, Leaf), Leaf)) t: Tree[Int] = Node(1,Node(2,Leaf,Leaf),Node(3,Node(4,Leaf,Leaf),Leaf)) scala&gt; val total = t.fold(0)((a, b, c) =&gt; a + b + c) total: Int = 10 scala&gt; val height = t.fold(0)((_, b, c) =&gt; 1 + (b max c)) height: Int = 3 And if you really want to be bad ass you can *further* generalize by removing the recursive structure altogether via the techniques alluded to [here](https://www.reddit.com/r/scala/comments/5qiv1d/simple_real_world_matryoshka_example/). Hope this helps, there are a kind of a lot of ways to go with your question. 
But... this adds nothing over what Slick already offers. All of those methods can be s/repo.meth/slick-call/g. Someone would be a lot better off just reading a good book on Slick to really get to know it. I highly recommend "Essential Slick."
Sure - the locking part is not in Slick. It could be a case where the example code on your github page is trivial slick code already so it does not show what you can do.
Yes it does. You just have to go through the READ ME file if you are really interested in learning what the library offers. The code on the project page is intentionally simple. We should not expect CRUD operations like the following to be complex and hard to read: personRepository.save(person) or val person = personRepository.findOne(1) The library is abstracting slick verbatim for you without requiring that much effort in configuring your tables/case classes like you would normally do. You just have to additionally extend some library traits. By the way, did you noticed that every operation that is provided by the library is backed up by a Slick compiled query? That is also abstracted away from you. 
https://github.com/rockymadden/stringmetric Here's a library that has string similarity functions.
Got any code you can share?
[Quasar](https://github.com/quasar-analytics/quasar) 
Asynchrony is a concept more general than multi-threading or `setTimeout`. You can think of it like this: multi-threading &lt;: asynchrony setTimeout &lt;: asynchrony If you want to represent it with a type, that would be: (Try[A] =&gt; Unit) =&gt; Unit If this looks ugly with all those `Unit`, it's because asynchrony is ugly. So it can be any task, thread, process, node somewhere on the network, that: 1. executes outside of your program's main flow or from the point of view of the caller, it doesn't execute on the current call-stack 2. receives a callback that will get called once the result is finished processing and 3. it provides no guarantee about when the result is signaled, no guarantee that a result will be signaled at all actually It's important to note asynchrony subsumes concurrency, but not necessarily multi-threading. But to answer your question more directly: &gt; coroutine-like task switching No, `Task` is a couroutine-like task management system in itself. &gt; setTimeout(0)-based tasks Yes, but not only that. Everything in Javascript is asynchronous, including UI interactions (e.g. detecting mouse, keyboard events), Ajax calls, file operations in Node.js and any kind of I/O actually.
You need to use Future[Either[Error, Result]], consider using EitherT from cats.
You want to hear something. Been working with finagle/finatra and that whole stack for the past 1.5 years. The docs have gotten better but yeah to really understand what is going on you will have to read the source code and the comments. Then the docs. Then the code. Then things might start clicking. Very performant stuff with some really advanced controls as a framework but not great explanations on what a lot of things mean. 
Do you think its a good framework for learning scala ? slight off topic , has anyone noticed twitter seems slower (just from past couple of weeks on android). It's slower at loading up and to refresh.
Why is everyone using `Either` instead of `Try`? It was made for exactly this case.
Probably not, unless you're already knowledgeable around networking and such. Dunno about slowness, but it's likely not related to Scala. 
Why is this downvoted?
Cause Either is more flexible in case if you wanna get something other than throwable as unhappy result.
Btw here is a little bit outdated article [Stacking Either And Future](http://eed3si9n.com/herding-cats/stacking-future-and-either.html) which might be helpful. Outdated cause now cats using native Either (since 2.12 Either became right biased) instead of Xor.
it's interesting to hear that your micro-services are that small. At some point there are trade-offs where that size can possibly lead to a lot of micro-services which in turn has a cost on deployment, management, monitoring etc. You mentioned having 20 micro-services in prod, that seems easily manageable. I've heard of other companies companies having several hundred! I wonder now if that is related to a very small size. Most of the micro-services I've worked on are kind of like what @lihaoyi mentioned. Basically, maybe 1 to a few API's with each API having some number of actions/endpoints, with perhaps a 10K line count ( very high-level estimation ). i"m just curious what the costs ( time, effort, management, etc ) are from having really tiny microservices, especially when there end up being a lot. 
fixed =) sorry for the inconvenience. Was looking for the programming books for my nephew.
Yeah I really hope someone comes along and writes a book on delving deep into Twitter's stack. It seems like a lot of blood, sweat and tears went into it but in order to figure out how to use it properly you have to read their source code. I tried reading their docs which are good but I don't think come close to the Akka docs 
We use mysql like a key/document store, something like this http://blog.wix.engineering/2015/12/10/scaling-to-100m-mysql-is-a-better-nosql/ So I have a layer that enables users to dump out a case class to the DB easily, with all the classes effectively using the same schema that has an ID, a schema ID and the serialized class itself. We do this so that we iterate on the schema and for speed. We don't need indexes into the class itself.
Brain dump incoming. ------ Trust me, after a couple of years of this we're aware of the trade-offs. ;) When I say we have 20 microservices (which equals 40 instances running due to replication), that's just for my squad. We have 3 squads right now sharing the same deployment infrastructure. So I guess we're players? :p Management is easy, cos after thinking _really hard_ about the API and the system it interfaces with, the implementation is generally obvious. We run our services in Kubernetes, so rolling updates, monitoring and lifecycle management is mostly automated. I think all the squads have moved to one repo for all their services and configuration at this point, which makes dev processes a lot more manageable. We've got Jenkins jobs to build and deploy, and we deploy pretty much whenever we want. Having services this small has a lot of benefits. Build times are shorter, and you have less code to read when you're extending the system or troubleshooting. We had some new guys join recently, and I think having a good understanding of a couple of services from endpoint to datastore really helped give them a sense that they understood what was going on. Dev time is great, cos we take an API spec and just code-gen the types and validation code. Other squads code-gen an initial implementation for their entire service and adapt it, but we just duplicate a recent one in mine. It's not perfect, but we've built the systems to make it easy for someone to write a service quickly. The problem we have with our nano/microservices is JVM overhead. 
This works just fine, but it might be against the title of the question (modelling without repetition). Perhaps in the OP real code there are a lot of "items" and duplicating all the data types is inconvenient
But I sort of think that's a good thing since I think that netty is a great lib (I would prefer go for writing servers but hey), and honestly in order to know any tech well you have to be able to dig to the lower layers. 
I work for one of the bigger Scala groups, but each team in the same org. has different interviewing questions/standards. ( /u/paultypes was my coworker but on a different team, for example). We usually want to see if a candidate is *capable* of thinking 'functionally', so we don't *expect* them to write recursive immutable code without any help, but we want to see that with a bit of help they can get to that kind of solution. If you can do most of the easy and medium questions in 99-scala problems in a functional way, we'd be very impressed. 
I'm trying to get my head around Cats and specifically monad transformers. I think I get the reader and State monads, but how do I compose them? How should I implement the `getPasswordAndLogin` function in the code below? The function should get the password from Config `c` and mutate State `s`. import cats.data._ object Test extends App { case class Config(url: String, username: String, password: String) case class Login(isLoggedIn: Boolean) val c = Config("www", "user", "pass") val s = Login(false) type ConfigReader[A] = Reader[Config, A] type LoginState[A] = State[Login, A] def password: ConfigReader[String] = Reader { c =&gt; c.password } println(password.run(c)) def login: LoginState[Unit] = State { state =&gt; (Login(true), Unit) } println(login.run(s).value) type Test[A, B] = StateT[ConfigReader, A, B] def getPasswordAndLogin: Test[String, Unit] = ??? }
It may be bad, but it got me to read the information I would have otherwise ignored and followed the link someone provided to the original
Thanks for the contribution. I've been trying to get something like this going for a while myself. 
Java 8 will be easy for you. The hard part will be learning all the frameworks that your company uses... 
Thanks. I'm just looking ahead for when I've completed the intro book stuff. A good framework to see quality code written by scala people who've been using it for a number for years. Ideally something not too big. just small / neat. 
I just had to do csv stuff at work :( ended up using clojure though
[removed]
Just a heads up - the function is using the map from the outer scope, rather than the "pets" parameter. It doesn't really affect the result but it did trip me up at first!
I'm trying to use Regex's built in unapply method to do some matching. The following is an example taken from Programming in Scala 3rd edition (Odersky, Venners, Spoon) val Decimal = """(-)?(\d+)(\.\d*)?""".r Decimal: scala.util.matching.Regex = (-)?(\d+)(\.\d*)? val Decimal(sign, integerPart, decimalPart) = "-1.23" sign: String = - integerPart: String = 1 decimalPart: String = .23 I am trying to apply this concept to parsing of a SQL statement. Below is a generic statement that creates an index. val stmt = "CREATE INDEX SOME_INDEX ON A_TABLE_I_MADE (A_COLUMN, ANOTHER_COLUMN) NOLOGGING PARALLEL 16" I then create a regex that attempts to capture the two columns, along with the comma and space in between: val Regexx = """(\w+),\s?(\w+)""".r Then I verify that this regex can actually find a match (and I do get a match): val findMatch = Regexx.findFirstMatchIn(stmt).get //success findMatch: scala.util.matching.Regex.Match = A_COLUMN, ANOTHER_COLUMN It's a pretty simple regex, with two capturing groups. I then try to get the values `group1` and `group2` out of it, and I get a scala Match error: val Regexx(group1, group2) = stmt I do not understand at all. Anyone got an idea?
Not sure why these kinds of post get so much attention. People switch the technology they are using all the time. Doesn't mean it's worthy of writing a 10 minute article about. 
I've been giving Ocaml a look this year, it has some fascinating concepts. Really worth it from just an academic point of view.
Way better typesafety of the model in general, it is impossible to construct content types which make no sense etc, expressing entities as streams (akka streams), addition of java api with zero allocation cost
I'm surprised anyone but twitter is using it tbqh. My experience was after reading a lot of the documentation it sounded interesting. I cloned Finagle and the build instructions didn't work because of a dependency to some internal twitter library or something. I opened a ticket which was promptly closed with "yeah we don't really care". That was the end of my interest in it.
I think an article like this should definitely try to show structural typing(duck typing) as it basically gives dynamism when needed. 
[removed]
You see when things like that happen to me I don't stop until I get it working, with or without help, that kind of obsessiveness has cost me a lot but it has also made me a pretty solid engineer.
Author here - of kantan.csv, not of the post. Happy to answer any question you may have.
this looks like the beginning of a book
Really only an idea, as I dread regexes.. I think the problem lies in your regex itself, there's no match for your complete stmt. My guess is findFirstMatchIn still works because it will traverse the input string until it finds a match, but unapplySeq doesn't. It works if you add some ".*" to your regex.. scala&gt; val stmt = "CREATE INDEX SOME_INDEX ON A_TABLE_I_MADE (A_COLUMN, ANOTHER_COLUMN) NOLOGGING PARALLEL 16" stmt: String = CREATE INDEX SOME_INDEX ON A_TABLE_I_MADE (A_COLUMN, ANOTHER_COLUMN) NOLOGGING PARALLEL 16 scala&gt; val Regexx = """(\w+),\s?(\w+)""".r Regexx: scala.util.matching.Regex = (\w+),\s?(\w+) scala&gt; Regexx.unapplySeq(stmt) res10: Option[List[String]] = None scala&gt; val Regexx2 = """.*\((\w+),\s?(\w+).*""".r Regexx2: scala.util.matching.Regex = .*\((\w+),\s?(\w+).* scala&gt; Regexx2.unapplySeq(stmt) res12: Option[List[String]] = Some(List(A_COLUMN, ANOTHER_COLUMN)) scala&gt; val Regexx2(g1,g2) = stmt g1: String = A_COLUMN g2: String = ANOTHER_COLUMN 
 &gt;&gt;&gt; [10,10,10].map(parseInt) [10, Nan, 2]
/u/alexelcu in case you're ever considering it: put me down for a pre-order.
This looks really good.. I deal with a ton of CSV parsing at work. Here's a few things I can't quite understand from your docs: * For decoding rows, do you support header mappings, with fallback to column/index mappings? The examples show headers in inputs but I don't see them being used by the decoders; I might be missing it. * How would you support a very large number of cells in a row, where you only want to extract a few? * You mention poorly formatted CSV. One of the most common issues I've run into is improper implementation of quote characters--fields are simply wrapped in the quote character and cell content is not escaped. Any help there?
In one project I used it for dependency injection, and I saw it used when working with proprietary code that you can't modify but would extend it's functionality (like making it autocloseable) 
I never understood why dynamic typing is associated with "fun". To me it was always a source of frustration, that i can't guarantee what my function will get, and a source of runtime bugs, crashes. But if a dynamic project get's a bit bigger it will require comments, that will contain the types (and will get outdated and will confuse the hell out of half of the team), and at point i just don't see the point dynamic typing. ^^cries ^^in ^^a ^^million ^^lines ^^of ^^php ^^code I can understand the frustration with types in languages like C where there are types, kindof, but don't guarantee anything and a lot of times they are burdensome, or in Java, where typically it's just too verbose ( although that is sometimes the problem of the conventions , not necessarily the type system's)
The easy part is the `ConfigReader`, you use `StateT.lift` to turn that into a `Test`. The hard part is the `State`. You can't turn a `Foo` into a `FooT` in the general case. You should write `login` as `def login[F[_]: Applicative]: StateT[F, Login, Unit] = StateT.set(Login(true))` instead - or more generally as `def login[F[_]](implicit ms: MonadState[F, Login]): F[Unit] = ms.set(Login(true))`. You should be able to call either of those definitions (possibly with suitable type parameters) and get a `Test[String, Unit]`. Once you have two `Test`s you can combine them with ordinary monad operations (e.g. `for`/`yield`) however you see fit. In the specific case of `State` you can also abuse the fact that `State[?, ?]` is an alias for `StateT[Eval, ?, ?]` and use `.transformF` instead. Unfortunately cats doesn't have `Hoist`, so as far as I can see you'd have to do this "by hand", something like `login.transformF{ e =&gt; Reader(_ =&gt; e.value) }` (again possibly with extra type parameters). I wouldn't recommend that approach myself. 
That's a good practice when it's your own time but I'm responsible for choosing tech for a team of 10 engineers and that means if we use FOSS then it has to have a large community of active users so we don't get left high and dry if we encounter issues in production.
1/ header mapping: unfortunately, the header might as well not exist as far as kantan.csv is concerned. This is something that has been requested a few times recently though, so I might try to work something out. The reason for that design choice is that I started writing kantan.csv for work, and the CSV I had to deal with never included headers. 2/ when you say "very large number", do you mean one that might not fit in memory? kantan.csv currently loads rows one by one, but it does load an entire row before decoding it. If your issue is not memory but cherry picking, it's pretty trivial: case class RowData(i: Int, f: Float, o: Option[Boolean], s: String) RowDecoder.decoder(0, 4, 9, 209)(RowData.apply _) This creates a decoder that grabs cells 0, 4, 9 and 209, decodes them to Int, Float, Option[Boolean] and String, and passes them to RowData.apply. 3/ I *think* this is either not dealt with properly, or not completely. Do you have documents you could share? 
With Scala.js you need Javascript in the same way that with the JVM you need to know about Java. But you don't need the nitty-gritty, just the general concepts and rules of the underlying platform. The ecosystem is also slightly different. As an example as a Scala developer you don't need to know the visibility rules of Java classes and members, or even the standard Java library until you need it, devs don't use `java.util.List` unless interroperating with something and you usually don't need to know about Spring, Hibernate, Guice and all that EE crap, unless you are unfortunate enough to work on projects started by Java developers 😄 The Scala.js ecosystem of libraries is doing pretty well too and interoperability is easy. The problem is that we aren't doing as well as ClojureScript in terms of tooling and Facebook React &amp; friends integrations. But then ClojureScript is the most popular alt-JS language (not counting JS skins).
Tried to switch to 2.12. Discovered there was no Treelog release. Worse, no scala-ide release. Gave up.
Personally I have this policy that if I see library dependencies that haven't upgraded to latest Scala version yet, I drop them like they are hot potatoes, because it is my general belief that well maintained libraries have a fast upgrade cycle and I want to depend on well maintained libraries only - otherwise you end up collecting technical debt. This is also a problem created by Scala the language. With Java or Clojure you can depend on libraries that haven't been updated in 6 years - as long as they do their job, then why not? But ALAS in Scala it's something we must live with. But then again, as a library author it isn't that hard to upgrade, most of the time being just a number increment and maybe fixing deprecation warnings. I mean if a library like [Scala STM](https://nbronson.github.io/scala-stm/), whose development has largely stopped, can be published for Scala 2.12 in a timely manner, then other libraries should be able to do that as well. That said, it's probably harder to upgrade the IDEs, I'm sure some serious work has gone in them. So maybe wait some more, because you're not missing that much, as most of the work has gone into the backend and Scala 2.11.8 is stable, it's pretty good (compared with 2.10) and there's also a 2.11.9 coming.
Sure yeah, my thinking for dependency injection was that Spring would use reflection as well and for me it really looks more nice. Yeah for Scala native its nogo but I'm sure Spring would be a nogo as well. (Macwire is on my todolist to check) 
I think that's the idea. The `val Regexx2(g1,g2) = stmt` syntax makes it looks like the regex is consuming the whole string, not just finding a match in it.
2.12 is in catch-up mode. It takes around one year after a stable release to get a stable ecosystem. Take a look at [Scaladex](https://index.scala-lang.org/search?q=) on the left bar (Scala Filters). 2.10 (2111 projects) 2.11 (2340 projects) 2.12 (844 projects) 2.12 has &lt;40% availability.
&gt; I drop them like they are hot potatoes and replace them with what? play, for instance, won't support 2.12 for a long time, should we all drop it? i don't think most of us with real jobs can afford such false elitism.
We are still using spray and it doesn't have a 2.12 version. Change to akka-http or http4s is in the backlog with hopefully having the time to actually do it at some point this year.
&gt; I felt they made the API drastically different and offered not enough guidance to make the move. Point I am making is that the API is radically different because akka-http does (almost) pure streaming, where as Spray doesn't. There is a reason behind it