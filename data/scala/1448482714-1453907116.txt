try scalafx - scala api for javafx? http://www.scalafx.org/ eclipse is not made with Swing, but SWT. Can't speak for Intellij, but there are various Look&amp;Feels for Swing available. And btw scala swing is now community maintained, I think: https://github.com/scala/scala-swing
Swing allows you to change the look-and-feel. IntelliJ IDEA is using its own, I don't know if you can extract it or not (since the community edition is open source). At the time they introduced 'Darcula', there was some talk that it's going to be eventually released as a standalone library, but that hasn't happened yet. The Nimbus look-and-feel is quite nice for consistent cross-platform look, although unfortunately it has some bugs that were never fixed. A while ago, I discovered a great look-and-feel called [WebLaF](https://github.com/mgarin/weblaf), they offer a GPL license or paid commercial license. In fact I like it so much, I released an artifact to Maven Central and use it in many of my projects. A major new version 1.29 is supposed to be released in January.
There's a discussion about what dependency injection means in the 2.4 Migration Guide: https://www.playframework.com/documentation/2.4.4/Migration24#Dependency-Injection The main page on dependency injection is here: https://www.playframework.com/documentation/2.4.4/ScalaDependencyInjection 
Our team used specs2 for several projects, before realizing that specs2 doesn't have as much test coverage as Scalatest does. Iirc, at the time we made the decision to switch for new projects, specs2 had so little coverage that we weren't sure if we could trust it. It's gotten better over time, but we've really enjoyed Scalatest and are sticking with it. It allows a little more choice and freedom of spec style.
I think I found my favourite internet troll :-D 
tpolecat is right to suggest [http4s](http://http4s.org), and too humble to say "and it's trivial to build an http4s microservice with [doobie](https://tpolecat.github.io/doobie-0.2.0/00-index.html)." But it is.
Awesome, thanks. I found SWT to be a good alternative to my problem as well.
You could clean up the read using getOrElse on Try. Try(json.convertTo[Boy]).getOrElse(json.convertTo[Girl]) You could also clean up write obj match { case b: Boy =&gt; b.toJson case g: Girl =&gt; g.toJson } 
I always appreciate making code more succinct. However, that's secondary to my main question - which is, what's the best/idiomatic way to code the de-serialization method, i.e. `read`, for `Parent`.
i am new to scala and this was helpful. can you tell what framework is best for security. i am looking for features like google / facebook signin and roles and permission. 
If you want a decent looking GUI, the only credible approach is SWT. Forget about Swing and JavaFX: they are still using peer widgets which just look terrible on all operating systems. 
When I have done this before I've added a 'type' field in the write method. The JsValue you create is actually a JsObject which you can add extra fields to. Then in your read method you can match on that field to determine which to deserialise. Something like: obj match { case boy: Boy =&gt; JsObject(boy.toJson.asJsObject.fields + ("type" -&gt; JsString("boy"))) case girl: Girl =&gt; JsObject(girl.toJson.asJsObject.fields + ("type" -&gt; JsString("girl"))) } then in your read: json.asJsObject.fields.get("type") match { case Some("boy") =&gt; json.convertTo[Boy] case Some("girl") =&gt; json.convertTo[Girl] } This would be better done by a macro, something that I always meant to do but haven't got around to.
&gt; With modules you very likely will end with a scenario where nearly everything depends on a subset of the collection library. And this subset is written in a way that needs only very few code. Hopefully, yes. And this is what I meant by *uncoupling* things in my first comment.
&gt;I only see one "operator", and even then you turn around and suggest using a different operator. Is this some sort of subtle attempt at trolling? Sorry, I should have said "a custom operator" or "an unconventional symbolic function". This is not an attempt at trolling. I greatly appreciate the response here. &gt;Because routing isn't decided by a path? How would you take into account he which user is performing the request? What about routing by contents of the request body instead? Using headers? I'll acknowledge I did not consider that. However, I have never needed to do this in an application, and I have not seen a web application or framework that uses some attribute other than paths for the top layer of routing. I feel like it would be nice to have some shortcuts for the most common use case. &gt;How would you reuse routing logic across multiple paths? `Map("/route1" -&gt; someFunc, "/route2" -&gt; someFunc)`, unless I misunderstand what you're suggesting. &gt;What would the type signature of that map even be? Map[String,Any]? How is that even useful. No, it would be a `Map[String, () =&gt; WSResponse]` or similar. Patterns like these are pretty customary in web frameworks from other languages I'm used to. In practice, probably a `Map[Regex, () =&gt; WSResponse]`. Depending on how the router maps the routes to their respective handlers, it could be `Map[Regex, WSPath =&gt; WSResponse]`, but I didn't want to deviate too much from the signature of the functions already used in the tutorial. &gt;No why would it be? It's a composition of two functions. I'm still having trouble dissecting it. When I read it, I see `get() { path() }` composed with `path() { get() }`. The order looks reversed to me. Could you break it down and explain how this works? &gt;Because outgoing messages aren't being sent to another actor. But if they were, what type safety do you actually get? It would prevent you from unintentionally sending an `IncomingMessage` to an actor expecting an outgoing message, or unintentionally broadcasting an `IncomingMessage` that's supposed to be a `SystemMessage`, but are these really errors that wouldn't be quickly caught through many other means? Is it idiomatic Scala to essentially wrap all strings with a class to ensure basic constraints are met? I'm not asking that in a hostile way, I'd just like to know how this could help me develop better and if this is an idiom I should be following. Again, I am not very used to static typing. &gt;You do understand it, but then you don't. Make up your mind. I suppose I *thought* I understood them, but clearly I don't. I just understand their general descriptions from the Akka documentation. I'm going to try to read more Akka tutorials.
Oh, now I get it what you meant. Thank's for your patience! For me this is again an example why written communication can waste a lot of time. I guess face to face (ideally with something to make a small sketch), it would only have taken half a minute until we had the same understanding. 
You can do it like this `val streets = (xml \ "Zone" \ "Street").filter(_ \@ "Name" == "S1")` (Also notice the use of `val`) If you're more comfortable using a for-comprehension it's just as simple but please don't iterate over the sequence using indexing, that's just poor practice. val streets = for { street &lt;- xml \ "Zone" \ "Street" if street \@ "Name" == "S1" } yield street edit: if you want all zone-nodes containing the specific street then it is `val zones = (xml \ "Zone").filter(_ \ "Street" \@ "Name" == "S1")`
The potential performance issues I know of are all compile time related. HLists are powered by the type system of scala and use implicits a ton to do the resolving, and if the HList grows too big, the type checker can take longer and longer to verify and compile your code.
Most of your questions are about the way aka-http decided to implement things. If you don't like that, maybe look at http4s (if you just want an http library) or Play framework (if you want a full framework). 
Also Finch.
see http://scalaxb.org/ for now
I have no idea. My uses cases are very sequential :)
Curious. Pitch Slick to me in 3 sentences.
Work with the data in your database just like you work with the data in your data structures (lists, etc.). Done. 
I guess for that just don't use it? An ORM is mostly for CRUD actions. 
[removed]
You can still map this to a slick `Query` type, and slick does give you a typesafe SQL string interpolator if you want to manually write your SQL as a last result
http://i.imgur.com/h6THPHv.png Yuck
As long as there's enough margin-bottom to read the last few lines of text, I wouldn't say that the CSS needs to be fixed. You might think it tacky, but it's not necessarily broken.
Even worse on an iPad: http://m.imgur.com/P7FW1o9
Reall? Look at the iPad version: http://m.imgur.com/P7FW1o9
Forwarded your feedback regarding the CSS to the folks taking care of the site. Thank you.
It's also wrong at tablet resolutions, the left side is covered. It's bad
DEAr Someone, when i had read Scala books, there was no mention of Maven, SBT or Activator, so no i am not interested, these are not part of Scala. No thanks.
Scala will be Freed when JetBrains will Die out.
Ah, that's broken.
Cool project. However, having glanced over the papers, especially [the one on QDSLs](http://homepages.inf.ed.ac.uk/wadler/papers/qdsl/qdsl.pdf), can someone enlighten me as to what the difference between macros or `reify` and QDSLs are? At first glance it looks like QDSLs present a new framework for EDSL development with some nice properties. I guess normalization is this property. But what is the exact purpose of the normalization step? Why is it necessary? I guess I need to look at it in more detail. In the end though the output is, like with macros, simple deep embedding, or Scala LMS some kind of AST that needs to be translated/compiled/transformed etc. Don't get me wrong. That is fine in and of itself, but why have all these guarantees of how normalization occurs etc. when the translation from the quoted representation can be introspected and produce incorrect results eventually? In a certain sense, I buy it with MetaML or MetaOCaml, where quotation and escaping and thus code composition at runtime is possible, but the composed code is not allowed (?) to be modified. This limits the ability to "reinterpret" quoted code at will for one's EDSL (e.g. make the "+" operator do special checks, or partially evaluate arguments etc.) but all the guarantees on correctness and soundness (types, name-binding, scope-extrusion) are be maintained. MetaML and MetaOCaml are conspicuously missing from the related work section. Also, in reference to (Sec 5): &gt; QDSL requires traversing the quoted term to ensure it only mentions permitted identifiers. In contrast, EDSL guarantees that if a term has the right type it will translate to the target. If the requirement to eyeball code to ensure only permitted identifiers are used is considered too onerous, it should be easy to build a preprocessor that checks this property. For example, in Haskell, it is possible to incorporate such a preprocessor using MetaHaskell (Mainland 2012). It seems, like macros, QDSLs don't really add that much EDSL development convenience. Again, I think there is some related work mention missing. I believe there is another paper presented last year about a macro-framework for Scala that used delimited blocks for DSLs and only allowed a selected set of operations within it that would be translated to LMS or something... I have to check my notes. I think they also did a form of normalization. EDIT: So, it seems QDSLs normalization is applied before the quoted code is inspected? Is this general, or does this have to be defined for every new EDSL? If the latter what is the point of QDSL approach? If the former, doesn't that mean that normalization may throw away or perform reductions that are valid in the host language, but not in the to-be-EDSL-reinterpreted version of the quoted code snippets? I must be missing something crucial here, which makes me feel stupid :(
I've been using http://kamon.io/ . It's amazing and has great Akka support. Kamon keeps track of everything, then you can use something like http://grafana.org/ as a dashboard frontend.
And 'request =&gt;' has been implicitly provided by Action
kamon.io really is a great tool for this. We have a production aka cluster being monitored via Kamon and it has been a joy to use. StatsD and Grafana isn't the greatest tools in my opinion but it gets the job done. You can also send the data through new relic and other vendors as well.
&gt;Compile times are a good bit longer than for Java Do you have rough numbers on the Java versus Scala duration for building? Do you split projects into separate builds? Or you have a monolith, i.e. multi-project build?
Both Kamon and Typesafe's product come with baked in knowledge of Akka actors, e.g. tracking mailbox sizes &amp; timings. Does Funnel have any support for this or would everything need to be hand rolled in an Akka application?
That's a good question. We distinguish between [monitoring and tracing](http://serverfault.com/questions/446956/whats-difference-between-monitoring-tracing-and-profiling). funnel is definitely a monitoring, rather than tracing, system. On the other hand, another system we've open-sourced, [remotely](http://oncue.github.io/remotely/), does provide tracing easily by attaching UUIDs to endpoints and making them available in a Context. I'll ask Tim if he can shake loose the [Quiver](http://oncue.github.io/quiver/) integration he did to construct and draw (as SVG) remotely call graphs. On the third hand, funnel can consume [StatsD](http://oncue.github.io/funnel/services/) metrics, so you might use Kamon for tracing Akka applications and funnel for other distributed monitoring tasks. We have excellent support for [Kibana](https://www.elastic.co/products/kibana) out of the box; it's trivially easy to instrument Scala code; and it scales extremely well if I do say so myself (it's kind of fun to watch Chaos Monkey take out a Flask or two, then see Chemist reassign suddenly-unmonitored systems to surviving Flasks).
Nice, would be awesome if we managed to get this integrated into stdlib
This is looking awesome, and it seems like a leaner/higher performance alternative to Slick, my immediate (without having a look into great detail) questions are How does Quill treat the separation of query's versus composed queries? In Slick, we have `Query` (which represents a valid SQL query, I think this would be analogous to your `query` quotation which gets turned internally into an AST), and then we have a `DBIO`. `DBIO` lets you compose multiple queries, but more importantly, you can embed custom business logic into a `DBIO`, so you can represent logic like resultFromQuery1 doSomeClientSideBusinessLogic resultFromQuery2 Atomically (that is, if `resultFromQuery2` fails, then it rolls back `resultFromQuery1`, and if `doSomeClientSideBusinessLogic` does something side effecting, you can revert it with a failover function) Support for custom table mappings. The automatic naming convention is fantastic, however in some rare cases (like working with another clients database), support for an arbitrary table mapping (similar to `Slick`'s current `table`) would be fantastic There is also the question of weird discrepancies in Scala logic vs Database logic (i.e. how do you treat stuff like &gt;= on a database `NON NULL` field). Maybe also stuff like runtime verification which checks that a `case class` properly maps to a database table without any such mismatches
Sure I can understand that argument, however it falls short for a lot of non trivial situations (actually even moreso for Quill rather than doobie) Having the ability to do atomic operations for a set of queries (rather than just one) is pretty much a requirement for any serious application. If a transaction fails for whatever reason, and its side effecting (i.e. retrieve a value, do an update, retrieve another value depending on update, do another update), you want to make sure it either fails completely, or it works. With slick this is very easy, for anything that satisfies a `DBIO` type, you can just append `.transactionally` on it. And since a `DBIO.sequence` also returns a `DBIO` (or any for comprehension over `DBIO`) you can create incredibly complex query combinations and you are sure that it will be atomic
I also think that combining this with Doobie offers an interesting separation of concerns; Doobie's ConnectionIO is quite happy to compose JDBC calls (which I hope quill could auto-generate, but experimentation is necessary). e.g. I've implemented a basic "upsert" for Postgres like this (I note the possible race condition, but will survive it for the use case): def upsertBlah(userId: UUID, blah: Blah, id: UUID = UUID.randomUUID, now: DateTime = DateTime.now): ConnectionIO[Int] = { for { save &lt;- HC.setSavepoint insertOrUpdate &lt;- sql.insertBlah(userId, blah, id, now).run.exceptSomeSqlState { case UNIQUE_VIOLATION =&gt; HC.rollback(save) &gt;&gt; sql.updateBlah(userId, blah, now).run } } yield insertOrUpdate } All the composed operations (including this one) are, by default, enclosed in a transaction, which is why the SAVEPOINT is necessary. Obviously then, ConnectionIO deals exclusively with a monadic environment for descriptions of JDBC calls.
The few questions I sampled weren't all that awesome. Asking candidates for the names of methods in specific classes seems a bit silly, unless you're looking to recruit parrots, or very language specific knowledge.
The only code snippet in this silly blog is JavaScript. In fact, "Scala" doesn't even exist on the page. Looking at your account history, it seems you spam this site pretty often. 
We're running them either in IntelliJ or through Maven. No sbt yet, but I guess we really should be evaluating SBT for our projects. We just didn't want to force a new build system on the team in addition to the new language.
&gt; However, having glanced over the papers, especially the one on QDSLs, can someone enlighten me as to what the difference between macros or reify and QDSLs are? I'm not aware of `reify`'s internals, but I guess you can see it as a sort of QDSL as it receives plain scala code and uses the AST to produce the macro's tree. &gt; At first glance it looks like QDSLs present a new framework for EDSL development with some nice properties. I guess normalization is this property. But what is the exact purpose of the normalization step? Why is it necessary? I guess I need to look at it in more detail QDSL is just a mechanism to obtain the AST from a quotation. Normalization is another technique and it could be used with ASTs produced using EDSLs as well. &gt; In the end though the output is, like with macros, simple deep embedding, or Scala LMS some kind of AST that needs to be translated/compiled/transformed etc. Don't get me wrong. That is fine in and of itself, but why have all these guarantees of how normalization occurs etc. when the translation from the quoted representation can be introspected and produce incorrect results eventually? Exactly, the QDSL doesn't introduce more guarantees to the normalization phase. &gt; QDSL requires traversing the quoted term to ensure it only mentions permitted identifiers. In contrast, EDSL guarantees that if a term has the right type it will translate to the target. If the requirement to eyeball code to ensure only permitted identifiers are used is considered too onerous, it should be easy to build a preprocessor that checks this property. For example, in Haskell, it is possible to incorporate such a preprocessor using MetaHaskell (Mainland 2012). Quill implements this additional step, it preprocesses the quotation to ensure the use only of supported identifiers at compile-time. &gt; It seems, like macros, QDSLs don't really add that much EDSL development convenience. QDSL is actually an alternative to EDSL.
Yes, Doobie provides a superior API for database interaction and Quill provides a superior API for query definition. A combination of both libraries seems promising. I've been talking to Doobie's developer and hope to have good news in the future.
&gt; For JDBC it uses a ThreadLocal and commits/rollbacks the transaction automatically. I forgot to document this feature and will update the readme soon. Nice thanks, what does this do for the postgresq/mysql async drivers? Or is it only for JDBC drivers. &gt; Quill's roadmap has an item to introduce a monad similar to Slick's DBIO. I've been talking to Doobie's developer and we're exploring the possibility to integrate both libraries, so I'd probably remove this item from the roadmap as Doobie provides a superior DBIO-like API. Unless something has changed, Doobie is tied heavily to JDBC. At least for me, one of the bonuses that Quill provides is support for Postgres/MySQL async driver. I think that rolling your own `DBIO` driver might be preferable for that reason. `DBIO` also supports streaming (something that Doobie doesn't), and at least in isolation, I think both DBIO and Doobie are equivalent in what they do (Doobie does do better runtime error handling, however `DBIO` also tracks effects such as reads/writes/transactionally/pinned session through the type system).
&gt; Nice thanks, what does this do for the postgresq/mysql async drivers? Or is it only for JDBC drivers. The async drivers use a custom execution context to propagate the transaction: https://github.com/getquill/quill/blob/master/quill-async/src/main/scala/io/getquill/source/async/TransactionalExecutionContext.scala https://github.com/getquill/quill/blob/master/quill-async/src/test/scala/io/getquill/source/async/mysql/PeopleMysqlAsyncSpec.scala#L16 &gt; Unless something has changed, Doobie is tied heavily to JDBC. At least for me, one of the bonuses that Quill provides is support for Postgres/MySQL async driver. I think that rolling your own DBIO driver might be preferable for that reason. Good point, but I'd say the priority would be lower if the Doobie integration works out. Would you be interested in working on it? I'm looking for contributors. :)
React to what or reacts to what ?
And how do you expect newbies to know that, without letting them first know that this is for solving some problem, when we cant even see where these Jars are ? OR you expect that every Man out there was a jack who went on to do SHITE called Java language before Scala ?
I am going to be using `DBIO` as an example here, since its what I am familiar with val queries = Seq( q1: DBIO[User,NoStream,Effect.Read], q2: DBIO[Post,NoStream,Effect.Read], q3: DBIO[Address,NoStream,Effect.Read] ) DBIO.sequence(queries) Without fusion, this may just do SELECT * from "users"; andThen SELECT * from "posts"; andThen SELECT * from "addresss"; All with seperate connections, and one after the other. Obviously this is slow, since you are using 3 seperate connections, and doing it sequentially With fusion, you can do this in parallel at once, either with separate "sessions", which might equate to (using Futures as a quick example) for { users &lt;- SELECT * from "users"; posts &lt;- SELECT * from "posts"; addresss &lt;- SELECT * from "addresss"; } yield (users,posts,authors) In Slick, you can force this to use a single session with `.withPinnedSession`. To do this properly, you need to track effects through `DBIO` (and what type of effects they are, i.e. read/write/transactional), because you still need to make sure your action is atomic (i.e. don't want to run queries in parallel when one of them needs a value from the other queries which makes a write). i.e. with val queries = Seq( q1: DBIO[User,NoStream,Effect.Read], q2: DBIO[Post,NoStream,Effect.Read], q3: DBIO[List[UpdateAddressResult],NoStream,Effect.Read with Effect.Write] ) You may do q1 and q2 in parallel, but if q3 depends on either q1/q2, it would have to be done after (i.e. analogous to `andThen`) In Slick, you can use `andThen` explicitly, but it will also handle the effects above. Theoretically speaking you could even do further optimizations to the AST (i.e. if you are dealing with a List of AST's that are just `Effect.Read`, you can fuse the AST's together into a single query, if possible, although this would be doing some heavy optimizations. A very simple optimization is if you see duplicate queries that are exactly the same, you can safely remove the duplicates)
Seems pretty cool @ochrons - will definitely check it out :)
It totally makes sense to me now, I though you were referring to the query's AST! I'm planning to explore an integration with http://getclump.io, it already has a very good mechanism to optimize the execution of multiple remote calls.
?
A very interesting framework for sure. What are your plans for the future? Do you want to grow it into a more Elm-like shape with signals, `foldp` and mailboxes?
&gt; The first obvious step to fusion is what I mentioned before (in regards to parallelization and whatnot) which is what I think your clump does. Yeah, I'm not sure that the benefit of fusing different queries into a single one would worth the complexity. Also, it'd be harder to reason about the code behavior. &gt; Clump integration (from what I see) sounds like a good idea, however something specific for Quill will provide better performance improvements, and abstraction as well. Clump sounds like it would work much better with something like Future[T] for abstract services (either by a HTTP call, or something like SOAP, or some other RPC system). The integration with Clump would be specialized to Quill's domain. &gt; Clump may now when to parallelize queries, but if you do something like "make one query for every user in our system", that is going to make a huge number of connections. Actually, batching is Clump's main feature. It's possible to request multiple keys from a source (a query in Quill's case) and it batches them in a single remote call. Thanks for the considerations! They are very helpful!
&gt; Yeah, I'm not sure that the benefit of fusing different queries into a single one would worth the complexity. Also, it'd be harder to reason about the code behavior. Yup agreed, just putting it out there as a possibility. It would definitely be an option that you would toggle &gt; The integration with Clump would be specialized to Quill's domain. &gt; Actually, batching is Clump's main feature. It's possible to request multiple keys from a source (a query in Quill's case) and it batches them in a single remote call. Cool, nice to see that Clump can support this
Well sorry for being a professional who actually reads manuals/tutorials/documentation and expects the others to do the same. They write those things for a reason.
Have you started your project? If so do you have a link to your git repo? It will help people look at it and let you know if / how they can help
Thank you for ur advice. I have some stuff i already started but i dont have it in public, cos i frozed it some time after start due to personal workload appeared. Honestly i'm looking for somebody interested to make a project in full and discover some best practices and patterns together while looking for right implementation. 
It's probably going to be hard to find someone interested without at least describing what you are making. CRUD by itself is not very interesting so it wouldn't attract someone more experienced developers (which it sounds like is your goal).
Cool! Please open issues if you run into trouble or have suggestions.
love the name
Ah ok. I guess my point is that a lot of the libraries that can be used to build SPAs also are very useful for those features as well if used in moderation.
I don't blame them, I would delete your post here too if I could. 
Better than intellij?
Asking the real question we are all wondering.
There are still people using Scala with Eclipse, so any improvements are most welcome. There seems to be a big shift towards IntelliJ for both Scala and Java developers in general though.
I feel Scala IDE has better macro support but Idea wins in overall ease of use due to better refactorings, code navigation and support for .sbt-files. It's just more productive for me. The Scala IDE plugin also requires Eclipse to have a much bigger heap space so you need to fiddle around in the global config which doesn't feel as practical. It *does* have better performance when compiling so it's a bit of a trade-off. I still prefer Idea's approach, especially if you're using a terminal and SBT to compile/test anyway, which makes them equally performant. Idea just feels more light-weight.
I think the issue with Slick, is that its development happened ages ago (starting from ScalaQuery days), which was way before Scala had any sought of macro engine (even though the current macro support is classed as experimental, back then it was non existent). I am pretty sure, if slick was made from Scratch today, it would probably use a QDSL versus an EDSL. In any case, Quill does look compelling. The main thing that is missing from my pov (which I have commented on before) is batching/fusion support (with fine grained control). Also I am pretty sure there are some types of SQL queries which Slick does support that Quill doesn't (although these shouldn't be too hard to add) Also I do believe work is being done to decouple Slick from JDBC. The first step that was needed was to actually add proper abstractions to allow this to happen (which is what Slicks `Query`/`DBIO`/`Future` distinction). After that is in place, all that is needed is to modularize JDBC out Another possible killer for me would be IDE support. Does Quill use whitebox macros? If so, the main IDE that I use (IntelliJ) would probably be unusable with Quill. I suppose this is one of the downsides of using macros so heavily
Play for scala - Peter hilton If you want to use play framework http://docs.scala-lang.org/tutorials/ &lt;- link unrelated to book above 
This uses sbt from command line or eclipse/intelliJ Not typesafe-activator (activator-ui) Or am I wrong?
Activator is more or less just a UI for sbt.
Given that Slick itself began circa 2012 and has had/continues to have production issues, there's a lot of grey area in "credible production story" ;-) Probably, "does it work for most use cases in production and are there workarounds for known issues?" is a better indicator of stability. With Slick, the answer is a qualified yes. With Quill, TBD, but given the author's background I would be surprised if it turns out to be a no.
I second this. Great course to get you started. [This book](http://www.amazon.com/Programming-Scala-Comprehensive-Step-Step/dp/0981531644) is also amazing (main author is the father of Scala himself, the same man that did the coursea course - M. Odersky).
Scala Worksheets are invaluable.
I haven't given the spa tutorial a go yet, will do that soon. But if you click the link in my post to the scala.js mailing list, you'll see some of the problems I'm experiencing.
Answering own question. &gt; Not sure how Quill handles custom functions a la Slick's SimpleFunction support. Also, string interpolated DSL, does Quill have that? both via [`infix`](https://github.com/getquill/quill#infix). Hope I'm wrong here but persistence looks to be [very painful](https://github.com/getquill/quill#actions) in Quill. Uggghh, needing to manually specify each property of a case class, no. One of the killer features of the Play/Slick combo is being able to bind/validate a form to a case class instance and then persisting that via Slick a la `Person.insert(p)`. Great gobs of boilerplate avoided compared to manual JDBC style where one has to manually specify all of the params that make up an insert/update/delete statement. Will try it out, maybe Quill allows one to pass in a case class instead of tuple.
(For /u/Aposperite's benefit) Scala worksheets are a feature of both the Eclipse and IntelliJ Scala plugins. Worksheets are files that the IDE treats specially; they display in two columns. You can edit in the left column, and as you edit, the IDE will run your code and display output in the right column (vertically aligned to the source line in the left column). In IntelliJ (and presumably in Eclipse as well), they are run within the context of the rest of your project, so you can access other classes and functions that you have written. They're a good way to play with Scala interactively.
First Find out if Scala is of any use for your needs, if you want to teach programming , Scala is your best bet, beyond that the only usable thing made in Scala is ScalaJS. Basically anything in scala that needs SBT is useless.
Serious question, people actually use activator? It just seems like 4 steps back from SBT + IntelliJ... what am I missing?
Someone is feeling cranky again. 
I have already enrolled on the that class although its long over and I am trying the first set of exercises right now. Thanks.
Regarding those books, I found that *Scala Cookbook* uses alot of concepts before explaining them, which is extremely frustrating for a beginner. But that's what you get from a cookbook. While checking out the Amazon reviews for *Programming Scala*, I noticed it too seems to have the same issues. So I wouldn't trust this dude at all, but that's just because I am allergic to what I just mentioned. One of those books is good though, and that's *Functional Programming in Scala*, I highly recommend it, but expect to spend some time on exercises.
We have some books here: http://underscore.io/books/ Creative Scala is very basic, quite fun (graphics!), and free. If you want to quickly get some tangible output before getting into the theory it's a good choice. Essential Scala is our book for teaching professional programmers to be productive in Scala. It takes a more rigorous and structured approach, but is a bit drier.
Can anyone tell me why I'd use Slick over just plain old SQL and mapping to case classes? I used Slick for a project, and ended up hating it. It's so complex. I've been using [Doobie](https://github.com/tpolecat/doobie) on my new projects, and love it. Forming queries using plain old SQL and then lifting them into a ConnectionIO monad feels a lot cleaner. Thoughts?
Slick pre 3 didn't have batching/fusion support, and its one of the things that annoyed me a lot about it EDIT: Sorry, didn't really your comment fully The main benefit of `DBIO` is that it gave very explicit control over atomicity in a non blocking fashion. JDBC being non blocking is inherently a performance problem, but the ability to control atomicity in a non blocking fashion is what `DBIO` allows (regardless of if its running JDBC on the backend, or something else)
&gt; Can you say more about this? I'm interested. The query engine that Slick uses has had quite a few regressions, even for simple queries. Then there are cases like generating really slow queries for DB engines like MySQL (this is detailed in the article though) There are other issues that are more fundamental to Scala, i.e. the tuple-22 limit causing issues in mapping large tables
&gt; Interesting that == works in Quill, Slick has always (since ScalaQuery days) needed === and =!= for equality. This is the difference between using a QDSL vs an EDSL. QDSL is basically a macro which rewrites valid Scala code (for some definition of valid) into an AST. An EDSL is just a "library" that allows you to represent your domain as valid Scala code. Since `==` and `!=` only work for actually equal types as defined by `hashCode`, an EDSL can't really overwrite this, hence why you have the `===` and `=!=` operators.
`DBIO` is actually a very good abstraction, and it was needed. `Query` \ `DBIO` \ `Future` represent the 3 abstractions the way you want them to abstracted when dealing with databases EDIT: Fixed slight problem in mixing Query/DBIO * `Query` = A valid SQL statement. Also gives control over the type of collection returned (`.to`) * `DBIO` = A collection of one, or more valid `Query`'s that track the effects (read/write) as well as the atomicity/session control (`transactional`/`withPinnedSession`) and allows you to atomically mix client side logic with database logic * `Future` = The non blocking result of executing a `DBIO` It looks like Quill will introduce an analogous concept to `DBIO`, because once you start doing non trivial database statement, you need control over how your queries are combined/executed, as well as fine grained atomicity While it is somewhat true that using non blocking `DBIO` over JDBC is pointless, Slick is trying to move away from JDBC, however to do that you first need the proper abstractions in place.
I suppose in the strictest sense, you are throwing away a little compile time safety. But Doobie provides a 'check' method you can call on queries, which will return a failure if your SQL is non valid. I just enjoy working with Doobie, where I write my queries using SQL, and run them, getting a failure or a success. The mapping code is trivial in comparison to Slick. I don't need to declare relationships in code. I declare them in my DB schema, and perform my queries. It either works or it doesn't. Just different I guess.
&gt; I suppose in the strictest sense, you are throwing away a little compile time safety. But Doobie provides a 'check' method you can call on queries, which will return a failure if your SQL is non valid. Yes, but what if you mis-spell a table name, or what if you do a `&gt;` comparison on a field that is `NULL` (when expecting it to be a `NOT NULL`) which can differ between database implementations/what operator you use . What about using the wrong ID's when you are doing a join (i.e. instead of using `"user_id"` in a join, you just use `"id"`). You can fix this in slick by create value classes of ID's (i.e. you have a `case class UserId (val id:Long) extends AnyVal`, and now all of your joins on that ID are typed) The SQL being valid is barely even 20% of the problems that happen in production, the rest of the 80% Doobie can't detect, and all of that gets pushed into runtime. Like I said, if your SQL queries are incredibly trivial, then you can get away with it, but our SQL queries are now approaching around 150 lines (if they were hand written). Even the premise of what happens when you change a `case class`. In Slick, this is a compile time error, in Doobie its a runtime error. Yes Doobie may reflect accurately these runtime errors through the types, but Slick puts it at compile time. Note I am reiterating that this isn't me dissing Doobie, I spoke with the author and he said that these kind of problems are outside of Doobie's design scope. But lets call a spade a spade, Doobie can't verify, at compile time, a whole class of errors that can be detected with Slick
&gt; Hope I'm wrong here but persistence looks to be very painful in Quill. Uggghh, needing to manually specify each property of a case class, no. Yeah, Quill doesn't allow insertion of case classes directly. It's on the backlog and should be available soon. Meanwhile, you could transform case classes to tuples using a function like this one: case class Task(emp: String, tsk: String) def toTuple[I, O](f: I =&gt; Option[O]) = (v: I) =&gt; f(v).get val tuple: (String, String) = toTuple(Task.unapply)(Task("a", "b"))
I work full-time in Scala, but I learned the principles best from Learn You a Haskell for Great Good. Then, FPiS, I would recommend. 
Why do you think that getting rid of a common library is desirable or necessary?
I've tried it and it gives me the same error as with every project: "(run-main-1) java.lang.ClassNotFoundException: spatutorial.client.SPAMain"
Lol. It does work in activator. But idea is still way nicer.
The image on this page https://github.com/getquill/quill/blob/master/README.md shows ScalaIDE being able to show the expanded quotation. (The SQL query is shown by mouse-hovering a method.)
IntelliJ is indeed a great ide, but my employer wants to have typesafe activator to make sure the code is typesafe. I'm just gonna practise scala.js in IntelliJ untill I find a fix for my bs errors. the stupid thing is that both me and my employer who uses osx (im using windows) get the same errors.
There is a video on youtube about Microservices using akka http, he is saying exact things you are looking for in it.
I would recommend Twitter's Scala courses (which they use to onboard developers internally): https://twitter.github.io/scala_school/ http://twitter.github.io/effectivescala/ 
Take a look at https://www.manning.com/books/functional-and-reactive-domain-modeling
Isn't it kind of a problem if a tech platform (scala)'s main recommendation on framework (play framework) comes with a recommendation of a DB layer (slick) - where learning that data layer is a losing battle? Looking for other perspectives here, but again....WTF.
I know, i started many projects with it and really wasted a lot of time typing all those table definitions. I think for them it's a sunk cost issue honestly. Play is also ~~unnecessarily complex~~ too complex to benefit most projects in my opinion, especially considering the trend towards client rendering with libs such as react, that complexity deserves to be handled by the users browser now. We just started a new project and chose Scalatra.
Slick is not that bad if you are good with google, but also not super obvious (if that makes sense). I could see learning play AND slick at once is hard. Maybe anorm + play would be an easier 1.0, then you could try out slick at a later date. Slick IS cool if you learn it, even though it will make you made sometimes... but you hardly NEED slick to have a good time on scala or play.
Can you talk more about why Scalatra? How does it fix all those table definitions problems for you?
Scalatra instead of play. It's good for us because we just want to send a super short index.html and then handle ajax requests for data. Scalatra is super simple for that so far. Doobie doesn't require table definitions, you just make a case class for your query and everything expected to come out of the DB typechecks according to the case class members. So I guess I'd say your case classes correspond to the queries you intend to write, rather than the entire schema. 
Yup. A lot of those frameworks you speak of are not type safe, and do runtime trickery to work. The promise of Slick is that you know at COMPILE time if a query is typesafe or not. To get that, you currently have to pay a bit of a price in higher complexity.
Why can't slick auto-gen the table mappings with reasonable defaults so I don't have to write the boilerplate? It would still be possible to keep the type safe property in that case.
You mean like http://slick.typesafe.com/doc/3.1.0/code-generation.html
You mean this? http://slick.typesafe.com/doc/3.0.0/code-generation.html
I haven't used Slick since v1, but IIRC Slick uses case classes to represent rows, and objects to represent tables. Then it does some magic to treat table objects as effectively lazy collections that you can filter/take/groupBy over. Slick is definitely the most 'enterprisey' of the choices. Like all enterprise tools, it's very stable but has a lot of customization and documentation (which is exactly what you want when you're trying to build robust software). If you're only serving simple requests, then Scalatra is definitely a better fit. This is not unlike ruby, where you have a choice between Rails and Sinatra (hence, 'Scala'-tra) as web frameworks. If you're just working on a side project, or just looking to dip your toes in the scala ecosystem, you might be better off with something more familiar like https://github.com/aselab/scala-activerecord. At the risk of sounding cliche, pick the tool that fits the job. 99% of what you read on the internet says 'XYZ is complicated, I hate it and don't use it. I use ABC instead, it just works'. Then again, 99% of projects don't have a huge codebase, don't need to be serving thousands of requests per second, or don't get maintained after a few months.
So, I use Slick / Play and I quite like it. The single largest problem though is documentation and making it newcomer friendly. Consider that both play 2.4 and slick 3.0+ have significant changes to their internal structures, coming into it right now is extraordinarily hard because pretty much all blog / tutorial /git repos are most likely obsolete. But if you can fight through that, they actually are a pretty fantastic way to do typesafe development. The trick for me that really makes Slick shine is the use of its "codegen" capability. Codgen is a mechanism in which slick reads through your DB schema and automatically generates all the boilerplate Tables / TableRow types that are otherwise quite a nuisance to manually type and keep up to date. In my setup, I can fully utilize play-evolutions and just write evolution scripts to keep my database up to date, then run a simple command (gen-table) and all my now updated tables are automatically lifted back into scala land (with compile time failures I can go readily fix). I think that capability is fantastically awesome and was worth struggling to figure out how to make it all work. The real trick with codegen is figuring out the CustomizedCodeGenerator logic and being able to teach slick to generate proper types in the various Row definitions it creates. This lets me store things like UserId and AccountId, or Email or Phone or whatever proper type (as opposed to Longs and Strings) in my tables directly, so I can pretend my DB stores those types (in my evoulution scripts, I still deal with text and big integer, but the customized code generator ends up mapping them to the right scala type). And with that major pain point solved, I now can use all of slicks query capability to write typesafe queries, and have very high confidence that everything I expect to get out of the database I will get, in just the way I expect it. 
Yeah, to me mapping case classes to queries is simpler, because you don't always want all your columns whether you're inserting or not. case class SimplePerson(name: String, age:Int) maps to both of these: sql"select name, age from person where name=$name and age=$age" sql"insert into person (name, age) values ($name, $age)" just fine. Even though your schema may have another hundred columns in it.
Building off of this, I've been looking at codegen in slick - I get why it's cool for precisely the reasons you're talking about. One concept I'm still missing here though is that while codegen will give you scala from tables, you still had to hand-write that table SQL. I'm seeking something more like spring-data, or python's django ORM, where you basically just specify classes with special annotations, and all of the SQL is done for you. You can still have migrations under those setups. But you don't generate table mapping code from SQL because there is no table mapping code, it's all in the framework. The code gen route seems appropriate to me only if you already have a database you're trying to read from. If your'e starting greenfield, it doesn't seem like the code gen is useful. If I were to write a SQL schema for my intended DB and then use code gen, that's really the same thing as writing my own table mappings, just in reverse, right? I just don't understand why the table mappings are even necessary in the first place, with so many other frameworks out there that basically make them unnecessary. 
Generating efficient MySQL queries is the main point of the new backend in 3.1. Yes, a few bugs have been discovered, and they will get fixed. We got it to a good level of stability during the RC phase but like in every .0 release there are always some corner cases that only show up once the masses start using it when a final release is out. There are no limitations due to Tuple22 in Slick and it has been like that since 2.1. You can use Slick HLists or similar implementations (e.g. from shapeless) instead of tuples. The lack of Scala's standard `tupled` and `unapply` methods for these means that you need to spell out the mapping functions even in simple cases. This will be addressed in 3.2 with [#1306](https://github.com/slick/slick/pull/1306).
We developed both the "Lifted Embedding" (Slick's current Scala API which is based on ScalaQuery) and the "Direct Embedding", a macro-based API, in the early days of Slick. We decided to go with the Lifted Embedding eventually. While simple cases are easier to do without an intermediate mapping between plain Scala objects and table row objects, this mapping also adds considerable flexibility. And a macro-based embedding comes with its own set of problems, like extensibility and composition across compilation units (everything looks like a plain Scala function but you can't actually call one from another compilation unit because there's no AST for it). Of course, a direct embedding like in Quill could be a viable alternative with its own sets of advantages and disadvantages, but it's not the case that we we didn't consider it (we did) or that the macro-related technology wasn't there at the time (it was actually developed alongside Slick at EPFL with Slick being a major driving force for Scala macros in the early days). Regarding JDBC, the Quill comparison page is misleading. Slick's architecture has been designed to be independent of JDBC, and with the new DBIO feature in 3.0, also independent of blocking execution. Slick does in fact come with `MemoryDriver`, which is a simple heap-based database engine that uses Slick's query interpreter. It is based on `RelationalProfile` but not `JdbcProfile`. A lot of the unit tests target this profile (unless they test JDBC- or SQL-specific features). All the drivers that you would use in practice, however, are JDBC-based.
The scala ecosystem gives us other things we want (like reactive programming and akka) so that's why we'd not want to go with rails. I don't understand why these things are incompatible though. Why must I choose between writing a lot of unnecessary boilerplate (slick) and having the nice reactive programming environment? What is it about what rails ORM does that's so fundamentally incompatible with scala? Again, if you want strong typing guarantees, I accept that's a good thing. Why then isn't the framework doing the work that when I have a String property, it ought to be column[String] in the DB?
Its Slick that isnt an ORM. Scala is perfectly happy with ORMs. And I am pretty sure there are Scala ORMs out there, but I think they have fallen out of favor a bit in the larger scala community (wild speculation, I could be wrong). I think the DB-centric style is just more prevalent way of thinking in the scala community, be it through Slick or Anorm (as the two most common DB frameworks used in Play) and neither of those are ORMs. They basically want to interact with the DB, using SQL (mostly) directly, because it is arguably better then Rails style ActiveRecord. So its not that it isn't possible, its just that we* think we have found a better way to go about doing it. I do think squeryl is more ORM-y, but i haven't used it. *we really just being me and some wild speculation
Yes this. You have to pay a tax somewhere. I would rather write my table structure in sql than to slap a case class and gen sql. Why? Well is a string a char or a varchar or a text data type? Varchar 12 or 20? You can't just take a case class and poof make a table. 
Love it
My bad, was writing this in a rush, fixing it now
Yeah I was mainly talking about way back in Scala query days, I don' think macros existed back then (either that, or they were pretty much pre alpha status) I was aware of direct embedding, and I remember it being an experimental feature (one which I personally never used) Nice read on how stuff historically happened
Eclipse has rudimentary support with whitebox macros, where as IntelliJ in general doesn't work well with them at all (note that this is documented, whitebox macros are documented as being much harder for IDE's to deal with)
&gt; There are no limitations due to Tuple22 in Slick and it has been like that since 2.1. You can use Slick HLists or similar implementations (e.g. from shapeless) instead of tuples. The lack of Scala's standard tupled and unapply methods for these means that you need to spell out the mapping functions even in simple cases. This will be addressed in 3.2 with #1306. Yeah by "issues", I didn't mean it was impossible, just that it wasn't ideal. We have to write a huge amount of boilerplate to the mapping for case classes with arity of more than 22 (im glad this is being addressed). The other issue is that when we started using Slick's HList, it did have a noticeable effect on compilation times, our compileIncrement takes around 30 seconds now
&gt; learning slick is really a losing battle - you can spend a lot of time writing a lot of boilerplate and really think you've accomplished something, but what you really want to do is write queries quickly and get data out. This isn't really a fair comparison. The reason you write your so called "boilterplate" (which isn't really boilerplate, but will get to that later), is so you can verify your queries are correct. With doobie, this sought of stuff gets pushed at runtime, not compile time. You may as well argue that using "types" is boilerplate. Yes it is "boilerplate", but it means that you don't have to write so many tests, and it also means that you can reason with your code a lot better.
ORM's can solve like 50-60% of your problems with 20-30% of the effort, but then are completely bullocks in that remaining 50-40%. The minute you start doing more complex joins/aggregates, the ORM model falls flat on its face. Usually its possible to do these things, but you end up getting pathetic performance because the ORM abstracts all of these things away (the famous n+1 problem which Slick docs talk about is one of the problems this causes) FRM (like Slick) solve like 40% of the problems with 20-30% of the effort, however they solve the rest of the 60% of the problems with like 40% of the effort. Doing complex joins (and mixing this with client side querying/business logic) in Slick is quite easy (and its also statically typed), in traditional ORMS this is painful
&gt; We discovered some performance issues in the Scala compiler around the deeply nested HList types, which have been fixed in 2.11.x bugfix releases. As in most such cases, explicit type annotations make things much faster, but of course that means more boilerplate. Thanks for the info I think manually annotating the type of a HList is going to get crazy pretty fast in terms of boilerplate, but hopefully this will be fixed when Scala adds proper support for unlimited arity records/tuples
It's 2015, so everyone knows about N+1 and all ORMs have some built-in solution for it (and obviously you can manage relations by yourself like you do in Slick). Problem with Slick is that it doesn't help with things that are relatively easy and well understood - basic CRUD and it can't help with things that are relatively complex, that is designing schema and optimizing performance because nothing can. 
It's pretty much mandatory to fiddle with idea.vmoptions so that it doesn't choke and die when you're working on really big Scala projects. That said, yeah, words cannot express how much I prefer IntelliJ to Eclipse.
I've also seen a lot of bad highlighting in IntelliJ, but in practice I've found it to be waaay better than Eclipse despite that.
writing an sql model isn't unnecessary in the real world, maybe it's in the small project world, but if you want to get it right a hand written UML diagram is necessary (Code-First approach is a no-no since it's too confusing for big models) For example this is my workflow: 1. Build SQL Schema with PGModeler 2. Directly Export to Database 3. Use codegen (repeat and drop schema while in development) when going live: 1. Export SQL Schema as first Migration when patching: 1. Change schema in PGModeler 2. Use "diff" feature to create migration (hand check it if it's right) 3. Use it locally, then run the codegen 4. patch code 5. deploy
Still. *activator* has *nothing* to do with type safety. Activator is a tool to quickly bootstrap a project. It becomes useless past the initial setup and exploration (just overhead). Scala and Scala.js give you type safety. It could even be argued that *sbt* gives you type safety. activator does not.
OK, I accept this argument. Still the functional mapping seems to have these big gaps in it, like namely why do I have to write all of these table mappings when their generation is obvious? (e.g. a String field is a column[String] in the DB) I'm not trying to argue that ORM should be the way, but how the FRM is implemented is pretty puzzling.
Painful to hear that, get over the hump (learning curve) and you'll have a completely transformed view. It's really tough to do everything at once. Back in the day the stairway book (Programming in Scala 2nd edition) really helped me to get my feet wet in the language; *and then* I dove into the available frameworks/libraries. Anyway, frustration is guaranteed due to having to learn so many (likely new) concepts in order to grok how and why Play, Slick, etc. are designed the way they are. As for documentation, I actually really like Play's, provides the basic building blocks. Slick docs? Yeah, could definitely be improved.
&gt; It provides all the type safety of Slick without the boilerplate thanks to macros Devils advocate: first of all, the mapping boilerplate can (and should be) generated by the code gen tool. Secondly, said boilerplate is actually *useful*, it contains entire meta data of underlying database model, which can be used to, for example, encode foreign key relationships. Agree in principle though, Quill looks *very compelling* out of the box. Hopefully issues like -- not supporting case classes for insert/update (i.e. you have to specify *each element* of statement manually, heh, talk about boilerplate), and perhaps implicitly creating query instances rather than having to wrap `query[T]` everywhere -- will be implemented/solved. 
This is helpful. I'll look into activerecord.
The Double percent sign is used in lieu of appending a scala version, the Triple percent sign is used in lieu of appending a scala.js version.
Yep. That's it.
nice, thanks for sharing! 
Nope, you're right, there's no reason. Slick sucks donkey asses. I'd rather use raw JDBC than deal with Slick's "there's 3 different ways to do it and they're all broken in some obscure fashion" approach to things. 
Not quite -- codegen by default will generate very simple types. So it will "push foward" types like text (becomes String) and big interger (becomes Long) But.. those are not very safe types. Strong types like UserId and Email make much better use of the type system then Long and String. Often those are simple case classes that extends AnyVal (tagging is an alternative, which is another way to enhance primitive types with more safety) So, out of the box codegen will generate all the Table / TableRow types, which in of itself is very useful -- it will properly generate all the right types to make left/right joins work (ie the def ?) as well as pick up all the foreign key constrains. That already is very useful and saves a bunch of time -- basically, you use evolutions to directly manipulate the database using everything your DB can do, then gen tables will lift all those changes and your queries will update and/or fail at compile time, with very little time actually thinking about what codegen actually generated. The CustomizedCodeGenerator gives us the opprotunity to hook codegen and generate tables / rows that have "rich" types like Userid and Email as mentioned above. Although the hooking API is very general and there are a bunch of ways that these type mappings could be done, one of the easiest I have found is just making use of the column name to "encode" type information into the db. For example, I will have column names in Postgres like user_email and account_email, then I do a simple .endsWith("email") check in the code generator code and instead of generating a String as a type, it generates "shared.validated.Email" -- so my scala code ends up seeing Email as the type of the column whenever the column name ends in "Email". It does mean I have to occasionally update my customized code generator when I want to store a new type, but it often vastly simplifies the client code (with much greater safety pretty much through the entire system). EDIT* There is a caveat about codegen / play evolutions that I find annoying. This problem is squarely Playframeworks fault -- basically, evolution scripts will *only* run if your code compiles. This is problematic if you are trying to refactor the generated code. Sometimes I know the code that "gen-tables" will generate will typecheck because I just refactored my evolution scripts, but I can't actually *run* the evolution script. Play could fix this by giving a way to manually force an evolution to run from the command line. My current workaround is basically an empty play project (which I happened to name 'evolve') that is a sibling to my main project, and basically only contains a hard link to the conf/default directory in the main project. This allows me to type "evolve/run" from the sbt prompt, which then starts the play server (and wont fail to compile, because it doesn't actually have any code), but will still properly run my evolutions. EDIT 2* Also, codegen doesn't do anything at run time. Codegen is kind of like a macro, but really the API it exposes predates proper macros. Codegen literally dumps a file into your project when you run the command "gen-tables" from sbt. So, while it is possible to run gen-tables every time compile is run, I don't actually do that, and only run it manually, usually after I update my evolution scripts. The goal is to make everything a compile time failure, and codegen ensures the scala compiler has a very good idea of the current state of the schema. 
How would you define a definition of M for List? You can't without the higher kinded type. This is how you do it properly implicit object listMonad extends M[List] { def unit[T](x: T): List[T] = List(x) def flatMap[T, U](ft: List[T])(f: T =&gt; List[U]): List[U] = ft.flatMap(f) }
Ah no, sorry, perhaps I should rewrite that part tomorrow. I meant that the analogy to M[T] was List[Int] (I wrote that later on in the article), not M[List[T]]. I didn't mean to parameterize M with monad type, but that the monad itself is M. So my ListMonad[T] would extend M[T] (not M[List]). Thanks, I'll try to clarify that part.
&gt; Yeah, really. Slick neither saves nor returns object graphs Sure, but how is this basic CRUD, which you originally said. I don't disagree that Slick doesn't return object graphs (nor should it, but nothing is stopping a library ontop of slick for doing so) If you are talking about updating Object graphs, well then you are basically going to be doing collection operations on your `Seq[Group]` in a similar way to how you would do it in Slick (it would just be structured differently). Difference is that Slick won't have terrible performance &gt;As I said, it's something that no ORM covers and designing a proper schema (deciding what is normalized and what is not, what indexes to use ) is a hard part. Slick does allow you to define index's, and you can even use them in queries, see http://slick.typesafe.com/doc/3.1.0/schemas.html#index-13. &gt; . I don't see much value in type safe query generators because first of all, they don't exactly mirror the capabilities of a specific database so something that compiles can still produce runtime errors I have had one runtime error when using Slick, which was on a unique column (one of the few things that should generate a runtime error, inserting an entry that violates unique), so at least for our app, this definitely is not the case. &gt; when designing complex the usual workflow is to produce SQL first, because it's easier. Yup, and with Slick, you generate your SQL through Query like collections, which basically (almost) look like the real SQL, except they are typed. I only go back to manually generating SQL if I really need the performance somewhere, and subsequent releases of slick have made that occurrence happen less often
&gt; Of those CRUD examples you gave, why oh why are none of those in the actual slick documentation or activator templates? Like I said, slick has terrible documentation. It should definitely document them &gt; Where did you get them? This is pretty much how our own internal slick code works. I mean here is the definition (should compile, tell me if it doesn't). I found most of this stuff in a Slick presentation, not officially documentation. // Note that this can be auto generated by Slick's codegen. Alternately you can write this // code, which is your table definition, and get the generated SQL via .schema method on a TableQuery, // in this case Users case class User(id: Option[Long], firstName: String, lastName: String) class UserTable(tag) extends Table[Table](tag,"users") { def id = Option[Long]("id", O.PrimaryKey, O.AutoInc) def firstName = column[String]("firstName",O.DbType("varchar")) def lastName = column[String]("firstName",O.DbType("varchar")) def * = ( id.?, firstName, lastName) &lt;&gt; ((User.apply _).tupled, User.unapply) } object Users extends TableQuery(new UserTable(_)) // If you want typed ID columns, which means any joins would be verified at compile time, you would do this case class UserId(val id: Long) extends AnyVal // This is the implicit mapper, needed so slick knows how to go to UserId -&gt; Long and vice versa implicit val UserId2Long = MappedColumnType.base[UserId,Long]( _.id,UserId ) case class User(id: Option[UserId], firstName: String, lastName: String) class UserTable(tag) extends Table[User](tag,"users") { def id = Option[UserId]("id", O.PrimaryKey, O.AutoInc) def firstName = column[String]("firstName",O.DbType("varchar")) def lastName = column[String]("firstName",O.DbType("varchar")) def * = ( id.?, firstName, lastName) &lt;&gt; ((User.apply _).tupled, User.unapply) } object Users extends TableQuery(new UserTable(_)) // Lets say we have another table, books which is a simple oneToMany case class BookId(val id: Long) extends AnyVal // This is the implicit mapper, needed so slick knows how to go to BookId -&gt; Long and vice versa implicit val BookId2Long = MappedColumnType.base[BookId,Long]( _.id,BookId ) case class Book(id: BookId, userId: UserId, author: String, genre: String) class BookTable(tag) extends Table[Book](tag,"books") { def id = Option[BookId]("id", O.PrimaryKey, O.AutoInc) def userId = column[UserID]("userId") // Make a foreign key on user def user = foreignKey("FK_user", userId, Users)(_.id) def author = column[String]("author",O.DbType("varchar")) def genre = column[String]("genre",O.DBType("varchar")) def * = ( id.?, userId, author, genre ) &lt;&gt; ((Genre.apply _).tupled, Genre.unapply) } // The use of Option[UserId] instead of Option[Id] means that when you are doing joins on UserId, it has to be of the right type. This is true for both queries (the column has representation of Rep[UserId]) and actual values you get from the database (the case class has UserId in its definition, not Long) Its a bit ceremonial, however all of those table definition can be skipped use codegen. Alternately (which is what we did), you can generate your Schema from Table/case class using the `.schema` method. In our company we combine this with FlywayDB to generate our initial table schema
&gt; Still the functional mapping seems to have these big gaps in it, like namely why do I have to write all of these table mappings when their generation is obvious? (e.g. a String field is a column[String] in the DB) You can apparently use codegen to create those mappings from your table schema (note that I have never done this, I went about it the opposite way. Defined our Schema in Scala using case class/table and generate SQL out of that) &gt; I'm not trying to argue that ORM should be the way, but how the FRM is implemented is pretty puzzling. In what regard?
&gt; Sure, but how is this basic CRUD If fetching users with their phone numbers in a single query isn't basic then I don't know what is. BTW, how would you do it with Slick? &gt; in a similar way to how you would do it in Slick Other ORMs do it automatically, setting keys properly and loading related entities AND allow a programmer to do all this stuff manually if he doesn't define relations at ORM level. With Slick you have to manage relations yourself even in simple cases where it could be done by a framework. &gt; Slick does allow you to define index's Maybe I wasn't clear enough. The easy part of dealing with RDBMS is getting data in and out - whatever the tool you use eventually you'll figure it out. The hard part in using RDBMS is design - figuring out how to put the data in tables, what indexes to use (not creating them, that's easy), tweaking engine configuration etc. I expect database access library to help me with easy part and most ORMs do. &gt; I only go back to manually generating SQL if I really need the performance somewhere I'm talking about workflow. People usually write complex queries using an interactive UI like `psql` or `phpmyadmin` viewing results, creating and dropping indexes, examining plans etc. Then they have a wall of SQL that needs to be translated into DSL.
If Play is considered the best one I wonder what horrible things exist in Java ecosystem. It's really puzzling that people managed to create such a bad solution for a well-known problem, there's a lot of MVC frameworks to take inspiration from. Your simple example looks relatively good but it doesn't show the real picture like *all* templates requiring at least two implicit parameters (something like `AuthRequest`to display "Hello, expatcoder" in the top right corner of the page and `MessageApi` to use form helpers (not mentioned in docs BTW)). Using so much dependency injection is a bad idea, the code looks incredibly noisy. Overall, it doesn't provide enough bang for the buck especially when features are more important than performance.
But I do want it to return an M[U]. If my M is a List and U is an Int, that List[Int] is a monad and its flatMap returns a List[Int].
Those are some kind words, thanks ;)
I copied your `build.sbt` into a new (empty) directory, and then added `addSbtPlugin("org.scala-js" % "sbt-scalajs" % "0.6.5")`, to `project/plugins.sbt` as described in http://www.scala-js.org/tutorial/basic/ The SBT definition compiles without any further changes. I would question whether IntelliJ and your command-line invocation are using different versions of SBT? What version of SBT is on your path? Do you have a version of SBT explicitly set for your build? Note that to use the latest SBT for your build, add the line `sbt.version=0.13.9` to your `project/build.properties` file. (and indeed, if I run your build with SBT 0.13.1 explicitly set, I get the error you refer to, AutoPlugins do not work with SBT before 0.13.5).
I'm using sbt 0.13.8 which is explicitly set. Intellj sbt(working): [info] This is sbt 0.13.8 [info] The current project is {file:~/IdeaProjects/ScalaJs/}scalajs 0.1-SNAPSHOT [info] The current project is built against Scala 2.11.7 [info] Available Plugins: sbt.plugins.IvyPlugin, sbt.plugins.JvmPlugin, sbt.plugins.CorePlugin, sbt.plugins.JUnitXmlReportPlugin, org.scalajs.sbtplugin.ScalaJSPlugin [info] sbt, sbt plugins, and build definitions are using Scala 2.10.4 normal sbt(fails to parse build.sbt): [info] This is sbt 0.13.8 [info] No project is currently loaded [info] sbt, sbt plugins, and build definitions are using Scala 2.10.3 
The first way you mention (define schema in scala and generate SQL out of that) is how I want to proceed. I don't want to write straight SQL for various reasons, but chiefly that SQL comes in dialects, and if I write a big complex model, I'll immediately be married to one dialect rather than getting married to my domain models, which is what I'm really interested in. FRM is puzzling to me in several ways: 1. While generally type safety is a good thing, w.r.t. the database it seems applied like a bit of a religion here - I can identify extra work that I have to do in order to get type safety, but it's hard to identify the value of type safety here in context (aside from general/high-level arguments about why it's valuable). In regular software engineering I can connect with why type safety saves pain, but in this particular corner, I have to say it seems to be solving a problem I never ran into before. 2. This isn't a comment about the FRM model per se but the way it's implemented in slick, there's no standard way of doing anything, as simple as an insert. That is puzzling, seeing as it's a layer for working with DBs. Everybody does insert differently...seriously, WTF (see my other comments for specific links demonstrating this) This is a potential big impediment for us. We want to use best practices, not invent them. 3. With ORM, yes there are pitfalls but the value is obvious - so much is handled for you, very little boilerplate. With FRM the value is much less obvious, much more theoretical (again, hearkening back to more general arguments about FP goodness). Example time: I did a lot of java for years. For a while I wasn't using ORMs. So I would write model classes, and I would have to write this layer that would translate back and forth from model classes to DB rows. Total pain in the ass. Then came ORMs -- the value was obvious in the sense that I would start with a model class, put some annotations on it, and except in exotic cases that was it -- everything else was done. Now I'm coming to slick and it feels like going back in history 8 years. It's better than writing the layer *myself*, in that slick gives me a structured way of writing and maintaining that layer (namely, mapping between case classes and DB objects). But it still frankly kinda sucks in that I don't want to write/maintain that layer at all. That is in fact why I want a framework, to free me from that work. So when I say the FRM is puzzling, that's what I mean - it doesn't save me the work that I don't want to do, and it does things in a strange way for benefits that are unclear to me. Again, I get the *high-level* arguments about FP and type safety, but in the context of this particular niche, I just need more than that to see why it's valuable as (via ORMs) I haven't encountered some of the problems in this niche that the high-level arguments address.
I'm not sure I got your point, but if you're saying that the article is too long, trust me, it started out as a 3-min read. But then I stareted introducing additional explanations, e.g. why would I say "look up flatMap online" when I can spend some extra time and explain it then and there for those who need it? In case you really want just a simple explanation of monad is, this part of my text should be enough: &gt; You can think of monads as wrappers. You just take an object and wrap it with a monad. It’s like wrapping a present; you take a scarf, a good book or a fancy watch, and wrap it with some shiny paper and a red ribbon. We wrap gifts with shiny paper because they look pretty, and we wrap objects with monads because monads provide us with the following two operations (if you read about monads somewhere and see the term “two natural transformations”, this is what they are talking about): identity (return in Haskell, unit in Scala) bind (&gt;&gt;= in Haskell, flatMap in Scala) However, instead of merely scratching the surface, I wanted to explain it properly. What is it that these operations do? Why is it useful? How can we combine them? etc... Topic like this cannot be fully covered in half a page of text.
My intention wasn't to criticize your article, it was to point out something larger about the difficulty in learning monads. What you say about flatMap is an indication of the difficulty of writing these things. Which knowledge do you assume your reader starts with, and what do you have to give them? It's tricky. &gt; However, instead of merely scratching the surface, I wanted to explain it properly. What is it that these operations do? Why is it useful? How can we combine them? etc... Topic like this cannot be fully covered in half a page of text. In the example you gave (using flatMap and option to avoid the null pointer problem) you show how to chain these things resulting in a final object, which I'm guessing is still Option[User]. OK, this shows a more compact syntax but still the question remains why is this better than the nested if/then? If I wanted to do something different depending on whether one link in that chain was the cause of the breakage, how would that be integrated? What are the pros and cons of doing it this way vs. nested ifs? The value of the monad really hinges on those answers, so readers can see *how* to use a monad, but might still wonder *why* it's better than several alternatives they have. Examples of why it's better are crucial, because that's the difference between "monads for monads sake" and "here's a practical tool you can get stuff done with that has advantages over your alternatives". That last point is something (as an immigrant) I always have a hard time with. Gaining the abstraction then is only part of the battle, the other part is the "who cares"? Prove it's better than an alternative for some task.
The latter log is incomplete, you are not showing the whole story 
I was just going to link to the [monad tutorial fallacy](http://two-wrongs.com/the-what-are-monads-fallacy). I'm glad you beat me to it. :-) The one-sentence summary that works best for me is "monads are programmable semicolons," which is to say, they sequence operations. What operations? Any operations, hence the "programmable" part. I also think it might help to just move on from the question and ask a more "practical" one: if I want to _create_ a monad for myself or others to use, how do I do that? [tpolecat](https://www.reddit.com/user/tpolecat)'s [Programs as Values](https://www.youtube.com/watch?v=M5MF6M7FHPo) presentation does the best job of this I know, grounding everything in JDBC `ResultSet`s, giving us a monad for free (another ha-ha-only-serious: it's a [free monad](http://eed3si9n.com/learning-scalaz/Free+Monad.html)). He spends no time on the monad laws, burritos, elephants, or any of that. He just talks about the problems with JDBC and how (scalaz free) monads solve them. Highly recommended.
Nono, it means you could write this class List[A] extends M[A] { override def flatMap[B](a =&gt; M[A]): M[A] {...} } According to your trait that would also satisfy your monad interface, but it's not right. 
&gt; Now I'm coming to slick and it feels like going back in history 8 years. Excactly. All that bullshit "Slick is not an ORM but a FRM" is a fancy way of saying that it just doesn't have enough features and you have to manually produce tons of boilerplate that you haven't written for years. It is an ORM, though it's not an Object-Relation Mapper, it's Object-Row mapper because that's all that it can do.
Is there a good, short, fast explanation of why monads/programmable semicolons are better than other available alternatives? There are some easy straw-mans; say I want to chain 5 operations. I could have a bunch of ugly sequential if/then/else blocks. Alright, easy to show that monad chaining is better than that. But there are other alternatives, say for example fluent interfaces that allow you to chain operations and guarantee non-null returns, but that aren't "monadic" in any sense. Why then would monads be better than that? What I really wish I had was a more mature understanding of the tradeoffs. Right or wrong, I start with the assumption that everything has pros and cons. I fundamentally reject that there's a construct which should always (or never) be used. So then, the name of the game is figuring out, "when this, and when that?"
I agree. But I never though of that trait as actual implementation that should be used for implementing monads. It's a concept, and seeing code us always helpful compared to just reading the text. As I said, even M. Odersky used a similar trait in his lecture (only he put unit outside the trait, but I find this simpler). But thanks for the comment, you are absolutely right.
&gt; Why then would monads be better than that? Monads are universal. They are not a feature of Scala or Haskell, they're a mathematical construct. You only need to learn them once and then they make sense everywhere. Most language/library APIs are idiosyncratic in small ways that are a non-transferrable investment of learning effort. Learning category theory is the first time I have ever felt like I am learning a programming skill that will still be useful in 30/40 years time when I retire.
We do something very similar, but dialled right back. Maybe you don't need to have an ADT error per method? One per sub-domain ought to suffice and in many cases you don't need to fully unwrap them as once you bubble up to the top a simple 500 error suffices for most situations. It's nice to have the extra type safety of an explicit error for when you need it, but most of the time you don't - or at least I never have.
Compose excludes scala, so I'm not sure this is the right place to post about it.
Your post really helped me. Thank you.
&gt; you have to manually produce tons of boilerplate that you haven't written for years Again, as has been repeated numerous times in this thread, code gen for the mapping. Otherwise, the query "boilerplate" is semantically the same as what you would write by hand in string-ly typed sql. In other words, it's not boilerplate but rather explicitly stating exactly what data you want retrieved from the database. &gt; It is an ORM, though it's not an Object-Relation Mapper It absolutely is a relational mapper, as per above you specify the relations -- without any performance penalty (i.e. N+1) and type safe to boot, neither of which Hibernate et al can lay claim to.
Yes, because Slick up to (and including) v2.x was designed around JDBC drivers. Slick 3 was redesigned to be driver agnostic (as was stated by Szeiger many times), its just that the Slick team hasn't had time to provide an official implementation of Slick using non JDBC drivers The point is that the design/abstractions in Slick are currently in place to allow Slick to integrate with any driver, whether it be JDBC or something else
&gt; If fetching users with their phone numbers in a single query isn't basic then I don't know what is. BTW, how would you do it with Slick? I think the problem here is, you are stating your arguments the wrong way. Instead of saying "what do you need to do", you state "this is how you do it", and present it as a solution. Obviously Slick doesn't have object graphs, because its not an ORM, but the question is, what are you solving with object graphs? Object graphs are just classes (that are already serialized, and have an extra synchronization layer ontop to deal with saving/updating), that have their relations mapped out Guess what, you can do this in Slick as well, there is nothing that is hard in Slick. Simply state what you want, and write query for it. If I want users with address's, I simply write for { u &lt;- Users.filter(_.id === someUserId).head a &lt;- Addresss if a.userId === u.id } yield (u,a) And here is your object graph. Its a collection representation a simple relation, its really not that hard. If you want, you can put it in a case class case class UsersWithAddresss(user: User, address's: List[Address]) for { u &lt;- Users.filter(_.id === someUserId).head a &lt;- Addresss if a.userId === u.id } yield UsersWithAddresss(u,a) Its really not that hard &gt; Other ORMs do it automatically, setting keys properly and loading related entities AND allow a programmer to do all this stuff manually if he doesn't define relations at ORM level. With Slick you have to manage relations yourself even in simple cases where it could be done by a framework. Sure, but guess what. Simple relations, are completely trivial to do. They take like one line. If you find yourself doing the same relations again and again, you can just write it into a function &gt; Maybe I wasn't clear enough. The easy part of dealing with RDBMS is getting data in and out - whatever the tool you use eventually you'll figure it out. The hard part in using RDBMS is design - figuring out how to put the data in tables, what indexes to use (not creating them, that's easy), tweaking engine configuration etc. I expect database access library to help me with easy part and most ORMs do. The problem is, that RDBMS are **relational** (thats what the R stands for). ORM's aren't really relation. There is an impedance mismatch between stateful objects (or classes) and tables. Using an ORM actually limits your ability to design your database problem, because unless your database design is almost completely flat, you will hit into problems where ORM actually hinders your design (and at this point, most people resort to manual SQL) The thing is, the advantages that ORM's give you, are actually trivial to do in a FRM, even if you have to be explicit about it. The reason why people wanted to deal with classes in an ORM in the first place, was to add some sought of typing/structure to their stringly typed SQL (no one likes coding with strings, even in dynamic languages). This is strictly a historical thing, people didn't wont to work with strings, they wanted to deal with what felt right for the language they were working in. &gt; I'm talking about workflow. People usually write complex queries using an interactive UI like psql or phpmyadmin viewing results, creating and dropping indexes, examining plans etc. Then they have a wall of SQL that needs to be translated into DSL. Sure, and I used to do that. Now I design my complex queries in Slick, and it achieves the same result, while being performant and type safe. If there is an issue with the performance of the query, you can turn on the logger for the SQL generator to find out what is causing it.
Codegen doesn't generate basic CRUD operations so you have to write that weird-ass `Posts returning Posts.map(_.id) into ...` code. Seriously, is adding `Posts.insert(post).returning(_.id)` shortcut that difficult? Mappings generated by codegen seem to forbid some legal database operations when there are auto-incrementing not null columns in the table. It's not like they are really needed but it goes against the whole "Slick is very close to database" design philosophy. Also, hooking it to Play application required more knowledge of `sbt` than I could care about. 
&gt; To address your point more directly (which seems to be "why bother with this sought of type safety"), at least if you are doing your typical backend for a webapp, a lot of your business logic is going to be database driven queries. So for the same reason why like type safety in dealing with backend business logic, we woud like to see that with database logic That is the high-level argument I'm referring to. It seems as though you're saying "there are general benefits to strong typing with business logic, therefore they apply to the database logic too". That argument is sound, but it's not really addressing what I'm asking, which is, within the niche of database programming, what problems is it fixing for me? Again I understand the high-level arguments, but I'm also saying that problems arising from lack of type safety at the DB level are not problems I have actually encountered (where I have encountered them elsewhere). So specifically what problem at this layer is being solved for me? Let me give an example: if I used a standard ORM, I'd have some model field (let's say a String) which got automapped on to a string in the DB. Because of this automapping, I've never encountered a type conflict, I'm not sure how it would even occur. When I write my own queries (e.g. SELECT COUNT) the query is either bound to an object ahead of time, or the type is apparent. It's true I could commit some errors (e.g. I might try to put SELECT COUNT into a String or something goofy like that, which would be wrong) but many of these sorts of things might be caught by unit tests, which I'm probably going to have in scala too (compiler and type guarantees are nice, but of course we still need to test). So in those cases, what common problem is there with pulling data out of the DB that's being solved by boosting up potential errors to compile-time? &gt; I am not sure what the standard way is. That's a problem I think. I've read piles and piles about this, and I don't know either. That's really troubling to me, because we should not all be implementing different ways of doing basic things. &gt; Slick mirrors Scala collections, so the standard way to insert an item using Slick is by using the += or ++= operators, which is pretty much the standard way to insert any item/items into any collection in Scala. It's funny -- this makes perfect sense, and it's a fine argument, my trouble is that this just isn't the way I've seen it work in so many examples; most other places and code I've seen actually has some sort of DB object where you're inserting something, not just adding/checking a virtual collection (which would be nice). I don't dispute that it's possible/desirable to do it that way, just that as a noob, that's not what the material is teaching me to do. &gt; Having your database logic statically typed is one very obvious advantage Still struggling to get to the specific argument why, this is just re-statement that static typing is good. &gt; Another very obvious advantage is performance, without having to manually write your queries in stringly typed SQL. Unlike ORM's, FRM's do not have the n+1 problem So a couple of problems here; first in my original post I linked to some problems I heard from the community about slick where performance was quite poor (arguably fixed since then), so as a beginner it's hard for me to accept the performance argument when I've heard otherwise. Second, while I might not have to write my queries in SQL, if I want to use the code gen feature, it's making me write the whole model in SQL (so I can code gen from that). This seems to me the opposite of saving time. Finally - ORMs did *used to* have the n+1 problem, but most don't these days, or at a minimum provide some features where with trivial modification that problem goes away. Most modern guides now indicate the n+1 problem as a "common performance anti-pattern, which is easily mitigated by using the framework correctly". So I accept that slick avoids this problem, but at this point that seems more like a basic expectation rather than a differentiating feature.
There seems to be a some/all problem here. In some cases, you might want to specify fine-grained details, e.g. in this case String should map onto VARCHAR(200) and not TEXT. But you don't want to always do that. As in, just because that comes up doesn't mean you should have to tell the DB every time that a Float should map to a Float, and so on. Most cases are simple and obvious. A few (like what you mention) are non-standard. So you do want flexibility to handle all situations, but there needs to be a recognition that most situations are not what you describe, they're much simpler. Second - most other ORMs provide support for that out of the box, like with [Django's ORM](https://docs.djangoproject.com/en/1.9/topics/db/models/) where you can specify things like max_length in the declaration. If you don't want to it picks a reasonable default, but if you do want to, it's usually just an extra parameter or annotation. &gt; You can't just take a case class and poof make a table. This I just don't understand. Because that is exactly how it works with other frameworks and languages. You absolutely **can** take a model class and poof make a table. You can even do that and *retain control* over field lengths, data types, etc. What do you see as the problem there, that necessitates doing everything by hand?
Thanks - I am trying to approach this as open-minded as possible. With FP in general you usually get this general feeling like you're struggling with hard concepts, but that there's something worthwhile on the other side of it. It is sometimes very difficult to get specifics on why something is better. For example, elsewhere in this big thread now, there's a lot of discussion that really boils down to "strong typing is good at the DB layer". This is a claim, and I understand it at a high-level (why strong typing is important for software engineering) but I am concerned it's over-applied here in slick. I need more: why is it good here? What are my alternatives? Prove its better, etc. I can't copy examples without understanding why they work because to me that feels like building a castle on quicksand. Sooner or later, something's going to get messed up then I'm completely adrift and won't have any clue how to fix it.
Because once you decorate your case class with how long every field is, you are conflating your concerns. Your case class is a data object. There is no better way to write a database table than in the databases SQL. Anything else is a crude aproximation. In any real production deployment you want to specify every single column type. Why would I put that crap all over my data objects? I can argue best practices all day but no thanks. This is how slick works. Use it or dont. Your choice. It's very reasonable how it works, but if you can't understand then use ruby.
ScalaJS doesn't really do anything angular does. Using it won't change your need or not need of angular. I use it with JQuery like it's 2010 but I am not a huge SPA fan. 
&gt; If I want users with address's, I simply write It returns *a user*, not *users*. If you need to return multiple users each having multiple phone numbers you have to use `groupBy` on the client side to convert `List[(UserRow, PhoneRow)]` to `List[User]`. Now suppose that you also need emails and that's even messier `groupBy`. While it's not hard to do it's messy and boring and I prefer that ORM does it behind the scenes. The main reason for ORMs is to convert abstractions that database is good at dealing with (rows) to abstractions a programming language is good at dealing with (graphs). In a dynamically typed language you can get away with associative arrays (usually leading to horrible code where everything is an associative array) and with statically typed languages you need proper classes and objects. Slick is pretty bad at converting rows to something that's easy to use in Scala.
&gt; It returns a user, not users. If you need to return multiple users each having multiple phone numbers you have to use groupBy on the client side to convert List[(UserRow, PhoneRow)] to List[User] Yes and? This is completely trivial. val q = (for { c &lt;- coffees s &lt;- c.supplier } yield (c, s)).groupBy(_._1.supID) (This is from the slick docs by the way) &gt; The main reason for ORMs is to convert abstractions that database is good at dealing with (rows) to abstractions a programming language is good at dealing with (graphs). And this is the problem 1. Programming languages are not implicitly good with "dealing with graphs". They are good for dealing whatever they are designed for. I agree that doing something like Slick in Java is really hard, because Java doesn't have any nice syntax/patterns for dealing with immutable data. 2. RDBMS are not graph orientated databases, Neo4J is an example of a graph orientated database. They are relational databases that are based off set algebra.
it is covariant type. see here: https://twitter.github.io/scala_school/type-basics.html
As for operators (which are actually methods) like -&gt;, !, |&gt;, ##: and all others weird things could be found in libraries like scalaz, I'm usually using IDE code navigation and just look at definition. This is often not the case in Python, which is dynamically typed and couldn't provide much info about any particular definition. Hope that help.
It denotes covariance, which basically means: &gt; If B is a subclass of A &gt; Then Request[B] is a subclass of Request[A]
It's an easy process, Only requirement is the JDK should be installed properly. The link of installation guide is showing all the steps properly
&gt; Also a side question, how do you usually search for scala operators in google? [Symbolhound](http://symbolhound.com/?q=scala+%3D%3E) is pretty good.
 path("customer" / LongNumber) { customerId =&gt; put { entity(Unmarshaller(MediaTypes.`application/json`) { case httpEntity: HttpEntity =&gt; read[Customer](httpEntity.asString(HttpCharsets.`UTF-8`)) }) { customer: Customer =&gt; ctx: RequestContext =&gt; handleRequest(ctx) { log.debug("Updating customer with id %d: %s".format(customerId, customer)) customerService.update(customerId, customer) } } } ~ delete { ctx: RequestContext =&gt; handleRequest(ctx) { log.debug("Deleting customer with id %d".format(customerId)) customerService.delete(customerId) } } ~ get { ctx: RequestContext =&gt; handleRequest(ctx) { log.debug("Retrieving customer with id %d".format(customerId)) customerService.get(customerId) } } } Really? Needs more boilerplate. I don't get why we have all those libs with crappy DSLs for building REST APIs. Aren't the principals of REST pretty clear? We have resources and CRUD operations on them. Why do we still define that by hand? All you should have to do is define the models on the operations on them.
My guess is that someone didn't bother reading the body of the post. Reddit's kind of terrible for actual mental processing.
I don't know why you think REST is synonymous with CRUD operations, but if thats what you want, define a polymorphic method to route that then. def crud[T](id: Long)(implicit reader: Reader[T], writer: Writer[T], repo: Repository[T]) = { writer { put { entity =&gt; repo.upate(reader(entity)) } ~ delete { entity =&gt; repo.delete(id) } ~ get { entity =&gt; repo.read(id) }} ("customer" / LongNumber) { crud[Customer] } ~ ("order" / LongNumber) { crud[Order] } ~ ("product" / LongNumber) { crud[Product] }
&gt; This is completely trivial. I'm not saying it's difficult, I'm saying converting rows to domain objects is boring and looks ugly. In Entity Framework the query would look like `var users = db.Users.Include(u =&gt; u.Phones) // It's a query object, ToListAsync() or GetEnumerator() returns actual result` &gt; Programming languages are not implicitly good with "dealing with graphs" It's obvious from the context that graphs in questions are "object graphs" not the ones you calculate min-cut for.
`groupBy` doesn't mirror anything, it's called on an actual Scala collection. Thing is these kinds of operations aren't necessarily something that every application has, for most of them `map`, `flatMap` and `filter` are enough because they deal with object graphs. From my experience with LINQ which is at least from syntax viewpoint very similar to Scala these aggregations on collections of tuples quickly go out of hand.
I would check out: * http://blog.kamkor.me/Covariance-And-Contravariance-In-Scala/ * https://medium.com/@sinisalouc/variance-in-java-and-scala-63af925d21dc#.xa15ayigv * http://naildrivin5.com/scalatour/wiki_pages/TypeVariance/ * https://ktoso.github.io/scala-types-of-types/#type-variance-in-scala * http://julien.richard-foy.fr/blog/2013/02/21/be-friend-with-covariance-and-contravariance/
You may like this: https://github.com/pathikrit/metarest Basically, given models and compile time annotation macros, you can generate entire routes and DB calls etc..
I wish people would drop this obsession with the word "REST". There's more to HTTP than just REST, and Spray is an HTTP library. REST is just an architectural style; it's just one particular way of using HTTP or other protocols. There are REST libraries that give a higher level abstraction. Don't criticize an HTTP library for having an API that gives you all the flexibility of HTTP.
In broader terms, what is with this sub and people posting their own crappy blog posts about how to do perfectly simple things in scala (with 2-3x as much code as was needed) like they just solved a puzzle?
I've created an issue to support case classes updates/deletes: https://github.com/getquill/quill/issues/12 
 brew install scala Maybe I should set up a blog post covered in the most annoying ad modals possible. This sub is becoming a dumping ground for spam.
Nice, that's an important feature, part of Quill's appeal is not having to dive down into manual jdbc style boilerplate. Speaking of bare bones syntax, providing an implicit conversion from `User` to `query[User]` could perhaps be achieved via an f-bounded type case class User(...) extends Table[User] where `Table` is a marker trait that a macro picks up via fundep materialization; i.e. the macro looks for the *return type* of an implicit method (in this case an implicit conversion from `T` to `query[T]`). That should allow you to pick up the concrete type (case class) and inspect its properties at compile time. Stripping the dsl down to something like: quote{ (t,p) &lt;- Team join Player on (_.id == p.teamId) } yield(t,p) would be wonderful, particularly in terms of being able to visually parse complex queries where syntactic noise detracts from the essence (i.e. what you're trying to "say" in sql or other Quill supported dialect). Looking forward to seeing how the library evolves!
&gt; As you can see clearly, monads are for sequencing computations where the next computation depends on the value wrapped in the context of the previous one. That’s all that needs to be said, at least from a pragmatic perspective. Simple, eh? How does that intuition map to something like List(1,2,3,4,5,6,7).flatMap(a =&gt; List(1,2,3,4,5,6,7).flatMap( b =&gt; List(1,2,3,4,5,6,7).flatMap( c=&gt; if (a*a +b*b == c*c) List((a,b,c) else List()))) What exactly is the 'context' here that 'the value' of the previous computation is wrapped in? Or is something like that not 'pragmatic'? 
Strange article, and I'm not sure what it has to do with Scala.
Just use the AWS Java SDK. Wrap function calls in option, blocking future as necessary. com.amazonaws aws-java-sdk-sqs 1.10.39 all (85)
Not an expert but I believe the 'context' is the fact that it's a list. Sometimes it's referred to as non-determinism. The way i think of it is that you have a list of possible values and you haven't determined the one true value, though I'm not sure if that's an accurate representation of non-determinism.
I always knew functional programmers as ones to handwave away criticism and go deeper into delusions. But you feel so disheveled you need to try to dox me. You think people who disagree with you are your enemy. What you don't recognize is, you're looking yourself in the mirror. What I'm writing it factual. Most of the time, I'm quoting scala and haskell programmers. Many of them, are angry. They also ramble. Sort of like trolls. I care about you. I hope scala does well as a programming language.
I don't get it. He obviously has mental issues. Why isn't he simply banned from the sub? They did that with that Agleiv dude on all the Java/PHP subs.
Seems biased. Yeah, Haskell is a statically typed language, but so is Java, C++, and other languages that the author mentions, and can 'detect' the sort of 'bugs' that he references.
Can't help the feeling of weird coincidence that just a couple of days after I wrote [ my post about demystifying monads ] (https://medium.com/@sinisalouc/demystifying-the-monad-in-scala-cc716bb6f534#.fibk5rjs0) a post with almost identical title, also using Option and Future as examples, suddenly appears. But pehaps it is indeed a coincidence. Anyway, good article!
This blog was written back in August, not that that fact will prevent you from being any less juvenile.
Could check out [AWScala](https://github.com/seratch/AWScala). I used it for S3 once and it was reasonably serviceable. I would expect its SQS support to be equally so. The main problem I had was a small mismatch between how it encoded a `Region` for AWS and how S3 regions actually worked. Edit: [This](https://github.com/seratch/AWScala/pull/10) is the issue I ran into. Looks like it was worked around a month after the original issue was closed in [this PR](https://github.com/seratch/AWScala/pull/71/files).
I'm pretty sure 12 means december and 8 means today. But whatever, it's not a popularity contest, I just thought that very similar title, examples, even links and references in the end, are a big coincidence given I posted mine here like five days ago. But I didn't come out arrogantly, I acknowledged the possibility of a coincidence, and even if it's inspired by my post I wouldn't feel mad or whatever, just flattered. Don't see anything juvenile in that.
I was not, that could do the trick. Though I guess I'd prefer the pseudo mapper since you avoid potential name clashes with existing lower case identifier names that may also be in scope -- something less likely to happen with `User`, `Role`, etc.
From http4s's example: implicit val fooDecode = DecodeJson(c =&gt; for { bar &lt;- (c --\ "bar").as[String] } yield Foo(bar)) As a newcomer to Scala, these esoteric DSLs seem really unnecessary. `c --\ "bar"`? I cannot understand Argonaut's design decisions: http://argonaut.io/doc/codec/ `jdecode2L` is sensible, but the rest just seems really overwrought.
well in that case you should be using an Applicative, not a monad
Applicative functors don't have flatMap. You can't rewrite the example above to return the same list by using only applicative functor interface.
Like this: import scalaz._, Scalaz._ (List( 1, 2, 3, 4, 5, 6, 7 ) |@| List( 1, 2, 3, 4, 5, 6, 7 ) |@| List( 1, 2, 3, 4, 5, 6, 7 )){ (a,b,c) =&gt; if ( a * a + b * b == c * c ) List( a, b, c ) else List() }.flatten 
granted that I "cheated" with the flatten at the end, but the point is that, if there is no need to chain operations, you are usually better off with applicative style
Should be no surprise, but we use it at [Kifi](https://www.kifi.com). Makes using SQS very easy, which is why we open sourced it. If it's missing anything you need, let me know and we'll see what we can do.
I'm not sure it's better to use ap+join instead of just using bind. If it has ap then it has fmap, and if it has fmap and join then it's a monad...
You're not *really* using just the Applicative, there. While you call flatten, you could actually call join because you're flattening a List[List[_]] instead of a List[SomeOtherSequence[_]]. An Applicative with join is a Monad.
hm, I'm a little confused about naming here, intuitively I would say that the three lists were combined as applicatives, because they are applicatives, and then transformed the result as a monad... 
Nice post, showing differences between approaches. Only bad part is you chose to do file watcher, and once you try dynamic registration rather than registration of everything at startup, you run into problems because of http://blog.omega-prime.co.uk/?p=161 
It's still the case you shouldn't need macros for that implicit class InlineOps[A](val xs: Traversable[A]) extends AnyVal { def inline: Inliner[A] = .... } @inline implicit def inline2Traverable[A](in: Inliner[A]): Traversable[A] = .... class Inliner[A](val xs: Traversable[A]) { @inline def foreach(f: A =&gt; Unit) = ... @inline def map[B](f: A =? B) = .... @inline def flatMap(f: A =&gt; Inliner[A]) = ... } val xs: Traversable[Int] = List(1,2,3).inline.map(_ * 2)
Asynchronous Scala Clients for Amazon Web Services Best way to be reactive ;) https://github.com/dwhjames/aws-wrap
The thing that helped my understanding most was watching this video: http://youtube.com/watch?v=R8yg6J9basE YMMV
Thanks for pointing to Camel! I am the author of the article. Sorry I did not make it clear but the goal of the article was to illustrate the shortcomings of the default Java API and come up with an actor-based watcher using no libraries - just the default SDKs of Java and Scala. Thanks for the feedback!
Yeah agreed, or ideally it should be part of the compiler. Dotty may do a lot of these optimizations automaticall (and especially as part of their deep linker)
I think the big difference is you don't allocate any lambdas at all in the macro. See the `foldLeft(_ + _)` example expansion. The function `_ + _` is completely inlined in the macro.
A major use case, not to say a killer feature for Dynamic is that it allows [Scala.js](http://www.scala-js.org/) to integrate really well with JavaScript.
&gt; Haha, I thought by Dynamic Programming you meant Dynamic Programming. Oh, I wasn't aware of the use of the term, I took it from the blog title.
AFAIK, SNAPSHOT indicates that the build is not part of an official release. Milestones are planned ahead as part of the release cycle. As development goes on, an artifact that completes a milestone is tagged accordingly. I don't see the issue. Randomly picking a few from maven's repository: * [Google Guava](http://mvnrepository.com/artifact/com.google.guava/guava) * [Jetty](http://mvnrepository.com/artifact/org.eclipse.jetty/jetty-server) * [Apache commons collection](http://mvnrepository.com/artifact/commons-collections/commons-collections)
Very useful explanation of the dynamic typing in Scala: http://stackoverflow.com/questions/15799811/how-does-type-dynamic-work-and-how-to-use-it
Stackoverflow is full of anally retentive geeks, especially mods, who find it necessary to downvote/close threads for the tiniest reasons. Some of the best answers I've ever seen are usually responding to questions like "what are the pros/cons of framework X, Y and Z". Technically these questions are "against the rules" so out come the nerd warriors to close down the thread despite thousands of people finding it useful. Pisses me off to no end. I don't fully know the answer to your question but I just wanted to rant about SO :P
&gt; Stackoverflow is full of anally retentive ~~geeks~~ asshats, especially mods,... FTFY 
Also, sbt (Ivy?) treats `-SNAPSHOT` specially so that you can for example use `publish-local` repeatedly and Ivy will wipe the previous snapshot, while it is complains / deprecates to overwrite non-snapshot versions if repeatedly published. Sbt also has a utility function `versionString.isSnapshot` for this.
&gt; I was always wondering, why Scala community uses it's own (or anyone else uses it?) versioning conventions. Scala inherited these versioning schemes from the Java community, so it's not unique to Scala.
&gt; Producing highly reusable code (e.g: Data Access class that coverts dynamic method name to a SQL query). I'm assuming that this is talking about the ActiveRecord style of `User.find_by_name_and_email`, where `method_missing` converts all methods of the form `find_by_xxx{_and_yyy}` into dynamic SQL. Is that sort of thing a good fit in a language like Scala? [Even ActiveRecord appears to be moving away from it.](https://github.com/rails/activerecord-deprecated_finders)
&gt; wish Scala had this to access unknown external data like JSON, YAML, XML etc. Ask, and ye shall [receive](http://rapture.io/).
I work at a hedge fund and our users are typically comfortable in R/Python-ish but not Scala. We expose to them a worksheet which is essentially a Scala DSL. It uses dynamic typing a lot e.g. when a user types `"AAPL".price`, it converts it into `select price from stocks where symbol = "AAPL"` by implicitly converting the string "AAPL" to a Dynamic..
I'm trying to understand how Diffy is testing anything without writing a test. Don't we have to write the requests with which it does the output comparison? After deploying the old version and release build, how exactly does it come up with the comparison?
I'm trying to understand how Diffy is testing anything without writing a test. Don't we have to write the requests with which it does the output comparison? After deploying the old version and release build, how exactly does it come up with the comparison results?
Scala dynamics were introduced in Scala 2.10, way before Scala.js
It's a proxy. You still have to set up a test suite that hits your service with test requests. Diffy proxies the requests across the old and new code and analyses it for "differences".
I used to like the old rapture website with the docs. Do you know where the JSON docs are? The new website is simply empty: http://rapture.io/mod/json/
Aren't ScalaDoc and Hoogle two fundamentally different things? Of course, API docs such as generated by ScalaDoc imply the possibility to search for things, but Hoogle is not a documentation system, is it? As an alternative to ScalaDoc, I would find it great if we could use Markdown both inside `/** */` in the source, as well as having different output formats from the tool other than HTML, e.g. again Markdown.
Also, see this: http://scala-search.org/
See also: synthesis: http://leon.epfl.ch/doc/synthesis.html repair: http://leon.epfl.ch/doc/repair.html
Yeah I believe that's how it works. I haven't used it :) Just basing my reply off the README on their github repo which says you run your old and new server, configure those endpoints with diffy, and then fire requests at the diffy proxy. If you read the article, the title says "without writing tests" but the actual content says "without writing many tests". You were right to be skeptical of the title!
It returns the "initial" elements of the list.
See also [scalex](https://github.com/ornicar/scalex) for prior work on a Hoogle for Scala.
So initial in contrast to last?
Slides from a talk this evening at the Austin Scala Meetup. Wasn't recorded, but hopefully the code is useful for anyone who knows one of the languages and is looking to learn the other.
I tried learning Rust for fun, because I liked the approach of zero-overhead functional programming and runtime-less binaries. Rust can be really nice but I'm not sure if it's worth the trouble, just yet. I cannot base this statement on any substantial testing and I would definetely welcome a thorough comparison scala vs rust, i.e. on a series of concrete examples, but my impression is (also from public benchmarks) that the gains in safety and performance (no GC, more data on the stack rather than heap) are rather slim whereas the pain to make the compiler, and in particular the borrow-checker, happy is not slim. It takes lots of time and regular exercise to get productive. Anybody else gave Rust a try? What are your experiences?
"Way before" is a bit of a stretch :-p Scala.js was born exactly 2 months after the release of 2.10.0.
You must be confusing with typed implicit conversions. Dynamic is not allowed to kick in after an implicit conversion: $ ~/opt/scala-2.11.2/bin/scala -language:dynamics Welcome to Scala version 2.11.2 (Java HotSpot(TM) 64-Bit Server VM, Java 1.6.0_39). Type in expressions to have them evaluated. Type :help for more information. scala&gt; implicit class Test(val s: String) extends Dynamic { def selectDynamic(s: String): String = "select "+s } defined class Test scala&gt; "AAPL".price &lt;console&gt;:9: error: value price is not a member of String "AAPL".price ^ scala&gt; "AAPL".selectDynamic("price") res1: String = select price scala&gt; new Test("AAPL").price res2: String = select price The chain implicit conversion followed by Dynamic treatment is not allowed in Scala. So your example can't work.
I love the [image of the monster](http://s3.amazonaws.com/lyah/listmonster.png) in the Haskell page.
Yes, it was a simplified example. Here is the gist of the code we have: import scala.language.dynamics case class DbAdapter(id: String)(entity: String) extends Dynamic { def selectDynamic(attr: String): String = s"SELECT $attr FROM $entity WHERE id = $id" } implicit class Entity(id: String) { val prototype = DbAdapter(id) _ def stock = prototype("stock") } "AAPL".stock.price 
Not a single one of the listed errors is specific to Scala. The indexOf one isn't even specific to the JVM, or any particular language. The TLS one doesn't make a lot of sense either: what if you're not doing HTTP? Otherwise, why would you be opening the socket manually instead of using one of the (literal) hundreds of available libraries?
Like Noel I don't know what your actual problem is. But let me just say, don't reinvent command line argument parsing, just use [scopt](https://github.com/scopt/scopt).
If your methods return Try[InputArgument] instead of InputArgument, you can flatMap those Trys inside a foldRight with a for-comprehension. Then your parse() method would could return Try[Set[InputParameter]] instead. It might look something like this: import scala.util.{ Try, Success } override def parse() : Try[Set[InputParameter]] = { args .takeWhile(_ != "--") .foldRight(Success(Set.empty) : Try[Set[InputParameter]]) { (token, trySet) =&gt; for { set &lt;- trySet parsedArg &lt;- token match { case t if t.startsWith("--") =&gt; parseLongOption(token.drop(2)) case t if t.startsWith("-") &amp;&amp; token != "-" =&gt; parseShortOption(token.drop(1)) case t =&gt; parseArgument(token) } } yield set + parsedArg } } It's worth nothing that this has the disadvantage that the Try at the end will contain only the first error encountered. A better solution might be to use the scalaz Validation[E, S] type for your parse* methods, which has an error type E on the left and a successful type S on the right. If we use NonEmptyList[String] as our error type, we get a nice convenient list of errors for the entire input List. We can also use Traverse, which allows us to to take the List[A] and a function A =&gt; F[B] and gives us a F[List[B]] (if we just used map instead of traverse we'd get a List[F[B]]). import scalaz._ import Scalaz._ override def parse() = { // We need traverseU here instead of traverse because the scala compiler can't infer the type correctly args.takeWhile(_ != "--").traverseU({ case token if token.startsWith("--") =&gt; parseLongOption(token.drop(2)) case token if token.startsWith("-") &amp;&amp; token != "-" =&gt; parseShortOption(token.drop(1)) case token =&gt; parseArgument(token) }).map(_.toSet) } The final return type is Validation[NonEmptyList[String], Set[InputParameter]]. Validation can be either an instance of scalaz.Success, which will contain your set of InputParameters, or if *any* of the tokens fail to parse, it will be an instance of scalaz.Failure containing a NonEmptyList of *all* failure messages.
Aah, `getArgument`, `getOption`, and `getOptionByShortName` all return `Option[InputParameter]`, and this class is calling `.get` on those - if a given parameter is not defined in the application, currently that is what throws and is caught higher up to tell the application that a parameter passed to the application doesn't exist. I'm just looking for a way to avoid exceptions here, as it's a completely expected thing, that should be handled. Also, I don't want to stop the loop when the exception is thrown, I want to be able to continue to gather all of the issues to display them to the user of the application.
As I mentioned in my original post, this is a learning exercise - I could pick one of many topics, and could almost guarantee that they'd already have been done; but that is besides the point. I've expanded upon my question in a response to Noel's comment, but essentially it's that I am using options, and would like to have a more concrete "this is an error, or this is not an error" that I can pass up through the application without it being an exception. From what I've read I could use an Either, but I'm not sure if I'm just overcomplicating things by doing so. I seem to find myself in this situation a lot at the moment where I'm getting too caught up trying to do things "the right way", so I'm still trying to understand when certain solutions are applicable or not.
This is exactly what I was looking for, so it seems like the scalaz Validation type is very similar to the built in Scala Either type? Looks like I have quite a lot of things to look into here, but I'm sure I'll be able to find a solution I'm happy with based on this. Thank you so much!
Scalaz has two main left-right types: disjunction \\/ and Validation. They each have slightly different mathematical properties about the way you combine different instances of them (disjunction has a Monad instance and so can be flatMapped; Validation just has Applicative, but accumulates errors on the left hand side which disjunction doesn't do). Both of them are more powerful than scala's built in Either.
Thank you for the links!
I so wish the meetup was down south, but I know their isn't a lot of tech companies down here :(.
I unfortunately had a few last minute conflicts, but hope to see all of you at the next meetup. I looked through the slides, thanks for sharing. See you at the next one!
I mean Slaughter south, but I could easily make it to anything downtown, that would be awesome. I don't really want to fracture the group ... just move everything further south :). 
&gt; It is batshit crazy to say it is the only way to handle errors, and that is why everyone should use scalaz. Speaking of things no one said... The `Validation`, `Applicative`, and `Semigroup` example is just that: one example of how scalaz makes _one_ common task as trivial as it should be. Remember, the claim was that scalaz "complects" (cult terminology alert!) things. Finally... &gt;other people cannot handle errors the same way scalaz handles errors... is false, by which I mean `Validation` is just a `Product`, and is ["isomorphic to scala.Either and scalaz.\\/"](http://docs.typelevel.org/api/scalaz/nightly/#scalaz.Validation). It even provides handy constructors like `fromTryCatchNonFatal()` and `fromEither()` so it's easy to use with non-scalaz code. You wouldn't say "other people cannot handle errors with `Either`," so why do you say it about `Validation`? The point, ultimately, is that _all_ of scalaz is like this: unfamiliar at first, but lacking the _actual_ madness of the standard library, and inaccurately maligned by people who I'd bet large sums of money have not read Eugene's series, or Functional Programming in Scala, and just see some unfamiliar names like `Monoid` and unfamiliar symbols like `&lt;*&gt;` and immediately say: too complicated.
I see Rust as being a much lower level language even though it has a stronger (or at least stricter) type system. Rust is a good replacement for C or C++ (and probably Fortran, although I can't tell for sure since I've never used it). Scala is a good alternative to Java/Python. Of course there is overlap, but which one is better will very much depend on what you're doing. (and on what libraries you need to interact with)
This is a lot of fun. Thanks for sharing it! For anyone else who's having issues getting the website to synthesize/repair/verify more complicated things, I think it has a pretty short timeout -- download it and run it locally and you won't have that problem.
There's been quite a bit of reorganization. The latest JSON docs are [here](https://github.com/propensive/rapture/blob/dev/doc/json.md).
Have to taken a look at [Scaled](https://github.com/scaled/scaled) as well?
I would find it confusing if the method to get all but the last element was called "last".
Fastparse gets you a Scala syntax highlighter in a few dozen lines of code, which is handy if you need such a thing. I use it for the [Ammonite-REPL](http://lihaoyi.github.io/Ammonite/#Ammonite-REPL). The presentation compiler provides some helpful autocomplete, I use that in Ammonite as well. 
I recently wrote a C IDE using scalafx and the javascript ace editor. Im really happy with how it turned out. IM me if you want to see the code.
iirc, from what I last hard, its going to be unlikely for RUST to get HKT for similar reasons that RUST wont get complete TCO. The RUST developers are unaware of a way to get HKT's to work without unnecessary boxing (which goes against the abstractions with no runtime cost mantra). Its possible to work around this with implicit parameters, but thats not a feature that RUST has (yet). Similar reasons why complete TCO didn't end up getting added onto RUST, apparently its not possible to add TCO while still maintaining the C call stack structure without a performance penalty
Yep for-yield is exactly what you want. It desugars to maps and flatMaps. So for List, Option, scalaz.\\/, State, etc.
At the moment, scala.meta only supports syntactic APIs, which only covers syntax highlighting from your list of requirements. We plan to eventually have an IDE-like API, but that will most likely be an underlying abstraction that enables tools like ENSIME not replaces them.
Something simple that could make it a lot more readable is to assign more variables. 
I just stumbled across this new 'ToyIDE' project by Pavel Fatin, probably useful to check it out: https://github.com/pavelfatin/toyide
I'm bumping an old thread, but can you clarify what you meant by using implicit classes? Did you mean context bounds, e.g. def foo[T &lt;% Foo](x: T, y: T) = ... would be rewritten to type FooView[T] = T =&gt; Foo def foo[T: FooView](x: T, y: T) = ... ?
You have to log-in / register, though, but it's free and very fast via Twitter
The article's examples are broken, they use the html codes instead of the actual symbols. Like: event.kind() match { case ENTRY_CREATE =&amp;gt; println(s"${event.context()} got created") case ENTRY_MODIFY =&amp;gt; println(s"${event.context()} got modified") case ENTRY_DELETE =&amp;gt; println(s"${event.context()} got deleted") case _ =&amp;gt; // This can happen when OS discards or loses an event. // See: http://docs.oracle.com/javase/8/docs/api/java/nio/file/StandardWatchEventKinds.html#OVERFLOW println(s"Unknown event $event happened at ${event.context()}") } (\&amp;gt; instead of &gt;) EDIT: It appears to be a careless copy of another article which [was posted previously in this sub](https://www.reddit.com/r/scala/comments/3w17me/reactive_file_system_monitoring_using_akka_actors/): http://www.javaadvent.com/2015/12/reactive-file-system-monitoring-using-akka-actors.html
Oh, I see. I usually only see view bounds and context bounds, with former being deprecated and in most cases replaced by the latter (e.g. replacing [T &lt;% Ordered[A]] with [T : Ordering[A]]). Are implicit classes also a common pattern?
Where is the link for the videos? Thanks.
You should see a time-table. All presentations that have a video link show a little play-icon.
Also this drawing: https://i.stack.imgur.com/KjDLw.png
Implicit classes are very common in type class oriented libraries such as Cats and Scalaz. The "syntax" in these libraries is constructed using implicit classes. To address your original problem, I would use a type class. Abstract the essential properties of `Foo` that you need into a type class and then define type class instances for the various types that you wish to act like a `Foo`.
Sorry, but this part is exactly what's confusing me... What are "type classes"? Scala has no explicit type classes like Haskell, but instead simulates them using implicits. When you say "type class", are you referring to an implicit class or a "normal", explicit generic class (trait) that would then be used with context bound? (e.g. Ordering[T])
The latter. Type classes as encoded with traits, implicit parameters, and implicit values.
So, implicit class is basically a more verbose version of implicit method (the difference is, as you said, that it makes explicit the point where conversion is happening), and type class is a "superset" of that. Type class is a concept achieved with generic trait combined with aforementioned implicit methods (or classes), implicit values, implicit parameters etc.
And the UI is written in Scala.js :-)
It is an ad for their code review service
I think a lot of people gave up on Mongo a year or so ago.
I mean all languages. Mongo is a terrible database. Use postgres if you fit on 1 box or cassandra if you need scale.
Interesting. I haven't come across that viewpoint. I am quite experienced with building programs Node and Python...the libraries that help you use MongoDB in those languages are spot-on. I have heard great things about postgres and cassandra. Can you point me in the right direction for using these databases with Scala / Play 2.4? Thanks so much
Thanks for explaining that!
As a byte-compiled language, applications in scala will *generally* be faster than an interpreted, dynamic language like PHP, but honestly, I don't think it's a good idea to switch your existing application from PHP to Scala if performance is your *only* consideration. There are many other benefits of using Scala over PHP. The language and standard library are much nicer and more consistent, Scala has a powerful static type system with the convenience of type inference; functional programming with immutable data structures makes concurrent and parallel programming easy, and you have access to the large Scala ecosystem and larger ecosystem of the JVM too.
I highly recommend [Functional Programming in Scala](https://www.manning.com/books/functional-programming-in-scala). It will introduce many of the ideas and techniques you will encounter when you look at scalaz or cats. 
We started using scalaz by just using Validation and \\/ because they are super useful and easy to use and create better code. You'll probably end up needing a little bit of traverse/Apply to do more advanced stuff with it but it's a gentle intro. The next thing we started using was NonEmptyList, since it allowed us to write safer code at no extra difficulty. The next thing we ended up using heavily after that was foldMap from the Foldable typeclass, along with the Map monoid I mentioned earlier. Finally, I had to refactor an algorithm that used to rely heavily on mutable state and lots of classes intermingled. I used ReaderWriterState to simplify that greatly. It kind of snowballs. Starting with Validation and \\/ and NonEmptyList alone will improve your code, help with writing pure FP code, and teach you some stuff along the way. If you want to write pure FP you have to make the leap and commit to it. If you break it all the time by using mutable containers/vars/loops it's worse than not using it at all.
My team made the transition incrementally; that's the approach I recommend for most people. Jumping into Scalaz or the FP in Scala book might be too much too soon for a lot of teams. (Or it might be fine, in which case, go for it.) We just started with a strong preference for immutability (ScalaIDE's semantic highlighting is great for this; vars are an ugly orange-brown by default) and went from there, building out our repertoire of patterns and approaches as we got comfortable with new things. This meant having conversations among the team about what we were comfortable with collectively ("how does everyone feel about type classes? I think one would be good here", etc), and a willingness to accept that we wouldn't get rid of all our vars right away (or ever) but that we'd give it our best try.
You will definitely get more performance using Scala, and the constructs in Scala (such as `Future`'s) will provide you with much better scalability than Laravel (and you can go really insane in this area, i.e. using something like Akka) The only problem can be, and its an area that Scala is lacking, in the frontend tooling/framework area. Scala is quite a bit weak in this area, a lot of web frameworks in Scala area created from the POV of engineers, where as frameworks like Laravel are created as design first frameworks. A more practical approach might be to use Scala just for the pure backend (i.e. as a http json rest API), and then just make calls to the backend from Laravel. If you want to migrate everything (i.e. the tooling as well), I would probably recommend Play, as it has better client side support compared to the other frameworks out there
you have access to https://github.com/fpinscala/fpinscala
Yes like crazy; eg: - you can’t apply a discount to zero items - you can’t have a complaint with zero problems - you can’t have an average product rating with no individual ratings - you can’t apply for a job with fluency in zero languages - you can’t place an order with zero items, etc. The cases start to come out of the woodwork once you have this tool in your chest.
Preferring immutability actually goes a long ways towards gradually picking up all the scalaz stuff. Most people really like immutability once they start to get the hang of it. It really is a huge asset to a codebase. What then happens is you hit things that you can't immediately cleanly do immutably. That leads you to various FP concepts like monads and monoids and traversables and foldables because they are the clean way to do those things. Another thing Scala is really good at is hiding this pure FP mumbo jumbo behind easy to understand facades. My team's architecture looks similar to a typical Java/Spring enterprisey app (controllers to handle requests, DAOs that interact with the DB, model objects for business logic, Spring DI in controllers, etc). This is because the app was initially a Java/Spring project and Scala took over "from the inside-out" so to speak. So even if there's a ReaderWriterState monad driving a body of business logic, it's usually hidden inside some sort of class or trait that is very Java/OO-esque.
You should open it up to full remote, way more takers. Life is too short to commute.
IIRC, one of my favorite programming books in the days when structured programming was hot was Primer on Programming by Conway and Gries. One of its basic rules was that programs should "Do nothing gracefully." If your program cannot properly accept a list or array of zero items as input, it's too complicated. There may be special cases in which one wants to be sure not to construct an empty collection, but straightforward algorithms (such as those in the standard library) will handle empty collections without getting their knickers in a twist. If there is only one unambiguous reason for a collection to be empty, I'd rather accept the empty collection and handle that case than create an additional type to handle only that case. &gt;&gt; Collections of errors is a frequent example Such a thing might normally be accumulated by adding each error to the collection when the error is detected. But one must have an empty collection to add the first error. Of course, one would use recursion or some kind of map/reduce logic instead of mutable vars and loops.
&gt; Many, really. It's hard to pinpoint exactly where it's useful, but the thinking usually goes like this: I initially use a list in my type signature, but then I have to handle empty list Why? `List` is a monad, just flatMap your way through it and you won't care whether it's empty or not. If your logic can accomodate `NonEmptyLists`, great, use them, but if not, monads are here so you don't have to special case empty lists. 
This is just wrong thinking. There are plenty of reasonable cases where a value should be a NonEmptyList, both in input and output. It declutters your program to enforce it at a type level and defer to the caller to handle the empty case. For instance, why output `List[Error] \/ A` when you could output `NonEmptyList[Error] \/ A`? One makes more sense than the other. `\/.left(Nil)` makes no sense. Why take a `List[Possibility]` and output `Option[Possibility]` when you could take a `NonEmptyList[Possibility]` and output a `Possibility`, no failure modes about it?
This has also been my understanding. I rarely see purely referentially transparent code outside of the scalaz/cats circle in whike projects. Thats not to say that people professionally don't use those libraries/idioms (I know, for example, that Verizon uses a lot of Scalaz), but I don't think its terribly common. I think most people have the approach have is that they are RT/functional in the areas that gives the most payoff, rather than needing to mandate that their entire (or almost entire) codebase is RT/functional
That would be awesome!! 
Thank you 
This is basically what happened here just without readerwriter :-P
Hello, &gt; I'm all for some good old Java bashing Just to be clear, the "Javaism" is not a reference to Java the language, but to Java's best practices as used by the community, often in a cargo-cult fashion, a phenomenon that's not even specific to Java, it just so happens that Java being the most popular ends up attracting such practices, even though some traits of the language certainly help. &gt; is using mocks the wrong way I would certainly be interested to know the good way of using mocks and to be proven wrong. &gt; he's literally advocating for no integration testing because there are other non enumerated things that are a better use of time I'm not advocating for no integration testing, but for actual unit testing and in my experience until now I've seen precisely zero projects that do both properly. As developers we love to talk about ideals, but in the real world our ideals never happen because of budget constraints. And when integration testing does happen, it's often because the code is so tightly coupled that unit testing is not feasible, so you need to cover that ground somehow. Would love to be proven wrong though. &gt; Three he's advocating using an observer pattern when Scala probably has the best replacement for this ever with actors Oh mate, actors are no replacement for the Observer pattern. On one hand I guess we are both thinking of signals being streamed from one producer to at least one consumer. But when actor A "knows" that it needs to send messages into actor B, that's the same tight coupling you get with regular composition and method calls. To do decoupling with actors, you have to build a subscription mechanism on top, effectively building an Observer pattern on top of actors, by yourself because they don't do this by default. You see, data can be viewed as a river. The river just flows and doesn't necessarily care who drinks from it. And when you're pushing your messages into a channel or queue that allows subscribers to connect to this data, that's when you achieve decoupling. And as I said, don't confuse actors with the observer pattern, as they are not. You can build the Observer pattern on top of actors and I'm fine with that. I did it myself, it works OK. EDIT: the article now includes a new sample that does that ;-) &gt; But instead of advocating actors the OP is saying we should temporally couple together observers (what if they block or fail?). This is sort of a strawman, as the concept of the Observer is orthogonal to such concerns and no observers are getting harmed in the process. The Observer pattern is just about connecting producers to consumers. Also the idea is not new, but rather old and applied all over the place, including for connecting micro-services at the network level. This is what queuing / messaging systems are about, as in things like [Kafka](https://kafka.apache.org/). If you'd be talking about [Rx](http://reactivex.io/), which is a class of libraries exposing the Observer pattern on steroids, well that does have many utilities for handling failure and even for overflow strategies and for slowing down producers that are too fast. Things that Akka actors do not do, I mean it took the latest Akka 2.4 to finally have a mailbox that starts dropping messages on overflow ;-)
We do not have the same definition for "*dependency injection*". For me and for most people, dependency injection is about achieving inversion of control by means of passing a dependency to a dependent object, which is the "injection". And that's precisely what my sample ends up avoiding, freeing our object from its dependency.
OK, I don't consider that to be an injection, speaking of what can be considered a primitive passed as a constructor argument. If that's dependency injection, then you might argue that `1 + 2` is also injection of primitives into the plus function. Clearly there should be a boundary to our vocabulary, otherwise we can't have a meaningful discussion. Thing is we can avoid doing that completely in this example, I did it that way to keep it shorter. Now I've added 2 more samples that should get the point across.
Agreed, as long as you have `var` local to a function which doesn't escape the given scope, it's fine to use them to improve performance/implement algorithms in a strightforward manner.
I was thinking more that Seattle isn't a huge city, and Scala isn't the most popular language. It's not like asking for 6 Java devs in San Francisco. To fill positions like this you may have to reach a bit further.
For extra credit, use [STRef](http://eed3si9n.com/learning-scalaz-day16).
&gt; I think most people have the approach have is that they are RT/functional in the areas that gives the most payoff, rather than needing to mandate that their entire (or almost entire) codebase is RT/functional That includes us at Verizon OnCue, BTW. It just so happens that our open-source offerings are referentially transparent except for the Instruments API of funnel, and we're even in the process of improving that. Anyway, the point is that even those of us who are architecturally committed to referential transparency get there incrementally in Scala. 
"Or." It's a dodgy representation of [∨ from formal logic](https://en.wikipedia.org/wiki/List_of_logic_symbols).
I agree that words should have meaning, so let me try to define what I mean. Remember that dependency injection was a reaction to an overuse of singletons to discover collaborators and an overuse of inheritance to customize behavior. With singletons, an instance is responsible for finding his own collaborators. In some cases, the singleton would *be* the collaborator; in other cases, the singleton was a repository that could *produce* the collaborator. But in either case, the instance itself was responsible for finding its own collaborators. Dependency injection flips that, and says that the instance should not reach outside of itself to acquire its collaborators. In my mind, the injected dependencies of an instance: * Are sent in from the outside world (via constructors or setters) * Affect the behavior of the instance (which usually means that they are stored in instance fields) * Serve a role that is more than just "data" We can say that a class is parameterized by its constructor parameters. If any of those parameters represents a strategy or policy or collaborator, then I would call these dependencies of the instance. Otherwise, they just represent data. One litmus test for whether something is a dependency or just data is whether the identity of the parameter matters, or just its value. I see `Channel` as a dependency because it's pointless to pass an arbitrary channel as the parameter to `ItemActor`; I need to pass a channel with a specific identity. That specific channel is the dependency. I'd argue that methods like `+` have no dependencies. In my mind, dependency injection is a structural thing. It's about building up an object graph where the outside world is responsible for deciding how the objects are wired up. On the other hand, `+` is ephemeral. It doesn't end up being part of any object graph. Once it's evaluated, it's gone. For that reason, it can't participate in dependency injection. Along those lines, I'd say that most method parameters are not dependencies. Method parameters may represent collaborators or strategies, but unless they are somehow made persistent in the object graph (e.g. they are parameters to a setter, and that setter is going to store them in a field, such that the instance's behavior has forever changed), they don't represent dependencies. I think this is a broader definition than the definition you were using, but still narrow enough to be useful. So yeah, I'm with you that Java and Scala are different beasts and that Scala provides other tools to, for example, make your code more testable. But testability is just one benefit of DI. Readability is another - being able to clearly identify all the collaborators of an instance can make code far more readable. For those other benefits, I think DI can still be a fine pattern in Scala. And DI is much preferred to singletons or inheritance. 
[removed]
Did someone say my name? ;-) Drop me an email (noel@underscore.io) and I'll send you a sample of Essential Scala (though perhaps Advanced Scala is more suitable for what you want? It's all about monads, applicatives, etc.) Putting free samples online is on the (overly large) TODO list.
Sent a PM. Man I love reddit :-)
45 = insert made up number to show that Seattle isn't New York City 
Cats is great, but I'm waiting for a stable release.
Not for Scala Not sure what the proper metric is... here http://www.indeed.com/q-Scala-jobs.html I see it ranked 5th in the US, but not sure indeed is a very good source ;) 
&gt; Spring of Scala What is this? Edit: Oh, you mean Spring like the Java library?
The title of both the article and this submission would have been so much cleaner if it had clarified that **`circe` is the name of a JSON framework**.
I'm not sure I understand why the argonaut example uses disjunction instead of Validation.
My thoughts exactly. This explains it: &gt; We are former IT professionals turned recruiters Where "IT professional" could mean just about anything.
I just applied. I'll let you guys know what scoop is.
 import argonaut._, Argonaut._ import scalaz._, Scalaz._ case class Foo(i: Int, s: String) object Foo { implicit val decoder: DecodeJson[Foo] = DecodeJson { c =&gt; val iResult = (c --\ "i").as[Int] val sResult = (c --\ "s").as[String] (iResult.toDisjunction, sResult.toDisjunction) match { case (\/-(i), \/-(s)) =&gt; DecodeResult.ok(Foo(i, s)) case (-\/((iMsg, history)), -\/((sMsg, _))) =&gt; DecodeResult.fail(s"$iMsg, $sMsg", history) case (-\/((msg, history)), _) =&gt; DecodeResult.fail(msg, history) case (_, -\/((msg, history))) =&gt; DecodeResult.fail(msg, history) } } } ಠ_ಠ
&gt; why wouldn't I declare all vals lazy? Lazy initialization is thread safe, so declaring all `val`s as lazy would incur the overhead of ensuring thread safety, which is often not needed and would result in some unnecessary overhead for the usual case. &gt; when I should be using 'lazy val' instead of just val One important reason would be that if initialization of a `lazy val` fails, it will be retried on next access. Not so with a `val`. That can be a double-edged sword, but may be useful, e.g. if retrieving required config is temporarily broken.
It's probably a fucking recruiter 
What exactly is unsafe about regular val declarations?
By comparison, Haskell behaves as if *everything* was marked as lazy, and it's great because like you say, if they aren't ever used it increases performance. However, there are also situations in which laziness decreases performance. For example, the obvious and space-efficient way to compute the sum of a list of numbers is to use an accumulator and to add each value to it as we go through the list. This is a space-efficient algorithm, in the sense that it only requires one extra variable to be allocated in addition to the list. In Haskell, due to the pervasive laziness, it's possible to accidentally write code which behaves like this instead: as we go through the list, accumulate the list of all the computations we'd have to perform if we actually wanted to compute the sum. Then, when the sum is printed, that computation is "forced" and the sum is computed for real. This algorithm is not space-efficient as it needs to allocate a description of the computation it would need to run whose size is proportional to the list. These kinds of situations in which an algorithm ends up using much more space than expected is typically called a "space leak". For this reason, the Haskell compiler has a "strictness analysis" pass in which it tries to figure out which parts of the code will be executed no matter what, and for those parts it turns off laziness. Since Scala does not use laziness as pervasively as in Haskell, it probably doesn't perform strictness analysis, so I think that using the `lazy` keyword everywhere would be more likely to cause the kind of space leaks I have described above.
Wish I had this last week when I started doing the exact same thing. Very useful info!
60k is the salary i pay my maid to clean my undies
If I'm brutally honest, this looks like a lot of work and over complication to "periodically FTP a bunch of gzipped XML files".
60 lines is a of lot of work?
It can be, you don't know how many hours it took to reach that result while, as parent says, writing code to do ftp + gzip is really trivial in any language. I suspect I could wing this in Java and RxJava in less than an hour and probably about the same code size. I also find it ironic that scalaz is praised as being extremely generic and reusable and even years after its inception, it's still necessary to fork it in order to patch it to get things done. Anyway, at the end of the day, what matters is being happy about the code you write. 
Off-topic: how did you make 'val', 'lazy', and 'synchronized' highlighted like that? I know about making code blocks with lines starting with 4 spaces, but those are inline.
Exactly. It's a fragment of a larger system, so it benefits from consistent use of scalaz-stream for continued processing. The next step, for example, is using [xs4s](https://github.com/ScalaWilliam/xs4s) and [scalaxb](http://scalaxb.org/) to parse the incoming XML into case classes. That's 8 lines involving `FileInputStream`, `GZipInputStream`, and [`io.iteratorR`](https://github.com/functional-streams-for-scala/fs2/blob/release/0.8/src/main/scala/scalaz/stream/io.scala#L171). After that, do something with the case classes... etc. etc. **Update:** to bring this home, there will ultimately be quite a bit more processing of the stream, with the final step probably being `.run.run` (process the stream for its effects), and this whole process will go in a Docker container based on [docker-zulu](https://github.com/delitescere/docker-zulu) and be managed by [Marathon](https://mesosphere.github.io/marathon/) on [Mesos](https://mesos.apache.org/). In other words, it's a batch process that should run forever, and there may be reasons to implement more sophisticated scheduling logic than we could get from, e.g. [Chronos](http://mesos.github.io/chronos/), to say nothing of avoiding [issues with Chronos reliability](https://aphyr.com/posts/326-jepsen-chronos).
Yes, that's much better! Thanks!
&gt; what matters is being happy about the code you write This not necessarily true if you work in a team. The team should be happy in the long term.
`Cool`, `thanks`!
Is that $60-80 **an hour**?
Do you work with paultypes?
Im also interested in answers to this, we're looking into docker atm for this
I'm am fan of using mutliple test configurations in SBT for situations like this. val Nightly = Config("nightly") val PreProd = Config("preprod") inconfig(Nightly)(Defaults.itSettings) inconfig(PreProd)(Defaults.itSettings) This configure's two new integration test configs in `src/nightly/scala` and `src/preprod/scala`where you can put environment specific tests. To run the test you then just do: sbt ";it:test ;nightly:test" or sbt ";it:test ;preProd:test" depending on which environment is running the test. &gt;At the end I want to POST some xml data; in production. I question the wisdom of testing in production, unless this is some sort of service availability tests.
No, I have no relationship with /u/paultypes.
i was wondering why the brown was on the outside the one day :( i should give her a raise
Good point. Let's explore this! First, what happens when exceptions happen in the `Task` monad? @ import scalaz._, Scalaz._ import scalaz._, Scalaz._ @ import scalaz.concurrent.Task import scalaz.concurrent.Task @ import java.util.Random import java.util.Random @ def coin: Task[String] = Task.delay { val rnd = new Random() if (rnd.nextInt(2) == 0) { "Heads" } else { throw new RuntimeException("Tails") } } defined function coin (Thanks, [/u/lihaoyi](https://www.reddit.com/user/lihaoyi), for [Ammonite REPL](https://lihaoyi.github.io/Ammonite/#Ammonite-REPL)!) OK. `Task`s don't do anything until you `.run` them: @ coin.run java.lang.RuntimeException: Tails cmd3$$anonfun$coin$1.apply(Main.scala:1395) cmd3$$anonfun$coin$1.apply(Main.scala:1390) scalaz.concurrent.Task$$anonfun$delay$1.apply(Task.scala:272) ... Welcome to Vegas, kid. My `download` `Task` is a bit more complex: it (tries to) connect to the FTP server, login, change directories, get a list of files matching some pattern, disconnect, and then download each of those files in parallel on their own connections, disconnecting when done, whether successfully or not. _Any_ of this can fail catastrophically. In particular, what happens when just one of the download `Task`s fails? [Here](http://lpaste.net/147309)'s the code again for reference. I'm using [`Nondeterminism`](http://docs.typelevel.org/api/scalaz/nightly/index.html#scalaz.Nondeterminism)'s `.gatherUnordered` to turn a `List[Task[File]]` into a `Task[List[File]]`, i.e. to get a `Task` that, when `.run`, will do the downloads in parallel and complete when they're all complete. But what if one fails? @ def three: Task[List[String]] = Nondeterminism[Task].gatherUnordered(List(coin, coin, coin)) defined function three @ three.run java.lang.RuntimeException: Tails cmd3$$anonfun$coin$1.apply(Main.scala:1395) cmd3$$anonfun$coin$1.apply(Main.scala:1390) scalaz.concurrent.Task$$anonfun$delay$1.apply(Task.scala:272) ... Of course. Honestly, I have no idea what other semantics `.gatherUnordered` could possibly have. But a joker in this deck is that `.run` is actually _not_ all you can do with a `Task`: @ three.attemptRun res7: Throwable \/ List[String] = \/-(List(Heads, Heads, Heads)) Oh, hey now! Not only do I get a perfectly good `\/` instead of an exception, I hit the jackpot this time! That's `Task`s. What about `Process`? @ import scalaz.stream._ import scalaz.stream._ @ val p = Process.eval(three) p: Process[Task, List[String]] = Await(scalaz.concurrent.Task@3a33055b, &lt;function1&gt;, &lt;function1&gt;) Remember, we can `Process.eval` an effect (`Task`) to get a stream (`Process`) of what that effect results in when run (`List[String]`). A stream of one element isn't very interesting: @ p.runLast.run java.lang.RuntimeException: Tails cmd3$$anonfun$coin$1.apply(Main.scala:1395) cmd3$$anonfun$coin$1.apply(Main.scala:1390) scalaz.concurrent.Task$$anonfun$delay$1.apply(Task.scala:272) ... Variations on a theme: @ p.attempt().runLast.run.get res13: Throwable \/ List[String] = -\/(java.lang.RuntimeException: Tails) I know I have an element, so I'm `.get`ing the `Option` for demonstration purposes. Don't do this for real. OK, so we can turn exceptions into `\/`. Can we use that to implement "retry up to n times with some delay in between" easily? There are a couple more ingredients we need: @ val g = Process.eval(Task.delay(println("Hi!"))) g: Process[Task, Unit] = Await(scalaz.concurrent.Task@61c30ea8, &lt;function1&gt;, &lt;function1&gt;) @ p.append(g).attempt().runLast.run.get res16: Throwable \/ Any = -\/(java.lang.RuntimeException: Tails) No cheery greeting from the `Process` we `.append`ed to `p`! This is crucial to understand: _the appended Process will only be run if the Process it's appended to succeeds_. Finally, remember, `time.awakeEvery` `.emit`s a `Duration` every period of its argument. OK, deep breath: def attemptRepeatedly[A](schedule: Process[Task, Any])(p: Process[Task, A]): Process[Task, A] = { val step = p.append(schedule.kill).attempt() val retries = schedule.zip(step.repeat).map(_._2) (step ++ retries).last.flatMap(_.fold(Process.fail, Process.emit)) } A `step` is an `.attempt` of `p` that, _if successful_, `.kill`s the retry `schedule`. Note the parens on `.attempt`. You can provide a handler for the exception here, and in production I'd log the exception with [Journal](http://oncue.github.io/journal/) and increment a [funnel](http://oncue.github.io/funnel/) `Counter` of failures. Also remember that the type of `step` is now `Process[Task, Throwable \/ A]`. `retries` just `.zip`s the `schedule` with an infinite stream of `step`s (I know, right?) and, since we care about the result from `step`, `.map`s to get the `._2` of the resulting pair. But remember, each `step` will `.kill` the `schedule` if it succeeds. Also, `.zip`ping a finite stream with an infinite one results in a stream whose length is the same as the finite one (more generally, `.zip`ping two sequences of lengths m and n where m &lt; n results in a sequence of length m, of course). So `(step ++ retries)` is kind of magical. If the `step` succeeds, `schedule` will be `.kill`ed, `retries` will be empty, and we'll have a stream of one successful `step` ("happy path"). If `step` fails, `schedule` won't be `.kill`ed, `retries` won't be empty, and more `step`s will be taken after the `schedule`d delay, until `step` succeeds and `.kill`s the `schedule` or we reach the `schedule`'s end. In other words, `(step ++ retries)` is a stream of one or more `step`s which is either a single success, n failures and a success, or n failures. No matter which, we want the `.last` one, and again on the "happy path" it's just the single success anyway. But it's a `Process[Task, Throwable \/ A]`, and we still want just a `Process[Task, A]` that is a success if we have an `A` and is a failure if we have a `Throwable`, so we `.flatMap` over the `Process` to get at the `\/` and `.fold` it with `Process.fail` for the left and `Process.emit` for the right. So, e.g. @ import scalaz.concurrent.Strategy.{ DefaultStrategy, DefaultTimeoutScheduler } import scalaz.concurrent.Strategy.{ DefaultStrategy, DefaultTimeoutScheduler } @ implicit val S = DefaultTimeoutScheduler S: java.util.concurrent.ScheduledExecutorService = java.util.concurrent.ScheduledThreadPoolExecutor@18b1dcc1[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0] @ val s = time.awakeEvery(5 seconds).take(9) s: Process[Task, Duration] = Append(Halt(End),Vector(&lt;function1&gt;)) That's "retry every five seconds up to nine times." Note that the `schedule` knows nothing about the `Process` to retry. Then: @ attemptRepeatedly(s)(p) res13: Process[Task, List[String]] = Append(Halt(End), Vector(&lt;function1&gt;)) @ res13.runLast.run res14: Option[List[String]] = Some(List(Heads, Heads, Heads)) I guess up to 10 times was enough for my three-coin toss! 
+1 for the book. If you make it through the whole thing, you can probably skip the intro course on coursera and jump to FRP.
I did exactly this and I can confirm it works well, just the course and book went kind of simultaneously. Also it would probably be a good idea to code a little on the side (I had the lucky opportunity to write Scala at work so I didn't need that).
Same question i faced one month ,as just switched to new company where my team code in scala ,my suggestions( it would much more helpful if you already coded in java) : 1. http://alvinalexander.com/scala , go through article which interest you ,as it help me to gain momentum 2. http://danielwestheide.com/scala/neophytes.html , this is god for me (MUST READ) 3. go through scala day talks 2013,2014 
If you're already an accomplished developer, I really recommend Scala for the Impatient. It's a fantastic book, well paced, and specifically aimed at experienced developers (namely, Java).
This is currently the most clean and scala-ish way of managing DI (IMHO). What do you guys think?
Isn't this service location, not dependency injection? You pass in the thing that holds all of the services into an object, and then the object grabs what it wants from that container. It does look like it has a nice DSL and seems to be documented well, but it doesn't look like DI. Unless I'm missing something? i.e: http://stackoverflow.com/a/4985582/1185534
The most idiomatically Scala way to do DI is to just use implicit parameters. trait MessageService { def greet: String } The use it like so: implicit val myMessageService = new MessageService { def greet = "Hello" } def login(implicit messages: MessageService) = { println(messages.greet) } login One feature scaladi illustrates on it's home page is qualification by some identifier, this can easily be done at a type level with a [tagged union](http://eed3si9n.com/learning-scalaz/Tagged+type.html)(you don't need to use scalaz, the definition of Tagged is trivial, but you should use it anyways). import scalaz._ trait Greeting class OfficialMessageService(implicit greeting: String @@ Greeting) extends MessageService { def greet = greeting } then use it like so: implicit val myGreeting = tagged[Greeting]("Hello") implicit val myMessageService = new OffficialMessageService login
Cleaner than macwire? I just peeked at it quickly but i thought the use of a constructor parameter for the injector and implementing an interface to be overly invasive. 
&gt; Isn't this service location, not dependency injection? I would consider service location in this context, a method to perform dependency injection. 
That _is_ part of the problem: if there is a feature in the language and it's not really obvious you shouldn't be using it (with compiler warnings or something like that), _some_ programmers will think they should use it. Some will do it sneakily, some will convince a team or subgroup it's a worthy feature, and suddenly everyone has to live with it. I know, I've been on the side of the experimenter - there's nothing wrong with experimenting as long as you manage to keep it contained (e.g. in some accessory tool) until you get buy-in of the majority of the group. I love a lot of stuff about Scala, and I would gladly use it for personal or small-team projects. I would not start a new project if it's planned to have or grow into 3 or more teams. In a more general way, I think Scala is a language that exacerbates the maxim of "your code is only as good as your worst developer" due to its flexibility.
Alternate TL;DR: we can't solve a people issue, so we switch the tech to one where our people are less likely to cause the issue.
This is all well said, and reinforces a fundamental philosophical point that is rarely discussed explicitly: you can view developers as interchangeable cogs and adopt a marching hordes "more is better" "HR" strategy, or you can adopt a "we need the right people to judge what the right technology is, do what we must to convince the best to join us, and develop a culture of ongoing learning both internally and externally." We at Verizon OnCue (for whom I obviously do not speak, right?) are keenly aware of the challenges Jim mentions, with an engineering staff of well over 200 people, and having made big commitments to Scala and the Typelevel stack, including especially scalaz and scalaz-stream. We've [open sourced](https://github.com/oncue) some of our work; we run reading groups on [Functional Programming in Scala](http://amzn.to/1muEKSG); we host meetups; we [present our work](https://www.parleys.com/tutorial/reasonable-rpc-remotely) at [conferences](http://functionaltalks.org/2014/11/23/runar-oli-bjarnason-free-monad/), etc. What we _don't_ do is introduce scalaz into the codebase so we can use the [`WriterT`](http://eed3si9n.com/learning-scalaz/Writer.html) monad transformer to add logging to our request/response pipeline. For one thing, we wrote [Journal](http://oncue.github.io/journal/) for that. For another, the FP community is [moving away](http://okmij.org/ftp/Haskell/extensible/index.html#introduction) from monad transformer stacks (see also the previous paragraph's last link). So yes, Jim has identified a problem... just not with Scala or scalaz.
Is go's lack of generics a case for or against the doom? 
It's just important to understand and communicate with your team about what you're doing. I'm currently taking a team away from Java6 to Scala and they were initially horrified at some of the Scala code, but have since eased into it and part of the ease of this was setting a very definitive style and dos/donts of writing the code. We also have a rule where if you implement something you feel may be confusing or highly abstract you have to hold a team meeting to explain it. Save though, we are a team of 5 people.
My question is why would it? you paramaterize the method as being "Nothing", unless there is an implicit from Nothing to string Scala wouldn't put it together for you. When you take out the [Nothing] it works perfectly fine for me. You just explicitly tell it that type will be Nothing and then pass in something else. It's not going to override you on that, unless I'm not understanding what you're trying to do here. This works fine for me. import scala.language.dynamics class DynamicObject extends Dynamic { def applyDynamic[A](name: String)(args: A*) = println(f"$name = $args") def applyDynamicNamed(name: String)(args: (String, Any)*) = println(f"$name = $args") def updateDynamic[A](name: String)(v: A) = println(f"$name = $v") def selectDynamic(name: String) = f"value of $name" } object DynamicObject extends App { val d = new DynamicObject d.namedParams("blah", name = "doug", job = "janitor") d.name = "blah" println(d.name) }
The main goal of an IoC container should be to manage scopes and lifetimes. While it is trivial to build even complex hierarchies of objects by hand, managing life scopes such as "one instance per web request" is much less easy. I strongly suggest to read this [AutoFac a lifetime primer](http://nblumhardt.com/2011/01/an-autofac-lifetime-primer/): even if about a C# library, it depicts very general topics about DI and IoC. 
The sbt extension is just used by scalaz to reduce boilerplate. Typeclasses are a common, valid, and useful design pattern in Scala. That's just a fact. It's silly to act like using typeclasses is going beyond the language. Here's a paper with Odersky's name on it that discusses them in Scala thoroughly: https://ropas.snu.ac.kr/~bruno/papers/TypeClasses.pdf
I was more thinking of fibers.
I didn't annotate it to Nothing, the Scala compiler did. That is the rewrite after the compiler applies its transformation, it should put String there. Nothing appears nowhere in my code.
Alternate Alternate TLDR: Don't use scalaz if you want readable code.
&gt; I just view Scala as a better java and not a place to write crazy code no one can easily follow. False dichotomy + an arbitrary value statement ("crazy code no on can easily follow" is a matter of perspective. "Better Java" code can often look that way to me because I am not constantly writing imperative and/or OO code myself) It isn't very difficult to write understandable code with scalaz. Maybe you'll have to understand some new fundamentals first (e.g. monads and combinators of them (all the \*M functions along with traverse) or how foldMap + Monoids work\*), but at that point it isn't a matter of the scalaz code being difficult to understand but rather the developer not learning something new. Tbh if you ban scalaz from your codebase, I'd just end up writing monomorphized versions of combinators all the time as helpers. That though sounds like the Go Way so that's koo. I do agree that if you don't bother to take proper measures when using new abstractions then you'll confuse your teammates. But if you're like me and work in a team of ~8 and every commit goes through CR that everyone has access to, that isn't very difficult. \* Note that I don't mention "how implicit parameters and the typeclass pattern" works because I consider that to be fundamental to Scala (not just scalaz). It's used in the standard library (math.Ordering)
The problems that are mentioned both involve scalaz. Maybe the problem is just scalaz?
One of the reasons i like Scala, is that it's trivial to manage something like one instance per request with implicits. You can just implicitly pass down the context.
the differences in comment sections between /r/golang and here are interesting. 
How did an engineer 'fuck up their codebase?'. It took me maybe two minutes to understand what was going on in the snippet. If it was written in Go it would be three pages long. Just kidding, you couldn't write it in Go because no generics. 
Some things which come to my mind: * IDE support * BuildSystem support * Write tutorials or guides * Go to schools or universities for teaching * Write libraries for Scala
Best ways to learn scala (or, how I learned having no experience with functional programming at all). 1. Start writing scala. 2. Read *Programming in Scala*, but also use it as a reference. You don't need to finish the book before you start writing scala. 3. Go to #scala on IRC (Freenode) and ask questions when you get stuck on something. 4. Read *Functional Programming in Scala*, and when you get stuck/confused/feel dumb, know that most people did. It's a really challenging (and rewarding) book. It takes a lot of time. 5. Keep programming Scala. 6. Ask more questions. I'm also a really big fan of http://underscore.io/blog/. They have some *fantastic* tutorials. It's all the stuff I would try to articulate to someone new to the language/functional programming, but done in a much simpler and eloquent way than I could come up with. 
Did you see this base page: http://scala-lang.org/contribute/ ? There are lot's of things, from documentation to tickets deemed suitable for contribution.
Jumping into compiler design is a huge undertaking a requires tremendous knowledge not only of Scala, but of a very niche and somewhat esoteric corner of computer science theory. If you want to have an impact you're much more likely to contribute meaningfully in the short run by contributing to a library you use. Find pain points and fix them. Alternatively, start your own project by picking a Java library you find useful/interesting and write a scala idiomatic wrapper.
Actually the more you think about it, the more your realise that your concurrency model is only as good as how universal it is. Languages like Python and Ruby have had their own extensions or libraries for concurrency models (fibre's, event loops, co-routines), but they end up being practically less than ideal, mainly because the libraries don't end up working with these concepts (which means at best you need to maintain wrappers, and at worst they need to be rewritten) There is a reason why Go,Erlang and Node shine in concurrency, and thats because they have an established form of concurrency which every library is expected to work with (node is however now having a little bit of a battle with Promises vs callbacks) This is one of the things I like about Scala, is that it has an established async/concurrent type, which is Future. Almost all frameworks that deal with async/concurrency expose Future in some way, and its actually made Scala a pleasure to work with in this space (now if we can only get the same story for things like Json...) Obviously there are different approaches, one being Task (which is essentially a forced lazy Future), but I am now fairly convinced, that if you want a good concurrency model, your language needs to pick an established concurrency model to be used throughout the ecosystem
You should have a look at https://www.youtube.com/watch?v=449j7oKQVkc CSP in theory has come a long way, and it has a lot of potential to be as strong in concurrency as any pure functional solution
Although I have a rough understanding of what that snippet does (I think it appears to be concatenating various requests together. Since the person also appears to be doing RT logging he is forced to carry around the WriterT logging statement). Thing is, I don't **really** understand if thats the case. Moreso, I would definitely would not want this to be in the codebase. Its not documented (I don't know if this is intentional, maybe in the actual author's codebase it is documented). I think the biggest thing is, if you didn't have the original author, you basically wouldn't be able to work with that code at all. Also any slight change with Scalaz or how things worked mean it would probably be quite brittle
Yep. The surprising thing, I guess, in retrospect is that one quite simple alternative, [monad coproducts](http://www.informatik.uni-bremen.de/~cxl/papers/icfp02.pdf), dates to 2002, but only within the last year has it really surfaced even in the Haskell community—and it's even arguable that the Scala community got there first.
In Go'land, this apparently is on of their least concerns. Also iirc, Go's official stance is that they aren't against generics, they just don't like the design behind Generic's for a lot of modern languages, and so they wan't to "wait and see" what the community comes up with. I suspect it might be typesafe macro's similar to C++, but a lot simpler, but who knows
&gt;you basically wouldn't be able to work with that code at all. Not sure I understand why. This is the beauty of RT code. If I don't understand what the method is doing, I know it's not interfering with *my* stuff, it's just doing some transformation on the types. And I can see that quite clearly. 
&gt; Not sure I understand why. This is the beauty of RT code. If I don't understand what the method is doing, If you have code lying around that is essentially "magic" (because no one apart from one single person knows what it does), its actually a big problem. Arguably much bigger than it "not effecting the rest of the program" &gt;I know it's not interfering with my stuff, it's just doing some transformation on the types. And I can see that quite clearly. This is very rarely a problem, and furthermore you can design code this way without being RT (or enforcing it in the type system). Actually this kind of thought process is actually what guides many LISP's, its what they call horizontal or flat scaling (that is, composing your program of many small parts that do one job, and they don't interfere with eachother). LISP is hardly a RT language (or one that enforces it).
&gt; Well, then you are one of the lucky few, because I can't parse that and I consider myself a Scala expert. This statement illustrates the real problem here, hubris.
IMO this is the actual tragedy: discovering that you have two (or more) engineering cultures, and being forced to anger and maybe lose half your developers, or to establish a set of compromise principles that don't result in clear winners or losers, but immiserate _everyone_ to some extent.
&gt; Obviously there are different approaches, one being Task (which is essentially a forced lazy Future), but I am now fairly convinced, that if you want a good concurrency model, your language needs to pick an established concurrency model to be used throughout the ecosystem Good point. As much as I appreciate [`Task`](http://timperrett.com/2014/07/20/scalaz-task-the-missing-documentation/), it does seem like every project we develop sprouts a `fromScalaFuture()` to lift stuff from APIs we don't control into `Task`. It would be better to have the language implement a good process calculus and be consistent about it.
Ahhh, okay I see. Yea, I'm not sure why it does that, but my version sets the proper type of A, are you trying to something different or would my method work all the same?
But where does the request variable actually coming from? I am reading the source code and it seems like first, the code is wrapped as a block and passed into the first apply final def apply(block: R[AnyContent] =&gt; Result): Action[AnyContent] = apply(BodyParsers.parse.anyContent)(block) // I am not sure where this BodyParsers.parse.anyContent is passed from and then the following apply is called final def apply[A](bodyParser: BodyParser[A])(block: R[A] =&gt; Result): Action[A] = async(bodyParser) { req: R[A] =&gt; block(req) match { case simple: SimpleResult =&gt; Future.successful(simple) case async: AsyncResult =&gt; async.unflatten } } // I am not sure where req is coming from too I think I am missing a step where some function pass the request into the apply 
This +1000 If something doesn't work in your Scala code, and you come across this function (because its being called somewhere down the line), then well, good luck. If I was the team leader, than I would be calling you in the middle of the night and getting you to fix it, not because I like being a d****, but because you are the only person that wrote it, so you are the only one that really understands it and so you are the only one that can fix it In our company, its unlikely that something like that would have even ended up in our code after CR. If the rest of your company can read and understand that code, then great! However using this style when its clear that very few other programmers understand it, is just going to create a divide, which creates the same problems the author is talking about
&gt;Just because it didn't alter the rest of the system doesn't mean it isn't going to do something surprising. This doesn't compute, can you be more specific?
&gt; What's surprising is the expectation that literally any piece of code, in any language, should be understandable by literally any "experienced" developer, at any time, without any study of the library being used. That seems... optimistic, to put it mildly. Typically business level code that is part of your application should be understandable by (ideally) most of your software developers. There is obviously argument to have really strong abstractions which people don't understand, in things like compilers, but there is a very big difference between seeing something like that for basic application code like routing, vs it being in a compiler There needs to be justification for the level of abstractness you use, and in this case, it really looks like overkill.
That's s a strawman argument. The expectation is that code should be understandable by people who know the problem domain.
&gt; But we're not talking about code, we're talking about Monad Transformers, which are a mathematical concept. No one is suggesting that the encoding the Monad Transformer was done in a way that's difficult to understand Honestly you are being pedantic at this point, and there isn't a complete disjunction between the two. &gt; they're rejecting the idea of Monad Transformers, dismissing them as if they're voodoo math and not real. I don't think anyone is claiming that, this seems like a strawman. I think people are claiming that certain abstract concepts are either 1. Overly complex 2. Overkill for the problem 3. Not suited for the problem they are trying to solve When people say magic, they are not saying that there is literally Harry Potter that is waving some wand which does the work. &gt; I do, which is why i decide to communicate using peer reviewed, universally understood principles of mathematics. Sure, if you have a team that understands this, then thats fine. I said its a problem if you don't have a clear sizeable size of your team that understands these concepts, and you are aware of the pitfalls of the approach, which is what this article demonstrates &gt; This is patently false, as demonstrated by the mere existence of libraries like scalaz, or fs2, or agronaut, which many people make great use of. I don't think you understand what I said. I am claiming that if you are doing a PHD in the area of category theory, then obviously you need to verse yourself in this area &gt;It literally is, Software engineering may include considerations about the economics of software, but Computer Science is just pure math. Quote from your article &gt; A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[26] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[27] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).[28] So not, it literally isn't what you said. There is one approach (the rationalist) which says that, out of 3 other approaches. &gt; The topic is not so complex where you need a whole semester's course on the subject, to make use of it while writing software. Not that whatever is taught is in university reflects what is practical to begin with. You don't have time in your undergrad for a course in every system of formal logic, that doesn't mean you or others don't benefit from using those concepts in software. Sure, but that doesn't mean there is a limit to which point such knowledge ends up having diminishing returns. As a crude example, Doctors need to have *some* knowledge of biology to be able to able to do their work effectively (or call themselves doctors), but they do not need to know everything (and in such detail) as a biologist does. It seems to me, that in general, you are claiming that to be a computer scientist you need to have a very good understanding in Category Theory/Abstract Algebra. That would probably disqualify around 95% of current computer scientists
&gt; Generics are inherently "complicated" and I'm fairly sure there isn't a Go Way to implement them. Oh agreed, which is why its not in Go, because they argue that the current representation of generics are complicated (and its true that they are). Thats why I think that Go will probably end up implementing some really light form of generics, or something (like macros) which solves the problem in a different way. Whether this will actually happen, and is possible, is another question A good macro system actually wouldn't be a bad way to solve this problem, macros can be quite simple if the semantics of the underlying language are very simple (which Go is)
This is a great page, I think the updates came out of the ScalaX hack day. @stonerbobo, I would recommend scoverage or ensime. Scoverage could really use some love. I'm bias towards Ensime ;)
&gt; That's just dismissing a prior knowledge requirement. The amount of knowledge a person can obtain is not infinite, nor does it give linear returns for productivity. There are certainly levels of knowledge that are deemed acceptable for the knowledge at hand &gt; There no specific problem given, so any attempt to assert this is just hand waving. It was possible to deduce the context of that function (it does appear to be concatenating requests). The point I was making is that its definitely possible to be overkill on abstractions for the problem in question. As a personal example, I think using a `WriterT` for logging is overkill 
&gt; It is literally what I said, the quote above, which you grabbed is from the description of philosophies of computer science, and not computer science itself. The areas of computer science, both applied and theoretical are math based. You're either being willfully deceptive, or have serious challenges in reading comprehension to come to your conclusion. Saying "this is literally what I said" and then missing the other 2/3rds which change the context of what is being said is the definition of deceptive, its also logically fallacious (its called cherry picking). The overall quote states that computer science is a combination of many approaches, **one of which is mathematics**, but it also includes other areas, such as engineering. Heck it can even use law (IP is a good example) &gt; No, I'm arguing when confronted with an application of Category Theory/Abstract Algebra, you shouldn't dismiss those applications as invalid because just because you aren't immediately familiar with that field. No, but it can be dismissed on a lot of other grounds, which are entirely reasonable, and have been given. You made a statement implying that computer scientists aren't really computer scientists if they aren't well versed in Category Theory/Abstract Algebra, which is what I have a problem with (mainly because from what I see, it isn't true, unless you are majoring in that specific field, which is obvious) &gt; To use your poor doctor analogy, a surgical oncologists doesn't get to ignore the benefits of radiation therapy, because he never learned much about electromagnetic physics in med school. Yes, but you argued that surgical oncologists aren't really surgical oncologists unless they have a similar level of understanding of a theoretical physicist. Furthermore the application of radiation theory is quite selective depending on the type of work the surgical oncologist does. The knowledge required to write/maintain a full blown compiler is very different to the knowledge required to write a backend webserver, or to do design work. Your broad statement implied that you need to know Category Theory/Abstract Algebra for all of these areas, which frankly I find ridiculous
I think this talk from ScalaDays 2015 is spot on this topic : https://www.parleys.com/tutorial/scala-needs-you
&gt; You made a statement implying that computer scientists aren't really computer scientists if they aren't well versed in Category Theory/Abstract Algebra Let me try one more time. I'm arguing when confronted with an application of Category Theory/Abstract Algebra, you shouldn't dismiss those applications as invalid because just because you aren't immediately familiar with that field. &gt;Yes, but you argued that surgical oncologists aren't really surgical oncologists unless they have a similar level of understanding of a theoretical physicist. Is there some half the of conversation I'm missing? &gt; To use your poor doctor analogy, a surgical oncologists doesn't get to ignore the benefits of radiation therapy, because he never learned much about electromagnetic physics in med school. THat quote doesn't match up with what you're refuting at all. &gt; Your broad statement implied that you need to know Category Theory/Abstract Algebra for all of these areas, which frankly I find ridiculous Where are you pulling these fictional arguments from? You can (and in the case of working with the Scala community, often do) encounter these things. Once you're working with them you can't just say "Too much math, I don't want to learn any this, you must instead adapt to my model of computation instead."
I'm a newcomer to Scala, but the 2 concurrency tools that seem to be used most are Akka and Future. (Though obviously Akka is more than just concurrency.) Are there other popular concurrency frameworks or libraries? 
On the other hand it is also as good as it makes code (as universally as possible) more readable, maintainable and easy to integrate and reuse.
&gt; you don't need to use scalaz ... but you should use it anyways Ahem, no. 
&gt; That's s a strawman argument. The expectation is that code should be understandable by people who know the problem domain. Bt that wasn't the question. The question was... &gt; What factors leads an experienced programmer to think it's OK to not be able to understand the _implementation_ of a library the team is using? Maybe the phraseology doesn't actually convey the intent of the question, but it's all I have to work with.
&gt; Typically business level code that is part of your application should be understandable by (ideally) most of your software developers. I certainly agree with this, which is why I've said, several times, that introducing scalaz, _or any other library that's likely to be unfamiliar_, without teaching the team about it, then taking vacation and being unavailable for questions, is somewhere on the spectrum from irresponsible to unethical. &gt; There is obviously argument to have really strong abstractions which people don't understand, in things like compilers, but there is a very big difference between seeing something like that for basic application code like routing, vs it being in a compiler I think we need a better word than "abstractness" here. There's nothing especially abstract about `WriterT`, and in fact my reaction to seeing the code was first to roll my eyes, not because what's being done is very abstract—it's not—but because this very basic, very _concrete_ thing—logging requests—is being expressed in such byzantine terms. I gather this was a few years back, so they couldn't take advantage of [http4s](http://http4s.org) and [Journal](http://oncue.github.io/journal/): import journal._ import scalaz.stream._ val logger = Logger[Foo] val debugLog = sink.lift(msg =&gt; Task.delay(logger.debug(msg))) // Make your model safe and streaming by using a scalaz-stream Process def getData(req: Request): Process[Task, String] = Process(...).observe(debugLog)... val service = HttpService { // Wire your data into your service case req@GET -&gt; Root / "streaming" =&gt; Ok(getData(req)) // You can use helpers to send any type of data with an available EntityEncoder[T] case GET -&gt; Root / "synchronous" =&gt; Ok("This is good to go right now.") } Still non-blocking; still referentially transparent; just as "abstract" (`Task` monad, scalaz-stream `Process` monad, "`sink`" as an effectful target to `.observe` a `Process` with); but I'd bet good money 90% of people (rightly) going "Huh?" at `WriterT` and `:++&gt;&gt;` can make sense of it.
&gt; It's 100% possible and expected in Go that a reasonably experienced developer should be able to look at any code written by anyone and tell what it is doing. Right: because Go offers no meaningful abstraction-building power beyond functions, and even those aren't guaranteed to compose. It's a _joke_. I mean, yes, statcally-linked binaries + CSP + good Linux syscall bindings gets you a Docker, but so would [OCaml with musl libc and static linking](https://github.com/ocaml/opam-repository/tree/master/compilers/4.02.3/4.02.3%2Bmusl%2Bstatic), [Async](https://realworldocaml.org/v1/en/html/concurrent-programming-with-async.html), and [ctypes](https://github.com/ocamllabs/ocaml-ctypes) without having to use a nerf language.
I would say that `Task` is actually strict, not lazy, where possible. But it is a description of something to run, so the code to execute is composed as a data structure, and then execution is a different activity. I don't see how this is not idiomatic Scala (unless idiomatic is defined as "the way Future does it"); I'll point at Akka Streams as another library that follows a separation between description and execution. For dealing with ExecutionContexts, `Task.apply`and `Task.fork` tend to be sufficient for my purposes, when I can reasonably hard-code the pool I want a subsection of the execution graph to be run in. If you want to select a number of pools dynamically on each call, yes it's more complicated, but less common, and I'm not sure `Future`'s implicit approach improves things that much either.
Sorry for the slow response, was away. Great answer. Exactly what I was looking for, thank you.
Sorry for the slow response, but I do appreciate your input. This sort of explanation can be very useful especially for someone new to a language and I appreciate it.
Just got to this thread. This looks like it would retry everything upon any failure and that you would only be aware of one of the failures. What would it take to surface all failures (maybe simply add some logging in the individual task before failing with an exception?) and to retry only the failed tasks?
The design pattern is useful specifically *because* Scala does not have type classes. It's kind of like Java's sigleton design pattern is useful in Java only because Java doesn't have support for singletons. In Scala, it would be silly to use the the Java singleton pattern for everything. I would be similarly silly to translate Scala code to Java by converting every `object` into a singleton pattern. That is because even though the singleton pattern is a common, valid, and useful design pattern in Java, it is not a feature of the language, and one should take that into account when translating code from a language where it is.
"does it kind of wrong" - I don't know if I agree, but I use a small subset of it's functionality. Agreed on the fake separation. I think using DI for providing config values is overkill; I've never wanted to stub out config values, and if I did, then I just passed them via parameters. Just use a config library like typesafe config and move on. 
I agree it is possible to encode Haskell type classes in Scala, much as it is also possible to encode SML functors in Haskell (given common Haskell extensions). That is a far cry from saying it should be done as a matter of course though. (or that they area feature of the language)
What you say is true but I don't see how it's a knock against scalaz
&gt; Let me try one more time. I'm arguing when confronted with an application of Category Theory/Abstract Algebra, you shouldn't dismiss those applications as invalid because just because you aren't immediately familiar with that field. I never did &gt; Where are you pulling these fictional arguments from? You can (and in the case of working with the Scala community, often do) encounter these things. Once you're working with them you can't just say "Too much math, I don't want to learn any this, you must instead adapt to my model of computation instead." Actually you can, you seem to be arguing from a duplicitous black and white point of view. Its not a question of maths vs no maths, its how much you need to know. Obviously in Scala, you do need to know more compared to other programming languages (and even that depends, there is a lot of Scala code out that is basically better Java) Your logic implies there is an infinite amount of knowledge that people can learn, and they have an infinite amount of time to do it, and they also the knowledge pays back dividends infinitely.
I really don't know what to tell you after linking to the paper from the Scala designers that introduced type constructor polymorphism to the language, complete with: &gt; Section 7 focusses on the integration with Scala’s implicits, which are used to encode Haskell’s type classes. Our extension lifts this encoding to type constructor classes. Furthermore, due to subtyping, Scala supports abstracting over type class contexts, so that the concept of a bounded monad can be expressed cleanly, which is not possible in (mainstream extensions of) Haskell. That's right: type constructor polymorphism is _better_ than typeclasses. Or when [this search](https://www.google.com/search?q=scala+typeclass&amp;oq=scala+typeclass&amp;aqs=chrome..69i57.6349j0j4&amp;sourceid=chrome&amp;es_sm=119&amp;ie=UTF-8#q=scala+typeclass&amp;start=0) yields lots of useful results in the first three pages at least, including [Simulacrum](https://github.com/mpilquist/simulacrum/). Now, if you want to make the case that Simulacrum solves a real problem, then we're in vehement agreement. :-)
&gt; I would say that Task is actually strict, not lazy, where possible. But it is a description of something to run, so the code to execute is composed as a data structure, and then execution is a different activity. I don't see how this is not idiomatic Scala (unless idiomatic is defined as "the way Future does it"); I'll point at Akka Streams as another library that follows a separation between description and execution. Idiomatic Scala uses the approach where you control the semantics of execution via `def`/`lazy val`/`val`, rather than defaulting to a specific execution model. To clarify, I am not saying that Task is un-idiomatic, just that Future is more idiomatic than Task &gt; If you want to select a number of pools dynamically on each call, yes it's more complicated, but less common, and I'm not sure Future's implicit approach improves things that much either. Yeah this is what I am talking about, it is less common, but when you get into scenarios where certain calls puts very heavy usage and you dont want it to effect other calls, you can seperate them out as resources when using Thread Pools
I am not really sure, which is probably why we are still stuck on monads. I think the problem is, that especially in the GUI/UX design space, you need to work with actual UX designers to see how they think ad reason when they design UX. I mean, even their workflow (and this is reflected in the very popular/good UI frameworks) is very dynamic and interactive, which is probably the opposite of the Haskell "work a lot to get it to compile and then run" Like things like FRP is what you would get if you asked a mathematician on how to do UI but the thing is, they generally aren't the best people to ask on how UI is done ;) I think if you got a really smart programmer, and a really good UX programmer, and got them to work together you would come up with an abstraction that actually models the thought process better than monads and monad transformers.
Yep, that's a great catch! I glossed over it to this point because, honestly, in a once-a-day batch process with a few megabytes in a few files and where things are generally pretty reliable it didn't seem worth pursuing, and I was already behind schedule thanks to wasting a couple of days with akka-camel (which, again, may be fine in a different context). But upon reflection, I really shouldn't have immediately reached for `Nondeterminism.gatherUnordered`. Instead, given a `List[Task[File]]`, I should probably use [`Task.reduceUnordered`](http://docs.typelevel.org/api/scalaz/nightly/index.html#scalaz.concurrent.Task$), which is what `gatherUnordered` is based on, to accumulate a partition of the `List` with failures on the left and successes on the right. Thanks!
&gt; I never did The article did, the article you're attempting to defend. &gt; Actually you can, you seem to be arguing from a duplicitous black and white point of view. Just because you say something doesn't make it true. &gt; Its not a question of maths vs no maths, its how much you need to know. That's not the argument being made. It is a particular area of math, which is relevant when programming in Scala and functional programming languages. &gt; Your logic implies there is an infinite amount of knowledge that people can learn, and they have an infinite amount of time to do it, and they also the knowledge pays back dividends infinitely. No it doesn't, this is an argument against some an irrational anti-intellectual aversion some people have against abstract algebra.
You may be right, but it might also be a function of how little exposure these people have had to those concepts. If you look at stuff like react, you can see it's built in a very functional way. Perhaps there will be more and more stuff built in a functional style, but lacking the purity of pure functional programming.
Yeah, I was wondering what you'd make of Journal not being referentially transparent. :-) And we're right there with you on `Task`/`Process`, but you know they're monads, right? ;-) I fully admit scalaz-stream's `sink` and `.observe` spoil me. Given a `Process[Task, A]` I can just `val s = sink.lift((a: A) =&gt; Task.delay(...a...))` and `.observe(s)` anywhere in the stream with no further thought required. And that's why all sorts of arguments that "scalaz is unusable voodoo" frustrate me.
fs2 (which is the next iteration of scalaz-stream) is going to be removing scalaz as a dependency (actually it appears that it will be dependency free, although there is a discussion if it will use some of cats, over here https://github.com/functional-streams-for-scala/fs2/issues/321). There is also another discussion here https://github.com/non/cats/issues/21 I think there is discussion in how concurrent would be used for cats. Note that the whole intention behind cats is for it to be modular, so the idea is that some idea library (which would implement concurrent) would use cats as a dependency. Its the same thing as collections, stuff like `Maybe` and `IList` which work with cats, exists, its just as a seperate non official module https://github.com/stew/dogs This is the ethos behind cats. Only implement the stuff that is really needed in the core (and make sure its ultra stable, safe and well documented) and let the community build modules ontop of it. That way you don't get into the problems that we had with Scalaz. Finally, obviously cats isn't there yet, there is a lot of code that needs to be redone, and then obviously migrated, but its definitely on their radar
&gt; You make claims about what you need to know in Scala to call yourself proficient, but you don't qualify them No, I haven't. The only claim I made to proficiency in Scala is that Category Theory isn't related, it's an entirely different domain. &gt; If your Haskell coders don't understand assembly, then "that is their problem" and they should learn assembly. It is. If there is a problem which can be easily solved in assembly but not in haskell, there's no reason you shouldn't use it. You only chose assembly in your example because there are few to no problems that can be solved in assembly versus haskell or any other higher level language. If you think Category Theory is useless in the same way you're patently wrong. A better analogy would if you're working a project to simulate physics, and you're familiar with newtonian mechanics. But other programmers instead use a relativistic model. Some problems are easier with the newtonian model, some problems are not. More people are immediately familiar with basics of newtonian physics, that doesn't make the relativistic model invalid, nor does it make it so that you can dismiss any programs using relative models, nor does it mean it's a good idea to stick your head in the sand and refuse to learn relativistic physics when you frequently encounter programs which use that model. &gt; Yes, and what is being said is the code provided made an incorrect assumption about its audience. No, it's not a incorrect assumption about the audience. You and I would deliberately choose to write for a different audience, and when you're not in the audience I choose to write for, it's not my fault. &gt; The people in the team did not understand the concepts being used Some people, according to the article only half. &gt;It may be idiomatic Scala code for people who use Scalaz all the time, but thats not the same thing as Scala in general. That's my point. They're two different domains.
I just realized, there is this proposal which recently came up in cats, which you may be interested in https://github.com/non/cats/issues/766 EDIT: nvm, just saw it was a simplified version of Eff, which you described below
Thank you for explaining the operator. I didn't mean to say that the function it provides is a monstrosity, just that the operator itself is hellishly ugly. I think it would be 1000x better as a function. Then at least it has a name, and you can google it, and you can talk about it with your coworkers. I can't even really imagine any benefit to being an operator, besides the gee whiz factor, because functions are so boring. And all of that is totally beside the point of what the operator actually does, which I can totally see as being very handy.
&gt; I also don't see anyone trying to emulate scalaz.Actors/scalaz.Future in cats (I think they are trying to make to interopt with Scala future, but I could be wrong) Well, here's current [fs2](https://github.com/functional-streams-for-scala/fs2/tree/topic/redesign/core/src/main/scala/fs2/internal) for you... :-)
It is literally "Where does the compiler find the implicit request in Play Action?" going by the title.
&gt; I have a philosophical problem with this line of reasoning, one which I'm unable to articulate clearly at the moment, I'll come back to this later. I'll take a crack at it. Some of us—those who espouse the _objective_ value of referential transparency and totality in programming—are essentially neo-Platonists. The truth is out there. We can expose people to it. If they reject it, they're fools. From this POV, the Curry-Howard Isomorphism is like the law of gravity: you're subject to it whether you apply it explicitly or not. Another point of view is that hysteresis, in the engineering sense, is inevitable, so pushing it outside the formal logical expression of intent (to the extent that's even recognized as a thing) is more effort than it's worth. You may want to use a model checker for an embedded or highly concurrent system. You want a really good test suite. There's a clear sense of professionalism, and some tools and techniques are better than others, but there's deep skepticism about idealistic crusading. Finally, there's the all-dynamic, all the time camp. The measure of a language's power is how much you can change at runtime. Reflection, monkeypatching, dependency injection, meta-object protocol, whatever, if it lets the programmer in at any layer of the code at all, it's good. There can be some overlap with the second camp, but the second camp will have one experience with two Python libraries both extending a base metaclass which can only have one subclass and say "never again" while the second will poke at all the objects, which are just `dict`s after all, and "make it work." I have, at one time or another, been in all three camps. I make no apology for choosing camp 1, very much deliberately.
since it can compile bytecode it should be possible, but I don't think its maintained anymore (or worth it)
Yes, the real world isn't the same as the ideal world... But I still find this kind of sad. I agree with what others have said that this is more of a people problem, letting some developers run away with a syntax that their organization couldn't cash... So then their solution was to dumb down their stack. That's sad. But hey, whatever works, right? I'm curious to see what kind of weaknesses/trade-offs have arisen due to the switch?
What is your experience level?
Oh yes, currently Dynamic doesn't support any of that kind of stuff. You'll have to use reflection to get any of that information.
I do now :) https://twitter.com/sinisalouc 
I don't believe so, GCJ is a Java *source* compiler, not a Java *bytecode* compiler. You would have to translate the Scala *source* to Java *source* for this to work, which would be less than ideal RoboVM compiles Java bytecode (which Scala produces) to machine code via LLVM. It is however closed source, and catered to IOS development.
But Scala's philosophy is to build complex features like typeclasses out of smaller, orthogonal features like traits and implicits and implicit search. That specifically is why Scala doesn't have a separate typeclass feature.
&gt; Then library users can use the word versions like x safeJoin y which makes my life much easier trying to read the code. You essentially already can: &gt; [`(f1 |@| f2 |@| ... |@| fn)((v1, v2, ... vn) =&gt; ...)` is an alternative to `Apply[F].applyN(f1, f2, ..., fn)((v1, v2, ... vn) =&gt; ...)`](https://github.com/scalaz/scalaz/blob/v7.1.6/core/src/main/scala/scalaz/syntax/ApplySyntax.scala#L20) Seriously, `safeJoin` reveals at least two problems: ambiguity and verbosity with chaining. "Join" isn't a good description of what an applicative functor does, or rather, to the extent it is, it's also a good description of what a Semigroup does, a Monoid does, a Monad does... 
Hah you can feed scala byte code to a Java decompiler but the source you get back isn't always 100% valid...
&gt; I agree simulacrum does look useful, but only because Scala doesn't have type classes. But you haven't addressed the fact that Scala has _type constructor polymorphism_, _which is more powerful than type classes_, _on purpose_, _from the language's designers_, _and lots of popular libraries use it_. Just saying "Scala doesn't have type classes" is either obtuse or intellectually dishonest. &gt; if you were to code in Scala as if it were Haskell, you would probably produce something like scalaz. :P Right: an extremely helpful, powerful library I use, professionally, literally every day. But as the search I posted before shows, _virtually all_ Scala libraries use type classes!
Oh relax, it never hurts to ask. And besides, this is a global sub without a huge readership. Any recruiter posting here who knows anything (I know, I know) should expect that most people reading their ad won't be from whatever city they're looking for.
It's very naïve to think that "This doesn't have side effects" is isomorphic to "This is easy to understand". Sometimes, side effecting code is easier to understand than referentially transparent one. 
&gt; Scala has type constructor polymorphism, which is more powerful than type classes I don't see how that's relevant. You're acting like I'm denying that it's possible to encode type classes in Scala, but I'm not, and I think I've been pretty clear about that. Did you even look at the README of the project you linked to? Here, let me copy pasta of the first section in case you're having trouble finding it: Type classes rock. Alas, their encoding in Scala requires a lot of boilerplate, which doesn't rock. There is inconsistency between projects, where type classes are encoded differently. There is inconsistency within projects, where object-oriented forwarders (aka. ops, syntax) accidentally differ in exact parameter lists or forwarders are missing where they are expected to be. Even in disciplined teams, the bike-shedding opportunities alone are a source of lost productivity. This project addresses these concerns by introducing first class support for type classes in Scala 2.11. What do you think the point of the project is if not to implement type classes in Scala? &gt; Right: an extremely helpful, powerful library I use, professionally, literally every day. I'll agree it's powerful, and I also use it professionally. How much of other people's scalaz code have you had to debug so far? &gt; But as the search I posted before shows, virtually all Scala libraries use type classes! Srsly?
Can you elaborate on the 30% own DSL FP language? Do you mean building a DSL with Scala or really writing your own full-blown FP language?
&gt;Sometimes, side effecting code is easier to understand than referentially transparent one. Why do you say this? 
My theorem is that you're an ass.
Because it's my opinion? 
Plenty of people find accumulating elements into a mutable list easier to follow than a `fold` or a `reduce`, for example. You can even do this inside a referentially transparent function. 
I take your point, but don't fully agree. Technically, if you've decided to embrace SQL for a project, and the project is larger than a very trivial app/service, inline SQL should be forbidden in any case (with a possible exception for very basic selects and inserts, and these should be abstracted into a very lightweight ORM if you choose to do that). Everything else should be using stored procs or views. Otherwise, the code will become extremely unmaintainable, extremely fast. 
Gladly-- so this individual would actually be building out the company's own "full-blown FP language", thus, having a broad understanding of parsers and compilers is important. Honestly though, it seems as though this may be more of a plus than it is a requirement from what I've heard from candidates that I've had interview. I believe that pared down they want someone very familiar with functional programming logic and Scala experience.
&gt; to find a senior resource Does "resource" mean "person", as in a human being?
There are multiple solutions that can turn java bytecode into a binary. And usually, it's not worth it. That would be even more so for scala, since you tend to have more objects due to immutability. 
A Sr. Scala role in Seattle needs to pay literally double your posted top end.
Lmfao 
Such dehumanizing language. I need a senior cog, plx!
[removed]
&gt; All of those are valid ways to represent type classes in Scala, but none of them are type classes. \*_shrug_\*. You're irredeemably hung up on syntax instead of semantics. When you contradict even the _language designers_ on the point, to say nothing of the staggering number of users of the language who use the term "type class" and know what's being referred to, there's really not much more to say—especially when you consider how GHC actually implements typeclasses! In any case, your original claim was: &gt; That is a far cry from saying it [encoding type classes in Scala] should be done as a matter of course though. But it is, even in the standard library, and yes, should be, in order to support the [open-world assumption](http://typeclassopedia.bitbucket.org/#slide-12). You've decided _a priori_ that using powerful language constructs to encode other possible ones is bad, and in a very selective way (encoding Scala to Java bytecode—fine; encoding something from Haskell to Scala—bad). You haven't even offered an argument as to _why_ they're bad. Meanwhile, the Scala community happily goes on using _type classes_ whether you think they're appropriate (or even type classes) or not. I mean, I don't really care. I'm just fascinated by this kind of divorced from reality, magic-word-based thinking.
I recommend passing dependencies as explicit arguments. This way the dependency is part of the type. You can abstract this away with a simple data type called Reader, or even with basic function types. I do not recommend using implicit parameters for passing dependencies because it is syntactic machinery that doesn't really buy you anything (it's not something you can abstract away). Rúnar gave a good [talk](https://www.youtube.com/watch?v=ZasXwtTRkio) about this a few years ago.
Give Excelsior JET a try. They have an evaluation version.
What do you do when you have trees like this: val sessionUrlService = InMemorySessionUrlService(config.sessionUrlServiceConfig) val databaseDeviceService = PostgresDeviceService(transactor, DeviceRepository, NetworkRepository, DeviceStateRepository) val databaseVideoManifestService = PostgresVideoManifestService(transactor, VideoManifestRepository) val webSocketService = JWebSocketService(config.webSocketConfig, databaseDeviceService, sessionUrlService) val pairingService = PairingService(transactor, PairingRepository, authClient, config.authConfig.serviceToken, config.authConfig.prefix) val httpApiServer = NvrHttpApi(config.httpConfig, authClient, databaseDeviceService, databaseVideoManifestService, webSocketService, pairingService, sessionUrlService) Just wear it? I personally don't mind it, but the Java guy in our team sees it and I can feel him itching for Spring/Guice.
I guess don't understand the question. Does your Java guy not like to construct values?
This is verbose because you're approaching DI from a Java perspective. Spring/Guice is always going to look better when you're stacking coarse grained modules. I would start by looking for ways to pass config in to individual functions instead of initialising entire classes. Usually only a few methods in a class need the config so I prefer to pass it direct. Then, instead of passing the config down through each of the functions, I make the functions themselves what you pass as a parameter instead. This is the key mindshift that you need to go through in order to let go of DI frameworks. Once you can get the hang of passing functions around, vast swathes of code become very simple as they simply have a dependency on a B =&gt; C function in order to do A =&gt; D and whether or not that is the real B =&gt; C database lookup function or a dummy, doesn't matter. Finally I would try and break `NvrHttpApi` into smaller pieces. It's a bit of a God module right now. If it literally doesn't do anything except wiring then fine, but it looks like it does HTTP stuff so that would be better off pulled out into _pure_ functions (as before).
The cake is _(now pretty widely understood to be)_ a lie. I wouldn't say that there is a big debate around runtime vs compile time DI. People who have been doing Scala longer generally prefer everything to be compile time. The debate I am aware of (such as there is one) is around Implicit Args vs Reader Monad vs Free Monad vs pure functions
We just wired everything up ourselves, passing deps as constructor arguments. Spring is atrocious, and Guice didn't do much except for adding low-value magic. We tried the cake pattern and found it really lacking. The Reader monad was cool when we tried it, but overly complex. 
As someone who works in finance in Houston, but probably not this company, I sorta find this fascinating - mostly glad to hear that other local finance companies are making use of Scala and functional programming, but... that also means we have more competition for hiring talent ;). Good luck!
That looks like a problem created by a java developer. That is, a java developer who now knows the perfect java framework to fix it.
This article takes a pretty good pragmatic approach: http://di-in-scala.github.io/ The TLDR is there's no one-size fits all. Pick the simplest one that is strong enough for your use-case. Also, don't make your dependency tree more complex than it needs to be. Rule of thumb: If your abstract class has one subclass, it shouldn't exist. 
&gt; I do not recommend using implicit parameters for passing dependencies because it is syntactic machinery that doesn't really buy you anything (it's not something you can abstract away). I don't get what you mean by this. Using an implicit parameter in this case is not about abstraction, but just about reducing boilerplate and repeated argument passing (unlike, for example, if you were passing type classes around). And you don't really lose generality, since in the cases where it is necessary you can still pass parameters explicitly.
You should try MacWire to reduce the boilerplace to sth like : lazy val sessionUrlService = wire[InMemorySessionUrlService] lazy val databaseDeviceService = wire[PostgresDeviceService] lazy val databaseVideoManifestService = wire[PostgresVideoManifestService] lazy val webSocketService = wire[JWebSocketService] lazy val pairingService = wire[PairingService] lazy val httpApiServer = wire[NvrHttpApi] Thanks to the macros system, it will generate a similar code than passing deps to constructor manually, and you will have compile time-check for missing dependencies
Having played DI frameworks in Python, I'm not sure these stand up. I'll concede the startup time if you're not using it long running processes (think web app vs cli tool). But debugging hasn't been an issue *yet*. I could see it if the framework was providing the same instance again and again instead of creating a fresh one (think web form objects). Again, that's coming from a Python perspective where DI frameworks and IOC containers aren't really mainstream and there's really good run time introspection (unsure about Scala's/JVM's run time introspection) 
I agree, using implicits when doing DI isn't about abstracting anything it's about reducing verbosity.
&gt; Is there something wrong with using something to pass dependencies in via the constructor? Are implicit values what should be used instead? These two things aren't mutually exclusive. &gt; With that, doesn't it get difficult to understand where the implicit values originate? I see this meme often, and I can never understand why it gets repeated so often, if you want to know what implicit value the compiler has chosen, just ask the compiler, either with your IDE if you're using one, or `-X-log-implicits`. The tooling exists to make that a non-issue.
If nothing can go wrong in subsequent sections of code then what you're saying makes a lot of sense. I've found that errors can turn up most places, however, so I usually end up returning some sort of disjunction nearly everywhere.
Quill now supports updates and inserts using case class instances: https://github.com/getquill/quill/blob/master/CHANGELOG.md#020--24-dec-2015
implicit FUD is everywhere. Because it uses the literal word "implicit" as a keyword, I've noticed many unfamiliar people can't help but kneejerk fear it due to the old "prefer to be explicit over implicit" truism along with flashbacks of C implicit casts.
There are basically two ways to do that: - Dependency passing (which is what /u/tpolecat is recommending) - Dependency injection Dependency passing has the advantage of maintaining referential transparency but it comes with costs: - Very verbose. Now, all your methods need to be passed explicitly all their dependencies. - Impacts callers and everyone using your functions (e.g. instead of returning `Int`, you now return `Reader[C, Int]` everywhere and your callers need to update their code). You also find yourself having to flatMap all the things instead of just reading an `Int` and you need to use monad transformers when you start mixing all these monads (which don't compose). - Dependency passing exposes the internals of your function. For example, if one day you decide you need a logger in a function, that logger becomes a part of your signature, so you're exposing all your internal details. Dependency injection breaks referential transparency but it addresses all these problems. Your signatures remain clean, they don't expose any internal details and when a function returns an `Int`, it's what your callers deal with, not a container. There are quite a few Scala DI frameworks: MacWire, Sindi, Subcut, etc... I've stuck to Guice so far because I need assisted injection but your mileage may vary. 
Oh.. well, in that case thanks for the heads-up. I'll take a look into it these days, feel free to post some sources if you have.
Compile time DI mechanisms are also provided by Play 2.4 (c.f. http://loicdescotte.github.io/posts/play24-compile-time-di)
+1, now, implicit conversion from `query[T]` =&gt; `T` is going to be a real nice-to-have wrt to attracting users, IMO. Once you go beyond the simple case, `query[..]` becomes syntactic clutter. Without it you're getting close to the metal; i.e SQL itself. Haven't had a chance to sit down with Quill (forked it), am looking forward to checking out. Really looks quite appealing -- hope Quill gets some traction in-house at Twitter as well.
So you can't differentiate between asking if something is possible and lecturing people that it should be possible?
&gt; You have the “it’s a better java ” camp you have the “I (heart) Applicative Functors” camp. Really? I love functional programming and don't know what an applicative functor is. The same goes for people I see using Spark.
THREE spaces, what the fuck?
Why would anyone use anything other than tabs? It's what they're for.
I've seen three space indent in *really* old Perl code. 
Right, and in my experience this is not a worthwhile goal. Abstraction is always superior to machinery.
Magnets are a bit more complicated than the simple implicit params that would be used to replace spring-like DI. You would declare an implicit param of type, say, Logger. If you forget to wire it up and it's missing, the compiler will be very helpful and let you know exactly that.
That was pretty common back in the days of fixed 80-char wide screens. In MSDOS it was popularized by Turbo Pascal IIRC
Not sure why you're being downvoted, it's an innocent enough question after all. The fact is that the coding scene is mired in weird legacies and deep-set practices that don't necessarily have one single explanation, but I'll try my best to pose a solid argument. A lot of developers, especially those who work primarily in open-source projects, prefer spaces because spaces offer control over how their code looks. Spaces are a monospace character while tabs are not. Tabs can look different in different editors while spaces always take up one character-width in any sane configuration. This means your space-indented code looks largely the same when viewed in a browser, in notepad, or in an IDE while your tab-indented code does not
I worked for a company using three spaces (for PHP); how come they's started using three spaces was a popular theme over a pint — apparently, it wasn't a compromise but rather a mistake; someone authoritative enough believed that he saw a code style guide advocating 3 spaces, and since noone cared enough to challenge that, it started this way. Later on, the same person admitted he wasn't able to find that code style guide or indeed any source of that brilliant idea; still, as the codebase was already noticeable, noone cared enough to change to anything more conventional, either. 
That's basically it; the conversation always boils down to "I want it to look like I like it" vs "I want it to look uniform". I don't like tabs, but in a language like `go` where the formatting is done for you, I don't have an issue with it. I'm a compulsive 'reformat the file' key-hitter in my IDE's anyway, so as long as everyone uses the same configuration of how to indent, I'm happy letting the machine do that for me, and I'll adjust my code to fit that.
To expand on that, you cannot write a method that abstracts over things that take an `implicit Woozle`, nor can you compose two such things. But with an abstraction as simple as `Woozle =&gt; A` you can do both, trivially. My claim is that this is useful.
The reason why &gt; Slick may be complex, but once you understand how it works, its basically writing your SQL with typed Scala collections, versus writing your queries as Strings. (same applies for jOOQ) is precisely to avoid that. Though it hasn't always achieved the ambition, things have been getting better.
Judging by 4%+1% in C/C++ (with 0% in every language other than these two), maybe that code guide you mention did exist after all.
So if I read this correctly, you end up with a result that is either a list of all successful files (the usual case) or a NEL of all failure reasons (which could be retried or logged or whatever), yes? Neat.
Well the idea is that the number of tabs represents the levels of indentation, so if there are three tabs at the beginning of the line you're three levels deep in the control flow. You can't have half a tab, but you can have something indented a number of spaces that is not a multiple of whatever convention has been decided as the number of spaces per indent level. Usually it's not a huge deal unless it is (i.e. in Python) It basically comes down to tabs having an actual semantic meaning aside from "how it looks", although one other benefit is that you can control how wide the indentation appears on your screen. The downside is of course that you do not control how it will appear elsewhere - i.e. when doing code reviews, the web-based tool I have to use indents tabs to 8 unconfigurable freaking spaces. FWIW I was always "spaces only" before I started at my current job where the convention is tabs, so it's at least possible to be converted. 
I know I'm a little late but I just wanted to point out that this is extremely insightful and you're completely spot on.
Does it do anything new in terms of, you know, documentation? Or is this just a style makeover?
Btw, Felix is also working on the mobile usability of Scaladoc. Current progress: https://github.com/scala/scala/pull/4888
I'm sure they will start off with simple three-line scripts that forfeits type annotations, methods and stuff, like println("what is your name?") val name = readLine() println(s"hello, $name!") and take it from there. Not too bad at all, it's almost as simple as Python. I'd argue that makes for a quicker introduction than a full-blown class like Java more or less enforces. edit: and besides, you can do monads in most other languages too. It's not limited to Scala :)
Frames without dividers is pretty horrendous. 
Maybe cross post to /r/design ?
&gt; Now we will have the coolest introductory programming course in Sweden”, Who care if the class is cool, the real question is: is it a good introduction to programming? The only vaguely positive thing I read in this article is that students joining the program might already know Java (but then they're no longer beginners) but otherwise, if feels to me like the decision was made more because the committee has a personal liking for Scala than in the interest of students. Time will tell I guess. 
Great plan, as long as you don't need any libraries. I'm the author of the "criticism" section of the Wikipedia page, and one of the criticisms I cited was that setting coding standards for a team new to Scala to be a simple subset of the language gets thwarted as soon as you bring in a library that uses complex language features.
Now that's a reasonable argument, but it's mentioned nowhere in the article. Actually, they double down on OOP in that very article: &gt; “This fits well with LTH’s strong tradition of research within object-oriented language technology.” In contrast, FP is barely mentioned and only to describe what Scala is. 
I've read a few articles in Swedish about this change, and the combination of fp and oop has been mentioned as a major argument before. But you are right, they do not state it in this article. Weird, I think it is the strongest argument.
I'd rather skip whatever client-side tooling Scala frameworks have to offer altogether and just stick to JavaScript tools current breed of front-end developers use.
I'd say Scala shines when there's complex business logic involved, but learning just a little bit of Scala that's enough to use Finatra could be a worthy investment of time and effort.
Why is an (explicit) compilation step important for a beginner? And why is a static type system better for a beginner? Imho the most important aspect is a **REPL**!
The problem with Scala as an introductory course is how there are so many ways to approach the problem. As an introductory course, students will be overwhelmed with traits, inheritance, implicits, etc. etc. Tbh, I think even Haskell works better in an introductory course, but I myself am partial to Scheme.
&gt; Static type system is good, because it teaches them types. I think you mix up static/dynamic with strong/weak 😉
I think that when it comes to teaching basic datatypes: static &gt; strong dynamic &gt; weak dynamic If a language has dynamic typing, it's easier to avoid its numeric tower, which may confuse students sooner or later. Stapling labels "this is an int, and this is a float" makes everything clearer.
Tbh, Scala doesn't really fulfil those requirements either. Python probably ticks off a lot more of those than Scala.
And my point is Scala can't really be argued to be better than Scala for academic purposes either. There are more universities that teach introduction to CS with Haskell than Scala. With regards to my comment about Python, if those were the main things needed for a good academic language, Python would win hands down.
&gt; Understanding that a static type system is your friend and not your enemy is an important lesson. Depends on the language 😈 Really, I think a **strong** type system is your friend - and you can learn that in both variants.
One of the key benefits of a static type system is to make it impossible to run certain kinds of invalid program, i.e. it catches many errors before you even run the program. This is something that is worth learning, and something that aids learning. Thinking about what your code is doing before you run it is an invaluable skill to learn, and anything that encourages you to do this, such as compile-time type-checking, is a good thing.
Cool, thanks! I only want to POST some xml documents once everything is deployed and fully in production. I don't want to POST anything when doing testing, probably just save it to a text file or print to the terminal.
I think you mostly got my second question. What I'm doing is taking in different json documents, converting the information to an object that I can send to different receivers, and then creating xml documents from the objects that will be POSTed to different urls. The xml creation and the POST process are within the same module package. I pretty much just want be able to do some testing that is identical to production, but stops after the xml document is created; not POSTing the document.
You have a few options in scala, like the cake pattern or (manual) dependency injection. http://di-in-scala.github.io/ http://www.cakesolutions.net/teamblogs/2011/12/19/cake-pattern-in-depth https://blog.8thlight.com/paul-pagel/2006/09/11/self-shunt.html https://dzone.com/articles/practical-php-testing/practical-php-testing-patterns-38 You can find some more reading material if you search for unit testing components and how to unit test services and stuff like that. 
I think you're right and I do think the criticism is fair (the reference to Yammer is muddying the waters a bit) but I also believe that they won't go that deep in an introductory course. Most likely they will stick with the standard library which does contain *some* arcane symbols but nothing too bad. One aspect that's often overlooked with Scala is that it unifies operations on data structures in an arguably cleaner way than say Java. The notion of seeing collections as [partial] functions may seem like a novelty at first but makes sense the more you think of it.
Introductory courses tend not to use external libraries except those endorsed by the course coordinator. In fact courses in programming in general tend not to require students to use anything but preselected libraries. Project courses do give that freedom but by then most students have completed both introductory and intermediate programming courses. This is my experience at least. 
&gt; It's better to teach those elementary data types and their behaviour as early as possible I don't see why. Again, we're talking about complete beginners. You don't teach skiing to beginners by dropping them on a black diamond. I bet most of us in this thread still had no idea what a type was years after starting to write code. It's really something that can be learned later. 
And you're purposely ignoring what I said, if those list of requirements were all that's needed for a good academic language, Python would fit the bill perfectly. But obviously that list of requirements is only part of what makes a good academic language.
I'd like to know more about the curriculum. While I do believe the language of pedagogy is important, I believe the curriculum is orders more important. 
The type system needs to be **strong** in order to enable a compiler in a statically typed langauge to save you from type related errors! I do not think we must discuss the benefits of static vs dynamic typing here! That has been done over and over again by so many genious brains, that we could hardly add something valuable to the different positions. Imho both approaches have benefits on their own. The key point is, that a type system needs to be **strong** in order to help a developer - nevertheless it could be static or dynamic. And as we talk about bloody beginners: You need a language with a **REPL**! That will hide the compilation step(s) anyway, but will show you errors. Type errors can be helpfull, but as the complexity and the length of beginners code is rather low, I doubt that it makes a difference whether he thinks about compiler or runtime errors! A beginner will struggle with simple syntax and semantics - things that every compiler will catch. To solve type problems it doesn't matter for simple programs if the compiler complains or there is a runtime error.
I know I'm a little late to the party, but why would you think GCJ generated code would be slower than JVM (javac)? AFAIK javac doesn't do any optimizations (except constant folding), it's pretty much just translation from Java to bytecode. Java optimizations are on the JVM, not on the compiler itself. I could be wrong, though.
Will they remove the 22-sized limitation in tuples?
I think there are plans for that, but not in 2.12: http://www.scala-lang.org/news/roadmap-next/ (See 'Scala "Don Giovanni"') &gt; Tuples can be decomposed recursively, overcoming current limits to tuple size, and leading to simpler, streamlined native support for abstractions like `HLists` or `HMaps` which are currently implemented in some form or other in various libraries.
I commend you for suggesting the simpler path of just passing arguments. I believe too many Scala developers constantly try to rationalize fancier methods (e.g. Cake Pattern, dependency injection frameworks, etc.). Effective Scala, to me, is an exercise in restraint. &gt;this can easily be done at a type level with a tagged union(you don't need to use scalaz, the definition of Tagged is trivial, but you should use it anyways). Damn, you were so close... case class Greeting(value: String) class OfficialMessageService(greeting: Greeting) extends MessageService { def greet = greeting.value } val greeting = Greeting("Hello") val myMessageService = new OfficialMessageService(greeting) No operator overloading, no implicit resolution, no 3rd party library. Just as succinct, expressive and type safe. Problem solved - on to the next one. 
Tuple, Function, Case Class fields are all limited to 22, given the underlying implementations are nothing more than a pile of boilerplate for Tuple2 through Tuple22. Will the new implementation simply generate these via macros at compile time? It's unclear how I can write code against a Tuple433 if it doesn't yet exist. This isn't that much of a problem, but in frameworks like Play! you can't have 23 params in a route, for example, for this reason. 
Wrong actually. Kernel uses _tabs_, but it is recommended to set 8 as the tab width. Just look at some sources: https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/ Or check the quote: &gt; Outside of comments, documentation and except in Kconfig, spaces are never used for indentation, and the above example is deliberately broken.
From the article: **Beginner languages for computer engineering students at Lund University** * Algol 1982 * Pascal 1985 * Simula 1990 * Java 1997 * Scala 2016 So these beginner languages lasted 3, 5, 7 and 9 years. Scala may stay until 2027.
You're right.
Interesting read. I'm still wondering... &gt; This DSL requires the target algebra be CloudFilesF. We can generalize this in the same way we generalized the interpreters: by requiring the target algebra be at least as powerful as CloudFilesF. Somehow I get the sense this is just a very cryptic way (clean looks but complicated in the number of abstractions involved) to attempt to do with FP what sub-typing and OO-composition (cake, mixin etc.) is doing anyway in a language such as Scala (I don't know why it's posted in /r/scala as basically all code is Haskell).
The author calls a subroutine "purely functional" when it has (by my count) seven side-effects and its output depends on the whether the network cable is connected.
I'm really confused. How is constructing the array a side-effect? What data am I touching that I'm not supposed to be?
`buffer.append(transform(d))` is side-effecting code - it is changing the state of the mutable `buffer` by adding to it each time it is called. You can avoid this by using immutable data structures like `Vector` or `List`. The methods themselves (`transformData` and `readInFile`) are not side-effecting, because they don't modify state outside their scope, but the for loops they contain do. 
The term *side-effecting* is used to refer to any function or expression that changes state. I don't think you are misunderstanding anything, just that there are multiple scopes to talk about. Like I said, your original code didn't modify state outside of the methods, but they did use `buffer.append(...)` internally that modifies the methods' *local* state. 
I believe that the end state of OO and FP is the same. See, for example, this excellent blog post which discusses similar issues: https://oleksandrmanzyuk.wordpress.com/2014/06/18/from-object-algebras-to-finally-tagless-interpreters-2/ The "finally tagless" approach is what FP programmers call OO so their FP friends don't laugh at them. (More formally, there are initial algebras, which is what the free monad uses, and their dual, final coalgebras, which is what the finally tagless approach is using. No, I don't *really* understand what this all means.) The finally tagless approach solves the same problems as John's blog post though the implementation is different. As the blog post shows it's the same stuff as the OO world has developed. So this is very cool. OO is FP. The dark side is the light side. There is no disharmony in the force. We are all one.
Okay, I got it. Thanks for your help.
&gt; The bigger problem is that you are exposing a mutable structure (Array) to the rest of the world. Thank you; point well taken. I'll look into `foldLeft`, etc.
How is it side-effecting? It is a mutable code, but the `buffer.append` does not modify neither outside scope, nor the parameters. No matter how many times you execute tansformData with the same parameters, it will always yield the same result (if `data` parameter is also side-effect free, of course).
I wonder if I could trouble you (or anyone else, for that matter) with a variation on my original code. I've incorporated the suggestions and have tried to combine some OOP principles. (Since Scala is multi-pardigmatic.) I'm wondering if this improves the code or not, from the point of view of good Scala practice. I've created a Trait with the idea that an implementing class could act to constrain the relationship between the type of data being used and the function acting on it. In this way I've simplified the signature for `transformData` and at the same time perhaps headed off a potential problem with a mismatch between the function passed as the second parameter to `transformData` and the data itself, passed as the third parameter. Better or worse? -------------- *Edit:* This is an over-engineered, toy example. I am over-engineering in order to get a feel for larger programs. import scala.io.Source import scala.language.reflectiveCalls object Sample2IO { /** * Model data from some source. */ trait Data { def getData(): Vector[String] } /** * Model data from a file on the file system. */ class FileData(source: String) extends Data { def getData(): Vector[String] = { Source.fromFile(source).getLines().toVector } } /** * Return Vector of transformed elements of data. */ def transformData(transform: String =&gt; String, data: Data): Vector[String] = { data.getData().map(transform) } /** * Return String transformed to upper-case. */ def transformCase(input: String): String = { input.toUpperCase } /** * Transform wordlist. */ def main(args: Array[String]): Unit = { val path = if (args.length &gt; 0) args(0) else "wordlist.txt" transformData(transformCase, new FileData(path)).foreach(println) } }
My general understanding is that SQL projects of non trivial size are not just stored procs or views, they pretty much contain everything. At least in our case, every type of join in SQL (which probably is most of your business logic) has been able to be expressed in Slick. If we were using MySQL, there may have been a performance issue with 3.0 (fixed in 3.1), but thankfully we are using postgres Views/procs can be mapped to Scala data types using Slick you have to manually write these for now, but its not the end of the world. I am not sure what you mean by maintainability, like 95% of our SQL code is using Slick, and we are only using views in a few areas (specifically aggregated stats, which you need to use views for). Our code is extremely maintainable, because as has been mentioned before, Slick treats SQL like collections library in Scala, so everything is typed, and everything is abstracted into functions
`transformData` can be referentially transparent iff all its arguments are RT. As we can see, the `data` argument is a function that does I/O, so it is not RT. So by association neither is `transformData`.
Yeah, I'm aware that I/O is a side-effect. My understanding is that the idea is to try to wall off such things from the rest of the program as much as possible. I'm sure there is plenty more for me to understand. I'll take another look at that link when it's likely to make more sense to me ;-)
You should also close your `Souce.fromFile`. A really good chapter on IO in scala can be found in the book `functional programming in Scala`.
More like that I/O, like other effects, should be declarative (it should be described instead of performed immediately). Then ultimate (outermost) function which does I/O, which should be the program's `main` function, executes all the descriptions sequentially. In Scala it doesn't quite work like that because everything is impure and effectful by default. But you can create some 'islands' of purity. So e.g. def greet(name: String): Task[Unit] = Task delay println(s"Hello, $name!") val logError: Task[Unit] = Task delay println("ERROR: something went wrong") val greetWithErrorHandling = greet("Bob") or logError greetWithErrorHandling.run Note that `run` is usually discouraged in favour of the safer `attemptRun`, but it does the job for demo purposes. Btw, `Task` is part of the scalaz library: http://docs.typelevel.org/api/scalaz/nightly/index.html#scalaz.concurrent.Task
&gt;Scala the beginner language on the introductory course in programming, on the MSc programme in Computer Science and Engineering. It seems to me that this is for the Masters program (MSc) not the Bachelors (BSc). (I might be reading this wrong!). In that case it is likely fine. 
That argument doesn't make sense. Purely functional means for a given input the output will be constant. To argue that the function returns different things because one of its arguments varies (i.e. the given input changes) doesn't make that function impure. By that definition any higher-order function in language that allows impure functions cannot be pure.
Yes, but that doesn't make sense. If I define a function `def add(x: =&gt; Int, y: =&gt; Int): Int = x + y`, are you saying that is not purely functional based on the idea that `x` might not be purely functional? By that definition, virtually nothing is purely functional, including your `flatMap` etc.
Mmmh... Yes. I can see both sides, though. I guess there is a theoretical side of purity and a practical one. I agree that the theoretical one would make for a pretty useless definition of purity. 
If you're interested, it's simple to roll your own non-empty list: class Nel[+A] private (val value: List[A]) extends AnyVal object Nel { def apply[A](x: A, xs: A*) = new Nel(x :: xs.toList) implicit def nel2list[A](nel: Nel[A]) = nel.value }
Thank you. Actually, my over-engineering wasn't aimed at reimplementing what the language already provides, though I know it looks that way. I was trying to include how one describes a function having another function as a parameter, how to use a Trait to allow for some data-function object specified at runtime, and so forth. I'm sure it must look ugly, but there is some kind of (half-baked) method to my madness. I definitely will take you up on learning about what the language provides. I'm totally about using what is given to me. Thanks, again!
An educated guess is: sbt might be executing your tests in parallel, with separate test class instances or processes. That could cause some of them to fail if SQLite requires all concurrent access to happen through a single object.
Project Euler is my go-to for new languages. The problems are very math-oriented, but you can spice them up by solving them in novel ways. http://projecteuler.net
http://scalapuzzlers.com/
Correct answer: http://aperiodic.net/phil/scala/s-99/
Have the same issue at the moment, large parallel data processing and Futures simply leak. To substantiate, here we go with some code. Set the VM flag depending on your machine so it doesn't dump right at start, "-Xmx100m -XX:+HeapDumpOnOutOfMemoryError". import scala.concurrent.Future import scala.concurrent.ExecutionContext.Implicits.global import scala.concurrent.Await import scala.concurrent.duration._ object Bug extends App { println("Memory leak") while(true){ Future { val data = new Array[Byte](1000000*1) // 1 MB println(".") } } } You should see it slow significantly after a while. PS: posted here for a possible answer: http://stackoverflow.com/questions/34553696/how-to-prevent-scala-futures-from-creating-a-memory-leak 
I support your analysis. A possible solution would be to use different table names for each test, if at all feasible
If you're serious about this kind of thing, I strongly encourage you look into Rust. Once you learn your way around the platform your coding style wont be significantly different than it is in Scala. I'm sure you realize it is not reasonable to write such things in a language with huge amounts of unnecessary boxing and indirection.
From the commit messages, I guess that the emulator is tested by running actual games (Megaman, Supermario, Zelda). Not automatization (afaik) but very meaningful.
You can't make a point by just dropping a bunch of benchmarks without talking about what specific qualities they measure. The last two links are irrelevant, Java and Scala have mostly the same problems, the difference between C and Rust will shrink over time, I am not surprised nor purturbed by the fact that Java and Scala are similar nor that Rust still lags behind C. The level generation benchmark *might* have a point even if it's not the point you're trying to make. I wouldn't expect this room colision code to run into a single cache miss, IE, none of the issues I have with Scala are going to come up here. The Rooms are allocated in the order in which they appear in the array, so wont they be pretty much contuigous despite scala's inability to enforce contiguity? And there are never any more than 75 in a single level (it tries to insert 50000 rooms [but the array is initialized to only 75](https://github.com/logicchains/Levgen-Parallel-Benchmarks/blob/master/scala/Bench.scala#L51) which means no more than 75 ever make it in.), so wont the entire widdly little dataset fit in a single cache line? Do you think that this benchmark is typical, that cache misses just arn't that big an issue generally?
Ehrm. Yes.
&gt; A monad is an object that wraps another object. Um no.
&gt; Scala set is a collection of pairwise elements of the same type. Er... what? What's a pairwise element? 
Those "benchmarks" are really old. Rust has come a long way and note that the RSS usage is due to jemalloc allocating more than it needs. You can use the native malloc which will only allocate what the program needs and the RSS stat will significantly decrease. I agree that Scala isn't the best language for something like this. Rust would be far more appropriate.
&gt; Those "benchmarks" are really old : &gt; rustc 1.5.0 (3d7cd77e4 2015-12-04) &gt; &gt; Scala compiler version 2.11.6 And don't think the only language and platform (JVM) whose performance increases over time is Rust.
It is possible that `xsolarwindx` intended to draw our attention to the /2013/08/23/ date.
If you're set on using SQLlite, a common pattern is using a different database file for each test using something like https://stackoverflow.com/questions/32160549/using-junit-rule-with-scalatest-e-g-temporaryfolder. It's probably easier and cleaner to use H2 like everyone else is suggesting though.
The file evaluates to a single "variable", but yeah I think the fail-fast behavior is the big draw. e.g. val cassandraHost = "1.2.3.4" val postgresHost = "5.6.7.8" MyConfiguration(cassandraHost, postgresHost, someOtherConfigParam = 4) 
For configurations I like to take the approach I found being used by Akka's `ActorSystem`, [here](https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/actor/ActorSystem.scala#L153). So, basically: object Foo { class Settings(config: Config) { val firstSetting: Boolean = config.getBoolean("Foo.firstSetting") val secondSetting: String = config.getString("Foo.secondSetting") //... } def apply(config: Config = ConfigFactory.load()): Foo = new FooImpl(new Settings(config)) } import Foo._ abstract class Foo { def settings: Settings // settings.firstSetting // settings.secondSetting } class FooImpl(settings: Settings) extends Foo
Granted with the typesafety. But regarding the polymorphic configuration, I guess you could maybe do something like.. /* application.conf Foo { type = remote host = localhost index = foobar } */ object Foo { sealed abstract class Settings { def fooType: String } class RemoteSettings(config: Config) extends Settings { val fooType = "remote" val host = config.getString("host") val index = config.getString("index") } class EmbeddedSettings(config: Config) extends Settings { val fooType = "embedded" val index = config.getString("index") } object Settings { def apply(config: Config) = config.getString("fooType") match { case "remote" =&gt; new RemoteSettings(config) case "embedded" =&gt; new EmbeddedSettings(config) case _ =&gt; println("error...") } } def apply(config: Config = ConfigFactory.load()): Foo = { val settings = Settings(config) settings match { case rs: RemoteSettings =&gt; new RemoteFoo(rs) case es: EmbeddedSettings =&gt; new EmbeddedFoo(es) } } } import Foo._ abstract class Foo { def settings: Settings } class RemoteFoo(settings: RemoteSettings) extends Foo class EmbeddedFoo(settings: EmbeddedSettings) extends Foo Just thinking out loud
one downside is that this could have security implementations.
The transition from Slick 2 to 3 is different when it comes to testing. The former is synchronous while the later is asynchronous, using the IO Monad. The database driver, though, is typically synchronous, as you might expect. See this project for Slick 3: https://github.com/objektwerks/slick See this project for Slick 2: https://github.com/objektwerks/scalatra Not to worry, Slick has a near zero adoption rate.:) NoSql stores like Cassandra and MongoDb tend to rule in the Scala space.
I sometimes think shouting out 'Scala' is the same as yelling 'Food Fight' in the movie, Animal House. It's fascinating to see how Scala generates so much passion from so many directions, unlike any other language. The author, from my experience, is correct in labeling the 2 prominent developer groups in the Scala space, one pragmatic and the other exotic. That said, both groups want to achieve the same result, just differently. I'm not sure the ratio --- but I suspect 80/20 in favor of the pragmatists. That said, even a pragmatist can't ignore the exotic side of Scala. The pool metaphor is appropriate, in that kids never swim in the shallow end for long, before they venture into the deep end. I've been using Scala for 3+ years. And seldom a day passes when I don't learn something new about Scala. I suspect that's both a bonus and a curse. When it comes to actually building something useful for other people, the libraries I use dictate my usage patterns. And I seldom, if ever, need Scalaz and the like to help me. Yet I'm currently going through the Underscore.io Advanced Scala book because I just gotta go into the deep end of the pool.;) I've actually done this a few times --- yet nobody seems to agree on Category Theory in terms of Scala. And, now, we have Cats. I do hope Dogs is next.; 
Alright, this post has gone on long enough without any comments. Let me fix that. This is really cool! Coroutines are fun, and so are macros, especially when the macros do severe rewriting to the program code. I'm hoping to get some time today to skim the source and figure out just how it works. 
On a related note, can someone more in the know give a relative average Scala developer an update on Dotty? When can we expect to start using this in production? Is it significantly faster than the existing compiler? What are the main benefits to the end user?
This was helpful and well presented.
You can do one of the previous course iterations. You don't need to wait. The sbt coursera grader should still work.
I guess, but it's not really something I can put on my linkedin profile to prove i know scala haha
- Do the exercises and put your solutions on Github, or don't, because this part is more about personal learning - Do one or two personal projects that are less academic &amp; more "real world" and put them on Github. Write good idiomatic Scala - Put Scala on your resume - Apply for Scala jobs The Scala community is pretty small. If you do the above you will have no problem getting to a phone screen. At that point your resume doesn't matter, it's all about what you've actually learned.
Embarrassed at all the trouble I am having with subtract in Anagrams...
[Hands-on Dotty by Dmitry Petrashko](https://www.youtube.com/watch?v=aftdOFuVU1o) gives an overview.
Talking 'bout personality -- I can openly carry a gun at my workplace, right?
Nice! Thanks. I love the idea of type unions and intersections.
Just curious. What is consider good idiomatic Scala? Just pure functional programming without OOP? Is there scala jobs for new grad? Our university uses scheme for our into to computer science class and scala for db class, I really like it, not sure if newly graduated bachelor is qualify for applying scala jobs though. 
Nope.
&gt; What is consider good idiomatic Scala? Just pure functional programming without OOP? Generally tending towards purer FP, yes. &gt; Is there scala jobs for new grad? I'm sure, especially if you have already demonstrated some proficiency in the language. New grad vs. experienced is about more than just how much time you've spent with a language. There are lots of tasks that you would assign to a junior programmer that are going to be required in any project and it won't matter whether you're using Java or Scala.
Is there a concentration of scala job in US? Most of scala jobs I found tend to be in the Bay area
This is a great piece of work from Rúnar, and is basically how we (Verizon OnCue) write Scala when we can't get away with just `Task` or maybe `Kleisli` and `Task`.
Following up on the paper, I gave an overly condensed lightning talk at LambdaConf on Stack Safety in Scala. I went over various ways of achieving stack safety (including trampolines), and some limitations/alternatives. *Probably* should have been a 20 minute or so talk, but the information is accurate... https://www.youtube.com/watch?v=K1eKsr-JV3c&amp;feature=youtu.be&amp;t=1980
Zookeeper?
&gt; How to set up Scala, sbt and ScalaTest for Spark application development 1) Create SBT project 2) Add the following lines to your build.sbt libraryDependencies ++= Seq( "org.apache.spark" %% "spark-core" % "1.5.2", "org.scalatest" %% "scalatest" % "2.2.4" % "test" )
Don't use injection for the clients. Inject a root actor that knows how to process a message to start up your HTTP CLIENT and cookie jar, then send it a message to start them. The message should contain the jar information.
I read "Programming Scala" by Dean Wampler, which I believe is a fantastic book. I'm now (slowly) going through "Functional Programming" by Runar Bjarnason to learn the fundamentals of the paradigm. Out of curiosity, is this "your" top 5 or is this based on facts like annual sales, best ratings, etc ? I'm asking because I might look for a third book after I'm done with the one I'm reading right now and I'm looking for a reliable source to help me chose which one it's going to be. Considering that your "community" website has nothing but 4 articles and no clear indication where this top 5 comes from, I can only be skeptical.
`scalaz.Task` allows you to run interruptably, so you can terminate a computation externally.
I built a small library called [Processor](https://github.com/Sciss/Processor) that extends `Future` with a possibility to call `abort` on it. It's up to the body of course to check for the abort status, as we don't want to call `Thread.interrupt`.
&gt; How big and unwieldy does an application have to get where frequent, unavoidable overflows are a concern? Rúnar might have some data on that, or [/u/tpolecat](https://www.reddit.com/user/tpolecat). Looking at the paper again, I like Rúnar's little motivating example of `zipIndex`: a trivial use of `State` and `foldLeft`, so it's obviously tail-recursive. "But when zipIndex is called, it crashes with a StackOverflowError in State.flatMap if the number of elements in the list exceeds the size of the virtual machine’s call stack." I read that as "it doesn't take much at all, and worse, is triggered by code the compiler/JVM should be able to make tail-recursive without my saying another word." &gt; How is the decision made that the right thing to do is replace stack with heap rather than split up the application? We frankly kind of dodge the question by using `Task` and `Free` so much, as well as whatever other bits of scalaz are trampolined and I don't even know it. :-) &gt; Sort of separately, what advantage do you find in building up a giant computation and running it at the end of the world? I get that you can have programs as values and such. Then you've answered your own question. :-) Teasing aside, I think the scalaz/scalaz-stream thread here is my best answer. In ~30 minutes after deciding to use ftp4j, I had the functionality I needed from this stateful, multithreaded, exception-happy Java API in a monadic function. Even if all I could do was `.attemptRun` it, it would be worth it for my increased confidence in its correctness (and my ability to deal easily with the resulting `Throwable \/ List[File]]` regardless). But of course that's far from all I got out of it: arbitrary stateless _or stateful_ stream processing of the files was just a `Process.eval` away. Passing the contents through [xs4s](https://github.com/ScalaWilliam/xs4s) is three lines of `GZIPInputStream`, `FileInputStream`, and `io.iteratorR`, again with essentially 100% confidence in correct behavior. Add a Shapeless Coproduct of several case classes generated from XSDs with scalaxb and a little mapping of file names to XML elements they contain, and I've established a relationship between the value level and type level such that, if I don't yet have a handler for the file/element, the code won't compile—and if it does, it means the data will insert into the database without exceptions (due to the data—the DB could be down, etc.), because the types carry all the way from the XSDs to the database. &gt; I have used http4s and watched tpolecat's videos. There is a kind of elegance to it all, but I don't find much practical value yet. A colleague went from his first DB queries to wrapping the queries in scalaz-stream to writing his first Argonaut `EncodeJson` to returning query results from http4s in JSON in ~2 days. So "delivering fast" is one practical value, with "the code has no unhandled failure modes by construction" being another. Sure, we'll want to add an `.attempt()` to add some logging, and we'll add funnel monitoring support before it's ready for production. &gt; I had to replace http4s in one app because it was simply too slow, for example. Not surprising. We're all waiting for [fs2](https://github.com/functional-streams-for-scala/fs2) with bated breath.
&gt; it doesn't take much at all... Ah, fair point. I read it as more contrived than illustrative since I rarely see code like that in practice. &gt; we frankly kind of dodge the question... Fair enough, haha. &gt; ftp4s... That was a good thread and thanks for reminding me of it. I hadn't made the connection to what we're talking about here. I think your additional points are all valid and useful consequences of using types and error values. My question then is, do all those extras come from being trampolined? Or do they come from using the monadic task construct which happens to be trampolined. My gut says the latter but maybe you can't get all of them without trampolines? I'm all for composition and correctness by construction, I am less convinced those require lifting everything to a trampoline. And if they do, it would be useful to have some rules of thumb or indicators for when that becomes necessary.
It really depends on the type of application you are writing. I'd agree with you that often times the risk of a SO is rare. My example in the video is traversing a tree structure. Depending on the call stack for instance, I can recurse ~2000 times without hitting an SOE on my local MBP with 16gigs of ram. How often will you see a tree structure that deep? Not often. But that doesn't mean you don't have a bug in your code that can be exploited or will be hit one day by an edge case. Remember though, a trampoline is *implemented* with a Free monad, but that is not the only advantage of using Free. It's just one advantage that ends up being important if you don't want a land mine lurking. 
See [the sources](https://github.com/scalaz/scalaz/blob/v7.1.6/concurrent/src/main/scala/scalaz/concurrent/Task.scala#L106), but briefly: 1. You call one of the `runAsyncInterruptibly` functions on the `Task`. 2. Which one to call depends on whether you want to cancel by setting an `AtomicBoolean` to `true` or call a function. 3. Tasks certainly do use threads and offer a lot of control over what thread pool they run on. See [Task: The Missing Documentation](http://timperrett.com/2014/07/20/scalaz-task-the-missing-documentation/) for more information.
They promised they'd implement it in the future?
Have I missed something? :: isnt a type, its list construction... :+ is general sequence construction.. EDIT: Downvoted? ..? cf. Above. 
Yes you [have](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.$colon$colon). `List[T]` has two subtypes, `::` and `Nil`.
I'm aware of the encoding, etc., but to me it still seems like a design flaw in the library :-) But at least they've been consistent about it: ::(1, List(2)) map identity // =&gt; List(1, 2): List[Int] Some(1) map identity // =&gt; Some(1): Option[Int] I guess I was just curious about if there would be any negative consequences from returning the specific subtype.
The question does not specify if compilation or runtime considerations matter. When memory is tight, like on a Raspberry Pi or a small JVM, the memory requirements of Scala's compiler could be a reason for going with Java.
HFT. That's probably it.
I took that as a challenge :) Here's how I managed to minimize the code duplication: sealed trait MyOption[A] { type Ctor[B] &lt;: MyOption[B] def ctor[B](): Ctor[B] def ctor[B](b: B): Ctor[B] def map[B](f: A =&gt; B): Ctor[B] = this match { case MyNone() =&gt; ctor[B]() case MySome(a) =&gt; ctor[B](f(a)) } } case class MyNone[A]() extends MyOption[A] { type Ctor[B] = MyNone[B] def ctor[B](): Ctor[B] = MyNone[B]() def ctor[B](b: B): Ctor[B] = ??? } case class MySome[A](a: A) extends MyOption[A] { type Ctor[B] = MySome[B] def ctor[B](): Ctor[B] = ??? def ctor[B](b: B): Ctor[B] = MySome[B](b) } object Main extends App { val r0: MyNone[Int] = MyNone[Int]().map(_+1) val r1: MySome[Int] = MySome[Int](41).map(_+1) } So the duplication is limited to the Ctor/ctor stuff, and all the other methods only have to be defined once. Here I only defined map, but I could also define foldMap etc. by reusing the existing Ctor/ctor boilerplate. The one part which is still annoying me is the lack of type safefy: since Scala doesn't specialize the return type `this.Ctor[B]` to `MyNone.Ctor[B]` when pattern-matching on `this` revealed that `this` was a `MyNone`, there is no difference between the two branches, so there is no way to statically check that I don't call the version of `ctor` which fails with `???`. *edit*: wait, no, I couldn't define `foldMap` because it doesn't necessarily return the same constructor as `this` nor the same constructor as the value returned by its function argument. Maybe `map2`?
I found a way to avoid the type-unsafety from my other comment! sealed trait MyOption[A] { type Ctor[B] &lt;: MyOption[B] def withCtor[B,C]( withNoneCtor: Ctor[B] =&gt; C )( withSomeCtor: (B =&gt; Ctor[B], A) =&gt; C ): C def map[B](f: A =&gt; B): Ctor[B] = withCtor[B,Ctor[B]]( none =&gt; none )( (some, a) =&gt; some(f(a)) ) } case class MyNone[A]() extends MyOption[A] { type Ctor[B] = MyNone[B] def withCtor[B,C]( withNoneCtor: Ctor[B] =&gt; C )( withSomeCtor: (B =&gt; Ctor[B], A) =&gt; C ): C = withNoneCtor(MyNone[B]()) } case class MySome[A](a: A) extends MyOption[A] { type Ctor[B] = MySome[B] def withCtor[B,C]( withNoneCtor: Ctor[B] =&gt; C )( withSomeCtor: (B =&gt; Ctor[B], A) =&gt; C ): C = withSomeCtor(MySome[B](_), a) } object Main extends App { val r0: MyNone[Int] = MyNone[Int]().map(_+1) val r1: MySome[Int] = MySome[Int](41).map(_+1) } 
OK, I just investigated and it looks like we'd need to do both of the above. We'd need to override the methods to specify the more specialised type of the implicit `CanBuildFrom` that these methods would take, and also we'd need to put the `CanBuildFrom` object in the proper place for implicit search to work. So in terms of raw work needed, it looks about the same as if there was no `CanBuildFrom` infrastructure and we had to override and implement all the methods manually.
Clever :-) This looks like the visitor pattern.
Ah, memory requirements. I hadn't thought of that. 
You're mapping over each line, including the first line with all the headers so when you do val label = parts(20).toDouble you're calling .toDouble on a string that can't be converted to a Double.
Here is one solution. It would be better if it was modified to work directly with the array rather than constructing a map, and use Vector for efficient random access. This is functional and RT though, because it delegates the logic of "append to mutable data structure if condition is true" to a `filter`. `map`, `filter`, `reduce`, `fold` etc. are very useful for working with collections in a functional way - i would recommend familiarizing yourself with those. &gt;import scala.util.Random &gt;val bools = Seq.fill(16)(Random.nextBoolean) &gt;val boolMap = bools.zipWithIndex.map { case (bool,idx) =&gt; ((idx/4,idx%4),bool) }.toMap &gt;def neighbourIdxs[T](m: Map[(Int,Int),T],x:Int,y:Int,xub:Int,yub:Int) = { val idxs = (for (dx &lt;- List(-1,0,1); dy &lt;- List(-1,0,1)) yield (x+dx,y+dy)).toList val valid = idxs.filter { case (xi,yi) =&gt; (xi &gt;= 0 &amp;&amp; xi &lt; xub &amp;&amp; yi &gt;= 0 &amp;&amp; yi &lt; yub) &amp;&amp; !(xi == x &amp;&amp; yi == y) } valid } &gt;def trueNeighbours(m: Map[(Int,Int),Boolean], x:Int, y:Int, xub: Int, yub: Int) = neighbours(m,x,y,xub,yub).filter{ case (x,y) =&gt; m((x,y)) } &gt; trueNeighbours(boolMap,2,2,4,4) List((1, 2), (1, 3), (3, 1), (3, 3))
High Frequency Trading? ...are you serious? People use Java for that?
Futures immediately start whatever computation you pass it. It's as impure as `System.out.println`. Whereas `Task.delay(System.out.println("hi"))’ is as pure a value as `2` or`List(1,2,3)`. It's just easier to compose pure values. 
So, by your definition, `lazy val a = {println("hi"); 2}` is also pure ? (just wondering) I mean all of this is pure until it's used (computed), right?
The fundamental problem is that constructing a `Future` with a side-effecting expression is itself a side-effect. You can only reason about `Future` for pure computations, which unfortunately is not how they are commonly used. Here is a demonstration of this operation breaking referential transparency: import scala.concurrent.Future import scala.concurrent.ExecutionContext.Implicits.global import scala.util.Random val f1 = { val r = new Random(0L) val x = Future(r.nextInt) for { a &lt;- x b &lt;- x } yield (a, b) } // Same as f1, but I inlined `x` val f2 = { val r = new Random(0L) for { a &lt;- Future(r.nextInt) b &lt;- Future(r.nextInt) } yield (a, b) } f1.onComplete(println) // Success((-1155484576,-1155484576)) f2.onComplete(println) // Success((-1155484576,-723955400)) &lt;-- not the same However this works fine with `Task`. Note that the interesting one is the *non*-inlined version, which manages to produce two distinct `Int` values. This is the important bit: `Task` has a constructor that captures side-effects as values, and `Future` does not. import scalaz.concurrent.Task val task1 = { val r = new Random(0L) val x = Task.delay(r.nextInt) for { a &lt;- x b &lt;- x } yield (a, b) } // Same as task1, but I inlined `x` val task2 = { val r = new Random(0L) for { a &lt;- Task.delay(r.nextInt) b &lt;- Task.delay(r.nextInt) } yield (a, b) } println(task1.run) // (-1155484576,-723955400) println(task2.run) // (-1155484576,-723955400) Most of the commonly-cited differences like "a `Task` doesn't run until you ask it to" and "you can compose the same `Task` over and over" trace back to this fundamental distinction. So the reason it's "totally unusable" is that once you're used to programming with pure values and relying on equational reasoning to understand and manipulate programs it's hard to go back to side-effecty world where things are much harder to understand.
Of course it's hyperbole. I find `Future` as a concept incredibly useful...want to kick off a bunch of tasks then wait for all of them? `Future` is a great way to represent that. He just seems to be upset that `scala.Future[T]` doesn't have identical semantics to `scalaz.Future[T]` (or Twitter simply isn't a good medium for expressing his actual problem).
Thanks! I see what you mean. I guess the crux of the argument is mainly a distinction between lazy and strict semantics and what the default behavior is? Fair point, but I was hoping there were more benefits. Because I could just as easily get the same semantics: val f1 = { val r = new Random(0L) val x = () =&gt; Future(r.nextInt) for { a &lt;- x() b &lt;- x() } yield (a, b) } I tend to like default strict with the option of becoming lazy. Laziness makes it harder for me to reason, locally about performance - but that's a whole different debate (that isn't greatly applicable to this example). :)
Right, that's what I thought, but why delayed "lazy" `Future` doing `println` and returning `2` is considered pure? Or I didn't understand ItsNotMineISwear's post?
What is the side effect in this example?
it pulls threads off of the thread pool that we are passing in here implicitly. The (_ + 1) function happens on another thread.
To give another opinion, I think Future is perfect for what it was designed to do. It's quite instructional to go through the design process of a Task. It's not hard at all and it just so happens that [I did that lately](https://gist.github.com/alexandru/55a6038c2fe61025d555). Yeah, it's great that task is lazy, but you know what else? Task is also a single-consumer producer, with a result that is not memoized to be shared with other consumers. As soon as you want a memoized Task (e.g. one that stores its result after the computation is over), one comes to realize that: 1. you can have a Future that models side-effects, or you can have a Future that supplants the need for callbacks; FP people want the former, but programmers at large want the later and not without good reason (both are needed) 2. with that in mind, when doing memoization, it's much more common sense to start the computation as soon as possible; this is very clear once you end up thinking about where to inject that ExecutionContext Lets do this as an exercise. Say we've got our Future and our runAsync method, very much similar with Future.onComplete. But compared to the standard Future, our Future is lazy so the map operator does not take an ExecutionContext. Like this: trait MyFuture[+T] { def runAsync(cb: Try[T] =&gt; Unit)(implicit ex: ExecutionContext): Unit def map[U](f: T =&gt; U): MyFuture[U] } But lets say that we want to memoize our result somehow, so we add a "memoize" utility, returning a "dirty" task that keeps state and that upon execution will store the result internally. This will be a "hot producer" meaning that multiple consumers can contend for and consume the same result: trait MyFuture[+T] { // ... def memoize: MyFuture[T] } But then the execution happens on the first "runAsync". And so whomever wins the race to that first "runAsync" will be the one consumer taking the penalty (along with its own ExecutioinContext), whereas the other consumers will be just freeload. And this is not common sense. If you want to memoize that result, then it's more common sense for the producer to produce it in advance. And the *act of memoization* itself is a side-effect and our memoize method should highlight that. And so we should be having this signature (and note that the ExecutionContext happens where the side-effects are :)): trait MyFuture[+T] { // ... def memoize(implicit ec: ExecutionContext): MyFuture[T] } OK, but then there's an API usability issue. Consider this: task.flatMap(f1).memoize.flatMap(f2) And now you've got a pipeline in which some parts are memoized (hot) and some are not (cold). This is manageable for streams of values, but it is not manageable for Future. This API probably breaks the principle of least surprise. For Future, once you go hot, it's better to stay that way imho. In other words we need a MemoizedFuture like so: trait MyFuture[+T] { // ... def memoize(implicit ec: ExecutionContext): MyMemoizedFuture[T] } trait MyMemoizedFuture[+T] { // ... def map[U](f: T =&gt; U)(implicit ec: ExecutionContext): MyMemoizedFuture[U] } But you know, at this point we should stop pretending and call that "memoize" method what it is: trait MyFuture[+T] { // ... def runAsync(implicit ec: ExecutionContext): MyMemoizedFuture[T] } And this surely beats having a `runAsync` that takes a callback or one that blocks the underlying thread, if you ask me ;-)
&gt; 3+ years with RESTful API development &gt; 1+ year of Scala experience LOL!
Just like the terrible design on the TypeSafe site *(like white copy on cyan backgrounds, WTF)*, this is yet another step in the direction of poor design and decreased readability. I'm sure &lt;whoever&gt; is doing their utmost and has the best intentions but they're obviously not a designer (or designers). I hope that they will seek advice from *actual* designers, (suggestions: * /r/Design * /r/design_critiques * /r/usability * /r/userexperience etc), and most importantly, I hope they'll heed the advice.
* compilation time * reliable JS compiler
`Task.gatherUnordered` fulfils that use case (running a number of tasks, and waiting for them). Some other useful calls are hidden in `Nondeterminism[Task]`, admittedly not the most discoverable of locations. For example, `Nondeterminism.mapBoth` which takes `Task[A]`, `Task[B]`, and an `(A, B) =&gt; C`, to return a new `Task[C]`. In terms of what OP's problem is, see the post by /u/tpolecat above. If you don't consider this an issue for you (with regards to side effects and referential transparency), I say feel free to use what you want. And as noted elsewhere, `Future` does include memoization, which is sometimes (but not usually) what people are looking for.
from a technical point of view: no reason (scala lang is more capable) from a 'practical' point of view: java is more popular, widely used and (maybe) easier to grasp
Thank you for the response. The zipWithIndex then filter is a clever approach. That is what I was looking for. Thanks.
What about `::[A]#flatMap` or `::[A]#filter`? They can't necessarily return a `::[A]`. 
&gt; It really illuminates that side effects are a matter of perspective. Well, no... referential transparency is actually well-defined. On the JVM there's nothing we can do about dynamic allocation and GC, but there are other languages where we can, and yes, allocation and deallocation is managed as an effect. "Implicitly and temporarily using a resource like a thread" certainly is an effect, because threads are observable scarce resources, and your application can and will starve, deadlock, race, etc. if you don't treat them as such. It's all too common to have relative newcomers to Scala say, for example, "JDBC is blocking, and I'm using my spiffy non-blocking web app framework... hey, I know! I'll just wrap my JDBC calls in `Future`s!" Put more generally, concurrency is an effect, and should be treated algebraically, like any other effect. Once you do, you (can) get magic stuff like [scalaz-stream](https://gist.github.com/djspiewak/d93a9c4983f63721c41c): streams of values _and effects_. That hinges on having an algebra of concurrency, and its resource safety hinges on having control over what threads come from what pools and go back to them, and when.
Well I would point at the first point of rediquette, "remember the human" (referring to the first post, not reply). And I would also suggest that most so-called "cultists" would fully support people choosing whichever tools/libraries they prefer to use. While people also (obviously) tend to be enthusiastic about technologies which they enjoy using.
&gt; it is the ability to reason about semantics that makes the difference. There is absolutely nothing about your example that is confusing, so I'm baffled as to what you think the problem is. It's not that it's confusing; it's that you can't restructure the code using algebraic identities without changing the code's meaning, which is what we do in (pure, total, typed) functional programming.
There's some material on it [here](https://uf-ias-2012.wikispaces.com/file/view/turner.pdf/401400700/turner.pdf), [here](http://www.math.waikato.ac.nz/~stokes/COMP340/dyn8.pdf), [here](https://eb.host.cs.st-andrews.ac.uk/drafts/effects.pdf), etc. [/u/tpolecat](https://www.reddit.com/user/tpolecat)'s comments in this thread all offer examples.
Thanks for this example...it does make it a lot clearer what you mean by algebraic manipulation. 
 scala &gt; println(java.lang.Runtime.getRuntime.freeMemory) 1387129360 scala&gt; new Array[Long](900000) ... scala&gt; println(java.lang.Runtime.getRuntime.freeMemory) 1366415880 There. I've "observed" a scare resource being depleted. It feels like I'm wrong. It feels like I'm being pedantic. But why exactly? Do we just carve out an exemption for this particular instance? Hand wave it away? Anyway, my point wasn't to disagree with anything he said. It was that his particular example really made me think about our definition of observation and how, like in the physical world, it's mainly a matter of perspective. Interesting stuff.
Ah, that makes some sense. 
[Image](http://imgs.xkcd.com/comics/random_number.png) **Title:** Random Number **Title-text:** RFC 1149.5 specifies 4 as the standard IEEE-vetted random number. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/221#Explanation) **Stats:** This comic has been referenced 429 times, representing 0.4530% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cyob697)
&gt; The best that has been presented so far is an example using a random number generator that was interesting, but so conceptually removed from real life code that I don't know what to do with it. But you asked for a simple real-world example. That's a simple real-world example. Feel free to replace the random number generator with, say, reading from a `File`, or accepting an HTTP request on a socket, or whatever. &gt;So it uses a thread...yes, I understand that threads are a scarce resource, but so are RAM and compute cycles. Again, the difference is that we can specify threads' lifecycles programmatically. We can't with CPU cycles or memory. (Well, kind of: see [scala-offheap](https://github.com/densh/scala-offheap) for an alternative approach to memory management). &gt; The scalaz crowd doesn't have any problem blowing through all of the RAM on my machine... I don't know what you mean by this. scalaz's just a library. How much memory are _you_ using with it? &gt; ...so why do threads get the special "algebraic" treatment? We're doing a lot of server-side stuff at scale, so it's helpful to be able to be very explicit about when and where we use threads, and in particular to be able to refactor our code at will without worrying, e.g. about introducing starvation etc. [funnel](https://github.com/oncue/funnel), our distributed monitoring system, is a good example of a system that benefits from `Task` etc. &gt;What is so special about the use of a different thread to compute 1 + 1 that makes it hard to reason about? It's not hard to reason about in isolation. It's that it may be fine in one place in your program and cause, e.g. thread starvation (depending on what `ExecutionContext` is implicitly in scope) elsewhere in your program. I mean, probably not with "1 + 1," but with people really doing stuff like wrapping JDBC calls with `Future` in, say, a web app that's supposed to scale, it's a real problem. &gt; I was hoping for a meaningful explanation of the concept that you find so powerful, and you didn't explain what anything meant, you linked to 250 pages of academic brain teasers using language that only computer scientists would bother with. So pick just one link. The "academic brain teasers" _are_ the actual explanation, and _are_ meaningful. When we talk about "functional programming," we mean "ensuring software functions and math functions are the same thing." I mean, they are anyway; it's just that the math inherent in functions that mutate variables, throw exceptions, etc. really _is_ incomprehensible. So we try to keep things _simple_. But _simple_ does not mean "familiar," necessarily. So when I say "algebraic identities," I just mean, to steal one of [/u/tpolecat](https://www.reddit.com/r/scala/comments/3zofjl/why_is_future_totally_unusable/cyns21h)'s examples: val x = Task.delay(r.nextInt) for { a &lt;- x b &lt;- x } yield (a, b) and for { a &lt;- Task.delay(r.nextInt) b &lt;- Task.delay(r.nextInt) } yield (a, b) are "equal," i.e. they have the same value, and equally or more importantly, the "state of the world" (the RNG `r`, in this case) is the same both ways. It's really the latter point Tim Perrett is driving at with his tweet. &gt; If I have to be a PhD computer scientist to find any sort of usefulness for the distinction between Future and Task, then you can count me as unimpressed. You don't; it's just that the ground-level explanations of what it means for pieces of code to "have the same value" or what is and is not a "side-effect" etc. are mathematical, and if you put a bit of effort into understanding them, they apply to all code. That's their real strength. Then you can make informed tool choices, pro or con. &gt; Frankly, this whole conversation makes me glad that the scalaz crowd decided to fork the compiler and go their own way. We use scalaz with good ol' scalac like 99% of all scalaz users. &gt; Please take all this bullshit with it. Your hostility just makes you sound extremely insecure. I'm a mostly self-taught programmer, too (had freshman year of university, flunked out). I've written a lot more imperative/OO code than anything else, and had never used scalaz before starting my current job. What I came to understand and appreciate is how much _easier_ programming is when functions are referentially transparent (like the example above) and total (don't throw exceptions, dump core, loop infinitely... i.e. return a result). So I'm sorry if the terminology is off-putting. I think, though, that if you work through [Functional Programming in Scala](http://amzn.to/1ObfZ4S), you'll find it's easier than it seems (with a bit of experience), and the benefits to your confidence in your code will be nearly immeasurable.
That is an awesome project! Thanks for introducing me to it and for your thoughts. Tangent: I heard Apache Spark was doing something with off-heap memory as well (see: https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html). They should have just used this!
&gt; https://xkcd.com/221/ One of my favorites, though I might say this is also accurate to the scala community: https://xkcd.com/1095/ 
Very good. To clarify a little, I'm defining a side-effect as the failure of a program to remain invariant under the substitution model of evaluation; i.e., inlining or factoring-out shouldn't make any difference, which implies a lot of other things (like order of evaluation shouldn't make any difference). We try to preserve this kind of invariance because this kind of program transformation is so useful. It lets us reason locally and mechanically, as if we were dealing with an algebraic expression rather than trying to think about a bunch of stuff "happening". Of course we now have to say what "invariant" means, which is what you're getting at. For your example we would probably say `Runtime.getRuntime.freeMemory` is just a source of nondeterminism like generating a random number or reading from the console, so you capture it in `Task` or `IO` or whatever and call it good; but if you intend for memory usage to be deterministic or otherwise constrained under substitution then your list of things that side-effect has grown substantially. And if we're writing realtime spacecraft software then we might care very much indeed about memory allocation, "taking time", nontermination, etc., which are not normally considered to be observable. In these cases one approach is to make your constraints manifest in the types, so you disallow substitutions that would (for instance) create an infinite loop or upset critical timing by making them into type errors. So rather than change the rules you just make them impossible to break by being clever with your types. Scala is not *particularly* well suited for this kind of thing (although it can do [more than you might think](https://github.com/milessabin/shapeless)) but languages like Idris do it quite well. 
It's worth noting that Haskell *needs* to control effects for purely pragmatic reasons, not just the philosophical ones. Being a lazy language it's difficult to statically determine order of evaluation (for humans, not computers), and since the order of evaluation of effects has observable consequences it is imperative (pun intended) that the programmer specify the order. This is one of the reasons for using monads---they require that order be explicitly specified. In a strict language like Scala or ML this is less important.
Ive had no issues deploying play framework apps into DO, however play has a minimum heap requirement of atleast 512 with a recommendation of having 1g.
&gt; It is useful to know what your program is doing and prevent it from doing something you do not want it do do. In many cases, preventing side effects is part of that reasoning. The great thing about _sequencing_ (not preventing!) effects is exactly that it enables "knowing what your program is doing and preventing it from doing something you do not want it to do." I get the impression a lot of people think this is somehow _hard_, when it's actually _easier_ than dealing with unconstrained effects correctly. &gt; In the case of adding 1 + 1 on another thread, it is not. This is a strange example to build a case for `Future` on, precisely because it's a pure value. Here's how you'd do it with `Task`: Task.now(1 + 1) It's nice because you can use it like any other `Task` (because it is), and you're not doing anything as silly as taking a thread from a thread pool at construction time to do it.
&gt; But for now, every now and then, I'll do "unreasonable" ( un-algebraicly-reasonable ;) ) things like call "log" instead of constructing an IO Monad. I guess I remain unenlightened. Well, you and us both, in the sense that we wrote [Journal](https://github.com/oncue/journal). Granted, we pretty much always `Task.delay(log.debug(...))` though. :-D
Ok, I admit I stole this from the [Scalawags bullet point list](https://gist.github.com/SethTisue/a11efd434e245d2d30f1#predictions-for-2016-4550)...
By the way, just a reminder: what Tim said was "_After years of using Scalaz Task_, Future is now totally unusable." It's only been about 10 months for me :-D but I definitely know what he means—it would be very difficult to go back at this point. But I do think "why" is more apparent if you've taken the plunge into referential transparency than if you haven't. Maybe a good thing to try would be developing a simple REST API with [http4s](http://http4s.org), [Argonaut](http://argonaut.io), and [Doobie](https://tpolecat.github.io/doobie-0.2.3/00-index.html). I think that would nicely ground the discussion, take it a bit beyond code snippets that are too trivial to really be informative, but still be bite-sized enough to knock off in an hour or so (a GET, a POST, save in DB, look up in DB, return JSON... very bread-and-butter). Anyone interested? **Update:** Or just look [here](https://bitbucket.org/da_terry/scalasyd-doobie-http4s/overview), I guess.
Thanks guys for the advices, I will need to increase the memory in my server. 
&gt; Found a lot issues regarding memory and compiling. The Scala compiler needs a lot of memory, maybe 1–2GB. If you're compiling your Play application on your server then you'll need a lot of memory. If you're compiling your application somewhere else (e.g. using *dist* to build it) then just running the already compiled code on your server you might be OK with 512MB of RAM, depending what you're doing on your server.
Delete `"provided"` Or for that matter delete the offending import line since you're not using it anyway.
Whoops, I use RandomForest in my next few lines, I forgot to include that sorry. But I do use it.
I have solved low memory problem during compilation by using a swap file. This should help: https://www.digitalocean.com/community/tutorials/how-to-add-swap-on-ubuntu-14-04
Although this was facilitated by Reddit draining old deleted accounts, which in turn moves us a bit away from the 10K ;)
Future isn't totally unusable, its a huge case of hyperbolic evangelism put forward by people pushing a certain ideology. `Future` uses the well established promise pattern, which is actually one of the major abstractions being used in computer programming (currently) for async programming. The other is CSP/Erlang style, and finally you have stuff like Fibre's (allow you to suspend threads, which simulates async programming) Personally, I use `Future` the majority of the time. The cases for pure RT versions like Task don't add up to any kind of net bonus. There hasn't been any code (where we use `Future`), where the supposed "refactoring" bonus given by Pure RT programming which wasn't possible with `Future` (and that is at best, there would have been zero cases of bugs being picked up by using something like `Task` in our code) Furthermore, the ability for `Future` to memoize properly is actually a **massive** bonus in the typical code that you use `Future` with. If you are doing a HTTP request, or a call from a database, and you have other code that is "observing" your Future, in 95% of the situations you do want it to be memoized (you don't want to make another HTTP call, nor do you want to redo your database call). If you do need to do this, obviously you can enclose your future in a `def`. That being said, I do like `Task`. It has a very good design, and it is better than `Future` for certain types of tasks (pun intended). Its just that when I do my own CBA, there is very little to gain from using Task versus the advantages that Future bring (ecosystem/library based, basically anything that is not in scalaz land that does async execution is wrapped in a `Future`) for the type of code we deal with. And obviously if you need RT Purely Functional programming which is captured in the type system, you should be using Task. This isn't really case for Future being unusable, its an actual debate on RT purely functional programming versus not RT purely functional programming.
Everything :)
I don't know whether @timperrett intended that but my issue with Future not being lazy is mostly regarding reasoning about it runtime behavior and separating its declaration from its execution: whereas a Future instantiation will immediately schedule the wrapped function, a Task will need to be run. It especially shows up when composing several futures/tasks together through flatMap or for-comprehension: tasks can be composed "off-line" and the resulting combination can be executed atomically. No incertitude. Future composition might be... Surprising: def tuple(f1: Future[A], Future[B]): Future[(A, B)] = for {a &lt;- f1 ; b &lt;- f2} yield (a, b) What if f1 and/or f2 completed before tuple is called ? Sure, we could alleviate this issue by building our underlying futures within the for-comprehention by passing their factories functions. Tedious. By contrast, Task explicit run request may ensure that underlying tasks are run after the composite. 
Instead of .map().flatten do .flatMap. A for comprehension would be much more idiomatic and readable as well. I'm on my phone now or I'd write it
VerbalExpressions is interesting, but I'd do a lot of things differently, from an interface point of view. edit: also I'd try to limit the number of added subpatterns and let the user control them freely
Ignoring the scalaz crowd and how they think future should perform (as compared to task), future does have some issues which may lead to memory leaks in the way you use it. Future will also provide different results if the futures call side effecting methods. val rng = Random(0) for { x &lt;- Future { rng.nextInt } y &lt;- Future { rng.nextInt } } yield { x + y } compared to val rng = Random(0) val futY = Future { rng.nextInt } val futX = Future { rng.nextInt } for { x &lt;- futX y &lt;- futY } yield { x + y } This is especially true if you're dealing with an external stateful service and your future calls impact it.
I'll clarify, the example wasn't related to memory leaking, just a variation in result if you call side affecting methods in the future (e.g. Random.nextInt) And yes, most of the problems people have with future are due to bad (functional) programming. 
I'm developing a bunch of libraries around sound processing, and I can always use help. For example there is a mini-IDE around [ScalaCollider](https://github.com/Sciss/ScalaColliderSwing) that can be improved in many ways (text editor, literate programming, wiki editing, online help, widgets such as oscilloscope, spectogram etc.). I'm working on an ambitious [computer music environment](https://github.com/Sciss/SoundProcesses) as well, but that is probably a too huge beast and also quite a moving target as I'm refactoring and changing lots of things.
Just a reminder: there's a very real-world thread describing a problem I had with using akka-camel for FTP, and how I solved it with ftp4j and `Task`, [here](https://www.reddit.com/r/scala/comments/3xavvi/scalaz_and_scalazstream_simplify_things/). The thread goes into detail, in particular, about error handling in `Task`, using `Task` as a source for scalaz-stream `Process` building, and derives a ludicrously simple `attemptRepeatedly` function for `Process`es that retries up to n times with a programmable delay. It's all referentially transparent. In fact, you couldn't use the FTP download in a stream if it _weren't_ referentially transparent, and you couldn't write a general-purpose `attemptRepeatedly` if streams couldn't contain effects, which it can only do because they're referentially transparent. So I think it's a highly motivating example. I'd also like to refer again to the [http4s/Doobie](https://bitbucket.org/da_terry/scalasyd-doobie-http4s) example, because it's also real-world, and if you blink you might miss that it's entirely referentially transparent.
Verbal expressions could be just a couple of predefined parsers built on an existing parser-combinator library.
@drubbo: PRs welcome! It started out as a direct port of the original JS implementation here: https://github.com/VerbalExpressions/JSVerbalExpressions/wiki @Milyardo: Can you elaborate? 
I didn't use TeX for quite some years already, so I may be a bit out of date and forgetting something. But I do not understand what this library is for, exactly. Are there any examples or use-cases described anywhere?
It doesn't seem like spark-mllib is included in the runtime classpath by the Spark server. So you may want to package up your application using a plugin like https://github.com/sbt/sbt-assembly to include extra dependencies (but keep spark-core marked as provided, as that is provided by the server). The build configuration here might help guide you towards a reasonable configuration: https://github.com/databricks/learning-spark
Journal looks interesting. I'll definitely check you out. And thanks for all your insight in this thread.
&gt; The question which starts this Reddit discussion (in my opinion) greatly mischaracterises the tweet. Just to be clear: I didn't do it purposefully and I linked to the context (i.e. a short tweet), so I don't think I "greatly" mischaracterized things. But, you're right, I wasn't perfectly clear. So I apologize. &gt; I need to give up reading reddit comments :-/ Oh, come on. This was a great thread full of interesting insights from people on both sides of the fence. I understand some are frustrated with the rhetoric from some parts of the community, but for the most part we kept it pretty technical. :) 
&gt; eventually, at the "end of the world" we call .run and perform the side effects. Aren't you exactly conceding OP's point that you can't program without side effects? At the very least, "patently false" is... well, patently false. 
absolutely not. if I stay pure all the way to the end of the world, and then in a main method I have "foo.run" which performs the side effects, wasn't everything I was doing until I call foo.run programming? During all that programming, wasn't it all free of side effects? Where are the side-effects in libraries like cats or scalaz? When I'm writing software for those libraries, am I not programming? I'm not causing side-effects when I do the programming there.
I run my app on DO but I don't compile it there just upload the jar. The version 2.4 uses a lot more memory than the versions before but with some swap 512 is enough, otherwise it just silently ooms.
That's very kind. Thanks! It's easy to forget sometimes that it really was only ~10-11 months ago that I had not used `Task` or scalaz-stream. I owe several people on this thread an apology for that lapse and taking my frustration at my own inability to convey why it matters out on other people. [/u/tpolecat](https://www.reddit.com/user/tpolecat), being a saint, has done a _much_ better job than I have, without once even mentioning he's the author of [Doobie](https://tpolecat.github.io/doobie-0.2.3/00-index.html). I get that people want evidence, which is why I suggested folks have a look at [this http4s/Doobie example](https://bitbucket.org/da_terry/scalasyd-doobie-http4s). That said, all is not peaches and cream here: scalaz-stream throughput is an order of magnitude out from where it should be, as of 0.8. That's a show-stopper for some apps, and I know people who have tried http4s and rightly walked away for that reason. It'll be fixed in fs2, but no one seems to have a good handle on when we'll have that. On the other thread I wrote about `Task` and scalaz-stream, I put quite a bit of effort into showing how error handling works and deriving `attemptRepeatedly`, with detailed commentary. I guess my hope is that, even if people still decide not to use it, they can at least see why those of us who do are excited about it. :-) Anyway, thanks for the kind words, and for being a voice of moderation here, even when I've failed to respond in kind.
&gt; You may be honest, but you're still wrong, the reason, as I've now explained several times, being that treating taking threads from a thread pool as an effect allows us to avoid lifecycle concerns just as treating opening a file or a socket as a resource does. I think everyone is missing the central point here. Its not that capturing side effects such as Thread/Forkjoin Pools shouldn't be done because its a limited resource, its that a free pass is being given for certain types of side effects **just to suit the argument**. So when people claim that the use of the `implicit executionContext` means that its not RT (this is true), you can also make the claim that memory allocation (memory is also a limited resource) should also be captured as a side effect in the type system (and people don't do that, this is completely ignored by the type system, and by libraries like Scalaz. You need something like Rust to do this). &gt; My example of putting JDBC calls in Futures to make them "non-blocking" isn't a hypothetical. People do it all the time Sure thing, but its also completely irrelevant to the point. RT programming doesn't give 2 hoots about what type of side effect you are dealing with, a side effect is a side effect. You are giving more importance to a certain type of side effect (in this case thread/fork/join pools), because it happens to be an actual physical problem which you deal with. **This is unrelated to maths, or category theory** (and its also completely subjective). Like you can't have your cake and eat it (too). If people are saying that all effects should be captured in the type system (which is what is needed to do actual RT "pure" functional programming), then you can't just give free passes to some side effects, and not others. On the other hand, if you start drawing lines about what side effects should be captured by the type system, and which ones should be completely ignored, that process is completely arbitrary. At this point, all it comes down to is people drawing different lines about what should be captured and what shouldn't be. &gt; , it brings their app to a screeching halt all the time, and recovering from the panoply of thread starvation, exceptions killing threads, orphaned JDBC resources, etc. is somewhere between excruciatingly difficult and impossible. These are the plain facts, as anyone who's programmed in Java or Scala for any amount of time knows. Sure thing, and improper use of things like scalaz can blow the GC heap due to memory allocations. Should I start claiming that memory allocation should be captured as a side effect in the type system due to this?
All right. My apologies for my last comments. Let's see if we can shed a little more heat than light (fingers crossed). &gt; a free pass is being given for certain types of side effects *just to suit the argument.* This is incorrect. I found it intellectually dishonest because you evinced a clear understanding of _why_ it's incorrect: because we cannot have the same control over memory allocation as we can over thread lifecycles. One principle we strive to follow is not to claim to guarantee things we can't. So we don't claim we can control allocations or GC. That's all. &gt; If people are saying that all effects should be captured in the type system (which is what is needed to do actual RT "pure" functional programming), then you can't just give free passes to some side effects, and not others. This is incorrect. It's perfectly reasonable in a language with dynamic memory allocation and GC to admit we can't control that, and to still insist on referential transparency for what we can control. By your standard, _Haskell_ is... I don't know; I can't actually identify your claim... maybe "deficient?" in referential transparency for the same reason. &gt; On the other hand, if you start drawing lines about what side effects should be captured by the type system, and which ones should be completely ignored, that process is completely arbitrary. At this point, all it comes down to is people drawing different lines about what should be captured and what shouldn't be. This is incorrect. It's not arbitrary. It depends entirely on what we _can_ control. I even linked to an example of a language that _does_ use region-based memory management in order to satisfy soft real-time requirements. &gt; improper use of things like scalaz can blow the GC heap due to memory allocations. Improper use of _anything_ can blow the GC heap. That's not even an indictment of scalaz, let alone of referential transparency. &gt; Should I start claiming that memory allocation should be captured as a side effect in the type system due to this? You could, but then you'd be asking for the impossible on the JVM. So you leave me with a quandary: you seem to want me to take your argument seriously, but it's based on an absolutist all-or-nothing perspective on referential transparency based on the impossibility—which you yourself have noted—of treating memory that way on the JVM, when not even Haskell can do what you're asking. Because treating _memory_ that way on the JVM is impossible, we're supposed to forego treating _threads_ that way on the JVM. That... doesn't make any sense. At all.
&gt; In other words, it's the principles that matter, not the particulars. Well actually they both do. The particulars are being ignored in this case. If a particular example of a real world example is not given (this can be small), and instead a joke solution is given, then its not very convincing. Principles describe the theory, which I don't think is being disputed (that much), and has already been provided. What is being discussed is its effect in the real world, which is actually quite particular. Think of it this way, the response that was given (at least to me) has appeared to have driven people away, because they to them it appeared as a joke example trying to prove something, which doesn't come across to well. &gt; What uncomfortable points? I find it intellectually dishonest when someone goes so far as to demonstrate that they know something is impossible (referentially transparent memory management on the JVM), and argue that something that is possible to manage referentially transparently shouldn't be, on that basis. I'm willing to call it something other than intellectually dishonest; I'm just not sure what it should be. &gt; I mean, OK, for some reason you haven't actually explained—at all—you don't want thread allocation to be referentially transparent. Fine. Don't use Task. Just don't act like the High Priest of the Cult of Moderation and imply the rest of us, getting enormous correctness and safety benefits from referential transparency, are hypocrites for not doing what you admit is impossible. This discussion is in another thread, so I will leave it there 
? It seems to me an RX builder, not a parser.
Thanks for the reply. I feel also that there is a lot of... vitriol(?) that can occasionally be seen at points of interaction between communities - the SLIP process, here on reddit, on the mailing lists. I suspect that the cause is people feeling threatened - we certainly have two different (but both quite valid) approaches to programming - and neither should (but I think, at times do) feel threatened by the existence of the other group, as priorities are different. But both should be able to to coexist and find reasonable common ground.
It turns a sequence into varargs SO: http://stackoverflow.com/questions/6051302/what-does-colon-underscore-star-do-in-scala
Actually, that's the whole "dilemma". I will be using SQLite in production for this application. Well, thing is, it's a desktop application that needs to save a lot of data. I could've written my own .xml-parser, but with no real support foreign keys and stuff it makes more sense to just use a SQL-Database and what better alternative than SQLite for such requirements? 2) sounded good, but it doesn't work. I still get SQLITE_BUSY for at least one test -.- 1) does the same as 2) Hell!
Well, I want to use SQLite in the real application and Slick seems pretty neat, apart from testing :) NoSQL isn't really an option as I kind of need ACID-support :)
uh, is scala already much more stable then the rest, or why seems the scala compiler so dead?
Regarding stability (ie, pending work) key numbers can be gleaned from https://twitter.com/extempore2/status/684042408498204672 and the ensuing discussion
New project has more commits than old one.
Do you know how long the Dotty project has been on GitHub?
The best library to contribute to is one you use all the time. What kind of software do you write when you're not contributing to a library?
&gt; It's very stable Very stable? Seriously? Just six months ago, the Typesafe team migrated the bug database and completely **wiped** thousands of open bugs simply because they felt they would never be fixed. You read that right. They didn't transfer these issues, they simply wiped them. To start from scratch. Because the technical debt was huge and looked bad. scalac has become this huge beast of a code base that's full of corner cases and hidden bugs that hardly anyone at Typesafe understands how it works. It's in part what led to Paul Philips resigning. Personally, I think the biggest threat to Scala right now is Dotty: it's sucking resources out of scalac (including Martin, who hardly works on Scala these days) and it's introducing a lot of uncertainty in public perception about whether Scala is still the main focus of the team. Dotty has also become this easy scapegoat answer to any Scala criticism: "Don't worry, this is fixed in Dotty". Which doesn't help Scala users one bit and is a lame excuse to justify not fixing bugs in scalac. 
The Scala compiler, which was first developed years before Java 8 existed, is first of all an order of magnitude more complex than TypeScript or Rust, and integration with the JVM ecosystem at all imposes complexity that neither TypeScript nor Rust have to contend with. Call me crazy, but when you actually want to _use_ a language, you don't _want_ the compiler changing much. I'd really like compilers not to succumb to the "software is never finished" memetic virus.
FUD much? a) The people working on dotty are not the people working on scalac. b) The cooperation between the projects has already benefited both projects by sharing ideas, approaches, fixes and designs. c) A lot of work is happening on scalac, it's just happening on additional staging repos like scala-opt and scala-dev. d) "the Typesafe team migrated the bug database and completely wiped thousands of open bugs simply because they felt they would never be fixed" uuuuhhh no? ... http://issues.scala-lang.org (And no, please don't try "but I meant a different database!!!")
No.
I don't think the changes will be that large from a user perspective, or cause a Python 2/Python 3 divide. scalac and dotc will probably keep converging,´ until it doesn't even matter whether you use scalac or dotc for Scala code in 99% of the cases.
I'm really nervous that Dotty may end up being a mess too. I hope it doesn't, but Swift and Clojure are looking better every day.
Did you read the associated link?
Next time you come across the confusing use of a symbol operator, try the symbolhound search engine, http://symbolhound.com/?q=scala+_*
The biggest one for me is you have to be able to create them yourself. I recently had a case (as I often do) where I had a set of disjoint types from a source I have no control over, and needed a Coproduct of them. With Shapeless, that was just a: type MyCoproduct = This :+: That :+: TheOther :+: CNil away, then: object handle extends Poly1 { implicit val handleThis = at[This] { this =&gt; ... } implicit val handleThat = at[That] { that =&gt; ... } implicit val handleOther = at[TheOther] { other =&gt; ... } } and val cp = Coproduct[MyCoproduct](...) ... cp.fold(handle) It's not a complete disaster, but check out [Ceylon](http://ceylon-lang.org/documentation/tour/types/)'s union types to see how nice it can be.
The dotty compiler is experimental, so new features/changes can go in quickly. Compared to Scala, dotty doesn't have to be backward compatible and doesn't have to support the ScalaIDE with each commit :) Whether it will be compatible to Scala or not, that's going to influence its adoption. So I assume it will support as much of Scala as possible, while still fixing the fundamental issues. 
I think you should prefer immutability when writing the code. Don't worry about performance unless you are very sure that you need to, and then profile. Imagine profiling the app and finding that this chunk of code takes 2% of the total execution time. Now even if you make it 10x more efficient, it will take 0.2% giving you a 1.8% speedup. This is why i stay away from trying to estimate performance besides asymptotically.
Good idea, I was also thinking this.
[Paul Phillips is being proven more correct every day](https://twitter.com/extempore2/status/569180670855028736): &gt; Why fix years-old bugs in scalac? Fix them in dotc and call it a day! 
Wow that's great insight, thanks! I'm migrating to Scala from Haskell (work-related reasons) and am still trying to wrap my head around what is where. The analogy was the best I could come up with before coffee. Later on I wondered if the difference was more Haskell vs Idris.
I ran into the same problem. The thing is that there is a bunch of variance between what people consider basic. I like https://speakerdeck.com/agemooij/between-zero-and-hero-scala-tips-and-tricks-for-the-intermediate-scala-developer as a good guide.
NEScala, PNWScala, Scala by the Bay, LambdaConf (general FP but a lot of Scala), Scala Days
Even though Paul Phillips is correct, I don't think his attitude is really helpful or realistic. I think the fairly accurate decision, has been made, that fixing certain bugs in scalac are really difficult, due to reasons arising either from possibility, or the high risk of consequences in other parts of the type system. Which is precisely why they are being fixed (or more accurately, not existing in the first place in dotc due to a better design). Labour is limited, and im sure if we had unlimited amounts of it, then the scalac team would fix every known problem in scalac. But that isn't how the world works.
One of the many things helping Dotty adoption would be some industrial partner investing in it. Just funding one PhD student would increase the Dotty workforce by 50% It's happening for oCaml: https://sympa.inria.fr/sympa/arc/caml-list/2015-02/msg00140.html http://www.cl.cam.ac.uk/projects/ocamllabs/collaboration.html Why not for Dotty?
and sometimes it sounds like you hit dead air. People either don't understand you and don't speak up, or don't feel like helping beginners.
I know I'm not even a "intermediate" programmer, but let me say something. What's really troubling me is rather tooling than complex programming ideas like monads, type classes and so on. How can I write sbt script and do what I really wanna do? How to set up my project in IDE etc.... I know it's sounds really stupid and it's because of my lack of experience (I'm rather hobbyist than serious developer), But I really don't know how to dive into...
Dotty certainly has the right contributors to pull it off. Really looking forward to using it. 
Uhm, no? Your example contradicts your own point.
We bumped into this exact problem on a much larger scale where we had multiple batch operations that could take tens of hours to run. The best solution that we found was somewhat ugly but it works great in production. We used [this interruptable futures](https://gist.github.com/viktorklang/5409467) implementation. The author is one of the core Typesafe contributors and we have not had any issues so far.
I personally find this pattern cropping up a lot in my own code. I write the concrete implementation first. Then sometime later, I find I need to do something similar: same outward behavior, but different implementation. It is at *this* point I think: abstract to tidy them up. And that's where a typeclass makes perfect sense. I can make the interface right when I need it, then back port it to existing concrete code bits 
this is really helpful, thanks!
There is actually a SLIP which I initiated which was designed to address this https://github.com/scala/slip/pull/28 So yes, definitely agreed that this area needs a lot of work
Worked for me. Here's [my first experience with Scala](https://tersesystems.com/2012/12/16/problems-scala-fixes/). I also recommend: * [The Neophyte's Guide to Scala](http://danielwestheide.com/scala/neophytes.html) * [Another Tour of Scala](http://naildrivin5.com/scalatour/) Both of which are oriented towards the practitioner rather than theorists.
`implicit def listEq[A](implicit aeq: Eq[A])` because you can only define equality for a list if there is already a definition of Eq for A. Can't do that with trait implementations. 
&gt; Just curious. What is consider good idiomatic Scala? Just pure functional programming without OOP? I would probably have a look at the style of code produced by typesafe. Pure FP code without OOP is only considered pure idiomatic code by certain groups of people
Great article. One nit though is that I thought even though view bounds were deprecated in favor of context bounds, you can still have the same functionality. e.g. instead of: def appendItems[T &lt;% Appendable[T]](a: T, b: T) = a append b have say def appendItems[T : Appendable.VB](a: T, b: T) = a append b with object Appendable { type VB[X] = X =&gt; Appendable[X] //... } 
Cool will check it out.
I have better, actually: a series of blog posts on the subject, [Type Classe from the Ground Up](http://nrinaudo.github.io/2015/11/21/tcgu-part-0.html). You might find section 2, [Composition](http://nrinaudo.github.io/2015/11/23/tcgu-part-2.html), of particular interest, although reading section 1 can't hurt and will help you see where the examples come from. Do note that this is still very much a work in progress - or, should I say, a recently resurrected work: it started as an internal training document, sort of evolved into something bigger, but working in a vacuum can be taxing and lack of feedback killed my motivation. Writing isn't something that comes naturally to me, especially in a language I wasn't born to. Your question convinced me to publish the series in its current, incomplete state, and I hope you find it useful. Do let me know if you have any feedback / questions, or if you feel you'd benefit from my completing the series, as that certainly would help with my dwindling motivation.
Point out that even the standard library makes extensive use of type classes? It doesn't get much more mainstream than that. If you're looking for a concrete example, look at all the "magical" list methods - how do `sum` or `max` work? Note that's also an example of where the standard library doesn't go far enough. `sum` and `max` are exactly the same operation (`fold`), but with a different monoid (`0, +` and `MinInt, max`). But maybe don't use the word monoid if you're trying to convince someone that type classes are not arcane...
The standard library is also a library. As I sad in my first lost the use case is clear for libraries 
GreAt thanks, I'll definitely take a look
The two primary things that typeclasses let you do that normal interfaces don't is 1. Conditional implementation based on generic parameters. For example, you could have an implementation for Reads[List[A]] (Reads enables you to parse JSON) iff there's an instance for Reads[A] but not if there isn't. 2. Typeclasses don't require an instance to provide methods. a List method like `def foldm(implicit mon: Monoid[A]): A` can provide a useful default value from the monoid implicit when you have an empty list. 
The comparison to Ruby, Rust, and Go isn't really apples-to-apples because Scala leans so heavily on the JVM, which obviously extremely mature, for core libraries and the runtime.
hey, we actually do the same thing.. I was more interested in which software people use to host play. We are now migrating to docker/jenkins/beanstalk stack, which works perfectly and I can only recommend it. Btw you can also use env variables in your configuration files with ${?ENV_VAR}, these will only override the variables if they actually exist. For example: db.url = localhost:5432 db.url = ${?DB_URL} this is great for deployment, since you won't have any deployment specific info in your git repo then :)
Yeah I have had a similar experience over the years ,which is why I am no longer really on the channel. The problematic people currently seem to be less active/aren't on the channel anymore, but its a little too late in my opinion
You *absolutely* can become productive in Scala in much less than six months. For reading material, I suggest you go through *[Programming in Scala, Second Edition](http://www.amazon.com/Programming-Scala-Comprehensive-Step-Step/dp/0981531644)* by Martin Odersky, the creator of Scala. Skip the stuff on Actors though, since Scala Actors aren't thing any more (they have been replaced by [akka](http://akka.io) actors). In addition, I recommend you read *Functional Programming in Scala*, and probably not before Programming in Scala. You may also want to check out the Coursera course *Functional Programming Principles in Scala*, also by Martin. Links to these resources (and more) are available in the sidebar. Edit: Wrong link
Thanks for this!
Thanks! I agree with everything except the last point (I like override because if I mistype the method, I am warned that I'm referring to a non-existing method), but I do agree with the "that's a personal one" part ;) I'll fix it when I have time. Thanks again for your feedback.
I found *Functional Programming in Scala* is a bit of hard for me :-(
It is quite a different way of thinking. You could try doing the exercises that you understand now, keep going, and then later come back and try the ones you skipped.
What's wrong with the actual manual for that?
I can confirm that Programming in Scala (first 20-or-so chapters; I would skip actors for the given reason, but also xml, swing etc.) is a great intro combined with the Coursera course by Martin (its sequel Principles of Reactive Programming is also pretty good). These will provide enough functional programming insights for a beginner; no need to jump into the "Functional Programming in Scala" right away, at least in my opinion. You could also check out other Scala techologies your company of choice uses, such as Akka, Play, Spark, etc. and get familiar with them.
Because it is hard. Just come back later to the exercises which you can't do right now.
It's not you. It's a good book but it's very advanced. I wouldn't recommend reading it without at least a couple of years of experience in Scala. And even then, it's likely that a lot of its material won't make sense to you. Most people who recommend it likely have years of experience with Scala, Haskell and some decent knowledge of category theory. 
Perhaps take a look at the answers [here](https://www.reddit.com/r/scala/comments/409lki/how_do_i_move_beyond_the_scala_basics/) … at least those from me will be the same.
I never said they were, but this knowledge certainly helps going through the book more easily. I would speculate that a lot of people who give up on FPiS after a few chapters do so because they lack a few foundations in these areas. 
Sure. I'd just like to forestall it being a self-fulfilling prophecy to the greatest extent possible.
This does seem quite nice. However, what is the deal with the strange attribute access syntax? [From the official Play JSON tutorial:](https://www.playframework.com/documentation/2.1.1/ScalaJson) JsValue = json \ "user" \ "name" What is wrong with `JsValue = json("user")("name")`, like what nearly every other language does? Why is there a symbolic function recommended for recursive searching? val emails: Seq[String] = json \ "user" \\ "email" emails: Seq[play.api.libs.json.JsValue] = List("toto@jmail.com", "tata@coldmail.com") Why can't this be `json("user").findAll("email")`? (Or some other name than `findAll`; it doesn't really matter.) I'm new to Scala, but I still struggle to see why all of these very use-case-specific, very unique custom symbolic functions are used seemingly everywhere.
You can't easily abstract over `copy` ... consider whether this is really a useful thing to do. You *can* do it by making `Command` an F-bounded type or (better) a typeclass but this introduces complexity that you're probably best off avoiding if you can. I wrote a [short novel](http://tpolecat.github.io/2015/04/29/f-bounds.html) about this problem. So I suggest starting with the simplest possible thing. Make a `sealed trait` for your command type and then `case class`es as constructors and see how far you get.
Your short novel is exactly what I was looking for, thanks! I just didn't know the name, and specifically how to implement it. Is there a name for the `&lt;:` part by the way? I always struggle to find more info on things like this because of the syntax.
It's an **upper bound** constraint. If you introduce a type variable `A` as `[A &lt;: Foo]` it specifies that `A` must be a subtype of `Foo` (or the same as `Foo`). Likewise there is a **lower bound** constraint `[A &gt;: Foo]` that specifies `A` must be a *supertype* of `Foo`. These can be combined as `[A &gt;: Foo &lt;: Bar]`. All type variables are bounded, but when unspecified the upper bound is `Any` and the lower bound is `Nothing`, so `[A]` is really just shorthand for `[A &gt;: Nothing &lt;: Any]`.
That's extremely useful to know, thanks! One final thing I'm just struggling with at the moment, is how you can pass one of these into something else. The implementation of the Command class looks really nice now, and configuration is super simple. But I can't figure out how to pass that command into something else to add it to a Map, what type do I specify that the Map has? i.e: val commands: Map[String, Command] = Map() ¯¯¯¯¯¯¯ Where Command looks like: trait Command[T &lt;: Command[T]] { // ... } I'm using this in another immutable class, which all of the commands in the application are registered within, so they're added via a `withCommand` method: `def withCommand(command: Command): Application = {` Obviously I need to specify the type, and I tried `Command[Command]`, but that gets me nowhere (I'm assuming because the Command that's specified as the type would also require a type). I also tried `Command[_]` to no avail.
I don't disagree that much with your other statements, apart from this one here &gt; is too abstract and theoretical to master, etc. are wrong. I don't think anyone can claim this, at least in a general sense. From my anecdotal experience, certain people grasp these concepts **much** easier than others (and this also goes in some way of explaining why other subjects, such as pure mathematics, also have a similar discrepancy)**. By extension (in my opinion), people who often pick up these concepts quite quickly tend to surround themselves with other people who pick up these concepts quite quickly so it does tend to create a bubble which isn't attuned to programmers in general. A lot of people claim that picking up these things is quite easy, but the people that they are anecdotally referencing have usually (either directly or indirectly) gone through some selection process. Note that I don't think this is a bad thing, if you have a group of really smart people, you should definitely use that to your advantage by picking a language and an ideology which uses that to your best advantage. &gt; They're just wrong, usually accompanied by a lot of bafflegab carefully constructed to sound like moderation and the voice of reason, with those of us who do this, for a Fortune 100 company, every day, and ship real products being lunatics. I think this statement does much service, although a single Fortune 100 using scalaz does do something to discredit the opinions you don't agree with, it also doesn't terribly do a lot to credit your opinion either. Like if hypothetically there are more Fortune 100 (or 500) companies, that use Scala in a non pure FP manner, does this mean in general that using Scala in a different way is more practical? I mean people at this point claim the "good = popular" fallacy, but that is essentially the same argument you are using, but in a cherry picked way. ** Some people at this point would claim that this is due to a systematic issue (i.e. we are teaching this things wrong, its a problem with university etc etc), at this point I would claim that this has empirically always been the case, both in terms of history and culture.
As I've written before, there's been enough verbal and virtual ink spilled on this, both by me and by others. I'll just say "Curry-Howard" and "good-enough typed functional programming language" once more, and those who are motivated to follow the bread crumbs will, and those who aren't, won't.
&gt;By extension (in my opinion), people who often pick up these concepts quite quickly tend to surround themselves with other people who pick up these concepts quite quickly so it does tend to create a bubble which isn't attuned to programmers in general. I have never heard anyone say they 'picked up functional programming quickly'. Rather, you often hear that while you may *think* you learned your imperative coding quickly, likely you aren't remembering just how clueless you felt in the beginning. Programming is hard to learn. Pointers are hard to learn. Recursion is hard to learn. Applicative composition is hard to learn. You just don't remember that the first two were hard, because it happened a long time ago. And the latter, like the others, becomes trivial once you have that lightbulb moment. &gt;that use Scala in a non pure FP manner, does this mean in general that using Scala in a different way is more practical? I don't hear many detractors of pure-fp/scalaz style scala saying "oh that's a perfectly valid way of writing code, it's just not as good as java++ style". Rather, many people say it is *impractical* or *too hard* to have a big team be productive in that style. I think that our company disproves that. 
I have re-read the manual and again I don't get it. The examples pile boilerplate over boilerplate without doing any real work. Worse even, the examples use macros to create even more boilerplate in the background. I have no understanding under which exact circumstances sbt will call which parts of the code. &gt; How to obfuscate code isn't relevent, how to setup a plugin is relevant. And here I have the completely opposite opinion: I have no real interest in setting up a plugin. I don't even have interest in the plug-in in the first place. I usually just want some quirks in my build. If I could reach this by calling a shell script, this would be totally fine for me. My expectation on the sbt documentation is to help me to reach this goal. I expect it to explain internals only in this context and skip them if they are not helpful for me. So with this context, the documentation is not helpful for me. Regarding the play plugin: its probably the most used plugin in sbt. And it modifies the sbt build very heavily. My expectation on the sbt documentation is to give me the tools to understand these modifications and their side effects. I expect the documentation to deal with interference between plugins (e.g. some plugins don't work when the Play plugin is present). But unfortunately the sbt documentation just disappoints me here. 
I'll take a look at this when I get home. I've been dabbling with it because it adds significant value to us at work if I can make this happen. Probably doesn't help that I have never used SBT before. The work project actually uses Maven.
Yep, that's it: Command of anything. Perfect for when the code doesn't touch the type the command is about.
Any link to a specific resource?
&gt; I have re-read the manual and again I don't get it. The examples pile boilerplate over boilerplate without doing any real work. Worse even, the examples use macros to create even more boilerplate in the background. I have no understanding under which exact circumstances sbt will call which parts of the code. Handwaving doesn't help. You should be more specific if you want to get your point across. &gt; And here I have the completely opposite opinion There is not opinion to be had here. A "SBT Manual" is about SBT. Not about code generation. Not about web development, not about Play. SBT is none of those things. &gt; I have no real interest in setting up a plugin. I don't even have interest in the plug-in in the first place. I usually just want some quirks in my build. If I could reach this by calling a shell script, this would be totally fine for me. My expectation on the sbt documentation is to help me to reach this goal. The manual certainly does help you reach that exact goal. Calling shell scripts isn't something SBT does, that's an API built into the standard Scala library, but the SBT Manual has a section on that [anyways](http://www.scala-sbt.org/0.13/docs/Process.html). More evidence, you haven't even attempted to read the manual, or if you did, then you have severe reading comprehension problems. &gt; its probably the most used plugin in sbt It's not(that title eaily belongs to intellij or ensime plugins) but even we pretended it was, it's still not a part of SBT, and why should it get any more special treament that all these [other plugins](http://www.scala-sbt.org/0.13/docs/Community-Plugins.html#Web+and+frontend+development+plugins)? &gt; My expectation on the sbt documentation is to give me the tools to understand these modifications and their side effects. You mean the Play documentation, which I already linked to the part of the play documentation which does explain a rather concrete list of effects that the plugin has on the build. I've never used Play, I don't know if the Play documentation is accurate, or complete, but it does exist. &gt; I expect the documentation to deal with interference between plugins (e.g. some plugins don't work when the Play plugin is present). I'm tempted to say that claim is bullshit, but I've never used Play, so I don't know for sure. I still don't understand why you would expect that be a part of SBT's documentation, but not Play's or the plugins that conflict with Play.
My installation script is here: https://gist.github.com/colindean/efa03ccc6c2eb4a0b91f The download URL is in it.
Scala for the Impatient is really useful combined with the Cousera course.
ScalaCourses.com has high-quality online courses: http://scalacourses.com
Holy shit dude. You are a golden god. It does seem to be working now. Thanks. Not sure I would have run into this if I was using my normal workflow but now I have this and learned a little bit on the side. 
It's really a good idea to use the type class approach rather than f-bounded polymorphism. The code ends up being much cleaner and more flexible. And there's still ways to cheat the type system with f-bounded
I used a work pulling pattern for this instead of a router to solve the same issue. Each worker would register with Actor1 and request work from Actor1. Actor1 keeps track of what work is left to do and what workers are available, and when all work is complete, it responds to the worker that there is no more work to do. The worker can then de-register from Actor1's worker pool. When there are no more workers in the pool then Actor1 knows all work is completed and can send the results to Actor2. Here is a much better explanation: http://www.michaelpollmeier.com/akka-work-pulling-pattern/
A good starting point would be to learn JAVA first, since theres much more documentation and courses on that. Then transition to scala.
This particular course, Stairway to Scala, was developed by and is offered by [Escalate Software](http://www.escalatesoft.com/training). Other companies, such as [Typesafe](http://www.typesafe.com/services/training) and [Underscore](http://underscore.io/training/), also offer trainings, but those are class room, not online, courses. 
You might want to check out rob Norris's doobie for some inspiration http://youtu.be/M5MF6M7FHPo
Thank you =)
I recommend [this](http://debasishg.blogspot.com/2009/07/dsl-composition-techniques-in-scala.html) blog post of Debasish Ghosh's, and not just because he's kind enough to give me a shout-out in it. :-) I'd also encourage you to read both the paper and the earlier posts he links to. A lot of us in the community these days see "Scala DSL" and do indeed think of scalaz's Free Monad. Others have already referred to [/u/tpolecat](https://www.reddit.com/user/tpolecat)'s Doobie project, which is great, but let me explicitly link to his [preso slides](https://tpolecat.github.io/presentations/lambdaconf-15.pdf), which are the best exposition of scalaz's Free Monad available anywhere, IMO. Finally, Free comes with some performance costs. A lot of recent work aims at addressing that, so maybe check out [FreeR](http://mandubian.com/2015/04/09/freer/) when you've developed a bit of familiarity with Free—assuming that's the way you decide to go, that is. :-) And don't hesitate to ask questions, of course!
Thank you kindly for your feedback. I will look into all the links you addressed. Also, after understanding the concept of type classes and implicit conversions i managed to understand Monads and Functors and where they are applied. Step by step i begin to make sense of things. In my job i am programming in scala and i am hoping to switch to FP as much as i can (hopefully even port some imperative code to FP). I figured using higher-order functions and chaining futures was not enough and i wanted to understand exactly what i was using but i admit i was not expecting this to be so complex and challenging but i am here to stay. Again, thank you for the time and advice,
I recommend looking at Cats instead of Scalaz. Cats places more value on approachability, and has some nice docs that might help you get started: http://non.github.io/cats/ From what you have said it seems you'd benefit from seeing the abstractions in practice. So looking at Cats now will probably be beneficial.
Although Scala allows you to write embedded DSLs with fancy syntax (ridiculous example [here](https://github.com/tpolecat/basic-dsl#an-example)) I encourage you to resist this temptation and instead focus on the operations you wish to provide, ways of combining these operations, and ways of interpreting these operations to perform some kind of useful work. Leave the syntax tricks until later! You probably won't need them. A couple people have linked to my work on doobie (thanks!) which uses a very general technique to provide a compositional API for writing JDBC programs. This has become a popular technique and is as good a place as any to start, but the types and abstractions are likely to be unfamiliar. So I encourage you to check out the material from /r/noel and of course [FPiS](https://www.manning.com/books/functional-programming-in-scala).
&gt; have yet to start looking into scalaz. Should i start looking at it before feeling confortable with FP concepts or should i learn how to use scalaz along with learning FP / other scala abstraction techniques (type classes, implicit conversions, DSL's, etc) I'd say go piece by piece. Learn one concept, check it out in Scalaz, start using it, etc. I'm a contributor to Scalaz and *I* don't know how all of Scalaz works. It's a large library that has a lot of synergy, but each piece is valuable by itself. It's not a framework where you really need to understand it as a whole. http://eed3si9n.com/learning-scalaz/ is an excellent resource for both learning the overall concepts (type classes) and how to use them in Scalaz. Drop by the #scalaz IRC channel too if you have trouble, there's a REPL in the chat so if you have compile issues we can help out in real time! 
http://www.scalakoans.org/
This is the second course in the series on using Scala for data science / big data analytics. It's developed by Typesafe.
liftweb-json is my go to json parsing library. Extract out into case classes and convert json to classes on the fly and vice versa.
I'm trying to do a similar thing using Maven with a work project (get coverage results of running external requests as tests against running application). Does adding the scalac-scoverage-runtime (as opposed to scalac-scoverage-plugin) to the pom.xml allow you to do this? 
It worked for me with sbt. I'm going to try it with maven soon.
Those capitalist pigs!
It's just a joke. Kind `* -&gt; * -&gt; *` means your contribution can allow three people to attend.
I've emailed them 3 days ago and they haven't replied back to me so far.
Thanks for your help! Hopefully they wont take much longer finding a new home.
Thanks for the links, very helpful. Free Monads have been on my radar for a few months now but I've never really understood them until I just read the slides from the presentation you linked to. I'm only left with one question, does this pattern have a place in our everyday applications - for instance a REST API or something similar? Or is its utility mainly for structuring libraries and such?
Another reasonable question is "why not use [Shapeless' generic typeclass derivation](https://meta.plasm.us/posts/2015/11/08/type-classes-and-generic-derivation/)?"
Zinc is used in pretty much any build tool that supports scala compilation. This doesn't really tie scalac to SBT in any significant way.
Welcome! People tend to use a variety of of development environments to work with Scala. The two most popular options are probably: * [IntelliJ IDEA](https://www.jetbrains.com/idea/) with the Scala plugin - this is what I personally use. IntelliJ's Scala plugin is really great. I find it integrates well with build tools like SBT, provides really nice interactive debugging, refactoring, code completion, testing support. * [ScalaIDE](http://scala-ide.org/) - this is a form of Eclipse, I've never used it but it seems about as popular as IntelliJ. It's also officially supported/sponsored by Typesafe. Others prefer to use text editors, and for Sublime, Atom, Vim, Emacs, etc, many use [Ensime](https://github.com/ensime). For Sublime, check out [Ensime Sublime](https://github.com/ensime/ensime-sublime)
[what an interesting name!](https://en.wikipedia.org/wiki/Scatology)
:-)
Where can I read up on the current status of scalaz vs cats? My understanding is there's some sort of drama/confrontation/schism but I have no sources. iirc scalaz used to be on typelevel's old website and it is used in several of the listed projects. We use scalaz at work mostly because at the time of our adoption of it cats wasn't even close to being worth using (not sure if it quite existed even.) How much has that changed? I'd say I'm still only comfortable using scalaz since cats is explicitly experimental and I know scalaz 7.1.x gets some industry use and is more fully featured and more stable. Will there be a point in the near future when it'll be a no-brainier to use just Cats?
I'm personally a huge fan of intellij. The scala plugin is very mature and works fine with the community edition. 
Nailed it.
I'm using spark and mllib for my master thesis in machine learning, more specifically text classification. I've been using it for 6 months now, and I love it a lot. 
not sure what would be the the use case of this? Why is it better than a monadic approach? 
Pardon the ignorance, what is the monadic approach in this context?
&gt; Oh ffs, after re-reading your post I think I've answered a question that you didn't ask. Nope you answered one of my questions which was a comparison of the technical merits of cats vs scalaz. So thanks! 
Also, do you know how far back in Scalaz those Free monad perf wins go? We use 7.1.3 at work currently. Not much Free monad stuff although I may use it soon. From my understanding the largest worry is quadratic complexity that would mainly become a problem when Traversing with Free?
Because it doesn't require monads? 
As others have pointed out, I'm not sure about your use case. But I've written a little helpful library a while ago available here: https://github.com/ExNexu/akka-actor-locking An alternative might be http://eng.kifi.com/introducing-reactive-lock-scala/
I haven't used it, but there's [ReactiveLock](https://github.com/kifi/ReactiveLock), which has a `withLock` and `withLockFuture` syntax similar to what you're suggesting, along with additional concurrency control. Otherwise, yeah, `AnyVal`-based extension methods would get you most or all of what you want.
I highly recommend looking into [Finch](https://github.com/finagle/finch), a purely functional library for REST services on top of Twitter's [Finagle](https://github.com/twitter/finagle). There's a great overview presentation by Vladimir Kostyukov called ["Finch: Your REST API as a Monad"](https://skillsmatter.com/skillscasts/6876-finch-your-rest-api-as-a-monad). 
When it was Cats 0.3.0 vs Scalaz 7.1.3, Scalaz was over twice as slow. Now that Scalaz is 7.2.0 it's faster than Cats. I'm not sure what the complexity is - I remember from an algorithms book I read waaaay back that there's an easy blackbox way to calculate it via measurements. I should dig that up and add that functionality to scalajs-benchmark when I have time.
Oh good, sometimes I can a lazy reader :)
I believe the person means encapsulating the code in terms of identity, map and flatMap operations, and types representing the value and effect.
I think there are two aspects to why this area is not especially well-serviced: - it's hard to fulfil all use cases when dealing with asynchronous code (e.g. Futures). It's quite possible that lock lifetimes may need to extend beyond the lifetime of the current block scope, and I don't think this is especially easy to do in a generically correct way with this approach (some comments here refer to partial solutions that work in some cases). - I know that in my code (and I'd say this is generically quite common with scala-only code), there is VERY little synchronisation going on outside the framework. I then deal with inter-thread communication using a primitive that has thread-safety built in. Many people use actors, there are other options. If you want to write your own, you have to be very careful when dealing with Futures, and as with most bugs around locking, you won't know you got it wrong until you get mysterious corruption and failures...
Why locks? val promise = Promise[Int]() val future = promise.future //some thread otherValue = Await.result(future, someTimeout) //some other thread promise.success(1) Even better, don't await, and compose (the monadic approach) with the future. //some thread otherValue = future.map { r =&gt; r + 1 }
Those are all traits, not type classes. RTFM if you don't believe me. The way they're used is actually closer to ML module signatures than to Haskell type classes. ML lets you name modules, and you can have multiple implementations of the same signature, even on the same type. For example for some types, it can make sense to define more than one ordering, so you could sort strings lexicographically in one place or by length in another. With ML modules, that is fine since you can define multiple implementations of `Ordering` on `String`. With type classes, you can't do that (at least not without restructuring your code in weird ways to keep scoping of multiple orphan instances from conflicting). So, in that sense, and also in terms of syntax, `Ordered` is closer to being a type class than `Ordering` is. Also, per Haskellwiki: &gt; Type class instances are special in that they don't have a name and cannot be imported explicitly. This also means that they cannot be excluded explicitly. All instances defined in a module A are imported automatically when importing A, or importing any module that imports A, directly or indirectly. Do you really think that is true of Scala traits as well?
[removed]
I'd definitely go this way. It's worth noting that most scala concurrency utilities (e.g. Futures, Actors) and most modern libraries in general don't do thread-level concurrency but re-use the same thread in order to host multiple potentially parallel computations; locks are to be avoided when using sub-thread concurrency as they will waste a lot of resources and could cause deadlocks. 
yeap , u r right about the syntax ,but "Thinking functionally" series of that blog ,the way it explain FP basic and correlation with scala in most simplest language(80 % of the time) is quiet good.
&gt; The reason Cats exists is entirely political I disagree with this, at least personally. While people have their own judgements about what happened, cats was a reaction to a community that was becoming highly antagonistic, personal and elitist, enough so that it causing detriment to the FP community. The person that you mention has personally insulted me numerous times (by insults, I mean the literal definition of the word, and bad enough that I don't want to mention it on Reddit), and it also wasn't tied to any technical context. Such instances meant that I personally had zero interest in using, contributing or recommending Scalaz (if anything, the opposite). His "communication style" has also killed channels places such as #scala (thank god for Gitter now). Saying "This is just a political and a vendatta against a person" is just an attempt of brushing away inconvenient truths. I don't agree how stuff was handled with scalaz and how he was attempted to be forcefully removed, but lets call a spade a spade, this wasn't purely political. Furthermore, not everyone on the panel that you talk about were directly involved with this incident (people can see the panel themselves here https://github.com/orgs/typelevel/people). Honestly the way I see it, your issue is primarily a made up one. EDIT: Also to be fair, Typelevel CoC is the same CoC that is used by many languages (including Haskell!, http://www.haskellnow.org/wiki/CodeOfConduct). Saying that its problematic because people are in charge of it is frankly hilarious. Resolving issues that relate to CoC is an issue that needs to be dealt with by people, because it is a people problem, you can't make an effective CoC that completely agnostic to human interference. P.S. I also love how people the same people that claim that this incident is entirely political, hypocritically use the word "communication style" which honestly is a politically correct statement that is hiding something far more obvious.
&gt; Dammit it's an old April fools! Whaaat! I took it completely seriously, skimmed it, and gave it an upvote because it covered important subjects like making illegal state unrepresentable, and even took the time to explain things like *why* conciseness is important, not just how to achieve it. Maybe it would be worthwhile for the author to compleye the conversion from F# to Scala and to make it a real website instead of an April fools? &gt; F# has problems far worse than scala Like what? Not trying to defend the language, since I've never used it, I'm just curious. Because dressed in Scala feathers, F# looked pretty good!
its an april fools you imbeciles your first clue something is wrong: 2. Scala syntax in 60 seconds A very quick overview on how to read Scala code
Alright. Now this is a very surface observation I admit, but one of my big problems with Shapeless(and many things in scala frankly) is this line right here &gt; Now, we may copy and paste the piece of code above and call it a day. Instead, this entire post is devoted to achieve a more deep understanding of this short, but very dense snippet. We then go on for quite a few paragraphs explaining a single line of code. To me this seems not good, and not worth it. What is gained in the end? A very heavy cognitive and semantic model to achieve something that, IMO, doesn't really seem to be worth it. I suppose if you mostly compose copy and paste snippets, it would be fine... I am sure I am missing something though. 
&gt; The person that you mention has personally insulted me numerous times (by insults, I mean the literal definition of the word, and bad enough that I don't want to mention it on Reddit), and it also wasn't tied to any technical context. I don't believe this, and the fact that you "don't want to mention it on Reddit," but just did while insisting it was a non-technical "literal insult" means we have to take your word for _everything_: that it happened, that it "wasn't technical," and that it was a "literal insult." I've never observed Tony Morris (you won't even name the accused!) to simply insult someone, as opposed to sharply question their approach to a problem or to him. Ever. And your style in this comment alone reflects no credibility on your claim. &gt; His "communication style" has also killed channels places such as #scala (thank god for Gitter now). My experience of #scala is that it's perfectly healthy. &gt; Saying "This is just a political and a vendatta against a person" is just an attempt of brushing away inconvenient truths. This is at least the second time you've said "inconvenient truths" without offering any. &gt; I don't agree how stuff was handled with scalaz and how he was attempted to be forcefully removed, but lets call a spade a spade, this wasn't purely political. The crux of the issue, and here you even manage to contradict yourself. &gt; Furthermore, not everyone on the panel that you talk about were directly involved with this incident (people can see the panel themselves here https://github.com/orgs/typelevel/people). I didn't say they were, know the overwhelming majority of them, work with two of them, and have linked to Stew O'Connor's thoughtful [state of cats](http://stew.vireo.org/posts/state-of-cats/) before. Let me quote the relevant portion in full, to forestall possible confusion: Many are sad that it got to this point. A lot of feelings have been hurt, sometimes unnecessarily, sometimes irreparably. Many other people are feeling discomfort because they don't want to participate in the discussions that have caused this rift in the scala functional programming community at all, but now feel like they are making an implicit statement in lieu of an explicit statement just by choosing which library they use, or which libraries their public projects depend on. I know no one, myself very much included, who disagrees with this. &gt; Typelevel CoC is the same CoC that is used by many languages (including Haskell!, http://www.haskellnow.org/wiki/CodeOfConduct). Saying that its problematic because people are in charge of it is frankly hilarious. I explicitly, twice, commented on the _non-democratic governance model_ and the _forceful removal_ problem that you, again, have acknowledged yourself. At this point, with a lengthy series of demonstrations of intellectual dishonesty on your part, I completely understand why Tony has no patience for you. You _do not discuss or debate in good faith_. &gt; P.S. I also love how people the same people that claim that this incident is entirely political, hypocritically use the word "communication style" which honestly is a politically correct statement that is hiding something far more obvious. You don't get to hide the insult you allege and accuse _others_ of hypocrisy.
 Now, I agree with you, these whole shapeless/scalaz etc things provide almost nothing compared to how much time you need to spend with it. This is the effect of Haskell where you master category theory and write unreasonably complex code(while not realizing it for a while). Some people still think about Scala as a 'worse Haskell' while they could just open their eyes and compare the bare power. Edit: the scalaz guys are here, I'm waiting for the 'clarifications'!
&gt;the members of both projects hate each other They don't. In fact most involved in both sides regularly see each other in person or work together, hang out together, argue in person over beers after conferences and generally like each other. Our company has four of the top ten contributors to Scalaz (for the latest 7.2 release) as well as a couple top contributors to Cats.
Does [this](https://www.reddit.com/r/scala/comments/41ohxv/be_like_water_a_shapeless_primer/cz4290i) help?
&gt; Do you think that any Java project which uses singletons "goes beyond using features of the language"? Or that any project using anonymous classes as poor man's lambdas did until Java 8? No, but I do think that any Java project that uses singletons or anonymous-classes-as-lambdas so much that they feel the need to build their own syntactic extensions in order to generate that code does. As another example, if you were to take a decent Scala codebase and translate it to Java by converting every `object` into a singleton (as opposed to converting some/most methods to `static` methods), the result would probably be quite terrible. &gt; What of this doesn't apply to Scalaz's "type classes" as well? Some of it does. Scalaz does go out of their way with what they call "syntax" in order to make their "type classes" more closely resemble Haskell type classes in usage. So, why do they call them "type classes" and go out of their way to make them feel more like Haskell? I don't know for sure, but I suspect it is because of where they're coming from.
Yeah, I suspected so, hence the thread :) hm, that rings a bell... this is you? http://noelwelsh.com/
Quick search: https://twitter.com/search?f=users&amp;vertical=default&amp;q=scala&amp;src=typd I'm also there (sjrdoeraene). I mostly tweet about Scala.js-related stuff, because I'm totally not biased ^^
Hi! Yeah, I know, but the search brings up the most popular guys around. I don't think that someone with 5k followers will care about what I have to say, or even notice me on the feed (given the fact that he does follow me back, which is unlikely). On the other hand, r/scala is a smaller and therefore more personal community which I see as my peers instead of Scala-celebrities. Perhaps I phrased it wrong - I'm not really looking for news like new Scala compiler is out. I'll get that from Odersky. But I'd love to see that some "random" John Smith (whom I met and followed because of r/scala) just made a cool library for, say, parsing CSV files. EDIT: Since you're the author of Scala.js, you're somewhat of a celebrity that I'm talking about so no hard feelings :) I just wanted to find some "equals" here; celebrities are easy to find. See, it's hard to get "in the game" as a Twitter newbie by simply following the famous Scala people who most surely will not follow you back. It feels stupid sharing something with 10 people :) (and I don't want to add random people around me who have nothing to do with programming because it will become a new Facebook for me; I don't care that someone's dog just ate a sock). BTW nice to "meet" you, I like your work!
 Finally! I thought you'll never see the quotation mark! I've used both shapeless and scalaz and such tools don't give any simplification - like your posted code. What you can spare with them is a few lines of code while introducing a lot of dependencies and unholy operators with rare terms. That is why Scala can't progress in popularity - because of people who write horrible code when they can and post it everywhere to scare the outsiders. Actually, you can't show me how they simplify things(I've already seen everything) because the vanilla lang has cleaner and more understandable powers. All the people who want to use Scala as a pure FP or pure OOP language or want to see it through an obtuse lib will write unnatural code. This is the same effect I've seen when I was with Haskell. One part of the community tried to create clean code and tried to use the abstractions well while the other(bigger) part didn't have any fear for inventing weird function/operator names, writing long 'where' parts and introducing complex term which only spare a few line while binds their hands in the future. Of course, there are some exceptions but in the summary it doesn't worth the effort to invest time in any extremist hype.
Yeah, mostly but for example they hosted Jon Pretty once and the talk was English. Also most of the participants are English speakers. You should also look out for the Scala Italy convention. Last year all the talks were in English. 
I found this to be very informative on how the underpinnings of shapeless' implementation of bijections/injections along with a great understanding of HLists and their potential. I personally found the Prolog sections a little dry, but that's just me. There's a lot more to shapeless than what's covered, but this was a really good start. I'm using this and [Fommil's Scala eXchange talk](https://skillsmatter.com/skillscasts/6875-workshop-shapeless-for-mortals) to learn. These two together are really nice.
+1 to all of those except @tpolecat (Rob Norris) who I suspect is just a cat mashing on the keyboard. If you start with that set you'll quickly accumulate more by reference. All the people Paul mentions are interested in FP and tweet broadly on the subject. Also fwiw various combinations of us are also on the `#scala` IRC channel and/or the `scala/scala` gitter channel.
I don't disagree. In fact, I think the issue is closely related to the observation that agile development methodologies surface team dysfunction rapidly: the question is how you cope with it, which is ultimately a question of _culture_. I don't refer to the http4s/Doobie example to say "See? It's easy!" in other words. Rather, I see it as embodying a question: how do we move forward in FP so things really are as easy as such an example makes it look? Admittedly, a REST API with a database and JSON is almost cheating: an HTTP handler is rather obviously some flavor of `Request =&gt; Response`, ditto encoding/decoding JSON, LINQ etc. have gotten many people used to the idea of "query monad," etc. But still, it feels to me like there's an opportunity, culturally, to achieve some of the same kinds of "of course" reactions to doing things functionally as in OOP. Finally, I have to say I'm encouraged, because people are discussing the issues; FRP in JavaScript is helping people in the browser; people see what teams like mine at work can do with nary a side-effect in sight; and so I think the overall trend is positive. I'm sure I'll be retired before FP has anything like OOP's cachet, but I don't at all think the situation is hopeless.
I'm encouraged too, and it's largely around software culture. Even in Blub-land, I'm seeing a lot more awareness of how immutability can help avoid bugs and provide for more scalability, so it's not at all hopeless. 
That talk and "there's a Prolog in Scala" were my main sources of inspiration for this post (they are referenced at the end). The Prolog section is there because, to me, it was really the turning point to getting a working mental model of the way implicit values can be used. Thanks for your feedback!
I'd just [guava](http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/Striped.html).
Nice meeting you, I also use Twitter for following Scala folks. I'm [@alexelcu](https://twitter.com/alexelcu) on Twitter, my website is at [bionicspirit.com](https://bionicspirit.com) and I'm currently interested in reactive programming, hence my current love affair with [Monix](https://github.com/monifu/monix).
I've seen a few people ask why. What if you wanted a shopping cart, or to calculate TF-IDF, or if you didn't have time to code up a count min sketch you may want to use one instead. There are plenty of needs, it came up today when trying to calculate whether one word was another anagram of another.
The reason people ask "why" is because a "multiset" is conceptionally just a `Map[T,Int]`. I don't know why you need an extra datatype for that.
You can find some out there, for example https://github.com/nicolasstucki/multisets The standard lib has enough crap, I think it's ok not to be in there.
No. As someone coming from Python, I found that going into Scala is like going into C++. There's a whole lot of extra stuff that you can do but you shouldn't do. And people who have been coding in Scala for a while don't even notice it. But the people who are coming into Scala do and they're like "wtf."
There's a huge learning curve. So no, I am not overcomplicating things. The language is.
&gt; In short, it completely and utterly fails to apply the principle that "there is only one best way to do something, and the language should do it that way". This principle doesn't exist, and it wouldn't be desirable if it did. &gt; I have no idea where that variable is coming from and I cannot right-click on it to find out. Yes, you [can](http://blog.jetbrains.com/scala/2015/03/05/scala-1-4-eap-brings-advanced-implicits-analyzer-and-faster-play-compiler/). &gt; You have case classes which require the programmer to manually make them "final" and manually make their super-interface "sealed". I don't understand what you're getting at here, why would these things be default? Are you specifically referring to the encoding of sum types? &gt; You can make case classes mutable which totally defeats the purpose of having a case class. It shouldn't be possible I think, but it does not defeat the purpose of case classes, which make it easy to pattern match on value in a class. &gt; The context of the language is overly nuanced and over-complicated. WHat does this even mean?
I appreciate the correction about finding where the implicit variables come from. I thought the whole point of case classes was to represent things that are like "number", "token", "fraction" - that are immutable, that you can modify only on copy, that you can apply mathematical type operations on, that you can map and pattern match. It seems like case classes should be final by default. Either that or case classes should be able to inherit from other case classes. But since case classes cannot inherit from other case classes it seems like the sensible thing to do would be to make case classes final by default. And fully immutable. You could have a programming language called "Scala --" that would be Scala but with a lot of restrictions put in and duplicate features taken out and it would compile faster and be easier to learn.
I mean maybe Scala is an easy language if you already have a lot of experience with Haskell, OCaml, and Java, but if you don't know those languages you're pretty much screwed.
&gt; whose idea was it to replace the word "void", meaning "emptiness", with the word "Unit", meaning "a unit of measurement"? Yeah, this is unfortunate, other languages do this too. For example, Python decides to replace a perfectly well understood term from type theory "unit", and replace it with the term "pass" which literally means "to throw a football". What does footballs have to do with anything? &gt; val map1 = Map( 1-&gt;2, 2-&gt;4, 3-&gt;6 ) &gt; val map2 = Map( (1,2), (2, 4), (3, 6) ) &gt; ^ These are exactly the same thing. Couldn't you just pick one and make it the only option? Yeah, you can, compile with "-Yno-predef" &gt; In short, [scala] completely and utterly fails to apply the principle that "there is only one best way to do something, and the language should do it that way". Yeah, you nailed it on the head. It reminds me of another language that kinda fails in this way, namely python. as an example of how python doesn't stick to this principal, here is just a tiny fraction of the ways I can write a program just to do something simple like "Hello world": print 'Hello {}'.format('world') print "Hello {}".format('world') print """Hello {}""".format('world') print "Hello" ,; print """world""" print "Hello %s" % "world" print "Hello {0}".format("world") print 'Hello {who}'.format(who="world") print "Hello {who}".format(**{"who": "world"}) d=dict(who="world"); print "Hello {who}".format(**d) d={"who":"world"}; print """Hello {who}""".format(**d) d=["world"]; print 'Hello {}'.format(*d) import string ; print string.Formatter().vformat("Hello {who}", (), dict(who="world")) print("Hello {0}".format("world")) print("""Hello {}""".format('''world''')) &gt; --Speaking of symbols, there are FAR too many symbols in the Scala language. And the problem with symbols is unlike with function names, you cannot Google symbols to see what they mean. Actually there are very few "symbols" in the scala language, You actually mean function names, but any way, Yeah, I've noticed lots of languages are allowing symbols, for instance, with python3, now someone can write this program: # -*- coding: utf-8 -*- def λ(α): print(α) λ("Hello World") How the heck am I supposed to know what λ is? (I mean in this case I see the definition right there). But what if. like in scala, it could be in some other file? How do I google for λ? Some of your complaints, however, I'm not sure I can get behind: &gt; I mean you have the null keyword pulled in from Java which you should never ever use. Yeah, so don't use it unless you are dealing with awful java libraries. &gt; You have case classes which require the programmer to manually make them "final" No, you don't have to do this. &gt; and manually make their super-interface "sealed". You don't have to do this, and you often don't want this. Clearly there are times when you want to define an interface for some third party to implement, so you don't want sealed. When you DO use sealed, there are huge advantages with the static analysis the compiler can provide for you (like, Did I check all the possible cases here?). This is an option that is good to have. &gt; You can make case classes mutable which totally defeats the purpose of having a case class. No it doesn't. There are plenty of reasons to not use mutable anything, this isn't one of them. One of the most ubiquitous types in scala is the List, which uses mutability in its cons constructor case class. &gt; The context of the language is overly nuanced and over-complicated. It could be less complicated and have the same functionality and readability (and be quicker to compile) This is not at all unique to scala. It really sounds like what you want is that language where it is impossible to write code that is hard to read for anyone. Unfortunately I don't think this language exists. Fortunately, I don't think this language exists, I'd think I'd really miss the lack of expressiveness that must come with it.
 huge learning curve != overcomplicated
"Yeah, so don't use it [null] unless you are dealing with awful java libraries." -- That's something you know because you have lots of experience. When noobs in college go from Java to Scala, they carry all the Java stuff with them. Like null. The language doesn't do anything to stop them, so they just go with what they know.
Not interested in debating OP's rubbish, but I thought I'd point out that there are more uses of `_` than the two you mentioned. A few examples: * assignment operators: `def foo_(i: Int) = ???` * punctuation in method names: `def bang_! = ???` * hiding imports: `import scala.io.{Source =&gt; _, _}` 
Yeah, that's kind of what happens when one language maintains backward compatibility or backward interoperability with another language *cough* C++ *cough*. Backward compatibility is super important (so many good Java libraries) but it mandates backwards cruft. I guess what I'm trying to say is that if you have a language that already inherits cruft from another language, it should be more careful not to add any extra cruft on top of that.
If it were all up to me! (which clearly it shouldn't be). calling a java function A =&gt; B would turn that into something like A =&gt; IO[Option[B]]. We'd get rid of null, but then TONS of poeople would say "OMG WHY DO THEY MAKE ME LEARN IO MONAD WHEN I JUST WANT TO CALL A FUNCTION". You can never please everyone.
I don't even know if it's worth pointing out, but should we take for granted that learning a programming language should be done with internet tutorials or a 1 day course? From what you say it seems to me that python is a very easy language to pick up, possibly because it restricts its syntax to fixed and rigid use cases (no intention to say that it's a bad thing). Scala starts with not so many orthogonal features which mingle independently. It's a deliberate design choice, but it can create a somewhat confusing scenario for a newcomer. This is why it's probably better to study a book or two to begin with. I learned scala reading odersky&amp;venners' book which I find pretty clear and covers the language in depth. What I'm striving to understand is why several people assume that it should not be so, that learning should be done picking bite-sized chunks of knowledge here and there and deriving the rest from previous expectations. It could be that knowing java could be helpful, but sometimes it's just an hindrance to learning different approaches to do stuff, so I'm not so sure if this is relevant or not to the issue (same with Haskell or whatever other language).
I think none of the "duplicate features" would have any impact on compile-time.
&gt; that are immutable, that you can modify only on copy, It's expected that they are, and if the compiler were to develop some ability to analyze mutability, I'm sure case classes would be the first application. &gt; that you can map Case classes do not generate a map method. &gt; and pattern match From the scala-lang tutorual on [case classes](http://docs.scala-lang.org/tutorials/tour/case-classes.html): &gt;Scala supports the notion of case classes. Case classes are regular classes which export their constructor parameters and which provide a recursive decomposition mechanism via pattern matching. Mutability is mentioned nowhere in this description, only pattern matching. &gt; Either that or case classes should be able to inherit from other case classes. But since case classes cannot inherit from other case classes it seems like the sensible thing to do would be to make case classes final by default. A long time ago they could extend each other, but that was deprecated. If the feature were added today, I'm sure it would be final by default.
Don't get us started. Let's just begin with CanBuildFrom. And Seq (a terrible awful collection with no guarantees. Could be infinite, could be immutable, who knows!) And Set which uses weird semantics to determine equality IIRC There's more. But im not here to argue about things not everybody agrees with. 
&gt; In short, it completely and utterly fails to apply the principle that "there is only one best way to do something [syntactically], and the language should do it that way". That's a Python principle (assuming you're coming from Python background), not a general principle. Every language has a different approach. Have you seen how many ways Ruby has of doing the same thing? Have you seen how many different ways there are to represent a date in Java?
&gt; I have been confused multiple times by the underscore in Scala. I think you'll have to show a specific example of how underscores are confusing. Are you talking about a case where you misunderstand what the code is doing because of the underscores, or a case of the underscores being surprising and mysterious? The first is potentially a real problem, but the latter is just a matter of familiarity with the language. &gt; Reading Scala isn't like that - you kind of have to read multiple lines in one chunk instead of reading each line individually, if that makes sense Are you talking about a case where a single expression is broken up across multiple lines? I don't write Python, but I believe that's at least somewhat frowned upon over there. Apart from that, the only case I can think of when you need to look at several lines would be when you're doing complex `fold`s or `map`s and need to kind of grok the entire lambda to know what the `fold` is doing. But that' no different from understanding loops in other languages.
&gt; It really sounds like what you want is that language where it is impossible to write code that is hard to read for anyone. Unfortunately I don't think this language exists. I think this is how Go succeeds. &gt; I'd think I'd really miss the lack of expressiveness that must come with it. I think this is how Go fails. 
_Philosophically_, I don't think many experienced Scala developers disagree with you, including the language designers. Minimalism is actually an ongoing value sought by Martin Odersky and team. Back in the day, I had fun playing with a real minimalistic language of his, [Funnel](http://lampwww.epfl.ch/funnel/). It's kind of a dream concurrency language a decade and a half before Go, but Martin et al. concluded it was a bit _too_ minimalistic, _too_ focused on one aspect of programming, and moved on (to [Pizza](http://pizzacompiler.sourceforge.net/), clearly Scala's precursor, then Scala). This search for the right balance continues. Many of us are excited about the new [Dotty](http://www.scala-lang.org/blog/2015/10/23/dotty-compiler-bootstraps.html) compiler and the underlying evolution of the language, which indeed features a simpler core, with more features provided by extension than by being "baked in," although some valuable features, such as sum types, are now primitive. So please stick around—I think the next year or two will yield some of the fruit you seek. :-)
Yeah, most programmers coming from Java or Python start that way. I definitely have trouble swallowing that this is a _strength_ of Scala's, but since Scala can be said to have relative mainstream success—certainly relative to Standard ML, OCaml, Haskell...—I kind of have to grit my teeth and concede it. That leaves me with a pretty tragic view of the history of programming—ML has been around since 1975. But if "running on the JVM and letting you do all the batshit insane things most programmers have learned to do" is the stepping stone to "no longer doing the batshit insane things most programmers have learned to do," I'll live with it. :-)
They say memory is the second thing to go with age. I forget what the first is. :-) Thanks!
I have. Thanks for the post though, will def check this out too!
there's this symbol searching tool: http://symbolhound.com/?q=scala+%3D%3E
`_ * 2` I too wish that you could just say `someList.map(* 2)`, but that flows from the fact that `*` is actually defined with two paramaters, and scala functions/methods aren't curried by default, and indicates that * is partially applied, just as if you wrote `*(2) _` right? The difference is that since `*` is symbolic you can write it in the infix form, but it does nothing different from regular partial application AFAIK. The thing that is magical, to me, is `_.fieldName` for object methods and field accessors. &gt; F-bound polymorphism Noel, how might you express that you want your `A` to be upper-bounded by `SomeType` without it?
Scala certainly needs to simplify. But the things you listed I don't agree with. Maybe you should try Go. It is designed for people like you. 
&gt; Other issues are just design flaws. We could lose a few uses of _ and few people would really care. It's something that beginners often find overwhelming. Though the ambiguity has never been a problem &gt;for me the value provided by function literals like _ * 2 is not great. I wouldn't miss that if it was gone. I also wouldn't miss F-bound polymorphism and self types if they went away. Yeah. 100% agree. I try to not use the _ to help with readability or folks new to scala at work. 
&gt; How do we know which one to use? Oh, right. We have to read a 200 page book learn the Scala conventions... If you've problem with reading than you should leave the programming... oh, all of the scientific areas. Seriously, if you wanna master a language(neither a throw-away nor a to-be-popularized one) you'll need to read thousands of pages for it. Currently, the developers trying to simplify Scala as much as possible and I hope that Scala's current power will stay as it is. Once you dive into PLT and learn many different languages you'll realize its true power. Scala gives you freedom and people who 'grow up' in the slavery of poor language design often come here to panic.
 runT1ME, I always find familiarity in your comments...
 I've followed Kotlin's development from the beginning and I would say that it lacks almost everything I like in Scala. Typeclasses, traits, implicits and all of the type system enhancements. It's like Scala's 10% + some easily implemented goodies. But definitely a better Java. If the devs would've considered a more powerful type system then Kotlin would be more attractive.
I meant more that if I had to write `_ * 2` as `x =&gt; x * 2` I would not be upset. As for F-bound polymorphism, I have never had a situation where I needed to use it. I don't use the patterns that can cause it to arise---type classes offer equivalent power. That said, it probably needs to be in Scala for compatibility with Java.
Given the kinds of things that OP seems to dislike about Scala, Kotlin might actually be a good fit.
I love it how you throw controversial statements out there and then you say you don't want to argue. I'll respect your wish and refrain from telling you how wrong you are about Seq (hint: it's a trait).
Looking forward to more concrete examples then. I have no idea what your subtyping problem is, but I sense an impedance mismatch between what you want and how Scala does it. You mentioned type classes : Scala does it, but not like other functional languages.
(disclaimer: I'm not an sbt contributor, this is an educated guess) There is nothing difficult about compiling sbt for Scala 2.11. It is *by design* that sbt sticks to 2.10 in the 0.13.x cycle. If sbt migrated to 2.11, it would be a backward binary incompatible change, which means that *all sbt plugins* would break and would need to be recompiled. Therefore, sbt can only migrate to 2.11 when they are ready to break binary incompatibility, which is going to be the case for 1.0.0, IIUC.
That is correct. An alternative would be to publish cross-compiled sbt plugins, but we believe this to be too much of a cognitive overload ("we can't use sbt-foo because the author hasn't decided to publish a 2.11 version of their plugin, and we use sbt-bar which is 2.11 only"). It would also require changes to the sbt launcher to support deciding which Scala variant of sbt your build wants to use. Right now we're thinking of migrating to Scala 2.11 for sbt 1.0 (but it might be Scala 2.12 see https://github.com/sbt/sbt/issues/2389 and https://github.com/sbt/sbt/issues/2388).
It seems like half the angst here is around Unit return type from procedures. Note that this feature is on the roadmap to be removed. See "Scala: Don Giovanni" point #4. http://www.scala-lang.org/news/roadmap-next/
&gt; print "Hello %s" % "world" Don't forget these two as well: `print "Hello %s" % ("world")` `print "Hello %s" % ("world" , )` Disclaimer: Whenever I hear references to "the Pythonic way", I'm unsure whether the person is being sarcastic.
Is it such an old world view to, you know, learn by reading an authoritative book? I spent two weeks reading Odersky's book cover to cover and then had a pretty good understanding of Scala, and FP. Should we demand that all languages, and the art of mastering programming, be readily accomplished by novices in 24 hours of reading a tutorial? Frankly, *learning to program* is the hard part. After you know all the common abstractions and various programming patterns, learning a new language is just mapping those ideas to different way of making a compiler produce the same AST. This reminds me a lot of DP's http://blog.goodstuff.im/yes-virginia-scala-is-hard
I don't mind the single widecard use `someList.map( _ * 2 ) // easy to read` But mapping across tuples with underscore indexing is quickly a head scratcher, so we don't use it: `tupleList.map { a ⇒ (a._1 + 4, a._2+a._1) } // mental WTF` But tupled really helps in such cases: `import scala.Function.tupled` `tupleList map tupled { (a,b) ⇒ (a+4, a+b) } // ok, got it` Regardless, you need to have standards with your group of devs, and strive for clean, readable code. This will be a requirement, whatever language you're using.
Sorry, what was the insult? 
In all fairness, case classes have structural equality and in the presence of vars, structural equality is broken. This is a well known gotcha from Java as well, as when overriding equals experienced developers know to not do that if the object has identity (eg state). In other words, only immutable values can have a meaningful equality operation, while mutable objects have identity and can only do reference equality. Btw, when I review code, I reject all PRs that are doing vars in case classes. I couldn't care less about people's need for syntactic sugar, as a broken equality is far worse. I've written some more here: https://github.com/alexandru/scala-best-practices/blob/master/sections/2-language-rules.md#25-must-not-use-var-inside-a-case-class
I would recommend you reconsider the idea of writing a style guide when dozens of people in this thread are pointing out you are still not actually as familiar with the language as you believe you are.
Stop bitching about how the language doesn't fit your preconceived notions of how a language should be designed. If you do not think the language is well designed, stop using it. No one cares how you think it ought to be designed. I'd suggest you read a few books about functional design and practice. 
I want you to go up to anyone who doesn't know Scala or Haskell and ask them to explain to you what they think Map(1 -&gt; 2) does. If think that it takes in the number "2" as a parameter into a function belonging to number "1", I will eat my hat.
That's the point. This style guide is meant for people who are not familiar with the language. And yes, I know you get a compiler warning for writing methods like "object method()". And I know this goes against each and every style recommendation in the book. But the book of style recommendations is huge. It's like C++. No one has time to learn and use all of C++.
If they don't know any functional languages then they wouldn't. If you're not willing to read a few books on Scala then maybe you shouldn't be reading it.
I appreciate that you put this list together, and don’t see anything obviously wrong with it. Have you found that each of these is actually helpful though, or is any of it still hypothesis? Thanks!
Shapeless is what you want. It has a Sized type that will be a vector of a static size. Sized[Vector[Int], _9] or Sized[Vector[Sized[Vector[Int], _3]], _3] sound like they'd work for your use-case. https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#collections-with-statically-known-sizes
Maybe they won't think that. But my point is that if it is absolutely necessary for a person who doesn't know Scala (or Haskell) to know that -&gt; is a function. Map(1 -&gt; 2) always looked to me (since the time I started learning Scala) as a very simple and intuitive way to express that they key is 1 and the value is 2, without caring about -&gt; being a function. 
"If you're not willing to read a few books on Scala then maybe you shouldn't be reading it." It's "If you're not willing to read a few books on Scala then maybe you shouldn't be WRITING it." That being said, I'll be writing it regardless, so if you have a problem with my coding style, you can go fuck yourself. Or maybe ask Martin to change the language so that you can't write lines "object verb(directObject);"
I'm doing that. And this is the way that makes sense for me and my team. And if it is IN COMPLETE CONTRADICTION with the style guides espoused in every Scala book.
I'm going to write however I want to write. Why? Because the language allows me to. As for no one caring what I think, do you think that 70 people would have responded if zero people gave a shit? As for functional design you can write a language with very easy, intuitive symbols and conventions and it has nothing to do with whether the language is OOP or Functional or Imperative.
They'll pick that up later. It isn't a big step to go from "-&gt;(2)" to just "-&gt;2". The point is to diminish the learning curve so that people can get started faster. Startups are using other languages? Why? Diminished learning curve. Training people costs money. Startups don't have a lot of money. Startups need people to get things done now. And if they can get things done now using Java, they will use Java. The fact that Scala is a more scalable language is something that doesn't even cross a manager's mind.
Short answer: Yes. Long answer: I am unique in that I forget things at an incredible rate. I once learned all of C++. Read Bjarne Stroustrup's books cover to cover (except the appendix). Now, I don't even remember how to make one class inherit from another class. Not only that, but I look up everything because I keep forgetting. My memory is so bad that I use Mapquest to go every single place - even to visit family who live a couple miles away - because I cannot remember anything and I would get completely lost and stranded otherwise. But that incredible forgetfulness is useful because I am constantly a beginner. Maybe all that I will remember is "noun object verb function" and I will derive all the design patters and UML's just from that. Anyway, by following strict, consistent rules and writing those rules down, even if you forget what a symbol means, you can re-learn it very quickly. But when the rules are non-trivial, the relearning process becomes painful. And since re-learning is something that happens again and again for me, I am pretty much the perfect person to test anything designed to minimize relearning. 
If what you say is true, Scala (and C++ for that matter) is quite possibly the worst language for you to *use*, let alone try to teach to others. They are big languages with large learning curves that allow you to do extremely powerful things but they demand that you *know the language*. If you want something extremely simple with extremely consistent syntax rules that are extremely easy to learn and re-learn, you should be using a LISP. Oh, and you should probably start using Google Maps. 
It isn't dishonest. Maybe your blind-faith is dishonest? This is my honest opinion from years of use and years of experience with many other toolchains. &gt; presumably refers to the empty-line syntax, which went away in version 0.13.7, released on Nov. 18th, 2014, and wasn't onerous in the first place. No, i mean the `:=` vs standard `=`, `++`, `++=`, .... I mean subproject having to `_*` settings. I mean the overloaded `%`. Symbols are great for math but they suck when they're randomly and meaninglessly defined by framework. &gt; is one (build.sbt). Sure, for the most simplistic of builds. My builds are far from simplistic because I write and work on real, production code, not toy projects, so I also require `assembly.sbt`, `plugins.sbt`, and `build.properties`. And regardless of alternative solutions, getting to those answers is a huge learning curve. How much someone has to learn about SBT just to be able to get to a real usable project structure is ridiculously steep. &gt; is nonsense (it's perfectly possible to have a parent project that does nothing but aggregate its subprojects; just don't say dependsOn). You're talking about multi-project builds? I'm not. I'm talking about wanting independent subprojects somewhat like Maven's parent-child POMs. Again, for simple things, SBT is fine. When you get to real projects, it is a mess. I have one relatively small set of projects that's a 200 line `build.sbt` (and the three other `project/` files) with nine subprojects and 30+ library dependencies. Any time I have to make a change to a subproject, that entire `build.sbt` is reprocessed. It's nonsensical that a subproject requires a full rescan. Why can't i just add the dependency to a much smaller project's build and the tool can determine which project changes and rescan and rebuild only that project?! &gt; but leaves you without a CI solution. Why would I use IntelliJ for CI? IDEs and CI are only loosely related and team-based CI should primarily be happening after pushes. Here's a fun one: How do you get the version of sbt? `sbt -v`? Nope! `sbt -V`? Nope! `sbt --version`? Nope! Google it. Ah, `sbt sbtVersion`! Oh, yea, that's obvious. And that simple example is the general usage of SBT as a whole. It's an unintuitive, overly complicated mess.
I'm only half kidding. In reality I love the language. I also hate the learning curve. And the irregularities. Like "&lt;=" and "=&gt;". Did you know that one means "class A extends class B" and the other means "function". The creator of the language could have picked up a little bit of advice from the creator of Ruby.
&gt; without caring about -&gt; being a function ^ Wait a second. How could you not care that "-&gt;" is a function? I would never trust a language where I did not know EXACTLY what it was doing. In addition, I rely on my IDE for everything. I haven't typed out an entire function or object name (other than the declaration) in a statically typed language in years. And when they are typed out, I expect a helpful pop-up telling me the type of the function and what to put in the parameter. I expect that from all functions. Even symbols like "-&gt;". If I didn't know that was a function I would be shocked.
Even the creator of C++ said that there are things in C++ that you should not be using. Like unions. Or void *. Amazingly enough, I remember how to use void *, but I don't remember how to type the C++ symbol for inheritance. Anyway, I'm going to use Scala, but only a part of it. And I'm going to write Scala. But only in a style that it the most intuitive to me. And if that style is completely different from the "official" style and if I don't use a lot of the features that other people use (or if I use some esoteric ones that they don't use), that's partially the language's fault for giving me that flexibility.
&gt;"&lt;=" and "=&gt;". Did you know that one means "class A extends class B" and the other means "function" "&lt;=" does not mean "extends", I think you might be thinking of the type constraint "&lt;:", as in: def foo[A &lt;: B](list: List[A]) = ... As for the the "=&gt;" symbol, that's actually a pretty common use of the symbol. Just have a look at [JavaScript](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions) or Java 8 lambdas (not exactly the same but close enough): (Person p) -&gt; p.getGender() == Person.Sex.MALE &amp;&amp; p.getAge() &gt;= 18 &amp;&amp; p.getAge() &lt;= 25
Sorry, I was thinking of this... type T &lt;: CharSequence type T :&gt; CharSequence type T &gt;: CharSequence ^ I always get confused with that. I mean I know that "A &lt;: B" means A is a subclass of (or same class as B), but I always expect that to mean "A :&gt; B" is the reverse. 
:grabs popcorn:
So? Man you're at about an 8.5 on the emotional scale, if you want people to take you seriously you need to dial it the fuck down. Saying things ALL IN CAPS doesn't lend your points any extra credibility. If you're going to tell me Scala's got it all wrong, just make your argument and let me evaluate it for myself. Either you've got good points or you don't, your personal excitement over the matter shouldn't even enter the equation. Calm down. I think you'll find that a lot of people in the Scala community are quite reasonable if you want to have a dialog.
Why don't you pass your vector elements as variadic parameters and use 'require' to enforce the lenght? Ok, it will be enforced at runtime but its an enforcement any way: case class Grid(grid: Int*) { require(grid.length == 9, "grid should have 9 elements") } Or: case class Grid(grid: Vector[Int]) { require(grid.length == 9, "grid should have 9 elements") } 
Ensuring on the type level that your grid has size 9 is not worth it in my opinion. I would recommend you not to use Shapeless _for this case_. It's total overkill and will make your life unnecessarily difficult. Just breathe in and out and do what /u/pfcoperez suggested, add a `require` in your constructor.
&gt; EDIT &gt; Abandon thread. Repeat, abandon thread. This is a very poor attempt at humor. OP thinks this is how you write satire. Abandon thread. Repeat, abandon thread. Commeter is too stupid to understand satire. Also, half the **** I write is legit you dumb broad.
 NOTE: THIS IS NOT A REAL STYLE GUIDE MEANT FOR USE IN A WORK ENVIRONMENT ^ I put this up especially for you. Now go delete all your comments. Please.
How about no. I'm not going to censor myself for your gain.
search for typelevel programming, best way to get a hint on it would be searching for typelevel programming natural numbers, this may help you setup a limit on the value of ints passed. Go from there. 
Oof. What am I supposed to say to this? It's _extremely_ well documented how `Setting` and `Task` keys are set by `:=` and the like, that those are expressions that accumulate into an immutable map of settings that control the build, how they behave with respect to `Scope`s, all of it. &gt; Symbols are great for math but they suck when they're randomly and meaninglessly defined by framework. There's nothing even _remotely_ random or meaningless about it. [Here](http://www.scala-sbt.org/0.12.1/docs/Getting-Started/Basic-Def.html) is the "Basic Definition" page for sbt 0.12.1, released in October, 2012. I'm tempted to copy and paste the whole page, because it explains `Setting`s, `Key`s, `:=`, `+=`, etc. at length and in detail. It even comes with an explicit ** PLEASE READ THIS SECTION ** because understanding that material is crucial. &gt; My builds are far from simplistic because I write and work on real, production code, not toy projects, so I also require assembly.sbt, plugins.sbt, and build.properties. The horror! An `addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.1")` that could just as easily go in `plugins.sbt` and a `sbt.version=0.13.x` that's a courtesy to people who might not use the same version of sbt as you! &gt; And regardless of alternative solutions, getting to those answers is a huge learning curve. How much someone has to learn about SBT just to be able to get to a real usable project structure is ridiculously steep. I'll be polite: horse-pucky. [Here](http://www.scala-sbt.org/0.12.1/docs/Getting-Started/index.html) is the Getting Started TOC, again for 0.12.1 back in late 2012. It is _exceedingly_ thorough and even includes a page with links to available plugins. _Especially_ an experienced developer is going to read the first five sections, might skip 6-7 and move straight to "Library Dependencies," and then "Using Plugins." And there's the "Available Plugins" link. &gt; Any time I have to make a change to a subproject, that entire build.sbt is reprocessed. It's nonsensical that a subproject requires a full rescan. Why can't i just add the dependency to a much smaller project's build and the tool can determine which project changes and rescan and rebuild only that project?! Go back to the beginning and re-read about "big list of `Setting`s that modify an immutable map." Now consider how many immutable maps are involved. &gt; Why would I use IntelliJ for CI? IDEs and CI are only loosely related and team-based CI should primarily be happening after pushes. My point was that you said "IntelliJ works great for Scala," which is true, but leaves out any sort of CI solution, which is indeed where sbt comes in, which brings us to... &gt; Here's a fun one: How do you get the version of sbt? sbt -v? Nope! sbt -V? Nope! sbt --version? Nope! Google it. Ah, sbt sbtVersion! Oh, yea, that's obvious. And that simple example is the general usage of SBT as a whole. sbtVersion is [just another `Setting`](http://www.scala-sbt.org/0.13/sxr/sbt/Keys.scala.html#sbt.Keys.sbtVersion), which, BTW, means you can say `sbt sbt-version` instead, which is a bit more human-readable. The [Batch mode](http://www.scala-sbt.org/0.12.1/docs/Getting-Started/Running.html#batch-mode) section of the "Running" docs under "Getting Started" explains this. The upshot is that all sbt-launch.jar does is launch _some version_ of sbt, and which version that is becomes a `Setting` so it's available to `Task`s, plugins, etc. on the same footing as any other `Setting`, and batch use of sbt just interprets commands like interactive use does, which facilitates automating sbt. &gt; It's an unintuitive, overly complicated mess. Don't confuse "unfamiliar" with "unintuitive." It leads you to erroneous conclusions like your last one. sbt is actually very simple, but for some reason people seem not to actually read even the "Getting Started" documentation (at least the starred "Please read this section" parts) to learn how to take advantage of its being built on a simple, consistent core, as your final complaint ironically demonstrates. **Update:** I don't enjoy yelling at you, even virtually. I get that you're smart and experienced and frustrated. It's not that I think sbt doesn't have warts or room for improvement, either. It's that, overwhelmingly—and your post was unfortunately an example—the complaints about sbt I hear with so much bitterness, anger, and dismissal are so difficult to align with belief that the complainant read the Getting Started guide. Now, this is admittedly a strong reaction, so strong that I should spend more time reflecting on how unlikely _that_ is. But yes, it does motivate me to defend sbt, both because these kinds of complaints are unfair, and because newcomers to the ecosystem may be discouraged by them despite their being unfair. I'm sorry if this comes across like _personal_ animus. It absolutely is not. I simply do not know what to make of the disconnect between what is explained thoroughly in years-old documentation and these complaints, _especially_ when I absolutely believe you are expert in the _real_ horror stories of Maven, Ant, Make, and probably more.
Where do you work?
I think I understand that yield is syntactic sugar that I can live without... for (p &lt;- persons; if !p.isMale; c &lt;- p.children) yield (p.name, c.name) . How about just: val women = people filter( person &lt;= !person.isMale ); val children = women flatMap( persion &lt;= person.children ); val childIDs = children map( child &lt;= (child.name, child.mother.name) ); ^ I can read that. Maybe I would have a better understanding of yield if the examples didn't look like this... http://docs.scala-lang.org/tutorials/FAQ/yield.html for(x &lt;- c1; y &lt;- c2; z &lt;-c3) {...} ^ c1, c2, c3? What the fuck is c1, c2, c3 supposed to mean? Child1, Child2, Child3? Container1, Container2, Container3? Programming isn't about writing the most compressed math possible. Programming is about expressing abstractions with elegance.
That being said, Scala is one of the best languages for writing descriptive, elegant, artistic code. Code that looks like: val ring = Promised(gift); match ring { case Kept(promise) =&gt; girl getMarried(); case Cancelled(promise) =&gt; girl goDancing(); } Rather than like this: for(x &lt;- c1; y &lt;- c2; z &lt;-c3) {x*y*z} I mean occasionally if you're doing some heavy math and it's on a coordinate plane then it makes sense to write like that but for virtually everything else you are generating worthless unreadable crap.
I mean, if I am a company or a individual and I hire a web developer or a team of web developers to do a web app for me, the moment I pay them and they hand over the app to me, every single line of code that they wrote becomes my intellectual property. I'm not just paying for the website. I'm also paying for ownership and rights for all the content and all the source code. And if I'm going to pay someone to write code for me, they better write that code so that I can read and understand every single line and every single variable. I would not take a dollar out of my wallet for code that looks like arcane mathematical formulas (unless they are trying to abstract over an arcane mathematical formula and document it). I would rent that code under the condition that they are under legal contract to do maintenance for a certain period or else face a heavy fine, but not buy it.
&gt;A completely redesigned internal API for the linker, whose main visible impact should be reduced memory usage and improved speed Are there any metrics on these changes?
Yeah. When I refer to "idiots and amateurs", I am referring to myself. I'm not a full time Scala developer. I am a web developer who uses Scala as a substitute for Java. If you're working in a big company with hundreds of employees working on some mathematical system, then please, follow all the conventions of your company and use all the esoteric features to your heart's content. But if you need your code to at some point be passed on to a Java developer or a Ruby developer, I personally would go light on the syntax.
No, there aren't, not in general, anyway. There are some profiling information about the performance of the Emitter in https://github.com/scala-js/scala-js/pull/1920#issuecomment-143780557 Other than that, it's mostly the new IR caching mechanism, which will avoid to cache in memory the same .sjsir files for different configurations (Compile/Test) and subprojects. The cache became global: one per sbt session, so they are shared by configs and subprojects.
Thanks. I'm not freelance, I just wondered where you work, since based on your proposed coding style, your team is not one I'd want to work on.
I agree with the author that it's a little bit unfortunate that today, you have to choose Typed actors or Persistent actors, but you can't really do both. It seems like it may be another year before we have a fully revamped API.
&gt; AFAIK, there's no real reason TCO can't be done on the JVM, it's just not a priority I think I read somewhere that it's for security reasons. So, self-TCO could probably be optimized, but mutual TCO might have additional complexities.
Yes, we're using it in [Sift](http://www.siftnlp.com/). We're using the [rediscala](https://github.com/etaty/rediscala) library. It's async, but I haven't used any other Redis library, so I don't know if it's better or worse than the others :)
have you tried appneta's TraceView? 
I guess they could also add a flag to disable tail-call elimination. That way, you could add that flag for debugging.
I've used the Typesafe Redis Play plugin without any issues.
I mention this is my post, but I want to reiterate: this is not meant to refute anything said in [the original article](https://www.reddit.com/r/scala/comments/42koq6/why_you_should_use_tail_recursion_in_scala/). I just wanted to provide some thoughts based on my own experience.
Had that exact same error message...
&gt; No one has time to learn and use all of C++. Yeah, no. I actually happen to know a few.
If you have an invariant, stick it in the type system, that's what it's for! Something like ScalaZ NonEmptyList is much more maintainable than a comment that a given list can't be empty. I'm not sure about the use case for separating by type - rather than sticking them in lists, maybe do what you need to do with them. I'd need to see it in context really. Certainly be aware of the possibility that local, well-encapsulated mutable state might be a more understandable way to describe a given operation. But a surprising number of things are easy to do with safer techniques once you get used to them.
&gt; Something like ScalaZ NonEmptyList is much more maintainable than a comment that a given list can't be empty. That's an interesting idea. How would I encode the idea of "a stack of queues, the first of which can be empty but the rest of which can't be empty"? I guess I could have one `Seq[V]` (to represent the head queue) and a separate `Seq[NonEmptyList[V]]` to represent the rest of the stack. I think that would work, but I fear it would make the code even less readable than it currently is. The invariant that I describe is very narrowly scoped - only within the context of that one function. &gt; I'm not sure about the use case for separating by type - rather than sticking them in lists, maybe do what you need to do with them. In my case, I was separating AST nodes by type. Within a given scope, I collect the set of function definitions in order to do dependency analysis and find the sets of mutually dependent functions. I can't do that as I encounter each function; rather, I need to see all the functions at once. I also hoist all data structure definitions, since they by definition can't depend on any functions or other data. I feel like I've needed to use this pattern once before as well, but I can't recall that particular use case. The `Shape` example was made-up but easily relatable. &gt; Certainly be aware of the possibility that local, well-encapsulated mutable state might be a more understandable way to describe a given operation. But a surprising number of things are easy to do with safer techniques once you get used to them. Yep. I've tried going full-functional with Scala. Some things work beautifully, while other things feel more awkward. I've been slowly relaxing my self-imposed "all functional, all the time" mindset, and I think the results have been better. I think "functional first" is probably a good attitude, but doggedly refusing to use any mutability is I think missing the point. If I want Haskell, I can always use Haskell. But I'd rather use Scala.
^ You mean that style?
Even if it did not, it would probably simplify the compiler.
&gt; http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala ^ These assholes censored me for posting that there are actually more than 10 uses. 
I *so* want to be able to multiple-upvote this -- I've wanted this functionality for logging a hundred times. (And the Enum trick is lovely...)
I changed it to "In short, it completely and utterly fails to even begin to apply either the principle that "there is only one best way to do something [syntactically]" or the principle of least surprise or the principle that explicit is better than implicit." Now it's both a Python principle and a Ruby principle. Also, Java Date is horrible.
"If you've problem with reading than you should leave the programming" - Go fuck yourself. If you don't have a problem reading, why don't you read the instruction manual for every single thing you ever bought. There is a 6 page instruction manual for my electric toothbrush. There is a 2 page instruction manual for my lamp. There is a ~120 page instruction manual for my videogame system. Most people don't want to read instruction manuals.
My problem with the _ isn't just that it is one of 15 or so different meaning for the _. The other problem with _ is that it prevents people from actually giving names to things. Example: for{ x &lt;- people if (!x.isMale) y &lt;- x.children } yield (z.name, y.name) ^ What the fuck is this? ^ ____________________________________________________________ val women = people filter( !_.isMale ); val children = women flatMap( _.children ); val childIDs = children map( (_.name, _.mother.name) ); ^ Hmm... It is starting to make sense. ^ ____________________________________________________________ val women = people filter( person &lt;= !person.isMale ); val children = women flatMap( persion &lt;= person.children ); val childIDs = children map( child &lt;= (child.name, child.mother.name) ); ^ I understand now! Look! The intent is obvious! ^
I made like 20 different corrections to the thing.
&gt; "half the angst here is around Unit" No, there is a lot of other stuff besides Unit.
I personally find that if code reads like English I am happy.
&gt; Syntactic sugar isn't what makes a language expressive. Nouns - classes. Verbs - functions. Gerrunds - mix-ins (flight, strength). Pluggable Interfaces -able adjectives. Generic participles - ListOf[Int]. Nouns and verbs that modify and describe other nouns and verbs - Maybe[Attending], Future[Expectation], Promised[Gift]. &gt; Expressiveness isn't about how many different functions there are or how many different ways there are to define a function. Expressiveness boils down to how good a language is at transforming ideas and abstractions from the human realm of ideas and thoughts into code.
Yes. Yes they do rub me the wrong way. 
Yes.
In a lot of instances it's actually a good thing to use non-descriptive variable names such as x,xs,y,ys. In many cases the function you are writing does not operate on any object but simply abstracts a programming pattern. In this case you should not care about what the variables are, just the pattern that they display. Descriptive variable names in this case would be confusing since they would distract from the pattern. Case in point: The compose, curry, uncurrry, and partial application functions are all such "Pattern abstraction functions". def compose[A,B,C](f:A=&gt;B,g:B=&gt;C):A=&gt;C = { (x) =&gt; g(f(x)) } Imagine in the function was canonically written as: def compose[A,B,C](innerFunction:A=&gt;B,outerFunction:B=&gt;C):A=&gt;C = { (value:A) =&gt; outerFunction(innerFunction(value)) } This is less readable and distracts from the pattern. 
I'd point out that those aren't necessarily the same. The first version would work even if a child doesn't have a reference to its mother, while the second version requires that. You could always split the difference: val women = people.filter(person =&gt; !person.isMale) for { mother &lt;- women child &lt;- mother.children } yield (mother.name, child.name) For comprehensions might not be familiar, but they're common enough in functional languages. 
There are syntax errors, but it was certainly close enough that you could see what he meant. 
Sorry if there is a compile error, but the second one should make it obvious what the writer's intent is, in this case to yield tuples consisting of children and their mothers from a list of people. Even if you are going to use for-yield, at the very least refrain from the one letter abbreviations that you see in the documentation such as: http://docs.scala-lang.org/tutorials/FAQ/yield.html . And yes I used the word "yield" to mean "collect", but when you put yield at the end of a block, the first meaning that a user would expect when they look at it is not "to collect" but "to return". That being said, I'm sick of this shit. If you cannot see how much more obvious the second one is to the first, especially to developers coming from Rails, Python, and Django, then I pray you luck if you had to work with those developers on a Web App.
Since we're on the topic of recursion, some may find the following paper interesting, where explicit recursion is the "goto" of functional languages: [Unifying Structured Recursion Scheme](http://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/urs.pdf)
Programming is not a science you dumb shit. Neither technically nor legally. Design patterns is not a science. Programming language is not a science. YouTube.com is not a science. The act of creation is an art. It is both an art in the technical sense and also in the legal sense (rights to software is designed the same was as rights to a painting)
My replies (bullet-for-bullet): * Don't get me wrong, I'm not arguing that tail recursion should be avoided or that non-tail recursion is an appropriate tool to use in a constrained environment or where safety is on the line. I don't know why the algorithm in the throttle control software was recursive, but suppose it was converted to an iterative algorithm and it had to maintain its own stack. That stack would either need to have a fixed size (meaning that it would be possible to eventually overflow) or it would need to be dynamically allocated (which seems like a bad idea on a memory-constrained device; what happens when an allocation fails?). Memory corruption is really bad, but it sounds like Toyota's software development practices were universally bad. There's no guarantee that they would have gotten it right even if they had managed their stack explicitly. (Of course, perhaps the algorithm could be converted to an iterative algorithm without the need to manage a stack, in which case... ding ding shame). * I'd argue that it depends on the criticality of your code. If you're working on an internal tool that can break temporarily without causing issues, and if non-tail recursion makes the tool easier to maintain, then maybe non-tail recursion is OK. This is especially if you have a good understanding of the data you will process, and are confident that you won't blow the stack. I guess I'm saying that "tail recursion is preferred to non-tail recursion in general, but non-tail recursion is OK in certain circumstances". * I'll admit that this is somewhat subjective. Code that I find unreadable is not necessarily hard to read; I might just not be used to a certain style. * I sheepishly admit to not running any benchmarks. I'm fairly certain that the fold version needs to do work that the loop version can avoid (namely, building and splitting a tuple), but that's a hollow claim without benchmarks. The JVM can be magical at times. * Again, without benchmarks it's hard to say. But I'm not trying to point to inlining or not inlining as a source of performance difference; rather, to the building and splitting of tuples. Maybe the compiler is clever; I didn't get a chance to look at the bytecode. * The examples that I provided were based on actual work that I had done; these were examples derived from "the wild" (of my personal projects). I chose to use `Shape` instead of my actual type, because everybody can easily relate to `Shape`s. But the idea is the same. I can see an argument that "folds and tail recursion encourage people to better organize their code, because managing huge loops in this style is difficult". Perhaps I'm comfortable with allowing loops because I've developed enough discipline to mostly avoid the traps you describe. Anyway, thanks for reading and taking the time to respond. 
Again, everything is relative. To paraphrase Rich Hickey, "German is hard for *me* to read. That's not surprising; I don't know German. That it's hard for *me* to read doesn't mean that it's hard to read."
 You're so dumb that you'd hit a whole new level. You take your own definitions and burn yourself in the front of other people...
Yeah... except that I know Scala a lot better than I know Python, lol.
I'll grant you that. But for example, don't look at Scalaz and say "this must be how all Scala developers write their code." That's a library specifically designed for people who want to do hardcore FP in Scala. 
&gt; How would I encode the idea of "a stack of queues, the first of which can be empty but the rest of which can't be empty"? I guess I could have one Seq[V] (to represent the head queue) and a separate Seq[NonEmptyList[V]] to represent the rest of the stack. I think that would work, but I fear it would make the code even less readable than it currently is. Yeah that's how I'd do it. I think it comes out ok: @tailrec def helper(work: Seq[A], rest: Seq[NonEmptyList[A]] = Seq.empty, inProgress: Set[A] = Set.empty, finished: Set[A] = Set.empty, result: Seq[A] = Seq.empty): Seq[A] = work match { case hd +: tl ⇒ if (finished contains hd) helper(tl, rest, inProgress, finished, result) else if (inProgress contains hd) throw new RuntimeException("Graph contains a cycle") else helper(edgeMap(hd).toSeq, NonEmptyList(hd, tl: _*) +: rest, inProgress + hd, finished, result) case Nil ⇒ rest match { case nel +: rest ⇒ helper(nel.tail, rest, inProgress - nel.head, finished + nel.head, nel.head +: result) case Nil ⇒ result } } I might incline to use `List` rather than `Seq` so you get the exhaustiveness checking too. &gt; In my case, I was separating AST nodes by type. Within a given scope, I collect the set of function definitions in order to do dependency analysis and find the sets of mutually dependent functions. I can't do that as I encounter each function; rather, I need to see all the functions at once. I also hoist all data structure definitions, since they by definition can't depend on any functions or other data. I feel like I've needed to use this pattern once before as well, but I can't recall that particular use case. The Shape example was made-up but easily relatable. Hmm. Thinking about it there could probably be a general method for this - like a generalization of scalaz's `.separate`. Shapeless generics understand sealed trait / case class families and can represent them as coproducts, and then that could transform into a `HList` of `Seq`s, so you'd call something like `shapes.separateGeneric` and that would return a `Seq[Circle] :: Seq[Square] :: ... :: HNil`. Would that be a valuable thing to have? I might try to implement it.
Heh, don't get me wrong, doing the shapeless thing is going to be difficult to say the least. It'll have to be very strictly recursive because that's the only way the types will line up. Type-level programming in Scala is far more complex than it needs to be. My point was more that there should be a library function for doing this (scalaz has .separate but that's only for Either-like things where there are two possibilities, not general coproducts). If you want to write it then I wish you the very best of luck, but brace yourself.
Programming as defined by the US legal system is the process of creating something, code, which has the same artistic protections as paintings and digitally recorded songs. Programming (or at the very least design) is an art and software is considered a creative work. Science involves making predictions and then verifying that those predictions were correct. Testing is a science. Engineering and art are almost the same thing depending on the medium and intent. And yes it does matter what it is because you told me to get out of science you annoying little son of a bitch who is too stupid to grasp the main point. It's not about whether or not someone is too lazy to read. It's the fact that it is impossible to figure out without reading. I don't know Python. I don't write Python. But I can read it.
&gt; I'm comfortable with allowing loops because I've developed enough discipline to mostly avoid the traps you describe. I'm fine with that. Myself I've adopted a more defensive attitude by working with people that are still very much beginners (well, we are all beginners, but some more than others). The big problem with software development is that at some level it's an art form in the sense that most techniques, algorithms, tools, preferences have a context in which they shine, but being able to recognize the context and apply the mythical "best tool for the job" is really freaking hard, because it takes knowledge, experience and a good nose. Therefore I find it easier to just adopt simple rules that can be followed. I agree for example that for-loops are actually cool when small and isolated, but there aren't that many people with the self restraint. And it's in human nature to take shortcuts. So for example instead of understanding and refactoring a loop, people have a tendency to introduce "if (p) return r", because what could possibly go wrong and there, job done. The alternative on the other hand is to enforce some common sense rules. Like no short-circuiting (e.g. return, break, continue) is ever allowed. And if you have a loop, prefer a tail recursion that works with `vals`, such I don't have to track where your "var" changes in order to infer the invariants. Or in other words, always prefer immutability whenever possible. Worked well thus far.
 1. Nobody cares about the US legal system 2. "Testing is a science" - false, testing is way too general to be defined like that 3. "Engineering and art are almost the same thing depending on the medium and intent." - false, art SHOULD involve creativity and originality. Engineering is about profit and efficiency. 4. " ...you told me to get out of science you annoying little son of a bitch who is too stupid to grasp..." - noisy spoiled kid. Grow up. You're too small and unqualified to be worth anything. You won't get a job with such a pathetic communication skill. 5. "It's not about whether or not someone is too lazy to read" - yes it is. I didn't know that there are such lazy persons exist like you. "I don't know Python. I don't write Python. But I can read it." is like "I don't know german. I don't write in german. But I can read it." - totally offtopic. If you don't know python(as nothing else) and programming then you shouldn't share your opinions about them in public. You're arguing about pointless definitions and formality in operators in an unpleasant manner... If you won't change you'll end up under the bridge.
Oh. I thought replying to yourself was how you append another message. I'm kind of fresh out of college and trying to create a start up. I don't see how a style of coding that looks so much like plain everyday English could possibly be repulsive. In Scala I often see type signatures that look like this... "def which[R : AsResult](f: (U) =&gt; R): OptionLikeCheckedMatcher[F, Nothing, U]" What is U? What is R? What is F? That is repulsive to me. I kind of have no budget, but I appreciate the advice. Thank you.
If I cannot read the code, I cannot correct it. Computers can count to much higher numbers than people can. If the computer is counting to 80 trillion in multiples of three, the algorithm has to be written in a simple enough way that I don't have to get out my dubugger and go 3, 6, 9, 12, 15, ... As for why you should program like an idiot - because idiot code is the simplest code possible that gets the job done. If you have a project to do, and it's your first time, you can make it as fancy as you like, but it probably won't work. The less fancy it is, the less trouble you will have. That's why a programming language should be as context independent as possible without sacrificing the ability to represent abstractions. To have the exact same symbol represent 15 different things in 15 different contexts is complicated. To have 15 different things represent 15 different things across all contexts is simple.
The thing is, for the business I want to go into, my competitors are all using PHP and cheap overseas labor. I really like Play for Scala, but if things start moving, Scala developers aren't cheap. And the reason they aren't cheap is because Scala developers (for the most part) aren't newbies or amateurs. They already know Java and like three other languages and many of them are beyond their BA. But if Scala was simpler - no confusing val or var in constructors, no ability to inadvertently call a method when passing it into another method, context independent (rather than context dependent) symbols, no confusing generics with names like R T U P and funky existential types that don't bound the way you would expect, no confusing language constructs that look similar to one another but do totally different things. Just the core classes, functions, mix-ins, interfaces, generic containers, generic classes, and monads and maybe the for 1 to n loop and basic arithmetics, core named functions, and the Java standard library. As horrible as this sounds, if Scala was an easier and more intuitive and accessible language, it would probably attract less elite programmers who charge lower wages. 
&gt; I thought replying to yourself was how you append another message It is. It's just that Reddit's collective cultural preference is to edit posts rather than self-reply. Who knows why, it just is. &gt; I don't see how a style of coding that looks so much like plain everyday English could possibly be repulsive. You don't, but I and others do. That's fine: code styles are subjective. Also, that your code style matches English, and that it's even a good idea for code to look like English, are also subjective matters and not objective truths. &gt; def which[R : AsResult](f: (U) =&gt; R): OptionLikeCheckedMatcher[F, Nothing, U] R is a type param with a context bound. U and F must be defined in some enclosing scope, since they're not set out in the signature of the method. Nothing's magic here. Reading and writing Scala will help you get more comfortable reading Scala - there's no shortcut, just like any other language. PS: &gt; I'm kind of fresh out of college I don't mean any offense at all, but this explains a lot. After you've been around the block a few times, you'll still have strong preferences (I have been, and sure do!) but with luck will not see the world in black-and-white. At least in terms of software (I'm not talking math, morality, or anything else), ideas are more or less suitable given the context and constraints surrounding them. Those could mean the team (if no one is comfortable with method signatures with context bounds, defining your own typeclass is not a good idea), the domain (even the dreaded goto is entirely appropriate when writing something like the JVM), or something else entirely. It's very, *very* rare that you'll encounter something that's truly 'repulsive'.
Good question. You could pass in `sourcecode.Foo.generate` manually into `testB` above if you want to force it to not-propagate. You could also shadow it, or have a wrapper method/class that takes in the implicit and forwards it to your block as a non-implicit. def testA(implicit pos: sourcecode.Line): Unit = testA0(pos) def testA0(pos: sourcecode.Line): Unit = { ... real logic ... } Honestly, I don't care too much. It's one line of boilerplate, not worth my effort trying to revolutionize the theoretical underpinnings of Scala in order to save.
Ok, sure. But why would Scala devs want lower wages and a less expressive language? (All the things you consider "confusing", like generics, I couldn't live without.) Why is it desirable to anyone but you for Scala to attract less-skilled, lower-paid devs? If what you want are cheap, low-skilled devs, why not just use PHP?
Anyone have any good or bad experiences using this in production yet?
Ruby has a principle of lease surprise, but it does have multiple ways of doing many tasks, multiple ways to shell out to other processes, multiple ways to handle dates and times, multiple overlapping ways of reading a file. etc. etc.
I think your question does not have anyhing to with the programming language. It's all about understanding the context. Depending on the context you might need to apply different approaches. If the program is going to have low usage, short lifetime, it's simple and it's not going to be updated there's no need to have heavy discussions about common vision and common rules. Just do the project, deploy and move on. Fix if needed. On the other hand, if the project has high performance requirements, it's going to be around for ten years, there's going to be 100 developers and the code base is huge you might want to have common vision and common practices. In our current project we have pretty big codebase with roughly 20-30 projects in various programming languages and with various tech stacks. The projects have been developed already at least for four years and they'll be in use for ten years or more. These are the goals I wish we'll achieve at some point: 1. Data models and rest endpoints should be designed based on actual usecases. Response to requests should be fast and there should be no caching. 2. The projects should be in good shape. Developers should be able to make changes and automated tests in less than 2h. 3. There shouldn't be silos between projects. It should be possible refactor the whole system (over projects) if needed.
As the author [points out](https://groups.google.com/forum/#!topic/scala-coroutines/rrf6TSEAM_Q), this works with the 0.5-SNAPSHOT version of the library and is just a demonstration right now that does not touch exception handling of the futures. What I find fascinating is how the thing is designed to work with this amount of type inference. I have been staring at this and the API docs for a while, but haven't fully figured out how the macros manage to preserve that information (well they are white box, so the API docs don't help much). __Edit:__ Reading the [composition section](http://storm-enroute.com/coroutines/docs/0.5/composition/), the main point is that inside a coroutine you can call `apply` on another coroutine and that will add its yieldvals to the outer coroutine's yieldvals stack and then return the inner's result as if it was a normal function invocation. This is exploited by the async-await implementation by sharing the state of an auxiliary cell between outer (async) and inner (await) coroutine. Very clever indeed.
Yeah I suppose the question was vague, but it was meant to be : I'm mainly talking about ADTs, sealed trait and case classes, products and coproducts that represent some entities that have relation to each other, or that make sense to group together, and in my case, they represent the "state" of the program. My problem is not about structuring actors or about process of development, it's mainly about ADT design. The instances from that ADT get passed around to different actors and services, but I'm mainly talking about whether there were some standards or good practice on ADT design. 
Yeah you have a point, the project is a bit short to worry to much about standards, but I was just wondering if there were ways of structuring sealed trait and case classes that made more sense than others, or if there were pitfalls to avoid. 
Ideally, you would just have C++ style generics and then you can do whatever fancy stuff you want as many levels deep and you won't need to make special exceptions for RTTI and higher kinded types and what not. In the case that this isn't possible, def countCats( cats : List[CatType] forA {CatType &lt;: Cat} ) is less cryptic than def countCats(x : List[_ &lt;: Cat]) Note that I might be making a syntactic mistake, but you get the gist. __________________________________________________________________________________________ &gt; One thing interesting about the language's initial choice of the word "forSome" for existential types is that in English, if something is "for Some", we mean it it "for someone (who meets a set of requirements i.e. for some baker)" and if we have a pizza that is for a person, we mean that we do not care about the type of the person who is receiving the pizza. "forSome" is not only three characters longer than "forA", but it means the exact opposite of what the author intended for it to mean. 
A language can be powerful without being cryptic. Take this example... def apply[A](a: A)(implicit w: Writable[A]): Result What is the writer of this declaration trying to express? Well, I think the writer of this method declaration is trying to express that some form of text - html, raw string, JSON, etc, goes into the method, and it becomes a Writable[A]. Is there a more elegant way of saying that? How about... def apply[TextType]( text: (TextType --&gt; Writable[TextType]) ): Result Wait. We just defined a new symbol. --&gt;. This isn't so confusing if the symbol is unique, but symbols are a pain to look up. We already have a keyword that is a simple present tense verb - "extends". Lets continue that pattern and create a new keywords to represent this conversion. def apply[TextType](text: TextType becomes Writable[TextType]): Result ^ Much better. You could argue that a user would want to define a function called "becomes", but if they follow convention, they would define a function whose name is "become" (past tense) and not "becomes" (simple present). If more keywords continue to follow this pattern, it should be obvious what is a keyword and what is not a keyword (even in the absence of syntax highlighting).
The kind of types you're talking about are values modeling the domain. I personally start with a "models" package that contains everything, initially with a flat organization. After it grows, I start splitting it in sub-packages, depending on the entity being modeled. For example in a recent project I've got "models.AssetConfig", which then contains a "Schedule", which is a more complex data-structure that involves multiple types. So I ended up separating those into their own "models.schedule" package, along with related functions operating on those of course. And this central "models" package is OK, because these definitions are basically defining the project. It's like how if you want to understand the usage of a database, you go and analyze the schema first. And having them in a central location untainted by anything else helps with discovery I think. One problem you'll have is with types created for a job at hand, like for exposing a certain JSON format. Sometimes you end up wanting to expose a JSON document that's a little different than your internal models. Sometimes you have certain definitions only related to a third party service you're interacting with and its usage is contained. So types related to interactions with other services/components and not related to modeling your domain. These definitions should go in the package where the code operating on them is. If you have a special format for your JSON, then that type should go in "controllers", etc.. This is because you want to limit their usage. REST for me is only a communication protocol and I do not find it useful to have a 1:1 correspondence between the "entities" exposed to the outside world and how things are modeled internally. How things get exposed to the outside world (the API) changes a lot, but your internal data-structures don't change so much (with proper domain modeling) or if they do, then you're in trouble. In other words, don't let the API dictate your internal representation of the domain. Do it the other way around. Start by modeling sane data structures and then expose the API. That's the Git way :)