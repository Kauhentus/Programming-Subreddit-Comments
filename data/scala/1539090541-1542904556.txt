Then any code interacting with Java libraries would be full of `?`.
UX is horrible, logs offently missing, trouble using their home made kafka, high pricing...
`JavaNull` exists because it's not practical to assign `|Null` to every type coming from a Java API.
Scala.js is a great platform – I very much enjoy using it myself – but I think it's a bit much for total beginners. You still need to understand the basics of JS and the browser environment to use it effectively. I would suggest learning plain JS for the frontend initially, and trying out Scala.js a bit later. You will have to learn more than one language eventually, anyway. It's no big deal. As for Scala itself, I think it's a great first language, just don't worry about hardcore libraries like cats / scalaz / shapeless / etc. Coursera 's "Functional programming in Scala" course is a great introduction to the language.
There's no one right answer here. I think more than difficulty what matters is how you learn. For instance, I started with C++, which is arguably harder than Scala, and it worked for me because the "bottom-up" style typically taken there aligns with how I learn. I usually recommend Python to most absolute beginners. It's definitely easier than Scala and usually comes with a more "top-down" approach to learning. While you'll certainly be able to learn to program via Scala, my concern with taking it on as a first language is that the tooling around it is not particularly easy to use and I am not aware of beginner-friendly books/guides/tutorials/etc that use Scala. So if Scala interest you, by all means go ahead and learn it, just be aware that you may have difficulty finding accessible content and may spend a fair amount of time dealing with things like build systems which are not directly Scala nor directly programming.
I haven't used it myself but I did find the \[scaladiagrams\]([https://github.com/mikeyhu/scaladiagrams](https://github.com/mikeyhu/scaladiagrams)) project which may do what you need.
It's not clear if you're asking if there's some scala-specific approach to designing diagrams, or if you're asking for a tool to generate diagrams.
Thanks for the response. Unfortunately, I don't quite understand: why is it practical to assign `JavaNull`, but not `Null`?
I am asking for scala specific approach. I am planning to design manually.
I don't know of any resources, but the most important Scala language features I would take into consideration for class diagrams would be implicit conversions, type classes, and sealed hierarchies. You may also want consider how to represent path dependent types and constrains on self type references in traits if you come across them. That said, the more functional a Scala codebase is, the more questionable the value class diagrams are. So it may be best to ignore functional features unless you feel important parts of your model are missing because of it.
Why would you invent a name for it? It already has a name. It's called a *self type*.
Thank you for your comments. I want to design class diagrams in order to understand Scala project which was developed by another developers. When I consider top, class diagrams and sequence diagrams seem nice options for understanding the code. I am a newbie in fp. Do you have any suggestions?
This is what the situation would look like in a perfect world with no interop: Known Nullability / \ Not Nullable Nullable The translation to a language like Scala would be straight-forward: "Not Nullable" would be `T` and "Nullable" would be `T|Null`. But sadly, we don't live in a perfect world, because there is Java interop. The key insight is to understand that the types you get from Java APIs don't fit into this neat distinction of "Not Nullable" and "Nullable", because there is a difference between _knowing_ whether something is nullable or nullable, and _not knowing_ this. Here is the above diagram, with "Java" types added: Nullability / \ Known Nullability Unknown Nullability / \ Not Nullable Nullable Now it might look appealing to just merge the "Unknown Nullability" category with one of the existing ones, and call it a day. Here is the walk-through if someone wanted to turn "unknown nullability types" into `T|Null`, which means null checking would be enforced strictly: **Problem 1:** Consider `T|Null` as an obligation to show that some value is not null at runtime before dereferencing. How can it be done? Easy, just do `value != null`! The problem is that this only works at the top-level! Consider this: `Map&lt;String, &lt;Set&lt;String&gt;&gt;` This would be interpreted as `Map[String|Null, Set[String|Null]|Null]|Null`. There is no way to generically prove that certain parts are not null. In easy cases like the collections API you could traverse everything and check for it. Remember that code like this would have to be written by hand for each and every data structure, and that's assuming the right methods exist! Imagine some random generic interface with few to none methods, as it is common in Java. Imagine receiving an `Iterator&lt;String|Null&gt;` where checking for nullability exhausts the datastructure. This approach is cumbersome, very slow, doesn't even work in many cases and is unlikely to be done by any programmer. So we are back to casting! Let's just do `value.asInstanceOf[Map[String, List[String]]]`! ... except that casting disables any and all compiler checks! Imagine refactoring something and having to go back and check every single cast you were forced to make because of this. In fact, the cast above is wrong, but the compiler didn't complain! Also, due to erasure we will see the crash only at some later point in the program, when the map's value is actually accessed! Turning "T"'s from Java indiscriminately into `T|Null` in Scala makes interop not safer, but more dangerous in practice. **Problem 2:** It sorts all Java APIs in to the same basket. Some Java dev invested a lot of effort to never return null from her API, opting for something like Optional instead? Congrats, that person gets additional punishment in the shape of Optional[Thing|Null]|Null! Now you can get deeper and deeper into all sorts of heuristics: - Pretend that the value inside Optional is never null! - Pretend that Optional is never null! - Analyse the class files! ... only works for static final fields and final classes ... you can't even reliably infer it for final methods, if they call any non-final method inside! - Do static whole-program analysis of the jar files on your classpath! ... great, your program might stop compiling simply by adding a new jar. - Work on symbolic execution! - etc. etc. etc. "But why not use `@NotNull` annotations where they exist?" ... that's already priced in, but is rather useless as practically no one uses them. Relying on nullability annotations will become practical the day Oracle releases a javac that won't compile code without such annotations. Chances for this to happen are rather slim. "But can't we write our own non-null metadata files for existing jars?" ... code written without any concerns about nullability is not really amenable to that style of outside analysis. Also, you would need to have such metadata for every single minor version of a library, because the author of that library might neither be aware of the additional type information grafted onto the side of his library nor does he care. Turning "T"'s from Java indiscriminately into `T|Null` in Scala makes types like `T|Null` less reliable, because they have different meanings coming from Scala code vs. coming from Java code. This will cause people to disregard nullability information where they shouldn't. --- Merging them with `T` has other problems, but those problems are perhaps more obvious to see. What `JavaNull` does is making the distinction between "known" and "unknown" nullability explicit, keeping those types separated, while trying to find a practical middle ground between usability and safety without infecting `T` or `T|Null` with uncertainty. As mentioned in another comment, `JavaNull` might be a bit of a misnomer, because people might also need such a type to allow for a gradual migration of their existing code. Hope this helps!
Functional Programming in Scala specifically emphasizes expressing as much information as you can about a function in it's type signature. Ideally each function can be it's own independent programing, composeable with other other functions to make larger programs. This results in any attempt to draw relations between functions in class diagrams pretty much useless. Functional programs also tend to have a much more clear separation between data in functions, and the idea of composition applies to data as much as it does to functions. Understanding a functional program is a matter listing what data is available, and a listing of what transformations of that data is available, and putting them together to achieve what you're looking to do. Sequence diagrams are still useful, but sequence diagrams help only with the ordering of effects in a concurrent system. It doesn't really help out with understanding the entirety of a functional program. 
When is your next meetup?
It isn't scheduled yet, but probably at the start of November. We would meet every week until we have covered all 15 chapters. It is a bit intense, but if we meet once a month it would take over a year. Once this book is finished, I will be grabbing another. 
I know that meetup.com has some communication tools, but I won't be relying on those. WhatsApp would definitely be a more direct way of communicating, but slack a more productive way of collaboration. Based on joiners feedback, I will create a channel on the preferred platform.
I am working on creating an online course to teach Scala. I have one published via Pluralsight ([https://app.pluralsight.com/profile/author/harit-himanshu](https://app.pluralsight.com/profile/author/harit-himanshu)) and now on to my next course for the year. Meanwhile, I am also thinking to start YouTube channel to teach Scala/Functional Programming Concepts using Scala (not sure how much value I would add there), but I will give it a try since I love working with the language!
Did you look at [https://www.jetbrains.com/help/idea/class-diagram.html](https://www.jetbrains.com/help/idea/class-diagram.html)? 
Thanks. Please update here or DM me if you do.
Great! I figured it out! 
\+1
I would add “and beyond” to the title of this post, OP...there is plenty of interest here for mid-level and even advanced Scala developers.
Thanks for explanation 
Yes, I looked at. I also sent an email to Jetbrains support. They wrote that class diagrams are not fully supporting Scala. I also tried to generate a class diagram. I had a class which is extended from a base class and a trait. Jetbrains just showed the mixin in the diagram, it did not show the inheritance relation in the diagram. So IntelliJ's class diagram may not be a good reference.
The repo is awesome, thanks for sharing
Will do
You could post it to the Scala slack channel here http://irishtechcommunity.com
The support for generic tuples looks very cool. It looks like the implementation is already using match types to achieve it: https://github.com/lampepfl/dotty/blob/master/library/src-scala3/scala/Tuple.scala#L31-L55
I didn't know that existed. Will definitely do.
TypeScript's typesystem isn't sound and I don't think flow's is either, so I don't think it has much value saying that this small part of my codebase is (should) be sound, if all the rest is wild west and data passed to the "sound garden" may not even be of expected types, so runtime errors may still occur in the better part of a codebase. I honestly don't think nominal typing is bringing, in TS or flow, much value. It would be interesting if they would have had some master switch to turn it on everywhere, but if it is only in some parts which must be explicitly stated, I think it is not worth the effort. I imagine there would still be issues with libraries not supporting it. JS languages trying to be as much compatible with JS as possible IMO will always have these issues. And that is probably the reason I really like PureScript (still noob at it). It is essentially Haskell with some tweaks and when one uses PS libraries with strong static typing, one is actually getting really strong guarantees from the compiler.
Yes, but as I said, for JavaScript we have explicit facades written in Scala. We can appropriately use `Foo | Null` or `Foo` depending on whether the particular value is nullable or not in the particular API that the facade describes. There is no need for the "unknown" nullability type `Foo | JavaNull`. When interoperating with Java, the Scala compiler needs to reverse-engineer Java's `Foo` into `Foo | JavaNull`, because neither `Foo` nor `Foo | Null` is a satisfactory answer.
`Int` is a primitive type, not a reference type. Only reference types (such as `String`) can be `null`. All primitive types have equivalent "boxed" reference types, e.g. `java.lang.Integer` boxes an `Int`. This is how Java does it, and since Scala primarily targets the JVM, it inherits this particular quirk.
First example does not work, because in \`var x1 = null\` expression you didn't specify what type x1 should be. If you write \`var x1 = 1\` then Scala can infer that x1 should be Int, because 1 is Int. If you pass null Scala will infer type Nothing, because Nothing is the bottom type. &amp;#x200B; Second example does not work because of how Java works - Int wraps Java type int, and int in java is a primitive type and can't be null, because in Java only subtypes of Object can be null. String is a subtype of Object, so it can be null.
Thank you!
Thank you.
Can I use this to make my program work? asking for a friend
Good point.
Hi, I want to force implementers of a trait to define a `case class`. Currently, the best I came up with is this: ``` trait A { type T //... } class B extends A { case class C(c: String) Type T = C //... } ``` But this does not force `T` to be a case class, and I need it to be one ! Any insights ? =) 
I figured out the second question! All I need help with now is the sumOfSquares function.
Let's assume you could constrain the type to be case classes. It could still be any case class and there is no guaranteed consistency that they call match the same way. For example you couldn't have `type T = C1(i: Int)` and `type T = C2(s: String)` in two different instances of `A` and then match on them identically. So what would constraining `T` to be only case classes really give you when you still need to know the actual type to do any matching?
Thanks for this well written answer, things are clearer now! 
 def sumOfSquares(arg: Int): Int = { def calcSum(x: Int): Int = if(x * x &gt; arg) 0 else x * x + calcSum(x + 1) calcSum(1) } sumOfSquares(1) sumOfSquares(2) sumOfSquares(3) sumOfSquares(4) sumOfSquares(100) Make sure you are able to explain *how* this works. 
Thank you so much! This worked perfectly! 
[https://www.scala-exercises.org/](https://www.scala-exercises.org/) is good. The scala docs seem pretty good: [https://docs.scala-lang.org/](https://docs.scala-lang.org/) The scala/scala gitter channel may be a good place to ask questions, or at least get directed to the right place: [https://gitter.im/scala/scala](https://gitter.im/scala/scala) The scala coursera course(s) are a good resource: [https://www.coursera.org/courses?query=scala](https://www.coursera.org/courses?query=scala) Scala for the Impatient targets people from a Java background: [https://horstmann.com/scala/](https://horstmann.com/scala/) Other than that, come up with a project and try to build it in Scala!
&gt;If you pass `null`, Scala will infer type `Nothing`, because `Nothing` is the bottom type. It actually infers `Null`, which is a subtype of all reference types, and whose only value is `null`. scala&gt; val a = null a: Null = null There is no value of type `Nothing`, and because `Nothing` is a subtype of `Null`, the type actually *can't* be `Nothing`. scala&gt; val b: Nothing = null &lt;console&gt;:11: error: type mismatch; found : Null(null) required: Nothing val b: Nothing = null ^ The [Scala docs](https://docs.scala-lang.org/tour/unified-types.html) do a pretty good job on this.
In order to consume message from kafka, you have several libraries at your disposal : * [https://github.com/monix/monix-kafka](https://github.com/monix/monix-kafka) * [https://github.com/ovotech/fs2-kafka-client](https://github.com/ovotech/fs2-kafka-client) * [https://github.com/akka/alpakka-kafka](https://github.com/akka/alpakka-kafka) * [https://github.com/cakesolutions/scala-kafka-client](https://github.com/cakesolutions/scala-kafka-client) These libraries will give you a binary blobs of the data carried by kafka events . Just read the documentation and choose whichever fits your usecase better and your level of knowledge. I personally like fs2 as a streaming abstraction, so I'd personally roll with that (or monix), but you might want to roll with an akka-based solution if you're more comfotable with the actor model. Then you need to parse the binary to XML using the Scala language. Just google it, there are several decent articles on the subject. Then you need to validate the parsed XML object against some data structure that matches your model (case classes/sealed trait). I'm not too aware of libraries that automate this process, so you might have to write a little bit of boilerplate there. Then from your fairly vague question, I assume you need to save that data structure into your relational database. I'd personally roll with [https://tpolecat.github.io/doobie/](https://tpolecat.github.io/doobie/) , but you could decide to go with [https://github.com/getquill/quill](https://github.com/getquill/quill) or [https://developer.lightbend.com/docs/alpakka/current/slick.html](https://developer.lightbend.com/docs/alpakka/current/slick.html) 
How to check at compile time if a conversion is possible ? The best I came up with it this: \`\`\` case class A(a1: Int, a2: String) case class B(a2: String) &amp;#x200B; def mapper (x: A): B = { new B(x.a2) } \`\`\` Which is okay, but \*\*in my use case I have lots of large case classes, most of which are just subsets of others\*\* (with the same attribute names). Is there a way to this from a higher level of abstraction and spare the implementation of all these "dumb" conversion function ? 
Any examples for why I would need or care about those Match Types? It seems even more power and new things to know and I don't see any example for its use besides "there, it's here, see how cool it is".
It's a type-level function. Looks like it's going to (hopefully) remove the need for `Aux` types
No mutation testing is not really for \`making a program work\` it's more for giving insights in the quality of your unit tests.
We were aware of the way piTest did mutation testing and why it isn't the way to go for scala (the byte code stuff). But did not see this article funny to see that they came up with the same solution as well.
"Asking for a friend" is the thinking mans version of '/s'
It's a real omission not to include any mention of a motivation for a new feature.
The main motivation is to fix the hardcoded arity limits that Scala has (i.e. the tuple 21 arity limit) and to avoid the needs for hacks like HList (which basically implements their own derivation of type level functions using the Aux pattern).
Awesome, thanks!
You might want to have a look at [https://github.com/scalalandio/chimney](https://github.com/scalalandio/chimney)
I think I never compiled a single file from the command line. I usually use [Ammonite](http://ammonite.io/) for trying out a few lines of code quickly and [SBT's](https://www.scala-sbt.org/1.x/docs/Running.html#Continuous+build+and+test) or [Mill's](https://www.lihaoyi.com/mill/#watch-and-re-evaluate) file watch modes if the code base gets bigger.
please be careful with this knowledge!
Very promising, thank you :) 
I think the point of docker is not just being a VM. The point of docker is being VM with just what you need to run your application so you or your load balancer can deploy as many instances of it as you need with little configuration. 
Pretty easy to do that with a jar file. The JVM is a much more mature runtime dependency than docker is. I mean, that’s just my experience on AWS, docker added a layer of maintenance and debugging difficulty that just didn’t solve any problems. 
Yes, this helped a lot! Thank you very much for the thorough explanation!
I don't think Scala is a great first language: it has a number of awkward edge cases around e.g. JVM compatibility that make sense in terms of production-code practicality but get in the way when learning. Personally the path I took went via Standard ML which I'd recommend (look up ML for the Working Programmer if you go that way): it's a simpler and more strictly one-way-to-do-it functional language, which will force you to solve problems in the ways that are generally good style in Scala.
What if you want to orchestrate self-healing deployments in something like kubernetes? 
Those concerns seem orthogonal to docker to me. I mean, you start another machine running the app? It’s not like that’s something that only docker does. Maybe I’m just used to immutable server deployments and docker doesn’t really add anything to that picture. 
Yeah I’m sure that’s the case. My employers are often deploying large scale distributed apps and a lot of them are using things like AWS under the covers. Most of the better Deployment orchestration soutiens I’ve seen leverage docker as the longues franca of a deployment which simplifies things. I’m trying to learn more about it and what the best solutions are ...
The distinction between `abstract class` and `trait` is not important enough to show on a diagram. You may well want to draw a conceptual distinction between "interface inheritance" and "implementation inheritance" but this will not align neatly with anything at the language level; some `trait`s are interfaces and some are implementation. At first blush `case class`es are values and should not participate in inheritance, so are not really OO at all and probably should not appear on class diagrams. However: Traditional `extends` inheritance combines three things: a) composition of datastructures, b) conformance to an interface, and c) implementation of that interface via delegation. Like most languages Scala offers a non-inheritance way of doing a) (`case class`es or indeed regular classes may contain others); *unlike* many OO languages Scala also offers a non-inheritance way of achieving b) via the typeclass pattern (note that typeclasses are a direct language feature with keywords etc. in e.g. Haskell or Rust, but in Scala they're a pattern implemented in terms of lower-level language features). This means that in some Scala codebases, inheritance-like problems are solved with `case class`es and typeclass instances, similar to how you'd solve them in e.g. Rust. But since Scala has no direct support for delegation, emulating a true OO hierarchy this way is cumbersome and relatively rare. All of which is to say that you might find some limited OO-hierarchy-like behaviour among `case class`es that have typeclass instances, and to the extent that you find inheritance diagrams useful you might also want to represent the existence of typeclass implementations in a similar way (*especially* when a typeclass instance works largely by delegation).
LOL. I work with Spark myself. Not my primary reason for learning Punjabi, but I the Sikhs (and Indians in general) are a huge and really supportive part of Spark and open source
Any examples of scala using built-in json parser from std library? It was here before scala.util.parsing.json._ But now in latest version seems to be missing?
and I don't want to use external libraries since my json usage is very minimal. 
have you take a look at `become`?
What are you hoping to accomplish by making the connection actor be a child of the torrent (or rather the pool)?
Well the \`PeerRouter\` pool needs to allocate requests to each peer and manage the lifecycle of the peer (timeouts, etc). Since each peer has different blocks, I need to request different data from each to maximize throughput. I was under the impression that akka good practices were about managing the life-cycle of actors through actor hierarchies. Since I can't move actors around I need to place them in the right place to begin with. &amp;#x200B; If I can't re-\`Register\` actors and can't move them around, I'm sort of stuck.. I want to maintain a strictly tree based message/lifecycle policy. I'm sort of puzzled that akka doesn't support anyway to do this. It seems like a pretty common use case. If it wasn't a Tcp connection I could just kill the actor and create a new one somewhere else, but I can't without closing the connection. &amp;#x200B; It seems like the only solution is to create 2 actors for each peer. One that just manages the lifecycle of the TCP connection and the other that will just route messages through it. This doubles the number of my actors though and just seems inelegant and complicated.
Was looking for something like this, thanks! So far only got half way through reading "Programming in Scala" and struggling to write any useful programs (still feels like a very foreign language). I should follow your example and make a repository I suppose!
That last paragraph is correct. Create an actor to track your TCP connection data. You will route data through that actor. Yes, this seems complicated and inelegant at first. &amp;#x200B; Try looking at it this way: I've got an actor responsible for managing a data connection. Only one, and no more. It also knows that it's got a peer manager that will periodically tell it to do something. So your lifecycle might be something like: Startup. Register with the PeeringManager. PeeringManager tells me to talk to PoolManager &lt;X&gt;. Tell PoolManager&lt;X&gt; that I'm alive. Tell PoolManager that I'm ready for work. Hear RequestBlocks(start,end, destinationActor). Send Request to TCP. Send responses to destinationActor. Finished? Tell PoolManager that I'm ready for work. &amp;#x200B; PeeringManager is responsible for *introducing* PoolManagers and ConnectionActors. The introduction pattern is the key, and is common in high-speed actor systems.
another thing you could try is [http://aperiodic.net/phil/scala/s-99/](http://aperiodic.net/phil/scala/s-99/). I have done some in the past and they help a lot
shameless plug: I have recently taught a course with Pluralsight on Scala. This assumes no knowledge of Functional Programming and is designed to learn from basic steps. The course is listed at [https://www.pluralsight.com/courses/scala-big-picture](https://www.pluralsight.com/courses/scala-big-picture). The course preview is also available at [https://www.youtube.com/watch?v=Edd09ecogvQ](https://www.youtube.com/watch?v=Edd09ecogvQ) Let me know if have any questions, I would love to help
Hello, Harit. Thank you for your suggest. I watched the youtube link. - “Knowledge of at least one PL is required”. Which “preliminary” PL would you suggest?
Any, If you know basic programming constructs, you should be good to watch this course
Looks so good! thanks for sharing :")
you're welcome. Happy Learning!
This would probably be easier to write if you made use of a `for` comprehension: `def update(id: String) = Action(parse.tolerantJson).async { implicit request =&gt;` `val body = request.body` `val data = body \ "data"` `val userIdOpt = for {` `type &lt;- (data \ "type").validate[String].asOpt.filter(_.equals("user"))` `if type.equals("user")` `userId &lt;- (data \ "attributes" \ "user").validate[String].asOpt` `} yield userId` `userIdOpt.fold(Future(BadRequest)) { userId =&gt;` `myService.find(userId).map { userOpt =&gt;` `userOpt.fold(NotFound) { user =&gt;` `// update with the new property and return a Created status code` `}` `}` `}` `}` Any time you are reaching for the `get` method on an Option you are probably doing things in a way that makes things more complicated. Generally these can be replaced with some combination of `map`, `flatMap` and `filter`. If you need to use several of these then you probably want a `for` comprehension. You can also make the above easier if you make use of a monad transformer, such as [OptionT](https://typelevel.org/cats/datatypes/optiont.html) from cats. Monad transformers make easier to work with stacks of monads, in this case `Future[Option[A]]`. It allows you to treat a nested monad as if it was a single layer monad. So, when you write transforms, you only have to unwrap one layer. Using OptionT you can write the above as: `def update(id: String) = Action(parse.tolerantJson).async { implicit request =&gt;` `val body = request.body` `val data = body \ "data"` `val userIdOpt = for {` `type &lt;- data \ "type").validate[String].asOpt.filter(_.equals("user"))` `if type.equals("user")` `userId &lt;- (data \ "attributes" \ "user").validate[String].asOpt` `} yield userId` `userIdOpt.fold(Future(BadRequest)) { userId =&gt;` `OptionT(myService.find(userId)).fold(NotFound) { user =&gt;` `// update with the new property and return a Created status code` `}` `}` `}`
I believe most people would decode the `json` into a case class and then validate it, unless it's a huge `json` in which case I would probably use something like `circe-fs2` but the idea would be the same.
Ok thanks, that makes sense. Would you suggest that I also route messages for connections I proactively create through the same connection actor? Even though it's not strictly necessary?
Thank you for the tip to use AshScriptPlugin. The image size for my current project went from 699MB to 139MB.
Something like this: (for { type &lt;- (data \ "type").validate[String].asOpt.filter(_.equals("user")) userId &lt;- (data \ "attributes" \ "user").validate[String].asOpt } yield { // no need to the for all, the for comprehension fails if one of them is not defined // if your service returns an Option, the map extracts already the model, you don't need the isDefined in the map block myService.find(userId).map(user =&gt; { // update with the new property and return a Created status code // do stuff // return created Created }).getOrElse( NotFound ) }).orElse { Future { BadRequest } } &amp;#x200B; &amp;#x200B;
That won't work because if the returned list is empty, the map won't do anything. The solution is pattern matching: &amp;#x200B; myService.findModelsByProperty("mock-property") match { case Nil =&gt; JsArray.empty case models =&gt; models.map(m =&gt; Json.toJsObject[A](m)) } The code you are showing only makes sense if you return a list of lists, in that case, it should be: myService.findModelsByProperty("mock-property").map(_ match { case Nil =&gt; JsArray.empty case models =&gt; models.map(m =&gt; Json.toJsObject[A](m)) }) &amp;#x200B;
Hmm. I was thinking you meant actor instance. So you would recommend that I have a actor for each connection, outbound and inbound? And then have a sibling actor that would route messages to each one? Sorry to keep asking you questions, it's just that this untyped actor stuff is making me nervous. The potential for bugs seems... large, to say the least (not that it matters for this project, but it matters for my job, so I want to do this right).
I've been experimenting with http://magnolia.work to do derivation of Spray json formats typeclasses with type hints in the resultant objects. I've also started reading FP for Mortals
Created a streaming app to filter and aggregate some big/"fast" data using Structured Streaming. It was pretty fun. 
If you are familiar with cats, Maybe [flatTraverse](https://github.com/typelevel/cats/blob/191fae4f5bb2bc58af967dcaef0fd97998a7b246/core/src/main/scala/cats/Traverse.scala#L50) works for you. Unfortunately cats don’t have Seq instance. I recommend you to use List or Vector `xs: List[A] f: A =&gt; Future[List[B]] import cats.implicits._ ys: Future[List[B]]: xs.flatTraverse(f)`
Thanks! I figured if there was a better way, I might as well ask! :)
Avoid using `Await`, just pattern match on the `onComplete` method from `Future`. The rest sounds okay. 
I don't have time at the moment to test this, but I suspect this is due to how the second is desugared. The assignment (probably) leads to a `map` call producing a tuple of `(date, data)`, and the `if` leads to a `withFilter` call. If that's right, then it means that an intermediate collection is made that contains all the tuples (with the results of the DB calls). Compare that to the first example, where the result of each DB call can be GC'd immediately after it's used. A workaround for the for-comp way would be to use an iterator, which would prevent making the intermediate collection, ie: for { date &lt;- dates data: Seq[Data] = findDataOnDate(date) if data.nonEmpty } { // do something } 
I'm not sure if there's a relation between the two code samples and the stack trace you've posts. It would help if you describe the type of `dates` and `findDataOnDates`. In the stack trace above there's a `Future` being evaluated, which is calling `map` on `Try` which is calling a closure in a function anmed `findCollateral` which you've made no mention of before, which is calling `toSet`. All of those types and methods above make no appearance in wither sample you've made, so there's nothing to really go off of.
It's most likely something like this.Your for comprehension is \`map\`s and \`flatMap\`s, so it's trying to hold on to all the data for every date in case you need it later in a \`yield\` block. If I were you I'd write it like dates .map(findDataOnDate) .filter(_.nonEmpty) .forEach(doSomething) &amp;#x200B;
Are you using `yield` with the for comprehension? You can check how Scala code is desugared in the REPL. ``` @ import scala.reflect.runtime.universe._ import scala.reflect.runtime.universe._ @ show { reify { for { x &lt;- Option("Hi") } yield println(x) } } res1: String = """ Expr[scala.Option[Unit]](Option.apply("Hi").map(((x) =&gt; Predef.println(x)))) """ @ show { reify { for { x &lt;- Option("Hi") } println(x) } } res2: String = """ Expr[Unit](Option.apply("Hi").foreach(((x) =&gt; Predef.println(x)))) ```
Set of Future is the output from Slick.
Not using yield with the for-comprehension, no. But thanks for this. I'll look into it. 
However, as I point out in response to /u/MyFunc's comment above, I'm not using a `yield` clause, so the loop de-sugars into a for-each. 
`findCollateral` is the equivalent of `findDataOnDates`. I scratched out the control structure in a text file and changed some of the names around to make it more generic/easier to follow. Thanks for catching that.
Actors are *incredibly* cheap, especially actors that are going to spend much of their time idle (but not holding a thread lock). Don't be afraid to create them for anything and everything. Messages are also incredibly cheap (when you're on a non-clustered ActorSystem). They'll be references in-memory with little or no copying. &amp;#x200B; &gt; I want to maintain a strictly tree based message/lifecycle policy. I'm sort of puzzled that akka doesn't support anyway to do this. &amp;#x200B; The lifecycle policy *is* tree-based. You just can't change parents mid-stream. The message-passing policy is non-hierarchical by design. I'm sure there's a template someplace to implement it, but I really wouldn't go there. &amp;#x200B; Remember that what you're doing with Actors is encapsulating *behavior*. Rather than breaking up your problem domain into nouns and verbs as you would for OO, break it up into Behaviors over a problem's subdomains. BitTorrent has several such subdomains: Network Comm, Peer Management, and Torrent storage. You could maybe even break out Torrent Storage into smaller pieces. &amp;#x200B; The key is that your TCP connection actor has one set of behaviors: setup a connection, tear down a connection, send data, and receive data. It might be smart enough to differentiate types of data, but I wouldn't go that route. **This** is where you relieve yourself of bugs -- each actor is so small you can reason about its behavior in your head. &amp;#x200B; I'm happy to provide more help, but I don't want to make a wall of text. Feel free to ask away. &amp;#x200B; A **great** resource is Alexandru Nedelcu's [Scala Best Practices github](https://github.com/alexandru/scala-best-practices/blob/master/sections/5-actors.md), specifically the Actor section (which I've linked you to). The entire series of notes is great, and **greatly** improved my scala code, but the Actor section (where he goes over exactly how you can have actors with absolutely no vars and why you should do this) took my Akka work into an entirely different level. &amp;#x200B;
I would say that's correct -- storing Futures in a Set will remove duplications as defined by reference equality of the Future objects, not as defined by the data they return or anything like that. 
For the past year or so, I've been working on an open source library called [nextime](https://github.com/lu4nm3/nextime). It uses strong typing to make working with cron expressions a safer and easier experience. It's still a work in progress but I felt the project was at a good place for an initial release.
Lucid Software | Software Engineer in Application, Dev Tools, or Site Reliability Engineering | Salt Lake City, UT, USA | Onsite | Full Time Lucid Software is the creators of Lucidchart and Lucidpress - world-class web applications that push the boundaries of what is possible in your browser. The entire application back end is written in Scala. The frontend stack uses TypeScript, Angular, WebGL, and the Google Closure Compiler. The build system is Bazel. We are hosted in AWS. We are a ~400 person company outside of Salt Lake City Utah, that is growing like mad. We are looking for * Application engineers to help build applications that users love. * Development tools engineers to help build a development experience that engineers love. * Site reliability engineers to help build a scalable, reliable, and performance production system. More information can be found here [Application Engineer](https://www.golucid.co/careers/f9cb83be-e016-4be5-91dc-598e17e8d04c?team=Engineering) [Development Tools Engineer](https://www.golucid.co/careers/c8de139c-6da7-44dc-a4af-95052aa2a086?team=Engineering) [Site Reliability Engineer](https://www.golucid.co/careers/be16cacc-4cd3-4be0-9ec0-070646f9ec69?team=Engineering) Please DM me if you would like to chat about any of the positions. Please mention this Reddit thread if you apply :D
I forgot to mention that we are also hiring interns. If you are looking for a Scala internship, I would love to chat with you. You can find more info here [Software Engineer Intern ](https://www.golucid.co/careers/90f63861-6418-4e94-9074-de89eaca47be?team=Engineering)
Is visa is sponsored by company?
Yep :)
It may de-sugar with a \`foreach\` at the end, but I suspect it may be doing a \`map\` in the middle. Can you do show in the repl and post the output so we know for sure? 
Hi, saw that there are positions for Data Scientists too, including Interns. Do you have relocation package for this also?
I believe it is the same for Data Science/Analytics as it is for SE, which I responded to above. However, I'm not 100% sure on that, as I'm in engineering and not our data org.
Ostensibly, everything in the entire ecosystem will "stop working", not just scalaz. However, the tools and libraries that get migrated to Scala 3 will be a factor of how much time and desire the volunteer contributors have to support a Scala 3 branch (or to continue supporting a Scala 2 branch), and the extent of changes that are necessary. For some projects, it may be trivial, for others it may require a rewrite. Nobody knows enough about the specifics of Scala 3 to answer your specific question. It is worth noting that Scalaz does not use any macros. The one compiler plugin it uses (kind-projector) can be replaced with boilerplate if necessary. My guess is that the build will be the biggest problem. Supporting 4+ versions of Scala 2, and cross building to both native and scalajs, is already pushing TravisCI to breaking point.
Or, you know, you could do your own homework.
You mean like Apache gearpump? I’m not familiar with celery.
EPFL | Compiler Engineer | Lausanne, Switzerland | ONSITE | Full Time Between my group at EPFL and the Scala Center we have a couple of vacancies for compiler engineers. The task is to work on the Scala 3 compiler (codebase known as dotty) and its supporting ecosystem of tools (i.e. IDE, build tooling, testing framework, core libraries). Experience: We are looking for people who are experts in compiler construction and know Scala well. When in doubt, the compiler aspect is the most important one. Starting date: Dec 1st, 2018, or up to 3 months afterwards (earlier is better). Salary: Depends on age and experience, as determined by EPFL HR. I would expect it to be somewhere in the 75k to 95k CHF/year range, depending on your experience. EPFL regulations don’t allow remote work (sorry), so you have to be prepared to move to Switzerland. Here’s a link to the [job description](https://recruiting.epfl.ch/Vacancies/693/Description/2)
Akka. It's different than celery (actor model vs queue with workers) but generally will serve the same purposes.
Thanks for sharing. Looks a lot helpful.
kafka doesn't really work as a task queue imo, rabbitmq is better for that
Also would like to see something like celery or resque. At work we end by building basic task queue on top of redis.
TBH, the data model looks slightly odd. Hubs and Spokes seem to have overlap - isn't a Hub just a Spoke with one ref? Modelling them as separate cases just adds more baggage. Get rid of Hub. Also this is basically just graph traversal - just rename Spoke to Edge. In fact I would be tempted to first rewrite the data model as an Adjacency Matrix. Under that representation operations like fromJumps can be reduced to matrix ops.
As I've already stated in the title of this thread, it would be absolutely great and utterly amazing to have any help and feedback about this project. I'm looking for developers as well as maintainers and reviewers. Helping newcomers to learn a FP language, and in particular Scala is not as simple as it seems, and having the backup of other volunteers makes the real difference. You have to consider that, at the moment, it's a lot easier to find a project like that in Case or JS rather than Scala. We should try to make the difference. 
No. Unfortunately you have to you roll your own.
Thanks for your feedback. Yes, if my only goal was to accomplish graph traversal, I would simplify both city types to just one city with a set of refs (one or many). But I intended to represent it using a Spoke-Hub distinction. Agreed that it isn't necessary if this were just a demonstration of graph traversal, but I wanted to track the distinction between Spoke and Hub cities throughout the entire state of the application.
Hello, thanks for your time and energy in working through my code. To clarify: My code certainly does produce the expected results in the context of the application. It has been been tested extensively with each specific requirement addressed. There is no off-by-one error. You got the general idea of the requirements (either return cities reached in the exact number of jumps, or return all cities reached within a maximum number of jumps). But one additional requirement I wrote conditions for was not to include the origin city in the results unless it is reached by a separate route (i.e. no routes loop around another way back to the origin city). Based on my cursory read of your rewrite, I don't think that case is being handled anymore. Had my requirements been simpler, I likely would've come up with a function that closely resembles yours.
There is at least 4 different IMDGs for JVM. They all have queues. Also Redis.. or pretty much any other DB. Any shared filesystem would work.
The off-by-one is still there in the handling of Set, it's just dead code, because ```c1``` is always either the origin city, or the city that was added to ```acc``` immediately before the ```jump``` method was called, therefore one of ``acc(c1)``` or ```n == city``` will always be true. In the handling of ```Hub``` I messed up about it being off-by-one, but stopping the trip as soon as you see a path home still causes strange things to happen in, for example, a triangular arrangement of three hubs (for the following examples I'm using your current GitHub code, but as far as I can see the bug is the same): scala&gt; val network = Network("Test", Map( "First" -&gt; Hub("First", Set("Third", "Second")), "Second" -&gt; Hub("Second", Set("First", "Third")), "Third" -&gt; Hub("Third", Set("First", "Second", "End")), "End" -&gt; Spoke("End", "Third"))) res0: ... scala&gt; network.fromJumps("First", 3) res1: Set[com.bcm.transit.City] = Set(Hub(First,Set(Third, Second))) Or if you add another hub: scala&gt; val network = Network("Test", Map( "First" -&gt; Hub("First", Set("Third", "Second")), "Second" -&gt; Hub("Second", Set("First", "Third")), "Third" -&gt; Hub("Third", Set("First", "Second", "Fourth")), "Fourth" -&gt; Hub("Fourth", Set("Third", "End")), "End" -&gt; Spoke("End", "Third"))) res2: ... scala&gt; network.fromJumps("First", 4) res4: Set[com.bcm.transit.City] = Set() This is not a problem if your network is acyclic, but in an acyclic network there will never be a valid route back to your origin anyway, so having a special case for it would be unnecessary.
I'll bite. 1. Writing code like sentences is pretty rad: `.` optional in places enables that. 2. Use someone else's VM to get their ecosystem. 
Probably didn't learn much of anything from Elixir. By the time Elixir was released, Scala was pretty established. Maybe some minor things have bled over.
The "allow circular routes back to the origin" requirement is a bit messy, but should be satisfied by going back to counting up, then selectively allowing the origin as a destination past the second hop: def fromJumps(origin: City, maxJumps: Int, allJumps: Boolean = false): Set[City] = { def jump(currentCity: City, nextJump: Int, visited: Set[City]): Set[City] = { val nextCities = if (nextJump &lt;= 2) routes(currentCity).diff(visited) else routes(currentCity).diff(visited - origin) if (nextJump == maxJumps || nextCities.isEmpty) if (allJumps) (visited - origin) ++ nextCities else nextCities else (nextCities - origin).flatMap( c =&gt; jump(c, nextJump + 1, visited + c) ) } if (maxJumps &lt; 1) Set() else jump(origin, 1, Set(origin)) } &amp;#x200B;
dynamic typing is no fun
Task queues can be implemented in Kafka using consumer groups
you are right!
Sorry - I posted here accidently. I've moved to /r/ScalaConferenceVideos and deleted this now.
I am doing a little side project with Elixir and in my work I use Scala fulltime, this things should be learned by Scala from Elixir (IMHO): * Pipe Operator syntax sugar is pretty * A build tool umbrella projects friendly like Mix * Pattern matching on method definition, less verbose and powerful, it is so powerful that you don't need a static type system. If you want "compilation" type checking you can use Dyalizer for type checking. * A built-in distributed process system, (you don't need external libraries as akka or akka-remote) (This feature came from ErlangVM is not unique of Elixir) * A hot-reload deploy system with version management....(This feature came from ErlangVM is not unique of Elixir) This is just my honest opinion :) &amp;#x200B;
&gt; Pipe Operator syntax sugar is pretty ScalaZ and Cats (I think) offer implementations of the `|&gt;` operator if you want it. &gt; A build tool umbrella projects friendly like Mix What do you mean by "umbrella projects friendly"? I've always been happy with Maven's support for multi-module projects and parent project definitions, what functionality are you missing? &gt; Pattern matching on method definition, less verbose and powerful You can do this in a context where a function is expected, but I agree it would be nice to have it for methods. (Actually I wish it was easier to use functions as methods in general). &gt; If you want "compilation" type checking you can use Dyalizer for type checking. My experience is that optional typechecking is a terrible idea (I came to Scala by way of the checkers framework for Java). If the types aren't a built-in part of the language standard then not all tools will understand them, which leads to e.g. automated refactoring not preserving types correctly. And if there's nothing that enforces that types are always checked then it's very easy to end up with e.g. published libraries with published type definitions that are simply wrong, which can lead to "impossible" errors a long way away from the call that actually caused the problem. &gt; A built-in distributed process system, (you don't need external libraries as akka or akka-remote) Eh maybe? Most programs don't need them, so my instinct is that it's better to have it as a library (which can then have its own release cycle etc.) rather than a mandatory part of the language. What are the advantages to being built in?
This is brilliant. Thank you so much op
This was a really good article, and i know not a ton of people like to look at things like this, but the implications of this could also show that performance could slow down due to cache misses if you're trying to keep your objects size small enough to fit in the cache while using value classes.
so you need to assign the result to a value. &amp;#x200B; `val myList = input` &amp;#x200B; Because your method `input` return the list. &amp;#x200B; Also an other error, I think that you need to have `readLine` inside the `while` loop otherwise the value of `x` does not change. 
&gt;That worked! Thanks. &amp;#x200B;
Nice article, but it doesn't tell the whole story. There's even more surprising limitation of value classes which involves universal traits. For example: ```scala trait Base extends Any { def method: String = "boxboxbox" } class Wrapper(private val str: String) extends AnyVal with Base ``` Now, calling `new Wrapper("").method` **will** instantiate `Wrapper` in runtime, which is absolutely terrible. Despite all their limitations, value classes are still worth using. Value classes are statically enforced to be non-null which is nice. However, they can still *wrap* a `null` value but [I wouldn't recommend that](https://github.com/scala/bug/issues/7396). One place where value classes yield a pretty clear benefit is when they are used as field types (e.g. in a case class). This might significantly reduce memory usage and make heap-stored data structures much faster. For example, [this benchmark](https://github.com/AVSystem/scala-commons/blob/master/commons-benchmark/jvm/src/main/scala/com/avsystem/commons/core/OptBenchmarks.scala) compares simple list-like data structures which use either `Option`, `Opt` (which is `Option` reimplemented as value class) and unwrapped, nullable values. `Option` turns out to be about two times slower which is exactly the expected result, because there are twice as many pointers in memory to traverse.
You are awesome
I think the main difference with celery/resque is a an emphasis on *persistant* job queue. Although I hear there is support for Actors persisting messages, I early see it in practice and/or in docs
I'd take the non-Java JVM section with a bit of salt. The survey was sourced mostly from Java User Group members
Clojure is the biggest non-java language? Yeah, I don't think I'll believe anything else in the survey either.
How does this handle Null annotations etc in Java Libraries?
&gt; ScalaZ and Cats (I think) offer implementations of the |&gt; operator if you want it. Might not even need it: https://github.com/scala/scala/pull/7326
I can recommend this approach as an alternative that truly does not box: http://nocandysw.com/opaque-types-to-infinity.pdf
&gt; Well, it's true, but use a library for functional style in a 'functional' language is strange, every library is alien code in your source code and a hard dependency. I don't think we should be so worried about libraries, especially in a functional language where we can trust that there will be no magic or special cases. Having things be a library rather than a language builtin means that we can be confident that they're plain old ordinary code, so the more we can push into libraries rather than the language, the better, IMO. &gt; https://elixir-lang.org/getting-started/mix-otp/dependencies-and-umbrella-projects.html this is umbrella project friendly for me, maven is a little hard compared with mix, imo What concrete difference are you talking about? I don't see how that's any different from what maven does. &gt; This case is a issue of your tool, not problem with the language type checking, in example, you can try to refactor a Scala class name on sublime and see if all works, is a language problem or a tool feature? Without these things being part of the language it's very hard to get everyone to co-ordinate on what should be supported. Does the maker of a text editor or profiler know about every possible language extension? Does the maker of a language extension know about every possible tool? Realistically the language specification is the only place for these people to co-ordinate. Sublime doesn't have Scala support (as far as I know), but at least people agree what "Scala support" is and that Sublime doesn't have it. Whereas with an external tool you can get the situation where: Sublime supports Elixir, Elixir supports static typing, but if you use Sublime to edit your Elixir then it will break your static typing (because the static typing support is given by Dyalizer and Sublime doesn't know anything about Dyalizer and Dyalizer doesn't know anything about Sublime). &gt; The libraries can be discontinued, your code can be closed or become commercial at any time, a VM is more difficult (although Oracle continues to try it) It works both ways though: a library can also be improved or replaced if necessary, whereas something baked into the VM is frozen in time. [The standard library is where modules go to die](http://www.leancrew.com/all-this/2012/04/where-modules-go-to-die/). If we believe that programming generally gets better over time, then having something in a library is better on average. &gt; if we ask why use something built in the vm / language instead of a library, we can expand it and ask, why the pattern matching or the lazy evaluation is part of scala and not a library? Because these things need special syntax and special tooling support - IDEs, profilers etc. have to know about them. If we can implement something as a plain old library, written in terms of plain old functions and values, then we should. But if something needs to be a special case that will need dedicated support, then it should be part of the language.
That is syntactically horrible. Why can't we just have a “strict” kind of `AnyVal` that never boxes, and in exchange, doesn't allow overriding any superclass methods? Surely we don't have to sacrifice the cleanness of `AnyVal` syntax just for that. Also, we really don't need to add an `opaque` modifier, because Scala already has a way to express this: annotations! import scala.annotation._ class Id extends Annotation with TypeConstraint object Id { inline def apply(s: String): String @Id = s.asInstanceOf[String @Id] implicit class IdOps(val id: String @Id) extends AnyVal { def isInDatabase(implicit db: SomeDatabase): Boolean = db.contains(id) } } val a: String = "argv_minus_one" val b: String @Id = a // Doesn't type check, because String ≰: String @Id val c: String @Id = Id(a) // OK val d: String = c // OK because String &gt;: String @Id …Or, it would if [`scala.annotation.TypeConstraint`](https://www.scala-lang.org/api/current/scala/annotation/TypeConstraint.html) was actually implemented.
&gt;SIP-35 was proposed I know. That's what I'm complaining about. The proposed syntax is awful. &gt;The whole point of this is that it's a strict, mandatory thing, so an annotation doesn't seem appropriate to me. What do those things have to do with each other? Scala already has some annotations with strict, mandatory, compiler-enforced effects, like `tailrec` and `switch`.
Unfortunately no. We only provide you with the slides, exercises, and project. But I put a reference to the underscore books into the README which should help to fill the gaps.
there is a book by chris okasaki on purely functional data structures (but there is haskell as i remember). also here is a repo with some examples https://github.com/vkostyukov/scalacaster/blob/master/README.md
Look up the documentation for apt and figure out how to view the contents of the deb file. Then you can see what the package provides. I’m guessing a directory under /use/share with some .class files or some jars. You could put that in your java class path and go from there perhaps?
[removed]
You basically need to define a Tree\[Type\] as a trait and then implement two case classes that extend this trait. The case classes are Leaf(value: Type) and Node(left: Tree\[Type\], right: Tree\[Type\]). These case classes you will be using for pattern matching over your Tree data structure. Martin Odersky's book tells a lot about Trees and other ADTs
The standard library tutorial at scala-examples.org has one
Nice! I found the previous version really useful.
I never cared for the speed of running but damn are they fast to write.
Sure, thanks a lot for those!
Is this a knock on the interpreter pattern? Great job as always!
It looks like you're doing a blocking http request in the middle there and a thread.sleep on error. My guess is your CPU is pegged doing thread switching desperately looking for a thread it can do work on while it's waiting for a reply from azure. My guess would be you need to tweak threadpool settings for jetty. In the jetty world you're going to need as many threads going as you have concurrent requests. You could also always come over to the dark side of non blocking Scala!
The docs you're looking for are here: http://slick.lightbend.com/doc/3.1.0/database.html See the section on connection pools in particular.
Thank you!!! Non blocking is a top priority for my next steps in learning Scala. Can you care to elaborate what will be different if I made the call to Azure AD non blocking? I don’t quite understand how blocking/non blocking would make a difference since I need the response from Azure before I can continue other tasks.
Thank you for pointing me to the doc I've actually read that doc multiple times but still can't figure where the problem is ?
Is your connection pool size 1?
How can I get the size of my connection pool ? I added the configuration I'm using to the post 
The opaque types proposal introduces a language keyword, so tools won't be able to silently ignore it the way they could with an annotation.
When you implement `Future` correctly the thread isn't pausing to wait for a result, so the thread state isn't temporarily saved and retrieved later. It can go do something else. I don't have a deep understanding how this works exactly, but scala manages your threads and `Futures` in such a way that they don't need to be paused and overhead is kept to a minimum https://docs.scala-lang.org/overviews/core/futures.html
Jetty is widely used and unlikely to have serious obvious bugs. Troubleshooting CPU use just from the code is virtually impossible. Run the service under profiler and see where that CPU is actually going.
Really great work guys. Enjoying the perf/flexibility. Keep it up 😁
Indeed; the code you wrote will desugar to the same thing that /u/kag0 wrote, the sugaring isn't your problem here and desugaring won't help. However, the `map` call forms the full collection in-memory before the code continues any further. You could use `.iterator` as /u/nutsack_dot_com suggested (whether you stick with the `for` comprehension form or prefer the desugared form), or you could use `.view` to form a lazy collection. 
This man is pure gold
Why does yearOfRelease not being found in the oldestDirectorAtTheTime &amp;#x200B; `package Objects` `class Director2(val firstName: String,` `val lastName: String,` `val yearOfBirth: Int)` `object Director2 {` `def apply(firstName: String, lastName: String, yearOfBirth: Int) =` `new Director2(firstName, lastName, yearOfBirth)` `def older(director1: Director2, director2: Director2): Director2 =` `if (director1.yearOfBirth &lt; director2.yearOfBirth) director1 else director2` `}` `class Film2(val name: String,` `val yearOfRelease: Int,` `val imdbRating: Double,` `val director: Director2)` `object Film2 {` `def apply(name: String,` `yearOfRelease: Int,` `imdbRating: Double,` `director: Director): Film =` `new Film(name, yearOfRelease, imdbRating, director)` `def highestRating(film1: Film2, film2: Film2): Film2 =` `if (film1.imdbRating &lt; film2.imdbRating) film2 else film1` `def oldestDirectorAtTheTime(director1: Director2,` `director2: Director2): String =` `if (yearOfRelease - director1.yearOfBirth &gt; yearOfRelease - director2.yearOfBirth)` `director1.lastName` `else director2.lastName` } 
&gt; Why can't we just have a “strict” kind of AnyVal that never boxes ... This was pretty much the plan for value types in Scala: Get the semantics right, and wait until the JVM supports value types which make most of the current boxing unnecessary. All the "issues with value types" SIP-35 points out were in fact explicit decisions favoring correctness over a short-term performance advantage. The sad thing is that the documentation is so incredibly poor in Scala, that nobody even understands which parts of value types are by-design and which ones are due to limitations of the JVM anymore. Example: &gt; Value classes are classes with exactly one value attribute. _This hurts to read._ The limitation of value types to exactly one value was directly caused by JVM limitations which were too expensive to work around. This wasn't something where people said "this is great, we want to limit it to one". This was always considered to be a temporary restriction, to be lifted when the JVM added value types. The reason why people favored correctness over performance? Because Scala, time after time, decided to go the opposite way and everyone suffered from it. Pick any random talk from Paul and you'll see tons of examples where people thought "it's not correct, but it's fast" was a good idea, and every time things backfired horribly. &gt; ... and in exchange, doesn't allow overriding any superclass methods With opaque types we can see how this would work out: Call me picky, but I don't think "extra-typesafety" and returning wrong results on `equals`, `==` go well together. I can't imagine the line of reasoning where people advertise a feature as "a lightweight way to add extra type-safety to strings, numbers, etc." and then have it return `true` for `List(Meter(123)).contains(Inch(123))`. So once again, people decided to discard all the painful lessons of the past, reinvent things poorly and favor speed over correctness. The results are predictable.
There's a stack overflow waiting to happen at line 31.
Can you recommend what way of retrying I would implement to prevent that? 
It feels like there are only a couple sources of complexity here all getting handled at the same time to meet requirements. The code may turn out cleaner if you handle them separately. The requirements I see: 1. Accumulate all visited cities vs only keep the last, i.e. the `range` flag 2. Do not include the origin unless it's reached from a hub on the last jump I also think trying to make use of the single `acc` and making sure it has the right values at every step is contributing to the complexity compared to tracking a few different pieces at each step. Here's how I might approach the problem. It uses more of a breadth-first approach instead of depth-first to avoid that `flatMap` problem. Note: I wrote this in-browser and didn't attempt to compile or run it, but it should be a close approximation. def fromJumps(originName: String, maxJumps: Int, includeAll: Boolean = false): Set[City] = { val origin = graph(originName) def routesPair(c: City): (City, Set[City]) = c -&gt; routes(c) @tailrec def eval(jump: Int, cities: Set[City], reachedOriginFromHub: Boolean, allVisitedCities: Set[City]): Set[City] = { if (jump &lt; maxJumps) { var hubToOrigin = false val neighbors = cities map { c =&gt; hubToOrigin = hubToOrigin || PartialFunction.cond(c) { case Hub(_, rs) =&gt; rs contains origin } routes(c) } eval(jump + 1, neighbors, hubToOrigin, if (includeAll) { cities ++ allVisitedCities } else { Set.empty }) } else { val baseResult = if (includeAll) { cities ++ allVisitedCities } else { cities } if (reachedOriginFromHub) { baseResult + origin } else { baseResult - origin } } } eval(1, Map(routesPair(origin)), Set.empty) }
Single on a list of more than 1 item will throw, while head just returns the first item and throws if it is empty. Different behavior. 
thanks but can you be more specific what i need to change
Kafka can't have more workers listening to a topic than it has partitions, a message queue like RabbitMQ is better suited for this task.
To add to my previous comment, I'd prefer fast to write stuff over fast to run stuff. I'm using scala after all. A cursory glances tells me that you haven't changed a lot but I do hope you keep user friendliness on th front as you usually do 
`.single()`/ `.singleOrNull()` - one-item list is rare in Scala because if you intend to have a list with zero or one item you can use `Option` and if it is filtered list you can use `find` instead of `filter` 
Sometimes you’d want to make sure that only one element is found, though.
Heya! You could parameterise the methods to return the generic type of the list. Also note that some `Seq`, like the mutable ones, are invariant: You can't pass one of these to a method with a signature like `Seq[Any]`. Another gotcha is the `seq.size != 1`, which will evaluate the length of the sequence before pulling the first element. For some seqs this is costly (List) and for Stream this is a non-terminating operation. You can base all your single* methods on a base method `singleOption` which solves the problem in a Scala-idiomatic way, and then use its result to get the other desired behaviours. def singleOption[A](seq: Seq[A]): Option[A] = seq match { case Seq(single) =&gt; Some(single) case _ =&gt; None } def single[A](seq: Seq[A]): A = singleOption(seq).getOrElse(throw new Exception("Not a single element")) def singleOrNull[A](seq: Seq[A])(implicit ev: Null &lt;:&lt; A): A = singleOption(seq).orNull 
Thanks a lot for this!!
With a small amount of boilerplate you can have streams that use your own types rather than GenericRecord. ``` object RecordFormatSerdes { def recordFormatSerdes[T &lt;: Product](implicit rf: RecordFormat[T]): Serde[T] = { Serdes.serdeFrom( new Avro4sSerializer(), new Avro4sDeserializer() ) } } class Avro4sSerializer[T &lt;: Product](implicit rf: RecordFormat[T]) extends Serializer[T] { private var inner = new GenericAvroSerializer() override def serialize(topic: String, data: T): Array[Byte] = { val gr = rf.to(data) inner.serialize(topic, gr) } override def configure(configs: util.Map[String, _], isKey: Boolean): Unit = { inner.configure(configs, isKey) } override def close(): Unit = { inner.close() } } object ImplicitSerdes { implicit val myTypeRecordFormat = RecordFormat[MyType] implicit val myTypeSerdes: Serde[MyType] = recordFormatSerdes def configure(schemaRegistryUrl: String): Unit = { val config: util.Map[String, _] = Map("schema.registry.url" -&gt; schemaRegistryUrl).asJava myTypeSerdes.configure(config, false) } } ``` Note I've missed out the implementation of `Avro4sDeserializer` because it's basically identical to `Avro4sSerializer` Now if you import `ImplicitSerdes._` you can use the kafka streams API with your own types. ``` import ImplicitSerdes._ ImplicitSerdes.configure(registryUrl) val myTypeStream: KStream[_, MyType] = streams.stream("some-topic") //Assuming a configured streams instance ``` 
&amp; will update the article accordingly
Please be careful!
It’s not about passing a list. It’s about being given a list when you know you only want one unique item from the list. Head works but not if more than one item is in the list. If I want to ensure that a list has only one unique item I can filter it to that unique item and use single() to pull it out of the list and store it as it’s type in a variable. If I use find instead of filter I could be covering up the fact that there are duplicates in the list. 
Great feedback, thanks
Yes there is an existing option for a ternary operation in Scala, I just like Java’s version better, it is even more concise. 
In my experience this is so uncommon that I would forget what to import for this. And in case I needed it, I would just pattern match, e.g., ``` xs match { case List(x) =&gt; Some(x) case _ =&gt; None } ```
Yes, singleOption checks that there is exactly one item in the seq and returns either Some(thatItem) or None in all other cases
Try moving `oldestDirectorAtTheTime` as-is to the body of the `Film2` class. class Film2(...) { def oldestDirectorAtTheTime(...): String = ??? } Then you'll have to create a film and directors then call that method on the. val film = new Film2(...) val d1 = new Director2(...) val d2 = new Director2(...) val oldest = film.oldestDirectorAtTheTime(d1, d2) Another option is to leave the method where it is, but add a `Film2` parameter. def oldestDirectorAtTheTime(film: Film2, director1: Director2, director2: Director2): String = ??? Then you can use `film.yearOfRelease` in the body of the method. Right now, you don't have an instance of `Film2` to access inside `oldestDirectorAtTheTime`, but that's the only thing with a `yearOfRelease` field. Both of these options give you access to that field.
You need to declare the type parameter on the class not on the methods. It works like this: ``` implicit class SeqSingle[A](seq: Seq[A]) { def singleOption: Option[A] = seq match { case Seq(element) =&gt; Some(element) case _ =&gt; None } } ```
&gt; It’s about being given a list when you know you only want one unique item from the list. Then a list is the wrong type to use
Sometimes you don’t have a choice, it’s what you are given from and endpoint for example. 
This library reminds me of `left-pad` in the npm world. You don't need to pull in a dependency for 3-4 lines of code, that's crazy
The idea is that there could be more in the future as I play with Kotlin more. But if it’s not useful then it’s not useful.
One immediate reason could be the possibility of of collisions, in which case the Scala compiler gives up and gives you `error: value asStringWithType is not a member of scala.this.Int println(42.asStringWithType)`. &amp;#x200B; [Full example at Scalafiddle](https://scalafiddle.io/sf/V58OL9Y/7) object CollidingOps { implicit class IntTypedAsStringSyntax(a: Int) { def asStringWithType(): String = s"$a is a ${a.getClass}" } } import CollidingOps._ import InstanceAndSyntax._ 42.asStringWithType // compile error: value asStringWithType is not a member of scala.this.Int println(42.asStringWithType) I know this seems contrived, but is a possibility. I usually follow patterns similar to what you find in Cats, Scalaz, Shapeless, etc.: trait TypedAsStrin[A] { def asStringWithType(a: A): String } object TypedAsString { // Standard "Summoning" constructor. def apply[A](implicit ev: TypedAsString[A]): TypedAsString[A] = ev } trait Instances { implicit val IntTypedAsString = new TypedAsString[Int] { def asStringWithType(a: Int): String = s"Int($a)" } } trait Ops { implicit class IntTypedAsStringSyntax(a: Int) { def asStringWithType()(implicit tc: TypedAsString[Int]): String = tc.asStringWithType(a) } } Failing Example: import all._ // Analogous to your implementation. import CollidingOps._ println(42.asStringWithType) // Compile error: value asStringWithType is not a member of scala.this.Int println(42.asStringWithType) Passing Example: import instances._ import CollidingOps._ println(42.asStringWithType) println(TypedAsString[Int].asStringWithType(42)) This gives users the option to bring everything or just the things they need. Choice is a Good Thing™ :)
First of all, that syntax helper would be usually generic and `AnyVal`, but let's ignore it for a moment. Imagine, in the first case, that you have a client code that looks like this: def f[A: TypedAsString](a:A) = ... If you call it with an `i : Int`, you'll get, after implicit resolution: f(i)(Instances.IntTypedAsString) Everything is nice and dandy. No extra allocation are happening except for maybe `Int` being boxed due to being passed to a polymorphic function. Now imagine the second case. Now you can no longer call `f` as is, since there is no implicit value of type `TypedAsString[Int]` in the scope. There is an implicit class, but there's no implicit `Int` to convert into it either. So your code would have to call explicitly as `f(i)(i)`. Or even worse, as `f(i)(1)`. Besides, code like `1.asStringWithType(2)` is now also valid. Let's now assume you replaced `f` with this: def g[A](a:TypedAsString[A]) = ... You can call `g(i)` no problem, but suddenly you have unavoidable pointless boxing (`AnyVal` won't help, since `TypedAsString` is a trait). Also, you cannot access the original integer from within the `TypedAsString` trait. And finally, you cannot use `a.asStringWithType()` inside `g`, you'll have to use `a.asStringWithType(a)`. 
How do I combine these two filters to one filter ``` def directedMoreThenOneFilm( numberOfFilms: Int):Seq[String] = directors.filter(_.films.size &gt; 1 ).map(_.lastName) def bornBefore(year: Int):Seq[String] = directors.filter(_.yearOfBirth &gt; year).map(_.lastName) ``` 
Let Odersky, Spoon and Venners answer you: &gt;Technically, Scala is a blend of object-oriented and functional program- &gt; &gt;ming concepts in a statically typed language. The fusion of object-oriented &gt; &gt;and functional programming shows up in many different aspects of Scala; &gt; &gt;it is probably more pervasive than in any other widely used language. The &gt; &gt;two programming styles have complementary strengths when it comes to &gt; &gt;scalability. Scala’s functional programming constructs make it easy to build &gt; &gt;interesting things quickly from simple parts. Its object-oriented constructs &gt; &gt;make it easy to structure larger systems and to adapt them to new demands. &gt; &gt;The combination of both styles in Scala makes it possible to express new &gt; &gt;kinds of programming patterns and component abstractions. It also leads to &gt; &gt;a legible and concise programming style. And because it is so malleable, &gt; &gt;programming in Scala can be a lot of fun. *Programming in Scala*, first edition
Kotlin is, imo, largely successful because they skillfully exploited the common developer fallacy that if a technology is easy to get started with, it is therefore, the best choice. 
Personally speaking, I think characteristics of Scala and Kotlin, and the current trend of modern programming are often mixed up. Scala is always widely considered as an unique beauty with both OO and FP elements, but in the past few years, a lot of things have changed in the landscape of imperative programming. Right now, nearly all the popular imperative languages have absorbed a lot of FP features, like lambda expression, higher order function, immutability and even lazy evaluation. To some limited extends, you can do nice FP things in verbose languages like Java, complex langauges like C++ and even Go which is famous for its simplicity and lack of features. This situation definitely to a certain degree, has weakened Scala's strong position in the world of programming; however, it is still very good, and its syntax absolutely holds an advantage over let's say, that of Java (Java stream is nice though). Now back to Kotlin, I have programmed a lot in Kotlin and some in Scala. I have to admit that I really have very often deja vu feelings when jumping from one to another occasionally. But in my eyes, eventually for Kotlin and Scala, the ultimate victory is not going to be decided by just language features or designs, instead, decisive factors would be the status of the community, the pace of pushing out new language versions, the amount and influence of written code bases (since a lot of us in the field are just maintaining), and libraries and frameworks. Kotlin is just an young language with a promising future, waiting for features to be mature enough (like coroutine). On the other hand, Scala 3.0 would be a game changer. I believe a few years later, they would be more different from each other. 
It's a good quote, but it doesn't to much in answering the question.
What kind of tree? Assuming you want something like a binary tree, you have a few options to choose from. The degenerate tree is a list. // L a = 1 + a * L a sealed trait List[A] final case class ::[A](head: A, tail: List[A]) extends List [A] final case class Nil[A]() extends List[A] And you have the a binary tree, which may look like this: // T a = 1 + a * T^2 a sealed BTree[A] final case class Branch[A](node: A, left: BTree[A], right: BTree[A]) extends BTree[A] final case class Tip[A]() exrtends BTree[A] And that's about as complicated as it is to build inductive types!
\`repeat\` is &lt;Collection&gt;.fill, e.g., \`List.fill(10)(math.random)\`
What if you wanted do perform a function and not just fill a collection though? Just syntactic sugar for a for loop that does the same thing repeatedly. 
I thought that the question was about how much functional programming was meant to be in Scala
Are Kotlin developers easy to find?
I always saw Scala as a research project, focused on understanding advanced type systems and the complexities of blending OO and FP. Kotlin is more focused on practical concerns for the professional programmer. So, different aims.
How do you integrate it with Akka scheduler?
If you'd need to incorporate/start new with using apache spark, would you avoid the scala implementation and rather go to python?
That quote makes it quite clear that functional programming concepts were always meant to be a part of the language, going beyond simply being 'a better Java'.
I think it's getting much easier to find Kotlin developers due to the ability to get devs from Android, Java, or from Scala. A lot of android developers are picking up Kotlin, Kotlin is a fairly easy transition for Java developers, and it's also a simpler Scala, so Scala developers can pick it up quickly if needed. Currently most Kotlin devs are still mostly android developers as most new android projects projects are in Kotlin or slowing transitioning to it. My company is actually transitioning a fairly sizeable android team from Java to Kotlin just like they did from objective-c to Swift for their mobile apps. I do think there is lot of growth on the server side though with projects now providing support for Kotlin. IMO, its also easier to recruit developers from Java looking to make a switch, who are interested in using a modern language with some good FP features ( just not at the level of Scala).
**I think Kotlin does a better job at being a "better java" than Scala**, for a couple of main reasons: * Better two-way interop: using Java code from Scala is easy, but using Scala code from Java can be a nightmare. It's easy to use Kotlin code from Java. * Easier to learn: learning Kotlin is easy for a Java programmer, but it's a lot more difficult to learn and to teach Scala. The functional style requires that you think in a completely different way, whereas Kotlin code is not too dissimilar from Java 8 **That said, I still think Scala is a better language**. The assumption made by the question is that Scala was _just_ supposed to be a "better Java", which does Scala a disservice. The features that Scala has that neither Java nor Kotlin have give it clear advantages over both, such as implicits (and the patterns that come with them like typeclasses) and the much more powerful type system.
I've written games in both. Kotlin has better tooling, Scala is much more ergonomic and productive (for me). Kotlin is more verbose and generally clunky, and using it feels like Scala with extra steps and restrictions. Kotlin doesn't even support automatic conversions of floats to doubles, or ints to floats, which is painful when writing games.
Dotty does not guarantee any kind of compatibility between releases right now, and libraries published using Dotty cannot be used from Scala 2 currently, so using as the sole compiler for your project isn't recommended. However, if you're interested in being forward-compatible, you could cross-compile your code between Scala 2.12 and Dotty, Dotty itself is developed this way currently (https://github.com/lampepfl/dotty-example-project has more information and a link to an example that cross-compiles). Even if you don't publish any Dotty artefact, cross-compiling with Dotty means you get to take advantage of its [IDE support](http://dotty.epfl.ch/docs/usage/ide-support.html).
Think of Dotty as Scala 3 now. There won't be Scala 3 before 2.13 and 2.14 at least, so there are almost no libraries available for Dotty now and it's still work-in-progress.
&gt; there are almost no libraries available for Dotty now This is true, but you can in fact use Scala 2.12 libraries from Dotty as long as you're not calling into a Scala 2.12 macro, see https://github.com/lampepfl/dotty-example-project#getting-your-project-to-compile-with-dotty
&gt; I also have a mobile product launching soon that is written in Kotlin (both client and server) Well, you could have easily enough written that in Scala/Scala.js, which is an absolutely *amazing* combo. I'm stuck in Scala + Angular/TypeScript on a side project and would love to go pure Scala. Unless KotlinJS has improved by leaps and bounds generated JS is still bloated compared to Scala.js (even with the Scala collections "tax"), and generally lags behind Scala.js in terms of maturity, functionality, and performance. If you're talking about writing native Android/iOS apps, Android's a given of course, but who's actually writing production iOS apps (that, you know, matter) with Kotlin?
In which release will Dotty start to provide compatibility between versions? It would be great to skip 2.13/2.14 and start using Dotty as soon as possible.
These vulnerabilities are fixed, are they not? I think my point stands: using a popular foundation library is safer than pure Scala one. Perfect is the enemy of good: you lose momentum worrying about model that seems "eccentric" or design that seems "broken", instead of focusing on things that matter - usability, performance, ergonomics (and by that I mean good error messages, actually good and helpful context information when things go wrong). I think Scala community underestimates how much the library fragmentation really is costing. Anyway, I am very optimistic for Scala 3, because it will bring to the table again features that no other language brings. That's what was Scala value proposition around 2.9 series, that's what it will be again.
That hasn't been decided yet, there's also multiple kind of compatibility: {forward, backward} {source, binary, TASTY} compatibility. There's a lot that needs to be done before we can commit to any of those and I don't think we'll provide any strong guarantee of compatibility before 3.0 is out, because it would make it much harder to evolve the compiler. What kind of compatibility matters to you most ?
Source compatibility isn't very important because I can change my own source when upgrading to newer releases. Binary compatibility is more important so I can keep using libraries released for 3.x when I upgrade to a version greater than x.
Those are the main ones; I use all of these in Scala and there have been times that I've been trying to do something in Kotlin or Java and have been annoyed that these features were missing, so had to do something in a way that was potentially unsafe. One other thing that comes to mind is existential types, but I don't use them. These also combine with implicits and their patterns in interesting ways via implicit resolution, eg using higher-kinded types with a JSON formatter typeclass to make a formatter for a collection type when CanBuildFrom exists.
Honestly, if you've to ask you probably know little about Scala nor have you made any effort to resrarch. Might as well not use either. 
This post itself is a research. 
Good shout. Will try and have a crack at that next week. 
Scala 2.14 is planned to be almost fully dedicated to Scala 3 compatibility and migration, so stick with 2.x for now.
Compared to Circe, I have had many issues using Jackson. Its saying that the difference comes down to perfection is quite reductionist, Circe's design fits much better into Scala's where you want to seperate data from serializers/deserializer (or \`Encoder\`s\`/\`Decoder\`s)
I guess that it depends on the company. If your company does a lot of Big Data, you will have a lot of Scala developers and other people who wants to learn Scala to use Apache Spark.
Pulsar is kafka like but allows out of order acks so is perfect for a workpile. Imo it combines the best of kafka and jms.
Well, nextime itself simply deals with cron expressions, so in that sense, if you wanted to use it with Akka's scheduler, you would need some interface to convert cron expressions to something that Akka's scheduler understands. Alternatively, you could use the Akka Quartz extension (https://github.com/enragedginger/akka-quartz-scheduler) which supports cron expressions. I don't have much experience with this library, but this is a quick example I whipped up of what it might look like to use the 2 together: val system = ActorSystem.apply("MyActorSystem") val date: Either[nextime.Error, Date] = Cron("0 0 3 11 4 ? *").map { cron =&gt; QuartzSchedulerExtension(system).createSchedule(name = "MySchedule", cronExpression = cron.mkString) QuartzSchedulerExtension(system).schedule("MySchedule", Actor.noSender, Actor.noSender) }
I will test it today! 
Removing the reliance on Twitter's Future moves it in the same direction as HTTP4s, which is both good and bad in that while the api becomes more flexible, the signatures also become more complex. Maybe this could be made a smoother transition by including some pre-packaged aliases for commonly used effect libraries. For Twitter Futures, the package could look something like package io.finch package object twitterfutures { type Endpoint[A] = io.finch.Endpoint[com.twitter.util.Future, A] … more aliases } 
This is FUD. Scala is and has always been a practical programming language (there was a previous research project that some parts of Scala were derived from).
At a low level, Scala wasn't intended for the extensively FP style that's the most mainstream way of using it these days (and many of Odersky's comments here make it clear that he doesn't favour that style). It was intended for a closer-to-Java-6 style and had various features that were oriented towards that (e.g. XML literals). So some aspects of Kotlin are closer to the original intentions of 2003-Scala than today's Scala is, sure. At a higher level Scala was intended as an effective general-purpose programming language (with good Java interop), and I'd argue it remains much better at that than Kotlin.
&gt; there will never be a pure Scala JSON library that is better than Jackson *triggered* Jackson is a horrible, horrible, unsafe, and awkward to use JSON library littered with a minefield of inconsistent behaviour and runtime exceptions that show up when you least expect it. It's incredibly inflexible and difficult to work with unless all you're doing is serialising the most trivial POJO. Circe is lightyears ahead of Jackson. They aren't even in the same ballpark. &gt; same database ORM Doobie, Slick, Quill - are all very good pure Scala "ORMs" that leave their Java brethren in the dust. Java doesn't have anything remotely competitive. As with the above, the Scala libraries aren't just better. They are in a completely different league. &gt; Irrationally Scala developers try to rewrite them, because for Scala it's not "idiomatic" to drop down to Java. As a Scala developer I have no problem dropping down to use Java libraries. I do so all the time. But the pure-scala libraries tend to be so much *better* than the native Java libraries that I prefer to use them where possible. &gt; Scala code is full of closures and pseudo-classes and crazy tricks that optimizers do not know how to untangle I'm not sure why you think that's the case. The JVM handles these just fine. &gt; That's why you hear advice about running your Scala code on GraalVM, etc. GraalVM is faster for both Java and Scala. It will also become the default JIT at some point in the future. &gt; Another beef, if only Scala had stayed with Maven or Gradle, and just invested a lot into making proper compiler plugin. If only, if only. Scala works perfectly fine with both Gradle and Maven. There are mature and up to date Scala plugins for both. &gt; Only they need to make the typeclasses concepts into full syntax. Currently the mechanics of typeclasses remind me gymnastics that people needed to do in C++ with pure abstract virtual classes, before Java knuckled down and introduced "interface" keyword. This I agree with.
&gt; Scala and Gradle do not work well together in IntelliJ, unfortunately. The IDE often reports errors that the compiler doesn't consider errors. That happens regardless of your build tool. IntelliJ uses its own presentation compiler, and it's occasionally insistent compared to scalac. I don't think SBT vs Gradle would make any difference here. I've worked on large Scala projects with both Gradle and SBT as the build tool - I haven't noticed any differences in behaviour in IntelliJ.
fs2 streams are their own sequence-like type; I'd say with fs2 you generally work in the context of a particular `F[_]` effect. The fs2 operators are generic enough that you could use them with a sequence-like type for `F` but I doubt that's a good idea: what value would you be getting from fs2's operations? The general way you use fs2 is with `F` being a `Task`/`Future`-like type representing async effects, and then an fs2 stream is, roughly, a sequence of elements with `F`-effects. Whereas in Spark's model the `RDD` abstraction covers both the fact that it's a sequence and the fact that there are distribution effects going on, and so there's no way to unpick those two aspects and fit them into FS2's model. You could form a stream of complete RDDs but that's unlikely to be what you want.
I wouldn't go so far to say that Jackson is generally buggy (as far as Java libraries go, its quite high quality). Its just that, as with many other things, Java's type system isn't as strong as Scala's so a lot of "verification" gets pushed to runtime.
Could you write an effect instance of Twitter future that adheres to the effect/async laws? I don't think you can, thus this alias is not possible. 
\- Why doesn't `lazy` cache and rethrow same exception, but reevaluates? What is the rationale? \- Why are *call by name* and *call by value* visually indistinguishable for the caller? Was it laziness of computation the primary goal or is it just a syntactic sugar over `() =&gt; SomeType` ?
&gt; - Why doesn't lazy cache and rethrow same exception, but reevaluates? What is the rationale? I don't know if it's the original rationale, but IMO exceptions should be for system failures where retrying is usually the desired behaviour. I can imagine scenarios where an exception might happen (e.g. `FileNotFound` when your application is deployed to NFS and should load some assets from there, network failures when loading an XML DTD, `OutOfMemoryError`) and you'd want to retry the next time the value gets accessed, and I struggle to imagine scenarios where you'd want to cache the exception; if you want a `lazy` that can "fail" in a way that's semantically meaningful and should be cached, have the value be an `Either` or similar. &gt; - Why are call by name and call by value visually indistinguishable for the caller? Note that both main Scala IDEs will highlight lazy calls (similar to highlighting implicit conversions), so it's not actually visually indistinguishable when working on the code. The original goal for that feature was to make it easy to write control-flow-like constructs in DSLs, e.g. mySpecialIf(someConditionExpression) { ... } &gt; Was it laziness of computation the primary goal or is it just a syntactic sugar over () =&gt; SomeType ? I don't understand the question - what distinction are you drawing?
In your example with specialIf I assume you wanted to defer computation (lazy evaluation) in the first place, not to run it multiple times. 
Well sure, but equally you could have specialWhile or some kind of specialFor.
&gt; Let's face it, there will never be a pure Scala JSON library that is better than Jackson. There already are several. I've lost too much time because Jackson fails at runtime rather than compile time, which means either maintaining unit tests, dealing with production issues, or, realistically, both. &gt; Irrationally Scala developers try to rewrite them, because for Scala it's not "idiomatic" to drop down to Java. It's not idiomatic to use a mutation-oriented library in FP-style code. This is not a "dropping down" problem - I encountered exactly the same problem in Python where some Python libraries are written in a style that you just can't use idiomatically from functional-style Python. And hobbyist developer effort is not fungible. I very much doubt the people writing FP-idiomatic libraries for these things would be e.g. improving the IDE if they weren't; more likely they just wouldn't be contributing to the Scala ecosystem at all. &gt; Java bytecode is lean and mean and simple and plays well with JIT. Scala code is full of closures and pseudo-classes and crazy tricks that optimizers do not know how to untangle. If my codebase's performance ever drops below 3 orders of magnitude above the requirement, maybe I'll care. People worry far, far too much about performance. There are mainstream, billions-of-dollars languages that can easily be two orders of magnitude slower than Scala (e.g. Python, Ruby, R). Correctness is a much bigger issue. &gt; Another beef, if only Scala had stayed with Maven or Gradle, and just invested a lot into making proper compiler plugin. If only, if only. Yeah I have no idea why people recommend SBT or what they get out of it, and I wish they'd stop recommending it to newbies. But Scala in Maven actually works fine if you just ignore the jeers and get on with your life. &gt; I agree that future of Scala is in very powerful FP and type mechanics. Only they need to make the typeclasses concepts into full syntax. Currently the mechanics of typeclasses remind me gymnastics that people needed to do in C++ with pure abstract virtual classes, before Java knuckled down and introduced "interface" keyword. Maybe. One of the real advantages of Scala over other ML-family languages is that typeclasses live in the ordinary first-class code universe and any ordinary functions/types/tools/... just work the way you'd expect them to. I would be very cautious about giving that up.
Exercises of the scala-with-cats before Im going to dive into the red book by Manning
That's the impression I was getting...plus, wouldn't I have to collect the RDD first (in which case its an array)? Thank you very much for taking the time to respond. 
What's the benefit of "lazy" being built into the language vs. just a type like: [http://www.vavr.io/vavr-docs/#\_lazy](http://www.vavr.io/vavr-docs/#_lazy) &amp;#x200B;
Slick 2 had a Session object that was passed implicitly in local code blocks. The Slick 3 way is completely different. You create a Database object and hold onto it yourself however you like. Then you use it to run composed DBIOs. 
are you still looking for a technical writer?
I am! Send me a DM and we can start chatting
Does Circe have this problem?
One of these issues is rooted in a `BigInteger` and `BigDecimal` so pretty much everything that uses those can be affected, not just Json libraries. It could be JDBC drivers, xml libraries, pasrer libs, anything really.
Sure. Use [Twirl](https://github.com/playframework/twirl).
Honestly I think that for a simple usage like you describe, string interpolation is the best. Use three double quotes so you don't have to escape anything, you can even have multiline. And stripMargin if needed. Just write a function for each template, return the string 
Moreover, even after successful parsing of `BigDecimal` value like `1e1000000000` or `1e-1000000000` users can be affected by any subsequent operations like `+`, `%`, `longValue`, etc. Just try how this code works in your Scala REPL: ``` scala&gt; val f = (x: BigDecimal) =&gt; x + 1 f: BigDecimal =&gt; scala.math.BigDecimal = $$Lambda$1210/93981118@790ac3e0 scala&gt; f(BigDecimal("1e1000000000")) ``` or ``` scala&gt; val g = (x: BigDecimal) =&gt; 1 + x g: BigDecimal =&gt; scala.math.BigDecimal = $$Lambda$1091/1954133542@e8ea697 scala&gt; g(BigDecimal("1e-1000000000", java.math.MathContext.UNLIMITED)) ``` To prevent this, parsers should avoid returning of `BigDecimal` with too big exponent or with `MathContext.UNLIMITED`. 
Yes, Circe is affected too until sources of vulnerabilities from the BEWARE section will be fixed or at least some runtime checks will be introduced to limit them. Scala team is going to provide safe hash map/set for 2.13.x: https://github.com/scala/bug/issues/11203 But not sure that all of them will be ported to earlier versions.
&gt;(i.e. even for simple stuff like &gt; &gt;1 to 5 &gt; &gt;, see the recent article by lihaoyi Fastparse2 where he ended up using static string constants with macros instead of using ranges for performance reasons; Did he use string literals because he didn't want to have closures/objects created? I don't think this point supports your claim, as he had other reasons to do so.
One line vs many lines. Lower verbosity.
You can use Twirl ([https://github.com/playframework/twirl](https://github.com/playframework/twirl)), the template engine from Play Framework. It will generate functions from templates and you can pass the variables to those functions to get the rendered result.
Each reported bug has a benchmark to reproduce the case. Just add printing of `jsonString` value to the system output before running of benches to see an exact input.
The original source is quite visible on my link too, but thanks for being super helpful!
Using string templating to construct SQL is a bad idea. Use a proper database library. [Slick](http://slick.lightbend.com/), for example, has '[plain SQL](http://slick.lightbend.com/doc/3.2.3/introduction.html#plain-sql-support)' support, which I think is similar to what you're looking for. Also, I guess you could use [JDBC prepared statements](https://docs.oracle.com/javase/tutorial/jdbc/basics/prepared.html), but I'd really recommend using a library that's built on top of JDBC, not JDBC directly.
String interpolation for SQL is a terrible idea - at best you'll have problems with escaping things like string literals, and at worst you're open to all sorts of injection attacks.
That's not any different than using a external template engine like Scalate as OP is saying. That's acceptable if you don't have any user input in your SQL string. But basically I agree that using something like Slick's plain SQL support, with interpolation-like syntax but escaped, is better. You have to be careful not to abuse #$ however because you end up having the same issues.
Well, for my particular use case an ORM is not applicable or desired. The SQL templates help facilitate highly-dynamic SQL building. It’s works well actually. The previous Ruby implementation was running against Netezza, whereas this new Scala version is Spark (SQL).
I would recomend Anorm https://github.com/playframework/anorm https://playframework.github.io/anorm/ Its almost string interpolation but without being string interpolation. The value will be properly escaped.
You are aware how string interpolation works? You don't have to interpolate into a new String and can instead interpolate into a prepared statement?
Tray.io | Senior Scala developer | London, UK | ONSITE | Full Time | # Description Tray.io is ushering in the era of the automated organisation We believe that any organisation can and should automate. With Tray.io, citizen automators throughout organisations can easily automate complex processes through a powerful, flexible platform, and can connect their entire cloud stack thanks to APIs. Today businesses like IBM, GitHub, Forbes, Lyft, and Digital Ocean rely on Tray.io to connect and automate data flow between the tools they use every day. With Tray.io visual workflow builder our customers create automations to drive their business processes without writing a single line of code. Our challenge is to build a cutting-edge product that is powerful and complete while also being beautiful and easy to use. You'll contribute directly to this mission with a team that fully supports you to do your best work. You'll join humble but fiercely ambitious people like yourself, who also take great pride in what they do, working in a culture built on friendship, transparency, and above all, looking out for one another. You'll have endless opportunities to learn and grow professionally in a fun, fast-paced, and open environment. Plus, you'll get to make your mark at a rapidly-growing company positioned to completely reinvent a multibillion-dollar industry. # Your mission Tray.io backend infrastructure processes millions of requests per day and is a mission-critical component of our customers' businesses. As a Software Engineer working in the Platform team at Tray.io, you’ll be part of a team responsible for designing, building and running the software and systems which underpin our large-scale, real-time, distributed infrastructure. We expect you to build flexible services and tooling which allows Tray.io to rapidly scale whilst delivering a seamless experience to our customers. The Platform team is responsible for providing a production environment where connectors integrations and automations can run reliably and at scale. This involves dealing with compute providers, networking, packaging, storage, monitoring, logging, and security. This necessitates building the services and APIs that expose these services to internal and external users of our infrastructure. # Responsibilities: * Developing flexible services &amp; tooling that allows people to store and run integrations at scale * Building internal and external facing tooling and API’s that allow our users to fluidly build on our platform * Working collaboratively with Product managers to deliver key security, scalability and reliability features * Growing a motivated, high-performing team * Automate the testing and deployment of your work * Envision new features that help our users connect services faster and easier ### Minimum qualifications: * BS degree in Computer Science or related technical field, or equivalent practical experience. * Solid experience in a stand-out production environment * Expert knowledge of at least two programming languages and paradigms (e.g. Scala, Java, Kotlin, Groovy, Go) * An irrational passion for building distributed systems * The desire to learn, improve and work together * Good knowledge of internet networking and performance * Passionate about troubleshooting, debugging, and automation * Experience building web services and APIs ### Preferred qualifications: * Experience launching cloud-based services * Experience with real-time, distributed systems * Experience with writing multi-threaded software * Passion for performance and tuning * Experience in capacity planning and load testing * Experience with clustering technologies (e.g. Kubernetes, CoreOS, Mesos) * Previous IaaS or PaaS experience ### Tech Stack * Scala, Go, JavaScript, TypeScript * PostgreSQL, Redis, ElasticSearch, Cassandra, AWS SQS, AWS Kinesis * Docker, Terraform, AWS Lambda, Serverless Framework * Jenkins, Grafana, Prometheus * AWS &amp; Linux # Benefits Working at Tray.io offers many perks, but most importantly we are a talented team with a passion for the product we are building. * Competitive salary * Stock options * Unrestricted holiday policy &amp; work from home days * Flexible working hours * A fun and supportive working environment * Top of the range equipment budget * Drinks fridge &amp; stocked kitchen * Social events (team breakfasts/lunches, evenings out &amp; trips) * Employer contributory pension scheme * Cycle to work scheme * Private healthcare * 50% off Virgin Active gym membership # To apply: Send us your resume here [https://tray-io.workable.com/j/50E49D5631](https://tray-io.workable.com/j/50E49D5631) or shoot me a PM # Equal Opportunity Tray.io is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, colour, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
Here, you're only preventing the use of `new Test(3)` which will result in an error. When you call `Test(3)` you're using the `apply` method that is automatically created in the companion object. If you want this to also result in an error, you need to either 1) make your case class abstract: abstract case class Test private (i: Int) which will prevent the `apply` method from getting created or 2) override the `apply` method yourself and mark it as `private`: object Test { private def apply(i: Int): Test = new Test(i) }
On the JVM, no type erasure and one less pointer to chase.
&gt; Well, you could have easily enough written that in Scala/Scala.js, which is an absolutely amazing combo. I'm stuck in Scala + Angular/TypeScript on a side project and would love to go pure Scala. Hi, do you know why they choose Angular instead of React? Did they have previous experience with Angular? Because Angular as a framework has many desired features out of the box? Also, what are the things you dislike the most about Angular? I've read in another comment already that it has much boilerplate. Finally, when you've been working with scala + scala.js, did you do your own thing, or have you used a library like scalajs-react? What are your pain points with scala.js (other than compile time ;p) 
It seems the purpose is to group an instance of a survey type with an instance of a question supported by that survey type while maintaining a separation of data and representation. The survey type defines how that particular survey is represented in a few different ways: in memory (via the `Question` and `Answer` type members), in json (via the question/answer encode/decode methods), and when interacting with a user (via the `askUser` method). The representation is not necessarily tied to any particular data, so a separate trait is used to bring the representation together with some data. Because the representation is separated, it allows the instances of `SurveyType` to be `object`s rather than necessarily creating mostly-identical instances just so they can hold different data. You could achieve something similar with a class like `AndQuestion(st: SurveyType)(q: surveyType.Question)` but I don't know if that's really any simpler
Well, you asked why this was happening which you seem to have already understood. As to whether or not this should be the expected behavior of using `private` is another question.
Having private constructors with public `apply` methods is actually relatively common, and often used to ensure invariants in a datatype before construction.
Yup, couldn't have put this better myself - it's a very nice pattern for doing input validation.
Very interesting. The road to Shapeless with ***no macros at all...***
No, just no. If you're going to use spark, use scala.
Hi! What is the technology stack you guys work with?
If you still need someone late next year I might be available. I've been working with Play, Akka Typed, and Akka Streams mostly recently, though I'm starting to dip into cats as well.
Scala, Play2, HTML, some Javascript, Git, Microservices deployed on AWS with Docker, Jenkins. Some teams use Catz, Scalaz I think. It's a very large organisation with over 50 scrum teams over the uk, I think they are setting up even more scrum teams in yorkshire. Great dev community, amazing place to work in an amazing location walking distance from the train station. &amp;#x200B;
thanks, I'll keep it in mind, probably will still be recruiting then as well
I have a quick question about how to select from a dataframe. If I have a dataframe with two columns: ColA = 1, 2, 3, 4, 5 ColB = 4, 6, 1, 1, 2 How do I select the value from ColA that had the lowest value in ColB and the lowest index? The selection should be 3. &amp;#x200B; Thanks
I'm going to guess Shipley if it's where I think it is. Although I'll let OP answer :)
I have been playing around with Scala, spark, jupyter etc...but will be available only in April next year...am an Australian citizen so getting a work permit should not be an issue. If you are still looking for passionate developers at that time...would like to throw my hat in the ring!
I don't know if a dataframe has a specialized API but assuming ColB is iterable and ColA can be indexed it's not terribly hard to write an extension method. implicit class IterableOps[T, I[T] &lt;: Iterable[T]](val i: I[T]) extends AnyVal { def minIndex(implicit ord: Ordering[T]): Option[Int] = { val len = i.size if (len &gt; 1) { val last = len - 1 val it = i.iterator var idx, min = 0 var cur = it.next() while (idx &lt; last) { idx += 1 val next = it.next() if (ord.compare(cur, next) &gt; 0) { min = idx } cur = next } Some(min) } else if (len == 1) { Some(0) } else { None } } } // elsewhere colB.minIndex map colA // returns Some(3) in your example &amp;#x200B;
Here is the link to the second job description (on the Scala Center side): https://recruiting.epfl.ch/Vacancies/701/Description/2
Maybe I should've asked that instead.
That makes sense. And in practice making the apply private wouldn't exclude that approach as you would need to define the apply by hand in order to make the necessary validations, thus preventing the default apply from being automatically generated.
I don't think dataframes are ordered (they're more designed for SQL-like operations), so I think you want to work with `RDD`s here. I'm going to write an intermediate `case class` which isn't completely necessary but makes things clearer: case class Entry(colA: Int, colB: Int, index: Long) object Entry { implicit object OrderingEntry extends Ordering[Entry] { override def compare(x: Entry, y: Entry) = (x.colB - y.colB) match { case 0 =&gt; x.index - y.index case r =&gt; r } } } Then we just need to zip the datasets together with each other and with an index: val combined: RDD[Entry] = colA.as[Int].rdd.zip(colB.as[Int].rdd).zipWithIndex.map {case ((a, b), i) =&gt; Entry(a, b, i)} And take the minimum, using the ordering on `Entry` that we defined val minimumA = combined.min().colA
Thank you!! :)
Thank you! I didn't even know about the zip option!
Can you explain your approach for modeling some not trivial domain in terms of Free/TF and GADTs? For example how you decouple business things to different algebras if some elements have interconnections.
you could use a 1d array and get an element at (row i, col j) with arr(9 * i + j)
A long time ago, when I wrote a sudoku puzzle solver in Scala, I had the input be a file with the puzzle in it: 9 lines with 9 columns. The content looked like this: __1__9_62 98_253__4 ____8__9_ 3__1_6__5 _9__3____ ____4__76 6__527_48 54_9__2__ I chose to use '\_' to represent empty squares. I read it all as one big string split it into an array and filtered out \\s and \\n. After that I just converted it to my internal representation of the puzzle.
really kind post!
Super cool! Very excited to see where Mill goes next.
What's a cloud intelligence? 
You can reasonably close with boring old Java: public interface Tree&lt;T&gt; { class Empty&lt;T&gt; implements Tree&lt;T&gt; { } class Node&lt;T&gt; implements Tree&lt;Node&gt; { public final Tree&lt;T&gt; left; public final Tree&lt;T&gt; right; public Node(Tree&lt;T&gt; left, Tree&lt;T&gt; right) { this.left = left; this.right = right; } } } but it starts to get relatively verbose once you add toString, equals, hashCode and traversal methods. I typically use something like the above in conjunction with [Immutables](https://immutables.github.io/). Although I haven't tried it, [Derive4j](https://github.com/derive4j/derive4j) would almost certainly be even better.
With `enum` in Dotty, aka. Scala 3, you can do: enum Tree[+A] { case Empty case Node(value: A, left: Tree[A], right: Tree[A]) } https://dotty.epfl.ch/docs/reference/enums/adts.html
🙌 The only way I could be more excited for Scala 3 is if I could use it in the workplace as well.
That's a good question @PrimozDelux. What I mean by that is the ability to embed Microsoft's Cognitive Services inside Spark computations. These services are generally referred to as "cloud intelligence" because Microsoft hosts the deep learning models in the cloud and users can leverage these intelligent models as web based function calls from within their clusters or applications.
FWIW, if interested, SML ADT: datatype 'a Tree = Empty | Node of 'a * 'a Tree * 'a Tree F# and OCaml are reasonably similar.
The problem is that you can only use this data using a visitor or a large if-else tree in Java, both of which are meh. To really make ADTs useful you need pattern matching, which Java does not (yet) have.
How does Azure Spark environment compare with Googles Dataproc and accompanied tools?
Thanks for reaching out. While I have not personally used Google Dataproc, perhaps a quick description of our tech stack might help! Spark is an open source distributed computing framework, that combines Google's Map Reduce, and SQL style parallelism (among other things). Spark can be run on all of the major clouds and so can the technology we are referencing in this post, MMLSpark. MMLSpark is a library that you can install on your spark cluster to dramatically expand the functionality of Spark. MMLSpark can be used on any spark cluster, independent of cloud. To create spark clusters, we at microsoft generally use Azure Databricks, a tool that makes it easy to manage these clusters and connect them to data sources around Azure and AWS. Here's some links to help: MMLSpark: www.aka.ms/spark Azure Databricks: https://azure.microsoft.com/en-us/services/databricks/ Apache Spark: https://spark.apache.org/ 
I've practically got an advent calendar on my wall counting down the days.
Sweet stuff. This is going to come in handy with one of my currently clients that we are building out a full Azure Spark / Scala Environment.
+1 for Azure Databricks. If you are running anything batch style it is absolutely wonderful and quick to get up and running and schedule tasks. 
Protip, Intellij offers the use of SBT for compiling/building. Highly recommend this for your projects as it will be far more consistent with general scalac.
I generally use the Object Algebra approach, but I agree it's not an ideal solution. [JEP 305](http://openjdk.java.net/jeps/305) should fix that.
This sounds like HMRC Shipley to me...
Offtopic but: Why SOAP instead of REST? REST maps much better with the functional nature of Scala.
It's client's requirement. Can't change that. It has to be done this way.
I suspect you could hand write WSDL file and basically use the other XML of Scala\Play Framework for the rest. I’m not aware of any automated tools.
I don't know about Play, but I've found http://scalaxb.org/ to be very useful when working with SOAP.
Both. See https://blog.jetbrains.com/scala/2017/03/23/scala-plugin-for-intellij-idea-2017-1-cleaner-ui-sbt-shell-repl-worksheet-akka-support-and-more/
&gt; For example how you decouple business things to different algebras if some elements have interconnections. The short answer is "don't do that". The longer answer is to do the kind of breaking up into distinct entities that you'd do when e.g. using an append-only eventually consistent datastore. E.g. rather than a user having a list of groups they're part of or a group having a list of users they're part of, maybe you need a separate "membership" that relates a user to a group. Rather than having a single "membership" entity, maybe you need "added to group" and "removed from group" events/commands. That's modelling from the events/input side. The other side is looking at the queries/output side. IME this tends to drift naturally to a CQRS style where you have a different representation output (which might well end up much more "coupled") than for input (which tends to be more of a "command" representation). This is (rightly) seen as a high-cost technique in OO-land, but it makes a lot of sense in an FP world where transforming data between one representation and another is much easier/better than any other kind of code and you want to do as much logic as possible in data transformations and as little as possible in commands/actions/effects/....
It's more than that. It uses SBT to build the idea setup for syntax highlighting. Give it a shot!
I wrote a thin layer that converted the SOAP to rest and put it on top of play. There are many good libs that will generate classes from a wsdl (I used CXF), you can then call your rest service from the thin layer, passing the xml payload which is now an object. Almost all the code including the payload classes were generated see part of the build process
Hi, &amp;#x200B; I'm trying to use Http4s with Monix. Does anyone know if this is a combination that works well ? I read some posts earlier that said you could but if someone has any examples or pointers they could share that would help me a lot. &amp;#x200B; Thanks!
Not sure what you mean by “mall kiosk demo” but we have included a few demo notebooks for Databricks in the MMLSpark install instructions at our website www.aka.ms/spark. Also if you would like any advice or help setting your client up, don’t hesitate to reach out to us at MMLSpark-support@micrososft.com
It is! It makes the life of a spark library developer 100x easier! 
Kotlin Example sealed class Tree { object Empty : Tree() { override fun toString()= "Empty" } class Node&lt;V&gt;(val value: V, val left: Tree, val right: Tree) : Tree() { override fun toString() = "Node $value $left $right" } } val tree = Tree.Node(42, Tree.Node(0, Tree.Empty, Tree.Empty), Tree.Empty) println(tree) Elm Example type Tree v = Empty | Node v (Tree v) (Tree v) tree : Tree tree = Node 42 (Node 0 Empty Empty) Empty Debug.log "tree" tree
Thank you! It's really helpful answer. 
 Thank you all for the replies. My problem is that I have big data and I sort them and I will process them like the r-tree and another way in order to reach maxim time efficiency 
This does not really address algebraic data types. ADT's are characterized by sum types (like the examples here) and product types, which are like tuples or records, and not explored in this article. The bulk of the power of ADT's is in how one decomposes them via pattern matching, which is also not explored here. In my opinion, the example of a tree is poorly chosen because the posting looks more like a 'data-structures-in-4-languages' piece rather than a 'functional-paradigm-in-4-languages-piece'.
&gt; That's a good question @PrimozDelux. You sound like an employed developer advocate.
you can use monads with for comprehension.. you just can't compose different kinds of monads, but that's something which is also not possible in theory
I guess the question is how come there's no Monad typeclass in standard Scala.
If you have Docker installed, you can try out a demo container with all the Jupyter notebooks using: ``` docker run -it -p 8888:8888 -e ACCEPT_EULA=yes microsoft/mmlspark ``` and navigating to http://localhost:8888 (Disclaimer: I work for Microsoft on the project)
Im an MMLSpark developer and I believe in being kind to those that are interested in our work! 
That is cats
Not happening. Language needs to get simpler not more complex and adding big-ass library wouldn't help achieving that. Besides, quite o lot of people doesn't want to go cats/scalaz way and the whole "Thanos approach" is unrealistic bullshit.
&gt; big ass-library *** ^(Bleep-bloop, I'm a bot. This comment was inspired by )^[xkcd#37](https://xkcd.com/37)
You realistically would want to have a set of related typeclasses, not only Monad. Otherwise users would define Monad instances for things that aren't valid, just to get the sweet syntactic sugar. So where's the problem? Some typeclasses are contravariant, and implicit resolution has been broken for contravariant types since approximately forever. See [this discussion from 2009](https://www.scala-lang.org/old/node/3786) about making `Ordering` contravariant. [A fix was developed in 2012](https://groups.google.com/forum/#!topic/scala-language/ZE83TvSWpT4), but was rejected until Dotty devs experienced the pain themselves when they added the `Eq` typeclass in 2018. So the previously impossible thing happened, and the implicit resolution was unceremoniously fixed. So the short answer is: things weren't added in the past because the language/compiler was far too buggy until very recently, and adding typeclasses in e. g. 2012, without knowing how future fixes might impact the resolution would have been a bad idea.
Finished up the final touches on [supporting 32-bit platforms](https://github.com/scala-native/scala-native/pull/1363) in Scala Native, started work on supporting [React 16.6 features](https://github.com/shadaj/slinky/issues/196) in Slinky, fixed up CPU instruction timing in my work-in-progress Scala NES emulator (mainly just to procrastinate on writing an essay for a class).
What precisely do you mean when you say you can't compose monads? In Haskell there are transformer stacks, which is not "composition" as I interpret the word (in a mathematical sense), but still compositional in another sense. 
[removed]
Good bot. 
good human
I'd not expect it before 2020 as mentioned in multiple sources But on general, when it's ready
I don't think it's just that. As u/raghar mentioned, langauges (and especially Scala) need to strive to be simpler, not more complex. The day Scala took the decision of going down the OOP+FP road, category theory typeclasses were doomed. I don't want to impose monads and the rest of the gang on OOP folks just as much as I don't want them imposing their factory presenter bridge adapter shit on me. :) Sure, one might argue that adding a typeclass should't bother those who don't want to use it (they can simply ignore it), but still it greatly increases the complexity of the standard library. I'm fine with scalaz/cats (I'd be happier if we all agreed to just one of them, but that's a different story).
"thanos approach"?
In Scala you don't need TypeClasses the way Haskell does. In Haskell you just can't define methods on data types you define. So you need typeclasses to have adhoc polymorphism. But in Scala and the rest of the OOP word, you just can define flatMap methods in your classes. You can even use for-comprehension without noticing it is a monad-fueled feature. Having the typeclass hierarchy is only usefull when you want to abstract or generalize your code. Scala could put all type classes definitions and instances in a sub package of the standard library like Play and other do. Only those interested in it would have to bother with it.
John A de Goes has a talk in which he proposed that we should *sacrifice half the community for the good of Scala*. Of course he meant non-FP part. Personally, I doubt hardcore FP programmers make even 50% of Scala community.
For comprehensions are more flexible than just monads. And Cats and scalaz includes much more than monads. Why don't you want to use cats or scalaz? Is it because adding it to a project is an extra step? If so perhaps a better question is, could adding cats/scalaz (or starting a new project that uses it) be made easier? 
Though visitor pattern is equivalent, and Derive4j generate a nice [fluent pattern matching syntax](https://github.com/derive4j/derive4j#pattern-matching-syntaxes) for it (that does compile time exhaustive check). Also, to be honest, JEP 305 will not change much for me until Java also does tail call optimization (TCO), cause until then I still need to write ugly while loops instead of proper recursive methods.
Of course it is better with Derive4J ;-) @org.derive4j.Data interface Tree&lt;A&gt; { interface Cases&lt;A, R&gt; { R Empty(); R Node(Tree&lt;A&gt; left, A value, Tree&lt;A&gt; right); } &lt;R&gt; R match(Cases&lt;A, R&gt; cases); } And if you change the definition slightly so that the visitor is an object algebra interface, Derive4J will also generate a catamorphism method that can be used to fold a tree using an object algebra: @org.derive4j.Data interface Tree&lt;A&gt; { interface Cases&lt;A, T, R&gt; { R Empty(); R Node(T left, A value, T right); } &lt;R&gt; R match(Cases&lt;A, Tree&lt;A&gt;, R&gt; cases); } (this is nice because [object algebras solve the expression problem](https://www.cs.utexas.edu/~wcook/Drafts/2012/ecoop2012.pdf)
The historical answer is probably that today's encoding of typeclasses in Scala was not so widely known when the language was originally created. OO support was and remains a key selling point for Scala; I would expect `for`/`yield` to continue to work on types that define a `flatMap` method directly, in traditional OO style. (Typeclasses are the only way to offer a `point` method, but the `for`/`yield` desugaring doesn't actually need one, only `map` and `flatMap` - in Haskell typeclass terms this is functor plus bind, not necessarily quite monad). That does raise the question of why there's not an OO-style `trait` that values have to implement to be valid for `for`/`yield` desugaring though. One answer might be a desire to support `for`/`yield` on types that don't form full functors/monads: `TreeSet` is not a functor but can usefully be used in a comprehension (and this is an open problem in Haskell); `BitSet` conforms to the almost-monadic `Set` interface but offers a more specific return type for particular `map` operations which it was felt desirable to preserve. Another side is that there is still ongoing library work about what these typeclasses should look like. Cats and ScalaZ are taking different approaches to stack safety in monads (not a problem that Haskell has). There's ongoing debate around the extent to which monad/applicative behaviour may vary (less of a pressing issue in Haskell since they have the `ApplicativeDo` language extension which people choose to enable or not). It makes sense for the language-specified syntax to be more permissive in a language that doesn't have an easy/widespread language extension model. I doubt dotty will include more support for these constructs. Odersky believes that he can come up with a better model for representing effects and that will be built into dotty, so if anything there's likely to be less support for FP style. (I am skeptical to say the least: many smart people have tried to come up with a more lightweight syntax to replace monadic style, including several macro-based libraries in Scala (e.g. thoughtworks Each for a recent example), and it's always turned out to have major problems compared to monads).
since you would transform all monads to the same type, you would compose the same kind of monads :)
What have you tried and what have you specifically got stuck on? Are you able to read a string that the user has input? Do you know what format you want them to input in? Is your difficulty in parsing that string to your internal format, or what?
It actually make things simpler while more difficult. The thing is algebraic programming is almost not taught in school/universities and bot widespread. So instead of having courses we have to learn it by ourselves. Even worse, when you're a junior, you can expect to be close to a fair number of experienced OOP or imperative programming people. But for FP there is a high probability there is no experienced FP dev arround you. So yes it makes learning difficult. But yet when you feel at ease with it, then developing becomes much simpler because it starts to follow principles, you earn the ability to reason about the code and you can compose pieces of code togheter easily. As an example, in pure FP you can always remove the call to a function whose result is ignored. You can always reorder function calls and variable values never change so you don't have to follow instruction by instruction to find a bug, just follow the data flow. OOP and imperative programing are not simpler, they are taught early and finding experienced people is much easier 
Yeah, I mentioned that this might be an argument pro typeclasses (of any kind, so category theory as well). I still don't think that they *should* add them. If it's in the standard library, it should be reused everywhere across it. Next thing you know, your simple fold on a List requires a Monoid all of a sudden. People would get scared away. I wouldn't (I would actually prefer to have a standardized lib rather than the scalaz-cats mess). But it's not just me who's using Scala. It serves as a perfect OOP --&gt; FP language and it owes most of its popularity to that; adding such typeclasses would move it too far to the FP side imho. I'm already considering moving to something like Haskell, and that's fine. But I'd keep Scala at the very point in the spectrum where it is at the moment.
I agree. It's like a thing about your mother tongue. I might know some extremely hard native language (e.g. Hungarian) and it's easy for me because I've used it for my whole life. For someone else, who is native to relatively easy language (let's say English), learning it as an adult might be an impossible challenge. Inverse is a different story: Hungarian learning English has a relatively easy job to do. Thing is, in our industry there is relatively little sources, mentors and experience with FP because for years industrial level FP was infeasible - speed of compilation and execution was too slow, tools were poor and communities didn't do anything to ease the transition: e.g. OCaml for quite a long time did not have a package manager, so everyone coding in it was forced to reinvent the wheel. Haskell improved tooling a lot but still thinks it's better to force you to use weird names and even weirder operators instead of change names to me more self-explanatory (self-explanatory after reading white-papers don't count). All compiled FP languages AFAIK without some real deep insight has too unpredictable memory and CPU footprint that they are virtually impossible to use in embedded or system programming (with Rust as a notable exception). Quite a lot of programmers working in a performance critical projects will refute FP upfront since they cannot fine grain tune every single CPU instruction. So that leaves FP relatively small field where it is a true winner, namely projects where performance on a single machine can be sacrificed for more correctness guarantees, like some financial industries or huge web applications, where workload can easily be improved by horizontal scaling and higher memory and CPU usage is not an issue as long you are avoiding deadlocks, concurrency issues etc. These are usually well paid nices, but still nices. Game programming will still ditch correctnes for few more FPS - risk of app crash doesn't impact the product much. In kernel development correctness is very important, but overhead imposed by most FP languages would make the implementation useless. I'm aware that you are able to e.g. sort array in-place in Haskell if you know some obscure language details, but for majority of devs that would be an impossible task. And what an average dev can do is what drives the industry.
Do you want to implement a SOAP API server? My advice is not to start with play, but to generate the SOAP server and start from there. You can use `sbt-cxf-wsdl2java`, a great plugin that takes a WSDL file and generates all the data classes and routing needed. You only need to implement methods that handle each call afterwards. CXF generates Java classes, but it's ok, you can handle the conversion between the Java boilerplate and your Scala logic within the methods that implement the server calls. Of course, I assume you have a WSDL file to work with. If not, you must begin by creating one. I've worked with SOAP in the way I described above, but I haven't created WSDL files from scratch. I suppose tools like SoapUI can help create such files. TL;DR you're back to Java-land, but it's not that bad.
&gt; As unfortunate as it is, we live in a world where lambadas are seen an obscure concept but runtime dependency injection is perfectly fine. A markedly salient point. We’ve seen languages and techniques which likely cause bugs and diminish maintainability wholeheartedly embraced, but those designed to guarantee program correctness are—as you say—shunned as obscure or too hard to learn.
Could you elaborate?
But now people are just implementing ad-hoc monads and functors and semigroups all over the place. It's ridiculous. Also, there are tons of libraries that use these typeclasses but none of them leverage scalaz/cats because they don't want to add another dependency. Scala is already a very complex language and not having a standardized FP programming library is a big pain point. All over company codebases everywhere, there are people using `\/`, `Either`, `Maybe`, `Option`, `Future`, `Task`, etc all at the same time. If a lot of features from base scala were removed that suck (like `Future`s) and replaced with scalaz constructs things would be a lot better.
Using scalaz is really annoying, and for a library, people don't want that dependency. The problem is not that people need to use scalaz, the problem is that they don't, and then the codebase is filled with all of these ad-hoc FP constructs.
CanBuildFrom is going away in Scala 2.13.
I wouldn't assume that people would agree on one format and stick to it. After all Tony Morris was the one that proposed `Either` to Scala, then he was displeased with it and so in Scalaz he implemented \/. `Future` and `Task` are not 2 implementation of the same thing - these are 2 different things, that is eagerly evaluated async value and lazily evaluated async value (see Monix docs as there is also Eval as synchronous lazy value). They have different us cases in code: `Task` and `IO`s are used to build up pipelines in functional way (referential transparency, deferred side effects and shit) while Future is what Tasks/IOs are materialized into when you want your execution to start. Also they serve as async primitives that glues different tools together. You might try to standardize the library but the fact that cats and scalaz exists in parallel shows that stewards of FP community _strongly_ disagree what is the right course for everyone. If you follow them on Twitter you'll see that these 2 communities gather 2 completely different kinds of people, so creating "one true standards FP library" now would only end up with core developers leaving or ignoring it.
You can totally try to be simple and fail at it. A failure to be simple is not evidence of a lack of trying. Collection building and mapping in scala is one of those situations where they tried to make something easy, and it turned out to bring a lot of complexity. As for why there is no `Monad` typeclass in the stdlib, the answer is probably that Monad by itself doesn't bring you that much, and implementing the whole FP menagerie that makes it useful is done to relative satisfaction by other libraries (cats, scalaz) (should `Monad` extend `Applicative` and `Functor`? Scato encode it? should there be a `Traverse`? should there be a `MonadFilter`?) cats-core + cats-kernel believes that it's a minimal fp library, with no unneeded stuff -- but cats-core and cats-kernel together are a bigger jar than the stdlib is. As for why `for` doesn't require `Monad`, that would tie the language (`for` is a language construct) to the library (A `Monad` typeclass is a library construct): It would mean that `for` would only ever work with the `Monad` in the standard library, and could never work with an encoding from elsewhere. Additionally, `Monad` is both too weak and too strong for `for`. Guards can't be implemented (they require something like `MonadFilter`, for which various encodings could exist), and a `for` that only does a single map would only require a functor. 
Do you need to call Soap services, or implement a Soap service that others will call? The former is pretty easy with jax-ws (a java lib, maybe even built into the JDK, but it's been10 years since I used it). The latter could be very bad, or pretty smooth, it depends. Integrating with Play is a wild card though.
I have the WSDL and I want to consume not implement the SOAP API.
&gt; Not happening. Language needs to get simpler not more complex and adding big-ass library wouldn't help achieving that. I don't think that's on Scala's agenda anymore. The thing is that multiple groups coming up with their own supplementary typeclasses has its own cost: You saved standard library space, but at the price of making everyone's lives worse, due to the scalaz typeclasses vs. cats typeclasses vs. home-grown typeclasses vs. no typeclasses situation from which no one benefits.
For the longest time, there weren't any studies that show that type safety helps to deliver better software. However, in [Baishakhi, et. al, 2017](http://web.cs.ucdavis.edu/~filkov/papers/lang_github.pdf), it was shown that languages have an effect on the number of defective commits. Haskell and Scala have the lowest number of defects -- if the average project has 4 defective commits, Haskell would have "3.18" defects and Scala would have: 3.02 defects. That is, one fewer defect on average. Features also matter: "Functional-Static-Strong-Managed" languages would have 3.11 defects (an average of one fewer defect). Domain has no effect on this problem. Another way of interpreting this is that "easy" languages without type systems or pure functional programming constructs, such as monads, produce buggier code. So functional programming and type systems \*DO\* matter, and while strongly correlated, the actual effect isn't that much. While there is a clear incentive to learn fp/categorical programming from a code quality metric, the incentive to learn and the amount of time required to absorb the necessary material and experience to apply it may not be profitable for most programmers or companies. I find FP easier to do and understand. That doesn't mean everybody will. I value the defect statistics above. Not everyone will. I think that Typeclasses/implicit classes make adhering to the Open/Closed principle of OOP much easier, but other's might think pattern soup (adapter, proxy) is just fine for that. I think implicit arguments make configuration quite simple, repeatable, flexible, and compiler verifiable, but others find it harder to understand that spring annotations. I do enjoy lawless for comprehensions for small programs. Requiring monad/monadFilter would make some programs more difficult to express, as has been stated elsewhere in the comments. I like extension for some things -- with is nice to have when you want to just slap something together. Having to write an extension class for everything would be, and is, a tedious way to encode small programs.
Simple stuff with Scalaxb is not exactly obvious but isn't terribly hard. WSDL files go in `src/main/wsdl`. XSD files go in `src/main/xsd`. Add the sbt plugin to your build in `project/plugins.sbt`: `addSbtPlugin("org.scalaxb" % "sbt-scalaxb" % "1.7.0")`. Enable the plugin in `build.sbt`: `enablePlugins(ScalaxbPlugin)`. You might change some settings to suit you, e.g. `scalaxbPackageName in (Compile, scalaxb) := "generated"`. That's enough to get the classes generated when you do `sbt compile`. Instantiating the client is done with mixins. It'll look something like this: val client: generated.ServiceSoapBindings = new generated.ServiceSoapBindings // maybe SoapClientsAsync if this is SOAP 1.2 with scalaxb.Soap11ClientsAsync // maybe another client if you use a non-dispatch backend, like gigahorse with scalaxb.DispatchHttpClientsAsync { // needed if your endpoint is not correctly set by the wsdl override val baseAddress: URI = new URI(config.serviceEndpoint) } Using the client is pretty discoverable by looking at the generated client or relying on IDE information.
I can bet you that if Scala merged e.g. Scalaz libraries into std library, Scalaz community would need only few months before they could arrive at the conclusion that these implementations are not FP enough and need to be rewritten, because there is that new white-paper showing better way of doing X and Haskell has some PoC already...
FPers make less than 50% total developers writing scala, but probably an equal amount when it comes to open source, since there's a lot of people that write scala for work because they have to and do not ever contribute upstream, so in that, I think that's what he meant by 50%, as the ones simply writing scala due to their work being in scala and only that don't have much say on the ecosystem. Aside from that, FPers are a loud minority. That said, being completely honest (as someone that participated in it), the reasons for fp in scala dwindle every day and loads of fpers burn out because of it: * The compiler team explicitly makes a point that they want OOFP and not haskell-style fp to be their thing. I mean, this is totally fine, they're the ones working on it, but it means you are directly and undoubtedly a second class citizen doing fp in scala. * FP has a perf overhead in languages whose compilers aren't explicitly crafted for it, and that optimize for cps-like constructs like continuations like ghc does. * [kmett's comment still holds 4 years later, especially the part about free theorems](https://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu/). * in the presence of java interop and io monad abuse (which is recent, and not covered by kmett as cats-effect and zio weren't a thing back then), the way people write fp in scala is... weird to say the least. It doesn't translate 1:1 with haskell, because of scala's idiosyncrasies and decisions from library developers on how java interop should happen, meaning that a lot of "fp" code in scala is still sort of like developing without any sort of safety net, and it kinda makes some code bases look like half of it is FFI. Eta is far more promising in this regard, [and encouraged by the compiler team to look into for fp-style](https://twitter.com/adriaanm/status/1017067318298447872). It might sound like a betrayal to my fp principles to admit that there's some truth that you shouldn't continue with fp in scala like the epfl team suggests, but seeing myself and my friends burn out and more and more contributors burn out due to being second class citizens to begin with kind of sucks.
you're breaking my heart martin odersky. i'd kill to work on the scala compiler (and make a bunch more money while doing it), but I got responsibilities to my current employer maybe next year :(
I would say that is is not a problem with _Scala the language_ but, more with _expectations about Scala_. I think many people who _got burned out_ though that Scala is some sort of soon-to-be-Haskell on JVM (which clearly was non-goal for Scala's creators since always). And this hope that with one more fix, and one more library they get closer to Haskell was what created this disappointment. I once had some expectations about C++. They weren't meet. And so I burned out and moved to Scala. I never seriously programmed in Haskell, so I haven't placed any high expectations on Scala. And I am really happy with the language and have no real complains about it even if my style of programming make me stumble on a bug in compiler every month or so (tagged types + value classes + shepeless + macros :D). I wrote posts about Scala, published libraries and I don't feel tired at all. I see the issues, and while I welcome any improvement I am not tired at all with them. On the other hand, when I wanted to try Haskell I found some of its pure ways intrusive and getting in my way of being productive. Have we programmed in different languages? No. It's just we have different expectations and hopes and mine were met and your weren't. If I were you, I would simply move on to another language, not because _Scala is bad_, but because what _Scala aims for_ is not what _you aim for_, I believe. The same way Google or Facebook or Microsoft might be great companies, but i wouldn't ever want to work there, just because we don't fit.
The gist that I remember from my jax-ws days was that you pointed your lib at the WSDL (the document that SOAP services expose that describes what "methods" are available and how to call them), and had the lib make you an object with methods you could call that matched the ones in the WSDL, and would in turn invoke the remote service. This usually happened via code generation (there were tools called `wsdl2java` for this), or at runtime via bytecode generation. Scalaxb is a Scala lib that takes the former approach. Here's something from its docs about WSDL support: http://scalaxb.org/wsdl-support . Good luck! 
Any change is hypothetical at this point, as the trenches have been dug way too deep. This was more of an explanation of my past plans and not really a recipe for the future.
&gt;No. It's just we have different expectations and hopes and mine were met and your weren't. This is the language of the passionless, and the inept. 
Man you took all this time to type this, can't tell how grateful I am. I'll try it first thing in the morning. Long may you live!
You can use https://downloads.typesafe.com/rp/play-soap/Home.html
Hope it helps! And feel free to ask me more about it. I've had my fair share of of experience with the library, to the point where I tried to contribute to it but am now (extremely slowly) working on a library of my own to eventually replace it.
I haven’t double checked but my guess is that it is. I recently learned it’s a trick used in the new Scala collections library too https://docs.scala-lang.org/overviews/core/architecture-of-scala-213-collections.html#four-branches-of-templates-traits
If such opinion helps you feel better about yourself...
&gt; // exactly two cases that need to be considered! &gt; loadClassDecl(name: String): ClassDecl | EmptyTree.type &gt; And now guess which feature enums don't support. ;-) The compiler error I get is "Singleton type is not allowed in a union type". Is that what you mean? &gt; PS: Oh, and enums need to be flat, so it's unlikely most existing sealed trait/class/object hierarchies can be converted. You can do e.g.: enum Vehicle { case Car case Train } enum Car extends Vehicle { case Tesla case Toyota }
It's okay. I just find using the higher-kinds trick results in harder to read code. If they get KEEP-87(?) and other language features into Kotlin, then it could be something great.
A little expensive though 45 for kindle edition. It would have been OK to have 45 for physical copy ;)
I found the website docs pretty good
pair programming on everything, YUK. ** I've done my time as a Scala contractor in many teams that pair on everything, its a toxic practice when dogmatically enforced. 
don't think HMRC are on the cloud no? If they are it must be a recent change.
Angular fails often at runtime (template layer errors, for example); the framework is complex, there's a ton to learn, and many non-obvious performance bottlenecks to avoid. TypeScript is pretty amazing, but it itself, being closely aligned with JavaScript, will blow up at runtime; not nearly as safe as Scala/Scala.js in this regard. Scala.js project was a rewrite, stripped out basically all external dependencies and wrote the foundation for the app in Scala.js. Amazing project, Scala.js is just incredible. As for compile time, Scala.js is *blazing fast* compared to the webpack/Angular AOT monstrosity -- I dream of incremental builds as fast as Scala.js.
The main "Making Tax Digital" stuff wasn't, but there were other projects at HMRC Shipley that used AWS. Also this is the first time I've heard "amazing location" in reference to Shipley!
It works, but whenever I used Http4s I used the Async monad it was built around: so scalaz.Task and fs2.Task (presumebly now you would use Cats.IO)
true that. https://doc.akka.io/docs/akka/current/index-actors.html?language=scala
Considering that the library (and the typeclass/HKT proposal) is developed by a member of the Scala Center Advisory Board, I think the i's will be dotted and the t's will be crossed based on lessons learned in Scala. It's likely that it will be the most popular functional programming library on the JVM in a short while.
No, they are different things. A typeclass defines a set of behaviors you then implement for specific types. For example I might have at a typeclass "Printer" that prints a string representation of a type. I then can define instances for String, MyObject and so on. One example of where there's some interaction with generics: I can define functions that accept any type as long as they're a typeclass instances in scope for it: def foo[A](value: A)(implicit printer : Printer[A]) = printer.print(value) Think of it as a more flexible way to add behavior to a type. Rather than altering its superclass or adding a decorator, I can add on little bits of in a flexible, composable way 
It seems like typeclasses are like adapters or transformers? This example is a bit too simple to show the benefits I think. Correct me if I'm wrong but this works because all classes have a toString method no? Otherwise the type `A`would need to be more specific? I'm having trouble visualizing a more complex example
If you like books https://www.amazon.com/Akka-Action-Raymond-Roestenburg/dp/1617291013/ref=sr_1_2?ie=UTF8&amp;qid=1540748780&amp;sr=8-2&amp;keywords=akka
No, they are different concepts, but the synergies between them is what makes for example Haskells and Scalas type systems really powerful. Generics are basically type parameters that can be added to data types and functions (and methods) to make them polymorphic, i.e. they can be used with many different types. It's an old concept from functional programming ([System F](https://en.wikipedia.org/wiki/System_F)) used in languages like ML and Haskell. Most modern statically typed languages have generics in some form. If the language support subtyping the type parameters can often be constrained with sub- and supertypes. Type classes are a newer form of polymorphism first introduced in Haskell and they are not as commonly available in programming languages (but Haskell, Scala, Rust, Swift have them in some form). They are used for constraining type parameters (generics) to types which have certain properties (i.e. belong to a certain type class). So in Haskell you use type classes to constrain type parameters while in Scala you can use both sub- or supertype constraints **and** type classes (implicit parameters).
**System F** System F, also known as the (Girard–Reynolds) polymorphic lambda calculus or the second-order lambda calculus, is a typed lambda calculus that differs from the simply typed lambda calculus by the introduction of a mechanism of universal quantification over types. System F thus formalizes the notion of parametric polymorphism in programming languages, and forms a theoretical basis for languages such as Haskell and ML. System F was discovered independently by logician Jean-Yves Girard (1972) and computer scientist John C. Reynolds (1974). Whereas simply typed lambda calculus has variables ranging over functions, and binders for them, System F additionally has variables ranging over types, and binders for them. As an example, the fact that the identity function can have any type of the form A→ A would be formalized in System F as the judgment ⊢ Λ α . *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Generics are a language-level feature allowing you to write code that works uniformly for any type, getting the same behavior regardless of the eventual type, like what you see with data structures. It's more formally called "parametric polymorphism". Typeclasses in Scala are not language-level. They are a pattern for how to construct your code in order to gain one form of "ad-hoc polymorphism" and get different behaviors depending on the type. A more familiar form of ad-hoc polymorphism is simple function overloading. The typeclass pattern makes use of the generics feature combined with the implicit feature.
&gt; This example is a bit too simple to show the benefits I think. Correct me if I'm wrong but this works because all classes have a toString method no? Otherwise the type Awould need to be more specific? No, this does not rely on the presence of `toString`. If you try to call `foo` with a type `A` for which no `Printer` is defined, you'll get an error because the implicit can't be resolved. You don't need to restrict the type `A` for that.
You mention the disadvantages of typeclasses but not the advantages. I'm still unclear how typeclasses are more powerful and flexible
Ah, I was thinking typeclasses and implicits could be separate but that seems to be wrong. It looks like from your example, where ever typeclasses appear, there will always need to be an implicit accompanying it, is that right?
Typeclasses are not specific to Scala. However, as far as I am aware, the only way to have typeclasses in Scala is to use implicits.
Any suggestions for interactive data analysis akin to R or python/pandas that *isn't* Spark? I would like to do the kinds of analyses I do on a single machine in R, but with a more production grade language that offers type checking. I think Spark is great, but it's overkill when all I want to do do is read modest-sized csv files into a heterogeneous data frame and start exploring and processing data.
Type classes are not at all like adapters or transformers. Think of it as interfaces than you can implement late. Conceptually type classes are a mechanism that allows you to define a bunch of ways a type can implement some behaviour, then you can define implementations for any type at a later point in time, even for types that you don’t own. The mechanics of this is Scala are a bit odd, you define your “library” with a certain behaviour which require a certain type class to be implemented for any type that wants to benefit from that behaviour. Then you can create this implementation of the type class, which is independent of the type it is defined for. Then, as long as you make it available to the behaviour implementation as an implicit, it will be picked up, and used. This allows you to create implementations for language native types, or for types provided by libraries which you wouldn’t be able to do with interfaces.
Is it thematically suitable for children? I have a couple elementary school kids I’d like to move beyond hour of code.
Hi, I'm one of the founders. I would say there is nothing unsuitable for kids in the game. The only caution I would give is that it really is mostly playable by people who have at least one real-world programming language under their belt. If you do get it and they get stuck anywhere, I'm happy to lend a hand. 
Thanks for the feedback. I'll make sure this gets fixed. 
It's the difference between **parametric polymorphism** (generics) and **ad-hoc polymorphism** (type classes). # What is Polymorphism? **The idea behind polymorphism is always the same:** we are trying to increase code re-use by writing code in a more generic fashion. That is to say, in a higher level of abstraction. **There are three main forms of polymorphism:** ^[1](https://en.wikipedia.org/wiki/Polymorphism_\(computer_science\)) - **Subtype Polymorphism** *(Inheritance)* &gt; when a name denotes instances of many different classes related by some common superclass - **Parametric Polymorphism** *(Generics)* &gt; when one or more types are not specified by name but by abstract symbols that can represent any type - **Ad-Hoc Polymorphism** *(Type Classes)* &gt; defines a common interface for an arbitrary set of individually specified types I think a quick review of how all three achieve the goal of code re-use will make it most clear to you what the fundamental differences are. 
**Polymorphism (computer science)** In programming languages and type theory, polymorphism is the provision of a single interface to entities of different types or the use of a single symbol to represent multiple different types.The most commonly recognised major classes of polymorphism are: Ad hoc polymorphism: defines a common interface for an arbitrary set of individually specified types. Parametric polymorphism: when one or more types are not specified by name but by abstract symbols that can represent any type. Subtyping (also called subtype polymorphism or inclusion polymorphism): when a name denotes instances of many different classes related by some common superclass. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
# Subtype Polymorphism (Inheritance) &gt; subtyping allows a function to be written to take an object of a certain type T, but also work correctly, if passed an object that belongs to a type S that is a subtype of T (according to the Liskov substitution principle) ^[2](https://en.wikipedia.org/wiki/Polymorphism_\(computer_science\)#Subtyping) This is the one we're most familiar with in OOP. ## Example *without* subtype polymorphism: class Cat { val meow = "meow" } class Dog { def bark = "woof!" } def dogSpeak(d: Dog) = println(d.bark) def catSpeak(c: Cat) = println(c.meow) ## Example *with* subtype polymorphism: trait Animal { def sound: String } class Cat extends Animal { val sound: String = "meow" } class Dog extends Animal { val sound: String = "woof!" } def speak(a: Animal) = println(a.sound) While this is a very basic example, we can see that using inheritance and subtype polymorphism, we are able to write code at a higher level of abstraction by using the most generic type in the type hierarchy that satisfies our needs. In this case, we know that all `Animal`s can make a `sound`, so we can write a single `speak` method which will now work for *any* `Animal`. Subtype polymorphism allows us to reduce boiler plate by treating entire groups of types the same based on their position in a type hierarchy, *despite having potentially disparate implementations in the invoked methods*. That is to say, both a `Dog` and a `Cat` could make a `sound` because all `Animal`s make `sound`s, but a `Cat`'s sound is **not** the same as a `Dog`'s sound. The implementations are not uniform across types in the hierarchy.
#Parametric Polymorphism (Generics) &gt; Parametric polymorphism allows a function or a data type to be written generically, so that it can handle values uniformly without depending on their type ^[3](https://en.wikipedia.org/wiki/Polymorphism_\(computer_science\)#Parametric_polymorphism) Subtype polymorphism gets us pretty far, but what about when we want to implement something such as a custom `Array` data type? trait Array { def set(index: Int, item: ???): Array def get(index: Int): ??? } What type can we use in place of `???`? Well, we could do what Java did pre-generics and use the top type. In Java, the closest thing to a top type is `Object`, and in Scala we have a *true* top type (because Scala has no primitives) of `Any`. In other words, *every* type is a subtype of type `Any`. So, we could do trait Array { def set(index: Int, item: Any): Array def get(index: Int): Any } But, of course, this means that we would have to do a bunch of runtime type checking and casting to actually make our custom `Array` type useful, since everything we put into it will come out as type `Any`, which is not a very useful type. val a: String = "hello" val b: String = "world" val strings = new Array() // Pretend this is implemented and not just a trait strings.set(0, a) // a goes in as a String strings.set(1, b) // b goes in as a String println(strings.get(0) + strings.get(1)) // strings.get(0) and strings.get(1) are both of type Any // ERROR! // type mismatch; // found : Any // required: String If we want to maintain static type checking and avoid runtime casts, we're stuck creating a specialized version of our custom `Array` type for each type we want to store in it. trait StringArray { def set(index: Int, item: String): StringArray def get(index: Int): String } trait IntArray { def set(index: Int, item: Int): IntArray def get(index: Int): Int } This obviously gets out of hand quickly. Furthermore, even though we aren't showing the implementatioon of our `set` or `get` methods, it's probably obvious to you that we don't actually ever need to *invoke* any methods on our `item` parameter or access any of its fields. Our `Array` type doesn't actually *care* what type it is, it's just storing a reference to it paired with an index value to look it up later. In other words, our `Array` can handle these values *uniformly*. We could take the implementations of `set` and `get` for `String` and for `Int` and just swap the types out and they would otherwise remain the same between `StringArray`and `IntArray`. This is the area where parametric polymorphism shines, as it allows us to *templatize* our code. trait Array[T] { def set(index: Int, item: T): Array def get(index: Int): T } val a: String = "hello" val b: String = "world" val strings: Array[String] = new Array() // Again, pretend it's implemented strings.set(0, a) // a goes in as a String strings.set(1, b) // b goes in as a String println(strings.get(0) + strings.get(1)) // both strings.get(0) and strings.get(1) are of type String! // We can also parameterize functions def first[T](items: Array[T]): T = items.get(1) Parametric polymorphism allows us to reduce boiler plate by writing code that can work **uniformly** over a range of types by using type parameters to templatize the code. An `Array` type can work for any type `T` because its implementation doesn't depend on any type-specific behavior - the `Array` implementation is uniform across all types.
53 USD if you live in South Africa, paperback will have the 15USD shipping fee + \~8USD import tax. Feelsbadman.
You can run Spark in local mode and avoid most of the overhead, with the bonus that if you ever do need to run distributed it's a simple config change. What specifically were you after? All the collection operations are just built in. Spire offers some mathematical features, Breeze offers its own set (IIRC a bit more vector/matrix-oriented).
Isn't http4s now polymorphic over your task monad? So using monix `Task` should work as well as any other?
The lack of proper higher kinds makes it unwieldy. The continuation encoding means that interaction of unmanaged side-effects and managed effects is *very* confusing, even more so than in Scala. And it will inherently never be able to interoperate nicely with Kotlin's preferred encoding of absence (null) because the nesting behaviour of Kotlin null means that it can't ever be even a functor, yet alone anything fancier.
Regarding generics (I’ve addressed type classes on the other thread), I take it you’re talking about type variables (which are a bit more powerful that just generics). What they do, is what their name suggest, at the basic level, they allow you to have variables at the type level instead of the value level. This allow amongst other things for what in some languages is called generics. The way they relate to type classes in scala is that the implementation that relies on the type class doesn’t know which types will implement the type class, so they rely on type variables constrained by the type class. This way, any implicit implementation of the type class will satisfy this constraint and therefore fit the type that the compiler is looking for when trying to resolve the implicit.
Can you tell me how can I make/get a `.xsd` file? I am only given with a `.wsdl. file.
&gt; The compiler error I get is "Singleton type is not allowed in a union type". Is that what you mean? This is just some unrelated restriction in union types, feel free to turn the object member into a class member and try again. The problem is that you can't refer to more specific types in the signature, because enum members (unlike in sealed trait hierarchies) simply don't get their own type. Features failing to work together seems to be a common theme in Scala, though. Not sure what this "the language should be orthogonal" is all about, if you pick some random feature like union types and try to combine them with other language constructs ... and pretty much everything of interest falls apart. Union types and numbers? Broken. Union types and enums? Broken. Union types and opaque types? Broken.
I'm looking for an agile workflow for the 99% of data analyses that aren't "big data". A heterogeneous collection of columns that can be indexed intuitively like a data.frame in R or pandas. Ideally it would have a csv reader that can infer column types. It should also support mutating individual columns and adding new ones; as well as joins, split-apply-combine paradigm, and row or column wise iteration. It should further be able to do this in a repl with instant feedback. Maybe spark can do all of that. My experience with it so far has been fighting configuration and running out of memory in local mode.
Thanks for the reply. Where do these template errors come from? Is it because they are not compile time checked like scala.js (scalatags, scala-react etc.) or TSX with Typescript in the react world? Are these "non-obvious performance bottlenecks" related to RxJs? &gt; As for compile time, Scala.js is blazing fast compared to the webpack/Angular AOT monstrosity -- I dream of incremental builds as fast as Scala.js. If scala's compile times are that much faster then I really need to try Angular just to see it for myself. Sounds really bad for Angular since with scala you at least get much better compile time safety for the longer compile times.
That may be enough as it is not strictly needed to have both. The wsdl may contain everything. But the xsd spec is online and you can always find an example file to start modifying should it be actually required. It's pretty much a verbose way to describe a class and all its fields.
How about we as a community stop using call-by-name parameters for things that are not call-by-name. Then it won't matter what the callsite looks like.
* https://rerun.me/2014/09/11/introducing-actors-akka-notes-part-1/ * https://www.beyondthelines.net/programming/akka-anti-corruption-layer/ * https://www.beyondthelines.net/computing/actor-synchrone-computation/ * https://blog.colinbreck.com/integrating-akka-streams-and-akka-actors-part-i/
Looks very interesting. Good work! Would you be so kind as to elaborate on exact 'material' that will be covered in phases 1-4 ? &amp;#x200B; In the video it's displayed like implicits and ADT's are going to be covered but I couldn't see a reference to that within the site. Thanks.
I just tried their official site, and followed the instructions mentioned there and the project is not even compiling. I am talking about their official `README.md` instructions from their github repository and the link you sent above.
If I add those lines to resolve dependencies in plugin.sbt, it just won't compile. I am using the play seed project.
Also, there is already a syntax for explicit thunks in Scala, if you this is what you actually want, use that. It will actually produce a compile error if you don't correctly apply a thunk. scala&gt; class Evaluator( codeA: () =&gt; Any , codeB: () =&gt; Any , codeC: () =&gt; Any ) { ... } scala&gt; val ev1 = new Evaluator(() =&gt; println("a"), () =&gt; println("b"), () =&gt; println("c")) ev1: Evaluator = Evaluator@6f911326 scala&gt; val ev1 = new Evaluator(println("a"), println("b"), println("c")) &lt;console&gt;:12: error: type mismatch; found : Unit required: () =&gt; Any val ev1 = new Evaluator(println("a"), println("b"), println("c")) ^ &lt;console&gt;:12: error: type mismatch; found : Unit required: () =&gt; Any val ev1 = new Evaluator(println("a"), println("b"), println("c")) ^ &lt;console&gt;:12: error: type mismatch; found : Unit required: () =&gt; Any
:')
One of the best explanations I've read yet!
&gt;This is just some unrelated restriction in union types, feel free to turn the object member into a class member and try again. &gt; &gt; The problem is that you can't refer to more specific types in the signature, because enum members (unlike in sealed trait hierarchies) simply don't get their own type. This compiles: enum Tree { case ClassDecl(data: String) case ObjectDecl case MethodDecl // ... 20 other trees ... case EmptyTree(reason: String) } def loadClassDecl(name: String): Tree.ClassDecl | Tree.EmptyTree = ??? So does this: def test(x: Tree.ClassDecl | Tree.EmptyTree) = x test(new Tree.ClassDecl("foo")) Am I misunderstanding anything? &gt;(Also, your example shouldn't work, because enums cannot be inherited as far as I understood.) But it does work. 
&gt; This compiles/So does this Interesting! Not sure if something changed here or I just tested things incorrectly, but I think I'll retract this point and maybe look at the commits to better understand what's up here. Looks like this only leaves union types with numbers and union types with opaque types broken! &gt; new Tree.ClassDecl("foo") This feels a bit weird though ... with classes `case` allows leaving out the `new` ... not sure if people are supposed to write `case case ClassDecl(data: String)` for enums ... &gt; But it does work. I think this is https://github.com/lampepfl/dotty/pull/5031.
This was a nice read. I've dabbled in compilers and built a scripting language so I was particularly interested the phases in the pipeline. It's also interesting, although not too surprising, that the longest phase was the typer as it checks implicits/macros. 
Never even heard of it before this. Looks great!
Interesting &amp;#x200B;
&gt;Looks like another victim of "never infer singleton types"... &gt;Or am I missing something? I think it's because the enum type (`Tree`) is always inferred. But I agree that this is a bit unexpected (the first two lines work, the last doesn't): val x: Tree.ClassDecl = new Tree.ClassDecl("foo") val y: Tree = Tree.ClassDecl("foo") val z: Tree.ClassDecl = Tree.ClassDecl("foo") // compile error From the docs: &gt; [...] the implementation case classes are not visible in the result types of their apply methods. This is a subtle difference with respect to normal case classes. The classes making up the cases do exist, and can be unveiled by constructing them directly with a new.
Not quite. You can cheat, like naming an object like a command prompt: val &gt;&gt; = new Assembly8085 \&gt;&gt; IN 00 &amp;#x200B; &amp;#x200B;
&gt; I think it's because the enum type (Tree) is always inferred. True, that's how the `apply` methods are specified, but I think there is no `apply` method involved for `case Foo`-style objects. It seems there is no way to avoid losing the precise type of the enum singleton objects (like you could do with `new` for enum member class values), except changing every method signature that a singleton type could ever touch to make sure that type is preserved.
https://stackoverflow.com/questions/13011204/scalas-postfix-ops
It's possible with postfix operator, but you will have to change `00` so it can be valid method name. object IO { def OO: Unit = {println("IO OO");} } &gt;&gt; IO OO also if you want `OO` part be dynamic you may use `Dynamic`. import scala.language.dynamics object IO extends Dynamic { def OO: Unit = {println("IO OO");} def selectDynamic(name: String): Unit = {println(s"IO $name");} } IO SS &gt;&gt; IO SS
Collections yes but implicits no. A full list is available on the homepage.
&gt; It would be perfectly logical for a developer to assume that afoo has changed, since it appears that the three .cat()s were called first, and then Evaluator's constructor was given their results. This seems like one of the roots of the problem here - is that "perfectly logical" in a language with by-name parameters? That's an assumption that might be being carried over to scala from another language where it would might have been more reliable. The other root of the problem might be the side-effecting `cat` method. Were this itself referentially transparent, it wouldn't matter how many times it was (or wasn't) evaluated. As for the convention, off the top of my head we use by name parameters in two situations 1) Enabling new syntax For example, https://docs.scala-lang.org/tour/by-name-parameters.html 2) A performance optimisation on a "normal" parameter that might not be used. Logging is the classic example here - most logging frameworks take a by-name parameter for messages, avoiding the construction of a potentially expensive string if it's not going to be useful. In the former (rarer) case, the annotation wouldn't hurt, but wouldn't really add anything that wouldn't (hopefully) be clear from the name of the function, and in the latter case it would add unnecessary clutter, eg: ``` log.info({{s"Frobnicating widgets ${widget1} and ${widget2} together."}}) ``` So I don't know that I'd find this terribly helpful personally.
Is that talk available publicly somewhere?
It is literally a hyperlink in the above post. https://www.youtube.com/results?search_query=odersky+scala+world+keynote
"Scala is a programming language created in 2014 by Martin Odersky." Nope.
Exactly, this solution would be perfect, if I could use it with numbers. &amp;#x200B; Anyway, thank you all, because I didn't know about this Dynamics stuff!!
Thank you! I never really understood this
Just wanted to say thanks! I use it in a couple of projects and it's great
The return inside println is *not* the same as the one without
Do implicit adapters really count as type classes? I'm new to scala, so you know, take my comments as worth the $0.02 they are actually worth, but I'm not really seeing them as the same thing. I mean... is it really *meaningfully* different to just passing an `AdaptorFactory` that returns `Adaptor[T]` to the method, which can apply the various operations? Sure, it's not exactly the same, but it's not really that different, it's just using some scala magic hand waving (implicit) to be slightly more ergonomic instead of explicitly implementing `AdaptorFactory`. By comparison, in rust for example, you can create a trait and implement it on arbitrary types, eg. int, float, etc. ``` trait Foo { fn foo(&amp;self) -&gt; f32; } impl Foo for i32 { fn foo(&amp;self) -&gt; f32 { *self as f32 } } impl Foo for &amp;'static str { fn foo(&amp;self) -&gt; f32 { self.parse::&lt;f32&gt;().unwrap() } } fn sum_foo_like(t: Vec&lt;impl Foo&gt;) { let sum: f32 = t.iter().map(|i| i.foo()).sum(); println!("Sum of values that are FOO: {:?}", sum); } fn main() { sum_foo_like(vec!(1, 3, 4)); sum_foo_like(vec!("1", "3", "4")); } ``` NB. `&lt;impl Foo&gt;` is basically rust syntax sugar for `sum_foo_like&lt;T&gt;(...) where T : Foo` Anyhow; the point is *this* is a type class right? You arbitrarily associate the interface Foo with any other type, and Foo itself is not a concrete type; you cannot 'have an instance' of Foo; you can only have an instance of T, where T is 'foo-like'. You can only ever have a single implementation of Foo for T. Where as... `NumberLikeDouble` is a case class, it's actually just an adaptor. You can have as many as you want and pick the one you want at runtime. It's... a different thing right? I mean, I'm happy to accept people call these type classes, but I thought, you know, type theory and all that... they're not actually language constructs, so really it's not actually meaningfully different from just having a bunch of adaptors in java with some sort of DI container to pick the right one to use...
The next step then is usually to create extension methods - for man in this case - which are just implicit decorators taking those implicit type-class things as strategies. Note that you cannot have more than one of these objects in scope for a type. The compiler will tell you about ambiguous implicits. 
There's a lot to reply to here and I'm having a hard time organizing my thoughts in the same order you present your question, so please bear with me as I'll probably quote you out of order. Also, I don't know Rust so any Rust code I show for comparison will be pseudo-code at best :) For reference, here is our basic type class implementation. I'm only going to implement `plus` to keep things shorter. // Our type class // We define the interface named NumberLike which // attempts to constraint its members to those which // have implemented `plus` for some type `T` trait NumberLike[T] { def plus(a: T, b: T): T } // Our type class member implementation for Double // This would be analogous to: // impl NumberLike for Double { fn plus(&amp;self, b: Double) Double -&gt; { *self + b } } implicit object NumberLikeDouble extends NumberLike[Double] { def plus(a: Double, b: Double): Double = a + b } // A method that makes use of the type class def sum[T](xs: List[T])(implicit ev: NumberLike[T]): T = xs.reduce(ev.plus(_, _)) &gt; You arbitrarily associate the interface Foo with any other type We've arbitrarily associated `Double` with `NumberLike`. &gt; and Foo itself is not a concrete type `NumberLike` is not a concrete type. &gt; you cannot 'have an instance' of Foo; you can only have an instance of T, where T is 'foo-like'. We have that, as well. However, there exists a single instance of `NumberLikeDouble` which holds our implementation of the type class API. This, I surmise, is what you take issue with. But, we'll come back to that. &gt; You can only ever have a single implementation of Foo for T. This is something we both have and don't have. I'll ty to explain. implicit object NumberLikeDouble extends NumberLike[Double] { ... } implicit object NumberLikeDoubleAgain extends NumberLike[Double] { ... } Those two objects are both valid and can co-exist. This breaks your rule. However, we rely on Scala's rules of implicits which dictate that you can only ever have a **single** matching implicit parameter in scope, otherwise you will get a compile error. It's not quite as cut and dry as that, because implicit matching rules are a bit complex, but you can rely on this consistency. You'll only ever get a single implementation or you'll get a compiler error. &gt; Where as... NumberLikeDouble is a case class Here is where I think you may be confusing the classic adapter pattern with what the type class encoding does. `NumberLikeDouble` is an **object** not a class. There is only a single instance of it. &gt; it's actually just an adaptor. You can have as many as you want and pick the one you want at runtime. As mentioned, you can only ever have one of any given implementation, but I think here you are referring to the situation I demonstrated above - implementing multiple versions for a single type `T` with different names. What is important to note is that none of this happens at runtime. This is 100% compile-time. There's no runtime type checking to decide what "adapter" to use. Instead, we use the implicit parameter to prove to the compiler that `T` is a member of our type class and to serve as a way to access the `T`-specific implementation of our interface. In scala, you can think of all methods on a type class interface as static methods, we just happen to have them stored in an object called `NumberLikeDouble` and the only way you'll get access to them, following the type class convention, is if your type `T` is a member of the type class, evidenced by an existence of a `NumberLike[T]` for your type `T`. &gt; so really it's not actually meaningfully different from just having a bunch of adaptors in java with some sort of DI container to pick the right one to use... My understanding of DI containers in Java are that they work via reflection at runtime. Type classes, as encoded in Scala, require zero runtime type reflection. But, to further explore your point, I think what you're getting at is this: def sum[T](xs: List[T], adapter: NumberLike[T]) Your argument is that `T` isn't *really* a member of `NumberLike`, we're just constraining the type checker using a second parameter to ensure we have an object available with the appropriate implementation for the `NumberLike` interface for the type `T`. And, you're right. To the type checker, `T` has nothing at all to do with `NumberLike`, and the only reason the compiler would complain is if we were to pass in a `NumberLike[T]` with the wrong `T` parameter. But isn't that *really* just an implementation detail? For all intents and purposes, this method **only** compiles if we have a way of accessing the required set of methods implemented for our specific type `T`. By using implicits, not only do we get the ergonomics, but we also get the guarantee of only ever having one possible implementation in scope. Have we not wholly achieved the goal of type classes, then, despite them not being built-in? From a type soundness perspective, it holds, no?
Anyone compared this with [ScalaMock](https://scalamock.org/)?
Cool! This is great
hmm... Interesting points. I'll have to think about it, and thanks for taking the time to answer thoughtfully. You might be passing a singleton instance of `(implicit ev: NumberLike[T])`, but it is still an object instance, and although you may rely on this behaviour to be resolved at compile time, you *can* supply the value of the implicit at runtime (why I don't know, but you can), and you can have different implementations in different scopes, even at compile time. ...but... &gt; Have we not wholly achieved the goal of type classes, then, despite them not being built-in? I guess so. :) ...and I see the value in the pattern. &gt; From a type soundness perspective, it holds, no? I'm just not sure you can say, it works mostly like a typeclass practically, so even if it isn't actually a typeclass, we'll call it typeclass. ...but, I suppose, if everyone refers to them as typeclasses, I'll just get used to it. :) 
I think he's cheating with implicits. He's providing an adapter that is implicitly called. So you need to create a new implicit for every new type. I don't see any benefit in this. 
I feel like we need something like the [Typescript Progression Ladder](http://www.techladder.io/?tech=typescript), but for Scala
Glad you like it!
Not sure I got it
Return exits the current *method*, not the current block (just like Java): You can't put a return at the inside end of a println and have it print something.
Thanks, this is fixed.
You're right it's worth mentioning. I will update the post, thanks :)
Awesome news! Me and the guys at Functional Works have been looking forward to this :)
&gt; I think he's cheating with implicits. In what way do you consider the usage of implicits here to be cheating? We are trying to encode a language level construct into the type system of a language that does not natively support said construct. And we are successful, largely in part due to the compile time semantics of implicits. To call this "cheating" just seems strange to me - what is the alternative? &gt; I don't see any benefit in this. You don't see any benefit in being able to arbitrarily group unrelated types together under a common interface so that you can write a single function against that interface? How would you solve the problem I described - writing a `mean` function that accepts lists of arbitrary "numeric" types and returns their average? 
My personal (very un-detailed) comparison is that ScalaMock is what you should use when you want Scala.js support, and mockito-scala is a little nicer if you only care about the JVM.
You could give the trait BaseModel a type parameter, say `ID`, then use a different type for each BaseModel subtype. trait BaseModel[ID] { def id: ID } case class ModelA(id: UUID) extends BaseModel[UUID] case class ModelB(id: String) extends BaseModel[String]
So, what real life type may be a Semigroupal or a Semiringal?
&gt;Well according to his blog post he used string literals (i.e. &gt; &gt;"1-5" &gt; &gt;) instead of Scala's range object (i.e. &gt; &gt;1 to 5 &gt; &gt;) so that he could use macros to generate optimized runtime code. True, but the performance did not get better because he could avoid "lot more closures/objects" but instead by statically knowing, what to check for. E.g. a \`Range(Start("1"), End("5"))\` would have brought the same performance while parsing even though more objects are involved. 
Awesome quote from Doyle and thanks for mentioning me, I really appreciate it.
Blaze runs http4s, for those who aren't familiar. Colossus is an actor-based, event-based http framework from tumblr. It's nice to see scala web frameworks at the top of these benchmarks, as they tend to not seem to do as well in techempower.
ScalaMock is nice when it works, but sadly it breaks pretty easily with more advanced language constructs. Mockito doesnt have these problems but is rather awkward to use. mockito-scala leverages the comprehensive backend but exposes better syntax and additional features not possible in raw mockito.
This benchmark is nonsense, how is blaze number3, but http4s with blaze number 143? What is it using to serialize json? blaze is just an http parser. json libraries on this list, like jawn for some reason on near the bottom?
How about this? ``` trait BaseModel { type ID } case class ModelA extends BaseModel { type ID = UUID } case class ModelB extends BaseModel { type ID = String } ```
Have you thought about using composition instead of inheritance? case class BaseModel(...) case class ModelWithId[Id](id: Id, model: BaseModel) type ModelUUID = ModelWithId[UUID] type ModelStr = ModelWithId[String] def doNotCareAboutId(model: ModelWithId[_]) = ??? def onlyUUID(model: ModelUUID) = ??? Feel free to name them better. Also, maybe take a look at CoFree.
Wow, that's really surprising.
Monads are not supported by the real world. Are we finally getting back to APL when we look "things" like scalaz (scalaz == APL)? Remember how "easy" it was to talk with APL guys about their relation to your real world? Sure with three symbols they were able to take care of processing with costly CPU time (and suck all the juice from 3090 mainframe dimming the lights in the building). CPU time today is more about knowledge acquisition time where Monads and scalz etc are capable of generating big bills if not in really really capable hands which in turn are costly and creating really complex strategies to survive in long term. Consider market situation, long term projections, availability of competences etc. and who is marginal and who has strong focus from business perspective.
Broken link?
Not finished yet, but [Scala High Performance Programming](https://www.oreilly.com/library/view/scala-high-performance/9781786466044/) has been good so far
Can you elaborate what features doesn't work in scala.js? I'd like to see if there is a way to make them work :)
i found the docs together with applied akka patterns by mike nash and wade waldron was a great overview and really highlighted not only what you can do but where you can really go wrong and how to avoid it. would recommend!
Great, thanks!
Great question! Since `Applicative` is a subclass of `Semigroupal`, there are a ton of instances for `Semigroupal`. E.g. `List`, `Future`, `Vector`, `Either`, `Validated`, `Option`, `Set`, `Map`, `IO`, but `Semigroupal` is also a superclass of the invariant and contravariant side, so things like various decoders and codecs also usually have a `Semigroupal` instance. As for `Semiringal` (or `Alternative`), there's slightly less, but you still have instances for `Vector`, `List`, `Chain`, `Option` and `Parser`. :)
Can somebody explain to me why printing is considered on the same level as other side effects, say a http call? In Scala most of the side effects I see are wrapped in Futures, so you can see those, but I don't see how printing causes any problems. (It can fill up the disk, if it's logged externally, but IO monad won't help you with that either). As I understand a print won't change anything in your program except what I mentioned. So I'm not that interested in a printing effect that I'd want it represented in the type signatures. 
I don't know but I can speculate. Logically a class has exactly one constructor (overloaded constructors are syntax sugar) whereas a trait has no constructor, which is what makes traits possible to mix in but the flipside is making them impossible to instantiate. So anything you can instantiate must be a class, and the only way to instantiate a trait is to form a class that extends it, one way or another. It makes sense to me that that means `new A` is not allowed but `new A {}` (which is really creating an anonymous class) is allowed. So possibly the most interesting question is why `A with B` is considered to define an anonymous class rather than being regarded as a trait. Which I don't have a good answer to. Maybe just pragmatism, as I can't think of a situation where you would write `A with B` and have it be important that it's a `trait`? Note the same thing happens with `abstract class`: scala&gt; abstract class A defined class A scala&gt; trait B defined trait B scala&gt; new A &lt;console&gt;:13: error: class A is abstract; cannot be instantiated new A ^ scala&gt; new A {} res1: A = $anon$1@7a1ddbf1 scala&gt; new A with B res2: A with B = $anon$1@7497a554
Usually tests will be run in JS as well, unless you configure them to run on the JVM through SBT. And then you couldn't use JS-specific code like you probably want.
&gt; (It can fill up the disk, if it's logged externally, but IO monad won't help you with that either). Well, it helps by giving you a red flag for which functions might fill up your disk, which makes it easier to diagnose possible failure modes. &gt; As I understand a print won't change anything in your program except what I mentioned. So I'm not that interested in a printing effect that I'd want it represented in the type signatures. The logic is that I/O in general might interact with other I/O. E.g. if you read a file somewhere in your program then your program's output might be redirected into that file, so it matters what order those two pieces of I/O get sequenced in. FWIW I don't generally consider local I/O worth explicitly sequencing. In code that interacts extensively with the filesystem where the sequencing of file operations is important (e.g. a file manager), I might use an I/O monad for file I/O. And in code that interacts extensively with the console (e.g. a text editor or a console game), I might use an I/O monad for console I/O. But in e.g. a web server I probably wouldn't bother explicitly sequencing local I/O: I'm happy to treat the local filesystem as immutable and read-only, and my logging output as append-only, and I don't particularly care which order messages to stdout are logged in, nor whether they're logged once, ten times, or never. (Of course if you don't care about whether messages get printed, that raises the question of why you're writing code to print them at all - personally I generally don't).
&gt; Anyhow; the point is this is a type class right? &gt; &gt; You arbitrarily associate the interface Foo with any other type, and Foo itself is not a concrete type; you cannot 'have an instance' of Foo; you can only have an instance of T, where T is 'foo-like'. You can only ever have a single implementation of Foo for T. &gt; &gt; Where as... NumberLikeDouble is a case class, it's actually just an adaptor. You can have as many as you want and pick the one you want at runtime. &gt; &gt; It's... a different thing right? Coherence of typeclass instances is a hard problem with no known-good solution (well, possibly ML modules are a solution?). The problem is that we do want to allow typeclass instances where neither the definition of the typeclass interface nor the definition of the value type knows about the typeclass instance. E.g. if we have libgraph and libjson, we want to have a `JsonSerializable[Graph]` typeclass instance, but we don't want everyone who depends on libgraph to have to depend on libjson, and we don't want everyone who depends on libjson to have to depend on libgraph. Really we probably want an "orphan" typeclass instance in a separate libgraph-json, but then what's to stop there being two separate (maybe incompatible) implementations of this? Haskell has a number of different language flags (orphan instances, overlapping instances) that in practice ends up with a similar risk of incoherence to Scala. Rust forbids you from separating out libgraph-json, robbing typeclasses of a lot of their power. &gt; it's not actually meaningfully different from just having a bunch of adaptors in java with some sort of DI container to pick the right one to use... It's resolved at compile time, which is a major advantage in practice even if it isn't in theory. If you try to json-serialize something that's simply impossible to serialize, your code won't compile; if you request a typeclass for which multiple instances exist you will at least get consistent behaviour. Almost all programming languages allow you to do a runtime cast between incompatible types. Does that mean there's no such thing as a typed programming language, that Scala has no type system? I would say no: most Scala code makes minimal use of casts, to the point that you can generally rely on a Scala program not to have type errors. It's the same way with typeclasses: most Scala code provides coherent typeclass instances, to the point that you can generally rely on a Scala program to respect parametricity etc.
Fair enough, then I agree that is safe to assume it wouldn’t work in scala.js
Thanks for the answer!
Perfect. I went with this! Thank you! 
I'd like to add, if you try to write to stdout, and the process that launched you is not reading from that pipe, eventually the kernel buffer will fill out (I think it was 4MB in linux), and you'll be blocked indefinitely. Blocking for ever is a pretty hard side effect.
It is, but it's one that can trivially happen in "pure" code in a language that permits general recursion. 
Traditional OO-style adapters means you convert every item in the List to an adapter, that is, you have `List[Adapter[_]]`. That means runtime type checking and potentially unsafe casting. The reason we setup the type class structure is to get *full* type safety without the use of *any* runtime type checks, casts, etc. In your particular case, what is cumbersome about using an HList? Because if you setup a generic type class for an HList of `NumberLike` items and have a `NumberLike` defined for each of the specific types in your HList, it will just work. Again, with zero runtime type checking or casts.
&gt; Traditional OO-style adapters means you convert every item in the List to an adapter, that is, you have List[Adapter[_]]. &gt; That means runtime type checking and potentially unsafe casting. &gt; The reason we setup the type class structure is to get full type safety without the use of any runtime type checks, casts, etc. Huhwhat? You pass a list of whatever interface you're adapting to, i.e. rather than `List[T]` for some `T: NumberLike` you'd have `List[NumberLike]` (and you'd use implicit conversions to `NumberLike` at the point where you form that list). You still have full type safety with no runtime checks or casts. &gt; In your particular case, what is cumbersome about using an HList? Because if you setup a generic type class for an HList of NumberLike items and have a NumberLike defined for each of the specific types in your HList, it will just work. The type signatures get difficult to work with, and the compiler doesn't always know that the types match. A lot of datastructures have no `HList`-equivalent available.
&gt; Huhwhat? You pass a list of whatever interface you're adapting to, i.e. rather than List[T] for some T: NumberLike you'd have List[NumberLike] And the compiler will perform the type resolution at compile time, but the additional object allocation occurs at runtime. Type classes avoid these allocations entirely. &gt; The type signatures get difficult to work with, and the compiler doesn't always know that the types match. That's fair. Not being built-in, they certainly can get cumbersome in some situations, and when you get it wrong the compiler isn't always super friendly. That's a point I will concede. &gt; A lot of datastructures have no HList-equivalent available. Sure, and in these situations we may have to rely on another construct to achieve our goal. &gt; I find it's very easy to fall into the same thing in Scala if you instinctively reach for type classes as a substitute for all interfaces. Sure. Just like you fall into traps if you reach for *any* tool for *all* potential applications.
&gt; So it is that way because the specification is written that way. But why is it written that way? I.e. what is the technical reason? I'd also like to know that. (In the past, there's been a good reason for little gotchas like this.) Maybe it just came down to pragmatism, but this does create some ambiguity: `with` implies a class when it's used on its own, as in `A with B`, but what I expected happened when making a trait that mixes in `A` and `B`: Welcome to Scala 2.12.7 (OpenJDK 64-Bit Server VM, Java 1.8.0-adoptopenjdk). Type in expressions for evaluation. Or try :help. scala&gt; trait A defined trait A scala&gt; trait B defined trait B scala&gt; trait C extends A with B defined trait C scala&gt; new C &lt;console&gt;:13: error: trait C is abstract; cannot be instantiated new C ^ 
What is the pay like?
This is an all volunteer/community effort. No one gets paid. And, to the best of my knowledge, the site does not generate any revenue. 
Wierd the website makes it seem like it's run as a commercial entity
There are people paying big bucks to get people to teach their engineers idomatic Scala
Your Terms of Service claim you're incorporated. If so then you do have to make money as a company some how. if you don't monetize the mentoring program, then what does Exercism generally do for revenue?
I've been occasionally participating **Exercism** since it came out. I like the platform because it teaches how to perform unit tests while learning a new language. I don't have to worry about which test framework is popular/standard and what a standard way of running a test is. The only weakness is that most of the problems are too easy and it's almost impossible to get feedbacks for individual problems that are not on the language *track*. 
My best guess is they're tying to plug into the boot camp/recruitment industry and take a cut.on referrals. They have a link to a boot camp section but it leads to a dead nginx route
Do you mean a mentee couldn't ask me for other career advice? More evidence this product is part of the referral pipeline. If I'm a mentor I would want to offer my good mentees jobs. 
What career advice? I think you completely got it wrong. Exorcism is just a coding platform like leetcode / hackerrank but with mentoring. It's called mentoring, but there is no requirement to be a mentor. So, it's just like leaving a comment, sharing/discussing better algorithms
I'm unfamiliar with either of those coding platforms. Though, what you are describing sounds more like a TA or a grader. I was using this definition of mentor: https://www.merriam-webster.com/dictionary/mentor
What did you mean by "The only weakness is that most of the problems are too easy and it's almost impossible to get feedbacks for individual problems that are not on the language track." If not something like career advice? 
I see why you're not getting it. Have you experienced any technical interviews though? Since you have not experienced such coding platforms, the platform like exercism/leetcode provides short algorithm questions. Some people do it for fun or to prepare interviews or to prepare algorithm competitions, and so on. What makes Exercism different from others like Hackerrank/LeetCode is that people can leave a comment like how it could be written better and you can run locally. The other 2 platforms you need to submit online and pass their test cases. Well, doing is better than saying. Just go try it.
Sound like the kind of thing you learn during a 3am PagerDuty adventure
Any insight why I barely heard about colossus? Anyone has experience using and compare it with more popular Play/Scalatra/Http4s?
You are the best! Thank you
Thanks!
It is difficult to get feedback on the non "core" exercises due to the low number of mentors. The mentors are spending their time reviewing the "core" exercises which are considered higher priority for review. The hope is that as the mentor pool grows, then feedback will be quicker for all the exercises.
Amazing!!
It is similar to contributing to an open source project, or contributing an answer to stack overflow. People contribute for a variety of reasons. The payoff doesn't always have to be about monetary compensation.
&gt;I think there are cases that suit both, and we do need to go into the messy details of what advantages typclasses offer over adapters and when one or the other is more appropriate. That screams for an in-depth article.
I like the concept and I registered as a mentor. I have an open PR on the mentor repo and I already reviewed a few solution. Thanks for hosting this platform ! 
I haven't used Lagom but I've been working with microservices for a while. I, however, like the way Lagom works, and I think it could be helpful for someone who is starting, but not for those who already developed a stack. Here are my comments: 1. Lagom is a framework and not a library, which means that you have to do things the way the framework expects. This could be a bit limiting but overall leads to code consistency. Another drawback of relying on Lagom is that you're tied to one language for most of the project. Some people tend to use the argument that using multiple languages seamlessly is an advantage of microservices. It is. But adding more languages makes the code more complex and inconsistent across the board. So you should try your best to stick to one language, in which case Lagom will be just fine. 2. CQRS is generally recommended but not absolutely necessary. Adopting it is a design decision that you have to make for your own project. 3. Even sourcing is also pretty useful, and some people go to the extent that if you don't have events then you don't have microservices. I don't really agree with that. It's still a design decision you have to make. 4. The most difficult part, in my opinion, is handling failure when a service is not responding for whatever reason. And just like everything else, your strategy for handling that depends on the project and the particular relation between the two services in question. Finally, a piece of advice, learn DDD before jumping into Lagom. It should make your life easier and it's also pretty easy and straightforward to get into.
Use Lagom. It saved us a lot of time...
Ah, the future of Scala, just don’t make any promises ...
I think HighHeathenHammer has some great points, here's my take and personal experience on working with Lagom for the last \~6 months. I've never worked with monix so can't say much about that. We had never built a CQRS/ES system so we decided to go with Lagom specifically for guidance on what best practices are and so far I'm grateful we did not roll our own solution. There is freedom in constraint. 1. Personally I have not found it to be restrictive - my team has yet to hit a wall where we need to work around Lagom. 2. We're building a financial system, auditing and building custom reports is crucial so event sourcing definitely makes that aspect better. There is definitely a learning curve when coming from a CRUD mentality though. 3. When designing your services keep in mind that there will be eventual consistency delays when working with kafka topics, it may only be 10 seconds but if you're sending time based otp's those 10 seconds could be an issue, you need to really think about each service and it's responsibility. 4. We deploy to GCE. &amp;#x200B; Hope that helps.
Slick has a pretty great write up on this. You can see your question answered in the relationships section: http://slick.lightbend.com/doc/2.1.0/orm-to-slick.html You're using Quill which is more low level than slick. I believe you'll want to describe your joins more manually. I tend to prefer that because ORMs aren't the best query generators in terms of performance. For what you're used to, slick is the closest representation. But I'd consider using Quill's (or doobie's) more low level approach instead. It has led to a lot less complexity in my experience. 
Just like the shift in paradigm you have to take from OO/Procedural programming to functional programming, you'll need to look at the problem with a different perspective. I think the issue you are having is the O-R impedance mismatch that you have been used to is no longer present and you can't pretend you are persisting an object. Slick maps the relational model more closely while giving you a typed, immutable way to work on your tables. I had the same struggle initially but once you make that shift, it's not all that bad. Also you can create dao and service layers that allow you to work on an object graph in your service by combining slick calls to construct your graph. http://slick.lightbend.com/doc/3.0.0/orm-to-slick.html
[removed]
\&gt; I now, I can "simulate" the ORM by pulling the objects on my own and building the model tree by myself, but I guess, this goes against the paradigm. Does it? You have a layer which reads from DB and constructs model. whether its your own ORM or someone else's..
I don't get it either. For applications without complex queries, JPA/Hibernate seems like godsend to me. :) Even if you do have complex queries, why not use SQL views or functions? OTOH, in Slick you do a sorted query, get flat results (rows), group them to get some meaningful objects, and voila, you gotta sort it again..? Are we missing something? 
Avoid microservices altogether. For modularity use `roles` pattern, see: https://github.com/7mind/slides/raw/master/02-roles/target/roles.pdf
It's a library for ad-hoc polymorphism and typed abstractions for a language that has no sane ways to model ad-hoc polymorphism and has quite weak types. It's quite useless, IMHO. I've tried to use it casually, but it would also completely destroy intellij IDEA and force hangs due to high amount of extension methods even when trying to implement simple things, like [Day convolution](https://gist.github.com/kaishh/04dccfc82fa6a23618036dd2ead0c507)
If referential transparency doesn't convince them, then I can't see a way forward. The notion that you can reason about your code in a RT manner is The Point. It's not just good for stroking your functional ego. It helps with reducing the burden of maintance and documentation, as well as the legibility of your code, which affects the bottom line. If you are not writing purely functional code, you are _wasting money_. Additionally if you were to adopt `F[_]: Effect`, as opposed to just `IO`, you would have the added benefit of being able to easily drop in various effect systems `zio.Task`, `monix.Task`, `cats.IO`, etc, and be able to immediately gauge the performance impact of each on your systems. This saves heaps of money and rending-of-clothing when you enter into the performance testing phase of your project. Both of those notions combined lead to a style of coding (typeclass-driven, algebraic) that is known to be consistently more performant than the ad-hoc solutions people come up with in alternatives, as well as self-documenting. If none of that sells them, jump ship and find a modern Scala company to work for. There's boatloads.
if someone ends up doing this, please share here too because i'd also like to know how this is different / better than vanilla 
I've just started work on Scala/Graal polyglot integration. The [list of features --complete and planned--](https://github.com/japgolly/scala-graal) is in the [repo](https://github.com/japgolly/scala-graal). My main "real-world" purpose is to have really fast (~1ms) React SSR (server-side rendering) in the JVM. As usual (like test-state), I'll be building this up in layers that are abstract and reusable outside of just SSR. I'm quite excited about this and it feels fun. Contributions welcome! *(Also, Twitter discussion: [here](https://twitter.com/japgolly/status/1058657919254679552))*
It depends on whether you think the _phrase_ "referential transparency" or _what referential transparency implies_ won't convince them. The former is understandable, at least _ceteris paribus_. "So I could replace expressions with their definitions without changing the meaning of the program. Why on earth would I want to do that?" It's a fair question. But it has an answer: "Because it means no matter how far in or how far out you zoom on the code, it's comprehensible by using a very small handful of rules, all of which have to do with how composition works. Composition of pure functions, composition of functions with effects, all of it. By the way, 'effects' includes not only I/O, but exception handling and concurrency, too." Let's look at a straightforward, but real-world, example: [Listing keys in an Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/ListingObjectKeysUsingJava.html). Here's my `build.sbt`: ```scala name := "delete" scalaVersion := "2.12.7" libraryDependencies ++= Vector( "co.fs2" %% "fs2-core" % "1.0.0", "com.amazonaws" % "aws-java-sdk-s3" % "1.11.442" ) ``` and here's my code: ```scala import scala.collection.JavaConverters._ import cats.effect.IO import fs2._ import com.amazonaws.auth.profile.ProfileCredentialsProvider import com.amazonaws.client.builder.AwsClientBuilder.EndpointConfiguration import com.amazonaws.services.s3.{ AmazonS3, AmazonS3ClientBuilder } import com.amazonaws.services.s3.model.{ ListObjectsV2Request, ListObjectsV2Result, S3ObjectSummary } object example { def getObjectSummaries(s3: AmazonS3)(bucketName: String): Stream[IO, S3ObjectSummary] = { val req = new ListObjectsV2Request().withBucketName(bucketName).withMaxKeys(2) def cont(res: ListObjectsV2Result): Option[String] = if (res.isTruncated) Some(res.getNextContinuationToken) else None def next(maybe: Option[String]): IO[Option[(ListObjectsV2Result, Option[String])]] = { IO { maybe.map { token =&gt; val res = s3.listObjectsV2(req.withContinuationToken(token)) (res, cont(res)) } } } val ress = Stream.eval(IO(s3.listObjectsV2(req))).flatMap { res =&gt; Stream.emit(res) ++ Stream.unfoldEval(cont(res))(next) } ress.flatMap(res =&gt; Stream.emits(res.getObjectSummaries.asScala)) } def main(args: Array[String]): Unit = { val h = Stream.emit("Listing objects") to Sink.lines[IO](Console.out) val l = for { s &lt;- Stream.bracket(IO { AmazonS3ClientBuilder.standard() .withEndpointConfiguration(new EndpointConfiguration("http://localhost:4572", "us-west-2")) .withPathStyleAccessEnabled(true) .build() })( s3 =&gt; IO(s3.shutdown()) ).flatMap { s3 =&gt; s3.createBucket("test-bucket") getObjectSummaries(s3)("test-bucket") } } yield s val o = l.flatMap(os =&gt; Stream.emit(s" - ${os.getKey()} (size: ${os.getSize()})\n") to Sink.lines[IO](Console.out)) val p = h ++ o p.compile.drain.unsafeRunSync } } ``` The code might be a bit idiosyncratic, given that I knocked it out quickly. Please ask any questions about it you may have. A few notes: 1. You can actually build it and run it against [LocalStack](https://localstack.cloud). 2. If you do, it dutifully prints "Listing objects" and then nothing, because it creates the `test-bucket`, which is then empty. :-) 2. By simply removing the `.withEndpointConfiguration` you should be able to run it against real AWS. 3. By changing the name of the bucket and removing the `s3.createBucket("test-bucket")` you should be able to list the contents of a real bucket. 4. I deliberately show writing to stdout with `Stream`. 5. I deliberately show building sequences of effects with `++`. 6. I deliberately show properly allocating and releasing the `AmazonS3` with `bracket`. 7. I deliberately don't bother handling failures I can't do anything about. The point, again, is the whole program is built up algebraically, so I can look at any expression and understand it in isolation, or look at any composition of expressions (e.g. `val p = h ++ o`, literally "the program `p` consists of printing the `h`eader followed by printing the `o`bject summaries' keys and their sizes") and understand it the same way. Again, feel free to ask questions, offer comments, etc. :-)
If only Scala were a language where MTL style was actually reasonably idiomatic and/or readable. Alas, Odersky seems to think he's found the True Answer via implicits. Oh, well.
I started the MOOC and everything is greed out for me in the syllabus section 
Awesome. Welcome aboard.
I recently gave a talk about Cats Effect and got good feedback. People that were not familiar with these concepts or people that knew how to use `IO` but didn't have a clear vision of the whole picture finally got it. Here are the slides but it probably won't make much sense until the video is out which I believe it'll be soon :) https://paidy.github.io/talks/scalaio2018/ TL;DR: Tagless Final + MTL + IO is the way forward :)
Question for ya with respect to this code: val generateChart: IO[Unit] = for { b &lt;- getBandsFromFile // IO[List[Band]] _ &lt;- IO.race(longProcess1(b), longProcess2(b)) id &lt;- generateId _ &lt;- publishRadioChart(id, SortedSet(b: _*)) _ &lt;- publishTvChart(id, SortedSet(b: _*)) } yield () I've only been writing Scala for 11 months or so, and I'm still adjusting to its patterns. Why is the above preferred so much over something like this? val b = getBandsFromFile Future.sequence(Seq(longProcess1(b), longProcess2(b)) onComplete { case Success(_) =&gt; val id = generateId publishRadioChart(id, SortedSet(b: _*) publishTvChart(id, SortedSet(b: _*) }
It'll be clearer when the video comes out where I explain about it following the slides but I'll make an effort to reply to your immediate question. TL;DR: Future is not referentially transparent (RT) and this is the key to doing Functional Programming (FP). When you have a program of type `IO[A]` all you have is a description of one or many computations, just a pure value that can be assigned to a variable or be passed around as a parameter. When you have a program of type `Future[A]` the same property does not hold true because Future is eager and once you create it it will be immediately running. This means you can't really do the same as you were able to do with a pure value such as `IO[A]`. Being able to fearlessly refactor your code, extract relevant pieces into different methods or files, is an incredibly powerful property. And this property only holds when your code is RT. In addition, it gives you **local reasoning**, meaning that by understanding the pieces you can understand the whole. Going further, the logic and interpretation are highly coupled making it impossible to test. But it's the same with `IO[Unit]`. And here's where `Tagless Final + MTL` comes into scope. If you look at the slides I start with this program written in terms of `IO` to later refactoring into algebras, programs and interpreters. Hope that's clear :)
Thanks for sharing a real-world code, that's refreshing and maybe what I was missing. I see that you use fs2 - do you use IO without it?
Thanks! looking forward for the video
I refactored an commented my code, and came up with this. I think it reads quite well now: ```scala import scala.collection.JavaConverters._ import cats.effect.IO import cats.implicits._ import fs2._ import com.amazonaws.client.builder.AwsClientBuilder.EndpointConfiguration import com.amazonaws.services.s3.{ AmazonS3, AmazonS3ClientBuilder } import com.amazonaws.services.s3.model.{ ListObjectsV2Request, ListObjectsV2Result, S3ObjectSummary } object example { def getObjectSummaries(s3: AmazonS3)(bucketName: String): Stream[IO, S3ObjectSummary] = { // Initial request, and the basis of continuation requests val req = new ListObjectsV2Request().withBucketName(bucketName).withMaxKeys(2) // Return the continuation token of the result, if any def cont(res: ListObjectsV2Result): Option[String] = if (res.isTruncated) Some(res.getNextContinuationToken) else None // Makes a continuation request if necessary, and indicates if ANOTHER is def next(maybe: Option[String]): IO[Option[(ListObjectsV2Result, Option[String])]] = { IO { maybe.map { token =&gt; val res = s3.listObjectsV2(req.withContinuationToken(token)) (res, cont(res)) } } } // A stream of all the `ListObjectsV2Result`s, with pagination val ress = for { res &lt;- Stream.eval(IO(s3.listObjectsV2(req))) all &lt;- Stream.emit(res) ++ Stream.unfoldEval(cont(res))(next) } yield all // A stream of the object summaries for { res &lt;- ress oss &lt;- Stream.emits(res.getObjectSummaries.asScala) } yield oss } // Construct an S3 client, which is an effect def s3Client: IO[AmazonS3] = IO { AmazonS3ClientBuilder.standard() .withEndpointConfiguration(new EndpointConfiguration("http://localhost:4572", "us-west-2")) .withPathStyleAccessEnabled(true) .build() } // Close an S3 client, which is an effect def closeS3Client(s3: AmazonS3): IO[Unit] = IO(s3.shutdown()) def main(args: Array[String]): Unit = { // A stream that has the effect of printing the header val h = Stream.emit("Listing objects").covary[IO] to Sink.showLinesStdOut[IO, String] /* A stream that has the effects of constructing the S3 client, creating the S3 bucket, getting the object summaries, and closing the S3 client safely */ val oss = for { s3 &lt;- Stream.bracket(s3Client)(closeS3Client) _ &lt;- Stream.eval(IO(s3.createBucket("test-bucket"))) s &lt;- getObjectSummaries(s3)("test-bucket") } yield s // A stream that has the effect of printing the key and size of each object summary val o = for { os &lt;- oss _ &lt;- Stream.emit(s" - ${os.getKey()} (size: ${os.getSize()})").covary[IO] to Sink.showLinesStdOut[IO, String] } yield () // A stream that has the effect of printing the header and key/size of each object summary val p = h ++ o /* Compile the stream to something we can construct an `IO` from, construct the `IO` by ignoring the stream's values, and run the `IO` synchronously, possibly throwing exceptions, which we ignore. */ p.compile.drain.unsafeRunSync } } ```
MTL style is not even reasonable in Haskell, it's a noobtrap that get's thrown out of projects faster than it gets in, because `n*k` transformer forwarding instances scales to like 10 algebras max.
I just did a talk on this. I come from a background of building applications with `Future`. Here are the benefits... No implicit queueing - If your program does a lot of map, flatMap combinations (for comprehensions), recall that every time one of those things is called we are actually creating an object that is run on a separate thread pool (potentially). If you are handling a lot of load, this will increase memory pressure, potentially resulting in an OOM for your application. With IO, you need to explicitly declare concurrency, so you are in full control of how that works (another talk will cover concurrency in IO). Predictable profiling - with Future, your for comprehensions map flatMap chains can be nested. When the onComplete call back is actually executed is non-deterministic. This makes profiling your application extremely difficult, and at times impossible. With IO, you can wrap your program (or an individual IO) and fully understand when it starts and when it stops, as there is no callback chain that the underlying machinery sorts out for you. Push your effects to the edges With Future - you have to think imperatively through your code, when your expression is evaluated, it is evaluated eagerly. With IO, you are truly "building programs", and then execute them "at the end of the world" near the edge of your system. Eliminate implicit ExecutionContext - If you have a Future based application, you know the pain of carrying implicit execution with you. With IO, you can get rid of all of that. Your application is already largely synchronous!! If you are using for comprehensions, map, flatMap with Future, your program is already synchronous. There is no need to be executing all these things on a separate thread pool. No more unexpected timeout errors in your Future tests - If you have seen seemingly sporadic timeout errors in your tests, they can be difficult to stomp out, and usually involve increasing the timeout to a possibly ridiculous value. With IO, all your tests can safely just .unsafeRunSync, not more unpredictable timeouts! 
I too, went through this struggle of being used to an ORM that takes care of fetching relationships automatically, and I ended up using plain SQL query and building those relationships manually. It’s more work, but it’s also far more customizable and performant, especially when you have complex relationships. I have a case class for all my entities, and my repository methods (e.g. ‘getById(...)’ have an optional ‘include: Seq[String]’ argument which takes care of fetching additional dependencies. I have one DTO for each entity that can be persisted/modified and a repository method that expects such dto as argument (e.g. ‘editCustomer(customerId: Long, dto: CustomerDTO)’ Again, it’s definitely more work, especially for writes but so far I’m liking it. It took me a long time to unlearn the ORM-way of doing things and appreciate the alternative of not using one.
Your class should be located in `src/main/scala/com/mypacakge` And not in `src/main/scala/com.mypacakge` &amp;#x200B;
Interesting, I came from the Java world as well and couldn’t wait to ditch Maven. SBT is much easier! For a simple project you really don’t need to do much config. 
I've got engaged in a project where it's used.
That is the folder structure which I use. Intellij displays package directories with a '.' on default.
How is mill going? Is it production ready? Are all plugins there?
That’s kind of what I was expecting. I’d be a bit cheeky and push for a change to SBT if I was you.
I still bump into the same issue I even simplified the project and put my main class MyClass.scala into src/main/scala directory. And changed the pom.xml accordingly. &amp;#x200B; `&lt;plugin&gt;` `&lt;!-- Build an executable JAR --&gt;` `&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;` `&lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;` `&lt;version&gt;3.1.0&lt;/version&gt;` `&lt;configuration&gt;` `&lt;archive&gt;` `&lt;manifest&gt;` `&lt;addClasspath&gt;true&lt;/addClasspath&gt;` `&lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;` `&lt;mainClass&gt;MyClass&lt;/mainClass&gt;` `&lt;/manifest&gt;` `&lt;/archive&gt;` `&lt;/configuration&gt;` `&lt;/plugin&gt;`
Very useful comment thanks ! If you want to be awarded as answer on stack overflow, copy past your comment and I will accept it as an answer Thanks for your time 
MTL is only slightly more bearable in Haskell, but both languages have the exact same fundamental issues with MTL, i.e. n\^2 boilerplate for every type on your MTL stack.
Can you put on github your project?
May I ask what you think of [this](https://www.reddit.com/r/scala/comments/9txltc/how_to_sell_catsio_to_my_team/e90yazs)? My experience has been that "getting the next dev to support my code going" as quickly as possible is a false economy. It's worth pulling back a bit and gaining the ability to zoom in or out to any level of detail you want and be able to understand the code with a _very_ small set of intellectual tools. Let's just take the heart of `getObjectSummaries` at the link I offered, which I'll argue is this: // A stream of all the `ListObjectsV2Result`s, with pagination val ress = for { res &lt;- Stream.eval(IO(s3.listObjectsV2(req))) all &lt;- Stream.emit(res) ++ Stream.unfoldEval(cont(res))(next) } yield all Let's assume you know the Amazon S3 SDK's `listObjectsV2` call is paginated, so you need to check to see if the result of the call is "truncated" or not and, if so, keep calling `listObjectsV2` with the same request, but updated with the "continuation toke" from the last result, until you get a result that's no longer truncated. Knowing nothing whatsoever about fs2, my bet is a reasonably thoughtful developer can look at the above and think: 1. I'll bet `res` is the "result" of that `listObjectsV2` call. Dunno about this `Stream.eval` or `IO` stuff, but whatever. Maybe it's only there so this can be in that for-comprehension? 2. OK, so... it looks like we're constructing a `Stream` with the result we already got, and then... somehow, `cont` determines whether there's more to get based on that result, `next` probably gets the next result somehow, and this `unfoldEval` somehow repeats that? Does that make sense? (Reasonably thoughtful developer looks up the ScalaDocs for `Stream.unfoldEval`, maybe. 3. Anyway, a `Stream` sounds reasonably intuitive, except we seem to be making arbitrary API calls, like to S3, and building elements of `Stream`s that way? 4. I guess it really is like any other sequence in some ways. Like, it looks like `++` means "append," just like I'm used to. 5. So let's see... hmmm. That `Stream.eval` builds a `Stream` with one result. If I understand correctly, that `Stream.unfoldEval` builds a `Stream` of zero or more results, based on what `cont` on the first result says, and every result we get from `next`. So `all` must be a `Stream` of one or more results. 6. If I'm right about this, that's a really easy way to make pagination work! But I wonder how `next` actually works? Well, that's easy: // Makes a continuation request if necessary, and indicates if ANOTHER is def next(maybe: Option[String]): IO[Option[(ListObjectsV2Result, Option[String])]] = { IO { maybe.map { token =&gt; val res = s3.listObjectsV2(req.withContinuationToken(token)) (res, cont(res)) } } } If given a continuation token, we make the request with that token, and return both the result and the _next_ continuation token, if there is one. Talking to AWS is an effect, so we do it in `IO`. You can easily guess what `cont` does, but here it is anyway: // Return the continuation token of the result, if any def cont(res: ListObjectsV2Result): Option[String] = if (res.isTruncated) Some(res.getNextContinuationToken) else None It's not even interesting. It's just a convenience function. OK, let's look at another snippet: // A stream that has the effect of printing the header val h = Stream.emit("Listing objects").covary[IO] to Sink.showLinesStdOut[IO, String] I'll be the first person to admit this takes some getting used to. Here we're hardcore treating an effect—printing a `String`—as a value, so we can say: // A stream that has the effect of printing the header and key/size of each object summary val p = h ++ o And `++` still means "append." It just means "append whatever `Stream` `o` does/emits to whatever `Stream` `h` does/emits," which we just saw is "Print 'Listing objects'." So when we actually run this, we know `p` will print "Listing objects," then whatever `o` does will happen. We can literally do this with any of the code. We can pick an expression—_any_ expression—and understand it solely in terms of what values go into it, what values come out of it, and what operations are available on it, like `++`. We can think about things like how many values something will deal with, what kinds of values they are, etc. We can have the assurance the values won't change out from under us. We can use powerful tools for "doing things, including talk to the network, repeatedly, until some condition is met" in one expression, and that expression can still be used in the usual way with other expressions. To be fair, there's a learning curve for all of this. But I hope this gives some ideas about why that might be worth it. Because our ability to understand our code, no ifs, ands, or buts, at any level of detail we want, dramatically increases our confidence in our code by equally dramatically increasing our understanding and reducing our defect rate.
I'm not affiliated, and am not watching super closely. But my experience with it so far has been very positive, and I like it a lot more than anything I've used in a long time. My sense is that it's a build tool written by someone who just wants a workhorse build tool, and with no other real agenda. A lot of what's plugins with sbt are in the core of mill (e.g. publishing to maven/ivy repos). And the documentation is a lot more user focused - I could never stand all the navel gazing in sbt's docs! The only thing that's stopping me from switching projects over wholesale is that the last time I checked there's not support yet for code coverage tools. A friend/colleague actually tried to implement it, but got tangled up in classloader problems that he couldn't resolve and the author wasn't (at least at the time) super interested in helping with. As soon as there's support for scoverage I'll be moving over.
Note that you don't really need to use Monad Transformers when using MTL. Eg: you can parameterize your functions with `MonadState`, `ApplicativeAsk`, etc and the edge of your program (main) you can just provide the instances for your type (eg. `IO`), that's what I've been doing so far and the performance is great. There's a nice talk called `A road trip with Monads` by Pawel Szulc that I really recommend watching if you're not familiar with these concepts.
I appreciate how much time you've taken to write all of this out. I've gone and done some more reading about fs2 and cats-effect, as well. I know this may prove frustrating, given how much time you've taken, but I'm still not convinced by your code or what I've read to look into adopting these libraries. Maybe it's because I'm too dumb to "get it," or maybe it's because I feel like I can do exactly what you're describing in statements like this with vanilla Scala: &gt; We can literally do this with any of the code. We can pick an expression—any expression—and understand it solely in terms of what values go into it, what values come out of it, and what operations are available on it, like ++. We can think about things like how many values something will deal with, what kinds of values they are, etc. We can have the assurance the values won't change out from under us. We can use powerful tools for "doing things, including talk to the network, repeatedly, until some condition is met" in one expression, and that expression can still be used in the usual way with other expressions. Maybe it's because I can't see how I can use these in my job. I work heavily with concurrency, Spark, building data pipelines and loaders. As an example, I may need to make a ton of requests within a bounded execution context, have them each do a thing, and then do a big final thing at the end. And have that happen all day every day. To me, a very simple version of that is: val futures = sendAllTheseComputeRequests(reqs) Future.sequence(futures) onComplete { case Success(results) =&gt; doFinalStuff(results) ... } To this day I haven't seen an example of something like this in any fs2/cats-effect stuff.I believe Monix Task can do this kind of sequencing, but it seems to not be enough of an improvement to justify the change. fs2 and cats-effect, as seemingly dominant as they are within Scala, have 1200 and 400 stars on github. While I realize that it's somewhat silly to cast judgment on that basis, what it tells me is that all of the vocal members of the Scala/FP community do not represent that large of a group. Whenever I look at fs2/cats-effect code, I can't help thinking it's messier looking code for FP code philosophers.
Your object will be written to a class whose name ends with "$". So you must adapt the class name in your manifest.
10 years using scala here, been through endless blogs, videos, books, people that came and went, drama, some haskell, some idris, early days of akka, cake pattern, virtualized scala and endless things in between. I still don't buy cats nor scalaz. My conclusion is that Scala (and to a degree, the JVM and its ecosystem) is not the place for that.
I [wrote about this](https://blog.bruchez.name/2013/02/mapmap-vs-mapmapvalues.html) a long time ago. I am not sure it is very useful to know why, as the collections are being overhauled for Scala 2.13. In particular, this specific issues is [addressed in 2.13 by returning a `MapView`](https://www.scala-lang.org/blog/2018/06/13/scala-213-collections.html#groupmap).
My apologies. I guess this was because I set the date of course start to 1 December 2018. I changed it now to 1 November. The only available now section is "Boolean type". Does it work for you now (video and assignments)?
Isn't the difference between TF and MTL - the usage of monad transformers and MonadTrans class? Creating incoherent instances at runtime is great, but it's not really MTL, it's just dependency injection.
Thanks. These are some good points
&gt;results What about using `parSequence` with `attempt` ? For example: `import cats.implicits._` `import cats.effect.IO` `implicit val cs: ContextShift[IO] = IOContextShift.global // Or create another with IO.contextShift` `val tasks: List[IO[Int]] = ???` `val sequence: IO[List[Int]] = tasks.parSequence // Or .sequence if you want to go sequentially` `sequence` `.attempt // To transform into Either` `.map {` `case Right(results) =&gt; ???` `case Left(e) =&gt; ???` `}` &amp;#x200B;
If you can't find a good argument on how these are better than futures - why sell it in first place? 
Not really. `Tagless final` is about the encoding of `DSL`s: `initial` (GADTs) vs `final` (algebras with possibly many interpreters). `MTL` is linked to Monad Transformers because of its historical name but it doesn't have to be related to them. Providing an instance for any MTL typeclass at the edge of your program doesn't allow incoherence as you can only have a single implicit instance in scope. The idea of doing it this way instead of using Monad Transformers is to mainly have a performance boost as well as to have the simplicity of working with a single type such as `IO`.
&gt; Providing an instance for any MTL typeclass at the edge of your program doesn't allow incoherence as you can only have a single implicit instance in scope. It's still about creating instances effectfully, which is using capabilities that haskell doesn't have (even with reflection, sans Given), which means it's fundamentally a different style than the original. I get that you're talking about MTL classes, Reader, State, etc. instead of the style of composing transformers that each a provide a separate instance, as in [here](https://github.com/lexi-lambda/mtl-style-example/blob/master/test-suite/MTLStyleExample/Test/Stubs.hs#L18) for example. But it's the latter that's usually referred to as mtl style. And as I vehemently dislike the latter style, I must try to quench all mentions of it, or some beginner might google for it, find some old haskell tutorials and end up with a mess of monad transformers. :^)
Honestly, I'm not sure. I haven't looked closely enough at the `Fiber` and cancellation machinery to be completely confident. But I do see that `IO` 1.0.0 has `race`, and I don't see evidence of Daniel Spiewak blowing a gasket, so I'd say the prognosis is good. :-)
Thanks for this! There's a lot here to chew on. I'll try to be brief, but unfortunately that necessarily means I'll have to cut some corners and risk still not coming up with something satisfactory. Please bear with me just a bit longer. &gt; I know this may prove frustrating, given how much time you've taken, but I'm still not convinced by your code or what I've read to look into adopting these libraries. Maybe it's because I'm too dumb to "get it..." That's clearly not the case. &gt; maybe it's because I feel like I can do exactly what you're describing in statements like this with vanilla Scala: &gt; &gt; We can literally do this with any of the code. We can pick an expression—any expression—and understand it solely in terms of what values go into it, what values come out of it, and what operations are available on it, like ++.
But `MapView` is still lazy, isn't it? I'm still in the 2.12 world. My current way of dealing with the problem, is addind a `mapVals` as extension method, which is implemented as strict version of `mapValues`. BTW, `filterKeys` is a lazy too. BTW#2: https://issues.scala-lang.org/browse/SI-4776
If `MapView` is lazy it's still bad. Scala's `Map` is mostly strict. Making certain operations lazy is extremely confusing. If someone wants lazy, the should call `.view` like they would on `List`.
I checked and 2.13 includes a deprecation warning that you should use \`view.mapValues\` if you want lazy evaluation, and furthermore \`mapValues\` will become strict in a later version. This isn't what I'd prefer, but it's better than keeping it lazy forever. [https://github.com/scala/scala/blob/ba9701059216c629410f4f23a2175d20ad62484b/src/library/scala/collection/Map.scala#L211](https://github.com/scala/scala/blob/ba9701059216c629410f4f23a2175d20ad62484b/src/library/scala/collection/Map.scala#L211)
I opened this, so hopefully the @deprecated doesn't go on forever. [https://github.com/scala/bug/issues/11245](https://github.com/scala/bug/issues/11245)
This is exactly how it will be done going forward in 2.13 (`mapValues` and `filterkeys` are deprecated and point towards using `.view.mapValues` and `.view.filterKeys` instead -- their strtict counterparts will replace them in upcoming version, like 2.14)
Thank you for this transcription. I quit in the middle of his talk because the video/sound quality was really awful. Anyway, does anyone know if one should always have some (case) class that represents the state or are there "smarter" ways of handling state in a real-world pure FP apps? How this relates to State or StateT monad?
I would not map Slick directly against Play. Instead, have a DAO layer that you work with through the controller, and use Slick as the backend implementation. This means you can write stubs easily without involving the database by swapping out the implementation, and you can keep your data isolated. There's a Play example here. https://github.com/playframework/play-scala-isolated-slick-example If you want to take it another layer, you should add presentation objects into the mix -- DTOs that are explicitly designed to map to forms / rendering, and have the controller work against the presentation objects. That is, if you've got a table you need to render, then you've got a case class which is `Table`, and you pass that `Table` into the template and have it render cleanly exactly as the template needs. You may have several queries going on in the backend that map DTOs to your data retrieval service, but that isn't your controller's concern. Your controller only cares about parsing form and parameter input and rendering pages.
I intend to do a similar thing for XML - this might prove to be useful inspiration, thanks.
I solved the issue and explained it in the post. If you have any remarks, go ahead. Thanks for the willingness to help.
What is the best IDE for Scala? I've been using IntelliJ for so long that I don't know all the options out there. It seems a bit weird that Scala critically depends on IntelliJ (as in I can't write Scala without IntelliJ), but IntelliJ creates Kotlin, which is somewhat a Scala competitor. I'm not assuming evil intent on any side. It just seems vulnerable in terms of strategy.
For now only Intellij, but the landscape is starting to change now - Scala 3.0 will include a LSP, on 2.x branch there is work being done on Metals, both of these solution will probably converge somehow in 3.0.
It works now. thanks!
The part with implicit Execution contexts is slightly untrue, since most libs out there will have you carry ContextShift, for instance. So you replace one implicit with a more flexible one.
I recognize you, you pure haskeller you ;)
what about slides?
Let me see if it is ok if I share some other material. Should be fine.
Thanks, looking forward to it. 
This is absolutely true. For things like parMapN, you need it. We only use those in a few places. I still have to carry ContextShift and to a lesser extent Timer. For the purposes of converting from Future to IO for our use case the reduction in implicits was dramatic and definitely a highlight, but like you mention not eliminated all together.
Here's what the output looks like when I run assembly. (fyi, the dependency in question is neo4j) [error] deduplicate: different file contents found in the following: [error] C:\Users\me\.ivy2\cache\org.neo4j\neo4j-kernel\jars\neo4j-kernel-3.2.3.jar:META-INF/services/org.neo4j.kernel.extension.KernelExtensionFactory [error] C:\Users\me\.ivy2\cache\org.neo4j\neo4j-lucene-index\jars\neo4j-lucene-index-3.2.3.jar:META-INF/services/org.neo4j.kernel.extension.KernelExtensionFactory [error] C:\Users\me\.ivy2\cache\org.neo4j\neo4j-udc\jars\neo4j-udc-3.2.3.jar:META-INF/services/org.neo4j.kernel.extension.KernelExtensionFactory [error] C:\Users\me\.ivy2\cache\org.neo4j\neo4j-jmx\jars\neo4j-jmx-3.2.3.jar:META-INF/services/org.neo4j.kernel.extension.KernelExtensionFactory So there are 4 classes named "KernelExtensionFactory" which exist in 4 different jars. The one I want to include is the second on that list of 4 (C:\Users\me\.ivy2\cache\org.neo4j\neo4j-lucene-index\jars\neo4j-lucene-index-3.2.3.jar:META-INF/services/org.neo4j.kernel.extension.KernelExtensionFactory) I agree that it's strange assembly doesn't understand these are separate files, but evidence has led me to believe that this is the case.
Could you exclude the other dependencies? Short of that you’ll have to write a merge strategy.
Seems the process part is not yet scala native portable?
Yeah I encountered this weirdness as well because I was trying to keep a Map of function pointers and use mapValues. I just wanna say is that even though what they have is confusing, if they had the implementation we were expecting they’d get criticised for pushing people down some poor performing path.
If I write a test suite using Specs 2 and type “sbt test”, how many threads does the code use? Can it fail or deadlock? How do I go about debugging randomly failing tests that only time out when the test suite is running?
I've used Lagom for a little over a year now so have some more to add. &gt;Lagom is a framework and not a library, which means that you have to do things the way the framework expects. This could be a bit limiting but overall leads to code consistency This is definitely the case. We've encountered a few times where we need to stray from the "lagom way" of doing things with some rather unexpected consequences. &gt;Another drawback of relying on Lagom is that you're tied to one language for most of the project. There is actually pretty good interop with non-lagom stuff. All you need to do is write the service description of anything in another tech and then your lagom services can interact with it.
You can't write polymorphic code that would work when interpreted in `IO` when you're 'just running it'/ 'interpreting into `Id`'. This style encourages unrestricted side-effects and creates confusion because you don't have to chain operations with `flatMap` to apply the effects. For people unfamilliar with FP, this results in imperative statements being used and monadic constructs being ignored. This happened on one of my previous team, they used `Either` for typed errors + some tagless final, but otherwise their code had unrestricted side-effects and statement blocks everywhere and was hard to migrate to IO. Interpreting into a strict monad / `Id` is nonsensical in presence of side-effects, it breaks reasoning by making `val x = opA(); opA() *&gt; opB()` have completely different effect when interpreted into a strict monad vs. a suspended monad.
Take a look at https://github.com/brettwooldridge/NuProcess mainly because: &gt; Additionally, on unix-based platforms such as Linux, when creating a new process java.lang.Process uses a fork()/exec() operation. This requires a temporary copy of the Java process (the fork), before the exec is performed. When running tests on Linux, in order to spawn 500 processes required setting the JVM max. memory to 3Gb (-Xmx3g). NuProcess uses a variant of fork() called vfork(), which does not impose this overhead. NuProcess can comfortably spawn 500 processes even when running the JVM with only 128Mb. 
Flink, Kafka, BEAM... I'd say those are the biggies. The issues are definitely not just a matter of not being on Scala 2.12 yet. I'd also say having to have Lightbend directly involved is in the problem set, not the solution set. Having to plan and execute a Spark upgrade is an opportunity to plan and execute a Spark departure. That may or may not happen; that's not solely my decision to make. But there's considerable appetite for it at work.
Why not change the behavior right away? I doubt anyone relies on that behavior
As a spark committer, I'd be interested in hearing more specifics... if you have the time, feel free to PM me.
In case there's interest, I was able to select the Second listed dependency in the following way: First, exclude the 3rd and 4th dependencies: libraryDependencies ++= Seq( ("org.neo4j" % "neo4j" % "3.2.3"). exclude("org.neo4j", "neo4j-jmx"). exclude("org.neo4j", "neo4j-udc") ) Then, use MergeStrategy.last to select the second of the remaining two conflicting dependencies. Unfortunately my program still doesn't run; it looks like BOTH the first and the second dependencies are required for the application to run. At least I know now.
You could try forcing serial execution: [docs](https://www.scala-sbt.org/1.x/docs/Testing.html#Disable+Parallel+Execution+of+Tests)
Thanks for this. Provided much food for thought which is what I was looking for. Went over DDD and everything definitely makes more sense now. Also, regarding 1), I don't think we're at the point where we're able to predict if we'll need to stray from the Scala. 
Immensely helpful - since exactly our situation: &gt;We had never built a CQRS/ES system so we decided to go with Lagom specifically for guidance on what best practices are and so far I'm grateful we did not roll our own solution. This is exactly where we are at and I think after the responses here I am becoming convinced we should go for Lagom. One question, did you need to use any of Lightbend's paid subscription solutions when deploying ?
We definitely need Event Sourcing but I don't believe we need CQRS that much. What stack would your recommend to roll our own microservices?
Naah. too much overhead. We just update records when we have to. The history is not import for us. so a update is fine. (that depends on the business use case).
With the tagless final style of writing programs, where do you stop modeling algebras? For example, if I need to read config from a file, do I model a file algebra (and thus capability) and a config algebra, or do I have an algebra and a configFromFileInterpreter? It seems that putting all logic in the interpreter defeats the purpose of the decoupling, on the other hand having to model/wrap all low-level actions in an algebra seems to be a path away from your program algebra. Any thoughts? 
dropRight(1)
Try the [`init`](https://www.scala-lang.org/api/2.12.0/scala/collection/Traversable.html#init:Repr) method.
Notably, [uPickle](https://github.com/lihaoyi/upickle) already has the basic infrastructure needed to do partial parsing of JSON, since it's built in the visitor pattern (which AFAIK is mostly isomorphic to recursion schemes). It's not built in, but you can build it yourself with not too much work: @ { class ReadyVisitor[T](chain: ujson.Visitor[ujson.Js, T]) extends ujson.Visitor[Nothing, Any]{ var ready = false def visitArray(index: Int) = if (ready) chain.visitArray(index) else ujson.NoOpVisitor.visitArray(index) def visitObject(index: Int) = if (ready) chain.visitObject(index) else ujson.NoOpVisitor.visitObject(index) def visitNull(index: Int) = if (ready) chain.visitNull(index) else ujson.Js.Null def visitFalse(index: Int) = if (ready) chain.visitFalse(index) else () def visitTrue(index: Int) = if (ready) chain.visitTrue(index) else () def visitNum(s: CharSequence, decIndex: Int, expIndex: Int, index: Int) = if (ready) chain.visitNum(s, decIndex, expIndex, index) else () def visitString(s: CharSequence, index: Int) = if (ready) chain.visitString(s, index) else () } case class Index[T](i: Int, chain: ujson.Visitor[ujson.Js, T]) extends ujson.CustomVisitor[ujson.Js, T]{ def expectedMsg = "???" override def visitArray(index: Int) = new ujson.ArrVisitor[Any, T]{ val subVisitor = new ReadyVisitor[T](chain) var result: Any = null.asInstanceOf[Any] var n = 0 subVisitor.ready = n == i def visitValue(v: Any, index: Int) = { n += 1 if (subVisitor.ready) result = v subVisitor.ready = n == i } def visitEnd(index: Int): T = result.asInstanceOf[T] } } case class Key[T](k: String, chain: ujson.Visitor[ujson.Js, T]) extends ujson.CustomVisitor[ujson.Js, T]{ def expectedMsg = "???" override def visitObject(index: Int) = new ujson.ObjVisitor[Any, T]{ val subVisitor = new ReadyVisitor[T](chain) var result: Any = null.asInstanceOf[Any] def visitKey(s: CharSequence, index: Int): Unit = { subVisitor.ready = s.toString == k } def visitValue(v: Any, index: Int) = { if (subVisitor.ready) result = v } def visitEnd(index: Int): T = result.asInstanceOf[T] } } } @ val json = """{ "guid": "c4d14538-e09f-4eeb-b877-1874c5a42b25", "tags": [ "pariatur", "dolor", "consequat" ], "friends": [ {"id": 0, "name": "Lang Pate"}, {"id": 1, "name": "Bette Whitehead"} ] }""" @ ujson.transform(json, Key("guid", ujson.Js)) res57: ujson.Js = Str("c4d14538-e09f-4eeb-b877-1874c5a42b25") @ ujson.transform(json, Key("tags", ujson.Js)) res58: ujson.Js = Arr(ArrayBuffer(Str("pariatur"), Str("dolor"), Str("consequat"))) @ ujson.transform(json, Key("tags", Index(1, ujson.Js))) res59: ujson.Js = Str("dolor") @ ujson.transform(json, Key("friends", Index(0, Key("name", ujson.Js)))) res60: ujson.Js = Str("Lang Pate") @ ujson.transform(json, Key("friends", Index(1, Key("name", ujson.Js)))) res61: ujson.Js = Str("Bette Whitehead") Here we define `Key` and `Index` visitors that can be chained to only parse the parts of the JSON struct you want. Only the portions matching the given keys get converted into an AST, the others are simply scanned over and ignored. If you want to be able to extract the data on-the-fly rather than waiting for parsing to complete, you can pass in your own visitor in place of the `ujson.Js` AST-builder and take callbacks as the JSON parses.
I work with a lot of data daily and I'm pretty sure that partial json parser would be super helpful 👍
Which element did you add last? The first? The last? Something in the middle? 
objects in instances become fields, the encoding is not the same as top level objects (because java has no top level objects), so the variable should be something like `appConfig.FriendApp().baseUrl()`. Note that `FriendApp()` is of type `FriendApp$`
You cannot add or remove anything from a vector. You can create a new reference from the old vector and remove the element using filter or a pair of slices and a concatenation. What you are trying to do sounds weird. Perhaps you need another data structure?
A queue data structure that’s implemented as a singly linked list with pushed and pop adding to the front is both immutable and o(1) for operations. Which library does it correctly, I don’t know.
It will slow the method from O(1) to O(n), and the same about the space. Therefore the update actually can spoil somebodies code.
Technically, it would be possible once Scala native is mature enough. Writing it on JVM would be pointless as, JVM has to run on _something_, that is operating system. But still, it would be pretty much impractical. Operating system should have total control over its memory management, time of running tasks and operations, and, in general, anything low level. After all only BIOS is lower. But, let's say, take a micro-controller - with a language suitable of system programming you can write something what would work reliable with 1kB of memory or less. Pretty much impossible when you use things like garbage collector (how do you explain fine-grained memory management and memory protection etc, when GC does it all for you, so you cannot really control it yourself?). Of all FP languages I know, only Rust has the right tools to enable system programming. There were attempts like JavaOS but they weren't successful. You also have Java ME and Dalvik, but they are more like a JVM on a hardware level, so not something implemented solely with Java.
We don't use Lightbends split brain service, instead we use [https://github.com/mbilski/akka-reasonable-downing](https://github.com/mbilski/akka-reasonable-downing) . There is a great article on it written over here [https://medium.com/stashaway-engineering/running-a-lagom-microservice-on-akka-cluster-with-split-brain-resolver-2a1c301659bd](https://medium.com/stashaway-engineering/running-a-lagom-microservice-on-akka-cluster-with-split-brain-resolver-2a1c301659bd) 
Cheers, Scala engineer here, seems to be broken in 2018.2.5 with lots of people posting same sort of issues since September. I could perhaps the Java thread too. I’ve been using IntelliJ for years, this is a just something that is frustrating me. 😔
Great shout. I’ll give that a go 👍🏻
shouldn't he use dropRight instead since it's safe for empty?
Possibly, but since OP used `pop` as an example I assumed the empty case didn't matter.
EAP 👍🏻 - this version is buggy as hell though!
1) You could make that task be part of a different microservice/artifact that you schedule separately 2) You can try and use a database to create a lock across services 3) I believe Akka works mostly w/ "At least once" type of guarantees but there's probably a way 4) Using kafka / rabbitmq you can set up a queue in such a way that you'll only have only one consumer Option 1) would probably be the best way to go though
Very nice little mini-tutorial. Just a couple of minor things: * It might have been nice to have a little one-line explanation of why one would want to use Iterant over Observable. You mention that it's pull-based, but as a noob it's hard to know what the consequences of that would be. (For example, does it help with early closing of resources, or similar?) * I'm sure it's just a lack of experience, but there were a couple of moments in the video where you "waffled" a little bit over whether flatMap or mapEval (or whatever) was the right thing. On the one hand I'm happy that even you sometimes don't *immediately* have the right answer :). However, I think it might make for a little smoother viewing if these segments could somehow be edited or practiced a little more beforehand. I'm not sure how your process was here, but I think you could probably have breaks in the script/monologue such that you could repeat only the troublesome bits as needed. A natural place to put these breaks might be when switching to the sbt terminal and back.
&gt; it would be possible once Scala native is mature enough What exactly do you think is missing from scala-native in order to write an operating system? All you need to do is write a boot loader to jump into your program like you would with C.
I am not up to date with Scala native, so it might be possible now, but I wondered if you already have the tooling allowing something that is not your typical program/library (already linked to some dependency that already needs operating system).
Use Akka Cluster Singleton.
I had a similar need and created this https://github.com/AmazingDreams/play-scheduler However, I have made it for play framework specifically
There’s no memory management as it’s not a part of the app, it’s a part of the runtime. To add it to the language makes Scala a much different language. Operating systems interact with hardware through volatile memory registers. You need a way to do that in Scala and then you need to use a mutilated optimised version of Scala to get the performance you need here. To operate without the JVM likely involves compromising some of Scala features such as type classes or some type deduction. It could be done but you end up writing a version of Scala which is a bastardised version of Java and C mixed together. It becomes pointless exercise as you chop off Scala features to make something usable.
Scala-Native is already a thing, that allows manual memory management and has a language for working CPU registers called NIR, which is basically LLVMIR annotated with scala types. You are grossly uninformed on the issue and are clearly working off pure conjecture. You should probably refrain from further comment until you have done some basic research on the subject at hand.
Come at me when you have your Scala operating system.
Thanks for the feedback, &gt; It might have been nice to have a little one-line explanation of why one would want to use Iterant over Observable. You mention that it's pull-based, but as a noob it's hard to know what the consequences of that would be Indeed, but couldn't fit that in this tutorial. Where Iterant could be used instead of Observable is a perfect subject for future tutorials. Until then this presentation I had at Scala World might help: [monix.io/presentations/2017-tale-two-monix-streams.html](https://monix.io/presentations/2017-tale-two-monix-streams.html) Note that I'm aggregating the presentations that I know about here: [monix.io/presentations/](https://monix.io/presentations/). &gt; You say or at least imply that Bracket is a sort of 'first-class' try/finally, but it might not be clear why that's a good/necessary thing. This is a good point. Well we need `try/finally` to ensure we don't end up with a resource leak (e.g. file handles remaining open, especially in the presence of errors or cancellation), but `try/finally` is meant for synchronous side-effectful actions, not pure functions that can evaluate asynchronously. Unfortunately I took this knowledge as a given. &gt; On the one hand I'm happy that even you sometimes don't immediately have the right answer :). However, I think it might make for a little smoother viewing if these segments could somehow be edited or practiced a little more beforehand. 🙂 I am only human and if I seem knowledgeable at times, that only happens because I've been focusing on certain problems for a long time — I have a piss poor memory too, my only real advantage is my passion for building software, which keeps me occupied. Unfortunately in making these videos, my goal is to make it cheap, otherwise I cannot do it — that video was made entirely in my free time and I spent way too much on figuring where to host it, the procedure for going forward, making the video, exporting it, configuring Vimeo, configuring the documentation website for Twitter cards and all sorts of other details. What you saw is basically unrehearsed live performance with no post production 🙂 and unfortunately it isn't feasible to spend more time on such tutorials. I do hope that with some experience they'll keep getting better.
Great tutorial. Thanks Alex! &amp;#x200B; I hope you carry on creating more tutorials like this one :)
Thanks for the link to the Iterant presentation -- I'm sure I had that on my radar at some point, but I guess I must have forgot about it. Absolutely fair enough regarding the "production value" bits (time, cost, etc.). I'm sure it'll automatically get even better the more practice you get :). All the "setup" ups (Vimeo, etc.) should also become second nature soon enough, so you'll probably have a little bit more time to spend on the content over time. Btw, I just realized that I only criticized (hopefully constructively), but didn't praise. For a first go the video was really *excellent* and my comments should only really be taken as nitpicks :).
Why would you want to?
Because scala is the programming language I feel the most comfortable with.
To be honest I have over 700 unit tests and they fail intermittently with 20 second timeouts even though the server can handle thousands of requests per second. What I really want is to understand what’s going on with Threads and unit tests and Specs2 so I can avoid what feels like a deadlock/starvation problem and keep the tests parallel and quick.
[removed]
I don't have the specifics memorized myself but the docs on that page also roughly describe the testing model. SBT has a test interface it supports, and specs2 (along with others) implements that interface. By default each test may be run in parallel to other tests. I'm not sure how many threads it spins up but a debugger can show you. If you have any globally shared resources created then parallel tests can certainly conflict. I try to keep my tests quick and reliable by \- Making them as isolated as possible such that no test depends on anything else - it creates and destroys precisely what it needs to achieve the test. Even sharing mocks can be problematic if one test sets a particular behavior, doesn't reset it, and the next test does not force what it wants assuming a default is fine. Using before and after blocks can be useful to avoid repeating yourself too much. \- Do not hit real servers during your unit tests. \- Keep individual tests single-threaded and synchronous; do not involve futures, actor systems, execution contexts, etc. If you must have them, try to limit them to same-thread or single-thread setups, and use \`Await.result\`. Sometimes async tests are unavoidable, in which case races can occur, and I find latches to be a pretty reliable way to force ordering in such scenarios.
&gt; You can't write polymorphic code that would work when interpreted in IO when you're 'just running it'/ 'interpreting into Id'. You absolutely can. Of course it's easy to get wrong if you don't test it, but it's possible. But my point is you shouldn't be working at the level where this is a concern in the first place: important effects should be represented by business-specific types. Writing `IO { some code that accesses the user database directly }` breaks `UpdateUser`'s laws just as badly as doing the same thing without `IO`. &gt; For people unfamilliar with FP, this results in imperative statements being used and monadic constructs being ignored. This happened on one of my previous teams, they used Either for typed errors + some tagless final, but otherwise their code had unrestricted side-effects and statement blocks everywhere and was hard to migrate to IO. And what would they have gained from using `IO`? Most of the value of `Either` doesn't come from `Either` per se, it comes from explicitly modelling the type on the left hand side, actually understanding what errors make sense for your domain and what the relations between them are. Whereas all `IO` can ever tell you is that running your code is equivalent to running your code; to stop there is to make the same mistake as the people who turn all their variables into `Option`s and then talk about how functional they've been by replacing `null` with `None`, rather than actually figuring out which variables can sensibly be absent and representing only those states that make sense. Unless you need to put some complex, business-logic-laden retry logic around an unmanaged effect, I'm starting to think `IO` does more harm than good - it discourages people from doing the actual domain modelling work of understanding what their relevant effects are (which is a significant amount of work, but where most of the value comes from), leaving them in an uncanny valley where they're paying most of the costs but getting few of the benefits. &gt; Interpreting into a strict monad / Id is nonsensical in presence of side-effects, it breaks reasoning by making val x = opA(); opA() *&gt; opB() have completely different effect when interpreted into a strict monad vs. a suspended monad. Monads in general are nonsensical in the presence of side effects. The solution to that isn't to use monads with a particular evaluation strategy, it's to not have uncontrolled side effects.
Great work, Alex! Monix is such an incredible library :)
Not really related to the worst part, but this 'Any' stinks, using a proper generic should be better, IMHO.
Maybe run your effect inside a MonadError, let it fail when it can’t get the value, and then use something like recoverWith?
That's awesome Alex, keep up the good work :)
I think I'm looking for something to modelize a project? Like the different parts and dependencies between each other. For example I'd write my functions, the functions they use, so that hopefully I can determine which set of things is common to the others, etc. I've heard of UML before, it never made sense to me but that might be it? If so, I could use a ELI5, good clients (emacs integration and android app would be great if they exist), what kind of automation I could get from it (I think I've read that you can automatically generate your classes in Java?); Not sure what kind of monster I'm poking and guidance would be welcome.
&gt; You absolutely can. Of course it's easy to get wrong if you don't test it, but it's possible. You lose nearly all refactorings that RT abstractions provide which are the entire reason for using them, tbh. You can't assign to a `val`. &gt; The solution to that isn't to use monads with a particular evaluation strategy, it's to not have uncontrolled side effects. Even if you wrap all business logic into algebras, without a wrapper you can't use referential transparency for refactoring and can't rely on any single monad law. What's even worse is that your code will be polymorphic and will look ordinary to a functional programmer, up until they hit something very strange while doing innocuous refactors. &gt; Most of the value of Either doesn't come from Either per se, it comes from explicitly modelling the type on the left hand side `IO` here is scalaz-zio IO, so we didn't lose typed errors, but also gained RT.
But why do you want to make an operating system
Allowing NPEs to be thrown (instead of testing for null) will kill performance. Pray your code is not used in a tight loop. https://shipilev.net/blog/2014/exceptional-performance/#_conclusion
That's what I figured. Gotta use the if clauses then. Oh well.
Because it seems fun?
If you're doing it a lot, you might consider spicing it up with some extension method: implicit class NullableGet[A](fa: Option[A]) { def ?[B](f: A =&gt; B) = fa.flatMap(a =&gt; Option(f(a))) } Option(thisObj).?(_.getNestedThing).?(_.finalKey).getOrElse(someDefault) Not that much worse than your version and doesn't rely on exceptions for control flow.
that's a cats feature
&gt; You lose nearly all refactorings that RT abstractions provide which are the entire reason for using them, tbh. If you're using a business-specific monadic structure to represent your effects, you get all the advantages of that whichever monad (or none) you interpret into - that's rather the point. If you're talking about the difference between using `IO` and using nothing at all, you don't gain any nontrivial refactorings because `IO` doesn't expose any nontrivial structure. In a language with unspecified evaluation order you'd gain associativity, but in a strict language like Scala you have it already. The only thing `IO` can actually give you is the ability to control how many times an I/O operation gets run (since running is the only thing you can do with `IO`). A lot of the time that's not something you care about (e.g. when reading a file from the filesystem, you may well not care about the difference between reading it from the filesystem twice, or reading it into memory and copying that in-memory value). &gt; Even if you wrap all business logic into algebras, without a wrapper you can't use referential transparency for refactoring and can't rely on any single monad law. This is just nonsense. Of course the monad laws will hold, for exactly the same reason they do with `IO` and with any number of strict monads like `Either`.
&gt; A road trip with Monads by Pawel Szulc Thanks for mentioning the video, seems like a great speaker. Looking at the simple weather app from the video, doesn't it get unwieldy for larger applications to provide the instances at the end or rather to declare how F[_] is constrained? For example here https://youtu.be/y_QHSDOVJM8?t=1657 we have: F[_]: Console: Weather: RequestState: ErrorHandler: Monad Doesn't this get larger and larger the more complex the program is? 
That's one thing about lihaoyi, he can be hard to approach so most of his projects are single coder (granted he's worth 4 but still)
I would also go with number 5. This way the instances don't have to know about each other and don't have to compete/interact in any way (be it through Akka Cluster or shared database). You're basically separating the intention from the execution then.
If you're interpreting to a strict monad, you can't assign to a val and have your program preserve the same effects. That's it. 
Hi, from what I've read one reason (the main reason?) that Scala.Net didn't work out was because the CLR uses reified generics. Why is a platform that uses reified generics harder to compile to compared to the jvm that uses erasure? Only because the CLR doesn't support higher kinded types (I would assume)? I know the CLR uses reified generics for performance reasons. With reified generics Scala.Net would have had the ability to use .Net code and .Net code would have the ability to call Scala.Net code, just like on the JVM. If Scala.Net would have cared only about the ability to call .Net (in order to use the ecosystem), but ignored the other way around, would that have made the project more likely to succeed?
I'm doing it a bit differently - I pass messages through Kafka and then even if I have multiple consumers vs one stream if they have the same consumer group only one of them picks up the data / task / message and performs logic with it.
If your microservice is running in K8S we use akka scheduler + leader-elector sidecar. At every job run the pod checks if the current container is the leader.
Almost missed the buzzwords ty for boldfacing
No worries, your feedback was good, I appreciate it. And thanks.
Verizon?
&amp;#x200B; case SOME_ENUM.FOO =&gt; for { x&lt;-Option(thisObj.getNestedThing) y&lt;-Option(x.finalKey) } yield x).getOrElse(fallback)
&gt;Don't seem to have asOption available You want `toOption`: Welcome to Scala 2.12.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191). Type in expressions for evaluation. Or try :help. scala&gt; import scala.util.Try import scala.util.Try scala&gt; Try(42).toOption res1: Option[Int] = Some(42) scala&gt; Try(throw new Exception).toOption res2: Option[Nothing] = None 
What on earth are you talking about?
There's no mention of blockchain or JavaScript though.
TypeScript has strict null checks. If you turn this on you could just write ... type Tree&lt;T&gt; = null | { left: Tree&lt;T&gt;, right: Tree&lt;T&gt;, value : T } This makes it a one liner, it's more efficient, and it's still safe from null pointer exceptions.
I copy/pastad the description
Pretty much any messaging solution that allows for pub/sub semantics can be used the same way (RabbitMQ, or SQS are the two I've used most recently). I have also used Kafka for work distribution.
javascript is a buzzword?
The contents of the resources directory is put at the root of the JAR. You should be able to load them consistently using getClass.getClassloader.getResource("README.txt") or getClass.getClassLoader.getResourceAsStream("README.txt") I believe that will only work for the packaged files but not for directories; you can load files nested under a directory but not list the contents of the directory. Since this is bahavior inherited from the JVM doing searches for the Java way of loading resources would lead you in the right direction if you need further details
yes 
 Don't use scala.io.Source. Use `class.getResource`. Also note that `class.getResource` behaves differently from `classLoader.getResource` in how it [finds resources](https://stackoverflow.com/questions/6608795/what-is-the-difference-between-class-getresource-and-classloader-getresource).
While I haven't written a lot of tagless final stuff myself, my instinct is to start simple and expand into tagless final as needed. At the moment I would choose tagless final as soon as I needed one of * Supporting both sync and async behavior (like for tests vs application) * Supporting multiple effects (like a library wanting to integrate with Task and IO) * Making good use of the algebra before running (like going through an optimization interpreter) For the config case specifically I don't see any of those being particularly relevant. That's usually something done once at startup and then passed to everything that needs it. For that I would just read the file synchronously via whatever method is handy and be done with it, e.g. `ConfigFactory.load` from Typesafe config.
Salary? PM me if not willing to disclose publicly
It could get very large but it all depends. Eg: this how the `Main` of our large application looks like, not really big: ```scala class Main[F[_]: Clock: ConcurrentEffect: ContextShift: HasAppConfig: Par: Timer] ``` I doubt any real application needs `Console` besides toy projects. And in my case I don't see `Weather` as an MTL class. Instead I like to pass that explicitly. Eg: ```scala class HigherLevelProgram[F[_]](weather: Weather[F]) { def foo: F[Unit] = ??? /// program that uses weather } ``` So I see `Weather` as a tagless final algebra that can have many interpreters. In general one for live apps and one for testing. 
Each collection’s underlying storage mechanism has its performance strengths and weakness and you would do well to study them. I suggest you start [here](https://docs.scala-lang.org/overviews/collections/performance-characteristics.html). I base my collections choice on the problem at hand. Also be aware that some collections are pure types and could represent one of many underlying storage mechanisms. [Seq](https://www.scala-lang.org/api/2.12.0/scala/collection/Seq.html) that has many subclasses including List and ArrayBuffer which have very different storage mechanisms. 
I worked for Verizon for a few years, pm if you want details. Can definitely be a cool place to work
Hey. What was verizon like? Why did you leave? What was the pay like? 
It turned into a meme for me the moment people wanted to use it for anything other than extremely basic browser tasks, like showing an alert box. Now people write business logic and servers in their 20 million immature frameworks, and the standard is so poorly understood that you have to "transpile" it to a lowest common denominator before you can even trust it to function. Hell functions often have to start with ```var self = this;``` to avoid seemingly random scoping issues. It's a huge bother to use, and it's only allowed to be this way because it's the only language browsers support. Efforts have been made to create other languages, but they still all transpile to JavaScript. Web design is a meme.
The question was about the cost of convertion from one type to another. 
For context, here's what Scala collection corresponds to a selection of some Java collections, at least logically if not in terms of implementation: | Java | Scala | | ----------------------------------------------- | --------------------------------------- | | interface List&lt;A&gt; | trait Seq[A] | | A[] | Array[A] extends Seq[A] | | ArrayList&lt;A&gt; extends List&lt;A&gt; | Vector[A] extends Seq[A] | | LinkedList&lt;A&gt; extends List&lt;A&gt; | List[A] extends Seq[A] | Since each of the Scala variants shown are sub types of Seq[A], calling toSeq is just a cast with no overhead. Converting between Array[A], Vector[A], or List[A] on the other hand will result in data being copied since each has different levels of chunking in memory. Also note that when creating a Seq[A] through Seq.apply -- for example, like Seq(1, 2, 3) -- then that will end up using a List[A] as the concrete class. In general I'd recommend defining APIs in terms of Seq[A] since in most cases how the data is laid out in memory is not as much if a concern as how the data is used. By keeping the API in terms of the trait you encapsulate the implementation detail and make it easier to switch to another implementation if desired. Of course, if the consumers of your APIs **do** care about how data is laid out in memory then by all means do include that info in the API. Iterators are a different beast. In both Java and Scala the only guarantee is that you can get the next element so long as there is one, but no way to look at past elements nor any guarantee that elements exist in memory until you retrieve it. Calling toSeq changes the implementation to one that guarantees that previous elements can be processed again, and doing something like toVector on it will also eagerly pull remaining elements into memory. If you know that you only needs a single pss over data, then keep it as an Iterator[A] for the reduced memory footprint. If you'll need multiple passes over that data then converting to a Seq sooner rather than later will reduce the scope in which some code could inadvertently call the `next` method and make some data unavailable for reprocessing.
Thank you @ARainyDayInSunnyCA. just placed `getClass.getClassloader.getResource("README.txt")` inside scala.io.Source.fromFile(...) and the rest of the code was the same.
Thank you @amazedballer. `class.getResource` didn't compile; `getClass.getResource` did. Thank you for the SO link too; I didn't know the right terms to search for an answer to this problem. I found this [simple guide](http://fruzenshtein.com/scala-working-with-resources-folders-files/) also helpful.
&gt; When it comes to the various collection converters, is there any cost to using them? Yes, you're creating a new collection and adding N elements into it, so the cost is linear in the size of your collection. If you want to avoid that, you can use https://www.scala-lang.org/api/2.12.7/scala/collection/JavaConverters$.html which _wrap_ the underlying collection instead of copying it. You could also just not convert it at all and use the Java APIs.
https://www.reddit.com/r/scala/comments/9txltc/how_to_sell_catsio_to_my_team/e95iek9/
&gt; In general I'd recommend defining APIs in terms of Seq[A] I wouldn't recommend that. `Seq` isn't a good abstraction. E.g., do all of your functions work correctly for an infinite stream? I prefer to use type classes for this. Do you want to have the same kind of collection back, but apply a function on each element? Use a functor. Do you want to collapse the collection into a value? Foldable. 
I use ensime with emacs. 
With TaglessFinal they are just interfaces and implementations. The interfaces abstract over an underlying effect type to make a new effect type. &amp;#x200B; For example, say you want to publish messages to a distributed topic. Obviously this is an effect, so you will return an effect type. The effect type could be Future, Try, IO, Writer, State, etc. You probably want to expose an action, `publish[A](a:A):MyConcreteEffectType[A]`. What tagless final allows you to do is to create an interface that decouples calling code of your publish action from `MyConcreteEffect`. `trait Publisher[F[_]]{` `def publish[A](a:A, queueName: QueueName):F[A]` `}` `object Publisher{` `implicit val defaultInstance[F[_]: Effect] = new Publisher[F]{` `import myQueueLibraryInstanceThatUsesSomeJavaLib.writeToQueue` `val eff: Effect[F] = implicitly[Effect[F]]` `override def publish[A](a:A, queueName: QueueName):F[A] = {` `effect.catchNonFatal{` `writeToQueue(queueName, a)` `a` `}` `}` `}` &amp;#x200B; Now you can use any Effect you want with that code. Dependent code can declare that it publishes to queues, as well as that it is an effect: &amp;#x200B; `def didSomethingWithUserLog[F[_]: Effect: Pubilsher](userLog: Userlog): F[UserLog] = implicitly[Publisher[F]].publish(userLog, 'auditQueue')` &amp;#x200B; If you are not doing anything that is side effecting, you can use a regular typeclass for your interface instead: &amp;#x200B; `def combineUsersLogs(user1Log: UserLog, user2Log: UserLog)(implicit semigroup: Semigroup[UserLog): UserLog = user1Log.combine(usel2Log)` &amp;#x200B; So, now you can compose your effects and pure dependencies together: &amp;#x200B; `def getUserLogsForUsersAndPublishCombinedLogToAuditQueue[F[_]: Effect: Publisher: UserLogRepository](user1: User, user2: User)(implicit semigroup: Semigroup[UserLog]): F[UserLog] = for{` `userLog1 &lt;- implicitly[UserLogRepository[F]].getUserLog(user1)` `userLog2 &lt;- implicitly[UserLogRepository[F]].getUserLog(user2)` `combinedUserLog = Semigroup[UserLog].combine(userLog1, userLog2)` `published &lt;- implicitly[Publisher[F]].publish(combinedUserLog, 'auditQueue')` `} yield published` Note that I included a second effect `UserLogRepository.` So where do you stop using tagless final? You stop using tagless final when your code you are writing an interface for does not do any side effects. If your code can side-effect, you need to encode a tagless final interface for it. If your code is pure, you can use regular typeclasses for injection, and they will probably exist in a library for you. If you do not plan on using fake implementations for a pure function and the output of your caller contains results derived from the application of that pure function, you don't need to inject it, so don't need a typeclass, though using a typeclass will add additional constraints to the function interface that make it more clear what the function is allowed to do. `def getUserLogsForUsersAndPublishCombinedLogToAuditQueue[F[_]: Effect: Publisher: UserLogRepository](user1: User, user2: User)(implicit semigroup: Semigroup[UserLog]): F[UserLog]` implies (not enforces) that you will fetch the logs for user1 and user2, combine them, and publish them somewhere, and return the result in an effect type. &amp;#x200B; Now you can call `getUserLogsForUsersAndPublishCombinedLogToAuditQueue[IO](myFirstUser, mySecondUser)` if you aren't assigning it to a value where IO can be inferred or simply `val myLongProgram: IO[ResultThing] = for {` `publishedUserLog &lt;- getUserLogsForUsersAndPublishCombinedLogToAuditQueue(myFirstUser, mySecondUser)` `....` `} yield myResult` &amp;#x200B; or `val myLongProgram: Task[ResultThing] = for {` `publishedUserLog &lt;- getUserLogsForUsersAndPublishCombinedLogToAuditQueue(myFirstUser, mySecondUser)` `....` `} yield myResult` when the IO/Task/Some other effect can be inferred. You are free to change the effect at any time, and you only have to do it on one place, at the top level of your program. Pretty neat. &amp;#x200B; A lot of scala programs delegate to java libraries, which love to throw all over the place and use void, etc. So most application code that you write should probably have a tagless final encoding if it wraps a java library. It's very rare to call out to a java lib and have it be pure. &amp;#x200B; Tagless final is overkill in programs that can fit on one page, but once you have five or so effect-full concerns, and the program is going to live for a while or be handed off to another team in the future, it's pretty useful. &amp;#x200B; Another use case for tagless final, outside of effects, is for the [expression problem](https://www.slideshare.net/etorreborre/the-expression-problem-solved-finally). Start on slide 44.
I'd recommend avoiding Scala collections completely and using Java streams instead.
https://www.youtube.com/watch?v=bxU2eqZtYmc
Yes, for parameter types you should generally use the least specific type you can handle. For return types you generally use the most specialized type you can guarantee. Both decisions should take into account possible future changes to the implementation.
[You can use the Cassandra Alpakka connector](https://developer.lightbend.com/docs/alpakka/current/cassandra.html)
Looks perfect.
finally
`x` will never hit the interpreter (it'll never be part of what the expression evaluates to), so it doesn't make a difference whether you interpret into `IO` or `Id`
*Experimental* Scala 2.12 support
If you have a function from A =&gt; B and a function from C =&gt; D, by the very nature of composition, you can compose them. What you can do, is create a third function that knows how to make a C out of a B (B =&gt;C) and compose the three functions instead C =&gt; D • B =&gt; C • A =&gt; B, which will give you a function from A =&gt; D.
Right, but how does one go about this in Scala? In my situation, all of my functions are composable, but I'm not sure how to do it besides performing the composition manually.
Can’t test it now, but it looks like your last example should work if f4 were composable with f3 (I.E. f4 would receive a Double).
Alright, that makes sense. I understand that even though I've happened to make a list of composible functions, a list of incompatible functions would end up having the same type. So is there some way to accomplish this? Something along the lines of: def RecursiveCompose[A, B, C](f: A =&gt; B, l: List[???]) = { if (l.isEmpty) f else { val g: B =&gt; C = l.head //Trick Scala into believing compatibility? RecursiveCompose(f andThen g, l.tail) } } Maybe?
The problem is that they don’t end up having the same type because of the variance of the input and output types. The input types are all different, Scala needs to infer a common supertype. It finds it in Any, because any accepts String, Int or Double and is also the only common super type for all three. Now the return type needs to be infered to a common sub type, I.E. something that is at least a String, but also at least an Int, and at least a Double. There is no such a type, so it can’t infer it, it expects you to tell it, which you can’t because there is no such type. Your example would also not work, A, B and C are type variables, B in the val declaration is not a different B than the one in the top function declaration. When RecursiveCompose is called, the type variables will be infered (or explicitly declared) for that specific call and will be the same throughout the call, so you would end up with the same inference problem.
I think I had an idea (typing on the phone so it might be a bit messed up): final case class Composed[A, B](f: A =&gt; B) { def chain[C](newF: B =&gt; C): Composed[A, C] = f andThen newF }
Perfect! I can use this. Will update my original post shortly.
Yeah, to be fair, I’m not sure what benefit would there be in storing the functions in any way other than the composed function itself. The function is the right abstraction for this and it perfectly hides the implementation details. Replacing andThen by chain or `,` brings marginal benefits at best, and a big downside that whatever structure we use to represent this composition, is exposing its intermediate states. In the end representing a function composition as a functions is better than representing it as a more tradition dara structure such as list or something similar.
What does “experimental Scala 2.12 support” mean? - are there known bugs or limitations around using Spark from Scala 2.12? - is there a lack of test coverage around using Spark from Scala 2.12? - is it simply that it’s not as battle-tested? - something else?
Looking back on it, you're definitely right. I don't really gain much from representing composed functions in this way. However, my reasoning was as follows: In my particular situation, I have many small component tasks which should be composed into larger tasks. These larger tasks will be applied to collections of data that often need just slightly different types of attention. For this reason, two similar major tasks could be represented as some subtask with a few component tasks appended or prepended, like: commonSubTask = List(???) majorTask1 = List(specialTask1, specialTask2, ...) ::: commonSubTask majorTask2 = commonSubTask :: someComponentTask I thought this was going to be a nice representation but it's a major hassle and just using function composition seems to be the better solution. &amp;#x200B; Also, I've come up with a working solution put into the original post.
Also another reason it wouldn’t work with fold is because it would have to be something like a monoid, and to make andThen a monoid over a type, that type would have to be something like A =&gt; A, so you could only compose functions using fold if their input and output type was the same.
Is there a reason you want put these functions into a `Seq`? It's unclear why you would bother with the collection at all and just do `val f4: Int =&gt; Double = f1 andThen f2 andThen f3`? Are you attempting to generalize over `andThen` to replace composition with your own operator? 
Since posting this, I have begun to realize this isn't really the best solution. What I'm setting up is meant to be used by a few non-seruous programmers and I thought setting up lists might be a bit easier for them to understand and work with. Sometimes they'll need to omit, modify, or add just a few tasks, and it might be a bit easier for them to just modify a list instead of defining a whole new composition where most parts are unchanged. These collections of tasks have ~50 entries typically so I thought a lack of copy and paste would be important.
I understand it's been a long road to support for Scala 2.12. Spark has always been a driving force behind adoption of Scala in a large number of workplaces, so it is great to see the continued support for new language versions. If any of the dev team are reading this I'd just like to say a huge thank you for all your hard work and continuing efforts. Great job!
It's just not been battle tested. It was only contributed a few months ago and this is the first release since then.
You already have this: Just place `"` around the literal. You can even use it for complex numbers, rationales, etc.
Thanks for creating this! Have you seen akka typed? What is different with your project?
The main reason it took so long for the 2.12 release was because it would have resulted in a breaking API change. The Scala team and others released a back port to 2.12 to mitigate the issue. Upgrading to 2.13 shouldn't be at big of an issue, but they're just doing it one major version at a time.
The interpreter in tagless final is the dictionary itself, not an external entity. When you call `Ops[F].opA()`, and your 'interpreter' Ops[F] is in side-effecting `Id` it will have a completely different effect than the same expression with `Ops[IO]`.
You have four options here for making SomeCompositionFunc(Seq(f1, f2, f3))work. 1. Give up the syntax and write f1 andThen f2 andThen f3 as suggested by /u/Milyardo 2. Give up on types and compiletime safety and write a reduce function that works on Any and composes them via reflection. 3. Use a datastructure that captures all the types and combine them at typelevel. You could use shapeless HList and it would look like this: someCompositionFunc(f1 :: f2 :: f3 :: HNil) 4. Use macros Honestly, if your motivation is only about syntax, go for first solution. The ones writing this will probably be able to cope with it.
There isn't a word mentioning free monads in your post though. Besides, reifying the algebra is as far from "interpreting directly" as it gets.
I didn't mention Free because ideally you form a domain-specific algebra which will have more equations than the free structure does. &gt; Besides, reifying the algebra is as far from "interpreting directly" as it gets. Disagree; without a command value to interpret there can be no interpreting. If you're going to have a value and interpret it, the simplest, most direct version of that is to have a domain-appropriate algebra and an interpreter that executes it directly.
I actually use Akka-typed and other Akka projects a lot. It's great! But I sometimes work on projects that do FP and are not heavy Akka users but require async message processing where a lightweight Actor solution with most commonly used Actor APIs is enough. [Scalaz actor](https://github.com/scalaz/scalaz/blob/series/7.3.x/concurrent/src/main/scala/scalaz/concurrent/Actor.scala) is another one.
Will be interesting how Spark will deal with the incompatible changes in collections.
Nice work! And thanks for mentioning of Scalaz actors! Also, to improve runtime characteristics you can borrow a lot of implementation tricks from the following repo: https://github.com/plokhotnyuk/actors
So /u/paul_f_snively beat me to a really great answer, and while using a heterogeneous list solves the type safety issues involved, I wanted to go into more depth about how to model the operations as data in functional programming. In functional programming with abstract algebra it's common to take a abstract operation(like addition, or composition), and describe that algebra using a datatype. The following is a bit of a crash course into the basics of abstract algebra. Addition is the most common kind operator abstract over in abstract algebra. Outside of adding integers, you could add strings together using concatenation. You can add lists, you can add coordinates in plane, and dates and times. Addition and addition-like operators are everywhere. Addition, and addition-like things are a group of operators that share a few laws, and that group of addition-like things form an algebra. So what is addition? Let's take a look a few examples. val n = 1 + 2 + 3 + 4 + 5 val s = "Hello" + " " + "World" + "!" Here we have integer addition and string concatenation. What properties do they share? First is that they are both total. For every integer and every string, you can use this operator, and there is a valid solution. Second is that they're associative. Using these properties, we'll find that integers have more one operation meet these constraints. We have describe multiplication as being addition-like, but not subtraction or division. val m = 1 * 2 * 3 * 4 * 5 //total and associative val sub = 1 - 2 - 3 - 4 - 5 //not associative val div = 1 / 2 / 3 / 4 / 5 //not total(division by 0 has no solution) and not associative So we've identified some properties we want to model. Let's attempt to capture these as a datatype. case class AddLike[A](left: A, right: A) def add[A](a: AddLike[A])(op: (A,A) =&gt; A): A = op(a.left,a.right) val ThreeAndFive = AddLike(3,5) val HelloAndWorld = AddLike("Hello","World") val n = add(ThreeAndFive)(_ + _) val m = add(ThreeAndFive)(_ * _) val s = add(HelloAndWorld)(_ + _) With this we can abstract over a kind of addition, but we're limited in that we can only represent one addition at a time. We need a recursive datatype to represent more than one addition operation at a time. trait AddLike[A] case class Add[A](left: A, right: AddLike[A]) extends AddLike[A] case class End[A](a: A) extends AddLike[A] def add[A](a: AddLike[A])(op: (A, A) =&gt; A): A = a match { case End(a) =&gt; a case Add(left,right) =&gt; op(left,add(right)(op)) } val numbers = Add(1,Add(2,Add(3,Add(4,End(5))))) val strings = Add("Hello",Add(" ", Add("World",End("!")))) val n = add(numbers)(_ + _) val m = add(numbers)(_ * _) val s = add(strings)(_ + _) Now we have made one of the sides of the `AddLike` recursive. You can either have more additions to perform on the right side, or you can end the computation. `AddLike` may now also look familiar, it's almost the exact definition of scala's `List`, the subtle difference being `End` takes an `A` while `Nil` takes no value and terminates all `List`s. This because `List` encodes the structure of the `AddLike` algebra(Formally we call `AddLike` `Semigroup` in abstract algebra). If a data type encodes the structure of an algebra like List does for `Semigroup` we call it the *free* encoding of that algebra. Now let's take a look at the `compose` operation. Does it fit the criteria of `AddLike`? No it does not. `compose` is not total. You can not apply any two functions to compose and get a solution. So attempting to use a `List`(heterogeneous or not) is not correct, compose belongs to a different algebra. So this begs the question, what algebra does `compose` belong to? What is `compose`-like? Well `compose`-like things belong to a an algebra called `Category`. A category has two fundamental properties., A `compose` operation for `Arrow`s between objects in a category and and identity arrow from an object back to itself. What does the free category datatype look like? trait FreeCat[Arrow[_,_],A,B] case class Compose[Arr[_,_], A,B,C](ab: Arr[A,B], bc: FreeCat[B,C]) extends Cat[Arr,A,C] case class Id[Arr,A]() extends Cat[Arr,A,A] //A higher-kinded function for unifying `B ` in compose. trait Comp[Arr[_,_]] { def apply[A,B,C](ab: Arr[A,B], Arr[B,C]): Arr[A,C] } def compose[Arr[_,_], A, B](fc: FreeCat[Arr,A,B])(c: Comp[Arr], id: Arr[A,B]): Arr[A,B] = fc match { case Id() =&gt; id case Compose(ab,bc) =&gt; c(ab, compose(bc)(c,id)) } val fc1: FreeCat[Function1,Int,Int] = Compose(_.toString, _.toInt + 1) val function1AndThen = Comp(_ andThen _) val function1Compose = Comp((l,r) =&gt; r compose l) val f1: Int =&gt; Int = compose(fc1)(function1AndThen, identity) val f2: Int =&gt; Int = compose(fc1)(function1Compose, identity) //same function but created with compose instead of andThen. Like with `List`, `FreeCat` is recursive(generalizing over recursive data is another interesting topic). It contains just enough structure to change the data into an arrow that when given an implementation. So to summarize, algebras are a group of similar operations that share some property. We attempt to encode the minimum amount of information about an algebra as data with free encoding of that data. This abstract over the operation and inspect the structure. If you're interested in seeing practical applications of free encodings, there's tons of resources on the Free Monad.
The whole point of Free is that it gives you flatMap and pure and you're free to add additional constructors by parameterizing. I don't see a point in avoiding Free and writing the same flatmap and pure constructors for every domain algebra. (Besides, how do you combine them? With Free InjectK is simple, but with custom? Or there's just one global?)
`actors` is definitely a very helpful project. Thank you for letting me know and sharing `actors`.
I think the main issue was that alternative platforms were always seen as "allowing Scala devs to target an additional platform" and not as "allowing C#/Android/... developers to use Scala", so the adoption was always very limited. Java will be getting new generics real soon now, so the claim that Scala.NET's failure was due to generics will be put up to the test. The only thing that's sure is that the new generics will require architectural changes to pretty much every phase after typer, and I guess nobody was up for it in the Scala.NET case. (Basically, with the current generics, every call to `foo[T]` refers to the same "target" after erasure. With the new generics, you have to keep calls separate, because the generic type is now part of the "target".)
Because he values some properties which he knows his team might not value as much or even understand them. So hes looking for other properties. For example, just referential transparency might be enough of a reason for him but not his team.
Here's a *very* quick outline: * A signature `s` is a tuple `(F, R, C, s')` such that `F`, `R`, and `C` are disjoint sets and `s'` is a function from the union of `F` and `R` to the positive integers. * For a given signature `s`, an `s`\-structure is a tuple `(M, F', R', C')` where `M` is a class and `F'`, `R'`, and `C'` are sets of functions, relations, and constants of `M` respectively. Specifically, for each symbol `f` in `F`, there is a `s'(f)`\-ary operation `f'` on `M` contained in `F'`, and such functions are exactly the elements of `F'`. A similar statement holds for `R'` and `C'`. * For a given signature `s`, the language of `s` is recursively defined as the set of all sentences created using the symbols defined in `s` (this is a very quick and dirty definition, but without LaTeX, I don't even want to try). The language of `s` might, for example, contain something of the form `x = f(y, z)` as long as `s'(f) = 2`, or it might contain `r(x, y, c, d)` where `c` and `d` are elements of `C` and `s'(r) = 4`. * For a given signature `s` and a set `L` of sentences in the language of `s`, we say an `s`\-structure `M` is a model of `L` if and only if every sentence in `L` is satisfied in `M`. I haven't defined "satisfied", once again because of the lack of LaTeX and motivation, but it pretty much is what one might expect. Hopefully the next example clarifies things. So... Let's look at what it would take to describe a cyclic group with three elements using the lens of Model Theory: * The signature of Abelian groups is the tuple `s = ({+, -}, {}, {e}, (s': + |-&gt; 2, - |-&gt; 1))`. * Within the language of Abelian groups, consider the following set of sentences, which we'll call the axioms of Abelian groups: `{ x+(y+z) = (x+y)+z, x+(-x) = e, x+e = x, x+y = y+x }`. Any model of this set of sentences is an Abelian group. Note, however, that not every structure with the signature of an Abelian group actually is one. * Consider the following tuple: `({0, 1, 2}, {+', -'}, {}, {e'})` where `e' := 0`, `+'` is addition modulo 3, and `-'` is negation modulo 3 (or, equivalently, `x |-&gt; (x + 2) % 3`). Through direct inspection, it is possible to verify that this tuple is a model of the axioms of Abelian groups. Here's a list of some signatures and sentences: * `({*}, {}, {}, (* |-&gt; 2))` is the signature of magmas and of semigroups. Magmas need not satisfy any sentences, but semigroups must satsify `x*(y*z) = (x*y)*z`. (I'd like to remark here that your semigroup isn't actually a semigroup since you've given it an additional unary operation. What you defined seems a bit more like a monoid). * `({*}, {}, {e}, (* |-&gt; 2))` is the signature of monoids. The sentences a monoid must satisfy are `x*(y*z) = (x*y)*z`, `x*e = x`, and `e*x = x`. * We've already seen the signature and axioms of Abelian groups. The signature of groups is equivalent to the signature of Abelian groups, although "less Abelian" symbols are typically used (e.g., `*` instead of `+`). The axioms of groups are the same as the axioms of Abelian groups, but with the commutativity axiom removed. * `({/}, {}, {e}, (/ |-&gt; 2))` is the signature of division groups. A division group must satisfy the following set of sentences: `{x/(e/(y/(e/z))) = (x/(e/y))/(e/z), x/x=e, x/(e/e)=x}`. * `({}, {~}, {}, (~ |-&gt; 2))` is the signature of partially ordered sets. A partially ordered set must satisfy the following set of sentences: `{~(x,x), ~(x,y)^~(y,x) -&gt; x = y, ~(x,y)^~(y,z) -&gt; ~(x,z)}`. * `({+,*,-}, {}, {0, 1}, (+ |-&gt; 2, * |-&gt; 2, - |-&gt; 1))` is the signature of Boolean algebras. Boolean algebras must satisfy quite a few sentences. (Something like 6 or 12). The great thing about the Model-Theoretic view of mathematical structures is that signatures are directly translatable into Scala's abstract classes. For example, we could define abstract class Monoid[A] { def *(x: A, y: A): A val e: A } Or using the signature of bounded lattices (with a bit extra expressiveness), `({M, J}, {Leq}, {top, bottom}, (M |-&gt; 2, J |-&gt; 2))`, we get abstract class Lattice[A] { def M(x: A, y: A): A def J(x: A, y: A): A def Leq(x: A, y: A): Boolean val top: A val bottom: A } This is wonderful, but the only really tricky thing is making sure a structure in a certain signature actually satisfies the correct axioms. A class that extends `Monoid` might not actually be a monoid, and might just have the signature of one. This isn't the worst thing in the world, it's just up to the programmer to make sure their specific structure satisfies the right sentences. Another thing that might seem tricky at first is figuring out how to work with multisorted partial structures, but you've done a good job of showing this. For more sorts, add more types, and for partial operations, use a bit of trickery. &amp;#x200B; To finish I'd just like to also mention that Model Theory might provide some insight on how there may be multiple ways to achieve the same goal through (definitional, polynomial, ...)-equivalence. Groups and division groups (described above) are *definitionally equivalent*, loosely meaning that any statement about groups can also be made about division groups, and any group can be naturally viewed as a division group. From a programming perspective, this means that the same abstraction can be achieved in multiple ways, and in certain situations, some ways may be preferable to others.
Hey, I understand it isn't particularly easy to jump from hello world to real FP applications. I would advice to write small applications over, and over again. Apply a new concept each time. An easy way to go about it is with [https://typelevel.org/cats/](https://typelevel.org/cats/). Go through each data type, and then type classes. This will help you generalise your code. Build on top of that with [http://http4s.org](http://http4s.org) for websites, [https://tpolecat.github.io/doobie/](https://tpolecat.github.io/doobie/) for SQL, and [http://fs2.io](http://fs2.io) for streams. Rome wasn't built in a day. Just stay strong, and you will get there. Good luck
Forgot to add a few great resources: * Functional Programming with Effects by Rob Norris - [https://www.youtube.com/watch?v=30q6BkBv5MY](https://www.youtube.com/watch?v=30q6BkBv5MY) * John De Goes - FP to the Max - [https://www.youtube.com/watch?v=sxudIMiOo68](https://www.youtube.com/watch?v=sxudIMiOo68) * Functional Structures in Scala - [https://www.youtube.com/playlist?list=PLFrwDVdSrYE6dy14XCmUtRAJuhCxuzJp0](https://www.youtube.com/playlist?list=PLFrwDVdSrYE6dy14XCmUtRAJuhCxuzJp0) * Book: Functional Programming in Scala * Book: Functional Programming, Simplified: (Scala Edition) (haven't read it yet) * Book: Category Theory for Programmers (haven't read it yet) And a shameless plug for my intro to cats: [https://medium.com/@pvinchon](https://medium.com/@pvinchon)
Ah, that's more in line with the tagless final example applications that I have seen. One of them being your backpack application that you open sourced. Thanks for that. :) On a related note, do you know of frontend projects with scalajs that use tagless final or cats/scalaz projects in combination with scalajs?
I've been accused of the same, but with OOP. "It's so simple. Everything is an object . . . and the dependency injection allows you to avoid class factories entirely!"
I found it pretty bad when I was learning. Pretty much every example wanted to go through Functors, Monoids, etc to teach you -- and often using Haskell (thankfully at least now people explain them with Scala!) The best you can really do for a developer coming from Java is a simple: "very similar to Optional and Streams in Java 8" &amp; "it has map/flatmap/'some constructor like method'". If that's really all you need to know as an introduction. Then transformers come naturally as you hit upon having to map twice. The laws and other parts can come later.
The reason people often try to explain it in Haskell, is because the implementations are much simpler than in Scala because in Haskell those implementations don’t have to have weird workarounds to make it work with OO. The error there, is that after learning Haskell, you realise how clean and simple the syntax is, and forget that for someone used to C like syntax none of that makes sense.
A monad is just a monoid in the category of endofunctors, what's the big deal?
The problem with using Haskell is that for many people it turns learning one thing into learning two things. 
:)
Can you map it? Probably a functor. Can you flatmap it? Probably a monad? Can you combine it? ...maybe a monoid. I have yet to figure out applicative functors well enough to give it such a short definition. 
Can you `map2` it? `map2` always made it more obvious to me than `ap`
I'm a firm believer that it's very hard to learn monads b to explicitly setting it to learn them. What usually happens is that you start using monadic interfaces, and then eventually you see the pattern, thus "discovering" monads. 
I found that watching videos didn't help but as soon as I started looking at examples it all made much more sense ...
&gt; Thanks for that. :) My pleasure :) If I find any time I'll try to update it but it's been running well for more than a year so no reason for me to stop that server ;) &gt; On a related note, do you know of frontend projects with scalajs that use tagless final or cats/scalaz projects in combination with scalajs? I have zero experience with scalajs. I know some people at work were using it with `cats-effect` but for a small project and unfortunately not open source :/
That’s not different in Scala though, unless you’re explaining category theory to people who write Scala in a java kind of style.
The cache should be created probably once in your main program, not for every time you run unsafeRunSync.
I don't know you but you're a nice person and I hope you always stay happy and positive. Thanks for being here.
Unfortunately, no matter where I create it - it will be a separate actions for separate `unsafeRunSync`. Toy example: ```scala val cache = IO { println("Initializing cache"); Map.empty[String, String] } def put(cache: Map[String, String], key: String, value: String): IO[Unit] = IO { println(s"putting $key -&gt; $value") } List(1,1,1).map { row =&gt; val action = for { c &lt;- cache _ &lt;- put(c, row.toString, row.toString) } yield c action.unsafeRunSync() } Initializing cache putting 1 -&gt; 1 Initializing cache putting 1 -&gt; 1 Initializing cache putting 1 -&gt; 1 res0: List[Map[String, String]] = List(Map(), Map(), Map()) ```
Could you provide a snippet of how you are mocking the object?
Stuff like this always reminds me to the eightfold path to monads on "What I wish I knew when learning Haskell": 1. Don't read the monad tutorials. 2. No really, don't read the monad tutorials. 3. Learn about [Haskell types](http://book.realworldhaskell.org/read/types-and-functions.html). 4. Learn what a typeclass is. 5. Read the [Typeclassopedia](http://wiki.haskell.org/Typeclassopedia). 6. Read the monad definitions. 7. Use monads in real code. 8. Don't write monad-analogy tutorials. I'm sure there is a Scala version somewhere, but the first 2 and last 3 apply to every language.
Your value `cache` above _creates an empty cache_, so it might as well be called e.g. `initCache`. Then the idea would be to create it in your application's main method and pass it into the components/functions that need it: ``` initCache.flatMap { cache =&gt; ... doSomething(cache, x).flatMap(y =&gt; doAnotherThing(cache, y)) ... } ``` Note that any function that takes the cache as an argument will be of type `IO[...]`, because accessing the cache is an effectful operation.
So I am currently tinkering around with pretty much exactly the same problem. A better interim solution is to use `mapPartitions` rather than `map` this will allow you to use caches per partition rather than per row. Now basically the problem is that you end up with `Dataset[IO[A]]` and I don't know if it's even possible to sensibly implement a `.traverse` for `Dataset` let's assume no for now. So for now you probably have to do `.unsafeRunSync` in your `mapPartitions`. Frameless might be interesting but I do not know how they do suspension internally. I am currently trying to figure out how to keep cache state alive over the whole executor lifetime using broadcasts but this will be impure code due to spark. Basically after each `mapPartitions` iteration you'll have to pull out cache state and put it into the broadcastable but I haven't figured out yet how to do this for things that aren't serializable. This should be possible because everything can be executor local.
Here's an example of the sort of code I'm writing: class PaymentControllerSpec(implicit ev: ExecutionEnv) extends PlaySpecification with Mockito with FreezeTime { val paymentRepository = mock[PaymentRepository] val app = new GuiceApplicationBuilder() .overrides( bind[PaymentRepository].to(paymentRepository), ) .build() class Context extends WithApplication(app) with Scope { val controller = app.injector.instanceOf[PaymentController] } "paymentController" should { "process a refund" in new Context { val fakePayment = Payment(BigDecimal("3.4", "faulty item")) paymentRepository.createRefund(any, any, any) returns Some(fakePayment) paymentRepository.findCaseById(any) returns Some((1234, fakePayment)) val result = controller.processRefund()(FakeRequest().withFormUrlEncodedBody(someFormData).loggedInAs(testUser).withCSRFToken) status(result) must_== OK contentAsJson must_=== Json.obj( "errors" -&gt; Json.arr(), "id" -&gt; 1234 ) there was one(paymentRepository).createRefund(any, any, any) } ... * I'm using mocks * I'm using GuiceApplicationBuilder for a unique app instance and individual dependency overriding * I mock out database access (in controller tests at least) * I'm not hitting real servers. * I'm not sharing mocks * The test itself doesn't attempt to use Futures or any concurrency within itself. Yet sometimes one or two just hang with a future timeout exception: sbt.ForkMain$ForkError: java.util.concurrent.TimeoutException: Futures timed out after [20 seconds] at play.api.test.ResultExtractors.status(Helpers.scala:379) at play.api.test.ResultExtractors.status$(Helpers.scala:379) at controllers.VerifiedControllerSpec.status(VerifiedControllerSpec.scala:20) at controllers.VerifiedControllerSpec$$anon$4.$anonfun$new$25(VerifiedControllerSpec.scala:141) at controllers.VerifiedControllerSpec$$anon$4.delayedEndpoint$controllers$VerifiedControllerSpec$$anon$4$1(VerifiedControllerSpec.scala:141) at controllers.VerifiedControllerSpec$$anon$4$delayedInit$body.apply(VerifiedControllerSpec.scala:134) at play.api.test.WithApplication.$anonfun$around$2(Specs.scala:46) at play.api.test.PlayRunners.$anonfun$running$2(Helpers.scala:75) at play.api.test.PlayRunners.runSynchronized(Helpers.scala:52) at play.api.test.PlayRunners.runSynchronized$(Helpers.scala:48) at play.api.test.Helpers$.runSynchronized(Helpers.scala:604) at play.api.test.PlayRunners.running(Helpers.scala:73) at play.api.test.PlayRunners.running$(Helpers.scala:71) at play.api.test.Helpers$.running(Helpers.scala:604) at play.api.test.WithApplication.around(Specs.scala:46) at play.api.test.WithApplication.delayedInit(Specs.scala:37) at controllers.VerifiedControllerSpec$$anon$4.&lt;init&gt;(VerifiedControllerSpec.scala:134) at controllers.VerifiedControllerSpec.$anonfun$new$23(VerifiedControllerSpec.scala:134) Like I say, using an implicit to change 20 seconds to 3 minutes doesn't seem to have an impact either (apart from the error message - which at least show Ive applied the change correctly)
It’s simple, but apparently difficult to explain. It’s difficult to explain monads without first understanding functors. A functor wraps a piece of data and provides a useful way for working with that data. The container usually has some kind of additional contextual information. A functor cannot modify the contextual information, but a monad can. Imagine we are implementing our own type class for handling errors. We’ve got two members of the type class: Success and Failure. Both of these wrap a piece of information. Let’s say we’re trying to look up a value in a database. If it works then we might return Success(“ask_me_about_cats”). If it fails then we’d return some kind of error like, Failure(CannotConnectToDatabase). Now we’ve got these wrapped values, but how can we work with the values in a type-safe way? That’s where the Functor’s map function comes in. Here’s the simplified type signature: case class Functor[A] { def map[B](fn: A =&gt; B): Functor[B] } So if we have some kind of functor wrapping and A, and a function that converts an A into a B, then we can use the map function to get a functor wrapping a B. That makes sense so far, right? So if we wanted to get the length of the username that came back, it might look something like this: val username: Failable[String, DatabaseError] = lookupUsernameById(42) val usernameLength: Failable[Int, DatabaseError] = username.map(_.length) This is working well, but now we need to write a function that recovers from an error. There’s a spot where we might get a DatabaseError and we want to recover from it. That means we need to convert from a Failure to a Success. The problem is that map’s type signature makes that impossible. We can change the wrapped value, but not the context. So we need something that allows us to change the wrapped value AND the context. That’s where monads come in. In Scala, monads come with a flatMap method. Let’s look at a simplified definition. case class Monad[A] { def flatMap[B](fn: A =&gt; Monad[B]): Monad[B] } There’s only one major difference here. Functor’s map takes a function returning a B, while Monad’s flatMap takes a function returning a Monad[B]. It seems like such a minor change, but it opens a lot of possibilities. So now we could do something like this: user.flapMap(u =&gt; if (u.isAdmin) { Success(u) } else { Failure(UnauthorizedAccess) }) So if the user isn’t an admin then we error out. We can only do that because flatMap allows us to switch from a Success to a Failure. That’s because flatMap wants you to return a wrapped value, and map wants an unwrapped value.
Curious to know what the pay range is 
Sounds like you're not using the mock because it's not injected or provided correctly. In a lot of languages doing this `val m = mock[Object]` means all behind-the-scenes-calls to `new Object()` would return the mock instead, but in Scala you need to need to do the wiring yourself.
Functor.map expects a function `A =&gt; B`. If you want to map but with two arguments aka if you want the function to be `(A, B) =&gt; C` then you need Applicative. Applicative extends Functor so more generally if you want to map it probably an applicative. The major difference is Applicative has `pure` where as `Functor` does not. There is also `Apply` which is just Applicative without `pure` (actually Applicative just extends Apply to add pure). NOTE: this is a simplification. There are more differences. Also I'm a noob at theses things.
I personally have no problem with local mutation. It's pretty much essential for the best performance and sometimes it's just the cleanest, clearest way to write the solution. I even think it's fine to have the mutable state cross private methods so long as the state was created in a public method of the same class - it's still an implementation detail at that point. That said, I generally try to consider mutation a specialized tool. It's there for when you need it, like a drill bit that cuts square holes. It's not all that frequent that you need a square hole, and likewise it's not all that frequent that you really need mutability. But Scala is not a pure functional language so if you can make good use of mutability then pretending is doesn't exist just makes your life needlessly difficult, like requiring a square hole and saying "nah, I'll just chisel it out instead"; you may get there, just slower and not as cleanly.
I like [this](https://hackernoon.com/functors-and-applicatives-b9af535b1440) explanation a lot. I understand them to a degree, but I feel like there's a deeper intuition I don't have with them. Basically, when I think of them I think "paralell" and when I think monad I think "sequential" in terms of processing. But, I'm also a functional noob. The more I understand over time, the more questions I end up with. Ha
Part of the problem also that we call monads different views of what a monad is. It's as much a "monoid in the category of endofunctors" as it is a programming pattern. Since we also call monad the functor itself, which is endowed with a monadic structure (return/bind eta/join), we are guaranteed to be confused even with good tutorials.
I tried not to do that here: [https://www.jamesward.com/2017/10/02/when-you-hear-monad-think-chainable/](https://www.jamesward.com/2017/10/02/when-you-hear-monad-think-chainable/) &amp;#x200B; Please give me feedback so I can make it better. &amp;#x200B; Disclaimer: Many FPers would say that this doesn't really explain what Monads are and hides some of the value of Monads. But hopefully it is still helpful in your journey to understanding Monads.
Read the original Eugenio Moggi papers - [here](https://github.com/cohomolo-gy/haskell-resources) (they're the first few links). The motivators is modeling programmatical equivalence (finding when two lambda terms are equivalent is hard), and it turns out one of best ways to solve this for many models at once is to construct a logical framework for talking about classes of models of programs. Monads, in the not-so-simplest sense, model the semantics of lambda programs nicely in certain categorical settings, along with their associated Kleisli constructions (all things you will learn about in the paper). Without deferring to algebra, or category theory, that is the basic notion of why we care. Now, considering how they function, you have really 3 things to be a monad: class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b -- it must be a functor class Functor m =&gt; Monad m where return :: a -&gt; m a -- i must be able to insert values into this class of program (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b -- i must be able to compose different programs of a given class, taking different values. It just so happens that within the theory, Monads are endofunctors, and `(&gt;&gt;=)` is the way one composes Monads - an all important principle for Functors. This is a stronger condition than your average functor, but it is still a composition law for functors nonetheless. It turns out that Monads arise naturally from dualities (adjunctions) in mathematics - they appear everywhere from abelianization maps (seen as an endofunctor in Grp), double negation, and the more common list, maybe, tree, state, etc. functors. If you'd like to know more, and work through some more examples, feel free to PM me for an invite to a discord where we talk about these things. 
Yeah, there's many different ways of explaining these concepts... For instance I've heard a lot of people explain functors as a way to lift functions from `A =&gt; B` into `F[A] =&gt; F[B]` which is technically true, but then usually the first application of functors is the `map` function which is quite a bit different from that definition (for beginners)
That stack trace points to `status(request)` as never terminating, which implies then that `controller.processRefund()(...)` returns a `Future` that is not completing. Without knowing what `processRefund` does I don't think I can help much more, but the failure to complete may be unrelated to what you have set up in the test so far and more what you haven't, e.g. something that should be mocked but isn't, or an exception in some other dependency of the controller that is not propagated to the controller itself. If you have control over the `ExecutionContext` in use by the controller/test and it's threaded through the application correctly, try replacing it with one that runs on a the same thread as that may yield a complete non-async stack trace or easier debugging. &gt; I'm not sharing mocks Unless that is the only test using the `Context`, then the `paymentRepository` is likely shared since it is supplied to every `PaymentController`, presumably. So if two tests run in parallel they could be affecting each other, especially if one resets the mock or anything. I'm not sure how granular the parallelism is for SBT - if it's class-level you're probably fine but if it's test-case-level then you could have an issue.
If you're interested in getting involved, you should look at the new [scalaz-actors](https://github.com/scalaz/scalaz-actors) project. 
In Scala, you can map a `Set`, flatMap an `Array`, and combine two `Map`s, so you need a stronger set of conditions (hint: laws) :) The principle for Applicative functors can be expressed in two ways - coinductively (what does it do?), and inductively (what analogies and laws can we draw prior to its definition that becomes its definition?). 1. Consider the following snippet of code for the coinductive definition: class Functor f where (&lt;$&gt;) :: (a -&gt; b) -&gt; f a -&gt; f b class Functor f =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b liftA :: Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b liftA = (&lt;$&gt;) liftA2 :: Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c liftA2 f a b = liftA f a &lt;*&gt; b liftA3 :: Applicative f =&gt; (a -&gt; b -&gt; c -&gt; d) -&gt; f a -&gt; f b -&gt; f c -&gt; f d liftA3 f a b c = liftA2 f a b &lt;*&gt; c liftA4 :: Applicative f =&gt; (a -&gt; b -&gt; c -&gt; d -&gt; e) -&gt; f a -&gt; f b -&gt; f c -&gt; f d -&gt; f e liftA4 f a b c d = liftA3 f a b c &lt;*&gt; d And so on. In the context of `fmap/(&lt;$&gt;)`, `Applicative` functors admit induction on the arity of `f`, with `fmap` as its base case, and still remain coherent. This looks exactly like a monoidal product inside of some context `f`! 2. Inductively, `Applicative` functors can be described as a "higher-order Monoid" of functors, in that `Applicatives` form a monoid over the product of any two functors (a product called _Day convolution_). You can generalize the notion of `Applicative` (which exists currently as a monoid of endofunctors) to that of a true Monoid object in the category of Functors thusly: data Day f g c where Day :: forall a b. f a -&gt; g b -&gt; ((a, b) -&gt; c) -&gt; Day f g c for which we obtain a higher-order functor i this category which forms our monoid object: instance HFunctor (Day f) where hfmap nt (Day x y t) = Day x (nt y) t ffmap h (Day x y t) = Day x y (h . t) So there you go. `Applicative`s!
Good advice; one small note: combine: Semigroup combine + empty: Monoid
I really liked John de Goes' presentation on Free Applicatives: [https://www.youtube.com/watch?v=H28QqxO7Ihc](https://www.youtube.com/watch?v=H28QqxO7Ihc) . It might seem that taking things up to the Free level would complicate things further, but I found the conceptual insights presented there very helpful in clearly distinguishing the underlying concepts.
Scala (picklers)[https://github.com/scala/pickling] are an extension of this idea, with support for OO ((paper)[https://infoscience.epfl.ch/record/187787/files/oopsla-pickling_1.pdf]) The theme is relevant for people who like to understand this part of the language
Well, I read the ORM-TO-SLICK document to catch the basic idea. I am now trying to make it happen in Quill. I am trying to map two tables through the join to case classes. However it is still missing something. How to get simple one-to-many result? I am giving you my Quill example, however I guess Slick etc. would be similar. &amp;#x200B; `override def getByIdWithTasks(id: Int): Future[Option[(Customer, List[Task])]] = {` `val q = quote {` `for {` `(c, t) &lt;- customers.filter(_.id == lift(id)).leftJoin(tasks).on((_c, _t) =&gt; _c.id == _t.id)` `} yield (c, t.toList)` `}` [`db.run`](https://db.run)`(q).map(_.headOption)` `}` I just want to get a Customer object and a List of his Tasks in one DB round trip. &amp;#x200B; This code generates error: scala.reflect.macros.TypecheckException: class List is abstract; cannot be instantiated
Or the for equivalent `(for { p1 &lt;- Option(foo.getMaybeNull) nested &lt;- Option(p1.getNestedMaybeNull) } yield nested).getOrElse(fallback)`
SQS is great and if the frequency isn’t too high you can operate on the free tier. FIFO queues can be used to have all nodes register the same job and have it processed once with deduplication AND in order with retry built in.
This place is sketchy though it might pay alright for Canada. I’m almost certain that their Glassdoor ratings are fake because when I worked there, they were sitting at a solid 2 stars. Throwaway for obv reasons.
Can you `zip`/`join` it to do `(F[A], F[B]) =&gt; F[(A, B)]`? Probably an Applicative. Futures, Options, Lists (these have two possible definitions Applicative: item-by-item zip and cartesian join) Most people can intuit what `zip` does while `ap`'s signature takes a lot more pondering, despite them being isomorphic. I also think `flatten` is a lot more intuitive than `flatMap`, again even though they are isomorphic. People with zero FP experience probably have already used some variant of `.zip` and `.flatten` in their programs, and it's not much of a stretch to generalize it from lists to options, futures, eithers, ...
What's your stack?
I've been here 2 years and I don't really know what this is referring to :/ The company has grown significantly, I'm not sure the way it was is how it used to be in any case. It's not "sketchy" by any means though.
https://old.reddit.com/r/scala/comments/9w7e4s/explaining_monad_or_functional_programming_or/
Sometimes fun projects turn into something big &gt; Hello everybody out there using minix – &gt; &gt; I’m doing a (free) operating system (just a hobby, won’t be big and professional like gnu) for 386(486) AT clones. &gt; ...
:)
In `ExternalService.doException` you throw an exception immediately rather than inside an AsyncResult. You need to create the AsyncResult or Future first. I don't know about your AsyncResult, but if you were using `scala.concurrent.Future`, you could do it like def doException(): Future[Int] = Future.failed { println("do exception") new NullPointerException("run time exception") } which creates the result in the current thread, or def doException(): Future[Int] = Future { println("do exception") throw new NullPointerException("run time exception") } which creates the exception in another thread.
I would use a broadcast variable. [https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables](https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables)
&gt; Scientific Software Engineer : https://grnh.se/66f7d1c31 &gt; &gt; Backend Eengineer : https://grnh.se/70335ee91 &gt; &gt; Infra Engineer: https://grnh.se/97339da41 &gt; &gt; Full Stack Engineer: https://grnh.se/b6998d121 &gt; &gt; Link to general job page: https://grnh.se/e6a21c281 Those links are broken, unfortunately.
Hey thanks for bringing it to notice. Links are fixed now. Appreciate the help. :)
On the Scala marketing side, what impact is Kotlin having on the language in terms of adoption? Is it a half-way house in terms of Functional programming that people will regretting not committing all the way and coming to Scala?
We will probably get heterogenous lists built into the language / std lib in a generic form. Will we also get the same for coproducts? Can pattern matching be made to just be syntatic sugar, built upon plain old functions and types when we have the generic products/coproducts, similar to for-comprehensions? (at least for the typesafe part of pattern matching - compare with the scalazzi subset of Scala) &amp;#x200B;
You should probably clarify that with "remote" you just mean "remote only in the US".
As someone already mentioned the `Cache` should be shared with the components that need to access it. So basically `flatMap` once and pass it as a parameter. And I would go even further and say that wrapping all the side-effects in `IO` does not mean you're doing FP. I gave a talk about it at Scala IO and going to give a similar talk later this year at Scala eXchange where I talk about "wrapping all the things in `IO` is not FP". TL;DR is abstract over the effect type `F[_]` and operate over interfaces (tagless final algebras).
Lucid Software | Software Engineer in Application, Dev Tools, or Site Reliability Engineering | Salt Lake City, UT, USA | Onsite | Full Time Lucid Software is the creators of Lucidchart and Lucidpress - world-class web applications that push the boundaries of what is possible in your browser. The entire application back end is written in Scala. The frontend stack uses TypeScript, Angular, WebGL, and the Google Closure Compiler. The build system is Bazel. We are hosted in AWS. We are a ~400 person company outside of Salt Lake City Utah, that is growing like mad. We are looking for - Application engineers to help build applications that users love. - Development tools engineers to help build a development experience that engineers love. - Site reliability engineers to help build a scalable, reliable, and performance production system. More information and the application can be found here - [Application Engineer](https://www.golucid.co/careers/f9cb83be-e016-4be5-91dc-598e17e8d04c?team=Engineering) - [Development Tools Engineer](https://www.golucid.co/careers/c8de139c-6da7-44dc-a4af-95052aa2a086?team=Engineering) - [Site Reliability Engineer](https://www.golucid.co/careers/be16cacc-4cd3-4be0-9ec0-070646f9ec69?team=Engineering) Please DM me if you would like to chat about any of the positions. Please mention this Reddit thread if you apply :D
I feel that it is unhelpful to treat the language as something that exists in an vacuum, unexposed to concerns like marketing. Competitors, adoption, public sentiment, etc. are all relevant to the people designing the language. Peyton Jones can be included in this question by including Haskell and Kotlin in the question. A few big names in the Scala community have left for Haskell recently and Kotlin is eating the adopters interested in a 'better Java.'
You've been reading Hacker News and Reddit and you underestimate the effects of a [vocal minority](https://tvtropes.org/pmwiki/pmwiki.php/Main/VocalMinority). I cannot tell you the number of times I've heard "Java is Dying," for example. It's always marketing, and it's always undebatable on the merits and has little to do with the language itself. Please check out the [Scala FUD guide for newbies](https://kubuszok.com/2018/scala-fud-faq-for-newbies/) for a complete list of all the FUD memes that have popped up at various times -- like Typesafe was giving up on Scala because they changed the name to Lightbend, or Odersky was leaving Scala for Dotty! It's so tiresome, because for every good argument you could have, you end up going through a gish gallop of crap non-sequiters.
Oh sounds great!! Here is the question I would like to ask: Considering that both languages are going towards dependent types, dependent object types for Scala and DependentHaskell to Haskell. I wonder how they plan to handle proofs. More precisely DOT would benefit greatly from the ability to witness equalities of paths and could rely on a solver to do so like FStar does with Z3. Is it something that is studied/planned? Same question in Haskell about plans to integrate Liquid Haskell ideas into mainline GHC. Thank you very much.
They still don't work for me, sadly. The first one takes me to https://citrine.io/jobs?gh_jid=262559&amp;gh_src=66f7d1c31 which has (almost) no content and the title "page not found". Maybe others are having more luck.
Don't see the point in asking Martin about Kotlin to be honest. And how exactly do you expect him, or indeed anybody, to quantify the impact of Kotlin on the adoption of Scala?
My question is: is there any work to bring effect handlers (like Koka) to Scala? Or maybe an effect tagging system like Nim?
I think the thing that people forget is that this is not a zero sum game. There are more programmers now than there were a few years ago, and languages grow all the time without being the hot new thing on the block. I remember when people were terrified that Microsoft was going to kill Java with J++, and then people were worried about the brain drain to interpreted languages generally. But FUD is everywhere, and it has little to do with reality. You don't see it in other languages, but people get the exact same FUD about Ruby, about Python, about Javascript even. It's everywhere. What kills languages (IMHO) is not attacks from other languages, but lack of investment in the language itself. Perl died because there was no development on it and it couldn't recover from its mistakes. Smalltalk died because... well, money and investment overhead and Reasons. But Odersky (and the Scala development team in general, notably Heather Miller) is doing exactly that investment. They are working on leveraging the features of Scala that other languages don't have and making it more powerful. No other mainstream language does implicits the way that Scala does. I've heard you can sort of do implicits in Haskell, but it's fiddly and very much against the grain of the language, and these are force multipliers for everything down the line. TL;DR Odersky gets shit done. It's not Odersky's job to hold your hand and tell you everything's going to be fine or talk about marketing, so don't ask him about it.
I had updated the links in the original comment. I do apologies for the confusion though. Being long time lurker hasn’t helped my posting skills, specifically job postings. (Pretty new to this stuff). Thanks.
Here's an old book about OS with Java example, perhaps you can reuse it and convert it to scala :) https://www.amazon.com/Operating-System-Concepts-Abraham-Silberschatz/dp/047050949X
Thank you for saying this too! About people leaving Scala for Haskell, the ones bashing Scala will soon realize that even if Haskell is a great language it has its own flaws too. Not all people switching from Scala to Haskell generate FUD! So I'm only speaking about the ones actively bashing Scala. Those ones have a very idealized view of Haskell and very little practice of the language. Just give them a few months to miss Scala relyable building 😁
I'd ask how they got soooooooooooo fineeeeeeee.... &amp;#x200B; Kidding I'd ask about the nature of open source and how important it was for the development/take up of their languages.
\[JOB\] - Tech Lead &amp; Mid Level Scala Position In London for Tech Start Up ( Europe Based) 📷 [https://functional.works-hub.com/jobs/software-engineer-platform-team-lead-in-london-united-kingdom-7de9a#utm\_source=reddit&amp;utm\_medium=job&amp;utm\_campaign=f.bojescu](https://functional.works-hub.com/jobs/software-engineer-platform-team-lead-in-london-united-kingdom-7de9a#utm_source=reddit&amp;utm_medium=job&amp;utm_campaign=f.bojescu)
Bit more info on this one, they are open to paying a relocation package within the USA if you're keen on a move to the Bay Area. There product is an IoT air quality device, they've got some great enterprise level applications as well including in the Salesforce building! The last engineer I placed there speaks very highly about life there, and points to getting to work closely with each team, from hardware engineers to data scientists and the technical challenges of building a backend that ingests real-time data from 6 different sensors in the device as some of the standouts. FYI, he was primarily from a LISP background, so if you've been using another JVM language and can demonstrate projects in Scala I can definitely advocate for you. The job posting lists commercial Scala experience as a requirement but I have a good relationship with the technical team there and can make a case for a strong Java engineer who is excited about using Scala in production if we both feel its a good match!
Some questions that I have: * Given that you could talk to yourself (and other core developers involved), what would you tell yourself? In other words what are some design decisions that are there because of backwards compatibility reasons that you wish would be different? For instance, would it be make sense to make all classes and methods final by default, and only open for extension like Kotlin has deliberately? * Scala has always been about mixing types, objects and functions. How does imperative programming play in this (imperative in my mind is not the same as object oriented)? Especially given that imperative programming is quite contradictory to functional programming. Is imperative programming just there for Java interop and performance reasons? And how will this be in the future? Are \`var\` and \`while\` part of idiomatic Scala? * How do you want Scala (or Haskell) to look like in 10 years? What are some larger things on the horizon that will not make it any time soon? * What is a third-party library that conveys the spirit of the language the best in your opinion (can be answered for both Scala and Haskell)?
It's not competitive to the likes of FB + Google, you're absolutely right. But $160K for 2 years' experience in San Francisco puts you close to the 75th Percentile, at least according to Stack Overflow https://stackoverflow.com/jobs/salary/results?l=San+Francisco%2c+CA%2c+United+States&amp;ed=1&amp;ex=3&amp;ff=1&amp;dr%5B0%5D=BackendDeveloper&amp;tl%5B0%5D=scala
Good luck finding a parking space in SoMa!
Hi giltig, Sorry for late answer, I didn't see the notification. Thanks for reading it! 1. We validate everything in the REST API querying the eventually consistent DBs. So if you need atomicity in your validation, this architecture won't work. We didn't need this in our case. 2. The "map" you're talking about is a distributed KTable. So everytime we get a new Post in, it will update the value for the key in the KTable. I don't see where is the inefficient part of this? Thanks
Those changes should be easier. My understanding is that 2.12 was so difficult because of the way scalac significantly changed its output JVM bytecode, making for job serialization changes. Think ABI rather than API.
Um. We don't do homework here. What you need to do is look up what you want to do and do it. &amp;#x200B; If it really is a small bit, here's how you can make progress, quickly: &amp;#x200B; 1. Use the repl. That's sbt console. Copy and paste your code into it. It will fail when things don't compile. 2. Map out your types from what is coming into your code to what you want your code to be: Map\[String, Char\] =&gt; List\[Char\] =&gt; (Foo =&gt; (Foo, String)) =&gt; (Foo, String) 3. Write function signatures that match your types val mapToList: Map\[String, Char\] =&gt; List\[Char\] = ??? val listAndFooAdder: List\[String\] =&gt; (Foo =&gt; (Foo, String)) =&gt; (Foo, String) = ??? val assignment: Map\[String, Char\] =&gt; (Foo, String) = ??? //use partial application (google it) and function composition (google it) to combine your previous two functions. 4. Write some simple tests that call your code and assert an equality on the return. If you do things this way, you will end up with many small, independent parts that are easy to test and build together. The type signatures will help you understand how to implement them. If you get stuck on one particular problem, move on to the next one. Repeat till you are done. &amp;#x200B; If you fail, you will have learned an important lesson about procrastination. The sidebar has a ton of info. For specific questions, (not answers to homework, but things like, what is map), you should be able to look them up on stack overflow. &amp;#x200B; Good luck.
I really don't think so. Currently, Scala still has a lot more to offer regarding functional programming. And with Scala 3.0 on the horizon I don't think that Kotlin will take over any time soon (as the facto functional-oriented language on the JVM). Also, from what I gather, people like Kotlin because it is a better Java. Not because it is a great functional programming language.
Removing for not being a post with actual specific questions about Scala. I'm not going to say that homework assignments are always forbidden, but they should at least be actual well-formulated questions about Scala, not just asking for PM's of help.
I'll leave this here because there's already discussion in this thread, but for next time, if there is a Who's Hiring thread on the front page, like there is right now, please post these advertisements there.
Understandable I’ll change it
The most noticeable difference between a `Task` and a `Future` is a `Task` is lazily executed. That has benefits like not having to track the execution context everywhere, and is sometimes simpler to compose a lot of tasks together because you don't have to create it at the point where it's okay to immediately start executing. However, there is nothing you *need* Monix to be able to do, and it requires a shift in your thinking to use cleanly. It's largely a matter of style and preference.
**InCrowd | Mid - Senior Backend Software Developer | Watertown, MA | Onsite | Full Time** &amp;#x200B; Simplify, Automate and Disrupt &amp;#x200B; InCrowd is changing the way Life Sciences companies do primary research about their products. The InCrowd dev team has already created a revolutionary self-service survey engine that allows customers to achieve in days what previously took weeks. We're building on that success to transform other areas of research through automation and simplification. &amp;#x200B; We're Agile and Semi-Full-Stack &amp;#x200B; We follow the agile methodology pretty closely, with two week sprints, daily scrum, and retrospectives. We're also striving to increase our "bus number" by working across disciplines often. &amp;#x200B; In this role you'll be an integral part in improving our existing Scala API and helping us take it into the future as we build out our next series of event and data driven services. &amp;#x200B; **Responsibilities:** &amp;#x200B; Build internal and external facing APIs that are easy to reason about, testable and scalable &amp;#x200B; Work closely with product, design and stakeholders to plan new features that help our users streamline their workflow &amp;#x200B; Identify and improve performance bottlenecks in existing services &amp;#x200B; Help onboard, mentor and grow junior developers into the codebase &amp;#x200B; **Minimum qualifications:** &amp;#x200B; BS degree in Computer Science or related technical field, or equivalent practical experience &amp;#x200B; 3+ years experience with Scala in a production environment (Scala is preferred however, we are more than happy to talk to Java developers who are interested in learning Scala. In this case, some previous exposure to functional programming languages or the Java Streams api would be beneficial.) &amp;#x200B; A strong understanding and production experience with SQL/RDBMS. Comfort with NoSQL databases desired &amp;#x200B; **Bonus Points:** &amp;#x200B; You value test driven development and have experience with testing in production (ScalaTest, ScalaCheck, JUnit) &amp;#x200B; Experience with some of the following: Cats, Scalaz, Monix, Shapeless, Doobie, HTTP4S &amp;#x200B; Front end experience with Javascript in a production environment (additional bonus points for React) &amp;#x200B; Production grade devops experience with AWS and Docker &amp;#x200B; **Tech Stack:** &amp;#x200B; Scala (Play, Akka, Monix), some backend services in Racket, JavaScript &amp;#x200B; MySQL, DynamoDB, Kinesis, SQS, S3 &amp;#x200B; AWS Lambda (Java, Node and Python) &amp;#x200B; **Company Benefits:** Stock options Health Insurance at 70-80% paid Generous vacation time 401k Flex remote days Weekly catered Lunches Modern, comfortable office (cafeteria and gym onsite) &amp;#x200B; Please email [techjobs@incrowdnow.com](mailto:techjobs@incrowdnow.com) with your resume to apply
Collection changes are both ABI and API incompatible. Projects have already started splitting their code into version-specific parts and conditionally compiling subsets according to the version.
Thanks for the YouTube ref
Not trying to talk for /u/MysteriousGenius, but quite likely is that they *are* doing FP in their library, but now occasionally just need to use it in Spark. At least this is what we're facing with very similar problem. As your approach, I don't think it is going to work, because problem is not in `Cache`, but in the fact that `process` returns `IO[A]` and at least in our case it would very hard to get rid of that because along with Spark we have fs2 backend that obviously relies on `IO` heavily. But in case it is possible OP just wraps something side-effecting into `IO` - then for sure it would be better just to get rid of it.
now that I see this thread, I think my question could fit here: https://www.reddit.com/r/scala/comments/9x25zt/whats_the_scala_plan_to_keep_up_with_the_new_jdk/? JDK started releasing new versions at every 6months. What's the scala plan to keep up with the new release cadence? furthermore, are there any plans to use new java and jvm features to backup scala features, probably increasing performance and/or bytecode generation? for instance: java var, value classes, records, and maybe even fibers/coroutines?
Scala already does releases every 6 months and done so for some time. As for specific JDK features, many of the things you list exists has improvements in the Java language/compiler and don't have a bytecode or VM feature to implement. Others you listed aren't finished features, or features in preview.
Even if they aren't finished features, the scala team is probably following the feature evolution no? I imagine they have some sort of plan, even if not 100% defined. 
Oof. I hate these patronizing SF comp replies.
[https://www.reddit.com/r/scala/comments/6t097r/what\_happened\_to\_scala\_for\_dotnet/](https://www.reddit.com/r/scala/comments/6t097r/what_happened_to_scala_for_dotnet/)
Dotty will have unions, so polymorphic variants and indeed coproducts could be expressed with unions: def x[Rest](poly: String | Int | Rest)(implicit smth: DoSomething[Rest]) = poly match { case _: String =&gt; case _: Int =&gt; case rest =&gt; smth.apply(rest) }
Oh, but they are tagged, the runtime class is the nominal tag. &gt; I can have a Coproduct of two Strings whereas a union type of two strings is just a string and does not allow to distinguish between them. The coproducts are actually not tagged, they depend on order i.e. you can't really distinguish between two strings because the coproduct may have been reversed before been passed into your function. The solution with unions is to introduce custom tags, for example with singleton literals: ``` def x[Rest](poly: ("Left", String) | ("Right", Int) | Rest)(implicit ev: RecordLike[Rest]) = poly match { case ("Left", s) =&gt; ??? case ("Right", i) =&gt; ??? case _ =&gt; ??? } ``` Now the branches are unambiguous and unlike coproducts, you can't "reverse" a union to create ambiguity. You can use an Int as an index instead of String to create positional unions similar to shapeless' coproducts.
Have a look at https://github.com/scala/scala-dev/issues/139
You can bundle it in an uber JAR and then `java -cp my-jar.jar MyMainClass`
Better support for functional programming in Kotlin has to come a long way before it is at the level Scala is at. A type class in Scala is fairly simple. Not as simple as in Haskell, but a lot easier than in Kotlin, that is really stretching with using extension functions in the Arrow library. And type classes are pretty common, also in non-fp to the max libraries. Such as Ordering in the base library, or JSON formatters in Play Json. I have rolled a couple of my own in production code I have been working on. I really doubt the same level is possible in Kotlin at the moment. Next to that are a lot of programmers used to more monadic programming in Scala than in Kotlin. A type safe null is a special construct, in Scala it is just an Option monad. Exception handling is just Java but safer, in Scala it is a Try monad. A Future is sort of a monad in Scala for a simple concurrency primitive (I don't really know what Kotlin provides here, but I doubt it is a Monad). Together with For comprehensions, higher kinded types, and ADTs make functional programming a lot more common in Scala. And will be more common in Scala 3 with better support for ADTs, GADTs, AnyKind, Union types, etc. Eta is interesting but not commonly used at all, so if you want a job you are more likely to go for Scala. As many people do at the moment. Also poor documentation, tooling and interop is nonsense. Intellij IDEA has a great plugin for Scala, and interop is fairly easy. Documentation could definitely be better, but isn't that poor. I think Kotlin is a good language, and likely to take over Scala. But not because it is so great at functional programming, but because it is great at being a better Java. Which is what most people want. In the end it is al speculation what will happen, I just doubt that Kotlin will be used more for fp than Scala in next couple of years.
A possibly better alternative to learnings answers to those questions is learning scala.
that's exactly what I was looking for :) will submit questions there if any doubts arise, thanks!
If you need a guide to tell you what a case class is then you'll not get far in a real interview.
What's the typical salary in SF for 7 years Scala 20 years Java?
We started with `Future` as well but later moved to `Task`. Other than the benefits /u/kbielefe already mentioned, it is sometimes a lot easier to do [parallel processing](https://monix.io/docs/3x/tutorials/parallelism.html) (batching / limits). And it also has an `Observable` implementation.
Half of these are just plain wrong.
A yearly Scala conference, not streamed... yet. But recordings are posted very fast after the talks. [https://skillsmatter.com/conferences/10488-scala-exchange-2018](https://skillsmatter.com/conferences/10488-scala-exchange-2018)
This is spam 
The code looks quite readable and concise. I would wish for some tests, but that's not strictly necessary. Nice! You could add Scala Native or GraalVM to your build. Then you would get a statically linked binary file, that runs without any JavaVM.
Thanks for the comment @affjskedbd. I appreciate your feedback and will look into it! &amp;#x200B;
What made you think that?
Anything that gets me closer to never using SBT again is a good thing in my eyes. 
Eric didn't "abandon" Scala either, he just switched jobs and doesn't use it anymore at work. I'm pretty sure he still maintains his libraries. 
Be interested to see a comparison with Coursier. 
I bet taking it off the JVM would make it faster / able to handle larger images faster. I'll look into it. And also, yes some tests would be ideal tbh, but I wasn't really sure *how* to test this at all. Thanks for taking the time.
Coursier is actually something you can use right now, for starters. More work on build tooling is great. Starting a project with a "private beta", a logo and zero code is ridiculous. I can't even take this shit seriously.
What do you want to do, produce a separate String for each pair? Also, is it really \`Map\[String, String\]\` -- only one file per directory? If so you can do \`for((dir, file) &lt;- args) yield s"$dir/data/$file"\` If you have a \`Map\[String, Seq\[String\]\]\` you would do \`for((dir, files) &lt;- args; file &lt;- files) yield s"$dir/data/$file"\` &amp;#x200B;
Exactly, perfect! Thank you
So you want to turn (101,100,010) into (1,0,1,1,0,0,0,1,0) ? val a= (101,100,010).flatMap(_.toString.chars) Idk I’m on my phone, something like that?
At least for GraalVM your guess is right on the spot: it gets [much faster](https://medium.com/graalvm/compiling-scala-faster-with-graalvm-86c5c0857fa3)
You are blatantly spamming. * all posts from your own domain * Every post reposted to 6-8 subs Stop. 
Coursier is not a build tool, it only deals with the dependency management. AFAIK Fury actually uses Coursier for dependency management.
How does this compare with [mill](https://github.com/lihaoyi/mill)?
Sorry for the click-bait. I couldn't resist.
He talked about this at [Scaladays](https://na.scaladays.org/schedule/preparing-for-scala-3). There are some patterns that are better expressed at a higher level.
It's not clickbait. It's actually a good title that summarises the main point. Thanks for sharing. 
There's a good talk from John going over what Fury is all about: https://youtu.be/OZoQmh0NR4w. 
They can't go away soon enough.
This is all nice and good until you realise that it brings yet more syntax to a language that is already syntax heavy (replaces 1 thing with at least 3). It also makes what are still essentially implicits less visible (specially the |=&gt; business). Thirdly, it reuses `with`, so it introduces more syntax ambiguity. So, while Odersky has a point about the implicits, witnesses seem like a bad solution that makes the language even more impenetrable.
You should use Mill!
I like their approach to testing if I understand it correctly its easier to write testonly and testquick kinda stuff. I don't know how mill handles this. 
Nice! Your lack of plugins could lead to a great feature: lightning fast Scala comes. The idea is to natively compile both, the build to and the Scala compiler with GraalVM. For the Scala compiler you would need different distributions containing different macros (or compile one distribution in the fly for the current project). You would then have basically no startup time for each command and a much faster throughout in the computer. First benchmarks look really [promising](https://medium.com/graalvm/compiling-scala-faster-with-graalvm-86c5c0857fa3): no startup time, 30% more throughout. This could become an USP for Furry that the competing build systems (SBT, CBT, mill) cannot easily get.
As they mention in the slides, Scala is not syntax heavy. Java, Kotlin, Swift and many others have a much more complex syntax (in terms of grammar size) and much more verbose. If you compare with (very mostly) untyped languages like Python (which has indeed a smaller grammar), then oviously Scala need some syntax for type related features. Extensions, erasure, implicit functions even if they bring more syntax also make things clearer and more efficient. Implicit classes are not the simplest things to understand for beginners but it is very useful. introducing a clean syntax for it is nice. Erasure of implicit is relevant performance-wise and implicit functions make the difference between methods and functions smaller so it makes using implicits again simpler. More synatx is not a bad thing by itself. If this added cost brings more value, then it is a good thing. implicits is one of the most complex and vastly misunderstood feature of Scala. Making them easier to use without encodings or logic programming mastery is nice. 
Does anybody know if there is a future for Metals? Its github page still says it's an experiment.
His proposal for witness-style implicit conversions seems to miss the ability to define dependent function implicit conversions.
I'm working full-time on Metals since September as part of my job at the Scala Center https://github.com/scalameta/metals/releases/tag/v0.1.0. So far most of the work has been to make "import sbt build" work OK. I'm optimistic there will be something to try out within the next week :) You can watch the repo for updates.
A new way to type implicits is not a solution to the mess that implicits brings. Also, this seems to be a nightmare for backwards compat efforts. Why not just work on enforcing good use of implicits via compiler flags? These types of decisions are why people are saying that Scala 3 is a new language and not an upgrade to Scala 2.
Do those changes go through the SIP process ? If not, why ? &amp;#x200B; While it's good to bring some novelties, they always seem super controversial when they are about language syntax changes; and rather spending X days coding the feature and arguing with the community afterward, why not proposing the feature and adopt it through the SIP process? &amp;#x200B; Is it because Dotty does not go through this process ? &amp;#x200B; Thanks for enlighting me
Language changes for Scala 3 will go through the SIP. Think of these PRs as getting incubated in Dotty so that the eventual SIPs are easier to review given the experience with how the changes fit in with the rest of Dotty.
I'm already watching the repo, but there was no updates for a month. Good to know that somebody works on it! Can't wait to try it out, thanks!
Most activity past month has been in https://github.com/scalacenter/bsp, my Metals fork and also https://github.com/scalacenter/bloop. LSP has limited out-of-the-box functionality for servers to notify the editor about long-running tasks such "importing sbt build" so that required extra work to implement. I will open a Metals PR in the next few days once https://github.com/scalacenter/bloop/pull/722 is in. 
A more restrictive type of implicit that can only do things that are considered "good ideas" very much could be a way to reduce the mess of implicits. That said, Martin explicitly states that this would cause existing-style implicits to be deprecated for removal further down the road, so it shouldn't be a compatibility nightmare either.
If they were already writing FP scala, you wouldn't have to explain them category theory.
Shitty clickbait title.
But that "small group of people" aka 90%+ Scala programmers who work with mainly Lightbend/Play/Spark and have little interest in FP are exactly the ones you'd have wanted to educate in the first place, because the others already know.
I am writing series of articles about programs composition in scala - mtl, tagless final, free. Part 1 - [http://dehun.space/articles/29\_oct\_2018-Better%20than%20IO,%20part%201.html](http://dehun.space/articles/29_oct_2018-Better%20than%20IO,%20part%201.html) Part 2 - [http://dehun.space/articles/15\_nov\_2018-Better%20than%20IO,%20part%202.html](http://dehun.space/articles/15_nov_2018-Better%20than%20IO,%20part%202.html) Feedback would be appreciated!
Basic difference is that fury is primarily a source based package manager, which means it doesn't deal with maven/ivy style jar's, rather the dependencies are linked directly against git repos/folders which need to be signed.
Dude is going around offering world class training and the best the Scala community can muster are some snide points about whether or not he's involved in Scala anymore, and downvoting his training course. Real class act, Scala.
\&gt; As they mention in the slides, Scala is not syntax heavy. Java, Kotlin, Swift and many others have a much more complex syntax (in terms of grammar size) and much more verbose. If you compare with (very mostly) untyped languages like Python (which has indeed a smaller grammar), then oviously Scala need some syntax for type related features. You are missing the point here, the issue is not that Scala is using less keywords. The issue is that the proposal reuses existing keywords in completely different contexts. In such a case, I would actually **prefer** if they came up with a new keyword \&gt; Extensions, erasure, implicit functions even if they bring more syntax also make things clearer and more efficient. Implicit classes are not the simplest things to understand for beginners but it is very useful. This is a bit of a red herring because \`implicit class\` is being removed/deprecated anyways. The only reason they existed in the first place was because of a Java quirk where everything is a class (including value classes). Then things like opaque types got added which threw this original assumption out of the window. \&gt; More synatx is not a bad thing by itself. If this added cost brings more value, then it is a good thing. implicits is one of the most complex and vastly misunderstood feature of Scala. Making them easier to use without encodings or logic programming mastery is nice. The only widely misunderstood/abused mechanic for implicits is implicit conversions and they are already deprecated. &amp;#x200B;
not for a couple of years
I think the biggest difference is that they take opposite approaches about how a build is defined. In Mill builds are scala programs, although they're very concise yet easy to understand thanks to the abstractions and constructs Mill provides. With Fury, the build is strictly data. Custom logic can be added only in specific ways. Also the data format is opaque; instead you access and manipulate the build definition by interacting with it from your shell. This is similar to how git works. 
Hmm id love for mill to link directly against git
&gt; Witnesses for Extension Methods And there the new modifier name makes zero sense. What does `StringOps` witness? It's an attempt to replace heterogenous uses of `implicit` with heterogenous uses of `witness`. You can near to nothing at the cost of having everyone learn new syntax.
Also &gt; `witness StringToToken for ImplicitConverter[String, Token] {` this is so much more inelegant than &gt; `implict def stringToToken(s: String): Token = ...` Nothing is gained with introducing the `witness` keyword IMO. Except adding another keyword, and Scala 3 seems to already suffer a bit from keyword inflation.
Exactly what i was looking for. Thanks
I am a full stack dev but only for Node. I never picked up Java, what would be the best way for me to learn Scala and the ecosystem around it? 
That's very much intentional. We _want_ to make implicit conversions harder and less obvious to write. It's the same reason why we prefer "asInstanceOf" instead of "as". Sure, "as" is much nicer, but since we want to steer you _away_ from casts we chose the longer name. The same holds for implicit conversions. 
I don't find that convincing. A feature should not lack elegance simply because it can be abused. That won't really deter the abuse of the feature, it just makes abusing code hard to read. Shouldn't we focus on restricting improper usage instead of obfuscating? Perhaps compiler flags? 
I did this workshop and it is really good for people with an OOP background. I found it specially good to build a notion of the big picture, something I couldn't get by reading blog posts. And John is a really cool guy. He's always keen to answer every question the group has and he's okay with spending time explaining some detail the group finds confusing. This interactivity is a real plus as opposed to just watching talks online. If you can afford to pay it (companies usually have budgets for staff training), I recomment it.
John has been very active in the community. See, for instance, the [ZIO project](https://github.com/scalaz/scalaz-zio/graphs/contributors?from=2017-11-12&amp;to=2018-11-18&amp;type=a). I believe you're confusing him with [Sam Halliday](https://twitter.com/fommil). John is usually critic about Scala, because he's vision is that Scala should be purely functional instead of a hybrid OOP/FP language.
For Scala-the-language I think Odersky's book [_Programming in Scala_](http://www.amazon.com/Programming-Scala-Updated-2-12/dp/0981531687) is a great intro. I found it extremely readable. One hurdle you might expect coming from JS based on my (limited) experience with JS is due to traditionally object-oriented concepts like inheritance or traditionally functional-programming concepts like closures have meanings and behavior in JS that are not quite the same as in most other languages despite the same terminology, and Scala is built around mixing OO and FP. So be prepared to relearn what some basic concepts mean in the context of a statically-typed mixed-paradigm language. For Scala-the-ecosystem I'd say start with the basic tools in the sidebar: IntelliJ + Scala plugin and SBT. They'll at least get you building and running some Scala code. I'd take finding libraries for what you need on a case-by-case basis, e.g. waiting until you actually need JSON serialization to before evaluating the various JSON libraries.
I find still quite confusing. Given that the goal is to make intent clear, and there already a number of new keywords (opaque, erased, witness, ...), I think the previous proposal - https://github.com/LukaJCB/typeclass-proposal/blob/master/docs/extensions.md - was more clear.
He has delivered a companion talk on deep learning as well. https://slideslive.com/38907971/dive-into-deep-learning-with-scala
I prefer the \`witness\` version because it's actually more explicit about what you're defining. You're creating some value (the witness) that allows for the implicit conversion from one type to another. That's actually easier for me to understand, even if a bit more verbose. 
&gt; an int, an empty string is not an int &gt; &gt; Try this in the schema: &gt; &gt; {"name" : "line_number", "type" : ["null", "int"]} &gt; &gt; Then pass nulls when they are needed. After that It showed an error 'expected start-union'. Got value_string. That was similar to an error I had recently, but that was an int but got VALUE_STRING. Which variable would I tweak next? 
Is it possible to automatically generate CRUD (create, read, update and delete) pages for different database tables in Play?
Anyone with protobuf experience? I’m currently using scalapb but I don’t want to couple myself to the generated classes so I have to write mapping functions back and forth in every component... I found protoless which seems a more direct translation between my model and protobuf, but it seems abandoned (and it also lacks verification between the .proto file and the encoding) TL;DR, does anyone have a nice approach to use protobuf without writing boilerplate everywhere?
Re question 1, the compiler provides syntax sugar for by-name method arguments. A thunk (lambda with zero arguments) is equivalent in concept (if evaluating it has no side-effects) but if the signature is a lambda then you must provide a lambda value (e.g. \`\_ =&gt; ...\`). Another consequence of by-name sugar is that the compiler controls evaluation of the argument, while if a lambda is used, its evaluation is explicit in the callee (e.g. in the body of \`def x(foo: () =&gt; T)\`, it is evaluated by \`foo()\`). A third point about by-name syntax is that it is only available in method signatures. If you need lazy evaluation in other contexts (e.g. a member of a case class value), you must use a thunk (lambda). This is why Cons is defined in terms of lambdas. Re question 2, I think by "scope" you mean "lifetime"? A lambda is just a value so their lifetime is the same as any other value. Creating a lambda value creates a closure as is familiar from any language with lexical scope. Re question 3, not clear what you're asking. The by-value and by-name definitions seem to be behaving as expected?
Use 4-space indentation for code blocks on Reddit. Github markdown isn't supported, alas. **question1** Nope. That's it. I am not sure what actually happens behind the scenes, ie: whether `=&gt; A` is just syntactic sugar for automatic call-site wrapping and usage-site unwrapping, but it's the equivalent functionality. That is to say, any expression passed by-name is executed every time the by-name argument is accessed by the function. **repl (ammonite) output:** @ def byVal(x: Int) = { println(x) println(x) } defined function byVal @ byVal({ println("test"); 5 }) test 5 5 @ def byName(x: =&gt; Int) = { println(x) println(x) } defined function byName @ byName({println("test"); 5}) test 5 test 5 Notice how with `byVal`, we only see "test" printed once. This happens when the expression is evaluated *prior* to being passed into the function body - strict evaluation. With `byNam`, we see "test" printed twice because it is executed each time the parameter is accessed inside the function body. **question2** Lazy vals are scoped to the function body like any other val. The difference is that the expression assigned to a lazy val is only executed upon the *first* usage of the val, after which it is *memoized* for the remainder of the function body. Note that this is different to by-name parameters, which are executed on *every* usage. However, the *scope* of the val doesn't change. It's cleaned up with the rest of the stack frame. @ def foo() { lazy val x = { println("test"); 5 } println(x) } defined function foo @ foo() test 5 @ foo() test 5 Note how see "test" printed on both calls to `foo()`. This is because we execute the lazy expression on each invocation of `foo()` - the scope of the memoization is per invocation, like any other function-scoped val. By the way, this leads to a very common pattern. Let's say you have an expensive expression that you want to pass into a function by-name but which you also need to access multiple times. A by-name parameter alone won't suffce since it will execute it on every access. Instead, you use a combination of a by-name argument and a lazy val. @ def foo(expensive: =&gt; Int) = { lazy val v = expensive println(v) println(v) } defined function foo @ foo({println("test"); 5}) test 5 5
#Reformatted: Hi guys, I have a few question as said in the title. Any additional resource or further reading are appreciated, too! The chapter starts providing this piece of code as a definition of a lazy List. sealed trait Stream[+A] { def toList: List[A] = this match { case Cons(h,t) =&gt; h() :: t().toList case _ =&gt; List() } } case object Empty extends Stream[Nothing] case class Cons[+A](h: () =&gt; A, t: () =&gt; Stream[A]) extends Stream[A] object Stream { def cons[A](hd: =&gt; A, tl: =&gt; Stream[A]): Stream[A] = { lazy val head = hd lazy val tail = tl Cons(() =&gt; head, () =&gt; tail) } def empty[A]: Stream[A] = Empty } *Question 1* the book introduces the syntax `=&gt; A` as a "nicer syntax" for `() =&gt; A` but then, playing around in the REPL they don't seem to be considered the same. My take on this is that `() =&gt; A` is literally a function that takes 0 arguments and returns an `A`, while `=&gt; A` is a call-by-name argument. Is it correct, is there anything deeper than this explaination behind the scene? *Question 2* also it's not clear what's the scope of the `lazy val`s declared in `cons`. Do they survive somewhere after `cons` returns, and any future reference to the `Cons` returned will refer to them? How does it work exactly? def ones: Stream[Int] = cons(1, ones) def constant[A](a: A): Stream[A] = cons(a, constant(a)) def constant_1[A](a: A): Stream[A] = { lazy val stream: Stream[A] = Cons(() =&gt; a, () =&gt; stream) stream } def constant_2[A](a: =&gt; A): Stream[A] = { lazy val stream: Stream[A] = Cons(() =&gt; a, () =&gt; stream) stream } *Question 3* In exercise 5.8 the reader is asked to generalize `ones` to create a `constant`. The solutions mention `constant` and `constant_1` saying that the latter is "more efficient since it's just one object referencing itself". Trying both in the console, they seem to have the same behaviour; scala&gt; Stream.constant({println("Hello!"); 1}) take(5) toList scala&gt; Hello! scala&gt; res0: List[Int] = List(1,1,1,1,1) scala&gt; Stream.constant_1({println("Hello!"); 1}) take(5) toList scala&gt; Hello! scala&gt; res1: List[Int] = List(1,1,1,1,1) in both cases. My first thought was that since both functions take an argument by value, the argument must be evaluated (once) before the call, hence the single "Hello!". In fact scala&gt; Stream.constant({println("Hello!"); 1}) take(5) scala&gt; Hello! scala&gt; res2: fpinscala.chapter5.Stream[Int] = Cons(...,...) scala&gt; Stream.constant_1({println("Hello!"); 1}) take(5) scala&gt; Hello! scala&gt; res3: fpinscala.chapter5.Stream[Int] = Cons(...,...) Still, this does not convince me. Also why using `Cons` instead of `cons` in `constant_1`? Then I came up with `constant_2`, which avoids the eager execution of its argument scala&gt; Stream.constant_2({println("Hello!"); 1}) take(5) scala&gt; res4: fpinscala.chapter5.Stream[Int] = Cons(...,...) but the if I try scala&gt; Stream.constant_2({println("Hello!"); 1}) take(3) scala&gt; Hello! scala&gt; Hello! scala&gt; Hello! scala&gt; res5: List[Int] = List(1,1,1) This surprised me, since I was expecting the function to "always referencing the same object". That;s it for now! Thanks a lot for your help!
Looks like those dips occur every 30 minutes which corresponds to HikariCP’s maxLifetime default value. That would seem to imply that what you’re seeing is corresponding to connections reaching their max age and being retired / replaced which is normal and generally desirable. I wouldn’t worry about it unless you’re seeing performance issues and even then, the max lifetime isn’t necessarily the first config setting to start tuning.
Thank you! This helped with the first question. In particular &gt;the compiler provides syntax sugar for by-name method arguments. A thunk (lambda with zero arguments) is equivalent in concept (if evaluating it has no side-effects) but if the signature is a lambda then you must provide a lambda value (e.g. `_ =&gt; ...`) this was not clear from the book: if a function expects a thunk as an argument you need to pass a thunk. I guess I was expecting the compiler to be able to wrap a value in a thunk if necessary, like I don't need to use a special syntax when I *call* a function with a value passed by name. I'm reading now the reply from /u/y0y, and it seems like they provide a thorough explaination to questions 2 &amp; 3. You are right, the key to question 2 is *lexical closure*, which is a concept I studied at the university but eventually forgot because probably I don't code enough!
Thank you very much for taking the time to write such a thorough answer! In the first question, I'm aware of the concept of by-value vs by-name argument passing. What I wasn't sure about was whether the syntax for thunks and for by-name arguments are interchangeable; and it seems that, in general, they are not! Also the answer to question 2 was really helpful! Here, it seems like the combo *lexical closure* (provided by Scala being a language with first-class functions) + *memoization* (provided by the `lazy val` declaration) explain everything. Reviewing some details about these concepts was enough to have a better understanding of the situation! With the answer to question 3 you actually got what I was trying to say (in a over-complicated way). I was indeed thinking about the "wrong" kind of efficiency achieved with `constant_1`. The thing about multiple object instantiations came to my mind at some point but since the chapter was all about lazyness and by-name argument passing, I ignored that. Also, after your answer, the new version of `constant_2` that you provide makes a lot of sense! &amp;#x200B;
Happy to help!
Our hiring platform, Functional Works, focuses solely on FP jobs. We have a few SF companies looking for functional scala developers. Here's one: [https://functional.works-hub.com/jobs/Scala-Engineer-San-Francisco-California-United-States-Aug-2017-57d2d?utm\_source=reddit&amp;utm\_medium=post&amp;utm\_campaign=k.cadima](https://functional.works-hub.com/jobs/Scala-Engineer-San-Francisco-California-United-States-Aug-2017-57d2d?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=k.cadima) Would love to hear your fb
Thanks! Im in SoCal and actually working with FW. Building the community out here for us! Great to hear that you use our platform :)
Sometimes in the Big Data space
Honestly, top devs get all the LinkedIn love. Have you checked out [Functional Works](https://functional.works-hub.com/jobs/?utm_source=twitter&amp;utm_medium=post&amp;utm_campaign=k.cadima)? Would love to hear how you think it compares to Signify??
Do you use any specific platform? Trying to make our platform, [Functional Works](https://functional.works-hub.com/jobs/?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=k.cadima), the most efficient and easiest way for Functional programmers to find jobs.
I don't mind working with recruiters. They do spam you a lot if you're in a job that you enjoy. But if you're looking for a job, and if it's for a niche position like Scala, they seem easy to work with. What do you mean you've been left in the dark? 
&gt;What do you mean you've been left in the dark? Working with a recruiter on job then not hearing back from them mid-process &amp;#x200B;
This is great. Just saw the video on this by John A de Goes :)
What are Scala companies in Atlanta? 
Ah I never had that experience. Most of the recruiters I've worked for seemed pretty good about communicating thoroughly. 
IBM (or Weather Channel), Turner, Career Builder are the recognizable names that use Scala. IBM actually has a big Clojure code base as well. There are other, smaller companies (Sharecare, TST, MobileWalla, PlayOn, CodeMettle, etc.) that use Scala. I think Scientific Games uses Erlang or some other functional programming language. There's probably a lot more out there. 
Cool, thanks!
There are already many articles about it For example, [http://eed3si9n.com/learning-scalaz/State.html](http://eed3si9n.com/learning-scalaz/State.html) for state monads What do you mean sequential pattern matching though? &amp;#x200B;
Bad news but you gotta hear so improvements can be worked on Tldr: Scala native is very damn slow, slower than jvm. Too slow. Don't even look. My comment- the absolute first step is getting everything to work native. Optimizations probably won't even be a part of v1. 
Graal is really exciting. I hope to see more experience reports of the wins. Thanks to Twitter for blazing the trail on Graal in prod.
I used to use the release plugin to automatically commit a bumped version number for my libraries and stuff, but for security, my new stack (drone) can't run git commands such as tag or pushing commits. What I'd ideally like is for my libraries to be tagged with a git sha and imported with them. Can that work in sbt? e.g. libraryDependencies += "com.github.tototoshi" %% "scala-csv" % "7181eaff" If not, the build environment can use tags with a git sbt plugin but as far as I can tell they always look for v1.2.3 style tags. I find tagging after every release a bit irritating and prone to mistakes. I've seen other teams in other languages successfully use a plugin that counts that number of commits from the last major version number so you would tag "v.1.2" and every commit after would be 1.2.0, 1.2.1, 1.2.2 etc (with branches being 1.2.1-bxx). Do any of the sbt git plugins other that feature?
If you care about performance at all, then don't.
Serious question: then what is the performant way to do IO while still being in fp land in Scala?
Website seems to be down for me.
draft of second part - [http://dehun.space/articles/15\_nov\_2018-Better%20than%20IO,%20part%202.html](http://dehun.space/articles/15_nov_2018-Better%20than%20IO,%20part%202.html) tagless final allows you to do that without much performance burden
just checked - seems to be up and working. it's fully static, not much to break there :)
Up for me now! 
There isn't any such way. Even just the IO monad is an "optimized" trampoline (quotes there since honest to god the benchmarks are all sort of biased for particular branches and don't usually tell the full story) you will run one or more of these when you write scala. Typical scala code in say, the typelevel stack which uses ResourceT, IO and _maybe_ Stream will all be essentially layer after another of trampolines which will halve your performance at best. There's bound to be someone that eventually bikesheds me on "Well, performance doesn't matter when...", so as a disclaimer, I'm answering a question about performance directly. The jvm in its current incarnation just isn't great for fp optimizations. For the sake of neutrality, I will say that even ghc doesn't perform all of the optimizations it can when it doesn't specialize mtl typeclasses in code (which it often fails to do), but it is leaps and bounds above scala in performance.
&gt; ResourceT `cats` doesn't even have ResourceT, brackets are built-in in `IO` and `Task`
[Your pov on this might be a bit out of date](https://typelevel.org/cats-effect/datatypes/resource.html)
`Resource` just wraps brackets, it can't do custom lifetimes ala Haskell `ResourceT`. It will also hardly halve your performance, because you either return it at the end or lift huge chunks of `IO` code into it
\&gt; Serious question: then what is the performant way to do IO while still being in fp land in Scala? &amp;#x200B; \&gt; There isn't any such way.\[..\] which will halve your performance at best. &amp;#x200B; This is a misleading answer. The fact the perf on flatMap chain is lower (say halved) doesn't mean that your total performance is halved (not saying that's what you meant, but it comes across that way when reading). \`IO/Task/Zio\` are very performant, way more performant than e.g. Future, which is commonly used for I/O even in fairly perf heavy environments. &amp;#x200B; \&gt; If you care about performance at all, then don't. &amp;#x200B; This is another massive hyperbole honestly :) For example, current doobie and http4s are based internally on transformers, and it really doesn't quite matter. &amp;#x200B; Is there a performance hit with transformers? yes. Are there cases where this can be critically problematic? yes. Does it matter in the majority of cases? no. &amp;#x200B; The main reason to not use transformers \_pervasively\_ is ergonomics (especially with type inference), which is why is advisable to use final tagless and flat \`F\`, with transformers used locally when needed. &amp;#x200B; &amp;#x200B;
She said **no** zero tolerance rules.
Deleted. I don't really have anything to add
Welcome to planet earth, where people flirt with each other
what about relocating jobs? I work down South America in Scala but I'm not sure if there are good relocation job offers.
&gt; This is a misleading answer. The fact the perf on flatMap chain is lower (say halved) doesn't mean that your total performance is halved (not saying that's what you meant, but it comes across that way when reading). It doesn't mean that the total performance is halved (it depends on what you're writing exactly) but that you will take a hit on the interpretation cost from all of the heap jumps. &gt; IO/Task/Zio are very performant, way more performant than e.g. Future, which is commonly used for I/O even in fairly perf heavy environments. `Future` forks @ every bind unless you explicitly make it not do so, so it's a bad comparison. Comparison to `FastFuture` would be more apt. On top of this, is there even a benchmark that tests combined actions in such a way that it doesn't abuse the branch predictor? Every benchmark I saw when I Was active in scala was along the lines of "perform an action, such as a flatmap, n times and see how good it goes". That's going to give you skewed numbers because you can't separate each case you bench individually in the IO data structure and just put them together like that accounts for IO's performance, but yet that's what the current benches actually do. &gt; This is another massive hyperbole honestly :) &gt; For example, current doobie and http4s are based internally on transformers, and it really doesn't quite matter Honest to god it's great I added that point about bikeshedding because that's exactly what's happening. There's a good reason I didn't bring up the libraries that _spend most of the time doing work over the network_. If you want to talk performance, and I mean performance alone and not some "Well good sir I'll have you know it doesn't matter because...", you gotta exclude the silly case of libraries where this won't make much of an impact. I'm not arguing whether it matters or not, simply whether it can be fast or no.
What's with coming into a thread about women in Scala, with a post history that includes you [telling women that they're unfit for motherhood if they don't believe your conspiracy theories from The Donald?](https://i.imgur.com/h7Ob6tc.png)
My general point is that you take a hit on performance if you are forced to reify your structure into the heap, and the more of these trampolines you stack the more it may suffer.
If you want to get rid of the boilerplate in slick I recommend using ActiveSlick.
We use both at work briefly. but we don't use db extensively. we use db to persist configs. so the use case might be too simple Doobie Pros \- plain sql, maximum flexibility \-integrate with typelevel eco very well (cats effect io, fs2, etc) \- May not integrate with scala type system very well, so it is a bit more boilerplate compared with Quill \* Quill \- Just magic for simple use case. simple queries/updates like scala codes.. &amp;#x200B;
Yeah but I didn't mean this as a direct correction to what you've written Jose :) I just meant that the initial question ("how to do fast IO in pure fp scala") can be interpreted either as "how to build an IO datatypes with super fast binds", or "how to perform I/O computations in scala purely functionally, with performance in mind". If you interpret it in the first way then "there's a perf hit due to trampolining" is a very good answer, if you interpret it the second way, "your performance is halved" is a very misleading answer", which is why I specificied "not saying that's you meant". &gt; m8 I didn't even bring up transformers Yeah I was replying to the comment above about not using transformers if you care about performance at all, which again is too broad and potentially very misleading. In hindsight my comment should have been top level, and not under yours, but too late now :P
I prefer Quill because of the following reasons 1. Its not just a wrapper over JDBC, which means it has other drivers (such as PostgresAsync) as well as supporting databases such as Cassandra with their own SQL dialects (i.e. CQL) 2. Although Doobie makes writing SQL less of a trainweck, doing so is still a PITA. A lot of your business logic in your app (at least the business logic in the DB layer) ends up being stringly typed. Even though there are ways to remedy this (i.e. Doobie has an ability to verify the SQL at compile time by introspecting SQL table structure) it doesn't solve all of the issues (i.e. refactoring in the context of SQL). Quill allows you to write strongly typed SQL using Scala like collections, and you can always write raw SQL in the same way Doobie can for the corner cases where you need to write raw SQL. 3. Doobie used to break (talking about bincompat/source) somewhat fairly often due to its dependency on cats and cats-effect. This is now less of an issue since cats is now released as stable however Quill is advertized to be dependency free (as much as possible) which is an asset for some. 
I came in here to whine about conflating “safe” with “welcoming”, because calling some asshole not knowing how to behave in a professional setting doesn’t actually make it unsafe, but you’ve convinced me otherwise. This is a professional conference. If you don’t have any human contact other than that, that’s your problem. Women don’t come to a conference to meet a man, they come to network professionally and learn about what’s happening in the industry. If the conference wants to include a social hour, maybe it’s just barely appropriate to hit on women there, but being treated like merchandise by the predominantly male industry has to be a big turn off.
Fair enough. I can agree with that. I wasn't entirely clear too that I meant "interpreter performance" vs "Algorithm performance".
&gt;For example, current doobie and http4s are based internally on transformers, and it really doesn't quite matter. To be fair, http4s doesn't perform too well in benchmarks. Though if it is because of transformers or something else, I don't know. 
omg the focus issue is horrible. My eyes are hurting haha 
I heard graalvm might be the solution to poor FP performance on the jvm.
Learn to read oh genius- I was responding to her having "a psychotic episode". If thats a woman that is fit to run a family according to you then you have bigger problems ...
Also, if you had to go so far back to find something that you can "catch" me for, you gotta ask yourself who's really right. And maybe take a hard look at things. 
For what it's worth, this seems like relevant and interesting spam. It's not a bad tutorial on how to get started with play. 
In a professional environment, sexual advances (including flirting) are inappropriate. There are plenty of other environments in which flirting is more likely to be appropriate. 
We use 0Auth for one of our internal systems. The first free 1000 logged-in users is pretty generous.
Flirting with a computer geek girl? You wish! 
[https://gitter.im/scala/job-board](https://gitter.im/scala/job-board) allows job and job-wanted posts
Thanks! Great resource.
I've seen SF based start-up companies offer $5K-$10K relo packages regularly. Not sure how many of them offer sponsorship though.
Intriguing - any chance could you elaborate that comment into a blog post somewhere? We are presented with all these FP patterns like Reader, but are they really anti-patterns (at least in Scala rather than Haskell)?
&gt; Doobie has an ability to verify the SQL at compile time by introspecting SQL table structure Is it in a separate project? AFAIK `doobie-core` only has a Yolo-mode which is runtime.
having read up on quill more, and finding [this page](https://github.com/getquill/quill/blob/master/SLICK.md), i think quill is a good choice for me to migrate to as a slick user. I've given slick my best shot, and the amount of code i've had to write to connect to my database is crazy, not to mention the errors that would occur when some slight issue happened with case class -&gt; lifted class mappings. To be honest, I think I'll be glad to be rid of it.
The release plugin can be customized to accommodate this I believe. You can have a ReleaseStep that inquires the commit hash and sets that as the version, or perhaps as the qualifier for the version, e.g. `1.2.3-7181eaff`. You can also write a release step that implements the number-of-commits version increment logic. The plugin can also be made to tag and push for every release to reduce that irritation. This is roughly what we do at my job where our versions are managed completely with tags and not a `version.sbt` file.
Thanks for the article. It's definitely necessary, and not something Scala talks about often, although, judging by the response in this dumpster fire of a thread, you've got a hard road moving forward :)
Is there a way to get SBT to check for the existence of an artifact in the `publishTo` resolver? I want to make my build fail fast before the compile/test/package steps if I can determine its publish will eventually fail due to a conflict. I have not been able to find anything in the release plugin that supports this, nor a path through an `sbt.Resolver` to an underlying Ivy `DependencyResolver` which does support such a query. I'd prefer to not have to have to issue and parse a curl command myself when all the pieces are theoretically available to give me the answer I want. So far Google and StackOverflow have not yielded anything other someone saying "I do this manually" and a years-old issue in the release plugin github that's sort of related but unresolved.
&gt; dumpster fire You just wont give up on stupidity, would you?
I think in the object is a reasonable place to put it since it ensures you don't accidentally use a variable outside the object's scope. My classes are almost always immutable, so I put helpers in the class just because I'm lazy and I know they aren't going to inadvertently get affected by state. But really in the object is the "safest" place to put it.
**Hivemind-Technologies | Scala Software Developer | Cologne, Germany | ONSITE or REMOTE (EU/EEA only)** &amp;#x200B; **Contact:** [jobs@hivemindtechnologies.com](mailto:jobs@hivemindtechnologies.com) You can also write me a private message if you have any questions. :) **About us:** Hivemind Technologies specializes in the development of high-performance and scalable big data systems. We help our customers gain more from their data and advise them how to use state-of-the-art technology to process, store and analyze their data. We are a small team of skilled Scala developers which like to write pure functional and clean Scala code. We are working mostly fullstack with little frontend but quite a bit of DevOps. Our stack includes a lot of OSS, including Jenkins, Spark, git, kafka, elasticsearch and libraries including cats, circe, monix, fs2, akka streams, http4s and shapeless. We make heavily use of AWS and cloud computing. We are nice work with, we have flexible working times and embrace remote work. We also sponsor visits to conferences and are offering free coffee and fruits in the office. Remote work is possible for all EU / EEA nationals or when having permission to work within Germany and live in one of these timezones: GMT, CET or EET. (No Visa sponsorship) Also visit **our website** for a more detailed job description: [http://www.hivemindtechnologies.com/jobs](http://www.hivemindtechnologies.com/jobs)
I asked a [question](https://www.reddit.com/r/scala/comments/9ud0bc/got_a_quick_question_ask_here_november_05_2018/e95d65p/) last time and wasn't really happy with the answers, so let me ask a more generic question: If I type "sbt test" on a default Play application, how can I find out: 1. how many threads get used? Is there a jvm inspection tool I can use? 2. how would I configure this value? 3. Is there Play specific documentation anywhere? 4. why would future created like this `Future.successful(1)` ever timeout? If there are 50,000 other futures in the queue to run, or one blocking Future before it? 5. How can I reason about and debug Futures that timeout? 6. Why are people's solution to test failures immediately to switch off running in parallel?
&gt; Honest to god it's great I added that point about bikeshedding because that's exactly what's happening. There's a good reason I didn't bring up the libraries that spend most of the time doing work over the network. If you want to talk performance, and I mean performance alone and not some "Well good sir I'll have you know it doesn't matter because...", you gotta exclude the silly case of libraries where this won't make much of an impact. I'm not arguing whether it matters or not, simply whether it can be fast or no. Is there a scenario where you're using a deep monad transformer stack for effects and *aren't* doing IO? Like everything else, use the right tool for the job instead of trying to generalise that screwdrivers are useless because they aren't very good at hammering in a nail. I have a Spring Boot application chugging along at about 100 requests per second and 1.5gb RAM. Workload is almost entirely network bound, and the app is essentially limited to the number of concurrent threads we can spawn. An early prototype of the same application written using Http4s on top of Undertow is getting around 40,000 requests per second on the same hardware in 128mb heap. I'm not going to lose sleep over the insignificantly small monad transformer overhead when it lets me run 100,000 concurrent Fibers with damn near zero overhead. 
I think /u/mdedetrich refers to [unit testing](https://tpolecat.github.io/doobie/docs/13-Unit-Testing.html) your queries with doobie.
I've managed to track this down to [this](https://github.com/gitsumanth/stackable-controller/blob/scala-2.12-play-2.6/core/src/main/scala/jp/t2v/lab/play2/stackc/StackableController.scala#L55). By calling the defaultContext the code eventually arrives at a shared lock controlled single Application instance. Using the play setting `play.useGlobalApplication = false` would effectively speed up my app and allow these tests to run parallel successfully if only make the required changes to support that value being false. That means a lot of work around passing implicit execution contexts around. :-(
It caches resolved compile-time ASTs instead of runtime values. You should never use `cachedImplicit` unless compilation is too slow.
Hey estauver 👋, thanks for the kind words 🤗. We work really hard on our articles to provide the best tutorials and guides for developers. It's not just about promoting a product, our readers read the Auth0 blog to learn about the latest technologies find all the they need about security, identity, and technology trends.
And even then you can quite often just derive implicit in a companion object or some trait/object you can import/extend to only derive it once, and that's it.
I've used Quill on a number of projects recently because of it's support for Async Postgres. Overall very satisfied with it. The only issue that required help was dealing with JSONB query.
No, it's not the same. Stop deflecting the issue here! The blog post is not about men and women flirting with each other. I've met so many guys thinking that I am flirting with them at conferences when I am not. This is exactly why conferences are becoming an unsafe environment. Guys somehow think that it is ok to flirt as long as they \*think\* the other one is also flirting. But no, stop having this mindset. Women are at conferences to learn and to network \*professionally\*. JUST LIKE YOU GUYS. So start treating us as professional peers.
🤖 😂😂 
What problems with jsonb did you experience?
&gt; conferences are becoming an unsafe environment So "becoming" implies it used to be different, have men been less prone to flirt with women in professional environments in the past? And what exactly is "unsafe" about flirting? Annoying sure, but unsafe?
Companion objects are meant to hold that kind of "static" methods. Or package objects. It depends on the scope you need this method to have. with the "protected\[&lt;id&gt;\]" qualifier you can control the scope in which the method is visible.
Scala compilation might get slow because of implicits resolution. To see how much you can use these flags \`-Xshow-phases -Ydebug\`. In project I'm working on the full compilation is 2 minutes and around half of that is implicit resolution. It helps when you use the same implicit more than once in a project and it is derived by macro or composed of more implicits.. Then you cache it and it will be done only once. Imagine \`asJson\` method needs Encoder\[X\] implicit. Compiler will look for it twice. In this case it will create the implicit only once in x and the calling of \`asJson\` will find this implicit in its scope so it will be fast. implicit val x: Encoder\[X\] = implicitly\[Encoder\[X\]\] x1.asJson x2.asJson And why you need cachedImplicit marco is here. It is not much code and you can do it yourself even without macro, but why to bother.. [https://stackoverflow.com/questions/34399288/caching-implicit-resolution](https://stackoverflow.com/questions/34399288/caching-implicit-resolution)
Hi, I'd like to share with you my experience, My first job was using Scala, we began all team together, there was a colleague (man) who helped me a lot to get started. When I made some improvements at work, it was unexpected to him and he regretted that he taught me (...) he suggested to work on Women's job. (I never think about programming is for men) I was in other planet, I only learned from some blogs to do my work, but after that bad experience with this colleague, I decided to invest in myself to find another source of learning and to make progress, I have a nice friend from Germany who told me about a Scala Conferences, I started attending conferences right away, I told people there about how much I was enthusiastic about learning, I invested my money for that (3 x months of work = able to attend 1 conference), some people said that I was crazy, and they couldn't understand the reason, they asked me what is my goal after that ? "I am enthusiastic" wasn't a good reason? I want to improve my knowledge and I want to communicate with the speakers, I want to contribute in open source projects, I want to create my own libraries, I want to be a speaker, I want to be able to teach people and share my knowledge. Is that enough? Unfortunately the most of people didn't understand, maybe they thought about "my enthusiasm" = I was flirting them? Oh :'( I don't want to imagine that. I tried to ignore all of that, to continue and make progress. There are many good person though, I am grateful to know them, and to learn from them. I got a job in Europe thanks to conferences too. I had and still have problems in and out conferences. So the major problem is the mentality. I respect men, and if they explain things I listen to them and I learn from them but sometimes when I try to explain something I see the EGO from them instead of respecting what I was going to say.. at that moment the motivation is gone. &amp;#x200B;
It's probably play. Twitter doesn't use it because Twitter has a giant engineering team to build something that fits their needs perfectly.
"becoming an unsafe environment" \*for me\*. I've started attending conferences in recent years. Didn't have to deal with/was not aware of this kind of nonsense before. The more I experienced/learn about what my other female friends experienced, the more I feel unsafe attending conferences alone. But hey, again, stop deflecting the attention on the main issue by picking on my words. And [godofpumpkins](https://www.reddit.com/user/godofpumpkins) explained why it's unsafe.
your last line of fibonacci, you do ```fib_(n,1,0)```, and your end condition for your recursive loop is ```count==n```. since you set ```count``` to ```n``` immediately when calling ```fib_```, it immediately exits and returns ```secondToLast```, which you set as 0.
That fixed it thanks! I'll try the bit you mentioned. I'm still real new to this what does (0 to 100).map(fibonacci).foreach(println) do compared to the one I wrote?
I think what she's saying is the rules are the same, but the consequences of breaking them take into consideration the context of who broke them. There's nothing radical here, it's the same as our legal systems where trials and sentencing are two different processes - trials apply exactly the same laws equally to everyone, sentencing takes into consideration a persons circumstances and dishes different punishments to different people for exactly the same crimes.
its equivalent in the final result, but it looks nicer imo, and it demonstrates a shorthand you can use in scala. since the input to map is roughly ```A =&gt; B``` then if you have a single argument function you can just pass the name in to map and it's functionally the same as doing ```.map(n =&gt; fibonacci(n))```. Likewise, ```foreach(println)``` is the equivalent of ```.foreach(n =&gt; println(n))```. The downside to doing ```.map(fibonacci).foreach(println)``` is that it almost certainly iterates over 100 elements twice. However, with your small set of data that's not a big problem performance-wise.
&gt; Secret Slack rooms, secret gitter rooms, irc channels, etc. Is this something that you've seen happening in the Scala community, or is it something more generally that happens in the industry? I'm not disagreeing with you that it happens, but I'm completely oblivious to it. Do you have any ideas for how can we be more aware of it, so that we can call it out and shut it down? Do you know if there any particular parts of the community where this is prone to happen? Do you know if there are any warning signs that this might be happening, and is it possible that some of us could be inadvertently participating in these groups without realising that they are exclusive? Sorry for the barrage of questions, but this just sounds like something terrible that shouldn't be hard to do something about.
Is Scalia the name of Scala framework?
Yeah I think. It’s a browser site where Scala is the language used
I only know it as the name of a Supreme Court judge.
Doobie has a test method "check". Because of database migrations, you don't really want compile time checking generally.
I really do feel for the women who feel unsafe and/or deal with men who can't handle rejection without turning violent. I don't condone guys who can't take a hint or think women can't be awesome software engineers. But.. like.. where exactly are we supposed to meet members of the opposite sex with common interests as men? Work is not ok. Conferences are not ok. Bars are not ok (women are just there to hang out with friends right?). Gyms are not ok (women want to work out without being harassed right?). As a woman, you may be constantly bombarded by interested men around you. As a man, every single place is systematically labeled off-limits. Let the downvotes flow!
Awesome, thanks for explanation :)
\&gt; The environment is unsafe for the attendee as long as it makes the person feel unsafe. People don't have to experience physical harassment to feel uncomfortable. You're as safe from violence at a conference as in any other place, because you're protected by criminal law already, certainly safer than a lot of places where criminal law is less likely to be respected. Conference rules don't add a meaningful degree of safety, crowds of witnesses and hired security do. Abusing the word "unsafe" to mean "uncomfortable" is just escalating the language for dramatic effect. &amp;#x200B;
&gt; how many threads get used? Is there a jvm inspection tool I can use? You can certainly see the threads at a specific point in time via e.g. jstack, and get a more sophisticated history view by using a profiler (e.g. jprofiler), but I would ask what question you're trying to answer. What do you expect knowing how many threads are used to tell you? What are you going to do differently based on the answer? &gt; how would I configure this value? In the worst case, any random bit of Java code could be firing up random threads. But hopefully the name of a thread will tell you where it comes from (e.g. a particular thread pool) and most thread pools are configurable; it's framework-specific how you configure any thread pools used by a given framework though. &gt; why would future created like this Future.successful(1) ever timeout? If there are 50,000 other futures in the queue to run, or one blocking Future before it? A future created like that would be created in a successfully completed state. But what do you mean by "time out"? Futures don't come with timeouts, you can form a future that will never be completed and it will just... never be completed. &gt; Why are people's solution to test failures immediately to switch off running in parallel? Because the libraries for traditional mock-based unit tests are inadequate for parallel testing; mockito in particular is known-broken in multithreaded scenarios by design. &gt; I understand the fork-join executor queues Futures up and reuses threads. How can I see those queue sizes and manipulate and fix those types of issues? I don't know, but I don't think you're expected to need to do that in the first place. If you have some extremely unusual requirements maybe you want to write a custom execution pool, or custom threads that do specific things rather than using a generic pool at all. &gt; It's frustrating that Futures are supposed to simplify concurrent programming, but given my issues, it's just moved the complexity else-where. How can I fill in the holes in my knowledge? I'm getting the sense of a big "XY problem" from your post. You're asking a lot of questions about low-level control of threads that I've never needed in 10+ years of professional JVM programming (including for systems with single-digit ms latency). Pretend you'd never heard of threads and were just using futures and executors without knowing anything about how they were implemented. Purely at that level, what's the problem you're trying to solve?
It's probably Play. Play's popularity continues to baffle me, but my best guess is that it's popular because it provides an all-in-one framework (including HTML templating) and is advertised by lightbend. Neither of those things would have been a priority for Twitter.
&gt; I don’t want to couple myself to the generated classes so I have to write mapping functions back and forth in every component... IMO: Scala's type system makes refactoring easy, so you shouldn't be so worried about coupling. If you need to switch out a protobuf class for a custom class and some kind of transformer at some point in the future, it'll be easy to change your code to do that.
I would never defend any sort of harassment you may have experienced within the community, but you do have a reputation for ripping on individuals, companies, and projects within the Scala ecosystem. Whether this behavior is born out of cynicism from being harassed, I don't know, but if it is I don't think it would really be a valid excuse anyway. I'm just saying that as an outside observer your prickly attitude turns a lot of people off from wanting to collaborate with you.
Doing a JSONB search with `-&gt;&gt;` required a bit of sorcery: https://github.com/salesforce/oss-request/blob/master/app/modules/DAOModule.scala#L376-L378
Fair enough. I've been making an effort to de-escalate from the negativity, which i hope has become apparent in my posting history on twitter, and reddit, as well as on the discords. I do point to Scala and my last 2 scala positions one of which ended due to sexism sexism respectively, in addition to the community troubles.
I see. The issue with tracing execution contexts hasn't occurred to me yet because I've been abstracting code into classes that each have their own dependency injected (with guice) execution context so basically each class has their own and there's nothing that needs to be kept track of. My issue I guess is just that I haven't come across a use case where it makes more sense to use `Task`. /u/pixelflat did mention batch / parallel processing which `Future.sequence` doesn't handle very well but `Task` is good for so I'll be looking for an opportunity to use `Task` for that! I think part of the decision is style but there should be legitimate reasons to use Monix over just Future like in the above case of parallel / batch processing otherwise you'd have to reinvent the wheel, reimplementing `Task` with scala `Future`.
The parallel processing is a good use case, thanks! I hope to find a place to use `Observable` as well. So many tools in the box that it's hard to pick