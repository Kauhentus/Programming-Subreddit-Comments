&gt; clamping down on infix notation and trying to make coding styles a bit more homogenized is not a bad thing. a huge complaint about scala is that code can look wildly different between two different people and yet still do the same thing. Haters gonna hate. I find infix notation at the syntax level is one of the best things about Scala and as the OP's example shows, giving it up would be a huge loss.
&gt; Implicits have real problems that go further than whether you like them or not. Just accepting the faults because Scala devs are well-paid and companies rely on implicits will result in a language that stops improving. True as far as it goes, but equally it's important to acknowledge that the language is successful and valuable in its current form, and take appropriate care not to damage that. "First, do no harm."
Sky UK| Scala Developer|Senior Scala Developer| London (TW7 5QD) | Onsite flexible working | Full time , Permanent &amp;#x200B; Join a cross functional software development team that uses cutting edge technology to provide applications that put Sky first for online content, customer service and sales. We ask for the very best from our development teams, so we'll make sure that you'll get the training, experience you need to have an enjoyable and fulfilling career at Sky. &amp;#x200B; How it looks like to work at Sky : [https://youtu.be/hJjs7rW7P70](https://youtu.be/hJjs7rW7P70) As a Software Developer, you’ll be championing software excellence, knowledgeable and passionate about technology, enjoy pairing with other smart people and use agile to ensure your team succeed. \- Develop excellent quality software using agile techniques such as Test-Driven Development and Pair Programming \- Take an active role in Story definition, assisting business stakeholders with Acceptance criteria \- Work on exciting products delivering significant new capabilities to Sky's applications, ensuring the User receive the best service every day, improving the codebase and have active participation in all aspects of the team - this includes daily stand up meetings, planning, showcases, retrospectives and more. \- Participate in regular showcases of new work to business stakeholders \- Share and generate new ideas, provide constructive and useful feedback with peers \- Communicate professionally with your team and other teams in your division, knowing how to contribute and how to listen. \- Delivered work is accurate and thorough and you strive towards excellence \- Play an active part in the Sky Technology communities and be an active contributor to internal and/or external open source projects \- Actively mentoring and coaching other members of the team, your help has a recognised impact on their development **You’ll have:** \- Good experience in software development preferably in functional programming languages such as Scala. \- Real time data processing and RESTful microservices in Scala (Finch, Spray/Akka HTTP and Play). \- Experience with the SMACK stack (Kafka, Spark Streaming and Cassandra), Big Data pipelines and Machine Learning \- Strong TDD and BDD background \- Experience with both Unit, Functional and Non-Functional testing \- Experience with Continuous Delivery \- Knowledge and experience of a wide range of best of breed open source and commercial technologies \- Experience of software development for high capacity, high traffic (1 million sign-ins a day), high availability websites/systems \- Delivery experience within an agile environment using Scrum/Kanban methodologies &amp;#x200B; Let me know if you would like to have a chat at : [Anthony.kalarytis@sky.uk](mailto:Anthony.kalarytis@sky.uk). &amp;#x200B; Anthony
Lunatech | Scala Developer | Amsterdam or Rotterdam, The Netherlands | ONSITE (No Visa required) | Full Time We’re looking for a passionate developer who is always looking for personal growth and the best for our customers. Because we don’t have managers to draw charts or go wild in Excel, we expect that you pick up responsibility of leading projects to successful completion. Lunatech is a sponsored IND company, that means we can initiate the process to relocate you to The Netherlands if you're outside of the EU. [**employment@lunatech.com**](mailto:employment+website@lunatech.com) **or** [https://lunatech.workable.com/jobs/1040550](https://lunatech.workable.com/jobs/1040550)
Zendesk | Scala Senior Engineer | Montpellier, France | Onsite (visa support possible) | Full time You’ll be joining a team that works on our Explore analytics data engine, data stores and ETL. Explore is a complex reporting application which provides analytics for Zendesk data. erwan at zendesk dot com or [https://stackoverflow.com/jobs/270484/senior-software-engineer-with-scala-data-engine-zendesk](https://stackoverflow.com/jobs/270484/senior-software-engineer-with-scala-data-engine-zendesk)
Yes! Thank you! &amp;#x200B; Sorry for the delayed response, I forgot I had asked.
Well, there are always problems, but what you guys have done is amazing!
Thx, will pass the message
&gt; However there seem to be some 'decent' use cases. I'm genuinesly curious if there are any use cases that I would accept. As for the magnet pattern, I think in Scala it is (unfortunately?) not a good idea to "overload" method names. Neither in the classic way (even without type erasure problems) nor with tricks like the magnet patterns. &gt; And I also have seen 'wrapper conversions' occasionally, where a simple type like a String is automatically wrapped to a more complex type like Uri, or Keyword, or anything like that. These conversions seem pretty harmless to me, the problem lies in going from one type to a completely unrelated type. Hm, I consider them as harmful. calling `"http://...".asUri` is much better than an implicit conversion imho. There are multiple reasons why I think they are harmful. On of the biggest is that a change in imports can the semantics/behaviour of the program without anyone noticing it. I much prefer if the compiler warns me and makes me decide if I want that change. &gt; That said, I personally have never designed code that either uses wrapper conversions, or the magnet pattern. Same for me and that is usually a sign that there is no reason to use them anyways as there are better techniques - or it is ignorance. For wrapping primitive types automatically, I'm pretty sure it is the former. When it comes to the magnet pattern I might have missed some problem that it solves which can't be solved in a better way. But I also don't see it in any of the rather new and famous libraries, that are written by people smarter than me.
You don't have to turn your entire actor into ordinary code. Instead, you can use `ReceiveMessage.Partial` and `ReceiveMessage` for some complex control flow and then returns a behavior written in original Akka Actor API. def myActor: Behavior[Command] = { initialize1(!ReceiveMessage.Partial[InitializationCommandStep1]) initialize2(!ReceiveMessage.Partial[InitializationCommandStep2]) initialize3(!ReceiveMessage.Partial[InitializationCommandStep3]) Behaviors.receive { ??? // Rest of the actor } }
Hi all, at my job I'm currently working on a project where I'm trying to recreate a python data analysis process as well as a machine learning model on my local and transform that into Scala code. One of the issues I'm having is the handling of categorical variables. In pandas this is made easy with the "get dummies" command, however I haven't come across a similar function within Scala. Given how many dataset works, I do need to handle the categorical variables in a way where each instance is represented as it's own column. Any help would be greatly appreciated, thank you!
[Events.com](https://Events.com) |Dev/Ops Scala Engineer | La Jolla, CA | Onsite or Possible Remote | Full Time | Salary DOE &amp;#x200B; We have an exciting opportunity for a Backend/Ops Software Engineer to join our growing dev team. We are need of a technical wizard who loves opening up the hood and tinkering around to reconfigure and optimize software and systems. Some of our Scala code needs a review and refresh - we are looking for an engineer who can assess and redesign systems for future use. This would be a project that would grow into system integrations of our other software, as well as building out functionality for growth and user adoption. [https://www.indeed.com/cmp/Events.com/jobs](https://www.indeed.com/cmp/Events.com/jobs) &amp;#x200B; [jobs@events.com](mailto:jobs@events.com) or [kflynn@events.com](mailto:kflynn@events.com)
SignalPath | Senior Software Engineer | Raleigh, NC | ONSITE | Full Time &amp;#x200B; You will join an interdependent, fun, engaged development organization that is focused on delivering high-quality software solutions. * Write and maintain our core platform. You will be expected to maintain an eye towards best practices, including testability, scalability, monitoring, and performance. * Partner with UI engineers and fellow platform engineers in the process of defining our RESTful API. * Participate in and encourage a collegial, helpful, results-driven working environment. &amp;#x200B; [https://signalpath.workable.com/j/BD6166940C](https://signalpath.workable.com/j/BD6166940C)
&gt;Hm, I consider them as harmful. calling "http://...".asUri is much better than an implicit conversion imho. There are multiple reasons why I think they are harmful. On of the biggest is that a change in imports can change the semantics/behaviour of the program without anyone noticing it. Given that Scala is statically typed I don't think these types of errors are that silently. Other than that, I agree. Being explicit in these cases is better. In hindsight it might have been better to not add them in the first place. The problem with removing implicit conversions however is that there is no straight rewrite rule, so I think that is out of the question.
True, it is a bit of a balance. And yes, implicits are currently already incredibly useful. That said, I think languages like Java and JavaScript go too far in being backwards compatible, and it shows. Getting rid of tech-debt - even in programming languages - will allow you to move faster and be more reliable. For me, it kind of falls or stands whether or not automatic rewrite tools are actually any good. I don't mind learning a new syntax, as long as migration is not too painful.
Salary?
If you are using SBT to build your project, you can put the jar file into the \`lib\` directory and it will be available both to sbt and to intellij. But probably simplest to add `libraryDependencies += "org.scalafx" %% "scalafx" % "12.0.1-R17"` to your `build.sbt` file. Then you don't even have to download and manage the jar files yourself.
FTE or contract? On-site or remote?
Oh. You may want to specify it explicitly next time... :)
deo and skill, $100k+
FT, onsite if we can get it, remote if we cant fill it locally
Oracle | Senior Software Engineer | Oakland, California, USA | ONSITE | Fulltime | Bay Area Engineering The Oracle Cloud Infrastructure, Registry team is looking for a senior software engineer to work on the Registry service. The Registry service is built on top of Scala/Play/Akka stack. If you have wondered what it's like to do Functional Programming or work on a real Event-Sourced, Clustered, Distributed system in production this would be a great opportunity to find out. Members of our team work on distributed systems in a cloud environment and have ownership of our service from top (Load Balancers) to bottom (Databases). Email justin.ko@oracle.com
I’m very flattered by Paul’s comments, especially coming from him, but this isn’t an accurate depiction of the bazel scala rules. A more accurate description would be that it is the result of an industrial partnership between build engineers at Wix and Stripe. If you look at the link you will actually see a considerable number of commits both from stripes (Andy, Long, Alex, Ian, myself) as well as Ittai and his team at Wix. In fact Natan at Wix has been giving some great talks about their experiences: E.g. https://youtu.be/lT8zpzyJW7I Many big users are using it (Spotify, Meetup are two other big users). Databricks uses an internal fork of these rules (unfortunately in my view). Bazel is for industrial use. That could change but right now it is for teams that expect to be building a lot of code. When you do that, correct caching and remote execution matter. Bazel solves that. The scala rules really aren’t very big and haven’t changed much in the past 3 years. If you expect to be building 500kloc or more sometime soon, look at bazel. If you are interested in your build as a pure function, look at bazel. If your build has many interacting parts (polyglot, proto, complex deployed artifacts) look at bazel. If you just don’t like the idea that each language community is making a new tool to solve build in a slightly different way, look at bazel. Bazel could become the Linux of build. It’s not the OSX of build: it is a tool you can use to build GIANT builds that perform well. It is pretty biased to more work to be more correct (and therefore cacheable).
Oh... I almost forgot, there is an sbt-bazel plugin: https://github.com/stripe/sbt-bazel It is used by stripe for a couple of projects. I haven’t worked on it at all. It is 100% the work of the scala team at Stripe. It’s not perfect but it’s usable to export a bazel build of your sbt project.
Yeah, obviously, you're right, Oscar. I said I didn't know what bazel was good for. That's overstating the case quite a bit, and I owe you an apology. I certainly don't mean to _dissuade_ people from using it, or regret that we use it. It really goes more like this: 1. There's a pretty specific use-case we have where the combination of bazel, Nix, and Haskell is painful, and this pain seems to be ongoing, meaning, we don't understand it well enough to resolve it. 2. Orthogonally, there have been suggestions to adopt bazel for our Scala code, too, out of a perfectly understandable desire to have a consistent build ecosystem. 3. Observation #1 has caused many of us, myself included, to conclude request #2 is untenable. The sbt plugin picture is even, in some sense, orthogonal to all of these, but only kinda-sorta. In any case, it's certainly not the case I'm claiming bazel is immature or not production-ready; that would be silly. And if a dozen or more shops are using the Scala rules successfully, I think that's great, and it's a nice reminder that, hey, maybe if your sbt builds depend on 10 sbt plugins or something like that, you have a _different_ kind of build problem.
What's also surprising and, quite honestly, very disappointing, is the way source code demos are displayed - as a bunch of screenshots. This makes following what's going on in the editor very difficult.
So the role is "be the only guy responsible for this thing we inherited that we probably aren't going to put much resources into?" That's a contract role for $$$. Sounds like a dreadful full-time role unless you guys actually do have plans to build a team around the first engineer.
Most jobs do take place because a company needs someone to take responsibility for something, have a contract, and involve $$$. That is how society works and is nothing to be ashamed about.
Who said anything about being ashamed of it? It just sounds like a crappy gig and I'm letting him know he's not exactly selling it well. The market is pretty good for developers right now, so the job needs to make up for the fact that it doesn't sound very good from purely a work perspective.
Thansk, I just discovered : [https://github.com/cloudify/sPDF](https://github.com/cloudify/sPDF) I might avoid using flying saucers next time I'll need to generate pdf !
Why does `coflatten` return `FocusedGrid((0,0), grid)` instead of `FocusedGrid(fa.focus, grid)` like `coflatMap`? I don't think the laws hold with `(0,0)`.
Actually the reason we acquired this company, and their tech is because it is a major piece of our overall product roadmap. There will be significant resources devoted to merging systems and we will build a team around the entire project. Just so happens we don't have a Scala engineer on our staff either. And who said anything about it having to be a Guy, or that I am a Guy ;) Sounds like it wouldn't be a good fit for you. Thanks for checking it out!
&gt; There will be significant resources devoted to merging systems and we will build a team around the entire project. Cool. You might want to mention that. The short version just reminds me of gigs I've had / seen in the past where it's one person on staff acting as life support for a critical but otherwise ignored system. They tend not to be exciting roles, and can be very difficult to hire for. That's why I suggested a contract role might be a better fit. &gt; And who said anything about it having to be a Guy, or that I am a Guy ;) I'm a guy, so I was translating it to what the role would be for *me* should I apply.
I linked to the jd in my post.
Hi everybody, this is my first post on the scala subreddit and I hope this is the right place to ask. I am not a professional developer, but I have been programming (mostly Python and some C++ in the past) for quite some time now and I decided to learn a bit of Scala and Functional Programming. I am attending the 2nd course in the scala specialization on Coursera "Functional Program Design in Scala" and I have found the material very hard to follow on Functional Reactive Programming very hard to follow. I was wondering if you could advice me on any resource that is a bit more digestible, possibly with a bit more examples? Thanks a lot for the help, I hope I can ask more interesting questions soon! Cheers, Luca
I actually clicked but the job description I saw was for another role. It looks like maybe you need a deep link to the specific role? Just FYI. Anyway, good luck finding a good fit!
Maybe paste (or link) some rows of data here to clarify inputs and desired outputs.
Hi everybody, this is my first post on the scala subreddit and I hope this is the right place to ask. I am not a professional developer, but I have been programming (mostly Python, some C++ in the past) for quite some time now and I decided to learn a bit of Scala and Functional Programming. I am attending the 2nd course in the scala specialization on Coursera "Functional Program Design in Scala" and I have found the material on Functional Reactive Programming very hard to follow. I was wondering if you could advice me on any resource that is a bit more digestible, possibly with a bit more examples? Thanks a lot for the help, I hope I can ask more interesting questions soon! Cheers, Luca
Hi sure! &amp;#x200B; I hope this works so imagine I have the first picture. &amp;#x200B; With State being the Column name. And my desired output would be. &amp;#x200B; And so on for State\_Wyoming, State\_Texas, etc. I also have to do this for multiple columns similar to State. If you need any more details I'm more than welcome to delve further, thanks for any assistance!
Thanks for pointing this out, I thought I had fixed this. Have updated the code and the post.
The book has problems and solutions hosted on github here [https://github.com/fpinscala/fpinscala](https://github.com/fpinscala/fpinscala). &amp;#x200B; According to build.sbt it is on scala 2.12.1 So, its pretty recent.
Lmao @ making the case that using Option complicates code.
Very cool. Love it
In this case is MyTrait modeled as an ADT?
Worth trying out https://github.com/lihaoyi/cask if your priorities are "mainly ease of use". It's a thin wrapper around the Undertow web server, which is itself a fast and stable piece of infrastructure that scores well on benchmarks. A clone of Python's Flask, it has no advanced DSLs like http4s or akka-http, and no complicated/advanced dependencies like Cats or Akka.
Playframework
This is excellent, the flask-like syntax is a nice bonus since my team is familiar and mill is so far a lot more intuitive (and prettier!) than sbt. If you don't mind, I just have a question about async operations, perhaps this is due to my poor understanding of Scala but I was able to successfully setup a route that queries a local database without using Futures. Is this something you handle under the hood, or should I look for some third party libraries like `scala-async` to ensure execution is predictable and non-blocking?
Something vaguely like... case class Record(stateName: String, otherStuff: ...) val records: Seq[Record] = getRecords(...) val stateNames = records.map(_.stateName).toSet val stateMaps = records.map(record =&gt; stateNames.map(stateName =&gt; stateName -&gt; if (record.stateName == stateName) 1 else 0).toMap)) Essentially that builds a map for each record.
I have used Akka’s http framework extensively. It is a bit of work to wrap your head around some aspects but I have found it to be really powerful especially if you need to stream data.
You can control constructor visibility like you can with regular classes: `case class FooEvent private (event: EventType)` etc.
&gt;case class Record(stateName: String, otherStuff: ...) val records: Seq\[Record\] = getRecords(...) Thanks for the response! So I'm really new with Scala so I'm not 100% certain how the syntax in the first two lines works/operates. I create a class named Record. other stuff is supposed to signify if I wanted to add other columns to the class? Also the val records line getRecords(), what exactly am I supposed to be doing here. The rest seems to make since, sorry I'm just new to this and don't quite understand some of the nuances yet.
If FooEvent can only be instantiated with FooType, then the best practice would be to have the typesystem do the work for you, so have FooType take a FooType instead of an EventType and make the invalid state non-representable.
So it's overriding `Event` which requires that field have the `EventType` type
I thought this might be useful for folks working on tools. For a preview of how I intend to use this client in Bloop [https://github.com/scalacenter/bloop](https://github.com/scalacenter/bloop), see this Twitter thread [https://twitter.com/jvican/status/1143982215384182791](https://twitter.com/jvican/status/1143982215384182791)
Traits don’t dictate how constructors work, you can still extend Event and have the field type by FooType.
creating a constructor that takes in a FooType doesn't erase the existing constructor that takes an EventType though which is what I want to do for safety
It was in 2.10+ long ago. Dotty just made it into a keyword inline instead of macro.
The "constructor" you get for free with a case class is really the `apply` method on the companion object, so I'd imagine it would be a matter of writing that out and adding your visibility modifier.
Yes, the above assumes you have a dataset with an a-priori known set of keys (columns) and corresponding values. In Scala, we would typically represent each row in the dataset by a case class structured in a 1:1 relationship with class members corresponding to the row columns, and of the correct type. A CSV record is a typical example. However, this sometimes isn't the case, i.e. the data is dynamic and where the column names are not known beforehand: Python handles such cases well, Scala less so, but there are solutions if necessary. That's assumed out-of-scope here. There's a trade-off between static and dynamic typing. A fully dynamic solution might just use a `Map[String, Any]` perhaps as the Record contents. The `getRecords()` call is just a placeholder to retrieve a sequence of records from a file or suchlike. The above example assumes you can traverse the sequence twice, and that in general is not the case, e.g. with an infinite stream. Again, somewhat out-of-scope.
Just because you can hit the server with more than one request at the same time doesn't mean it's non-blocking. If the server uses one thread per request it just needs enough threads to meet the demand. It's how most servers used to work on the JVM before people realized how inefficient that is.
You can make a apply method with the same signature, but make it private. That overrides it properly
Website seems down
Thank you so much for the explanation, info like this is tough to come by
Just started messing around with Lagom for a small toy project before we start using it at work. Getting the hang of creating a service and think being familiar with Akka has helped. However, looking at dockerizing / deploying looks a bit daunting...
You can use a custom decorator to give Cask async support, but I'd suggest you just use multiple threads to handle concurrency. You can easily get into 100s of concurrent threads on the JVM without significant performance issues. Take note that high concurrency is not the same as high performance: While it's not uncommon to have a server handling 100s to 1000s of requests per second, it's pretty uncommon to have a server doing more than a few 100 things concurrently. More likely than not threaded/blocking concurrency will work perfectly well and predictably.
You just experienced first hand, why it is discouraged to use (implementation) inheritance. The suggestions of other people here only work if you don't inherit an existing constructor. &amp;#x200B; You can try to workaround your problem by guarding at runtime and throwing an "InvalidArgumentException" when your inherited constructor is called with BarType, but you can't make the compiler to catch it for you. Also, throwing an exception will violate the Liskov principle. &amp;#x200B; Your other option is to revise your design and make Event be a (possibly parametrized) sealed trait or type class - or at least remove the default constructor. If you can't do that because it comes from a 3rd party library... well then you should probably not use Event in your code and write a wrapper on top of it to cover the problems it otherwise introduces into your code base.
This is probably the worst article I read in years.
I need a Dotty T shirt
So I have a function that takes a collection of indices and increments and uses them to update a vector, and I implemented it with a fold like this: ```scala def bulkUpdate(oldVec: Vector[Int], updates: Vector[(Int, Int)]) = updates.foldLeft(oldVec){case (vec, (ind, inc)) =&gt; vec.updated(ind, vec(ind) + inc)} ``` Now, that works, but I'm wondering if there's a faster way to do bulk updates on vectors. Or, perhaps, is there a data structure better suited for that sort of operation? My intention is to stick with functional/immutable data structures, so I'm staying away from Arrays.
I think it's an interesting and novel design idea, basically a macro are a special type of function (and can be used wherever a function can be). I can't think of any other commonly used programming language where macros and functions are unified in this way.
Maybe this : https://gist.github.com/tpolecat/a5cb0dc9adeacc93f846835ed21c92d2
&amp;#x200B; JUST EAT | Scala Data Engineer | London, UK | Full Time &amp;#x200B; We are looking to speak to Scala Data Engineers to join us at Just Eat. &amp;#x200B; Do you want to join one of the most innovative Tech startups in London? We have a mature cloud-based Data Infrastructure based on GCP and work to a massive Scale! Currently, we complete over 3000 orders per minute. &amp;#x200B; This means you will be working purely on cutting edge problems. Focusing on implementing the technologies you find most interesting. &amp;#x200B; We are somewhere where you can challenge your coding ability with some of the brightest minds in software engineering. The environment is pretty cooperative, with a lot of teamwork as well as work-life balance. &amp;#x200B; This is a short video about Data Engineering at Just Eat [https://www.youtube.com/watch?v=vAtPQZ0SfZ](https://www.youtube.com/watch?v=vAtPQZ0SfZ4) &amp;#x200B; APPLY HERE! [https://careers.just-eat.com/opportunities/R-0003182](https://careers.just-eat.com/opportunities/R-0003182)
'I don't know of any other language where macros and functions are unified in this way' - Clojure?
I meant statically typed ones. The challenge is to "put types" on macros and still make them powerful enough.
Okie
We've quickly brought up small services at work using [Finagle](https://github.com/twitter/finagle) and [Twitter Server](https://github.com/twitter/twitter-server), and they've been very stable. I feel these libraries don't get enough fanfare. They are thoroughly road tested, come with a lot of developer-centric utilities included, and operate well with other libraries and services.
This sounds like you're using the vector more like a `Map[Int, Int]` - if you have `oldMap: Map[Int, Int]` and `updates: Map[Int, Int]` then your update method is just `oldMap ++ updates`. (Vector will quite possibly perform better though).
`FooType` is a subtype of `EventType`, so it's fine to override a val of type `EventType` with one of type `FooType`.
Instead of REST services, have you considered gRPC via ScalaPB? gRPC + Monix is my current go-to setup for microservices. Happy to point you to some example code if you are interested.
My personal recommendations would be: * Play Framework for a newbie because it's a full stack framework, so you don't have to make too many decisions about what libraries to use, the structure of your project, etc * Akka HTTP once you get to know better the Scala ecosystem and want more freedom in the way you organise your project and the libs you use, or if you want to expose an HTTP interface to an app that's not mainly a web app * http4s if you want to go full-FP I'm not a big fan of Finch/Finatra personally, it doesn't integrate well with other libraries (with Twitter Futures for example).
Can you add some usage/scenarios where one would use nailgun.. looks like an interesting client and project, but for the Scala community it's not clear where the natural fit for for these systems would be.. is nailgun more useful for other languages to incorporate services best provided/written in the jvm? Some more context would be nice as their own repo and pages seem to be equally non descript about the why beyond just the what.
Surprised that Scalatra hasnt been brought up yet.. It's extremely newcomer friendly, is tiny, async and efficient, comes up with very minimal fuss, and perfectly suited for small microservices, apis and the like. It is stable, actively maintained, and has been used by the likes of LinkedIn, Guardian, gov.uk and so forth for almost a decade. If you're looking for performant but lean framework without all the bells and whistles of things like Play (or expecting the horse and pony show of expansive Java frameworks), it can be a great choice.
I think Hackett has made some interesting progress in that direction: https://github.com/lexi-lambda/hackett &gt;Since the Hackett typechecker is actually a part of macroexpansion, macros both have access to type information and can influence the typechecking process.
I am deeply troubled by one thing. You didn't name is Snailgun. Just imagine the logo icon, a gun looking like a snail shooting more snails.
`sealed abstract case class` might be what you're looking for. Even if you make the constructor private when using case classes nothing stops you from doing `foo.copy(eventType = BarType)`,
Damn, I use \`Seq\` everywhere for years. I'll switch to a different type from now. One question: why is Vector better than List?
I didn't know the Scala community had elected a spokesperson to communicate on what it's not clear about, haha Troll aside , the JVM is not well suited for command line tools. I'm on a phone so won't give any links, but you can lookup "cold/hot JVM". The point is, if you're running a short lived program very often, you'll pay a startup time on every execution. The startup time is not insignificant, so it could give you a somewhat sluggish user experience. Nailgun aims at solving that problem by re-using the same JVM to run your short lived programs, and let's you communicate with it via a ephemeral client that doesn't suffer of the same handicap with relation to startup times. It does one thing and does it well.
Hm I'm very curious, I've never heard of gRPC before now and it seems fascinating. If you wouldn't mind sharing some example code that would be amazingly helpful
Accessing the element at a given index is O(log n) rather than O(n). List is efficient for accessing the head or traversing head to tail, but very inefficient for accessing elements near the end or appending. For Vector virtually everything is O(log n), so it's a better default if you don't know the specific usage pattern.
That'd be a pretty cool name! I didn't give the name much thought. There was a Ruby client called railgun so I just defaulted on Sailgun. I might actually consider changing the name, thanks for the suggestion @Skayaar
Thank you. Most of the times, I don't really care about the performance. But since Vector is only 2 character longer than List, I might as well use Vector.
Yes, and to expand on @Baccata64's great explanation, Nailgun was done with the goal of making JVM applications usable from the CLI but there are also many cases where a Nailgun server can be implemented in another language or you might want to have the client be another JVM process itself. For any of those cases, you can now use Sailgun (these use cases weren't supported in the default implementation hosted at facebook/nailgun). &amp;#x200B; In a way, Nailgun is just a simple protocol to abstract communicate CLI clients with servers of any kind.
Migration completed to snailgun here [https://github.com/jvican/snailgun](https://github.com/jvican/snailgun)
&gt; Passing argument into 95% of functions with some 'Context'? Seems a bit tedious. That is what actually methods (vs functions) are actually doing though, even if it is tedious. They just pass the `this` reference along with every other argument to the method. &gt; Reader monad? - not very FP experienced team Seems like to me Reader monad would be the best option, and no better time to learn than the present.
You might want to look at Monix Local. There is an impure version that should work even with Future as long as you use TracingScheduler as ExecutionContext. Unfortunately it isn't well documented yet (mostly scaladoc right now) but if you feel like it is worth a shot, write on monix/monix gitter and we can look at it together and I could improve docs in the process :D
Here are a few resources to get you started: * [ScalaPB's gRPC documentation](https://scalapb.github.io/grpc.html) * [Google's guidance on API design](https://cloud.google.com/apis/design/). They advocate [resource oriented APIs](https://cloud.google.com/apis/design/resources). I.e. the good parts of REST. It's easy to automatically translate a gRPC/Protobuf API to a REST/JSON one. Here's an example service definition: ``` service LibraryService { rpc GetBook(GetBookRequest) returns (Book) { option (google.api.http) = { get: "/v1/{name=shelves/*/books/*}" }; }; rpc CreateBook(CreateBookRequest) returns (Book) { option (google.api.http) = { post: "/v1/{parent=shelves/*}/books" body: "book" }; }; } message Book { // Resource name of the book. It must have the format of "shelves/*/books/*". // For example: "shelves/shelf1/books/book2". string name = 1; // ... other properties } message GetBookRequest { // Resource name of a book. For example: "shelves/shelf1/books/book2". string name = 1; } message CreateBookRequest { // Resource name of the parent resource where to create the book. // For example: "shelves/shelf1". string parent = 1; // The Book resource to be created. Client must not set the `Book.name` field. Book book = 2; } ``` I have a [sample reverse geocoder gRPC service](https://github.com/vyshane/reverse-geocoder) that you can look at. Note that I'm using grpc-monix on top of vanilla ScalaPB. Vanilla ScalaPB will look very similar. Here's the entry point for the [service implementation](https://github.com/vyshane/reverse-geocoder/blob/master/src/main/scala/ReverseGeocoderService.scala). Here's what an [integration test looks like](https://github.com/vyshane/reverse-geocoder/blob/master/src/test/scala/GrpcServerIntegrationSpec.scala). We've been using gRPC in production for the last couple of years, and it's been a game changer for us: * Proto files are much easier to read (and diff!) than OpenAPI yaml. Code reviews on our gRPC API changes are way more enjoyable than on our OpenAPI ones. * Protobuf has built-in support for evolving APIs so that they remain backwards-compatible * Client side libraries are taken care of. To build the server side, you simply take the generated interface and fill in the blanks. * gRPC supports uni- and bi-directional streaming, and that makes things like these easy: Consuming from queues, push instead of pull, integration with streaming platforms (we were using Apache Beam, RxJava and gRPC) * Overall gRPC has made the mechanical/uninteresting parts of building microservices trivial for us
I think Nemerle has been doing macros in a statically-typed language for a long time, though I don't think their macros are typed. I don't understand the obsession with typed macros anyways. I think a macro should just be a function of type syntaxTree -&gt; syntaxTree, and it's up to the compiler to sort out whether the resulting syntax is well-typed or not.
I think nemerle has been doing macros in a statically typed language very well for a long time: https://github.com/rsdn/nemerle/wiki/Macros-tutorial. Unlike Scala, Nemerle was designed from the ground up to support macros, so it seems to integrate them much better.
I'd look at inverting the structure. Instead of having thread local data, have local instances. Create a tenant class, and give it a bunch of DAOs in the constructor. When you create a tenant instance, instead of handing it the DAOs directly, hand it proxies to the DAO that have all the methods filtered by the tenant ID. As far as the tenant is concerned, it's calling out to the DAOs as normal, and it doesn't care about any kind of access token, and doesn't need thread locals. This also means that you can change up your authentication however you want, as it's not managed by the tenant. This sort of pattern is called object capabilities. I got seriously interested in it a few years back and put together a [tutorial on it](https://wsargent.github.io/ocaps/guide/introduction.html).
Thanks for your feedback and will try to improve
Hahahaha, awesome!!
Hello, I prepared a talk on this exact point that I presented in my company and planning to present in conferences. Basically you can use Cats and make your base type ‘Kleisli’ (same as readerT) to pass some context around. As it would be tedious to use the ‘Kleisli’ type everywhere because it’s long and tedious to type (3 type parameters) you can simply alias it. So I am doing a bit of self promotion but it solves exactly this problem. Please have a look at https://github.com/lforite/rio-talk. The project is structured in 3 parts: - play example (open wip pr, you got to check it for examples for your case) - http4s examples yet to be done - slides, done in scala You will find in the description of the readme the link to the slides. Please PM for any extra help
Factor out all the code that uses the thread locals into functions that take parameters. Then you should be left with a bunch of "coordinating" code that is basically working with ThreadLocals and passing them to functions, perhaps setting functions' return values into thread locals, etc. At that point, it should be relatively straightforward to convert it to whatever other paradigm or tool you choose. You might want to defer choosing until that point, since it will give you more insight into the coordination structure. Once you've converted the thread-locals into something else, you can inline the functions back if you choose. These may seem like a lot of work and very roundabout, however: * You might do this for one isolatable subset of your codebase at a time. Perhaps after you do it for one "piece," it will be more obvious how to tackle the other parts. * IntelliJ can automate the pulling-out-a-function and inlining-the-function-back-in steps, although you still have to do it many times. * A more scorched-earth approach may be faster but it's much more risky, since you can't prove to yourself that the code is equivalent in behavior (although unit tests can). &amp;#x200B; I interpreted your question to be about "approaches to the refactoring process," but if your question was about "approaches to deal solve the same problem as thread-locals without thread-locals" (i.e. the end-state of the refactor), then... * As I said above, I think you're in a better position to decide based on the patterns that the thread-local usage forms * Thread-locals are about dynamic scope. They are a way to pass data along only to code running on the same thread. "Running in the same thread" is dynamic scope because it's a statement about runtime behavior that cannot directly be predicted from the code. If you think about it deeply, this is the root cause of why it falls short. The alternative to dynamic is scope is lexical scope. The way you pass data along lexical scope is with ordinary parameters and values, because normal code, including closures, works based on lexical scope. * Passing along a lot of parameters often can get tedious, but there are a lot of solutions to this. However, they all are variations within the broader idea of "just pass it as a parameter." 1. If a lot of things that need the same data go together, you can put them inside a class and make the data a parameter to the class. 2. If a lot of the data that you need to pass around goes together, you can group it into a class that you pass as one parameter. 3. If even after doing all of that, the parameters are sort of "noise" and don't make the code more understandable, and there should never be more than one if its type around, make them implicit. 4. If you're passing the same thing to a bunch of functions in a row, but you also want to be able to pass data from earlier ones to later ones, Reader monad a.k.a. Kleisli can be convenient. You don't have to introduce FP. Take the one from [https://earldouglas.com/posts/itof/di-to-reader.html](https://earldouglas.com/posts/itof/di-to-reader.html). You can rename it to something like, I don't know, FunctionChain. 5. If after all these you still have a lot of parameter passing and it's bothering you, try to change your mindset. A parameter are not an eyesore, it is a gift, because it means that the function is transparent to you (assuming it's pure). It tells you exactly what the function knows. And so by looking at the parameters you can also be sure that the function is not making any assumptions it isn't telling you about. So parameters are something to be grateful for!
It's the classical use case for implicit parameters.
Epic!
No. But to be a good Scala developer, you must know your platform - be it JVM, node.js or web browser. &amp;#x200B; Thus for JVM knowing Java and java libraries is inevitable, but surely can be learned later.
This is the correct answer.
Put up another blog post in my "How to X in Scala" series. I'm trying to put together a collection of hands-on tutorials on how to do common things, not necessarily Scala-specific tasks, but using the Scala language. Hope someone finds it useful!
This is the correct way. And actually there is a minimum of overhead (only implied by flatMap), as \`Future::successful\` creates already complete future. &amp;#x200B; If you wish, you can create implicit conversion &amp;#x200B; implicit def aAsFuture\[A\](a: A): Future\[A\] = Future.successful(a) &amp;#x200B; but as a rule of thumb, the fewer there are implicit conversions in your code, the better. &amp;#x200B; Explicit instantiation a way better from my POV. Maybe explicit import of sorts could be safe middleground: &amp;#x200B; implicit class AsFuture\[A\](val self: A) extends AnyVal { def asFuture(): Future\[A\] = Future.successful(self) }
Surprisingly low salary numbers in London compared to SF/NY.
I'd use a `for`-comprehension to make it more readable, and psychologically, I think even new readers of Scala intuitively pick up that `for`-comprehensions "get values out of" the things to the right of the `&lt;-`, so maybe the question would seem less pressing. Better yet, IMO, replace `Future.successful(0)` with `Future.failed(new AuthFailedException(...))` You probably shouldn't lie and say you successfully got 0 results, but you also presumably want the failure to be modeled by the failure semantics of `Future`, which, e.g. could already be in a failed state because of failure to contact the LDAP server, or whatever else you're doing for authentication.
Please, fix DoS vulnerablility in uJson: https://github.com/lihaoyi/upickle/issues/273 And a nasty validation bug: https://github.com/lihaoyi/upickle/issues/259 Users of your libs deserve more respect! Also, there are much safer and efficient libraries in Scala to work with JSON, like [jsoniter-scala](https://github.com/plokhotnyuk/jsoniter-scala) or [borer](https://github.com/sirthias/borer)
JavaScript's `then` is either scala's `map` or `flatMap` depending on the return type. It's a clever use/abuse of JavaScript's lack of typing, and something that does not really map (hah) well to current Scala 2. In Scala 3, you can implement it thanks to union types: ``` def (fut: Future[T]) then[T, U](f: T =&gt; U | Future[U]): Future[U] = fut .map(f) .flatMap({ case futU: Future[U] =&gt; futU case u =&gt; Future.succesful(u) }) ```
I can highly recommend the TypeLevel stack: `http4s` for HTTP, `doobie` for SQL, `circe` for JSON, etc you can find some example services here: * https://github.com/tpolecat/doobie-http4s-sangria-grapgql-example * https://github.com/jaspervz/todo-http4s-doobie these are g8 templates: * https://github.com/profunktor/typelevel-stack.g8 * https://github.com/olivierschultz/typelevel-template.g8 other resources: * [https://scalac.io/typelevel-ecosystem-overview/](https://scalac.io/typelevel-ecosystem-overview/) * [https://kubukoz.github.io/talks/http4s-doobie-micro/slides/](https://kubukoz.github.io/talks/http4s-doobie-micro/slides/)
Note there is an overhead in the JS version, although you don't see it directly. It has some code somewhere that has to check for a promise and do something different if it isn't.
Lack of proper documentation tremendously adds to the Scala learning curve. It is the first language I have used that doesn't have official documentation of the language. A tour is not Documentation. I am assuming most libraries are created by people who spent many years learning Scala by the "Poke, Prod, Dig, Poke, Prod, trial and failure" Method so therefore they use parts of the language regular developers never get exposed to and would have to dig, prod, poke to discover. It is taking me longer to learn Scala than any other language I have ever, ever, ever laid eyes on. If I didn't think AKKA was a superior Actor framework for creating modularized apps then I would not bother with Scala, it's just not a good payoff. Having to dig, poke, prod and discover via google and trial and error really takes the fun out of learning a language. Just reading the Rust Documentation book makes me want to learn Rust.
This book by Alvin Alexander takes FP apart and explains it in tiny 1 to 5 page chapters. It is a 800+ page book. He literally takes the concepts apart and describes them sometimes in a slow story type fashion building up over several chapters. [https://www.amazon.com/Functional-Programming-Simplified-Alvin-Alexander/dp/1979788782/ref=sr\_1\_4?keywords=functional+scala&amp;qid=1561748116&amp;s=books&amp;sr=1-4](https://www.amazon.com/Functional-Programming-Simplified-Alvin-Alexander/dp/1979788782/ref=sr_1_4?keywords=functional+scala&amp;qid=1561748116&amp;s=books&amp;sr=1-4)
Thanks! I've been able to throw together a pretty simple gRPC service and client in JS (via gRPC-Web). I'm just wondering how you're consuming those `.proto` files that come with `protoc`, e.g. `import "google/protobuf/timestamp.proto";`. Should those be included in my directory structure? I don't see them anywhere in that repo, and I've tried importing them via absolute path (mine are in `/usr/local/includes`) but then my `.proto` file fails to compile.
Maybe [http://monadless.io/](http://monadless.io/) could help?
This is a pretty sneaky/nasty comment.. if you want to state you wrote a library that is better for xyz reason, you just state that directly. You're making a comment that makes it appear you're a user of uJson and he's not caring enough about your issues, when in reality, looks like you wrote jsoniter-scala did comparisons, found/reported those bugs, and are now making somewhat personal attacks using those.. Just to be clear, we appreciate that you have written the other library, and that you've identified issues and places for improvement on uJson and uPickle.. thats great! ... just be more constructive and transparent about it, there's no need to be slimy about it or start making personal sounding insults on whether he's giving his users enough respect or not... especially when its really pointing at performance of scala hashmap implementation.. are you going around insulting the scala collection library maintainers too that they arent giving enough respect to scala users because the hashmap implementations arent collison resistant? I sure hope not, so use the same decency here.
I also can't access the site. I've tried 3 times on different days. No videos (including those from other conferences) work on portal.klewel.com.
Scala Future or JavaScript Promise are type classes that describe the computation. It can be side effect such as API call or DB operation, or it can be purely functional without side effect. So the type class which gives you a “pure” operation is Applicative, while flatMap comes from Monad and map comes from Functor. The monad here is Future, let’s just think of it as F[_]. If you use cats library, you can have something like this: def auth(): F[Boolean] def getNum(): F[Int] def doStuff(): F[Int] = { auth().flatMap { case true =&gt; getNumber() case false =&gt; 0.pure[F] } Like other comment mentioned, return 0.pure[F] isn’t great. What we can do here is SomeError.raiseError[F] or something similar.
Nitpick: Future is not a type class.
Yep. I kind of don’t want to go down too many rabbit holes in one comment
Surprisingly low salary numbers in the rest of the world compared to SF/NY. London the only way to make money as a dev is contracting although I heard the IR35 tax rules is making that less attractive,
Are you asking what Cassandra type the field in the database should have? Or have you already changed the type of the field in the database, and you’re asking how to store your Scala values in that field? Which library are you using to access Cassandra?
Why not just use Circe? It’s kind of the standard. https://github.com/circe/circe/blob/master/README.md
It can actually solved by the magnet pattern (see e.g. [https://www.clianz.com/2016/04/26/scala-magnet-pattern/](https://www.clianz.com/2016/04/26/scala-magnet-pattern/)). But it is not best practice - just use an implicit class as /u/seigert has shown. Imho, \`0.future\` or \`0.asF\` is short enough and much more safe!
&gt;London the only way to make money as a dev is contracting What a preposterous statement
Don't ever be that rude again!
Sorry to be terse, but I just thought that that statement projected such a horrible level of greed and entitlement, one which I would rather not have associated with the profession.
I'd like to know what benefits developers in SF/NY get with that salary, as well as how much they're paying for rent.
I think, in general, developers are pretty much underselling themselves on average.
I'm just joshing friend
:-)
logo suggestion [![snailgun](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/6ff886bc-f96b-45b7-9866-ad8c14fc2e23/db5r7nb-eb7458e1-714c-4128-b0d8-26c0f371a817.gif?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzZmZjg4NmJjLWY5NmItNDViNy05ODY2LWFkOGMxNGZjMmUyM1wvZGI1cjduYi1lYjc0NThlMS03MTRjLTQxMjgtYjBkOC0yNmMwZjM3MWE4MTcuZ2lmIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.qUPqqBhpRDBkzKBsm2qYIBZEZjQeWXAy88j1WbLMlTI)](https://www.deviantart.com/majormoonie/art/The-Gun-Snail-674795639)
I was asking what I would need to change the type of field in the database to, I figured it out though. I did not realize you needed to use Tuple() in Cassandra
Everyone works to earn a living, having a interest in what you do and enjoying it is a plus, doesn’t take away the bills and costs of living, in London it’s not cheap. No matter what job someone does most money is a factor for the simple fact living costs money. If you go on a full time permanent role you are looking at 60-85k salary range, sure there’s outliers on more. Contracting you’ll be on 500 a day+. Contracting if you are good it’s easier to get better rates, full time getting salary increases is much harder with the hassle of full time employment, annual performance reviews, office politics etc etc. You can do the same job and get paid less full time or go contracting and get significantly more for the same job and see a greater variety off work. All the good ex colleagues and friends I worked with now contract for the simple reason they earn significantly more than they did work as a permanent, with young families they choose to have more period off with the family as they only need to work half the year to make their previous salaries. Even though they earn significantly more it’s still less than some of the advertised starting salaries and offers you see quoted from the US. Contracting has took there careers to the next level also with variety of work compared to the sometimes isolated tunnel vision view you get at a single company after a few years.
In case of multi-tenancy, you probably want to change the behaviour based on the tenant. So if you would pass the tenant to a method, it could look like this: ``` trait FooService { def doTheFoo(tenant: Tenant, foo: Foo): Task[Unit] } class FooServiceImpl(...) { def doTheFoo(tenant: Tenant, foo: Foo): Task[Unit] = tenant match { case TenantA =&gt; ??? case TenantB =&gt; ??? } } ``` Instead you could have several implementation of `FooService` and select the right one based on the tenant. So the trait could look like this: ``` trait FooService[T] { def doTheFoo(foo: Foo): Task[Unit] } ``` and somewhere at the composition root of your app you would have something like ``` trait Modules { type T val fooService: FooService[T] val barService: BarService[T] // ... } object Modules { val tenantAModules: Modules = new Modules { type T = TenantA.type val fooService: FooService[T] = ??? val barService: BarService[T] = ??? } val tenantBModules: Modules = new Modules { type T = TenantB.type val fooService: FooService[T] = ??? val barService: BarService[T] = ??? } def apply: Tenant =&gt; Modules = { case TenantA =&gt; tenantAModules case TenantB =&gt; tenantBModules } } ``` Now instead of ``` fooService.doTheFoo(tenant, foo) barService.nanana(tenant, 25) ``` you could use it like this: ``` val modules = Modules(tenant) import modules._ // ... fooService.doTheFoo(foo) barService.nanana(25) ``` The type parameter ensures that you cannot mix a `FooService[TenantA.type]` and a `BarService[TenantB.type]` in a `Modules` instance, but you could also drop it.
I'm more thinking about the data flow than the type signature -- in capabilities, the API may look exactly the same (there may be only one `Tenant` type) but the reference is to a different instance and access to various instances is strictly limited. I agree it's a good idea to add type-safety where you can -- leaning on refined types as well. I've played around with using dependent types to enforce capabilities, but it breaks down when I start putting dependent types into collections. I don't know if there's a way around that.
Interesting. Do you have some code examples that show off what capabilities can do?
https://github.com/wsargent/ocaps/blob/master/src/test/scala/ocaps/examples/RepositoryComposition.scala
I guess, that you'll want to give [scala-async](https://github.com/scala/scala-async) a try: ```scala def getCount: Future[Integer] = async { if (await(isAuthenticated)) { await(getNumberOfThings) } else { 0 } } ```
We are using sbt-native-packager to build Docker images and didn't have any problems whatsoever, it works great. I haven't used docker-it-scala but with the other similar library (testcontainers) the problem was that it did take a very long time to start containers on a Gitlab Docker runner so I ended up using Gitlab's built-in feature for running additional containers - \`services\` directive [https://docs.gitlab.com/ee/ci/services/mysql.html](https://docs.gitlab.com/ee/ci/services/mysql.html)
Mill has a docker module that is straight forward. http://www.lihaoyi.com/mill/page/contrib-modules.html#docker
There is this wonderful library https://github.com/scalalandio/chimney I think that’s what you are looking for
Often, the concept of peristence or identification exists within the domain and is not something technical. I.e. on reddit, your user-id which is used to identify exactly you and only you is part of the domain. Then, this concept is used to persist and retrieve elements, because the developer usually has the choice of how to do so. In your case, if you can't choose how persistence and identification works, I think you have two choices. 1. Let the identification/persistence leak into the rest of your code, i.e. by doing `case class Persisted(obj: DomainObject, ref: StorageType)`. Can work but can also be ugly. 2. Have your own mapping between your customly created/managed ids and the `StorageType`. I.e. your repository has an internal `Map[MyId, StorageType]` and then you have put `MyId` into your domain. Versioning comes on top, but I don't think it really makes a difference.
Netflix | Data Engineer | San Francisco, USA | Onsite | Full Time | Top of the market &amp;#x200B; Hi, &amp;#x200B; I'm a data engineer in the Streaming Data Science &amp; Engineering at Netflix and we are looking for data engineers [https://jobs.netflix.com/jobs/864557](https://jobs.netflix.com/jobs/864557). &amp;#x200B; Feel free to contact me either for more info, apply or just to have a chat because you are curious. &amp;#x200B; Thanks
Is there a software engineer who will help Li to maintain uJson/uPickle and fix all reported bugs or vulnerabilities just in time, without panic? I would never see into these libraries if there no so provoking title "How to X in Scala" for tutorials about Li's libraries. Not taking into account other options is a form of a lie for such titles. As an example the following article has mentioned uJson in the title, but he still lie about performance and safety here: http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html Many authors are falling to such style: https://medium.com/@djoepramono/how-to-parse-json-in-scala-c024cb44f66b http://fruzenshtein.com/scala-working-with-json/ But, there was others who tried to keep up with new JSON tooling and mentioned different alternatives: https://manuel.bernhardt.io/2015/11/06/a-quick-tour-of-json-libraries-in-scala/
You can implement it in Scala 2 with simple overloading tbh. (Or if that doesn't cut it - magnet pattern)
It can be good practice to write Rosetta Code examples, there are plenty missing a Scala implementation and a lot of the existing Scala ones are not very Scala-style.
I think the currently most popular FP approach to this is Scala is what’s called tagless final. This is essentially what you describe already, you define a trait that defines your algebra (in OO this is the interface based on the domain needs), the major difference is that your algebra has a second level of polymorphism (parametric) they abstracts over the effect type. And then, you provide the implementations of your algebra (usually also polymorphic on the effect type) for different back ends.
I answered the topic, but since that seems to be gone I'll put my answer here: ``` def dropWhile[T](lst: List[T])(cond: T =&gt; Boolean): List[T] = LazyList.unfold(lst){ case e +: es if cond(e) =&gt; Some((es, es)) case _ =&gt; None }.last ```
Maybe with magnet (not very familiar with it), but not with overloading. There is no Scala signature other than `then(f: ... =&gt; Any)`that will accept the following: ``` Future(5).then { x =&gt; if(x &gt; 10) true else Future(false) } ``` Altough an implicit conversion to a Future will work with just flatMap.
I'm a data scientist struggling to learn Spark with Scala. I'm already fluent in Python and R and have picked up core Scala reasonably well, so learning the Scala language isn't my issue. I'm interested in buying a physical book that focuses on ML with Spark written in Scala. It would be awesome if this book focused on both cluster computing and actually compiling Scala programs, which is something most books and tutorials leave out! I already own the Karau book which is great, but outdated. Any suggestions at all would be appreciated. Thank you all in advance!
Try `spark` http://sparkjava.com/documentation#getting-started Very simple and clear framework. Use Java API.
For some reason it didn't work for me intially, but after lowering my scala version to version 12 point something, it worked. Thanks for your help! :D
Don’t try and circumvent their standard kubernetes docker deploy plugin. Implementing everything yourself is a complete nightmare. We wanted to go with an all hashicorp setup (consul and nomad). Implementing everything like Cluster management etc is a huge task. In a small team, it’s practically not worth it.
sys.env is for getting environment variables at runtime. I'm not super familiar with jenkins but by the looks of it these secrets can be made available to jenkins build scripts, so you would then use those to insert the values into config files (or source files I guess) that would get bundled into the build.
I just released a new version of my programming language targeting 8-bit computers: https://github.com/KarolS/millfork The language has grown quite a bit. I added support for compound datatypes, typed pointers, const arrays, and more. There's even Intel 8086 support now, but it's just a toy implementation of an 8080-to-8086 crossassembler and the resulting code is awful. But you are probably also interested in Scala-related things: I got GraalVM's native-image running. You might have though a second ago "isn't Scala not well suited for short lived programs like compilers" and you'd be right. Compiling the compiler to native code turned a ~1.6s invocation of `java -jar millfork.jar --help` to ~0.1s `millfork.exe --help`. The executable isn't even that big. Pattern matching and recursing down a list are a godsend for writing compilers, but stack overflows started becoming a problem. Luckily, I discovered that Scala has `scala.util.control.TailRec`, which allowed me to rewrite the overflowing functions without much work. Still on topic of Scala discoveries, I also found out the hard way that `Map#mapValues` does not return an actual map, but instead calculates everything on demand. If you traverse the result multiple times, you recreate the values multiple times. If your new values are mutable, then you suddenly have multiple versions of them and it's a mess. I spent too much time debugging one weird issue related to this.
Most people I know that have decent Scala knowledge are contractors in London. And starting salary as a contractor in London is the average salary in SF/NY.
Note: this specifically refers to the Typelevel fork of the Scala compiler (and not to Typelevel generally, as I initially thought...).
Thanks for posting this!
I'd prefer abstraction leak over internal `Map` any day, managing mutable state is generally very hard.
Chartboost | Software Engineer, Scala (Senior and mid level positions) | San Francisco, CA and Barcelona, Spain | Onsite | Full-Time My name is Mark and I'm looking to grow my Ad Serving team here at Chartboost! We are currently looking to hire for our San Francisco and Barcelona offices, and we are open to considering relocation for interested candidates. We are looking for passionate backend engineers who love Scala and typed functional programming (cats, cats-effect, http4s, akka-http, shapeless) to join our team and help us build the best advertising platform for mobile developers. You'll work on a system that processes tens of thousands of requests per second and conducts a real-time auction to find and deliver the most effective ads from the Chartboost Network. The team's working on some exciting initiatives! Please find the job descriptions listed below and apply directly. [San Francisco](https://boards.greenhouse.io/chartboost/jobs/1596880) [Barcelona](https://boards.greenhouse.io/chartboost/jobs/1533886)
In a functional programming setting is just a function as simple as `(S, A) =&gt; S` where `S` is the domain object, `A` is the property being set. If you define this function as a data type, you can require it implicitly like so: case class Setter[S,A](set: (S,A) =&gt; S) implicit val domainSetter: Setter[Domain, StorageTypev1] = Setter((d,s) =&gt; d.storage.copy(storageProperty = convert(d.property)) ...) From here you can have functions that generic over the type of domain object or storage object. def updateDomain[S](d: Domain)(implicit setter: Setter[Domain, S]): ??? = { ... val new domain = setter.set(d,...) ... } def deleteStorage[D](d: D)(implicit setter: Setter[D,StorageTypeV1]): D = setter.set(d, EmptyStorageV1) Adding a getter is as easy as adding a second function `S =&gt; A`. case class Property[S,A](get: S =&gt; A, set: (S,A) =&gt; S) Instead of writing these datatype yourself however, these datatypes are actually known as Lenses in functional programming, and by including a popular existing Lens library like [Monocle](https://github.com/julien-truffaut/Monocle) you can get these and many more abstractions around the idea of functional getters and setter.
Recently moved from lightbend's lagom to tapir as our service definition format at work -- nice that it's explicitly instrospectable / annotateable which makes it much easier to build tooling for &amp; with.
For updating individual fields, there is shapeless lens which can be great when you have multiple case classes with the same fields. https://www.scala-exercises.org/shapeless/lenses You can then use typeclasses to represent the operations of your domain, and use these lens to automatically derive instances of your typeclasses. For example, recently at work we created an `Auditable` typeclass, and all case classes with these two fields: `lastUpdatedBy: UserId`, `lastUpdatedAt: Instant` can automatically have an instance of `Auditable` derived for it.
For updating individual fields, there is shapeless lens which can be great when you have multiple case classes with the same fields. https://www.scala-exercises.org/shapeless/lenses You can then use typeclasses to represent the operations of your domain, and use these lens to automatically derive instances of your typeclasses. For example, recently at work we created an `Auditable` typeclass, and all case classes with these two fields: `lastUpdatedBy: UserId`, `lastUpdatedAt: Instant` can automatically have an instance of `Auditable` derived for it.
I ended up with similar thing for my model. I'm glad to read, that my approach is on the right track. I tried using Monocle, however I found out that defining fine-grained monocle Lenses and combining them is very tedious for my case. I guess it's fine when you want to update fields in different places of your code, and sometimes you need to updated one set of fields, and another time, the other set, and sometimes all of them, so robust combination of lenses is needed. My priority was avoiding allocations (i.e. set all fields using a single copy) and I only need to morph domain to storage once, on repository level, where I copy all fields of domain object to the backing storage object. So if I were to use monocle, I'd have to use its isomorphism `Iso`. `Iso` is just a pair of functions with some combinators. And I don't really need the latter.
Don't you find it inconvenient, that Shapeless lenses rely on symbolic literals to refer to fields? How would you approach simple refactorings, f.ex. renaming `lastUpdatedAt` to something else? Do you write tests for your shapeless lenses?
3-4 year ago I was learning Scala, but ended up abandoning it more or less because I felt that real FP in Scala was a bit underrepresented, and learning pure FP was one of the reasons I actually started learning Scala. Now, after coming back to have a look at how far the language has come, I'm actually very pleasantly surprised to see that the FP community in Scala-land has flourished quite a bit and people have been building many wonderful libraries and tools. My question is, has there been any work on exploring the same space in ScalaJS? Are there any examples, libraries or framework that show or explore how (pure) FP can be used in the world of frontend applications with ScalaJS? For example, I've not seen any example on how to structure a ScalaJS application that relies heavily on the IO monad for example. Or currently trends such as Tagless Final or libraries like Cats usable in a frontend application, and are there any examples out there?
FWIW, I ended up embracing Scala's support for multiple paradigms instead of fighting it :) My model has some FP flavor to it. It uses FP elements, such as lenses and my model &lt;-&gt; storage converter is called morphism (correct name is the half of the job, right? /jk). Each storage type defines a set of optics to mutate them easily. My domain is not fully decoupled from the storage, is it has to know what kind of storage object backs it. I find it cleaner than maintaining cache on repository level. The downside though, is that I can't uniformly mutate my domain model (for example merging domain objects obtained from different versions of the storage into a single list and traversing through it). If it appears to be a problem, I'll try to come up with the solution. ``` case class Domain[R](..., backingResource: R) trait DomainMorphism[R] { def toDomain(r: R): Domain[R] def toResource(d: Domain[R]): R } object StorageResourceV1 { // skuber is all polymorphic, you only need to define types type CRD = CustomResource[...](...) // optics grouped together for convenience object optics { val setProperty: Property =&gt; CRD =&gt; CRD = { value =&gt; ... } } // since my repositories are versioned too, I do not need to make this implicit val morphism = new DomainMorphism[CRD] { import optics._ ... } } ``` Repository and service code are OOP flavored. I define separate repository for each version of resource I have (because domain object maps to different number of storage resources, depending on the version, so even different number of calls to backed is often necessary). Repositories are exposed via a common interface. Same goes for services, where needed. It means, that sometimes I have duplicated code, but it's much easier to maintain, than dealing with very generic repository and then abstracting every little detail with a typeclass. I tried this, and number of typeclasses has grown so quickly, that amount of code supporting this infrastructure was far greater, than amount of business logic. ``` trait Repository[R] { def get: F[R] def update(d: Domain[R]): F[Unit] } class RepositoryV1(morph: DomainMorphism[ResourceV1]) extends Repository[ResourceV1] { override def get: F[ResourceV1] = ... override def update(d: Domain[R]): F[Unit] = ... morph.toResource(d) ... } class RepositoryV2(morph: DomainMorphism[ResourceV1]) extends Repository[ResourceV2] { ... } class MyService(repoV1: Repository[V1], repoV2: Repository[V2]) { ... } ``` I'm still open to suggestions and would be curious to know, how other people organize their code.
For those who don't read usernames (unless accidentally like me)- this is by Miles Sabin, a frequent scala contributor and creator of TypeLevel "fork" of Scala.
What a legend
Refactoring is an issue, but tests are not really needed - it's typesafe. If you change your \`lastUpdatedAt\` to something else, your code won't compile.
Do you have the **sbt** variable `requireJsDomEnv in Test := true` set?
It seems that you are asking Scala.js to emit an ES module (with something like scalaJSLinkerConfig ~= { _.withModuleKind(ModuleKind.ESModule) } ) but then you're trying to execute/test the code in a JavaScript VM that does not support ES modules (hence the `SyntaxError`). With Node.js you have to use special flags for it to work. See https://www.scala-js.org/news/2018/11/29/announcing-scalajs-0.6.26/ &gt; In addition, you will need a few additional settings: &gt; &gt; jsEnv := { &gt; new org.scalajs.jsenv.NodeJSEnv( &gt; org.scalajs.jsenv.NODEJSEnv.Config() &gt; .withArguments(List("--experimental-modules")) &gt; ) &gt; } &gt; &gt; artifactPath in (proj, Compile, fastOptJS) := &gt; (crossTarget in (proj, Compile)).value / "myproject.mjs" &gt; &gt; artifactPath in (proj, Test, fastOptJS) := &gt; (crossTarget in (proj, Test)).value / "myproject-test.mjs" &gt; &gt; The first setting is required to enable the support of ES modules in Node.js. The other two make sure that the JavaScript produced have the extension .mjs, which is required for Node.js to interpret them as ES modules. &gt; &gt; The support for running and testing ES modules with Node.js is experimental, as the support of ES modules by Node.js is itself experimental. Things could change in future versions of Node.js and/or Scala.js.
**Sky | Mid &amp; Senior Scala Engineers | Osterley (West London) | Onsite with flex/remote options | Full Time | Competitive Salary &amp; Benefits!** We at Sky ([https://www.sky.com/](https://www.sky.com/)) are looking for Java and/or Scala developers to help develop our NOW TV platform ([http://www.nowtv.com/](http://www.nowtv.com/)) - our most interactive product. We think you could be a great fit for the team. There are 2 Java/Scala squads in NOW TV with 8-10 developers in each team working in an Agile Scrum environment. We work with the latest technologies and encourage developers to put forward new ideas about technology. Here is a link that shows our fantastic West London (TW7 5QD) campus: [https://www.youtube.com/watch?v=xbbqD17hoMM](https://www.youtube.com/watch?v=xbbqD17hoMM) and our great new building: [https://www.youtube.com/watch?v=TWbB8Kq2QD8](https://www.youtube.com/watch?v=TWbB8Kq2QD8) Want to find out more? Get in touch directly - [daniel.mccaughan@sky.uk](mailto:daniel.mccaughan@sky.uk)
Scala treats \`null\` values differently when unboxing (unfortunately?) (see e.g. [https://github.com/scala/bug/issues/4437](https://github.com/scala/bug/issues/4437)). In general, \`null\` values are treated as the respective type's "default value". You can find the relevant pieces of code in the class \`BoxesRunTime\`, e.g. for boolean values: \`\`\` public static boolean unboxToBoolean(Object b) { return b == null ? false : ((java.lang.Boolean)b).booleanValue(); } \`\`\` Now in your case you are getting different values at different lines of the code, because in the condition it does do the unboxing to \`Boolean\` (i.e. the \`null\` value will be converted to \`false) whereas in the \`println\` statement it does not unbox the value. The following code illustrates the difference as well: &amp;#x200B; \`\`\` scala&gt; val b = new java.util.concurrent.atomic.AtomicReference\[Boolean\]() b: java.util.concurrent.atomic.AtomicReference\[Boolean\] = null scala&gt; b.get() res0: Boolean = false scala&gt; b.get() == false res1: Boolean = true scala&gt; (b.get(): Any) res2: Any = null scala&gt; println("Value: " + b.get()) Value: null scala&gt; println("Value: " + (b.get(): Boolean)) Value: false \`\`\`
Looks seriously good. Such a shortage of frameworks like this.
I had a quick look and the DSL feels really nice and natural. I would definitely use it if the need comes. Good job!
One thing to strongly consider is not using Scala.js on the back end.
Real slick man. Good work
Scala distinguishes strongly between boxed and unboxed types, as you know. So it's important to keep in mind `null` isn't a null _value_, but a null _reference_ (see [the `Null` API docs](https://www.scala-lang.org/api/2.12.8/scala/Null.html)): &gt; Since `Null` is not a subtype of value types, `null` is not a member of any such type. For instance, it is not possible to assign `null` to a variable of type `scala.Int`. So if there's an argument to make, I'd say it's that it's incoherent to talk about "unboxing a primitive value." `BoxesRunTime` issues its barbaric yawp, bellows "I am large; I contain multitudes," and embraces the incoherence.
Interesting! On one of my projects, I have a [sbt task that copies all files](https://github.com/stryker-mutator/stryker4s/blob/master/project/GitHooks.scala) from a folder with [a bunch of git-hooks](https://github.com/stryker-mutator/stryker4s/tree/master/git-hooks) to the `.git` folder. This task is then called [`onLoad` of sbt](https://github.com/stryker-mutator/stryker4s/blob/master/build.sbt#L6), so anyone who starts sbt will have the task installed
It'll fail to compile if you, say, rename a field or change its type. (Instances can no longer be derived for your case class that was just modified). You can write tests to test the actual behaviour (even though they all rely on the typeclass derivation).
[scalajs-react](https://github.com/japgolly/scalajs-react) is very much into purely functional frontend applications. Also note that most major Scala libraries cross -compile for Scala.js, Cats included.
Care to elaborate on * Why? (Otherwise your sentence is really just a troll comment.) * How that's even related to the question? (Nothing in OP's message suggests that they are taking about the backend.)
A new highlighting schema for the functional code will be very useful. Good idea.
It's not for the backend. It's supposed to be consumed by a client side react app to communicate with the backend. I am using scala for by backend.
&gt; We work with the latest technologies Do you have a comprehensive list? &gt; There are 2 Java/Scala squads in NOW TV with 8-10 developers in each team Are the teams mixed or is it one team Java and one team Scala? &amp;#x200B; Some more questions that people will probably be interested in: \- Do you sponsor visa? \- Do you sponsor relocation? \- What experience level / position do you expect? Junior/Senior/Lead/...? \- Any more detailed description in terms of what the tasks will be? E.g. is it more about data engineering, building a backend, fullstack, ...?
Hi, thanks for the heads up. If you email me then I can send you the detailed job descriptions, otherwise you can find out more at [https://careers.sky.com](https://careers.sky.com/)
As long as it doesn't break cursor movement...
It's possible I'm confused by some terminology. I don't think you're saying you're running Node.js in the browser, right? I'm assuming you're running Node.js on a server, hence "backend." But it sounds like it might be more accurately described as a "middle-end," that is, something the front-end communicates with that intermediates with some other server-side business logic that you might consider more properly called "backend." If this is the case, it doesn't change my thinking about possible solutions, but to be clear, I don't know what the motivation for using Scala with Node.js is to begin with.
I'd say [this reply](https://www.reddit.com/r/scala/comments/c84rae/unit_testing_in_scalajs/eskcrtc?utm_source=share&amp;utm_medium=web2x) spells out the details, which I'd summarize as: if you're using Node.js on a server to run Scala, you will inevitably have _some_ issues you don't if you run Scala in its "native habitat," namely the JVM, on that same server. That's not a criticism of Scala.js, which is literally genius work, or even of Node.js, which is an amazing engine built around an amazing implementation (V8) of one of the crappiest programming languages on God's green earth (JavaScript), and even _that_ can't really be called Brendan Eich's fault. It just reflects that Scala.js-on-Node.js has some twitchy bits, partly because JavaScript ecosystem (cf. "ES modules with Node.js is experimental, as the support of ES modules by Node.js is itself experimental") and partly just because that combination is nowhere near as mature as Scala-on-JVM. As for "Nothing in OP's message suggests that they are talking about the backend," that's interesting. It's possible "Node.js" doesn't imply "backend," but I admit I'm not seeing how.
Was hoping for DB transaction management but this is cool too.
I am not using node.js for my backend. Currently my backend is comprised of a combination of different AWS lambdas running scala. Probably in the near future ECS running the Play framework. I have API gateway in front of my infrastructure. I am making a SDK that react needs to use to communicate with API gateway.
&gt;requireJsDomEnv in Test := true no, I don't have that set.
thanks, I found where you got these commands from. Although I have a problem running them in my build.sbt. &amp;#x200B; &gt;/home/shane/code/LawnnannyApiSdk/lawnnannysdk/build.sbt:15: error: type NodeJSEnv is not a member of package org.scalajs.jsenv &gt; &gt;new org.scalajs.jsenv.NodeJSEnv( &gt; &gt;\^ &gt; &gt;/home/shane/code/LawnnannyApiSdk/lawnnannysdk/build.sbt:16: error: object NODEJSEnv is not a member of package org.scalajs.jsenv &gt; &gt;org.scalajs.jsenv.NODEJSEnv.Config()
You are making a public job posting (and this appears to be part of your job). I would have expected that you can then post more details here. Instead, apparently I have to find the job posting myself - and there does not seem to be any job postings for Scala that list the technologies used by the developers in these teams.
If I remember it correctly, you also have to install some node packages. But I cannot really remember which, sorry. The idea is that your unit tests are executed in an isolated node environment which simulates the behavior of a real browser. The project I am working on at the moment uses yarn though. There also is a `useYarn := true` flag set in `build.sbt`. Finally, I remember there being an error in a specific scalatest version regarding scala.js tests. But that was over 6 months ago, so it might be fixed by now. Sorry if I cannot help you more. It has been a long time since I last touched that part of scala.js Asking in the scala.js gitter channel might help you a lot quicker.
node is usually used when running unit tests in scala.js as an alternative to a headless browser.
This is great news! I am programming a lot of pseudo html frontend code with lots of deeply nested functions in which case error highlighting is absolutely useless.
&gt;Thanks I will check out the scalajs-bundler or checkout the gitter channel if I cant get it to work.
Also want to note that when I upgraded to version "1.0.0-M8" that export everything by ES by default it seems like the tests are not actually running?
Oh, there's a typo in the full class name. It should be `org.scalajs.jsenv.nodejs.NodeJSEnv` (with the `.nodejs.` subpackage).
&gt; that export everything by ES by default I'm not sure what you're referring to. Scala.js 1.x doesn't export anything by default. Not any more than 0.6.x.
When choosing to use Scala.js on Node.js on the server, you also get benefits compared to the JVM, which can offset the " non-native habitat", as you call it. For starters, you'll actually get the exact same language on frontend and backend (instead of two dialects of the same language). Depending on your use case, you might be better served by a Node.js server than a JVM server. For example people use Scala.js instead of Scala/JVM on AWS Lambda, because the startup time is much shorter. And Node.js does not imply server because unit tests are executed on a Node.js VM by default, even if you're writing a frontend application. This is the case in Scala.js, but also true in the larger JavaScript ecosystem.
Circe is great. So is ujson
I have nothing to say about their greatness, greatness being a subjective concept and all, and I have never tried ujson before. But I did wonder because the ujson examples look very non-idiomatic and seem to let the underlying format leak into the code. Seems more like a solution I would expect to see in PHP rather than in Scala.
What to do if compensation action will fail?
This is amazing!
Hello u/pottyfromuranus we inform you that the ScalaDays 2019 videos are back online [https://portal.klewel.com/watch/nice\_url/scala-days-2019/](https://portal.klewel.com/watch/nice_url/scala-days-2019/) we apology for this late fix.
everyone who has a hole in the ass knows spark and stuff around, it is probably the most saturated technology on the planet, add kafka and shit and it is like 10k devs are all over it, it ia not even funny
I have a function called timed that looks like this: def timed[A](name: String)(f: =&gt; A)(implicit metricsRecorder :MetricsRecorder = null) = { val start = System.currentTimeMillis() try { f } finally { val diff = System.currentTimeMillis() - start Logger.info(s"Timed: ${name}: ${diff}ms") Option(metricsRecorder).map { _.record(name, diff) } } } } Is there any way to remove the null and yet keep the implicit optional?
Saga pattern itself instruct us to keep retrying compensation action until it's done, but that's not always a case so zio-saga provides you with a choice, you could use \`retryableCompensate\` to keep trying to compensate given a specific Schedule. Note: in case of sequence combination of saga steps failed compensation will not stop other compensations from executing, but in case of parallel - one failed compensation could stop the other from executing (it would be fixed in [\#9](https://github.com/VladKopanev/zio-saga/issues/9))
It doesn't
Thanks for the shout-out!
Thanks for the shout-out!
Hello u/expatcoder we inform you that the ScalaDays 2019 videos are now again accessible [https://portal.klewel.com/watch/nice\_url/scala-days-2019/](https://portal.klewel.com/watch/nice_url/scala-days-2019/) we apology for this late fix.
Hello [u/plokhotnyuk](https://www.reddit.com/user/plokhotnyuk/) we inform you that the ScalaDays 2019 videos are now again accessible [https://portal.klewel.com/watch/nice\_url/scala-days-2019/](https://portal.klewel.com/watch/nice_url/scala-days-2019/) we apology for this late fix.
Thank you so much! Please also keep in mind that most videos these days are played in mobile platforms!
We are happy to announce that the 2019 [~~#~~**ScalaDays**](https://twitter.com/hashtag/ScalaDays?src=hash) videos are now somehow "resurrected" [https://portal.klewel.com/watch/webcast/scala-days-2019/](https://portal.klewel.com/watch/webcast/scala-days-2019/) We now fixed the 504 time-out / 502 error server issue. Sorry for the delay, thank you for your understanding.
That's also a good point. Thanks for the reminder!
Yeah, the Lambda observation is sort of the thing I was imagining was the explanation for "not-browser," whether you call that "backend" or not. Thanks for the thorough explanation (and reminder about the testing situation, which is another good point).
This seems to me like it's calling out for an approach taken from [Independently Extensible Solutions to the Expression Problem](http://lampwww.epfl.ch/papers/IC_TECH_REPORT_200433.pdf). That is, you want to be able to independently add new operations or new types to a system over time, without the latter changes implying the need to recompile the earlier, or the earlier to be aware of the possibility of the latter in ways that violate type-safety. As someone else noted, you also have an orthogonal concern, namely bidirectional data transformation between your domain and storage types, which is addressed extremely well by Chimney.
Hello u/Zwu3FlidKo we inform you that the ScalaDays 2019 videos are now again accessible [https://portal.klewel.com/watch/nice\_url/scala-days-2019/](https://portal.klewel.com/watch/nice_url/scala-days-2019/) we apology for this late fix.
Here's a place to look if you haven't seen it yet. https://remoteok.io/remote-scala-jobs
How many of them have X years of experience Vs X times one year of experience?
Regardless, I probably read something wrong. Would you know how to resolve the error from above?
I will give it a shot in a moment and let you know show it goes.
 assert(x == x*1)
Starting with the left foot Dan... Publishing the job spec as detailed as possible will save both you and developers time and you'll get a respectable reputation.
Makes sense. ujson seems to come from more of a scripting philosophy. I assume someone coming from a language like php or Python would have an easier time picking up ujson off the bat than Circe, which appears to be more principled on FP.
You haven't given quite enough info here (or maybe the code is slightly off). Do you want exhaustive matching? I.e, is the case _ =&gt; None a case that you expect to hit, or is it just there to be a safety rail whilst you don't have type safety? If you know all the valid options at compile time, it's usually worth trying to translate this into the type system rather than getting an Option involved. What's the type of supplierInfo? In particular, is it sealed or not? Is every combination of (supplierInfo, searchType) valid? Like, are you going to have a SupplierAClient[RectangularGeolocation]? Or is it that SupplierA want to use CircularGeolocation where SupplierB want to use RectangularGeolocation?
Thanks. I'll check.
It's certainly going to be simpler to bring them fully into memory and then pass the whole thing to non-streaming clients. Only question is how much memory you have and how many simultaneous uploads you're likely to have. If maxFileSize \* maxSimultaneousConnections is significantly &lt; availableMemory then hey, skip the flows. Also, I believe there is also a setting somewhere that determines if an entity should be streamed or strict based on it's size. If your files are under this size then streaming to s3 and elasticsearch are clearly not going to give you a benefit. Lastly, do you have a streaming xml parser? If not, then you're going to have to load the whole thing into memory anyway, and you won't get any benefit from streaming to s3 and simultaneously accumulating it in memory to be parsed.
Hi, thanks for asking these clarifying questions! case _ =&gt; None is indeed a safety rail for now. SupplierInfo is sealed and each supplier should only have one kind of implementation. Therefore it is the case that SupplierA uses CircularGeolocation and SupplierB uses RectangularGeolocation.
Not sure where about you are but theres a company in Ontario Canada that might be a great fit for you. Dm me if you want the details
So I looked at some examples online on Github and changed my SBT to look like this. jsEnv := new NodeJSEnv( NodeJSEnv.Config().withArgs("--experimental-modules" :: Nil) ) artifactPath in (Compile, fastOptJS) := (crossTarget in (Compile)).value / "myproject.mjs" artifactPath in (Test, fastOptJS) := (crossTarget in (Test)).value / "myproject-test.mjs" I was able to get it to generate the required files but my tests dont seem to run when I run scala-test. Have any other ideas.
Scala Js for lambda actually seems like a good idea. It takes a while to start a JVM lambda. I might consider that if I can figure out scalajs.
Something I considered doing but I feel is the time is to use a cross build project so I can use native scala tests for scala and integration for javascript. I am not sure what scalajs was trying to with my previous config but I feel the timebox for that initial approach has expired. Any thoughts on this approach?
I just sent u a chat. Thanks.
Ok, so in that case you probably want search type to be a type parameter on the supplier. I'm on my phone right now so I can't type it out properly, but if you had something like: sealed trait SupplierInfo[ST &lt;: SearchType] And SearchClient[ST &lt;: SearchType, SI &lt;: SupplierInfo[ST]] Then I think you could reference this in getRates[ST], AND exhaustive match on SupplierInfo to get the relevant SearchClient.
Aha thanks that makes sense
[removed]
Nice work :-)
I love the idea. My current workflow: I start with jq for simple queries. Then it get more complicated and I open my text editor with nodejs. jqs would be perfect for small to medium size queries.
So I'm trying to determine if any of the decimal digits of a number are even: ``` //First try num.toString.contains(_.asDigit%2 == 0) //Second try num.toString.toVector.map(_.asDigit).contains(n =&gt; n%2 == 0) //Third try num.toString.toVector.map(_.asDigit).contains{case n: Int =&gt; n%2 == 0} //Another try num.toString.contains{case c: Char =&gt; c.asDigit%2 == 0} ``` But it keeps telling me that it's a collection of type Any and that I can't compare unrelated types. This is kind of confounding, because I can't see where the type becomes ambiguous. I was working with SafeLongs, but I tried Ints as well. Anyone have any ideas?
Cool, I am going to publish something usable enough tomorrow. For the future I think tab completion on the json would be awesome and possible
Is that even enough to survive in London?
Hi, I work as a Scala developer in a 3rd world country. When I started Scala, I didn't have any experience what it is but now I am in love with it. I have been working on data engineering applications, ETLs, play framework applications and so far enjoying it a lot. It has been 1y since I started working in this language. Now the thing is, not many companies here work in Scala, there's no where I can switch to. Current company I am in is very small company, I get no benefits and salary here is so less that I can't even mention here but let's just say what I get payed monthly is less than what usually developers get in half a week in US. Don't get me wrong, comparative to salaries payed to developers in my country I will lie in \`payed well\` category. Whenever I try to switch they slightly increase it but why I am not switching it because if I switch to some other company they will make me start working on some other language/technology, which I can do easily but I won't enjoy doing as I love to work in Scala. So my questions is, if I want to work in some Western country, what can I do? My country doesn't have a good reputation so getting visa without sponsorship will be almost impossible. Has any of you gone this path in career? How did you find a job/company? Can you guide me how can I do that please?
Speaking as a software engineer in London in that Salary range: yes, absolutely. You'd want to be in the latter half of the range to be completely comfortable, especially if you want to live centrally and on your own. That's probably a pretty privileged perspective though, plenty do just fine on much less.
Even in finance it’s not much beyond that and super hard to actually find the higher paying ones.
If they offer anywhere near the upper range in Manchester that’s a very good salary against cost of living. I was paying 650 a month rent for a new build 3 bed townhouse with garage 20 min train ride away from the city in 2014.
Is there a viable alternative to Scala Worksheets in IDEA? They're currently broken for me in 2019.1.3, and I have no idea how much of a priority the bug is for JetBrains. (Since this problem has existed in some form since [at least 2017](https://stackoverflow.com/questions/46822166/intellij-scala-worksheet-dont-pick-up-code-changes-without-restart), I'm guessing it's not heavily used internally at the company) The specific problem is that the worksheets won't pick up changes you make to imported packages until restarting IDEA.
I think there's a lot of accidental complexity is the problem you're trying to describe, so I'm not actually sure what the root of the problem you're trying to solve. If you had to describe your problem without talking about any code at all in a single sentence what would it be? My understanding of what you want is a abstraction that describes how to update and retrieve a domain object(with identity?), to a (given?) serialized format that is versioned. Updating and retrieving is effectual and want to capture those effects.
Depends on your rent I guess, but I know a lot of people in that range that are doing OK. You'd want to be on the higher end of that though :)
Hey guys! &amp;#x200B; Having recruited in London for Software Engineers using a range of languages, it's absolutely enough to live comfortably, renting on your own in London is expensive, but Shoreditch, where the London role is located, is very commutable from the outskirts. For Manchester, it's a great salary indeed and you can live very comfortably! &amp;#x200B; Scala Engineers with over 3 years of experience regularly receive higher than average salaries, normally at least £65k+ in London. If you have over 5 years of experience you can expect £75k+. But these are rough guidelines, and great engineers who are adept problem solvers will almost always be paid more than what I've stated above. &amp;#x200B; On a side note, Finance companies, pay ridiculously well. But competition is high for work that is unlikely to be interesting, and you can pigeon-hole yourself into that world. If you live to work on greenfield projects that have a social impact and sense of purpose, you're likely to be very unfilled working in finance companies. ( Unless it's Monzo or ImaginationCurve ;) ) &amp;#x200B; Any more questions let me know! If you sign up and apply through Functional works, let me know, I can check on the progress on your application for you.
True - though I had assume they'll expect to pay the higher end of the range only in London. If not, then yes move to Manchester if you can: a) The north is just better ;-) b) £85k is a fortune relative to cost of living, especially if you don't want to live *in* Manchester itself.
Some example which shows the main idea. E.g. I want to get all hotels with a score over 300, and only the names `curl` [`https://www.holidaycheck.de/svc/search-api/search/mall\?tenant\=test\`](https://www.holidaycheck.de/svc/search-api/search/mall\?tenant\=test\) `| sjq -a 'root.destinations.entities.filter(_.rankingScore &gt; 300).map(_.name)'` gives: `[` `"Mallorca",` `"Malles Venosta / Mals",` `"Palma de Mallorca"` `]`
it's a great idea! I always want to use something la jq in my scala code as a dependency, may be you can use/inspire by json lenses: [https://github.com/jrudolph/json-lenses](https://github.com/jrudolph/json-lenses)
Thanks - I'm writing a command line Scala app and this is a good example for me.
I'm guessing it won't be very different from detecting one in Java (here: [https://www.youtube.com/watch?v=MFOAbpfrJ8g](https://www.youtube.com/watch?v=MFOAbpfrJ8g)). Unless there's already some built-in method ;)
How do we write in Scala?
You want to detect a loop without writing a loop? Something's got to loop regardless if it is your code or a library you're calling's code.
Feels like someone's working on an assignment :-)
What are the rules of the assignment? :) That you cannot use a a loop at all or you're just not allowed to use a \`while\` and \`for\`? Can you use recursion? You can loop through a list using recursion with means you don't need a while.
Considering the same account also posted [this](https://www.reddit.com/r/scala/comments/c3pfqi/handling_inputstream_in_scala/?ref=share&amp;ref_source=link) I'm inclined to agree. While I'm not against helping someone out, these questions seem more like generic CS questions than specific to Scala.
Can also hash or store a pointed to each item and find the duplicate. Still N time if the size of the linked list is roughly known.
Thank you!
If you want help with your homework, you should show us what you've tried, or at the very least explain what exactly you don't understand. Right now, it just comes off as if you want someone to do your work for you.
You should also have a look at using ammonite for this. I use it extensively at work for making http requests and parsing the result.
[https://stackoverflow.com/questions/56892519/dynamically-apply-aggregate-function-in-spark-data-frame](https://stackoverflow.com/questions/56892519/dynamically-apply-aggregate-function-in-spark-data-frame)
Yes ammonite is great, I use it as well, but I do not get quick ad hoc case class like access, right?
How does one get started doing contracting work? Your description makes it sound very appealing.
uJson is a great option for quick-and-dirty JSON handling, in my experience. Circe is fantastic for larger projects and my personal choice most of the time, but occasionally I'll have to throw something together on the fly (scripting, a quick tool for a colleague, etc) and uJson is much faster to do that.
Get in touch with a few recruiters, most get introduced that way. I am not in the UK anymore but it seems most of my old friends/colleagues are using Equal Experts or have used them.
I once made a post about galling testing, may be useful https://sysgears.com/articles/restful-service-load-testing-using-gatling-2/
I fixed some bugs for scala-json-rpc. The only reason it hasn't been updated in so long is simply because it works. Something to remember is that use of serialization method is important especially if you're sending lots of varied classes. The reason being that I found Pickle libraries to be far less flexible and harder to deal with than Circe.
It's missing server with cross build shared sources between the the web and server projects.
Bump the version, you see it's important )
Yep, it is currently a template of static web project. There was a template for full stack Scala project: [https://github.com/Algomancer/Full-Stack-Scala-Starter](https://github.com/Algomancer/Full-Stack-Scala-Starter) &amp;#x200B; Unfortunately it is out of date. It would be good if there is a new template with new tools in 2019 like \`scalajs-bundler\` and \`ScalablyTyped\`.
A possibly outlandish suggestion: 1. Write your service using [fs2-grpc](https://github.com/fiadliel/fs2-grpc/tree/v0.5.0-M1). 2. Proxy it with [grpbwebproxy's websockets support](https://github.com/improbable-eng/grpc-web/tree/v0.9.6/go/grpcwebproxy#enabling-websocket-transport). 3. Use [ts-protoc-gen](https://github.com/improbable-eng/ts-protoc-gen), [scala-js-ts-importer](https://github.com/sjrd/scala-js-ts-importer), and [the grpc-web client](https://github.com/improbable-eng/grpc-web/tree/v0.9.6/client/grpc-web) with websockets in the browser. The websockets support is assuming you want to use streaming APIs. If you just want classic RPC, you can forego the websockets. But since you need the `grpcwebproxy` anyway, you might as well use the websockets transport anyway. Oh, and I'd stick to `proto2` and use `required` in my APIs.
Your suggestions sound interesting, thanks!
Makes sense for small libs, I see. Thanks for the tip also.
&gt; No use of frameworks, built from scratch No one would do this. If they developed their own principled way to send and receive messages over HTTP they would partition it into a separate library like akka-http or Play. And if they _didn't_ use a framework and _didn't_ come up with a principled way of receiving and sending http messages on their own, then their solution is probably not worth reading.
Agreed. But maybe OP is open to a smaller HTTP framework like scalatra or spray?
If I were to develop full stack app totally from the scratch I would kill myself: interaction with database itself is something that you could spend several months/years to get right (no matter which language you pick). If you allow libraries instead of frameworks this is doable in a principled way: (e.g. endpoints library + Akka + Doobie), though I have some doubts if this approach would be as readable to a newcomer as just using a framework.
If you or someone you know is contemplating suicide, please do not hesitate to talk to someone. **US:** Call 1-800-273-8255 or text HOME to 741-741 **Non-US:** [https://en.wikipedia.org/wiki/List_of_suicide_crisis_lines](https://en.wikipedia.org/wiki/List_of_suicide_crisis_lines) --- ^^I ^^am ^^a ^^bot. ^^Feedback ^^appreciated.
I'm glad you find them helpful! Honestly, I don't know why I didn't think of BooPickle first, since you described your project as a "toy," The crazy gRPC and grpc-web idea leapt to my mind because it's what I'd do if I were doing what I usually do, namely, developing a service to host in a Kubernetes or OpenShift cluster, where either the Istio or Linkerd 2 service meshes have gRPC routing and load balancing, and I want to use streaming, but I want browser clients to be able to consume those services, too. To that end, I would likely choose Istio, since [Istio already does grpc-web proxying](https://blogs.vmware.com/networkvirtualization/2019/04/grpc-web-and-istio.html/), and [Improbable's client supports server streaming](https://github.com/improbable-eng/grpc-web#client-side-streaming), which covers every use-case I can envision having.
Good bot.
Why include frontend technologies like React and Jest and a cloud provider like AWS? The only interaction the backend should have with the frontend technologies is maybe compiling or serving the assets (even then usually JavaScript is used to compile frontend assets and they get served with a static file server). As for using Scala without frameworks to build an HTTP server, it might be useful but you'll never use anything you learn from it at work. It would be a lot more useful to get to know the language if you don't, specifically the type system and functional programming concepts, and whatever frameworks your new team is using.
It's possible - but unlikely - that OP is referring to ScalaJS, so an application that has not only the backend code but also compiles and provides a full front-end experience in Scala.
I agree, this does seem possible.
Ah you're right. I completely forgot about that project. It still seems odd that they want to learn to build backend servers without frameworks but are asking about specific frontend ones...
The Scala Pet Store has been very helpful and it’s actively being worked on. https://github.com/pauljamescleary/scala-pet-store
Not true at all. Most of the enterprise production Scala code I’ve dealt with was written using http4s which is not a framework.
You start on Monday and you’re asking now? 😂 The pet store project someone else linked is a nice one. No frameworks is a stupid requirement. Although I guess I wouldn’t call HTTP4S, Doobie, etc frameworks so maybe they’re ok.
We're using cornerman/sloth in production. It seems stable and the developer is very responsive.
I think his use of the word framework was not canonical. I would not class akka as a framework. Play yes.
I think some of the difficulty associated with this problem shows up if you put some example implementation into `Abstraction[F[_]]` - for example in this case trait Abstraction[F[_]] { def point[A](a: A): F[A] def extract[A](fa: F[A]): A } What does it mean to have extract universally quantified over `A`, when a vast majority of possible instantiations for `A` are completely impossible given your instantiations of `F[_]` ? the signature doesn't make sense anymore. Similarly what does it mean to universally quantify point over `A`, when a vast majority of `A` are _impossible_ to obtain an `F[A]` over? The function would be impossible to implement. Depending on whether Abstraction has covariant, contravariant, or invariant references to `F`, an `F` that is not universally quantified needs it's entire domain/codomain known in order to successfully generalize in any meaningful way, (I suspect this is why you seem forced to specify the bound concretely to get even a bare definition of `Abstraction` to compile). Also depending on what your use case is, I'd suspect you have alternative approaches available like context bounds (often less prone to generative effects than type bounds) that can address the requirements your subtyping constraints are addressing, but get out of your way in cases you want to leverage more general structures in terms of them. Context bounds (or honestly even just sealed abstract classes) can allow you to prove things about how your values are constructed rather than what they are (a la subtyping, car is a vehicle, usual oo stuff.)
Wasn't satisfied with hand-wavy suggestion of context bounds, here's an example - https://scalafiddle.io/sf/GqxE8B2/0
You can pass both Bar and Baz to Abstraction if you declare it as: trait Abstraction[X[_ &lt;: Foo1 with Foo2]] And you can generalize that to arbitrary types by writing: trait Abstraction[X[_ &lt;: Nothing]] But at this point you can't really do anything useful with `X` anymore. Alternatively you could have something like: trait Abstraction[A, X[_ &lt;: A]] Which allows you to write `Abstraction[Foo1, Bar]` and `Abstraction[Foo2, Baz]` for example.
I don't know what you're trying to achieve, but here's a way to do it under the constraint you stated ("do **not** want an extra type parameter on `Abstraction`"): trait Foo1 trait Foo2 trait WithUB { type UB } trait Bar[X &lt;: Foo1] extends WithUB { type UB = Foo1 } trait Baz[X &lt;: Foo2] extends WithUB { type UB = Foo2 } trait Abstraction[X[A &lt;: X[A]#UB] &lt;: WithUB] An example use, with some implementations: trait Bar[X &lt;: Foo1] extends WithUB { type UB = Foo1; def x: X } trait Baz[X &lt;: Foo2] extends WithUB { type UB = Foo2; def x: X } trait Abstraction[B[X &lt;: B[X]#UB] &lt;: WithUB] { def test[X &lt;: B[X]#UB](b: B[X]): X } val a1 = new Abstraction[Bar] { def test[X &lt;: Foo1](b: Bar[X]): X = b.x } val a2 = new Abstraction[Baz] { def test[X &lt;: Foo2](b: Baz[X]): X = b.x } Fun facts: 1. such type projections will likely stop working in Scala 3 — you're probably better off trying to stick with path-dependent types; 2. this relies on F-bounded quantification, an obscure corner of Scala which AFAIK Martin Odersky considers a mistake given how little use it has, yet how great a pain it is to implement in the type system... the corollary is that it has great potential for abuse! :\^D
Well Spray is the ancestor of akka http, but for some reason OP doesn't want akka. I think we need more details about OP not wanting frameworks or libraries.
Thanks for your reply. Your solution of context bounds does fit my question exactly, especially with the other posts worked example, so thank you for that. Unfortunately it's not the entire picture. In reality this is about type-level functions, so there are no context bounds and there is no run-time. I want to restrict the number of arguments in `Abstraction` to make it easier for the user. If they had to put the argument they cared about in as well as a bound it would be a rubbish user experience. I was hoping that the following: trait Abstraction[T[_ &lt;: _]] could be expressed as something like the following: trait Abstraction[T[_ &lt;: U forSome { type U }]] and then in the body it could look like the following: trait Abstraction[T[_ &lt;: U forSome { type U }]] { def point[A &lt;: U]: T[A] = ??? } This doesn't work for two reasons: `Abstraction[Bar]` still complains that `&lt;: Foo1` is more strict than `&lt;: U forSome U` for some reason I don't understand. And `U` is not accessible inside `def point`, so that line also fails. It really feels like there should be a way around it though - I don't think I'm asking for something impossible to be expressed.
&gt; But at this point you can't really do anything useful with X anymore I'm actually working entirely on a type-level algorithm, so `X` will never see a runtime... So `&lt;: Nothing` might be the precise hack I am looking for. I'll try it with my example this evening - thanks
Unfortunately I don't have control over `Bar` and `Baz` so can't redefine them in this way. It's an interesting post though, I've never heard of F-bounded quantification. I'll definitely be looking it up! Thank you
You should definitely take a look at [Lila](https://github.com/ornicar/lila/) the implementation of the [lichess.org](https://lichess.org/) online chess server. It is the largest free and open source chess server out there and its codebase is almost 70% scala.
&gt; left-pad as an example of NPM dependency. I see what you did there...
By definition an anonymous `U` won't be available inside `Abstraction`. Would trait Abstraction[U, T[_ &lt;: U]] do what you want?
I haven't done this, but my first thought is using rho to generate the server-side API with swagger/HAL documentation and then generate the client code from that? Really generating the client code directly should be the sort of thing rho shines at, but I haven't seen that implemented.
I'm working on [TopShell](https://github.com/ahnfelt/topshell), a purely functional, reactive alternative to the terminal.
Finishing DataStax architect course
Good to know!
As far as I understand (and I might be wrong) Iterates are one of the many ways to implement a version of Streams. This might be interesting https://youtu.be/wFpUG2jGxVg
I think a stream is more abstract than an iterable, as the later i would expect to be associated with a sequence/collection, where a stream of values could come from anywhere and could be infinite. At least that's what i would expect.
Building a suite of Data Science tools just on my own using an incredible tech stack: Scala, Play, ScalaJS, Slinky React. Looking forward to adding ZIO and ZIO-Saga and moving away from Play.
FTR. Functional streams != vanilla Stream(s) &amp;&amp; Iteratees != Iterators
Published version 0.5.0 of the Mill build tool. The major new feature is a zero-install bootstrap-launcher you can commit to your project codebase, and pinning of Mill version to your project to avoid accidental incompatibility with whatever version people happen to have installed - https://github.com/lihaoyi/mill#050
I'm not as familiar with other functional stream implementations, but [Monix Observables](https://monix.io/api/3.0/monix/reactive/Observable.html) have materialize/dematerialize functions that convert between an Iteratee-like representation.
Building a batteries included framework for operations management services . Typed Akka + ZIO is excellent. Front-end stuff is not. Haven't tried Slinky React tho.
Slinky React looks nice. Thanks for the pointer!
It's incredibly well done. Elegant, straightforward and well documented. Even more impressive is that it was done by a university student in between his studies. I actually wish Play would include ScalaJS and something like Slinky and provide a true end to end web framework.
Without calling out specific implementations neither term means anything on it's own. Just speaking in general.
You should ask on their Gitter channel. They are pretty helpful and active there.
I think the memoize method would help here. https://github.com/zio/zio/blob/ad6e44604f4789428b470b289b8e1dfd3017fc2d/core/shared/src/main/scala/zio/ZIO.scala
I suspect that is not supported since the library is supposed to be pure FP and that kind of memoization breaks referential transparency. Indeed, the only way to get a Promise is to go through IO, which always evaluates again, giving you a new Promise. If you find out otherwise I'd love to know too.
Note that the reason to use effects is so that you perform side effects in a pure way, and get referential transparency, local reasoning and so on. The moment you cache the result between calls, that’s all gone, and you end up with just a clunky and overly complex way to do what you could have done by simply calling the effectful code.
Can you try making it a lazy val?
Not the best place here, but I do have other problems with Intellij too. It's quite buggy these days.
Its a good explanation on why it doesn't cache results in the first place. Though, I still think it is better to use memoize (as threeseed suggests) rather than not using IO. It is still pure, since it doesn't do anything yet.
Note that memorize still returns a ZIO with another ZIO inside of it (IO is just a type alias for ZIO with a fixed R), if you run the internal ZIO twice you’ll still get the effect twice. That’s why it’s still pure. Using the effect to cache execution results seems like an architecture issue though, as it does one of two things: 1. either it defeats the purpose by performing an unsafe (non-pure) operation 2. It implements something like memorize (which looks more like it should be called something like defer or delegate) which mixes the concerns making it unclear to someone without a background on a cleaner effect type what’s the purpose of an effect, and also increasing complexity.
I can't really understand the functional streams model. Iteratees made a lot of sense to me once I actually took a couple of days to carefully read the original Haskell papers/code, so they're my go-to approach.
These are both well understood concepts in the FP ecosystem, each with a number of implementations.
I had a feeling after I realized you were the same person who did ternary nats, cool stuff. It's been a while since I have done much type-level scala, I'm still gun shy from an aggressive foray into shapeless that we had to back out because of compile time cost. I think Idris gets by without subtyping, so there's at least an example of type-level programming being possible without it. I know that's not directly useful to your original question, but maybe it means there's something approximate you could achieve in scala that works around subtyping and still gets you in the direction of the functionality you're after. Big maybe.
I think what you are really looking for is a way of starting your task asynchronously and accessing the result later multiple times. Fibers might be the solution you are looking for? val t = Task { println(s"Compute") 12 } val r = unsafeRun(for { fiber &lt;- t.fork tResult = fibre.join tt1 &lt;- tResult tt2 &lt;- tResult } yield { tt1 + tt2 }) println(r) Use ‘tResult’ instead of ‘t’ to get the result. ’t’ will always have the same effect, it’s a Task that prints ”Compute” and returns 12. By using fibers you can start running ‘t’, and using ‘join’ will give you access to the result of the fiber. I haven’t tried running this code, but it should work.
Nope they’re not working. I had the same issue and I that support has been removed...
Which IDE are you using or recommended to use.
&gt; cool stuff. Thanks :) Yeah the shapeless compile time is an issue, it's the entire reason I spent so long developing a faster Nat. Unfortunately my idea works flawlessly as long as `Bar` and `Baz` have free type parameters rather than bounded, so I know my idea is sound. I just sadly have to try to integrate with existing scala code where the provided `Bar`s have type bounds inside. I'll probably publish something on the initial version anyway soon, since I think it's a cool idea on its own.
Thanks. Does the principle of least power apply to FP or OO or both?
Memoize is probably not what he wants to do here, since he making a asynchronous call. His example includes numerous mistakes. The first problem includes side effects in pure constructors for the promise. Also in `p` he creates a promise but he does not return the promise, he returns the result of awaiting on that promise while also fulfilling the promise. If he wants to asynchronously fulfill the promise he should do that elsewhere instead of including that effect with the creation of the promise. That said, the answer below using fibers is the correct one.
Visual Studio Code with metals looks to be an excellent choice. https://scalameta.org/metals/docs/editors/overview.html
&gt; this relies on F-bounded quantification, an obscure corner of Scala which AFAIK Martin Odersky considers a mistake given how little use it has, yet how great a pain it is to implement in the type system... the corollary is that it has great potential for abuse! :^D I'm not too sure about it being a mistake, since Java actually supports F-Bounded quantification supporting it non-negotiable. Maybe including when Odersky worked on the Java compiler was the real mistake.
That works but it stops working if I need \`t\` somewhere completely different in the code base. It recompute it all over again :(
That is the nature of a pure FP code base. You will need to pass the ‘join’ effect down to where it is needed and not rely on global variables.
So \`\`\` def computeResults(): Int = { println(s"Compute") **12** } lazy val results = computeResults() val t = ZIO.succeedLazy(results) &amp;#x200B; val r = unsafeRun(for { tt1 &lt;- t tt2 &lt;- t } yield { tt1 + tt2 }) val r2 = unsafeRun(for { tt1 &lt;- t tt2 &lt;- t } yield { tt1 + tt2 }) println(r + r2) \`\`\` That worked. But what if \`computeResults\` need input wrapped in \`ZIO\` ?
Both have side-effects. I'd say they are barely FP.
What is the correct way to use the same source of data twice then ? I don't want to query my database twice to get the same exact data.
look forward to it.
The same way you'd do that in a non purely functional program: by passing the value where it's needed. You wouldn't want to load a value from a database and store it as a global variable in that case either. How would you test that kind of program? Since this value is something that will never change, it should be loaded at the start of the program and probably put into some 'ReferenceData' case class along with any other non changing data.
Not directly Scala related, though Brian Goetz (Java Language Architect at Oracle) does mention Scala and Akka a few times in the talk, and I thought it was a good message for how to think about how OOP and FP are complimentary rather than conflicting, which is arguably what Scala is all about.
&gt; and I that support ?
I had this exact issue couple days ago with cats-effect. My approach is to use scalacache to cache the effect so that it only gets evaluated once for same parameters. Surely it only breaks the referencial transparency if you actually need the side effects to be executed twice or more, but if you only need the result, which is returned from the mentioned side effect, then I think it still makes sense even with cached effects.
[Switched to pad](https://github.com/Atry/scalajs-all-in-one-template/pull/10) now
OOP vs FP 90% of the time is a red herring. It's imperative and procedural programming(Of which Java is a language supporting all 3 paradigms) that are at odds with Functional Programming. And no one really talks about how does the language work when it functional and imperative at the same time. Imperative and functional programming do conflict in ways that neither Java or Scala(or any other language AFAIK) has yet to rectify.
Goetz actually talks about that in this exact video..
Some useful nuggets in there. The "full toolkit" kind of Functional Streams do seem to be at risk of turning your app inside-out in the same way the J2EE frameworks typically did.
I guess so. I personally look at Streams as it's own paradigm even though this is not how they are usually presented (just another concept from the FP world). I recently made a few videos about them: [here](https://youtu.be/evEqqt62u8Q) and [here](https://youtu.be/wgYVDigEDAM). Maybe you'll find them useful. Cheers.
&gt;Imperative and functional programming do conflict in ways that neither Java or Scala(or any other language AFAIK) has yet to rectify. What about Rust? Admittedly, not as powerful FPL as Scala or Haskell, but better than Java IMHO.
Actually I did talk about that in my Scala Matsuri talk 2 weeks ago. I'm not sure it was recorded, but in case it wasn't, I'll give the same talk at Scala Italy in a few months.
My experience with Scala Worksheets in IntelliJ has been garbage for quite some time. I would strongly recommend avoiding using them. Instead, consider using a REPL or ScalaTest to run code-snippets.
Sup zack
That won't work. Lazy vals delays the initialisation of a value, but with ZIO and other functional effect types (Cats' IO, Monix's Task etc.), the effect **is** the value. Another way to think of effects is that they are descriptions of a computation, not the computation itself. `val x = IO(42)` can be loosely likened with `val x = () =&gt; 42`. Doing `lazy val y = x` achieves nothing other than delay the assignment of the function `x` to a new variable `y`
That may work for this particular example but it doesn't achieve what you want, and as a matter of fact you're working against the main purpose of ZIO: If computeResult() raises an error you'll be stuck with that error inside the ZIO with no way to retry the call. Instead of working against the library, rethink your program flow. When you want to incorporate a call to a DB for instance you flatMap over that specific effect and pass on the result. Here's a silly example: https://scastie.scala-lang.org/nWlCODWJTo2r0llIeGSTew
That's what `Ref` is for. I'd give a concrete example, but I'm on mobile.
I would love to see a more concrete example or a link to a tutorial if you have.
Hello ! Looking at this thread, it feels like you're not used to pure-functional-programming yet. The thing you're currently struggling against is actually the most important feature of ZIO (or IO implementations in general). One thing it's important to mention is that when you use ZIO, \`Tasks\` are "description of a program", and they do not hold any internal state that would let them store results. So running the same task twice will run the same thing twice. If you need to avoid re-running the same result, you need to "memoize" the task. But memoizing is a side-effect itself (the creation of a mutable variable is a side effect), and therefore the result of memoizing is expressed as a task (which sends you back to square one). So, long story short, the only way to re-use a result is to pass it around as a parameter to the things that need its value. Nah, that being said, ZIO provides a mechanism that would let you [reduce the boilerplate involved](https://skillsmatter.com/skillscasts/13247-scala-matters), but I'd encourage you to not use it just yet until you have a better instinct for what pure-functional programs are.
Ah, my bad, thanks for the clarifying answer!
Admittedly, I didn't look at it so well, nor do I know that much about ZIO. So, my bad, luckily some other people are paying more attention than I am!
OCAML
Hi everyone! This question is more about sbt than Scala itself. Suppose you have a project with two dependencies - A and B. Both of them need same dependency C, but different versions of it, which isn't binary and/or backward compatible. Without loss of generality suppose that B's version of C is lower than A. Sbt actualy warns you about it, what I want to do is to fix those warnings and reasons that lead to them. The first thing to come up is that maybe sbt can have multiple versions of C, one for A, second for B, but I can't find anything about this in docs. So, the question is - can one resolve this situation without patching B to use new version of C?
Do you mean OCAML influences the OO part of Scala so that it makes the OO part of Scala a better Java? Does OCAML influence the FP part of Scala as much as Haskell does?
To be precise Haskell heavily influenced _functional Scala libraries_ - Cats / Scalaz - and indirectly everything that build upon them. I doubt that you would see much of a Haskell influence within Lightbend stack. The exceptions I remember about are type class idea (in form of implicits) and for comprehension (poorer do-notation) - but syntax-wise they don't resemble their inspiration in the slightest. Typically, the FP part is said to be influenced mostly by ML system. You know: no statements, everything is an expression, Unit/(), type system and inference in general. Macros letting you work on AST using Scala itself were inspired by Lisp (you wouldn't guess that seeing how they look like... we have high hopes for "principled metaprogramming" in Dotty). From Java Scala borrowed mainly some of the syntaxes (curly brackets `{}`) and module system in form of imports, packages and classes. Some of the design decisions were forced by _JVM_ but not necessarily Java (`@specialize` annotation exists because of primitives, ClassTag and anything reflection related is a consequence of how it works on JVM, type erasure...). Some effort was put to make Scala able to call Java code and that might influenced some solutions, but that's it. I mean, imagine that instead of `{}` you have something else e.g. begin-end or whatever. Then you'll see that Scala code doesn't really resemble Java - how you define methods/values is different, how you define parameters, how much you can define about parameters, if-else being expression, return doing something different (and being discouraged because of what it actually does), etc.
&gt; Typically, the FP part is said to be influenced mostly by ML system. Does learning ML (Ocaml in particular) help learning FP part of Scala more than learning Haskell does? &gt; Scala code doesn't really resemble Java What OO language(s) does OO part of Scala resemble the most?
You probably use the function to SAM conversion from a function introduced in 2.12: `implcit val reader: Reads[T] = o -&gt; ...` The equivalent code compatible with both 2.11 and 2.12 is: ``` implicit val reader: Reads[T] = new Reads[T] { override def reads(o: JsValue) = ... } ```
To answer this, it may help to recall Martin Odersky was one of Nicklaus Wirth's (Pascal, Modula, Modula-2, Oberon) students, and partly as a consequence of that, partly simply as a consequence of being European, his influences tend to come from the [Scandinavian school of object-orientation](https://xamat.github.io/Thesis/html-thesis/node20.html). That school has its roots in the [Simula](https://en.wikipedia.org/wiki/Simula) language, but I would suggest the most direct influence on Scala was a language called [Beta](https://en.wikipedia.org/wiki/BETA_(programming_language)). In particular, "you can define almost anything almost anywhere" comes from Beta. Arguably, so does "objects are modules," although I think there's some more squinting involved there. Actually, let me just quote [Scala: A Scalable Language](https://www.artima.com/scalazine/articles/scalable-language.html): Scala also owes much to other languages. Its uniform object model was pioneered by Smalltalk and taken up subsequently by Ruby. Its idea of universal nesting (almost every construct in Scala can be nested inside any other construct) is also present in Algol, Simula, and, more recently in Beta and gbeta. Its uniform access principle for method invocation and field selection comes from Eiffel. Its approach to functional programming is quite similar in spirit to the ML family of languages, which has SML, OCaml, and F\# as prominent members. Many higher-order functions in Scala’s standard library are also present in ML or Haskell. Scala’s implicit parameters were motivated by Haskell’s type classes; they achieve analogous results in a more classical object- oriented setting. Scala’s actor-based concurrency library was heavily inspired by Erlang.
I don't think it has - OCAML devs as a community don't seem to be afraid of vars, but then again, OCAML is not yet multithreaded, relying upon a [Global Interpreter Lock](http://ocamllabs.io/doc/multicore.html) to keep only one thread executing at a time. As you can imagine, this reduces resource contention in parallel effects quite a bit, reducing necessity of sequencing primitives like Monad and Functor, and reducing the need of parallel primitives like Applicative and Parallel. [Jane Street OCAML](https://ocaml.janestreet.com/ocaml-core/latest/doc/base/Base/Monad/) does have these primitives, and they are a pretty serious OCAML shop, though [the community doesn't seem to care about those abstractions as much](https://www.reddit.com/r/ocaml/comments/759tzw/working_with_monads_in_ocaml/?utm_medium=android_app&amp;utm_source=share). OCAML lacks higher-kinded types, which can be modeled but aren't first-class type citizens. Implicit parameters are entirely missing. OCAML functor modules are extension modules (implicit classes), and not classes. There are some other differences. Pattern matching is almost identical in Scala. Scala's concurrency is truly multicore, necessitating sequencing operations, and parallel operations. var makes mutable data structures possible, and locking necessary for concurrently accessed resources (well, normal ones, lock-free and atomic operations exist), but without a GIL, they can also be hazardous to use for beginners. Implicit parameters make dependency injection a breeze and reduce call-site boilerplate. I think the FP parts are similar, but the features of the languages greatly influence their flavor and syntax, but that's just my opinion.
change to: def objAsSeqReader[T](field: String, converter: String =&gt; JsValue)(implicit format: Reads[T]): Reads[Seq[T]] = new Reads[Seq[T]] { override def reads(o: JsValue): JsResult[Seq[T]] = { case obj: JsObject =&gt; val cats = obj.value.map({ case (id, catJson) =&gt; catJson.asOpt[JsObject].map(_ + (field -&gt; converter(id))).flatMap(_.asOpt[T]) }) JsSuccess(cats.flatten.toList) case _ =&gt; JsError() } } but: Error:(31, 56) missing parameter type for expanded function The argument types of an anonymous function must be fully known. (SLS 8.5) Expected type was: play.api.libs.json.JsResult[Seq[T]] override def reads(o: JsValue): JsResult[Seq[T]] = {
Thanks. Scala is a mixture of a lot of features so can be more difficult to learn. Knowledge of what languages do you think will benefit understanding Scala in a significant way, comparing to studying Scala directly (and possibly not understanding well so writing bad programs)?
&gt; Does learning ML (Ocaml in particular) help learning FP part of Scala more than learning Haskell does? I found a Standard ML background very helpful for learning the FP part of Scala. I'd guess that Haskell experience would also help, but Haskell is notoriously difficult to learn, so I don't think it would be the best strategy if your end goal is to learn Scala. (Indeed I might advise people who want to learn Haskell to start by learning Scala).
The correct transformation is this: ``` override def reads(o: JsValue): JsResult[Seq[T]] = o match { ... } ``` Otherwise how can the compiler know what you're pattern matching on?
Define OOP and FP first... then a reasonable discussion can be had. All other discussions of OOP and/vs FP I've seen end up throughly derailed because people have different ideas of what OOP and FP actually *are*.
&gt; Does learning ML (Ocaml in particular) help learning FP part of Scala more than learning Haskell does? In learning syntax neither helps. In learning FP either helps. &gt; What OO language(s) does OO part of Scala resemble the most? For me all statically typed, garbage collected OO languages looks virtually the same, so I cannot tell. Differences lie in standard libraries and tooling, but you are asking about language.
Thank you very much, you really helped me!
&gt; OCAML Isn't this canonically written as [OCaml](http://ocaml.org/)? Why uppercase everything?
https://purelyfunctional.tv/guide/clojure-jvm/
Thanks ! This was amazing video !
Does it matter? What I was referring to was the class structure from the language: class ['a] stack init = object val mutable v : 'a list = init method pop = match v with | hd :: tl -&gt; v &lt;- tl; Some hd | [] -&gt; None method push hd = v &lt;- hd :: v end ;; Looks similar to class stack(var v:List[A]){ def pop = v match { hd::tail -&gt; v &lt;- tail Some(hd) Nil -&gt; None } } Of course these are also similar to lots of other OOP languages. The ML type system for classes is inferable, as long as it has enough info to go off of, and OCaml's ability to declare mutable or immutable fields is similar to Scala's. Traits are declared as classes, and added using the inherit keyword, and classes are exposed from modules as class types in OCaml. Extension from classes is done via the inherit keyword and constructor parameters are passed at class creation time without any other explicit constructor. Virtual is the abstract keyword. Initializers perform the constructor body duties. I'd say that minus the implicits and functors and unions, the languages are very similar. There is no concept of a companion object, which is another of Scala's OOP niceties.
Probably true, but the writer barely provided anything of substance I think. I would have appreciated more real world examples, not the lame stories they provided as real world examples. Considering we're software developers, we don't need examples that are IRL, we need examples in code, which this article was severely lacking. Rather than provide much for discussion, it was a long rant with nice formatting, quotes and pictures.
That would be a good follow-up question, tbh. Say you are thinking about going either imperative or functional. Then, ask how long the list is on average. If it's 100 or 1000, then you say I'll just go with a simpler code. If it's 1,000,000, then a faster version is needed.
You seem like a much more reasonable interviewer than some of the people I've interviewed with :) But your point about size is noted!
I was right at the start of the OOP evolution. I adored smalltalk and was amazed that it could do everything. Just everything. Very simple and clear language syntax. Pdf displayer, 3D, web-browser, database-stuff. It had clsoures/lamdas some low level management. Major updates were done in a few weeks. It only was relatively slow and missed modularisation on different levels. The enterprise versions (IBM) were really bad. Then C++ happened and I saw it sink every update. They tried to put everything in it, in a way that was hard to combine. Manual memory management does not seem very not compatible with OO design. Never saw a project really finish and polished. Java tried to stay with a simpler version, and it worked until they got lost in the design patterns (and M$ patents). These new languages all forgot to add lamdas/closures. So instead they used many classes, where Smalltalk used closures. It all became too complex. Then Enterprise versions exploded the systems. I looked at "pure functional programming', but this was a disaster to work with. It is like working with a spreadsheet, without a spreadsheet. The most horrible part is that it pushes towards very inefficient solutions. The final efficiency is dependent on the compiler. A for-loop in C is fast, but a non-tail recursion with lists is extremely slow. And the most horrible thing for me is currying in combination with lazy evaluation. Which is like moving the spreadsheet towards the stack. The funny thing is that "functional programming" is already in Smalltalk. That is because Smalltalk also contains some Lisp. So I never saw real new things in functional programming. Scala extends this experience with types. And that made me interested in Scala. The types make it both faster and more complex. The only new thing in functional programming (compared to Smalltalk), is that you can use types in functional system, and expand types to get a wider functionality. It is like having some kind of inheritance around functionality. Which you can also hack into Smalltalk, if you are brave enough. The pure functional programming usually encounters the problem of managing the non-pure IO. So it needs a type-system that manages the state of the sub-system. Again a bit like how objects store state. Usually they are managed as state-machines, which often works better than encapsulation in Objects/classes. The functional solutions encourage us to put state more out of the system. And this is what I really like about it. The object oriented solutions encourage us to manage state as small easy-to-understand subsystems. So in the end, I think there is no real difference. I think we can do both.
Companies that mainly use Scala will let you solve problems using Scala. In my experience they will often give you a choice of languages, such as Java. Also there are web-based pair programming tools used in screening interviews, and most of they support Scala.
The only way I know is to use OSGi. This essentially uses multiple classloaders to allow multiple versions of what would otherwise be identical module/classpath entries. A would load classes from C.y and B would load classes from C.x. It's been long enough since I've used it that I don't recall for sure what would happen if A and B both used types from C in their public interfaces, but I'm pretty sure that would result in yet another conflict for your application, which would then need some wrapper libraries created in such a way that the parts requiring C.x are completely separated from the parts requiring C.y.
Can someone rewrite the article, so it isn't its own disaster?
r/programmingcirclejerk
Most of this comment could be it’s own short blog article. The historical context helps illustrate how it must have felt living through these changes in language.
If an interviewer asks you to implement quicksort, politely decline and ask for an actual god damn question.
I find junior interview kind of annoying. It's not like interviewer always know what to ask and how important is the knowledge. Senior interview questions are more about concepts.
&gt; Also, do you always opt for the functional solution to a question even if the imperative is faster? Context matters: * Ask the interviewer. For example, "This could be solved with recursion or a simple loop. The big-O complexity of both is identical, so does it make a difference to you which approach I use?" * Wrap imperative style code within a preferentially -transparent function. Having a \`var\` within a function is fairly normal and common and doesn't immediately break referential transparency, assuming that var never leaves the scope of that function. I always vocalize "Yes, this is a var, but it does not break referential transparency" and make sure they understand that response, because I've worked with numerous "writing Java in Scala syntax" devs who only learned "var is bad" but never bothered to gain a deep understanding why. * The imperative style is sometimes more readable than the "fancy" FP style. This is especially true if you work with a lot of people who aren't heavily into FP.
As a software engineer, the most important trait your code can have is not performance, but correctness (that's why most developers don't work with C/C++) Functional programming and the added concision of the Scala language massively help in that regard. Trying to to prematurely optimise single functions is wasted time : whether a quicksort written in functional style is a bottleneck is unknown until you run the whole program that it participates in : your implementation might not be the most performant it can be in isolation, but its lack of performance might be completely irrelevant when compared to other parts of your program. Once you have the full program, then you can plug a profiler and identify performance bottlenecks. Until you do that, whichever way is correct and readable is the best way to go, whether functional or imperative.
Good post, it stands out in the sea of “everything has the same value” blank statements that predominate software engineering these days.
Especially these days
&gt; The most horrible part is that it pushes towards very inefficient solutions. Do you consider message passing an efficient solution? Ruby and Smalltalk are notoriously slow, way more slow than Haskell or OCaml.
The goal is that variable names should be readable and easy to pronounce; two developers reading the same codebase should never end up using different names for the same variable. IMO `✔` and `✕` are clear and unambiguous enough that it's fine to use them.
&gt; So, the question is - can one resolve this situation without patching B to use new version of C? In traditional Java 8 with a flat classpath, no. Fundamentally at runtime you will only have one version of `com.ccorp.CClass` on your application's classpath - if there are multiple versions then whichever appears first will be loaded, whether the calling code is in A or B. As /u/zzyzzyxx says, you can use OSGi; a more forward-looking approach might be to use the Java 9 module system (JMPS), which works similarly. But these are notoriously complex solutions; I would definitely treat them as a last resort.
&gt; In your experience, have you been allowed to interview in Scala? Yes - always when applying for Scala jobs, and sometimes even when not. &gt; Also, do you always opt for the functional solution to a question even if the imperative is faster? Yes - correctness and clarity are far more important than performance, the overwhelming majority of the time. If I ever encountered performance requirements (actual business requirements, not just "as fast as possible") that would warrant the use of mutable state then I would use it; so far I haven't.
I think this makes the code more readable for this specific case, it's in a test suite (where more leeway in conventions is usually fine), so I'd say go for it.
In this exact case the downside is that you can't type these on your keyboard. The best compromise I can think of is rendering false and true as emojis a la ligatures, but then you depend on editor support
I there a possibility to sort it by Score or Benchmark name?
To see throughput scores for JSON parsers you can just press a button on the top with corresponding name of the commit hash. For a whole picture for Scala 2.13 please see: https://plokhotnyuk.github.io/jsoniter-scala/ Previous versions for Scala 2.12 can be browsed locally by cloning the repo and hard resetting to some commit in the `gh-pages` branch and opening of the `index.html` or remotely by gathering links to raw JSON reports from the `gh-pages` branch and then switching from the `Summary` to `Compare` mode like here it is done for the last run with Scala 2.12: http://jmh.morethan.io/?sources=https://raw.githubusercontent.com/plokhotnyuk/jsoniter-scala/fed34cf214b08311a86e2c9eeaae286a36f9117a/openjdk8.json,https://raw.githubusercontent.com/plokhotnyuk/jsoniter-scala/fed34cf214b08311a86e2c9eeaae286a36f9117a/openjdk11.json,https://raw.githubusercontent.com/plokhotnyuk/jsoniter-scala/fed34cf214b08311a86e2c9eeaae286a36f9117a/openjdk12.json,https://raw.githubusercontent.com/plokhotnyuk/jsoniter-scala/fed34cf214b08311a86e2c9eeaae286a36f9117a/graalvmce19.json,https://raw.githubusercontent.com/plokhotnyuk/jsoniter-scala/fed34cf214b08311a86e2c9eeaae286a36f9117a/graalvmee19.json
You could also use t and f
1 and 0.
I'm finishing up a round of interviews right now and about to select an offer. I've gone through several coderpad-ish technical screening rounds where I was asked to write code. In nearly every case, I was given the option to choose whatever language I wanted to complete the problems. At this stage companies seem to mainly be interested in learning, "can this person even write code?" more than "is this person an expert at XYZ language." Even in several on-site interviews, for white-boarding, "use anything but pseudocode" seemed to be the norm. I chose scala in every case but I was applying for scala-centric roles and it's my everyday language. 7 years ago in my Ruby prime, I'd have used Ruby. If given the option, use whatever you're more comfortable with. &amp;#x200B; As for functional vs imperative, I usually opt for a functional style in interviews just because that's what I use day to day. In one screen, I was asked to solve the problem in each of functional, imperative, and recursive styles. Unless you're asked, just go with what seems natural for you for that problem in that moment. If you're more comfortable with a functional or recursive style but feel an imperative style would be more efficient, maybe call that out. If they really want a loop, they'll ask you to go ahead and do it that way but they probably don't care. One thing I found myself calling out often during these was non tail-recursive implementations, like, "this obviously isn't tail recursive. In real code I'd probably make sure this was in tail-call position but in the interest of time, I'll just go with this." Not once did I have someone say, "no, I really want to see you do that now." They're usually more interested in you solving the larger problem than getting lost in the weeds on optimizations. But it never hurts to show that you're at least aware of potential optimizations.
Last I checked the Java 9 module system is not versioned so I'm not sure it can be used in this case. Does it have tags or another means to differentiate otherwise identical classpath entries?
 Seq(t, f, f, t, f, f) Seq(f, t, f, t, f, f) Seq(f, f, f, t, t, f) Probably not the best solution
Possibly. Just giving an alternative to smileys.
I'll tolerate some non-alphanumeric, but non-ASCII crosses the line. The exception is if you are writing code for a only-chinese-speaking audience, only-arabic-speaking audience, or only-wingding-speaking audience. Otherwise, stick with the 128 characters that your colleagues will know how to type without googling.
Consider plotting categorical data in bars instead of a line chart, which implies that the X values are samples along some continuous dimension (like Time).
You can easily switch to the bar view by pressing one of the button with JVM name on the top of the page.
To be honest, I wasn't expecting big changes, but it seems there are. Thanks for sharing.
I'm curious why play-json is so slow? (Especially because I use it, as the default option for Play!)
In my experience, if the interview involves coding on a whiteboard, I do not want to work with that company. They don’t even know how to manage time in an interview. If they give you an online coding exercise, that is fine. Interviews should mostly focus on a candidate’s real world problem solving capability (not puzzles — e.g., how would you start to approach this real problem the company has right now?), discussion of past experience, and cultural fit.
You can see the root cause by plotting and analyzing of flame graphs of CPU usage. Scroll down the README of the jsoniter-scala repo to the [Run benchmarks](https://github.com/plokhotnyuk/jsoniter-scala#run-benchmarks) section and pick instructions of how to plot them with help of JFR or Async-Profiler extensions of the sbt-jmh plugin.
What about this then? val x = true val o = false foo should equal(Seq(x, o, o, x, o, o)) bar should equal(Seq(o, x, o, o, x, o)) baz should equal(Seq(o, o, x, o, o, x))
I love this conceptually, but I think what we can see from the comments here is that the presentation may be confusing (although it's really slick!). I wonder: Could you summarize what the takeaway is? (If there is one... what I looked at looked like a bit of a mixed bag, though I'm happy to report that a JDK8-&gt;JDK11 dramatically improved 'sbt compile' performance for us.)
Try it out and see. There isn't really much established best practice on stuff like this. Another possibility is to just have some Seq[Boolean] =&gt; String and print it out when running the test. Do what you feel is right, just be wary of special unicode or emoji or w/e the checkmark is
Great potatoes this looks great! Love the ability to include scalafix rules.
The top end you mentioned translates to the absolute minimum for entry level engineers in Los Angeles. Is demand just that much lower or am I undervaluing differences in income taxes?
Almost all my senior interviews are 90% whiteboard coding. It used to be about concepts, now it is about "code challenges".
playlists: * [keynotes](https://www.youtube.com/playlist?list=PLLMLOC3WM2r460iOm_Hx1lk6NkZb8Pj6A) * [the rest](https://www.youtube.com/playlist?list=PLLMLOC3WM2r5KDwkSRrLJ1_O6kZqlhhFt)
Working on a cryptocurrency portfolio management system for myself using Akka HTTP :P I push a commit now and then when I have time. In the future I will add UI, user and database support. This is my first application project in Scala (used to be a Python developer) [https://github.com/zheli/shijianji](https://github.com/zheli/shijianji)
What’s wrong in posting for-profit books here until they are useful to anyone.
I mean nothing except profit. Like thousands of books built on copy paste from google searches
Didn't use a single var for few years already. Never felt like I need one.
SBT runs inside a JVM. -D args are JVM configurations that are parsed by the JVM on startup. If you fork off a JVM with your SBT command, those args will not get passed to that forked process. -J-D args are parsed by SBT to be passed to those forked processes.
I like it better. o and x are more than good enough to distinguish them visually, the have the same character size (= well formatted code) and they are typeable and talkable. I'd go for it.
What is the reason why forking requires special case? Why can't parameters always be passed no matter whether things are forked or not? Reason I'm asking is that I have wasted hours of my life debugging things because the behavior changes between fork and non-fork
Are -J-D args also parsed by the parent JVM that runs the SBT?
What if you actually wanted JMX on SBT? You would need to pass the parameter to SBT, but not your forked process (otherwise, your forked process would try to listen on the same port and die). And there are a bunch of other system properties that need to be able to be passed to SBT that are used to configure the JVM.
I believe so. A quick test with jconsole appears to verify that.
Anyone have a favorite talk or 3? I'd like to watch a few of them.
Woo, thanks for posting this list. Having them stuck on that klewel site was kind of a pain.
I think some people use the Cats or scalaz Validation libraries ([https://typelevel.org/cats/datatypes/validated.html](https://typelevel.org/cats/datatypes/validated.html)). The "Scala with Cats" book has a nice chapter on this as well ([https://underscore.io/books/scala-with-cats/](https://underscore.io/books/scala-with-cats/) \- under Semigroupal and Applicative -&gt; Validated). &amp;#x200B; I had not seen that validation library before so am excited to check it out more. &amp;#x200B; I also found Wix's "Accord" library after a bit of searching, which seems to be fairly popular and maintained: [https://github.com/wix/accord](https://github.com/wix/accord). I have not used it myself but am curious to give it a try. &amp;#x200B; Additionally there seem to be a handful of decent articles describing people's approaches to rolling this type of thing themselves: &amp;#x200B; [https://www.fyber.com/2018/06/29/introduction-to-validation-on-scala/](https://www.fyber.com/2018/06/29/introduction-to-validation-on-scala/) [http://davegurnell.com/articles/simple-data-validation-in-scala/](http://davegurnell.com/articles/simple-data-validation-in-scala/) (from davegurnell who made the library you linked) [http://fruzenshtein.com/akka-http-another-one-validation-directive/](http://fruzenshtein.com/akka-http-another-one-validation-directive/) [https://dzone.com/articles/akka-http-case-class-validation-field-by-field](https://dzone.com/articles/akka-http-case-class-validation-field-by-field) &amp;#x200B; If you end up trying out Accord, I'd be interested to hear about your experience with it.
I really liked Rob Norris's talk on Functional database access. Actually it teaches a lots of cats effect and tagless final.
Working on a type safe validation library for scala. Basics covered ..need to add support for IO, ZIO etc.. Also need to add support for converting validation errors to HTTP response codes so that the library can be used in web frameworks.. [https://github.com/ayushworks/pareeksha](https://github.com/ayushworks/pareeksha)
The link doesn't seem to be to the right place. Is this a library you made? 😅
😂..thanks.. I might be forgetting my native language Hindi if I keep learning new stuff like Scala
So you program in English even though you speak Hindi? That sounds extremely difficult
I'm learning Scala in my spare time because a local company I admire uses it. I'm coming from a mostly Java background, with some university experience of Haskell. My plan is to learn the language and several of the tools in the company's stack, then use them in a personal project that I can list on my CV when I apply. I've seen some of their roles listing Akka HTTP, http4s, Play, Akka Streams, Spark, Kafka, Kubernetes and Terraform. Any suggestions for learning materials? I'm currently working through [Scala and Functional Programming for Beginners](https://www.udemy.com/course/rock-the-jvm-scala-for-beginners/) on Udemy, although I am considering switching to Oderky's [Scala Coursera specialisation](https://www.coursera.org/specializations/scala) and/or textbook (Programming in Scala 3e).
Well I speak/write English 99% of the times.. I thought it would be cool to name the library in Hindi. But I misspelled the first time.. Anyways it's correct now
Gotcha, I'm glad to check it out!
I am struggling to see "The Trillion Dollar Disaster".
 I used to work for sky uk, as a sw scala engineer and I would NOT reccomend it. They are very good at promising things but they are very bad at honouring the made promises. eg Thet have not honoured the payment of a referral for a colligue they have hired. Promised a salary review in 3 months, never had it. London is plenty of more serious scala companies.
The one thing I miss in Scala is a good validation library. Great initiative!
Also really liked "Scala best practices" by Nicolas Rinaudo
Like most of the world.
Great work! Bahut badhiya!
There are a few out there already .. like [octopus](https://github.com/krzemin/octopus) or [wix-accord](https://github.com/wix/accord) . I would like to extend the same principles and use the library inside a web server to validate all incoming requests for known security vulnerabilities or use it in front of a database access library to pre check the data..
It's also worth mentioning the cats datatype Validated. [https://typelevel.org/cats/datatypes/validated.html](https://typelevel.org/cats/datatypes/validated.html) for worked examples. &amp;#x200B; You can achieve something very similar to this library using: import cats.data._ import cats.implicits._ def validateEmployee(employee: Employee): ValidatedNel[String, Employee] = { def check(test: Employee =&gt; Boolean, msg: String) = Validated.condNel(test(employee), (), msg) ( check(_.name.nonEmpty, "name must not be empty"), check(_.age &gt; 18, "age must be above 18"), check(_.name != "Bob Vance", "He owns Vance Refrigeration and is not an employee") ).tupled.map(_ =&gt; employee) }
From the Spec: &amp;#x200B; \&gt;The associativity of an operator is determined by the operator’s last character. Operators ending in a colon ‘:’ are right-associative. All other operators are left- associative. &amp;#x200B; So, basically, you are applying new member to the existing stream by prepending this new member.
yes and how to I append it?
&gt;Bahut badhiya! What language is that and what does it mean?
&gt;Does learning ML (Ocaml in particular) help learning FP part of Scala more than learning Haskell does? If you want to learn the FP part of Scala, the best language to learn is ... just Scala!
The standard library scaladoc is invaluable for these kinds of questions. [Here's the one from `Stream`](https://www.scala-lang.org/api/2.12.0/scala/collection/immutable/Stream.html). You can see it has a method `append`. For you use case, using `Stream` (or 2.13 `LazyList`) defeats the purpose of its laziness. Using a normal `Seq`/`Vector` with `append` or creating a normal `List` and `.reverse`ing it makes more sense.
thank you :)
That's more of a question for /r/cscareerquestions, but still: Get references. Especially if you work in outsource and have contact with anyone Western – get references from them, if you can make anyone Western vouch for you that'll increase your chances of getting hired dramatically. Some people in Scala open source can help promote you, if you contribute to their OSS and ask them, e.g. John de Goes – although it's not easy to make useful OSS of course. Yes, you'll need sponsorship, but it's actually not that hard to get it for relocation, the key is to understand that you'll be cheaper for them than hiring a local developer – in Europe a local will ask for 50k EUR+, you can ask for 30K or less – use the lowest possible bar allowed for immigration – the company will make a massive profit and will have high incentives to choose you over anyone else; You'll be able to leave them for a new company in a ~year (depending on your contract - which is very important), although you may have to charge a bit below market for awhile until your immigration status improves and you become less dependent on employer.
Here's a sneak peek of /r/cscareerquestions using the [top posts](https://np.reddit.com/r/cscareerquestions/top/?sort=top&amp;t=year) of the year! \#1: [Applied to our main competitor, they told my boss about it](https://np.reddit.com/r/cscareerquestions/comments/9ptj1r/applied_to_our_main_competitor_they_told_my_boss/) \#2: [My CS story contradicts everything I’ve read on this subreddit](https://np.reddit.com/r/cscareerquestions/comments/cbkwp4/my_cs_story_contradicts_everything_ive_read_on/) \#3: [I am absolutely mortified and embarrassed beyond belief and I have zero idea what to do](https://np.reddit.com/r/cscareerquestions/comments/95dgrx/i_am_absolutely_mortified_and_embarrassed_beyond/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
Or just do BigInt(123456).toString.grouped(2).to(LazyList).map(_.toInt)
I'm actively working on a [library](https://github.com/oleg-py/shironeko) to use [slinky](https://github.com/shadaj/slinky) together with cats/cats-effect/fs2 nicely. It's not 100% pure at the edges (I don't feel like rewrapping every html node, and every external component uses impure callbacks anyway), but your app logic still gets to use cats-effect without unsafeRuns everywhere.
Function with multiple parameter lists are called by providing multiple parameter lists :) val list = List(Country("C2", 1), Country("C1", 10)) sort(list)((a, b) =&gt; a.name &lt; b.name)
It is Hindi and it means "very good" :) saw the OP has Hindi as his mother tongue in comments, tried sharing appreciation in his language as we share the same language.
what do I write inside the function?
&gt;what do I write inside the function? Looks like someone is asking other people to do their homework for them \^\^
This has already been done for you - so unless you want to do that for learning purposes, I'd suggest one of the following two options: case class Country(name: String, id: Int) val countries = List(Country("USA", 23), Country("Poland", 56), Country("Korea", 42)) val sortedCountries = countries.sortBy(_.name) ([https://scalafiddle.io/sf/tupZVQM/0](https://scalafiddle.io/sf/tupZVQM/0)) &amp;#x200B; case class Country(name: String, id: Int) object Country { implicit val countryOrdering: Ordering[Country] = Ordering.by(_.name) } val countries = List(Country("USA", 23), Country("Poland", 56), Country("Korea", 42)) val sortedCountries = countries.sorted ([https://scalafiddle.io/sf/LjZBxeb/0](https://scalafiddle.io/sf/LjZBxeb/0))
Looks like someone is trying to learn scala!!!
Capitals (T, F) might be easier. &amp;#x200B; Of course, there are other options, like X and O, or Y and N.
You're literally asking people to implement a sort function for you, which is something that 2 minutes of googling would get you. &amp;#x200B; If you're genuinely trying to learn, at least post what you've tried so far, and what you don't understand. What's your learning process ? Are you following a book, some exercises online ?
What you are asking for is not learning Scala, it's learning how to solve a problem. How would you develop your fundamentals if you keep on asking?
Excellent, best Scala 3 summary M. Odersky has presented to-date. Looking forward to the "new" language, particularly on the frontend with Scala.js
you may want to post your question on the new General Discussion thread, since this one has been expired now.
Watched half of it so far, I'm not fully convinced: - enums vs sealed trait/case classes: what's the benefit? Having both seems confusing. Also the example provided with the Tree[T] enum wasn't clear to me. - union types: what is a real use case for that? The username / password example did not seem realistic - givens and conversions, extension methods: what's the problem with implicits? As he said they are a distinctive feature of Scala, so why getting rid of them? Scala is a special language, why is Martin trying to make it look like any other programming language?
You have just listed equivalent features. The reason to introduce the "simplified" variant, is to make the language more ergonomic. (In my view) the existence of design patterns in a language is a sign it is has design flaws. The language should naturally express the patterns people think in.
 Patterns will always be discovered/introduced, in any language and any paradigm. In fact the enum example brought up in the video looks to me as an attempt to describe a pattern (GADTs as Martin refers to it in the video): are we going to get a new set of keywords on Scala 4 to better express GADTS? I'm not fully convinced this is necessary
it seems to me that this syntax fully expresses GADTs, so no.. but yes -- if you find yourself repeating complex syntax over-and-over, in such a way that the abstraction needs to be explained and cannot be understood by reading alone -- then that's a sign that syntax should be introduced. The cost of keeping a language static is increased complexity at the user-layer. A design pattern is a high cost to pay for the sake of a keyword. It adds a small section to an intro book that is far simpler than the relevant section of the blog post you're going to have to read anyway.
Enums gives you ordinals for the values and possibility to enumerate them. So, it's similar to Java enums. Union types are the duality to intersection types so I think if you have either it makes sense to support both. One interesting use case for union types is in combination with `inline` functions, for example: inline def test(x: Int) &lt;: Int | String = if (x &gt; 0) x else "zero" &amp;#x200B; def main(args: Array\[String\]): Unit = { val a: String = test(0) val b: Int = test(1) val c: Int | String = test(args.length) println(a + ", " + b + ", " + c) } The new design for implicits are basically to make them more ergonomic for common use cases and to avoid pitfalls and problems with current implicits. [In this talk](https://youtu.be/1h8xNBykZqM) Martin motivates the new design.
Two questions! 1. Will the new tasty-reflect macros be hygienic? 2. In terms of Scala 3 and the new ways of using implicit functionality (I believe they are calling it "delegate"), will all existing use cases be supported or are there some simplifications being made so that certain use cases won't work anymore! Thanks from someone who is learning and trying to jump into Scala :)
Union types can also be useful as a variant of nullable types with: `val c: Null | String = doSomethingNasty()` This was the approach Ceylon took, with `?` serving as a shorthand to resolve/narrow it down to just the `String` or an NPE (I could be wrong here tho?)
one question &amp;#x200B; Do i have to learn java before i jump into scala?.
The expressing GADTs with enums is an example of syntax unification. implicit and extensions/typeclasses/constraints/contextual values are examples where meaning can be lost in unification.
What’s wrong with implicit is that the same keyword is used for 3-4 different kinds of functionality, and the line between those different functionalities is rather small differences in syntax (hell, implicit conversions and generic typeclasses are generally only differentiable by the implicit def taking a regular parameter instead of an implicit parameter) This isn’t so bad once you’re used to the concepts around implicit, but when you’re new to the language it’s p daunting. Splitting that functionality up helps people not so familiar with the language deduce what is happening.
In my opinion this change adds little to the language. The motivation is to make the language more friendly to new comers. Arguably new comers wouldn't start from implicits, would they? The word "given" doesn't sound right to me. Call it whatever you want: it will still be passed implicitly. That's the main aspect: how it is provided, not that fact that it is provided. At the end of the day every parameter is given. Some implicitly, others explicitly. To any one having troubles understanding how a given param is passed to a function you would still have to explain how it is given... implicitly. Also if you watch the whole video you will see a pretty bizarre design choice: I don't understand how the word "the" is better than "implicitly". The word "the" tells me absolutely nothing. Lastly the underscore has many more usages than the implicit keyword.https://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala. That's something a new comer would probably find more confusing than the use of the word implicit. Not everything in the presentation was bad, some of the new features are quite interesting, especially the advanced ones, but I don't completely understand certain choices. I hope I'll change my mind, but at this stage I'm not fully convinced.
But that's not idiomatic Scala, is it? That function should return an Option[String]. I can't see the advantage of the "union type" approach
For Scala 2 sure, but in a new world new idiomacy may come with it. I suspect for option and anything monady you'd probably not want a change, but things with tri-state maybe. Or multi-catch is a form of union types.
`the` is a direct consequence of Miles Sabin being one of the core developers. Although it would be odd to leave `implicitly` given that `implicit` keyword is getting dropped
&gt; given I think the word 'given' is intended to be interpreted in a logical context, rather than a 'supply' context. So a function: def foo(i: Int)(implicit a: A): B = ??? reads "foo(5), given A" Implicits can be read as logical restrictions for the compiler. "If A is 'true', allow me to call foo". Or, put another way, "given A define foo"
It's not necessary. However, you should have a general knowledge about programming. After you achieve higher level in Scala it might be useful to gain some Java related experience.
aite.. thanks mate
Hi! Why not getting rid of servlets/container instead?
It's too big application. Container has to stay :/
Never used that particular feature, but http4s supports servlets out of the box : [https://medium.com/@takezoe/http4s-on-the-servlet-container-436dba2f142e](https://medium.com/@takezoe/http4s-on-the-servlet-container-436dba2f142e) Regarding reverse routing, not sure you'll be able to find a library that would fit exactly your usecase. Something like [http://higherkindness.io/mu/idl-generation](http://higherkindness.io/mu/idl-generation) could do the trick, but that particular one is aiming at gRPC rather than http/REST. Maybe [https://github.com/softwaremill/tapir](https://github.com/softwaremill/tapir) could help you not eliminate, but reduce the boilerplate.
underscore is another thing the scala devs want to cut down on. &gt; The motivation is to make the language more friendly to new comers. Arguably new comers wouldn't start from implicits, would they? not right now, thanks to how muddled the syntax for them is. with new syntax that distinguishes the types of implicits (typeclass, implicit conversion, extension methods, etc), then it'll be easier for beginners to approach. Specifically the extension method usage of implicits
Not strictly relevant but we had a legacy application based on Play 1. After a multi-year incremental migration effort we decided to rewrite it as a cloud-native platform based on Akka (so we didn't use Play 2 or Lagom). This has been a way more effective approach in our case.
There are a few rough edges in Scala that only make sense if you understand the JVM (e.g. you need a `ClassTag` to instantiate a new array, you can't define two method overloads if they would have the same erasure). If you're willing to accept those then it's perfectly possible to learn Scala directly without knowing Java first.
OP (Endtest) is spamming up tech subs, every day with multiple accounts [1,](https://www.reddit.com/user/boss_scarbos) [2,](https://www.reddit.com/user/dragnea_presedinte) [3,](https://www.reddit.com/user/llupei) [4](https://www.reddit.com/user/wernerklaus), [5](https://www.reddit.com/user/jos_cu_klaus), [6](https://www.reddit.com/user/sa_vina_werner), [7](https://www.reddit.com/user/ihavelepower), [8](https://www.reddit.com/user/viorica_presedinte), [9](https://www.reddit.com/user/werner_sclavul), [10](https://www.reddit.com/user/basist_infect) ultimately in an attempt to make you pay money for the service he runs (endtest). [This is the kind of person you're dealing with here](https://imgur.com/xyfZ59P) Still want to give endtest money? **Vote and report accordingly.**
Thanks mate.. Noted.
Jesus Christ. A question thats a bit off topic. Other than Selenium, what do you recommend for automation and scraping other than Selenium?
I agree with the other posters. But my question to you: what area of Scala would you like to focus in? FP-side (Cats, Zio), ML-side, ScalaJs-Side, etc. If any of the above, then maybe it’s better with no prior Java knowledge. 🙂
OP (Endtest) is spamming up tech subs, every day with multiple accounts [1,](https://www.reddit.com/user/boss_scarbos) [2,](https://www.reddit.com/user/dragnea_presedinte) [3,](https://www.reddit.com/user/llupei) [4](https://www.reddit.com/user/wernerklaus), [5](https://www.reddit.com/user/jos_cu_klaus), [6](https://www.reddit.com/user/sa_vina_werner), [7](https://www.reddit.com/user/ihavelepower), [8](https://www.reddit.com/user/viorica_presedinte), [9](https://www.reddit.com/user/werner_sclavul), [10](https://www.reddit.com/user/basist_infect), [11](https://www.reddit.com/user/felix_presedinte), [12](https://www.reddit.com/user/werner_la_puscarie) ultimately in an attempt to make you pay money for the service he runs (endtest). [This is the kind of person you're dealing with here](https://imgur.com/xyfZ59P) Still want to give endtest money? **Vote and report accordingly.**
Doobie, all day.
&gt; some of our developers are not very familiar with Scala and they feel overwhelmed Honestly, I think this is all you need to have said. Most libraries in Scala worth using are going to engender this reaction, because most libraries in Scala worth using are going to be farther out on the "from better Java to worse Haskell" curve than people "not very familiar with Scala" are going to be comfortable with. From where I sit, at least, Slick is quite a bit closer to "better Java" than virtually all of the reasonable alternatives. And this is unfortunate, I think, because once you _do_ learn a bit about libraries like Cats and fs2, the unity that underlies libraries such as [http4s](http://http4s.org) and [Doobie](https://tpolecat.github.io/doobie/docs/01-Introduction.html) is simply breathtaking. But the price of entry is admittedly learning some rather abstract concepts up front. i know this doesn't sound like good news, and isn't helpful in and of itself, so let me encourage you to ask any questions you may have, and let's see if we can't tease out some answers that are good _enough_ for you and your team right now. :-)
To add to this - Doobie is both simple and advanced in the sense that it depends on some advanced concepts but usage of the library is actually very straightforward once you figure it out. Essentially it works very similarly to Slick in some ways, except for the part about creating table mappings. In Slick there is no abstraction over using SQL itself, you write your own SQL queries all the time, and you can map the results to whatever types make sense. They are both similar in that you develop a "plan" before executing it - Slick turns DBIOActions into Futures for example, whereas Doobie turns ConnectionIOs into "F". "F" can be any type you want that satisfies certain constraints, but a good default to use is \`cats.effect.IO\` which comes bundled with it as a transitive dependency. You can then call \`unsafeToFuture\` on it and interop with existing Future-using code just like you use Slick. IO is basically like Future except it represents the description of computation, not the final result. So you can re-run an IO whereas a Future is always running and only runs once (the result is memoized). Doobie is a bit of a leap from Slick in terms of the level of underlying complexity (not necessarily upfront complexity), so you need to understand things like what a typeclass is in order to effectively use it (since it's based on cats, a library that uses them heavily). But once you know, it's a breath of fresh air and you might find it to be a little less surprising in production as well.
I have started using quill ([https://github.com/getquill/quill](https://github.com/getquill/quill)) on a small project recently and so far am pretty impressed. It seems pretty polished at this point, and offers a lot of the same benefits of slick with seemingly less boilerplate and fewer rough edges. The downside is it's much newer and (so far) less popular, so it's much harder to find resources like example projects on github, stackoverflow answers, tutorials, etc. &amp;#x200B; I have not pushed it very hard so far, so I can't say yet how it will scale out to a larger project with complex demands. But so far it definitely has me wondering why it is not more popular or widely used.
Exactly this. Scala isn't Java++. (Probably Kotlin is.) Those developers have to understand the philosophy behind it or it would be code full of vars and mutable structures everywhere, etc. Have you ever seen C++ programmers with m\_variables in Java? I've seen a lot. This would be worse than that.
**Scalikejdbc**. Writing your own SQL is still a straight-forward and efficient way to use a database. SQL templating with Scalikejdbc is easy.
Awesome tips! I'm currently learning Scala but if I remember correctly JDBC is blocking and not async in that the thread itself that is actually executing the call is blocked until the database returns, which futures can't really solve totally. Does Doobie do anything to make database operations async?
Both Slick and Doobie do the generally-accepted best practice of having a separate thread pool for your database connections. So every time you run a query it is actually being ran through there first. On Doobie it then shifts back to the calling thread (I believe) whereas in Slick you are expected to map/flatMap on the returned Future and provide an execution context. Doobie does do things differently though. For example, you pass in execution contexts of your own when creating a transactor or connection pool. It also takes separate execution contexts for both connections as well as transactions. Slick will create those for you in the background - I don't really like that kind of magic, I'd prefer to pass things in and feel like the application isn't doing things behind my back. If you'd like to look at general best practices, look here for a good idea of how to organize your thread pools: [https://gist.github.com/djspiewak/46b543800958cf61af6efa8e072bfd5c](https://gist.github.com/djspiewak/46b543800958cf61af6efa8e072bfd5c) The recommended best practice for handling blocking actions - as mentioned - is a dedicated, unbounded cached thread pool. The reason for that is scheduling work on a thread pool is in fact a blocking action itself. So you can defer that by using a non-blocking queue to feed work into the pool (fs2 has a pretty great one) or some other kind of structure, because it then provides backpressure while allowing pending work to still run.
doobie. you will love it.
Also keep Skunk on your radar if you use PostGres.
Thanks! That was a very thorough explanation, I appreciate it!
You can partition the string, like this: ``` scala&gt; "123MB".partition(_.isDigit) res7: (String, String) = (123,MB) ``` And then pattern match on the result: ``` val output = res7 match { case (num, "MB") =&gt; ??? case (num, "KB") =&gt; ??? } ```
I like [Quill](https://getquill.io/). Way simpler than Slick. As far as i see it does not really have a downside compared to Slick. &amp;#x200B; Altough i have to admit i prefer to just write plain SQL
I'd argue that the fact that using null is not idiomatic is part of the reason for using union types for nullability. It's a clunky type to write and read and so it would not be used unless for very specific reasons.
&gt; http4s I see a lot of people recommend http4s on here all the time, but it _is_ still in active development pre first-version with _huge_ refactors between versions and failed experiments left right and centre. I would not class it as entirely production ready and wouldn't mention it in a thread about libraries for scala newbies.
It’s not because of the ListBuffer. The groupBy method returns a Map regardless of the type of a collection . I guess you have to sort a resulted Map based on the original ListBuffer.
You can start from the simple function like this: def parseAndConvertToBytes(s: String): Long = if (s.endsWith("GB")) s.substring(s.length - 2).toLong * 1024 * 1024 * 1024 else if (s.endsWith("MB")) s.substring(s.length - 2).toLong * 1024 * 1024 else if (s.endsWith("KB")) s.substring(s.length - 2).toLong * 1024 else if (s.endsWith("B")) s.substring(s.length).toLong else s.substring(s.length).toLong
&gt; Essentially it works very similarly to Slick in some ways, except for the part about creating table mappings I would say the table mappings and SQL generation are the selling point of slick. It's disingenuous to say it and doobie are very similar. Doobie is high risk in a way that Slick isn't, because of the danger of writing your own SQL and it getting out of date. Obviously Slick is unsafe is ways Doobie isn't, too.
Hi, Thanks for the comment. I tried to implement as suggested shown below val res7=part1.map(_._2) scala&gt; res7.collect() res9: Array[(String, String)] = Array((3,B), (431,MB), (231,KB), (29,MB), (238,B), (32,MB), (21,KB)) &amp;#x200B; But when I try to implement the case scenario I run into the error as follows scala&gt; val byteCnt = res7 match { | case (num, "MB") =&gt; num.toInt * 1024 * 1024 | case (num, "KB") =&gt; num.toInt * 1024 | } &lt;console&gt;:26: error: constructor cannot be instantiated to expected type; found : (T1, T2) required: org.apache.spark.rdd.RDD[(String, String)] case (num, "MB") =&gt; num.toInt * 1024 * 1024 ^ &lt;console&gt;:27: error: constructor cannot be instantiated to expected type; found : (T1, T2) required: org.apache.spark.rdd.RDD[(String, String)] case (num, "KB") =&gt; num.toInt * 1024 &amp;#x200B; I tried using foreach instead but still got another error scala&gt; val new1= res7.foreach{ | case (num, "MB") =&gt; num.toInt * 1024 * 1024 | case (num, "KB") =&gt; num.toInt * 1024 | } Error: ERROR Executor: Exception in task 0.0 in stage 13.0 (TID 13) scala.MatchError: (3,B) (of class scala.Tuple2) at $line58.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(&lt;console&gt;:25) at $line58.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(&lt;console&gt;:25) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) &amp;#x200B; What am I doing wrong here? &amp;#x200B; Thanks
Is it production-ready?
`res7` in your case is an `RDD`, hence you can't pattern match on it. I guess you have to filter out empty strings in the file as your input file contains empty lines. The `foreach` function is partial, it doesn't cover all cases. Try this out: ``` scala&gt; val new1 = res7.foreach { | case (num, "MB") =&gt; num.toInt * 1024 * 1024 | case (num, "KB") =&gt; num.toInt * 1024 | case _ =&gt; -1 | } ```
&amp;#x200B; Okay, I had already omitted the empty lines from the input and run the following. scala&gt; val new1 = res7.foreach{ | case (num, "MB") =&gt; num.toInt * 1024 * 1024 | case (num, "KB") =&gt; num.toInt * 1024 | case _ =&gt; -1 | } &lt;console&gt;:28: warning: a pure expression does nothing in statement position; you may be omitting necessary parentheses case _ =&gt; -1 ^ new1: Unit = () but I am not able to see the output nor write the output to a text file. How do I access the elements in `new1` scala&gt; new1.collect() &lt;console&gt;:26: error: value collect is not a member of Unit new1.collect() scala&gt; new1.saveAsTextFile(outputpath) &lt;console&gt;:26: error: value saveAsTextFile is not a member of Unit Thanks
It can't. Dotty doesn't support macro annotations or anything equivalent that would let you manipulate untyped trees. A possible alternative is to use a separate tool (e.g., scalameta-based) to do code generation.
I haven't found anything in current Dotty documentation but a year ago the plan was described here [https://www.scala-lang.org/blog/2018/04/30/in-a-nutshell.html](https://www.scala-lang.org/blog/2018/04/30/in-a-nutshell.html) (*Future macros* section): &gt;But the scheme also restricts the kind of macros that can be expressed: macros will be [blackbox](https://docs.scala-lang.org/overviews/macros/blackbox-whitebox.html). This means that a macro expansion cannot influence the type of the expanded expression as seen from the typechecker. As long as that constraint is satisfied, we should be able to support both classical def macros and macro annotations. &gt; &gt;For instance, one will be able to define a macro annotation @json that adds a JSON serializer to a type. The difference with respect to today’s [macro paradise](https://docs.scala-lang.org/overviews/macros/paradise.html) [annotation macros](https://docs.scala-lang.org/overviews/macros/annotations.html) (which are currently not part of the official Scala distribution) is that in Scala 3 the generated serializers can be seen only in downstream projects, because the expansion driven by the annotation happens after type checking. I am not sure if anything changed since then, so someone engaged would have to confirm/deny.
Looks like you're learning \`Scala\` &amp; \`Spark\`. Try this out: \`\`\` def countByte(str: String): Int = { str.partition(\_.isDigit) match { case (n, "MB") =&gt; n.toInt \* 1024 \* 1024 case (n, "KB") =&gt; n.toInt \* 1024 case (n, "B") =&gt; n.toInt case \_ =&gt; 0 } } &amp;#x200B; def stats(line: String): Option\[(String, Int)\] = { val parts = line.split(",") val byteCnt = parts .lastOption .map(countByte) .getOrElse(0) parts .headOption .filter(s =&gt; !s.isEmpty) .map(url =&gt; (url, byteCnt)) } &amp;#x200B; val file = sc.textFile("inputfile.txt") &amp;#x200B; val urlStats = file .map(stats) .filter(opt =&gt; opt.isDefined) .map(opt =&gt; opt.get) urlStats.saveAsTextFile("/tmp/output") \`\`\`
Thank you very much for looking into this. Yes, I am new to scala programming and am trying to understand the syntax of doing things in scala. &amp;#x200B; I tried your suggested code but the output is not as expected. The values for KB, MB is not getting converted to Bytes. The output is generated in 2 txt files part-00000 &amp;part-00001 and is as follows part-00000 http://subdom0001.example.com,/endpoint0001,GET,3B http://subdom0002.example.com,/endpoint0002,GET,431MB http://subdom0003.example.com,/endpoint0003,GET,231KB http://subdom0002.example.com,/endpoint0002,GET,29MB part-00001 http://subdom0001.example.com,/endpoint0001,GET,238B http://subdom0002.example.com,/endpoint0002,GET,32MB http://subdom0003.example.com,/endpoint0003,GET,21KB Do I need to add something to the above code or am I missing something? &amp;#x200B; Thanks
You're probably missing something. Are you running in `spark-shell`? Mine looks like this: `cat /tmp/output/part-0000*`: ``` (http://subdom0001.example.com,3) (http://subdom0002.example.com,451936256) (http://subdom0003.example.com,236544) (http://subdom0002.example.com,30408704) (http://subdom0001.example.com,238) (http://subdom0002.example.com,33554432) (http://subdom0003.example.com,21504) ```
Yes I am running spark-shell version 2.4.3 Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0\_212) &amp;#x200B; I tried it again from scratch and it worked! Appreciate your effort in looking into this. Thank you so much for your help and suggestions. Thanks
You’re welcome :)
There's also now a quill query generator for Doobie as well.
\&gt; Have you ever seen C++ programmers with m\_variables in Java I maintain a large java code base with m\_ members variables. Curious what's the issue with this?
&gt;m\_variables It is just one sign of old times (working without good IDE, colouring your member variables and other stuff, like m\_pVar, m\_sVar and even m\_lpsz). Now it just mostly a habit or from C++ coding conventions. It was never in the mainstream Java coding conventions. This is just a sign and the one might expect more C++-ish style and code. I was a C++ programmer myself (until C++ 11) and still remember Hungarian notations and had to work in IDE no much better than a simple text editor, but it is 2019 now.
No, but there might be other and better ways to do this depending on your problem. E.g. if you want to have access to a case classes fields it supports type class derivation
I'm struggling with some things going wrong in my app that I can't seem to fix. Take this code for example. It runs a future which throws an exception and therefore fails. Because of this.. the map does not run. scala&gt; val f :Future[Int] = Future { throw new Exception("Shit!") }.map { _ =&gt; println("it's finished"); 4 } f: scala.concurrent.Future[Int] = Future(&lt;not completed&gt;) scala&gt; f res0: scala.concurrent.Future[Int] = Future(Failure(java.lang.Exception: Shit!)) Since I don't want web requests to silently fail without ever indicating there was a problem I use recoverWith to catch the exception like this. The goal of this code is to return the error to the log the issue and return *something* up the stack the program can progress with: scala&gt; val f :Future[Either[String, Int]] = Future { throw new Exception("Shit!") }.map { _ =&gt; println("success"); Right(4) }.recoverWith { case ex :Exception =&gt; println(s"log the issue: ${ex.getMessage}"); Future.successful(Left(ex.getMessage)) } f: scala.concurrent.Future[Either[String,Int]] = Future(&lt;not completed&gt;) log the issue: Shit! scala&gt; f res1: scala.concurrent.Future[Either[String,Int]] = Future(Success(Left(Shit!))) I've used this pattern extensively in my code but it appears to have an issue in that it's possible to bypass the recoverWith block if the issue is a type of Throwable instead of an Exception. For example this problem does not get logged: scala&gt; val f :Future[Either[String, Int]] = Future { throw new Throwable("Shit!") }.map { _ =&gt; println("success"); Right(4) }.recoverWith { case ex :Exception =&gt; println(s"log the issue: ${ex.getMessage}"); Future.successful(Left(ex.getMessage)) } f: scala.concurrent.Future[Either[String,Int]] = Future(&lt;not completed&gt;) scala&gt; f res2: scala.concurrent.Future[Either[String,Int]] = Future(Failure(java.lang.Throwable: Shit!)) The code above does not log the problem, because it is not of type Exception. I could fix the code by catching a Throwable instead of an Exception like this: val f :Future[Either[String, Int]] = Future { throw new Throwable("Shit!") }.map { _ =&gt; println("success"); Right(4) }.recoverWith { case ex =&gt; println(s"log the issue: ${ex.getMessage}"); Future.successful(Left(ex.getMessage)) } f: scala.concurrent.Future[Either[String,Int]] = Future(&lt;not completed&gt;) log the issue: Shit! but articles on the internet talk about never catching Throwable as it can hide issues the JVM needs to handle. Therefore I think I can only goes as far as logging and rethrowing. Is this the most acceptable error handling I can achieve? scala&gt; val f :Future[Either[String, Int]] = Future { throw new Throwable("Shit!") }.map { _ =&gt; println("success"); Right(4) }.recoverWith { case ex :Exception =&gt; println(s"log the issue: ${ex.getMessage}"); Future.successful(Left(ex.getMessage)); case th :Throwable =&gt; println(s"log the issue and rethrow: $th"); throw th } f: scala.concurrent.Future[Either[String,Int]] = Future(&lt;not completed&gt;) scala&gt; log the issue and rethrow: java.lang.Throwable: Shit! Any thoughts?
Maybe he meant sorting of groups (aka map values). Either way, I think it is easiest to just do the sorting anew after grouping. If the original Listbuffer is supposed to be grouped, it makes no sense to sort it in the first place.
Extension methods
OP (Endtest) is spamming up tech subs, every day with multiple accounts [1,](https://www.reddit.com/user/boss_scarbos) [2,](https://www.reddit.com/user/dragnea_presedinte) [3,](https://www.reddit.com/user/llupei) [4](https://www.reddit.com/user/wernerklaus), [5](https://www.reddit.com/user/jos_cu_klaus), [6](https://www.reddit.com/user/sa_vina_werner), [7](https://www.reddit.com/user/ihavelepower), [8](https://www.reddit.com/user/viorica_presedinte), [9](https://www.reddit.com/user/werner_sclavul), [10](https://www.reddit.com/user/basist_infect), [11](https://www.reddit.com/user/felix_presedinte), [12](https://www.reddit.com/user/werner_la_puscarie) ultimately in an attempt to make you pay money for the service he runs (endtest). [This is the kind of person you're dealing with here](https://imgur.com/xyfZ59P) Still want to give endtest money? **Vote and report accordingly.**
Thank you, removed
I haven't used it in an app yet but I would like to try out jooq ( https://www.jooq.org/).
[Here](https://github.com/plokhotnyuk/jsoniter-scala/blob/9484981eb16932f91cdd0f765eeef90075f4ba4b/jsoniter-scala-macros/src/main/scala/com/github/plokhotnyuk/jsoniter_scala/macros/JsonCodecMaker.scala#L1481-L1488) is a function which groups a Scala sequence without losing of the initial order.
&gt; but it is still in active development pre first-version... I don't know how familiar you are with th ecosystem, but for many years now, there's been a trend of not calling something "1.0," even though it's been in production for years. I don't pretend to know why, but http4s falls into that category. &gt; huge refactors between versions and failed experiments left right and centre. This is baseless hyperbole. http4s has been in production, at scale, at _least_ since the 0.15 versions. What you call "failed experiments" people who have used http4s in production call "improvements based on experience." &gt; I would not class it as entirely production ready... In spite of it being in production at scale, e.g. with Verizon. &gt; ... and wouldn't mention it in a thread about libraries for scala newbies. You've misunderstood my point, which was that the newbies would be well-serviced by becoming better acquainted with the language and libraries based on the Typelevel ecosystem, including Cats, fs2, http4s, and Doobie, at least in part due to the extreme level of conceptual unity. For example, you can use Doobie to query a database, returning an fs2 `Stream` of results, and you can literally say `Ok` from http4s around that `Stream`, and http4s will automatically send that `Stream` to the client using chunked encoding. But admittedly, gaining an understanding of fs2 etc. requires a bit of up-front investment, and that's what I was trying to be clear about: both the value and the necessity of it.
&gt; This is baseless hyperbole. http4s has been in production, at scale, at least since the 0.15 versions. What you call "failed experiments" people who have used http4s in production call "improvements based on experience." &gt; &gt; didnt' they abandon 0.19 entirely because they fucked up too badly on their flagship Blaze client/server? And now they're on 0.20, deprecating their flagship Blaze client or server (can't remember which) and recommend you use a binding through to another third party client/server? That's a big fuckup for something 'production ready'. &gt; which was that the newbies would be well-served by becoming better acquainted with the language and libraries based on the Typelevel ecosystem No, I do agree. It just wasn't what the OP wanted. My response was unclear.
Hijacking this to ask - is it possible in 2.12/2.13/Dotty to create an annotation which causes a pure expression to become a singleton val? eg. def foo(o: Observer): Unit = { o.observe(@Static(generateObservation("foo" + "bar", Baz.Option1))) } would compile as if the code were written as val anon1 = generateObservation("foo" + "bar", Baz.Option1) def foo(o: Observer): Unit = { o.observe(anon1) } I'd appreciate help implementing this if it's possible! My codebase is on 2.12.4.
&gt; I am not sure if anything changed since then, so someone engaged would have to confirm/deny. I just learning Dotty and seen Dotty Macro. but not found macro annotation Document for Dotty. I just wondered for that :)
https://http4s.org/v0.20/client/ They still develop and recommend Blaze.
[removed]
&gt; didn't they abandon 0.19 entirely because they fucked up too badly on their flagship Blaze client/server? And now they're on 0.20, deprecating their flagship Blaze client or server (can't remember which) and recommend you use a binding through to another third party client/server? It's true that they've deprecated the Blaze client. &gt; That's a big fuckup for something 'production ready'. I'm sorry, but it's obvious you're just picking a fight. First of all, they have never deprecated any of the server-side code for any reason other than to say "we've evolved the implementation and you should use the new version," and have been exceedingly clear about when support for older versions ends. Almost no one uses the clients compared to the servers. When they do, they can simply switch from the Blaze client to one of the others with literally no client code changes. That speaks to the insanely _high_ quality of the architecture and implementation, not its _low_ quality. Blaze is an NIO2 framework—insanely hard code to write. Thankfully, http4s can (and does) take advantage of other projects' hard work in that space. &gt; No, I do agree. It just wasn't what the OP wanted. My response was unclear. That's fair enough, but your response still isn't _reasonable_. Given roughly a three month range of study, I'm completely confident I can help anyone currently working in Scala learn enough Cats, fs2, http4, and Doobie to be producing production-quality services. Getting familiar with the _concepts_ is the hurdle. The code itself is ready and waiting, and more than up to the task.
Wow I fit all the requirements, but moving to Japan would be very disruptive
that's called memoization, and it's common for macros in languages to provide it -- seems very likely it will be here
Suggest using pattern matching on the special class **NonFatal** which will not catch those really serious Throwables, q.v. [https://stackoverflow.com/questions/56384762/is-it-safe-to-catch-an-exception-object](https://stackoverflow.com/questions/56384762/is-it-safe-to-catch-an-exception-object) and the last example here: [https://gist.github.com/frgomes/89cc1ef2e816f49c4425](https://gist.github.com/frgomes/89cc1ef2e816f49c4425)
Great work! I've [switched to it](https://github.com/plokhotnyuk/jsoniter-scala/commit/a8827248935d1c13f42c41fa065c60a03ae9cb37) already. Could you, please, add direct links to binaries that can be used for [CI configs](https://github.com/plokhotnyuk/jsoniter-scala/blob/a8827248935d1c13f42c41fa065c60a03ae9cb37/appveyor.yml#L9)?
No, it's interesting and good thing to study.
Srsly. That's going to be a tough sell.
Fold (e.g `.foldLeft`) your list of dataframes, use the first one as the accumulator and the rest of the list.
For the styling part I do: \`\`\` addCommandAlias("style"**,** "; compile:scalafix; test:scalafix; compile:scalafmt; test:scalafmt; scalafmtSbt") addCommandAlias("styleCheck"**,** "; compile:scalafix --check; test:scalafix --check; compile:scalafmtCheck; test:scalafmtCheck; scalafmtSbtCheck") \`\`\`
&gt; Paidy is Japan’s largest cardless online payment service, handling thousands of transactions per day for famous local and international brands. -&gt; website only in japanese... I can imagine how many international brands they have...
Seems like a standard inner join (using the “join” method) would fit the bill here. Is there a reason you can’t go that route here?
Hi, I am facing another issue which I am not able to resolve, it would be great if you could share your suggestions on this. So now I am trying to calculate min, max, mean &amp; variance for each URL in order as shown below Expected output http://subdom0001.example.com,3B,238B,120B,13806B http://subdom0002.example.com,30408704B,451936256B,171966464B,39193191483703296B http://subdom0003.example.com,21504B,236544B,129024B,11560550400B &amp;#x200B; I have the following code for min and max val grouped_min = urlStats.reduceByKey(math.min(_, _)) val grouped_max = urlStats.reduceByKey(math.max(_, _)) &amp;#x200B; For mean val counts = urlStats.map(item =&gt; (item._1, (1, item._2.toDouble)) ) val countSums = counts.reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2) ) val keyMeans = countSums.mapValues(avgCount =&gt; (avgCount._2 / avgCount._1).toInt) &amp;#x200B; The above is working okay but I am having an issue in finding the variance for each Subdomain. Currently, I have tried the following scala&gt; urlStats.collect().foreach(println) (http://subdom0001.example.com,3) (http://subdom0002.example.com,451936256) (http://subdom0003.example.com,236544) (http://subdom0002.example.com,30408704) (http://subdom0001.example.com,238) (http://subdom0002.example.com,33554432) (http://subdom0003.example.com,21504) &amp;#x200B; scala&gt; val group1=urlStats.groupByKey() group1: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[67] at groupByKey at &lt;console&gt;:25 scala&gt; group1.collect().foreach(println) (http://subdom0003.example.com,CompactBuffer(236544, 21504)) (http://subdom0001.example.com,CompactBuffer(3, 238)) (http://subdom0002.example.com,CompactBuffer(451936256, 30408704, 33554432)) &amp;#x200B; I am trying to use the inbuilt variance() function to calculate the variance so I am trying to convert the CompactBuffer into a List scala&gt; val com = group1.map(x =&gt; (x._1, x._2.toList.sortBy(x =&gt; x))) com: org.apache.spark.rdd.RDD[(String, List[Int])] = MapPartitionsRDD[68] at map at &lt;console&gt;:25 scala&gt; com.collect().foreach(println) (http://subdom0003.example.com,List(21504, 236544)) (http://subdom0001.example.com,List(3, 238)) (http://subdom0002.example.com,List(30408704, 33554432, 451936256)) &amp;#x200B; Parallelize the list and calculate the variance as follows scala&gt; val list2=com.map(x=&gt;(x._1,sc.parallelize(x._2))) list2: org.apache.spark.rdd.RDD[(String, org.apache.spark.rdd.RDD[Int])] = MapPartitionsRDD[69] at map at &lt;console&gt;:26 scala&gt; val list3=list2.map(x=&gt;(x._1,x._2.variance())) list3: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[70] at map at &lt;console&gt;:25 &amp;#x200B; But when I try to collect the results or save to output to text, I get java.lang.NullPointerException scala&gt; list3.collect() 19/07/18 12:01:15 ERROR Executor: Exception in task 0.0 in stage 103.0 (TID 144) java.lang.NullPointerException at $line174.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(&lt;console&gt;:26) at $line174.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(&lt;console&gt;:26) at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) at scala.collection.Iterator$$anon$11.next(Iterator.scala:410) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310) at scala.collection.AbstractIterator.to(Iterator.scala:1334) at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302) at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289) at scala.collection.AbstractIterator.toArray(Iterator.scala:1334) at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945) &amp;#x200B; When I try the using the variance() function on another list, it works scala&gt; val try1_1=List(4 ,100 ,20) try1_1: List[Int] = List(4, 100, 20) scala&gt; variance(try1_1) res67: Double = 1763.5555555555557 &amp;#x200B; Any suggestion on what I am doing wrong. Thanks
Spark currently does not support nested RDD which is what you're trying to do here. ``` scala&gt; val list2=com.map(x=&gt;(x._1,sc.parallelize(x._2))) list2: org.apache.spark.rdd.RDD[(String, org.apache.spark.rdd.RDD[Int])] = MapPartitionsRDD[69] at map at &lt;console&gt;:26 ``` You can read more on this answer: https://stackoverflow.com/a/23793399
We've done it with 9 engineers so far since beginning to work with them (some with their families). It's a total lifestyle change, but the relocation, settling in and integration into the team has been made as easy as possible.
You'd be surprised, it depends on your life situation, if you've always wanted to experience living in Japan, this is one of the best opportunities to do so. We've placed 9 engineers in their team and they're all enjoying it there.
You'd be surprised, but a totally fair point about the website, the company was started by two American founders.
That sounds like a pretty huge restriction in scala 3? Autogenerated json serializers can only be used in downstream modules? That's nearly useless.
Engineer here who works at that company. Feel free to ask questions. :)
Is the work culture closer to Western standards (standard 8h work day with occasional paid overtime) or Japanese (Long office hours, firm hierarchical structure)?
That sounds bad for monocle `@Lenses`
I've mostly lost hope for macros in Scala. The scala team's treatment of them shows that they they don't really get macros, nor do they like them. I think it's fine if Scala macros end up being weak and mostly unused. There are very few non-Lisp languages that have good macro systems, and expecting Scala to retrofit a good macro system in after-the-fact is probably asking for too much.
No. I'd give it six months or so.
doesn't seem to be actively developed [https://github.com/tpolecat/skunk/commits/master](https://github.com/tpolecat/skunk/commits/master)
`Future` already avoids catching those `Throwable`s that shouldn't be caught (it uses `NonFatal` internally), so you don't need to worry about that. You just need to accept `Throwable` (or not specify a type at all) in your `recoverWith` block. Note that generally it's inadvisable to rely on side-effects of a `map` operation being run - `map` is expected to be a pure data transformation (compare e.g. calling `map` on `Option`, or on a `Seq` that might be empty).
Use map(). You would need flatMap() if you would do something like List(1,2,3).flatMap(x =&gt; List(x, x+2)).
Flatmap is map + flatten , you don't need flatten in this case.
You see this error because it tries to convert the `Int` in the List to something that can to be flatten. It finds two implicits that permits it to convert, one to `Float` and one to `Long`. Neither is applicable, and it fails to compile. Also, it looks like you're trying to run this in a Jupyter notebook or equivalent ? The errors can sometime be funky in there.
`scala.util.chaining` is MVP tho
What's the salary range? I am asking because Japan is known for low salary for programmers.
It was committed to in _June_ of this year. It's July. Rob has a day job that Skunk is irrelevant to. Is this some sort of weird new performance art where people post complete non-sequiturs?
Nice post! #2, #3, and #5 were accidentally omitted from the 2.13 release notes; I've fixed that now.
Nice list. My favorite is the new `LazyList`.
I think lenses will be trivial to implement given new typeclass derivation scheme, so instead of annotating @Lenses you will write \`class A deriving Lenses\`
Given some thought i think most of the use-cases of macro annotations will be surpassed by typeclass derivation (your generated code from `derive` method can be using new macros that should be able to do most of the things old ones could)
`sizeIs` looks like sizels. I think `isSize` would've been easier to read.
Yes that is pretty much what I am doing haha ! [https://github.com/leobenkel/safety\_plugin/blob/bd4a30b9b981999a32c4e71a782e45d00befcd4e/src/main/scala/com/leobenkel/safetyplugin/SafetyPlugin.scala#L60-L65](https://github.com/leobenkel/safety_plugin/blob/bd4a30b9b981999a32c4e71a782e45d00befcd4e/src/main/scala/com/leobenkel/safetyplugin/SafetyPlugin.scala#L60-L65)
Definitely closer to Western standards. There are some Japanese flavours here and there, mostly due to legal regulations. E.g. costs for commute is mostly covered by the company (which is pretty standard in Japan)
‘If (sizeIs &gt; 100) doIt()’ reads better.
Good point
Definitely closer to Western standards. There are some Japanese flavours here and there, mostly due to legal regulations. E.g. costs for commute is mostly covered by the company (which is pretty standard in Japan)
I got the same error in scala REPL also.
OP (Endtest) is spamming up tech subs, every day with multiple accounts [1,](https://www.reddit.com/user/boss_scarbos) [2,](https://www.reddit.com/user/dragnea_presedinte) [3,](https://www.reddit.com/user/llupei) [4](https://www.reddit.com/user/wernerklaus), [5](https://www.reddit.com/user/jos_cu_klaus), [6](https://www.reddit.com/user/sa_vina_werner), [7](https://www.reddit.com/user/ihavelepower), [8](https://www.reddit.com/user/viorica_presedinte), [9](https://www.reddit.com/user/werner_sclavul), [10](https://www.reddit.com/user/basist_infect), [11](https://www.reddit.com/user/felix_presedinte), [12](https://www.reddit.com/user/werner_la_puscarie), [13](https://www.reddit.com/user/carnati_si_varza) ultimately in an attempt to make you pay money for the service he runs (endtest). [This is the kind of person you're dealing with here](https://imgur.com/xyfZ59P) Still want to give endtest money? **Vote and report accordingly.**
Coursier out of the box support. Now this is a release to be celebrated ! &amp;#x200B; Great work guys!
Or even better: `size_is`! JK
Is there a good example on how to use `WriterT` for logging? My main issue is that running `WriterT[F, Log, A]` will result in `F[(Log, A)]`, meaning that aborting execution with `F.raiseError()` will effectively erase log preceding that.
Could you maybe post an entire buildable example on [Scastie](https://scastie.scala-lang.org/)?
Yes! Good idea.
Most likely the jar packaging is failing. I remember that it was quite tricky to get everything right.
I'm glad you think so. :-) Here's what I did: I copied the source code from your StackOverflow post into a tiny sbt project. Of course I had to add the scala-logging dependency, but no big deal. I'm not sure where `ArgParser` came from, and a search revealed no such `trait` quickly (there is a `class` in caseapp, but not a `trait`, so that can't be it). So I removed that. And of course I added the AWS dependency you mentioned. Once I got it to compile, I use `javap` to disassemble the `.class` file and confirm that the method with the right signature exists and is `public`, which it is. So my working hypothesis is that the issue isn't with your code; it's with your build process for the `.jar` file. So I think it will be helpful to look at the whole thing, and maybe even try it out myself. On that note, if you aren't aware of it already, check out [LocalStack](https://localstack.cloud/) for testing out AWS-based stuff without running up your AWS bill. :-) Also, see [testcontainers-scala](https://github.com/testcontainers/testcontainers-scala) and be aware that [testcontainers-java has a LocalStack container](https://github.com/testcontainers/testcontainers-java/tree/master/modules/localstack), but it's not exposed by testcontainers-scala. It's very easy to write a Scala class for it, based on any of the other examples of wrapping one of the testcontainers-java containers.
Thank you! Cheers man, that's really helpful. I'll post an update when I have something.
[https://docs.aws.amazon.com/lambda/latest/dg/java-handler-using-predefined-interfaces.html](https://docs.aws.amazon.com/lambda/latest/dg/java-handler-using-predefined-interfaces.html) Go through the above. Keep in mind these (regarding your issues): 1. DONT "extends App". Lambda service run the function for you - no main is required. 2. You need to provide the class that includes the handler. e.g example.Main
Hi, I've followed these docs previously. I believe Paul's answer around the packaging of the jar seems to be the issue.
What's the approximate salary range? Do they supply expat tax specialists?
I had no idea about localstack. Looks uber-useful. Thanks!
There's an sbt plugin, which knows how to package it correctly. You should extend the relevant handlers
Very unfortunate, as macros are really useful overall.
Do you need to speak Japanese and/or do they offer any classes/support for learning? Do you get time to learn? As in, maybe a couple hours a week or every other week that you can put towards just learning something new.
I think if you just make your `LambdaHandler` a class instead of an object and then in the lambda configuration, ask it to invoke `io.github.tjheslin1.dmspredictor.LambdaHandler::handleRequest` I would think that would work. Provided it gets packaged correctly.
This! I never spoke with a single person from Europe that lived in a Japan and praised japanese people for accepting foreigners. Social life is a very important thing for me. Therefore, Japan is a no go for me.
Lenses aren't type classes. It won't work with `deriving`&gt;
Not really useless, it's my primary use case for deriving json. I keep my transport classes in a separate module that's also published as a jar.
I've lived in Tokyo for 5 years and and Japanese are very welcoming, if you speak Japanese of course. You can have a rich social life among the expatriate community, but if you want to be "integrated" in the Japanese society of course you'd have to learn Japanese.
It means you won't be able to generate manual serializers, based on automatic ones if I understand it correctly. @Json case class A(foo: Int, bar: String) case class B(a: A, other: Boolean) object B { // ??? uses `a` somehow implicit val jsonSerializer(implicit a: ToJson[A]): ToJson[B] = ??? } I think a very simple use-case like the above will become impossible.
this is awesome release
Thoughts on ZIO and ZIO architecture
Pretty much everything really cool shapeless can do are macros getting around scala bugs they don't bother fixing
Yeah, haven't thought of that. It's quite frightening.
What's his motivation for the keyword?
Design patterns have to exist because the way developers use a language evolves as their understanding and approach to satisfying their requirements evolves with time. This is the core premise of scala: to be a scalable language on which you can build a system structured in a way that best reflects your needs for maintainability. If you want an opinionated language that will restrict you to its featureset rather than provide you with the platform to build off of, then you should go look at kotlin.
``` psnively-psnively@ import $ivy.`co.fs2::fs2-core:1.0.5` import $ivy.$ psnively-psnively@ import cats._, implicits._ import cats._, implicits._ psnively-psnively@ import cats.effect._, cats.effect.implicits._ import cats.effect._, cats.effect.implicits._ psnively-psnively@ import fs2._ import fs2._ psnively-psnively@ import scala.concurrent.ExecutionContext import scala.concurrent.ExecutionContext psnively-psnively@ import scala.concurrent.duration._ import scala.concurrent.duration._ psnively-psnively@ implicit val cs = IO.contextShift(ExecutionContext.global) cs: ContextShift[IO] = cats.effect.internals.IOContextShift@7f030c72 psnively-psnively@ implicit val tio = IO.timer(ExecutionContext.global) tio: Timer[IO] = cats.effect.internals.IOTimer@16e4db59 psnively-psnively@ (Stream.emit("Basic setup...").covary[IO].showLinesStdOut ++ Stream.emit("Rest of the app...").covary[IO].showLinesStdOut ++ Stream.sleep(5 seconds)).concurrently(Stream.awakeEvery[IO](1 second).flatMap(_ =&gt; Stream.emit("foo").covary[IO].showLinesStdOut)) res17: Stream[IO[x], Unit] = Stream(..) psnively-psnively@ res17.compile.drain.unsafeRunSync Basic setup... Rest of the app... foo foo foo foo ``` Some basic building blocks: 1. For using the concurrency features of [fs2](http://fs2.io), you'll need a `ContextShift` and a `Timer` for `IO`. In production, don't use the `global` `ExecutionContext`; please construct one with [Linebacker](https://christopherdavenport.github.io/linebacker/) instead. 2. `showLinesStdOut` is a handy way to dump a `Stream` to the console. 3. `concurrently` runs a `Stream`... er... concurrently... with the first `Stream`. Be aware that the substream terminates when the first one does, which is why I added the `Stream.sleep` to the first one, so the second one would get some time to do anything. 4. `Stream.awakeEvery` is a handy scheduling tool.
You should be able to run your loop as \`\_ &lt;- printRepeatedly.start\`, which will create a "fiber" that will run concurrently. See [https://typelevel.org/cats-effect/datatypes/fiber.html](https://typelevel.org/cats-effect/datatypes/fiber.html)
Hey, I think a recursive approach might work best with cats, but I would suggest to look at fs2. Fs2 is a streaming library. It allows to represent an application as a pipeline with an input, transformation (s), and an output. Your input would be an event every fixed duration. This would allow a function to be called (println as a demo, and read sqs later) for every event. Finally outputs could be ignored. This would work great with http4s as it is built with fs2 (input request, transformations router method ..., output exit code). I don't have a handy snippet, but fs2 has a few very good YouTube videos. Good luck
``` import cats.effect._ import cats.implicits._ val ec = scala.concurrent.ExecutionContext.global implicit val timer = IO.timer(ec) implicit val cs = IO.contextShift(ec) def repeat(io : IO[Unit]) : IO[Nothing] = io *&gt; IO.sleep(1.second) *&gt; repeat(io) val printFoo: IO[Unit] = IO.delay(println(s"foo")) val app = for { _ &lt;- IO.delay(println("Basic Setup...")) fooThread &lt;- repeat(printFoo).start // runs the computation in the background _ &lt;- IO.delay(println("Rest of the app...")) _ &lt;- fooThread.cancel // stops foo when you're done } yield () app.unsafeRunSync() ```
Author of sloth here. For what it's worth, I just made a release today: version 0.1.0 is now available on maven :)
I decided to take a stab at implementing a little streaming "receive messages from SQS" example. My original thought was to take advantage of the fact that the AWS SDK 2.x APIs implement the [Reactive Streams](https://www.reactive-streams.org/) APIs and [fs2 has a Reactive Streams module](https://github.com/functional-streams-for-scala/fs2/tree/v1.0.5/reactive-streams/src). Bizarrely, though, I can find no evidence the SQS APIs take advantage of Reactive Streams. Not a particularly big deal; I just ended up writing a dirt-simple constructor of a `Stream[IO, Message]` around an `SqsAsyncClient`, URL (as a `String`), and the amount of time for the client to wait for messages in seconds (as an `Int`). I also decided to write a reasonably straightforward integration test using localstack-scala and ScalaTest. As things stand, the test to demonstrate that `SQS.messages` does, in fact, return messages fails... because I don't put any messages in the test queue. :-) I've left that as an exercise for the reader (hint: add that to `beforeAll` after creating the queue). The point, rather is: 1. It's pretty easy to wrap even asynchronous, stateful Java APIs with `IO` and fs2. 2. testcontainers-scala rocks. Please let me know if you have any more questions. [Here's my code](https://drive.google.com/open?id=1AXTTX8TQX-Kku9CmcWbartK4RpHjX-yt).
I think this can work recursively. Look at how you can go from the result of one list to the result of another list with something prepended to the first list.
Does this not work? def all[A](xs: Iterable[A]): Iterable[(A, Boolean)] = { val bools = List(true, false) xs.flatMap(x =&gt; bools.map(b =&gt; (x, b))) } scala&gt; def all[A](xs: Iterable[A]): Iterable[(A, Boolean)] = { | val bools = List(true, false) | xs.flatMap(x =&gt; bools.map(b =&gt; (x, b))) | } all: [A](xs: Iterable[A])Iterable[(A, Boolean)] scala&gt; all(Stream('a', 'b')) res0: Iterable[(Char, Boolean)] = Stream((a,true), ?)
`def all[T](names : List[T]) : Stream[List[(T, Boolean)]] = {` `val N = names.length` `val biStream = BigInt(0).until(BigInt(1) &lt;&lt; N).toStream` `biStream map { bi =&gt;` [`names.zip`](https://names.zip)`(0.until(N).map(bi.testBit))` `}` `}`
Thank you , can you give me some example to use it
case class?
But i need to define return type while defining the method, it can't be inside the method
 sealed abstract class RetVal extends Product with Serializable final case class IntVal(value: Int) extends RetVal final case class StringVal(value: String) extends RetVal final case class BooleanVal(value: Boolean) extends RetVal
Thank you
4 spaces creates indentation and monospace. Checkout the [formatting help](https://old.reddit.com/wiki/commenting#wiki_posting) in the comment form.
can you give some example to use this
``` def myFunction: RetVal = { if (x) IntVal(5) else StringVal("Five") } ```
Thank you
Can you tell us the method signature? Maybe we can do better than a sealed trait. I.e. tell us what the inputs are and maybe what the method is supposed to do. E.g. if you wanted a shuffle method: \`def shuffle\[A\](list: List\[A\]): ???\`
I dont have any specific method with signature, i need to show an example with any one of the return type among 3 types
This is one of the basic use cases for the good old foldLeft() function, simply build up from the tail to the head. Example (could do with memory optimiseatoin): def all[A, B](datum: List[A], opts: List[B]): Stream[List[(A,B)]] = { datum.reverse.foldLeft[List[List[(A,B)]]](Nil) { case (res, data) =&gt; opts.map(data -&gt; _).flatMap(pair =&gt; res.map(tail =&gt; pair :: tail)) }.toStream }
Your best bet is a custom sealed trait + case class, or a generic one: ``` sealed trait Either3[+A,+B,+C] case class E3A[T](value: T) extends Either3[T, Nothing, Nothing) case class E3B[T](value: T) extends Either3[Nothing, T, Nothing) case class E3C[T](value: T) extends Either3[Nothing, Nothing, T) ``` Note that this is an excellent use case for union types from Scala 3, there the above could just be `def f: A | B | C`
Thank you,i will try this
I got it, my for comprehension was working, i just used .toStream at the end now :D
Have you considered simply returning an Either\[String,Either\[Int, Boolean\]\], then defining an implicit class of utiltity methods that can run on Either\[A,Either\[B,C\]\]?
Thank you , i will try it
Would be good to type these a bit. If you had something like the below, then you could always call value even when not knowing the type that should be returned. sealed abstract class RetVal[T] { def value: [T] }
If you want to do something interesting with the wrapped value, you need to know its type, usually by pattern-matching on it. Let's say you want to write a function that takes a `RetVal` and prints a different string depending on the type of the wrapped value. How do you do this when `RetVal` is tagged with the type it contains?
Sooo, you have \`Map\[List\[(String, String, String)\], ListBugger\[data\]\]\`?
yes
Block code formatting in reddit is indent-by-4-spaces, not triple backtick. If you highlight the text the entry box will do it for your by clicking the code (&lt;&gt;) button between the quote and bullet point buttons
Block code formatting in reddit is indent-by-4-spaces, not triple backtick. If you highlight the text the entry box will do it for you if you click the code (&lt;&gt;) button between the quote and bullet point buttons
You can always use map and then you will map over a pair of values: ``` grouped.map { case (key, value) =&gt; // example using names for sorting key.sortBy(_._1) -&gt; value } ``` If user decides what you will sort by, then turn user input into if/else or pattern matching to decide which property you will pick up in `.sortBy`.
Thank you,i will try this
Ah, sorry, was simply stating that the abstract would have a typing, I dident meen to suggest leaving out the case classes (thus why I left in sealed and abstract modifiers).
How would you implement def f(r: RetVal): String = r match { case IntVal(i) =&gt; s"Int: $i" case StringVal(s) =&gt; s"String: $s" case BooleanVal(b) =&gt; s"Boolean: $b" }
Exactly like that. I would like to improve the abstract class, not remove the case classes. I have edited my original comment to make this clearer.
Create REST API using scala (Play if you like frameworks, Akka Http if you like weird DSL and http4s if you like functional stuff) and call it from your frontend part (should be fairly straightforward).
actually, depends on the reddit client you're using. the webapp has a markdown mode which handles triple backticks. I realised it wasn't the case on mobile though :s
Really? Huh, news to me
on what sort of drugs are you on?
I would look at [Slinky](https://slinky.dev/) for the UI and [http4s](http://http4s.org) and its websockets support for the service. For a real, scalable, production-quality music player you wouldn't use websockets, but this is a plenty big enough chunk to bite off to start with.
Just use IntelliJ with Scala plugin. I cannot say anything about Maven or Gradle (I use sbt), though quite a lot of people I know used Maven and/or Gradle and had the very same experience I have with sbt and which you got used to with Java.
??? | ??? | Amsterdam, NL | ??? &amp;#x200B; So, for the first time in 3 years my company isn't actively hiring for Scala roles (we're pending on one role that has an offer letter out), but if anyone is looking for roles in Amsterdam I'd be happy to try and help connect you with other companies I know doing work here. I'm not a recruiter and don't get any kickback/fees, but just would be happy to help anyone who's looking for a role here find someone.
I just did an overhaul to a few parts of my [esoteric language interpreter](https://github.com/Dash-Lambda/Eso). I also added the ability to break up large methods to the BrainFuck compiler so it can run exceptionally large programs like lostKingdom.b. That program specifically actually generates nearly 3500 methods and 200,000 lines of code.
&gt; Block code formatting in reddit is indent-by-4-spaces, not triple backtick. That only applies to the old design or to mobile. Tripe backticks render well in the new design and that's my preference as well, hate non-standard formatters.
Good to see you around here Paul, we miss you on Twitter! :)
**Permutive | Backend Scala Engineers | London, England| ONSITE | Full Time** &amp;#x200B; We're hiring Scala Backend Engineers to join our team in London (Old Street). Our stack is \*functional\* Scala (Typelevel, Http4s, Circe, Cats-effect), Haskell, Elm, Docker/k8s, Google Cloud and Kafka. As a company we're really passionate about functional programming and strong type systems! &amp;#x200B; We came out of Y Combinator and raised a $10m Series A last year. Our engineering team is currently 15 people but we're growing it to 40+ by the end of the year. &amp;#x200B; As a company, we believe the edge is going to change the way people build software and we're building products to support engineers with infrastructure to do that! &amp;#x200B; [https://permutive.workable.com/j/6619FD2B88?viewed=true](https://permutive.workable.com/j/6619FD2B88?viewed=true) &amp;#x200B; Please contact me directly on [alice@permutive.com](mailto:alice@permutive.com) or [talent@permutive.com](mailto:talent@permutive.com) for further details!
I did not know this! I definitely prefer it too. But alas I can't stand the redesign. So this is the beginning of the great reddit class wars, old vs new design. What an awful choice to make, to make one version unreadable in the other.
Use `&gt;&gt;` instead of `*&gt;` in recursive functions. High risk of getting a stack overflow there (only saved by the `IO.sleep`).
iofficecorp.com/jobs | Senior Software Engineer | Houston, Texas, USA | ONSITE | Full Time Check out our website for what we do I will get straight to the job description. We have a lot of exciting back-end projects in progress and coming up soon like micro-services to consume IoT sensor data into Azure Data Hubs, Kafka message queues for real time communication and data processing, GeoJSON tile layers and SVG optimization for our floor viewer engine and much more. Typically our microservices are developed in Scala using Play Framework but we do believe in using the best tool for the job so we are open to other technologies and have micro services deployed in NodeJS, Python and more. Our backend is orchestrated using Mesos and Marathon if you have experience with any orchestration tools (like Kubernetes) we'd love to talk. We want quick learners so if you think you can contribute if given some time and instruction don't hesitate to apply. &amp;#x200B; mmoore@iofficecorp.com
TIL ! Thanks for the tip. I'd usually use \`IO.defer\` manually when calling upon unsafe recursive references, but that operator is actually more elegant :)
Wow. A good person spotted
Anyone wanna get a referral to Twitter? https://careers.twitter.com/en.html
Why websockets are not for production?
Here is a mostly working example: [https://github.com/littlenag/scala-pet-store/](https://github.com/littlenag/scala-pet-store/) &amp;#x200B; The goal is to merge this back upstream so that other folks can leverage it. Let me know if you have any questions.
It's more about Reddit screwing it up XD
Audio is _extremely_ latency-and-packet-loss sensitive. You could probably get away with doing audio over websockets for demonstration or learning purposes, but for a real web-based music player, I imagine you're going to want to implement something like [RTSP](https://tools.ietf.org/html/rfc2326).
Got it, thanks!
Thanks man, I'm trying.
Netflix | Data Engineer | San Francisco, USA | Onsite (visa sponsored) | Full Time | Top of the marketHi, I'm a Data Engineer in the Streaming Data Science &amp; Engineering at Netflix and we are looking for data engineers [https://jobs.netflix.com/jobs/864557](https://slack-redir.net/link?url=https%3A%2F%2Fjobs.netflix.com%2Fjobs%2F864557). Feel free to contact me either for more info, apply or just to have a chat because you are curious. Thanks
Thanks a lot for that detailed answer - I will take the code as a skeleton and insert my "real logic" (including http4s routes) into it.
Thank you for that input - I will indeed now use fs2 with cats' IO as Effect type
[https://github.com/cornerman/sloth](https://github.com/cornerman/sloth) This is the best RPC I used to connect the frontend with the backend! Together with [https://github.com/japgolly/scalajs-react](https://github.com/japgolly/scalajs-react) and [https://http4s.org](https://http4s.org/)
Thanks, i had somehow missed Fibers / `.start` on IO.
Yes, `start` / fibers was indeed what I was missing, thank you!
Cool, I'll most definetely check that out when my application's basics are ready!
M1 Finance | Senior Software Engineer and Software Engineer | Chicago, Illinois, USA | ONSITE | Full Time Company: M1 is a young, rapidly growing company revolutionizing personal finance. The current tools used to invest, borrow, or spend your money are woefully undershooting what’s possible -- so we have created a new generation of investment and banking tools that will entirely reinvent how people interact with their money. (More info can be found on our site [www.m1finance.com](https://www.m1finance.com) or our subreddit /r/m1finance ;) ) &amp;#x200B; Job Description: We are a small team of driven engineers committed to executing a clear, exciting vision. We adopt an iterative approach, believe in continuous improvement, and embrace first-principles thinking. Everyone plays an integral part in developing our product and is empowered to make impactful decisions on features, architecture, and implementation. The backend engineering team is in charge of building, deploying, and maintaining mission critical systems that power the operation of a brokerage and a bank. The systems range in responsibility and complexity -- from login and account management to banking and trading. Overall we adopt a microservices architecture and rely heavily on the Lightbend stack (Lagom / Play! / Akka / Scala). Most services follow the CQRS / ES pattern and are decoupled though message brokers like kafka. We currently have 7 backend engineers and are looking at hiring an additional 4. If you are interested feel free to [apply online](http://smrtr.io/YBhQ) or [shoot me an email](mailto:s.gall@m1finance.com)
So I guess I was a bit hasty. :-) On the browser side, it seems like you'd prefer [webrtc-scalajs](https://github.com/coreyauger/scala-js-webrtc), and from there, it seems like the shortest path is... a websocket server on the server side. Frankly, I'm surprised: websockets use TCP, with its well-known scaling and not-necessarily-optimal fallback-and-retry strategies, which is why RTP uses UDP. But hey, if it's good enough, it's good enough.
Does Twitter hire international contractors? (Have a corp registered in Canada)
If you're going to `reverse`, why not `foldRight`?
Just generally curious for my next job probably, what sort of tech is happening in London at Twitter? Is any Scala involved there?
Here's the PR that added it and the discussion https://github.com/lampepfl/dotty/pull/5893 `the` exists in shapeless and is found in some provers (e.g. Idris).
Railroad19 | Cloud Scala / Spark Developers| U.S. Remote\] United States | {PREFER Eastern Standard Time Zone, (REMOTE} | {Full Time} | Competitive Salary **Cloud Scala/ Spark Software Developer (Remote United States)** At Railroad19, we develop customized software solutions and provide software development services. We are currently seeking a Cloud Scala / Spark Software Developer that is fluent in both AWS, Scala &amp; Spark &amp; to be a technical resource for the development of clean and maintainable code and to work on the integration of enterprise data into a cohesive analytical data warehouse. In addition to contributing code and tangible deliverables the role is expected to work as an adviser to help identify, educate, and foster best-in-class solutions. Creating these relationships requires strong communication skills. At Railroad19, you are part of a company that values your work and gives you the tools you need to succeed. We are headquartered in Saratoga Springs, New York, but we are a distributed team of remote developers across the US. **This is a full-time role with vacation, full benefits and 401k.** **Railroad19 provides competitive compensation with excellent benefits and a great corporate culture**. The role is remote - **U.S. located (EST Zone is preferred ),** only full time (**NO**\- contractors, Corp-to-Corp or 1099). **Core responsibilities:** • Understand our client's fast-moving business requirements • Negotiate appropriate solutions with multiple stakeholders • Write and maintain scalable enterprise quality software • Develop new applications and production application support • Participate in detailed technical design, development, implementation and support of Big Data applications using existing and emerging technology platforms. • Work with large streams of data with tools like Spark and Kafka. • Manage the complete software development life cycle • Writing functional and unit tests in order to maintain code quality • Develop understanding of client business processes, objectives, and solution requirements. • Participate in project work groups with subject matter experts and stakeholders to understand data specific needs • Collaborate with other teams in order to deliver a highly performance application that contains little or no defects • Identify new opportunities, tools, and services to enhance the custom software platform • Support and troubleshoot issues (process &amp; system), identify root cause, and proactively recommend sustainable corrective actions **Skills &amp; Experience:** • Advanced Scala development-based software solutions • Strong experience with Spark and Kafka working with large streams of data and experience building data pipelines • Enterprise experience with Apache Hadoop tools • Enterprise experience with NoSQL implementation, including stream processing • Experience with Transactional and BI reporting systems - design, development, tuning, and production support 5 • Experience with database design and modeling - logical and physical, ETL development, performance tuning - table partitioning and indexing, process threading, and database storage sizing and maintenance • Hands on experience with Java 8 (especially streaming collections and functional interfaces) • Hands on experience with NoSQL technologies is a plus • Hands on experience with MySQL, ETL • Hands on experience with AngularJS and/or similar JavaScript frameworks is a plus • Demonstrates willingness to learn new technologies and takes pride in delivering working software • Excellent oral and written communication skills, analytical, and problem-solving skills • Experience participating on an agile team • Is self-directed and can effectively contribute with little supervision • Bachelor's or master's degree in computer science, computer engineering, or other technical discipline; or equivalent work experience &amp;#x200B; Interested qualified candidates please sendresume and cover letter to [Speranza@railroad19.com](mailto:Speranza@railroad19.com) or apply on line at [www.railroad19.com/careers](https://www.railroad19.com/careers)
**AWS (Amazon Web Services) | Software Development Engineer (all levels!) | Seattle, WA, USA | ONSITE | Full Time** &amp;#x200B; AWS is building services for the Internet of Things (IoT). AWS IoT launched with a missing to easily and securely connect billions of devices to the cloud. We are the hub that enables the next wave of innovation in technology for smart homes, wearables, and industrial automation. &amp;#x200B; The AWS IoT Core Rules Engine (made with Scala) handles processing and acting on data generated by connected devices at global scale. Rules Engine evaluates inbound messages, transforms and enhances the payload data, filters those messages, and publishes to other devices or services based on customer defined business rules. We are seeking Software Developers of all levels who are passionate about delighting customers, who are proud of their work, and who want to be on the front-line of growing Amazon’s IoT business. Apply here: [https://www.amazon.jobs/en/jobs/808788/software-development-engineer-aws-iot-core](https://www.amazon.jobs/en/jobs/808788/software-development-engineer-aws-iot-core) [https://www.amazon.jobs/en/jobs/874559/software-development-engineer-aws-iot-core](https://www.amazon.jobs/en/jobs/874559/software-development-engineer-aws-iot-core) [https://www.amazon.jobs/en/jobs/686690/software-development-engineer-aws-iot-core](https://www.amazon.jobs/en/jobs/686690/software-development-engineer-aws-iot-core)
foldRight is quite a lot slower than foldLeft. Plus this way, as I build the output list via appending, it is built in the correct order. To do this with a foldright I would have to use an append instead of a prepend. On a List in scala, a prepend in O\[1\], and an append on O\[N\], as the entire singularly linked list has to be rebuilt for an append.
It makes sense when I see it used, but I think it's going to have a huge searchability issue.
Hi! I'd recommend this [amazing tutorial](https://medium.com/@jon.froiland/data-analysis-with-scala-and-spark-part-1-cda0ef2ac2cc) on Medium. It's composed of 7 parts and gave me much when I started with Scala.
Thanks.
I like [Programming Scala](http://shop.oreilly.com/product/0636920033073.do)
Give [Essential Scala](https://underscore.io/books/essential-scala/) a try. It should get you going with Scala in less than 400 pages.
It looks good, but I wish the API would allow Recorders to return something else than \`Unit\` (Either, Try, or whatever type). OP, would you consider accepting a PR adding that change ? (ie, Recoder would become Recoder\[A\] and the macros would return A, calling upon the Listener to create the value of type A )
I'm a Scala newbie and I have got a quick code question, if anyone would be so kind as to help me: I have this bit of code: \` val parsedLines : List\[Map\[String, String\]\] = lines.collect(x =&gt; x match { case parseExpression(date, time, name, msg) =&gt; Map( "date" -&gt; date, "time" -&gt; time, "name" -&gt; name, "msg" -&gt; msg) }) \` &amp;#x200B; I use it to parse some messages that always have the same format (date, time, name, msg). I couldn't find a cleaner way to do it. I am getting some messages that say "Implicit conversion found: "date" =&gt; ArrowAssoc("date"): ArrowAssoc\[String\] I really would like to learn how to write clean Scala code, so any help would be greatly appreciated! Thanks.
Here is a simple example from Scala REPL: scala&gt; :paste // Entering paste mode (ctrl-D to finish) def groupByOrdered[A, K](xs: collection.Seq[A])(f: A =&gt; K): collection.Seq[(K, collection.Seq[A])] = { val m = collection.mutable.LinkedHashMap.empty[K, collection.Seq[A]].withDefault(_ =&gt; new collection.mutable.ArrayBuffer[A]) xs.foreach { x =&gt; val k = f(x) m(k) = m(k) :+ x } m.toSeq } // Exiting paste mode, now interpreting. groupByOrdered: [A, K](xs: Seq[A])(f: A =&gt; K)Seq[(K, Seq[A])] scala&gt; groupByOrdered(List(1,2,3,6,7,8))(x =&gt; x % 2) res0: Seq[(Int, Seq[Int])] = ArrayBuffer((1,ArrayBuffer(1, 3, 7)), (0,ArrayBuffer(2, 6, 8))) scala&gt; groupByOrdered(List(8,7,6,3,2,1))(x =&gt; x % 2) res1: Seq[(Int, Seq[Int])] = ArrayBuffer((0,ArrayBuffer(8, 6, 2)), (1,ArrayBuffer(7, 3, 1)))
For pure Scala I like Martin Odersky's book [Programming in Scala](https://www.lirmm.fr/~ducour/Doc-objets/scalabook.pdf) and Manning's [Functional Programming in Scala](https://www.manning.com/books/functional-programming-in-scala).
I've worked through programming in scala and got a lot out of it, but I've just picked up functional programming in scala aka the red book and it's great, instead of being a better java it's teaching me a completely different programming paradigm
I don't think we can help you without posting a little more code, but the message implies that parseExpression (lowercase class name?) might have an unapply method with a definition like this def unapply(str :String) :Option[(ArrowAssoc[Date], String, String, String)] instead of just 4 string types? idk... just a guess. Is lines a list of strings?
Thanks. Yeah the println is only there to help visualise code flow.
Thank you
Thank you
Real-time chat backend or online game server. If you don't have to be \_really\_ fast actors are too low level and will feel unnecessarily hard compared to e.g. streams, so pick something where you would feel (or at least imagine) that it really matters that you picked them (as opposed to e.g. normal CRUD app that could be handled with Akka Stream/Akka HTTP or FS2/HTTP2s).
Sorry, I totally missed pointing out that parseExpression is a regular expression with 4 groups! I'll edit the original post.
I'm also getting these implicit conversions in another case: val splitPattern = """\W""" val vocabularyByUser = parsedMessages .groupBy(_("name")) .mapValues(x =&gt; x.map( _("msg") .toLowerCase .split(splitPattern) .map(_.trim))) Here I group my messages (which are maps with keys (date, time, name, msg) ) by the user names, and then I just lower case each message, split it, and trim it. However, when I add the trim step, I get "Implicit conversion found: \_("msg").toLowerCase.split(splitPattern) =&gt; refArrayOps(\_("msg").toLowerCase.split(splitPattern)) : scala.collection.mutable.ArrayOps.ofRef\[String\]" &amp;#x200B; I'm sorry if this is a really specific error: I guess what I am having trouble figuring out is if this warning is something I should be mindful about, and try to wrtie my code in a way that it doesn't show up, or if it's fine. &amp;#x200B; Thanks a lot!
Make a neural network where a single neuron is represented by an actor. Use a genetic algorithm to set up the network topology and neuron activation thresholds. Figure out how to distribute the work across multiple machines. Akka should make this relatively straightforward. This kind of neural network is probably not usable in practice (very hard to train properly/efficiently). This is something wanted to do ages ago but never found the time to do it, would be interesting to see it something like that runs at all.