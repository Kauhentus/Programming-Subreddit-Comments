&gt; The XML schema is too rigid. Any setting that isn't defined by the schema has to go inside a plugin configuration section, even if multiple plugins need it. What kind of thing does this cause problems for? If you have a repeated value you can pull it out into a property. But mostly each plugin has a single responsibility and it doesn't make sense for there to be any commonality between them. &gt; The XML schema is too verbose. Dependency group/artifact/version identifiers are written as elements, not attributes. Agreed, but I think at this point keeping the single consistent schema is better than changing it. Autocomplete and tools deal with the verbosity well enough. It's certainly not ideal though.
&gt; For better or worse, scalaz is part of our common ecosystem. Let's find a way to cohabitate if we cannot collaborate. I don't think that's a sustainable way forward for Scala. The concepts in ScalaZ/Cats are too important and widespread to not have a single implementation. The community needs to reach a consensus and either move to cats, or not.
how about android development?
Linus does insult, but he doesn't do it personally. In some cases he does insult people specifically but its only the specific people that he put a lot of trust in (i.e. mainly Kernel maintainers, who have a huge amount of power and Linus puts a huge amount of trust in as they accept the patches which then get merged into a kernal that is used in millions/billions of devices).
&gt; He wasn't targeted, he just upset too many people once too often, particularly on irc and twitter I believe. He was part of the reason why people believe Scala is a toxic, unpleasant community, something many others have had to work very hard to overcome. Basically this. I wasn't personally involved in the scalaz/cats debacle but Tony Morris in social channels (primarily #scala/#scalaz in irc) was very abrasive, especially if you didn't agree with his thoughts about Purely Functional Programming. Particularly in those channels, a lot his actions were borderline trolling due to his very strong views about programming. I am just saying this to put some context into this. He wasn't randomly targeted, and the reason why he was targeted had little to do with him not being strategic. He was creating quite a negative impact in the social programming scene because of his behaviour (and unfortunately humans are animals, not robots, so behaviour does matter). People at the time (who were also proponents of Purely Functional Programming) saw that his behaviour was having creating a negative image of both the scalaz project and Purely Functional Programming in general for Scala, which was the catalyst of this entire incident
No. Those are a brand new language feature I haven't even explored yet and I'm not sure what amount of work I'd need to do to bridge the feature with Vert.x.
this is what you want: http://docs.scala-lang.org/overviews/parallel-collections/overview.html
One thing I have discovered that is a downside to SBT: You cannot really compile groovy with SBT: https://github.com/eed3si9n/sbt-groovy-test Went through this exercise with eed3si9n trying to figure out how to do it, and he graciously wrote a compiler plugin that reaches into the guts of SBT. So unless SBT supports it internally, you don't get to take advantage of groovy at all. :( I know that's not a deal breaker, but I would've liked to use it.
Gigahorse uses AHC 1.9.x and is built specifically for Scala 2.10 / JDK 1.6, so it's compatible with everything in SBT.
Thanks for the response. I couldn't really understand fully what Future.fromTry does and I have implemented below method to handle the blocking code. def blockingIO[A](f: =&gt; Try[A]): Future[A] = Future { blocking { f match { case Success(x) =&gt; x case Failure(ex) =&gt; throw new Exception(ex) } } } Would this method make sense?
In terms of akka streams which Play uses, or in terms of HTTP streaming?
For what it's worth, I think I'm the person you're referring to, and I'd like to correct the record here: some of the animosity directed at me in the past was based on a misunderstanding of my role in certain historical events, and we worked out the misunderstanding a couple of months ago. Tony and I agreed that he'd think I'm an asshole, I'd think he's an asshole, and we'd leave it at that. I reserve the right to criticize things he does, and I assume he does the same for me, but I think my time in the role of arch-enemy is in the past.
Thanks for the explanation. Quite useful! :)
Lots of FUD here. Some in the community are moving towards Cats, and some are doubling down on Scalaz. Monocle is going to find some issues in Cats as originally Cats was hesitant to add some of the 'esoteric' stuff that Scalaz embraced, so we will see how that plays out. Since you spend time in typelevel channels I'm sure you see people who support typelevel, but there are other places outside of that bubble. 
&gt; Also, and probably shouldn't add this, I find something very funny about this FAQ and Tony's attitude in general. If he's really as smart and he declares and Scala is so useless as he estates, why did it take him so long to realise? Like, how many years? Under that prism, and knowing Tony IS smart (he's proven that), the only conclusion I can make is that he's throwing a tantrum like a child with this kind of posts. And, yeah, not interested. Is this comment part toxic?
I find it more likely than not that his approach is very effective, absolutely and relatively compared to other alternatives, and I also believe that Linus continues with that approach as a conscious choice. I do not find it that controversial, to be honest, in the same sense that if a highly paid developer works on software where people will die if it is not done properly, and that developer does a horribly sloppy job, that the developer is chewed out a lot by his or her boss or manager and possibly fired. I do admit that this example is a much more extreme situation than is present here. EDIT: Of course, if the library scalaz was to be used in a project where life and death is at stake, the quality of code in scalaz would begin to matter a whole lot. And that is not just theory, since I know personally about software written in Scala where correctness and the like DOES affect whether people live or die.
I'll never contribute to a typelevel project after the way I was treated when the split happened and shortly after when there was a concerted effort to disparage anyone who was involved with Scalaz. As I said elsewhere, some of the Typelevel people are great but it's now more clear than ever that the whole "welcoming community" (with their CoC) was mostly just marketing as some of their committers are particular unwelcoming. Some of the people responsible for starting Cats did so for business reasons, not for altruistic ones. This leaves a bad taste in my mouth. I know many committers however are doing it for good reasons. Never the less, I also don't think it was fair to the downstream library authors to be put in this situation, a fork didn't have to happen. Many of us wanted to work together to change things. Anyway, there are talented people working hard on Scalaz who are also trying to bring positivity to the wider Scala community. These people are helping newcomers, giving talks, answering questions and more. Maybe that's not enough to overcome some or the damage, but speaking for myself, I at least have to try. There are good things in the pipeline for Scalaz and like it or not many many people depend on the library who don't even know about all this stuff. I don't want to leave those folks hanging. I think we will hopefully see more changes technically and otherwise. 
With Play &amp; Guice, you can configure your application to be split up into multiple modules that can be enabled/disabled in your config file.
&gt;But mostly each plugin has a single responsibility and it doesn't make sense for there to be any commonality between them. Nowhere is this more false than in the [maven-toolchains-plugin](https://maven.apache.org/plugins/maven-toolchains-plugin/), whose *entire purpose* is to discover, supply, and validate common configuration information for other plugins. That information belongs in `&lt;prerequisites&gt;` or `&lt;dependencies&gt;`, not plugin configuration. &gt;Agreed, but I think at this point keeping the single consistent schema is better than changing it. Then [prepare](https://cwiki.apache.org/confluence/display/MAVEN/POM+Model+Version+5.0.0) to be [disappointed](https://issues.apache.org/jira/browse/MNG-3397).
It has been updated.
I am not saying he wasn't targeted, I am just stating the reasons why. He wasn't randomly targeted, and it also wasn't because he wasn't "strategic" in how Scalaz worked (honestly a lot of programmers aren't strategic and usually don't care for this kind of stuff) Tony Morris was targeted (either directly or indirectly, how much is up to debate), but there are very concrete reasons why, and the point I was making is that a lot of these reasons came down to his behavior. It otherwise doesn't make any sense what happened
&gt; Lots of FUD here. Some in the community are moving towards Cats, and some are doubling down on Scalaz. Monocle is going to find some issues in Cats as originally Cats was hesitant to add some of the 'esoteric' stuff that Scalaz embraced, so we will see how that plays out. I am seeing a fair few projects either supporting Cats along with Scalaz or planning to move directly (Monocle and Eff are examples of projects which are planning to drop support for Scalaz at one point). Even the projects which are planning to support both is already a testament to the point I am making because it takes a huge amount of effort to supporting slightly different FP libraries. Obviously Scalaz has a lot of traction so its simply not possible for every project to move to cats, and in some cases its not worth it because of the economic cost. However at least for the projects where its feasible, that is the general trend I am seeing. The only real exception I have noticed is scalajs-react, and maybe some other projects which have hard dependencies on some of the data structures in Scalaz (albet this is being solved with dogs). In any case I am not trying to spread FUD, I am literally looking at this in regards to the projects I deal with and which ones are either moving to cats or are supporting cats right from the start with no intention to support Scalaz (i.e. Monix)
Some projects are sticking with Scalaz because of technical reasons and there are green field projects that are choosing to use Scalaz. You can try all you want to engage in some pro-Cats anti-Scalaz campaign but it doesn't change the fact that Cats made some bad technical decisions with no plans to address them and that it's an alpha level library. 
&gt; Some projects are sticking with Scalaz because of technical reasons and there are green field projects that are choosing to use Scalaz. Examples. The only one I can see is the 1.x release of scalajs-react &gt; You can try all you want to engage in some pro-Cats anti-Scalaz campaign but it doesn't change the fact that Cats made some bad technical decisions with no plans to address them and that it's an alpha level library. Again examples. Also I think that conversely at this point I can claim you are being anti Cats because of your involvement in Scalaz. I mean in my opinion Scalaz made a lot of bad technical decisions, and the reason why I personally stopped using Scalaz was for technical reasons, of which I have expressed numerous times. At this point its a bit of the pot calling the kettle black
&gt; Hmmm. Methinks you don't fully grock gradle. The project name does not have to be configured in a separate file, why do you believe this? At one point, and as far as I can tell this is still [the case](https://docs.gradle.org/current/dsl/org.gradle.api.initialization.Settings.html#gsc.tab=0), the project name was determined by the name of the directory the project was in or you had to set it in a file called `settings.gradle` you couldn't set this in the main `build.gradle`. This doesn't seem like that big of a problem to me (although it is as strange design choice) but it does seem to really bother some people. &gt; I don't really understand your "mutable DSL" assertion either. You change the SBT configuration by mutating one or more configuration files as well, no? I assume this is a reference to Gradle builds being described in terms of mutable data structures and treating the filesystem as mutable as well (as opposed to SBT where everything is treated as immutable). So, this means that you can have a build task in Gradle that that changes the configuration. As a result, you can have subsequent tasks that will run differently depending on if it is run before or after the task that changes the configuration. If you screwed up your declared task dependencies than this could happen differently each time you run the build. This is easy to do and can be difficult to figure out the cause of this problem. SBT on the other hand has an immutable configuration. So once the build is running it can't change under you. If you depend on a configuration that needs to be determined at run time you will automatically depend on the build task that generates it. 
Most of the build slowness for SBT, I've seen, has been doing to artifact resolution slowness (because Ivy is slow). The actual build part has been at least as fast as any other build tool (usually faster if run from the SBT console). Does this match your experience? If so, have you tried Coursier?
I am not sure why you are being so defensive, I am only stating things as I see them (and I am not intentionally convincing anyone of using either Cats or Scalaz). People for the general part are mature and they will pick the outweight the cons and pros as they see them and will pick the library they see as better, all things considered
Insults are not disallowed. Also, can I ask, how many maintainers do you see insulting people? Can you show me in the pull requests? 
&gt; the build.sbt file, is dramatically different from the stuff you do down under the project/ directory The latter is discouraged now and will disappear in sbt 1.0.0 afaik. (as others have noted, special blank line treatment is already gone since a while)
Wow, you either fell for it hard or let me guess, do you work for Underscore?
Are there any open source libraries or apps built using this? I'd really like to see an example in action
fs2 and task for parallelizing, but you are probably just garbage collecting a lot because you have no backpressure. Try this with Play Iteratees. Add libraryDependencies += "com.typesafe.play" %% "play-iteratees" % "2.6.1" To your build.sbt. With Play Iteratees, you: Enumerator.enumerate (iterator).run ( Iteratee.fold (Option .empty[MyResult]){ (acc: Option[MyResult], item: ItemType) =&gt; acc.map{ result =&gt; addDataToResult (item, result) } } ) Where addDataToResult knows how to add data from item to your result. You'll get a Future [Option [MyResult]] out of it, which you can .map to access the result with, or Await.result(theFoldResult, Duration.Infinity ) That will keep you from garbage collecting all the time, and will probably speed things up a bit. After that, research fs2 and figure out how to use it with Task and bracket and parallelism. The above is sequential but memory safer, assuming you are not adding everything from your query to the result object.
Thank you!
I won't claim to have any specific insight into the way different JVM implementations run applications, but you had better believe there is a server side cost to lazy loading classes on the server, and it is seriously costly to the worst case performance possibility of Scala on the server. On the client... It's complicated, but you definitely can't amortize the cost of thousands of tiny class definitions because you simply won't use them enough to break even. Java has the same limitations, but Scala is exploiting the JVM in a way that exacerbates the worst case for one time use. Classes are expensive. There are benefits to using them purely on-demand, especially if you don't ever use them, but the alternative is ahead of time compilation. In summary- the question is nuanced but fundamentally absurd on its face and the answer is only relevant to your specific functionality. Good luck measuring the performance of any system, btw. The performance of the language you choose is entirely specific to the way that you use it and the way your runtime works.
&gt; Dozens of other factors what are the most important ones?
The good news is that I while ago I removed Scalaz as a dependency on the `core` `extra` and `test` modules. Only the `monocle` module depends on the `scalaz` module and that's because Monocle itself depends on Scalaz. In other words: 1. if you don't include the `scalaz` module it won't be on your classpath. 2. it should be very easy for someone to contribute a `cats` module. It could even be a copy-paste with the imports changed. I actually thought about this but seeing as I don't use Cats I think it better that someone who uses it for real get involved.
This looks so cool, but it looks like it'll take a while to wrap my head around it.
&gt; Insults are not disallowed. &gt; We invite you to join us 
Off topic but I really wish Scala would become a First class citizen on Android. I think it would bring the language to a lot of people and also make programming in Android much more"modern". I think Kotlin will gain a lot of momentum if they keep the barrier for using it on Android low. I'm not implying anyone is responsible or anything, I would just love seeing the Scala libraries packaged with Android by default and some clean Scala libraries and APIs for it.
Imo algorithmic complexity and I/O efficiency are the main ones. If those are bad, your programming language isn't gonna save you.
It might not seem inviting to you, but to me I'd rather handle insults than walking on eggshells. Bullying can be done without using insults after all, and it's not like it is encouraged. I don't necessary agree, I think the initial draft was a rambling disaster, but I'm not a fan of outright dismissing a project because you disagree with them.
It's getting better. Don't be so negative. Compare it with Eclipse which reports bunch of errors and lags like hell.
Practice what you preach
yes, configurations can be used of course, but some of the frameworks provide an "admin panel" to change those values in runtime, i.e., it's basically a "configurations microservice", that decides for each user/entity if a feature is enabled or not. A/B testing is a nice use case. Load tests are also useful: if you have a new feature but you don't know how much overload will it add to your system, you may want to release it only to some subset of your users. Facebook, Twitter, Instagram, etc, they all release the features for a subset of users progressively so that they can control (and eventually rollback) who has access to each feature. I've found it avoids a bad experience for users, by letting a small number of (beta/admin) users to test it first. One could argue that proper (unit) test would have the same result here, but sometimes you just don't have the time to code extensive tests...
I am a bit uncertain, but I cannot help but wonder if you are seeking to concern troll Tony Morris. Do note that I have not downvoted any of your comments in this thread (I haven't downvoted anyone in this thread, for that matter).
To answer the question as asked, they are not equivalent. `flatMap`, or `&gt;&gt;`, only continues the computation with successful tasks. If `fut(x)` fails, `sideEffect` will be run in the first code snippet, but not in the second (with `Task`). You may use `myTask.attempt` to lift the exception into a value, or `recover`/`recoverWith`. In this case, you probably get negative value from using `Task`, since you have a self-imposed limitation not to compose them (by requiring the return type to be `Unit`).
awww. you are being too harsh. 
They'll still be different, since no matter how you slice it, a call to fut(value) begins executing on the supplied ExecutionContext immediately. A Task is essentially a free monad - in that it doesn't evaluate until you call run on it, which you can do multiple times. No matter how many times you evaluate that task, sideEffect() will only ever be called once, which violates the logical and semantic contract of what a Task is.
Good point on &gt;&gt; - thanks. I question the negative value point. Making functions to use Task, even if they're independent and surrounded by Future and Unit types, still provides incremental value, I think. Edit - I see your point on negative value in this case.
is this written using scala.js?
Yeah. Just because it's pain for me sometimes. No offense =( Anyway it's the only one IDE that doesn't make me wanna puke. And I have active subscription for ultimate version. I'm not only rude, I'm also supportive. 
&gt; Cats was started for business reasons, not for altruistic ones As one of the people that started Cats I can say with confidence that this is patently false, and I'm amazed that someone might actually think this. We really only wanted to work on pure FP in Scala in a welcoming environment. The amazing number of people that immediately came to the party and started flooding us with pull requests was a clear sign that there was a lot of desire for it. To somehow believe that all these people were showing up and volunteering their time in order to somehow make money for some particular consulting firm is just not the reality. 
So you claim.
Just a note, if you'd use the [Monix Task](https://monix.io/docs/2x/eval/task.html), you wouldn't need some third party library to do the `Future` conversion for you, since it does it out of the box, in multiple variants actually: - [Task.fromFuture](https://monix.io/docs/2x/eval/task.html#taskfromfuture) - [Task.deferFuture](https://monix.io/docs/2x/eval/task.html#taskdeferfuture) - [Task.deferFutureAction](https://monix.io/docs/2x/eval/task.html#taskdeferfutureaction) So for example in order to build your `result`, you don't necessarily need an implicit `ExecutionContext`, since `Task` can provide that for you: val result: Task[Unit] = { val f = Task.deferFutureAction { implicit s =&gt; fut(x) } f.flatMap(_ =&gt; sideEffect()) } And it gets better: val future: CancelableFuture[Unit] = result.runAsync Well, in this case it's not actually cancelable, since there's nothing in our logic about that, but it's a `Future` nonetheless, so you don't necessarily need to do that callback-based dance, which is always ugly. Kevin, given we follow each other on Twitter and I remember helping you in the past, I do wonder why it is that you're using Scalaz's Task. Just looking for some feedback.
Well, that opportunity is gone completely. Scala had a years of head start with Android, and the combination of disinterest and downright sabotage made sure it never got the attention or mindshare. Kotlin completely obliterated within a few months the tiny inroads Scala on Android made. I expect the same to happen with Scala.js and Scala-Native.
&gt; sbt 0.13.14 is a technology preview of what's to come in sbt 1.0. Is there a rough timeline for sbt 1.0?
thanks
We released the first technology preview 0.13.5 in May of 2014. Having said that, I think parts of sbt 1.0 are shaping up, and hopefully we can get something out sometime this year. 
When the beta comes out, people can help by migrating the plugin ecosystem. For more immediate help, there are a bunch of issues on Zinc (https://github.com/sbt/zinc/issues) and sbt (https://github.com/sbt/sbt/issues) marked with "Community", "help wanted" etc.
There are days where all I really want is an sbt 0.14.x which would be exactly like 0.13.x but running on Scala 2.12 ([today, for example](https://github.com/scala-js/scala-js/issues/2798)). I wonder whether the benefit/cost would be worth it for the community.
I would suggest to use Scala Stream instead as it's more efficient way.
IIRC intellij uses zinc which is SBT's resident compiler +nailgun runner. I think CBT does the same.
I'd be interested to see a benchmark for that claim
I'm sick of you spreading FUD. Scala on Android has never been publicized because there is no official support for it. Scalajs and Scala Native are projects that do offer official support and have people working full-time on them to make the experience of using Scala on other platforms seamless. When Scala on Android gets people full-time working on it and offer official support, the Scala team will reconsider this decision. Until then, publicizing it in the official Scala website will cause more harm than good.
&gt; I also don't think it was fair to the downstream library authors to be put in this situation, a fork didn't have to happen. As a downstream library author who sometimes asked for help on the scala/scalaz IRC channels while Morris was there I think having him talk on the channel the way he did put downstream library authors in a far worse position. The Scala community needs a library of common typeclasses and an obvious place to go to seek help using that library without the fear that an intelligent and experienced bully will snipe at them relentlessly until he eventually finds a way to genuinely upset them. Maybe a better outcome was possible, but the existence of the fork puts me in a much better position than I was before the fork. I'm unhappy with Typelevel/Cats on many levels. I think they handled things extremely badly; after that whole incident I don't trust their[1] honesty or judgement as project maintainers (and there's an element in their CoC that would make me unwelcome in their social spaces even if I did trust it would be enforced fairly). But even given all that, fundamentally I can still trust them to maintain some basic level of common courtesy (at least as long as I do), which means I can work with them. I can grit my teeth and step into their channel for a bit to ask for help (and for whatever it's worth they've often given me a lot of help) or send a PR. Whereas at this point you couldn't pay me enough to do something that meant I risked encountering Morris. [1] Since the whole affair was all secret back channels, I don't know who specifically was involved; as such I mistrust the whole organisation.
I'm on mobile, but basically, you pass the array and a function that takes two arguments which will be the same type as the elements of the array. The function needs to return true if the left argument comes before the right one in the sorting you want to test for. 
put a ! in front of the ordered. :paste it into REPL then run like this in REPL val d = Array(1,2,3,4,5,6) isSorted[Int](d,(a,b) =&gt; a &lt;= b) or for val d = Array(6,5,4,3,2,1) isSorted[Int](d,(a,b) =&gt; a &gt;= b) 
how about fibonacci numbers larger than bigint as the next challenge :)
It's not perfect but it's better than a text editor. Certainly worth putting up with a few bugs or inefficiencies.
I would love to see sbt cross-build against 2.12/2.11/2.10 like the rest of the ecosystem. This is probably my biggest feature request for sbt. It would make it possible (a lot easier, at least) to write plugins that use libraries that don't support 2.10 (scala.meta in my case).
I do not think that cross-compiling sbt itself would help in the least. We would need to cross-compile all the sbt plugins, which would only make it harder.
&gt; believe that all these people were showing up and volunteering their time in order to somehow make money for some particular consulting firm That is not what I believe. I believe that the majority of people showing up and volunteering and maintaining are doing good work for good reasons I 100% support. That wasn't fair that I didn't clarify that, and I will edit my original post. 
I was looking through the source of remotely, the other day Verizon seems to love typelevel's stuff which I find weird since like right next to that is Go (I use Go a lot too hate the language itself but it's useful.). I was playing with Algebird a bit yesterday one thing confused me a bit are rollers and cubers like crappy lenses? In my opinion from using the projects and reading the code the twitter stack is probably as functional you're going to get without going Typelevel. A lot of the engineers use Haskell or Ocaml, and follow Typelevel projects. Vkostyukov is big fan of Typelevel stuff, twitter utils take a lot of useful things from ML's . The code itself for these projects is pretty functional, but they don't treat scala like an ML on the JVM. But they still get all the advantages relating to concurrency and composability. 
When I joined Intel Media and for a good chunk of the time after the Verizon acquisition, it's fair to say the architectural direction was pure FP based on scalaz etc. and that's reflected in Verizon Labs' open-source Scala code to this day. Along the way, a few things happened: 1. More and more non-Intel-Media business units got combined with the former Intel Media business unit, and they weren't even necessarily Scala units, let alone FP-in-Scala units. 2. My boss from Intel Media and Verizon Labs early on, a brilliant guy with an over-11-year career with Intel was (quite rightly) promoted. Unfortunately, that meant he had too many direct reports, and so the architecture team was disbanded. I think the thinking was we could all guide architecture from our respective engineering teams. For some that might be true; for me, at least, it wasn't, and eventually I left. 3. Some other key personnel, e.g. Rúnar Bjarnason and Daniel Spiewak, also moved on. 4. Yet other key architectural players ended up working on infrastructure automation once it became clear that was a significant gap the organization faced, and like it or not, in infrastructure automation, you're going to be facing a pile of Ruby and/or Python and/or, increasingly, Go. I certainly promoted ocaml-musl-static as an alternative to Go in that context, but it didn't take, for reasons I think only Tim Perrett is privy to, but as a guess I can imagine there are other folks involved in the development and maintenance of the code who don't know Scala and therefore aren't thinking "sure, I'll do one-off command-line utilities in OCaml, which is even better." Today, Verizon Labs continues to employ about half of the scalaz committers, Ross Baker (head http4s guy), and about half of the Cats committers. So yeah, the pure-FP-in-Scala ethos remains very strong there. It'll be interesting (especially as an outsider now!) to see how that plays out.
Do you really want to tell your users "you have to use sbt 0.13.x *with Scala 2.12*" rather than "you have to use sbt 0.14.x"?
The Free Monad brings the ability to have different interpretations (easy testing, yeah!!!) but also reduces performance (not really a good selling point to the higher ups). So when do you choose to use the Free monad over the standard approach (presuming that the rest of the team is comfortable with such a change). Also has anyone used it at scale, maybe porting an old non-FP program to use the Free monad. PS: I know nothing much about the Free monad or functional programming concepts in general.
I don't know if you get what I am saying the state monad is not tvar, mvar, or ioref there is no synchronization, so sharing the state monad between threads is stupid, mutable or not. So the way to track state using the state monad in a concurrent environment is to start a process and have it perform actions based off of certain requests, creates a new value with a function, calls itself on that value, state monad or not it is still immutable. The significant parts of the state monad is not really immutability, it is the fact that it enforces separation of IO from computation, and lazy evaluation. Also don't lecture me on concurrency I probably know more about it than you do. Also it's the only part of Haskell where I would consider myself strong in. 
I don't think it's maintained anymore. The only other client was Eclipse, but it replaced it with Zinc sometime in 2011. Zinc with Nailgun is probably slower but more reliable. 
my only problem with autowire is that I can't use it for file transfers.. is that possible with this library? I can't find an example for that (or anything in the docs)
Dotty has a feature which makes crossbuilding obsolete, but I bet that won't make it into scalac with 2.13..
Thanks a lot
Premature optimization is the root of all evil. Make it correct first, then make it fast if you need to, after profiling. If you ask the business how fast to make it they'll say "as fast as possible" (because what other answer would make sense?), but business people understand the ideas of cost/benefit tradeoffs, good enough, and not gold-plating things. Getting some metrics on it and setting an explicit performance requirement (e.g. 300ms for a web backend I've worked on - you have to be doing something massively wrong to get latency anywhere near that, but that's the point) with graphs and so on may give them more confidence. I've used the Free monad in production multiple times, mostly just for database access. E.g. part of my current project was adding failover to a system built without it, which meant introducing database transaction boundaries that made business sense, and I used Free (under a more business-friendly name) for that. We've hit a number of performance issues, but they've always been underlying database issues (and by that really I mean flaws in our data model, not issues with the database engine implementation). Scala is plenty fast - you're probably leaving some performance on the table by using Free, but it's very unlikely at a level that would make a difference for your use case.
`sealed trait List[+A]` is defining a new `List` datatype. If you're trying to work with something like this, doing your own implementations of types that exist in the standard library (which is a useful exercise!) then I'd highly recommend giving them different names and/or building/running the interpreter with `-Yno-imports`, so that the "default" built-in `List` type doesn't cause confusion.
Eclipse doesn't report any spurious errors for Scala if you use the problems view. There are occasionally spurious errors in the presentation compiler (i.e. red underlines) but the icon for them is subtly different, and if you're ever uncertain you can always check the problems view. In contrast IntelliJ errors on all sorts of working code. Recently they changed a lot of the false positives into false negatives but that's not any better. As far as I can see if you use a higher-kinded type member of a parameter as part of a function return type IntelliJ has no way of understanding that function signature, and will just always error, or in more recent versions never error. For a long time I was willing to give all the people who claimed there were errors the benefit of the doubt. But at this point I'm calling it out as FUD. I'm writing [some quite complex Scala as part of tierney](https://github.com/m50d/tierney/blob/master/free/src/main/scala/tierney/free/package.scala), and I work on it in Eclipse and see no spurious errors (I saw one thing that looked like a spurious error once, but it happens in command-line builds just as it happens in Eclipse, so it's either a real error or a bug in kind-projector). Anyone talking about errors in Eclipse, please put up or shut up. Give specific examples.
Hi! I'm currently working on Akkeeper (https://github.com/akkeeper-project/akkeeper) - project that allows to easily deploy and maintain Akka services on Hadoop. The project is built keeping reactive principals in mind. Currently I'm working on it on my own and actively seeking for a feedback. I'll be glad if someone decides to join it.
How does the compiler knows, that I can write a List like val t = List(1,2,3,4) In the example above. I am coming from haskell world it seems to be equal to `data` in haskell.
Yes, I use it to download mp3 files for instance. You could also implement file upload but there is nothing already provided.
Not on the project it's building, no. But on all sbt plugins, definitely.
Currently translating a lot of python+numpy into scala+nd4s for my AI projects, https://www.udacity.com/ai
Working on a MongoDB client library that's not completely insane - with a proper BSON AST, and type classes for encoding / decoding any type as / from a BSON document. A lot of the work has already been done - the AST is complete and tested, as well as most default type class instances &amp; automatic shapeless derivation for product and sum types. Of course, no integration with MongoDB yet, just BSON encoding / decoding, but it should allow me to write stuff like: import kantan.bson.generic._ case cass User(id: Int, name: String, birth: DateTime) collection.insert(User(1, "foobar", DateTime.now()) Current state of development can be found here: https://github.com/nrinaudo/kantan.mongodb
Thanks a lot guys.
I'm always going to be overly verbose and not assume that someone is an expert because * 1. It's more efficient that way to communicate * 2. Most folks aren't butt hurt if I go over something they already know * 3. Other people read comments who don't have the same background as either of us and may want a more thorough explanation even if it's superfluous for us. So, while I didn't lecture you on concurrency, I will now. :-P There is value using the state monad in Scala, you can run it against immutable data in atomic references (j.u.c.a.AtomicReference) so you can get transactional semantics for concurrent operations (or you can lock if you wish) and still maintain thread safety. This is useful if you want to build a cache up through multiple concurrent requests (assuming you aren't also side effecting). It's just one tool, doesn't solve every problem but it can come in handy. 
The things you use Free for are likely not CPU bound tasks, any other method for achieving anything similar is going to have similar overhead.
Is there a way to find all / most of the unused code in an application as defined by whether or not it is used (transitively) by the main method? 
Same book, page 25: &gt; When we define a function literal, what is actually being defined in Scala is an object with a method called `apply`. Scala has a special rule for this method name, so that objects that have an `apply` method can be called as if they were themselves methods.
That's pretty neat similar IOref and atomicallymodifyIOref while this works fine, and worst case scenario you get some contention, although I am pretty sure that you personally usually just end up using Mvar at that point instead of the state monad. 
Edit: replied to the original post instead of here.
Don't know really. Right now it's just an itch to scratch - I use MongoDB for data exploration quite a bit and the horrible standard API upsets me. What you're suggesting sounds like actual work :) Once I've got this where I want it, I'll be sure to look at SDBC and see whether this can work.
And I'm always happy to meet users - always a surprise when people actually see value in what I do in my spare time!
Please share the code and describe steps you did in order to compile stuff. This is my guess: You first defined a `trait List` and then attempting to define `object List` inside a file and then attempted to import it in Scala REPL. Scala REPL doesn't import classes with companions correctly. You should be using `:paste` mode. See here: http://stackoverflow.com/questions/3551981/is-it-possible-to-define-companion-classes-modules-in-the-scala-interpreter Basically you should put this inside single object (so classes and companion objects won't exist at the top level in your file). If you are just starting out with Scala then maybe you can check out Scala worksheets (in both IntelliJ or Scala IDE)
I also suggest you call these things something other than `List` and `Nil` so you don't get them mixed up with the identifiers from stdlib. I realize a lot of books and tutorials and exercises tell you to do this, but they're wrong. Don't. It will confuse you.
really nice. thanks a lot! From your experience, what are the major limitations on this finagle-toggle?
for everybody interested, an example of feature flag/toggle: https://launchdarkly.com/features/index.html
I created SDBC 1.0 for a work project when I was sick of dealing with JDBC. It started as a JDBC wrapper, but then I added Cassandra support. This was before Doobie.
I think this is a pretty sadistic exercise to give as a response to "what is a trait"
In Scala, when you do that it would use the companion object of your defined List datatype and in this case the type can be inferred by the compiler. It's the same when in Haskell you use a data constructor: ```Prelude&gt; let x = [1,2,3,4] ``` ```Prelude&gt; :t x ``` ```x :: Num t =&gt; [t] ``` And as you guessed, data is used in Haskell for the same purpose as a sealed trait is used in Scala (not exactly the same though): ```Prelude&gt; :info [] ``` ```data [] a = [] | a : [a] ```
&gt; You first defined a trait List and then attempting to define object List Exactly. &gt; Scala worksheets Where can I find it? 
Awesome, thats good to know! Essentially only waiting for Monocle to change over to cats then
It's a vararg expansion. It basically means that you want to construct a Sequence from multiple parameters, so you can use comma separated parameters to build a List. Find a bit more of information [here](http://docs.scala-lang.org/tutorials/FAQ/finding-symbols.html).
&gt; Is Cons(_,_) a function? For me it is a data constructor. Yes it is a function. `Cons(_, _)` is syntactic sugar for `(x, y) =&gt; Cons.apply(x, y)`
You can also use an online fiddle like https://scalafiddle.io/
Which page do I miss?
I know, because I just start learning scala.
What he's saying is that you need to slow down and read more carefully. 
but it is a function but without a body.
Scala works on Android pretty well. You might visit sbt's plugin home page http://scala-android.org/ for more info and gitter channel if you have any suggestions / questions etc. https://gitter.im/scala-android/sbt-android Also Android got more mature lately, proguard, more powerful devices etc. So it's getting better and better. You might be also interested in Sri - https://github.com/chandu0101/sri which is Scala.js using react native for ios and android, this will make probably more momentum. I personally expect Scala on Android getting more look when Scala Native will be used on iOS Regarding kotlin - it has less classes but you need to add more libraries to have better functional support, also annotation processing with kotlin is kinda slow and buggy.
I don't mean to sound dismissive, but why would one want to? Do companies really still believe in these? 
What do you mean?
Where can I post my questions? Is there a thread like https://www.reddit.com/r/haskellquestions/ for scala?
First time I'm hearing of it, I'll check it out. I do think Scala is a perfect fit for real-world AI/ML
Not sure I'm understanding you correctly but the body is `Cons.apply(x, y)` and the function signature is `(A, List[A]) =&gt; Cons`
At which level? Interfaces, or down to the implementation? What I'm writing is mostly computational/numerical code, so the interfaces follow the best practices (immutability, no shared global state). A guiding principle: can the properties be easily verified using scalacheck? At the implementation level, anything goes (Array, var, mutable state, while loops).
A riding crop
You don't understand because you're not thinking from a marketing perspective where your audience is a typical developer. I'm not likely to convince this audience to use Scala by telling them I have access to higher-rank existential types. However, telling them that Scala native gives them startup times, memory footprint, and statically linked binaries like Go or Rust might help convince someone to try Scala for their next project. And, as a side note, I also think Go and javascript place poorly on the spectrum of language awesomeness.
Scala was always marketed as a language that scales, but in both directions, whether for writing a simple script or for writing large-scale programs. See, for instance: http://www.artima.com/scalazine/articles/scalable-language.html To directly quote the article: "The name Scala stands for “scalable language.” The language is so named because it was designed to grow with the demands of its users. You can apply Scala to a wide range of programming tasks, from writing small scripts to building large systems." If Scala performs significantly worse at Go for the smaller scripts/programs, it would at least be good to understand why Scala failed at its initial design goal, and maybe how it could improve in a future version of the language and/or standard libraries.
As /u/paultypes suggests, purity and parametricity are disciplines in Scala, not something the compiler or existing tooling can enforce strictly. So it's really up to you to ensure that your code and culture stay in-bounds if that's the kind of programming you like to do (and it should be). Tools like Wartremover and ScalaStyle and even [a good set of compiler options](http://tpolecat.github.io/2014/04/11/scalac-flags.html) can help keep you in the groove so do use those when you can. If you run into particular challenges with your team and need to produce arguments to convince them you're right, ask here or on Gitter and we'll help you out. ;-) N.B. I need to update that list of compiler options. It's still a good start but it's getting old.
Go has very serious advantages compared to other languages when it comes to stuff like compile time (and hence things like iteration speed) and small binaries. It also has a strong stdlib, and has at least settled on a concurrency primitive which makes it easier to write concurrent applications in the context of an ecosystem. The very fast compile times (Go was deliberately designed to compile as fast as possible) means that it also has very fast iteration speed for tests and deploys, which matters in the environment where it has its niche in. I am reminded of this all the time, especially when I compile a test (with only a single line change) in Scala and that takes roughly 8-10 seconds to execute (and yes this is with incremental compile). Then stuff dies because of not enough metaspace...
&gt; FUD &gt; there is no official support for it you just said it yourself, didn't you?
I really like this approach and is something I'm moving towards slowly. A quick question though arising from this example, I've noticed the ExecutionContext has been dropped in the rewrite to UserOp. If we want to not import it globally when providing the instance for Future, how could we write this to have an EC on a per-method basis ?
We just use LaunchDarkly's java API. Then we wrap it with something that takes an implicit parameter representing the currently authed user. something like: class LDFeatureFlagClient @Inject() (ldClient: LDClient) { def variation(featureKey: String, default: Boolean)(implicit ctx: UserContext): Boolean = { this.ldClient.boolVariation(featureKey, this.createLdUser(ctx), default) } } 
togglz seems similar, but built in java https://github.com/togglz/togglz
Somewhat tangentially, note that if you care about pure functional programming you can't use the stblib Future, which is not pure (i.e. referentially transparent). Use Scalaz Task instead.
Same here! To me it feels more like a modern Java than anything else. A safe, portable, dead simple language that aims to replace some of the higher level uses of C++ while ignoring the low level/deep embedded uses. There is more of a focus on performance with Go than Java had in the earlier days, but that's about it. I was super excited about Go when it was first announced but it didn't take long to dive in and just get disappointed. EDIT: This also ignores Go's compile time story. It was never really interesting to me so I just kind of forget about it. I get why people care, but for me I've always found away to deal with even the worst compile times.
With the above encoding: implicit def userOpsFutureInstance(implicit ec: ExecutionContext):UserOps[Future] = new UserOps[Future]{ ... } This way you can override the ExecutionContext at need. Task (fs2, monix, scalaz, etc) is better at this, as you can supply the executor when you run the task, which doesn't require that you embed the implicit in the instance creation. Be careful while chaining implicit instances though. When it tells you you don't have implicit paramater x, try to instantiate it directly (by calling the implicit def `userOpsFutureInstance`, it will tell you which implicits in the implicit def you are missing. In general, anything the function/method needs should be declared on the method or function, as you can apply context bounds on it. This is how it's handeled in cats, for example: https://github.com/typelevel/cats/blob/master/core/src/main/scala/cats/instances/future.scala#L9 You can encode the object above into the following: sealed trait UserOps[A] case class GetUser(id: String) extends UserOps[User] case class ListUsers() extends UserOps[List[User]] case class PutUser(user: User) extends UserOps[Unit] and write a Free implimentation using this tutorial http://typelevel.org/cats/datatypes/freemonad.html That gets rid of your F, and lets you write the same expression above. In your interpreter you can specify a specific F and take the ec if it is future. 
I did two POCs one with actors one with futures. both of them worked. However the only reason I went with actor is the error handling. with futures I was still doing try catch block (or recoverWith) and the re-try symantics were hard. with actors it was a breeze for me to re-try after error post the supervision. In my project/domain... its very very important that we re-try if there is a failure are processed. Just logging and throwing an error will not work. Thus our team agreed to using the actor model simply because of the retry mechanism which we feel is stronger with messages and supervision rather than try catch. (or recoverWith) Edit: More specifically, with Future and RecoverWith it was easy for me to retry once in case of failure. but if we want to retry say 3 times and then try alternate storage (for future processing and alerting) it was hard. it was not impossible, but the code inside recoverwith was just awkward.
That's a fact. Scala Native counts with a full-time PhD, several students and several engineers at the Scala Center that actively develop and maintain the project. The same goes for Scalajs, which on top of that has been heavily battle-tested and used in the biggest Scala shops. If you think Android on Scala gets official support and funding, please provide facts.
This is not really good. Scala is much more complex for default developer. If Scala attract wrong people - they will do wrong things and spam internet with articles like "why i quit Scala and happy about it" Had a job in Ruby, now i feel that Scala is addictive. Rubby stopped being enjoyable
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/dfw_scala_enthusiasts] [In the context of how to enforce Pure Functional programming within Scala, from r\/scala](https://np.reddit.com/r/DFW_Scala_Enthusiasts/comments/5zgpil/in_the_context_of_how_to_enforce_pure_functional/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Java libraries: not until the Scala compiler can compile Java code. Scala libraries: if [Scala.js](https://www.scala-js.org/) is any indication, many pure Scala libraries will port easily. A good part of the Java standard library has been ported to Scala.js, and similar work is going on for Scala Native. (The above from somebody who is completely external to the Scala Native project.)
Video [here](https://www.youtube.com/watch?v=7xSfLPD6tiQ) … the slides may be hard to follow without the yelling and handwaving.
I wish so much, I would be smart like you ANAL. 
I think every language is worth taking seriously in the sense that it's worth studying the positive and negative aspects of a language. But I agree that Go doesn't really bring anything exciting to the table language-wise. As for JavaScript, it's worth taking seriously because it's so ubiquitous. Of course, it didn't reach that status by virtue of language design, but you can ask questions like why it moved to the server space and perhaps get interesting answers. The fact that every programmer seems to be able to deal with Javascript, that quite some languages can be compiled to Javascript and that is has such a massive ecosystem also make it interesting as a target language for a source-to-source compiler. 
I would ask you to consider deleting your post. Even if you're irritated by the OP, it's uncalled for to be so insulting.
That looks more than interesting to me, being a software developer and also a musician. I'll try it out whenever I get the chance, thanks for sharing!
AWS Lambda Scala: https://github.com/mkotsur/aws-lambda-scala An small utility library for writing AWS Lambda handlers in Scala. [De]serialization included and it automatically detects if your input/output need to be converted from/to JSON (case classes) or not (primitive types). Plans: - Support returning Futures; - Better docs.
Ah okay. This will be interesting as I rely on a lot of Java libraries.
To be fair, the /r/programming comments appear to be mostly the effect of one troll and the corresponding replies.
The priorities of this project baffle me. HTTP/2 is no small undertaking...much larger than a switch to scala futures. And twitter futures are the biggest impediment to adoption. It's almost as if they don't care about adoption. 
They don't ;)
I think Scala Native would need a very good concurrency story to compete against Go for the terms you note, since Go has a decent story there (goroutines and channels), at least from what I hear, and Scala Native currently does not support multi-threading of any kind (you can do C-based multi-threading, but [it is not officially supported at the moment](http://www.scala-native.org/en/latest/user/lang.html), and a quick search I made did not reveal future plans in this regard).
I believe the priories are whatever we need at Twitter. If they​ need http/2, but are fine with Twitter futures, then why not. That's one of the reasons I prefer Akka HTTP, it's supported by lighbend who do care about adoption and responding to the needs of the greater number.
I started with the book by `Underscore.io`: Advanced Scala with Cats http://underscore.io/books/advanced-scala/ but be warned, the book isn't complete and its already starting to get a little out of date (IE: `Xor` was removed in the latest version of Cats, but you can do the book exercises using Cats version of `Either`) Having said that... If you're already familiar with the concepts of functional programming from Haskell I think the documentation you linked is actually a good place to start and build stuff using it. Also: https://www.scala-exercises.org/cats
Mastering Advance Scala covers Cats https://leanpub.com/mastering-advanced-scala
https://medium.com/@abu_nadhr/scala-cats-library-for-dummies-part-1-8ec47af7a144#.nt19mjwc4 + api docs + the scala exercises demius posted. Or seriously just learn haskell first, then it becomes really easy.
Even though the book is out of date, it is still really good. There are still some case studies that haven't been completed yet but I know the guys are working hard on it. 
The cats doc is intended to be good, so if you find that it's not please open an issue. Also, ask questions on the `typelevel/cats` Gitter channel. If people are talking about transfinite zygomorphoids or whatever just butt in and ask. We're friendly.
This is not a question but whine... recently i started working on java codebase of some other teams (after 2yrs of pure scala) and the amount of NPEs in production. And some classcasts exceptions... I miss scala : ( 
Heh - we encountered an NPE in akka-http this last week. The first NPE I've had to deal with in a long time.
You can't just make a blocking API non-blocking. In most cases, you should be able to find a non-blocking replacement, though. For file I/O, HTTP, DNS, MongoDB, Cassandra, and even PostgreSQL, non-blocking APIs are available afaik. According to https://aws.amazon.com/articles/5496117154196801, the AWS Java SDK provides at least some async APIs as well. If you're forced to use a blocking API, wrapping it in a Future has the advantage that you can run the blocking calls on a separate ExecutionContext, so you can have a dedicated thread pool for blocking I/O operations. This is the approach [Slick](http://slick.lightbend.com/doc/3.2.0/introduction.html#reactive-applications) takes for example, to provide an async interface on top of blocking JDBC.
I can think of a few use cases where you want full control over performance and memory usage for example in games and embedded software. However, a language like Rust seems more suitable for these use cases, but it will be interesting to see if Scala Native can be competitive. Note that Scala Native still use a GC (Boehm) but it supports explicit pointer and memory management (both on the stack and the heap). So, it will take some effort to make a program without GC interruptions (basically avoiding 'new' in the main loop, meaning you can't use much from the Java and Scala standard libraries) and it will not be as safe as Rust when dealing with pointers and memory. I wonder how easy it is to hook in your own GC implementation (for example a reference counting one) in Scala Native.
Spring is not really a great web framework for scala. just looking at the syntax I would never prefer something like that over vertx, akka or play.
I think Scala needs Spring less than Kotlin does, largely because it's a more powerful language. I think you will find that functional programming alleviates the need a lot of the things spring provides, that people are able to do dep injection in vanilla scala, and that if that gets tedious that we have macwire and bindings to things like guice to help. The monolithic framework just doesn't mesh well with scala programming style in my opinion. So I have no interest in spring for my projects. EDIT: misspellings etc
Don't necessarily mean only the DI part spring gives loads of libraries under one banner see [cloud native java](https://www.youtube.com/watch?v=JDcl4kT6Qmo) 
I'd never suggest this to a beginner, but you say you already know the concepts from Haskell, so why not try reading the code (in addition to the resources others have pointed out)? You will get to a good understanding of the library, and learn a *lot* of Scala in the process. Obviously, I understand this might be an unacceptable amount of effort, so feel free to ignore my suggestion, but it's worked very well for me in the past (e.g. with Scalaz, Shapeless or Spray) and it's taught me a lot (e.g. how to structure a library)
I think the problem is that Scala isn't trying to be Java. Scala coders code more like Haskell or ML or Erlang than Java. Kotlin seems happy to be a better Java and God bless em. But I don't think the core scala constituency has any interest in that. I could be wrong. 
I doubt there's anything that limits support for Spring, other than somebody has to write it, and then support it. Usually this long-term support comes from people who will use the code on a daily basis (e.g. in their job). My impression is that Spring is that distant annoying thing that happens to other people. Both in work, and conferences, the topic of Spring seems to come up rarely or never. Your first job is to find people willing to own such a project - perhaps some company who use Spring extensively already, and are moving to Scala. But sorry, none come to mind off the top of my head.
The AWS async API is still blocking under the hood - just in a different thread pool, and uses Java Futures (which are rather horrible to use). So I wouldn't recommend them; the `Future` recommendation does work reasonably.
Thanks, yeah that's sort of what I thought. Although I think it would help adoption since there are so many enterprise java shops that use Spring especially now there is an increased moved towards cloud deployments, maybe a small push would help. 
Maybe the best chance is a consultancy who might target the Spring market with Scala solutions :-) Who knows, maybe there's a space here.
I've used Spring in Scala before. It mostly Just Works - most of the things on that page don't need any special support, just documentation of the fact that you can do the obvious thing in Scala and it will work. Dedicated support for autowiring of `Option`s (the same way your link talks about support for autowiring kotlin nullables) would be nice - indeed it's something I'd benefit from in my actual job - and that seems like a small-scoped feature that would add real value. But the main thing is just the documentation and the politics of having people promote this use style. Are you in touch with the people who run start.spring.io? It's them you need to convince.
JVM startup is still slow. It's optimized for high-throughput in server-like use cases, at the expense of latency in interactive use cases. (I'm interested because I'd like to be able to write cryptographic code in a Scala-like language, which I don't think is possible in a GCed language (too easy to leak timing information) and requires a way to have semantics that certain information isn't copied etc.)
I'm struggling to remember where I saw this number quoted, but I have a feeling they're aiming for 2020.
All of the Spring libraries are available to Scala if they're needed and so is Spring itself but I've never felt the need for two reasons: * Having a large amount of reflection / byte weaving based magic framework happening is counter intuitive when writing Scala and gets in the way * There's usually a tighter, simpler library already available with more idiomatic, scala friendly APIs (or at least a wrapper around a java one) I can't think of anything I miss from Spring, and I wouldn't want to encourage people to use Spring as a starting point for Scala for fear they'd miss much of the point in it.
&gt; Referential transparency is one of the requirements to have a pure function. Besides referential transparency, a pure function must also be free of side effects. Side effects can include IO operations(stdout, files, database), modification of global variables, etc. This is false, and the constant repetition of this myth stands in the way of understanding pure functional programming for many people. Referential transparency is the **only** requirement for a function to be pure, because the definition of a side effect is something that breaks referential transparency. In particular, IO is an _effect_, which can or cannot be a _side effect_ depending on how you do it. In other words, it's perfectly possible to have referentially transparent IO, i.e _purely functional IO_. Sorry if it came out harsh, it's not meant as a criticism to you, but if we keep telling people that a pure function cannot do IO, they will think that they can't use functional programming, because they will need IO in the real world. On the other hand, we should stress that referential transparency is not a mere definition, but the source of many good things about FP. Referentially transparent expressions are context insensitive, which enables local reasoning on one hand, and high composability and compositionally on the other.
thanks a lot for the feedback! Please let me know if you find any other inconsistency. I will fix it ASAP. And yes, that's one of the complains I always heard against FP "pure function cannot do IO, they will think that they can't use functional programming, because they will need IO in the real world" And you are totally right on the last point, referential transparency comes with a ton of good things. Probably I should stress it more along the text
&gt; I could be wrong. You couldn't be more right. The fact that Spring *isn't actually needed* is one of Scala's biggest selling points to me.
That is the bane of all modern presentations. If you to any hadoop/spark conference, they are showing the same stupid "word count" program for last 5 years. I stopped attending conferences just so that I don't have to see another word count implementation and puke.
Spring??? why on earth would Scala use something as hideous as Spring. 
Kotlin doesn't require a Spring support, this jobs is only a nice plus. IMHO Spring is fat and looks like a large Java (language lack) workaround. I'm playing with Kotlin's coroutine and Vert.x and them look very promising. Scala has coroutine and Vert.x 3.4 supports Scala: togheter may be very pleasure. Spring probably isn't the right choice for Scala development - luckily.
Strong stdlib?! You need to `import "strconv"` and then convert a string to integer like this: `strconv.Atoi("100")`. And because of that lack of generics the language and its stdlib feels *barbaric*, not strong... &gt; and has at least settled on a concurrency primitive This maybe appealing for beginners but different problems may require different tools. &gt; The very fast compile times (Go was deliberately designed to compile as fast as possible) means that it also has very fast iteration speed for tests and deploys, which matters in the environment where it has its niche in. [Nim](https://nim-lang.org/) also has really fast compilation times while still having *generics* and other good stuff. But the *true* iteration speed with Nim and golang is still slower compared to Scala due to their inexpressivity. At least, for me and for those I knew... &gt; I compile a test (with only a single line change) in Scala and that takes roughly 8-10 seconds to execute Too much implicits(when using scalaz etc.), macros and files with multiple classes and traits can slow down your compilation speed drastically. If you avoid that it can be 1-3 seconds.
&gt; while you could use Scala for a throwaway 30-liner it's never going to look as good as a language specialised for them But what about Scala's capabilities in DSL design? You can create a powerful library which simplifies your work in Scala even more than in most languages. For example, think about [ammonite](https://github.com/lihaoyi/Ammonite) - bash is ok for small scripts, functions and aliases but if your workflow is getting more complex it won't be able to handle the requirements - while still being easy to use and read in the terminal. That's where ammonite and similars come - they maybe verbose for smaller use cases but they *scale well* as [mercurialmaven said](https://www.reddit.com/r/scala/comments/5zc3uu/scala_native_01_is_here/dex73lx/?st=j0cu59w7&amp;sh=ee4b6cd5) - and that's the most comfortable if you're a developer. Nowadays, I just bundle the jars created by [sbt-assembly](https://github.com/sbt/sbt-assembly) with a [bash header](https://ideone.com/WtUQ1q) instead of writing a bash/perl/python script if I need one. It's easier than one would think...
I was just about to say this, after today's [Scala Times](http://scalatimes.com/)
Right, sorry for being unclear. Let me explain. &gt; How is it possible to have a pure function that has IO effects? (without wrapping in a monad e.g IO) I never said you could do that without wrapping it in a monad such as IO. My point is the a function returning an instance of the IO datatype is just as pure as `def plus(a: Int, b: Int) = a + b`. &gt; My understanding is that all IO is a side effect unless it is captured in some way for later execution, usually inside an effect capturing monad like IO or Task. The computation of the execution plan is pure, but when it is run it is unsafe and impure because it contains IO effects. Your understanding is entirely correct. However, we should put this discussion into context. You already know about the IO monad, and about the difference between returning an IO expression and running it. Therefore, if I were to talk about purely functional IO with you, I wouldn't use the same explanation I used above. However, my experience teaching FP in my company tells me that it's not just about what to show, but also about what to hide. If you tell an absolute beginner (which appears to be the target of the blog post) that pure functions can't do IO, and that purely functional programming only uses pure functions, they will infer that purely functional programming cannot do IO, and is therefore useless. Once they know about the use of higher-kinded types to represent computations, and about the use of algebras such as Functor, Applicative or Monad to combine such computations, and finally about all the advantages of referential transparency, you can then explain purely functional IO just like you did, and they will get it. Finally, my correction was to the sentence: &gt; Besides referential transparency, a pure function must also be free of side effects. That's incorrect, since a function (by definition) cannot be referentially transparent and still have side effects. Sorry if I caused confusion, it wasn't my intention.
By the way, thanks for not getting defensive or offended. It's really hard to convey tone in a text and not appear harsher than what one actually meant :)
Ok thanks for elaborating on that!
They need to be cross-compiled. (BTW scalaz already has been for native)
I purchased this book and I found that this book tries to cover too many topics but in a very superficial way. I don't recommend this book.
Are you referring to `-Ystop-after=typer` scalac flag?
You can use futures with a reasonably large thread pool. The akka (and I think also Play) docs have good information on how to do this. 
What definition is being used in your book? ;)
You could define an sbt task that compiles with that flag? Call it `check` even. Then you could do `~check` which would be nice ;)
+1, if someone makes a plugin with that I might use it. ;) I'm guessing it's just a matter of making a custom task that calls compile, and setting `scalacOptions in check += ...` 
naftoligug made a good answer. Since you asked about terminology, I'll add a few: value classes - units of measure (classes that do not exist during run time) Refined - dependent types (the thing with strings matching a pattern) implicit classes - can be used for extension methods or treating a class as another class locally 
You should probably go on for one more phase, `refchecks`. Errors involving inheritance are only checked there. Some errors might still be missing, for instance overloaded methods that erase to the same type are reported by `erasure`, but that's rare enough that it probably doesn't matter.
I guess [this](http://ktoso.github.io/scala-types-of-types/) is a good starting point.
it's more important to discuss and learn than to get over-defensive or offended, as long as both parts are respectful :) One of the main reasons I write and share is to learn myself (before writing I always take some time to deep my knowledge on the subject - unfortunately some resources have mistakes, and fortunately some people want to contribute to improve them), and to provide another resource with yet another way of explaining those subjects. So, all feedback is welcome :)
http://stackoverflow.com/questions/42854210/why-covariant-in-data-constructor Thanks for help
Sorry for the slightly off-topic question but lately I've stumbled in a related problem so maybe you can help. I have two questions: - ~~what compiler phase checks for inheritance conflicts (two members with the same name and type)? Is it `refchecks`? (I assumed it was not the typer, but didn't know the compiler internals well enough to be sure)~~ - The scala macro api exposes a `typecheck` macro, which however doesn't cover (some) inheritance errors, do you know if there is something similar that includes `refchecks`? EDIT: I can answer the first question: yes, that error is caught during the `refchecks` phase. Still curious about the second one.
I don't understand the question? We declare `Some` covariant because we want to. We want it to be the case that e.g. a `Some[Float]` is-a (in the Liskov sense) `Some[Number]`. Are you asking about scala's covariance in general?
But on the type `Option[+A]` the covariant is already define. Why do I need to define `Some[+A]` not only `Some[A]`?
I have to learn Scala for work. There are a lots of guide, so I want to ask, which is the best free guide I can find for starting Scala? And a second question, for when I know a little more about Scala, any good machine learning tutorial? Thaaaanks!
Well, implementing a competitive GC is hard and it seems like [LLVM doesn't support parallel GCs yet](http://llvm.1065342.n5.nabble.com/Future-plans-for-GC-in-LLVM-td75030.html). Maybe borrowing OCaml's GC(which is famous for its low latency) and implementing a multiprocess model instead of multithreading would be a great experiment...
You don't need the homebrew version unless you especially want to be able to run the REPL directly by launching "scala". I find activator horrendously complicated, though I assume some people must feel differently. What I'd do is just install an IDE (I prefer eclipse/scala-IDE, some people prefer intellij) and install the scala plugin for it. It will download its own scala compiler/implementation. For REPL-like playing around use a "scala worksheet" (like a REPL but inside the IDE). Then just start an actual project in the IDE.
Thanks for this interesting article. You choosed spark and it seems like it becate the go to for every distributed coputation engines now. That being said, people I spoke with who tryied spark told how difficult it was to use in production. At work we decided to use akka stream + kafka when our clients need distributed systems. We pàrtition kafka according to our needs and make one instance of our akka stream application for each of those partition to read it. It prove to be much more usefull. Do you think that in their case, spark was the right tool for their needs ? What is your experience with spark ?
I think the terminology you're reaching for is [soundness](https://en.wikipedia.org/wiki/Type_system#Static_type_checking). TypeScript is a gradually typed language, and most gradually typed languages are unsound, because it would require [expensive type-checking](https://blog.acolyer.org/2016/02/05/is-sound-gradual-typing-dead/) and error handling wherever control flow transitions from untyped code to typed code. Scala's type system isn't quite sound. You still have to worry about `null`s when working with some APIs and in dealing with some class initialization scenarios. There are also a lot of type-unsafe methods in the standard library, like `Map`'s `apply` and `List`'s `head`. But, I think most people think that Scala finds a nice compromise of rigor and pragmatism.
It matters when `A` is a subtype of some type. E.g. it means you can pass `l` to a function that takes `Option[Number]`.
Ping the authors for an update. Sometimes they'll give you the latest and greatest. BTW, their work on Cats is the best, IMHO. And their other work is also top notch. Naturally, I'm a paid underscore.io ad writer in my dreams.;)
Pawel Kaczor https://github.com/pawelkaczor/akka-ddd and perhaps maybe http://www.funcqrs.io/ has sagas, but not sure right now.
Yes.
I think i read somewhere (on this subreddit?) that there is some new sbt plugin that can enable your CI to do incremental compilation between runs as well... any idea or was I dreaming?
I use homebrew, cause I don't see much point in activator (especially now that `sbt new` is a thing). However, don't install `scala` via homebrew, install `sbt` directly, it will download the appropriate version of scala for you
Any thoughts on WebAssembly and how this could cross pollinate with scala.js?
Glad you like it! It supports Cassandra collections as scala.collection.immutable.Seq, Set, and Map. The Cassandra support hasn't been used much outside of the unit tests, so please submit bug reports if you encounter a problem.
Once you find out how to handle those none async api's, could you please come back and tell us how your CRUD app benefited from all the async actions and increased complexity ? 
P.S. If the majority of your team want unit + end-to-end tests you have to comply with it. There is no more important thing than a good and healthy relationship in the team. As a compromise you could agree to have integration tests for 1-2 services that involve complex computation in order to have more confidence in regard to these services. And you never know, in the end you might be right. Or maybe not... who knows.
You can call C libraries from the JVM, it's just more difficult.
It would help if you provide a small example of what you've done in Java, even if it's only two or three of the 50 classes, including how the classes/interfaces are declared and how they're used. It's one thing to try and interpret a description and another to actually have code to work with.
Can you provide a code example with 2 or 3 of those classes, and what the method is trying to achieve?
&gt;" this makes it easy to just for each through the classes" Could you restate this? Maybe move the sentence in brackets out to a separate paragraph. It breaks your train of thought terribly.
I think a good heuristic is when your shitty toy DSL takes as much time to compile as Scala. :D 
/u/nagatofag /u/denisrosset /u/zzyzzyxx Updated post with example class.
Do I get it right that only one implementor of OurInterface should process parameter? Also, could you elaborate a bit on a while loop condition, and what is ID inside it (is it like event type)?
Another one is here https://www.youtube.com/watch?v=KGJLeHhsZBo
Regardless of the attractiveness (or lack thereof) of Spring, there is indeed no technical reason why start.spring.io shouldn't offer Scala as the language for the generated seed. It already, in addition to Java, supports Groovy and Kotlin. The code for start.spring.io seems to be https://github.com/spring-io/initializr You could add Scala support and submit a PR for inclusion. Or, if you want to sort out the chances of it being included first, create an issue. 
&gt; Scala has coroutine Which library or language feature are you referring to? 
Can you show an example of having a list of these instances, filtering it, and then applying foreach on it? Trying to get a feel for what exactly your end result looks like.
Wow thanks 
Maybe you want to fold over an input to produce an instance of ExampleClass?
/u/ItsNotMineISwear, /u/naftoligug, /u/drfisk: Here's a first pass at an attempt: https://github.com/jeffreyolchovy/sbt-check Had to crib a bit from the actual compile task to get it to work, but it should do what was specified in these comment threads. Run or reference the `scripted` test for a trivial setup/use-case. I wasn't able to make it work for the`refchecks` phase, only `typer`. And moreover, I wasn't able to make `refchecks` work even in a standalone sbt project where I simply added the flag to the project's `scalacOptions`.
If that's the case then you might be able to get away with regular function and not bother with extra classes and inheritance at all. def doStuff(so: SomeObject): Unit = { if (conditionA(so)) { doA(so); } else if (conditionB(so)) { doB(S); } // etc // or use match if that's clearer } Don't get complicated until it's necessary.
Sorry, I'm not sure what you're trying to say. Are you talking about the time to compile the parser, or the time to compile something written in the DSL? Either way, you seen to be suggesting that parser combinators are slow, and that using ANTLR will give more performance results. Are you?
This question is for you @olafurpg!
So, the only difference is probably the performance. Thanks for the explanation.
Generators (yield keyword) as a coroutines type. More in general the Continuation Passing Style is supported through http://www.scala-lang.org/files/archive/nightly/docs/library/index.html#scala.util.continuations.package I'm interested to this argument, if I'm wrong please to notify me. Thank you.
If you are looking for something similar to flask on python try akka-http or http4s. If you want something more like Django go for Play. 
Just don't catch the exception (or you can rethrow it). When you perform an action on that RDD it will throw the exception and you can handle it there. The full action will not be completed. eg.) def rdd: RDD[String] = ??? rdd.map(_.toInt) //Can obviously fail rdd.saveAsTextFile("Output") The saveAsTextFile will throw an exception if the toInt operation fails. The an incomplete file named temporary will be left at "Output"
How about [gRPC](http://www.grpc.io) via [ScalaPB](https://scalapb.github.io/grpc.html)? Here's a [Sample project](https://github.com/vyshane/grpc-scala-microservice-kit).
In regards to parsers, there are (AFAIK) two main different types of grammars (I think you could argue that regular expressions is its own major type as well, but the number of languages that they can describe is typically too limited for typical programming languages), namely: * Context-free grammars (https://en.wikipedia.org/wiki/Context-free_grammar). * Parsing expression grammars (https://en.wikipedia.org/wiki/Parsing_expression_grammar). While the syntax for grammars written in the two different grammar types can be very similar, the same grammar might describe different languages depending on which grammar type it is interpreted as. The historically most widespread type of grammars is AFAIK the **context-free grammars**, where in practice developers have used a combination of a parser type with a specific subset of context-free grammars depending on the constraints of the parser types. Parser types include LL(1), LR, LALR, etc. There are a number of tools that enables one to check a grammar as well as generate a parser from it for context-free grammars, historical ones include Yacc (https://en.wikipedia.org/wiki/Yacc) and ANTLR (https://en.wikipedia.org/wiki/ANTLR, http://www.antlr.org/). One thing I really like about these grammars is that in certain cases, you can get various checks such as ambiguity checks - if I recall correctly, if ANTLR accepts a given LL(1) grammar as valid (you have to remember to restrict the grammar to only be LL(1), since ANTLR can also accept other parser types than LL(1) - I do not know if (newer) versions of ANTLR also has support for non-context-free grammars, the ANTLR developer(s) has/have done a lot of great and continuous work on it as I understand things), the grammar is guaranteed to be unambiguous. The code generation and constraints imposed (as well as better error reporting and other features that are nice to have in production-quality parsers used for major languages) are sometimes used as arguments against using parser generators and instead (at least once the syntax of the language is nailed down) implement a parser partially or fully by hand. Parser generators can still be useful for syntax experimentation, and I personally like the ambiguity checks. For some parser types, you also get the nice property that you are guaranteed that it is possible to create a very efficient parser for the language - this is the case for LL(1) - and the parser generators typically guarantee that they generate very efficient code, at least asymptotically. For context-free grammars, parsing theory, and language theory in general, I recall the book [Introduction to Automata Theory, Languages, and Computation](https://en.wikipedia.org/wiki/Introduction_to_Automata_Theory,_Languages,_and_Computation) being great, though I might remember poorly. It is not focused on designing programming languages or writing parsers in practice, however (other relevant resources for that is probably [Types and Programming Languages](https://www.cis.upenn.edu/~bcpierce/tapl/) and the website http://lambda-the-ultimate.org/). It may also be a bit old in regards to which topics it covers, but much or most of its theory is definitely relevant still. The **parsing expression grammars** are newer AFAIK, and are typically relevant in regards to parser combinators, a likewise newer approach to parser implementation (I recall something about parsing expression grammars being invented to help describe and determine which languages parser combinators can describe, though I might remember wrongly). The main advantage compared to context-free grammars as far as I understand things is that you do not typically need a separate grammar and tool to generate a parser implementation from that grammar, but can instead simply write the parsers directly in your main development language - like the author (you?) have done here. scala.util.parsing is an example of a library based on parser combinators, and there are also other libraries using them in Scala. Haskell likewise has different parser combinator libraries. ~~One thing I personally dislike a little bit is that you cannot as such have your grammar checked for ambiguity~~ (EDIT: See /u/marcopennekamp's reply), and if you are not careful, understand how parser combinators work sufficiently, and think through things, you may end up with extremely poor performance or infinite loops in your parsers. Parser combinator libraries (at least basic ones) can also be very easy to implement, and they can also be flexible syntactically. There may also be some parser combinator variants that can express more languages than what can be describe with PEG. Finally, some also **write parsers by hand** to some degree, due to parsing speed, error reporting, extra syntactic flexibility in the language and the like, though this can be very slow and error-prone to develop, and thus is typically mostly done once the syntax has more or less been nailed down through experimentation by using a parser generator or using a parser combinator library. Of course, if a parser generator or parser combinator fulfills ones needs, one can just continue using that. While I do have a fair bit of experience in these things, I am definitely not an expert, so please be wary of any inaccuracies. EDIT: Added minor bits of formatting. EDIT2: Minor corrections. EDIT3: Also there's /r/ProgrammingLanguages/. EDIT4: Correction thanks to /u/marcopennekamp.
Finatra is built on Finagle, which abstracts Clients and Servers into an abstraction called a Service. That abstraction is intended to be protocol agnostic, which allows you to load up common things like tracing and metrics consistently across your stack. You also get stuff like client side load balancing, circuit breaking, and retries built in. There's a fair amount of learning curve, which I've seen people struggle with. Having worked with both Scalatra and Finatra, I would say go with Scalatra if you're doing JSON or using a data access toolset like Slick. If you want integrated metrics and all that, I'd at least evaluate Finatra. I'm 9 months into a project with Finatra, and I've hit more hurdles than when I worked with Scalatra. They've been easy to overcome but not everyone on my team would be comfortable knowing where to start (you often have to look at source or github issues, sometimes ask a question on gitter). If you logically layer things and expose domain services as Futures, assuming your microservices are small, you should be able to easily evaluate different frameworks. Finatra previously required Twitter's futures, but as of 2.8 (I think), it'll perform a bijection on Scala Futures to convert to Twitter Futures.
finding the balance between these two was never an easy job :)
You can't make it non-blocking, but you can isolate it by putting an asychronous boundary around it. I have a Java example showing this: * https://github.com/playframework/play-java-rest-api-example/blob/master/app/v1/post/JPAPostRepository.java
That's totally fair. To add some color to this discussion, we're a young company that is growing quickly. We currently have a monolith web/mobile application using Django. We're essentially at the point where we are starting to decouple our infrastructure into a number of pieces. Our web backends today are completely JSON driven to the outside world (via HTTP to Mobile/Web). We are looking to move to an architecture, a piece at a time, that breaks apart that monolith. Be it authentication, user management, inventory or transaction history, that kind of thing. We will likely experiment with a couple of small services first to learn and react to challenges we discover along the way. Outside of our public-facing API layer which will continue to be JSON based, our internal services will likely be able to communicate through other RPC methods, like Protobuf or something of the ilk. I think in the ideal scenario whatever format we choose will have the option to produce JSON as well. We're still discussing new pieces of platform infrastructure, be it technologies like mesos/kubernetes, or kafka for event-based message distribution. As far as the team goes, I don't think we're at the size yet where we can really take on giant infrastructure problems. Developers are largely still polyglots who are required to at least consider how and where their code is going. That being said, it should remain a minor concern for them, not a daily question, since their primary role is to improve/fix our products. I see the transition that we're thinking about as a gradual move where we will try and get experience in what works and doesn't work for us, and then invest in accelerating the plan once we better understand the technical landscape. I hope thats helpful, and I appreciate you taking the time to respond (thanks to everyone in this thread, I really appreciate your comments!)
So when you perform the map, 'nothing happens'. Not a single line of data will be processed. You're just informing spark that when it needs to, the data should be processed like so. When it needs to is when the exception is thrown and when it needs to is when you perform an 'action'. In my example the saveAsTextFile would throw an error. However if you handled the error and then did rdd.count then it would throw another error. If the processing is going to fail then that rdd is unusable. Ivr tried to cover a somewhat wide amount of stuff briefly here since I'm not exactly sure of your aim. Hope this helps.
the same old self-promotion spam again?!?
I used to really like [finatra](https://twitter.github.io/finatra/), but I think I like akka-http more. I did spray.io back in the day, and loved it, even with it's complexity, and akka-http is a great future path forward of it. For learning I'd do akka-http, for production (like long-term mainteance), I'd probably use Play. If play is completely distasteful, I'd eye [squbes](http://paypal.github.io/squbs/) which is paypal's "enterprise ready" akka-http/spray.io framework. Basically builds upon it and embellishes it with some of the enterprise things that all teams need. That being said, it depends on your team and what you're doing. Play is pretty good, but the latest version of play doesn't cooperate as well as I'd like with Cloud Foundry, and the abstraction made it hard to bend it to fit, a little. I really liked the lower level feeling that akka-http gave me, and I like challenging myself with it.
I learnt Webpack and wired it up to Scala.JS as a proof-of-concept. Freaking awesome! Compile-time validated links to assets (JS, images etc), DCE, dev/prod control, can enable lots of cool features just for prod like use hash-based filenames for everything, compression, CDN links, minification (obviously), JS bundle splitting. https://github.com/japgolly/misc/tree/webpack
Yes, but I haven't read it yet. Thank you very much for writing it. I'll read it and let you know if I have any question.
Well, it was more of a joke, but here we go: I am talking about the performance of the resulting parser. A parser combinator is usually slower than a generated parser. Both are usually slower than handwritten parsers. Usability/time-to-develop obviously go the other way around. If you write a parser by hand, you better know your parser theory and have a language with a finished syntax. If you need to prototype your language, a parser generator or combinator is better. Have a look at the following link: http://www.lihaoyi.com/fastparse/#Performance Especially the performance comparison with the Scala parser is interesting. Of course, FastParse loses by a factor of 3 to Parboiled2. FastParse is a parser combinator, Parboiled2 a macro-based parser generator. The handwritten scalac is much faster, but was probably harder to write and is harder to maintain, too. :P Another interesting aspect is how slow the scala-parser-combinators are compared to FastParse. FastParse is about 80 times faster in the JSON benchmark. It is to be taken with a grain of salt, since the benchmarks were conducted by the author of FastParse, but I am inclined to trust these results regardless. &gt; Either way, you seen to be suggesting that parser combinators are slow, and that using ANTLR will give more performance results. Are you? I haven't benchmarked ANTLR against FastParse, but I find both ANTLR and FastParse to be quite usable in their respective domains, so I would recommend both. In general, unless you know what you are doing, you should not worry about parser performance at the beginning. Using Parboiled2 instead of FastParse may get you better results, but it may also decrease your development/prototyping speed. You can still switch to a different parser later, when the language is more mature, perhaps even a handwritten one. However, I would suggest using FastParse over scala-parser-combinators, since they both roughly follow the same "parser combinator" workflow, but FastParse is, as discussed above, most likely about 80x faster than scala-parser-combinators.
downvoting... because this is low quality work. there are far better scala learning resources out there. 
Two more options: [Finch](https://finagle.github.io/finch/) and [Colossus](http://tumblr.github.io/colossus/).
Folks, please upload to Vimeo instead because: - we can download those vids for offline viewing, like in a 12 hour flight to US and back - no ads
I don't disagree, but you can roll with uBlock Origin and YouTube Video Downloader add-ons...
If you also want a swagger file, [rho](https://github.com/http4s/rho) will generate one for you.
I disagree, because I would never even think of looking for them on vimeo. Upload to both instead. 
you live in denial
It's too bad they didn't make the Scala API more idiomatic. Other than returning Scala Futures this looks like semicolon free java code. 
Thanks so much.
&gt; &gt; One thing I personally dislike a little bit is that you cannot as such have your grammar checked for ambiguity &gt; As described in the original paper, a PEG can not be ambiguous, since the priority operator inherently gets rid of ambiguities by being deterministic in choosing one alternative over another. It is noted in the paper: "As with discovering ambiguity in CFGs, however, we have the hope of finding automatic algorithms to identify order sensitivity or insensitivity conservatively in common situations." Which presents the different problem of order in the context of PEGs. Ah, yes, you are absolutely correct, thank you, it is the order-sensitivity that I did not like so much. I have edited my reply.
if your low-effort, low-quality blog posts were even a little bit interesting, someone else other than yourself would post them here. reddit exists to *share* interesting things we *find*, not to promote ourselves.
&gt; as long as they aren't low quality/effort \^\^\^ this
Excuse me, attacking people on their command of English is either classist or racist (and can be both). Moreover, your latest comment shows you don't hesitate to go to homophobia and ableism as well. You deserved to be reported, and hope you will be banned.
language is not race. i'm not a native speaker myself.
I still can see them, not that it's a good thing.
Oh, telling the truth is not an attack? Well, then. You're a douche, a jerk and an asshole. That's a fact.
Right, but I'd prefer to not circumvent the site's protections and violate copyright law in the process of viewing those videos. Not to mention it's not something you can do directly on a tablet or phone.
I do understand that you are busy. But there are people who just come in and then start abusing straight away and there is nothing is done about it. I have lost faith in this subreddit and the reddit community as a whole and I will not submit my posts here again. I have deleted my post submissions as of now until the community further decides on this. I think there should enough mods to deal with abusive people else it turns out to be a complete mess. This person here - https://www.reddit.com/user/kpws started putting abusive comments and then removes them after some time so that there is no proof that he is abusive. Just check out the entire article comments. If the subreddit decides against posting once own content, it is perfectly fine and I shall look for alternative platforms but under no means abusive people get away with that. For further communications, please DM me directly since I have deleted this post.
In my (admittedly limited, personal) experience, I discover most of the Scala presentations on YouTube. For the subreddit to even show up on my Frontpage is the exception. ;)
[removed]
Hey, thank you for reporting that kind of behaviour, but I did have to remove your post form /r/Scala for posting information about his/her real name and location. 
I see, so you would argue that there is an advantage for data processing that does not fit in memory? So then an intermediate step would be to break up your processing into steps so that each individual mapping function does not occupy too much memory.
50 nodes isn't a ton of capability for HPC, I found these slides that suggest that default spark tops out at about 100-ish cores: http://cdn.opensfs.org/wp-content/uploads/2016/04/LUG2016D2_Scaling-Apache-Spark-On-Lustre_Chaimov.pdf but can be tuned to get up to 10k-ish cores. (There are a few sub-10k machines on the latest top500, but not many....)
Scala will remain a higher-level language due to its reference model of variables--we generally never have to think about memory management in Scala, which [makes a lot of things simpler](http://softwareengineering.stackexchange.com/questions/118295/did-the-developers-of-java-consciously-abandon-raii/118444#118444).
lol has generics
Does anyone actually use Scaladex? Google is a thing...
You have to look at it at a more general level. the type is in some pseudo-notation like this: def sprintf(str: String): (X* =&gt;) String where `X` is exactly the amount of parameters you need, with exactly the correct type (so it can be different for each parameter). So the type of `sprintf("name=%s, age=%d, location=%d")` should be `String =&gt; Int =&gt; String =&gt; String`. The type of `sprintf("hello, %s")` should be `String =&gt; String`, etc... So the content of what is being printed (in the context of the post, the content of the `F`, because it is much more difficult to also parse a string at the type-level) determines what is the exact type of the `sprintf` function. Is that more clear? edit: Without this type-level stuff we have to list out all the combinations: def print0Arg(str: String): String = str def print1Arg[A](str1: String, a: A, str2: String): A =&gt; String = s"$str1$a$str2" def print2Arg[A, B](str1: String, a:A, str2: String, b: B, str3: String): A =&gt; B =&gt; String = ... etc ... `sprintf` is supposed to be the combination of all these `printXArg` functions in one. edit2: Okay I think I understand your question more clearly now, and yes in a way string interpolation already gives you some way to sort of do this, by assigning the input types as the method parameters. But the intent of the post wasn't really to convince people to use this `sprintf`, it was to teach how I ported the type family, there are a lot of other type family use cases. Just look at [shapeless](https://github.com/milessabin/shapeless).
interesting, thanks for sharing. 
just to let you know the kind of person we are dealing with here: http://imgur.com/2nSvaQn
well, I guess that's one way to handle critique :D
&gt; I started learning Rust recently and honestly it's everything I wanted Go to be...It's kind of a shame, since Rust is a great language (much better than go)... Rust and golang aren't in the same domain. The first is a system programming language concentrating on **safety** with **zero-cost abstractions** and a good-enough typesystem(not ML-level, but still better than 99% I've seen). It's designed to give you full control. The latter is ... for google's own needs(in my theory they created it to catch the masses of people who can't use generics and don't know much about PLT)... &gt; ...and I really don't think Go is more popular than Rust because of Google backing it... Are you sure? golang is there for years and Rust isn't that old yet. Also, if you look around there are plenty of [interesting projects](https://github.com/redox-os/redox) written in it. golang is aiming webdev where people rewrite their stack every year while rust is aiming system dev where people like to stay with legacy stuff. &gt; it's just that Go has no learning curve It's really easy to only learn to count to 100. It'll be enough for most of the "real-life" use cases... "How many fingers do you have?" "10!" "See? the 100-numbers system is enough for us!". This is just my edgy opinion ;) &gt; Rust has a pretty big one for most people, cuz RAII + FP. [Here](https://www.reddit.com/r/programming/comments/600rhk/the_redmonk_programming_language_rankings_january/df34qh4/?context=3&amp;st=j0ktibbz&amp;sh=bb455de3), I and some of Rust's developers and users started to discuss rust's place at high-level domains - which is probably in your interest if you've asked this question in this sub.
Really? That would be great - I will put it up on github shortly. Luckily everything so far also has a nice java/scala counterpart such as https://github.com/aimacode/ . What is your githubname I'll invite you? Mine is qorrect
Scala &amp; Rust are my two favourite languages. Both stop me from making stupid mistakes (which I'm inclined to do) - Go OTOH seems to encourage the worst in me.
Will add the links to Youtube videos.
How does scalafmt's feature set compare? Could the two be merged, at least in spirit?
scalafmt is quite different from scalariform since it has a max column setting. I understand why some people prefer a less intrusive formatter like scalariform, where you are in control of most line breaks. I think it should be quite possible to write a scalariform-like formatter (no maxColumn) with scala.meta. Such a formatter might have less maintenance burden than scalariform since scala.meta already provides an AST, a rich token api and parser (including support for sbt files and other dialects of Scala). The biggest challenge I faced implementing scalafmt was to support the max column setting, without that I suspect it would have taken a lot less time to implement and it would probably be around 5x faster. PS. I'd be happy to include this scalariform-like formatter (implemented with scala.meta) into the scalafmt repo. That way the two formatters could share integrations, decreasing the maintenance burden even more.
That's very easy in Idris - I'm thinking specifically about the last part of article &gt; Now if only we could also parse "name=%s, age=%d" as it’s corresponding F type (at compile-time) The following short implementation does all this plus 'compile-time parse' (you can find similar version in "Type-driven development in Idris") -- formatters like %s czy %d data Format = Number Format -- %d | Str Format -- %s | Lit String Format -- literal string | Ch Format -- %c | Dbl Format -- %f | End -- end marker -- "%s = %d" we are representing as -- Str (Lit " = ") Number End -- or "Answer: %d" we are representing as -- (Lit "Answer: ") Number End -- so based on Format type we're computing the type of our printf recursively -- eg. PrintfType (Str (Lit " = ") Number End) =&gt; (str: String) -&gt; (i: Int) -&gt; String -- PrintfType ((Lit "Answer: ") Number End) =&gt; (i: Int) -&gt; String PrintfType : Format -&gt; Type PrintfType (Number fmt) = (i : Int) -&gt; PrintfType fmt PrintfType (Str fmt) = (str : String) -&gt; PrintfType fmt PrintfType (Lit str fmt) = PrintfType fmt PrintfType (Ch fmt) = (c : Char) -&gt; PrintfType fmt PrintfType (Dbl fmt) = (d : Double) -&gt; PrintfType fmt PrintfType End = String -- to output result of printf we compute an instance of PrintfType -- eg. from (Str (Lit " = ") Number End) we get (str: String) -&gt; (i: Int) -&gt; String -- it works by adding formatted string to an accumulator printfFmt : (fmt : Format) -&gt; (acc : String) -&gt; PrintfType fmt printfFmt (Number fmt) acc = \i =&gt; printfFmt fmt (acc ++ show i) printfFmt (Str fmt) acc = \str =&gt; printfFmt fmt (acc ++ str) printfFmt (Ch fmt) acc = \c =&gt; printfFmt fmt (acc ++ show c) printfFmt (Dbl fmt) acc = \d =&gt; printfFmt fmt (acc ++ show d) printfFmt (Lit lit fmt) acc = printfFmt fmt (acc ++ lit) printfFmt End acc = acc -- compile-time parsing toFormat : (xs : List Char) -&gt; Format toFormat [] = End toFormat ('%' :: 'd' :: chars) = Number (toFormat chars) toFormat ('%' :: 's' :: chars) = Str (toFormat chars) toFormat ('%' :: 'c' :: chars) = Ch (toFormat chars) toFormat ('%' :: 'f' :: chars) = Dbl (toFormat chars) toFormat ('%' :: chars) = Lit "%" (toFormat chars) toFormat (c :: chars) = case toFormat chars of Lit lit chars' =&gt; Lit (strCons c lit) chars' fmt =&gt; Lit (strCons c "") fmt -- that's impossible part in Scala printf : (fmt : String) -&gt; PrintfType (toFormat (unpack fmt)) printf fmt = printfFmt _ "" It works :-) &gt; :t printf "%s = %d" printf "%s = %d" : String -&gt; Int -&gt; String &gt; :t printf "Answer is %d" printf "Answer is %d" : Int -&gt; String &gt; printf "%s = %d" "answer" 10 "answer = 10" : String &gt; printf "%s = %d" "answer" "10" builtin:Type mismatch between String (Type of "10") and Int (Expected type)
No. There are many ways to write Scala and we keep discovering more. We would never agree on what such a term should mean. This is a good thing.
better merge it with scalafmt 
I viewed is as problem when I started doing language, but now I pretty much agree. It doesn't take much to learn to understand all flavors of scala. It's just text, sematics behind it are more important.
Scalaesque?
So the ruby community was this way, and then rails came along and helped solidify some of those conventions. Would it not be good to have a few conventions? Some things in the ruby community are the "ruby way" and they generally result in higher quality code that's easier for folks to understand.
I agree, you were the victim of very nasty behavior, and you have my sympathies. I'm not disputing your conclusions (or discussing them at all). I'm disputing your reaction, and the relevance of your conclusion. I don't think we ought to have a policy that says crazy, paranoid, nasty people are not welcome. I think the policy is and should be that crazy, paranoid, and nasty behavior within the community will not be tolerated and are grounds for banning. Again, one can argue that a DM can be considered behavior within the community. But it isn't the argument you made. I was responding to the argument you did make.
I like "scalastic"-sounds scholarly ;).
The same is true for ``idiomatic`` - everybody could state that for none idiomatic code 😉
I like it as well. Some "new" things are coming up that resemble stuff from Scala-land. i.e.: https://github.com/lloydmeta/frunk/ 
We're talking about apples and oranges. The N commodity machines I borrow from amazon for a half hour have nothing to do with your list of biggest supercomputers. Also "use fortran" doesn't say anything about how you scale your algorithm from one machine to N machines. 
Also, why on earth would you use spark on a cray other than as an academic curiosity? That's really not its use case. 
username checks out
It's an impressive language, but the user interface needs a lot of work. Short list of unnecessary weirdnesses: - Generics with `&lt;&gt;`. It's 2017 by now, we know it's a bad idea. Just stop it or otherwise your language suffers from abominations like `null::&lt;_&gt;()`. - Strings don't offer indexing, because it doesn't make sense for UTF-8. Correct! But Strings offer slicing ... WAT? - Misusing`[]` for indexed access. Having both `()` and `[]` doing roughly the same thing, especially since `[]` can be used to do arbitrary things, doesn't make sense. Pick one, use the other for generics. - Completely inconsistent naming. `str` and `String`, `Path` and `PathBuf` etc. - `::` vs. `.` is kind of unnecessary. - Mandatory semicola, but with some exceptions in arbitrary places. `struct Foo;` vs. `struct Foo {}` - `extern crate` should just go away. The compiler should get the hint that I want to use that `foo` crate, after adding it to the dependencies _and_ `use`ing it in code. - Closures could be made to look much closer to functions, but somehow aren't. - "associated" functions in trait impls. I'd prefer separating them from normal functions and drop the `self`where possible. - Arbitrary abbreviations all over the place. It's 2017, your computer won't run out of memory just because your compiler's symbol table stores `Buffer` instead of `Buf`. - Can someone decide on a casing rule for types, please, instead of mixing lowercase and uppercase names? - Also, having both `CamelCase` and `methods_with_underscores`? - Library stutter: `std::option::Option`, `std::result::Result`, `std::default::Default` ... - `iter()`, `iter_mut()`, `into_iter()` ... can we just decide whether we do prefix or postfix style and stick with it? - Coercions do too many things. For instance, they are the default way to convert `i32` to `i64`, instead of just using methods. - Also, converting numbers is still completely broken. For instance, `f32` to `i32` might result in either an undefined value or undefined behavior. (Forgotten which one it is.) - Bitcasting integers to floats is unsafe, because the bits could be a signaling NaN, causing the CPU to raise an FP exception if not disabled. - Forward and backward annotations: `#[foo] struct Foo {}` vs `struct Foo { #![foo] }`. - Type bounds are `Sized` by default, with some weird special syntax to opt out (`?Sized`). - `///` for normal documentation, `//!` for module level documentation ... documentation already uses Markdown, so maybe just let people drop a markdown file in the module dir? That would make documentation much more accessible when browsing through GitHub. - Also, documentation can cause compiler errors ... that's especially fun if you just commented a piece of code for testing/prototyping. - type alias misuse: In e.g. `io` crate: `type Result&lt;T&gt; = Result&lt;T, io::Error&gt;` ... just call it `IoResult`. - Macros are not very good. They are over-used due to the fact that Rust lacks varargs and abused due to the fact that they require special syntax at call-site (`some_macro!()`). - Pattern matching in macros is also weird. `x` binds some match to a name in "normal" pattern matching, but matches on a literal "x" in "macro" pattern matching. - `println!` and `format!` are very disappointing given that they use macros. - Compiler errors ... ugh. So many things. My pet peeve: "Compilation failed due to 2 errors" ... _87 compiler errors printed before that_. Overall, it's a very impressive language with a clear vision and priorities. Also, the collection library is much better.
The problem with rails is that it made terrible patterns the idiomatic norm
If `match` were a method, you could compose partial functions using `orElse` and just pass the result in. If `PartialFunction` had an `unapply`, could it be used in this way without needing `extract`? I guess it would probably be more difficult, but could `Function[A, Option[B]]` be implicitly enhanced to have this property, too? It would also be nice if it were possible to capture the idea of a pattern match as a type, in and of itself. It can feel weird to use `PartialFunction` in cases where you don't actually care about the output of a match, but only want to know if a match succeeds. I've seen this before in Scalatest with the `... should match { case blahBlah =&gt; }` construct. A more compact `exists` that works off patterns could be another use case. As someone who loves `collect`, I'm all for enhanced matching.
scalaskell ;)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust] [What are your thoughts on rust? • r\/scala](https://np.reddit.com/r/rust/comments/60ygb2/what_are_your_thoughts_on_rust_rscala/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
reasonable 
I am not gonna abandon Scala for Rust or any other language any time soon, if I were I'd go clojure because despite having a better ML background I gravitate towards lisps way of thinking, I have a lot of fun writing clojure (even with cryptic ass errors), and lein is amazing. But that's not gonna happen, Scala is the best language for my needs, in spite of the fact that plenty of communities are more interested in those needs. As for HKT dedetrich cleared that, I actually use HKT in scala often, but I can live without them in Rust.
&gt; null::&lt;_&gt;() There is no null in Rust, so I'm not sure what you mean. Do you have another example that shows the issue you're referring to? I've written a good deal of rust and have yet to be put off by usage of the turbofish (::&lt;&gt;) Some of these pain points are being addressed (like extern crate) while for a lot of others, at least for me, don't cause any real-world pain.
&gt; Scala FP From an FP perspective it's nothing new, it has less FP support than standard lib scala, let alone scala + typelevel shit, RAII would be the bitch but it's easier to learn in Rust than in CPP. Also it won't compile unless the code is totally safe. 
Stutter occurs *a lot*. `use foo::Foo;`, *&amp;c.* These three are poor examples because they’re in the prelude, but for other types I think it is a genuine papercut of Rust (but I’ve never come up with a good way to avoid it).
Rust’s convention is to module-prefix functions but not types—but there is an exception with names like `Error` and `Result` so that the recommendation *is* in fact `io::Error` and `io::Result` rather than `Error` and `Result` as it would otherwise be.
&gt; pure functional data structures in Rust is quite a bit harder due to having to deal with stuff like cyclic references I'm not sure how this is an issue, since immutable data normally can't have reference cycles anyway. In my experience, ref counting works pretty well for persistent data structures. Actually, I think ref counting is superior to gc for persistent structures, because it allows you to avoid copies when the ref count is 1. This means you can write persistent structures with asymptotic performance that is at least as good as the non-persistent equivalent. With gc, you have to copy unconditionally. 
Presumably, they're referring to std::ptr::null.
1. `Function[A, Option[B]]` is supported as well 2. You can create `PartialFunction[A, Unit]` for `exists`: `Seq(1).exists(({ case 2 =&gt; }: PartialFunction[Int, Unit]).isDefinedAt)`
Rust style traits (in Scala we call them typeclasses) *can be implemented* with the help of implicits. To some extent, implicit based typeclasses even more powerful than Rust. for example, with implicits you can swap out an implementation, whereas with Rust you can't (at least not until specialization makes its way into the language). But they're also more boilerplate-y, and don't exactly hit the same use cases. Implicit classes can also function in a very similar way to Rust traits, and they have much less boilerplate but can also be more dangerous if used carelessly. But that's not the only thing they're used for. There quite a few [powerful patterns](http://www.lihaoyi.com/post/ImplicitDesignPatternsinScala.html) enabled by implicits, and that's just an intro. More advanced uses enable things like [type safe builder patterns without exceptions](http://blog.rafaelferreira.net/2008/07/type-safe-builder-pattern-in-scala.html), meaning you can make it impossible to compile a builder that doesn't meet the requirements of the builder. Also, scala has some of the most powerful [generic programming](https://github.com/milessabin/shapeless) facilities around, largely thanks to HKTs and implicits. 
So does Scala. What's you're point?
Can you give a concrete example? I don't recall ever using the `Foo` type. :P
Rust programmers use .fold a lot. While we do have opt-in mutation but we don't a c like for loop, so you'll likely reach for fold after you've written enough imperative code.
 use collections::HashMap; use collections::hash_map::Entry;
Yes I agree. There are those who would like to write Scala in a Haskell style so perhaps it applies to that style, but it can't really be done and excludes some of the advantages Scala has over Haskell.
Great. Lots of jobs. Google functional programming jobs and see how much % scala takes.
Correct. Without a max column setting, I estimate scalafmt would be at lot faster and not fail on deeply nested code.
Our dev agency uses Scala as our main back-end language, along with Akka HTTP. I'm actively hiring for Scala devs right now, and it's not easy. It's a developer's market for Scala devs right now - you could find a job in your sleep.
&gt; The ! lets the annotation/documentation apply to the enclosing item. I don't really see an issue with having both. If you don't like one don't use it. Not to mention, last I checked, the consensus in the effort to come up with a default style guide for `rustfmt` was that `!` should be limited to only documenting the file as a whole and there are a lot of things you can document where there is no block to put the `!` form inside inside. (If it's a wart, it's the least warty of the options which satisfy all requirements.)
&gt; A feature I'm missing is an explicit way to guarantee TCO. The `become` keyword is reserved for implementing "TCO this call or error at compile time" and it's still an [open issue](https://github.com/rust-lang/rfcs/issues/271). There's even an [effort ongoing](https://github.com/rust-lang/rfcs/pull/1888) to solidify and implement it. It's just something that got postponed when 1.0 came onto the horizon because implementing it isn't a breaking change.
It's definitely an improvement over C++ IMHO, and the lack of null pointer exception is nice, but the syntax, the lack of GC and the borrow checker makes it awkward to use compared to Scala for example. For example something as simple as this Scala example: import collection.mutable.HashSet class Player(val game: Game, val name: String) { val friends = new HashSet[Player] } class Game { val players = new HashSet[Player] } requires the use of Rc, Weak and RefCell: use std::rc::{Rc, Weak}; use std::cell::RefCell; use std::collections::HashSet; type GamePtr = Rc&lt;RefCell&lt;Game&gt;&gt;; type GameWeakPtr = Weak&lt;RefCell&lt;Game&gt;&gt;; type PlayerPtr = Rc&lt;RefCell&lt;Player&gt;&gt;; struct Player { game: GameWeakPtr, name: String, friends: HashSet&lt;PlayerPtr&gt; } struct Game { players: HashSet&lt;PlayerPtr&gt; } And it will still not work with HashSet because Hash is not implemented for RefCell. RefCell also imposes a runtime cost because the borrowing rules are checked at runtime instead of compile time, even if I use this in a safe single threaded environment.
I've only ever used Scalariform. Does scalafmt do weird things with code coverage like Scalariform does? I recently found that, with Scalariform enabled, I couldn't get coverage details (via scoverage) for multiple tasks, such as `sbt test:test fun:test`.
You could rewrite this as: let v : Vec&lt;_&gt; = some_iterator_expression.collect(); 
I've used "Scalatastic" or "idiomatic in/for Scala." Howsabout a name for Scala developers? There's Rubyists, Pythonistas, Haskellers, etc. I've called Scala developers "Scalatypes" for years. It seems to go pretty well with the whole types thing.
As long as you don't expose state outside of your method you are fine. I'd probably use while if the loop is really tight. Yes, prefer immutable stuff, in any language, unless you've profiled and copying allocations are really killing you. That's not scala good practices though, that's just good programming practice. If you don't have to track changes in your head over time, you can think about more important things than your assignment order. I think you are wasting time if you don't use typed generic fp in scala. There are equally suitable language equivalents that won't require you to change the way you think about problems and program architecture. Why use a language of its features are of no benefit to you? But fully encapsulated oop with message passing is fine with me too, just nobody in the Java or .Net world actually does that. Imperative programming without abstraction is the complete opposite way from the way we should program, and yet that's what I keep hearing is easy to understand. That, in the large, is not good in any language that offers encapsulation. Meaning it has no place on the jvm. 
London
!cj: There's a dismissive copypasta floating around about Go ("lol no generics") which, along with Rust, is one of the most circle-jerked languages at the time. So 'no generics' becomes 'has generics' for Rust. cj: real programmers write FORTRAN in Scala 
That is actually what I do on the receiving end, but it often isn't possible on the sending end, which I think is unfortunate and is mostly down to the type inference not being quite as bright as I am. (Sorry for the weird terminology, but "receiving end" and "sending end" is all I can come up with.) Not that I'm all that bright. The most common case where I see `.collect::&lt;Vec&lt;_&gt;&gt;()` being required is when I have a function that returns a vector that I first collect and then sort, or something (I don't remember), before returning--it isn't immediately returned, so the inferencer-er-er doesn't put two and two together that I want it collected into the return type for the function.
Yes, I know these arguments, however I don't use any enum in my example, and while the iterator invalidation problem might occur (also in Scala, causing an exception), I don't think it's worth having the inconvenience at most one mutable reference rule just to solve it in the single-threaded case. However, for multi-threaded programs I definitely think the Rust reference rules are useful.
That sounds like an integration bug in sbt-scalariform.
Thanks yeah when I asked this I wasnt sure what I wanted it to do still not sure. As in if some of the data is different should I isolate the data or kill the spark job. I'm still not sure but I am thinking that if the amount of data which is different is only a few out of 100's of records then I would just catch the exception and write the all correct data to the db. 
Many crates have an equivalently-named type inside them which is really all people want (and is commonly, though not always, the only thing actually in the crate). `use anymap::AnyMap;` is a concrete example.
Chris, that's *your* library. :P The Rust developers don't have a whole lot of ability to influence third-party library authors!
This is exactly what I mean: you don't need to type `use std::collections::hash_map::HashMap;`, because it's deliberately re-exported as `std::collections::HashMap` in order to avoid stutter.
Autistic
Here is a potentially good advice.... A long time ago I was studying in school. I was learning C++.. someone told me that Visual basic has a lot of "potential". I threw away all my C++ books and learnt Visual Basic. I got a very high paying job and at that point I felt I have made the right decision. Years later I realized that what a bad decision I made. I sucked as a developer and as a technologist. The world changed very fast and no one needed VB anymore. Suddently people started talking about Performance, Concurrency, Parallelism, Throughput.... and all i knew was drag and drop. If I could build a time machine and go back. I would tell my younger self that never pick up a language which has "potential"... today something has potential ... tomorrow not so much. Learn something which you are comfortable solving problems with. In school I could have learnt Prolog, LISP but instead I wasted that opportunity. If you go this way, you will explore more and become a better engineer... (IMO).
I doubt that Go is popular merely because Google is backing it, and Google certainly isn't backing Go much at all. They are pretty distant with it. They could have pushed for Go as a first class language for their Android platform, but they aren't.
Note that glob imports are discouraged for much the same reasons as in Python.
In Melbourne, Australia I even can't find some companies using Scala and very hard to get a job.
That's pretty surprising! Our office is near the startup sector in London - Scala and Akka is pretty common among fintech startups. The last few larger companies I've worked for had Java stacks that they were refactoring to Scala - I figured that was a pretty common occurrence!
Functional programming is not just about style/syntax. It's a whole different way of thinking. I just finished reading "Functional and Reactive Domain Modeling" which gives a fantastic overview of how to think of domain modeling from a functional perspective. It has many examples and code snippets that in my opinion use a good coding style as far as Scala goes.
Just what I was looking for. Thanks!
I'm sure you will also find "Functional Programming in Scala", aka the red book, to be an excellent read. Though make sure to really dig in to the material and thoroughly do the exercises for the best experience. 
Thanks! Yeah, agreed it's much more than syntax. We've been working with a Scala / Spray / Akka stack for a couple of years, so pretty clear on the nuts and bolts. We just occasionally stray into Java-like patterns and that's something I'd like to avoid. I'll take a look at that :)
Some crate authors use re-exports + globbing to provide a *crate prelude* (e.g. rayon). I personally try to avoid those crates if possible. I like knowing exactly where the types and traits I'm using are coming from. I assume it makes life easier for tools like `racer` too.
What you are doing there is a fold: ``` something.foldLeft(null)(foo)``` You can replace `null` with the initial value of `state`.
I'm not the author (I believe /u/mergeconflict is); I just thought this was a good articulation of how functional programming takes what many consider good design principles (in this case, [loose coupling](https://en.wikipedia.org/wiki/Loose_coupling)) to their logical conclusions. The [expression problem](https://oleksandrmanzyuk.wordpress.com/2014/06/18/from-object-algebras-to-finally-tagless-interpreters-2/) and its solutions are worth keeping in mind when thinking about this, though.
I'm not sure if this is how you meant it, but: There are both single-thread non-atomic and multithread-atomic refcounting smartpointers in Rusts std lib.
[removed]
Thanks, sounds promising.
Same experience with Akka . I am also working on a better architecture use just functions. The book is good, although I haven't gotten to the point where I have a complete picture in my head. Will be curious of your results. If I get an example app together I will post it in gh 
Well, ref counting is famous for its worse performance/latency/complexity. But itt has a smaller overhead...
&gt; I’ve learnt to beware of implicits. While they enable important use cases, they are used way more than they should be. I wasn’t able to completely get rid of them in the ostinato codebase (partly because I was bound by the superclass’ method signatures; thanks OOP), but overall I believe I’ve tamed the beast and learnt a valuable lesson. I'm not sure what that lesson exactly is, it's the first time and last time you've mentioned implicits. I'd be willing to bet you have certainly learned the wrong lesson.
Basically, self-referencing structures with pointers pointing up, down, and all around are bad programming practices. In your simplifed example, the following would work easily: struct Player { name: String, friends: Vec&lt;UID&gt; } struct Game { players: HashMap&lt;UID, Player&gt;, } impl Game { fn get_friends_list(ids: &amp;[UID]) -&gt; Vec&lt;Player&gt; {} fn get_friend(id: UID) -&gt; Player {} }
The best resource is really Programming in Scala, Third Edition. Man that book is worth its weight in gold!
&gt; Basically, self-referencing structures with pointers pointing up, down, and all around are bad programming practices. Why? If all the references are updated correctly (basically private fields with helper methods) I don't see how this is a bad practice. Your solution has really bad performance (requiring map lookups and creation of temporary vectors) and it's inconvenient because a lot of the time you have to pass along the corresponding Game reference with a Player reference.
Is that a complement, or alternative to ScalaCenter's scalajs-bundler?
Thanks for the blog link, I'll check it out! Your service looks cool, I have Sonar set up for static analysis of Scala though.
We actually have a Jenkins plugin with some kind of quality gates. If you are interested in testing you can always try a discount with the sales team for the enterprise version. Send an email to mark at codacy dot com if you are interested.
If you have: trait X trait Y trait A { def foo: X } trait B { def foo: Y } val x: A &amp; B = new A with B { def foo = new X with Y } Then `x.foo` has type `X &amp; Y` Scala does not allow unrestricted multiple inheritance, and in the situations where multiple inheritance is allowed, the order of the parents of the class does not affect the types of the class members, so `new A with B` and `new B with A` will both have the same type (that can be written `A &amp; B` or `B &amp; A`), no matter how you define `A` and `B`.
My understanding of the above post is that `http4s`, `doobie` and `Matryoshka` will, in the future, only support `cats`, i.e. excluding `scalaz`. Given that assumption, what choice makes sense for a team with an existing `scalaz` code-base: 1. Migrate all `scalaz` to `cats` 2. Use the above library * From a maintenance point of view, how reasonable is it to expect a full converter to be kept up-to-date with the latest `cats` and `scalaz`? * Note - I have not used the above library, nor have any assumptions about it - only questions. 3. Don't use or rip out those libraries from their code? For a new project, it seems to me that a team has the incentive to choose `cats` over `scalaz` if they will use the above libraries. What do you think? Note - I have no dog in the race, to so speak. I'm just interested in the above questions. 
&gt; For a new project, it seems to me that a team has the incentive to choose cats over scalaz if they will use the above libraries. What do you think? I'm not sure what the incentive is here you're referring to.
For actual interoperability, there should also be a unified story for purely functional IO as the base monad for effects. The current solution proposed in cats' FAQ (choose one among FS2 and Monix Task) is unacceptable, especially in light of the fact that side effects cannot be abstracted over.
Doobie and http4s are using FS2's Task -- e.g., https://twitter.com/http4s/status/845375093345861633
I used to use reflection based deserialization, but it caused far too many headaches. I usually stick to Circe and Argonaut now. Those use typeclasses, and usually you can get away with macro based deserializers, so there's minimal boilerplate. Reflection is scary and unpredictable; these are not. 
Currently scalaz does have more tools (and its definitely more stable), but on the former point its also more of a design choice Cats has an explicit goal to be more minimal and modular, and so things (such as lenses) are intended to be provided by other libraries rather than being included in the core (https://github.com/julien-truffaut/Monocle is a lens library that is also going to migrate to cats and Scalaz 8 is going to have optics in its core anyways) Likewise extra data structures is being done in dogs (https://github.com/stew/dogs) which again is not part of the standard library. Honestly this difference I am huge fan of, currently working on a codebase at work which used to have 3 either types (scala `Either`, cats `Xor` and Scalaz `\/`) and it was an annoying nightmare, especially for new people working on the codebase.
djspiewak is also working on a new version of shims that gives you 2-way interop with a single import.
There is some talk of pulling out a library with typeclasses that abstract overt the basic operations of IO-like types (constructing primitive operations, trapping/raising exceptions, asynchrony, etc.) but as you say these are lawless, although they could be characterized in terms of tests that rely on observing side-effects. We're doing an unsession on this at NEScala today so I'll report back. As for this announcement in particular both http4s and doobie use fs2 for streaming and will use fs2.Task as the IO type.
Presumably the incentive of all the FP libraries moving to cats.
https://lichess.org/ is also in Scala: https://github.com/ornicar/lila
For example what are some Java-like patterns you'd like to avoid in Scala?
something else in same category: http://www.originate.com/library/scala-guide
I think it would be better if the signature was A =&gt; Traversable[B], then you'd be able to implement backtracking, I can't count the number of times I've tried to implement set match { case Contains(3) =&gt; _ } With Traversable instead of Option that would be easy, would need some compiler smarts so that it's optimised for Option (and Boolean).
&gt; Given that we're committing to long term binary compatibility of the fs2-core and fs2-io JARs I think this makes a big difference, and it addresses most of my concerns. The remaining concerns I have are more of a personal preference (i.e. pretty useless :)), but it was always kinda quirky that the de facto IO monad of the scala fp ecosystem was in scalaz.concurrent, so I find it equally quirky that the new de facto IO monad is in a streaming library. In my ideal world, we would define an IO datatype and a concurrency (and stack safety, and scala-js) aware runtime system, standardise on it, and use it as the base monad for effects, as well as to define concurrency primitives like MVar and ideally STM, concurrency tools being another important thing that the cats ecosystem seems to lack (even though scalaz.concurrent never really took off because, funnily enough, some of the things there are defined in terms of scalaz.effect.IO, and some in terms of scalaz.concurrent.Task, i.e. the same multiple IO problem we are discussing here)
The decision to standardize on Cats and drop support for Scalaz will reduce the amount of work that the (unpaid) developers of http4s have to invest into each release, while not reducing the functionality of the library (only, potentially, its market). Most who contribute to open source probably empathize with how difficult it is to design and maintain for multiple foundational libraries—be they FP libraries, or JSON libraries. Choosing to support just one makes a lot of pragmatic sense, and Cats has certainly matured to the point where it's ready to see production use. As with everything in the world of open source, if you don't like this decision, you don't have to settle for it—but you do have to roll up your sleeves and get to work. Whether that's maintaining a branch of http4s for Scalaz, creating an alternative to http4s that is compatible with Scalaz, or working on one of the shims projects until they reach a point where they solve these concerns for library authors. As for Cats versus Scalaz, both are quite nice, but they are about to diverge significantly. Cats, derived from the Scalaz 7.x lineage, has embraced subtyping for modeling type class hierarchies, a decision that has historically led to a lot of pain for many developers (but promises to improve with coherent type classes in Dotty). Meanwhile, Scalaz 8 is eschewing subtyping in favor of a new approach, the large-scale consequences of which are not yet clearly understood (at least, by me). While the existence of two FP libraries is painful for library authors, recall that in Javascript, there are more than a thousand UI libraries, dozens of which are actively maintained and extensively supported. The diversity in the ecosystem allows Javascript to rapidly and extensively explore the consequences of different design choices, which has led to fast evolution and a lot of excellent hybrids built from the learnings. Two FP libraries that are basically the same don't really help anyone, but two FP libraries that take radically different approaches can be useful (if painful) in exploring different techniques to minimizing the pain of doing purely functional programming in Scala. Honestly, I hope it does not stop at just Cats and Scalaz, because an FP library built closer to the category theoretic roots of modern FP abstractions would be an interesting direction, and something I might prefer to the watered down abstractions in current libraries; and yet another library focused on leveraging Dotty to reduce pain and boilerplate would be equally interesting. In summary, the decision for library authors to just support one of Cats and Scalaz is completely understandable, is by no means the end of the world (those on Scalaz can maintain or develop an alternate http library, or migrate to Cats), and the growing differences between Cats and Scalaz could provide the entire Scala community with useful information on how we can simplify pure FP in Scala.
Yes, that's me :-)
This talk was very confusing because he kept saying things "may execute in parallel" but the whole point of writing a Future is that you want things to execute on different threads right?? What's the point of starting a Task when that Task is running on the same thread as me? I also found the hybrid nature of Task (Cats Eval + Future) confusing. why to merge the two? the task behaves and acts as if its a future and an eval.
Hello /u/SystemFw, I am the lead developer of [Monix](https://monix.io/). Let me share my opinion and let me make this clear that this is only a *personal* opinion ... &gt; The current solution proposed in cats' FAQ (choose one among FS2 and Monix Task) is unacceptable IMO, standardization means stagnation. Examples: 1. Scala actors from the Scala library were broken and haven't flourished until they were stripped out and redesigned into Akka 2. the Scala collections are being reworked, because a lot of decisions have been proven bad, even if done in good faith and with initial good results - and this will break a lot of people's code, even if (I'm sure) they'll try to preserve source compatibility as much as possible - e.g. have you ever [noticed this little gem](https://github.com/scala/collection-strawman/issues/50)? 3. show me any reasonable JSON or XML parser integrated into any standard library *ever* and I'll eat it "Unacceptable" is a strong word and I find some user expectations to be unreasonable. &gt; especially in light of the fact that side effects cannot be abstracted over The reason for why side effects cannot be easily abstracted away is because these data types tend to have optimizations based on certain priorities. For example FS2 values the correctness pushed forward by FS2's streaming abstractions, with their Task implementation being built to support that streaming. The Monix Task on the other hand values scheduling fairness and cancellation, being a little at odds, at least currently, with the values of FS2's authors. I am not convinced that we can't build at least some bridges. After all, [at least one precedent](http://www.reactive-streams.org/) exists and if Java developers can find that middle ground with less expressiveness than we have, then either we can do that too, or there must be something wrong in our approach for abstraction. And the contrast gets actually appalling when comparing streaming approaches. I could name problems that are a joy to solve with Monix Observables, which are built for reactive programming, the same kind of problems that are hard for FS2 to tackle, but at the same time I can think of operations that are much easier to implement in FS2 due to its pull based approach. And in regards to this situation, I am not mature enough to realize whether that's a good or bad place to be in and if one of them will win, of course I wish the winner would be Monix. And if that won't happen, assuming my beliefs for the "*right way*" to approach this problem remain unchanged in the future, then I'll do what open source developers always do best: I'll roll up my sleeves and get to work, creating even more choice 😏 But regardless of that, I must say that even though I admit that choice can be bad for the ecosystem (e.g. the paradox of choice and all that), if I wouldn't have valued some choice in how I can approach problem solving, then I wouldn't have picked Scala, I wouldn't have become a believer in Typelevel and I wouldn't have sought membership for Monix. Of course, Typelevel projects will naturally seek collaboration and standardization. I'm honored to have met many Typelevel members at NEScala and this is a community of really smart, hard working and reasonable people, so I'm sure that we can find solutions.
That's more or less what it is. Think of it as a lazy Future. From this you have many benefits. I'll be at Scala Days btw, presenting this, including new developments.
wow! the man himself!! The Cancellable and the Delay Scheduling is very cool!!
Cats will have bincompat guarantees when version 1.0 arrives, which should be soon. But yes that's certainly a valid concern.
Here is another piece of gold from martin - https://www.youtube.com/watch?v=WxyyJyB_Ssc (Compilers are databases)
I don't think rust can be compared with scala on the JVM. A better comparison would be scala native. With that said, I am not quite sure FP can be done without a good GC backed runtime. Problems without GC - Circular references - Atomic reference counting, this could have impact on performance in highly concurrent/multi-threaded environments since it needs locking on the variable counter 
&gt; IMO, standardization means stagnation. Examples: Actually, this is a pretty black and white viewpoint. Too much standardization is a problem, too little standerdization is also a problem. Case in point, with Scala *not* have a JSON parser (or even AST), we now have 7 JSON libraries, all of which differ in trivial ways and which makes JSON interopt within libraries an absolute nightmare. Also have a look at the story of String in Haskell, which is a complete joke. There are certain types which are so common that you have to have standardized in some way, the question is finding the write abstraction. For JSON, as an example, there is a current proposal at https://contributors.scala-lang.org/t/scala-json-ast-sp-proposal/175 with implementation at https://github.com/mdedetrich/scala-json-ast. This library however has very strict design in terms of only providing what is necessary, and was done in contribution with a lot of the current Scala community. The issue with the current JSON library is that it was introduced for the wrong reasons, i.e. it was providing a proof of concept (parser combinators). This is the exact same issue with the scala enumerations, which were a proof of concept of path dependent times. These libraries were introduced for all of the wrong reasons. I do agree that a Task type in general is probably too large too put in scope of the new Scala platform, however I don't think that have both a pull based (fs2) and push based (monix) in the Scala platform is necessarily a bad idea
Apart from the good answers here. I would also recommend taking a look at open source code bases such as - Apache spark - Apache Kafka - Akka Even try contributing to one so that way you will get a lot of peer reviews and your code quality will improve a lot.
Nice :-D
On android chrome the sidebar gets in the text and it wont go away making the blog unreadable
Blending some of my key interests, had to check if this was /r/scala or /r/networking.
Which feature is that?
Let's call it how it is. Cats exists for the sole purpose the authors dislike Tony Morris, it's as simple as that. They created a new library and the Typelevel umbrella to compete. Cats exists and was created for no other purpose other than tit for tat personal disagreements. The documentation of Cats while more than Scalaz is little more than a few paragraphs on each data type / type class, at the minute it doesn't live up to what was proposed. On a technical level Cats is less complete than Scalaz by design for it's goal of being friendly easy to start using. It's absolutely understandable that library maintainers cannot support both Cats and Scalaz, it's a maintenance nightmare and compromises will always have to be made to support both. It's also absolutely understandable that libraries under Typelevel choose to go with Cats which is a Typelevel project. As an agnostic library user who does not care about the library histories but use based purely on technical functionality you have to start to wonder about Typelevel's moral high ground and maybe what some of what Tony Morris rant's about has some basis. Fork the code, create a competing library, onboard projects, force the hand cut out the other project. As a end user you are worse off, use old unmaintained projects or migrate to Cats sacrificing the functionality you had available for someone elses personal battles. Realistically businesses will always take the safe option, migrate to Cats it is most widely used by some core projects. As a huge Http4s fan and light user of Doobie I understand the choices but disappointed they've had to move to Cats for non technical merit but political.
Community culture issues aside, I chose cats over scalaz for my current project (and I would, even if I wasn't a Spire maintainer). cats has a better story for the integration of numerical code: the semigroup/monoid/group hierarchy is distinct from the additive and multiplicative groups, and the naming of operations is consistent (combine/empty, plus/zero, times/one for these groups). Scalaz, on the other hand, mixes naming schemes (append and zero in Monoid for example). Typelevel maintains a fork of the Scala compiler which fixes several issues relevant for FP, so they have a better story to push FP in Scala. The momentum behind cats-derived projects is currently stronger in the areas I care about. I prefer being part of a community that values friendliness (be it MINSWAN or some reasonable code of conduct), because contributors to my projects are going to be impacted as well --- it's not a question of who was right in the past, but what the present atmosphere is. I don't know Tony personally; cultural issues apart, I am grateful for his work bridging Haskell and Scala.
&gt;[**Hands-on Dotty — Dmitry Petrashko [99:24]**](http://youtu.be/aftdOFuVU1o) &gt;&gt;Dotty is a platform to try out new language concepts and compiler technologies for Scala. Learning from experiences with scalac extensibility, we have made a huge step forward making dotc a true pleasure to work with. This talk will be an introduction to internals of Dotty compiler. &gt; [*^Scala ^World*](https://www.youtube.com/channel/UCc0j7uOItUDh7vEvPb-TeCg) ^in ^Science ^&amp; ^Technology &gt;*^6,604 ^views ^since ^Oct ^2015* [^bot ^info](/r/youtubefactsbot/wiki/index)
The free monad is great to model your code with the interpreter pattern. Rúnar Bjarnason has some great talks about it, you might find them useful. Macros are confirmed to "go away" at some point, or be replaced at the very least. So you'll have to rewrite your macro based code eventually.
You've edited your post :-) If you want answers, you have to ask these questions in ways that I can see. We also have a Gitter channel btw: https://gitter.im/monix/monix &gt; "may execute in parallel" but the whole point of writing a Future is that you want things to execute on different threads Not necessarily. `Future` is about modeling asynchronous computations and as proof it is extremely valuable on top of Javascript as well, an environment not capable of multi-threading. Parallelism doesn't come without a cost, the cost of synchronization and context switching. Jumping threads means jumping between CPU cores, which destroys cache locality. Etc. The pro-Future argument for always executing stuff asynchronously is *fairness*, meaning that if multiple future producers have committed callbacks, all of them will get a chance to run and have their futures completed. However that's also inefficient. So what the Monix Task does is to prefer execution on the current thread, by default, for those cases where it can do that. If you want parallelism, you have to be explicit about it (e.g. `Task.fork`, `Task#asyncBoundary`, `Task.sequence` versus `Task.gather`). But then for *fairness* what's unique about the Monix Task (compared with Scalaz's Task) is that it does batch processing, so when modeling a `flatMap` loop, it will execute the first N iterations on the current thread and then fork, thus forcing async boundaries automatically, thus increasing *fairness* without performance degradation. See the info on the [execution model](https://monix.io/docs/2x/execution/scheduler.html#execution-model) for details. &gt; What's the point of starting a Task when that Task is running on the same thread as me? `Task` can suspend the execution of side-effects, can be composed with other tasks seamlessly and you can choose to not execute stuff on the same thread as you. And again, both `Future` and `Task` are about *asynchrony*, not about *multi-threading* or even parallelism. But then if you want parallelism, Monix gives you better options because of `Task`: https://monix.io/docs/2x/tutorials/parallelism.html &gt; I also found the hybrid nature of Task (Cats Eval + Future) confusing. why to merge the two? Because being lazy has many benefits for many use-cases. Especially if you want to do *functional programming*. I gave a talk recently about "Functional Programming Design". Unfortunately it wasn't recorded, but you can see my slides here: https://speakerdeck.com/alexandru/fp-design-a-tale-of-monix
 I'm trying to write a JSON microservice in Scala that runs inside a Docker container and manages pre-configured CUPS Daemon so that web applications in my company can separate out managing printers and interfacing with printers to actually using printers, which can be made a simple HTTPS post instead now. What's the "best" (yes, I know that's subjective) mechanism for starting other programs from Scala? Google suggests the use of this !! operator: http://alvinalexander.com/scala/scala-execute-exec-external-system-commands-in-scala But I don't know, and I'm not a fan of the syntax and it's confusing how I extract stdout and the result of the call at the same time.
&gt; confirmed to "go away" at some point, or be replaced at the very least Woo, scare quotes. I think it's fair to assert that macros are not going anywhere. There is demand, and there is work on the next generation macro system, Scala Meta. Anyway, that's what the OP said: &gt; in Scala the only reasons I can see for not using Macros is because it's labeled as experimental. I would agree to that. When Scala Meta has stabilized, I guess we'll see macros used a lot more liberally across code bases. 
See also _Polymorphic Embedding of DSLs_ for an approach centered on Scala and object algebras.
By the way, Haskell does have macros and they are even hygienic. See https://wiki.haskell.org/A_practical_Template_Haskell_Tutorial My extremely uniformed belief is that the Haskell community discourages macros because they like doing things through the type system as much as possible. I believe I have even seen pushback in the TypeLevel community about the addition of macros to Scala. It's a shame because I think macros are a great tool.
Hi, Reddit guys, recently, we released [DeepLearning.scala](http://deeplearning.thoughtworks.school/) 1.0.0. Along with the library, we created [a series of tutorials](http://deeplearning.thoughtworks.school/doc/) for Scala developers who want to learn deep learning algorithms. The tutorials are based on DeepLearning.scala. Every article is a runnable DEMO. We encourage you download this tutorial and run locally on your own computer. We assume the readers already has some experiences in Scala. The tutorials focus on the deep learning algorithms and how to implement them. We understood our tutorials may contain mistake or bad English writing. Feel free to create Pull Request at [this repository](https://github.com/ThoughtWorksInc/DeepLearning.scala-website) if you want to improve them. Any contributions or ideas are welcome. I hope these tutorials may help if you are interested in the area.
Why not both free monad and macros? http://rea.tech/business-friendly-functional-programming-part-1-asynchronous-operations/ After you read the article, you will be able to model many of your DSLs into one universal DSL, with the help of monads.
Whoops. I meant to link to template Haskell. Thanks for the correction.
Of course! I actually just didn't know if Husk had added something that I wasn't expecting, or perhaps Husk used it's macro system to generate Haskell itself (tho I hadn't seen it, that doesn't mean I couldn't be wrong).
On the topic of Free Monads, what is Paul Chiusano's point in the following? &gt; Any time tempted to use Free, ask self: am I going to be doing &gt; any interesting analysis of the Free? ... https://twitter.com/pchiusano/status/845306411928223744
Here's a very big reason not to upgrade: https://youtrack.jetbrains.com/oauth?state=%2Fissue%2FJRE-205 There's fontconfig workarounds but this shouldn't have made it to release in the first place..
Cmon.... If you working on Linux you are used to ugly stuff... :) 
Akka has a great built in FSM library, but other than that from scratch. If there's existing libraries they are great I'd love to hear about them...
awesome! Thanks for the detailed response.
React, react-router, and react-redux facade for Scala.js. Trying to get the APIs as close to the original as possible. https://github.com/shogowada/scalajs-reactjs
You are right to the extent that its a new feature so I haven't totally mastered it. My first impressions were that the SBT tab in IntelliJ is generally useless and the sbt console didn't add anything to it. I may be wrong and I will try to learn/use the technology if I can.
You misread - it is `O(min(idx, n-idx))` not `O(min(n, n-idx))`. For many practical uses (e.g. appends/prepends or inserts/deletes at front or back of large lists) is very fast.
Here's some stuff I found useful, mostly specific to Scala: Worth reading to get started, at your leisure * [Programming in Scala](https://booksites.artima.com/programming_in_scala_3ed) (Martin Odersky) * [Functional Programming in Scala](https://www.manning.com/books/functional-programming-in-scala) (Paul Chiusano, Runar Bjarnason) * [Ammonite docs](http://www.lihaoyi.com/Ammonite/) (/u/lihaoyi) * [Principle of Least Power](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html) (/u/lihaoyi) Worth reading afterward, whenever they start to sound interesting * [Learning Scalaz](http://eed3si9n.com/learning-scalaz/) (eugene yokota) * [Futures and Promises](http://docs.scala-lang.org/overviews/core/futures.html) * [Scalaz Task - the missing documentation](http://timperrett.com/2014/07/20/scalaz-task-the-missing-documentation/) (Tim Perrett) * [Intro to Functional Streams for Scala (FS2)](https://www.youtube.com/playlist?list=PLFrwDVdSrYE6PVD_p6YQLAbNaEHagx9bW) (/u/mpilquist) * [Scalaz-stream example](https://www.reddit.com/r/scala/comments/3xavvi/scalaz_and_scalazstream_simplify_things/) (/u/paultypes) * [Previous example implemented with a Free monad](https://www.reddit.com/r/scala/comments/3xavvi/scalaz_and_scalazstream_simplify_things/cy3aool/) (comment by /u/Milyardo) * [Free Monads are Simple](http://underscore.io/blog/posts/2015/04/14/free-monads-are-simple.html) (Noel Welsh) * [Free Monads in Scalaz - how to use them](https://www.chrisstucchio.com/blog/2015/free_monads_in_scalaz.html) (/u/stucchio) * [A Modern Architecture for FP](http://degoes.net/articles/modern-fp) and [Part II](http://degoes.net/articles/modern-fp-part-2) (John A De Goes) * [A very simple technique for making DSLs extensible](https://pchiusano.github.io/2014-06-12/extensible-dsls.html) (Paul Chiusano) * [Becoming More Functional](https://m50d.github.io/2017/01/23/becoming-more-functional.html) (/u/m50d) * [Introduction to Recursion Schemes with Matryoshka](http://akmetiuk.com/posts/2017-03-10-matryoshka-intro.html) (Anatolii Kmetiuk) * [A Shapeless Primer](http://rnduja.github.io/2016/01/19/a_shapeless_primer/) (Edoardo Vacchi) * [F-Bounded Types](https://tpolecat.github.io/2015/04/29/f-bounds.html) (/u/tpolecat) * [Don't Use Return in Scala](https://tpolecat.github.io/2014/05/09/return.html) (/u/tpolecat) * [Methods are not Functions](https://tpolecat.github.io/2014/06/09/methods-functions.html) (/u/tpolecat) * [Frameworks are fundamentally broken](http://timperrett.com/2016/11/12/frameworks-are-fundimentally-broken/) (Tim Perrett) * [Design for Experts, Accomodate Beginners](http://pchiusano.github.io/2016-02-25/tech-adoption.html) (Paul Chiusano) * [Coupling in OOP](http://mergeconflict.com/coupling-in-object-oriented-programming/) (/u/mergeconflict) Some motivation for doing all of this in Scala * [Constraints Liberate, Liberties Constrain](https://www.youtube.com/watch?v=GqmsQeSzMdw) (Runar Bjarnason) * [The advantages of static typing, simply stated](https://pchiusano.github.io/2016-09-15/static-vs-dynamic.html) (Paul Chiusano) * [Higher-kinded types](http://typelevel.org/blog/2016/08/21/hkts-moving-forward.html) (/u/S11001001) * [Spoiled by higher-kinded types](https://www.youtube.com/watch?v=t7bOKAIQG4Q) (Adelbert Chang) 
This update messed up my key bindings. Weird, haven't had any issues previously.
I tip "compile" and CMD+ENTER and nothing happens :( 
I'd love the simple "Terminal" plugin to be able to recognize scala files and allow me jumping to the source code pointers. Also, SBT auto-complete doesn't seem to work with aliases from `~/.sbtrc`. Other than that, seems cool, thanks!
&gt; A type inference / type checking that doesn't diverge from what scalac can compile (especially when using cats/scalaz) Don't expect this any time soon. Intellij has to use its own type checking mechanism because all of the inspections/completions are based ontop of their own PsiTree data structure, and integrating Intellij with psc (the presentation compiler which is what is required to get completely "correct" results) is probably not going to happen, its either impossible (in the sense that the psc wont provide the things that Intellij needs) or practically unfeasible. &gt; , but it seems like this IDE is highly biased towards the part of the community that doesn't care that much about types and pure FP, Intellij is a commercial product, and will cater for what the audience needs the most. So far the most glaring issues have been stuff like SBT integration, and for this reason this release is really good. Although Intellij is not entirely correct when it comes to typechecking, it has gotten a lot better over time Honestly, providing an IDE that is as good as Intellij when it comes to things like autocompletions/inspections (with decent performance) + supporting pure FP are at conflicts with eachother. A lot of the typical pure FP code, especially code which heavily uses implicits, creates a lot of computational scenarios which exhibit great completely in terms of computation. Unfortunately its kind of a "can't have your cake and eat it too scenario"
Very useful collection type. Is using a power of two array length really noticeable performance wise? For large sizes this might waste a lot of memory.
I have 2 final classes from external library and I would like to write generic function which allows me to modify and return one of those classes. Is there some kind of neat trick for this? I want behaviour like: class ExternalFinalClassA[K, V] class ExternalFinalClassB[K, V] trait X[K, V] class MyClassA[K, V] extends ExternalFinalClassA with X[K, V] class MyClassB[K, V] extends ExternalFinalClassB with X[K, V] val classA2 = new MyClassA[Int, Int] val classB2 = new MyClassB[Int, Int] def function[F[_, _] &lt;: X[_, _], K, V](settings: F[K, V]): F[K, V] function[MyClassA, Int, Int](classA2) But in my case those classes are final so it prevents me from extending. Also if you could show me how to write my example in nicer way I would really appreciate it, I'm just learning type-level functional programming. Thank you in advance. :)
Could dotty solve this? If IntelliJ could hook into one of compiler phases and get typed tree of code or something?
Yes, modular arithmetic (which is in a critical path of this data structure) is much faster if the divisor is a multiple of 2 - if n is a multiple of 2, mod n is same as (&amp; (n-1)) which is much faster. Regarding memory pressure, this is as bad as Java's ArrayDeque which also doubles each time. OTOH, Java's ArrayList grows at 1.5x. In practice, I have seen that it is better to grow at 4x or even 8x when dequeue is small and 2x at large sizes..
One straight forward approach would be to use composition instead of inheritance. Rather than having `MyClassA` extend `ExternalFinalClassA`, have it take an instance as a constructor parameter, then you can implement `X` in `MyClassA` by delegating to the wrapped instance of `ExternalFinalClassA`. However this is one of the use cases for _typeclasses_. Typeclasses are a way of associating behavior to types rather than having that behavior be "inherent" and provided by the class itself. You can then be generic over the typeclass' behavior. The beauty of this approach is that it allows you to define behavior that works not only for classes you don't own (your current use case) but also for classes which don't even exist yet! When those classes are eventually created, by you or someone else (like a user of your library), then a typeclass instance gets created for that class and everything automagically works. There are other benefits too, like not requiring complicated inheritance or initialization setups, improving testability, and often composing well with other typeclasses. Here's a way that might look. // external classes @ final case class ExternalA[+K, +V](a: String = "A") defined class ExternalA @ final case class ExternalB[+K, +V](b: String = "B") defined class ExternalB // typeclass, works for anything accepting two type parameters @ trait X[E[_, _], K, V] { def x(e: E[K, V]): String } defined trait X // generic behavior, delegates specific behavior to the typeclass instance, found implicitly @ def genx[E[_, _], K, V](e: E[K, V])(implicit ev: X[E, K, V]) = println("Typeclass " + ev.x(e)) defined function genx // define the typeclass' behavior for the external classes @ implicit def eva[K, V]: X[ExternalA, K, V] = new X[ExternalA, K, V] { def x(e: ExternalA[K, V]): String = e.a } defined function eva @ implicit def evb[K, V]: X[ExternalB, K, V] = new X[ExternalB, K, V] { def x(e: ExternalB[K, V]): String = e.b } defined function evb // get some instances of external classes @ val a = new ExternalA a: ExternalA[Nothing, Nothing] = ExternalA("A") @ val b = new ExternalB b: ExternalB[Nothing, Nothing] = ExternalB("B") // use generic behavior, finds the right typeclass based on the input parameter type @ genx(a) Typeclass A @ genx(b) Typeclass B
Why can't IntelliJ receive type-checked trees from the compiler and do exactly the same things it does now on them, save all the frontend work (symbol resolution and typing)? Type-checked trees even have positions to map them to the editor text. I imagine this is what Eclipse does though, and that it's why it's less stable.
 &gt; "Why Concurrency is Awesome with Scala" Concurrency is not awesome anywhere. its an effect which programmers need to handle once they decide to write multi-threaded code. Certain things make concurrency easier to manage like Akka.... but it is still extra work. &gt; Unlike Java, Scala is not limited by default to Thread primitives for concurrency Threads are not primitives for concurrency. They cause the programmer to think about concurrency. 
&gt; Intellij is a commercial product IntelliJ IDEA Ultimate Edition is the commercial product. The Scala plugin for IntelliJ IDEA (IntelliJ-Scala), and IntelliJ IDEA Community Edition that it's built on, are both Apache 2.0 licensed software. &gt; A type inference / type checking that doesn't diverge from what scalac can compile (especially when using cats/scalaz) They both need to behave according to the Scala specification. IntelliJ-Scala is in some ways an alternative Scala compiler and when it doesn't always agree with scalac on what is valid or invalid Scala code it's sometimes an issue in the spec (not up to date, unclear, unspecified, imprecise..), or either scalac or intellij-scala not follow the spec. Solving this problem would help refine the spec, which would an amazing objective for Scala. One initial step here would be to be able to run intellij-scala against code in CI. This would allow users to test that their Scala code compiles using both scalac and intellij-scala. Allowing this to happen is being tracked as [SCL-11572](https://youtrack.jetbrains.com/issue/SCL-11572).
I have played around with it and changing my build.sbt with the sbt beta makes intellij unsuable for me. It just hangs for a long time (sometimes 5 minutes) and then sometimes even quits on me
TIMTOWDI! :)
Will the new collections coming in scala 2.13 help reduce the size of the scala.js output by simplifying the inheritance graphs of the collections? EDIT: also, why is Hello World in Scala.js now apparently bigger than back in 2013? 2013 (2.63 kb): https://github.com/scala-js/scala-js/issues/4#issuecomment-20937230
We do not intend to support WebAssembly in any way, it's out of scope for the project. Scala.js is amazing, and is the way to go for JavaScript environment. 
code is here https://github.com/openmole/scaladget
&gt; mostly build Python scripts to automate mundane tasks at work based on that I would go with http://www.atomicscala.com/ but it's a subjective advice 
error: No resource identifier found for attribute 'srcCompat' in package 'com.mypackage.test' 
do you mean https://github.com/ntu-sec/sbt-android-hw/blob/master/src/main/res/layout/main.xml#L22 ?
Vert.x isn't Lagom. It sits somewhere between Akka and Play in terms of functionality -- it doesn't deal with persistence or resiliency. Lagom is Play with service descriptors on top of Akka Clustering, using Cassandra for the persistence involved in cluster sharding. So it's several layers of tech, designed to solve a specific problem for maximum resiliency. I would really recommend reading Vaughn Vernon's books on Implementing Domain-Driven Design for a background on Lagom and the DDD / CQRS model involved in Lagom, and [Data on the Inside vs Data on the Outside](https://blog.acolyer.org/2016/09/13/data-on-the-outside-versus-data-on-the-inside/) for why eventual consistency and out of order messaging is a reality in microservices.
Split brain resolution is part of Lightbend Production Suite. You have an old link, the new one is: http://doc.akka.io/docs/akka/akka-commercial-addons-1.0/scala/split-brain-resolver.html and you can read about the [application resilience](http://www.lightbend.com/platform/production/application-resilience) features in general.
Hey Alexandru, Thanks for the detailed feedback here! I could definitely do with some clarification around immutability -- I wanted to touch on it without spending too much time talking through it all, but I realize after reading your comment I need to clarify things a little. I'll update the article tonight. The Thread example IS terrible, yes. It's not really meant to be realistic, just a short example of a Thread class. I really hope nobody copies this code. In fact I may add a comment to indicate that it is terrible :-) I share your love for Kafka -- I first used it several years ago and it has only grown better as Jay, Neha and the team have built a company around it. Again, thanks for the great feedback, and expect some article changes soon :-) 
Hey, thanks for the feedback! I disagree - concurrency is totally awesome with Scala :-). I used to write C++ and Java, and really it is a very different world. The grammar maybe isn't correct, but it reads nicely. Maybe primitive is the wrong word. That's fair criticism and I'll update the article to reflect that. Thanks a bunch! 
Alright, updated with a few changes to clarify some things. :-)
I recommend Scala School by twitter: https://twitter.github.io/scala_school/ If you're looking for books, I made a list: https://blog.matthewrathbone.com/2017/02/14/scala-books.html
If you have anything reproducible, could you create an issue for that at https://youtrack.jetbrains.com/issues/SCL please?
Consider the following example: onSelectIndex(index) { console.log(index); } render() { return &lt;div&gt; entries.map((e, i) =&gt; &lt;EntryView entry={e} onSelect={() =&gt; this.onSelectIndex(i)} /&gt;) &lt;/div&gt; } To give EntryView a correct shouldComponentUpdate method, you'd need to have equality on all the props, including `() =&gt; this.onSelectIndex(i)`. Memoization is one way to do this, but it's quite painful to use. 
I think that comments like "*Don't do this at home 😄*" are very helpful for readers. Threads are very low level and most of the time when you're working with threads, it's just a loop that pulls from some queue, at which point you might as well work with actors or other higher level abstractions.
[This comment](https://www.reddit.com/r/scala/comments/626c97/experienced_with_vertx_scala/dfknz7b/) mentioned Stringly typed in the context of vertx. The vertx MessageBus/EventBus doesn't deal with classes "natively", usually JSON is used to pass data thru the bus. You can work around that but it requires some level boilerplate. You basically have to register a "converter" for every class that you want to pass thru the bus, or manually convert to/from JSON (or the vertx Buffer class). I was thinking about creating something to automatically take care of that "issue" using Kotlin data classes that had a particular attribute or implemented an interface, and an approach that used Scala case classes would probably work the same. There's a good reason for the EventBus working the way it does, vertx is meant to work across many languages, not just JVM based ones, so it needs to use a more "universal" way of passing data.
Yeah I agree with that, but i think if you're just learning concurrency for the first time it can help to start at the bottom. Once you've built something with threads you'll appreciate the other stuff more
I guess I could change it to just blank out completely like the regular sbt shell does (where you also can continue typing). But maybe there's a nicer way. What do you think?
At a glance I think I'd go with a variation of the second but move the implementation of `eval` into each concrete `Expression`. type Calculation = Try[Int] trait Expression { def eval: Calculation } final case class Addition(left: Expression, right: Expression) extends Expression { def eval: Calculation = { for { l &lt;- left.eval r &lt;- right.eval } yield { l + r } } } That keeps each piece relatively contained rather than having some central dispatch logic.
The political move was absolutely necessary. We need a library of functional programming concepts that enforces a level of basic human decency in its discussion spaces. I wish that could have been achieved without a fork, but apparently not. 
If you're only going to run a free, you might as well never construct it. It's similar to the idea that if the only thing you do with a list is fold it then you shouldn't instantiate it in the first place. 
You work on the ws client? Are there any plans on implementing a backpressure mechanism? Or controllable retries? 
Play-WS currently sits on top of AHC, which sits on top of Netty -- we'd have to make use of netty-reactive-streams to be able to use backpressure there, but you can at least stream the response incrementally using an Akka Stream Sink now. You can use Gigahorse, which has an Akka-HTTP client backend, but I don't know if that backpressure semantics were built into it. If you want complex retries it's probably better to do it outside of WS/AHC, and use something like a Failsafe retry: see https://github.com/jhalterman/failsafe/ and https://github.com/wsargent/scala-failsafe and then you can set up an exponential retry with jitter or what have you.
Perfetto
One more question. If I write code in ScalaJS. How do you publish it to npm repository? Is there a plugin for that?
Well Scala.js produces one .js file, at the end of the day. You can then simply use `npm` itself to publish that file to the npm repositiory, I guess. I don't think there is an sbt plugin to do it.
Don't try to solve using plain Scala Js. Use frameworks like udash.io or Scala Js react. They have a lot of components built in
Nice write up. Please take a look at Jekyll + GitHub pages as an alternative blogging platform. It's much better than Google Blogspot. 
Has the Akka HTTP pre compiling improved yet? The CPU shoots up when I type code in high level DSL
I have heard IDEA and Eclipse are the two main ones. Sounds like Eclipse may have had some issues at one time? Maybe it is better now. 
I use Atom with syntax highlighting and nothing else, with sbt running nearby in a terminal window. That's it.
Whole lotta typin', shame. Try Scala IDE with shared SBT compile target, tends to get things right modulo the odd macro where sea of red squigglies appear for a time and then magically vanish after a number of compile/refresh cycles. Maybe when Dotty cometh we'll have better tooling...
[neovim](https://github.com/neovim/neovim), [scalafmt](http://scalameta.org/scalafmt/), [gnome-terminal](https://help.gnome.org/users/gnome-terminal/stable/), [sbt](http://www.scala-sbt.org/) and when I'm lazy then [vist](https://gitlab.com/stevendobay/vist) too which is my (neo)vim plugin which helps in error highlighting, rename-refactoring, definition search etc. I've left IntelliJ like half a year ago and I don't miss it.
I be pythoner. I do job with python and anaconda. Job pleaz? 
For scala Atom + sbt + maybe api docs open in browser granted I am also that douchebag who uses a tiling window manager and I have an almost eidetic memory so I never really felt the need to use an ide in my entire life, and for me to actually use intellij or eclipse without it fucking up on me I would have to log out, and switch to XFCE everytime I wanted to code, since WMNAME LG3D doesn't help when I'm using eclipse, also when I played around with intellij it would always will give me shit about my code being too dense.
Brings back old memories of a CompSci teacher in college showing us the french variation of Pascal. I never found actual references to it, but I couldn't imagine him just making it up.
Hi, we are looking for a highly experienced and qualified Pythonist for an exciting opportunity. **We expect:** * At least 5 years of documented experience working with Spark (Snake Protection And Recovery Kit™). * Documented experience working with at least 10 different types of highly venomous snakes. * At least 3 years of documented experience extracting venom from snakes. It is an advantage but not required if you also know Java (where our central base of operations is) and Scala (Snake Catching And Luring Armor™). You must also be able to demonstrate professional experience and capability with SOP (snake-oriented platforms) and related snake catching methods. You will also have to expect a great deal of traveling around the world to exciting locations such as Sri Lanka (which currently suffers from a high rate of snake-bite related deaths). We expect you to be open-minded and easy-going, approach work with a can-do attitude, and to be able to work well with a diverse set of people under potentially challenging conditions (such as when people are screaming in panic from frequently lethal snake bites). Finally, you must possess a Master's degree or a Ph.D. in biology, chemistry, or zoology from an accredited education institute. **We offer:** A professional work environment with high excellence, fantastic spirit, great benefits (including, but not limited to, free treatment of snake bites), and a highly competitive wage. You will also get to see the world and a wide variety of very venomous snakes. You will also get the chance to help save lives by being a critical part of the development of snake venom cures. (Note that we take no responsibility for any deaths caused by snake bites). **About us:** SnakeCures4You is a world-renowned snake venom extraction and cure development research company with locations in various countries around the world such as Java. We have 20 years of international experience, and we are currently expanding our workforce due to continued growth in the snake extraction and snake venom cure market. **Contact** me at jackmiller@snakecures4you.com for more information or for applying. Deadline for applying is 1st of April, 2017.
Fantastisch!
uh, April fools?
*agreeable noises*
I use IDEA, but I disabled that type-aware highlight stuff because it is rarely handy (highlights errors that are not errors and silently skips something that doesn't fit). Having the ability to get Ctrl+Space and stuff for Gradle reimport and unit tests by one-click overcomes the drawbacks. 
Skala! Das ist, was jeder wollte! Uberfantastisch!
Can you elaborate a bit about natural transformations separating the logic from actors communication? Sounds very interesting.
I'm working on a project that blends Akka persistence and deeplearning4j. You can train neural nets, keep both training data (memory) and the model (consciousness). You can have one model output be an input to another. I can scale it out to ridiculous levels. It's not useful or anything....but it works like a human brain...so that's cool. Maybe eventually I'll teach it to play agar.io .
https://github.com/tpolecat/doobie pure functional non academic performant (as in dominated by DB request), supports streams etc. also check out https://github.com/Verizon 
... ja!!
This is sadly less a joke then it should be. Most famously there is MS Excel which has translated all build in functions into german and cant handle the english names at all, try to copy and paste from stackoverflow with this. Or german companies trying to force their own developers to code in german, which works perfectly fine with Java's beans. You would write a boolean getter like "istSchnell" istead of "isFast" which results in the the bean name "tschnell".
Nein, we don't do that in Deutschland. 
I had ensime choke pretty hard on doobie, but it handles fs2 fairly well. I'm not entirely sure if I'm doing it wrong. Despite some hiccups I'm very happy with ensime as a whole.
Thanks for the pointers! lxscala seems smaller (only 1 day), than the others. Any experience from Scala days? Scala Swarm seems to have many of the same speakers (but fewer talks).
Look at almost any [Typelevel](http://typelevel.org) project.
Hi, we are looking for a highly experienced and qualified Pythonist for an exciting opportunity. **We expect:** * At least 5 years of documented experience working with Spark (Snake Protection And Recovery Kit™). * Documented experience working with at least 10 different types of highly venomous snakes. * At least 3 years of documented experience extracting venom from snakes. It is an advantage but not required if you also know Java (where our central base of operations is) and Scala (Snake Catching And Luring Armor™). You must also be able to demonstrate professional experience and capability with SOP (snake-oriented platforms) and related snake catching methods. You will also have to expect a great deal of traveling around the world to exciting locations such as Sri Lanka (which currently suffers from a high rate of snake-bite related deaths). We expect you to be open-minded and easy-going, approach work with a can-do attitude, and to be able to work well with a diverse set of people under potentially challenging conditions (such as when people are screaming in panic from frequently lethal snake bites). Finally, you must possess a Master's degree or a Ph.D. in biology, chemistry, or zoology from an accredited education institute. **We offer:** A professional work environment with high excellence, fantastic spirit, great benefits (including, but not limited to, free treatment of snake bites), and a highly competitive wage. You will also get to see the world and a wide variety of very venomous snakes. You will also get the chance to help save lives by being a critical part of the development of snake venom cures. (Note that we take no responsibility for any deaths caused by snake bites). **About us:** SnakeCures4You is a world-renowned snake venom extraction and cure development research company with locations in various countries around the world such as Java. We have 20 years of international experience, and we are currently expanding our workforce due to continued growth in the snake extraction and snake venom cure market. **Contact** me at jackmiller@snakecures4you.com for more information or for applying. Deadline for applying is 1st of April, 2017.
Thanks for your input, Scala Days definitely looks packed with interesting stuff!
Hundreds of them in Verizon Labs. Probably the largest effort I know of that's open source is [Funnel](https://github.com/Verizon/funnel), a complete distributed monitoring system. They no longer maintain it, but I can tell you for a fact funnel-core is still used with a funnel2prometheus module they haven't open-sourced. The complete application component would be Chemist, which is a fault-tolerant distributed manager of the "flasks" for collecting metrics. The underlying IaaS is abstracted away from: there's an implementation for AWS and a static one for testing. I think it makes a good case study for fairly intense scalaz/scalaz-stream programming. Today I imagine we'd use [http4s](http://http4s.org) instead of wrapping Unfiltered, but that's about it.
Oh it was definitely a thing at some point, to translate programming languages – especially those used for education. There are also versions of BASIC in different languages.
I do the same, but disable error reporting for problematic files only. I still think IntelliJ is a net win, for: instant variable/field/class renaming; finding definitions, usages and overridden versions of any declaration in a keystroke; indexing the whole code base so I can instantly jump to any declared class/object/method; the nice grepping capability; showing scala/java-doc; auto-complete; etc. You can even still get correct type information from a shortcut in many cases, and get at least approximate type info most of the time.
Slow clap.
I use scalafmt because when I've tried to use scalariform I needed to set a lot of options and I got confused. And the result wasn't really satisfying - some parts of the code got weird like the formatting and indentation of match/case. I use intellij's style(`--config-str "style=IntelliJ"`) and it formats the code well. I don't like how it indents long argument lists but otherwise it's pretty good.
I found scalafmt easier to set up and it's better maintained
If you want your failure type to be a sealed sum type use either. If you only plan on encountering exceptions for failure then use direct future
Future[Either] is a nice way of signaling expected failures that are not of exceptional nature. Like, if you were to call a validation service with some data and there was a number of known validation errors that could occur, these would make more sense to keep on the left side of an Either, whereas the Failure case would mean that the call to the service failed for reasons beyond pure validation, like the service or a database being inaccessible. Remember, a Failure only guarantees to contain a Throwable, which could really be any type of error, whereas an Either clearly lets the caller know that the result may be one of two things.
/u/joshlemer approached the /r/python mods with this idea. I think it worked out pretty well, and got a few laughs without being super disruptive. Thanks /r/Scala for the switcheroo!
Yep, good laughs all around :-)
Sorry it took a while for me to get around to actually giving you the theme. You see, as a python developer, I am employed and very very busy.
I heavily use Future[Either[]] in my code base. simply because I like ErrorCode on the left hand side rather than deal with dangerous Throwables.
What is the the difference between Finch, Finnagle and Finnatra?
https://www.reddit.com/r/scala/comments/60tygk/scalariform_needs_a_new_maintainer/
I echo everything everyone else is saying. Reserve failed futures for unrecoverable errors, and handle them at one layer with logging and alerts. Represent regular contingencies on the value level, within the `Future` [1]. `Either` works for this, or you can use an alternative result type. I personally can't wait until I can use union types, to effectively create and compose ad hoc sealed supertypes on-demand, at each API. If you don't mind [`CList` syntax](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#coproducts-and-discriminated-unions), you can do this today. [1] Or, alternative async monad, since I'm sure the FP police are watching! 
[EitherT monad transformers](http://underscore.io/blog/posts/2013/12/20/scalaz-monad-transformers.html) 
So you have an example of usage?
Why? Whats the problem?
Yea, this guy wrote up some decent practical examples: http://blog.leifbattermann.de/2017/03/16/7-most-convenient-ways-to-create-a-future-either-stack/
Finagle is a framework created by Twitter used to build concurrent servers. It allows you to define services in terms of functions and then use them to handle client requests and generate responses with one of the supported protocols. It provides a lot of useful abstractions (like using Futures for concurrency) and various tools for diagnostics and monitoring. Finagle is great, but on it's own it's not the best pick for building a minimal HTTP web service, since it doesn't incorporate tools to compose endpoints, match specific requests and parse different types of request bodies. Finch on the other hand, is built on top of Finagle and allows you to do all the things mentioned above, using a set of combinators. It's focused on building APIs, so you define your application as a service composed of endpoints. Finatra differs from Finch in the intended use case. Finatra is meant to be used to build entire web services (somewhat like Play Framework), it allows you to serve templated HTML pages and applies typical web service design patterns (MVC).
Why do you call Throwable's dangerous ?
&gt; How do sequence, i.e. walk over, your Future [Either[A,B]]'s? Use Cats Monad Transformers to use them in for comprehensions and extract values. 
Seems like doing this fixed it case Node(data: A, _: Stack[A]) if e &lt; data =&gt; s.push(e) I would still like to know why
`languageWordCounts.mkstring(text)` will turn languageWordCounts into a string using text as a separator between each element. The problem is mkstring takes in a string argument but text is an array. Did you maybe mean to use text.mkstring(" ") which would turn your Array[String] into a single string with a space between each element?
mkString works quite differently. The thing that is made into a String is the collection you call it on. In this case your whole map with keys and values. The argument of mkString is the separator: List("Anna", "Bob", "Carla").mkString(" likes ") &gt; Anna likes Bob likes Carla
[Tjis](http://stackoverflow.com/questions/12719435/what-are-skolems/12719617#12719617) is for Haskell but I would assume the problem to be the same. 
It's basically like a whole new world to me; I'd only heard of functional programming in passing prior to scala. It's taking me a while to fully comprehend what things do and why they're done like that. Even the syntax gets me mixed up sometimes. It's been frustrating.
Best of luck. It can be hard but it's totally worth it
I really enjoyed this talk. It's great to see how authors approach modifying the internals of the libraries they work with. 
* What is stored in the string array? Words? Lines? * Why are you using doubles to count? Integers should be fine. * Using `mkstring` just to count words is pretty inefficient. And if the array is storing lines, just counting spaces is wrong (unless your lines end in a space).
Heh, thanks. I absolutely love it; I haven't felt this... free since C++. It's just far steeper a learning curve than I was expecting.
&gt; Volatile Types What's a volatile type?
If you have a single algebra and you'e not nesting/stacking/interleaving languages everything is pretty straightforward and I don't think any extra machinery is necessary.
In a function type `A =&gt; B` the type variable `A` appears in contravariant/negative position. This is ultimately the original of all contravariance. A type variable appearing in this position cannot be marked covariant (i.e., `[+A]`) because this would be unsound; it would imply that if I asked for an `Animal =&gt; Foo` you could give me a `Monkey =&gt; Foo` which is clearly unsafe because I might pass a `Penguin`, which is an animal but not a monkey. Scala ensures that your use of variance is consistent, and as you have probably noticed it will complain when you mess something up. There are many possible follow-up questions so I won't try to anticipate where you're going. 
Here is the way over-simplified answer that won't satisfy any of the academics. `+T` is covariant. Since there is a plus it has to be returned from a method, otherwise, it will be wrong position. This is correct. class Foo[+T](t:T) { def bar():T = t } Still using `+T`, the following would be incorrect since covariant types cannot put into method parameters. You will get `covariant type T occurs in contravariant position in type T of value t`. The contravariant position is `bar` method parameter. class Foo[+T](t:T) { def bar(t:T):Unit = new Foo(t) //bar cannot have T as a method parameter } `-T` is contravariant. Since there is a minus it has to be used as a method parameter, but not as a return value. therefore this is correct class Foo[-T](t:T) { def bar(t:T):Unit = new Foo(t) } And if you use `-T` as a return in a method that would not work, and you'll get `contravariant type T occurs in covariant position in type ()T of method bar`. The covariant position is the return type of `bar`. class Foo[-T](t:T) { def bar():T = t } Just remember, covariant position is the return, or source. contravariant are the parameters, or sink. I like the `-` and `+`. `-` is sink, `+` is source. Hope that helps get you started.
thank you very much!
true, my questions/answers don't focus a lot on types. I would also say these questions are more of a basic level, and type-related questions can be considered more intermediate/advanced level. I think it could be interesting in having a follow up on this one day. In the meanwhile, if anyone wants to learn a bit more about types, I read this article a couple of weeks ago, and found it worthy: http://ktoso.github.io/scala-types-of-types/
Definitely looking forward to that, ensime takes a huge mental burden off the programmer
Is Scala a good choice for learning functional programming? Also, as a newbie to programming, would you recommend me to learn an OO language, before I learn Scala/FP?
&gt;So let's say I have 2 files HelloScala.java and HelloJava.scala in 2 directories. HelloScala outputs "Hello, Scala world" and HelloJava outputs "Hello, Java world". If I ran both of the compiled files using the same JVM could I assume the output would be as expected? Yep! You can very easily incorporate Java sources and Scala sources in the same project, including 1 or more main classes/objects in either language. Here is how you can do that https://github.com/joshlemer/java_plus_scala_example &gt;Also: Is there a specific Java version that scala supports up to, like say 7 or 8? Different versions of Scala support different versions of Java. The latest major version line (Scala 2.12) supports only Java 8 and later, while the previous line (Scala 2.11) supported Java 7. 
&gt; while the previous line (Scala 2.11) supported Java 7. Java 6 actually!
I'm gonna say yes to the first question, and say to use Scala for the second question, since it is made such that it can be used in either way. To [pull from Wikipedia](https://en.wikipedia.org/wiki/Scala_programming_language): &gt; Like Java, Scala is object-oriented, and uses a curly-brace syntax reminiscent of the C programming language. Unlike Java, Scala has many features of functional programming languages like Scheme, Standard ML and Haskell, including currying, type inference, immutability, lazy evaluation, and pattern matching. 
Non-Mobile link: https://en.wikipedia.org/wiki/Scala_(programming_language *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^51595
This question is so commonly asked, I hear it a lot from people new to Scala and unfortunately I don't have any good answer for them. I hear doobie and circe, but those are libraries and not apps. I often wonder if there should be some kind of template project with variations (like TODO or PetShop). Most normal applications have the following: - API Authentication (OAuth for example) - Json serialization - Business Rules / validations (things post JSON) - Glue Code "Services" - Database Access - External Web Service Access TODO is a little too trivial, PetShop maybe too big. An Order Management System might be the right size as a domain. 
The beauty of Scala is that it's a mix. Trying to use Scala purely functionally is not the -best- idea, and is not the way it was intended to be used. The same is true for ignoring functional aspects entirely. Projects should contain aspects of both (unless for some sort of challenge or something you only want to use FP, which is cool, but not the best of ideas for a great application)
Maybe he means volatile values, which are not serialized and reevaluated when deserialized?
functional and object oriented are not opposites. you *can* have both. 
I don't think that's wrong -- they are two different things. That's why Scala is often described as an object-functional language. I tend to think of the main feature of OO programming as a data type is grouped with the routines that act on it. It happens that that it often also has mutable state contained within a class, accessor methods, dot notation, inheritance, etc. Likewise I think the main feature of functional programming is immutable values and no side effects. It happens that it also tends to favor recursion, polymorphic functions, higher kinded types, etc. Where I think Scala has done really well is marrying the best of both. You get types declared with their methods, dot notation, efficient recursion, first class functions and immutability. At least idiomatically, you can cheat if you want to. Whether to use partially applied functions or case classes depends on what you're trying to do. I try to make my code correct, clear, and performant in roughly that order. So if both work, all else being equal, I'll pick the one that makes it easier for someone else to figure out what the code does. This is up to you though. If your goal is to make your code fun to write or as few lines as possible, there's nothing inherently wrong with that.
&gt; Also, curious on guidance here. When is it good form to use partially applied functions vs. case classes? It's a matter of style. You can choose to go more point-free if it produces code you find to be more maintainable. Or you can go more for an immutable "fluent-OO" style. Or you can go mutable, if you want. I think the biggest concern is how blocks of your program relate to each other. When APIs expose observable mutability, that's complexity begins to compound.
No, if you mean to *learn* functional programming rather than use as a production functional programming language. You should try OCaml/Haskell firstly. And it's really good to write some java *snippets* in order to get understanding of Scala; but if you only mean to learn FP, you don't need to learn OO at all. But I guess for most of the time, you don't mean to only learn FP.
I remain a bit perplexed about why there's so much excitement around the free monad. I mean, I can see its value when you want to be able to dynamically construct, inspect, manipulate, serialize, or execute abstract computations. Like, perhaps it would make a good basis for an SBT-like tool. But for writing the typical service in a system, it seems overly complicated. I've gotten a ton of mileage out of writing my services as simple `case class`s. They take their service dependencies as function arguments in their constructors and expose the services they provide as public methods. Testing is really easy. I just pass in stub functions of the expected signature. I think there's an underlying idea here that has been captured as the initial/final duality (if I understand that correctly). An initial encoding reifies computations as values that can be flexibly composed and interpreted as monads or applicatives. A final encoding simply defines computations as the functions that _actually do the work_, which are composed by the native syntactic function composition of the programming language. The reification is lost, but to me this is [YAGNI](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it) for most applications. (Apologies if I've butchered the initial vs. final thing. I've followed these developments only from a distance.)
I agree. When I interview people I open a random Easy hackerrank.com task that neither of us have done yet and we both implement it. "both haven't seen it" is so that I don't get the impression the task is easy after having seen 20 candidates do it. Or if the candidate has provided code for some project of his or hers, we just do a code review of that and I ask him to refactor something.
Here type `B` is volatile type because it has upper bound. trait A { type B &lt;: Int } I first stuck with volatile types when was writing unit tests using play-ws 2.6. They have hell like this there now: trait WSRequest extends StandaloneWSRequest { override type Self &lt;: WSRequest { type Self &lt;: WSRequest.this.Self } override type Response &lt;: WSResponse //... } I still can't figure out what the type will be in implementation of `Response` because it keeps complaining about volatile types :( Also you can read about them [here](https://www.scala-lang.org/files/archive/spec/2.11/03-types.html#volatile-types) but it is too complicated for me.
One major thing objects are good at is encapsulating side effects. I believe this is especially true when you compare them with functions. For example, changing state is considered side effect too, and using objects, you can encapsulate the side effect of state change (state management). State management library like Redux takes advantage of this; it has one main object (store) which encapsulates state management so that bunch of other functions (reducers) can stay as pure functions. If it didn't have a store object encapsulating state management, I personally think the whole, both Redux itself and its dependents, would be more complicated. If you are not familiar with Redux, this is what I am talking about: http://redux.js.org/docs/introduction/CoreConcepts.html &gt; I'm pretty confused and have a hard time logically separating the functional bits from the OO As far as I understand, at its core, OOP is programming with objects, and FP is programming with functions. If your understanding is they are mutually exclusive, I believe that's where your confusion is coming from because they can co-exist; you can program with both objects and functions. &gt; it seems to me like I'm not doing OOP anymore As long as you are using Scala, you are using objects all the time. Consider the following example: val values = Seq(1, 2, 3) val newValues = values.map(value =&gt; value * 2) This code may be considered functional because `map` is taking a pure function, but it's also object-oriented because `map` is a method of `values` object. Also, in Scala, that "pure function" is object, too. &gt; It was a long time since I had a type inherit another Inheritance is just one feature of objects. Code doesn't need to use inheritance to be object-oriented, just like code doesn't need to use currying to be functional😊 If it's programmed with objects, it's object-oriented. If it's programmed with functions, it's functional. And they can co-exist.
When I put on my abstraction goggles it looks to me as if an object, in the sense it is discussed above, is simply a type-safe map from member_symbol -&gt; object — so a recursive structure. This in itself doesn't seem to encompass the "gist" of OOP. After all, most (if not all) languages have the need for a map-like data structure. Taking JavaScript's prototypal inheritance as an example, the OOP bits feel more clearly defined in prototypal inheritance. Yet, JavaScript has an object type that wouldn't have to depend on this form of inheritance (but does so for seemingly legacy reasons, which is an entirely different discussion in and of itself.) The answer I'm hoping to distill from my initial question is where Scala's OO support is an orthogonal benefit to its support for functional programming; where — or even if? — the OO paradigm can be applied to better effect than FP. ---------- edit: It feels like the answer boils down to inheritance and dynamic dispatch, where inheritance is a solution that's subjectively superseded by typeclasses.
&gt; where — or even if? — the OO paradigm can be applied to better effect than FP. I think I get that. That's why I mentioned encapsulation of side effects first thing. Are you saying OOP isn't any better than FP when it comes to encapsulating side effects? Or are you saying you don't see a need for encapsulating side effects? 
Read the Red Book - [Functional Programming in Scala](https://www.manning.com/books/functional-programming-in-scala) if you haven't already. It helped me understand the "relationship" between FP/OO, although I'm not sure that's the right way to put it.
Sorry, I glossed over that part as I didn't quite understand what you meant by encapsulating side effects. To me, encapsulating — or rather controlling — side effects is what you use Scalaz-constructs like `IO` or `Task`for, constructs which are inspired by Haskell which is strictly FP. If you mean updating properties on a shared object, I don't believe that has anything to do with OOP intrinsically. Maps can be changed either through mutation or immutable sharing which basically reduces the state-change thing to "updating a variable", which while not considered strictly pure functional programming still is available in most FP languages.
&gt; Who is the author of Scala Programming language? https://github.com/scala/scala/graphs/contributors
&gt; It also is eager, so it isn't inlineable without potentially changing program behavior. That's actually the definition of pure functional programming, so Future is not fit for encapsulating side effects. In the same vein, wrapping effects in object does not encapsulate them at all, they are still entirely observable. That being said, I think encapsulating effects in pure FP is a pretty massive advantage both for ease of reasoning about your code and for composability. It is true though that it can get complicated (in Scala more so than necessary, unfortunately), but once you start to appreciate the advantages of referential transparency (which takes a while), you don't want to give them up anymore. 
That is (one of) the problem(s) I've been having: understanding at an intuitive level where FP paradigms should be used. I've been reading Modern Web Development With Scala and that's helped give me some context, but it's still rough going. 
That code is not OOP. If you expose the state of an object (Seq' s members), than it is not encapsulated, thus you have broken the fundamental property of OOP. That code is purely imperative. It might be immutable, but map being defined on the data type and the data type leaking its internal data via accessors makes it imperative. To be FP you have to define your data (value objects) separate from your behavior (functions). To be OOP you must not ever return any values from method calls, other than factories and constructors. Getters must not exist. All behavior is encapsulated in the instances you construct. Communication is done via notification using the Observable pattern. You can't throw in an OO program, either. Yes, Java, c#, and c++ programs routinely violate this and have to be rewritten a lot. Nobody really programs Pure FP or Pure OOP. They do structs with methods imperative programming, because that's all anyone has ever taught beginning programmers, and programmers are too busy solving today's problem to take time to learn what they are doing and how to do it better. Programmers tend to see something different, and rather than trying to take a few minutes to understand it, cry readability and complexity (when they mean familiarity on both counts). They just want to do their job as quickly as possible, and go home. You can do better. You can write programs that are resilient to change over time, can swap out libraries like changing clothes, document themselves, and handle errors like an Ericsson phone switch. Learn encapsulation in OOP. Learn side-effect encapsulation (in your terms) and stacks and Free and generics in FP. You will have a competitive advantage over most programners -- you will understand how to do what you do.
Thank you for your response. It may not be immediately obvious from my original post, but my question arises directly from the process of getting more into functional programming. We've probably taken similar paths. I have completed the Coursera classes in functional programming and I feel rewarded by what I have learned. As with all knowledge, gaining an insight raises a new question. Scala is touted as a blend of OOP and FP, but I find myself more and more annoyed by the object oriented bits. It usually feels like what you get is slightly more than you asked for, which ends up a liability. Take as an example the ubiquitous `Animal` example, where an implementor wants to model a [Linnaeuesque](https://en.wikipedia.org/wiki/Carl_Linnaeus) hierarchy of the animal domain. So you may have a base type `Animal { def animalNoise: String }` and then a `Duck &lt;: Animal { def animalNoise = "quack" }`. But not all animals make noise so they have to inherit from something else. The implementor may restructure the hierarchy so that `Animal` is just a marker trait but then the value of having a common base type is basically nil. Or, to use an entertaining example, all `Mammal`s should have a `def giveBirth: Offspring` but `Platypus` needs a `def layEgg` and has to implement `def giveBirth = throw new PlatypusException()` This is of course a contrived example but pretty easy to get stuck in with OO-modelling, which makes the paradigm a liability. What I'm looking for is concrete examples of when it is actually beneficial.
Scalariform is an awesome project and still provides a lot of value for tons of developers. We can make some simple changes to transition the project to it's unmaintained status gracefully. 1. Bump the project to v0.2.0 to [make all the latest changes easily accessible](https://github.com/scala-ide/scalariform/compare/c4bbdd9e1f428ebd9b919d164177cd00c34d84e6...f53978c60579fa834ac9e56986a6133c0a621bfd) 2. Update the README to make it clear that the project isn't maintained anymore. 3. Update sbt-scalariform to use v0.2.0. Can you do #1? I'm happy to do #2 and #3 :)
snapshots for version 0.2.0 are already published
Can you send me a link to the location? Thanks!
Yes. Once you enter a paradigm you shouldn't exit it. You should not abandon your core abstraction. It is perfectly valid to mix OOP and FP. OOP methods can have their internals be a FP program. FP programs can run OOP programs to achieve side-effects. However, mixing the two should be purposeful, not because of familiarity, but because it simplifies the design. I think OOP is complex, because you cannot, by definition, look at an opaque object and reason about its behavior. It's lawless, and reasoning about it is much more like scientific observation and experimentation than logical construction. Which is what makes it possible to use anywhere in your program, and gives it the maintenance properties that are desirable, but makes it difficult to test without breaking encapsulation. Others think FP is complex because it is a restrictive model that takes time to learn well enough to be expressive. FP is transparent. It should tell you everything you need to know via the type signatures of the function you are working on, and that's all you should need to know to work on the function. I think this makes it less complex than OOP, because you can reason about your code logically. However, it is possible to test without breaking encapsulation of side effects, so that's another point in it's favor. The one thing that is terrible is unconstrained imperative programming, but only if you care about maintenance costs. 
Very cool
https://www.gitignore.io
There is no "IDEA" but has "JetBrains" which has too much.
You probably want "intellij".
https://oss.sonatype.org/content/repositories/snapshots/org/scalariform/scalariform_2.11/0.2.0-SNAPSHOT/
What I mean is that you must control side effects for your program to have any real use or meaning and not be spaghetti in any paradigm. Updating a variable is in a map in fp is not ok, because you cannot inline that statement and not change the behavior of your program. You have to make a copy of the map and assign it to a new name, leaving the old one alone, ideally delaying applying the updates until you are done manipulating the structure (using State). That doesn't mean you can't use mutation, just that if you do it must not be the returned value of a function. Update must occur on a var defined within the function, not a global or passed variable. The return should be a copy of the state of your internal var. That way you don't leak a refernce. The same is true for OOP maps. You cannot read from them outside of the map structure and be purely oop. You can implement Map to get similar behavior in your object, but reading the internal state of any other object breaks encapsulation. That includes all data structures, like queues, linked lists, heaps, etc. All that means in practice is that any breaks in encapsulation must be private to an object and not exposed to the outside world. It doesn't mean you can't use traditional data structures, just that you can't return them from your methods. Ideally, your traditional data structures would communicate via message passing (observer observable), so that they don't break encapsulation, either, but I don't know of a std lib that does that in the real world. So, all maps/queues/etc must be private to your well-behaved objects to make proper use of them. Basically, you cannot leak references anywhere in OOP or FP and be pureX.
&gt; Pure FP really doesn't have to be complicated. Here's a simple way - &gt; Use Future to wrap every return in your program. Use map and flatmap to chain all of your functions together. After you get to the last map, call map one last time and log the result, then call recover and log the failure. Done. This isn't pure FP, so its not what I was talking about. If you wanted to be pure FP in your example, you would have to use some State monad to track side effects, so you would probably have have a `Future[StateMonadT]` or something along those lines. You also probably won't be throwing exceptions (this violates PFP) so you would need another transformer because you are going to likely be dealing with `Either`. What you are talking is actually what I mean by non pure FP, if you want a language that forces you to do pure FP so you can tell the difference you can try Haskell
Ahah, so now fleeing the jvm is the new cool thing^TM [back to native code again.](https://www.youtube.com/watch?v=GibiNy4d4gc) 
Say I want to handle `ErrorA1` in a specific way, and the rest generic. If there was an error, I cannot just match on `ErrorA1` and `_` but I will first need to check if the `ErrorA` case and then if it is `ErrorA1` because it is not flat. At least not with Scalas built in pattern matching.
I've made a release: `libraryDependencies += "com.gitlab.stevendobay" %% "nvim-scala" % "1.0"`
Not a .gitignore but our .hgignore is pretty simple, and seems to do the trick for SBT projects using IntelliJ. &gt; syntax: glob &gt; .idea &gt; project/target &gt; target &gt; log Looking at the JetBrains.gitignore above, I'm unclear why you would bother not just ignoring the whole .idea directory? 
This is --&gt; &gt; A good solution is to swap out Future with Task[Either] from fs2, cats either syntax for right based eithers, using MonadError.catchNonFatal from cats to wrap your function calls in an either that will contain the throwable from any errors. Then you can .map(_.map(z=&gt;catchNonFatal(yourcode))) And it really isn't difficult.
As a Scala consultant, I never see clients actively seek out category theory libraries as a solution to their problems. Sure, they might use Shapeless indirectly, for instance, never being the wiser. Instead, their Scala projects are grounded in simple-to-use concrete Scala libraries. Well, that might be a bit of a stretch.;) There is this 1% of Scala developers, though, who value functional purity above all else and are paid to apply it in their daily work. Moreover, they blog and talk about it almost ad nauseam.;) And I think it has an impact on a small percentage of Scala developers, who are torn between the Scala worlds of 1) concrete and 2) abstract. Intellectually, they're bored with option 1 and want to adopt option 2. Yet they can't convince their fellow developers to make the change. More likely, they don't even know how to effectuate the change themselves. This internal struggle is visible in many of the posts on this sub reddit. There's always been a small percentage of developers, for any language, who want to push the envelope. And that's a great thing. Scala is no different. Yet it seldom works out for the envelope pushers. Whether we like it or not - money, time, perception and reality are indefatigable foes against such ambitious thought - category theory being an ideal example. In the future, though, a new language that values category theory might emerge successful, provided it garners universal support as a successor to its natural predecessor. Haskell tried - and failed. That said, Freestyle does, indeed, look interesting.:) 
Hmm would upgrading libraries be so horrible? If `fn` is simply a restricted version of `def`, all code would still work exactly the same way it does today. It's only if you want to call functions from an `fn`, those need to be `fn`s as well, or it's no longer guaranteed pure. It will be a migration process for libraries to declare all their pure functions as pure (with `fn`), but nothing will break along the way.
In general, the compilers that target JVM don't perform very aggressive optimizations since it's not trivial to assess what kind of optimizations are worth it once you consider the JIT compiler. Some optimizations might actually make it generate slower native code, since it's not the kind of usage it's been tuned for. As far as I know the standard Java compiler doesn't do any optimizations at all and there are no plans to change it. Scalac can indeed perform some optimizations though. As for your code example, the fastest way to find out is to compile it and look at the bytecode. For demonstration I tried the following snippet (based on your code): object test extends App { def getRadioStationInfo = (104.3, "The fan") for (_ &lt;- 0 until 50000) { val (a, b) = getRadioStationInfo println(a) println(b) } } The compiler will generate a method that will be called for every iteration over this range. This is how this method looks for Scala 2.12.1 without any optimizations: public static final void $anonfun$new$1(test$, int); Code: 0: aload_0 1: invokevirtual #89 // Method getRadioStationInfo:()Lscala/Tuple2; 4: astore 4 6: aload 4 8: ifnull 46 11: aload 4 13: invokevirtual #93 // Method scala/Tuple2._1$mcD$sp:()D 16: dstore 5 18: aload 4 20: invokevirtual #97 // Method scala/Tuple2._2:()Ljava/lang/Object; 23: checkcast #99 // class java/lang/String 26: astore 7 28: new #70 // class scala/Tuple2 31: dup 32: dload 5 34: invokestatic #78 // Method scala/runtime/BoxesRunTime.boxToDouble:(D)Ljava/lang/Double; 37: aload 7 39: invokespecial #83 // Method scala/Tuple2."&lt;init&gt;":(Ljava/lang/Object;Ljava/lang/Object;)V 42: astore_2 43: goto 59 46: goto 49 49: new #101 // class scala/MatchError 52: dup 53: aload 4 55: invokespecial #104 // Method scala/MatchError."&lt;init&gt;":(Ljava/lang/Object;)V 58: athrow 59: aload_2 60: astore_3 61: aload_3 62: invokevirtual #93 // Method scala/Tuple2._1$mcD$sp:()D 65: dstore 8 67: aload_3 68: invokevirtual #97 // Method scala/Tuple2._2:()Ljava/lang/Object; 71: checkcast #99 // class java/lang/String 74: astore 10 76: getstatic #109 // Field scala/Predef$.MODULE$:Lscala/Predef$; 79: dload 8 81: invokestatic #78 // Method scala/runtime/BoxesRunTime.boxToDouble:(D)Ljava/lang/Double; 84: invokevirtual #112 // Method scala/Predef$.println:(Ljava/lang/Object;)V 87: getstatic #109 // Field scala/Predef$.MODULE$:Lscala/Predef$; 90: aload 10 92: invokevirtual #112 // Method scala/Predef$.println:(Ljava/lang/Object;)V 95: return This doesn't look very well, let's see what happens with the optimizations (-opt:l:classpath flag): public static final void $anonfun$new$1(test$, int); Code: 0: aload_0 1: invokevirtual #84 // Method getRadioStationInfo:()Lscala/Tuple2; 4: astore_2 5: aload_2 6: ifnull 26 9: aload_2 10: invokevirtual #88 // Method scala/Tuple2._1$mcD$sp:()D 13: dstore_3 14: aload_2 15: invokevirtual #92 // Method scala/Tuple2._2:()Ljava/lang/Object; 18: checkcast #94 // class java/lang/String 21: astore 5 23: goto 35 26: new #96 // class scala/MatchError 29: dup 30: aload_2 31: invokespecial #99 // Method scala/MatchError."&lt;init&gt;":(Ljava/lang/Object;)V 34: athrow 35: getstatic #104 // Field scala/Predef$.MODULE$:Lscala/Predef$; 38: dload_3 39: invokestatic #73 // Method scala/runtime/BoxesRunTime.boxToDouble:(D)Ljava/lang/Double; 42: invokevirtual #107 // Method scala/Predef$.println:(Ljava/lang/Object;)V 45: getstatic #104 // Field scala/Predef$.MODULE$:Lscala/Predef$; 48: aload 5 50: invokevirtual #107 // Method scala/Predef$.println:(Ljava/lang/Object;)V 53: return As you can see, the code is significantly shorter, but it still has some of the tuple destructuring code (which is a pattern match with a runtime typecheck on the String object) and invokes the element accessors. So, in general, you can't trust the compiler do to this sort of thing for you (at least for now). However, you can achieve what you want if you mark getRadioStationInfo as @inline. The optimizer will do it's job pretty well once the tuple is instantiated locally and the resulting code looks like this: public static final void $anonfun$new$1(test$, int); Code: 0: ldc2_w #66 // double 104.3d 3: dstore_3 4: ldc #75 // String The fan 6: astore_2 7: getstatic #87 // Field scala/Predef$.MODULE$:Lscala/Predef$; 10: dload_3 11: invokestatic #73 // Method scala/runtime/BoxesRunTime.boxToDouble:(D)Ljava/lang/Double; 14: invokevirtual #91 // Method scala/Predef$.println:(Ljava/lang/Object;)V 17: getstatic #87 // Field scala/Predef$.MODULE$:Lscala/Predef$; 20: aload_2 21: invokevirtual #91 // Method scala/Predef$.println:(Ljava/lang/Object;)V 24: return It's an interesting experiment to do, but in most cases runtime performance is a whole another matter and it always needs to be measured before drawing any conclusions!
Prof Odersky has floated A -&gt; B as the notation for a pure function, retaining A =&gt; B as a potentially impure function. Coordinating this with an effects system based on implicit functions and changes to the standard library to take advantage seems to be an open area ATM. I don't have a reference for this, but Martin did discuss it after his ScalaSyd talk in February. 
I dunno about your current project. But there's nothing uncommon or embarrassing about not being able to write complex regex without consulting the docs. 
I think inside Intellij the most convenient way is "File-&gt;New-&gt;.ignore file -&gt;.gitignore file" and select "Example User Template"+"Scala"+"Eclipse"+"SBT"+"Java", then "Generate without duplicates".
what is the best resource for learning advanced stuff and its not too python/perl centric.
Every time I have to do anything above basic level, I use [this](http://www.regexplanet.com/advanced/java/index.html). It's saved so many hours.
What version of scala did you perform these in?
Does it matter that they're Python or Perl centric? Regular expressions don't differ much between languages. [This page](https://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html) notes the differences between Java and Perl regex.
Do you know why it is `Kleisli` used for this example and not the `Reader`? I mean: what is the point of wrapping the created object in `Option`(or anything other than `Id`) ? Or is it just an example and for real case when I need dependency injection I should use `Reader`?
Some project-wide settings might make sense for him to share (code formatting options, inspections)
Ok, I see! Even if I have an intermediate algebra of operations that is mapped to Doobie/the filesystem/Git, ... depending on the interpreter? Do you know where I can find code samples?
I'm starting to use cats, circe and ValidatedNel to parse JSON. Compared to the play-json library, it's much easier to use when you know the patterns --- even if you don't understand the underlying mechanics; simples cases are not more difficult than the examples provided in Play documentation. You cannot escape applicative builders/monads when parsing structures: the only question is whether they are named as such in your code, and you get documentation for free, or whether you wrote a specialized implementation. However, the culture of pure-ish FP languages attract people who are already familiar with abstract mathematics, so the documentation is sparser and code samples are at a higher level of abstraction. It is getting better over time. The main problem is that the hype train has passed, and early implementations coloured people impressions about Scala in the wrong direction.
&gt; But for the time being, the actual competition for FP developers are OCaml and Haskell. OCaml is interesting because it doesn't have a memory model yet, but they are doing inter-process communication and it has a really low-latency GC. And on the other side of the spectrum you've got Golang, which I personally hate, but which people have picked because it allows them to build native binaries. Swift is becoming more and more portable. Rust is another popular choice lately, especially because it doesn't have a GC, while preserving memory safety, thus being useful for scenarios that are impossible for the aforementioned ones. And there's always C++, which is insane, but still the most popular. OCaml has had a real struggle trying to catch on, not exactly sure why this is. Also like you said, OCaml doesn't have a proper multithreading or memory model, which will also put their low latency GC into question if they ever end up implementing it (doing a low latency GC in a multithreaded environment is a real PITA) In any case, what I meant be "competitor" were languages which have an existing backing on the JVM but are moving to native, only Kotlin is basically in this area. There are obviously other languages like Rust, Swift and Golang, but none of these languages apart from Swift is similar to Scala (it will also be interesting to see where Swift will go)
I use https://regex101.com
What kind of code do you write where a Tuple2 allocation is a gotcha? If you write modular code on the JVM, you pay the price of memory semantics when composing objects/data structures. - In numerical software, I resort to very plain imperative code internally, and wrap everything behind a nice immutable API. - What performs best (for numerical code): @tailrec functions or cfor macros, primitive arrays, and operating on bigger pieces of the data. Use mutation sparingly by creating builder objects. You may lose many advantages of Scala (immutability, readability), only to recover them after wrapping the internals. - Use a profiler. Detect the hot spots: not only where you spend CPU time, but also where you allocate lots of small objects (which kills memory locality).
This is true, although we might see languages moving in the opposite direction. For example I'm interested in this implementation of Haskell: http://eta-lang.org/ ; And I would like seeing PureScript or Idris, because those are strict by default and might handle the JVM better. Good times :-)
Kleisli is like ReaderT, used when functions return a monad like Future or Task
&gt; My projects this would require writing very complex regex and I really don't know how recursion works in regex. Regular expressions can't be recursive. That's what the "regular" means. I'd recommend you try using parser combinators instead.
is typed tagless unfeasible in Scala due to ambiguous implicits?
Thanks! Going to give this a try!
Exactly, that's why I said that this reading doesn't solve your initial question, merely gives you a bit different point of view if you are not familiar with FM already. It's fine to mark my answer "semi-offtopic". 
**hope that everything will be better on Java 10**
This seems to work... object EnvironmentSyntax { implicit class EnvironmentOps[F[_], A, B](k: Kleisli[F, A, B]) { def provide(f: Environment =&gt; A) = k.contramap[Environment](f) } } trait Environment { def userRepo: UserRepository def groupRepo: GroupRepository } object SampleService { import EnvironmentSyntax._ import scala.concurrent.ExecutionContext.Implicits.global import scalaz.std.scalaFuture._ def getUser(userId: String): Kleisli[Future, UserRepository, User] = ??? def getGroup(groupId: String): Kleisli[Future, GroupRepository, Group] = ??? def saveAll(u: String, g: String): Kleisli[Future, Environment, Unit] = { for { _ &lt;- getUser(u).provide(_.userRepo) _ &lt;- getGroup(g).provide(_.groupRepo) } yield () } }
ReaderT is kleisli specialized to the Id monad
If you wanna go down that road I think the better approach would be to create a new keyword `pure` and then you would declare functions with `pure def ...` and `pure class ...`.
Why not an annotation along the lines of @tailrec? 
Only real way to learn is practice and trying to do things that are beyond your current skill level.
Unfortunately they differ enough just enough to make them infuriating :/. Elisp regex's are the absolute worst.
I agree with this. But are regex's something to get good at ? What's the future of regex's at this point, they have no compile time checking, it's all test-and-revise... I wonder, strongly typed regular expressions where the compiler could help you, is that doable ?
That's honestly fine. I don't see a reason to become an expert in regex unless it becomes a big part of your job. Regex is a tool to get what you need. It's like becoming an expert in awk when you don't use it that much.
If you learn the main parts then the differences are easy to deal with, since most of the differences are in the more advanced features that you'll use less often. I learned in Perl originally and it's easy enough to remember that I can't use conditional expressions in Java, but honestly I rarely use them anyway. When it comes features supported by various shell programs it's a bit harder, but it sounds like that's not your main concern. The absolute basics are about 90% of what you'll use an any language. That is: * character classes, e.g. \d \S . [a-z] [\^:] * quantifiers, e.g. + * {2,5} * capturing/non capturing groups, e.g. () (?:) * the or operator, | * basic assertions, e.g. ^ $ Understand just those and you're set most of the time. Another 8% and you might need ungreedy quantifiers (just add ?), more complex assertions, or custom look-ahead and look-behind assertions. The final 2% might be things specific to the actual implementation you're using, but it's probably even less than that.
That makes sense. I guess there's just some pedantic part of me that likes the repository to be IDE neutral!
http://shop.oreilly.com/product/9780596528126.do
I second that advice: there is a lot of good stuff on this site. The author deserves donations for this work. (I am not affiliated.)
Literally posted by the OP 2 days ago
Just responded to one!
well, it is called pcre for a reason
There's also [Idris for VBA](https://github.com/jystic/idris-vba), in case you ever feel the urge to do FP in MS Office :-)
Are you easing into the easier parts first?
Hey there! Slick has a quite good approach to this, effectively quarantining the blocking jdbc operations on a separate threadpool. It also gives you a DBIO as a return type for most operations, which you can run at the boundaries of your application. It will also let you drop down and get direct jdbc action, if you want to compose them that way...
Is there a way to rename string interpolators? Doobie uses fr for SQL fragments
Right click on the directory tree. Add a package with a name of your choosing. Drag your file into the package. It should refactor/add the correct package annotation at the top of the file. That should do it.
 import rapture.i18n.{fr =&gt; french}
the total lack of scaladocs may be the reason behind it
Thanks, I wasn't sure if that's how it worked
Thanks a lot. your suggestion worked.
You could use unapply methods trait Foo trait Bar def complexFilterLogic(foo: Foo, bar: Bar): Boolean = ??? val foos: List[Foo] = ??? object ExpensiveComputation { def unapply(foo: Foo): Option[(Foo, Bar)] = ??? } val bars: List[Bar] = foos.collect { case ExpensiveComputation(foo, bar) if complexFilterLogic(foo, bar) =&gt; bar } However, I guess one downside of this is that there's no way to just perform the expensive computation one time, and have multiple cases, for example: val bars: List[Bar] = foos.collect { case ExpensiveComputation(foo, bar) if complexFilterLogic(foo, bar) =&gt; bar case ExpensiveComputation(foo, bar) if complexFilterLogic2(foo, bar) =&gt; bar case ExpensiveComputation(foo, bar) if complexFilterLogic3(foo, bar) =&gt; bar } Here you'll be recomputing ExpensiveComputation on each case 
Thanks for the Edward Kmett video link.
Definitely look at the Cats library. you will find some of the constructs which you have already learnt in Haskell.
There's no reason to match an Option and have a `case _ =&gt; None` -- use `collect`: foos.flatMap { foo =&gt; expensiveComputation(foo).collect { case bar if complexFilterLogic(foo, bar) =&gt; foo -&gt; bar } } Or even filter if the pattern match is just a filter: foos.flatMap { foo =&gt; expensiveComputation(foo) .filter(complexFilterLogic(foo, _)) .map(foo -&gt; _) } Alternatively, if `expensiveComputation` returned back `Option[(Foo, Bar)]`: foos.flatMap(expensiveComputation) .filter(complexFilterLogic _ tupled)
Who has positive experiences with microservices? Can anyone share their experiences, and say, going back, that they did well by using microservices? I just feel unwell whenever I'm forced to split compile safe contract / well structured module, for http request just because 'reazonz'
I think Cats, scalaz or any other library with implicit conversions is too hard for person who just started learn Scala. I agree with [comment](https://www.reddit.com/r/scala/comments/63s1q3/as_a_programmer_with_a_solid_background_in/dfwion2/) above - "The red book" will be good for learning.
&gt; *since most of us must monitor &amp; police object allocations in the JVM, especially inside nested loops.* I'm pretty sure that most of us don't do that. I've been building some pretty intensive apps and I've rarely had to police object allocations in hot loops, because the GC is fairly efficient at allocating and deallocating short-term objects. It's so efficient in fact that it's close to stack de/allocation, because (well, depending on the GC used), the JVM keeps the heap defragmented and allocations involve incrementing a pointer and deallocations from the young generation happens in bulk. So this is not your father's `malloc` and `free`. Heck, it's probably doing a better job than your object pool. The problems that happen are due to indirection, maybe you end up spending more CPU resources because of that. Tuples in particular could have benefited from being stack allocated. Latest C# allocates tuples on the stack, although F# still allocates them on the heap by default. It would have been nice to have this flexibility in Scala, but it's also easy to abuse, because at some point it becomes more effective to allocate on the heap (e.g. classes or tuples with many values). And note that the JVM also tries to do *escape analysis* to eliminate locks and to presumably allocate stuff on the stack. It's not doing a good job because of the implemented algorithm, but as always, it's not easy to make a generalization about the performance impact without *actual profiling*. As for functional programming, well there we end up talking about other problems, like *persistent data-structures* invalidating the presumptions of the garbage collector, linked lists being used over arrays, laziness being abused, etc. But for most people, that trade-off is worth it, until profiling shows you an actual problem and then you can get dirty. &gt; *Perhaps this is why there are so many reports of people writing java code in Scala* Well no, I don't think so, people that write Java in Scala don't know much Scala. Which is fine, they learn eventually.
Thanks for the reply, Well then the question is what JVM we're talking about. Most of my customers use the default that oracle distributes &amp; would not shell out the cash for something better. C# doing tuple allocation on the stack is news to me, is it true for full blown objects? Sorry but I've grown an extreme aversion to microsoft tech over the years - too many let downs. But I only wished the JVM could do that. It partially works for primitive types at least. Any resources/links about those other problems? If anything, that is part of my original question.
Well noted, I'm not there yet with scala. I need to do far more testing and study a lot. It looks promising, but I'm cautious.
I wasn't trying to deny your point. The truth is that Scalac can't do much about your problem. One of the basic goals behind Java and the JVM was to provide an environment where you can be productive and write highly OOP code without worrying too much about performance thanks to the modern JIT compiler and an efficient GC. This idea applies well most of the time. There are certain situations where you might consider writing 'uglier' code for certain performance benefits, but that should apply to bottlenecks identified during profiling. It's very well known that writing functional code puts you at risk of allocating a lot more objects. This problem has been largely diminished with the development of copying garbage collectors, but it's still an issue in some circumstances. Yet, a lot of people and even some big companies, still use Scala in large projects where performance is very important (Finagle, Akka) and in many cases they perform better than implementations in lower level, imperative languages. That's because they profile their code, and apply better performing solutions (but sometimes 'uglier') in places where it actually matters.
Does this have any advantage against Quill? 
+1 you stole my question!
question 2: Do you support table scans? suppose I want to read all the rows of a table.
I have had success. But my microservices are not necessarily separate code. I share my models and code in the same repo (granted different SBT projects in many cases, but often still boils down to a fat jar), I just start different daemons/actors/etc at runtime based on configuration. This model allows easy cross-project development while still letting you control deployment. Just make sure the communications between daemons/actors/whatever still happen over network ifaces instead of directly (and don't even try wasting your time abstracting the communication mechanism too much where it can be local or remote or you'll get impedance mismatches with different serialization approaches and you won't feel comfortable w/ your test cases that are local only). As for "reazonz", mine are usually to allow configuration in deployment that I won't care about. In many cases many of my "microservices" run in the same JVM on the same server because there is no need to separate them yet. But I am sure happy I can move em around in the future w/out rewriting code, and I have done so on several occasions. However I will say don't separate concerns just for fun. I usually say separate something into it's own piece if there is a network boundary between it anyways (e.g. Kafka messages) or if it carries enough of its own baggage you can foresee it being a singular entity on its own to solve a problem or, especially, you need it to be HA on its own.
You can get rid of the initial foos.flatMap by adding foo &lt;- foos at the top of the for comprehension.
Thank you!
The article purports to be about "building scala projects" in general, but is really only talking about the special case of a major library like Spark. SBT has specific, built-in support for one specific kind of cross-building, but that's all; if you look at projects that have to cross-build across any other axis than Scala version (e.g. multiple versions of akka, multiple versions of scalaz-stream, cats/scalaz variants) then their build definitions are just as awful and complex as any project that tries to cross-build with maven. If you need that specific kind of cross-building and no other, SBT is much better than maven, sure. But for a "normal" project that only needs to build against a single version of Scala, Maven is a much better option. Most people don't need to cross-build, and an article about building Scala projects in general should really address the more common use case.
It's important to be precise about the types because your question is about a type error. What is the type of `p_wordGivenLanguage`? What is the type of `document`? What is the type of `documentMap`? What is the type of `p_Lg`? Etc. If you can stub out a minimal failing example I'm sure someone can help but right now it requires too much guessing.
ahh yes sorry. p_wordGivenLg(word:String, language:Language, lambda:Double) p_Lg(language: Language) document is an Array[String] p_docAndLg(document:Array[String], language:Language,lambda:Double) DocumentMap = Map[Language,String]() 
You're missing types all over the place, but it's kind of incidental to the puzzle. Any is the Scala equivalent of Object. Pretend you're trying to diagnose an inheritance problem and it'll probably make sense.
I am going through this course. If anyone else is also doing it and needs a study partner, then PM me.
What makes maven a better choice?
post author here, thanks for the feedback! A couple of responses: &gt; is really only talking about the special case of a major library like Spark. Sorry if it seems that way. Most of my day-to-day is on small- and medium-sized libraries (see modules under [hammerlab/spark-genomics](https://github.com/hammerlab/spark-genomics)), so I was definitely not intending to only talk about Spark-sized things; they are a little easier to make examples of because lots of people recognize them and 1000s of highly-skilled eng-hours have been spent creating and maintaining their builds. &gt; SBT has specific, built-in support for one specific kind of cross-building, but that's all; The half of the post from ["Beyond cross-publishing"](http://www.hammerlab.org/2017/04/06/scala-build-tools/#beyond-cross-publishing) onward deals with what I consider to be the bigger revelation for me here, which I wanted to share: using a fully featured language for something like build configuration, which can [mimic simple key/values in the 90% cases](http://www.hammerlab.org/2017/04/06/scala-build-tools/#key-value-style-configuration-is-succinct-and-sufficient-in-common-cases) while [seamlessly incorporating arbitrary logic/decomposition/etc. in trickier cases](http://www.hammerlab.org/2017/04/06/scala-build-tools/#where-necessary-advanced-language-features-can-be-deployed-and-blocks-factored-out-and-reused) is **powerful**, and preferable to trying to express everything in e.g. flat XML. [This build.sbt from one of my projects](https://github.com/hammerlab/genomic-loci/blob/1.5.2/build.sbt) (also linked to in the post) is maybe a good case study: each line maps to between 1 and, idk, 100 lines of POM XML; see [the dozen instances of "shade" in the POM that used to build this library's code](https://github.com/hammerlab/guacamole/blob/9d330aeb3a7a040c174b851511f19b42d7717508/pom.xml), for example. There's just no contest in terms of expressive power between a build where you are writing Scala and one where you are writing XML. &gt; if you look at projects that have to cross-build across any other axis than Scala version (e.g. multiple versions of akka, multiple versions of scalaz-stream, cats/scalaz variants) then their build definitions are just as awful and complex as any project that tries to cross-build with maven. I'd be curious to see any examples you have in mind of such libraries. [akka](https://github.com/akka/akka/tree/master/project), [scalaz](https://github.com/scalaz/scalaz/blob/series/7.3.x/build.sbt), [cats](https://github.com/typelevel/cats/blob/master/build.sbt), and most of the Scala ecosystem (I realized much later than I would have liked) all use SBT, and in many cases already understand the distinction above about [not configuring builds with one hand tied behind your back by e.g. trying to just use XML with no logic] that I am making. I certainly will not argue that those SBT builds, or most SBT builds in the wild, are things of particular beauty, but it doesn't seem very controversial to me that there are many classes of simple, logical tasks that are hard or impossible to express in XML, and easy or at least possible in Scala. &gt; But for a "normal" project that only needs to build against a single version of Scala, Maven is a much better option. Most people don't need to cross-build, and an article about building Scala projects in general should really address the more common use case. FWIW, when I wrote this article I was publishing some libraries for {2.10,2.11}, others for {2.11,2.12}, a few for all three, and one not at all ([it's simple enough that a 2.11 build published as "universal" seems to be fine](https://github.com/hammerlab/args4s/blob/1.2.2/build.sbt)). Since then, I've dropped 2.10 entirely, and most of my libraries just publish 2.11 versions, though a few do both 2.11 and 2.12 (I want to do {2.11,2.12} for all but they depend on Spark, which [is still ironing out 2.12 support](https://issues.apache.org/jira/browse/SPARK-14220)). I don't think I agree that most people don't need to cross-build, and in any case I think I provided a lot of information in the post and here about why SBT could be considered better than Maven that have nothing to do with scala-binary-version-cross-publishing. Truly interested in any examples you may be able to offer though, as I know that I have only seen a fraction of what's out there :) P.S. this is less here nor there, but a favorite build/deployment example of mine is [spark-notebook](https://github.com/spark-notebook/spark-notebook), which offers downloads via a webapp (which is down atm, go figure) that exposes [a plethora of choices about what versions of which dependencies are desired](https://d3vv6lp55qjaqc.cloudfront.net/items/3C3m242L1n2W2C302A2f/Screen%20Shot%202017-03-16%20at%203.35.33%20PM.png), serves up a cached build matching that [if it's ever seen that configuration requested before](https://d3vv6lp55qjaqc.cloudfront.net/items/273o3a1w0o3t0I2H340g/Screen%20Shot%202017-03-16%20at%203.36.49%20PM.png), or kicks off a (SBT-based) build if not and emails you a download link a few minutes later. You can probably do that approximately as easily in Maven or SBT, so it's more just something I think is interesting and related to some of the problems discussed here.
Sidebar'd!
Very well. If you have any questions or comments (about lectures or exercises) please write them there (below each "step" of the course). Good luck.
As far as I understand it, Quill translates their own AST into Cassandra query string, but you are still responsible for executing that query and unmarshalling the result to whatever type you provide. Troy takes care of the unmarshalling to case classes for you (yay for Shapeless). Also it's "schema-safe", in the sense that your query have to be syntactically and semantically right : in the sense that the code will fail to compile if the syntax of your query is wrong OR if your query refers to columns that do not exist in the schema of your table. 
If anybody else is interested I installed an ASM plugin for intellij and viewed the bytecode of a Scala class. Variables are defined like in Java apart from everything inside the function gets pushed onto the stack first. Inner functions are defined as "private" and "static" meaning the compiler must be in control of the scopes of the functions. This also stops code accessing these methods from outside the class. 
Troy takes the opposite approach. Quill goes from DSL to CQL queries. Troy let's you write raw queries directly (think about copy/paste convenience from cqlsh).
You can write raw queries in Quill as well, or even infix it in the DSL.
&gt; I'd be curious to see any examples you have in mind of such libraries. akka, scalaz, cats, and most of the Scala ecosystem (I realized much later than I would have liked) all use SBT, and in many cases already understand the distinction above about [not configuring builds with one hand tied behind your back by e.g. trying to just use XML with no logic] that I am making. This isn't really a good example. SBT became the defacto build tool for historical, not necessarily technical reasons and so most people use it because its the default. The historical reasons for SBT becoming the defacto choice is that in its early days, Scala used to break binary compatibility quite often, and SBT was the only build tool at the time which handled cross compiling against different Scala versions sanely. The thing is, that the build tool itself is incredibly complex and also slow (not slow in terms of pure performance POV, but slow in how it works with typical user flow, i.e. the SBT startup is painfully slow and you can't do anything in SBT before it starts even though its not required in all stages). The biggest issue that SBT has, imho, is that its underlying mental model is a huge amount of magic which very few people understand, and instead of SBT reusing Scala's language features it implements its own which just compounds on this problem. Case in point, someone on my team who just started using SBT was under the impression that if he used the `lazy` Scala keyword that it would make settings lazy, however thats not the case. SBT basically made its own pseudo-Scala DSL which isn't completely valid Scala that runs a lot of macro magic to try and make it usable, and the SBT codebase itself is almost pretty much impenetrable to anyone outside of SBT and a few individuals who a have a lot of experience with it. Maven may be annoying because its declaration is in XML, but this limiting factor is actually what makes Maven overall an easier and better build tool to understand, because using XML forces your builds to be simple and declarative (in the definition sense) which is overall a net win imho. The bad thing about Maven is it doesn't handle cross compiling against specific versions well, but given that it was designed for Java which doesn't really have this problem, its understandable why But as https://www.reddit.com/user/m50d pointed out, once you get past really trivial builds, stuff like `crossScalaVersions` doesn't really work, and any other type of cross building that isn't Scala versions is actually a real PITA (try doing selective dependencies across scala versions, thats a lot of fun)
You can even do it without a case class: val rawQuery = quote { (id: Int) =&gt; infix"""SELECT id AS "_1", name AS "_2" FROM my_entity WHERE id = $id""".as[Query[(Int, String)]]} you can find all of the examples here: [extending quill](https://github.com/getquill/quill#extending-quill) 
Oh I didn't know that, that's cool ! However, the "check your query against the database at compile time" is a bit weird. Does it mean that the compiler fails if can't access the db ? What kind of workflow would you recommend doing this with ? 
Yes, lists, sets, and maps.
First, I believe you may be misunderstanding what `foreach` does. `foreach` returns `Unit` which is to say it doesn't return anything at all. It's supposed to be used for causing side-effects (which you won't care about return values of). Given the purpose of foreach, the p_wordGivenLg * p_Lg is non-sensical since you won't ever get anything back. I believe you may be looking tup `map` in this case. To put it in simple terms, the `=&gt;` is assigning a local symbol to the element you're looking at in the foreach. Please don't take this as a dismissal or insult, but I'd highly recommend going back and reading the tutorials on scala-lang.org. You're missing the basic vocabulary and understanding to ask your question, unfortunately.
I think you're over estimating how much Maven enforces good behavior. It doesn't force you to use XML because you're still allowed to put a bunch of logic in a custom plugin written in Java. I've seen this done at multiple companies. There's also the bash case that OP mentioned, but I haven't had the pleasure of seeing that bit of magic myself. Good Maven is good just like good Gradle is good. I do think Maven makes it a little more difficult to do crazy stuff, but once you do, it's full on crazy (i.e. XML+Java, build logic in multiple locations, etc).
Oh, sure. I agree with what you said about SBT. I'm a Gradle fan who has seen some messed up Maven in the past, and I'm often perplexed when someone suggests you can't write bad Maven. I admit Gradle makes it much easier to write a bad Gradle build (i.e. confusing, difficult to reason about), but it's still my preferred build tool. Different strokes and all that. 
Aren't existential types going away with Dotty? Weren't they considered a source of unsound type behavior in the current system? 
Is the OP judging Coproducts to be clunky in the case of: `def doB: Either[ErrorA :+: ErrorB :+: ..., SuccessB]`? In that case, would it help, cosmetically, to do: `type ParentError = ErrorA :+: ErrorB :+: ..., SuccessB` Then, you'd do: `def doB: Either[ParentError, SuccessB]` In other words, does adding the type class help you at all?
In 10 of the 15 sub-modules at [hammerlab/spark-genomics](https://github.com/hammerlab/spark-genomics), I have the following line in build.sbt: addSparkDeps A plugin that I wrote [unrolls this to add dependencies on](https://github.com/hammerlab/sbt-parent/blob/1.7.5/src/main/scala/org/hammerlab/sbt/ParentPlugin.scala#L185-L194): * spark (provided) * hadoop (provided) * a spark testing-library (test scoped; with a problematic hadoop-client transitive dependency excluded) * kryo To mimic this in Maven, I would write something like the following: &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_${scala.binary.version}&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hammerlab&lt;/groupId&gt; &lt;artifactId&gt;spark-tests_${scala.binary.version}&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware.kryo&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;/dependency&gt; This assumes that the scopes, versions, and exclusions on these deps are all normalized in a parent POM. To make the line-breaking more apples-to-apples to how it'd typically be written in SBT, we might write: &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_${scala.binary.version}&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hammerlab&lt;/groupId&gt; &lt;artifactId&gt;spark-tests_${scala.binary.version}&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware.kryo&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;/dependency&gt; How would you recommend reusing this block of dependencies across many projects? cc u/mdedetrich as well
&gt; How would you recommend reusing this block of dependencies across many projects? Create a (possibly `&lt;packaging&gt;pom&lt;/packaging&gt;` if you really don't have any code to put there) module that has those dependencies, then have the others depend on that.
Thank you, I am trying out Slick right now but might check out Doobie as well. About waiting for connections, that might not be significant with my current setup of on-disk H2 DB. But I'll definitely keep a look out for potential issues.
You can make profiles property-activated, though only one property per profile at the moment (other than with hacks). What are the profiles for? If you're shading anyway it might be worth splitting out the different jars into their own modules.
What's this one called? 8===D~
Hi, That code will loop over the "document" array, and on each iteration of the loop, "cnGrams" will contain a member of the array (a word of the document?). Your computation will be called with that member. However, "foreach" is probably not what you want, as it discards the result of any computation. (It's intended for cases where the per-iteration code produces a side effect, like outputting something to the screen.) "map" is probably what you want. It will return an array of your computation results. If you want to iterate not over words, but over word-based n-grams, you may want to check out "[sliding](http://alvinalexander.com/scala/how-to-split-sequences-subsets-groupby-partition-scala-cookbook#sliding)": scala&gt; val document = List("Fourscore", "and", "seven", "years", "ago") document: List[String] = List(Fourscore, and, seven, years, ago) scala&gt; document.sliding(3, 1).toList res0: List[List[String]] = List(List(Fourscore, and, seven), List(and, seven, years), List(seven, years, ago))
The Sperming Dick Operator is defined as: def 8===D~(left: Any) = throw new Exception("Sperming Dick!")
Or simply use a worksheet in an IDE. An alternative in IntelliJ is to select code and have it sent to the sbt console. No need to copy-paste back and forth at all. 
Perfect, very easy to follow. Gonna forward this to my co-workers.
Hi Alex, I am not aware of any way to get both features at the same time. However, I suggest you that you drop by the Ammonite repo and propose to fix tab completion within expressions in blocks '{}'. See the related fix adding partial multi-line support: https://github.com/lihaoyi/Ammonite/issues/81. @lihaoyi can probably give you some guidance on how to fix it, especially if you volunteer to do so! I would not expect the implementation to be difficult. Hope you settle to fix this known pain point, I would myself enjoy it.
Hey @ryanusky, thank you for your rumblings. Knowing how people perceive sbt is always good, and even more if concrete problems / points in favor of the tool are provided. I wish I saw this kind of posts more often. They are inspiring and help pinpoint where we should be improving sbt.
A small update: Ammonite is correctly showing me the names in scope when I write a declared member inside a block, but it does not autocomplete the input/nor filter the candidates where I add letters to my name. It seems to be a bug of Ammonite, so hopefully you don't need to implement it all by yourself. You just need to make sure that the input triggers an appropriate tab completion for the line you are writing.
loved these slides. your "wish i had this when i was starting out" point was dead on. i wish i had heard it live. any chance you're going to expand it into some kind of document (blog post)?
Please do, it's important that the community gets involved into sbt development as much as possible. I'll be active when you do ;)
also, i think your 0.7-ish cats usage is stale wrt to 0.9?
Which part?
Yea, property-activation was about as far as I got, and leaves some things to be desired. Doing some git archaeology… I had three different JARs that I wanted to produce that each involved similar but different shading operations: * a library JAR with just my library classes, but also with guava and breeze shaded+relocated, because my library (privately) needed Guava (resp. Breeze) versions newer than what I was forced to inherit (at runtime) from the Hadoop (resp. Spark) runtimes I was targeting. * an assembly JAR, also with guava+breeze relocated as above * a "dependencies JAR": basically the assembly JAR *minus my library's classes, guava, and breeze*, the use case being that while developing, I can build this ≈78MB JAR once (only changing it when I change my project's dependencies) ship it to my test servers, etc., and dramatically cut down my iteration time by only packaging+shipping the ≈2MB library JAR (bullet #1 above) each time I change my library code, instead of the ≈80MB assembly JAR (bullet #2). These three JAR targets overlapped in configuration in a way that approximates the {A,B,C} case I outlined above (though in actuality it was more like {{A},{B},{A,B}} than {{A,B},{A,C},{B,C}}, so I was able to eke it out, but barely): **A**: should guava+breeze be included+relocated? (yes for #1 (library) and #2 (assembly), no for #3 (deps), same as above) **B**: should all other transitive-dependencies be shaded into the JAR? (yes for #2 (assembly) and #3 (deps), no for #1 (library)) To satisfy this, I had to do some contortions; [here is the POM I ended up with](https://github.com/hammerlab/guacamole/blob/f4debafee5b7c64cfeef630c2823da01f967f4fb/pom.xml#L45-L100). I essentially enabled {**A**,**B**} by default in the maven-shade-plugin config, [relocating guava+breeze](https://github.com/hammerlab/guacamole/blob/f4debafee5b7c64cfeef630c2823da01f967f4fb/pom.xml#L77-L85) (**A**), and relying on the plugin's default behavior of shading all trans-deps (**B**), then selectively disabling **A** or **B** in a profile for each JAR: * JAR #1 (library+relocated classes only: {**A**}): [restrict shading to only library+relocated classes](https://github.com/hammerlab/guacamole/blob/f4debafee5b7c64cfeef630c2823da01f967f4fb/pom.xml#L268-L272) (aka **¬B**) * JAR #2 (assembly JAR with relocated classes: {**A** ∧ **B**}): [exactly the maven-shade-plugin default described above](https://github.com/hammerlab/guacamole/blob/f4debafee5b7c64cfeef630c2823da01f967f4fb/pom.xml#L307-L330) * JAR #3 (assembly JAR of dependencies: {**B**}): [exclude library+relocated classes](https://github.com/hammerlab/guacamole/blob/f4debafee5b7c64cfeef630c2823da01f967f4fb/pom.xml#L353-L368) (aka **¬A**), rely on default plugin config to pick up all other trans-deps (aka **B**). Looking at this today, I also see that setting the main class in the manifest should probably only have happened in JARs #1 and #2 and not JAR #3, but I was failing to exclude it from JAR #3 (it almost seems like I intentionally kept it on JAR #3 by setting "append" [here](https://github.com/hammerlab/guacamole/blob/f4debafee5b7c64cfeef630c2823da01f967f4fb/pom.xml#L350)), but anyway that's the same pattern as **A** (JARs #1 and #2, not #3), so it shouldn't have added any extra redundancy to the matrix (i.e. I'd either manually add it to #1 and #2, or add it to the default config and then disable it in #3, just like **A**), though having it coincide with **A** feels like luck as much as anything, i.e. I ended up with {{A,C},{B},{A,B,C}} (so that we could effectively just think of **C** (setting main class) as part of **A** (shading+relocating guava+breeze)) instead of a more pathological {{A,C},{B,C},{A,B}}. So, I was able to get these overlapping configurations to work, but I used (maybe over-used) the one level of abstraction that I see available: setting things in the default plugin config, then disabling them on a profile-by-profile basis, vs. enabling them on a profile-by-profile basis. If there was any more orthogonality to the config blocks I was mixing-in, I'd be out of luck, afaict. Even as is, I had to express the set of relocated dependencies ({guava,breeze}) in 3 places: * default plugin config: relocate guava+breeze (aka **A**) * library JAR #1's profile: include guava+breeze while excluding other trans-deps (aka **¬B**) * "deps" JAR #3: exclude them (aka **¬A**) If I had 3 or 4 or N dependencies to do this to, it'd be nice to be able to encode that list once, rather than effectively denormalizing that list in 3 places… Anyway, if you've made it this far I am as always interested in your thoughts/wisdom :) 
Troy doesn't connect to a DB (at compile time), instead the schema is part of the codebase. 
Yes. Because I did 4 years ago. And so should you! /s I just recently looked into building a commercial Android app. The clear language winner, IMHO, was Kotlin. That said, Android Studio should do a much better job of providing first-class Kotlin support during the creation of a new project. It could also provide first-class support for Scala. For Scala developers, Kotlin falls short in many ways. The Kotlin nullability (?) feature, for instance, is just another path to NPE hell. And the 'return' keyword versus pure expressions, for identical code, is a huge mistake. Needless to say, I'm being overly kind in my evaluation of Kotlin from a Scala perspective. After perusing the language tutorial, Kotlin is arguably more complex than Scala. Just look at the coroutines tutorial for an example of my assertion. Interestingly, a fairly robust set of Kotlin open source projects have surfaced to alleviate the functional shortcomings of Kotlin. There are no Kotlin equivalents, per se, of Scalaz or Cats that I can find. Yet the functional projects, to date, fill in many of the obvious functional voids in Kotlin. It's noteworthy that Jetbrains has made a public decision not to monadify the Kotlin core library like Scala. Oddly enough, there might be only one Java 8 functional library - Javaslang ( http://www.javaslang.io/ ). Not sure. But, if true, how sad is that? So I think Kotlin is the ideal transition language for Java developers. For one, it's integration with Java is far superior to that of Scala's. Intellij Kotlin support is nothing short of brilliant. For those of us who still use Intellij for Scala, it's so incredibly nice to have your entire Kotlin code base compile in just a few seconds. And instead of visibly painful Scala source file analyzation times, like an animated visual code-coloring waterfall, Kotlin code, like Java code, instantly verifies and yields that visually-cherished 'check-mark' in the upper right-corner - a beautiful sight to behold. If only it were also true for Scala source files.;) So, no, IMHO, the remaining majority of Java developers, at this late stage in the game, do not generally possess the desire to transition to Scala. Yet they could readily transition to Kotlin. Will they, though? Interestingly, Kotlin has literally taken the Android space by storm. Even Google is using Kotlin. Gradle has added Kotlin support. And Spring 5 will support Kotlin as a first-class language. Moreover, Spring owner Pivotal is using Kotlin! The community vibe around Kotlin is unmistakably positive! Will Kotlin become the true successor to Java? I haven't a clue. And I have my doubts. Yet I'm inclined to bet against such an outcome - given the human tendency to avoid change at all cost.;)
Yes, specifically the `for Some` syntax will be removed. 
&gt; Oddly enough, there might be only one Java 8 functional library - Javaslang ( http://www.javaslang.io/ ). Not sure. But, if true, how sad is that? I know this isn't the point of what you are saying, but there are more: https://bitbucket.org/atlassian/fugue http://www.functionaljava.org/ I think higher-kinded types and dependent types really set Scala apart from Kotlin and Java (and many others) as a language
May I ask what type of application you work on?
Binding.scala is similar to Redux or MobX, except ... When I use other libraries that provide reactive stream, I found it is difficult to determine the timing to unsubscribe, especially in a complicated reactive system. Missing unsubscription usually leads to memory leaks and event duplicates. However, there is no public subscribe or unsubscribe method for Binding.scala users. Guess why.
&gt; For Scala developers, Kotlin falls short in many ways. The Kotlin nullability (?) feature, for instance, is just another path to NPE hell. And the 'return' keyword versus pure expressions, for identical code, is a huge mistake. Needless to say, I'm being overly kind in my evaluation of Kotlin from a Scala perspective. I think these criticisms of Kotlin are a bit... weird. I don't know Kotlin, but I would imagine that the significant drawbacks of Kotlin compared to Scala are considerably different. For instance, I believe that Kotlin has a less advanced/complicated type system than Scala's, with the various benefits and drawbacks that tends to have. Some of these drawbacks include that work-arounds might end up being employed a lot in practice when the language is not flexible enough, such as with using code generation (for instance based on XML), reflection, annotations that change the behaviour of the language, etc., to handle issues such as dependency injection, which can be much more frequently avoided in languages like Scala due in large part to the expressiveness of the language's type system. Of course, these are non-issues in dynamically typed languages, but then you have other drawbacks. That said, I do not know Kotlin, and so I do not know if these issues hold true for Kotlin or not, and which other drawbacks Kotlin may have (I think you do a fair job enumerating some of the actual and possible advantages of Kotlin). &gt; [...] The community vibe around Kotlin is unmistakably positive! [...] This... sounds a bit like evangelism. I honestly get a bit of a weird vibe from your post.
I thought the same. Binding.scala is certainly an interesting library that id like to dig into. But the complaints levied against React feel a little bit contrived. React isn't a full app framework and never claimed to be, the problems presented are pretty commonly solved with Redux et al. 
&gt; By "lot" you mean lambdas. Even the way how you define a lambda parameter is alien. Yeah, sorry, I think you are right, though they did also make a number of additions to the standard library that benefits functional programming, such as the streams library, and a number of minor features that helps FP a little bit, such as method references, type annotations, and improved type inference. But yeah, it is fairly limited - no tagged unions or pattern matching, for instance. The support for basic FP is still considerably better in Scala than in Java 8.
When I was freelancing I would look exclusively for Scala jobs, but whenever the recruiter excitedly told me "The client is a **huge** users of Scalaz", I, honestly, always got a little nervous. Experience has shown me that there's a huge chance that the code base went south really quickly due to Scalaz's mis/overuse and the complexity that followed. The usual situation is: A small handful of really intelligent coders (who are often ready to jump ship to a different company) turn everything into an exercise into how thoroughly they can make every piece of code perfectly abstract using higher-kinded types from category theory whether that genericism and abstraction is warranted or not. The rest of the org lagged behind because of a lack of training, a lack of full appreciation of category theory, and/or a lacking in the ability to quickly grok the business logic behind its abstractions. I say all this not to disparage Scalaz, but to posit that functional programming is a spectrum: the basic requirement is just pure, deterministic, total, composable functions; the more "extreme" end is a dogmatic use of category theory. So, when someone slightly disparages the later end of that spectrum, they aren't necessarily saying "We should go back to OOP". **A quick and dirty heuristic I use to assess how far the team is on the spectrum is to ask: if I have a pure function and I want to write a couple lines to the log from it, do I need to re-write the function to use the IOMonad? Saying "yes, you absolutely must" usually raises a red flag to me.** (And before people think I'm just bashing Scalaz: I also get nervous with big users of Akka - another library I love. But actors are often misued as a concurrency primitive when they should be used for very specific use cases)
The type inference in Java 8 is fairly limited AFAIK. The only example I can see is local type inference of anonymous, locally defined functions in certain cases, like `Stream.of("test1", "test2").forEach(s -&gt; System.out.println(s));`, similar to how Scala can infer the parameter types of closures when used in certain cases. Java 7 also had limited added type inference with the "diamond" operator, though that kind of type inference seems very basic.
&gt; if I have a pure function and I want to log a couple logging lines from it, do I need to re-write the function to use the IOMonad? Saying "yes, you absolutely must" usually raises a red flag to me. Or what about the `WriterMonad`? :D Btw, I don't like radically purist code either because most of the time it makes the code more complex while not doing anything useful. For example they used to solve dependency injection(or 'passing' is better here) with the `ReaderMonad` but I don't know why implicit parameters and [macwire](https://github.com/adamw/macwire) isn't enough. Or the case of [monad transformers](https://www.47deg.com/blog/fp-for-the-average-joe-part-2-scalaz-monad-transformers/) - it doesn't seem to be nice at all. I'm coming from Haskell and I'd the time to practice category theory(~2 years) but I've never really got any advantage from most of the stuff in CT.
My general approach is that when mixins start getting too complicated it's time to promote them to modules in their own right. Could you have one module that declares the dependencies and then another that contains the code (and depends on the dependencies)? Then the shaded jar of the dependencies module is your "dependencies jar", and you could reuse a lot of the shade config between the two modules. You still end up with the denormalized list though. My view is that shading really shouldn't happen, but I appreciate that hadoop ties your hands in cases like this. Hopefully the new modularity stuff in Java 9(?) will offer a better approach.
Maybe, but for me it is still a codesmell. Why not just use something like Task instead and care about the `ExecutionContext` at the very end and only one time, instead of passing it all the time around?
&gt; While you can argue that you should be passing these things manually, if you are constantly passing around a HttpConfig without every doing any modifications (or .copy) with that value, odds are it should be an implicit. Or - and that is how I see it - it shows that it should not be passed at all into the business logic. Instead, create an execution-graph (e.g. with the free monad) and use your context/config at the end when you execute it. Now, only your interpreter will depend on the context/config.
All sorts of stuff. The most performance-intensive thing we do is let people bid in art auctions in real-time. Our throughput requirements are extremely modest, but the latency does need to be low.
[Github repo is here](https://github.com/locationtech/geomesa). Saw this today and thought folks might wanna check it out, one of the biggest open source Scala apps I've seen out there.
The reference on backticks (`) is broken.
Alternatively, trait UserRepo; trait User trait GroupRepo; trait Group // as above def getUser(userId: String): UserRepo =&gt; User = ??? def getGroup(groupId: String): GroupRepo =&gt; Group = ??? trait Environment { def userRepo: UserRepo def groupRepo: GroupRepo } // a program that calls sub-programs that use slices of the environment def program(u: String, g: String): Environment =&gt; (User, Group) = e =&gt; (getUser(u)(e.userRepo), getGroup(g)(e.groupRepo)) 
I see, yes your approach would be sufficient in that case. I write for android and PCs with low end desktop specs. I have to deal with framerate lag issues and large amounts of state for real time graphics. We routinely optimize all allocation outside of all sensitive code as a de facto approach. Dalvik &amp; ART are very unforgiving although the Hotspot JVM is a bit forgiving at times.
Having them implicit also shows that you expect the object to be (under normal circumstances) the same context used throughout. For the abnormal circumstances where you suddenly need to pass in a different context, you can, and the code reads very very differently.
Are geo spatial databases the same thing as graph databases? 
If I am not wrong, Haskell calls the flatMap as bind.
This week I played with matryoshka and also learned that extractor objects can be values (even though IntelliJ will complain about it) which gave me the idea to generate extractors using shapeless to remove `Fix` in long-winded pattern matches. Thought that was fun and possibly useful for someone here. https://gist.github.com/boggle/a9064416f060388683a123b91b388820
I wanted to have some fun and write simple game with libgdx but all templates were outdated. So before game I created giter8 template for sbt libgdx. Supports Android and desktop https://github.com/charafau/scala-libgdx.g8. PR s for iOS (with Moe) welcomed
&gt;In this post, we have demonstrated the dangers of ... Dangers of writing complex scala code. The code I would personally, not accept for production release.
This is a really neat blog post, thanks for sharing!
You don't need to write it down, but you still pass it around. Which means, if I want to test some of your functions down the call graph I will also need to get some appropriate context from somewhere. Makes it harder to test. And for Monix Task: that is exactly what I mean. You don't need to pass your context through the call graph, but your callgraph returns your the Task and *then* at the very end you pass your context to the Task and you are down. No reason to use implicits here.
I'm working on webasm utilities. http://www.github.com/shawjef3/webasm
&gt; You just need to import it. That's pretty simple. What if there is no context to import for testing purposes? I will need to create one myself or mock one. &gt; You'll either pass the context through the call graph or use implicits. There is no other way. Not sure if I don't understand you or the other way around. I have `fa`, `fb`, `fc` and `fc` expects to get some context to create/execute a future. I will need to declare the context for `fa` if it calls `fb` and for `fb` if it calls `fc`. If I use Task, however, I can just call `fa` (which calls `fb` which calls `fc`) and I will have a `Task` return which has not been executed and is still waiting for the context. I can now pass the context to the task. All my functions now don't need a context in their parameter list. You worked with Monix Task, so I guess you understand that - so why do you say I'll still pass it down the call graph or use implicits?
Someone at a meetup called anything like _._1, _._2 the butthole operator.
&gt; Some events are exposed through handlers (like `onclick`) `onclick` is a native DHTML DOM handler, not a FRP event handler. It's out of the reactive data-binding system per se. &gt; 'monotonically increasing' from `None` to `Some`. I did not understand the meaning of this sentence. `Binding` means data-binding, i.e. the result always changes according to source data changing. It's not a reactive stream. I tend to call it *reactive data-binding* or *reactive templating* instead.
That may be true(maybe?). My point was that React was never meant to "be enough". It was only meant to be 1 layer of a multi-layer architecture. 
&gt; I see. So your system is only data binding, not FRP. It depends on the meaning of FRP. - Binding.scala is Functional, as the `Binding` expressions are built from a `Monad` with no side effects (unlike other impure FRP frameworks). - Binding.scala is Reactive, since the result changes reactively according to source changes. - Binding.scala is Programming. You can use any Scala expressions in `Binding` blocks (unlike Vue's XML templates, which consist with a lot of `v:` attributes, not regular programming language)
Thank you for that information. I think I understand Binding.scala better now.
Working on a library whose primary focus is a query DSL for Doobie. Right now it's fully functional with Postgres, assuming you only care about ANSI SQL instructions (adding support for others would be trivial assuming still ANSI only. The "driver" is [very small](https://github.com/Jacoby6000/scoobie/blob/master/doobie-postgres/src/main/scala/scoobie/doobie/doo/postgres.scala)). It's called [scoobie](https://github.com/jacoby6000/scoobie) and although doobie is the primary focus, you could also have it produce queries for Slick, Squeryl or any other database library if you wanted. Maybe even Spark (not so sure about that). Another thing that's cool is that the DSL is written completely separate from the drivers and the AST, so you can use any query DSL that you want. Right now I've only got a SQL DSL, but making new ones should not be very difficult. 
yes I have. one of milestones in my FP learning.
I... think so? I'm mostly just rehearsing syntax, messing around with Play. Recently I've been delving a bit more into the FP principals behind scala, and after a bit more practice I'll prolly return to Oderski's course. What would you recommend?
It looks like it's a template full of utilities and configs for an akka-based microservice. It's like putting out a code style guide, but specific to how a given service should be built and talk to other services/the outside world. 
Who tried or using Emacs for Scala development? Is it good compared to IntelliJ Community with Scala plugin?
Did you not read this: http://paypal.github.io/squbs ? It's aiming to be an enterprise level application framework for building highly scalable applications. They packaged up a bunch of components that enterprises often need to manage their fleet of applications and stacked it all together under squbs. It's probably one of the few (besides play) that I know of that is a reactive application stack.
Is there a way to do this with grade?
I checked few at random and they all had a line similar to this in their READMEs: &gt;Generated from TypeScript definitions based on React v15.0. But hey, that means there's a semblance of interop between Scala and Typescript. Neat.
Ok so its not just me
Here's the [React4s](https://github.com/Ahnfelt/react4s) implementation. It's the same number of lines as Binding.scala. Unlike Binding.scala, no macros are involved. case class TagContainer() extends Component[NoEmit] { val tags = State(List("initial-tag-1", "initial-tag-2")) override def render() = E.div( Component(TagPicker, tags()).withHandler(newTags =&gt; tags.set(newTags)), E.h3(Text("全部标签：")), E.ol(Tags(for(tag &lt;- tags()) yield E.li(Text(tag)))) ) } case class TagPicker(tags : P[List[String]]) extends Component[List[String]] { val tag = State("") def addHandler() = { if(tag() != "" &amp;&amp; !tags().contains(tag())) { emit(tags() :+ tag()) tag.set("") } } override def render() = E("section", E.div(Tags( for(t &lt;- tags()) yield E("q", Text(t), E.button(A.onClick(_ =&gt; emit(tags().filterNot(_ == t))), Text("x")) ) )), E.div(E.input(A.bindValue(tag)), E.button(A.onClick(_ =&gt; addHandler()), Text("Add"))) ) } 
Right, my point is that maven means your normal code-quality processes (including but not limited to code review) automatically apply to build-time logic, because all your build-time logic has to be placed in plain old first-class code. Whereas using SBT will lead to build-time logic bypassing all your usual processes unless you make a deliberate effort to introduce suitable process to deal with it.
A very good example showing the difference between a **reactive templating language** and **raw reactive components**
Interesting. Can you share an example of how the code looks without macros?
I don't recommend this style. However you can have a try.
Yep, that's exactly what I did - automated translations from DefinitelyTyped's TypeScript definitions.
A boolean flag like that in your object is a red flag - it usually means you should have a `sealed trait` with two possible subtypes instead. Or just make `valid` a function that checks whether `messages` is empty. Make invalid states unrepresentable - what state does `Result(false, Set("error!"))` represent? Think in terms of representing your intermediate states as values - rather than modifying a value, try combining a value and an action to produce a new value. So think of how you could go from a `Result` and another rule, or the `Result`s of two different rules, to a combined `Result`. And then it's actually the same case as "use the sum function instead of looping over the collection. I've got to go so to skip to the end, `Validation` already exists in scalaz and cats, so you'd write something like: val result = rules foldMapA { rule =&gt; data foldMapA { el =&gt; if(conforms(el, rule)) Success({}) else Failure(Set("...")) } } Better still, try to find a way to include the object in the `Success` so that if validation fails the object simply doesn't exist and can't be accessed. Better still, build the validation into the definition of `X` rather than doing it independently. Again, make invalid states unrepresentable.
I'm working on extending the [React4s](https://github.com/Ahnfelt/react4s) coverage of CSS styles and HTML5 elements.
you're awesome
The next step would then be to realize that this could be understood as a computation in suitable monad that allows collecting errors as a side effect. There are plenty of monad tutorials out there for haskell but also for scala.
Since you're an experienced programmer, I really recommend either the [Coursera course](https://www.coursera.org/learn/progfun1) or the [Red Book](https://www.manning.com/books/functional-programming-in-scala). Both are great at building that "functional muscle". The former is a bit easier of an on-ramp.
 object Main { class X(val name: String, val num: Int) def validate(inputs: Seq[X]) = { def validateX(x: X) = { def validateName(s: String): (Boolean, Option[String]) = { val isValid = !s.isEmpty &amp;&amp; (s charAt 0).isUpper val msg = if (!isValid) Some(s"Ouch! First letter should be upper case in $s") else None (isValid, msg) } def validateNum(n: Int): (Boolean, Option[String]) = { val isValid = n &gt; 0 val msg = if (!isValid) Some(s"Ouch! Num should be greater than 0 in $n") else None (isValid, msg) } val elements = Seq(validateName(x.name), validateNum(x.num)) val isValid = elements forall { case (isValid, msg) =&gt; isValid } val msgs = elements flatMap { case (isValid, msg) =&gt; msg } (isValid, msgs) } val validations = inputs map validateX val isValid = validations forall { case (isValid, msg) =&gt; isValid } val messages = validations flatMap { case (isValid, msgs) =&gt; msgs } case class Result(val valid: Boolean, val messages: Set[String]) new Result(isValid, messages.toSet) } def main(args: Array[String]) = { val xs = Seq( new X("Bob", 30), new X("tom", 40), new X("John", 1), new X("bob", 0)) println(validate(xs)) val xs2 = Seq( new X("Bob", 30), new X("John", 1)) println(validate(xs2)) } }
Are the translations completely automatic, with https://github.com/sjrd/scala-js-ts-importer ? I thought its output needed some manual adjustments in certain edge cases...
There used to be 2 pdfs on the main scala site, not sure if they are still around and they'd be a little dated now, but probably still good. One was longer than the other. They had good problems and sample code, from what I recall. I think 2 of these might be them: http://scala-lang.org/docu/files/ScalaTour-1.5.pdf https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/ScalaByExample.pdf http://www.scala-lang.org/docu/files/ScalaOverview.pdf But I don't think I'd try to learn a language by playing with a large framework. That's not what I'd call easing into it. :) They probably use tons of implicits and clever syntax. I wouldn't go totally nuts with attempting to be 110% functional. Odersky's course was good.
Actually, Scala's `Foo with Bar` is pretty similar to TypeScript's `Foo &amp; Bar`.
Since you are a master's student, it would be worth reading this: http://www.cs.nott.ac.uk/~pszgmh/fold.pdf
So, I am coming from Java as well and am just now starting to get this functional stuff. "Try" and write your program as static functions (i.e. on an object in Scala) and grow from there. For your use case, the question is what do you do with the negative results? I would point you toward validated: http://typelevel.org/cats/datatypes/validated.html Ok, so what the heck does that mean. Imagine I have a function to validate my things. That function could EITHER return a List of errors, OR, a VALID object that I could actually work with. In standard scala-isms, I might have an Either[List[Error], X] where X is the type I am looking for. What I want to do is take some input, in your example, let's say it is "InputData" because that is generic enough to ruffle feathers but not too much. So what I want is the following: def validate(in: InputData): Either[List[Error], Result] Now, each of your rules would be something like: def myNumberCheck(num: Int): Either[Error, Int] def myEmailCheck(email: String): Either[Error, String] If you are using Either (in Scala 2.12), then these compose nicely as Either is a monad: for { _ &lt;- myNumberCheck(in.number) _ &lt;- myEmailCheck(in.email) } yield Result(in.number, in.email) Ok, but Monads short circuit, meaning if the numberCheck fails, you will will not know the email check fails, because it will not be checked! OUCH! So, you want to run ALL the checks. This is where Validated from cats come in, and specifically Applicatives. Whereas Monads will shortcircuit your checks (stop at the first failure), Applicatives will run ALL of your checks, and will give you a list of ALL errors, or the valid result. The downside is there is no such thing in Scala as a for comprehension with syntax for Applicatives. You have to use the Cartesian syntax that comes with Cats. It looks like a Tie Fighter from star wars, the at sign between two bars |@| So, it is a little unweidly to work with because it isn't a for comprehension, but if you manage your whitespace, it can look pretty good. So, "ValidatedNEL" is "Validated non empty list", which means that you have at least ONE error on the "Left" side, or negative side". On the right side you have the "Result" you are looking for. Let me know if this makes sense. This damn reddit textarea is so small, I hope I get the gist across. Edit: This is a good example using the cats library: https://gist.github.com/owainlewis/87d2e9c43d3d9e188cdfa839136f1e9e That validateUser example is what I am talking about. I don't see the Cartesian syntax (does Cats have this?) but I think this is what you need. 
OK, so looking at your code, you need a list of messages to shove into your `Result`. Let's say that list of messages is your desired output. And you start with a list of rules and a list of elements. That's your input. When dealing with collection operations, it's useful to plan out a mental path from your inputs to your outputs. You've actually sketched it out with your loop. The nested loops imply that you want to check every rule against every element. Let's say that you have M rules and N elements. This means that you could produce up to M*N messages. But you don't want that many messages. You want to cut that number down. You only want the messages where the element doesn't conform to the rule. So let's do that. Let's find the rule/element pairs, then let's find the pairs that do not conform, and then let's turn each nonconforming pair into a message. It's useful to keep in mind the cardinality of each of the list operations. For example, [`map`](http://www.scala-lang.org/api/2.12.0/scala/collection/Seq.html#map[B]\(f:A=&gt;B\):Seq[B]) always produces a collection with the same length as the input collection. [`filter`](http://www.scala-lang.org/api/2.12.0/scala/collection/Seq.html#filter\(p:A=&gt;Boolean\):Repr) produces an output collection that's potentially shorter than the input collection, and [`flatMap`](http://www.scala-lang.org/api/2.12.0/scala/collection/Seq.html#flatMap[B]\(f:A=&gt;scala.collection.GenTraversableOnce[B]\):Seq[B]) can produce an output collection that's larger *or* shorter than the input collection, depending on the particular callback function that is used. I focus on these three because they are the relevant ones for your code. OK, so lets first get the M*N pairs of rules and elements. That's easy: for { rule &lt;- rules el &lt;- elements } yield (rule, el) Or, we can remove the syntactic sugar: rules.flatMap { rule =&gt; elements.map { el =&gt; (rule, el) } } The Scala compiler does this transformation internally. The `for` expression is perhaps more intuitive, but it's worth trying to understand the desugared form as well. Start on the inside: given that we already have a `rule`, the call to `map` will produce a list with the same length as `elements` (N). But each item in that new list will actually be a pair. OK, so what about the call to `flatMap`? Well, we'll call the `flatMap` callback function M times. And as we already said, each invocation of the `flatMap` callback produces N pairs, so that's how we get our M*N pairs. But that's too many pairs. We want just the ones that do not correspond. That's a job for `filter`: rules.flatMap { rule =&gt; elements.map { el =&gt; (rule, el) } }.filter { case (rule, el) =&gt; !conforms(el, rule) } Note the pattern matching syntax in the `filter` callback. It's nothing special; it just breaks apart each pair that we built in the previous step. This will cut our collection down to just the rules that we care about. OK, but we still have pairs. We really want messages. We want as many messages as we have surviving pairs. That's a job for `map`: val messages = rules.flatMap { rule =&gt; elements.map { el =&gt; (rule, el) } }.filter { case (rule, el) =&gt; !conforms(el, rule) }.map { case (rule, el) =&gt; s"Rule $rule failed on element $el" } And now that we have that, we can build our result: new Result(messages.isEmpty, messages) OK, but we don't have to leave our code looking like that. Because collection transforms are so common, we can return to the syntactic sugar that we had up above: val messages = for { rule &lt;- rules el &lt;- elements if !conforms(el, rule) } yield s"Rule $rule failed on element $el" Note that these aren't quite the same as our long-form code. Technically, that shorter code desugars to: val messages = rules.flatMap { rule =&gt; elements.withFilter { el =&gt; !conforms(el, rule) }.map { el =&gt; s"Rule $rule failed on element $el" } } In this case, we're never actually constructing the pairs; instead, as we're processing each rule, we filter the elements to just those that don't confirm to the current rule. We also map the result immediately, rather than producing pairs to be formatted later. As an added bonus, this will likely be more efficient.
Maybe something like this: for(rule &lt;- rules; el &lt;- data if !conforms(el, rule) { yield (rule, el) } }.map { case (rule, el) =&gt; "Rule " + rule + " failed on element " + el } I don't see how it's conceptually difficult to create the set of messages before adding it to the Result object.
The idea is to think of it as just a series of data transformations. You want to transform your list of objects to a list of true/false values. The rule is the transformer. This is called `map`. Or we could say that you want to transform your list of objects to a smaller list, that is, only keeping ones that match your rule. Once again, the rule is the transformer. This is called `filter`. In either case, we end up with a list of items, that is, the ones that don't match our rule. We want to get a single value out of this list. The value we're going for is the count, which we get by iterating through each item and adding one for each one we find. This is called `reduce`. Once we have the count, we can then compare it against 0 to tell if it's valid or not. We also happened to have a list of all the items that didn't match the rule, so we can return those at the same time as well. 
I'm building a low level DOM manipulation library for Scala.js. The purpose is to make a type safe, unopinionated foundation for other, higher level UI libraries that does not use virtual dom or macros. More detailed rationale here: https://github.com/raquo/scala-dom-builder It's still very much a work in progress. I'm building it because I need this functionality for my higher level UI library (Laminar) which is currently based on a virtual DOM paradigm for no good reason. I can simplify its internals very much if I drop virtual DOM. So I thought let's make this replacement low level DOM lib reusable and publish it separately.
Because attempting to Google how to generate the two UDFs properly was challenging. From when I started (assuming it would be trivial and someone else had surely done it) to when I finished ended up being ~6h of work (including fiddling with it as I discovered nuances of using a UDF from Spark SQL). IOW, there wasn't anywhere that modeled this in a nice simple applied way within Spark. And the places I did find were actually more confusing than helpful.
That seems sound, however the data contract with the client is set for now and the deserialization of that Person object would no longer be as expected { "id":"asdfasdf", "firstName": "bob", "phoneNumbers":["333", "444"], "email":"333@null.com" } would now be { "id":"asdfasdf", "firstName": "bob", "phoneNumbers":{numbers:["333", "444"]}, "email":{"email":"333@null.com"} } edit: formatting
Great post!
Actually it depends a little bit on *why* you want exactly those fields in the List. Depending on this, you would either catch their names or their positions, or their type, ... I made an example for their positions, take a look here: https://scalafiddle.io/sf/8DmjJuV/0 This is not beautiful, but I don't want to spend much time making it look nice. You will see that you don't need to annotate anything here. You *could* in fact still do (if you want to not rely on the position of the fields) but your example is still a little strange to be honest. ;) Still, this code uses no reflection! It is complete runtime safe. E.g. try to remove the email field and you will get a compile error, not a runtime error like you would get with reflection.
Hi, how did you represent tables in your types? I play a football management game for which I want to transform the player game stats, and calculate more advanced stats etc. and maybe thought that might be useful. At the moment the code is really repetitive.
A case class covering row level data is common, and then the table is a collection of such rows
I don't like it. Your types are trivial and you're beginning to write code. Probably because your types are too trivial.
Yup–we were using Spark, so Dataset[RowType] for us.
 psnively@Ragnarok ~$ amm Loading... Welcome to the Ammonite Repl 0.8.2 (Scala 2.11.8 Java 1.8.0_31) @ import $ivy.`org.scalaz::scalaz-core:7.2.10`, scalaz._, Scalaz._ import $ivy.$ , scalaz._, Scalaz._ @ case class X(val name: String, val num: Int) defined class X @ def nameRule(n: String): ValidationNel[String, String] = if (n(0).isUpper) n.successNel else "Invalid name.".failureNel defined function nameRule @ def numRule(n: Int): ValidationNel[String, Int] = if (n &gt; 0) n.successNel else "Invalid number.".failureNel defined function numRule @ def validate(x: X) = (nameRule(x.name) |@| numRule(x.num))((a, b) =&gt; x) defined function validate @ val input = List(X("Paul", 25), X("steve", 12), X("John", 0), X("robert", 0), X("Dogbert", 100), ("William", 15)) input: List[Product with Serializable with Object] = List(X(Paul,25), X(steve,12), X(John,0), X(robert,0), X(Dogbert,100), (William,15)) @ input.map(validate) cmd6.sc:1: type mismatch; found : $sess.cmd1.X =&gt; scalaz.Validation[scalaz.NonEmptyList[String],$sess.cmd1.X] required: Product with Serializable =&gt; ? val res6 = input.map(validate) ^ Compilation Failed @ val input = List(X("Paul", 25), X("steve", 12), X("John", 0), X("robert", 0), X("Dogbert", 100), X("William", 15)) input: List[X] = List(X("Paul", 25), X("steve", 12), X("John", 0), X("robert", 0), X("Dogbert", 100), X("William", 15)) @ input.map(validate) res7: List[Validation[NonEmptyList[String], X]] = List( Success(X("Paul", 25)), Failure(NonEmpty[Invalid name.]), Failure(NonEmpty[Invalid number.]), Failure(NonEmpty[Invalid name.,Invalid number.]), Success(X("Dogbert", 100)), Success(X("William", 15)) ) @ res7.map(_.map(Set(_))) res8: List[Validation[NonEmptyList[String], Set[X]]] = List( Success(Set(X("Paul", 25))), Failure(NonEmpty[Invalid name.]), Failure(NonEmpty[Invalid number.]), Failure(NonEmpty[Invalid name.,Invalid number.]), Success(Set(X("Dogbert", 100))), Success(Set(X("William", 15))) ) @ res8.partition(_.isSuccess) res9: (List[Validation[NonEmptyList[String], Set[X]]], List[Validation[NonEmptyList[String], Set[X]]]) = ( List(Success(Set(X("Paul", 25))), Success(Set(X("Dogbert", 100))), Success(Set(X("William", 15)))), List(Failure(NonEmpty[Invalid name.]), Failure(NonEmpty[Invalid number.]), Failure(NonEmpty[Invalid name.,Invalid number.])) ) @ val (good, bad) = res8.partition(_.isSuccess) good: List[Validation[NonEmptyList[String], Set[X]]] = List(Success(Set(X("Paul", 25))), Success(Set(X("Dogbert", 100))), Success(Set(X("William", 15)))) bad: List[Validation[NonEmptyList[String], Set[X]]] = List(Failure(NonEmpty[Invalid name.]), Failure(NonEmpty[Invalid number.]), Failure(NonEmpty[Invalid name.,Invalid number.])) @ good.foldMap() res11: Validation[NonEmptyList[String], Set[X]] = Success(Set(X("Paul", 25), X("Dogbert", 100), X("William", 15))) @ bad.foldMap() res12: Validation[NonEmptyList[String], Set[X]] = Failure(NonEmpty[Invalid name.,Invalid number.,Invalid name.,Invalid number.]) To really make this useful, I guess I'd include the invalid name and number in their messages. 
To stack on - cause I think I must be missing something here - how the heck can you get anything done in a reasonably performant manner when you add a minimum of 100 milliseconds of network IO to every single function call in your application? Great, scale horizontally, but clearly you are having to scale way too broadly far faster than you would without going micros. 
Thank you for taking time to write answer. Bunch of questions: How does it allow loose coupling of components more than simple modules (/packages/interfaces/classes) do ? If people don't do circular dependencies and reference anything that comes under their hand (i.e. persistence layer referencing view layer etc.) then surely it's not a problem. Plus you can (and I've seen) people do circular dependencies with microservices aas well :/ Also I would say it "decouples" parts, which is not good. IMO good is loosely coupled, not decoupled (hard to follow, reason about etc.). What's your take on my point of view?
Hey everyone, I'm new to Scala and I'm loving it so far! I have a question about the conventions behind parameter-less methods. So I know the idea is to include parens if the method has side effects and leave them off otherwise, but what's considered a side effect? Should you include them for something like System.currentTimeMillis()? Or Random.nextInt()?
&gt;Should you include them for something like System.currentTimeMillis()? Or Random.nextInt() Yes to both, neither of these two functions are referentially transparent, meaning that different calls to System.currentTimeInMillis() or Random.nextInd() are not guaranteed to yield the same value (because they perform the side effect of performing IO to get the system time, or generate a random number).
One team to one service sounds nice, at my work we seem to be approaching 1-1.5 services per developer
Print preview suggests that it's actually 27 pages! I kid, I kid. This reminds me somewhat of the [Visibone references](https://visibone.com/), which is a good thing! Concise references with clear examples are always welcome.
Maybe just change the one-word comment from "implicit" to "inferred".
makes sense, we can update that. thx.
well, anything can fit in 1 page when the page is long enough :-)
Scala for the Impatient was really helpful to me as a person with experience in C#, JavaScript, and Ruby. You can decide if that's similar enough to your background I guess.
Neat structure, well done!
Ah, ok, I also use it like that so I can just use the builtin union, intersection etc. Had thought there would be a better representation that I'm missing, good to know I use the common approach.
Use polymorphism, where all the gates implements a trait that defines a getOutput() trait HasLogicOutput { def output: Boolean } case class AND(a: HasLogicOutput, b: HasLogicOutput) extends HasLogicOutput { override def output: Boolean = a.output &amp;&amp; b.output } case class OR(a: HasLogicOutput, b: HasLogicOutput) extends HasLogicOutput { override def output: Boolean = a.output || b.output } case class NOT(a: HasLogicOutput) extends HasLogicOutput { override def output: Boolean = !a.output } case class XOR(a: HasLogicOutput, b: HasLogicOutput) extends HasLogicOutput { override def output: Boolean = AND(OR(a, b), NOT(AND(a, b))).output } case class NAND(a: HasLogicOutput, b: HasLogicOutput) extends HasLogicOutput { override def output: Boolean = NOT(AND(a, b)).output } case class NOR(a: HasLogicOutput, b: HasLogicOutput) extends HasLogicOutput { override def output: Boolean = NOT(OR(a, b)).output } case class XNOR(a: HasLogicOutput, b: HasLogicOutput) extends HasLogicOutput { override def output: Boolean = NOT(XOR(a, b)).output } case class Switch(var state: Boolean = false) extends HasLogicOutput { override def output: Boolean = state } If you so desire, you can cache the internal gate objects used in the more complex gates to prevent needless object creation 
I'm still really new to Scala and don't really understand this stuff that well and need to study more, but thanks a lot anyway! The gates are now working. Since you seem to know a lot, I have another question, if you don't mind: If I can somehow connect the output of a gate to one of its own inputs (e.g. connecting the output of an OR gate to one of its inputs, which would cause it to be permanently "locked on" as soon as either one of the inputs was switched on), can Scala handle the feedback loop without going haywire? I tried this on Python and it lead to excessive recursion which caused the program to crash. I got the impression that Scala should be able to do this as it's a functional programming language, but is that actually the case? E: Apparently not. I built an SR latch and it got stuck in an infinite recursion loop and crashed just like it did on Python. I think I'll have to find some other way of doing this because many logic circuits (CPUs come to mind) have feedback loops.
we will update that. thanks.
that is correct, we forgot to include that. i'll fix it. thanks.
Separate the running out the logic from the description. Use free for this task, the one in cats is already stack safe.
Where is this linked from?
This is actually an external service so alas we went be getting any of those juicy email addresses.
Your simulator is missing the concept of time, as digital logic operates as an event loop. If you introduce a high/low flag, each element can memoize steady state value. Then a feedback loop wouldn't cause infinite recursion. You could still model clocks which would tick using this base unit. Think more in terms of digital logic, less in terms of Scala.
What language have you been exposed to previously? We may be able to draw parallels. 
Awesome, I just added your link to a sbt-scalariform issue I created. Hopefully they'll update sbt-scalariform with the latest version!
One thing you could do as well, is create a description of the circuit in scala as an abstract syntax tree, then substitute out known circuits / optimizations as it recognizes them.
So basically using algorithms to simplify the circuit as much as possible? That would work if I was only interested in the end result, but I'm trying to achieve something that shows the entire circuit so I can watch every component do its thing in real time.
Have you been exposed to Minecraft's Redstone? having decaying power work like that over tiles certainly sounds that way :P
Yes, but undefined in the same sense as digital logic has. Then when he introduces a clock to drive the process it will mirror a hardware designer's expectations.
https://issues.scala-lang.org/browse/SI-7141 https://www.safaribooksonline.com/library/view/programming-scala-2nd/9781491950135/ch04.html http://docs.scala-lang.org/tutorials/tour/pattern-matching.html
My work project is currently moving all its data processing, caching, and business rules into Postgres. We are trying to keep our Scala backend/frontend dead simple. Out of curiosity, what problems did you face with SQL?
Thanks for that! Definitely learned quite a bit from your comment. BTW your username checks out :)
It's funny because fromscalatohaskell's username is very applicable to their comment. ^^*beep* ^^*bop* ^^if ^^you ^^hate ^^me, ^^reply ^^with ^^"stop". ^^If ^^you ^^just ^^got ^^smart, ^^reply ^^with ^^"start".
I'm using Scala for real time calculations, and I keep tripping on performance traps like generic functions causing integer boxing, or for loops being way slower than while loops. Things you'd expect to be innocuous in any other language but due to the tradeoffs inherent in Scala's design are unexpectedly expensive. What's a good source to learn, essentially, how to avoid borking performance in Scala-specific ways?
Looks like you're trying folding, which is great, but let me point out something perhaps a bit surprising: building up a list of errors by repeatedly adding on to a mutable list would also be a perfectly purely functional approach, as long as you follow the golden rule of FP: always return the same output for the same input! So you might try: class X private (...) object X { def apply(...): Either[List[String], X] = // Do each check and build the error list // If the error list is empty, result is Right(new X(...)) // Otherwise, result is Left(errorList) } Also note that I made the X main constructor private--now the only way to get an X value is by using the smart constructor like: X(...), which gives you an Either[List[String], X]. The important points are: internally you're using a mutable list and building it up, but externally no one can know that--in functional terms we say that the outside world cannot _observe_ the impurity and thus it doesn't matter! And the second point is as I mentioned above--you always return the same output for the same input.
Both are great ideas that I'll at least practice. The second one will probably be the path because compile-time-safety would be fantastic alarm bell and help attract maintainers' attention when things change. 
We've experienced the same problem. Probably my biggest regret in choosing Scala. I don't know if there is any easy answer. Basically, I studied the generated bytecode until I got a good feel for how various constructs were getting compiled. The performant subset ... looks like Java. While loops, explicit primitive types, no type parameters (or specialization), and no Scala collections. But be wary of specialization, it definitely interacts badly with other language features and lead to incorrectly generated bytecode. See: https://axel22.github.io/2013/11/03/specialization-quirks.html (Quirks, ha!) I haven't read it yet, but Chapter 3 of High Performance Scala Programming discusses various language features and the corresponding bytecode: https://www.amazon.com/Scala-Performance-Programming-Vincent-Theron/dp/178646604X 
I don't have the exact context of your problem but I could see the argument that if you are adding an argument with a default value, that default value should be a valid state. If it is invalid to have the default value, it should either be represented with a different type such as Option[Int] instead or not have a default value. I mention this because I believe that if you have use cases where the default value could be invalid, you will have more in the future. If the default value is valid for new uses of the type but older uses should be corrected, maybe it should be a different type. You could also do the refactoring in 2 passes. Add the argument to the type without default value first, update the uses of the type that don't have the use of the new argument, then add the default value.
is there any work currently being done to make scala able to take advantage of java 9 modules? 
Well from talks with Akka maintainers and many people that worked with Akka these problems are to be expected: * Akka Actor is pretty low level construct - actually it is the lowest level construct in an actor model, * as actor model primitive it is fast - faster than Akka Streams build on top of it - but it is also untyped (everyone who use Actors waits for Akka Typed for reaching maturity), * usually tutorials try to show things in simplified manner... which they often do by putting logic inside actor. That is not a good approach - domain logic should lie in separate place and actors should be only used as infrastructure on which this logic is run while the results shoudl be wrapped in messages - this way one can test the domain logic without making it dependent on actor infrastructure. But I guess you already found out all of that the hard way...
I read lots of interesting job posts / blog posts where companies are doing cool stuff with reactive streams, and they all sound really interesting to me. As a learning exercsive I have written a few toy apps that use Akka streams, but with the scale of the data is very small. I am looking for ideas of a project I could make that would use a larger amount of data, so I can encounter (and solve) problems that don’t arise in my current toy apps. My current roadblock is figuring out what the source of my data would be. 
Yes in the 2.13 branch
Looking for Open Source project to contribute. I have 1 year Scala experience but zero open source contribution. All I want to jumpstart is bunch of issues which I can assign.
&gt; I'm using Scala for real time calculations Honestly no JVM language is ever going to be any good for real time work because of the GC.
I mean, three of the biggest stream processing engines to date including Apache Flink, Twitter Heron, and Apache Storm are written in Java, Scala, and Clojure so that is a significantly controversial claim.
Stream processing and real time are not the same. It's possible you're using a different definition of real time than i've been exposed to, but I would not consider any of Flink, Heron, or Storm to be real time frameworks. My experience in real time programming was robotics applications in C++, but the general features were that there was a rate limiting code execution path such that updates could be guaranteed to be processed within a regular interval, say 100us. This typically requires a dedicated, real time operating system such as RT Linux, xPC, or QNX. JVM languages simply aren't suitable for these types of applications, because they cannot guarantee execution within a particular upper bound. When the GC triggers you're going to have a task execution time over run.
You really put your business rules into a database? Sounds weird to me!
Try [`shapeless.Poly`](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#polymorphic-function-values)
I think some of the Doobie stuff is a little inaccurate. With Doobie, inside one transaction is synchronous, but you'll can compose transactions in parallel using the monad you interpret to. Also, while it doesn't currently support type checking queries at compile time, it's on the roadmap. Given correct queries, which are easy thanks to the check function, the rest of the library is type safe; you don't get back am untyped map, and there's nothing that will through exceptions like the `nextString` example. So I think it's more accurate to say "partial" for route safety with Doobie. Also, I'd love to see a similar comparison of http frameworks
&gt; Akka-streams is probably a bit better though. Akka-Streams and actors are two completely different things. Like raghar pointed out, actors are an incredibly low level primitive which is used in concurrent programming, it should be treated as a last resort rather than a first. The thing is, you do need these low level primitives to actually create these high level libraries which we can reason about in a way that is performant i.e. iirc, scalaz task depends on scalaz future (which is a slightly different implementation of Scala future) which in turns depends on scalaz actor (which is a limited implementation of akka actor).
How so? It makes perfect sense to me--the database already has all the data, powerful declarative and procedural syntax for manipulating it, and can process and give me exactly the data I need, with no wasted traffic. I can't imagine a more ideal architecture.
There's good coverage of variance in the Coursera course : Functional Programming Principles in Scala, Lecture 4.4 Variance. There Martin Odersky relates contra/co-variance back to Liskov Substitutability. You may want to read up on Liskov Substitution elsewhere though as that lecture by itself will probably leave you scratching your head. Liskov Substitution is closely related to Design By Contract from Bertrand Meyer - see the book Object-Oriented Software Construction, an excellent book but not for the faint of heart. 
I think it's production-ready in the sense that the queries it generates are correct, however in some cases, for more complex queries, you'll still encounter bugs that might force you to use a plain query. That is of course a problem, but I can bet the other libraries have bugs as well :) Quill seems more light-weight - no meta-model, less DSL to learn (you use more of Scala's syntax) - however, the macros can sometimes fail and leave you puzzled with a weird error message. So, "it depends" :)
&gt; Doobie looks nice, but docs are incomplete and very chaotic. Plus they are currently migrating from Scalaz to Cats, and some things (streaming result sets) are just not possible with Cats backend. The Cats-branch uses FS2 to provide out-of-time constructs and should be stable. I also quite like the [Book of Doobie](http://tpolecat.github.io/doobie-cats-0.4.0/00-index.html)
There seems not much hope to rely on IDE when working with functional/generic style libraries (e.g., cats/scalaz/shapeless) due to Scala's (relatively) *powerful* type system.
I agree! I've gotten bitten by the shortcut of using default params to expand my `case class`s, too. It's best to resist the urge!
You may want to see [Doobie's Fragments](http://tpolecat.github.io/doobie-scalaz-0.4.0/08-Fragments.html). Doobie actually constructs `PreparedStatement`s even from its `sql` interpolator—it's not a free-for-all.
PL/pgSQL is typed. Edit: I've never heard about untyped sql dialects...
Quill has way less boilerplate, that alone makes it worth the trouble. The problems mentioned are not grave, there are multiple ways to express the same thing in quill, and sometime multiple joins are problematic, you might need to use a simpler for comprehension, or just chain method calls, there where no instance where one of the approaches would not work. Quill is still under heavy, active development .
Oops, that's an old comment. The initial cats port didn't support streaming but it does now. Arguably better because fs2 fixes a lot of issues in scalaz-stream. (fixed)
Another option if you like writing your own queries is https://github.com/rocketfuel/sdbc.
Will Postgres somehow reject to save such functions when I try to do bad stuff instead of giving me a runtime error? But even if so: I would still rather define my functions in a way that constrains datatypes in the best way that my typesystem allows, so that I am not able to multiply two IDs or add the salary to number of sold articles or stuff like that.
&gt; Will Scala do so? Because that's not what static typing means... That's why I said "somehow", because there needs to be a mechanism that stops me from doing bad stuff before it can even get executed. &gt; Like in postgres... &gt; CRUD functions for ya'. Errr, I'm sorry, but I have no clue what you mean. Should I take this as a "yes, you can't statically prevent that in postgres"?
&gt; That's why I said "somehow", because there needs to be a mechanism that stops me from doing bad stuff before it can even get executed. It's impossible in a turing-complete language. Especially, if the type system is tc too... &gt; Errr, I'm sorry, but I have no clue what you mean. Should I take this as a "yes, you can't statically prevent that in postgres"? Nope, with an algorithm. As in Scala.
* How about unit tests? * In general you need much more infrastructure to develop / debug / maintain business logic I don't know much about PL/pgSQL, but what are its capabilities to mind DRY? Also indirection / all kind of strategies seems difficult to realize in SQL to me.
In scala-ide I find the presentation compiler highlights are sometimes wrong, but if you use the "Problems" tab instead that's 100% reliable. (And there's a very subtle difference in the icon you see in the editor between genuine problems and presentation-only compile failures).
&gt; Scala 2.12.2 adds support for trailing commas I wouldn't expect a minor release to include a syntax change in the language. Now there is code that will compile with 2.12.2 but not with 2.12.1. Opinions on this?
* Weite integration tests in Scala that hit the Postgres DB and verify stuff like, your CRUD operations have the desired effects, etc. * Not sure what you mean by infrastructure. You can get started with Postgres on a single box. PL/pgSQL is a powerful procedural programming language with user-defined functions, loops, conditionals, and the other conveniences. But on top of that, Postgres also comes built in with Perl, Python, and other language support. Check out the user manual, it is really excellent.
When one fixes a compiler bug that prevents compilation of valid code, one also creates the same situation where code compiles with the next version but not the older one. Yet no one would argue that compiler bugs should not be fixed in minor versions.
One option is to use method overloading. Methods with different type signatures can coexist and do related tasks: class A { def m(i: Int): String = i.toString def m(s: String): String = s } It's a nice way to achieve ad-hoc pattern matching without having to build a sealed trait hierarchy (aka a sum type).
Thanks for your reply! Yes, regarding synchronicity that's what I meant - if there's indeed a noticeable speedup thanks to Slick's connection-sharing would be a subject for yet another blog, with some benchmarks :)
Could you maybe let me know which OS/browser? Definitely scrolling should work fine, sorry for the trouble :)
That's a good point, and that's true that in both cases you do need tests. You always need tests ;) But I think there's space for a middle ground. With a meta-model and type-safe queries, the area where you could have bugs is centralized - you only need to verify that the meta-model corresponds to the DDL. In fact, maybe it would make sense to have `.check`-like method for Slick's table representation? Once you know that the meta-model (or boilerplate, however you call it :) ) is correct, if there are type-safe queries, you know that they are going to be correct as well - or at least, chances are much higher :) With a pure-sql approach, you need to check every query, not only that it maps correctly, but also that it uses the correct column/table names etc. Btw. - in Slick you can also verify at compile-time that a plain sql query is correct (https://github.com/softwaremill/scala-sql-compare/blob/master/slick/src/main/scala/com/softwaremill/sql/SlickTests.scala#L188), and I think Doobie had/will have something similar - however having a DB running at compile-time might be problematic.
That's like arguing that fixing a bug in a library changes the semantics of some calls, so it's fine to change the semantics of calls in a patch version. The semver rule is that a patch version should only change behaviour where the previous behaviour was incorrect.
As always, there is relevant XKCD comic https://xkcd.com/1172/
No it is fundamentally different. You're not changing the semantics of any valid program. You're only making more programs valid. Programs that did compile before preserve their behavior. The library equivalent is *adding* a method, not changing an existing one.
&gt; The library equivalent is adding a method, not changing an existing one. Agreed - which is something you're not supposed to do in patch versions, only minor or above.
Is Scala 2.11.11 similar to 2.11.9 and 2.11.10 ie sorta unstable? I recall someone mentioning that those two were never publicly announced due to regression issues.
Is Scala 2.11.11 similar to 2.11.9 and 2.11.10 ie sorta unstable? I recall someone mentioning that those two were never publicly announced due to regression issues.
No, at least if one trusts the announcement. This is a proper release announcement that implies they've fixed the issues with 2.11.9/10.
This is a huge deal for cats users who are doing pure FP, especially for middleware projects like doobie and http4s. Being able to rely on a standard stable IO type will really smooth a lot of rough edges.
Scala doesn't have patch versions. It's not using semantic versioning. It *is* the minor version that's incremented.
Would you say the beta(? M5) is stable enough to use for @home-development? edit: I'd like to try it out for my own enjoyment but also reap some of the listed benefits.
&gt; One might argue That's *precisely* the point. For scala 2.X.* there is a guarantee of binary compatibility. There is no guarantee of source forward compatibility (the ability to lower the scala version and recompile without changes). 
&gt; There is no guarantee of source forward compatibility (the ability to lower the scala version and recompile without changes). Three examples how this is opposite to existing precedents: 1. I literally had to fix deprecations targeting minor versions because it was considered unacceptable that warnings were introduced for minor versions. So it would be surprising if introducing mere deprecation warnings for minor versions was a big no-no, but introducing source incompatibility was fine. 2. Partial unification was only added to 2.11 behind a compiler flag, despite having pretty much the same compatibility footprint as trailing commas. This has been the exclusive approach of how language additions in minor versions have been dealt with for a long time. 3. &gt; Although Scala 2.11 and 2.12 are mostly source compatible to facilitate cross-building, they are not binary compatible. This kind of implies in my opinion that there are _stronger_ source compatibility assurances _in_ a major version than _between_ different major versions. Would be kind of weird to say "we tried to keep 2.11 and 2.12 as source compatible as possible, but we kind of lost interest in that for 2.12's minor releases themselves." What would be the point of having major releases if 2.11.x and 2.12.1 were more source compatible than 2.12.1 and 2.12.2? --- Ignoring all that existing evidence of how source compat has been handled in the past for a minute, one has to ask the fundamental question of how it _should_ work ideally: Does it make sense to have a strict set of rules for binary compatibility (which are easy to verify and enforce with existing tooling), but very lenient rules for source compatibility (where it is much harder for developers to ensure they are not breaking their users' code)?
Am I the only one who thinks it's rather disturbing that a compilation speed that should be on par with that of other popular programming languages (granted Scala is quite different) is a premium feature? Am I missing something here? My gut feeling is putting a paywall on such potentially high-impact tooling on a significantly open-source community shouldn't be the case.
Not OP, I don't have a problem with them charging for the tools; just the cost of doing business. I'm really curious to see how they charge for this. I wonder what the risk of Lightbend incorporating these changes into the base compiler would be..
Let me first say, that I'm okayish with breaking binary or source compatibility if there are compelling reasons to do so, for instance fixing unsoundness (although even that was often handled with optional compiler flags to opt into instead of making it the default in minor versions right away.) I think the main question is: what does casually breaking source compatibility buy us? Would it preferable to take a page like [Binary Compatibility of Scala Releases](http://docs.scala-lang.org/overviews/core/binary-compatibility-of-scala-releases.html) and just dropping the "binary" word, effectively establishing that there is only one consistent set of rules for compatibility, or would it be preferable to have one set of rules for binary compatibility and a slightly different set for source compatibility on the off-chance that Scala devs want to add language features in minor releases again? I think the current situation of having one page which spells out the all the great guarantees of binary compatibility, but doesn't even tell users that source compatibility is a completely different matter (or at least will be, starting with 2.12) smells a lot like the embarrassing stance some people have on language simplicity – "I counted the pages of Java's, Kotlin's and Scala's specs, Scala is the smallest and therefore the simplest" while conveniently forgetting to mention that one of the major crucial parts that interacts with every other feature in Scala, type inference, is completely missing in the spec.
Fair enough, but that makes for a pretty misleading version number. Most people expect x.y.z to be major.minor.patch. 
Indeed. You can find many discussions on this on the mailing lists.
It's absolutely brilliant. The same guys that made the total mess the scala compiler is now charge for an improved version. +1. &gt; The backend of Quala is called “tinbox” and is written in Scala, using many type-intensive libraries such as Shapeless, Circe, Grafter, and http4s/rho. but.. &gt; compile time can increase substantially woohaaa, surprise surprise. Maybe stop toying around with the language and actually stick to implementing features?
Here I'd kind of have to disagree with you. The compilation times are not that awful if you stick to simple stuff and avoid toying around trying to be fancy. The guy from the post clearly states they toy around with all kind of scala features. Good thing Zalando is doing well enough to have their teams toying around endlessly just to then have Swiss consultants go there to rip them off for 3 days by explaining them how the compiler works., which should be something any competent developer should know anyway. But oh well, at least now they can make long blog posts. Guys, we're back to the good ol' " Java Enterprise" days.
I'll rephrase: Do you even know which concerns are addressed by those libraries?
So apparently you don't know. But you don't seem to care either, so let's leave it at that. EDIT: You edited your comment, so I want to address that. Of course there are alternatives, but this is not really relevant here. If there was an alternative that was better in all aspects, the choice would be simple. You can write software in Java, so why even use Scala, with its slower compiler etc.? Why did people even adopt Java, when C++ was available? One of the main reasons to use Scala is that its type system allows us to have the compiler do work for us that otherwise would happen at run time, or would have to be asserted with tests. Libraries that use shapeless use it for generic programming, and it uses macros where usually reflection is used. You gain type-safety and performance, at the cost of higher compile times (that's obvious, since the compiler has to do more work now). That said, you could perhaps dial the condescension down a notch.
Maybe to indicate that you might be able to mutate it (override)?
&gt; if you stick to simple stuff and avoid toying around trying to be fancy Reducing boilerblate and improving typesafety is not the same as toying around. Of course you *can* toy around with advanced Scala but you seem to assume that advanced Scala always implies toying around.
You're of course right, I'm just probing trying to figure out how stable it is. I don't have the time right now to do proper beta-testing but are curious to try it out anyway.
If you allow remote, I can point you to lots of very skilled Scala devs across few different timezones. Or you can check scala gitter etc. edit: just to clarify, Im not recruiter or anything, I just worked in several teams on across world and made lots of contacts.
&gt; I basically have two layers, where the first layer decides which "mini-program" to run. The second layer is comprised of several mini-programs which all have the same return value, but different implementations of those "fetchX" methods described above. Sounds like you already have a `Free` or free-like use case then? &gt; Preferably, the first layer should do the reporting. I think using the Writer monad forces me to put the actual reports in the second layer, which is something I'm trying to avoid. If the reporting is part of the responsibility of the first layer then that's where it should happen, sure. That might mean the first layer interprets a high-level monad into a lower-level monad (or monad transformer stack) that includes a writer effect. &gt; I'm considering letting the second layer return a stream to the first layer, which can then look at the individual results and report them as they come in, but it doesn't feel like a perfect match either. In particular, terminating the stream on a Either.left/failed step is something that needs to be considered. Could you clarify more about what these "layers" are? I think you might be using the term differently from how I understand it.
this, also I don't think it's black magic what they are doing there.. I hope this will be part of scalac (in some other form) in the near future
&gt; putting a paywall on such potentially high-impact tooling &gt; but this isn't typesafe, is it? I hope whatever they are doing will be part of scalac in the near future
there exists a scalajs gitter channel for jobs
Nice project! Do you plan to support integration with the API Gateway?
Why is everybody excited about some apparently closed source commercial product that, although the text says has a plain speedup of two, when you look at the graph, achieves perhaps a 1.5x speedup by parallelizing the build - something we should hopefully expect from mainline Scala/sbt - and some secret "certain tooling" to find that throwing together the most type heavy Scala libraries with macros, inference and implicit search slows down your machine drastically? While I applaud the experiment, the overall pitch of the article is no different from the average Lightbend blog, where when two engineers meet to debug some code, we get an amazing 'white paper' and 'case study' and what not.
&gt; sbt 1.0 isn’t vaporware The mere fact that one has to state this prominently… ;)
Very helpful with a supplement, it's kind of hard to follow but definitely interesting
I'm hoping dotty will make this irrelevant.
This is not a product of Lightbend. It's some individuals (I believe former Lightbend employees) creating their own startup.
https://gitter.im/scala/job-board
I understand the result, but not the "why". What happens here?
we do not allow remote, but we offer a complete relocation package to Norway. does that help? 
Is this really so different from the difference between for { a &lt;- List(1, 2) b &lt;- List(true, false) c = a } yield (c, b) and for { b &lt;- List(true, false) c &lt;- List(1, 2) } yield (c, b) ? I suppose `unlift` is a bit less "obviously syntax" than `&lt;-`.
It's definitely a good fit for the project as I think its functionality can be very tightly coupled. However, I get really bad latency (500-800ms) on running JVM Lambdas so I'm not personally excited about running them synchronously right now. The story is better on Javascript NodeJS Lambdas. But I think it's indicative that in a recent article on High Scalability - [Serverless Architecture for doing an AWS/Slack based ask an expert site](http://highscalability.com/blog/2017/3/15/architecture-of-probot-my-slack-and-messenger-bot-for-answer.html) the one part of it that required synchronous interaction (the Slack webhook) got put on a Node.js EC2 instance instead of an API Gateway fronted Lambda. We have a Javascript team at ITV that recently built a really really simple proxy REST API with Lambda/API Gateway. Even on a warmed Lambda they got 4-50ms latencies and ended up re-writing it as a full fat Node.js service for more consistency. So... my medium term goal is to make it pleasant to write Scala JS (ie Node shimmed) Lambdas and I will certainly revisit API Gateway at that point. At the very least it will be a great prototyping tool.
&gt; import TimeSeries._ That's trying to import from an `object` (or the static parts of a java class) called `TimeSeries`. You quite possibly have got the `.jar` on your classpath, then if you want to use a class from it you have to import that class under its name. If you must use a local jar would strongly recommend putting it anywhere other than `src/main/scala`, that is likely to cause you problems (in particular the jar will probably get packaged inside your own jar). If at all possible I would avoid using `&lt;scope&gt;system&lt;/scope&gt;` at all (which will make it very hard to build and distribute your project). Instead [install the jar into your local maven cache using `mvn install:install-file`](https://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html), then you can use a normal maven dependency on it. When you get to the stage of using a repository you can do the same thing with `mvn deploy:deploy-file`. Better still of course is to build your `TimeSeries` as a maven project and then just install it locally with `mvn install`.
You can keep the `if`s while using `flatMap`. If you want to do early return of error responses the truly idiomatic way is probably to use `EitherT[Future, ErrorResponse, ?]` in a for comprehension. I tried to work your example into this style but I don't understand your example that mixes `if` with use of the value. What on earth is the type of `output1`?
Confusing to me also, i thought RT means for same set of inputs you get same output. tpolecat's example input's are in different order so output will be ordered different. 
&gt; tpolecat's example input's are in different order so output will be ordered different. Well the point is that it looks like you've just inlined `a`, that's the only thing that's changed between the two code examples.
and the ordering of the input ?
No, there is no change to the ordering of the input to any function; each value is the result of the same function call with the same arguments in the same order in each of the two examples.
That's precisely tpolecat's point. We would normally expect `val a = foo(); val b = bar(); (a, b)` to have the same result as `val b = bar(); val a = foo(); (a, b)`, but if you use this library then that isn't always the case.
Essentially, yes. What you need is actually an instance of `fs2.Async`, which boils down to implementing a concurrent, atomic reference for `IO`. I think the plan for fs2 is a bit more ambitious though, all the type classes in `fs2.util` will be dropped in favour of the cats-effect typeclasses, `fs2.Async`will be renamed to `fs2.Concurrent` and, _possibly_, `fs2.Task` will be dropped entirely in favour of cats-effect `IO`
oh yes i see now. thanks. 
No, it doesn't. It's syntactic sugar similar to for-comprehensions: ordering is important for non-commutative monads. for { a &lt;- v1; b &lt;- v2 } yield (a, b) == for { b &lt;- v2; a &lt;-v1 } yield (a, b) is always true only for commutative monads.
Use for comprehension on the futures. easiest way of busting the nested code you have above.
Which chart, specifically, are you talking about? Hydra appears in two charts, the first one gives 2.3x and the second 2.0x speedup. Note that speedup is dependent on the number of cores, so this can scale well beyond that given enough hardware. Regarding tooling, you over-simplify. The problem was: which one of the 168 files in the project is the one taking several seconds to compile alone? And yes, it's no surprise that macros slow down your compilation. But the most important question is: is it really necessary to compile for all that time? What if the implicit you *thought* is being used is instead recomputed using macros that were "winning" the implicit race against the other value? Disclaimer: I'm one of the Triplequote founders.
In general, `onComplete` is a smell that brings pitfalls, though there are a few cases where you will have to use it. You want to think in terms of transformations (A =&gt; B) instead of side-effects. The conditionals in the places you have them make things trickier, but assuming your responses can form an ADT (like `Some`/`None`, `Success`/`Failure`, etc), you don't need monad transformers or anything like that: final case class Row(a: Char, b: Char) sealed trait Response final case class SuccessResponse(row: Row) extends Response case object ErrorResponse1 extends Response case object ErrorResponse2 extends Response def query1: Future[Row] = ??? def query2(row: Row): Future[Row] = ??? def query3(row: Row): Future[Row] = ??? implicit val executionContext: ExecutionContext = ??? //Staying in the Future monad val futureResponse1: Future[Response] = { query1.flatMap { output1 =&gt; if(output1.a == 'a') { query2(output1).flatMap { output2 =&gt; if(output2.b == 'b') { query3(output2).map(output3 =&gt; SuccessResponse(output3)) } else { Future.successful(ErrorResponse1) } } } else { Future.successful(ErrorResponse2) } } } //Extracting some methods, looks a little nicer val futureResponse2: Future[Response] = { def toResponse3(output3: Row): Response = SuccessResponse(output3) def toResponse2(output2: Row): Future[Response] = { if(output2.b == 'b') { query3(output2).map(toResponse3) } else { Future.successful(ErrorResponse1) } } def toResponse1(output1: Row): Future[Response] = { if(output1.a == 'a') { query2(output1).flatMap(toResponse2) } else { Future.successful(ErrorResponse2) } } query1.flatMap(toResponse1) } //With for-comprehensions (not an obvious improvement) val futureResponse3: Future[Response] = { def toResponse3(output3: Row): Response = SuccessResponse(output3) def toResponse2(output2: Row): Future[Response] = { if(output2.b == 'b') { for { output3 &lt;- query3(output2) } yield { toResponse3(output3) } } else { Future.successful(ErrorResponse1) } } def toResponse1(output1: Row): Future[Response] = { if(output1.a == 'a') { for { output2 &lt;- query2(output1) response &lt;- toResponse2(output2) } yield response } else { Future.successful(ErrorResponse2) } } for { output1 &lt;- query1 response &lt;- toResponse1(output1) } yield response }
&gt; ./src/main/resources/ Sorry, I should have been clearer: it shouldn't be in `src` at all. &gt; import TimeSeries.MyClass Why are you putting the `TimeSeries.` here? Just import the class from the package it's declared as being in in its source code (e.g. `import com.myorganization.MyClass` if you the source is `package com.myorganization; public class MyClass ...`). (How did you even make this jar in the first place?) &gt; mvn clean package exec:java -Dexec.mainClass=com.myorganization.App FWIW you don't need to `package` if you're just `exec:java`-ing, and most of the time you won't want to `clean` either, though it's a good habit when debugging issues.
If you can't replace a variable with its bound expression it's a side-effect; i.e., `unlift` is a side-effect that has been introduced into pure code. 
Once again, this is very much appreciated! Okay, here's the thing. My `TimeSeries.jar` isn't a part of any package, it's built using `protoc` for the purpose of deserializing a protocol buffer. Maybe the best idea is to back up a step and compile my `.proto` file such that it is added to my project package? Or back up two steps and add the protobuf compiler to my build instructions in maven?
The language rules are a means to an end, we can change them if we think a different set of rules would be better for maintainability. There's certainly a lot of value to be gained from having all things that look like functions behave as functions - I don't think I'd want to adopt this with a construct as un-syntax-ey as `unlift` - but at the same time I think `for`/`yield` does impose more syntax overhead than is desirable a lot of the time, and it's worth exploring lighter-weight alternatives.
I agree with that. My objection is that within a `lift` block the normal rules of the substitution model no longer work, which is too high a cost to pay for any claimed benefit of this DSL.
Yep +1 to this. You need traversable functors and monad transformers to do this in a reasonable way and it's unfortunate (but understandable) that Scala stdlib doesn't give these to you. I would definitely look into Cats.
Try to use `map`/`flatMap`, they short-circuit on errors for instance (your impl throws a `MatchError` for failed queries). `onComplete` is a low-level API normally used to interact with callback-based APIs or to perform side effects. Another alternative is http://monadless.io. I think this is the equivalent of your code with monadless: package io.monadless.stdlib import scala.concurrent.Future import scala.concurrent.ExecutionContext.Implicits.global class Test { import io.monadless.stdlib.MonadlessFuture._ case class Output1(a: Char) case class Output2(b: Char) case class Output3() def query1(): Future[Output1] = ??? def query2(o: Output1): Future[Output2] = ??? def query3(o: Output2): Future[Output3] = ??? trait Response def successReponse(o: Output3): Response = ??? def errorResponse1(): Future[Response] = ??? def errorResponse2(): Future[Response] = ??? val f: Future[Response] = lift { val output1 = unlift(query1()) if (output1.a == 'a') { val output2 = unlift(query2(output1)) if (output2.b == 'b') { val output3 = unlift(query3(output2)) successReponse(output3) } else unlift(errorResponse1) } else unlift(errorResponse2) } }
Why discord? Isn't an irc or email list much more useful. Plus doesn't discord market mainly towards gamers? They have a cool system from what I can see from their blog. But it's a niche service.
How about using #scala on freenode? Its well established since 2013 and there are &gt;300 people inside. Hint: the guys on freenode are experts. You can downvote me, but it does not change the fact that it does not help a lot if you fragment a small community. There is a reason why developers are still using freenode.
So I think that depends. Discord has consistent cross platform support which is better than IRC. It also has rooms etc... which is better than email. Finally, all Discord logs are searchable by default which is better than Slack. On the other hand, whilst I love Discord for a wide array of uses, I would prefer to see more activity here rather than the community adopting yet another meeting ground. Each to their own I suppose :)
Nice talk! I've come to use this "smart constructor" pattern from @tpolecat - https://gist.github.com/tpolecat/a5cb0dc9adeacc93f846835ed21c92d2.
Could you share how you have concluded that? I wonder if the Monadless readme is not clear. Just to number a few differences: - Monadless is not specific to scalaz monads - Seamless try/catch support -- no need to use a different method - support for `if` conditions - support for methods (including recursion) - support for pattern matching - support for while loops Indeed, Monadless doesn't support for-comprehensions yet. 
[Quiver](https://github.com/Verizon/quiver) is an open source project from Verizon Labs that is an implementation of Erwig's inductive graphs. If you're interested in a pure functional approach this might interest you.
Cool. Sounds like no longer having to support ad hoc type class hierarchies or two different stacks will make things a lot simpler. Interop shouldn't be too bad.
&gt; Monadless is not specific to scalaz monads * Monadless is specific to its own monads. Other monad requires a proxy to Monadless's monads. * Each is specific to scalaz monads. Other monad requires a proxy to Scalaz monads. So, what's the difference? &gt; Seamless try/catch support -- no need to use a different method The same as what Each did. &gt; support for if conditions The same as what Each did. &gt; support for methods (including recursion) The same as what Each did. &gt; support for pattern matching The same as what Each did. &gt; support for while loops The same as what Each did. &gt; Indeed, Monadless doesn't support for-comprehensions yet. The only difference is here.
Maybe try just try the libs and compare by yourself? I'm pretty sure you'll be able to see the differences I've mentioned ;)
sorry, I've just noticed that I mentioned `if` conditions when I wanted to mention `boolean` expressions.
Care to comment /u/fwbrasil?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/graph] [Scala networkX equivalent?](https://np.reddit.com/r/graph/comments/66vzzp/scala_networkx_equivalent/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Where can we find more in-depth details of changes and timelines? 
tpolecat is missing the point here. This lib provides syntactic sugar the same as for-comprehensions do: ordering is important for non-commutative monads -- and I'd say that they are pretty widely used and not useless at all. Monadless is a great improvement over for-comprehensions and I believe could make onboarding in Scala much easier. Note that his discussion isn't even relevant for monads that aren't referentially transparent like Future and Try. It's frustrating to see tpolecat's comment hijacking the thread, but I really wouldn't expect much more from the Scala Reddit community. I hope at least people are able to form their our opinion by trying out the lib.
I'm not sure who's right or wrong here...but...: The library looks really nice, and is exactly what I've been wanting for a while -- an "async-await" but for monads. I've even asked around for something like that on gitter a couple of times. This is perfect! Thanks! And hey, if tpocolecat is right, maybe it's possible to improve on it somehow to make it perfectly RT? That or we just have to pay a little bit attention when we use it.
I think you are making a categorical error here. Referential transparency applies to functions, which `unlift` is clearly not. The choice of syntax is okay because `f(a)` long ceased to be a syntax for functions alone since the introduction of macros. Idris has a very similar notation for monadic structures. See some conversation here: https://twitter.com/missingfaktor/status/496403683211366400. 
That doesn't make sense because "unlift" is not a function. The whole point of Monads is sequencing and of course order matters.
Does anyone else think that the Scala people are as awful at naming as they are brilliant at language development? Typesafe-&gt;Lightbend: Awful Scala 2.x -&gt; Dotty (instead of just calling it Scala 3): Make people think Scala is dying instead of getting them excited for the new version
Spark has a well typed interface: the RDD. I think building data frames and Spark SQL the way they did was a huge mistake. As a result we don't use either in production code. 
Definitely check out this blog post for a bit on Monad Transformers from Scalaz. They were my gateway drug into scalaz usage: https://www.47deg.com/blog/fp-for-the-average-joe-part-2-scalaz-monad-transformers/ http://koff.io/posts/290071-make-async-with-scalaz-either-and-futures/
dotty is the project codename. the final product *will* be named scala 3. i don't disagree that lightbend is a terrible name, though.
It isn't, but syntactically it looks like one, which could be rather misleading. 
Isn't that true of any macro, ever?
&gt; In a typical usage of IO / Task, that A =&gt; F[B] function in flatMap also has side-effects quite often. It has effects, not side effects. The monad signature does not make sense if the kleisli arrow is not a function. Nothing in functional programming does. I do agree that Future is a useful Monad when used purely, the problem is that it normally isn't. Anyway, perhaps there's no point discussing this ad infinitum, I just think that the sentence: &gt; monads that aren't referentially transparent is an oxymoron. Feel free to disagree :)
So many questions in this thread can be answered by slide 2. Dotty is an "experimental compiler for a scala-like language." Its "not ready for production use" but "will become scala 3"
&gt; experimental compiler for a scala-like language What does it mean?
ok thanks. 
I feel kind of dumb asking this, but what does this all mean? I get Scala Meta's usage as a macro library, but I'm in the dark about what this semantic API and database stuff does. Unrelated, love the part in the slides killing the meme of "Twitter is ditching Scala". 
To add to this, what I'd expect as issues are missing tools rather than incorrect behavior
What informs this expectation? I don't know enough to agree or disagree, but curious of the reason behind your statement.
Yep, there are lots of companies using Cats in production. For example: SoundCloud, Stripe, Twilio, Zalando, SoFi, Globo, the BBC, and the Guardian. Those are just a few of the companies that have added themselves to the [Finch](https://github.com/finagle/finch) or [Circe](https://github.com/circe/circe) adopter lists: * https://github.com/finagle/finch#adopters * https://github.com/circe/circe#adopters This of course doesn't mean that all of these organizations are using Cats extensively, but it does mean that they've decided that they're comfortable with a pre-1.0 Cats dependency (via at least either finch-core or circe-core). In addition I have one client using Cats (but not either Finch or Circe) in a Spray application, primarily for `OptionT` and `EitherT`, but also `Validated` and some other pieces. I've also used Cats in projects for other clients (in data munging pipelines, etc.). I'm sure this is the tip of the iceberg—if I can come up with two or three dozen companies from my own experience and self-reported lists from two open source projects, we're probably talking at least a hundred organizations in total, maybe more.
Her Majesty's Revenue and Customs aka HMRC (basically the Tax office in the UK) have a large scala base and use cats extensively https://github.com/search?q=org%3Ahmrc+cats&amp;type=Code
Out of curiosity, what led you to choose it over scalaz?
Because none of the concepts in cats are original or new. Pretty much everything in it has been done before both in Scala and other languages. Parametricity also is a safeguard against wrong implementation, as are law checks and property based testing. Much of what's in the library is tedious boilerplate, and of a specific form which is easy to verify and get code coverage on tests. I'm not saying you'll never hit a behavioral bug, but I'm willing to bet you'll not hit one quickly or easily, and cats won't be the first library you have upstream problems with
i'm excited to see (some) standardization of I/O types—I've spent far too long worrying about the differences of `concurrent.future`, `fs2.Task`, and `monix.Task`. Hopefully this will play nicely with `cats.effect.IO`.
Dependent Object Types. Hence, Dotty. Isn't not a marketing name.
Boy, do I hate to harsh on [/u/lihaoyi](https://www.reddit.com/user/lihaoyi), but if you're going to write: &gt; If you want to argue whether breaking referential transparency in this specific way is good or bad tradeoff overall, you need to first understand why it is _good_, and argue weighing the tradeoffs. If you don't understand the other side of an argument _at all_, it's hard to make an interesting argument, and so far I don't see anyone here being able/willing to argue the other half... you need to be particularly careful with your terminology. A bit later, we find: &gt; The point of such a library, as I see it, is to bridge the gap between monadic and non-monadic code. It essentially allows you to write monadic code in the same "structure" that you would write non-monadic code... No, it emphatically does no such thing. As [/u/SystemFw](https://www.reddit.com/user/SystemFw) [said](https://www.reddit.com/r/scala/comments/66mzqc/monadless_syntactic_sugar_for_monad_composition/dgmxsqw/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=scala): "'monads that aren't referentially transparent' is an oxymoron." Note that I'm not saying a word about whether the library accomplishes its goals or not, or whether any or many people will like it or not, or whether it has "the right to exist" or not. I'm only addressing a very simple matter of fact: that code using the library is not monadic, and it's misleading to claim it is, just as "JavaScript generated to interact correctly with the server-side code it's generated by" is not "isomorphic."
That rebuttal isn't successful, otherwise I wouldn't have replied. What's interesting, in some sense, is that no one is claiming the library _doesn't_ break referential transparency: it manifestly does, and everyone admits it. It's even being claimed as a virtue. I'm explicitly setting all of that aside, because not-referentially-transparent implies not-monadic. That's not complicated at all, nor controversial because it's not an opinion. What _might_ be interesting to some people is one of the following: 1. What's the value in pretending non-monadic code is monadic? 2. What are the relative merits of referentially transparent code and non-referentially-transparent code? 1) is germane to discussing this library; 2) arises once you get past the idea using this library results in "non-referentially-transparent monadic code," which, again, is an oxymoron. To be clear: in a sense, I'm trying to support your effort: if the library helps you, great! I just want clarity about _how_ it helps, and it's not by being monadic, that's all.
Marketing is important even if you don't care for it to be.
Looks really interesting. Are there any videos associated with this?
/u/tpolecat look at it this way: A programming language requires some syntactic mechanism to tell apart ordinary assignments, from context abstracted do-notation style assignments. In Scala that's `val ... = ...` vs `for{ ... &lt;- ... } yield ...`. This library just provides an alternative syntax for the second, that's on purpose more similar to the first: `lift{ val ... = unlift(...) }`. I can see how people may feel different about this approach, but I understand both sides! One could argue that it is too subtle or potentially confusing, but I understand the people who find `for` irregular or too noisy. (And once there are blocks with different syntactic semantics, a library like this has the chance to throw in a bunch of convenience features for loops or ifs.) It's not going to be without problems, but I'd love to see this road explored, teaching us more about it as we go. I definitely like a general implementation for this better than the one-offs in sbt and async.
Future breaks identity. `Task({}).map{_ =&gt; someMethod()}` is equivalent (and can therefore be blindly refactored) to `Task(someMethod())`. `Future({}).map{_ =&gt; someMethod()}` is [almost](http://wiki.c2.com/?AlmostConsideredHarmful) equivalent to `Future(someMethod())`. FWIW I consider `Try` to be a cromulent monad as far as I know, since I don't think it makes sense to talk about the monad laws in the presence of `catch` at all.
Discord has a nicer UX though.
&gt; the expressions in question must be referentially transparent I accept this, but if you will humor me, lets dig into this: what does this mean, concretely? We've already established that changing the order of generators in a for-comprehension can change the value of the results: for { a &lt;- v1; b &lt;- v2 } yield (a, b) for { b &lt;- v2; a &lt;- v1 } yield (a, b) Using the unlift macro, we can see that changing the order of `unlift`s inside a `lift` also changes the order of results, both in the same way: lift { val a = unlift(v1); val b = unlift(v2); (a, b) } lift { val b = unlift(v2); val a = unlift(v1); (a, b) } What makes the for-comprehension case "referentially transparent", and the `lift`-case not? It seems to me that their structure and semantics are 100% identical, and both have a "desugaring" phase, except... - One desugaring happens in the parser (!) and the other happens in the typer/macro. - One uses `for`/`&lt;-`/`yield`, the other uses `lift`/`unlift`/`` - One only allows one `&lt;-` per "line", the other allows `lift`s to take place multiple times per statement, which are effectively treated as haskell's Idiom Brackets (Using a naive "applicative instance derived from monad" instance) Is one of these the key factor that causes `lift`/`unlift` to not be "referentially transparent"? Or is there some other difference I failed to list here? ---------------------------------------------------------------- &gt; the core of your argument seems to be that monads force to use unnecessary names (A-Normal-Form), which I showed not to be true &gt; in exchange for what I consider a negligible amount of convenience The core wasn't just A-Normal-Form, but that you have to restructure your code non-trivially: - Normally, you call functions with the function on the left, arguments on the right. - With applicative builders, you have to call all your lifted functions "backwards": argument on the left, function in the middle, non-lifted arguments on the right You have done so for the expression I gave, and can certainly do so for more complex expressions. But while I think using an applicative builder is just fine, I also think there are a lot of people who would find the `async`/`await` version of that expression easier to read, especially once you leave the FP community.
&gt; One uses for/&lt;-/yield, the other uses lift/unlift/`` I think this is a nontrivial distinction; it is important and valuable that syntax be clearly distinguishable from functions. &gt; Is one of these the key factor that causes lift/unlift to not be "referentially transparent"? Or is there some other difference I failed to list here? To my mind the key difference is that one is part of the language specification. I can reasonably expect anyone who lists Scala on their CV to know that `&lt;-` is not a function. I would expect many competent Scala developers to assume that `unlift` was a function.
These are valid points, but I don't find them very compelling. Perhaps others would, but I don't think I would feel materially differently if fwbrasil wrote a compiler plugin that let you write `&lt;-(foo)` rather than `unlift(foo)`, which would be unambiguously new syntax. In the end, I find it only a superficial difference, and it's easy to rename `unlift` into some kind of operator if we want it to stand out more. Similarly, "nobody knows what `unlift` is" is going to apply to any new idea. Even if Martin Odersky himself came down a mountain with `unlift` engraved on a stone tablet, nobody with Scala on their CV would know what it is until later. Perhaps a good argument not to try these new-and-unusual ideas in your own project, but to me not really an argument against the idea itself
You don't need to sell me on how awesome referential transparency is; I'm sold on all that already =P What I'm not sold on is the idea that the `lift`/`unlift` syntax in this library is materially different from the `for`/`&lt;-` syntax we're all used to, and presumably have no referential-transparency qualms about using. /u/tpolecat's example at the top *does* change the order of things. Here is it again, laid out in multiple lines: lift { val a = unlift(List(1,2)) val b = unlift(List(true, false)) (a, b) } res4: List[(Int, Boolean)] = List((1,true), (1,false), (2,true), (2,false)) lift { val b = unlift(List(true, false)) (unlift(List(1,2)), b) // inline a } res5: List[(Int, Boolean)] = List((1,true), (2,true), (1,false), (2,false)) As you can see, he is moving the `unlift(List(1, 2))` from the first statement to the second. This is not unlike moving a `&lt;- List(1, 2)` from the first generator to the second, at least in the context of this library, but entirely unlike inlining a "normal" `val`. Both result in (the same) change of semantics, and I argue they should be thought of equivalently. `unlift()` in this library is not just "another expression": it should be thought of as "special syntax" just like `&lt;-` is in a for-comprehension, with semantics basically identical to `&lt;-`'s semantics in a for-comprehension. If you think of this library as introducing new semantics for existing syntax, it breaks referential transparency. If you think of it as a new syntax over an existing semantics, the semantics of the underlying semantics are just as referentially transparent as before. ---------------------------------------------------------------- You are right that using Applicatives does reduce some of the pain of having to define new names. I still think it is non-trivial to have to shuffle the arguments around like that though. I *do* think that e.g. the f a b c d -&gt; f &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c d Conversion (IIRC) in Haskell is a much easier sell than the equivalent f(a, b, c) -&gt; (a |@| b |@| c)(f(_, _, _, d)) conversion in Scala. To me, it doesn't matter whether it's the type-inferencer's fault or something else's: the transformation, while *mechanical*, is nontrivial and honestly a bit annoying to have to do.
&gt; it's easy to rename unlift into some kind of operator if we want it to stand out more. It's imortant we use the same one though, and it's important that everyone does it. Fundamentally I really don't want to ever have to maintain a codebase where some ordinary-word functions like `unlift` have "magic" semantics. &gt; Similarly, "nobody knows what unlift is" is going to apply to any new idea. Even if Martin Odersky himself came down a mountain with unlift engraved on a stone tablet, nobody with Scala on their CV would know what it is until later. Perhaps a good argument not to try these new-and-unusual ideas in your own project, but to me not really an argument against the idea itself I think this is the sort of thing that needs to be part of the language, not just a macro (a macro is an excellent way to prototype it though). A language needs to be defined enough that two developers can maintain each other's code; leaving too much to macros leaves developers working in effectively different languages. If the authors want to push this (and I do think the goals are admirable, though I disagree with the implementation syntax) they should be working towards a SIP, not just a library.
Alternative suggestions? 'Cats with Lasers'?
We use cats heavily here at drivetribe (along with a bunch of other typelevel projects) - it's been very stable and to build on what travisbrown has said, I don't think I've ever seen a cats-core bug pop up in our code
Completely agreed To expand on this, Scala is a language that is strict by default and so this is intended idiomatic Scala behaviour. You are not going to get referentially transparency in this context by default on a language level in Scala (in contrast to languages like Haskell)
There's a difference. You can't call anything a Monad in Scala strictly speaking, but you can do so loosely if you assume you're in the referentially transparent subset of Scala. My point is that with Task the normal usage respects this assumption (having effects in a `Task {}` still results in a proper `A =&gt; F[B]`), whereas with Future the normal usage doesn't. Sure, Future is still a Monad if you don't have any effects in it, but that's not how it's used. In other words, there is a way to break referential transparency in any Monad in Scala, but with Future there's *no way not to break it*, if you use it as it's normally used (asynchronous side-effecting operations). I think that's a worthwhile difference.
I think one of the main reasons why a lot of people that heavily push for RT kind of end up missing the point and why we get into this argument, so I am going to make a statement here. On a syntactic language level, Scala is fundamentally designed to be a *strictly evaluated language*. The behaviour that /u/lihaoyi in regards to inlining and behaving like a val is **precisely the point**. In Scala, unless specified otherwise (i.e. defining something in a thunk), things are **evaluated strictly by default**. So when you claim something like this &gt; It looks like another expression, but it's not. Actually, I would say (from a language design PoV), you are actually wrong in this one. In Scala, expressions are strict and are evaluated immediately, they are not lazy by default. If we were talking about Haskell, you would be 100% correct, but Scala being strict by default (which is a deliberate design decision made by Ordersky), this is **completely intended and normal behaviour**.
&gt; but you can do so loosely if you assume you're in the referentially transparent subset of Scala. Unless you define what you mean by loose, this statement is an oxymoron &gt; In other words, there is a way to break referential transparency in any Monad in Scala, but with Future there's no way not to break it, if you use it as it's normally used (asynchronous side-effecting operations). I think that's a worthwhile difference. If you care about referential transparency, then you can use `Future` fine (in fact I do all of the time). This statement isn't really true, both in the sense of reality and in the sense of the design of the language of Scala (Scala is not lazy, nor is it referentially transparent by default. If you want RT, its opt-in, not opt-out, its designed to be a conscious decision by the user)
I think you are now moving the goalpost to the following point by /u/paultypes: &gt; What are the relative merits of referentially transparent code and non-referentially-transparent code? I am not interested in having that discussion. If you don't care about referential transparency, none of my arguments apply, and we won't be able to agree on anything. And that's fine! Have a nice day :)
&gt; I think we do not actually have the same definition of "monadic", and that there is interesting discussion to be had in trying to find the concrete points of disagreement. If we're working from different definitions of the word, then we can repeat the same assertions endlessly without any progress in understanding. Let me turn this around: presumably we agree that the state monad is monadic and a mutable `var` is not, `Try` is monadic (I know some would dispute this but I don't) and `throw` is not. But as you've pointed out, the code style and behaviour when using this library is quite akin to that one gets when using mutable `var` and `throw`. So in what sense would you say that code using this library is monadic? (To my mind the notion of explicitly tracking not just which effects can occur but also how they're ordered is core to the concept of being monadic. If we were to only do lightweight tracking of which effects can occur in a given block and not explicit sequencing of those effects (which is what I think using this library amounts to) then we end up with something akin to checked exceptions in use. And I find code that uses `Try` substantially easier to understand and safer to refactor than code that uses checked exceptions, because the sequencing is explicit; if statement `A` precedes statement `B` it becomes very clear whether that ordering is intentional or accidental, which as you've said previously is what functional programming is all about).
I've got no dog in this fight but that's exactly what someone missing the point would say.
Hi /u/fwbrasil, Just dropping a note to say that a project like Monadless is very useful for beginners. I have seen [scala-async](https://github.com/scala/async) being used by beginners successfully, even though I eventually fell out of love with it, as I became more and more familiar with structuring code around `flatMap`, plus the lingering problem is that `scala-async` has non-obvious limitations versus blocking by means of `Await.result` (e.g. closures, try/catch, etc). But then I do miss a more potent monad comprehensions support and I may find myself using Monadless. Anyway, awesome work you're doing. Cheers,
strictness by default and referential transparency are two orthogonal concepts: idris and purescript are strict and pure. Also, how the `lift` macro is translated has nothing to do with evaluation (all the examples given are about a strictly evaluated List). This is really about whether we should consider `lift` syntax, or not.
&gt; thread-unsafe lazy vals by default This is most likely a dangerous mistake.
&gt; This is really up to debate and I agree with Martin (and others) when they see that so far that all attempts to try and manage side effects haven fallen short for many reasons. /u/pron98 for example has done some excellent talks demonstrating the severe shortcomings when trying to manage side effects, and that also trying to verify side effects properly is a very fruiticious exercise. I don't know what's different about Odersky's codebases, because I've had a very different experience. E.g. implicit parameters used as parameters have been a major source of bugs in my experience, to the extent that I'll deliberately avoid them, whereas he seems to be a big fan and want to expand their use. Likewise with pron (who I've had extensive discussions with here and elsewhere) although I'm less clear where he's coming from because I've not seen so many examples (I can't/won't watch videos). Certainly I think continuation-based approaches have been substantially less successful than value-based approaches to effects, at least to date. I agree there's not a clear consensus, but I think the balance of Scala users tends towards a more pure/managed-effect style. I also think that in the current language landscape the pure/managed style is where Scala can most usefully offer something that other languages don't. &gt; I don't think comparing managed side effects to XML is really an apt example. XML literals were a bad idea only because they were part of core when it should be part of stdlib Maybe. My point was just that we shouldn't be massively deferential to the language's original design decisions.
&gt; I've spent far too long worrying about the differences of `concurrent.future`, `fs2.Task`, and `monix.Task` Those difference are a strength imo. When it comes to dealing with laziness, side-effects, concurrency there is no silver bullet unfortunately. So what we're trying to do right now is to standardize on some type-classes, along with `cats.effect.IO`. That's where the Effects4s and the recent `cats-effect` fall. I'm the initiator of Effects4s, am the author of the Monix Task and also chose to be involved in `cats-effect`. If all goes well, we'll have a good interop story soon. Cheers,
&gt; [...] what is the idiomatic scala-way to do such Future Chaining? As you can tell, Scala is chock full of idioms, so there's really no one idiomatic way. I personally enjoy using [Scala Async](https://github.com/scala/async). With that, your logic becomes: ``` async { val output1 = await(query1()) if (output1.a == 'a') { val output2 = await(query2(output1)) if (output2.b == 'b') { val output3 = await(query3(output2)) successResponse(output3) } else { errorResponse1() } } else { errorResponse2() } } ``` Your end result would be a `Future` of whatever the least-upper bound type is of your successResponse &amp; errorResponse functions.
But it's a closed source proprietary application. Which is far too huge of an issue to overcome regardless of any benefits. That doesn't even account for the fact that the high bandwidth and specialized nature of it being a saas product segregates the developer community based on always on internet. Not to mention government firewalls.
&gt; This is really about whether we should consider lift syntax, or not. Then why are we (or even people like /u/tpolecat) debating about referential transparency (or not) when really the argument is just about syntax?
Great question! Semantic API is something that I demoed at ScalaDays, but didn't include in the slides. We don't have a lot of documentation, because that's a very young feature, but here's a write-up that introduces the main concepts: https://github.com/scalameta/sbt-semantic-example. Please let me know if you have further questions. Unrelated, it felt really good to have a chance to dispel the ditching-Scala myth, so we couldn't resist :)
I haven't used them but maybe one of graphx ( http://spark.apache.org/graphx/) or jung (http://jung.sourceforge.net/index.html) might work? 
I've been following the development of Amora, but I found the data schema to be a little bit too generic. That's probably because one of the goals of the project is to index arbitrary data (https://github.com/sschaef/amora/issues/13#issuecomment-195717455). To the contrast, the semantic db schema is deliberately Scala-specific, and as a result is just ~50 lines of protobuf code.
FWIW, the next release of scalafix will build entirely on this new semantic API. The semantic API has reached a point where I'm able to remove the scalafix-nsc compiler plugin in favor of the scalameta semantic API https://github.com/scalacenter/scalafix/pull/127 I hope to have something cool to show in the next 2-3 weeks :) The key selling point of this semantic API from my perspective is that there's a clear separation of concerns between 1. extracting useful data from the compiler (e.g., naming resolution) and 2. analysis on this data. The data is extracted once by scalameta and reused across multiple tools like scalafix, code search, doc generator, ctags, ... I think (or hope at least :)) that we'll soon see a lot of exciting tools/applications built on this new infrastructure. Not only at Twitter, I'm working hard (as part of my job at the Scala Center) to make all of this available for the OSS community, for example via sbt plugins
Ops I need to disable the print CSS.
stairmaster! :-)
Monad
As someone who couldn't go, I'm interested in knowing what some of the highlights were!
If you've never touched Scala before, Scala for the Impatient is great as a quick introduction to get you up and running. Can't speak to how it would match up/help with the Coursera course, but it has helped me learn the language.
Scalawag? My favorite is Racket, they like to call themselves racketeers.
Thanks for the recommendation.I will surely check it out.
Quill. https://github.com/getquill
My impression is that most proficient Scala programmers are very pragmatic. They know that the language has its quirks and shortcomings. For example many know Haskell, and Haskell has an extremely clean syntax---Scala is far inferior to Haskell in this regard. I think most would switch to a better language (for whatever job they currently use Scala for), as soon as it becomes available. And if you would do this, I don't think there is such a strong bond between yourself and the language that justifies taking on funny names.
&gt; He already explained multiple times why he prefers implicits (and there are good reasons why) Yeah, I just don't understand how he's managed to avoid creating bugs when using them. They've been a major source of bugs in every codebase I've seen that uses them. &gt; Its a bit hard to paraphrase what he says because he explains it fully in videos, but he made a very good talk which basically claims that attempts to prove features through your program via a type systems has very fast diminishing returns relative to the amount of effort (for the user) and the complexity it causes in the language (and there are mathematical foundations for this). I've talked through that argument with him and I'm not at all convinced; it seems to be an argument from failure of imagination. He's very focused on this specific model of program execution, whereas to me the whole value of a type system comes from modeling things in the way that's best suited to your goals and what you care about. More fundamentally I don't believe any argument that humans can do something that computers fundamentally can't. Whenever I've seen people resort to reasoning informally about side effects, it's only ever taken a little effort to make that formal, and the cases that are difficult are often the cases that aren't actually sound. &gt; I don't think there is any evidence of this, on the contrary (and this seems to be a complaint from the FP guys), most Scala code out there tends to be a pragmatic hybrid of purely functional where it matters else its functional (and sometimes C#/Java style code). Unmanaged effects and C#/Java-style code are common enough, true; there's plenty of Scala out there that could have been written in any language. What I don't see a lot of is this semi-managed effect style; I've seen only a handful codebases that used implicits the way Odersky's examples do and most of them were disasters. Indeed the closest analogue to this style I've seen professionally is checked exceptions, which are the most notable *removed* feature in Scala compared to Java. &gt; Something like for comprehension and def/val are a lot more central to a languages design rather than having XML as a batteries included module. The former is a central part of language design, an XML library is not Is that a reason to think the decisions are any better? In any case Dotty is bringing substantial changes to central aspects of the design as well. (Which I think is great, to be clear)
&gt; But it's a closed source proprietary application. Which is far too huge of an issue to overcome regardless of any benefits. Do you take the same position with regard to e.g. doing development on GitHub? &gt; That doesn't even account for the fact that the high bandwidth and specialized nature of it being a saas product segregates the developer community based on always on internet. Huh? Discord is much, much better for users without always on internet than IRC is. &gt; Not to mention government firewalls. Again, huh? I was in China a few weeks ago and Discord was the only way I could communicate with my friends (my email was blocked).
It's unfortunate to have to overload `=` thusly, but that's probably the best we can do within Scala's syntactic limits. I quite like F#'s computation expression syntax. They have `let x = foo arg` for regular binding, and `let! x = fooM arg` for applicative or monadic (depending on the context) binding. This is similar to `&lt;-` except the syntax isn't as rigid and inflexible, since it was designed with many control patterns in mind, other than binding. So you also have `return!`, `try!` etc.
Odersky quoted Rust's ergonomics initiative in a recent slide, so I expect he's sensitive to an argument in that direction. But I'd try to propose this as a convention in, say, Typelevel projects. That's maybe the most fertile ground, but I'd bet you still find resistance. Maybe a Scalafix rewrite would help. Or associating breaking changes coincide with some other breakage? But to be sure, Martin cares about compatibility (up to automatic migration). Regarding trends on purity: I agree with you on blog posts​. I'd love stronger metrics. Also because, without a proper optimizer (not an easy problem), FP style will keep being unnecessarily slower. Look at reviews on collection code. The "sufficiently smart compiler" is probably a myth, but GHC and Scala are at complete opposites. (In truth, powerful optimizations can also make code slower, which is one reason Scala's optimizers are very conservative).
Course is loosely based on Structure and Interpretation of Computer Programs, so read that.
I'd say thread safety is the primary reason I've seen lazy vals used in code. Indeed I'm struggling to think what the use case for an unsynchronized lazy val would be. (I guess only as a performance optimization for something that would still be correct when given as `def`? But a safety-sacrificing performance optimization probably shouldn't be the default)
"Implicits are what make Scala, Scala" - I couldn't agree more, they very accurately represent the double edged sword Scala can be.
/u/aaronlevin I'd like to know this as well. Im trying to incorporate either into my project
Scalatags is also pretty good
Thanks! this is very useful.
You should probably take a look at the scala library tut. Instead of turning scala code into html, you can write scala code inside of markdown files, and the generated documentation will capture the output of those scala blocks (so your code examples in documentation is guaranteed to compile and run). It's actually a great way to write scala in general. Getting html from the generated markdown files should be pretty easy, whether rendered on the browser client side, or however you're going to do it. https://github.com/tpolecat/tut 
Could you use Pygments through Jython?
While the additional restrictions on implicits and the addition of implicit funtion types sound great in principle, I'm wondering what affect the new rules around implicit conversions will have on how scala authors approach DSL development. As I understand it the below code will not compile since the definition of the implicit conversion and its application exist in separate source files. // author code package dsl trait Query[T, U] { def flatMap[A, B](...) def map[A, B](...) ... } trait Table[T] implicit def table2Query[T &lt;: Table[_], U](t: T): Query[T, U] = new Query[T, U]{} // client code package com.company import dsl._ object User extends Table {...} object Role extends Table {...} val q = for { u &lt;- User r &lt;- Role if u.id ... } yield (u, r) 
Meta comment: the roadmap seems to be taking more and more of a concrete shape with each Dotty update. * Just research for features that may make their way into Scala * Actually, might be future Scala * Will be future Scala (but a long way off) * Definitely will be Scala 3.0 (and here's the timeline) Scala 2.13 is slated to land end of Q1 2018 (i.e. less than a year from now) and then apparently Scala 2.14 and Scala 3.0/Dotty will be launched in parallel. Could be mid-late 2019 when Dotty lands. Would be thrilled if that were the case...
`=` (unlike `&lt;-`) is not rebindable at the user level, and any such rebinding results in a sub-language with nonstandard rules of substitution. The end result in this case is a language with rules that are precisely those that you would expect if `unlift` were a side-effect. The mechanism by which this happens is irrelevant as far as I'm concerned. The code may appear superficially "simpler" but is harder to reason about.
Oh, this one has my (however unimportant) vote! I'm removing my other suggestion (Scalarite)!
Python people just care about getting shit done. Scala is far more about purity and immutability etc.
You could try JGraphT or another Java graph library. In my experience scala-graph is totally fine for small stuff, but performs poorly on problems of that scale and can be difficult for people to pick up and maintain. If you find something as good as NetworkX, do let me know! Every once in a while I consider writing a similar library for Scala just because of how fondly I remember NetworkX.
But isn't `object` initialization also guaranteed to run only once, no matter whether there is concurrency or not? Like this: object Test { val rnd = scala.util.Random.nextInt() } Is `Test.rnd` not guaranteed (in current Scala) to return only one single value, even under concurrent access? This would make the behavior of `lazy val`s very unintuitive, I think.