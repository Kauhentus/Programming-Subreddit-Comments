I'd suggest getting a copy of [Programming in Scala 3^rd ed](http://www.artima.com/shop/programming_in_scala_3ed), which is an extremely readable introductory book by the creator of the language which was recently updated to cover up through Scala 2.12. The second edition only covered through 2.8 as I recall, maybe 2.9, but would still be a good resource if that's the one you can get your hands on. Read through the book once just to get a feel for what the language has to offer and why it's designed the way it is. I found understanding Scala's foundations made it easier for me to understand and break down the code I came across later, not to mention the fact that I had some idea about the terms I'd need to Google in advance. The book can make a good reference later but I don't think you need much more than a quick read to start out. After that, I'd suggest some simple exercises to get used to writing Scala code. Attempt to be idiomatic, not clever, and definitely not concerned with performance at first. Maybe try the [Wonderland Scala Katas](https://github.com/bsamaripa/Wonderland-Scala-Katas) or any of the other myriad coding exercise sites. You don't necessarily know what "idiomatic" Scala is at first but you'll start to recognize patterns as you search for what you don't know how to do and compare your solutions with others. You can actually get a long way if you do two things: avoid null and attempt to use immutable constructs as much as possible. Getting through the book, the exercises, and with the job, I expect you'll be moderately comfortable with language about as quickly and effectively as you can be. I also expect the job will introduce you to what you need to know immediately as far as an "enterprise perspective" goes. You could also try to familiarize yourself with some frameworks, tools, and libraries you're likely to come across, many of which are listed [here](https://github.com/lauris/awesome-scala). Maybe pick one each from the database, web, testing, and json sections and build yourself a tiny web page that displays some information pulled from Cassandra.
Not to sound dismissive but I have tried to explain the virtues of functional-style programming to people accustomed to PHP and it's been an uphill battle. Sure, you can throw caution to the wind and code Scala like it was PHP but then what's the point? Scala-like languages require a different mindset that might be difficult to grasp for people who have been working in a throw-code-at-it-until-it-works approach.
FWIW, I'm a Java programmer that's moved to Scala (+Play2/Akka). It's OK provided it's someone with a good amount of experience, and ML exposure helps (mainly the functional aspects of the language, I got my exposure through my CS course back in the day). In case you didn't know, Coursera has a few courses given by Martin Odersky et al. which I'd say are suitable for relatively experienced programmers to get up to speed with Scala. https://www.coursera.org/course/progfun and https://www.coursera.org/course/reactive
Case classes can get automatic instances, so that will help.
I was assuming people have experience in real languages, that aren't awful. I went from Lisp and Java to Scala, and had absolutely 0 issues. 
Any suggestion, comment or contribution is very welcome, this is very early stages for this component but I'd really like to push it forward!
One issue is meeting space. If you have some, and host the meetup (buy pizza and soda) you will have a nice platform to recruit
Please help bring sanity to my team. We are having a very lively discussion about using locking and the `synchronized` classes like a Map vs using Akka actors to manage critical resources. The key criteria is performance, and overhead with the locking going on along with how many actor systems to spin up and what not. 
IMO performance disputes are best handled with measurement. Have proponents for each side code up a solution, write down the actual performance requirements (throughput, response time, memory use, whatever), then measure each solution against each requirement. I'm almost willing to bet that either solution will be sufficient for your actual needs, in which case you pick the one you want to maintain.
 It seems to be you're bored which could be a cause for burnout. I'd recommend you to: + either get involved into the project(s) more by spending more time decoupling the architecture and the design patterns to get more serious tasks from your manager/employer + or put yourself on energy-saving mode at work, so when you get home you'll be able to learn on your own and work on new ideas - it's not a long-term solution that's why you'll need to do interviews You can also share the field you're working on - for example the financial/healthcare/military areas are the most boring and bureaucratic which are definitely not suitable for juniors(neither for "innovators"). Working at a startup on web projects, big data and such is more refreshing. 
Where are you geographically? PM me if you'd like.
I love the work that is being done with Dotty. I just hope I can take advantage of it someday soon!
Good post, this is the type of content I like to see in /r/scala!
I'm in Upstate NY but willing to relocate.
Here's some starting point you might find useful: https://stackoverflow.com/questions/20331680/get-same-completion-candidates-for-imain-as-would-appear-in-a-repl
Would also like to say that Slick isn't an ORM, its a FRM (Functional Relational Mapper). There is a difference, ORM's tend to conflate query building with execution of query (which is why they usually have terrible performance for certain edge cases)
I hope this isn't taken the wrong way but from my experience at my company it actually could hurt. I really want to find somewhere to work where they have an actual track for a junior or mid level developer to become a senior developer. I'm open to any opportunity but I don't want to set myself up for failure and more of the same of what I'm currently doing.
I read many articles, and it never clicked. I totally get it now. Thanks!
 @struct class Vec( val x: Double, val y: Double, val z: Double ) How is that different from a case class?
I can't really tell you the mechanics behind it but he suggested in his talk that it was compiled to a normal C struct and was thus faster. 
A "normal C struct" at the machine level is just 3 chunks of memory next to each other of the right size for the member types. I don't see why a case class couldn't be compiled to the same thing.
Awesome! Cant wait to play with it!
But why couldn't a case class be, is my point?
&gt; I would say the type system is the most baffling thing for me at the moment. The IDE is my friend. Ctr-Q is my friend. Just press Ctr-q in a few places and it will tell you the types. Once you have that in your head you can just follow it like you would in Java. Other than that, the co-variance/contrivance thing can be confusing (T1 &lt;: T2 vs T2 &gt;: T2). As long as you can "follow the types" and you understand the type signatures, you should be able to write code with inferred types. As for reading it, at first I got fixated on what the type of every little thing is, but gradually it became easier and more familiar and I could follow it.
&gt; also implicits and pattern matching stick out as other areas &amp;nbsp;&amp;nbsp; You can think of implicits (implicit parameters and implicit conversions) as a place where the compiler is supposed to "grab" something. &amp;nbsp;&amp;nbsp; In the case of implicit parameters, the compiler is "grabbing" a value that is implicit, either from an import or from one of the associated classes/objects. If you press Ctr-p in the IDE, it should look for the location where the implicit is being "grabbed" from. &amp;nbsp;&amp;nbsp; In the case of implicit transformations, the compiler is "grabbing" an implicit method or an implicit class (the implicit class has a constructor that you can think of as kind of "encompassing" the original object, giving it bonus functionality). I am not 100% sure, but I think that the scope where the compiler looks for implicit variables overlaps with where the compiler looks for implicit transformations. &amp;nbsp;&amp;nbsp; That's how I visualize implicits.
My take on a Scala client for Elasticsearch is https://github.com/skytteren/elasticala I agree that there are issues with elastic4s but what it is depends on the user. 
A case class is a class. Classes aren't the same as structs. Case classes have behaviors that extend beyond just having values.
The struct part :) I don't see why it needs a specific annotation.
It is an annotation new to Scala Native.
Relevant presentation: https://github.com/densh/talks/blob/517b20c30dd4aaf390785039cdd002f623eaa91e/2016-05-11-scala-goes-native.pdf
Yep, some good stuff!
I'm quite new to functional programming, but am trying to understand how I can keep things pure, and referentially transparent. It's difficult coming from a world of mutability though! My question is; are context objects, or state objects quite commonly used? I'm trying to write an idiomatic console application library just to learn the language and functional programming in general, but am struggling to find whether or not what I'm doing is actually idiomatic or not. In my current implementation, there is an `Application` class. This class is what is given the commands that will run, it handles how the input is routed through the console application, so on. It's "state" is built-up over time though, after instantiation, currently by having a case class that contains some "state" (like how many arguments have been processed, for example); an initial state is created as a sensible starting point, and then as the application runs, that initial state is transformed several times until it reaches the point where a command will run, and everything has been done. At no point is anything mutable in this approach. Super simplified: class App { def run(input: List[String]): Int { val state = AppState(0) val mapped = mapInput(state, input) // more changes to state // run command, etc. } def mapInput(state: AppState, input: List[String]): AppState = { // ... newState } } case class AppState(argIdx: Int) Are the state objects just lazy? Should I be using individual properties on things as the "initial state" instead of sharing a single object with a few things in? Are there just completely different approaches to managing state? (I've briefly looked into things like a state monad, and it makes sense to make these things a bit more concise, but past that I've not yet been able to see alternative solutions).
&gt; You can't tell me case classes can't be operated on with assembly No one said differently, and I'm not sure what you're arguing. You asked how a struct is different from a case class and I've explained why.
My point of view, from what I know, is that `@struct` is a needless addition to the language. If I was implementing something like that in plain Scala, I'd just use a case class. Case classes can be easily implemented in assembly already, so I don't see why this needs new language feature needs to be added.
Perfect, gotcha :) Thanks :) 
And what's wrong with "extends anyval"?
What I want to know is, does it do away with null? If it can it should at least be an option.
It seems an EPFL project. Any idea whether this will be officially supported or not?
Oh, it will be, eventually, once structs support inheriting from traits (all case classes implicitly inherit from Product and Serializable). They don't at the moment. 
Perfect! Thanks :) 
&gt; Is there any information available of what can be used? [Stay tuned for future updates on twitter](http://twitter.com/scala_native). We'll get more documentation in the near future. &gt; Can we use Java classes? All the code has to be in Scala. We'll provide implementation of subset of java.* libraries. &gt; Can I compile the same code to both the JVM and Scala native? Yes. The base language is the same. &gt; What platforms are supported right now? It's currently developed on Darwin/Intel/x64. We mostly aim at POSIX-compatible environments at the moment. &gt; Is there a roadmap/release schedule? Github issues and milestones are our roadmap. [M3 is next milestone.](https://github.com/scala-native/scala-native/milestones/M3)
Cool project, but I wish people would stop repeating the trite "native is better" argumemt so lightly. It is just as crappy to say running on the JVM is always better, but many of the "good" features mentioned come from running on the JVM which has been optimized for decades. Case in point: &gt; Boehm GC This is just in no way close to a "real" GC and more like a big compromise. I know they will probably build a proper GC, but that is a lot of effort again.
Inject the actor with the time source. The basic approach is to never give anything direct access to `DateTime.now` or a scheduler. Hide those things behind an interface. Then you can replace it with something that produces the same side effect, but without any connection to real time.
Honestly, I don't care about most of those things. A sufficiently motivated and resourceful person with strong dev skills in any modern language could learn all of that in a week. Show me someone who's heard great things about Scala, but can tell me in thoughtful detail how they delivered a React+Redux application on their own, with little previous JS experience, for example. I can work with that. At my previous job, within a year, I went from being a contractor doing front-end JS work to leading a Scala team. Granted, I have a lot of experience doing plenty of other work, but I knew literally *none* of the things you mention when I was initially brought in as a senior engineer. Don't get me wrong, sometimes you know what you need, and you want it now. But often times, I think companies hire on the wrong proxies for effectiveness, focusing on skills rather than capabilities and track record.
&gt;Scala Native is compiled ahead-of-time via LLVM. This means that there is no sluggish warm-up phase that's common for just-in-time compilers. Your code is immediately fast and ready for action. Ready for action, yes. Fast, not so much. AOT compilation comes with performance penalties of its own, especially when combined with dynamic linking. HotSpot uses JIT compilation for a reason. Overall, this project looks like a waste of time. As skilled as the developers must be to implement something like this in the first place, their time is surely too valuable to waste on this.
&gt; Could you expand I was thinking that "extends AnyVal" can be used to indicate that we want to use a value rather than a reference, so it could be used instead of @struct. It probably wouldn't be compatible with vanilla Scala if you have more than one val in the constructor, but I'm not sure.
&gt; Ready for action, yes. Fast, not so much. AOT compilation comes with performance penalties of its own, especially when combined with dynamic linking. HotSpot uses JIT compilation for a reason. Scala native may not even have dynamic linking, which would give it a niche of its own. There are plenty of advantages of scala-native over JVM that can be seen already - No overhead access to C libraries (this is probably being able to compile C sources to LLVM and Scala sources to LLVM and combining) - Much easier go style deployment (single static library that is fast to load). I actually have a project idea that needs this, and without Scala native I just wouldn't be able to use Scala - Able to squeeze much better performance and lower latency due to provide explicit features like `struct` and `malloc` (as is seen) - Able to target platforms like iOS, which are not allowed to have a JIT Honestly this comment is incredibly short sighted
What is your goal? Are you trying to bring Scala to embedded systems that do not have a JVM? Or are you trying to bring Scala to client side applications that are normally written in C++?
 Hi, where can I find the " org.scala-native#nscplugin_2.11;0.1-SNAPSHOT:" dependency to compile it? Is it possible to use it as an sbt plugin now?
When I last developed on the JVM you couldn't really do anything without writting a good amount of XML. For you working Scala devs out there. How often do you have to write or edit XML?
Also posting here because I guess Scala programmers would be interested in it as well. Ende also has first-class instances. And `implicitly` in the article is obviously from Scala.
&gt;No overhead access to C libraries (this is probably being able to compile C sources to LLVM and Scala sources to LLVM and combining) Do you have actual benchmarks indicating that your actual application has an actual problem with JNI overhead? Because that sounds like hot air. &gt;Much easier go style deployment (single static library that is fast to load). How on Earth is that an advantage? &gt;Able to squeeze much better performance and lower latency due to provide explicit features like struct Full support for value types would be nice, I admit, but there are very few applications where it is so compelling that it makes up for everything you lose by abandoning the JVM. &gt;and malloc I assume you mean `stackalloc`, i.e. explicit stack allocation. `malloc` allocates heap objects, which is already quite possible on the JVM. Does Scala Native perform escape analysis on every `stackalloc`? I'm guessing it doesn't, because if it did, there would be no point in making it explicit. If it doesn't, then that is a gaping hole in the language's memory safety. Do you want use-after-free bugs? Because that's how you get use-after-free bugs. HotSpot does perform escape analysis, and allocates objects on the stack, when it can prove that they will never escape the stack frame they were allocated in. That is the only safe way to do stack allocations, and HotSpot already does it. Scala Native has no advantage here. &gt;Able to target platforms like iOS, which are not allowed to have a JIT And use what GUI toolkit? &gt;Honestly this comment is incredibly short sighted I'd say the “native is better” crowd is who's shortsighted. Did you people learn nothing from the mistake Kernighan and Ritchie made when they invented C?
&gt; Ende is a hypothetical programming language. I've a "hypothetical prog lang" too if you're interested: [ave](https://github.com/stevendobay/ave)
Ugh. HTML/CSS is a horrible GUI toolkit.
Yeah, well. I don't think there's another GUI toolkit that would reliably run everywhere (including as a web app without any browser plugins). Honestly I cringe more about the shipping-embeded-node.js part. That's downright horrifying, but this technique is used more and more often now.
&gt; Do you have actual benchmarks indicating that your actual application has an actual problem with JNI overhead? Because that sounds like hot air. JNI is ridiculously difficult to use properly (and because of this most implementations do add some overhead due to not doing it in the most performant manner). Everything that is not JNI will also be slower. It could even be possible that with an ideal JNI implementation, scala-native would still be faster due to WPO (whole program optimization) which JNI doesn't do. scala-native will generate LLVM bytecode from the C libraries, and LLVM bytecode from Scala and combine them. The LLVM optimizer takes care of fusing them. In JVM + JNI this is not possible. &gt; How on Earth is that an advantage? - If you need to develop applications with zero dependencies and no downtime, then its a huge advantage. i.e. Amazon use go for the ECS agent because its a hell of a lot easier to replace a single binary (with no downtime) on a minimal server image versus heaving to deal with JVM + Scala. It also guarnatees that the binary will work as expected (unlike JVM) - Small devices/microcontrollers - Platforms where JVM doesn't work well or doesn't exist &gt; Does Scala Native perform escape analysis on every stackalloc? I'm guessing it doesn't, because if it did, there would be no point in making it explicit. If it doesn't, then that is a gaping hole in the language's memory safety. Do you want use-after-free bugs? Because that's how you get use-after-free bugs. The language is adding `unsafe` (blocks and methods) to designate parts of code which are memory unsafe. This is similar to Rust. &gt; And use what GUI toolkit? Uh Cocoa? Both XCode and Scala-Native use the LLVM toolkit. Robokit (which just recently got killed), transformed JVM bytecode to LLVM and had native interopt with the Cocoa UI toolkit for iOS and Desktop apps. You can't do that with current Scala. &gt; I'd say the “native is better” crowd is who's shortsighted. Did you people learn nothing from the mistake Kernighan and Ritchie made when they invented C? This is honestly an incredibly shortsighted and misguided comment.
Cool! I'll read your specification after it's released.
&gt;scala-native will generate LLVM bytecode from the C libraries I find that very difficult to believe. Sounds like either a pipe dream or an outright misrepresentation. &gt;If you need to develop applications with zero dependencies and no downtime Then you use `jlink`, which will probably be stable long before Scala Native. &gt;heaving to deal with JVM + Scala You say that as if it's some kind of big deal. It isn't. Your application can come with a bundled JRE, right now. My employer has been shipping desktop apps in exactly that configuration for over a decade, to customers that probably don't even know what Java *is.* Deployment of JVM applications does *not* have to be difficult. &gt;It also guarnatees that the binary will work as expected (unlike JVM) No such guarantee is possible for any software, regardless of language or compilation strategy. Don't be ridiculous. &gt;Small devices/microcontrollers Scala Native uses a garbage collector. You will not be using it on any microcontroller. Again, don't be ridiculous. &gt;Platforms where JVM doesn't work well or doesn't exist Other than microcontrollers, I don't know of many platforms that have no way of executing JVM bytecode. &gt;The language is adding unsafe (blocks and methods) to designate parts of code which are memory unsafe. This is similar to Rust. Yes, and just like Rust, as the name “unsafe” implies, you are supposed to *not use it* unless absolutely necessary. In the case of stack allocations, that basically means never using it at all, making the whole feature useless. Escape analysis, on the other hand, can be and is used in actual practice. &gt;Uh Cocoa? Then your code is specific to a single, shitty platform. And who the hell is going to do the Herculean task of writing a Cocoa binding specifically for Scala Native? &gt;Robokit (which just recently got killed), transformed JVM bytecode to LLVM and had native interopt with the Cocoa UI toolkit for iOS and Desktop apps. You can't do that with current Scala. If it transformed JVM bytecode, then yes, yes I can. Current Scala compiles to JVM bytecode, remember? &gt;This is honestly an incredibly shortsighted and misguided comment. I'm sorry you feel that way. How many generations must there be before this misplaced reverence for K&amp;R finally dies off? C was a mediocre, glorified PDP-11 assembly that should never have found its way out of Bell Labs, let alone become as absurdly popular as it did.
Why is “including as a web app” a constraint?
https://github.com/scala-native/scala-native/pull/53/files#diff-04c6e90faac2675aa89e2176d2eec7d8R13 &gt; sbt rtlib/publishLocal nscplugin/publishLocal to try demo: &gt; sbt demoNative/run 
It wouldn't be, which I assume is the reason. This way an `@struct case class ...` can work more efficiently on scala-native but still be compatible with scala-jvm and scala-js.
At the moment the only solution is "compile it", which really isn't a solution at all for this use case :/
Honestly that sounds more complicated than it needs to be. Maybe it'd make more sense with a code example that includes what it's actually doing? Are you talking about having some "base state" and applying a small number of changes as a result of input? That might be modeled nicely by using lenses to build up a changeset and then applying it all at the end. Or it might be easiest to just pass the state down and up - splitting it on the way down and combining on the way up. 
anything uploaded yet?
I've seen lenses before, but do you have an example of that last thing you described, the splitting and combining? That sounds very interesting and might suit this very well. I am pretty much talking about having some kind of state that evolves over the course of "booting", once it's done it's done, but I'm quite used to mutability to achieve this and am just trying to figure out what approaches there are available to solving this kind of problem in a functional manner. Thank you for taking the time to answer these questions by the way :)
Does "kantan" stand for かんたん = "easy" in Japanese, or is it a happy coincidence? :-p
Don't think I have any public examples. Just have your functions take the same structure as your data, if that makes sense: if a user has a name and an address, the function that does something to a user calls the function that does something to the name and the function thar does something to the address.
I'm getting this for it: &gt; Unresolved dependencies: `#scala.Serializable` scala.scalanative.linker.LinkingError 
Hey, I'm looking for scala devs in Chicago. Let's talk 
The only XML I deal with these days is when I work with Maven, or simple config like Log4j2, both rare. At my last job there was a little bit more because we had to integrate with a Java platform designed when XML was all the rage, but it still really wasn't all that present.
Ouch.
I think this would provoke an interesting discussion in /r/programming so I stole your link and put it there. Great article!
&gt; Scala code tends to create a lot of intermediate objects (i.e. using collections), and although the hotspot does a good job of dealing with that, its not completely optimised for it compared to other things. My understanding is that all the JVM's GCs are definitely optimized for short-lived garbage
How is the macro supposed to know where to write the file?
It's the same compiler, just a different backend. Scala is going towards better `null` handling, but that's not there yet. See also [this slide](http://image.slidesharecdn.com/scaladays-nyc-2016-160510135215/95/scala-days-nyc-2016-53-638.jpg?cb=1462901685) from [this presentation](http://www.slideshare.net/Odersky/scala-days-nyc-2016).
For simple use cases (algorithms, prototyping functions etc.) try out ScalaKata. It also has a collaborative mode :)
Seconded heartily. Java 8 with Lombok actually is really not that bad 
Thank you for your suggestions.
Even in straight Java you don't need it any more. Spring can be configured with code and annotations. Hibernate can be configured with annotations. AOP has largely been recognised as terrible. Jackson can be configured with annotations on interfaces you "mix in" via a registry of class -&gt; class maps which is hilariously awful but better than XML. I still use XML for maven (where I think it's actually appropriate) but that's all. 
The lack of a free JVM on iOS is dumb, as is the way that corporate environments are perfectly happy to install web browsers but look askance at Java Web Start. Nevertheless, here we are, raging against these things will do nothing, so let's try to work around them. Also a (free) way to have Scala apps that start much faster is genuinely exciting. It opens the path to writing small command-line utilities in Scala. Possibly JVM work could achieve the same goal, but so far it hasn't.
Believe it or not, people are using Scala.js to write CLI tools with Node.js, nowadays. Significantly lower throughput, but no noticeable startup time.
Can't use multiple fields nor mutable fields.
What a great writeup. Especially the test directory layout, and what "partest" actually is was very clarifying. Would love to see this linked from the GitHub page, or adapted in to it.
I would add Andy Petrella's [Spark Notebook](https://github.com/andypetrella/spark-notebook) to this list. Honestly, any of these notebooks would be a fine tool for learning Scala and Spark. All of them provide at least some degree of graphing/charting support. Where I think they tend to diverge is in the extent to which the focus is being the best all-around interactive data science platform regardless of language (Jupyter, Beaker) or emphasize keeping up-to-date with respect to Scala/the Spark ecosystem and deeply integrating that with a particular stack for presentation (Andy Petrella's Spark Notebook). So the advantage of choosing plain Jupyter, for example, would be lots of access to other notebooks, possibly in other languages, because Jupyter is kind of the mother of them all. Zeppelin would be kind of a nice middle ground: an Apache project, good community, and a bit more focus on Scala/Spark. Andy's notebook, again, is at the other extreme: all about Scala/Spark, so more tightly integrated. So if you're just starting out today, I think for a nice mix of ease of installation, documentation, Scala/Spark support, and graphing out of the box, I suggest installing Zeppelin. That's my $0.02, anyway. :-)
When I first saw sbt I thought like you. But I read the book "Sbt in action" Now I think sbt is a really great build tool. https://www.manning.com/books/sbt-in-action
That's amazing, thank you very much!
Thanks for the enlightening walk-through!
I tried to do an sbt build, and I hit a/the circular dependency problem. I could have sworn I read about this before, but I can't remember what the solution was. &gt; [error] (library/*:update) java.lang.IllegalArgumentException: a module is not authorized to depend on itself: org.scala-lang#scala-library;2.12.0-M4 Anyone remember what that was?
What is the actual error the compiler gives you? If it is "integer number too large", you can put the number in quotes in the BigInt constructor like this: scala.math.BigInt("218922995834555169026")
Ok, here is the code. Edit: I'm new to reddit, I don't know how to format the code. Sorry. hehe // X: The nth fibonacci number to be calculated, 100 for example. def fibonacci(x: Int): scala.math.BigInt = { def loop(x: Int, past: scala.math.BigInt, current: Int, nth: Int): Int = { if(x == 0) { past } else if (x == 1) { current } else if (x == nth) { past } else { loop(x, past + current, past, nth + 1) } } loop(x, 0, 1, 0) } Thank you.
You just need to indent your entire block by four spaces, then the formatting will be preserved, like this: // X: The nth fibonacci number to be calculated, 100 for example. def fibonacci(x: Int): scala.math.BigInt = { def loop(x: Int, past: scala.math.BigInt, current: Int, nth: Int): Int = { if(x == 0) { past } else if (x == 1) { current } else if (x == nth) { past } else { loop(x, past + current, past, nth + 1) } } loop(x, 0, 1, 0) } *edit*: hmm, why is `loop` returning an `Int`? Shouldn't it be `BigInt`?
Yeah, you are exactly right :) Thanks to all guys.
ScalaMock is a good framework to take a look at
Nice sintax, however there is the need of some n arriving as paramater so you can generate the nth fibonacci bumber. Thank you anyway.
`fibonacci(N)` will give you the nth number, in the example above
Are you trying to build the same version you're depending on? You need to change the version number.
I don't think so; I'm trying to build HEAD of 2.12.x The sbt file is too long to quickly make sense of, but from the settings name I'm guessing globalVersionSettings baseVersion in Global := "2.12.0" baseVersionSuffix in Global := "SNAPSHOT" means I'm trying to build 2.12.0-SNAPSHOT The compile output up to the error is compile Getting Scala 2.12.0-M4 ... downloading http://repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.0-M4/scala-compiler-2.12.0-M4.jar ... [SUCCESSFUL ] org.scala-lang#scala-compiler;2.12.0-M4!scala-compiler.jar (1553ms) downloading http://repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.0-M4/scala-library-2.12.0-M4.jar ... [SUCCESSFUL ] org.scala-lang#scala-library;2.12.0-M4!scala-library.jar (886ms) downloading http://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.0-M4/scala-reflect-2.12.0-M4.jar ... [SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.0-M4!scala-reflect.jar (599ms) downloading http://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12.0-M4/1.0.5/scala-xml_2.12.0-M4-1.0.5.jar ... [SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.12.0-M4;1.0.5!scala-xml_2.12.0-M4.jar(bundle) (127ms) downloading http://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.12.0-M4/1.0.4/scala-parser-combinators_2.12.0-M4-1.0.4.jar ... [SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.12.0-M4;1.0.4!scala-parser-combinators_2.12.0-M4.jar(bundle) (73ms) :: retrieving :: org.scala-sbt#boot-scala confs: [default] 6 artifacts copied, 0 already retrieved (18416kB/46ms) [info] Updating {file:/D:/scala/scala/}library... [info] Updating {file:/D:/scala/scala/}root... [warn] Binary version (2.10) for dependency org.scala-lang#scala-compiler;2.10.6 [warn] in org.scala-lang#scala-library;2.12.0-SNAPSHOT differs from Scala binary version in project (2.12.0-M4). [warn] Binary version (2.10) for dependency org.scala-lang#scala-library;2.10.6 [warn] in org.scala-lang#scala-library;2.12.0-SNAPSHOT differs from Scala binary version in project (2.12.0-M4). [warn] Binary version (2.10) for dependency org.scala-lang#scala-reflect;2.10.6 [warn] in org.scala-lang#scala-library;2.12.0-SNAPSHOT differs from Scala binary version in project (2.12.0-M4). [warn] Binary version (2.10) for dependency org.scala-lang#scalap;2.10.6 [warn] in org.scala-lang#scala-library;2.12.0-SNAPSHOT differs from Scala binary version in project (2.12.0-M4). [info] Resolving org.scala-lang#scalap;2.10.6 ... [info] Done updating. [error] a module is not authorized to depend on itself: org.scala-lang#scala-library;2.10.6 
Stream memoizes its results for every ..., N-2, N-1, N; so that's what makes it fast (albeit O(n))
Im dealing with an unexpected exception, [SO](http://stackoverflow.com/questions/37217369/unexpected-exception-thrown-within-a-for-loop-with-an-inner-try-block?noredirect=1#comment61966415_37217369) link.
2.10 is used by sbt. Do you have any plugins in your global sbt configuration? You could also try removing the cached (or even locally published) artifacts from your Ivy cache.
Streams in Scala are super cool: http://philwantsfish.github.io/scala/streamsandprimes (Seen recently on /r/scala)
making progress by disabling plugins. No full compile yet due to running out of heap, but that should be fixable.
When do you get the future out of the map and what do you do with it?
I dont get a future out of the map, I only the promises there. I return a future from a promise when I put an item into a stream. Then the stream gets processed. Once an item is processed its placed into another stream, from that stream I pattern match, and depending on the type of class, I then pull out the promise out of the map and then I fail it if its a failure I then succeed it if its the success class.
Hacking on dotty is almost exactly the same. But "drawing the owl" is probably easier since the dotty source code is really nice. It took me about two days from cloning the repository to implementing once-curried update method syntax. And I have been working on another related thing: var a = 1 var b = 2 val arr = Array(1,2,3) def method() = (3,4) (a,b) = method() (arr,b) = method() That much compiles and works as expected. Nested tuples work too. The implementation so far is about 100 lines. But next I want it to work with multiple parameters in the RHS of a once-curried update. I have also been thinking it would be nice to have: `def method(a,b,c:Int)` === `def method(a:Int,b:Int,c:Int)` Since this syntax is already valid in scalac: `val a,b,c:Int = 42` I will probably submit a SIP and PR of various syntax tweaks some time in the future. These new syntax do not get past the parser in scalac, so there could be no issues with backward compatibility. Also did you know that update methods can have any return type?! object X { def update(x:Any):Double = 8.24 } val wat:Double = { X() = null } That compiles on scalac.
Thanks. Glad to hear about the improvements. I'll have to give Scala and jthe JVM another try.
Loved it, but I am really, really impressed by the stamina of all the Scala folks who had to be there for nine hours a day, for three days, making conversation, watching technical talks and fielding questions almost constantly. Especially when the weather outside was so amazing I just wanted to run topside and bask in the plaza.
What is the use case? What you can easily do is generate an `Array[Byte]` that resides in a field of an annotated class file. But you don't get from a macro to the build system (it may be sbt, it may be not). For an sbt build, you may want to use resource-generators.
Disabling the Ensime plugin is a workaround for this. I'll update the post with a proper fix as soon as I know what it is.
Awesome! Thanks for pointing this out ... I'll update the post. Also thanks for the heads up about comments not working.
&gt;What is the use case? Compact binary XML + interpolation indices. The idea was to develop an alternative to Scala's built-in XML literals. The binary XML would be read via `Class#getResource`, and XML nodes constructed from that. On further consideration, though, I decided not to bother with this, and just emit code to construct the nodes the simple way. &gt;What you can easily do is generate an Array[Byte] that resides in a field of an annotated class file. But this results in code that, at run time, fills in the array one byte at a time.
&gt; But this results in code that, at run time, fills in the array one byte at a time. I think the Java byte code supports array literals. It would be a matter of fixing `Array.apply`. Not sure this is on the table somewhere for future Scala versions.
[Nope.](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.4) It does support *string* literals, which I've used for this purpose in the past, but not array literals. Problem: because strings are encoded different ways at different times (UTF-8 in the class file, but UTF-16 or possibly ISO-8859-1 in memory), this may result in memory being wasted. And in any case, string literals remain in memory as long as the class remains loaded, which can be wasteful.
To start using dotty all you have to do your scala version to `0.1-SNAPSHOT` using the newest version of sbt like in this [project](https://github.com/smarter/dotty-example-project/blob/master/build.sbt).
I agree Scala is becoming what the "cool kids" use and I see it helping you both get a job and attract candidates. However, for Data Science, R is the weapon of choice (or Python)...strange they discouraged Python.
The contrast of the text on that website seems incredibly low.
The `if_` here is ScalaZ `??` I wouldn't parse the String representation multiple times for strikes and the like; better to model the cases with `case object`s and separate the parsing from the score calculation, I think. A `foldLeft` is probably better than explicit recursion. I'm not sure what the magic `18` and `19` are. If it's just about safely not going off the end of the list, wouldn't `applyOrElse`or similar be better?
Oh wow, a lot here, thanks for taking the time! Might take me a while to get my head around all of it. I'll gradually push the changes as I read up on it all. I don't know the Scala standard library at all really, which is why I did some things the long way, so thanks for pointing me in the right direction. I would like to use monads for the IO, just to learn them more than anything else. Thanks for the flattery as well. You've been really helpful.
This looks very promising and it has a lot of documentation. Unfortunately my browser seems to have a big problem with scrolling on the documentation page. I can only scroll down to "[Query reducers](http://sangria-graphql.org/learn/#query-reducers)". I am quite curious if there is more text coming.
I want to use type safe enumerations for a Play project with JSON / REST fronted and a Slick backend. For me it looks like [enumeraton](https://github.com/lloydmeta/enumeratum) is the simples but also most complete choice: There already is Play JSON support built in and for Slick I just need a `MappedColumnType`. Are there other options I am overlooking? Shapeless? Scalaz co-products? Something else?
Too lazy to review your code but I wrote a Brainfuck interpreter by myself years ago, you can compare it to yours: https://gist.github.com/sschaef/8c5c3027edcad2a89ea08f72ca5dcea7 It basically has half the lines of code than yours.
Yeah, the built-in enumeration class is pretty terrible. You could just use case objects.
Did pattern matching come from ML?
m50d beat me to most of it but here are a few more things: In `nextState()` there is a `case _ =&gt; this` which seems fishy. You filtered the input for `"[]&lt;&gt;+-.,"`, so I would rather crash than fail silently there. I do like the way you gave the parameters names before passing them into the constructor; but you could instead give the constructor parameters themselves informative names and then pass them in as named parameters. And if you make it a case class then you can also skip the ones you don't need: copy(left = tailOrZero(left), right = current::right, current = headOrZero(left)) I find it is easier to think about when you can immediately see what things do not change. In Scala it is conventional to add `()` at the end of a zero-parameter method name if it is side effecting; vice-versa the convention seems to be weaker but I would remove `()` from the ends of most of those methods. I personally would have nested most methods into `nextState()`, but that is largely a matter of personal taste; and then you lose the possibility to override. In a larger code base it can be better to expose a minimal amount of methods, because it sucks having huge amount of irrelevant methods show up in autocomplete. Here is my attempt at refactoring `matchBracket`: def matchBracket(bracket: Char): Machine = { val otherBracket = if (bracket == '[') ']' else '[' val inc = if (bracket == '[') 1 else -1 def stepWise(shifted:Int, depth: Int):Machine = program(shifted) match { case `bracket` =&gt; stepWise(shifted+1,depth-inc) case `otherBracket` =&gt; if (depth == 0) copy(at = shifted) else stepWise(shifted+1,depth + inc) case nonBracket =&gt; stepWise(shifted+1,depth) } stepWise(at+inc,0) } I do like the way sschaef_ did the braces, under some conditions it would be faster
I use Java `enum`s whenever I need an enumeration - sadly they're better than anything available in Scala.
IRC: #scala@freenode, #scala: Total of 457 nicks [0 ops, 0 halfops, 0 voices, 457 normal]
Why is the pattern match easier to read?
I would personally prefer the if-else.
The slides are nice. Is the audio track available somewhere?
Python and R are great for data exploration (both have many, many packages, of varying quality though). For production in Big Data environments, scala+spark is very hard to beat.
What's the best web framework to work with? I am new to Scala and the normal way I learn a language is to take my default project, a web application, and rewrite it. It has plain HTML pages and JSON APIs. I've been looking at Play (large, lots of functionality) and Finatra (simple to start with, quite powerful). I am tempted to go with Finagle, as I think that the Twitter stack is great and links in with my job (devops) but I'd like some feedback from people who know Scala.
I'd also say that there is more ML influence, but Martin Odersky [worked](http://lampwww.epfl.ch/~odersky/papers/) with Phil Wadler, who contributed to the design of Haskell, on a number of occasions, so it shouldn't surprise that Scala picked up ideas from Haskell.
Can you clarify what you mean by "returning" the tail. Also you write you use "a mutable list declared with `var`". That is kind of contradictory. Either you want a mutable list, then you would store it in a `val` and mutate it with methods that do not return new copies of the list (that's what `tail` does), or you want an immutable list, then you would store it in a `var` and replace it with immutable copies as returned by `tail`. In that case your approach is correct: var myList = List(1, 2, 3) myList = myList.tail // List(2, 3)
I don't know why you want to complicate things with a type class here. Looks very clear to me. If you want to disallow the `override`, declare the class `final`.
I recommend starting an app and then just asking a lot of questions. A lot of us are on Twitter and IRC to answer questions (#scala on freenode). The irc room has a REPL in the chat so you can get real time help on 'why doesn't this compile?' or 'how do I do this better'? I used "Programming in Scala" as more of a reference, and just sort of dove in writing code and asking for feedback. Functional Programming in Scala is also a *fantastic* (and also challenging) book, but more about how to design programs in a pure way, and less about the language itself. I would spend a bit of time getting comfortable with the language, and then definitely add that book to your reading list. 
Lame. Takes forever to get underway and then it's just lame.
Look at the companion object of the collection type you're interested in. E.g., `List.fill`: http://www.scala-lang.org/api/2.11.7/#scala.collection.immutable.List$
This is really great stuff. Are you ever in the #Scalaz channel on freenode? 
Hm, reactive streams to integrate with hardware devices is nice. But there are already well established M2M protocols there. Have you looked into [MQTT](https://en.wikipedia.org/wiki/MQTT) or [CoAP](https://en.wikipedia.org/wiki/Constrained_Application_Protocol) before you decided to built your own? Did you have special concerns that did not fit the established alternatives? Or was it more a research approach to explore how well Akka streams preforms against the established competition?
No, unfortunately there isn't. The code pieces shown during the presentation are part of our code base at Measurence.
Out of interest, I'd be interested in hearing why you went with Scalaz for this series. I have been quite conservative in introducing more monads/FP typeclasses at work, but we plan to try it out more in places where they would be appropriate... And I'm not sure whether to go with Scalaz or Cats. There seems to be a move in the community towards Cats - which seems to have more approachable scaladoc and names for things. Also, for lenses specifically, Monocle seems to be very popular.
It's awesome to read how people solve this problem. But the solutions are almost always for scoring a full game. I'd be interested to see more solutions on how to implement a partially completed game. 
Singletons with mutable state will never be idiomatic. If you want idiomatic see if you can structure what you're doing as a `foldLeft` down the list or similar. 
I choked so hard at episode 3. 
MQTT and CoAP are designed for devices with batteries and low bandwidth: we didn't (and we don't) need their features, our protocol is much simpler :) Furthermore, we focused on implementing a robust and reliable system able to ingest data, we didn't focus on interoperability.
Cats is probably a better choice in the long term, but at the moment it's immature; it's pre-1.0 and comes with big warnings about the lack of compatibility guarantees, and it's missing a lot of functionality that's present in ScalaZ. Monocle does indeed seem to be the consensus lens library, but if you're using ScalaZ already then I don't think Monocle necessarily offers enough advantages to justify using a second library.
This is a great idea! Well delivered.
&gt; In particular, I don't understand the `but does not ...` part with respect to the OO version. Could you please give an example that demonstrates your point? If I want to ask "what is the `resourceType` for `Application`?" I have no way to answer the question without an instance of `Application`, and any given instance might give a different answer. In the the typeclass representation this information is associated with the *type*, not with any individual instance. See `// no instance needed!` above. What is the mistake in your code above? I don't think I'm seeing it.
&gt; Singletons with mutable state will never be idiomatic Ok yea. I reworked my code to use recursion and keep calling a method and pass the tail to that method. But everything is still a `val`. Thanks for the help!
&gt; What is the mistake in your code above? I don't think I'm seeing it. Note that the `ApiResource` has a `resourceType` of value `App`. Per the above protocol/requirement, only an `AppResource` must have a `resourceType` of value `App`. And `ApiResource` must have a `resourceType` of value `Api`. Ideally this requirement would be enforced at compile-time.
I don't think I understand ... the association is arbitrary and the typeclass instance is what you use to specify it. Is this a mistake or not? class Foo(val name: String) object Foo { implicit val FooResource: Resource[Foo] = Resource.instance(App, _.name) } 
Honestly, if you're doing a lot with concurrency and/or networking, you still need scalaz.concurrent and probably scalaz-stream at this point. That's not a _complaint_; I _like_ scalaz and scalaz-stream. It's just to underscore m50d's point that Cats, at this point, is not only immature, but is simply incomplete. That said, there's no question that it benefits tremendously from the focus on modern implementation approaches, its commitment to documentation, etc.
Thanks for the info! I am surprised about the Finatra/Finagle, I've talked to 2 (big) companies that use Scala (I work in operations) and both of them used the Twitter stack. Reading about Play makes me not wanna use it. I will give akka-http a go, I like the concept of Akka (I'm a big Erlang/Elixir fan) and, if works as an introduction to the framework, it will help me reach where I want. Regarding Wicket - I'm not a fan, having done some work with it on Java, admittedly quite a long time ago (7 years), but I will have another look. Again, thanks.
They did target the java ecosystem first, because that's where the money is to be made. I can't really begrudge them that. Given that it is mostly scala under the hood, there's probably a reasonable chance that it will have a nice scala API one day.
I believe I've seen them say that it's effectively 1.0 at the moment and they're just adding finishing touches.
I can't figure out how to do a joining stream collector in Scala. In Java I can use Collectors.joining(), e.g. Stream.of("abc","def","ghi").collect(Collectors.joining("+")) gives "abc+def+ghi". What would be the equivalent in Scala? Array("abc","def","ghi").toStream. ...???
I think either one is better than none. I'll try to be unbiased(disclaimer: I'm a Scalaz committer and two of my coworkers/friends are some of the most prolific Cats committers) 1. Cats is a more modular approach, it's going to be smaller for sure, and depending on if you're already using some other dependencies, a smaller footprint overall. This may matter if you're writing client code or for scalajs. 2. Cats is putting more emphasis on being beginner friendly. This isn't to say Scalaz is *anti*-beginner, it's just that the community contributions haven't generally been focused on it compared to Cats. This might be important to your team. PR's welcome on this front though :) 3. Scalaz has a *lot* of things you might(?) one day want, and they all integrate nicely together. There is a whole host of very nice data structures, concurrency tools, safer alternatives to standard library offerings and some higher level abstractions. Originally I don't think Cats planned to add this sort of thing. Also the reason for #1. 4. Anecdotally, Scalaz seems to be putting a fair bit of effort into porting/researching/inventing some newer FP ideas. There's some very cool/big changes coming in the pipeline, being developed under another repo, which is why it may seem less active than Cats. 5. Scalaz probably has a slight edge in library compatibility since it's been around longer. 
Sort of, but not quite. `List.patch` is more general than `std::fill`, but in my use case I would have to use `List.fill`first to create container filled with given value and then `List.patch` to replace the slice. It is more convoluted than C++ API.
Man I dunno how I feel about Scala native. Seriously cool effort but a large part of the draw to Scala is access to the massive Java library ecosystem.
No.
I don't think there's a function in the standard library, but it's not terribly difficult to write one. A quick version sans validation might be def fill[T](s: mutable.Seq[T], r: Range, v: T): mutable.Seq[T] = { r foreach (i =&gt; s.update(i, v)) s }
No, lauris/awesome-scala is not related to this project.
I think this will take some time... (they are usually uploaded to Parleys)
Let me tell you about `fold`. The most basic of operation on collections is known as a fold. Folds can do basically anything, from filtering and mapping to aggregation. As an example, `List(1,2,3).foldLeft(0) {_ + _}` will sum all values of the list, while `List(1,2,3).foldLeft(1) {_ * _}`gives the product. `foldLeft` starts from the "left" of an ordered collection, which is good for collections like List as accessing the head (first element) and tail (successive elements) is a constant time operation, whereas `foldRight` starts with the "rightmost" or last element. As accessing the last element of a list is an order of O(n) operation, foldRight on a List is an O(n^2) operation. Why do I talk so much about folds? Well, as I mentioned earlier it's basically the most general operation you can use for collections. More specific operations like `mkString` may be implemented to be more performant (using a StringBuilder rather than string concatenation in this specific case) but they basically result in the same thing.
Your assumption is correct.
More precisely. The compiler points out in the error message: "double definition: ... have same type after erasure: (f: Function0)...". This is because by-name parameters of type T are internally represented as Function0[T]. After erasure, the type arguments of Function0[...] are thrown away, which leaves Function0 (not Function0[Something], but just Function0).
Oh i see. I guess it makes sense, thanks. I had read about Function signatures but had not really thought that the parameters (or return) types would be excluded after type erasure. Much obliged
Shouldn't that be pretty straightforward? The compiler's written in Scala and mostly performing pure computation - I don't imagine it's the kind of thing that would make heavy use of execution contexts or the like.
This doesn't necessarily stop you in your tracks though. What you can potentially do is take in an "evidence" that encapsulate your operation on a certain type. Something like import scala.concurrent._ import scala.concurrent.duration._ trait Retryable[T] { def retryAndRetrive(delay: FiniteDuration, maxRetries: Int)(t: T) = { val result = execute(t) // check result, decide on delay/repeat result } protected def execute(thing: T): T } object Retryable { implicit def retryableT[T] = new Retryable[T] { override protected def execute(thing: T) = thing } implicit def retryableFutureT[T] = new Retryable[Future[T]] { override protected def execute(thing: Future[T]) = thing } } def retryDefault[T](f: =&gt; T)(implicit evidence: Retryable[T]): T = { evidence.retryAndRetrive(5.milliseconds, 5)(f) } retryDefault(42) retryDefault(Future.successful(42)) edit: There are some logic flaws in the above code, like how you are supposed to decide wether a non-specific "T" should be retried or not. The bottom line though is that this allows your API to look like you wanted, kinda.
Thanks for the response. Regarding 4, I look look forward to the announcement. :) Based on your response and Paul's, I'm thinking at Cats is probably the right choice to introduce into a team that's new to this. We won't be discussing scalaz.concurrent.Task until everyone's comfortable with Read/Writer/State/Xor etc. Maybe we'll have a cats Task by then. It'll be interesting to see how the usage of the two libraries varies over time. It sounds like there are good reasons to use Scalaz, but the impression I (and therefore others) had was that cats is being lined up as the "new user friendly Scalaz".
Would you advise using several methods with different names and thus being self explanatory. Or following a type class approach and attempt to construct a base trait and build different implementations on top of it.
There is a super easy way to deal with this: add a dummy implict. def bypassesErasure[T](f: =&gt; Option[T]) = ??? def bypassesErasure[T](f: =&gt; T)(implicit d:DummyImplicit) = ??? bypassesErasure("hello") bypassesErasure(None) That compiles and works as expected. Do take into account that if you want to call the more general method (=&gt;T) with the more specific type (=&gt;Option[T]) you will have to cast to AnyRef or something like that to select the right method and cast it back later. And generally speaking overloading makes your code less readable. Just today I was reading `sys.process.Process` and it took me a while to prove to myself that all the twelve apply methods eventually delegate to a single one... Now if you really want to have """""fun""""" with erasure: try matching over erased types or constructing an array of an erased type.
There's sometimes available a middle ground between code generation and type class (using implicits), and one way to leverage it is the [magnet pattern](http://spray.io/blog/2012-12-13-the-magnet-pattern), which is a good way to get rid of those overloaded methods
I don't happen to use the release plugin anymore, and when I did we weren't cross building. Do you recall any specific challenges? It seems like it should be possible to have the release plugin execute a goal (or whatever the term is) which creates both artifacts instead of just one.
&gt; If the programmers like each other, they play a game called "pair programming". And if not, then the game is called "peer review". &gt; - Anna Nachesa
I've used mkString before on arrays. Now I learned it works with streams, too :-)
Awesome
What company hires Scala interns?
they made you write it on a piece of paper?
Figuring out Eclipse's UI is like sitting in the cockpit of a 1980's fighter jet and reading the 500 page manual on what button Fa62 does when Switch b8fe is Switched on. 
I was hoping someone would comment on how ensime-atom is. I just installed it, but haven't gotten to try it yet
Mine?
I've asked a similar question some time ago. https://www.reddit.com/r/scala/comments/4clx69/what_questions_have_you_been_asked_at_your_scala/
ha, precisely that :). fortunately in Poland, where I live, there's some of those companies :)
oh, I also had those "scala vs Java" and "likes and dislikes about scala" :)
I'd rather have button Fa62 and not need it than need button Fa62 and not have it. I'd echo the above, the IntelliJ plugin for scala makes it 'better' for scala development. Only thing to note is that IntelliJ seems to roll it's own solution to problems, meaning if something's not working as expected, good luck trying to get intellij to tell you what it's actually doing.
That's interesting, I didn't know about `DummyImplicit`. It kinda feels like a hack in a way.. However, if a dummy implicit singleton could be generated and required automatically, it would basically allow erasure-safe overloading.
Just use IntelliJ + Scala plugin, there's no comparison. 
I disagree. IntelliJ gives spurious error highlights all the time. Something like 1/3 of my lines are red in IntelliJ ( most immediately due to https://youtrack.jetbrains.com/issue/SCL-10168 ). Eclipse is slow and crashes occasionally but at least when it tells you your code compiles or doesn't it gets it right.
Someone seriously asked me what happens if you do None map { throw new Exception } 
&gt; Also did you know that update methods can have any return type?! I did; the same is also true of `map`/`flatMap`/`withFilter`/etc. that `for`/`yield` desugar to, so you can define some real WTF implementations of those. To my all these things badly need to be more semantically defined (perhaps tied to a typeclass) rather than the pure syntax transformations they currently are.
Do you mean the "32 elided" bit? That means it hid the names of 32 other calls in the stack trace.
Scala on the JVM serves a different niche. Scala native is aimed at bare metal programming, which is the same target as Rust.
Still, Rust is interesting if you really need to care about every single byte, every allocation, every instruction. If these are not concerns for you, you will take a huge productivity hit for nothing and it might be a much better choice to use Scala-Native instead. Haskell ... well, the tooling just isn't there and given the lack of interest, I don't think that will change anytime soon.
I think they don't expect that much (well, probably depends on position and company), but if you want to have work in Scala it's good to have at least some knowledge about those things, after all they are important part of why many choose Scala. during interview they specifically mentioned that they tried hiring experienced Java devs and that although their code worked, it was still written with another mentality that didn't satisfy the spirit of the team / wasn't easily scalable and maintainable. well scala is not just a "better Java", so I think it's reasonable to ask about its distinctive features
And in which city that is? I am myself Pole and I would LOVE to work in Scala
I PMd you with some extra info 
Yeah there are specific circumstances here. Also even if I could use Eclipse my coworkers blame me when their IDE shows my changes as errors.
I love that Maven forces every project to build simply and consistently, but the flipside of that is that if you do need to do something exotic it becomes very hard.
It's the difference between a block and a function. The argument of `map` has the type `A =&gt; B`. That means that it is expecting to receive a function. Scala evaluates its arguments eagerly like Java and unlike Haskell. If we look at a function `def t(x: Int, y: Int) = x` and call `t(3, 12 + 56)`, the language will still evaluate the value of `y` even though it is never used because the first thing that is done when a function is called is computing the values of its arguments. When you write `map {throw new Exception}` the language is evaluating a *block* that it hopes will return a `A =&gt; B`. The `throw` statement has the special type `Nothing` which is a subclass of each class so it is always legal to throw an exception in any expression, thus this block typechecks. Now when you write this function, when it tries to evaluate your block and get a function out, the block instead throws an exception as its first statement and that's what you see in your REPL. On the other hand, when you write `{x =&gt; ....}`, this is now a *function* that has the correct type, not a block. If you need a function that specifically takes a block rather than a function, you can use the special by-name syntax of `def t(=&gt; Int) = ...` which will now only work if you pass it a block that yields and `Int`, not a function.
Thanks for the explanation! Indeed I just checked the source for `Option.map` and saw this: ``` ... def map[B](f: A =&gt; B): Option[B] = ... ``` But I then wonder, Scala functions has the option to evaluate its arguments lazily (i.e. by name), right? I'm curious why the `map` function is not written: ``` ... def map[B](f: =&gt;A =&gt; B): Option[B] = ... ``` Would that not make the block following `map` be evaluated lazily?
Ah thanks, english hard.
It would, but I don't think there's any value in that - for all sensible use cases you accept the `A` and the rest of what you wrote is still lazy. And it would add overhead.
You're right, nothing beats fine control. But when you're just starting out like me, I won't be writing things(yet) that will need such complexity. I just need to see if my Fibonacci function works. 
This diagram really helped me understand the difference between covariance and contravariance: https://i.stack.imgur.com/KjDLw.png
I had similar issues with other pages, when they contain separate "scroll areas" (probably iframes) for source code, videos or revealing pictures. Sometimes the pages go into a state, where only the separate are scrolls in mobile Safari, but not the containing page.
So covariance is generally for producers -- types that return something, so method returns. Contravariance is for consumers -- usually a method argument type. The canonical example is: trait Function1[-T, +R] extends AnyRef { def apply(v1: T): R } where T is contravariant parameter, and R is covariant return.
You can't have a contravariant return -- it won't compile. You can do other things though: http://blog.kamkor.me/Covariance-And-Contravariance-In-Scala/
Cheers. One issue I ran into lately with Java Optionals, is an API I was working with returned an Optional&lt;? extends Text&gt; which prevented me from doing optional.orElse(myText) as I couldn't possibly know the type. Does Optional act as a producer, consumer both? Is it wrong for Optional to be defined as contra OR co-varient? Edit: I replied before I finished reading the link, it's answered a couple of questions but made me realize that it's a larger problem then I expected, and that I still don't fully grasp it yet.
http://scala-android.org/ is more or less a one stop resource for developing on adroid with scala, could use improvements however.
This is how I would implement it: object TreeFunExperiment { final class MacroHelper[C &lt;: blackbox.Context](val c: C) { val treeFun = { import c.universe._ q"""println("Hello, world!")""" } } object printHelloWorldTwice { def apply(): Unit = macro impl def impl(c: blackbox.Context)(): c.Expr[Unit] = { import c.universe._ import scala.language.existentials val tree = (new MacroHelper[c.type](c)).treeFun val result = q""" $tree; println("Hello again, world!"); """ c.Expr[Unit](result) } } } input: TreeFunExperiment.printHelloWorldTwice(); output: Hello, world! Hello again, world! 
Take a look at scala's Option implementation. https://github.com/scala/scala/blob/v2.11.6/src/library/scala/Option.scala#L1
&gt; evaluated lazily But then the argument would be re-evaluated every time it is used. If you want to map to an exception, you just write something like `.map(new Exception(_))` (not `throw`).
I don't get problems though. I use Eclipse and it all works. Maybe IntelliJ has some kind of super-completion feature but I can't even begin to use anything like that when it's putting spurious errors all over my code. 
Seems like a crap question.
Thanks for your helpful insight.
Yes, you'd need two functions, definitely. The question as stated doesn't make much sense. Constructing a TreeSet might be better because that would sort it for you.
It's best to prepare for anything to be asked in the interview. Especially if you heard about the job through a connection/recruiter and haven't seen an actual job ad. Even if you haven't seen an actual job ad, many companies don't want to give too much of a clue about what will be in the interview, for obvious reasons. Also, if you don't know Java, be prepared for companies who assume that all Scala developers know Java! (I do, but I know some who don't.)
I meant constructing a TreeSet by starting with an empty TreeSet and appending each element to it one by one, in a fold, then deconstructing the TreeSet in an unfold. That's the opposite of what the question stated by OP asks for, but like I said, it doesn't make much sense, so maybe he or she misremembered the question.
You can use Wart Remover for that.
The fact that it needs two functions is evident when pointed out, so I definitely mis remembered. But actually, the interviewer said that you can find a way *via* unfold fold and a way *via* fold unfold. No TreeSet, we want to sort a List, and it would make duplicates disappear. The goal here is to play with types.
Did my explanation and example `f` make sense to you? I imagine that unlike fold-unfold (where f adds the next sorted element to the accumulator), the unfold-fold method would involve g removing the next minimum element from the ~~accumulator~~ source. Does that make sense too?
How can you sort the list, when you have no guarantee that you can compare its elements? For example how would you sort a list of lists? Shortest list first? Compare the elements? And how a list of complex numbers? There is no well defined order for that. If you have an order, things are easy: replace B with some container that sorts its contents for you. Fold then just adds the provided element to that container and returns the result. Unfold extracts the "lowest" element, and then returns that element and the container minus that element.
The question was this. I guess there was an order on it, because yeah, it doesn't have much sense otherwise. And no, I can't use external help. `f` must be O(1). 
So I can just read any list from a file in O(n), sort it in n steps, each O(1) and write the result back in O(n)? So effectively building an O(n) sorter for arbitrary input? I'm also really curious about the implementation. But I'm not holding my breath...
&gt; no guarantee that you can compare its elements? This is determined by your `f`, so you can make it require an Ordering or comparator function as an argument. I did a simple implementation with Int in my first comment, I think that would be fine since it can be generalised. I think they would be interested in how you use fold, not about what is being compared.
It's really easy, when you can define the ordering: I just define the correct order as whatever my f returns. It might look like cheating, but the rules allow it. It's basically the same principle I use when I file my taxes...
Here's my thought process. First, let's get the obvious things out of the way: we clearly can't use the same `f` for both the fold and the unfold, so the actual question must be to find two functions. And since it is fold which expects a list as input, the actual question must be to write sort as `unfold(g)(fold(f)(list))`, that is, the unfolding of a fold, not the folding of an unfold. Now, to find the underlying sort algorithm. Using fold forces you to examine the elements one at a time, so probably something like insertion sort in which you have an intermediate representation which is already sorted and you're adding new elements to it one at a time while preserving the order. For insertion sort the intermediate structure is already a list, so the unfold would be redundant. If that was the solution the interviewer had in mind, they would probably have asked about sorting using a single fold, not using the unfolding of a fold. So we need to do something like insertion sort, except we're allowed to insert our elements into an intermediate data-structure for which preserving the order is more efficient than with a list. Aha! It's heap sort: insert the elements into a heap (a.k.a. a priority queue), for O(log n) each, then repeatedly remove the highest-priority element from the heap until there are no more left. For the result to be sorted from smallest to largest, we need a priority queue which considers small elements to have the highest priority. So: when f receives None, it returns an empty priority queue, and when it receives Some pair of an element and a heap, it adds the element to the heap and returns it. When the heap received by g is empty, it returns None, otherwise it removes the highest-priority element from the heap and returns Some pair of that element and the rest of the heap.
Sounds about right, although I'd note that I think you could do a fold(g)(unfold(f)(list)) if your intermediate data structure (essentially B) is a List[A] (although this would restrict the efficiency of your solution).
That's actually a valid concern... Hum. I don't know. The question is just interesting and I wanted thoughts of other persons on it. I don't know any other scala developers so reddit was my only option. 
Other people have provided good commentary on the solution to the problem. I'll point out that this Scala code is pretty bad, and should definitely not be used in production. `fold` and `unfold` are both non-tail recursive. This means that they will consume stack space as they iterate. That's only OK if you know that the list `l` and the data structure `u` have tightly bounded sizes. If you plan to use `fold` with arbitrarily long lists (or `unfold` with arbitrarily large data structures), you have a stack overflow error waiting to pounce. If you really want to express your sort algorithm with this `fold`/`unfold` pattern, you probably want to make `fold` return a `TailRec[B]` and `unfold` return a `TailRec[List[A]]`. This will enable trampoline-style iteration, which means you won't run out of stack space. It's slower, and it's a little clunky, but it would avoid the stack overflow. But really, what's the benefit of expressing a sort in this way? It's great to be exposed to this kind of higher-order functional programming, but it's also important to know when it's appropriate. This is especially true in Scala. Scala's not Haskell, and it's important to keep in mind that appropriate solutions to problems in one language aren't necessarily appropriate in the other. 
&gt; And since it is fold which expects a list as input, the actual question must be to write sort as unfold(g)(fold(f)(list)), that is, the unfolding of a fold, not the folding of an unfold. The accumulator type `B` could also be bound to `List[A]`, in which case `fold :: (Option[(A, List[A])] =&gt; List[A])(List[A]): List[A]` and `unfold :: (List[A] =&gt; Option[(A, List[A])])(List[A]): List[A]`. That at least makes the types line up so that you can do a fold of an unfold and get back a list, though whether there's a solution down that path, I'm not completely sure. 
What am I looking at here
This macro doesn't do exactly what you are asking for, but it may be a good starting point for you: import scala.language.experimental.macros, scala.reflect.macros.blackbox def desugarImpl[T](c: blackbox.Context)(expr: c.Expr[T]): c.Expr[Unit] = { import c.universe._ val exp = show(expr.tree) val typ = expr.actualType.toString takeWhile '('.!= println(s"$exp: $typ") reify { (): Unit } } def desugar[T](expr: T): Unit = macro desugarImpl[T]
&gt; That's actually a valid concern... Not all of us agree. I've been on both sides of the interviewing table hundreds of times; there's nothing wrong with what you asked here. I hope the job pays well, because this question seems pretty hard-core for the great majority of (even Scala) dev positions. Hopefully the interviewer wasn't asking it just to boost his or her ego.
It's a junior position, and the pay's not bad for it. &gt; this question seems pretty hard-core for the great majority of (even Scala) dev positions. I guess the interviewer just wanted to see my reaction toward the question and my thinking process.
&amp;nbsp; If you use macros, the best you can get is a processed version of the code that for the most part won't match what you see. Compile with `"-Xprint:typer"` will give you an idea of what the processed version of the code looks like. &amp;nbsp; I believe your best bet is to write one program (ProgramA) that compiles the other program (ProgramB) by using a call to an external program like Scalac. Before ProgramA calls scalac on Foo.scala, ProgramA can also read the text file of Foo.scala. &amp;nbsp; If you're just starting out this might be a little over your head, but if you read [this article](http://alvinalexander.com/java/edu/pj/pj010016) it will tell you how to make a call to an external program from the command line using a program you wrote. You can pass in an array of strings containing the name of each file (Foo.scala) and also the text string for that file. In addition, the code that is being compiled (ProgramB) can be made aware of what file it is in using this [this library](https://github.com/lihaoyi/sourcecode) with a call to `sourcecode.File()`. &amp;nbsp; Basically, ProgramB can make a call that says "get the name of my file and then find a file in the array with that same name and print its contents". _____________________ Also, if you're a beginner, try out [my macro library](https://github.com/JohnReedLOL/scala-trace-debug). Instead of typing `println("Hello World")`, you can type in `println("Hello World" + Pos())` and it will append a clickable stack trace to your print statements like [this](https://camo.githubusercontent.com/ddc08138f5e4b824ba85dcc987fe072d8dddf980/687474703a2f2f692e696d6775722e636f6d2f346876475130742e706e67).
I am not the author of this code. To my best understanding, `typelog` is a macro annotation that translates the prolog program into a sequence of (implicit) Scala statements so that the compiler's implicit search algorithms will solve the prolog program.
Seconded. This is a terrible, overcomplicated, non-performant piece of code. The answer I would give is that I'd refactor it so it didn't look like this. It's pretty amazing that people think this is a valid interview question. 
&gt; unfolding of a fold, not the folding of an unfold Actually you could do it either way (although I used the former for the solution I gave in the first comment, because it’s more efficient and more intuitive for most people).
That's kind of amazing. Is there somewhere with a deeper explanation of how this works?
Also this would be very helpful http://stackoverflow.com/questions/37267034/how-to-store-phantom-type-using-scala-slick
You mean the company doesn't search for any new candidates for some period of time? I didn't get that from the text. (If they do, we might end up in a situation where sharing a question alters responses of candidates.)
Check out macros talk from lambdaconf 2016 on Vimeo
That reminds me of the time I wanted to evaluate a candidate's ability to collaborate and to communicate technical ideas to others. I said something like this: &gt; I see on your CV that you have experience with financial systems. Let's pretend we're coworkers and we were asked to implement something like BitCoin. I have some vague idea of how to do it, you probably have a better idea than I do, let's try to figure out how we would implement this. I was expecting that we'd discuss our respective understanding of BitCoin, probably refining our understanding in the process, that we'd sketch a very high-level plan of how the system would work, and that we'd get into a sufficiently deep technical discussion that I'd get a very good idea of what it would feel like to be a coworker with the candidate. But instead the candidate insisted that we should not rush our fake project by taking technical decisions so early, that we should first make sure we know what the legal restrictions of implementing such a currency would be, and that we should read papers about how the existing BitCoin protocol works before trying to implement our own alternative. All very sound objections if we were really tasked with creating a BitCoin alternative in real life, and maybe that's what an interviewer with more practical concerns might have liked to hear, but in my case I was just using BitCoin as an excuse to have a technical discussion, so it was really frustrating that I couldn't convince the candidate that this wasn't a trick question. All this to say: you might be right that this isn't the most performant way to write a fold, but if the interviewer was trying to check whether you know how to use and reason about folds (and as a "bonus", whether you're familiar with sorting algorithms, another common but unrealistic interview subject), not whether you know how to implement recursive functions, saying so might not be the answer which gets you hired. Worse: you've now revealed that you're the kind of employee who will get easily distracted improving the code of your coworkers until it's perfect in your eyes instead of working on the task at hand, and I've had coworkers who have been fired after they did that for too long. Although in that I case I guess the more important cause was that he continued doing that after the rest of the team explicitly asked him to stop, rather than the fact that he started doing so in the first place.
SBT uses its own Scala version to run your SBT project. Scala 2.10 is where macros were introduced, which is a recent feature.
You have said you don't use Scalaz, are you doing a lot of implicit a and HKT/type lambdas? That's what seems to kill IntelliJ.
So I want to to some basic http requests. Like really basic. I don't need headers or cookies. Asynchronous is a plus, but doesn't have to be. What's the best library to use? I'll be making a request every 5-10 seconds, so I would want it to be as quick and lightweight as possible. I was thinking of calling `curl` from command line, but that gives me compatibility issues when on windows.
add ```scalaVersion := "2.11.8"``` to your build.sbt. sbt will download and use scala 2.11.8 to build and run your project. If an update is released you can just change the scalaVersion to the new version you want and sbt will download that.
Does Scala support dependent types? For example, if I wanted to make a type that represented length and another type that represented width and I wanted to make an assertion that it is only legal to append two grids if they have equal length or equal width.
&gt; Let's try to figure out how we would implement this. I get what you're saying, and I think the question of "how would we implement this" is far more relevant to the process of development. Starting from general concepts and refining that into code is what most developers do every day. &gt; Although in that I case I guess the more important cause was that he continued doing that after the rest of the team explicitly asked him to stop, Knowing how to response to feedback and move on is important, and it sounds like the candidate didn't respond well. If he'd raised the issue and then moved on once it was clear it was a precondition of the discussion, would you have been able to accept that as a valid concern? 
Yes, but keep in mind that many aspects of Scala types cannot be expressed in Java. So if you do this I recommend you define the "edge" of your Scala code in terms of Java interfaces.
&gt; If he'd raised the issue and then moved on once it was clear it was a precondition of the discussion, would you have been able to accept that as a valid concern? Of course! And by the way, while I didn't manage to move on to a technical discussion, I didn't blame him for that, I blamed my inexperience as an interviewer. The other interviewers did manage to get the information they wanted, and he was hired.
That's not quite what I meant. I was saying more that this particular candidate's interview was over; they were under no obligation to stay silent. I'm not sure what you're concerned about. The point of interviews is to gauge whether the candidate will be a productive member of the team. They will be productive by having been exposed to a variety of ideas. Apparently, this fold/unfold pattern is something that comes up in category theory. I had not seen this pattern before (I'm only aware of the more common fold pattern). But maybe, having seen this pattern, I'll see a place in my work where it would be appropriate to use. I'm enriched for this interview question having been shared in public. Saying that we shouldn't discuss interview questions is like saying that people in the same university class shouldn't discuss lecture topics outside of class. Yes, students shouldn't copy homework answers verbatim, but they should feel free to discuss the concepts openly.
Probably not. If it's a java project, then it needs to be maintained by java developers who may or may not know anything about scala.
There's another point to be made, distinct from "this is a bad implementation": asking someone to sort something in general makes for a poor interview question, because it's far too low level and specific. * http://deirdre.net/the-sort-implementation-interview-question/ More generally, brainteaser and puzzle interview questions are poor because they don't reflect the reality of the job and they rely far too much on specific knowledge. Wrote a blog post about it here: * https://tersesystems.com/2012/02/22/interviews-without-puzzles/ I recommend [Hiring The Best Knowledge Workers, Techies &amp; Nerds: The Secrets &amp; Science Of Hiring Technical People](http://smile.amazon.com/dp/0932633595) as a great book that has good examples of interview questions -- it helped me out lots.
I just finished watching [this video](https://vimeo.com/165837504) from flatMap 2016, where Daniel Spiewak concludes that Scala does support dependent types, but it's really cumbersome. I'm by no means an expert in these kinds of type shenanigans and probably can't help too much but Daniel does type-level calculations in his presentation. Perhaps seeing what he does will give you some ideas/direction.
[http4s](https://github.com/http4s/http4s) is my go to these days. It's not the highest performing thing out there, but once every 5 seconds is pretty slow as far as computers are concerned, and it can definitely handle that.
Technically yes it'd work fine. But if they want you to do it in Java it's because they imagine it would be maintained by people that know Java. You can ask but it wouldn't be cool if you snuck Scala into their project, especially as an intern.
Is this a serious suggestion? Start dumping intermediate $1.$a classes into a Java repo and assume everyone will be cool with that?
I use ScalaZ when it makes sense, at work it's banned so I implement similar functionality myself. It looks like anything using type members breaks IntelliJ (see the bug I linked to), which unfortunately is the only way around SI-1272 (I don't think the compiler plugin is an option for this client), so it goes wrong for any line using any typeclass-based operators. 
&gt;Is this a serious suggestion this...was not.
Oh right, I got you now. So anything dealing with the concrete class of ExtravagentDispatcher knows it's returning MagicalUnicornScriptText because it's in the method return type, and as MagicalUnicornScriptText fufils the signature of ? extends Text it's valid. Where as returning MagicalUnicornScriptText when the interface has Text means that classes with a reference to the concrete type of ExtravagentDispatcher only ever see that it returns Text? is that right?
You might want to take a look at [sourcecode](https://github.com/lihaoyi/sourcecode). It doesn't do exactly your example but does have cool features like the `Text[T]` macro: scala&gt; sourcecode.Text("foobar".length == 1) res5: sourcecode.Text[Boolean] = Text( false, &lt;- value """ "foobar".length == 1 &lt;- source """ ) ```
In Java use the "final" keyword and the Streams API -- enjoy some benefits of Scala without violating the team's norms.
As an intern you might indeed have a good chance to work on a small, fairly isolated project where you can experiment. Showing to a busy team what an inexperienced developer can achieve by switching from Java/Maven/Spring to Scala/SBT/Play (for instance) seems like a good goal for an internship. As for mentorship, this shouldn't be too much of an issue.
Before you ask your boss, ask a senior freindly programmer about the possibility. That way if the answer is no you don't leave a bad taste in your bosses mouth. If the answer is no, then accept it for what it is. Look for small projects to start on. Do not frame it as "I want to do scala", rather as "I heard good things about scala. I spent some time learning it and would like to try it out to see if it lives up to it's promise."
You can't import implicitly but you can do this: object ModuleB { implicit val myImplicitValue = ModuleA.value //... } Note: implicits can't be top level objects - they should be in a class/object/trait. Also, you need "ModuleA.value" to be static. 
Do you have problems with large projects (e.g. 1000's of classes)? My experience (15mo ago) with Atom was it couldn't handle large projects. I think the JS vm was going nuts. Perhaps they've fixed that recently.
[Scalaj-http](https://github.com/scalaj/scalaj-http) is another option. Here is a simple GET request: import scalaj.http._ val response: HttpResponse[String] = Http("http://foo.com/search").param("q","monkeys").asString You can wrap it in a `Future` to get asynchronous.
You could use an immutable list and use `List.drop`and reassign the result to your `var`: scala&gt; var foo = List(1,2,3,4,5) foo: List[Int] = List(1, 2, 3, 4, 5) scala&gt; foo = foo.drop(3) foo: List[Int] = List(4, 5) scala&gt; foo res4: List[Int] = List(4, 5) In general, you should prefer `immutable val` over `immutable var` over `mutable val` over `mutable var`. In your example, you are using `mutable var`. The example above uses `immutable var`.
list-packages doesn't show it. I googled scala-mode2 melpa and got this. https://melpa.org/#/scala-mode2 "Package not found." So something wonky is going on.
I can't answer your questions directly - but I would suggest pinging the [ensime-emacs](https://gitter.im/ensime/ensime-emacs) gitter channel. I know there are also changes going on with scala-mode - check out the instructions on [ensime docs](http://ensime.github.io/editors/emacs/scala-mode/)
If you are a student there is a lot of google summer of code opportunities in Scala. http://scala-lang.org/gsoc/2016.html. The application for 2016 is over. The student application period March 14 - 25, 2016. Keep this in mind for 2017. 
If Scala is a no no, Kotlin is a more productive than Java, Java like JVM language. It will also generate way less organizational friction. Some shops a Java or nothing, but that does seem to be a receding trend.
I pulled his repo from GitHub: https://github.com/JohnReedLOL/shapely It's crazy. 
Not knowing how ModuleA + B are defined, I'd add that you can add implicits to class definitions as well, and can be organized to be imported along with ModuleA. implicit defaultB = ModuleB { ??? } case class ModuleA(example:Example)(implicit b : ModuleB) { ??? } 
I actually meant the following: discussing real answers taken on an interview might lead to a situation where a (new) candidate both knows the question and is asked the same question on _his_ interview. It might be easier for the interviewer to ask the same question to all candidates.. Note that I don't really see any value in this exact question at all (my personal estimation of the question, sorry). So my concern is only about the general situation of sharing interview questions.
Ive been working with play for about a year and a half with my current company with several apps in production and can safely say play is doing pretty damn well as a framework for us and more or less lives up to its promises. However unlike rails/node and other more popular language/framework pairs your going to have to get really good about reading docs/apis and getting under the hood and understanding the inter workings of play and the underlying akka systems. The majority of the issues we have faced in production or when adapting technologies that are a bit off plays path like JWT's for auth we had to really figure it out internally since there just wasnt the noise level of other people running into these issues as with more popular technologies. 
That's good to know. I have been finally getting some progress on the framework, I like it, but I am also new to scala and the language flexibility makes it a little hard to follow sometimes. Scala kind of reminds me of Javascript, as in there multiple ways to do both things and it has functional and FP styles. But I am focusing on the FP side of scala for now
Don't thumb down, watch the video
Scala's type system isn't crazy. You're looking at a functional programming library based off lots of mathematical concepts and type theory. It's presented at flatMap() -- a functional programming oriented conference -- as an exercise of what Scala can do, not as a production level solution. Look at Atomic Scala or Scala for the Impatient or Programming Scala -- all of those books will give you a reasoned introduction to the language and where and when using those features makes sense. 
That's right you don't have to program like that. Unless you want to encode certain properties in your types. But Scala was not designed to do this to that extent. It's just that it has enough that some people tried to do it with some degree of success using a language that is used in the industry, as opposed to a purely academic language. If you want to encode in your types the dimensions of your 2D grid that will surely be painful because the language was not designed for that. 
&gt; Um...the length of a path is proportional to the length of the path. Look at the code in https://github.com/JohnReedLOL/shapely/blob/master/src/test/scala/shapely/HListTest.scala val xs: shapely.:::[Int, shapely.:::[Boolean, shapely.:::[String, HNil0.type]]] = 1 :: false :: "hi" :: HNil &amp;nbsp; ^ If I append 1 to false to "hi" to HNil, I get a list of type `Int :: Boolean :: String :: HNil0.type`. The type signature increases in length for reach element that you add. &amp;nbsp; In addition, in order to avoid creating Size1, Size2, Size3, Size4, etc. They created a recursive type `Successor[Successor[Successor[Zero]]]` to represent type 0 + 1 + 1 + 1, or three. In order to change an Int to a size type, they use [a macro](https://github.com/JohnReedLOL/shapely/blob/master/src/main/scala/shapely/Nat.scala) to repeatedly decrement the Int and nest `q"shapely.Successor($accumulator)")` around it, where $accumulator is either the base case (Zero.type) or Zero wrapped in one or more successors. &amp;nbsp; You missed the actual implementation of `def +(other: Grid2D[Width,Height,T]) =`. It's not trivial. If you call `Grid2D[Size200,Size300,T] + Grid2D[Size200,Size300,T]`, if the append is top to bottom you want to get something of type `Grid2D[Size200,Size600,T]`. If the append is left to right, you want to get back something of type `Grid2D[Size400,Size300,T]`. And you want it to work for any size without defining each Size type from 1 to 2^32. &amp;nbsp; Also, when you construct the 2D array you don't want to construct it with all zeros, partly because it's inefficient to re-assign every element. You want to initialize with non-zero values, which means passing in rows such that the compiler can verify at compile time that all the rows have equal length. It's possible to make the length of a row part of its type signature. &amp;nbsp; I'm not saying it's impossible. It's just really hard.
&gt; But Scala was not designed to do this to that extent. But people do it anyway! Surely you must agree that the code in [this repo](https://github.com/JohnReedLOL/shapely) is crazy? Try to explain how the removal of low priority implicits works, what exactly Aux does, and how Nther and corecurse work to expand a call to `xs.nth(1)` into: /* The code below is deconstructed from "xs.nth(1)" "shapely.this.`package`.HListSyntax[ shapely.package.:::[ Int,shapely.package.:::[Boolean,shapely.HNil.type] ] ] (xs).nth( shapely.Succ.apply[shapely.Zero0.type] (shapely.`package`.Zero) ) ( shapely.this.Nther.corecurse[ Int, shapely.HCons[Boolean,shapely.HNil.type], shapely.Zero0.type ](shapely.this.Nther.base[ Boolean, shapely.HNil.type ]) ) */
Exactly, my point is that the super completion feature that IntelliJ has is very hard to get right for a type system like Scala's, and so there are edge cases where it stops working. It used to be a lot worse a couple of years ago but IntelliJ has slowly been filling these edge cases in. I think the biggest issue they have on their end is trying to store the index's in a performant and low memory usage fashion
Well then whose fault is it for writing code like [this](https://github.com/JohnReedLOL/shapely/blob/master/src/main/scala/shapely/ops.scala). Is it Daniel's fault for writing it or the language's fault for allowing it? I'm all for dependent types, but the way that it's being done now is basically a hack.
&gt; Huh? Apologies, I meant isn't. Edited &gt; You probably don't feel this way, but from my point of view it is both crazy interesting and also a wild abuse of existing language features. I feel the exact same way, hence why I don't know what point is being made, if any?
I don't know. When I saw it it blew my mind. Basically everything from the methods `xs.nth(2)` to the elements have types whose length increases linearly with the length of the underlying data structure or a given Integer. Can you explain Nther?
If you know anyone working in the Scala compiler, let them know they dependent types can be implemented another way and these corner cases should be removed.
I usually search for "play scala" or something like that. Also of you use intellij it is quite helpful to click into the framework methods.
Well Play doesn't have much to do with Actors (user facing anyway). If you want to expose your Akka application over HTTP use the aptly named Akka-HTTP module. Play is good if you're coming from the Java or Rails/Django world, where you have one monolithic framework that knows how to do everything. That is what Play fundamentally is. It wants to do your Auth, JSON parsing, templating, database IO, and so on. Having said that, it still allows you to get fairly deep into the Scala/FP style of app development, so it's a nice platform to start from the familar Java/OOP world of web frameworks and move into the FP style. As others have said, the documentation is really good for Play. If there's something missing I have found the stackoverflow playframework tag to be great help (or just ask questions if there's none existing). I think at the moment a promising alternative to Play is the aforementioned Akka HTTP. With Akka Streams it provides a great, high-level and most importantly modular approach to developing web apps. Very well suited for microservices and so on. However I have found the docs lacking for the HTTP module. Which is unsurprising, since it is very new. All the best
On mobile but I guess I would return Grid2D[Size,Height]. I see you’ve modified your OP. If you want an algebra, that is not something I’m going to write for a Reddit reply.
So you’ve got a beef with heterogenous lists? That’s fine, though you may be interested in work underway on Scala Dotty, which is specifically about the future of the type system and syntax based on modelling path-dependent types.
Of course HList's type signature increases with each new element, that's the whole point. You're complaining that a list of types has a size that increases by one for each element you add to it. I don't think this confuses you at the value level, why should it at the type level? I'm not familiar enough with your other issues to address them in a useful way, and you might even be right. But given the way you've phrased your post, I'm not convinced at all you're looking for useful replies anyway.
But I'm not even talking about completion, I don't even try most of the time because there are too many spurious errors and incorrect inferred types for any such feature to be useful. I don't care what advanced features IntelliJ may have when it's so far behind Eclipse on the basics.
totaly agreeing with you here. I tried akka http for microservices and saw me rewriting a lot of features I get in Play for free
If you want a language to encode useful and non trivial properties in your type system/compiler that is practical to use, then you may need to invent it. Scala is not that language. If you find one, let me know. I'm interested. I've looked at Idris and I think it's still too complex and cumbersome. 
&gt; If you want a language to encode useful and non trivial properties in your type system/compiler that is practical to use, then you may need to invent it. Scala is actually really close. All it needs is types that correspond to natural number and type ops. type Three = 3 type Two = 2 type Five = Three + Two val list: SizedList[Double, Five] = SizedList(1.0, 2.0, 3.0, 4.0, 5.0) val list2: SizedList[Double, 6] = list :+ SizedList[Double, 1](6.0) Currently shapeless is using natural numbers from 0 to 22. If Dotty can just define transformations between constant integers and types, everything necessary will be there.
Since they made dependency injection as default, Play became a lot harder to work with and to make readable code. Almost like they want it to become the new Spring. Which is expected since the creator of Spring went to work in Typesafe/Lighbend ([Link](https://jaxenter.com/spring-framework-creator-rod-johnson-to-join-typesafe-105003.html)). As far as how its doing I would say its gaining more traction with Java developers but probably losing traction with some Scala developers.
It's possible to make it useful. I believe I can write a library for typelevel numerics in Scala. For example: val five = getValue[V5[Nil]] val fiftyfive = getValue[V5[V5[Nil]]] val fiveType: V5[Nil] = getType(5) val fiftyfiveType: V5[V5[Nil]] = getType(5) val fivePlusFiveType: A[V5[Nil], V5[Nil]] = typeAdd(five, five) val ten = fivePlusFiveType.toInt val isEqual = isEqual(fiveType, fiftyfiveType) // false If I implemented a useful type level numerics library that represented type in decimal ex. 123 = V1[V2[V3[Nil]]], would you consider it?
yeah, there is a dependency injection problem that I am facing already, reminds me when I first started learning Rails with unfamiliar middle ware errors.
&gt; Which is expected since the creator of Spring went to work in Typesafe/Lighbend (Link). Rod Johnson was never involved in any Play discussion, and isn't with Lightbend at all now. Dependency Injection is because global state is a problem in multiple contexts, and using singleton objects for plugins and controllers and routing causes all kinds of subtle issues in Play. From a functional programming perspective, you don't want global mutable state, so moving away from that is actually a positive for Scala developers. 
I guess I jumped the gun on that one. Personally I prefer to deal with the global mutable state problem than to deal with dependency injection.
It would be a remote possibility. I would have to see the benefits beyond doing simple type arithmetic. 
I'm not fully familiar with Slick but once you have your List[(User, Address)] it's as simple as doing `usersAddresses.groupBy(_._1)`, provided your `User` model has an equals method (if it is a case class this is provided out of the box) edit: groupBy yields a `Map[User, List[(User, Address)]`. You most likely want to follow up with `.map { case (user, uas) =&gt; user -&gt; uas.map(_._2) }` to get a `Map[User, List[Address]]` edit2: you can also use `.mapValues(_.map(_._2))` on the group to create a "view-like" map where the map, once asked for its value, will perform the inner map operation on the `List[(User, Address)]` value-part of the group. The net effect is that the map collection becomes lazy when evaluating its members, so if you're going to work through the collection several times this may have performance implications.
My personal preferences are Spray for REST APIs (async, very much using the power/flexibility of Scala - possibly deprecated in favour of akka-http, but I still use it at least for now) and Wicket for HTML UIs (single-thread-per-request, but a very well designed library).
Hey, thanks for the link love! Edit: my smart-ass answer to the manhole cover question was always "because manholes are round."
Having `cons` be contravariant like that allows you to e.g have `xs: List[Float]` and write `(xs cons 4): List[Number]`. In fact it's the same reason as /u/ryan_the_leach 's `Optional` case below - really Java's `Optional#getOrElse` should be defined as `public &lt;U super T&gt; U getOrElse(U other) { ... }` and this would avoid the problem he's hitting.
I use eclipse/maven/scala for Android (and indeed for everything). Works pretty well for me, though I don't know to what extent the android pieces for eclipse are still supported.
I've added a user flair for [EPFL](https://www.epfl.ch/) now. Please let me know if you have a Scala library / project / organization you would like to have user flair for. Also, thanks to suggestions from /u/zzyzzyxx, weekly discussion threads are by default sorted by 'new'.
Hey - ENSIME EMACS user here. Install sbt. Add the sbt install location to emacs's exec-path. Go to http://ensime.github.io/build_tools/sbt/#install Follow the guide. Then open emacs and open your initial file (~/.emacs, probably). Make sure your package archives are set up to have melpa, like this ``` ;; the package manager (require 'package) (setq use-package-always-ensure t package-archives '(("gnu" . "http://elpa.gnu.org/packages/") ("org" . "http://orgmode.org/elpa/") ("melpa" . "http://melpa.org/packages/"))) ``` Now install as in this guide: http://ensime.github.io/editors/emacs/install/ To use on a project, open a project in the terminal. Type `sbt ensimeConfig` and hit enter. After it is done, open a scala file in the project you ran that in. M-x ensime RET and you should be good to go.
This. A variation I have seen and liked is: implicit val _ = ModuleA.value
These specialization courses usually have a new session starting each month, so they run continuously, as far as I can tell.
The course contents look very similar, yes.
In the previous announcement they said it's the same or at least very similar.
Why is ```|+|``` any more standard than ```&lt;*&gt;```? I think ```&lt;*&gt;``` for ```ap``` and ```&gt;&gt;=``` for ```bind``` are fair to be standard if you're working with the Functor-Applicative-Monad abstractions.
As does the second course.
Oh thanks! I have a question: what does the **&lt;&gt;** operator do: def * = (name, color) &lt;&gt; (Cat.tupled, Cat.unapply _)
That's a bi-directional mapping operator -- it maps between table columns (on the left) with case classes (on the right): * http://slick.lightbend.com/doc/3.1.1/schemas.html#mapped-tables
I hated the simulation part of the past course (the second one, principles of reactive programming). It required juggling tens of mutable states which looked neither functional nor reactive. The first one was good but only focused on the functional paradigm while missing some important computational aspects- many of my coworkers, who learned Scala from the first course, abused List() in places where they should've used Seq() and wrote convoluted @tailrec with inner functions or foldLeft with cryptic lambdas where they could've just introduced one mutable variable. I skimmed through the syllabus and it looks like they have added quite a lot of contents, I wish they guide people this time to a balanced way between purely functional and practical points of view. 
thanks for the explanation, I am still a bit confused, if function subtype relationship is defined as Function1[-A, +B] Should 't I define my `cons` definition as `def cons[B &gt;: A][C &lt;: A](v: B): List[C]` instead of `def cons[B &gt;: A](v: B): List[B]` ? this matters because Say that I have val tiger_list: List[Tiger] = List(tiger) this would imply `tiger_list.con(animal)`output List(Animal)? But that is counter intuitive since appending `tiger_list` should result in another `tiger_list` 
Are any of them... Optional?
I agree, the Reactive class was not very good. But that's been dismantled, by popular demand, so hopefully the replacements are better. I don't share the same complaints about the Functional class. Sure, there are imperative ways to solve some of the problems, that's not the point. To be honest, most of the imperative approaches to problem solving are still right there in Scala, but they're hardly worth spending time on when most people who take the class will be quite familiar with imperative/mutable/traditional OO programming. I'm glad they spent the time in the places where Scala provides a more unique approach, instead. Perhaps the Scala center could also consider offering a course on Practical System Architecture in Scala to address some of your concerns. Because I do agree there's a bit of a gap in the ecosystem for such material.
Seconding scala tags for HTML generation especially for non play projects! Twirl was just awful for a non play projeft
If I had a dime foreach pun in this thread...
The FAQs say: &gt; Each course in the Specialization is offered on demand, and may be taken at any time. But I'm still confused. I want to do all of the courses as fast as possible, but in sequential order. So far I've only purchased the first course and I'm wondering if I will be able to start the second course at any time or if there could be a delay after I finish the first one. 
You can't create a new class except with a macro - what would the new class be at runtime? You can map a `HList` or `Record` type using a type-level function (so you could derive `type UpdateUser = 'id -&gt; Option[Long] :: 'name -&gt; Option[String] :: ... :: HNil`) easily enough, but there's no way to create a case class out of nowhere that that record then corresponds to.
I had been using Process.lineStream as part of a unit test.
Monoid.
In reality you don't have to do much at all. What i do is the following: create a basic `build.sbt` file $ mkdir myproject $ cd myproject $ echo 'name := "myproject"' &gt; build.sbt Open IntelliJ, File-&gt;Open myproject, and check "Create directories for empty content roots automatically" boom!
have you seen https://github.com/bitcoin-s/bitcoin-s-core and https://github.com/ScorexProject/Scorex ?
The resulting code is still hard to maintain and to reason about, just as it is with the cake pattern. My take on it would be an actor which is maintaining the "global mutable" [state](http://typelevel.org/cats/tut/state.html). Actors are excellent at keeping mutable data and having it being mutated by multiple parties concurrently. You would still need to have an ActorRef (or ActorPath) always available to this actor, however this still seems like an easier to maintain approach comparing to dependency injection.
Yep, and it works well and seamlessly on android with `sbt-android` 
Thanks, that seems good enough! If you could post it to stackoverflow as answer I would happily accept it (and make sure future people find it?)
Hey everybody, Scalanator was created by me to try and build a better online learning experience (not just more online videos) - we aim to be highly interactive. This is our first trial course - its a Slick introductory course based on the Essential Slick workshop and book by Dave Gurnell and (https://underscore.io). I would love to know if the medium works/is better and how we could make it even better.
the idea is to use an appropriate abstraction for each situation like using Map(...) instead of HashMap(...)
Okay thanks, I decided to do the first two courses in parallel ( I'm halfway through the first one anyway).
I wasn't clear. I meant adding this "implicit arguments by name" as an additional feature. Not replacing the existing implicit arguments. I love type classes.
I’m not sure how you mean “implicit arguments replace closures”? What about simple examples like `Seq(1, 2, 3).map(_ + Math.PI)`? Were you thinking: def addPi(x: Double)(implicit PI: Double) = x + PI Seq(1, 2, 3).map(addPi) This wouldn’t be compatible, so for your implicit we’d have to write: def addPi(x: Double)(implicit PI: Double) = x + PI { implicit val PI = Math.PI; Seq(1, 2, 3).map(x =&gt; addPi(x)) } I don’t think our code is improved, and anyway we can normally just write this: object PureModule { def sumWith(c: Double)(x: Double) = c + x } Seq(1, 2, 3).map(PureModule.sumWith(Math.PI)) 
That's a key point: when is it appropriate to use an abstraction vs. not? In the case of `Seq(1, 2, 3)` it doesn't makes much sense to me.
You can do something like this: val join = users.join(address).on((u, a) =&gt; u.id === a.user_id) for { rows &lt;- join.result } yield rows.groupBy(_._1).mapValues(_.map(_._2)).toList
It's actually a case class using infix notation: scala&gt; $colon$colon("b",$colon$colon("a",Nil)) res1: scala.collection.immutable.::[String] = List(b, a) Hence pattern matching working.
http://symbolhound.com/?q=%3A%3A+scala
[Symbolhound](http://symbolhound.com/) is your friend.
Thanks for the reply, I just spent a half an hour typing up a follow up question by illustrating an example. But half way through I realized I was wrong, and then I identified the flaw in my logic. 
right, take a look at basic list implementations then it will maybe make more sense to you. A List is normaly implemented with a head item, and a tail which is again a List 
Yes, but keep in mind than `Nil` and `null` are [not the same thing](http://tpolecat.github.io/presentations/nnnnn.pdf) (PDF link to presentation on N-words). To be clear: List(8) match { case x :: xs =&gt; // over here x = 8 and xs = Nil } List(1, 2, 3) match { case x :: xs =&gt; // over here x = 1 and xs = List(2, 3) }
It's special syntax for prepending to lists or pattern matching on a list. It's better to use +:, which is the same but more general.
Duckduckgo could also be helpful in this search 
Is this a library which similar to Slick or Quill?
&gt; purely mathematical function I thought that’s exactly what motivated the OP too, that a closure (function) should only depend on its functional inputs (arguments/parameters). The OP’s suggestion entailed two things: preventing access to other variables, and using a name-based ‘implicit’ mechanism?! &gt; If the outer variable is mutable, for example, then this does have an impact Yes, I think the OP was proposing to treat these as function arguments (thus they would be captured as constants, like with normal function calls). So instead of `f(x) = x + danger` it would be `f(x)(safe) = x + safe`. Of course, we can do this already as `f(safe)(x) = x + safe`, but the compiler won’t prevent us from doing `f(safe)(x) = x + safe + danger`.
Java 8 has similar restrictions on lambdas as C++, they can only capture final variables. I find that extremely annoying and have occasionally wrapped things into size 1 arrays just to mutate them. &gt;If our function accesses an external variable, but it's immutable, is our function still pure? It would be pure, but you cannot prove immutability in general. Is this immutable: `val foo = Vector[X](a,b,c)`? It has to be immutable *all the way down*: `a,b,c` and everything they contain must be immutable. Even if we know class `X` is immutable: any of `a,b,c` could be a sub-class which has some mutability attached to it. Anything containing a Java array could be mutable in practice. But honestly, as far as functional purity goes, I cannot see why people get so worked up about it. Just assume purity wherever it matters and if somebody wants to shoot themselves in the foot: that is none of our concern.
If you mean mine, absolutely not. If you mean 47deg's, yes, probably. I haven't used it at all. You probably know as much as I do.
hi, Fetch author here. As far as i know, Slick and Quill are libraries that you can use for performing SQL queries, insertions and updates in a database-independent fashion. Fetch is a bit higher-level than that but it should play well with those libraries. If you use Fetch's DataSource abstraction to implement the read queries to your database and execute those reads with Fetch you can benefit from the optimizations it makes. You can also easily mix reads to data from a relational DB and an HTTP API in the same DSL, and the library will arrange the fetches efficiently. I'll add various examples about how to integrate Fetch with database access libraries like Doobie or Slick [soon](https://github.com/47deg/fetch/issues/48), let me know if you'd like to see examples of other integrations in the issue!
This may muddy the waters a bit but also explains the different **symbols** of `::` `::` is a method on type List; as a method ending with the character `:` it is right-binding, which means that `foo :: bar` is the exact same thing as `bar.::(foo)`. For the types to line up, `bar` must be of type `List[X]` and `foo` of type `X` There is also `::` the Type. In this form, `::[X]` — also known as Cons — is a case class that takes two parameters: a `head: X` and a `tail: List[X]`. As `::` the type is a case class it has an automatically generated extractor useful for pattern matching. You can use it in its "standard" form like `cons match { case ::(head, tail) =&gt; … }` or its infix form `cons match { case head :: tail =&gt; … }`. The two styles are equivalent but the second allows for non-nested chaining like `case head :: tailHead :: restOfTail` as opposed to `case ::(head, ::(tailHead, restOfTail))`. Infix matching like this is available for all binary types, so a binary tuple can actually be matched like `case first Tuple2 second =&gt;`
The Scala authors want you to buy their book, and hence chose to use ungoogleable names ;-) By the way: [Programming in Scala](http://www.artima.com/shop/programming_in_scala_3ed) is really good.
well, if foo is not of type X then the type of the list will widen to AnyVal/AnyRef. For example, if I have ```val l: List[Int]``` and i do ```5.5f :: l``` the resulting list will be of type ```List[AnyVal]```
Hmm, are `f(x)(y) = x + y` and `f = (x =&gt; (y =&gt; x + y))` considered to be completely different beasts in this conversation?
Scala has several cases of syntactic sugar, where the actual order of things is altered: 1 case class And(b: Int, c: Int) val And(b, c) = ... //destructuring using unapply(), like in pattern matching is similar to case class And(b: Int, c: Int) val b And c = ... 2 `val v: And[B, C] = ...` to `val v: B And C = ...` 3 `a +: b` to `b.+:(a)` 4 `def f[A: Seq]() = ...` to `def f[A]()(implicit evidence$1: Seq[A]) = ...`
You're absolutely correct. For posterity the actual signature of `List#::` is something like `def ::[Y &gt;: X](item: Y): List[Y]`
Assuming the OP wants to “replace closures” syntactically. Personally I’m not on board due to the lack of details.
Cool, didn't know about that one. This is where I usually go to: http://docs.scala-lang.org/tutorials/FAQ/finding-symbols.html
Oh i get it now **::** is an alias class rather than an operator. I am new to both Java and Scala, so the idea of creating things like def + (two: Poly) seems crazy to me, but cool. But yeah I totally agree with the symbols being confusing since it can lead to insane levels of abstraction. Thanks for the reply, I wouldn't have known to consider this as a possibility. 
A couple of additional tips: 1. In ScalaDocs, e.g. the [standard library docs](http://www.scala-lang.org/api/2.11.8/#package), note the "#" to the left of "A B C D..." in the index. That's where you'll find the symbols in the ScalaDocs. 2. [SymbolHound](http://symbolhound.com) is a nice search engine that respects symbols. :-)
How do I add [Quill](http://getquill.io) to the subreddit description? Also, would it be possible to add its logo as a flair?
Cool, thanks! Going to try this out in my test cases. It drives me crazy how difficult it is to figure out what is different in a large case class that fails an equality check. 
I have a lot of issues trying to actually build anything with IntelliJ, which sometimes randomly loses the ability to find dependencies that are *still in the same ansidjfnsk place they've always been*, but I'm not tempted to switch to Eclipse. It works great otherwise.
I am currently using sublime text and activator UI. Right now I am too early in learning scala to deal with configuring development environments. 
Totally understand. When you get further along, my recommendation stands.
I have added Quill userflair, and added it to your user (you can still edit the flair text, or remove it if you like), and have added a link in the sidebar :)
I think the documentation and API's in Scala are great. However, if you're coming at all of it from the Java API, the world is much more painful, mainly due to lack of proper FP support in Java. We started in Java Play, then ported to Scala Play, and were able to cut the tooling more than in half.
Video of a talk by /u/dialelo introducing Fetch: https://www.youtube.com/watch?v=45fcKYFb0EU
thank you!
mvnrepository.com is pretty useful.
as jnd-au mentioned, you can get that specific behaviour, but I think the most simple and most conventional solution is to just manually get latest version from either lib's github readme page or from their releases page. why? 1) people you're collaborating with will have no trouble to understand and your dependencies, as it's convention to specify them in build.sbt 2) going to github readme page seems like less jumping through hoops than to have automatic version, then looking it up to insert manually, remembering to disable updating after initial lookup. 3) if you really need the latest latest version, github readme/maven is better to see if it's full release, some RC / m / alpha / beta. 4) that solution also works outside of Scala. new JS / Java project and another build tool? instead of figuring out automatic-latest version, just get version from github readme, specify it and you're done It might seem as additional work at first, but you soon get used to it, and once you think about it it's pretty convenient - if you're deciding on lib, you must've looked at it's github / docs and the version is always there specified in the first place of "getting started" section
I use this: https://github.com/rtimush/sbt-updates It lets you check for the latest version of all your dependencies with SBT. This keeps you safe for production and lets you make the decision to update the dependencies you want. I've been burned before by bugs introduced in newer versions of libraries. So always pin your dependencies. 
It sounds like what you want is some form of natural language processing (NLP). This is very difficult. However there a several libraries that exist that have done the hard work for you. From what I've seen (difficult to link any since I'm on mobile, but a Google search should suffice) the libraries suffer from lacking documentation. Depending on how important this part of the project is I'd write a simple abstraction. Create a parser that maps a string to a "sentence" or "command" that your code uses that just does simple things like the examples you listed. Then you can swap it out later for a library. If you start using a library and run into problems due to documentation or just not understand then usually you can post in the relevant mailing list contact the listed people or package maintainer.
Thanks for the links, I will try to learn from their codes.
NLP is extremely hard. It is a big area of research for computer science, computational linguistics, and cognitive science for a reason. The easiest way to do this (And honestly java is going to be easier for this I think, or python would be easy as well) would be to have key words to look for in structured statements. So I would break up commands into something like: [command] [object] The easiest way to do this would be to parse the string, going between spaces. So for example the user inputs "lift chair". First you read the first word, up to the space. You see the word "lift" then you have some kind of command block. You then perform the action of lift. Lift then has some code that looks at the next word. Then you have some kind of code block that says "if chair do this, if table do this, if person do this." There are better ways to do this, that require a bit more knowledge of programming, but this would be the *easiest*.
How experienced are you with akka? I'd say go pure akka if you know it, otherwise just go play. Play is insanly easy and does everything for you.
I have no experience with Akka. My first thought was Play too, but it doesn't support NoSQL databases. 
http4s.
There's no such thing as a web framework supporting or not supporting a database. They're two different unrelated things entirely. I would say go with akka for the ultimate flexibility. But it doesn't really matter. Your Cassandra library is completely independent of your web framework. For a pure functional library, http4s is a good choice. Otherwise, akka http. I have a production system I made with akka http as my first ever project in Scala and I didn't have problems with it. 
What makes http4s good? I've heard of it before, but don't know much about it 
I think Play is much easier to work with but akka-http is good too.
Akka HTTP + [Quill](https://github.com/getquill/quill/blob/master/CASSANDRA.md).
Thanks, I'll have to give it a try
If you want something out of the box, then Play with the Datastax back-end driver will work fine. If you are looking for an "industrialized, never go down" service, then I would look at [Lagom](http://www.lagomframework.com/documentation/1.0.x/Home.html) -- basically it's Play with event sourcing, a REST API, and an [akka persistence layer that talks to a Cassandra backend](https://github.com/akka/akka-persistence-cassandra). 
Play 2.5 uses Netty 4.0 -- there is an Akka HTTP integration but it's experimental for now. Use the Datastax driver for Cassandra.
Amazing! Now you can develop your server-side in Scala.js too! Not advertised by the library, but another cool usage is using the Node.js API to build command-line applications with no startup penalty.
Why would someone want to develop server side in Scala.js rather than just Scala? 
Great article, important topic. I work with large raster data on a project called GeoTrellis and we use many of these and similar techniques to optimize our operations. I would suggest looking at the spire.syntax.cfor macro, it has a better syntax than while (IMO) and macro expands to something with basically the same performance. You might want to also check out Scaliper (https://github.com/azavea/scaliper), a partial port of Google's Caliper microbenchmarking library that allows you to write scalatest runnable microbenchmarks. Some examples are here https://github.com/geotrellis/geotrellis-benchmark
Informally, I can tell you, FWIW, that this issue was raised at least by several of us at Verizon Labs, to be presented to the board by our representative, Tim Perrett. From its inclusion in this list, I can only assume that we (Verizon Labs) were not the only board member who wanted to see this identified as a priority.
I meant Cassandra doesn't have specific plugins for Play like for example MongoDB does.
&gt; Does Play require the db to be specified in the conf at all costs? Play does not require a database. Make sure you take out the "db" module out of the libraryDependencies section in build.sbt and there will be no database related functionality. &gt; use the spark-cassandra-connector library directly for my functionalities where necessary REST interfaces have very different memory and CPU requirements than data analytic engines. You should run your REST interface in a different JVM from Spark, and then use a service / driver to connect to Spark. In this case, you can use the Datastax Cassandra Java driver with Play, and write to Cassandra. You may want to set up a module which isolates Cassandra -- for example: https://github.com/playframework/play-isolated-slick shows Slick behind a domain specific API, so you could do the same thing with Cassandra. You can see some example blog posts on REST APIs in Play: * http://spr.com/building-a-simple-rest-api-with-scala-play-part-1/ * http://nordicapis.com/building-a-rest-api-in-java-scala-using-play-framework-2-part-1/
How is akka http maturity-wise? I know within the last year, they were saying it is behind Spray in terms of performance - has that improved?
What kind of start up times are you thinking of? I think we're at around 200ms to start up for our Finagle servers, for example. I was thinking of greenfield development; are you thinking more of an established project written in Node that wants to use Scala.js for getting type safety without having to start from scratch? 
I don't know which ecosystem is larger right now, but there is no doubt JavaScript is still improving and growing and, alongside it, Node and its ecosystem. Who knows which will be bigger/better in 5-10 years, but I would bet that Node will be large enough to remain attractive for a very long time. And I would bet that in any large ecosystem, you will find great libraries which will attract developers to it. But also, some devs/organizations have chosen Node as their main platform: in such cases it makes sense to be able to run server-side Scala on that platform instead of mixing two platforms next to each other. Whether Scala.js-on-Node will become mainstream remains to be seen, but what I really like about it is that: - It is competition for the JVM, and competition is generally good. - It feels fresh, unlike the good old Oracle-owned JVM. - It gives Scala more opportunities: Scala is no longer just a server-side JVM language: it's JVM, Android, JavaScript-in-the-browser, JavaScript-on-Node, and native (which in turn could open up some other platforms). 
Absolutely right. Our feedback to the board is only intended to indicate where we think it should be in the list of priorities, not that we would expect such a roadmap immediately, even if it were in our power to demand such a thing.
If you need very fast and simple http then you can try [colossus](https://github.com/tumblr/colossus) . It don't have lot of functionalities and is still in very changing - but I tested it in one service and I like its simplicity and performance. I have lot of experience in play and spray.io and they are very good frameworks :). 
I would choose Python because it supports lambda functions which is your first big foray into writing functional Scala. As of java8 I believe Java has predicate functions which I think are the same thing but double check me because I haven't written Java since college. You can use all the libraries Java has in Scala although the syntax is slightly different. Ex: val map: HashMap[String, Int] = Map.empty Is how I would create an empty map in Scala. In the next two days embrace immutability. It will enforce proper use of the collections API. Speaking of collections API, verse yourself with map, filter, reduce (all Python built ins). IMO proper use of those three functions willl put you on the right course to write functional Scala. and infinite streams. Learn those. 
Honestly these are just syntax issues. I'd recommend python because it's a bit more forgiving, but just run through the tutorials again. By the way: `zipWithIndex` is `enumerate`. `file.open(filename)` is `with open(filename) as file:`. `myList.flatmap(f)` is `[f(x) for x in my_list]`.
I would look for another job.
I completely agree with the competition; there are countless instances where one language or framework has innovated, spurring others to either catch up, find a niche, or fade away. I also completely agree on needing to see what the next five to ten years will bring. Will Oracle mismanage Java to the point where it tanks, taking Scala along with it? Will browsers begin consuming a common bytecode so that no one transpiles to JavaScript any more and just writes in their preferred language? I will also concede that if you have an existing inclination to Node, there probably would be benefit in pulling in the static guarantees that Scala.js would give you. This does seem like a good use case. So what are the remaining points of disagreement? The major one is &gt; some devs/organizations have chosen Node [...] to run server-side Scala on that platform You're not running Scala on Node. You're running JavaScript transpiled from code which adheres to a subset of Scala. Some of this might represent the current state of Scala.js but some parts explicitly called out as part of the language -- like interop with Java or all the `scala.*` packages, such as `scala.reflect.*`-- will likely never be implemented in Scala.js without substantive changes to JavaScript since there's no good intersection of functionality between the platforms. I suspect that developers targeting Node will end up developing a Scala-ish pidgin, perhaps something akin to Microsoft's VisualJ++ though by accident rather than intent. On the other hand, that might have been the motivation behind C#, so maybe this will lead to a new language that's an advancement beyond its forebears? --- Other than that: &gt; It feels fresh, unlike the good old Oracle-owned JVM. Having that not so fresh feeling? My condolences. But age has been used as a justification to keep things as much as it has been to move on. Give me a real reason. I mean, c'mon, it's not like there's no lack of actual reasons to bash Oracle. At the same time, what are the criticisms of the JVM outside of Oracle? &gt; Scala is no longer just a server-side JVM language: it's JVM, Android I'll take the point that Android is Dalvik instead of JVM, but since it can translate any Java bytecode it's always been able to fully support Scala. In that way Scala was never "just" server-side, though it'd also be fair to say that Scala's focus was always on distributed computing rather than mobile. &gt; JavaScript-in-the-browser, JavaScript-on-Node Much better, not trying to say that it's actually Scala running in these instances. &gt; native Worth watching, though I gather the use case is more focused on replacing JNI than trying to do AOT native compilation on programs in their entirety. The latter might only have minimal gains over JIT native compilation, and presumably will lose out on runtime optimizations as hotspots are recompiled with runtime information.
On a whiteboard Scala should be clear enough. Just tell them it's pseudocode. 
That's not flatMap, that's map
&gt; I think if Scala sits on these issues for 1-2 years then it will lose a lot of momentum. There really isn't any evidence of this. Scala has been sitting on some of these issues for 5+ years already, and if anything Scala has gained a lot more momentum recently, they definitely aren't losing it. People do what people do, they work around these issues, avoid them, or they implement work arounds using stuff like Macros (if possible). The benefits of stability arguably are far more important then the cost associated with these issues (i.e. if they break bincompat, or it appears that they are low hanging fruit but in reality they aren't because it causes regressions, etc etc)
I came here to say the same thing...
&gt; You're not running Scala on Node. You're running JavaScript transpiled from code which adheres to a subset of Scala. Let's examine this: 1. the "transpilation" part 2. the "subset" part The fact that there is transpilation is not an argument for saying that you are not running Scala. Otherwise with that kind of reasoning you are not "running Scala" currently because your Scala code is compiled to Java bytecode first, then interpreted and JITted by the JVM. Running Scala compiled to JavaScript and then interpreted and JITted by V8 is "running Scala" as much as it in the case of the JVM. There is no fundamental difference from a technical standpoint other than the "bytecode" is in a textual format in one case and a binary format in the other. On the subset part: Scala.js runs the entire Scala *language*. Now Scala.js lacks full runtime reflection, and there is only a subset of the *Java* library (but most of the common Java APIs are available). In short there are a few differences between the two environments. This means that you cannot just take a fully-packaged, say, server-side Play or Akka app and run it on Node. That is granted, and if that's the expectation, then probably nothing else but the JVM will ever "run Scala", because the JVM and the standard Java library and the semantic of the Java runtime are a bit too much to fully emulate. But I don't think that's a very helpful way of looking at what it means to run Scala today, and even less in the future. So I stand by the statement that we indeed *are* "running Scala" today in the browser and on Node. It is not a statement devoid of meaning. &gt; Give me a real reason I am in no way a Node guy (I have only toyed with it), but I do feel the appeal of a new, active platform, which does some things a bit differently and is not tied to as much historical baggage. Bad reason? That's subjective. I suspect that lots of younger programmers choose Node when they can because they feel closer to the JavaScript ecosystem, and/or had a bad experience with Java (like being forced to use it in college, say). 
Thanks, this was helpful. I'll try change DataBaseSerializable into a tree with child node either a string or a DataBaseSerializable.
I am not deserializing, I am serializing objects into strings.
Shrug. Anyone who can't figure out there are much better options than Kotlin deserves to use Kotlin.
Nice, sane solution. Great for Scala on whatever back-end. And thank you very much for the links. Now if only there was clean solution for multi-level / wrapped exceptions (which you routinely get from the Java side), and I'll even call Scala/JVM usable ;-) EDIT: To clarify, I don't like JVM (as ugly as .NET, just differently). Scala is great, doing away with lot of JVM ugly. Scala without JVM, with union types (used throughout std. lib.) will be awesome.
&gt; It feels fresh, unlike the good old Oracle-owned JVM. uuuggggghhhhhhhhhh (also, there are other jvms beside the oracle jvm, are these also "unfresh"?)
&gt; uuuggggghhhhhhhhhh Great argument. &gt; (also, there are other jvms beside the oracle jvm, are these also "unfresh"?) Probably, although I am mostly talking about the Oracle JVM / OpenJDK and its derivatives. Which other JVMs do you have in mind? I happen to like the standard JVM because it is rock-solid and fast. But it is also heavy, not modular, owned by a single corporation which is not very likeable to say the least, moves slowly, and it drags with it more than 20 years of history.
The key there is "_when it's not clear_". If you have something like `val arkl = arlki leiao eor` and there's no precedent for that code anywhere else, then yeah, the lack of explicit types could be a detriment. But something like `val profile = profileCache get userName` is already pretty clear. Having `val profile: Option[Profile] = ...` doesn't improve much in my view. Yet the type is arguably useful if you had `val op: Option[Profile] = ...` because the variable naming otherwise doesn't carry much information. While I'm pretty comfortable relying on the compiler and other tools, I try to strike a balance and aim to minimize non-local reasoning. I use explicit types for all functions even if the return value can be inferred. I generally rely on inference for completely private class variables and local function variables because the scope/context you need to understand is limited. If I rely on a particular implementation detail from outside the class I use an explicit type, like if I needed the performance of an `Array` and a `List` will not do even if all I do is use methods of `Iterable`. Otherwise it's a judgement call based on how useful I think the type would be to a reader who hasn't seen the code before.
Yeah, but for a Java noob everything is unclear and they need all the clarity they can get when understanding. That's why I'm going to write an SBT plugin that inserts the types back in to make maintenance easier for Java developers.
To be pedantic, there are no for loops in scala. Only comprehensions. Given that, why is there any expectation to them to desugar to while loops like similarly named constructs in other languages. EDIT: Why use scalaxy and not define a simple C style for loop: scala&gt; @inline def cfor(init: =&gt; Unit, pred: =&gt; Boolean, step: =&gt; Unit)(body: =&gt; Unit) = { init; while(pred) { body; step } } cfor: (init: =&gt; A, pred: =&gt; Boolean, step: =&gt; Unit)(body: =&gt; Unit)Unit scala&gt; var i = 0 i: Int = 0 scala&gt; cfor(i = 0, i &lt; 10, i += 1) { println(i) } 0 1 2 3 4 5 6 7 8 9 scala&gt; 
I think it's good style for public API to always have the return type specified (if that is your question?). And to avoid surprises with implicit vals or defs, it is also advised to use a type annotation here (I think it's even mandatory in some cases?).
&gt; Because... performance? That's the entire point of scalaxy-streams. No that's not the point of scalaxy(assuming your point, which is not clearly defined is to make for imperative), and that ignores the point I made. for in scala, and for in other languages are not the same construct, despite sharing a name.
Thanks, Rob, that's very helpful. I will study the example.
And then what, you'd commit the output of the plugin? If so, how do you undo it? If not, how is it going to show up on GitHub?
&gt; Akka Gitter channel Yes! Thanks so much!
A lot of programmers have not been exposed to higher order concepts such as first class functions or higher kinded types. Those users, when given the choice, will prefer Kotlin simply because their favorite Java IDE maker is backing the language (and Scala's reputation for complexity, partly undue -- e.g. crazy symbolic operators, which are way less a problem now). This is why I'm a big fan of the friendliness advocated e.g. by the cats project.
I wish this article touched on the implementation of structural types. The way scalac converts such calls into reflective calls is intetesting. Edit: The way in which `val A = new {...}` and `object A {...}` differ is also interesting, since their semanics are fairly similar.
I have a very simple backend solution written in scalatra for the thesis I'm working on atm: https://github.com/Habitats/corpus/tree/master/web/src/main/scala Currently my only public repo with scalatra, unfortunately. 
&gt; Unless they're so new to programming that any language is unclear, I don't buy that at all. When I started programming in Scala, to understand the code base, I would press "Ctr-Q" and put the types back in because I could not follow the types. I mean now I can just follow the types from the method parameters through the body but initially, especially when looking at code that other people wrote, I couldn't. So believe it. If you don't, I know of Java developers who have had a similar experience. For example, I talked to one guy whose team consisted of 50% contracted Java developers who had serious maintenance issues because of it. They actually took all the Scala code and re-wrote it in Java to make it easier for them to maintain. If given the choice between replacing all the Scala code with Java code or using an SBT plugin to make things more palatable to Java programmers, I go with the SBT plugin. &gt; And if they're that new I sincerely doubt adding types and the visual/cognitive load that goes along with it is going to help. No, it helps. It might not really do that much, but it reduces the "wtf is going on" factor that is some people's reaction to the Scala type system. And believe me, there are quite a few people who have thought "wtf is going on" when it comes to the type system. &gt; Python doesn't express types that way and it's considered extremely beginner friendly, for instance. Yes, but in Python types don't matter as much and the type system is MUCH simpler. &gt; Feel free, but on a collaborative project that's likely to produce very noisy diffs Meh. They don't need to actually commit the changes - they could just run it, read the code with the types in and, and then `git reset --hard` to undo. And even if they did commit they would probably make "Added explicit types" its own little commit that just modifies the type on the vals and vars and nothing else. Just go to the commit before that and everything is back to normal. &gt; I have to question the wisdom of prioritizing Java maintenance of a Scala project. Meh. Let's say that some developers wrote some Scala code and now those developers are gone. Will you help me? 
Just starting to use Scala coming from a python/JS world. Why is the inatantiation each time useful? Just seems slower to me. 
Well if you wanted to undo it you could just `git reset --hard`. Java developer could just run it, read the code with the types in it, and then `git reset --hard`. The plugin could also have an undo functionality. Like you could say "make explicit all type signatures for vals and vars that do not have the type name in the line" and then you could say "make inferred all type signatures for vals and vars that do not have the type name in the line" and it will undo it. You could also just restrict it to a single file, reload the file in the IDE, read it, and then undo the change with Ctr-Z. If you wanted it to show up on GitHub, you could just commit it to a separate branch called "ExplicitTypesBranch" and then when people want to read the code if they don't feel comfortable with the type inference they could read the "ExplicitTypesBranch". I recently talked to a Java developer where some code was written in Scala but the team was mostly contracted Java developers and they couldn't maintain the code so rather than putting in the types manually and making some of the implicit conversions into explicit methods wrapping the variable, they just re-wrote all the Scala code in Java. My goal with this SBT plugin is to have something that can allow Java developers to go "I'm having trouble following along with the type inference and implicits... boom". Problem fixed. That being said, I don't know if I can implement something that fancy. If I were to implement this on my own, I would probably just compile with the "-Xprint:parser" command line option and then go through each file searching for the variables declared with "var" or "val" and copying the inferred type from the parser compilation stage into the source code. Making the implicits explicit would be tricky and allowing the user to customize what types should and should not be inferred would also be tricky. I mean if the user explicitly said "infer every type except for Int" it would be easy but if they said "infer every type except for sub-types of Animal", I don't know how to implement that.
No, totally wrong question. I recently talked to a Java developer where someone on their team wrote Scala code and the teach was 50% contractors and the Java developers couldn't maintain the Scala code so they just re-wrote it. I want an SBT plugin that can take Scala code and turn it from inferred and implicit to not inferred and explicit. Something like the "-Xprint:parser" compilation phase, but that will actually compile.
Fair enough - everyone has their own way of learning. My experience, having known and helped a fair number of people ramp up on Scala from zero Scala knowledge and various levels of Java knowledge, has been that not a single person needed to visually see the types to understand the code. Sure, they might not immediately grok how implicit resolution works or grasp the nuances of linearization and the trait system, but "this type on the left is the result of that thing on the right" was never a stumbling block or prohibitive to understanding what the code did. &gt; Python types don't matter as much and the type system is MUCH simpler My point with Python was not the type system and inference and what not, but the syntax; the complete lack of explicit type declarations that is so prevalent but not seen as an issue when it comes to readability or understanding. &gt; team consisted of 50% contracted Java developers who had serious maintenance issues because of it. They actually took all the Scala code and re-wrote it in Java to make it easier for them to maintain. . .Let's say that some developers wrote some Scala code and now those developers are gone. Honestly that sounds like an organizational issue. Any company should expect to spend the necessary time and resources training people to work on their software, which includes learning the languages. If Scala is the language the company uses then they should either hire Scala developers or understand they'll need to invest in Scala education. If a company allows developers to choose all their own tools with little to no collaboration or consequences, they must accept the risk that the person will disappear one day and whatever they worked on will need to be picked up by someone else, which again means resources invested in getting up to speed. Frankly, if they're not willing to assume those costs then Scala (or any other language) should not be allowed. I suspect making things "easier" by allowing some people to maybe sometimes have types and other not depending on their whims is likely to result in more inconsistency and frustration than enforcing a style guide and educating. &gt; Will you help me? Probably :). But my time can get expensive.
&gt; Probably :). But my time can get expensive. Eek. Well let's make this fun. How would you implement a Scala code simplifier for Java developers? Pretend that developers with as little as 2-4 years of coding experience will be using it.
I did that a while back. http://japgolly.blogspot.com/2013/10/scala-methods-vs-functions.html
Should note this should no longer matter with Scala 2.12 and support for java 8 lambdas. http://www.scala-lang.org/news/2.12.0-M4/#java-8-style-lambdas
Scala.js can also "use" Java classes, as long as they've been reimplemented in Scala.js; and we did reimplement quite a lot of them already. Soon (after this summer's GSoC), even that will be lifted, as we'll be able to compile .java source code with the Scala.js pipeline. (but not .class files) &gt; going on to describe some of those packages, such as scala.collection._ and scala.reflect._. Scala.js supports `scala.collection._` and `scala.reflect._` (the parts that are in `scala-library.jar` anyway). `scala-reflect.jar` is not supported at run-time, but it is supported at compile-time with macros. &gt; Is there a guarantee that arrays will be contiguous in memory? There's no guarantee *per spec*, but all JS VMs provide that feature. So the practical answer is yes. &gt; Can all integers less than 2^63 be stored in 64-bits without a loss of precision? Yes. Scala.js faithfully implements `Long`s. They're slow, but they're correct. They might become faster in the future. &gt; Are Node modules defined such that you can load different versions of the same module into a single process, separated by different, siloed run contexts? Not that I know of. In any case, Scala.js does not support classloaders (because that's what you're reffering to, aren't you?) I think you're vastly underestimating the situation of Scala.js.
&gt; By 8 weeks of constant use I think it's reasonable to expect anyone with 2+ years of experience to be doing okay with the language. I would not expect expert by any means, but certainly they should feel comfortable with the most common patterns and generally be able to read code and be productive. &amp;nbsp; NOOOOOOOOOOOOOOOO. You're making the assumption that the person learning the language has both a teacher and a book. When I learned Scala, I had no teacher and no book. I had an IDE and I had the Scala compiler and on occasion I googled things and that was about it. I also think you're making the assumption that all the users are professional programmers. I don't mean 2+ years of professional experience. I mean like first programming class happened two years ago experience. I mean like never worked a programming job in your life experience. &amp;nbsp; Watch my video on learning existential types: https://www.youtube.com/watch?v=NFnsFda82Yo. &amp;nbsp; I don't know how many minutes you're going to watch (it's pretty slow paced to make sure they fully grasp the concept), but one thing that I found really helpful and that at least one viewer has said was really helpful in learning the language was compiling with "-Xprint:parser" and then "-Xprint:typer" and seeing what the code compiled into. Where the parenthesis went, what the full type signatures were, seeing "extends AnyRef", seeing the implicits turned into explicit calls, etc. It's one thing to hear about name mangling in Scala, it's another to see !#$&amp;*&lt;&gt; turned into the demanged form callable from Java code by compiling it with "-Xprint:parser". Learning the language this way is a form of learning by doing and it actually works. &amp;nbsp; Personally, I'm one of those control freaks who has to know exactly what my code compiles into to feel comfortable and before I started using those command line options I just felt very uneasy about the language. Like if I see a language feature like view bounds and context bounds I have to understand it in terms of what I already know - I have to be able to look at it and desugar it into implicit parameters and conversion. If I can't just look at something like a for comprehension and imagine what it would desugar into in the parser phase, I feel uncomfortable with the code and I don't want to read it, even if I could have understood the coder's intent if I really tried. It's not just about understanding the intent, I also have to be able to debug it in my head without a debugger and doing that requires a detailed understanding of what is going on. &gt; Assuming there were still a legitimate issue training people and we decided we needed a training tool, I'd consider contributing to a Scala plugin a feature that displayed types inline when they were absent, possibly for a selection of code or the current method or whatever. &amp;nbsp; One user asked for a feature that would display fully desugared code inline. Not just types put in, but `value.implicitFunc` replaced with `implicitConversion(value).implicitFunc`, for comprehensions replaced with explicit calls to map, flatmap, filter, withFilter, whitespace delimited method calls replaced with dot delimited method calls, etc. Fully desugared code. Something that a Java programmer (who maybe learned one other functional programming language) could decipher without actually having learned Scala's syntactic sugar. &amp;nbsp; The real problem isn't just the types. It's that people (who happen to meander into the language accidentally rather than for any reason related to work) find the syntactic sugar confusing and they have to use special compile options to de-sugar it to understand what is going on. &gt; But if I were in charge of the team I wouldn't want something that changed code directly and automatically, &amp;nbsp; A beginner said to me "A cool plugin for eclipse / IntelliJ would be the ability to show the desugared code side by side." This wouldn't necessarily change the source code, but it has to show desugared code in its own little tab that the user could pull up and match up with the not desugared code. That being said, this isn't for a team of professional programmers (although they could use it if they wanted to). This is for a team of college students who know Java. Not like super hardcore college students, but like average college students who just want to understand wtf is going on. &amp;nbsp; That being said, maybe an IDE plugin or a command line tool that desugars a particular file would be a better option. "-Xprint:typer" is okay, but it generates humongous amounts of terminal output and a lot of it is `&lt;synthetic&gt;&lt;tag&gt; fsbfhjdsd &lt;tag&gt;&lt;synthetic&gt;` and it would be a lot more convenient to be able to see de-sugared code that could actually compile. If it appeared in the IDE with syntax highlighting, even better.
Yeah; for-comprehensions desugar to calls to .map(), .flatMap(), and .withFilter(), all of which are implemented in terms of .foreach(). All of the above will still be faster in 2.12 because of its improved support for anonymous functions (or rather, because it takes advantage of Java's/JVM's improved support for anonymous funcs) and other more general optimizations.
I'm learning Scala coming from an extensive Python background with some Java and C. I'd like learn to write idiomatic Scala as well as understand why the idioms exist. Can anyone recommend a book that will give me both some basic understand on Scala and its features but also allows me to dive deeper into the language? I'd like to gain an 
Did you profile or are you just waving hands? You won't get any statistically significant difference of runtime performance; this is a drop in the river of intermediate buffers within collection methods, composition wrappers, Futures, and so on. Now what does matter is amount of generated classes that blow up jar size. Also, applying FunctionN instance is by no means a static method call.
Yes, i know that. The thing is, in my company scala is not really used in the most functional approach ever. Nevertheless i am extremely interested in adopting these practices because i think they will be very valuable in the future. Thank you for your input
Yeah, that can be a tough situation. Are people writing Scala as a sort of Java++? Perhaps they can be wooed by the prospect of never seeing another NPE. 
Gotcha. Then I'd probably use Monadic transformation in the private parts of the code, and only return raw values at the edges, where someone else may consume the functions you write. 
It's a minor point I guess I could have been clearer. I meant basic comfort _with writing Scala_ in an IDE, not just comfort using the IDE itself. Parsing the compiler output might be a reasonable start but seems a little brittle. I'd probably check out the IntelliJ plugin first and try to find where I could hook into its code, especially since it's done a lot of the hard work around integrating with the IDE for you.
Good point. There is one caveat though, converting generic method to an anonymous function gives you `Any` (or upper bound in a generic case) as a parameter type. class Test { def m[T](x: T) = x.toString def y = println(List(1, 2, 3).map(m)) } You may expect `m` to be converted to `Int =&gt; String`, in fact it's `Any = &gt; String`. So it's probably easier to write: val f = (x: Any) =&gt; x.toString
Oracle guys optimize Java and JVM; not Lightbend. For any low-level optimization I'd rather place my bet on Oracle. This includes lambdas, SAM types, and JIT optimizations. I am not quite sure that a new SAM instance is created for *each* method to lambda conversion *invocation*, at least [javadoc](https://docs.oracle.com/javase/8/docs/api/java/lang/invoke/LambdaMetafactory.html) and some answers at SO hint that an existing instance may be returned for the same *call site*. Though, different call sites will create different instances (there's an intermediate proxy method for each call site). Could you elaborate about difference between function and a method bigger in 2.12? In 2.11 both function and method result in generation of a heavy-weight "nested" class, which is a crutch. In 2.12 both do not, and rely on runtime SAM type generation.
Am I reading the results correctly in that the opposite of what OP is showing was reflected in the timings?
Yeah; the optimization they link to basically implements .foreach() as a C-style for-loop that should perform as well as a user-written while-loop, which is what OP was asking about (the optimization is specific to Ranges, though). I was just adding that for-comprehensions will still be faster in 2.12 for other reasons. [Box removal](https://github.com/scala/scala/pull/4858) and smarter inlining are probably just as important, but they apply to both while-loops and higher-order functions/for-comprehensions. 2.12 is pretty awesome.
I confess I can't remember the conclusions of this post but according to my old measurements, using a function as value/object is sliiiiiiiiii¹⁰⁰ghtly more expensive than using a method.
I'm going to start with the command line. [https://github.com/JohnReedLOL/scalatype/blob/master/README.md](https://github.com/JohnReedLOL/scalatype/blob/master/README.md) The plan is basically to just generate the desugared code files and then for each file get all the lines which contain a var or val declaration into a list, find each of those matching declarations in the non-desugared code, copy over the type from the desugared code, and then pop the list until the list is empty. Then go to the next file. Repeat until all sourcecode files have typed. I'm actually planning on modifying the user source file instead of generating a copy because that's what my prospective user asked for, but generating a copy with the types put in (sourcefile.scala.desugared) shouldn't be too hard. The IDE plugin can just hook to the command line tool (I think) and display whatever is in sourcefile.scala.desugared.
You gotta let Akka HTTP know how to convert your List[A] into a data type that can be sent as a response (json for instance). Look up Marshalling in Akka HTTP / Spray. 
new Array(2) means you've allocated a new array with size = 2 and with no given sub type. Why would it be Nothing instead of Array[Nothing]?
You have two main test frameworks, [ScalaTest](http://www.scalatest.org/) and [Specs2](https://etorreborre.github.io/specs2/). [Here](https://stackoverflow.com/questions/2220815/what-s-the-difference-between-scalatest-and-scala-specs-unit-test-frameworks) is some discussion of the differences. Each website has simple tutorials. Additionally, you can throw in property based checks with [ScalaCheck](https://www.scalacheck.org/), using either of the above frameworks. If you build your project with sbt, you can simply add a dependency to the test framework and then run `sbt test` (I know this works with ScalaTest, I haven't worked with Specs2, but I assume it's similar). Voilà! Or in IntelliJ IDEA, right click on the test class in the project browser and select `Run`. ---- Using Google, you can find plenty of sites getting you started. E.g. [here](https://semaphoreci.com/community/tutorials/a-hands-on-introduction-to-scalatest) or [here](http://alvinalexander.com/scala/writing-tdd-unit-tests-with-scalatest).
Sorry for my poor explanation. I mean why would it be Array[Nothing] instead of Array[Any] (Just like you do not give any type parameter, so any type is OK. Similar to the raw types of Java but with [Any] type parameter.) or something else. Though I think Array[Nothing] is also good, because you do not give any type parameter, and compiler knows nothing, so this is Array[Nothing]. I guess I asked a stupid question, but I came up with this question this afternoon and thought a lot.
 [leaf for tree in forest for leaf in tree]
This. This is what i want (i expect to remain here for more 6 months.) But you need to understand that for a noob changing an entire code base is not easy. Because literally everything is written in a certain way. We use Futures, Promises, Options extensively but all the code is wrapped in a way that is just impossible (from my point of view) to chain them. I was the first to insert Try blocks btw.
One of the chapters in FP in scala features implementing something similar to scalatest which predef mentioned, so if you like functional programming you can get some useful learning from trying out scalatest. 
I generally see sealed abstract classes instead. There isn't a difference for ADTs. But you should definitely be using pattern-matching more!
In Haskell you would write that as `data X = A Int | B String` which gives you data type `X` with *constructors* `A` and `B`, and any operations you define are always for `X` and must handle both cases. So this is the traditional way to think about ADTs. In Scala it's typical to do the same thing (i.e., treat `A` and `B` as constructors of `X` rather than types of their own), and much of the time this makes sense, particularly for generic data types. You would never have a variable of type `Some[String]` or `Left[Int]` or `Nil.type`, for instance. But Scala is not Haskell, and you do have the flexibility to use individual cases as their own types and to use a subclassing as a means of abstraction. For instance in my work I have a sealed trait for different kinds of astronomical objects so I can match on an arbitrary one, but I also have code that only works on asteroids for instance. So I think, as always, "it depends". 
&gt; I generally see sealed abstract classes instead. There isn't a difference for ADTs. This post notes that there are quite a few differences - http://stackoverflow.com/a/35251513/409976.
I don't have any suggestions for a first project, I would however suggest a different book. A new edition of Programming in Scala just [came out](http://www.artima.com/shop/programming_in_scala_3ed)(The Second edition is now Free) and [Functional Programming in Scala](https://www.manning.com/books/functional-programming-in-scala) are must reads now.
What is the structure of your tests? Do you (anyone) has a good example? I am thinking about, how are you separating integration/unit etc tests? 
&gt; Note - I don't understand DataKinds. I do, and I can assure you that they are not relevant to the current discussion :)
None of those relate to ADTs though. I'd be remiss if I didn't point out point 4 contradicts [Martin Odersky's thinking](https://www.artima.com/pins1ed/traits.html#12.7) on traits vs. abstract classes.
Here you go http://www.amazon.com/Testing-Scala-Daniel-Hinojosa/dp/1449315119/ref=sr_1_1?ie=UTF8&amp;qid=1464995693&amp;sr=8-1&amp;keywords=scala+testing
Note that `Future`s are a kind of generalised case of `Try`s that have the additional case of being 'pending' (so success with a value, failure with an exception, pending with no value [yet]). If your code is extensively using Futures you may get a long way by using the error-handling methods of the `Future` type wherever you're dealing with Futures, instead of explicitly breaking out Trys.
Hmm, /u/Milyardo's implementation looks like it will return a `Try[Try[B]]`. Anyway, we can fix that and at the end do a `Try#get` to get back the `B` value or force the exception we want: def attempt[A, B]( left: Int, operation: =&gt; A, processResult: A =&gt; B)(tryAgain: =&gt; B) = (Try(operation) map processResult recover { case _ if left &gt; 0 =&gt; tryAgain case e =&gt; (LoggerFactory getLogger classOf[retry]) .error( s"Reached maximum retry attempts. Error: ${e.getMessage}", e) throw e }).get
&gt; You probably want `dp` in the companion object of `RetryProfile`. I would argue against that. It would bring the `implicit val dp: RetryProfile` into scope wherever the name `RetryProfile` is imported, and make it a potentially confusing compile error to create another `RetryProfile` somewhere else. In fact, /u/verytrade, I would recommend not making this an implicit at all. Implicit values of concrete types, if you ask me, are not worth the hassle of you carefully, manually managing their scope so that they get passed in at the right moment. If you decide to encode retry behaviour in a `Retryable[A]` [typeclass](https://docs.google.com/presentation/d/1xPkY-g0cHzOjS-hSKeZa-RTAWXMuE2aAfAvbWjsWLs4/edit?usp=sharing), I would say go for it, because then you get a great benefit from the implicit search rules.
Would you ever write a function?: def g(o: Some[Int]) = ... There you go :)
&gt; Numeric widening does not happen. The following code compiles in 2.11 (with -Xexperimental) but not in 2.12 M4: &gt; abstract class A { def bar(): Long } &gt; def foo(): Int = 1 val a: A = foo It does _not_ compile under 2.11, at least not with the latest version 2.11.8. Return type of `bar` must be `Int` for this to work. Why should numeric widening apply here? But yes, in Scala 2.12.0-M4 this doesn't work even if you declare `def bar(): Int`. Should it? If I look for example at http://downloads.typesafe.com/website/presentations/ScalaDaysSF2015/T2_Rytz_Backend_Optimizer.pdf - this doesn't say anywhere anything about eta expansion and SAMs. All examples are using function literals, like so: new Thread(() =&gt; println("hi!")).run() Attempt: def test() = println("hi!") new Thread(test _).run() &lt;console&gt;:13: error: type mismatch; found : () =&gt; Unit required: Runnable new Thread(test _).run() ^ Whereas: scala&gt; val test: Runnable = () =&gt; println("Hi") test: Runnable = $$Lambda$1794/327806847@3917c163 To me this makes sense. As to the silly click bait and misleading post title - SAM support in Scala 2.11 was `-Xexperimental` (experimental!). ------------- __Edit__: The last comment on [SI-9178](https://issues.scala-lang.org/browse/SI-9178) suggests that it was decided eta expansion should not kick in for `Function0` SAM: &gt; [Jason Zaugg] I'd be inclined to go further and deprecate eta-expansion given an expected type of Function0, and avoid triggering it with SAM types of the same shape from the outset. So let's try with arity 1: def test(e: java.awt.event.ActionEvent) = println("Action!) new javax.swing.Timer(1000, test) // works!
Thanks for the detailed analysis, which is all the more necessary given that: http://stackoverflow.com/questions/25321482/does-scala-specification-2-10-and-2-11-exist
 If you want to learn both the basics and the advanced stuff I'd recommend [Programming in Scala 3rd Edition](http://www.artima.com/shop/programming_in_scala_3ed).
Try looking one line below that error message :) On my machine at least, the compiler gives me the following very helpful hint: &gt; Note: `Nothing &lt;: A` (and `Empty &lt;: TreeNode[Nothing]`), but trait `TreeNode` is invariant in type `A`. &gt; You may wish to define `A` as `+A` instead. (SLS 4.5) That is, you say that `Empty` extends `TreeNode[A]`, but that's not quite correct: you have declared `Empty` to extend `TreeNode[Empty]`, but that doesn't automatically entail that it also extends `TreeNode[A]`. To tell Scala that you do want `TreeNode[A]` to be a subtype of `TreeNode[B]` whenever `A` is a subtype of `B`, you need to use a "+" annotation to indicate that `TreeNode` is "covariant" in `A`, like this: trait TreeNode[+A] Data types are not covariant by default because covariance imposes some constraints on the data type, and Scala doesn't want to constrain you unless you tell it that you want to be constrained.
So there's a couple of issues. Your issue is that your TreeNode trait is type invariant. This basically means that even though Nothing is a subtype of A (Nothing is a subtype of everything) the compiler cannot infer anything about whether TreeType[Nothing] compared to TreeType[A]. There are two ways to solve this. The first is to define your TreeNode as trait TreeNode[+A] This means that TreeNode is 'covariant'. Variance is fairly tricky to fully understand and instead of me trying to explain it I'd recommend googling it. The way I'd solve it however is to get rif of your empty type and have your function to return an Option[TreeNode[A]] EDIT: Ninja'd
I've just recently gotten into scala, and it's small snippets like this that make me really love this language.
Could you help with regexs? I'm trying to regex parse "blah blah blah // Hello.scala" and "blah blah blah // Hello.java". I tried these two patterns: val JavaFileRegEx = """\S* \s+ // \s{1} ([^\.java]+) \.java """.replaceAll("(\\s)", "").r val ScalaFileRegEx = """\S* \s+ // \s{1} ([^\.scala]+) \.scala """.replaceAll("(\\s)", "").r The pattern to match Java files works, the one to match Scala files does not.
This might be a noob question, but can you explain the benefit of implicit class again? What does `extend a type with methods at compile-time ` mean ?
No: val node = Node(Empty, "value", Empty) node.binarySearch("searchValue") // binarySearch will be a method of node
&gt; get(key: A): Option[Tree[A]] because the key might not exist in the Tree! Then why not just Empty? You can spare an additional instantiation if you found a match and make the return type simpler. &gt; Like I mentioned, callers shouldn’t be constructing their BSTs manually: instead, library methods should be written to build the BSTs from input data. But how would you write unit tests when testing insert, remove (and balance for RBT)? &gt; Currently the BST type also has an untyped dependency on ascending Orderings. These are compromises due to the partial implementation. Isn't the ascending Ordering the default for BSTs? &gt; Hmm, it shouldn’t. Edit: ah, I wrote it for 2.11. I've tested with "Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL".
For further investigation you should check the documentation about [SIP-13](http://docs.scala-lang.org/sips/completed/implicit-classes.html).
Still a factor 20 to 30 behind Java. C'mon Scala FLOSS!
At the risk of embarrassing myself, I have no idea what's going on here. Can someone explain? And secondly, if it's non-obvious enough that I need it explained (being experienced, but with no exposure to scalaz), is it desirable?
&gt; Firstly, the standard Scala collections return Options for get/lift (and bare values for apply), so it’s a good idea to follow the same convention with custom collections. From [TreeSet 2.11.8](http://www.scala-lang.org/api/2.11.8/#scala.collection.immutable.TreeSet): "def apply(elem: A): Boolean" and there is no "get" or "lift" there. Those might be present in scalaz but not in the standard library. &gt; Second, it means we can build pipelines from the result of get, like normal. Given a properly implemented BST can't you do the same with it? Or wouldn't it be better if you would get a simple Tree and you wouldn't need to unwrap it every time? We could actually call it "findBranch" or "findNode" instead of binarySearch while implementing the "contains" method which would return with a Boolean. &gt; Thirdly, there’s a conceptual/semantic difference between ‘tree does not contain a node for that key’ versus ‘tree contains an empty tree for that key’. That's a useless information - if you want to know that your tree is empty or not, you can check it by yourself. &gt; For example, should Map(...).get(badKey) return None or Map.empty? If we're talking about Maps(Map[A, B]) and not Sets(Set[A]) than it should return with Option[B] and not Option[Tree[(A, B)]](or whatever) right? Map is an associative collection while BST is a Set. Returning with a branch isn't that useful anyway and returning with the elem on a match is only useful at "mutable situations" - which isn't idiomatic. &gt; Yes, however the OP’s algorithm will fail if people manually construct a Tree that doesn’t match the Ordering they provide. This loophole can be closed by treating tree construction as an internal matter for the BST library dependent upon the given Ordering. If we need to treat it as an internal problem then we could just use a wrapper instead and all of our problem "disappears" since our wrapper defines "A" to be orderable and hides BST. Except if we want the wrapper to be a value class but value classes has two uncomfortable limitations at this case: 1. may not define concrete equals or hashCode methods - if we can't override equals then we can't provide a way to check if two Sets have the same set of elements(insertion, deletion and balancing could bring trouble at graphs), only that they have the same structure. 2. must have exactly one parameter, which is marked with val and which has public accessibility - can't hide the information, so the user can create the wrapper from a raw BST. &gt; Hmm same, works fine for me... Tried it at [ideone](http://ideone.com/tLej7j) still fails.
It looks more like an example of using the State Monad than something you'd actually want to use for logging instead of something like log4j. It shows how to mutate state with lenses. The log-ness of it is incidental.
&gt; The log-ness of it is incidental Kind of. You would still use something like log4j to actually side effect on the logging data, but this is actually a powerful approach to logging. You are able to accumulate data as you encounter it, as opposed to at the time that you actually write to the log. It saves you from re-assembling the same "state of the world" every time you might encounter a loggable point in your code (extremely common and extremely frustrating pattern in procedural code) and it allows you to handle your actual logging process all in one place, "at the end of the world" (see first [paragraph](https://pchiusano.github.io/2014-05-21/what-effects-are-worth-tracking.html)) Much better than throwing calls to some logging interface all over your codebase, especially if you have multiple backends that concern with different bits of data, pii to scrub, programmatic actions to trigger, etc etc.
 Guess which is taught at almost any university as students' first OOP language: Scala or Java? Knowing that which language will be used at home works? And that's why these comparisons are unfair.
&gt; &gt; Personal I think you are going off on a tangent. &gt; &gt; Care to explain? ... scala.collection.immutable.TreeSet ... create a tree without a wrapper There, you’ve done it again, twice. Nevertheless, you’re free to write monolithic TreeSets if you wish. 
I cloned and ran it, and with the help of one of your other replies, I can see what's going on. Sorry, I think it's over-complicated; logging shouldn't be notable enough to merit a GitHub and reddit post IMHO, and ideally IntelliJ [15.0.1] ought to parse the syntax, too :-) But thanks for explaining, anyway.
Whether logging qualifies as an "effect" or not is contentious, even among several of the experts on FP I know. However, I'd like to call attention to [/u/beezeee](https://www.reddit.com/user/beezeee)'s "You are able to accumulate data as you encounter it, as opposed to at the time that you actually write to the log." This can be a critical difference in some contexts. For example, the Doobie functional JDBC API has an open issue requesting [structured logging](https://github.com/tpolecat/doobie/issues/7) with expected results like [this](https://github.com/tpolecat/doobie/blob/master/log.png). An early attempt at it (in fact, IIUC, the code that did, in fact, generate the expected result, but is inadequate for reasons described in the issue) is [here](https://github.com/tpolecat/doobie/blob/v0.1/wastebin/core/src/main/scala/doobie/util/TreeLogger.scala). It makes good use of scalaz's immutable `Tree` type, the `TreeLoc` (more commonly known in FP as a `zipper`) type, scalaz's `IORef` and `IO` monads, and Argonaut and its `HCursor` (history-tracking zipper) type. The point, as always with pure FP, is that you can add even this level of richness of functionality at arbitrary points in your program without worrying about the addition interfering with the rest of your program or the rest of your program interfering with it. Even something as seemingly simple as logging often turns out to need some sort of context carried along with it, and the thing pure FP is exceedingly good at is "carrying some sort of context along" in ways that remain both isolated and compositional. It takes a bit of getting used to, for sure. But in the end, it's worth it. 
There is nothing unfair about that, it's just a statistic.
Thanks /u/paultypes - you hit the nail on the head in calling out context as the primary benefit. I have yet to work with production software where this is not extremely relevant in logging application behavior. Interesting stuff going on in that doobie logger.
&gt; especially how to "translate" the Java 8 lambdas is puzzling me. Do you have an example for this?
Milestones are released for library authors to test whether any serious issues remain. Other than that and tinkering around with the new features, I wouldn't use a milestone in production.
(Also since there were some questions popping up about SAM support in Scala 2.12)
A small note about re-running tests. With SBT, you can run tests "interactively" (daemonized for each source change), and you can run only those tests that failed previously or touch changed sources (the `testQuick` task).
Especially considering milestones haven't been binary compatible between each other 
There are several other restrictions, and even so there are many cases where value classes get boxed. Not a magic bullet yet.
Where I work we're currently cross building against 2.12-M4, the most limiting factor in doing that is the availability of dependencies built against 2.12-M4.
dont forget to close the io
Whoops. val file = io.Source.fromFile("foo") val sortedChars = file.getLines.mkString .groupBy(identity) // same as (x =&gt; x) .mapValues(_.length) // same as (x =&gt; x.length) .toSeq .sortWith(_._2 &gt; _._2) file.close() sortedChars.take(10).foreach(println) Gosh dang side-effects.
Watch out: `.getLines.mkString` does not work for this assignment, because it loses the encoding of newlines — but the assignment says you’re supposed to print them as `\n`. In fact, you don’t need/want to use .getLines at all. The following implementation complies better with the assignment specification, including that “the character with the smaller ASCII value should be considered as being used more frequently”: val displayCharAsString: Char =&gt; String = { case '\n' =&gt; "\\n" case '\t' =&gt; "\\t" case char =&gt; char.toString } val source = io.Source.fromFile("path/to/file") try { val sortedChars = source.map(displayCharAsString).toSeq .groupBy(identity) .mapValues(_.length).toSeq .sortBy{case (char, count) =&gt; (-count, char)} // *note sortedChars.take(10).foreach(println) } finally { source.close } \*Note: This is just one possible syntax for achieving the correct sorted order. Scala can easily handle multiple sort keys (tiebreakers) because its standard library knows about tuples (i.e. it knows that `(a, -10) &lt; (a, -5) &lt; (b, -5)`). Thus, we can explicitly map our data tuples `(char, count)` into our sort key `(count descending, char ascending)` to get the right result.
One of the nice things about lifting into a Coproduct using Inject is that order of types don't matter, It'd be great if FreeK could preserve that convenience and keep the syntactic sugar of building the coproduct with an HList.
If you had to pick one, would you prefer it to Function Programming in Scala?
It's okay, you can provide emotional support. I got to the point where I have one while loop iterating through the source file and another while loop iterating through the desugared source file. Regular expresssions are being used to match and break apart all the declarations and definitions and insert the types. https://github.com/JohnReedLOL/scalatype/blob/master/src/main/scala/com/example/Hello.scala Unfortunately, I have some major arrow code, null checks from Java code, and I had this nasty bug where I was skipping a line in the source code after every while loop completion because my inner while loop check was reading a line. In addition, test cases need to be written because right now the source file is parsing itself and testing is done by comparing the file itself with the parsed version manually. I'm hoping to have something that I can ask a programmer to try by the end of the week. 
This isn't a book and might be too late, but Daniel Westheide has a good [guide to Scala](http://danielwestheide.com/scala/neophytes.html) for beginners that is heavy on types. The first time I was convinced that types were awesome was when I read the part about [Options](http://danielwestheide.com/blog/2012/12/19/the-neophytes-guide-to-scala-part-5-the-option-type.html).
I've rebuilt CoproductK to manipulate higher-kinded structures because exisiting Coproduct isn't able to represent that easily... So for now, it is custom for Freek but in shapeless3.0, Miles Sabin is going to add a common representation for HList &amp; Coproduct using SMT and also make it easier to build things like CoproductK... maybe it will be deprecated then
You cannot mix "extractor parameters" and "extractor results". I.e. you cannot write a case clause where `"he"` is a parameter of the extractor and `body` is the result of the extractor. Instead you must create an extractor with `"he"` first. For example: case class PrefixedString(prefix: String) { def unapply(s: String): Option[String] = if (s.startsWith(prefix)) Some(s.substring(prefix.length)) else None } val HePrefix = PrefixedString("he") "hello" match { case HePrefix(body) =&gt; body.length // body should be "llo" case _ =&gt; -1 } 
`Foo` is a mix in with initialization side effects (like reading a config file or opening a database connection) that should only be executed once. 
I think you're looking for a regex or a guard. Guard: "hello" match { case he_ if he_.startsWith("he") =&gt; he_.length - 2 // 'if' is a guard case _ =&gt; -1 } but regex doesn't need that magic number of '-2': val he_rest = "(he)(.*)".r "hello" match { case he_rest(a,b) =&gt; b.length case _ =&gt; -1 } It's hard to know for sure if this is what you want, because your example doesn't totally make sense. How would your extractor know to splitAt index 2?
If you have a mega object, say the single import to access a complex library, this pattern allows the library author to separate the object's implementation across multiple files while ensuring none of the components are reused in isolation. 
Thanks for the reply, yeah I didn't know how my extractor was supposed to split at 2, I just thought it might be possible since for example in this sample: List(1,2) match { case List(1, a) =&gt; a } An `Int` is being matched at the same time that the second value in the list is being bound to `a`.
Is Scala's Set a monad?
Answers: https://groups.google.com/d/msg/scala-internals/lv4sTGorDLU/Pi8UTC3aBAAJ
Yeah, that's why I added that last comment. I'm not sure you're running into the problem you think you are. There are more than two parts to pattern matching - one is to match a literal, and the other is to use an extractor. The object extractor works, but won't be exhaustive on the length of the List: List(1,2, 3) match { case List(1, a) =&gt; a case List(_, _) =&gt; -1 } // MatchError So your problem isn't with matching on literals, but with figuring out which extractor is available. It's a reasonable problem. "hello".toList match { case 'h' :: 'e' :: body =&gt; body.size // works case _ =&gt; -1 but neither of these work: case "he" + body =&gt; // because '+' is not an extractor case 'h' +: 'e' +: body =&gt; // because "hello" is not a list So in some cases if you wanted to just take a look at the first few characters, you could match on the character list: word.toList match { case 'h' :: 'e' :: rest =&gt; rest.size case 'g' :: 'o' :: rest =&gt; rest.size case _ =&gt; -1 } because then you get to use List's extractor and match on the character literal. String literals work too, they just don't have the extractor you were hoping for: "Hello" match { case "Hello" =&gt; 1 case "Goodbye" =&gt; -1 case _ =&gt; 0 } // returns 1
The main, legit, use case for this is decomposing an `object` holding `implicit` things (like typeclasses) in various levels of inheritance, so that implicits are different levels have different priorities. The superclasses/traits are really only there to provide the priorities, and they're not meant to be extended by anyone but that `object`. Note that `sealed` all the way down is pretty much as valid in terms of protection; it does not prevent you from doing wrong inheritance in the same compilation unit, but anyway you have the control on the compilation. The self type trick is just a bit clearer on the intent (at least to me).
You can refactor inner classes by writing `class Inner[O &lt;: Outer with Singleton](val o: O)`. You can then easily define objects that are valid in a specific context (the same pattern is used for a macro's Context). Also singleton types can be easily checked using reference equality.
No shit! That's like that sinking feeling you get when you find reference to some code that does *exactly* what you need and eagerly do a pull of it... only to realize that the code relies on about 100 different files and therefore makes using it for your own use futile.
Also, it makes your API documentation unreasonably difficult to follow.
Good guess; union types are already in Dotty [right now](http://www.slideshare.net/Odersky/scala-days-nyc-2016) (check out slide 36). Hopefully they begin transitioning to Dotty (or porting its advances to scalac) soon.
Maybe you can answer this question which had been bugging me. http://stackoverflow.com/questions/28908861/using-scala-singleton-as-a-type-constraint-is-inconsistent/28932375#28932375
No, Scala is not easy, idiomatic or not. If you can look at something like: val ii = for (i &lt;- 0 to 5; j &lt;- 1 to 10) yield i*j And expect it to become: val ii = 0.to(5).flatMap(((i) =&gt; 1.to(10).map(((j) =&gt; i.*(j))))) Then yeah, Scala is easy. 
Please reduce your code until you have a small self-contained bit of code reproducing the problem to post here. Just linking to your repo and asking volunteers to debug it for you is [poor form](http://www.catb.org/esr/faqs/smart-questions.html#code).
If I understand correctly, Scalatype is a tool to explicitly annotate statements with their inferred types. This project does something similar: [scala xray](https://github.com/harrah/browse). For example: [Here](http://harrah.github.io/browse/samples/library/scala/Math.scala.html#123579), put your cursor over the value x. I don't use IDE and I have to say it's really helpful to go through a codebase quickly. Unfortunately, it's getting out of date. Another thing to explore is [scalameta](https://github.com/scalameta/scalameta). 
Hold on,...you’re trying to recompile the current project while running the current project? That’s not a sustainable way of working. If you just want to force something to happen anyway, maybe `set target &lt;&lt;= baseDirectory / "tmp/target"` so that it builds in a different directory. OTOH I tried that but Utils.shortenTypeSignature has several bugs so I didn’t get far.
You misinterpret this as a request to fix a stack overflow problem. I was looking for something more along the lines of "try using scalameta to get the explicit types" or "don't recompile the current project while running it".
I agree with that, although for argument’s sake I’ll point out that it can be fudged a bit more like OP /u/joshlemer was wondering: case class PrefixedString(prefixLength: Int) { def unapply(str: String): Option[(String,String)] = if (str.length &lt; prefixLength) None else Some(str.splitAt(prefixLength)) } val PrefixedString2 = PrefixedString(2) "hello" match { case PrefixedString2("he", body) =&gt; body.length // 3 case _ =&gt; -1 } For OP: this lets you organise your cases nicely, but the risk of it is that you might hard-code a literal "hel" for a PrefixedString2 matcher, which will never match due to the mismatched lengths (latent bug).
Depends what category you're working in. If all your values are equality-comparable (and all your functions are pure with respect to this) then yes. But generally we tend to regard e.g. functions as values, in which case `flatMap` can't be associative (since you can't generally compare functions for equality).
For that scenario I don't see the difference between object Bar extends Foo object Baz extends Foo and class Quxxl extends Foo new Quxxl() new Quxxl()
This is good advice for pretty much any optimization that isn't obviously, categorically superior to what it replaces. Always test!
+1 for those keyboard shortcuts.
No, it's resolved when you call the method that takes an `implicit` parameter - i.e. at the point where you make a call to `testInstance.myFunc()`, that call will also pass a value for `ord`, resolved implicitly in the current scope (or fail to compile if it can't find one, or there are ambiguous implicits).
Thanks.
Every API takes some learning, just as every language does. My experience is that ScalaZ is so immensely useful in so many situations that it's well worth the time it takes to learn - certainly more so than many logging APIs (which often rely on reflection, macros, agents, classpath manipulation or other such shenanigans). The crucial selling point of this kind of style is that it keeps logs with the thing that they're logging, but also keeps everything as ordinary values that are refactor-safe. In my experience that's extremely valuable in large systems. (My view is that if it's worth logging it's worth logging in a structured way; I don't find value in logging arbitrary Strings. And I find the modifiable state counterproductive (e.g. this API lets you lower the severity of a message, potentially quite far away from where you initially created the message) - I would generally just use a `Writer` approach. But the point of making log entries values that can be composed before you finally write them out is very valid)
If he's got profiles/benchmarks demonstrating that `require` is causing noticeable overhead then he's right. Otherwise it's extreme premature optimization.
What do you think about using nested objects and case classes in an Http client to provide an api that mimics the rest interface it uses? For instance: /* GET /users/{userId} GET /users/{userId}/friends PUT /users/{userId} GET /posts PUT /posts GET /posts/{postId} GET /posts/{postId}/comments GET /posts/{postId}/comments/{commentId} POST /posts/{postId}/comments/{commentId} */ class Client(url: String) { object posts { def get(): Future[List[Post]] = ??? def put() = ??? } case class posts(postId: String) { def get() = ??? object comments { def get() = ??? } case class comments(commentId: String) { def get() = ??? def post() = ??? } } case class users(userId: String) { def get() = ??? def put() = ??? object friends { def get() = ??? } } } val client = new Client("www.website.com") client.users("abc").get() client.posts("123").comments.get() client.posts("123").comments("456").post() It's not a style of programming I've seen anywhere else in Scala code I've read but it does produce an API that closely resembles the underlying api calls. Edit: I had this in mind for producing an open source "client-sdk" AKA a rest client for a particular service, specifically the client-server API for www.matrix.org, which is a [pretty big api](http://matrix.org/docs/spec/client_server/r0.1.0.html), so wanted a come up with a clean and consistent api.
It's not even about the performance overhead of require vs assert. It is basically due to the fact that you can disable assert on the vm. This leads to two conclusions: * If the asserts can be disabled their overhead will be zero (allegedly) * If disabling the asserts is even an option then one might say that the conditions they are checking might be optional I show a pieace of code where you can see how the asserts are being used and IMO this does not look like something you might want to disable.
I've tried web frameworks and they all stink. Really, you don't need frameworks for everything. Just remember to use https as soon as you're handling sensitive data, use httponly cookies and do input/output parsing/validation. Also keep headers like X-Csrf-Token and X-Frame-Options in mind. Not sure about something ... take a look at the OWASP site. 
If he's talking about "overhead" then he's making a performance argument. No?
Did you know `(?x)` exists, right? https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#COMMENTS
 Yes but only because he said that asserts are something that can be disabled on the vm: -Dscalac.args=-Xdisable-assertions Allowing them to be virtually non-existent. Obviously that will have performance impact. You might have misunderstood me but the question in hand is the fact that one can be disabled while the other cant.
So why does he want the ability to disable them? You said he said "zero cost". That suggests it's about performance, doesn't it? I can't imagine he would have any other reason. 
&gt; Scalameta looks nice. I contacted Scalafmt which uses scalameta and they implied that an explicit typing feature would not work because "scalafmt doesn't have the semantic information to implement this feature". Scala meta requires an import and I don't even know if it has this functionality. Did you follow their links to https://github.com/scala-ide/scala-refactoring ? Seems like that's the library you want. If you do end up parsing yourself I'd recommend at least looking at scala-parser-combinators rather than regexes. But you're surely going to end up with the same problem as scalastyle: parsing isn't enough to tell you the semantic information you need to add types.
&gt; the amount of boilerplate for writing web services with akka-http is higher Not true IME, especially if you're really using Scala functionality. akka-http is much better at the whole "manage effects with `F[_]` types" - e.g. you can do database-session-in-view with something like doobie in a principled, refactor-safe way but with very little code overhead, by defining a meta-marshaller for `ConnectionIO`. &gt; also you kind of mix the routes, which I'd argue is "configuration", into your code, while with Play you put it in your routes-file This is what I hate the most about Play. Drawing a distinction between code and configuration is wrong IMO, but with routing especially it's really valuable to just be able to factor out common parts using the normal rules of Scala, because your routes are just normal Scala code.
I agree with most of your impressions (can't speak one way or the other about Spring Boot, and I think the issues of incompatibility are overstated). If you do go with Spring I would strongly recommend using constructor injection since it lets you write much more idiomatic scala classes: class MyClass @Autowired() (myDependency: MyDependency) { ... } is so much nicer (and easier to test) than class MyClass { @Autowired var myDependency: MyDependency = _ ... } I use Spray (which I understand akka-http is the successor to) for REST APIs and Wicket (Java - there's a wicket-scala project with helpers to make it easier to use) for server-side HTML UIs. Spray I like because there's very little magic: everything's just plain old Scala that you can reason about as ordinary Scala, which allows a very high level of abstraction when you need it; also I like that it has a strongly typed model of HTTP (so e.g. `Content-Type` is a class, that accepts a well-typed argument like `application/json`, so you can't make a typo in either of them, or mix up the parameters to `Content-Type` with those to `Cache-Control` - you can fall back to custom strings if you need to, but the default way is much safer). Wicket I love because it has a truly object-oriented way of building UIs: you build small self-contained components and then bigger components out of those, so you end up with these really nicely structured, testable UIs.
Just checked to be sure- runs fine for me, prints the correct output. The file is read through at the declaration of sortedChars, and doesn't need to remain open afterwards.
Or better yet, what if it was a web site. You uploaded your sbt project to the web server, then the web server cloned the project with explicit type annotations in all the files and sent the type annotated version of the sbt project back to the user in a zip file. The web site could provide a GUI with a checkbox for customization, as opposed to using a command line utility with command line options.
Ah yeah, missed the `.mkString`.
Mutable doesn't necessarily mean mutated, depending on the particular Java use case. I guess the main point is I find the ScalaZ `Foldable` or `Traversable` interfaces cleaner and easier to implement than a scala collections based wrapper. Also with a wrapper you have to worry about when it gets unwrapped, which is a problem you don't have with a typeclass.
If you're going down this route (which I would say is very much jumping in at the deep end; it's where you want to end up, but maybe not the best place for a scala noob to start), http://www.slideshare.net/GaryCoady/http4s-doobie-and-circe-the-functional-web-stack may be helpful.
I think it is valuable because it means that if somebody knows the rest api, then they know this api, and vice versa. It also seems non-arbitrary and non-surprising. However, most users probably would want to use the service at a higher level, and don't want to go learn the rest api, so I think a good approach might be to expose two levels of api, the above, low level one, and a higher level one that handles most of the common use cases, and internally uses the low level api, but looks nothing like it. As for stubbing, it would be really easy if I used something like Spray or Akka-Http, and passed in a `sendReceive: HttpRequest =&gt; Future[HttpResponse]` constructor parameter. In tests, one can pass in any stub, and in the real application, the real sendReceive function gets passed. Not sure how I would do this in Play-WS though, since that's what I was planning on using.
You can't, you just can't. ScalaMock can only mock classes with a default (i.e, no parameters) public constructor: https://github.com/paulbutcher/ScalaMock/issues/56 This was supposed to have been fixed in December 2014 (!): http://scalamock.org/roadmap/ &gt; ScalaMock 3.3: Mocking case classes and classes without default constructor &gt; &gt; ScalaMock 4: mocking final classes and classes with final methods or private constructors I would avoid using ScalaMock in any project. It seems to be abandonware by now and, quite frankly, it has so many limitations that renders it basically useless for any real-world, practical use cases (looks great for simple, artificial examples, though). 
 Do you want to compare key-value pairs? I thought we used to compare keys and the keys refer to values. 
Why not provide a real example of what you're trying to do? That'll lead to, I suspect, suggestions on how to concretely, via that example, improve your test(s) by *not* using mocks.
I would use Play. We have started using spray/akka-http but reverted to Play. Play is more of a framework and will have a lot of things right out of the box. Spray is more of a library, and in my opinion, it's not a great library for a Scala noobs to use. You have to figure out what to import or what traits to extend so you won't get those unmarshallable/marshallable errors, and other "magic" it seems to be doing with implicits. Also, the way routes are organized is too flexible. It's not as straightforward as Play where you can write out the URI and which Controller it goes to. Sure, with simpler routes, spray is fine, but if you have to add your own custom directives and routes, it can get messy. The way you organize your routes can have undesirable effects if you don't think about how the routes are structured. 
You need both semantics for different operations, which is the problem.
 And can't we implement methods for those cases?
I contacted IntelliJ and this what they had to say... 1. You can press [Alt + Enter](https://www.jetbrains.com/help/idea/2016.1/working-with-scala-show-type-info-action.html#invoke_show_type) on a variable to toggle explicit types and [Alt + =] on a highlighted expression to get the type of the expression. 2. You can set the [IDE Settings](http://s33.postimg.org/7cc73shdb/type_Annotation_Settings1.png) to control what types are explicit and you can [enforce it](http://s33.postimg.org/sq54b8ulr/type_Annotation2.png) like [this](http://s33.postimg.org/cg0pjqyun/enforced.png). 3. In addition to [features for implicits](https://www.jetbrains.com/help/idea/2016.1/working-with-scala-implicit-conversions.html#invoke_implicit_conv), they are working on a code [transformation/desugaring](https://youtrack.jetbrains.com/issue/SCL-10327) feature.
I probably not going to be worried to much about view logic. Most of the apps I've built are html/js -&gt; some backend (90% of the time Java). I'll definitely check into Slick, and as far as HATEOAS goes, I'm most familiar with HAL, and that's not something I've really in Scala. So, I was really just throwing that one out there to see if anyone had any generic recommendations. I really appreciate the feedback.
How mature would you say Scala.js is? It seems these days that everything js has a transpiler for everything. lol
Is there really a big difference (type mismatch) here? Do they not implement the same interface/type match? Maybe I'm still in Java land here. 
Definitely something to think about here. I've never been a fan of Node, but transpilers do make js more manageable (I'm a big fan of both Elm and Cljs). How does this scale, though? I do a lot of enterprise work, and I'd like to put together a bevy of small projects I can present to leadership hoping to refactor some of our components into Scala (where it makes sense, naturally)
Yea, constructor injection is the only way to roll. I'm not much with the die hard "if you use field injection your objects will suffer from slow death and failing dependencies" crowd, but it's an easy pattern to follow that will show you when a class is "doing too much". The last project I was on had beans with 15+ dependencies wired in. It was a nightmare. Spray has been added to the list, and Wicket too. The last server side ui framework I played with was Thymeleaf. 
I don't know, they have their place. What I like about them is they're common solutions to a problem. When I pick up a new language, I like to first grok the syntax, constructs, and rules, and then pick a problem I understand and see how people are solving that problem with my language. Frameworks are an easy way to do that. 
I don't know, they have their place. What I like about them is they're common solutions to a problem. When I pick up a new language, I like to first grok the syntax, constructs, and rules, and then pick a problem I understand and see how people are solving that problem with my language. Frameworks are an easy way to do that. 
&gt; implicits are... new. I'm still groking the how/where/why part of those. Yeah. In this case, it's mostly so you can have an arbitrary Scala case class and say `.asJson` or something like that on it directly, even though Scala case classes themselves don't have an `.asJson` method, and have it work, or conversely, have some JSON and say `.decode[Person]` and get a `Person` object. The general pattern is often called "API enrichment" or "API bedazzlement," and uses an approach called [typeclasses](http://www.cakesolutions.net/teamblogs/demystifying-implicits-and-typeclasses-in-scala) to do this kind of after-the-fact API extension. It's very powerful, but does offer kind of a speed bump in the learning curve. :-) &gt; I've always been a _paddle out to the middle of the ocean, grab a big rock, jump in, and try to make my way back to the surface before I drown_ kind of person. :) Since aversion therapy seems to be your thing, I think you'll enjoy learning Scala. :-)
Can you give some advice, total noob question here, on where you should install the play framework on a Mac? I read the description at their site warning not to put it in /opt or /usr/local but I can't decide where to put it and I always get concerned with nix locations. I understand that this might just be totally up to me, but I'd like to install somewhere that is sort of standard for production installs, at least what some devs would do on their local machine.
Mockito has subtle issues with mocking functions in scala, and matching deep mocks. Although I like having a no mock mentality, when working with java devs on a scala project that doesn't go very far. What library would you suggest using for a mixed-dev team?
We can, but if you have two values that compare equal but some method call on then returns different results, that's very hard to reason about and so difficult to maintain. i.e. a hack. 
Seemed like a fun little challenge. [Here's my original](https://gist.github.com/anonymous/d406f08b4d3ca8ea2c153f5d56c02701) and [with a bug fix](https://gist.github.com/anonymous/0b1c75fac7a81e099061a030aa2fb761). And [with support for negative numbers](https://gist.github.com/anonymous/bffd2b975a1c9151363f2ddae2d2c54e), for funsies.
Yes that's in ActionBuilder. How is Action.async calling the async in ActionBuilder?
It's [scala xray](https://github.com/harrah/browse/blob/master/src/main/scala/BrowsePlugin.scala#L1). It would be pretty cool if you could revive this project. Instead of showing types when you hover with the cursor, you could show them next to it's owner.
Dope &gt;&lt;, I need to learn to calm down. That explains it perfectly. I need to get more used to the idea of companion objects. Knowing when to use/look for them is still a bit foreign to me. Thanks!
&amp;nbsp; I would, but with my IDE I can press "Alt + Enter" to [toggle type annotations](https://www.jetbrains.com/help/idea/2016.1/working-with-scala-show-type-info-action.html#invoke_show_type) on a variable on/off. I can also press "Alt + =" to show the type of any variable or expression as a tooltip without adding the type annotation to the source code.
Side note: last time you helped me, my GitHub project got 41 stars. I'm usually pretty bad at being thankful, but at a time when I was futsing around with sonotype, you actually saved me a good afternoon of frustration.
Thanks! I appreciate that.
Thanks! I see your flair references Scala kata. I had done some of those in a github project. I didn't realize there was a separate website. \* Oh, lol, and it's not even the same thing as the katas I had in mind. I'm a moron sometimes.
In general, Scala code is affected by Java API [Exceptions](https://docs.oracle.com/javase/7/docs/api/java/lang/Exception.html). So if you’re accessing an Arrays you can expect [ArrayIndexOutOfBoundsException](https://docs.oracle.com/javase/7/docs/api/java/lang/ArrayIndexOutOfBoundsException.html), and if you are accessing a File you can expect [IOException](https://docs.oracle.com/javase/7/docs/api/java/io/IOException.html), for example. &gt; writing my own exceptions As a rule of thumb, avoid writing Exceptions and null in functional code except when you have to deal with them from other people’s APIs. &gt; what exception to process in a Try monad Firstly, refer to the API documentation of the methods you’re calling. For example, if you try to access a Java Array using Scala’s `array(element)`, the documentation for [`Array.apply`](http://www.scala-lang.org/api/current/index.html#scala.Array@apply(i:Int\):T) says “Exceptions thrown: ArrayIndexOutOfBoundsException”. If possible, prefer to use non-Exception throwing methods.
if your on a mac use homebrew to install activator and scala it will sort out the $paths for you.
&gt; Thoughts? Sounds bad. I think it’s intentional that `require` is not disabled by `-Xdisable-assertions` (only `assert` and `assume` are). From the point of view of your method, assumptions &amp; assertions are your responsibility and can be covered by your unit tests, while requirements cannot — they reflect ‘external’ errors in the runtime environment and are thus mainly relevant at runtime. Of course, if it is a matter of input validation then it would be better to return a Type than can express success/failure.
I would say that's it's very mature. I've got a couple of apps in production running Angular + Scala.js, and about 5 hobby projects using Scala.js, including: 1. https://github.com/ldaniels528/means-js-todomvc 2. https://github.com/ldaniels528/means-js-socialized 
Is there a static-analysis tool that would **restrict the usage** of specific libraries or **language features**? * **Example 1**: let's say I'm a tutor and giving an introductory Scala course and want to unleash the power of Scala in a "controlled" way. Just for the fun of it I want to prevent the usage of pattern-matching and Option//Try, only to allow it later. * **Example 2**: I am a lead developer with an heterogeneous team (different expertise levels) and want to prevent advanced team members from introducing "clever" (?) Scala code into the codebase (macros, type-level programming, scalaz, etc.) that the other members would have difficulties dealing with. Basically the idea is to allow "sub-sets" of Scala in a controlled fashion (like the [Racket Scheme](https://racket-lang.org/) environment does). I am aware of [Scapegoat](https://github.com/sksamuel/scapegoat) and [Scalastyle](http://www.scalastyle.org/), but I don't think they can be used for this...
It's production ready. There are lots of languages that can be compiled into javascript - python, c++, java.. and then there are js++ languages (typescript, coffeescript). IMO scala.js is the best of these. None of the other properly compiled languages have the interoperability convenience of scala.js and the js++ languages leave you with many of js's flaws.
What about them?
Idiomatic Scala generally doesn't use exceptions (or `Try`) - instead use `Either` (or one of the improved replacements from ScalaZ or Cats) and monadic (`for`/`yield`, `flatMap`) or applicative composition. When you need to make a library call that might throw, use `catching(classOf[PossibleException) either callThatMightThrow()` (from `scala.util.control.Exception._`) to convert it to `Either`. With `Either` your possible errors can just be a normal `sealed trait`/`case class` hierarchy. (To answer the question, I'm sure the existing exceptions will be in the standard library scaladocs; the standard library also uses a lot of Java standard library exceptions like `IllegalArgumentError`)
The scala standard library grew organically over ten years and retains many things that turned out to be bad ideas. I don't recommend using either of those classes.
If you saw my example we are talking about class construction parameter evaluation. If certain conditions are not met, the class should never be able to be constructed, thus, the require expressions. I could wrap the construction of the class and other code in a Try block and deal with the failure in a later phase though. One thing is certain, i do not think leaving a require/assert open to throw exceptions and not wrap them around something that can deal with them will be extremely painful in the future.
My comment was about preferring require over assert. I think it doesn’t make sense to use assert and then disable it, when you actually do want the runtime semantics of require. On your second point ‘if certain conditions are not met, the class should never be able to be constructed’ then you can provide a method that returns Some(constructedThing)/None depending whether the requirements are met or not.
Maybe a really weird reference to Dotty?? (Scala 3.0)
Thanks for the pointer. Will look into this.
Could you elaborate a bit on why you think one should not use `Future`?
seems I should really get into scalaz one of these days \^\^ thanks!
Java is much more simple and straightforward, has better backwards compat (enterprise love), documentation and support, so I wouldn't say it's bad. Also, Scala is playing on Java's home field, so IMO knowing Java is somewhere from useful to necessary to be a good Scala dev. Languages fully dedicated to some concepts, like Haskell to FP, are better for learning than mix and mash. In fact, I would consider learning Scala as first language harmful.
Ah, so you discovered I did barely any testing :). Should be a quick fix to choose the minimum between the max index and the amount to show. Should also validate positive arguments and that max is greater than min. I can do that when I am not mobile anymore. Edit: [fixed](https://gist.github.com/anonymous/0b1c75fac7a81e099061a030aa2fb761) and [lifted the positive number requirement](https://gist.github.com/anonymous/bffd2b975a1c9151363f2ddae2d2c54e)
Hi m50d. Thanks you for your feedback! I welcome the opportunity for us to learn from each other. Please see below for my responses. &gt; I don't think it's a new idea - it's an existing pattern, no? E.g. Scrooge uses this approach to allow using the same interface definition for sync and async (I pushed this a little further in my fork: https://github.com/VisualDNA/scrooge). My apologies but one of the problems in Scala is socialization of ideas and I certainly have been guilty of re-inventing wheels. I'm especially unfamiliar with Twitter-Scala-verse. That said, on first glance at scrooge, this doesn't appear to be the case. Scrooge is a thrift code generation library, perhaps you can point me at something specific in the library that uses the same generic monad service pattern? &gt; The implementations can only be effect-generic like this when they don't actually contain any effecty logic, i.e. only the "middle" layers of the stack, right (orchestration/business logic)? I don't know how common such pure-orchestration services actually are. Effect-generic traits with concrete implementations seem like a more common use case, though obv. the fully generic version is cooler. It has been my experience that the "middle" of the stack is the most prone to library lock-in (e.g. dependencies on scalaz or Akka, etc). This reduces the portability of this code significantly. I have yet to see a solution for writing business logic in a way that is portable across the entire Scala ecosystem (and can be effect-free). Additionally, while I don't cover it in the readme, lower level "effect" services can absolutely be made into effectful services. See [SqlDriver.scala](https://github.com/lancegatlin/effectful-demo/blob/master/src/test/scala/effectful/examples/effects/sql/SqlDriver.scala) for a non-trivial example. &gt; I would incline to use either ScalaZ MonadPartialOrder constraints or some kind of union-of-free-coproducts approach (with FreeK or my Paperdoll or a project on those lines) for dependent services, so that you don't have to use exactly the same monad in all the services that a given service calls but rather can use different monads as long as the monad for this implementation "contains" the monads that every service it depends on uses. Again I don't go into depth about it, but I provide the [LiftService](https://github.com/lancegatlin/effectful-demo/blob/master/src/main/scala/effectful/LiftService.scala) typeclass for achieving this effect. The exact details are intentionally hand-wavy in this demo. I plan on creating a version of the library for scalaz, so I'll revisit your suggestion then. Thanks! &gt; Fundamentally the biggest issue is whether this is actually useful. Since the only things you can write in this fully generic style are orchestration/business logic, you would be unlikely to use it in a library, so being generic in which framework/environment you depend on isn't actually useful (since the concrete project will only depend on one). We will have to agree to disagree here. The un-reuseability of Scala code locked into library dependencies has been one of the largest stumbling blocks I've encoutered over the years I've been coding Scala now. I would love to reuse some of the code I've written for clients over the years but each new job is invariably using different libraries and my old code just isn't portable. I end up coding something very similar for the new set of dependencies. Also I look at libraries like [Silhouette](https://github.com/mohiva/play-silhouette) and wistfully sigh when I have to rewrite something there when I'm using something other than Play. &gt; It adds overhead to your code, and how much value does it actually give compared to working in a fixed monad (possibly a transformer stack or free coproduct or similar)? No, abstracting the monad to a generic adds no additional runtime overhead. If you will inspect the code a bit closer it adds no runtime burden beyond want is already incurred by using monadic coding in the first place.
[software requirements](http://www.bpminstitute.org/sites/default/files//Adams_BPMreqsFig1.jpg) * just kidding the negative pagination is pretty cool :P
&gt; just be a normal sealed trait/case class hierarchy [S] - extendability
https://m.reddit.com/r/scala/comments/3zofjl/why_is_future_totally_unusable/cyns21h
Not sure if you're still interested in this topic, but I'm developing applications via Scala.js on Node. I've developed a rather large set of bindings as well for anyone else who's considering doing the same (https://github.com/ldaniels528/means-js). To answer your questions: 1. I'm also new to the Node ecosystem; however, IMHO the learning curve for learning Node is significantly lower than many native Scala frameworks like Lift, Spray and Play. 2. Yes, but if you're going to use Node libraries on the front-end, you'll likely need something like Browserify (http://browserify.org/). Otherwise, you can just use Bower. 3. True. If you want to see a simple example go here (https://github.com/ldaniels528/means-js-todomvc). For a more sophisticated real world example, go here (https://github.com/ldaniels528/means-js-socialized). 4. The SDK I've built allows you to replace most callbacks with Promises (Futures), which allow you to use `for` comprehensions, etc. 5. I've provided many of the facades that you'll need, and I'm adding more on a weekly basis. 6. I've already implemented MongoDB, MySQL and Cassandra. More are coming... 7. Yes, today that's true, but I'm hoping to change that.
The following is the output from the same application (means-js-socialized - https://github.com/ldaniels528/means-js-socialized) running with two distinctly different backends. The first uses MEANS.js (AngularJS + Node + Scala.js) and the second uses the Play Framework (AngularJS + Play + Scala). The results as you can see, are in some cases very close, but in other cases, Node beats out Play by a nice margin... MEANS.js (AngularJS + Node + Scala.js) `Server now listening on port 8888` `[node] application - GET /api/session/574612f3000cd443065801d6 ~&gt; 200 [20 ms]` `[node] application - GET /api/user/5633c756d9d5baa77a714803 ~&gt; 200 [5 ms]` `[node] application - GET /api/groups?maxResults=5 ~&gt; 200 [6 ms]` `[node] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [5 ms]` `[node] application - GET /api/questions ~&gt; 200 [6 ms]` `[node] application - GET /api/exam/56dc8f69e71b479736b5cab2 ~&gt; 200 [4 ms]` `[node] application - GET /api/events/user/5633c756d9d5baa77a714803 ~&gt; 200 [5 ms]` `[node] application - GET /api/sessions?userIDs=56340a6f3c21a4b485d47c55&amp;userIDs=5647c480e7aabcedf6dfb4b4 ~&gt; 200 [12 ms]` `[node] application - GET /api/exams ~&gt; 200 [54 ms]` `[node] application - GET /api/notifications/user/5633c756d9d5baa77a714803/true ~&gt; 200 [3 ms]` `[node] application - GET /api/examresults/user/5633c756d9d5baa77a714803 ~&gt; 200 [4 ms]` `[node] application - GET /api/exam/56dc8f69e71b479736b5cab2 ~&gt; 200 [2 ms]` `[node] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [12 ms]` `[node] application - GET /api/posts/user/5633c756d9d5baa77a714803/newsfeed ~&gt; 200 [1 ms]` `[node] application - GET /api/posts/attachments/5743cbe2a662e780141ad6e6 ~&gt; 200 [15 ms]` `[node] application - GET /api/posts/attachments/5743cbe2a662e780141ad6ea ~&gt; 200 [14 ms]` `[node] application - GET /api/posts/attachments/5743cbe1a662e780141ad6e4 ~&gt; 200 [13 ms]` `[node] application - GET /api/posts/attachments/5743cbe2a662e780141ad6ee ~&gt; 200 [8 ms]` `[node] application - GET /api/posts/attachments/5743cbe2a662e780141ad6e8 ~&gt; 200 [23 ms]` `[node] application - GET /api/posts/attachments/5743cbe2a662e780141ad6ec ~&gt; 200 [26 ms]` `[node] application - GET /api/questions ~&gt; 200 [12 ms]` `[node] application - GET /api/exams ~&gt; 200 [6 ms]` `[node] application - GET /api/examresults/user/5633c756d9d5baa77a714803 ~&gt; 200 [5 ms]` `[node] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [5 ms]` `[node] application - GET /api/sessions?userIDs=56340a6f3c21a4b485d47c55&amp;userIDs=5647c480e7aabcedf6dfb4b4 ~&gt; 200 [5 ms]` `[node] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [4 ms]` `[node] application - GET /api/sessions?userIDs=56340a6f3c21a4b485d47c55&amp;userIDs=5647c480e7aabcedf6dfb4b4 ~&gt; 200 [3 ms]` Angular + Play + Scala `[info] application - GET /api/session/574612f3000cd443065801d6 ~&gt; 200 [374.1 ms]` `[info] application - GET /api/user/5633c756d9d5baa77a714803 ~&gt; 200 [32.4 ms]` `[info] application - Registering web socket actor for session # 9a27aec4-a203-46ce-bb6e-2e2a30bfb03f...` `[info] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [21.9 ms]` `[info] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [10.9 ms]` `[info] application - GET /api/sessions?userIDs=56340a6f3c21a4b485d47c55&amp;userIDs=5647c480e7aabcedf6dfb4b4 ~&gt; 200 [42.1 ms]` `[info] application - GET /api/groups?maxResults=5 ~&gt; 200 [166.1 ms]` `[info] application - GET /api/events/user/5633c756d9d5baa77a714803 ~&gt; 200 [191.8 ms]` `[info] application - GET /api/posts/user/5633c756d9d5baa77a714803/newsfeed ~&gt; 200 [233.8 ms]` `[info] application - GET /api/notifications/user/5633c756d9d5baa77a714803/true ~&gt; 200 [150.0 ms]` `[info] application - GET /api/posts/attachments/5743cbe2a662e780141ad6e6 ~&gt; 200 [9.2 ms]` `[info] application - GET /api/posts/attachments/5743cbe2a662e780141ad6e8 ~&gt; 200 [9.3 ms]` `[info] application - GET /api/posts/attachments/5743cbe1a662e780141ad6e4 ~&gt; 200 [9.3 ms]` `[info] application - GET /api/posts/attachments/5743cbe2a662e780141ad6ec ~&gt; 200 [9.4 ms]` `[info] application - GET /api/posts/attachments/5743cbe2a662e780141ad6ee ~&gt; 200 [8.5 ms]` `[info] application - GET /api/posts/attachments/5743cbe2a662e780141ad6ea ~&gt; 200 [9.8 ms]` `[info] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [4.7 ms]` `[info] application - GET /api/posts/user/5633c756d9d5baa77a714803/newsfeed ~&gt; 200 [4.1 ms]` `[info] application - GET /api/questions ~&gt; 200 [56.8 ms]` `[info] application - GET /api/exams ~&gt; 200 [48.2 ms]` `[info] application - GET /api/examresults/user/5633c756d9d5baa77a714803 ~&gt; 200 [52.0 ms]` `[info] application - GET /api/questions ~&gt; 200 [33.7 ms]` `[info] application - GET /api/exams ~&gt; 200 [16.1 ms]` `[info] application - GET /api/examresults/user/5633c756d9d5baa77a714803 ~&gt; 200 [7.5 ms]` `[info] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [6.3 ms]` `[info] application - GET /api/posts/user/5633c756d9d5baa77a714803/newsfeed ~&gt; 200 [3.9 ms]` `[info] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [6.9 ms]` `[info] application - GET /api/sessions?userIDs=56340a6f3c21a4b485d47c55&amp;userIDs=5647c480e7aabcedf6dfb4b4 ~&gt; 200 [14.9 ms]` `[info] application - GET /api/user/5633c756d9d5baa77a714803/followers ~&gt; 200 [6.3 ms]` `[info] application - GET /api/sessions?userIDs=56340a6f3c21a4b485d47c55&amp;userIDs=5647c480e7aabcedf6dfb4b4 ~&gt; 200 [3.0 ms]` 
What's the proper way to format a chain of method invocations when I want to have them on individual lines? obj. m1. m2. m3 or obj .m1 .m2 .m3
The link provided by OP wasn't too clear (at least to me). A better page might be: https://olafurpg.github.io/scalafmt/ On Point: Looks worth trying out, thanks. 
I'm glad you like it :)
I prefer the second but have seen both in the wild. 
I wanted to ask... why don't u use an IDE (if you don't mind)?
That's unfortunate, clearly a ton of work has been put into the project. Would be nice if the author recognized your contribution to the code base. With the rise of OSS and Github code "reuse" without attribution has also risen.
&gt; Java is much more simple and straightforward Except when you need to write something - then it'll look like cthulhu. &gt; has better backwards compat (enterprise love) Java EE is one of the worst thing I've seen in programming - every ee user constantly mixes anti-patterns and tries hide their failures by bashing agile, tdd etc. while allocate far more resources and spending a lot of time with useless meetings. This is java ee - "we've created a mess what we don't want to touch, so hire someone who'll work with it". &gt; Languages fully dedicated to some concepts, like Haskell to FP, are better for learning than mix and mash. In fact, I would consider learning Scala as first language harmful. There are universities where students' first language is Scala - I wouldn't say it's bad since they won't need to deal with the boilerplate while getting familiar with the FP approach instead of sinking in the fizz-buzz enterprise. For example C++ is a lot harder than Scala and it's the first programming language in many universities - C++ still contains a lot more boilerplate than Scala, lacks a simple standard library and is messy.
Well trivially `_2` in your approach is an example. If `a == b` then someone refactoring could very reasonably replace a call to `a._2` with a call to `b._2`, but with your approach that would break everything. Using a `Map` with `Unit` to implement `Set` is not a hack. It's a well established technique: you implement the general case, and then recover the simpler cases by using `Unit` or `identity` or `Id`. Compare how ScalaZ implements `Reader` and `Writer`, or most things Edward Kmett writes (e.g. recursion-schemes)
&gt; Well trivially _2 in your approach is an example. If a == b then someone refactoring could very reasonably replace a call to a._2 with a call to b._2, but with your approach that would break everything. Why would it break everything? How would the method look like? I'm interested in a real example because this doesn't make any sense. I want real examples on why we can't implement a TreeMap with a TreeSet. &gt; Using a Map with Unit to implement Set is not a hack. It's a well established technique: you implement the general case, and then recover the simpler cases by using Unit or identity or Id. TreeSet is a general case - TreeMap is specialized. All Maps are Sets except that their values are keys to values.
I'd definitely prefer the second.
Have you spoken to any legal people? There are organizations like the software freedom law center that help enforce OSS licenses. As the copyright holder you're the only person in a position to do anything about infringement of your code.
For option 2 you will want to do shapeless typeclass derivation - there are plenty of examples around the internet. Something like: trait DescribedCodec[A] { val description: MyDescription val codec: JsCodec[A] } object DescribedCodec extends LabelledProductTypeClassCompanion[DescribedCodec]{ override val typeClass = new ... { override def emptyProduct = new DescribedCodec[HNil] { val codec = ... //may be easier to implement your own JsCodec (or use spray) //rather than figure out how to hook into argonaut val description = MyDescription(type="object", fields=Seq(), required="true") } override def product[H, T &lt;: HList](name: String, dch: DescribedCodec[H], dct: DescribedCodec[T]) = new ... { val codec = ... val description = dht.description.copy(fields = dch.description +: dct.fields) } } // implement the "primitive" cases yourself implicit def int = new DescribedCodec[Int] { val codec = ... val description = MyDescription(type="int", required=true) } implicit def option[A](implicit dca: DescribedCodec[A]) = new DescribedCodec[Option[A]] { val codec = ... val description = dca.description.copy(required=false) } } If you want access to e.g. the class name when generating the description then you might have to do the derivation "by hand" rather than using `LabelledProductTypeClassCompanion` (since `project` doesn't get the arguments you'd want) but that's reasonably simple - just inline the bits of that class you were using and then go from there. Have a look at spray-json-shapeless for an example of doing this kind of thing. (If you don't mind using spray-json instead of argonaut I'd even suggest forking it and starting from there, then you have the project structure already and just have to add in the description field).
Is it a good idea trying to develop a game on Scala? Yet I have some experience on programming, I never created a game and never used Scala. I'm willing to learn.
Games are hard. If you're going to do it I would definitely start with clones of simple existing games. Scala is a good general-purpose language, so fine for a game. Some people will be concerned about GC pauses but IME those concerns are very much overblown on modern hardware. That said, there may not be as many game-oriented libraries available on the JVM - or rather, you'll probably end up having to use immature bindings with some rough edges.
I'd always suggest EasyMock if you have to mock. The explicit replay step means you can at least tell what's being called that wasn't mocked (e.g. default parameter definitions).
It sounds like we need software to get the license for a given project, and find uses of its code in other projects with noncompliant licenses.
It leaves xml literals alone.
I can't find any license, neither as a separate file, nor as part of the source files. And all the sources are their on Github. And there are no binaries. I am no lawyer, but I think as long as there is only a compilation of source files, the LGPL doesn't apply. For my understanding it only kicks in when one distributes binaries. Are you really sure there is a license violation?
[Here](http://tpolecat.github.io/2015/04/29/f-bounds.html) is a discussion. The tl;dr is that you want a typeclass.
Thanks for the question. My apologies but this is the first I've heard of "finally tagless". A casual google search turns up these two links: * https://pchiusano.github.io/2014-05-20/scala-gadts.html * http://okmij.org/ftp/tagless-final/ Please feel free to recommend other sources. From that quick look I'd say that at the trait level declaration both ideas are the same. They both start with having a trait accepting a generic monad parameter and wrapping all method return types with it to allow using the trait with any possible monad. But their purposes and audiences are different. finally-tagless is about writing interpreters without an ADT and targets a Haskell/deep Scala FP/academic audience. effectful-services is about combining service-orientation programming patterns with generic monadic programming to write portable, library-independent pure code and targets general FP Scala developers.
There is a ton of issues with this project. One is to copy and paste other people source code and then throw out binaries under a "BSD license", this is what the guy did when the project was still on Google Code. It's ok, we can all make mistakes. So I tried to talk to him on the mailing list of Scalalab. He was unregenerate and refused to accept that he cannot simply copy code, modify it and release under a new license. The LGPL clearly states that you can release derivate works under a compatible license. It's not that my library is super duper extensive or particularly original, but there was a genuine disregard for intellectual work, which is the more concerning giving that this guy works in academia and wants to build a tool for academia. I just submitted a grant application and had to fill out an entire sheet detailing how I plan to ensure compliance with European ethics in science standards, going to the length of dealing with plagiarism, misattribution of intellectual work etc. You simply cannot take the stance of this guy in academia, it's very wrong. Also the whole action had the flavour of vandalism, basically we was looking around for all the bits and pieces he thought should go into Scalalab and pasted them all together in the source directories. Not a single license is mentioned or reproduced. It even contains (or contained at the time, I don't know about the current state) problematic code taken from Numerical Recipes which clearly states that it cannot be redistributed. In the end I gave up trying to argue.
After talking to the author of Scalalab directly without tangible results, I contacted Google Code where the project was hosted before. I never heard anything back from them. I also don't have energy or resources to make a big case of this.
&gt; Future and Task represent orthogonal concerns, neither is "bad". Future's are eager, and automatically memoized, where as Task's are lazy and need to be executed (this also makes memoization harder in specific circumstances with Task, but that is another debate). They're not orthogonal, because Future *also* represents async. Which is again the problem; memoization is a separate concern that should be handled separately (as it is when using Task). &gt; Saying they support the same use cases is reductionist in reasoning Reasoning should be reductionist, no? &gt; refactoring is only one concern (out of many) when it comes to using a library. It's one concern but it's a big one, probably the biggest. &gt; plenty of people use Exceptions and Future, and there are many valid use cases for that. There may be valid use cases for exceptions. I don't believe there are for Try, or Future. &gt; There are also valid uses for scala enumeration (i.e. if you are concerned about memory usage, each case in an Enumeration only takes up a single primitive value and doesn't need to create a class, it also maps nicely to bit/int type values for this reason). That use case may be small, but its definitely not comparable to exceptions or Future. Maybe. Certainly it's not a big enough use case to justify it being added to the standard library if it wasn't there already.
Hey I know you... We are using it for logging as well as DI. I've used writer for it in the past, see https://github.com/localytics/wdj - but as the written type gets more complicated the monoid for it gets weird. I've thought about using RWS to designate certain parts as read-only, and I suppose there may be some things that are better encoded in a monoid instance for the write side, but it seemed like adding extra complexity for little gain, especially when the use of the monad stack in question is prominent throughout the majority of our codebase. Combining the lens also makes the interactions really concise and consistent, and things start looking more familiar to those coming from an imperative background.
See my below comment re: Writer. I've used it in the past but complex log types result in complex monoids. And using a writer doesn't prevent your example of lowering severity, unless you encode that in the monoid. State is funny in that it does look like you can "modify" it, but IME it's just more practical than an ever growing monoid. I also take some comfort knowing that even though this looks like mutation, it's still just a value threaded through function calls.
&gt; Why would it break everything? How would the method look like? I'm interested in a real example because this doesn't make any sense. I want real examples on why we can't implement a TreeMap with a TreeSet. I mean if we implemented it your way, `TreeMap#get` would be implemented something like: def get(k: K) = underlying.find(Pair(k, null).==).map(_._2) and a maintenance programmer could perfectly reasonably refactor that to def get(k: K) = underlying.contains(Pair(k, null)).option(null) and then all lookups return null. &gt; TreeSet is a general case - TreeMap is specialized. All Maps are Sets except that their values are keys to values. `Map`s are not `Set`s. They're not Liskov-substitutable for Sets (e.g. for any sets `a` and `b`, `a - b ++ b == a`). In contrast a `Set[A]` is Liskov-substitutable for a `Map[A, Unit]`.
 Why not just def get(key: K) : Option[V] = tree.get(key).map(_._2) ~~or~~ ~~def get(key: K) : Option[V] = tree.find(_._1 == key).map(_._2)~~ &gt; and a maintenance programmer could perfectly reasonably refactor that to ... That refactoring is everything but reasonable. And even if someone would choose to implement in that way unittests would still catch that mess, right?
I am using Play with mustache templates with scalate but I don't seem to be able to find the templates. I have tried Ok(scalate.render("/app/views/my_view.mustache", Map("message" -&gt; "hello"))) // Full relative path to the template Ok(scalate.render("my_view.mustache", Map("message" -&gt; "hello"))) // Just the template name And I always get the same error, [ResourceNotFoundException: Could not load resource: [path_i_specified_to_the_template]] All the documentation indicates that it goes in webapp/WEB-INF/... but that folder does not exist (it seems to be an Scalatra path). Is there a way to set up the default folder for storing the templates? On the documentation for using Scalate with Spring-MVC looks like there should be some 'prefix' variable but I just can't seem to find how to set it. Thanks!
&gt; Why not just &gt; def get(key: K) : Option[V] = tree.get(key).map(_._2) Set doesn't implement `get` - why should it? &gt; or &gt; def get(key: K) : Option[V] = tree.find(._1 == key).map(._2) That would inefficiently search through the whole set, as you possibly realised. &gt; And even if someone would choose to implement in that way unittests would still catch that mess, right? If you need more unit tests to achieve the same level of confidence for one implementation than another, that makes it a worse implementation. 
&gt; I've used it in the past but complex log types result in complex monoids. Depends what your entries look like. I wonder if it's possible to apply the same technique - separate the "extend to full value" (a lenslike operation) from the monoid. &gt; And using a writer doesn't prevent your example of lowering severity, unless you encode that in the monoid. I think I would - if you merge two log messages, the severity of the combined message is the higher of their severities. &gt; I also take some comfort knowing that even though this looks like mutation, it's still just a value threaded through function calls. It's the same thing - just it happens to be on the stack rather than named. I am very wary of using state for something that isn't actually mutated, because part of the value of state is that it makes mutation very explicit and obvious, so you pay attention to it when reading the code.
&gt; We can decide what we implement as helpers, right? Sure, we could pollute the interface of our Set with a bunch of methods that don't make any sense to use from Map. Or we could implement Set on top of Map, which you can do just using the public interface. &gt; Or we can just implement the search and we won't need the map(_._2). Set is supposed to be generic, so for `Pair`-specific methods it would need a `=:=` hack. &gt; Yes, I've got the idea from your snippet. Edit: I still don't get the idea why would someone refactor it with 'contains' and why would someone use 'find' by default with 'Pair(k, null).=='. That was the only way I could think of to do it, because Set does not have a natural method for doing something like this, because it doesn't make any sense for a Map. &gt; If "and then all lookups return null." then it would only require the general test case, right? Anyway, ~3-4 UTs are required for all cases. Any way you slice it, code that's easy to break when refactoring is bad. It means either more test coverage or a higher defect rate compared to code that can be safely refactored according to the normal rules of the language.
I don't think it's the same at all. When you are dealing with a named value (take this to mean global) you have absolutely no guarantees about where it came from or where it's going. Completely unrelated code can impact the result of a computation. Introduce concurrency and all bets are off. "Stateful" computations using the state monad are still capable of being pure, and referentially transparent, where referencing a named, mutable value is certainly not. Interesting thought to integrate the lens approach with the writers monoid, would love to see what that approach would look like. Ultimately I found the cost in flexibility to be undesirable over time, hence the move to state.
Sigh. Saying that Future and Task are not orthogonal because they don't handle async is like saying the following expressions aren't orthogonal val number = 1 + 2 val anonymouseClass = new Clazz { def rawr = "dsfsd" } The two are doing completely different things, the only thing they have in common is that they are synchronous. Which is the whole reason why the previous `Future` and `Task` got scapeboated, people were not arguing over "safety", they were arguing about the merits of having a forced lazy `Future` (which is what `Task` is). This is of course fine, but Scala as a language was intentionally designed to have strict evaluation by default. This is for very good reasons, its typically very hard to reason about performance for control flow that is forced to be lazy, and `Task` has had these issues in non trivial code. Its also very difficult to go from forced lazy to strict, as evidenced by Haskell (in some cases its impossible to force strictness due to how certain data structures are created). On the other hand, forcing laziness on a strict language is very easy (hence the existence of stuff like `Task` in the first place). The thing is, when you have `val number = 5`, this is exactly the same as `val number = Future { 5 }` (apart from the fact that one is synchronous and the other is asynchronous). Both are strict (evaluated immediately) and both are memoized. In fact, if you want to make a `Task` without using any library, its easy to do so. `def lazyNumber = Future { 5 }` There you go, there is your `Task`. It is lazily evaluated, and it will get computed every time. The main difference, of course, is that its not forced lazy evaluation, but that doesn't make any real difference for the callee. All `Future` is doing is mirroring what the intentional design of Scala (which it borrowed/inspired from in ML) is having strictness by default. You should watch this very good talk about the guy that created Monix/Monifu (an implementation of `Task` that also works for Scala.js) https://vimeo.com/channels/flatmap2016/165922572. You honestly are conflating very different concepts.
The second way has the huge advantage that when adding or deleting a line: obj .m1 .m2 the diff will also be exactly one line. Or more generally, adding or deleting N lines will result in a N line diff. For the first method, adding or deleting N lines will result in a N+1 line diff.
&gt; Scala's collection library is already polluted with ugly stuff, so we can't make anything worse. Yes we can. If you follow the "boy scout rule" then even the worst codebases quickly get better. &gt; We need a proper wrapper anyway, so why? What do you mean? My point is we shouldn't be adding methods that are specific to `Set[Pair[A, B]]` to the general `Set[A]` implementation, and would have to use a `=:=` hack to do so (but that would have to be how we implemented the helpers that we would need to implement `Map` on top of `Set`). I don't know what you mean about a "proper wrapper" - implementing `Set` on top of `Map` does not require any wrappers, just `Map` and then `Set` whose implementation uses the public interface of `Map`. &gt; If you implement the treemap first then your RBT(or whatever) will hold tuples probably and your treeset will also hold tuples. &gt; Anybody can break any code by refactoring Talk of can vs can't is misleading. Talk about how often they do so and you get a very different picture. Also specific refactors (the word has two different but related meanings) can be made safe 100% of the time. Replacing `a` with `b` when `a == b` is one of those. &gt; your posted implementation is nonsense. Because the whole idea is nonsense. Whatever. Implement it and you'll see.
I do that too, it has the advantage of having a more expressive namespace. You can have Import Fruit._ Banana.blah() Or fruit matches { case Fruit.Apple(size) =&gt; hey() } Also when what you want is an enum-like, I suggest you use case objects. It's not that well known sealed trait Fruit object Fruit { case object Banana extends Fruit case object Apple extends Fruit }
&gt; "Stateful" computations using the state monad are still capable of being pure, and referentially transparent http://conal.net/blog/posts/the-c-language-is-purely-functional . We can regard any language as pure (when written) or impure (when executed); the better question is the extent to which we can reason about it denotationally or operationally. Whether State is declarative or imperative is not a yes/no question, but certainly it's more imperative and less declarative than Writer.
&gt; The two are doing completely different things, the only thing they have in common is that they are synchronous. Leaving aside whether it's a good design decision or not, in the Scala that we have, synchronicity is implicit and pervasive and asynchronicity is explicit and exceptional. 100% of the real-world use cases I've seen for `Future` were using it solely to handle asynchronicity, and its memoization was incidental at best. &gt; The thing is, when you have val number = 5, this is exactly the same as val number = Future { 5 } (apart from the fact that one is synchronous and the other is asynchronous). Both are strict (evaluated immediately) and both are memoized. Right, but that's not a legitimate use case for `Future`. The overhead of forking off a task going to be much bigger than the computation it takes to calculate 5. In the general case `Future` doesn't makes sense for CPU- or memory-bound workloads - the eager evaluation forces `map`/`flatMap` to be implemented less efficiently than for scalaz `Task` (the fancy work-stealing scheduler recovers some of the cost, but not all) - and probably not for local disk I/O either. When we're talking about `Future` we're talking about the world of, realistically, network I/O. I actually agree that there's a lot of value in eager evaluation in terms of making performance easier to reason about - I certainly wouldn't want Scala to become pervasively lazy like Haskell - but in the actual use case for `Future` the concerns are very different from those for general (pure computational) code. &gt; You should watch this very good talk I don't watch videos - if there's a transcript or article I'd very happily read it.
&gt; Yes we can. If you follow the "boy scout rule" then even the worst codebases quickly get better. My statement was sarcastic. And you can't really write good code in a shitty codebase because you'll be restricted to its interfaces and if you try to refactor everything then nothing gonna work because UT is a rare beast nowadays(we are too lazy to write a UT for Set's "get", right?). And the deadlines... The spaghetti should be isolated instead. &gt; My point is we shouldn't be adding methods that are specific to Set[Pair[A, B]] to the general Set[A] implementation, and would have to use a =:= hack to do so (but that would have to be how we implemented the helpers that we would need to implement Map on top of Set). What is that "=:="? Am I supposed to know that? How is it relevant here? &gt; I don't know what you mean about a "proper wrapper" - implementing Set on top of Map does not require any wrappers, just Map and then Set whose implementation uses the public interface of Map. Yes it requires a wrapper because you don't want "get(key: T): Option[(T, Unit)]" and such methods in your TreeSet what you would inherit from TreeMap. By proper wrapper I've meant "class TreeMap[A: Ordering, B](private val tree: TreeSet[Pair[A, B]] = Leaf) { //methods here }". Most probably, you're thinking about the OOPish way to implement a collection hierarchy: "class TreeMap(...) extends Map blah-blah". But I don't like to think with inheritance - it makes the code a mess(and creates an unnecessary overhead for your Set in this case). I like to use typeclasses or nothing. With typeclasses, we need to implement most methods at the type's definition and tie it to a typeclass implicitly. OOP is the reason of "GenTraversableOnce" and such spaghetti particles in Scala's collection library. &gt; Talk of can vs can't is misleading. Talk about how often they do so and you get a very different picture. Why would someone use "Pair(k, null).==", replace "find" with "contains" and "map(_._2)" with "option(null)" in a refactoring? It doesn't make any sense. If it's "perfectly reasonable" for you then I don't know what to say... &gt; Because the whole idea is nonsense. Whatever. Implement it and you'll see. I've done that before and I don't think we should use a heavy collection to implement a lightweight one: your TreeMap contains tuples and you order it by the key, right? Then your Set will still contain tuples. If you want collections with decent performance then you should specialize them properly.
That link is satire and such a definition of pure vs impure is entirely academic. But yes you are right, state is more imperative than writer. You pay for safety with flexibility, and vice versa. Having done it both ways in real production systems, I'm confident where I've settled, but of course each approach has it's merits.
&gt; How is it relative here? In production you use what's given. What? &gt; Adding &gt; def get(key: T): Option[T] = ... &gt; is nonsense? Yes, it's nonsense. What use case would you ever have for that in a normal set? &gt; So, you've mentioned the "hack operator" to express what? Because I'm too honest to say "it's literally impossible to write a method on `Set[A]` that only works when it's actually a `Set[Pair[A, B]]`". It is possible. But it's a bad idea. &gt; I didn't said that I need a method in TreeSet which depends on Pair[A, B] - where did you get the idea? You said we could implement the lookup in the set and avoid having to `map(_._2)`. But we can't do that unless the set somehow knows that it's a `Set[Pair[A, B]]`. &gt; And TreeSet is a concrete type in a normal universe. Wtf? TreeSet is generic, as in, parameterized. &gt; Yes, you're. Because you still want to implement it in the "old way" and you still talk about Liskov substitution and you still want to inherit stuff from TreeMap which won't going to work. Enough of the ad hominem. You know nothing about me, but even if you were right it wouldn't matter. &gt; Who had those requirements? Only you because I've never said it. Then why are we having this conversation at all? If we don't need to implement Map on top of Set then we don't need `Pair(k, null)` or any of it. &gt; Replacing "find" with "contains"? I hope you do know that find is "(pred: T =&gt; Boolean): Option[T]" and contains is "(elem: T): Boolean" at sane places, right? Read the whole line. `find(Pair(k, null).==).map(_._2)` is equivalent to `contains(Pair(k, null)).option(null)` for any sane `Pair`. &gt; Putting Map on top of Set by and ordered pair brings no performance penalty compared to putting a Set on a Map and creating a tuple for every elements of the Set at every insert/balance. You don't have to do that, you can reuse the same tuples, you're only creating one new tuple on each insert. Implementing a map as a set of ordered pairs means you've added an extra dereference to every comparison. &gt; AnyVal can't be used here since you store your key-value pairs as tuples in your TreeMap[A, Unit] You couldn't use `AnyVal` as currently implemented no. The `TreeMap` could contain something that implements the interface of a tuple. Or the compiler could specialize the implementation of the whole of `TreeMap` for the `Unit` case. &gt; And you still need a wrapper for your TreeSet class: I've never said `TreeSet` would be anything other than a class containing a `TreeMap`; I've been confused by how you've been using "wrapper". My point is that you can implement such a `TreeSet` using only the public interface of `TreeMap`, whereas to do it your way you need some extra map-specific methods *on the `TreeSet`* (because they have to access the internals of the `TreeSet` to be implemented efficiently), so you either have to pollute the public interface of `TreeSet` with extra methods that don't make sense for a general `TreeSet`, or you have to introduce a *third* class for those methods.
[I think this works](https://gist.github.com/torholm/d8dcf8bd73330b6377b96bf52a0a156a)
Dunno if this helps, but most browsers support "Print to pdf." Your users could load the report in a browser normally (rendered via Javascript) &amp; press cmd-p (shortcut for print) and then select pdf as the destination. It might save you a ton of work with writing a pdf renderer if you have an open-minded client.
Thank you. Have you used it before? How would it fit to architecture I had described?
It is something we had considered. But we may need server to mail it as attachment :(
Along with dogs vs cats, tea vs coffee and Stones vs Beatles, Functional vs Imperative is doomed to be one of the eternally-debated false dichotomies. Evidently all such discussions of the latter must do so in the context of either the Sieve of Eratosthenes, or the Quicksort of, er, Hoare.
Any "known" ETA on Scala 2.12 ? http://www.scala-lang.org/news/2.12-roadmap/ says it should be out for couple of months by now :)
&gt; That is interesting but this is code generation then? The code happens to be generated, but the generated code is `F[_]`-generic. &gt; Does it handle nested monads/ monad transformers? It doesn't have any particular support for them, but there's no reason it wouldn't work with an `F[_]` type that happened to be a monad transformer stack. &gt; I haven't worked out the best compromise to avoid getting locked into either scalaz or cats. Some Monad typeclass interface is required, the question is should it be an alias? a "neutral" one? I can think of a few solutions, but they are all less than ideal. If I could just use a structural type without a runtime penalty I'd probably use that. I would say that there's no value in adding your own wrapper/"decoupling" layer - both cats and scalaz offer the correct interface for `Monad`, and the community benefits from there being as few versions of it as possible. In the worst case it is easy enough to adapt typeclass instances of one as instances of the other. My advice is that cats is the future but scalaz is the present: if you are doing this as a learning experience or want to do the Right Thing, use cats; if you want to create the most pragmatic library you can to use today, use scalaz. &gt; I cover the motivation behind wanting to swap out effects in the readme. Hmm. I guess I never feel the need to unit-test this kind of service. If I have business logic that's complex enough to demand unit testing I would probably put that in its own (pure) function, separated from functions that do service composition. There's not enough to go wrong in the composition for it to be worth testing (often the parametricity theorem guarantees it must be written correctly). In terms of being able to migrate between libraries, I tend to alias my monad stack in one place (`type Action[A] = EitherT[({type L[B] = WriterT[Future, Vector[MyLogEntry], B]})#L, MyError, A])` or similar), which provides that advantage. I just have never found it was worth using e.g. `Future` and `Task` at the same time in the same code. &gt; Abstracting away the exact monad stack also allows one to write without monad transformers or needing to lift anything. Isn't it just the opposite? You're requiring `userDao`, `passwords` and `logger` to all use the same `E` type. Monad transformers and lifting are ugly but they allow doing something that's simply not possible in your model. To reenable the flexibility you'd get from a concrete monad transformer stack, you have to write it like: class UsersImpl[E[_], F[_], G[_], H[_]]( usersDao: SqlDocDao[UUID,UserData,F], passwords: Passwords[G], logger: Logger[H] )(implicit mo:Monad[E], mp1: MonadPartialOrder[E, F], mp2: MonadPartialOrder[E, G], mp3: MonadPartialOrder[E, H]) extends Users[E] { def something = for { digest &lt;- mp2.promote(passwords.mkDigest(plainTextPassword)) result &lt;- mp1.promote(usersDao.insert(...) } yield ... } which is no prettier than what you get when using monad transformers and lifting. &gt; Yes that is why I've added the Capture typeclass, since bind &amp; point aren't enough. You can see in BlockingDelay that it requires Capture[E] to ensure the effect is properly captured in the monad. &gt; To support generically handling execution system effects (such as exceptions) I've added "augment" type-classes such as Exceptions to demonstrate how generic handling of almost any execution capability could be done. It's kind of buried, but if you look at effects/sql/package you'll see that in a sugar method inTransaction I perform transaction rollback when an exception occurrs for any monad type that implements Exceptions. Right, yeah. Scalaz/cats have a whole bunch of these - you can represent each concrete effect as a constraint like `MonadReader` on your monad stack. I'm not convinced that buys you a lot - you can make service A return a `Reader[Int, X]`, service B return a `Writer[String, Y]`, and then when you compose them you explicitly lift them both into `ReaderWriter[Int, String, ?]`, or you can make service A return a `F[X]` for generic `F[_]: MonadReader[Int, ?]` and service B return `G[Y]` for generic `G[_]: MonadWriter[String, ?]` and then instantiate them both with `ReaderWriter[Int, String, ?]` which I guess saves you a bit of code, but it also means you can't see where the reading and where the writing is coming from in your composed service. I mean the whole point of using these monads is that the effects are explicit where we can see them, right? And I don't think `MonadReader` is really any more generic than `Reader` - the interfaces are basically the same, adding another layer of wrapper doesn't help anything. &gt; Play Action's are essentially Request =&gt; Future[Response]. Future may not have the pedigree of Task, but as far as Scala is concerned it is effectively a monad. Sure - I don't think of `Future` as a Play-specific type though (and this technique doesn't help you decouple from `Request` or `Response`, right?). In terms of writing code for use with Play, either you write a piece of pure logic (in which case it can be a pure function), or you write something that actually needs to do async operations, in which case it needs some way to represent async-ness. And sure, you can write some meta-abstraction that represents `Future` or `Task` - but implementing that meta-abstraction is going to be just as complicated as the code to convert between `Future` and `Task` is, you might as well just write the code concretely in terms of one or the other. &gt; I'm unsure what you mean by this. What is the difference between "code overhead" and "additional runtime overhead"? I mean you're adding complexity to the code that people have to read and edit when maintaining. I'm not worried about the runtime performance at all.
Wouldn't that just be a separate article? Your point is valid, but it's off-topic from the title of the article. He very clearly shows the difference in prime calculation using functional techniques vs. iterative, the code for each, and the performance differences for each. Having the interface for that function be referentially transparent would probably be a good idea, but the article wasn't about that. 
Take a look at https://github.com/matthewpflueger/isomorphic/blob/master/jvm/src/main/scala/com/github/matthewpflueger/isomorphic/server/HelloWorld.scala and also https://github.com/japgolly/scalajs-react/issues/160#issuecomment-222320799
&gt;You can't be serious to think it's "perfectly reasonable" to call "option(null)" on a boolean... It's as reasonable as `Pair(k, null)`. Which you keep claiming is unnecessary, but you're not posting any code. So show me: how do you implement `Map#get` doing it your way? &gt;If you implement rbtree as a Map you'll get the same, so I don't know what's the problem. You'll need to get over the key-value pairs' containers anyway. If your rbtree is a Map your Set will be bloated. If you implement Set on top of Map then each node contains one extra reference field (compared to having two dedicated tree types). Not nothing, but not a huge cost either. If you implement Map on top of Set then you have an extra Pair instance for each entry. Not only does that consume more memory directly than an extra field, it also means an extra object for GC and an extra pointer dereference for any operation on a key (such as comparing them for insertion). &gt;If you implement it that way your nodes will contain a lot of useless crap at the case of Set and your tree's implementation will be less than trivial. You wanted to avoid the the "Pair hack" but you've got an ugly tree and a bloated TreeSet. How so? The tree implementation looks the same as always. There's no ugliness. There's one extra field in each node in the set case, that's all. Might not even end up actually consuming any memory at runtime given that references are 4 bytes but objects are always 8-byte aligned. &gt;Not junk because my Set doesn't introduce any additional tuple and the treemap only contains a key-value pair. But a tuple and a pair are the same thing! Your map contains an extra wrapper object for every entry. My set only has an extra field in each entry object.
Why does it feels "kind of sketchy" ?
[Implicit instance lookup](https://confluence.jetbrains.com/display/IntelliJIDEA/Working+with+Scala+Implicit+Conversions) is the big win here.
If you're doing scala with emacs and sbt you probably want to try out ensime (http://ensime.org/).
Thank you
Is there a way for configure / force play-json to enode optionals as arrays of max lenght 1 in json? i.e. case class Foo(x: Option[Int]) Foo(None) =~ { "x" : [] } Edit: I know I can manully define reads/writes but I have lots of classes where Id like to reuse the macro which works just fine. 
IntelliJ let me fall in love with JVM languages all over again, especially Scala.
You need to show the full code that gives the error, no-one can help you without that. What is `x`? This sounds like a problem of path-dependent types, not really specific to macros. Per the error message, `x.typeSignature` is of type `whitebox.Context#Type` (i.e. `c.Type` for *some* unknown `c: Context`), but you haven't proven that it's the same path as the `c` that you're passing in.
&gt; I don't think it's a path dependend problem It is, you pattern matched on a method symbol on 67, but you have not proven that symbol comes from the context `c`.
I don't have access to gists where I work, so I can't see that. It's definitely a path-dependent types problem going by the error you've posted though.
I use IntelliJ IDEA. Most of my ad-hoc development is in Scala worksheets (which are like an extended REPL.) If I need to make use of a large codebase, I'll use `sbt console` to compile it and load it in to the REPL. If I'm using Spark, I'll generally use `spark-shell`. Doing your work in a REPL-focused way (especially at the beginning) dramatically cuts down on most of the setup overhead, and you can learn that stuff later. And using Intellij gives you a lot of useful stuff such as linting and error-correcting, as well as very useful keyboard shortcuts that are hard to replicate in text editors including refactoring and jumping to function declarations.
.
I'm sorry I'm quite new at scala can you elaborate a bit or better yet can you give me an example. As for path dependend problem I understood that you guys think that this is a classpath problem =) I come from the Java world, but now I see that you meant in a pattern matching way. 
Sublime and atom both have plugins that support ensime, which can do some in depth things, like call hierarchies, show types, and error highlights. It can be a bit funky to get them working at first, but very useful.
I actually have quite a bit of experience when it comes to functional vs imperative programming. I know the pros and cons of recursion, tail recursion, ect. I had a very good computer science teacher and I'm looking for a language to implement these tools.
I actually don't know what that Unit = sign is. I did not have that on my hello world function. I really appreciate the very explanatory feedback and I will not hesitate to ask questions!
Sure, and I made a point to say "plain text editor". /u/paradigmatic seemed to be advocating for minimalism, which is honestly a fine way to learn a language. But I didn't want OP to come away with the idea that the one true way to write Scala is to use text editors that provide syntax highlighting and nothing more. 
Gotcha, the blurry line between text editors and full blown IDEs has blurred significantly over the years :D
 Where did you get the idea of "option(null)" and "Pair(k, null)"? Because the first doesn't exist neither of this nor at other contexts in Scala(most probably) and Pair(k, null) is absolutely useless at any contexts. Anyway, remember my first statement: "you can implement the TreeMap with a TreeSet" and then you came with "No you can't, at least not without hacks". "Pair" isn't hack - it's a key-value pair ordered by the key and transfers the overhead from TreeSet and _Tree to TreeMap. Also, there is no "extra dereference" - there is only a bare access to a method.
&gt; I actually don't know what that Unit = sign is. I did not have that on my hello world function. Right. Let me try to break it down a bit. The function "signature" (name, arguments, and return type) looks like this: def main(args: Array[String]): Unit As you no doubt have learned by now, types in Scala are declared to the right of the thing they're the type of, after a `:`. So this literally says "define a function called `main` that takes an argument called `args`. `args` is an `Array` of `String`s. The return type of `main` is `Unit`." `Unit`, BTW, is like `void` in several other languages, i.e. it's a type with one meaningless value. In Scala, that value is called `()`. That is, `(): Unit` ("The unit value has type `Unit`.") A few words on the return type: first of all, Scala will let you get away with not declaring it. def main(args: Array[String]) = { println("Hello, world!") } This works for two reasons: 1. The return type of `println`is `Unit` (i.e. `println` doesn't return a meaningful value). 2. If you don't declare the return type of a function, its return type is the type of the last expression in the function. Since the last expression in our `main` is the `println`, the return type of `main` is the return type of `println`, which is `Unit`. This satisfies Scala's requirement that the return type of `main` be `Unit`. The `=` should essentially always be there. "The definition of main equals..." If you leave it out, you're also saying the return type of the function is `Unit`. In this case, that works, but if you leave out the `=` when the return type of a function is _not_ `Unit`, you'll get a pretty confusing error message when you compile the code. So I follow these rules of thumb: 1. Always declare the return type of your functions. It makes your intent clear, speeds up compiling slightly (because now the compiler only has to check, not infer, the return type), and probably most importantly, if the type of the function _isn't_ what you declare, the compiler will catch it for you. 2. Always put the `=` sign before the function body. It does no harm in the `Unit` case, avoids confusion in the non-`Unit` case, and results in source code that reads consistently. Win-win-win. I'm glad you found my comments helpful!
You definitely do not need to learn Java to learn Scala, and if you have FP experience Java might actually drag you down. I would recommend getting the book *Functional Programming in Scala*, which will show you how to port most of the functional idioms to Scala.
.
I used SublimeText 3 for 2 years building Scala code professionally on a large complicated codebase and it was completely fine. I was not alone in using ST3. However, some of my team used Vim. Some used Emacs. Some used IntelliJ. One weirdo used Eclipse. I guess what I'm saying is: just work with tools you're comfortable with. If that's ST then great! Unlike Java, Scala is not a language with huge amounts of boilerplate you need an IDE to manage for you, so you can really just use anything :-)
I think you can define a generic version to work for a bunch of things, though I didn't try. Something like implicit def optionAsArray[T: Writes]: Writes[Option[T]] = { new Writes[Option[T]] { override def writes(o: Option[T]): JsValue = { JsArray(o.toSeq map Json.toJson) } }
Here is a full code sample of one of my macro methods. How could I refactor case classes ClassInfo, ColumnInfo and other methods out? I would really appreciate the help. def select[A](query: String, conn: java.sql.Connection): Seq[A] = macro selectImpl[A] def selectImpl[T: c.WeakTypeTag](c: whitebox.Context)(query: c.Tree, conn: c.Tree): c.Tree = { import c.universe._ val mapGet = Map( c.typeOf[String] -&gt; "getString", c.typeOf[Int] -&gt; "getInt", c.typeOf[java.sql.Timestamp] -&gt; "getTimestamp" ) case class ClassInfo(schema: String, table: String) case class ColumnInfo(name: String, tp: c.Type, primaryKey: Boolean, generated: Boolean, versionCheck: Boolean, ignored: Boolean, optional: Boolean) def getBaseClass(tp: c.Type): c.Type = { if (tp.baseClasses.map(_.fullName).contains(symbolOf[Option[_]].fullName)) tp.typeArgs.head else tp } def getClassInfo(tp: c.Type): Option[ClassInfo] = { val result = tp.typeSymbol.annotations.find(_.tree.tpe == c.typeOf[ninja.bizzy.framework.jdbcorm.Table]) if (result.isDefined) { return Some(ClassInfo(result.get.tree.children(1).toString(), result.get.tree.children(2).toString())) } None } def getConstructorParameters(tp: c.Type): Seq[ColumnInfo] = { val optionType = symbolOf[Option[_]] val primaryKeyType = typeOf[ninja.bizzy.framework.jdbcorm.PrimaryKey] val generatedType = typeOf[ninja.bizzy.framework.jdbcorm.Generated] val versionCheckType = typeOf[ninja.bizzy.framework.jdbcorm.VersionCheck] val ignoreType = typeOf[ninja.bizzy.framework.jdbcorm.Ignore] tp.decls.collect { case m: MethodSymbol if m.isConstructor =&gt; m.paramLists.map(_.map(x =&gt; ColumnInfo(x.name.toString, getBaseClass(x.typeSignature), if (x.annotations.nonEmpty) x.annotations.map(_.tree.tpe).contains(primaryKeyType) else false, if (x.annotations.nonEmpty) x.annotations.map(_.tree.tpe).contains(generatedType) else false, if (x.annotations.nonEmpty) x.annotations.map(_.tree.tpe).contains(versionCheckType) else false, if (x.annotations.nonEmpty) x.annotations.map(_.tree.tpe).contains(ignoreType) else false, x.typeSignature.baseClasses.map(_.fullName).contains(optionType.fullName) ) )) }.head.head } def selectFullTermName(sym: c.Symbol): c.Tree = { sym.fullName.split('.').toList match { case Nil =&gt; throw new RuntimeException("unreachable") case head :: tail =&gt; tail.foldLeft(Ident(TermName(head)).asInstanceOf[c.Tree]) { case (qualifier, next) =&gt; Select(qualifier, TermName(next)) } } } val entityType = c.weakTypeOf[T] val columnInfos = getConstructorParameters(entityType) val columns = columnInfos.filter(!_.ignored) val ignoredColumns = columnInfos.filter(_.ignored).toList for (ic &lt;- ignoredColumns if !ic.optional) { c.error(c.enclosingPosition, "All ignored columns should be of type Option[_]") } val columnNames = columns.map(_.name) val statements = new ListBuffer[c.Tree] val termClassColumnNames = TermName(c.freshName("_classColumnNames")) val termSt = TermName(c.freshName("_st")) val termRs = TermName(c.freshName("_rs")) val termMeta = TermName(c.freshName("_meta")) val termRCount = TermName(c.freshName("_rCount")) val termI = TermName(c.freshName("i")) val termName = TermName(c.freshName("_name")) val termResults = TermName(c.freshName("_results")) statements += q"import ninja.bizzy.framework.jdbcorm.ExtendedResultSet._" statements += q"val $termClassColumnNames = List(..$columnNames)" statements += q"val $termSt = $conn.createStatement()" statements += q"val $termRs = $termSt.executeQuery($query)" statements += q"val $termMeta = $termRs.getMetaData()" statements += q"val $termRCount = $termMeta.getColumnCount()" statements += q"""if($termClassColumnNames.length != $termRCount) throw new ninja.bizzy.framework.jdbcorm.MappingException("Returned column count doesn not equal to non-ignored constructor parameter count!")""" statements += q""" for($termI &lt;- 1 to $termRCount){ val $termName = $termMeta.getColumnName($termI) if(!$termClassColumnNames.contains($termName)) throw new ninja.bizzy.framework.jdbcorm.MappingException("Returned column name "+$termName+" is not a constructor parameter!") } """ statements += q"val $termResults = new scala.collection.mutable.ListBuffer[$entityType]" val parameters = new ListBuffer[c.Tree] for (column &lt;- columns) { val functionName = mapGet.get(column.tp) if (functionName.isEmpty) { c.error(c.enclosingPosition, "No mapping for type " + column.tp) } if (column.optional) { parameters += AssignOrNamedArg( Ident(TermName(column.name)), Apply(Select( Ident(termRs), TermName("getOption")), List(Literal(Constant(column.name)), Select(Ident(termRs), TermName(functionName.get))))) } else { parameters += AssignOrNamedArg(Ident(TermName(column.name)), Apply(Select(Ident(termRs), TermName(functionName.get)), List(Literal(Constant(column.name))))) } } val applyMethod = entityType.companion.member(TermName("apply")) statements += q""" while($termRs.next()){ $termResults += ${selectFullTermName(applyMethod)}(..$parameters) } $termResults.toList """ q"..$statements" }
I'll say it: no. A lot of people think `\/` is scalaz's gateway drug. I think `Task` is a considerably more compelling one.
&gt; Because the first doesn't exist neither of this nor at other contexts in Scala(most probably) `.option` is from scalaz - apologies for not making the import explicit, I'm used to working with scalaz everywhere. &gt; `Pair(k, null)` is absolutely useless at any contexts. Then once again: show me, with code, how you implement `Map#get` without `Pair(k, null)`. &gt; "Pair" isn't hack - it's a key-value pair ordered by the key and transfers the overhead from TreeSet and _Tree to TreeMap. It is absolutely a hack. The idea that equal things are equal is about as basic as they come. Once you start having that `Pair` type your functions aren't even functions any more. &gt; Also, there is no "extra dereference" - there is only a bare access to a method. What do you think that method is going to do? How is it you think the pair is represented in memory? If you have a `TreeMap` implemented directly, each node contains two references - one to the key, one to the value - so you can figure out where a new node goes in the tree by comparing its key to the keys of existing nodes. If you have a `TreeSet` implemented directly, each node only contains one reference - to the value. If you have a map implemented as a set of pairs, the nodes contain references to the pairs (which are objects that contain references to keys and values), so to get to a key comparison when you're inserting a new node you have to go node -&gt; pair -&gt; key - an extra dereference.
thanks for the explanation, that makes sense.
This trolling is beyond answerable...
&gt; (basically Scala as a language is "just" a Java Library), That's really not true. Scala is a separate language, with its own grammar and compiler(s), and when targeting non-JVM platforms (Scala.js, Scala Native) it works just fine without Java interop.
What do you mean by "does not work"? How doesn't it work? Why should `Json.format[Foo]` work when that implicit def is for formatting an `Option` and not a `Foo`?
Maybe [sqlite](https://www.sqlite.org/) is for you.
Eclipse works very well for Scala. I've been using it for 5-6 years, maybe more. IntelliJ is very popular, but for some of us, like me, IntelliJ never made any sense, and Eclipse, while admittedly whacky, matches my mental models better. IntelliJ also uses its own Scala parser, and maybe compiler, and it gets things wrong sometimes, reporting red squiggles for things that are valid. That problem happens much less often in Eclipse, which uses bits of the "official" scalac. However, if you're just doing simple things, then use a text editor. VSCode, Sublime, Atom, Emacs, whatever.
Hijacking this thread to say that while I don't like IntelliJ personally, lots of people do, and it works for them. That's totally fine, but we in the Scala community should be very wary of everyone jumping to one IDE. Healthy competition benefits users, and if we collectively choose our way into an IDE monoculture, we'll all be worse off in the long run.
I've been using slick for a project I'm doing. You can use sqlite if you want more of a local database. You can create the database in SQL, and then configure the slick code generator to generate the table classes for you. 
I agree with you when it comes to beginners. Being explicit can help to solidify concepts. But we have more advanced tools than simple text editors, and I'm willing to take as a given that people will be using them. In fact, I'd argue that learning how to use our tools is just as important as learning the intricacies of our programming languages. It's possible to write code without tooling, and that's great at certain times, but it's not the one true way. And it's not like you said anything of the sort. But I still thought it worthwhile to provide a different perspective.
Oh I see now, sorry. I don't have an immediate answer but I'll try find one. Edit: just to be sure - you have the options as arrays implicit in scope when calling `Json.format` yes?
SBT + Emacs + Ensime + Helm + Projectile is the most fully-featured IDE you can get in scala. Intellij and the Scala IDE in Eclipse are close seconds, with more refactoring and debugging capabilities, but with more highlighting/compilation bugs and less active completion and code inspection capabilities. I pair with coworkers on their Intellij instances every day, but when we code at my desk we use emacs/ensime, so I use both equally, and I prefer emacs. Intellij is pretty good, though, and what you lose in features you gain in ease of setup, and the number of basic commands you have to learn is fewer.
&gt; .option is from scalaz - apologies for not making the import explicit, I'm used to working with scalaz everywhere. Glad to know the origin of "option" finally. But about the hacks - if there is a place for hacks then that place is scalaz. So, you shouldn't complain about hacks if you "used to working with scalaz everywhere." &gt; Then once again: show me, with code, how you implement Map#get without Pair(k, null). Can you imagine a binary search for me, please? &gt; It is absolutely a hack. The idea that equal things are equal is about as basic as they come. Once you start having that Pair type your functions aren't even functions any more. "your functions aren't even functions any more." - wtf!? Anyway, Pair is a specific and hidden type why would I care? &gt; What do you think that method is going to do? How is it you think the pair is represented in memory? If you have a TreeMap implemented directly, each node contains two references - one to the key, one to the value - so you can figure out where a new node goes in the tree by comparing its key to the keys of existing nodes. If you have a TreeSet implemented directly, each node only contains one reference - to the value. If you have a map implemented as a set of pairs, the nodes contain references to the pairs (which are objects that contain references to keys and values), so to get to a key comparison when you're inserting a new node you have to go node -&gt; pair -&gt; key - an extra dereference. That's not a "dereference" that's just memory access. "case Node(left, Pair(k, v), right)" shouldn't be more expensive than "Node(left, key, value, right)". And for further clarification - you do NOT need to explain how treeset/treemap used to work/be implemented. And I think I don't need to explain how would I implement a treemap with a treeset given the presence of "Pair" - it's a really basic idea.
.
From [the source](https://github.com/playframework/playframework/blob/2.5.4/framework/src/play-json/src/main/scala/play/api/libs/json/JsMacroImpl.scala#L176), `Json.format` has a special check for `Option` and invokes `JsPath.formatNullable` in that case. I don't see anything that allows you to change that behavior. So I think your best option might be an `AnyVal` wrapper. @ class MyOpt[T](val t: Option[T]) extends AnyVal {} defined class MyOpt @ implicit def optionsAsArrays[A](implicit fmtA: Format[A]): Format[MyOpt[A]] = new Format[MyOpt[A]] { override def writes(o: MyOpt[A]): JsValue = JsArray(o.t.map(fmtA.writes).toSeq) override def reads(json: JsValue): JsResult[MyOpt[A]] = json match { case JsArray(x) if x.isEmpty =&gt; JsSuccess(new MyOpt(Option.empty[A])) case JsArray(x) if x.size == 1 =&gt; x.head.validate[A].map(v =&gt; new MyOpt(Option(v))) case JsArray(x) if x.size &gt; 1 =&gt; JsError( "Invalid representation of optional parameter. Contains Array of &gt;1 elements") case other =&gt; JsError(s"$other is invalid representation of optional") } } defined function optionsAsArrays @ case class Foo(opt: MyOpt[String]) defined class Foo @ implicit val fooFormat = Json.format[Foo] fooFormat: OFormat[Foo] = play.api.libs.json.OFormat$$anon$1@16e7ba74 @ fooFormat writes Foo(new MyOpt(Some("test"))) res16: JsObject = {"opt":["test"]} @ fooFormat writes Foo(new MyOpt(None)) res17: JsObject = {"opt":[]} You can of course add convenience methods for converting to/from the wrapper. There's also the option (heh) of creating/finding a specialized "zero or one" sequence type and using that instead of `Option` since it more closely mirrors the intent, leaving `Option` for something truly optional.
Hi, thanks for answer! It was educational. But I'll stick to my solutions and rather specify formatting explicitly than having to change domain model because of json formatters (wrong way dependency)...
&gt; Can you imagine a binary search for me, please? Using the internals of `TreeSet`? So are you going to pollute the public interface with them, or split off another type? &gt; "your functions aren't even functions any more." - wtf!? Exactly. wtf!? is absolutely the correct reaction to that `Pair` type. &gt; That's not a "dereference" that's just memory access. "case Node(left, Pair(k, v), right)" shouldn't be more expensive than "Node(left, key, value, right)". Shouldn't in some abstract sense perhaps, but it absolutely is more expensive on the JVM - it means an extra object, and accessing `k` or `v` requires two deferencees rather than one.
Fair enough! I do appreciate explicitness. Though I'd still encourage considering what the impedance mismatch between your desired JSON and your domain model might mean. It could mean that JSON is not an appropriate format for your model since options are typically represented as null or missing values and you want something else. It could mean that your domain model is imprecise and that you don't truly have something "optional" and that instead you have a non-optional sequence with no more than one element, in which case changing the model could make sense anyway. It could mean nothing at all.
I think pattern matching by default matches the class name case class myClassName(i: Value, j: Value) someStuff match { myClassName =&gt; println(someStuff) } 
Yes it is indeed valid point I had considered. Keep in mind though, the "truth" is in case class. Value IS optional. Only problem is, the frontend guy in team does wish to have field ALWAYS present. That's javascript way of typesafety I suppose. One lightweight way to achieve optional is to represent it as Array - but only on Javascript. So my backend code is not polluted and types tell truth there. But I agree, it is mismatched on frontend. But thats kind of their problems, it is javascript and you know... they don't care I suppose.
Haha been there. Sounds to me like the conversion should be on the frontend but ¯\\\_(ツ)\_/¯
&gt; There are many overlapping concepts, it appears as if every file introduces a new operator, ... Can you give an example of what you have in mind?
I remember reading that positional type parameters are going away. (Those are the types declared in square braces.)
I do use ensime, but my understanding is the debugger part is new and I have some trouble setting it up. 
Dotty is not so much about changing the syntax as to putting the Scala type system onto a new simpler foundation. AFAIK, you will still be able to compile old code bases using a compatibility switch (`-scala2` or something like this). But if you take that out, I think there are the following changes: - existential types and higher-kinded types are collapsed into partially defined refined types - type constructor parameters and type members become the same thing See also here: http://wirth-symposium.ethz.ch/slides/odersky.pdf starting from slide 30. Furthermore: - type projections are restricted to value dependent types (?) - early initalizers are gone in favor of trait constructor arguments - XML you already mentioned - procedure syntax is gone or deprecated - tuples might be modelled like HLists and do away with the 23 individual types - macros and reflection will be replaced by scala.meta - increase modularity of the standard library, simplify collections Note that these last items are not necessarily depending on Dotty but are on the future Scala roadmap, along with many other clean-ups: http://www.scala-lang.org/news/roadmap-next/
You can definitely use play's json library to serialize to and from byte arrays, but if you have any non trivial use case for Kafka you shouldn't be using json anyway. See this other recent post for serialization performance benchmarks: https://medium.com/@dkomanov/scala-serialization-419d175c888a#.scbz9ook I'm a little disappointed that it didn't also include avro, but you should look into that too.
Hmm, I don't think that's right. It will match an object (`::` is the name of a class and the name of an object) or an extractor that starts with an upper case letter. A pattern that starts with a lower case letter is just a fresh variable. e.g. try this: case class myClassName(i: Int, j: Int) 3 match { case myClassName =&gt; println("I guess 3 is myClassName") } I don't think the first example in the original post works -- when I try it, I get this error: error: pattern type is incompatible with expected type; found : ::.type required: List[Int] Note: if you intended to match against the class, try `case $colon$colon(_,_)` case :: =&gt; println(myList) ^ The second example works for the reason /u/argv_minus_one gave. Hope this helps!
&gt;It will match an object (`::` is the name of a class and the name of an object) or an extractor that starts with an upper case letter. Or if there are parens in the pattern, or if the name is surrounded in backticks. ``case `myClassName` `` and `case myClassName(_, _)` will not match, for instance. &gt;I don't think the first example in the original post works scala&gt; List(1, 2) match { case ::(x, y) =&gt; println(x); println(y) } &lt;console&gt;:8: warning: match may not be exhaustive. It would fail on the following input: Nil List(1, 2) match { case ::(x, y) =&gt; println(x); println(y) } ^ 1 List(2)
That's not true, although they will become syntactic sugar for abstract type members.
Ah the brave new world. Where you provide stuff for free and folks still feel entitled to demand more. Kinda like folks who use adblock, they want the content for free and can't be bothered to see a miserable ad to give pennies back.
I just read &gt; Simple &gt; &gt; * Step 1 : solve compilation issue &gt; * Step 2 : use scalaz and glanced at the src and figured out what this is about in few minutes. It's nice and simple. Clone the repo, run sbt test, fix errors to learn scalaz.
Thank you! The links and the list gave quite an insight of what is to be expected.
Anyone else doing the Scala Coursera course? Does the certificate mean anything to employers?
I prefer the name "Cinnabon operator". Much more fun to talk about. I also don't think "applicatives in to tuple" is a useful description. Perhaps "Combines applicatives to make a tuple, potentially accumulating errors." But I'm not even happy with that. I think that a small table may not be sufficient to describe the power of some of these. 
Agree, I use Atom/Vim + sbt. Best to learn a new language with less training wheels. IDE's change all the time but the commandline will look the same anytime.
So we are either forced to write our code in completely different styles depending on the library whose methods we are using, or we keep our codebase written in a consistent style and get nagging warnings which might drown more important compiler messages? That seems to be a pretty bad idea.
&gt; Scala doesn't have interfaces. My bad. &gt; I think with a bit of experience things will start coming together and you will see why things were designed this way. I hope so! &gt; Creating an instance of something is a very special operation with all kinds of rules affecting initialization, accessibility, mutability, visibility, serialization etc. I'm not a huge fan of the new keyword, but the distinction serves its purpose very well. new is doing a very specific single thing, and apply has more flexibility regarding what instances (if any) are created and what work is done before/after that. I see. For a beginner it's just confusing when there are object created with and without `new`. For example: `case class` allows to drop it and normal `class` doesn't. Kotlin dropped it entirely (which I prefer) &gt; You will get a deprecation warning for this. I just read that functions without parentheses indicate purity, which is kind of neat, although confusing if one doesn't know about it. Which version will deprecate it, because 2.11.8 doesn't say a thing? &gt; `&lt;:` and `:&gt;` are pretty much standard for denoting subtyping relationships in literature Fine. But not everyone has read it before. Don't they overlap with `extends` then? &gt; There are no such operators. Yes there are. I can't find the definition of `||=` and `:+=` anymore. But `/:` and `:\` are functions that Intellij suggests on `(1 to 10)`. Here is the signature: `[B](z: B)(op: (Int, B) =&gt; B)` for `:\`. While looking for `||=` I found new ones: `##`, `+:`, `++:`, `::` that Intellij suggested. 
how can a coverage plugin be useful for this question?
I've been a staunch advocate for calling the applicative builder the TIE Fighter.
I thought code coverage was only available when testing. Do you have a working config example?
You could also run your app for a few hours with code coverage on, and check the report once you're done activating everything.
`sbt coverage run`
I was referring to the following: &gt; Functional programming is great, but may not always be the appropriate solution when a speedup is necessary and keeping things functional will add unneeded complexity. In that case, you write an imperative algorithm that use local mutable state. From the outside, it's still "functional" (pure, referentially transparent and so on). So you get the best of both worlds: speed where it matters, and reduced complexity when coupling parts of the system, because you keep side-effects in check. Evidently, I did not downvote the article: for example, every simple implementation of quick-sort with lists and pattern matching (see! it's great! you can write near-pseudo code and it runs!) does not have `n log n` complexity anymore
Now, replace your for loop with a while loop (thus avoiding calls to `Range.foreach`) and use a primitive `Array[Long]` as a bitset (`ArrayBuffer` interface is boxed), you probably gain another order of magnitude.
You can already enable/disable warnings with a degree a granularity. If you disagree with the library writer on how to write things, you will probably be able to disable the warning easily. Meanwhile, these modifications will allow consistent style among users of the same library, which is a good thing.
So would that be like MongoDB?
A document store is an option (though I'd suggest Riak or Cassandra as they have a better reliability record than MongoDB, and are very similar to use). Personally I prefer to have referential integrity constraints so I'd use PostgreSQL or similar (well actually *I'd* use MySQL because I'm used to it, but PostgreSQL has fewer gotchas if you're starting from scratch). *Particularly* if you're already thinking in terms of tables. But with the strong safety guarantees Scala gives you you can probably get away with something that just stores arbitrary JSON (or arbitrary strings) if you want.
The second position can be any type that conforms with `Int`, so it could also be `Nothing` or `Int with FuzzyWuzzy`, and so on. But I think the general issue is that Scala simply doesn't infer parameter types. Unless you give it an ascription or some other context (like passing it to a method that's expecting an `(Int, Int, Int) =&gt; Int`) I think you're out of luck.
https://geirsson.com/assets/flatmap/#/29 https://geirsson.com/assets/flatmap/#/44 Kill... with... fire... I realize it's an "opinionated" formatter, but vertical alignment is the wrong opinion...
&gt; but vertical alignment is the wrong opinion No!! This is actually my biggest gripe with any existing formatters (e.g. IntelliJ). These destroy my neat vertical formatting. There are many good reasons to use it, foremost it's easier to spot mistakes.
There is a short comment on R in the text. From usage statistics, one can assume that R is very popular. But if you have ever used it, you probably know that as a language it's a pretty horrible "design", even more obscure than MATLAB.
Thanks. That clicked. It's good to see that this works (per your last statement): scala&gt; def foo(x: Int, y: Int, z: Int) = x + y + z scala&gt; def bar(f: (Int) =&gt; Int) = f(10) scala&gt; bar(foo(1, _, 3)) res0: Int = 14
true, in ubuntu repository, compiler is too old :(
Me! I doubt the certificate means anything. We both have access to the assignments and quizzes, and it's going on LinkedIn either way. Personally, I'm not paying for the certificate.
&gt; Just want to thank /r/lihaoyi for this series. Should be /u/lihaoyi. He's a big deal, but not yet so big as to merit his own subreddit.
I'm not sure I understand. If you're getting the warnings does that not mean you should address the warning and remove the unused import and not discard the value? Or if you don't care about them maybe not have the warnings enabled at all? But my answer to "is it possible" is: _probably_, but I don't know how off the top of my head. I doubt it's part of default sbt config/options. I can conceive of a build setup with sbt tasks/scopes that compiles some files one way and some files another way. I'd argue that's not really a good idea though.
It's really not that bad. For example: I have a data frame with two columns. The first column holds either an a or b. The second holds a random number between 0 and 1. And I want the mean by group. Using the dplyr package from Hadley, who was mentioned in the article: df %&gt;% group_by(first_col) %&gt;% summarise(mean_2nd_col = mean(second_col)) ----- Source: local data frame [2 x 2] first_col mean_2nd_col (fctr) (dbl) 1 a 0.6120802 2 b 0.4252329 
Please dont share links like this. This will send a false signal to Intellij. It's borderline spam.
The thing is: I have the -Xfatal-warnings enabled, which makes compilation to fail if any warning was emitted by the compiler. But adding some of the flags you mentioned, I got an 'Unused import' warning on my Play routes file, which does not have any import (not sure if a bug, or just some magic that happens on compilation time to that file). for instance: [info] Compiling 9 Scala sources and 2 Java sources to /Users/pedrorijo/git/&lt;MY-PROJECT&gt;/target/scala-2.11/classes... [error] /Users/pedrorijo/git/&lt;MY-PROJECT&gt;/conf/routes: Unused import [error] /Users/pedrorijo/git/&lt;MY-PROJECT&gt;/conf/routes: Unused import [error] /Users/pedrorijo/git/&lt;MY-PROJECT&gt;/conf/routes: Unused import [error] /Users/pedrorijo/git/&lt;MY-PROJECT&gt;/conf/routes:15: Unused import [error] GET /versionX/something/$id&lt;[0-9]+&gt;/update controllers.SomeController.updateSomething(id: Long, forced: Boolean ?= false) because of this behaviour, my compilation fails, but I don't really care about this case (or should I care?)
done :-) /r/lihaoyi/
My preference is to not have unused imports at all, so I'd say figure out what's actually causing that and fix it if you can. But unused imports are one of those things that don't actually harm much so not warning on those is a reasonable thing to do.
It's not bad for performing ordinary arithmetic or statistical operations - that's why it's successful. But as a programming language it's horrible IMO, with respect to functions and types. You don't want to write an algorithm in R. It was created by people who knew maths but had not the slightest idea about PL design.
I don't think you should focus on monads and other high level abstractions too much at this point. I recommend you to take a more pragmatic approach to it. Forget monads, focus on be comfortable using map and flatMap. Futures are weird, but they are weird for everybody, not only beginners. For the moment, think of Futures as Options that require the annoying ExecutionContext implicit. Your main focus should be in achieving simplicity and in writing declarative code. Those two goals are *really hard* to achieve, but Scala **will** help you better than a non functional language. 
I'm totally agree with /u/caente. Just want to add few things. First of all it takes some time to get used to it (same as any concept in programming!), but when you actually start to "feel" it - you will be wondering how you lived without Monoids, Applicatives, State, Free Monad etc. At least that happened to me. For me also it was a long road (and I still cannot see its end), but I enjoyed this "clicking" moments when after fourth rereading of some article I at last understand some concept. So, it's worth it for me from both pleasure and practical point of view. I also think if you want to see benefits of pure FP approach you should start to use some libraries that embrace this approach. Cats or scalaz at general and probably things like FS2 or doobie as more specialized libs. P.S. For some reasons I found this [talk](https://www.youtube.com/watch?v=M5MF6M7FHPo) very enlightening. It clearly shows you how much FP approach better than old-school Javaish because if you worked with JDBC you probably know it can be a nightmare.
&gt; I think of map and flatMap as a way to convert the elements inside the container, nothing more That's exactly what it is. No need to think more deeply about it. More formally, if you have a container for a type T, let's call it val c = Container[T] what map does is: take a function `T -&gt; S`, and apply it to the content. So whenever you do `c.map(fn)` you get a `Container[S]` and you effectively did `Container[T] -&gt; Container[S]` using only `fn: T -&gt; S` And flatmap takes a functions `T -&gt; Container[S]` and gets you a `Container[S]` as well.
I don't really have any advice, other than let the types guide you. If you can wrangle the types so they align, you will find it works as you expect. Oh I need a Future of S, when I have a Future of T - lets's just map it. Don't worry too much how it works.
There's nothing about the `Par` type that guarantees that the `ExecutorService` will be used at all, let alone that it will be used the right way, at the right time. `fork` is a combinator (a function that takes a function as an argument and returns another function) that guarantees that, when the `Par` is run, it will run on a thread taken from the provided `ExecutorService`. Being a combinator, it's not all that special taken in isolation. What makes it interesting is its use as a building block for the rest of the chapter. And that's true of combinators generally: their value isn't often apparent in isolation. It's only in the context of their related combinators that their value really becomes apparent.
Sssshhhh! You'll scare them away.
&gt; `||=`, `:+=`, `:/`, `:\:` There are no such operators. The first two are assignment operators per [6.12.4](http://www.scala-lang.org/files/archive/spec/2.11/06-expressions.html#assignment-operators), symbolic operator `a op= b` is rewritten as `a = a op b`. Thus we have both `i += 1` and `list :+= item`.
I think the issue is how the API/DSL is designed. The Scala community and even more the FP people there seem to prefer the cryptic symbol method names and funky DSL with implicits, leaving out dot etc. In my opinion it is a bad practice (almost like code golfing but applied to your library's users) and harms code readability.
It's a known bug: https://github.com/playframework/playframework/issues/6131
Don't think of it in terms of registering callbacks - that's implementation details. Think in terms of values. A `Future[A]` is a value of type `A` with an effect (async). A function `A =&gt; Future[B]` is a function from `A` to `B` that does some asyncing. Put those together and we have a `B` that's still async. A `Future[Future[B]]` is kind of absurd in this view - it's not like you can make it "twice as async". I think thinking of `flatMap` as a container operation is a big stumbling block for a lot of people getting into monads. Containers are if anything an unusual special case of a monad - most monads behave quite differently. My advice is to forget everything you know about `map` and `flatMap`, and start trying to understand how they work on `Writer`, `Either` and `Future`. Once you've got a solid understanding of the general version, you'll be able to see how the version for containers fits. The other thing I'd say is that there's no point using the monad abstraction until you find yourself actually wanting to do the same operation with two different monads. And at that point it's just following ordinary good programming practice - factoring out commonality between two similar functions. http://m50d.github.io/2013/01/16/generic-contexts.html was my effort at explaining this route.
I would agree that cryptic symbol names are often bad. lihaoyi had a great post on naming, and this point is worth repeating: that the cost of a short symbolic name should be offset by being used so frequently that it makes sense. By that theory, it might make sense to (sometimes) use a symbol, if it's used extremely regularly by your code. So I think when you see symbols in FP code, you should judge two things: is there a non-symbolic option, and is it used often enough in code using that library, that it becomes familiar, and is worth the cost. IMHO the SQL library (doobie) mentioned above is very readable (and is pure FP): here's an example from the GitHub account which retrieves an (optional) row from a database, and fills a case class with the result: case class Country(code: String, name: String, population: Long) def find(n: String): ConnectionIO[Option[Country]] = sql"select code, name, population from country where name = $n".query[Country].option 
It's not magic, just abstraction. flatMap abstracts the process of doing two computations where the second one depends on the first one. What does it mean to run a future computation dependant on the value of another? Well if the first failed then we can't run it so it has to stay a failure. If it succeeded then we just run it. The same is true of option. How do you compute optionally? Well if the value isn't there you can't do it, so None. If it is there, use it; get back Some. 
Just looked quickly over it, timestamptz should be mapped to java.time.OffsetDateTime (don't have a doc link for this atm, but I'm pretty sure on this)
timestamptz doesn't actually save any timezone data, it only saves the offset (which is a more stable datatype for an instant) it can happen that the timezone changes of date, since multiple timezones can have the same offset and pg only saves the offset and then maps that to timezone which "fits" if I'm wrong on this please correct me :)
How about not relying on conversion that violates the associative law, regardless of scalac's ability to infer the conversion for you. `parse` should be a partial function, lifted to an Option where that makes sense, and you should apply partial functions to Lists using collect.
sbt fun! On a Spark/Cassandra/Elastic search project, I'm getting the following compile error: Cause: java.lang.IllegalStateException: Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError. See also http://www.slf4j.org/codes.html#log4jDelegationLoop for more details. despite going as far as adding excludeAll(ExclusionRule(organization = "org.slf4j", artifact="log4j-over-slf4j")) to every line in by libraryDependencies in build.sbt How do I go about resolving this? How can I find the dependency that is causing this conflict and how do I correctly exclude the conflicting jar? Looking at the dependency graph is tough. These projects have a lot of dependencies and it's hard to parse it all out. 
I can't speak to SBT at all, I use it as little as I possibly can. (What I'd do is right click -&gt; exclude in eclipse, which will add the exclusion to the pom, but obv. that doesn't help you get it back into the SBT build definition).
&gt; What law does it violate? The [Associative law](http://eed3si9n.com/learning-scalaz/Monad+laws.html#Associativity). `List#flatMap` is associative. `Option#flatMap` is associative. `GenTraversableOnce#flatMap` is not. &gt;I find partial functions in Scala exceedingly error-prone (e.g. they implicitly convert to total functions) No they don't. They are [total functions](http://www.scala-lang.org/api/current/index.html#scala.PartialFunction). That said the subtyping relation is wrong. 
I'll leave my previous comment for posterity - it made sense but is factually incorrect. I was making incorrect assumptions about `flatMap`. You're right, `GenTraversableSeq.flatMap` isn't called - `List.flatMap` is, but it's a weird `flatMap` that takes a `GenTraversableSeq` as a parameter, not a `List`. So the laws that Milyardo was talking about don't apply: they're monadic laws, and the `flatMap` you're using isn't actually monadic (it mixes two different monads, `List` and `Option`).
Probably one of the `scalac -Y` flags will help the cause. Can also dig through Zaugg's [compiler optimization experiments](https://gist.github.com/retronym/86ec6ad9ccd2c22f6148) and see if you can dig out some timing info.
It's also possible to get that graph without going through Maven by using [sbt-dependency-graph](https://github.com/jrudolph/sbt-dependency-graph). You may be able to exclude the artifact with something like libraryDependencies ++= Seq( ... ).map(_.exclude("org.slf4j", "log4j-over-slf4j")) I also saw one example that looked like libraryDependencies ~= { _.map(_.exclude("org.slf4j", "log4j-over-slf4j")) }
https://github.com/milessabin/shapeless/pull/595 Shapeless recently added a macro to measure compile time.
Yeah, I can get the dependency graph, but it's huge so kinda hard to work with. I was able to track back from log4j-over-slf4j to a dependency on Storm that I don't need. Excluding *that* seemed to work where exlcuding log4j-over-slf4j itself didn't. Fun times. 
I feel bad after leaving a comment that might give an impression of Slick having a dissatisfactory API in general. That's certainly not true - for 95% use cases that can be covered with table mappings it is very nice! It's not just good-looking and has sanely-named methods - it makes code robust and maintainable, which means more as coding is no beauty contest. Definitely my #1 SQL favorite. Also mature and supports commercial databases. For those 5% or less where one need's plain SQL it's god awful, as if an enemy spy wrote that API as a diversion.
&gt; Are you referring to this post? Yes, that post! Very well written, like most of his work :-) &gt; Pretty much any existing SQL library is sufficiently readable for simple selects. Problems will probably start about when one needs upsert (merge), UDF/procedures with return values, grouping with subqueries... Real world stuff. Here's a basic approach for upsert with PostgreSQL (it obviously doesn't work perfectly for certain race conditions, and could be improved to deal with that - or you can modify use use Postgres 9.5 syntax): def upsertProduct(userId: UUID, product: Product): ConnectionIO[Int] = for { save &lt;- HC.setSavepoint insertOrUpdate &lt;- insertProduct(userId, product).run.exceptSomeSqlState { case UNIQUE_VIOLATION =&gt; HC.rollback(save) &gt;&gt; updateProduct(userId, product).run } } yield insertOrUpdate This uses the monadic `ConnectionIO` to sequence JDBC commands over a connection. The one weird operator here is `&gt;&gt;` which is the same as `.flatMap(_ =&gt; SOMETHING)` - you can use the second version instead if you want. I haven't had a need for a stored procedure call (yet), there is an example in https://github.com/tpolecat/doobie/issues/273 - def names(limit: Int): ConnectionIO[List[String]] = HC.prepareCall("{ call foo(?, ?) }") { for { _ &lt;- FCS.setInt(1, limit) _ &lt;- FCS.registerOutParameter(2, Other.toInt) _ &lt;- FCS.execute rs &lt;- FCS.getObject(2).map(_.asInstanceOf[java.sql.ResultSet]) ns &lt;- FCS.liftResultSet(rs, HRS.list[String] ensuring FRS.close) } yield ns } If you're used to how stored procedures are represented in the JDBC API, I will assume this makes sense to you! Grouping is a matter of massaging the result set into whatever form you want, with standard combinators. If I wanted a `Map[Int, Person]`, I could do something like: case class Person(name: String, age: Int) def getPeople: ConnectionIO[Map[Int, Person]] = sql"select id, name, age from people".query[(Int, Person)].list.map(_.toMap) If I wanted a `Map[Person, Seq[Person]]` using a self-join from parent to child on a "parent_id" column, one might see something like: case class Person(id: Int, name: String, age: Int) def getPeopleAndChildren: ConnectionIO[Map[Person, List[Person]]] = sql""" select p1.id, p1.name, p1.age, p2.id, p2.name, p2.age from people p1 join people p2 on p1.id = p2.parent_id """.query[(Person, Person)].list.map(_.groupBy(_._1).mapValues(_.map(_._2))) Obviously you might actually want a left outer join here, sadly this is one of the areas that could do with improvement, see "How do I handle outer joins?" in the FAQ http://tpolecat.github.io/doobie-0.2.3/15-FAQ.html
Great write-up. Yeah, the typechecker is wacky that way. You have to help it along sometimes. I would recommend not relying on implicit conversions to 'fix up' things returned from your functions. Instead, do it by hand. Bonus, it will be easier to read later. List("1", "2", "nope", "2.71828", "3") flatMap (parse2 andThen (_.toList))
Actually `flatMap` is still monadic--and it's not mixing the two different monads. The reason it's monadic but has a signature `(f: (A) =&gt; GenTraversableOnce[B])List[B]` is because of subtyping. Because of the deep type hierarchy, you have to think of the collection classes as a single algebraic data type with many cases. So in Haskell terms: data GenTraversableOnce a = GenTraversableLike a | GenTraversable a | ... // all the other cases data GenTraversableLike a = ... // all the cases // ... all the types ... data List a = ... So as you can see, to implement a `flatMap` for `List`, you really have to do it for `GenTraversableOnce` and match against all the possible cases (subtypes) just like when you'd implement `(&gt;&gt;=)` for a Haskell `[a]` you'd need to match against the two cases (subtypes) of `[a]`: `x : xs` and `[]`. It's just that in Scala, instead of matching against all the cases in a giant `match` expression, the technique has been to put all the individual cases in their own `flatMap` methods in all the subclasses of `GenTraversableOnce[A]`. I actually went down the rabbit hole of trying to understand this a few months ago: https://www.reddit.com/r/scala/comments/3zmdkf/why_does_amap_return_a_lista/cyoqvpn And finally, there's no monad mixing going on here, just good ol' implicit conversions from `Option[A]` to `Iterable[A]`.
Turns out it works like this def getBaseClass(c: whitebox.Context)(tp: c.Type) = { import c.universe._ if (tp.baseClasses.map(_.fullName).contains(symbolOf[Option[_]].fullName)) tp.typeArgs.head else tp } It compiles but IntelliJ gives an error: Type missmatch, expected: c.Type, actual: whitebox.Context#Type So it's an IDE bug =)
run sbt command in recfun directory
This is the correct answer (I too had exactly this problem at the start of the course before I knew what sbt was and what it meant in the context of building scala projects). It's worth noting that IntelliJ has support for sbt, i.e. you can just import an existing project from an existing sbt project (and also keep the intellij project in-sync with changes to the build.sbt). I'd recommend IntelliJ over ScalaIDE at the moment, at least until the ScalaIDE gets better tooling. Eclipse uses a .project file to store the definition of a project, the alternative way of trying to do this would be to create a new scala project through the ScalaIDE and then just copying the relevant files in.
A lot of people love Play, but I am not a giant fan. I have mostly been using a homegrown framework based on Xitrum and Squeryl for the last several projects, but in the future am considering a move to Finch and Doobie for my next project. As for the three you mention, Spray is *probably* the best for a pure REST server, but if you want to have nice communication between the server and client Lift is probably king. Good luck! 
I see you adopted the purely functional frameworks. I am afraid i have yet to ascend to such heights and i still use dirty full of side-effects functional programming xD. Thanks for your input 
You could always give Xitrum a try... far from purely functional. If you have ever used Django or Rails you will feel right at home! 
For a purely functional approach, I've been impressed with [Finch](https://github.com/finagle/finch) (using [Circe](https://github.com/travisbrown/circe) for JSON support) and [Doobie](https://github.com/tpolecat/doobie) for database access.
Why do you believe Play is overkill?
Play does have a lot of features but it's not like they get in your way if you don't need them. I use Play for a backend rest API, and it works quite well. Started off with a minimal framework (Finatra) at first but I found myself re-implementing things that Play made really easy.
Thanks for getting these up. 
I'd love to watch these but they load circle almost constantly. 
I fully admitted it was a contrived example. I know better than to actually implement the above, I probably wouldn't even define a function to handle it unless I need to deal with a collection (and Scala provides awesome anonymous functions, so even that isn't an issue). It was more to illustrate what I was after than anything else.
As others have mentioned, you just use a function/method. It is considered wrong to have `if (99) ...` thus the idiomatic thing is to avoid it. Instead, prefer something like `Seq(1, 2, 3).isEmpty` that returns a Boolean for example. These methods can be part of the type or can be implicit extensions. For example it is perfectly good to write isOdd for Ints so you can call `99.isOdd`. If you write a unary operator (?) or method (_?) it is generally considered bad style unless it is for something that is strongly needed in the domain, like numeric negation (-). 
We do the same in Python, we just express it differently `if xs` in Python and `if(!xs.isEmpty)` in Scala are analogous, except with less duck typing in Scala. `if` is one of the few cases where Python will coerce a type for you. And to my understanding, outside of implicits, Scala never does this. But considering I have four years with Python and a whopping week with Scala, that's a very soft never. 
Try h2 db. It's pretty pleasant and provides more features than sqlite
It seems that coming from Python, your perspective is that library writers choose what truth is. In Scala it’s the other way around, the user chooses what truth is, by calling a boolean function. This can be done with a function call (f(x)), postfix method (x.f) or a comparison operator like &gt; (or implicitly with an implicit conversion, but unlikely, since the appropriate truthiness can change from line to line). So with a Scala Seq, *you* might want truth to be seq.isEmpty, or you might want truth to be seq.nonEmpty. Scala’s standard library does not choose that `if seq` means either of those. You choose. On one line you might want x==0, in another line you might want x!=0. Scala does not define an Int with any inherent non-contextual truth. In Scala, you choose the implicits by importing or defining them. The built-in exceptions are string concatenation (+) and [Predef](http://www.scala-lang.org/api/current/index.html#scala.Predef$) which is imported by default: &gt;The Predef object provides definitions that are accessible in all Scala compilation units without explicit qualification. &gt; &gt; **Implicit Conversions** &gt; &gt;A number of commonly applied implicit conversions are also defined here, and in the parent type scala.LowPriorityImplicits. Implicit conversions are provided for the "widening" of numeric values, for instance, converting a Short value to a Long value as required, and to add additional higher-order functions to Array values. These are described in more detail in the documentation of scala.Array. 
It depends on a) performance requirements b) feature requirements c) your qualification. If you are into "getting things done" motto, then take Play. It will provide you with well-thought framework for web apps with some performance and flexibility implications. But still great choice if you do not need to handle thousands of users and/or expose public API with strict behavior. On the other side is twitter-server(application skeleton from you-know-who) + Finagle(dumbest/fastest HTTP server/client) + jackson-scala-module (fastest json parser/emitter out there) + JDBC. Best performance, best flexibility, best suited for wheel-inventing type of people. Choose what fits best to you.
What's the question?
I enrolled, and I don't care about certificates saying I have a skill - I'll use my resulting portfolio instead.
I have several teams at work using it to deploy full-fledged front-ends. The kind that take years to develop sometimes. For this, i would want something that is simple to use, learn and that is lightweight. It does not need to have all that play may offer. I may be overthinking this, but all i need is a quick to learn rest API framework and then bind it to the data model. Tofinish, a guy building a front-end on angular or react.
You can read https://loicdescotte.github.io/posts/scala-compose-option-future/ and https://tersesystems.com/2014/07/10/composing-dependent-futures/ for more information.
Much obliged. I will look into that for sure.
&gt;I am trying to wrap my head around Futures but I don't really know how is the best to handle errors. The usual way is to throw an exception from inside the future block, same as if you weren't using futures. If you then await the future's completion, the exception is re-thrown from there, so it's all good. &gt;For example, I have a UserStorage and I have a findById method on it, this returns a Future. What would you return, a Future[User] or a Future[Option[User]]? Good question. I guess it depends on whether failure is actually exceptional, or if it is to be expected and handled by the caller. &gt;I quite like using Options for this case for example, but same applies when there are more errors, I'd rather return a Either[List[Error], Result] than a Future[Result] and just throw when it fails. You could use Java 7's `Throwable#addSuppressed` for reporting multiple failures at once. &gt;Also, sometimes I feel that Futures are overused, like I use a library which even wrapped a simple function call in a Future, does this have any advantages? Depends on the function call. Usually, the point of doing something in a future is that it will take a long time to finish, or that it needs to happen on a specific thread (e.g. the JavaFX application thread). If that isn't the case, then yeah, it's probably excessive. &gt;Seems like if I use Futures with Option it's quite hard to use for comprehensions, what do you think? for (theOption &lt;- theFuture; theValue &lt;- theOption; …) … Seems simple enough to me. &gt;A lot of the time I just wrap the Future in an Await to reduce my thinking about it. Why not use `onComplete`, `map`, `foreach`, or the like? &gt;How are you using Futures? By stringing them together! Once a future completes, the next step runs, then the next step, and so on. The final step is updating the GUI with the results, sending them to the HTTP client, or whatever. It's gloriously concurrent.
 Task[B \/ Option[A]]
Firstly, if you have code that *genuinely* has to block, then use [blocking { ... }](http://www.scala-lang.org/api/current/index.html#scala.concurrent.package@blocking[T](body:=&gt;T\):T) so it has a chance of getting its own thread to be idle in (see [docs](http://docs.scala-lang.org/overviews/core/futures.html#blocking)). Secondly, if you only need to *schedule* something to happen independently in the future (no pun intended) you can use Akka’s [system.scheduler.schedule](http://doc.akka.io/api/akka/2.3.14/#akka.actor.Scheduler) or Java’s [ScheduledExecutorService](https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ScheduledExecutorService.html). I can’t remember if I’ve ever used akka.pattern.after so I’m not sure.
When I design an API with Futures, I think the Future away and reason what I'd return if the call wasn't asynchronous. For your find-method, Option[X] makes sense as you want to communicate back to the caller that the method may actually not return a User for a given id. Now let's say I want to create a method for booking a laundry room at a specific date/time. There's an expected number of error outcomes like NotFound and AlreadyBooked, so I would create a sealed trait hierarchy and use Either (actually, I'd probably use Xor from the cats library, but that's beside the point really). The nice thing with Future is that it doesn't really care what its outcome will be: it represents that the outcome is asynchronous, and guarantees that any exceptions thrown during this asynchronous computation will be presentable to the caller if needed be. So don't think "Oh I'm using a Future so I might as well throw `new Exception("user not found")`"; this would be analogue to throwing the same exception for a non-future computation, which you would hopefully never do :)
This answer looks simple enough: http://stackoverflow.com/a/16363444
They are an abstraction to combine disparate monadic types into a cohesive type for usage in a for comprehension. Googling further will provide more details
what about "explicit is better than implicit"? isn't it a base rule of Python style?
Here's another good one, http://www.47deg.com/blog/fp-for-the-average-joe-part-2-scalaz-monad-transformers When I was (and still am) learning this stuff I found it helpful to read multiple blogs/examples, but this is pretty much more or the same. Just showing the motivation for OptionT. I'd also note that when I started using Scalaz, I really didn't use anything else *but* OptionT, so if you go with that solution don't feel like you have to understand anything else in the grab-bag tool box that is the scalaz library. 
The play-scala template for play framework has an example for this https://github.com/playframework/playframework/blob/master/templates/play-scala/app/controllers/AsyncController.scala
&gt; I see. For a beginner it's just confusing when there are object created with and without new. For example: case class allows to drop it and normal class doesn't. Kotlin dropped it entirely (which I prefer) I think Kotlin's design is (as usual) not well considered. One of the very cool things of Scala's design is that things like `Seq(1,2,3)` work just like `List(1,2,3)` or `Vector(1,2,3)` _despite_ `Seq` being a trait. In that sense, Scala preserves the "good" syntax for broader use-cases and makes a special case (constructor call) use less "good" syntax. This wouldn't work if the good syntax was constrained to "constructor call", as interfaces can't have constructors. One of the reasons why Koltin has to use the more clunky utility methods `listOf`, `arrayOf` etc. &gt; Yes there are. These are methods. I'm not a fan of them, but calling them operators is quite misleading given the history of the usage of this term, and considering that Scala is nothing like that. (Operators usually being - globally available - statically dispatched - unable to participate in inheritance/subtyping .)
Right, is there a way to follow a for expression with a match without resorting to wrapping the for in parentheses or assigning the result to a variable?
No, to the best of my knowledge.
Just to expand on this, I’d expect them to run interactive functional tests and integration tests separately from unit tests (e.g. using `src/fun/`, `it:test`). However, the quick-and-dirty hack for the OP is that they can just make as many `App`s and `object`s with `main` methods as they like, and then use `test:run` or `test:run-main` to run them on demand.
Just write a class with a main method in `src/test` and run it with `test:runMain`. 
You are right. Thanks for your feedback. Now we are using OffsetDateTime :)
No need to import an external framework. Scala has native support for JSON parsing in its `utils.parsers.json`. In Scala &lt; 2.10 it is included natively. In 2.11 you have to import it. Note that I am using my phone for this comment or I'd tell you exactly what you need to import, but some quick googling should help
Can you expand on src/fun and it::test? Googling scala "src/fun/" and scala "it::test" does not yield any useful results.
Yes, exactly. Look through the Scala standard library. Especially look through types that may be similar to yours. In this case I highly recommend looking through the methods of the `Future` trait. It will give you a fairly good idea of what is idiomatic. In this case I would recommend using that explicit method. Instead of `?`, call it `isNotFailed`, or something similarly succinct and descriptive.
just keep trying. try to write pseudocode/algorithm/flowchart. using pen and paper helped me a lot. pick the easiest one and solve it first. let me know once u finish it :-)
If it’s the Scala that’s tripping you up, step away from the computer for a moment and figure out on paper how you’d do it manually for some given inputs (e.g. start with the basic/terminal cases), and then generalise and translate into Scala. Eventually you will become accustomed to writing it in Scala directly.
How you can detect unused code in Idea?
I like where we're headed although I don't really understand the `@static` annotation. Well, I *understand* it but it seems to go against the general gist of genericity and platform independence as it seems very JVM specific. Same thing for the @struct annotations of Scala-native really. Maybe the compiler could be smarter about these things and staticize method calls that are deemed hot enough to warrant such optimisation.
Are you saying compiler should take guesses whether you want java interop to work or not? Good Java interop has always been a goal for Scala, and there are other java-specific annotations already, varying from convenience stuff (@BeanProperty) to major super-important like @volatile. I think a compiler for another platform can simply ignore annotations if they don't make sense for that given platform.
It doesn't make any sense to make compiler flags that change APIs of produced artifacts. It is also an oxymoron to make a code that both interacts with a platform and is platform independent. Either these platform-specific features are not required, and then no annotations are needed either, or these features _are_ required, and stuff won't work without them. Just because a _language_ can compile to different platforms, doesn't make any _application_ or _library_ work on any of those platforms.
as /u/jnd-au said, `if (99.isOdd)` is always better (not just in Scala), than `if (99)`, which is a way straight to hell of weak typing (js-like) ps: in my experience, it's a big mistake to treat anyone as an adult :)
Is http4s tied to the pattern-matching approach shown in its examples? It feels like the spray route model would be more compositional and make combinators more elegant than anything based on partial functions (I really hate working with Scala partial functions), e.g. being able to create new directives by `for`/`yield` on predefined directives is really nice. I'm a big fan of functional/strongly-typed approaches, but Spray gives me that request/task-response model if I want (with a couple of ugly stateful lines for doing the chunked response thing, but I only had to do them once and can brush them under the carpet the rest of the time) in a way that I can extend to direct support for doobie (i.e. making the `transact` happen at the route level via typeclasses if I want that), and gives me very good JSON derivation via spray-json-shapeless. (It looks like circe finally gets to the same point). I guess what I'm saying is I don't see the advantage of http4s, and I get the feeling it's less mature/widespread.
I don't see how the discussion of a matter would be denying anyone anything; I'm simply voicing concern.
Since i have yet to learn front-end development and i love Scala. What arguments would you use to convince someone to adopt Scala.js? Aside from being typesafe and you know, scala. Can you easily build modern looking websites? Or should you use frameworks like /u/paultypes mentioned: Sri? 
Sorry Im also not frontend dev and I have not done enough with scalajs beyond toy project, so I am not qualified to answer that. I played with https://github.com/japgolly/scalajs-react... so sri should be fine as well... As for arguments, scala, with most of its useful libraries, type safety, refactoring, types being documentation, mantainability, you know, all the benefits of (static typing U scala U mature ecosystem (yes, I consider scalajs with sbt more mature than Node Js, you can observe their engineering practices as well) ) is more than enough of reasons. You can also check this talk: https://www.youtube.com/watch?v=Q1D4ATzXvcQ
And for anyone wondering, they released 1.0 recently. I believe scala.meta is only for building ASTs and other syntax APIs at this point and they are adding the ability to evaluate and compile ASTs at runtime in there soon, but xeno can correct me if I'm wrong. Playing with the library a bit it's really really nice so far.
I was trying to do this but can you give me a quick example on how this definition would be?
I always import at top level. I think there's too much potential for confusion otherwise.
&gt; Are you saying compiler should take guesses whether you want java interop to work or not? I think "I want Java interop to work or not" - at the class or even package level - is a far more appropriate level for an annotation most of the time than "I want this specific JVM-only optimization to be applied to this method".
I have no doubt marrying Spray and scalaz-stream can be successful. I suppose I should add that we're removing Akka from our stack, so that militates against Spray pretty directly. Looking at [Spray routing's dependencies](http://mvnrepository.com/artifact/io.spray/spray-routing/1.3.1), I see Akka directly, spray-can, etc. so that would be a factor in our choosing against it. If an argument could be made that you could use spray-routing strictly as a standalone DSL, that might change things.
`Task` isn't relevant at all, he was asking how to approach the problem specifically using `Future`. This is almost as bad as someone asking "how do you solve this problem in Scala" and someone says "This is how you do it in Python!" You come across as a door salesman selling a product that can solve every problem (in this case `Task`) more than you are answering his question (which is how to deal with errors with `Future`). Even in regards to error handling, there is an approach to using this in `Future` (instead you use this as some sought segway towards Task). Let me repost the thread title, it is **How are you using Futures** not **How are you using Task** or **are there any alternatives to Future**. Furthermore `Future` isn't doing anything "un-functional" when it comes to error handling, it just catches any exception thrown and transforms it into a failed `Future`. The other alternative would be to completely ignore exceptions, which isn't any better. If none of the code that uses `Future` throws any exceptions, it would behave the exact same way as `Task` in this regard.
This is less an "optimization", and more like "contract requirement". Static modifier applies to class _members_ on JVM. I am puzzled how exactly do people even imagine that satisfying a binary functional requirement "works / does not work" can be seen as "optimization". Apparently, as you can see in the comments js and llvm can also benefit from this. The counterstatement, as far as I see, is "it smells like Java so I dont want it anywhere near my lovely pure Scala".
&gt; Nobody's problem is "how to use Future." Future is one means to an end. Task is a similar, but superior, means to very similar ends, deliberately. In fact, let me quote the source code: No, not really, its not superior. The main difference between the two is one is eager/strict, and the other one is lazy. There are other more subtle differences which don't amount to anything significant. &gt; Completely absurd. No its quite apt, he is asking how to use `Future`, not `Task` &gt; Look, here's a hard dose of reality: the rest of this thread goes off into the need for monad transformers to handle the Future[Option[_]] case, exactly as Tim describes in his blog post. The OP, correctly, points out "Seems like if I use Futures with Option it's quite hard to use for comprehensions, what do you think?" If you think you have a better answer in terms of Future, by all means, please share it. (Hint: it doesn't exist.) Right, and you could have done it without referring to `Task`, since that what the OP was *actually asking for* &gt; All I've done is actually answer the OP's questions in an easily actionable way. No you have actually answered your own made up question (which is why I think `Task` is superior to `Future`), not the OP's. &gt; Look, here's a hard dose of reality: the rest of this thread goes off into the need for monad transformers to handle the Future[Option[_]] case, exactly as Tim describes in his blog post. The OP, correctly, points out "Seems like if I use Futures with Option it's quite hard to use for comprehensions, what do you think?" If you think you have a better answer in terms of Future, by all means, please share it. (Hint: it doesn't exist.) Yes its called monad transformers, which is completely orthoginal to `Task`. In fact your colleague actually answered the question without even mentioning the word `Task` here https://www.reddit.com/r/scala/comments/4onz3j/how_are_you_using_futures/d4ehvbl &gt; No one claimed Future was doing anything "un-functional," and here we get to the crux of the issue. You don't like pure FP or scalaz. I get it. Unfortunately for you, it has no bearing on what the best answer to the OP's questions is. You do realise that I (and coworkers at work) use cats (and also monix/monifu `Task`) at work, right? I have nothing against pure FP, but thanks for the ad hominem. &gt; All you've done is throw a tantrum because I had the temerity to suggest using a different library than the standard library to accomplish the OP's goals. Actually I couldn't care less if it was `Future`, or some other 3rd party library. &gt; In the meantime, stop, think hard, and consider why the OP hasn't complained about my answer I don't need to consider anything, apart from the fact that the OP probably ignored your answer (since its the only answer he didn't respond to in the thread). Might be because they actually answered his question, who knows. &gt; why it is you feel compelled to not answer the OP's questions, give me a profoundly ignorant grief storm, and generally be a pain in the ass. /u/runT1ME and others already answered the question, I would just be repeating their answers. I just upvoted their answers, totally normal behaviour here ;)
I am afraid I still don't quite understand how can one "desugar" a lower-level construct into a higher-level abstraction in general, and what it could mean in case of these particular ones (because then the "desugared if" would have to be desugared into (or contain) a common if.. which is where we started in the first place). Could you possibly help me find where to read more on this?
These things don't have to be primitives, and I'm not used to thinking of them as primitives - if you come from a Smalltalk background, the primitive is polymorphism, and constructs like `if` and `while` are just ordinary polymorphic methods. You're right that they're represented as primitive opcodes in the JVM bytecode, but I view that as an implementation detail. You are aware of how `for` already works, right? Something like `for { x &lt;- xs } foo(x)` desugars into `xs foreach foo`, and `for {x &lt;- xs } yield foo(x)` desugars into `xs map foo`, and other variations desugar into calls that use `flatMap` and `withFilter`. The proposal as I understand it is something like: `if(x) y` would desugar into `x.ifTrue(y)`, `if(x) y else z` would desugar into `x.ifElse(y, z)`, and `while` would do something similar (though it would need to somehow capture the conditional lazily? I don't remember the details). Within the abstract Scala language, the `Boolean` type gains two extra methods, `def ifTrue(block: =&gt; Unit)` and `def ifElse[A](ifTrue: =&gt; A, ifFalse: =&gt; A)`, and `true` and `false` become `object`s that override those two methods (i.e. the way you'd do it in Smalltalk). In practice, you would want the compiler to spot the common pattern of `if(x: Boolean)` and compile it down to the usual opcodes for `if`, not compile it as a virtual method call, but that's an implementation detail.
Fascinating video from the vantage point of a non-academic here. The two things that are nagging at me are: 1. Would the removal of the "late init" feature negatively affect the use of dependency injection methodologies? From what I hear, this feature can be pretty important when passing user code to frameworks like in Android... 2. Would the removal of general type projections weaken the expressivity / power of the type system to the point that it no longer was Turing Complete? Would it limit the ability to simulate dependent types? Some languages are looking to increase support for approximating dependent typing and if this change moves the language further away from these capabilities it seems like a bit of a loss... Though M.O. did mention they already tried to remedy the situation, so maybe the thought process is that soundness is more important? EDIT: Spelling and clarity
Really great work so far. I dream of a future where I can just type: @foobar BusinessObject(x: Int, y: String) And automatically generate a JSON serialization typeclass instance, a repo object for reading/writing from a database and RESTful endpoints for accessing it. How close do you think we'll eventually be able to get? Or is some/all of that completely off the table?
Is there an up to date tutorial for basic usage of current version of Scala IDE? There are many online "Hello World" tutorials, but they have missing steps, e.g. Run Configuration. Do I have to read a book about Eclipse to learn about these type of concepts?
I see... but whats the point? Though I'm probably asking a wrong person here. I mean, checking a predicate in `if` or `while` is simple and straightforward, one can put his custom logic there, what does it not cover? And there are higher-order ways of composing control flow, in case one needs rich semantics. On the other hand, "implementation details" can be quite important sometimes. It would absolutely suck if one has to go back to java if he needs more control over performance. So I think it is important for Scala to expose a set of low-level (on Java scale of course) reliable primitives that the compiler does not get too smart about.
I use guice. It's simple and gets the job done. No complaints.
I'll bite again; not trying to disprove you or anything, just giving my reasoning. It's not that I vehemently want to avoid everything Java but rather that I feel this goes against the grain of the Scala feature set. The `object` meta-type as it stands is arguably a better way of doing what Java is doing with static members of classes: A middle ground where a singleton instance can implement a contract and be passed around, while still having members that behave much like their static Java counterpart. By reintroducing static as an annotation it will be possible to declare static members of classes *as well as* members of a companion object. We end up having more solutions than problems, a broader and overlapping set of concepts that may seem ambiguous to both newcomers and more experienced users. The contract requirement you speak of is only interesting from the other side of the pond: as a tool for interacting with other platforms. To me, this seems like a leak of other platforms' concepts into a language which is meant to be platform agnostic. I'm not saying it is an unholy union that will doom us all but I am concerned about how this will play out against the stated goal of minimizing the syntactic footprint and the mental requirements for working with Scala.
IoC/DI is a good idea. I mostly do it by hand, because I don't think the amount of code you save by using something like guice makes it worth having magic annotations in your codebase, but I can live with guice or (java-defined, constructor-injection-based) spring.
Run configurations should be generated for you automatically when you try to run an executable - make sure you have everything else correct. In particular check that there's nothing in the "problems" tab, and that your source file is showing up as scala source (i.e. your project has the scala nature and your source folder is visibly a source folder). But yeah, "Scala IDE" = Eclipse + a small plugin, and Eclipse is large and complex; I would expect tutorials will talk about the Scala-specific parts rather than trying to recapitulate an Eclipse tutorial, so it's probably worth finding a basic Eclipse tutorial too.
I do agree that things like that are adding to complexity, and Scala has plenty of it already. But is is more important (in my opinion) for Scala to be a practical tool that can get a job done - this is the reason it gained such popularity in the first place. We're not using it in fintech for aesthetics, for example. I would argue about "platform-agnostic" part. "Write once, run anywhere" was a reason JVM was made in the first place (and even that was not completely successful). If one provides a common API across different platforms, then that API becomes a platform, this is exactly what JVM does. But Scala does not aim to create it's own new platform, it literally targets different platforms. This was also a reason for scala native - to escape the JVM sandbox.
+1 to [@paultypes](https://www.reddit.com/user/paultypes) for taking the time to explain this in details. This is more or less the way I would solve it. 
&gt; From someone who only used futures/promises do you think it is worth to look into a bit of scalaz or just start using http4s and doobie and hope to slowly start getting into the concepts? This may surprise some who think I'm a hopeless fanboi, but: the latter. If you have a look at the [SydScala](https://bitbucket.org/da_terry/scalasyd-doobie-http4s) code, I think you'll see it doesn't scream "Look, I'm using FP and scalaz!" I think that's a _good_ thing. FP should be like _good_ visual effects: helping tell the story without drawing attention to themselves as such. That said, Eugene Yokota's [Learning scalaz](http://eed3si9n.com/learning-scalaz/) is a _great_ resource, and just firing up Li Haoyi's amazing [Ammonite REPL](http://www.lihaoyi.com/Ammonite/#Ammonite-REPL) and saying: load.ivy("org.scalaz" % "scalaz-concurrent_2.11" % "7.1.8") and working through it is _very_ worthwhile, IMO. &gt; I already have a significant background on Monads composition, category theory, etc but it's all mostly theoretical It'll come in handy, although sometimes you may wonder exactly what category you're working in. Sometimes we talk about "Scal, the category of all Scala programs," but rarely. But if you're familiar with Kleisli composition, monads, monoids, functors, semigroups, etc. you'll definitely have an easier path. &gt; I mentioned tasks because allegedly they "solve" some issues that scala Future has? Right now i am not sure why scalaz tasks are better than just using promises/futures but i remember there was a reason. The short answer is the same as it always is: referential transparency. For a longer answer, see the [comments](https://github.com/scalaz/scalaz/blob/v7.1.8/concurrent/src/main/scala/scalaz/concurrent/Future.scala#L16) on `scalaz.concurrent.Future`. Personally, I think what I appreciate most about `Task` and `Nondeterminism` is how well they support wrapping callback-y threaded Java APIs (with `Task.async`) and how easy it is to run a bunch of `Task`s that can fail in parallel, accumulating any errors in a `NonEmptyList` or aggregating the successful results in a `Monoid` of my choice (with `Task.attempt`, `\/.validationNel`, and `Nondeterminism.aggregate`). At work, I wrapped [ftp4j](http://www.sauronsoftware.it/projects/ftp4j/) in `Task`, which is plenty nice enough in and of itself. But in this case, the big win was being able to use the ftp4j `Task` with [scalaz-stream](https://gist.github.com/djspiewak/d93a9c4983f63721c41c) to build an ingestion pipeline for the XML files on the FTP server in question, since all it takes to turn a `Task` into a scalaz-stream `Process` is `Process.eval(myTask)`. scalaz-stream is _very_ worth learning, BTW, but that's another post. :-)
Hello! I was looking through the mod queue and the last three of your posts were all automatically removed somehow, despite you having an account over 3 years old, and obviously not being a spammer. Not sure if you were shadowbanned, or something ticked off Reddit's Spam filter.
We have made also a version to get it as an activator template: http://www.lightbend.com/activator/template/play-scala-slickpg-play2auth
I had to change my password for "suspicious activity" about a week ago. I had to message the Reddit admins to get un-shadow banned. Thanks for following up on my posts though! :)
That's because you're doing it in the repl, which introduces another layer of opacity. Your classes, defined on different lines, are in completely different packages. scala&gt; class A_ { class plus } defined class A_ scala&gt; val a = new A_ a: A_ = A_@67f89fa3 scala&gt; println(a.getClass) class $line5.$read$$iw$$iw$A_ It turns out that these days it warns you when you compile those conflicting classes together. Not that the warning has anything to do with the problem. It's a warning for a different, completely separate problem, which happens to be triggered in this situation. (This situation has nothing to do with case-sensitivity: the classes will overwrite one another on ALL filesystems.) scala&gt; class A_ { class plus } ; class A_+ &lt;console&gt;:11: warning: Class A_$plus differs only in case from A_$plus. Such classes will overwrite one another on case-insensitive filesystems. class A_ { class plus } ; class A_+ ^ defined class A_ defined class A_$plus scala&gt; new A_+ java.lang.NoSuchMethodError: A_$plus: method &lt;init&gt;()V not found ... 32 elided 
[Here's me trying to do something about a significantly easier problem.](https://issues.scala-lang.org/browse/SI-2034) It concludes with the scala compiler lead observing "changing name mangling is an epic undertaking." "Simon is not up against a few binary incompatibilities. He is up against the binary compatibility apocalypse."
You mean, like where runT1ME suggeset using scalaz monad transformers? As someone with absolutely no dog in this fight, I found paultypes's answer super informative. Pointed me towards a set of tools I was unaware of and am now interested to investigate. I'm also learning a good deal from the other answers. This isn't SO. Answers here tend to be more meandering and conversational and discussions rather than specific exact question/exact answer formats are the norm. Different spaces for different kinds of interactions. I have no idea what you're so upset at with someone taking the time to write up a long, thoughtful, informative comment that has information that's clearly directly related to the question posed. How can more information be *worse* for the discussion? 
But it's not just removing curly braces. It's replacing curly braces with indentation sensitivity. 
Oh, I agree. It has extremely limited value and almost nobody uses the capability. Even for those who know about it and might be tempted, the aesthetics are stomach-turning. Personally I'd much rather mandate that identifiers must be separated by whitespace or other delimiters outside the identifier set (e.g. parens, brackets). This would make the underscore rule unnecessary and enable useful but unavailable identifiers like A'. Far too much energy is expended inventing and shuffling around names which are only locally meaningful, and only meaningful that far to the extent that they are distinct from other local names. 
Because alphanumerics are valid in identifier (type, method, and value) names. E.g. `1.+(2)`, and `scalaz.\/`.
AWS Beanstalk is great imo (but traffic is expensive)
&gt; You mean, like where runT1ME suggeset using scalaz monad transformers? Yes (or monad transformers in general, they don't have to be from Scalaz) &gt; This isn't SO. Answers here tend to be more meandering and conversational and discussions rather than specific exact question/exact answer formats are the norm. Different spaces for different kinds of interactions. Sure, but the poster was also asking a pretty specific question. In SO you are only allowed to make specific questions (which you may answer yourself). However if someone asks a pretty specific question, often they are looking for a pretty specific reply. &gt; I have no idea what you're so upset at with someone taking the time to write up a long, thoughtful, informative comment that has information that's clearly directly related to the question posed. How can more information be worse for the discussion? I am not upset at all, I am just pointing out that he wasn't really answering the OP's question. I didn't want this to get dragged into some long debate.
You're not supposed to compile it on the server. On your laptop you can run the dist command to produce a zip you can upload and extract, or other commands to produce other artifact types via sbt-native-packager
&gt; By reintroducing static as an annotation it will be possible to declare static members of classes as well as members of a companion object. In fact, no. @static members are only allowed in companion objects, never in classes. So, roughly speaking, it's just a hint how they should be implemented.
I have only skimmed through other answers here, and parts of my answer are unavoidably going to be a rehashing of the things others have already said, but I think it may be worth posting anyway. As our codebases in one of the projects I have worked on grew, we had a number ways of dealing with failures in context of futures: * `Future[A @throws[E]]`: A future where the underlying computation can yield a value of type `A` or fail with an exception of type `E`. (Scala does not track the exception effect; not today at least. I just invented that notation for the purpose of this discussion.) * `Future[Option[A]]`: A future that yields a value of type `Option[A]`. `Some[A]` indicates success. `None` indicates failure; more specifically, an "absence" of some sort. The exact meaning would depend on the context. * `Future[Either[E, A]]`: A future that yields a value of type `Either[E, A]`. `Right[A]` signifies success, while `Left[E]` signifies failure. * `Future[Try[A]]`: Similar to above, except the failure part has to be a `Throwable`. * `Future[BananaResult[A]]`: Say you make calls to an API called Banana. You may end up creating a custom ADT called `BananaResult` to capture the semantics of Banana API. There are some obvious problems with having these many approaches to communicate failure: * The interception, recovery, etc for each is different, leading to inconsistencies in code, incurring needless complexity and cognitive overhead. * The data types would often need to be interconverted. That adds some more glue code that has nothing to do with the domain or the business. Signal to noise ratio is affected adversely. * The nested data types among these are awkward to deal with. The logic is drowned in the sea of pattern matches and lefts and successes and what not. That said, each approach has its pros and cons. We wanted to cut this down to just **one** data type, that combines the best of all of these worlds. We defined a new data type `Async[E, A]`, which is essentially a wrapper over `Future[Either[E, A]]`. Some more details on this unification effort: * We made it a point not to throw exceptions for the erroneous behaviours that we care about. We started returning more and more `Either`s. We added our own type hierarchy distinct from `Exception` for the failures that would live in these `Either`s. * The failure type is tracked at type level (`E`) giving you more type-safety. * `Async` has many factory functions to initialise from `Either[E, A]`, `Future[A @throws[E]]` (these will be coming from underlying libraries you will call) etc. * With `Option`s, the meaning of `None` is context dependent. Up the stack where this value might be used, this context can be lost. This inhibits our ability to recover/handle better and to product good error messages. We replaced many `Option`s with `Either`s where the left side will provide information about what exactly went wrong. * Like it or not `Exception`s are the reality of the world. Of the JVM world, at least. And you need to handle them. We only handle them at the system edges, such as in controllers to produce 500. * `Async` defines `map` and `flatMap` functions (and the whole shebang) that deal with both `Future` and `Either` effects in one go, so you never have awkward pattern matches. You will have quite linear code with `for` comprehensions. * In cases where you must have a value-level access to failures, there is `Async[E, A]#reifyFailure: Async[Nothing, Either[E, A]]`. * We removed custom "result" ADTs like `BananaResult`. We instead replaced them with two distinct ADTs (or, hierarchies) `BananaSuccess[A]` and `BananaFailure`, and started using `type BanaResult[A] = Either[BananaFailure, BananaSuccess[A]]` instead. * We defined some extension methods on `Seq` that made dealing with asynchronous code almost as easy as regular code. Observe the signatures: % Seq[A]#map(f: A =&gt; B): Seq[B] Seq[A]#mapAsync(f: A =&gt; Async[E, B]): Async[E, Seq[B]] Seq[A]#flatMap(f: A =&gt; Seq[B]): Seq[B] Seq[A]#flatMapAsync(f: A =&gt; Async[E, Seq[B]]): Async[E, Seq[B]] Seq[A]#filter(f: A =&gt; Boolean): Seq[A] Seq[A]#filterAsync(f: A =&gt; Async[E, Boolean]): Async[E, Seq[A]] // etc After these transformations, we ended up with a fairly regular and consistent code. We even discovered and fixed some missed error handling cases and bugs in the process, and also improved our error handling. The kind of side-effects we love! We have since evolved and used this utility on more projects. **Aren't these just monad transformers?** If you really asked that question: Yes, `Async[E, A]` could have been implemented as `EitherT[Future, E, A]`. However, there are some problems with it: It's quite noisy to use it directly. (`EitherT` calls everywhere). It would inhibit us from achieving some of our future visions. And it requires Scalaz/Cats, libraries that not all teams wish to have as dependencies. (Please don't hate me for writing this. I love Scalaz and Cats as much as the next person, but there are good reasons for why some teams don't want to use them.) If you did not ask that question: Monad transformers are cool! You may want to spend some time and learn about `EitherT` (and other `T`s). A nice slide deck here: https://speakerdeck.com/eamelink/flatten-your-code. **Future directions** Today `Async[E, A]` LUBs failures on the left side. If you have a `JsonPathNotFoundFailure` from one computation and `JsonKeyOfWrongTypeFailure` from another computation, you will end up with `Async[JsonFailure, A]` when the two computations come one after another. We are planning to avoid this LUB'ing, and track all the failures at the type level, like so: `Async[JsonPathNotFoundFailure :+: JsonKeyOfWrongTypeFailure, A]`. `map` and `flatMap` would ensure they concatenate all the failure types. `recover` would remove the failure type that has been handled from the tracked list. And so on. Don't be scared. You should not have more than four or so failure types in one `Async`. Each layer must respect the abstraction boundaries and not leak anything too specific to upper layers. Even with four, the type annotation is admittedly long, and in such cases, you would use type level aliases. Hope this helps.
I have some good news for you, akka is a great use case for this and we are currently using this in production for our webrtc recording app! And yes I highly recommend to have your mixing process take place elsewhere or when you feel there will be a lull in traffic. I recommend ffmpeg but thats a whole other kettle of fish depending on how you handle muted or null audio ie (cutting the stream entirely or keeping the stream up but passing no audio). You should be able to wrap the kurento media server to be controlled by an actor and then throw a rest layer ontop of that to control when you want the "dummy peer / recording peer" to join and begin recording. You will want to segment the remaining parts such as reassembling the audio streams down the line and have that get kicked off when the webrtc "session" no longer has any users or peers within it. Ok then now for your other question for starters the 10$ digital ocean server is plenty for a simple play rest api it wont handle much load at all but you should be able to record 2-3 users in a webrtc session audio only. Be careful though as DO instances have very little disk space space due to being all ssd's all the time. I recommend staying away from heroku and google app engine and the like since you are doing a pretty off the rails exercise and will need as much control of your servers as you can manage. 
 scala&gt; val xs = Vector("test", "hi") xs: scala.collection.immutable.Vector[String] = Vector(test, hi) scala&gt; xs.sortBy(_.length) res0: scala.collection.immutable.Vector[String] = Vector(hi, test) scala&gt; xs res1: scala.collection.immutable.Vector[String] = Vector(test, hi) By this logic, a regular user would also assume that sortBy would sort in-place and therefore sortBy is poorly named. I don't see any real problem with count_++. Like given what you set up, maybe it's confusing. But I really have my doubts that it could cause any problems in the wild.
&gt;Scalaz is to Scala what Guava is to Java : a must to know library ! But unfortunately, learning scalaz is not so easy. first sentence is enough to turn anyone away ;p both parts..
I remember seeing something like this before, and as a fan of both Python and Scala, it looking pretty orgasmic.
Which Scala version are you using? I cannot reproduce this in any way in Scala 2.11.8.
So are non-nullable types not considered? 
Looks odd to me. It should work. Is it possible that there exist two different `DataSetPreProcessor` types in different packages and you imported the wrong one? If you use an IDE like IntelliJ, you should be able to paste the unimplemented method by calling `Ctrl-I` (implement).
thank god for that plotting library, will take a look at it!
It depends on the scenario — if you want to filter and map a collection in one go, collect is your dude. I'd recommend using `Success(value) =&gt; value` though (and the type parameter is ignored so you might as well remove it) edit: unless, of course, you are dealing with `someSeq: Seq[Any]`, in which case the underlying issue is that the Seq contains arbitrary data
Null is already its own type, and currently in Dotty every type that isn't an union `X | Null` can also be null, so there's no typesafety enforced. I'm not saying remove null, I'm saying make it explicit just like the union above.
It's not a problem if it gets squashed on merge.
If we're going to have convenient Java interop (which is maybe not the priority it once was) then we need to either allow null everywhere or at least have a *very* concise syntax for coercing nullable-X to X. 
I can’t imagine what problem you have with `.collect` or pattern matching, but because partial functions are a large field, the answer is ‘it depends’. Your example isn’t necessarily the best example of using collect, but it’s impossible to say without context. Partial functions are common for pattern matching, but they also crop up in other ways. Technically every Java function is a partial function since it can throw unchecked exceptions, but some like `seq.head` are ‘designed to fail’ and are better done via alternative means like pattern matching (for extraction), `seq.take(1)` (if you need a sequence), or `seq.headOption` (if you need an option), depending on the context. PartialFunctions (the type) can commonly be used for pattern matching and case switching.
Because imports often introduce implicits into a scope, I've tried (with only moderate success—old habits die hard) to put _all_ imports in the narrowest scope possible. This is another one of those things that tends to make Scala code resemble Python (where the same strategy is often used to break cyclic imports) and makes ex-Java developers heave, but the plain fact is that "import" means quite a bit more in Scala than it does in Java, so scoping is more important.
I'd work through Berkeley's [AMP Camp](http://ampcamp.berkeley.edu/5/) exercises on your own time.
I would use google cloud or aws. Spin up and shut down the box as needed for compiling. Only pay for what you need.
Is there any realistic possibility that it might be possible to use XML literals via some library with future versions of Scala? I kinda depend on them...
All right, thanks for the info. That's definitely better than nothing, but I wonder how well that'll support syntax highlighting, indentation, and compile-time well-formedness check, all of which are really nice.
I believe IntelliJ already does syntax highlighting for custom string interpolators. If the xml interpolator is a macro then you can get compile time checking.
https://mitpress.mit.edu/sicp/full-text/book/book.html I'm not telling you to learn Scheme instead. The language is incidental, the structure directly corresponds to much of what is being covered in that online course.
That's great news. Thanks for the information!
I built a poller in the Play framework before and what you will need is to use the akka scheduler and schedule tasks. Another approach for polling is to have 2 polls instead. The first is the poll of the resource itself, so your going to hit a microservice and get some data. Once you get data, fill in an internal buffer. The 2nd poller is polling this buffer and this enables you to control how much data the client gets back. If your concern is threading go ahead and make your own thread pool, its what I did when I implemented kafka that interacts with my Play app. If your polling a microservice and its over http, then its non-blocking if its done via Play since their networking calls are non-blocking so don't worry about blocking a thread too much. For the poller itself, I use actors that will perform a task and has a future callback. 
Do you know why support for XML literals is being dropped?
Based on what Odersky said in talks, it's because XML is no longer the big thing it was going to be, and so it doesn't make sense to make it core to the language. At least it doesn't make more sense than, say, having JSON part of the core of the language. A counterpoint to this argument would be that front-end technologies now have things like [JSX](https://facebook.github.io/react/docs/jsx-in-depth.html), and therefore that XML-ish syntax is still very much current. Further, Scala's XML library (`scala-xml`) is, by most accounts (including mine), pretty bad. In theory you could dissociate Scala's ability to parse XML-ish syntax from `scala-xml` and fix that. Still, I doubt the Scala deciders will go back on the decision, given that even if you agree that XML-ish syntax would be a good thing, string interpolation will provide a reasonably good alternative and be more flexible. 
How is this unrelated to ML? Or are you referring to the programming language ML? Then the bad news is, machine learning is all over the place, and so there is every reason to use the acronym ML for it.
The plan is to make the refactoring completely automatic. Translating xml to interpolated string literals is one of the easiest tasks of an automatic rewriting tool. And of course IDEs will be able to do syntax highlighting in such literals, just as they do now. The main advantage of dropping xml literals from the language are: - library and compiler are disentangled, which means it will be easier to provide alternatives to scala.xml. - The spec will be simplified. Technically speaking a full Scala spec would have to contain the full xml spec as a part. But the xml spec is probably several times larger than the rest of Scala. So again, a disentanglement of Scala spec and XML spec is a relief here. 
And, forgot to mention: compile-time welformedness checks will be provided by making the `xml` interpolator a macro.
Alright! Just what I was thinking Scala really needed -- another JSON library! 
I think it's a solution looking for a problem.
I appreciate author's effort and time he put into it, but these are exactly my thoughts.
:) ^i ^am ^a ^bot, ^and ^i ^want ^to ^make ^you ^happy ^again
Describe better your reasons and goals. Most languages are crowded with yet another libraries. But with good reasons, docs, tests and support it works out!
Thanks for the clarification. Given the rise of React and JSX I wonder if pure literals syntax (i.e. non-quoted) based compiler plugin will appear in the ecosystem. I say that because I can't see many developers champing at the bit for string interpolated xml regardless of IDE support; it simply does not read as well, particularly wrt composition, just too much syntactic noise. Anyway, far more interested in production Dotty, Godspeed!
I like the fact that it comes with some neat usage examples. I'm curious about the performance numbers relative to existing libs. 
But author have is own goals and interests, maybe he will make new standard, who knows
Also if you can join the existing team that work on the top competitive library - maybe it will be productive for everyone. Havin own name on the library is great, but working in well established team can give you a lot more. Also having your name in contributors or mantainers list of evereyone-used lib can be really helpful to your career. Anyway, this is a huge plus that you published your lib :)
What has it got over argonaut | circe ? 
What I think trips most people up is the pattern matching in the first assignments. I work with some pretty solid programmers, who still needed a moment to get comfortable with pattern matching. Just keep going with it and it'll come eventually :) 
Don't know how much it gets better but the longest class names aren't necessarily closures so I don't expect the range to change much, even if the average length will go down. You know about **-Xmax-classfile-name**?
I mean, you could say that, until you're regularly dealing with things like ... } } } } }
Macros won't be dropped only the "reflection-based kind", see [slide 30](http://image.slidesharecdn.com/scaladays-nyc-2016-160510135215/95/scala-days-nyc-2016-30-638.jpg?cb=1462901685) from [here](http://www.slideshare.net/Odersky/scala-days-nyc-2016).
So the Java approach is to create a separate class that you translate to and then serialize with reflection -- generally handled by some generic translation layer. Play takes the approach of formatters, where you create functions to take you from your data objects to the json ast. (I like this approach the best). You seem to be taking the Java approach, but providing some conversion methods (like js) -- what's the point of using your library over just Jackson with scala friendly (de)serializers? 
 I think it'll be (like?) [scala.meta](http://scalameta.org/). Edit: [There is a thread](https://www.reddit.com/r/scala/comments/49caes/scalawags_38_the_sound_of_dotty_w_martin_odersky/) with (explanatory) comments on the future of macros.
Does this proxy use NTLM authentication? http://stackoverflow.com/questions/1251192/how-do-i-use-maven-through-a-proxy
I would like to push back a bit on the following two statements: &gt; Technically speaking a full Scala spec would have to contain the full xml spec as a part. I don't think that is the case. Scala does not really support XML as in [Extensible Markup Language (XML) 1.0 (Fifth Edition)](https://www.w3.org/TR/REC-xml/). What Scala supports today is a superset of a subset of the XML 1.0 spec deemed relatively appropriate for embedding within Scala (the [spec](http://www.scala-lang.org/files/archive/spec/2.11/10-xml-expressions-and-patterns.html) currently says "It follows as closely as possible the XML 1.0 specification, changes being mandated by the possibility of embedding Scala code fragments."). Here are examples of differences: - There is no literal DTD support (thanks $deity). - Character encoding issues are not relevant because the encoding comes from the Scala source file. - Scala supports embedded expression within `{...}` brackets for expressing attributes and text as Scala code fragments. And there might be more differences. That's why I refer to Scala's XML support as being XML-ish. It is not strictly XML 1.0, but it is of course close from it. The `scala-xml` library also supports a custom XML document object model, parsing and serialization, but that has little to do with Scala's support for XML literals. A core specification for XML-ish literals should of course specify what the compiler does with the result of parsing literals and embedded expressions, but that could be very small and modular, and doesn't need to rely on any existing XML specification (as is already the case now). &gt; But the xml spec is probably several times larger than the rest of Scala. So again, a disentanglement of Scala spec and XML spec is a relief here The XML 1.0 spec is 37 pages when printed as PDF, and a lot of it is not relevant to embedding within Scala (there are probably 10 pages related to the DTD syntax). The [Scala 2.9 spec in PDF format](http://www.scala-lang.org/docu/files/ScalaReference.pdf) is 191 pages long. I don't think a fully-formalized specification for XML-ish literals in a Scala spec would need to be much longer than what's [currently in the spec](http://www.scala-lang.org/files/archive/spec/2.11/10-xml-expressions-and-patterns.html). I might be wrong but I don't think there is or ever was a requirement or desire to fully embed XML-as-in-the-XML 1.0-spec as part of Scala. The desire is more to have a "pointy brackets" syntax which is close enough from XML or HTML and which can be used as templates for producing XML or HTML. At least this is how Scala's XML literals are used today, and probably why some users will be sad to see it go. I am personally a bit ambivalent to see XML literal go, not because of XML proper but because "pointy brackets syntax" is not going away soon it seems (see HTML, JSX and the likes). Still, my purpose above is not primarily to push for keeping XML literals in a future version of Scala, but I would prefer if the arguments for taking them away remain more factual.
I recently found that some software which used an old version of apache httpclient (4.x) under the hood suffered from this. I couldn't find a way to make it authenticate using a certificate deployed in the truststore and after poring through old apache code came to the conclusion that httpclient was to blame for forcing a null truststore and ignore the environment variables. I wonder if the default ivyresolver SBT uses relies on it? If so, maybe there is an ivyresolver which doesn't?
What's the point to another reflection based JSON facade over Jackson?
Thanks. Looks like others have the same issue: https://github.com/sbt/sbt/issues/2365 https://github.com/sbt/sbt/issues/2536 Unfortunately it's a graveyard as for any actual answers...
its not reflection based, all compile-time unless you *need* reflection for the case of message passing (for example), and thats possible through the registry only. 
The subset of XML that Scala supports is much bigger than the XML 1.0 spec. For instance, that spec does not even talk about namespaces, one of the most complex (and sort of supported by Scala) aspects of XML. I am not an XML expert (nor want to become one), but I definitely got the impression that XML added a lot of complexity and in the end is not useful enough to pay that price. The fact that Scala does not exactly follow the XML spec makes it worse. It means that we really should have provided a full spec. I believe our energy is much better spent elsewhere. 
The only other suggestion I can think of is using a repository manager like artifactory or nexus. They may cope better with the MITM cert, or you might be able to get your IT / Infosec people to agree to a clear route out for the server since it's controlled. 
Thanks for your reply! I am getting this error though when I do something like this. [code image](http://i.imgur.com/SxBeWEZ.png)
YES!! That worked!!! Thank you very much Trui92!!!!!! :) https://gist.github.com/shehaaz/7ea265d8b43006d7b395a035c3479925
Yeeei! :D
Nothing stops you from manually re-implementing the late initialization pattern if/when you need it.
Point taken. Thanks for clearing things up!
[obligatory xkcd comic](https://xkcd.com/927/)
Hey Eugene, what is gonna happen for whitebox macros ? 
In many cases the AST is the most important distinguishing factor between competing JSON libraries - the main reason to write a new JSON library is because you think you can do the AST better than the others.
you can call the first one like this: measure(myHistogram) { //do something }
yes
Yeah with brexit doesn't seem the best place right now, i would agree. I've sent you a message. Thank you !
Yes. Please come work for us in Rotterdam or Paris http://www.lunatech.com/careers
It varies between teams. There's a lot of FUD about ScalaZ, but it's not helped by the project's deliberate policy of not documenting anything and the way its founder enjoys upsetting people who come asking for help. I've found most codebases need some of the functionality that's in ScalaZ, but will often prefer to write their own alternative with more readable names and better documentation (hopefully Cats will become a reasonable replacement for this kind of use case pretty soon). Honestly if you understand a piece of functionality from ScalaZ well enough for it to be worth using at all it's usually pretty straightforward to implement it yourself. Custom typeclasses are rarely necessary IME. Certainly when you need one write it, but they end up taking a lot more code compared to an abstract method that you implement in subclasses, so prefer that approach when it can do what you need.
Immediately before opening /r/scala today I thought to myself, "I can't imagine I'll read anything Brexit related there." We live in interesting times.
lol
yep, just using jackson to build the intermediary AST from a JSON string. For case classes I'm using macros to assemble the 'factories' that go from the AST to the typed structure. The base accessors and scala collections are statically defined factories. 
The future of whitebox macros remains to be seen. On the one hand, whitebox macros are harder to understand and support in IDEs than blackbox macros. On the other hand, whitebox macros have already found use in strategic libraries of our ecosystem. We’ll need to strike the balance between the desire for simplification and compatibility.
It is true, I am paranoid- but it is based on bad experience - because often I need function `Foo`, which is now in fat class, so I get another 10 methods I don't need. And class has lots of dependencies to support those 10 methods. Which I don't need. And thus I am in world of pain.
May I ask how big is your team? How long are you (as team on average) doing Scala? Are you happy with it?
Dependencies in what sense? If you're worrying about the number of jars on your classpath, my advice is: don't. If there's a class you'd like to inherit from that requires you to implement too much, maybe you can factor the part you want out into a mixinable trait, or into a class that's composed in and you then delegate to (which adds overhead to the code, but still less than a typeclass does IME).
Lunatech allows working from home if the client allows it. Unfortunately, we do not have a lot of such clients currently. On my current project, I do work from home occasionally, but it's by no means structural. That being said, we do have a policy where we won't send someone to a project if their travel time is over a certain limit. I need to check the actual number, but I think it's somewhere around an hour. In some cases, we can work on the train, which takes the sting out a little. In my opinion the non-boring work is worth the commute, but I guess everyone has a different limit. If you're interested, please get in touch, so we can discuss these things a little more!
I'd read [this](https://github.com/scala/scala/pull/2848), but basically: 'name-based' pattern matching requires that an extractor return a type T with particular methods instead of an Option[T]. The advantage is efficiency- 'name-based' pattern matching avoids some overhead. The way it is phrased on the Dotty website ("based on name-based patmat") makes it seem like they do something slightly different, but the key point is that they avoid Options and the extra allocations/performance hit they entail.
Some of functions in Scalaz Stream such as foldRight are recursive (not tail recursive) and so prone to stack overflow (https://github.com/scalaz/scalaz/blob/series/7.3.x/core/src/main/scala/scalaz/std/Stream.scala#L55). You may think that issue with foldRight is easy to fix, just re-implement the foldRight via foldLeft (tail recursive and a reverse function); problem solved;well to some extend, you do not face the stack overflow issue anymore but you loose one behavior of the original foldRight. The second argument to f is called by name (=&gt; in front of the argument type B) and so the function is not restrict and can be used for short circuiting (refer to "exists" in https://github.com/fpinscala/fpinscala/blob/master/answers/src/main/scala/fpinscala/laziness/Stream.scala#L82). Once you re-implement the foldRight via foldLeft then you lose the mentioned behaviour. That is a kind of leaky abstraction.
So first thing: the first syntax is a syntax sugar for: def measure[A](histogram: Histogram): (=&gt; A) =&gt; A = ... So, as you know, a method which returns a function, i.e. (more or less) a curried function. Second, when you call a polymorphic method like `measure` and bind it to a name like `time`, Scala has to 'monomorphise' (choose a concrete type for) the value `time`. But, in the binding of `val time = measure(...)`, there's no indication of what the concrete type of `A` should be. Scala only knows it has to be some concrete type `A`, and the only type that satisfies that is `Nothing`, which is the subtype of all Scala types. So by elimination it arrives at `Nothing`. Then, when you call the `time` function with something of type `Unit`, there is a type mismatch because it expects `Nothing`. The solution is that the `measure` method was really designed to be called as /u/corvus_192 [described](https://www.reddit.com/r/scala/comments/4pjejo/question_anonymous_function_as_parameter_vs_next/d4m2m7c) because passing in the second argument immediately allows Scala's local type inference to kick in and infer `A` to be the type of the second argument.
I took the "progfun" Coursera course and audited the original Reactive Scala Coursera course a couple years ago, before doing any professional Scala work. The first class, especially, was a great way to get the toolchain up and running and to get used to many of the basic idioms. If you really want to learn application development in Scala, I haven't found a great one-stop resource. That said, if you like the language, I'm a firm believer that you can basically learn it as you go. You might try putting together a Play or Akka app. As for jobs, we're hiring at Artsy. I'm lead engineer of auction products, and we've reimplemented our core bidding logic and APIs as an Akka app. Elsewhere at the company, we also use Scala in our data science work. We hire remote engineers. https://www.artsy.net/article/artsy-jobs-full-stack-software-engineer
It is essentially name-based pattern matching. We tried to clean up the rules a bit. A SIP for this (also for Scala 2.x) will be forthcoming.
Will working with Qt for instance require a lot of non-scala like code?
Qt requires integration with C++. I don't think that it is even possible with the current Scala native. It is much easier to use a pure C library. Take a look at libraries like Gtk or libui. It requires a lot of boilerplate writing definitions, but at least it is doable. 
Checked out your site and glad you posted here. I now have another page to visit on a regular basis for inspiration.
Or 4/ methods inside a typeclass I.e., behaviour safely compartmentalised away into orthogonal Scala types that know about the data type, but the data types don't (directly) know about them.
I saw this recently but have no experience with it: https://github.com/vurtun/nuklear
I actually like one of the commenter's recommendation of having an implicit class inside the companion object. I wouldn't have thought about that. What are your guys' thoughts on that method? 
Scala is overrated trash. Overly complicated for the sake of it. All functional languages are unituitive and non-user friendly. 
Even if it used a different backend, it would be nice to have something that is compatible with the current coding methods. Of course it might not have complete feature parity but it would be nice to be able to have at least a basic subset of features that work "cross platform".
I'm in the US (Bay Area), not the EU, but I've interviewed and hired people who don't know any Scala and who have never written a line of it. I don't care about what programming languages you know, just your ability to program. Having Scala experience is a plus, don't get me wrong, but not having it doesn't count against candidates in the slightest. Outside of certain niches, it's unreasonable to expect to only hire people who know Scala already, as there simply aren't enough of them (and I'd consider it a poor heuristic if your actual goal is to hire good Scala programmers anyway).
There is no built-in GUI framework in Scala Native and we don't plan to support any of the GUI frameworks that are currently available in JDK. Your current options are: 1. Bridge to existing GUI framework that exposes C interface (e.g. GTK.) This should be as easy as providing signatures for the C functions in Scala Native terms (via extern objects.) 1. Bridge to existing C++ framework like Qt. This is a very non-trivial undertaking that's still possible technically. Expect at least a few man years of effort to complete the bindings. You'll probably need to develop some kind of binding generator for C++ to make this doable (that would bridge C++ apis to C calls, and use those for interop.) Projects like PyQt demonstrate that with enough effort invested one can use Qt from languages other than C++. 1. Write your own framework using lower-level APIs like OpenGL/SDL2/etc. This might actually take less time than binding to Qt. You could also experiment with finding idiomatic Scala-style APIs that are nice to use given that you'll be able to control how things are done under the hood. Whichever path you pick, feel free to drop by to [scala-native's gitter channel](https://gitter.im/scala-native/scala-native) to discuss the details.
It's still possible to call C++ from Scala Native as long as you wrap all the things you want to call into `extern "C"` functions on C++ side and call those via extern methods on Scala Native side.
A binding for Scala native to something like libui https://github.com/andlabs/libui would be awesome. I believe the intention would be for Scalac to just bind to the native libraries, although doing this with Qt would be interesting
The macros in Play and ReactiveMongo **are** reflection based — there is nothing else to base macros on right now. scala.meta is meant to replace the experimental-yet-useful-and-used stuff we currently have.
An outsider with some interest in learning Scala. How is the game development scene in scala rigth now and there are any libraries made in scala for game development. I am not looking for performance, i am just looking for a way to learn about Scala and kind of have some fun with it. Any kind of game examples in the command line are also welcome (like rogue likes).
Artifactory can present virtual repos in maven or ivy formats and SBT can resolve from both so that should work, the trick will be getting Artifactory to accept the MITM cert.
There's not a lot of game development in Scala AFAIK. (Though I think I remember some part of minecraft starting to use Scala, which caused some fuss as the standard library is a big jar and it caused issues for maven central). I've heard good things about ScalaFX if you want to do graphics. Maybe someone with direct experience will show up and give a better answer. 
No idea, but you can use libGDX in Scala - https://github.com/libgdx/libgdx/wiki/Using-libgdx-with-Scala
scala.js could also be a good option for making a game as you could build on a wealth of JS libraries in that space.
I downloaded a small sbt project and opened it in intelliJ. It consists of several coding challenges (in "src/main/scala") and each one has a file associated with (scala)tests (in "src/test/scala") that check your solutions. I want the test and code files to be in the same directory, or better yet, in the same file, since they seem associated and (in my opinion, in this case) belong together. I tried to modify the build.sbt such that the compile sources and test sources are in the same directory, but it didn't recognize the tests anymore. In IntelliJ's own build system the option I found to mark a folder as "scala test" or "scala" were mutually exclusive, so no luck there as well so far. Has anyone tried to do this before?
That's not really something you would ever do. Tests are always in a different directory that the source. This is to keep the separation of concerns and make sure test code doesn't end up in a jar.
Thanks for the answer. So, judging from the existence of this sentiment (which I was not aware of before :) ), it's understandable why this is not easily possible or even intentionally made difficult. It is a bit surprising though since the idea originates from the "Programming in Scala" book, [in the chapter on Packages](http://www.artima.com/pins1ed/packages-and-imports.html) they write "Among other things, this syntax lets you put different parts of a file into different packages. For example, you might include a class's tests in the same file as the original code, but put the tests in a different package," so I figured, why not do this? In this case, I don't need anyone to build a jar out of the project (or even build the whole project at once), it is just a collection of stand-alone programming exercises. But to be able to keep the nice syntax of scalatest instead of writing my own testSolution() method into the class file, I will probably stick to the solution of using parallel directories; the user will then have to jump to the test file to test his solution to a problem.
Could sbt-extras help? You can specify the repo URL for downloading the sbt jar with `-sbt-launch-repo`. https://github.com/paulp/sbt-extras/blob/master/sbt#L328 We use it in CI so that we only download artifacts from our own Nexus repository. 
Well, that's going to be confusing given that even the "opaque" implementation is transparent. (In the sense of https://www.cs.cmu.edu/~rwh/introsml/modules/sigstruct.htm). *All* of the example modules expose their implementation details, making them non-interchangeable. Ie, transparent.
It’s not clear how simple or complex your use case is. In the simplest case, you manually configure one server as the master and the others as clients. In the general case, you use ‘leader election’, where a master must be chosen by a consensus algorithm among homogenous distributed peers. 
&gt; The interpreter is the über pattern of functional programming. &gt; Most large programs written in a functional style can be viewed &gt; as using this pattern. Are there any large open source codebases in Haskell or Scala which use this style?
Really surprising benchmark results for Twitter's Algebird. Never heard of JMH (only tried Scalameter).
If you're using kafka you're probably using zookeeper already? It's designed for this sort of thing.
Ayayay, the String hack is nasty. Did you hear that in Java 9, the inner char[] will be replaced by a byte[]?
[removed]
The Muse just did a profile on Curalate: https://www.themuse.com/companies/curalate Here's the actual listing on the SE position: https://www.themuse.com/jobs/curalate/software-engineer-d0d442
Kinda jealous of OP now if they get hired and get to work with Marconi!
I hope when Java 9 comes out we have a mass boycott to bring sun.misc.Unsafe back. When performance matters, manual memory allocation and layout and unsafe memory access are hard to beat.
I think you need the @varargs annotation http://alvinalexander.com/scala/how-to-annotate-varargs-methods-in-scala-annotation
I think you want args: Array[String] or args: List[String] then you can access it via `args(0)` to get the first index.
Not really. The user looking at your project would run sbt from the toplevel project directory and either run all tests, or a single test class, either way with a single command. But, yes, they would have to jump between files to actually look at the source code versus the test code.
 Or /u/lihaoyi's [scalatags](https://github.com/lihaoyi/scalatags). [Scalate is really slow](https://gitlab.com/stevendobay/scala-template-engines-benchmark) but scalatags [will perform better in the future](https://github.com/lihaoyi/scalatags/pull/126).
&gt; #5102 adds partial unification of type constructors (behind -Ypartial-unification) and fixes SI-2712. Thanks @milessabin! That's what I'm talking about.
I see, thank you! But how about when you start a long running computation and you get back a resource / url you can poll for process...? Such as Richardson Maturity Model level3 stuff...
&gt; EU, but I've interviewed and hired people who don't know any Scala and who have never written a line of it. I don't care about what programming languages you know, just your ability to program. Having Scala experience is a plus, don't get me wrong But Scala takes FOREVER to learn for people with no functional programming background. I mean let's just say that you need to hire and form a set of 5 scrum teams to ground up build a web app entirely in Scala. You could hire Java programmers and teach them everything they need to know to write good Scala code. It could take several months to get them up to stuff. Or you could hire people who can pass a test and show that they know stuff. &gt; Outside of certain niches, it's unreasonable to expect to only hire people who know Scala already WRONG. There are people like me who know Scala better than anything else who are students looking for Scala jobs. Seriously. You don't need to ask for any industry experience what so ever. Zero. Just put together a test with questions like "What is the difference between an implicit parameter and an implicit conversion?" and "when would you use an immutable data structure over a mutable one?" and "write a monad (any monad) that has the ability to be used in a for-yield construct". If they can pass the test, hire them. The biggest mistake I see companies making is they hire senior Java developers with zero functional programming experience what so ever (no F#, Erlang, Haskell, etc) and tell them to program in Scala. There are Scala developers out there. You just have to know how to find them. And even if you're having a hard time finding them, companies that I have interviewed with have told me that it is much, much easier transitioning from F# to Scala (backend) than say Java to Scala not only because of the similarities of the languages but because of the developers already knowing functional programming concepts.
US beginner here In general jobs are better in big cities than in rural areas. Look in places like Paris, Berlin, etc. In addition, keep in mind that a lot of companies don't know how to hire Scala developers. They list insane amounts of Java experience as a prerequisite thinking that a Scala developers is a glorified Java developer when in fact it's different. That being said, the good ones will actually care if you know functional programming. Start with the basics - core language https://github.com/aashack/programming_in_scala_2nd/blob/master/Programming%20in%20Scala%2C%202nd%20edition%20(Artima%2C%202011%2C%200981531644)(1).pdf Functional Programming https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1467346928&amp;sr=1-1&amp;keywords=functional+programming+in+Scala From then people tend to go either toward big data (Spark, Scala wrapper for Hadoop) or toward web dev (Akka, Play). Big data pays more, lol. If you're getting a masters degree, the big data tests don't actually need you to know much Scala - just be able to solve simple programming puzzles. If you need to do ground up backend infrastructure with a team (like with actual design patterns and scalaz and what not), actually getting a job is hell because all the senior devs take up the jobs and if you want them you have them you impress the hell out of your interviewer (at least I had to) and also your interviewer has to be open minded (if they say something like "5+ years Java for Scala position", you're screwed). Java jobs are easier to find, and you don't have to travel to big cities, but they pay less.
Hey guys, I need a little career advice :) I am a ruby developer with a few years of experience. And I am planning to make a move into the scala/spark world. So I have just started learning. My question is how do you think I can increase my chance to get hired in a Scala role in a year or so? Thanks :)