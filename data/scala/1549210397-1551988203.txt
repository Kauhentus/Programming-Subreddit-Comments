It is all quite interesting, but Scalaz 8 runs on what ? Does it run on Scala2.13 ? Or on dotty ? Is there a way to install it on those mainstream Scala versions ? If not, then I am loosing interest. When the hell is it coming out. I see a lot of interesting conferences, but how can I use it without loosing my current environment ?
Some of my favourite bits. [Extension methods](https://github.com/lampepfl/dotty/blob/5f3775511d264e863df704f4c7784424078b652e/docs/docs/reference/contextual/extension-methods.md#translation-of-extension-methods) - automatic support of dot infix and "function" call syntax: def circumference(c: Circle): Double = c.radius * math.Pi * 2 assert(circle.circumference == circumference(circle)) Now if that were only the case with _every_ method, not just extension methods; or we just have to use [more type classes](https://github.com/lampepfl/dotty/blob/5f3775511d264e863df704f4c7784424078b652e/docs/docs/reference/contextual/typeclasses.md#semigroups-and-monoids) :) [Implied instances](https://github.com/lampepfl/dotty/blob/5f3775511d264e863df704f4c7784424078b652e/docs/docs/reference/contextual/instance-defs.md#implied-alias-instances): implied for Position = // new implicit val x: Position = // old saved four characters ✓ [Inferable parameters](https://github.com/lampepfl/dotty/blob/5f3775511d264e863df704f4c7784424078b652e/docs/docs/reference/contextual/inferable-params.md): given (ord: Ord[T]) // new (implicit ord: Ord[T]) // old infer[Ord[List[Int]]] // new implicitly[Ord[List[Int]]] // old saved three and five characters respectively ✓ "Mixing Inferable And Normal Parameters" ; that's just great IMO. And you can have non-implicit parameters depend on prior implicit (implied) parameters.
Actually here the pull request that also links to those files: https://github.com/lampepfl/dotty/pull/5825
Definitely like "implied" and "infer" over variants of "implicit". 
The standard collections don't make copies of their data. In Scala (/JVM) most data is on the heap, and the collections only hold references to the objects. [Example](https://scastie.scala-lang.org/JKMl2D6NSPS9vvRIzn0fIg) ``` class CanBeCloned extends Cloneable { override def clone = super.clone() } val a = new CanBeCloned() val b = a.clone() // Is a cloned object equal to the original? false println(s"Is a cloned object equal to the original? ${a == b}") val immutable = scala.collection.immutable.Set(a, b) val immutableChanged = immutable - b // Equal? true println(s"Equal? ${a eq immutableChanged.head}") val mutable = scala.collection.mutable.Set(a, b) mutable -= b // Equal? true println(s"Equal? ${a eq mutable.head}") ``` Also first make your code work right, only then optimize the hotspots. [Haoyi has a good guide on optimizing scala guide](http://www.lihaoyi.com/post/MicrooptimizingyourScalacode.html), but optimized code will end up as C in Scala syntax.
I've only been distantly following this "alternatives to `implicit`" experiment, so apologies if I missed some existing discussion (I found [this](https://github.com/lampepfl/dotty/pull/5458#issuecomment-441723021) and [this](https://github.com/lampepfl/dotty/pull/5458#issuecomment-441454468), but not much else)... Given that most of the changes here are moving towards explicitness, uniformity, and learnability I was a little surprised to see that the context bounds sugar is still around as an alternative way to express type class constraints. Does anyone know the reasoning? Is it just to save typing out "`given`"? Now that implicit parameters can be anonymous that seems like less of a burden. I've always liked how the context bound syntax tells readers "this is just another type constraint" which is not as obvious to newcomers looking at `implicit`/`given` parameter lists, but it might not be worth the baggage of having multiple ways to do things. Rust has a similar setup with multiple ways to specify trait bounds on functions and I've seen it confuse people. Another "one way to do things" would be enhancing context bounds enough that `implicit`/`given` parameters are unnecessary, but I don't see that happening given the trajectory of things.
Keystrokes is one thing, but "given" "infers" makes so much more sense to me on a linguistic level :D
I also hate everybody, hiding behind aliases. Assume yourself and use your names.
Thank you, will check it out!
I like the change to extension methods, but I can't say I'm crazy about the new syntax for implicits. The old way seems very transparent in that it simply marks certain values. The new way looks more *magical* IMHO
I'm sorry that I hadn't read it completely before I replied you \~\~
I agree -- I lean toward thinking that context bounds should go. While the new syntax would be less concise, it would be more self-documenting and unload the type parameter list, which can sometimes get pretty hard to mentally parse.
I'm so happy about this, and I hope it makes it through SIP. Implicits are one of the most interesting features of Scala, yet also one of the most cryptic. I think this proposal shows much clearer ways to express the most common use cases of the implicit mechanism. There's a counter argument that \`implicit\` is good enough and that change will be really costly. I get that, but I also think that it's time to iron out the warts of the language, and the ergonomics and comprehensibility of \`implicit\` is one of the bigger hurdles to Scala fluency, in my observation. You largely can't write real programs without \`implicit\`. Either you're doing Lightbend-style programming and you need \`ActorSystem\` and \`ExecutionContext\` everywhere, or you're doing FP and you need typeclasses. I hope we can one day do even better when it comes to propagating contextual information through a program in a locally controllable way. I'd love to even see "context inference". But that's probably not in the cards. Maybe ever, for Scala.
Tell me about it. Spent half a day updating all the dependencies, converting WS code, fixing implicits. Have around half the services working but still have to fix the other half and that's while running in global application mode.
With any sort of microservice development prepare, that 8GB of RAM might be not enough. (And I don't limit this to JVM-based microservices). For me 16GB is a reasonable minimum. It is slightly better if your company has a setup using docker-machine where you only run locally services, that you need to change ATM, and the rest runs in cloud. But such setup might be expensive, so most companies will just suggest you use docker-compose to start all that crap locally.
Well, there is no any silver bullet yet to solve this with JVM. JVM easily eats up to 400-512Mb for simple (micro) services (-Xmx limit + some overhead for GC). So, you really need high-memory machines. Maybe in future projects like Scala-Native or Graal solve this issue, but this is the reality today. Also, you could look at "serverless" approach (for some cases) or other languages like Haskell if it is the main issue for you.
Kubernetes!
I don't think I'm of a fan of a lot of this. I like the fact that once I understand the \`implicit\` keyword I know how implicit definitions and implicit parameters work. I also know how extension methods work although \`implicit class\` is a bit of an abuse of notations. &amp;#x200B; Now I have many more keywords and ways of defining methods to memorise. I understand the new ways make it more like talking but that just seems at odds with the rest of my code which is calling traverses, binds and folds which make little sense as talking either. &amp;#x200B; I'd prefer the existing syntax just had its warts and inconsistencies removed. 
Now you have two problems. 
Newer versions of the JVM supports [class data sharing](https://docs.oracle.com/en/java/javase/11/vm/class-data-sharing.html#GUID-7EAA3411-8CF0-4D19-BD05-DF5E1780AA91) for reducing the amount memory consumed by running [multiple processes for the same application](https://openjdk.java.net/jeps/310). Once enabled, after paying that initial 300-400MB penalty in RAM, you shouldn't need much more than the heap allocated for each process.
Ultimately when it comes to performance Scala absolute rocks - but that's not because it's straight execution speed is best in class. It's because you can "horizontally scale" by adding more and more servers into the mix. It all depends on the size, scale and scope of your applications but microservices are good for big problems - not necessarily small ones. If your whole solution is going to run on a single box, maybe microservices arne't the way forward.
graal vm native compilation if you can get it working for your app
You don’t have to run a separate JVM per service, I think of micro services as being microdomains rather than an entire implementation stack per service
&gt;splitting it into just 3-4 separate processes the memory overheads are 1GB+. TBH I'm not seeing the problem here. 1GB isn't much given cloud infrastructure is dirt cheap. We have Akka HTTP microservices with a footprint of around 500MB without a massive problem. If that's not the case for you, unfortunately there's no silver bullet at the moment. There's a pretty high amount JVM overhead even for a simple microservice. You may want to reconsider your choice of technology if this is really a dealbreaker for you.
First ask yourself do you really need that? It’s make sense only if you need to scale horizontally, otherwise it will create an overhead from a developer to add new features and support existing code. If before you have to call a single method, for example, in micro service world you need to do a remote call to another jvm. Also you have to add a retry logic, distributed translations, think about api versions. In my experience having separate build modules and dedicated teams assigned to them make more sense. Don’t do premature optimizations.
It's just one way to do orchestration. Without Kubernetes you have to get that functionality some other way.
With a monolithic application we could comfortably run the application on a 1GB VM, the microservices version requires 4GB. Multiply that by whatever amount of instances are required at a given time and that's not an insignificant cost difference. We're a university and our resources are quite limited.
The jdk is modularizing itself with project jigsaw. Hopefully soon the Scala ask will be modularized
It's not about premature optimizations, there are basically two reasons I am considering this: - CPU/IO scheduling: there is a service that does occasional CPU/network heavy processing. To make sure that it doesn't bog the whole server down while it's doing its thing, our current solution in the monolithic application is to manually throttle it (e.g. using Thread.sleep or Akka scheduler). By moving it to a separate process I can then simply rely on the OS features (cgroups etc.) to handle the scheduling and drop the iffy throttling code. - Iteration times: there are two other services that require up to 20 minutes of pre-processing to come online whenever the application is restarted. There is absolutely no reason for them to be restarted when iterating on other parts of the application and it's quite annoying if you need to talk to them. One solution would be to preserve the data across restarts in a cache of some sort but that has its own implications. Simply moving them to separate processes solves the problem. When it comes to IPC, a simple message broker works just fine and also gives other benefits (load balancing, crash tolerance etc.).
If resources are limited, microservices probably aren't an appropriate choice. There is nothing wrong with a well-designed monolith with cleanly defined and isolated subdomains. 
Do you have a lot of long-lived threads that do IO of some sort? (Including network?) Could this be your issue? http://www.evanjones.ca/java-bytebuffer-leak.html What version of the JVM are you using? As others have pointed out and as my limited experience confirms, microservices are not the way to go if you're tight on resources (human or computational). (Which is of course not to say that you should needlessly use 10x more than you need.) 
Try switching to 32-bit JVM, it will greatly help. Also see: https://blog.buoyant.io/2016/06/17/small-memory-jvm-techniques-for-microservice-sidecars/
 * I don't think 300-400mb is normal? I've run Scala servers comfortably on 192mb (admittedly a year or two ago) * A lot of the value of microservices is in the kind of isolation that you don't generally need separate processes for in Scala. Consider using separate modules (at the build system level) instead. * Java application servers were the original container runtime. You can run a static Tomcat (or similar) server and deploy service instances into it, as `war`s or the like.
Collection name(original: _*) should work Deep copy is much harder.
If you need minimal RAM usage, JVM is probably not what you want. That said, microservices are not really about minimal resource usage (CPU, RAM). They're about decoupling things from each other so that multiple teams can test and deploy their parts of the app at their own pace. If you don't have multiple teams that need to do that, you don't need microservices.
Hm that's not right. I work for Clever Cloud; our smaller instance has 256MB of RAM and will happily run small Scala applications. That's probably just a case of bad memory parameters (which maybe just default values based on available memory).
Buoyant eventually migrated their sidecar from Scala/JVM to Rust. &amp;#x200B; [https://blog.linkerd.io/2018/09/18/announcing-linkerd-2-0/](https://blog.linkerd.io/2018/09/18/announcing-linkerd-2-0/)
If you care about that amount of ram usage, JVM/Scala is not the right tool for you. Rust, Go or C++ might be a better fit. OTOH, an 2 vcpu 1GB VPS is $7.50/month on AWS (t3.micro). That's more than enough to run a Scala micro-service. If you can run 10+ micro-services for a month for 1 hour billed time of a developer, is it really something you should be optimizing for?
Good answer, thanks! I still haven't gotten a chance to look at laminar but since you're answering questions, might as well ask a quick yes/no on whether laminar can do the following: I have a scala map: Map( "USA" -&gt; 300e6, "China" -&gt; 1.4e6, "India" -&gt; 1.3e6 ) or realtime population that maps to HTML nodes: &lt;div&gt; &lt;b&gt; _.0 &lt;/b&gt; : _.1 &lt;/div&gt;. If I delete india from this, would the corresponding node be autodeleted? That is to ask- are all operations on scala collections supported?
in Scala 2.13, the standard library only uses the compact1 profile: https://github.com/scala/scala/pull/6164
You're right that it's not exactly a legitimate financial concern but more of a bad feeling about wasting resources so flagrantly. If 90% of the application's memory usage is overheads I can't help but think that we could do much better.
&gt; It's because you can "horizontally scale" by adding more and more servers into the mix Isn't this also true for *any* ridiculously slow technology, like Ruby on Rails for instance? 
I'm excited to listen.
Your OP was complaining about memory usage, not speed. &amp;#x200B; And scala is not "ridiculously slow", it's still at the top of the heap for network applications: [https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=json](https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=json)
Sure, AMA. Short answer – yes, as long as * you can convert your map into an immutable.Seq[A], * you can convert A into a Laminar element, and * ever A has a unique identifier (such as country code in your case) Then you use this little helper class (see full implementation [here](https://github.com/raquo/Laminar/issues/38): class Collection[Model, El, Key]( models: Signal[immutable.Seq[Model]], renderModel: Model =&gt; El, memoizeKey: Model =&gt; Key ) { val children: Signal[immutable.Seq[El]] = ... } // usage: div("Countries:", children &lt;-- collection.children) And bam, idiomatic and efficient children updates. This helper class is a small wrapper on Laminar's built-in `children` feature which is more flexible but less efficient on its own. I will make this helper class part of Laminar proper soon, but it's just a few lines of code, no big deal to just copy it into your project. Alternatively, there's another API that takes change commands as input: [`children.command`](https://github.com/raquo/Laminar/blob/v0.6/docs/Documentation.md#children-commands). It makes sense when your code operates on change events without explicitly keeping track of accumulated state.
What's the current state of ScalaJS 1.0? I need some JS in a *personal* project i'm starting, and i'm notrn between 0.6.x and 1.0.x pre-release. On one hand, even finding mentions of 1.0 is hard on the official site, on the other hand, it seem to pop up a lot in instructions of other libs that are related to sjs. Main concern is - are there any breaking changes that would require significant rewrites to switch from 0.6 to 1.0 when it comes up?
very cool- i'm definitely going to try this out. 
But that doesn't solve op's problem though.
Thanks for doing this! Looking forward to listening to it :D (I didn't know about scalalaz the Russian Scala podcast either!)
☺️ thank you, will do my best to keep it interesting!
I didn't say Scala was slow, I said that horizontal scaling will work for any technology no matter how slow (e.g. Ruby on Rails) provided your app so it scales horizontally
(I forgot to upvote haha) One more question since you're kindly offering an AMA- would you have any estimate on how the general framework performs? I would guess (blindly) that it's better than react and vue for general use cases? (Since no v-dom). Would you happen to have any kind of rough non-quantitative estimate of performance against other frameworks js/scala.js?
I am working on a problem where I need to call a number of services where the type of service, and the order in which they are called is pulled from a json metadata. I've been trying to create a type system where I can get the most help from the compiler but I have hit a wall and don't know how to proceed or what exactly to google! The problem is better explained with code... //Each service call has a unique request and response type **trait** Request **case class** ServiceARequest(name: String) **extends** Request **case class** ServiceBRequest(name: String) **extends** Request &amp;#x200B; **trait** Response **case class** ServiceAResponse(name: String) **extends** Response **case class** ServiceBResponse(name: String) **extends** Response //Some services can be run in parallel **trait** Service\[I &lt;: Request, O &lt;: Response\]{ **def** request: I **def** response: O **def** runInParallel: Boolean } &amp;#x200B; **case class** ServiceA(request: ServiceARequest, response: ServiceAResponse, runInParallel: Boolean = **true**) **case class** ServiceAB(request: ServiceBRequest, response: ServiceBResponse, runInParallel: Boolean = **false**) &amp;#x200B; &amp;#x200B; &amp;#x200B; //Describes my service calls, should probably be a map **type** Steps = List\[Step\[Service\[\_,\_\]\]\] &amp;#x200B; //A service + function returning the next step **sealed trait** Step\[Service\[I,O\]\] //The next service to be called can only be known after invoking this service **case class** Single\[I,O\](service: Service\[I,O\], nextStep: (O, Steps) =&gt; Step\[Service\[X,Y\]\]) **extends** Step\[Service\[\_,\_\]\] **case class** Multi\[I,O,X,Y\](steps: List\[Single\[I,O,X,Y\]\]) **extends** Step\[Service\[I,O\]\] &amp;#x200B; &amp;#x200B; **def** combine(first: Steps, second: Steps): Steps = first.foldRight(second)((step, acc) =&gt;{ step **match** { **case** *Single*(service,f) =&gt; f(service.response, first)::acc //Combine steps of the same type into a multi **case** *Multi*(steps) =&gt; } } ) &amp;#x200B; problems: [service.response](//service.response) is of type Any when I expected it to be a subtype of Response Is there a way to store Steps in a map and pull the right Step based on it's type, kind of like pattern match the map? &amp;#x200B; Thanks! &amp;#x200B;
Congrats on the release! I'm skeptical of the benchmark results, though. They seem to use the custom benchmark runner, which might have a number of issues. For instance, this method [https://github.com/scalqa/stream/blob/master/src/main/scala/scalqa/Util/Benchmark.scala#L7](https://github.com/scalqa/stream/blob/master/src/main/scala/scalqa/Util/Benchmark.scala#L7) can be optimized by the JIT as a no-op if it can determine that the method doesn't produce side effects. It'd be nice if you could convert all benchmarks to jmh and publish the results again.
I didn't benchmark Laminar's performance, but it should perform better than virtual dom solutions like Scalajs-React or outwatch, and there are a few reasons for this: * Laminar simply needs to do less work than virtual DOM solutions. There is virtual DOM tree diffing / reconciliation. * Laminar uses persistent (and mutable) data structures for representing the DOM tree, it does not re-create immutable DOM subtrees on every update like virtual DOM libraries. * See conceptual comparison with virtual DOM specifically [here](https://github.com/raquo/Laminar/blob/master/docs/Virtual-DOM.md) Like with any library, you need to be aware of certain performance edge cases. When you understand Laminar's API these should be very apparent because the API has no magic to it whatsoever. All the magic is in the observables, and that's not much.
Scala.js 1.x is as stable as 0.6.x. You can definitely use it for personal projects. The main difficulty is the availability of libraries, which for the most part do not publish artifacts for milestones of Scala.js 1.x. You should evaluate which libraries you are likely to need, and if they are not available for 1.x, you should stick to Scala.js 0.6.x. You can also try and lobby/help to get them to cross-compile. The official site has few mentions 1.x because virtually all the documentation applies equally to 0.6.x and 1.x. Re breaking changes, they are all listed in the release announcements of 1.x milestones. The latest milestone as of this writing is Scala.js 1.0.0-M6: https://www.scala-js.org/news/2018/10/24/announcing-scalajs-1.0.0-M6/
Lucid Software | Software Engineer in Application, Dev Tools, or Site Reliability Engineering | Salt Lake City, UT, USA | Onsite | Full Time Lucid Software is the creators of Lucidchart and Lucidpress - world-class web applications that push the boundaries of what is possible in your browser. The entire application back end is written in Scala. The frontend stack uses TypeScript, Angular, WebGL, and the Google Closure Compiler. The build system is Bazel. We are hosted in AWS. We are a ~400 person company outside of Salt Lake City Utah, that is growing like mad. We are looking for - Application engineers to help build applications that users love. - Development tools engineers to help build a development experience that engineers love. - Site reliability engineers to help build a scalable, reliable, and performance production system. More information and the application can be found here - [Application Engineer](https://www.golucid.co/careers/f9cb83be-e016-4be5-91dc-598e17e8d04c?team=Engineering) - [Development Tools Engineer](https://www.golucid.co/careers/c8de139c-6da7-44dc-a4af-95052aa2a086?team=Engineering) - [Site Reliability Engineer](https://www.golucid.co/careers/be16cacc-4cd3-4be0-9ec0-070646f9ec69?team=Engineering) Please DM me if you would like to chat about any of the positions. Please mention this Reddit thread if you apply :D
I am again working on improving performance of HashMap/HashSet concatenation by using mutation when possible [here](https://github.com/scala/scala/pull/7718). I am also working the mutable Map and Set `filterInPlace` implementations in these PR's: * [1](https://github.com/scala/scala/pull/7711) [2](https://github.com/scala/scala/pull/7709) [3](https://github.com/scala/scala/pull/7708) [4](https://github.com/scala/scala/pull/7705) and porting the structural sharing of HashMap#concat to HashSet#concat [here](https://github.com/scala/scala/pull/7657) A few other collections things as well, it's getting really close to code freeze for 2.13.0 though so it's a bit of a rush to get some things in. 
sucks that moderators for scala are like that
Cool, amigos
JMH results are [available](http://scalqa.org/doc-stream/Performance/JMH_Benchmark.html) too, and they prove the idea. But, which benchmark is closer to real life is still under question.
Why not use JMH for all benchmarks? Is it because of some limitation?
Hard to use. Everyone can cut/paste my benchmark code and run it instantly from anywhere. With JMH it is too much hassle, when one needs a quick answer: which one is faster.
A benchmark harder to execute is much better than a potentially wrong benchmark
Hard to argue. All the tests are duplicated in JMH. You might be right, and I should re-evaluate primary presentation, thanks
AWS ECS Fargate
Awesome! Can't wait to add Scala Love to my podcast rotation
It's not a waste if it works. You're paying to not have to think about it.
That only works on the same machine.
Thank you again for the answer. I am going to check out airstream carefully. 
Vscode + Dotty just works. Feels good. As dumb as it may seem, I think lack of seamless ide experience hurt Scala. I'm ready to dump intellij
To minimize startup time and memory usage you can use GraalVM EE and compile Scala/Java services to binaries: https://www.codacy.com/blog/scala-faster-and-slimmer-with-graalvm/
If you've got a classic db and Ruby web app you can throw up more stateless web app instances to horizontally scale. However there aren't many frameworks like Akka or Spark with first class support for Ruby or Perl afaik. Python has Twisted and Perl has AnyEvent::Loop but I'm not sure they are packed with the high support that Scala has for concurrent programming. Perl has poor threading support and Python had a global lock in its early implementations - so they couldn't really scale out with concurrent programming techniques to take full support of available multicore processors on a single machine. Scala's Futures and Execution Contexts patterns do allow for this and then further horizontal scaling across machines is easier to program, I think. With Ruby and Perl you'd more likely be writing full microservices to distribute load. I'm not knocking those languages. They're easier than Scala.
He thinks that Erlang copy - Akka - is something new and amazing and a silver bullet.
The fact you put quotation marks around micro implies to me you think each service should have a small memory footprint. I think this is a misunderstanding. They are in terms of discrete business logic but there's never been any denial that microservice patterns means more code, more overhead and work and obviously more RAM. https://blog.philliptaylor.net/microservices-are-the-right-solution-for-true-monoliths/
Linkerd 2.0 is an entirely new product, with different features and even somewhat different motivation behind it. Not sure that migration is the proper word here.
I would echo this. In 2009, I was running a web service written in Scala on a 64MB RAM VPS.
Isn’t the Scala plugin for IDEA full featured enough? What features is it missing?
I recommend [underscore.io](https://underscore.io) books, they are free and are well written. &amp;#x200B; *Essential scala* ([https://underscore.io/books/essential-scala/](https://underscore.io/books/essential-scala/)) is a good book for beginners. &amp;#x200B; For more experienced developers, *Scala with cats* and *The Type Astronaut’s Guide to Shapeless* are great to learn Cats and Shapeless: [https://underscore.io/books/](https://underscore.io/books/) &amp;#x200B; Guidelines and best practices: [http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html) [https://nrinaudo.github.io/talk-scala-best-practices/#1](https://nrinaudo.github.io/talk-scala-best-practices/#1)
how about instead of ``` class MyClass[T: MyTypeClass] ``` we do ``` class MyClass[MyTypeClass[T]] ``` ? it would also work well with multi-parameter type classes 
IMO the whole point of implicits is to make it practical to keep track of things that would be too cumbersome to track explicitly, and concision is vital for that. A context bound (usually) represents a fact about the type parameter rather than a value that we're interested in as a value, so having it in the type parameter makes sense.
That syntax is already used for higher-kinded type parameters e.g. `class MyClass[CC[X]]`
FYI you can throttle without blocking a thread (like \`Thread.sleep\`) by instead using \`IO.sleep\` from cats-effect. Also, what if you just run two instances of your application (one to do the background job, and another to do everything else)?
&gt; Also, what if you just run two instances of your application Isn't this the whole point of microservices? :)
Not really. Microservices are meant to separate concerns, not necessarily duplicate code. It sounds like it would be difficult to split your background job from the main application (and turn it into a service), but maybe I misunderstood. &amp;#x200B; It seems like there's only one task (the occasional CPU/network heavy processing) that's bogging down your application, and adopting a microservice architecture to solve that one problem is like using a hammer to twist a screw.
I mean if you naturally arrive at a solution where splitting your application in two sounds like a good idea, you have basically invented microservices.
Awesome!!!
Agreed, I don't see how this is solving any real problems. Also we are losing important contextual information (\`val\` vs \`def\` vs \`lazy val\` which is important in Scala considering the rest of the language works with this)
hmm, really? I feel like I've never seen anything like this... could you please give me some bigger example with this? thanks! :) 
I've encountered an error and solved it in a way that I didn't understand, and I'd like to learn why/how it happened. Here's the error: ``` Error injecting constructor, java.lang.AbstractMethodError: Method framework/PostgresProfile$MyAPI$.slick$basic$BasicProfile$API$_setter_$Database_$eq(Ljava/lang/Object;)V is abstract ``` The below is the code that causes the error: ``` object PostgresProfile extends ExPostgresProfile { ... override val api = MyAPI object MyAPI extends API { ... } } ``` But when I change the code to: ``` trait PostgresProfile extends ExPostgresProfile { ... override val api = MyAPI object MyAPI extends API { ... } } object PostgresProfile extends PostgresProfile ``` It works. Why? Or specifically, why do I need to make a trait? 
You're aware of higher-kinded type parameters using `[_]`? You can also name the type-parameter-to-the-type-parameter, which you have to do if you want to write upper/lower bounds that use it. E.g. class TransformHolder[CC[X] &lt;: GenTraversableLike[X, CC[X]]] { def stringToInt(ss: CC[String])(implicit bf: CanBuildFrom[CC[String], Int, CC[Int]]): CC[Int] = ss.map[Int, CC[Int]](_.toInt) def intToString(is: CC[Int])(implicit bf: CanBuildFrom[CC[Int], String, CC[String]]): CC[String] = is.map[String, CC[String]](_.toString) } new TransformHolder[Seq].stringToInt(Seq("2", "5")) new TransformHolder[Set].intToString(Set(4, 8)) new TransformHolder[Vector].intToString(Vector(9))
Is it your full time job to astroturf for Lightbend? You're moderator of this subreddit this is quite sad to see.
I'm actually not affiliated with Lightbend, I just enjoy Scala and post links here that I see on Twitter/etc if I think that a lot of Scala devs would be interested in them, sorry if you're tired of my posts :-)
Ah was wondering why you didn't seem to have any community new in your feed from typelevel or whatnot.
Might want to give your post formatting another try.
It's been a while since I've done a big Scala project, so it's tough for me to judge which would be better. I guess maybe it's not the worse thing to keep context bounds around. I personally would considering advocating for `given` syntax in team style, though. Unless I was on a team of advanced users, which has just never been the case in my Scala career so far.
not just for multiple teams working on the project but also for modularity and horizontal scalability. 
Got u, here is a very simple demo: https://github.com/rickynils/scalacheck/tree/master/examples/simple-sbt If you want to customize how your list can be randomly generated, put your custom `Arbitrary[Array[Int]]` in scope.
That \`Gen\` looks fine to me. What property are you trying to test?
I could maybe understand if it was a paid course. But why are you bitching about a free course which is very useful and made by qualified people?
NOT with Scala 2.x but with Scala 2.12, 2.13 and 2.14.
I can't read your code (three backticks only works on some parts of reddit, and the error goes off the right of your post) but `AbstractMethodError` happens when your runtime classpath doesn't match your compile-time classpath or the compile-time classpath of one of the libraries you were using, perhaps because you have different versions of a transitive dependency. Figure out which dependency contains the method that's giving the error and where the code that's calling that method is, and then look at your dependencies (in your build manager) and see which version's getting there at runtime and why.
Ditto, I've run Java and Scala apps on OpenJDK 10 with at little as 64mb ram (According to docker stats). I can get into the single digit mb usage when testing with Graal native-image, but I'm not sure I trust it in production yet. If you're using a big bloaty framework you'll need more, but build on something lightweight and you're fine. Vert.x is full featured, has Scala support out of the box, and is very lightweight. 
I'm trying to test equivalence partitions for the array: Array|Element :--|:-- Single value| In array Single value| Not in array More than 1 value| First element in array More than 1 value| Last element in array More than 1 value| Middle element in array More than 1 value| Not in array
I did all courses from lightbent and they all was great
It's probably best not to ask questions about this book here, the author of it has been banned from this subreddit and asking about it here will likely get you banned as well.
I appreciate the warning. I do. But that seems...insane, no? It is like two steps from book burning...
Just avoid any negative sentiments and you're good.
Seems like you are running into [https://github.com/tminglei/slick-pg/issues/367](https://github.com/tminglei/slick-pg/issues/367) which is caused by [https://github.com/scala/bug/issues/10477](https://github.com/scala/bug/issues/10477)
There’s a library called shapeless which at its heart translates specific types and classes you write into a more generic format called HLists. It can support trees and all sorts as well. Whilst it doesn’t answer your question, it might give you some ideas or food for thought.
So, what is `|@|` really pronouncex? This article claims scream, but cin(abun) and tie (fighter) are in my experience more common. Votes?
Thank you. That looks like the exact problem I have.
Ah, man, I gotta stop using this backticks on reddit. Anyway, it seems there's a bug with Slick and Scala. Please see /u/amund91 answer.
Talking about this book and/or asking questions will not get anybody banned.
interesting! thanks for showing me, I wasn't really aware of this
I'd be thinking about a pattern match with unapply for the various cases, q.v. [https://danielwestheide.com/blog/2012/11/28/the-neophytes-guide-to-scala-part-2-extracting-sequences.html](https://danielwestheide.com/blog/2012/11/28/the-neophytes-guide-to-scala-part-2-extracting-sequences.html) with the result being the appropriate property check(s), q.v. [https://github.com/rickynils/scalacheck/blob/master/doc/UserGuide.md#combining-properties](https://github.com/rickynils/scalacheck/blob/master/doc/UserGuide.md#combining-properties) &amp;#x200B;
Scala Syntax (official) 0.2.0 from scala-lang. You can also try Metals. it is experimental but allows you "go to definition" and gives you squiggles for compile time errors. 
Can you elaborate?
it's in the post's body :)
"😃".length
2 - exactly what you would expect! So what is the issue you are talking about? (I used Scastie repl - so no Scala 3, which doesn't even has preview versions, hasn't it?) 
If what you mean is that Scala strings are still Java strings, and therefore backed by arrays of Java `Char`s, then yes, this is not changed. Ultimately it's a problem with the Java specification and APIs which date back to the 1990s. It's the same issue in JavaScript with Scala.js. 
I think he means that a character in Scala does not correspond exactly to a Unicode codepoint. It's the same in plain Java and in JavaScript, and it is not exactly a good thing, but this is what 25 years of legacy get us.
OK. But that's nothing new to Scala 3! And string types that operate on code points doesn't provide so much advantages compared to code units. If you want to support good natural language support, you might better need grapheme clusters either way.
&gt; 2 &gt; exactly .. expect you are so corrupted by java that you probably see the big beautiful line of teeth in black too.
What would you have expected? And which language provides what you have expected? (Python comedy to my mind only) I would recommend the excellent http://utf8everywhere.org website to you. It explains in a good way, why code points aren't so benefitial you might think 😉
At the moment metals is your best option if you don t want to use IntelliJ. I have switched recently from metals to intelliJ still.
&gt; so no Scala 3, which doesn't even has preview versions, hasn't it ? It does, we release one every six weeks: http://dotty.epfl.ch/blog/, example project here: https://github.com/lampepfl/dotty-example-project (And no, we're not going to suddenly change the behavior of java.lang.String, but I'm sure you can find libraries to handle strings the way you want).
Is there a stylecheck tool or a compiler option to prevent the below mistake? object Test { object OneObject val A_LIST = List(Test.OneObject) } println(Test.A_LIST)
Java 9 introduced Compact Strings, which is backed by byte array.
I call it “ap”. That’s the name used in Haskell. Not sure if common. 
&gt; We'll get a stackoverflow error with the code above. I'm not sure what are you expecting to cause it... https://imgur.com/KQA0JfB As for NPE - the only thing i can think of is you not posting actual code but oversimplified version of it, and original code having a complex mix of traits where list and object are in separate traits and object ends up initialising after list
It's just an optimization which doesn't change anything to the semantics, so it's not really relevant to the initial point.
I don't want to be pedantic, but those are Dotty versions - some day it will be called Scala 3, but right now there is nothing with this name right?
Here: [https://scastie.scala-lang.org/tGPc55MfTyyUa1HbjiwsgA](https://scastie.scala-lang.org/tGPc55MfTyyUa1HbjiwsgA) Your class name seems to have the word "line3$read". Maybe it's different for a console.
Once again - no issues: https://imgur.com/Qfu7cqQ That seem to be problem with how Scastie interprets your code.
 Powerinbox | Sr. Scala Engineer | Anywhere in the US | Remote | Full Time **If you join us, what will you do?** Write, test, and maintain computer software that will double power**inbox**’s capacity to recommend ads each year for the next three years. Also to provide technical specifications and unit test cases for the software. *Specific Goals* * Scale our ad recommender platform to double its’ capacity. * Increase revenue per 1,000 items by $0.10 each quarter. * Have unit tests to cover all code paths for all code written. * Establish continuous integration by writing automatic deployment scripts. * Maintain 99% uptime of the software when deployed into production. * Write technical specifications for software being created. **In order to be great at your job...** *You Are* A faster learner; have great analytical skills; relentless and persistence in accomplishing goals; enthusiastic with an infectious personality. *You Work* Efficiently; with flexibility; proactively; with attention to detail; to high standards. *Together We* Emphasize honesty and integrity; require teamwork; have open communication; follow-through on commitment; stay calm under pressure. *You Have* Advanced Scala skills (at least 5 years of experience); computer science knowledge (education or actual work experience); Linux experience; relational database skills; no-SQL database experience; Hadoop skills; and experience with Kafka, Storm, or Kinesis. **This is extra, but if you have it, it will make us happy** * Experience in working remotely * Knowledge of/interest in the digital and AdTech landscape **About PowerInbox** *Who We Are* We are a digital monetization startup ecosystem that is always open to new talent *Why We Are* Personalization is key and we at PowerInbox believe that email is not meant to be stationary and static, but relevant and filled with dynamic content and advertisements. *What We Are* We at PowerInbox boost your revenue and brand engagement through real-time advertising, and native ad displays. If interested please send your resume to hr@powerinbox.com
Stop attacking people, and stop dancing around your point. Engage constructively or not at all.
OMG sorry I should have added a sarcasm in there. I was trying to make a terrible joke
The preview versions of Scala 3 are called Dotty.
Hi, I was in a similar position in my company (80% php and 20% scala) 2 years ago. This is my opinionated (of course!) advise based on what I've learned helping switching more than 10 devs from php to scala. Don't teach them FP yet. Show them the language: the maps, the flatmaps, the futures, option, etc... all the cool stuff that has scala. What i've seen is a lof of ppl don't like to feel like they are stupids and on first contact with FP, category theory... most of them will feel this way (I felt this too XD) and they will return to the comfortable php imperative world and refuse irrationally all that relates to scala. I think we should show ppl how to do things in their old way in Scala and then gradually switching them to FP. Showing FP by example like "look, u did this like this before, I will show you another way to do it" is more effective than talking about FP hardcore concepts. &amp;#x200B; Good luck! 
I'd focus on the benefits. Referential transparency and immutability should be front and center on day one. Drill into their head the idea of everything being a value. The second day should be about mapping, folding, flatmapping, etc. I would avoid talking about monoids, monads, applicative functors, etc. 2 days isn't long enough. Avoid all the lingo that will make your audience's eyes glaze over. Instead, show them direct benefits of FP by using the language. Focus on things like error control, program flow, composition, etc.
Thank you for your comments. Looking at what you said, I realize I might be too ambitious on the theory part of the course. I surely don't want my students to feel stupid or overwhelmed :) I think I will focus on the "everything is a value" part, first-class functions and immutable data structures, using practical examples of how they improve readibility and control flow :) I could always do a second, more advanced, training for those more interested in the theory :)
evilness of side effect
Did you folks take a look at enumeratum?
This could be useful - https://github.com/ashwinbhaskar/functional-way
To clarify, I don't claim that "«Scream»" is *the* authoritative alphabetic rendering :). It is, however, the one I _remember_ hearing the most. I've also heard the "TIE fighter" variant. I strongly suspect the preference is localized - e.g. there aren't that many Cinnabons outside the US, and "cinnamon roll" doesn't roll off the tongue quite well (I'll get my coat). In any case, purists would probably argue that the only *proper* names are ones derived from category theory (probably something like "applicative product"?) :).
thx for making this clear - I must have missed the transition from Dotty being *just* an experimental compiler platform to its clear focus on being the Scala 3 development alias :-)
Here's the announcement: https://scala-lang.org/blog/2018/04/19/scala-3.html
You should check this real world project on github &amp;#x200B; [https://github.com/gothinkster/scala-play-realworld-example-app](https://github.com/gothinkster/scala-play-realworld-example-app)
I created a 2- and 5-days Scala/FP workshop for developers in my company and open sourced the 5-days version. You can find it here: [https://github.com/scalasummerschool/lectures](https://github.com/scalasummerschool/lectures) &amp;#x200B; The 2-days version are basically the first 4 lectures and depending on the pace of the group you might be able to do a small project.
Hmm, I'm in the same boat as you, having to design something similar at work. I'd say come up with a realistic goal of what you'd like to accomplish in two days. There's a lot to cover with Scala. One of the problems with teaching Scala is that you're typically teaching two things at the same time: FP and Scala, and they are actuallly different things. Most tutorials don't do a good job of teasing them apart. The result, I think, is people just cargo culting Scala snippets without really understanding the principles behind them. I think getting people of nulls to Option is a good goal. It's so hard to say. There's so many juicy things to teach and not enough time. At my previous company we also had in-house training and the one thing people responded to the most (whether they used it or not) was homework and office hours.
I've found http://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html to be enormously helpful in pointing out the value of values and immutability.
The Java runtime has a proper way to handle UTF characters, but it's not obvious and not taught to beginners. The concept is called code points. Unfortunately Java calls UTF16 encoded code points characters. This is extremely confusing. To make this make sense, when you call \`length\`, remember you are counting the integers making UTF16 encoded characters, not the actual UTF characters, or code points. If you want to know about the UTF characters in your string, pay no attention to Java characters. The correct concept is called code points. scala&gt; "😃".codePoints.toArray res9: Array[Int] = Array(128515) scala&gt; new String("😃".codePoints.toArray.flatMap(Character.toChars(_))) res17: String = 😃
[cedarlakeventures.com](https://cedarlakeventures.com) | Full Stack Web Developer | Orono (Minneapolis area), MN, USA | Onsite | Full Time ## About Cedar Lake Ventures, Inc. is a software company based in Orono, Minnesota, that has been delivering innovative online and desktop software products for more than 10 years. Our primary focus is on advanced proprietary image processing algorithms delivered and sold as subscription-based web applications (SaaS). We are a small and nimble team that offers great potential for gaining broad experience and assuming greater responsibilities over time. We offer competitive benefits, paid time off, paid holidays, and a regular 40-hour work week. Employees are not expected (or allowed) to work from home. ## Description We are looking to a hire a Full Stack Web Developer to work out of our Orono office near Lake Minnetonka. Areas of responsibility would include: * Develop and improve existing rich-client web applications using Scala, Play Framework, HTML, Javascript, Typescript, Canvas, Web Sockets, and in-house technologies. Our existing customer-facing web applications are image processing tools, but future work may relate to other topics. Likewise for related APIs. * Develop internal and contractor-facing web applications and management systems to collect and annotate data to train machine learning systems. * Work with UI/UX Designer and Digital Marketing Specialist to publish lightly-interactive and static educational and promotional content. Work with Digital Marketing Specialist to configure Analytics to track marketing campaign efficacy. * Manage AWS resources and production operations: production cluster management, instance reservations, compilation servers, and our deployment system. * Monitor third-party OS &amp; framework dependencies and ensure timely updates. * Develop and improve in-house framework and back-end systems, improving functionality and speed of components while enforcing code and design consistency. Existing components include Scala-based HTML, CSS, and SQL DSLs; a federated user management system shared between our sites; credit card and PayPal-based common billing system with support for one-time and subscription-based payment modalities; and a system to manage seamless deployments of new code with zero downtime. * Manage bug reports and fix actual bugs on all sites. Interface with front-line customer support regarding bug reports and functionality inquiries. Employee will be furnished with an electric sit-stand desk and a high-performance workstation with the employee’s choice of operating system. ## Qualifications * Languages: Scala developer with 3+ years of professional experience. Experience with Java, HTML, Javascript/Typescript/ScalaJS, and SQL also very helpful. * Other technologies: Linux, AWS infrastructure, GIT, Play Framework. * Must be willing to learn new languages and frameworks. * Bachelor’s Degree or higher in relevant field. We are looking for a self-directed self starter who can juggle a number of potentially unrelated concurrent responsibilities, and who will actively seek to take on new tasks as time permits. This is not a good position for someone who requires frequent active management to stay productive. Applicants must be authorized to work in the United States. ## Disclaimer The details above are not set in stone and are not a promise as to the nature or scope of the job. They are intended to help you decide if this sounds like a good fit that warrants submitting your resume for consideration. Job responsibilities are at your manager’s discretion and will probably change over time. ## Contact Please email your resume to [jobs@cedarlakeventures.com](mailto:jobs@cedarlakeventures.com) &amp;#x200B; &amp;#x200B;
I am trying to register a component. I have to send two parameters in body of post request. I get error bad request or unsupported media type. This is Scala play. `def postInstance(compType: String, name: String): Action[AnyContent] = Action.async` `{` `request =&gt;` `ws.url(instanceRegistryUri + "/instances" + "/deploy")` `.withHttpHeaders(("Authorization",s"Bearer ${authheader}"))` `//.contentType("Accept" -&gt; "application/json")` `.post(Map("ComponentType" -&gt; compType, "InstanceName" -&gt; name))` `.map { response =&gt;` `response.status match {` `// scalastyle:off magic.number` `case 202 =&gt;` `// scalastyle:on magic.number` `Ok(response.body)` `case x: Any =&gt;` `new Status(x)` `}` `}(myExecutionContext)` `}`
&gt; "😃".length This is such a poor response to someone asking for an elaboration on a poorly written post to begin with.
[removed]
Heh? Thanks for providing a contradictory example. This means my first intuition is so far off. I can't really think of other root causes. Here is a simplified version of my code: object Currency { sealed abstract class Value object THB extends Value object USD extends Value object SGD extends Value // There are like 10 more currencies val SUPPORTED_CURRENCIES = Seq( Currency.THB, // a few more currencies Currency.SGD, // Currency.SGD is the only one that is null. The currencies before it aren't null // Then, a few more currencies ) } // In another file, I do: Currency.SUPPORTED_CURRENCIES.map(_.toString) // The NPE occurs here. We certainly use Currency.SGD in a lot of other places. I can't really tell why Currency.SGD is the only one that is null. We do call Currency.SGD directly in other parts of code because we are in Singapore, so SGD is the default for many things. We don't do this with other currencies. Could you tell me your best guesses if you have one?
Thank you for your reply! I appreciate your taking my comments in good faith. 
There are different ways you can set this up, but you can do something like: &amp;#x200B; `property("singleValueInArray") = forAll(arrayOfN(50), Gen.chooseNum(0, 50)) { (arr, chosenValueIdx) =&gt;` `val chosenValue = arr(chosenValueIdx)` `binarySearch(arr.sorted) == Some(chosenValue)` `}` &amp;#x200B; I'm assuming here that you're testing a binary search of a sorted list (is there any other use for binary search?). You sometimes have to be a bit clever how construct your scenarios, which is what I'm trying to demonstrate here. If you need a value that's in the array, well, pick a value from that array, but let Scalacheck decide which one it will be. &amp;#x200B; When using this technique, I often explicitly test boundary conditions, since you can't always guarantee they'll be excercised. I hope this helps get you started!
Nvm, I've figured it out. The issue is extremely tricky. Essentially, there's a code that eventually refers to `Currency.SGD` using Scala's reflection (using `reflect.runtime.currentMirror.staticModule(symbol.fullName)`) before `Currency` is initialized. That returns null and, then, Currency doesn't want to initialize `object SGD` anymore. I'm still figuring out the exact root cause. But, after switching the currencies around (e.g. changing names), this seems to be the root cause. Thanks again for providing a contradictory example.
*Oh my god, please embed the whole code block into "code" formatting, not individual lines.* For one, are you sure you intend to sent your post as form data? Because i'm 90% sure that just .post() wihtout providing the Content-Type header (not Accept one) will result in your map being sent as form data. If you want to send JSON: import play.api.libs.json._ val data = Json.obj( "key1" -&gt; "value1", "key2" -&gt; "value2" ) val futureResponse: Future[WSResponse] = ws.url(url).post(data)
So, the above sample works, so i assume NPE actually happens inside the currency. And Lazy only supports that because it would be deferred until after object is completely initialised. And seeing an abstract class... Try this article: https://docs.scala-lang.org/tutorials/FAQ/initialization-order.html Init order gets messy with inheritance - you might accidentally call something on base class that is supposed to be implemented by child, before child initialisation is done. Just a guess.
I have done some two-hour workshops at work, and polled the students to see what they were most interested in. Mostly keep in mind that the students aren't necessarily interested in the same topics as you, they will learn them more slowly than you likely think, and will require simpler examples than you likely think. I used a jupyter notebook to avoid setup issues. We did the following sessions: * Intro to the Scala standard library. This was doing map, flatMap, filter, and fold using a number of different types in the standard library. No recursion allowed. What I thought were super-simple examples like 'sum all the digits in the string "12345"' took quite a while. * Easier, cleaner error handling code. This was delving deeper into the error-handling parts of Futures, Options, Eithers, and Trys. * Converting mutable code to immutable. Fairly self-explanatory. * Gentle, practical introduction to functors, applicatives, and monads. Only after they had the above lessons, were people interested *at all* in category theory stuff.
your initial plan has all the most important aspects in it. ADTs and pattern matching might be a nice addition, and maybe introduce by-name parameters and lazyness, co-lists aka streams ('forward' recursive algos and data structures without stopping conditions). Property-based testing with ScalaCheck is a must if you ask me.
I've figured it out. Please see my comment here: https://www.reddit.com/r/scala/comments/akmvfg/got_a_quick_question_ask_here_january_28_2019/efytdrl/ TL;DR accessing the inner object through reflection wouldn't initialize the outer object correctly. The accessed inner object becomes null.
If that really is your audience, i.e. most people coming from PHP, I'd start simply with (static) types, and why they can provide improved guarantees of program correctness. The trade-off is you spend more time compiling up-front, but correspondingly less writing tests, to say nothing of production support. For example, in Scala you can safely and (reasonably) efficiently force distinctions between a customer first and last names, address line, telephone number, account number, email address, etc etc; in PHP these are all Strings, and guess what that means for program maintenance. Roll on opaque types, but you get the idea. Also, a key aspect of FP is about the re-use of proven algorithms, (patterns may be a more familiar term) and I don't see that highlighted.
Also check out https://github.com/lightbend/lightbend-emoji
Coming from C# I have learned Scala last summer and pattern matching was definitely one of the things that made me like this language instantly :) honestly I've been working for four months now in Scala and I've never heard any of my coworkers speak about monads... I think this is way too advanced for your two days class, unless all of your students are geniuses ! Anyway, good luck !! 
Yep, it's great. At least for me, since when starting using Scala i more or less ended up writing my own ghetto version of it.
IntelliJ
Thanks- I will see how it plays with CJK
dont waaste ur time with eclipse like I did
Intellij is by far the best I have used.
Intellij or vscode with metals if you for some reason don't want to use Intellij and want something lighter. 
IntelliJ is the only proper option in IDE.
Intellij is the de facto standard IDE for Scala developers. In fact even the guys who developed the „ScalaIDE“ used Intellij when i met them during a conference 😁
Thanks. Lol, I was about to do that, since I was using eclipse earlier
&gt; lighter VS Code in made in Electron... If you want "lighter" - it would be somthing like sublime or emacs
🙄, lightER, relative to Intellij. I also don't recommend sublime or Emacs because I don't believe Ensime has very active support anymore. OP is looking for an IDE. 
Vscode with metal gives you much, much less help than IntelliJ. It's fine if you're already used to vscode and want to stick with it, but you should try IntelliJ to see what you're losing.
Then checkout those two: - https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/ - http://utf8everywhere.org/
Emacs. If you know vim get spacemacs. It has everything you'd expect IDE to have and you can select vim controls in initial setup.
IntelliJ works well. However, it does choke on some of the functional libraries like cats (false highlighting). 
I know the Joel one. I will read the other one. I don't know Unicode code units mean. But I'm aware that emoji + color is two code points but "lenght" 1. Similarly for ligatures or dependant vowels in some eastern scripts. But my point is that for cases where its not ambiguous what the length is- I should get the right notion of length. And the Java way is 100% wrong. Where there is ambiguity- I should get one of the right values. 
&gt; But my point is that for cases where its not ambiguous what the length is- I should get the right notion of length. And the Java way is 100% wrong. Where there is ambiguity- I should get one of the right values. There is nothing like a *length* concept in Unicode! So don't confuse a string API with Unicode concepts. So there is nothing like a *right* value. In Java the String type is built upon *code units* of 16 bit. So a ``length()``-method will rely on those and give you the amount of *code units*. Other languages like Rust rely on UTF-8 for an *internal unicode aware* [string type](https://doc.rust-lang.org/stable/std/string/struct.String.html#utf-8). Besides other consequences - like no indexing - their [``len()`` function](https://doc.rust-lang.org/stable/std/string/struct.String.html#method.len) returns the number of bytes (which *is* the code unit for UTF-8!) The *desired* result of a *length* method depends always on the context! Do you want to know the length in term of *what a human thinks the length is* (tweets in twitter) or in a more *technical* context, for example, does the string fit into the capacity of my storage backend (max size of a column) or similar. As the latter conforms to more use cases and is possible to be implemented in O(1), most languages has chosen to support primarely the more technical related API. Besides that in the beginning of the 90ies UTF-16 was equal to Unicode in the way, that ``code units == code points``. Nowadays the Unicode space has drasticaly increased, so 2 bytes aren't sufficient anymore. You would need 4 bytes instead, which is a tremendous waste of memory for most strings within a program. Python uses a [*complicated* approach](https://www.python.org/dev/peps/pep-0393/) to handle strings - the *benefit* is that ``len()`` returns *code points*; but at the cost or *performance*: In [6]: "\u203d" Out[6]: '‽' In [7]: len("\u203d") Out[7]: 1 So it's a complicated topic and it's not enough to simply blame Scala on being crippled 😉
vim + ensime. Anything else will eat your RAM
can you give an example? I've been using cats and didn't encounter such issues (at least with partial unification turned on)
Electron + language server is not lighter than IntelliJ. &gt; OP is looking for an IDE. Which VS Code is not. It's a glorified notepad written on top of electron with LSP baked in instead of being external plugin as in case of Vim, Selenium or Emacs. The only actual IDEs are IntelliJ And the "ScalaIDE" (aka Eclipse)
Something I was doing yesterday: if you use a lower case type parameter, like you have to in a match expression, then intellij won't treat that as a proper type. Adapting from memory: sealed trait FunctionAndArgument case class FunctionAndArgumentImpl[A](function: A =&gt; String, argument: A) extends FunctionAndArgument def callFunction(faa: FunctionAndArgument) = faa match { case faai: FunctionAndArgumentImpl[a] =&gt; faai.function(faai.argument) // intellij gives an error highlight here } Workaround was to declare an upper-class type and match again: def callFunction(faa: FunctionAndArgument) = faa match { case faai: FunctionAndArgumentImpl[a] =&gt; type A = a val FunctionAndArgument(function, argument): FunctionAndArgument[A] = faai function(argument) }
And anything with macros. Shapeless, parboiled2, and others turn Intellij into a text editor. I’m hoping they interface with Dotty’s compiler service at some point and that fixed Intellij’s issues.
Most will say IntelliJ, but I'll put in my vote for Scala IDE. If you delegate building to SBT (in a separate terminal session) and use the IDE for the presentation compiler, then it works quite well -- few spurious errors arise (which I think is common in IntelliJ?), and when they do, you can easily bind \*restart presentation compiler\* to a hot key, like \*ctrl-backspace\*. Mixed Scala/Scala.js projects are fairly seamless, and \*sbt-eclipse\* will generate your .classpath/.project files for you. Despite Lightbend having abandoned the project there's a small group of us that keep it going. IOW, if you prefer Eclipse, it's an option. Scala 2.13 support is a WIP, but it will happen. Otherwise, VS Code + Dottyy/Scala 3 seems to be the future.
Nothing earth shattering, but just finished up a playoff seedings projection for a hockey scouting client. Pretty fun, taking existing game results to-date and projecting end of season results based on various Pairwise rankings (RPI, head-to-head results, etc.). Next step will be implementing a Monte Carlo simulation to predict the odds of teams getting into the playoffs. Have a rough draft in place (really just a spin on the above, but repeated 20K times), which is...not going to be possible in real-time like we can do with the playoff projection. Took a look at Scala Breeze, but general purpose Scala is not yet amenable to computation on the GPU. Maybe at some future time we'll be able to run the simulation in real-time; until then we'll probably just run it nightly and display the results from cache otherwise.
Yep. In my project ScalaIDE is able to infer types and propose completions for code that uses shapeless. All IntelliJ does is highlight it in red, even though it compiles and runs successfully.
[Twitter](https://careers.twitter.com/content/careers-twitter/en/jobs-search.html?q=scala&amp;location=careers-twitter%3Alocation%2Fsan-francisco-ca)
Whether the application is a microservice or monolith is irrelevant to whether Scala is a good choise for the application.
"why are you booing me? im right"
It seems to me your conflating making a sales pitch for functional programming with actually teaching it. I'd work on making sure those concerns are separated when it come to this course as 2 days isn't a whole lot of time.
[Atom](https://jobs.lever.co/atomtickets.com/d1e37316-1b1b-47b8-8b2f-d20ddf82b178) 
&gt; Whether the application is a micro-service or monolith is irrelevant to whether Scala is a good choice for the application. What i shoud take into account? 
IMHO Scala IS better suited for monolithic apps because memory footprint of each java process is more likely than not going to be large, and therefore the less services you have the better.
Report this in their YouTrack issue tracker.
Cant the official thrift compiler output Java?
Yes, it can output Java, but I was hoping for something that can plug in with SBT without requiring the Thrift executable. &amp;#x200B; The build pipeline is not easy to change for me, and to include an executable in build environment is going to be a hassle. However, if I could integrate it with SBT itself (like Scrooge does), it would be easier.
So I found this project: [https://github.com/intenthq/sbt-thrift-plugin](https://github.com/intenthq/sbt-thrift-plugin), which does solve the problem. However, it does not use the latest version of "libthrift." &amp;#x200B; If I get some free time, I might send a PR or fork it to support newer version of Thrift.
Ah, i see... But does scalac actually compile it? I recall having an issue with my extractor object being lowercase and `match` not having any of it, but i'm pretty sure that in tha case it didn't compile either
Microservice architectures don't always require JVMs for each service. An Akka cluster (when the actor system is properly designed) is a great example of this -- you can deploy all the actors in the cluster on a single JVM or you can scale it out with several VMs, the choice is yours.
Is there a way I can guarantee that a type parameter has certain methods? I've got a thing that takes a Double, and want to templatize so I can use things like breeze.math.Complex instead, but with the obvious ways such as def f[A](x: A) : A = { x+x } the compiler complains about "+" not existing for A, which is fair. I'm hoping there's something basic I'm missing, but I'm not seeing what just yet.
Isn't this an easily answered question?
&gt; But does scalac actually compile it? I recall having an issue with my extractor object being lowercase and match not having any of it, but i'm pretty sure that in tha case it didn't compile either Yes, it works in scalac, and in Eclipse/Scala-IDE. If something in a pattern starts with a capital letter it's a constant you're matching against, if it starts with a lower case level it's a value you're extracting. This is the case even for type parameters, which is the whole point of the code I wrote (the type `A` is "hidden" - if you have a `FunctionAndArgument` you know that the argument is the same type as the function takes, but you don't know what type that is - and then when we pattern match it as `a` we get access to that type).
How so? There aren't a lot of companies hiring Scala devs. Most jobs post just lists a bunch of languages including Scala, but aren't Scala shops. 
It is good language for everything, except for high frequency trading - it is almost impossible to eliminate object allocation. Application performance/memory usage mostly depends on used libraries and frameworks, not the language itself.
Typeclasses! trait Add[T] { def add(l: T, r: T): T } object Add { implicit val addDouble: Add[Double] = new Add[Double] { def add(l: Double, r: Double): Double = l + r } // same for breeze.math.Complex object Syntax { implicit class AddOps[T](l: T)(implicit A: Add[T]) { def +(r: T): T = A.add(l, r) } } } // means "double anything that is addable to itself" def doubleDirect[T](t: T)(implicit A: Add[T]): T = A.add(t, t) def doubleSyntax[T](t: T)(implicit A: Add[T]): T = { import Add.Syntax._ t + t } doubleDirect(2.2) // 4.4 doubleSyntax(3.3) // 6.6
I just saw this posting today on my twitter feed: [https://mewe.breezy.hr/p/1367b4f6378901-senior-scala-developer](https://mewe.breezy.hr/p/1367b4f6378901-senior-scala-developer)
I would argue that Seq gives the caller control over the performance by being able to choose an implementation, while List removes it by forcing the specific implementation.
I agree that you'd like to be able to keep out Streams and other exotic Seq implementations most of the time, but unfortunately the class hierarchy doesn't currently contain `StictSeq[A]`. However, I think that &gt;you should always know what performance you're getting out of your data structures and never leave it up to chance. is too strong. One benefit of leaving it as `Seq` is that you don't need to copy all non-List Seqs into a list before constructing the object. Or maybe in some places you want to have List performance characteristics, and other times WrappedArray performance characteristics. Anyways I think the restriction of having to always know the performance characteristics of a data structure is kind of at odds with a lot of functional programming concepts. You can't say anything about the underlying performance characteristics of `F[_]: Functor` is, or of any given instance of `JsonWrites[T]`, `Iterable[T]` or `Map[K, V]` is, for instance, but we still find those abstractions to be useful. 
This restriction feels artificial to me. Unless the algorithm specifically requires List I tend to choose more general type which would be Seq. I also don't subscribe to the opinion that a case class is for data at rest. Case class is for pattern matching which has nothing to do with internals of its data.
&gt; What's your opinion on using `Seq` in case classes? I think it's totally fine. While I understand the principled stance in favor of concrete types like `List` that Cats and Spiewak take, I've found it annoying to have to put `.toList` everywhere now that I'm working on a project that uses cats-effect. For all the mania that `CanBuildFrom` introduced, being able to pass any "bunch of futures", even an `Iterator[Future[...]]`(!) to, say, `Future.sequence`, instead of having to do `bunchOfIOs.toList.sequence`, is nice. It's not a big deal, but the push for `List` to be the One True Collection Type is what feels wrong to me. &gt; I was always under the impression that a case class is for data at rest. Case classes are just regular classes with some syntactic sugar added: `equals`, `hashCode`, `&lt;Companion&gt;.{apply,unapply}`, etc. That's really it. Seeing them leads to misconceptions that come up all the time, like that case classes can't be mutable. (They probably *should* be - as with any class - but they don't *have* to be.) I don't recommend thinking of case classes as being *for* anything in particular, at least not any more than plain classes are *for* something. &gt; `Seq` feels like it's inviting things like lazy evaluation and infinite streams. I guess, but then that's on you and your team. I've used `Seq` heavily since ~2009, and I can't remember a bug caused by an accidentally-infinite or accidentally-lazy `Seq`. I *can* remember bugs caused by the laziness of `Map.mapValues` or the over-eager `Future.apply`, so I *am* tuned into that sort of thing in general. I suppose this will out me as an uncool non-FP-purist (as if I haven't already!) but I just don't get the hate for OO subtyping. TLDR: `Seq` is fine. If it causes problems for you (it hasn't for me in ~10 years), use concrete types instead.
Fair enough. I guess I never thought about actor-based systems as true examples of microservice architecture.
Yeah, we used thrift at my last gig, and the Scala guys had conversions for all the Twitter Apis and used scrooge. Short of writing your own sbt plugin you're scrooged
It should be an interface and not a concrete type though, I have definitely had to back up and change everything to a `Vector[T]` when I discovered the `Seq[T]`'s I was naively using were * **O**(n)* for my access pattern. 
But you can use case classes without pattern matching, and pattern match on things that are not case classes, so it does not seem right to say that "case class is for pattern matching". 
Intellij. This is going to stay true until we get scala 3 that will come with its LSP (Language Server Protocol) and will make vscode a serious alternative.
&gt; It should be an interface and not a concrete type though `Seq` *is* the abstract type here. Maybe you meant the other way around? &gt; I have definitely had to back up and change everything to a Vector[T] when I discovered the Seq[T]'s I was naively using were O(n) for my access pattern. Me too. I say that if code cares about the performance characteristics of its arguments, it should specify those arguments as concretely as it needs to. (ie, you might want to require a `Set` instead of an `Iterable`, but not care if you get a `HashSet` or some other kind.) Along those lines, maybe `IndexedSeq` would have been appropriate. (Obviously I'm just guessing.) But one should think carefully about what code really has those requirements, since by placing them everywhere, you lose out on the looser coupling and smoother refactoring that abstract types enable.
Have you looked at Scala with Cats, https://underscore.io/books/scala-with-cats/ ?
Oh, cool, thanks! That works and is giving me stuff to think about. 
I was using Seq(x,y,z) to instantiate Seq’s. I’m unsure what it was instantiating
Secure with respect to what properties? You might want to start with a [STRIDE threat model](https://en.wikipedia.org/wiki/STRIDE_\(security\)) * Spoofing of user identity? * Tamping? * Repudiation? * Information disclosure? * Denial of service? * Elevation of privilege? 
Thank you ! I looked at it just now, will go through it. But in the prerequisites, it is mentioned a year of Scala experience is needed. I have just &lt; 3 months experience and have read Odersky's book. Is it enough? Or do you suggest to read any additional books like red/blue book or the essentials book present in [underscore.io](https://underscore.io) ?
Usually, you get a `List`: scala&gt; Seq(1,2,3) res0: Seq[Int] = List(1, 2, 3) Note that when you say `Seq(...)` you're saying, "give me a sequence", not saying what kind you want. I can imagine an argument for being explicit about what type you want when *creating* `Seq`s like that (`List(1,2,3)` or `Vector(1,2,3)` instead). But when accepting a sequence of things, I tend to type my params as `Seq` unless I know I need some more specific type. If I just need a bunch of things in any order, I often reach for `Iterable`. 
Seq gives the caller control to choose an implementation and immediately takes it back by losing the type information about what implementation was used. If I wanted to give caller control I would use a type `T &lt;: Seq` and let him keep that information.
You can't really create a secure call when only one party is involved. The server and client need to both be involved for information to be transferred securely. Https is going to be the most straightforward way to get there. If you are controlling the client and server code you could encrypt the body of request and response in your application code instead, but I wouldn't really recommend that. If you control both the server and client code I think the question would then be how could it be possible that https is not an option? I think there is definitely going to be a much better and cost effective solution on the operations/infrastructure side than anything you could possible implement on your own.
https://leanpub.com/fpmortals is quite good. Its about scalaz but it doesnt really matter.
I got a little further and am now confused again and missing something. If I do: trait subt[A] { def combine(l: Seq[A], r: Seq[A])(implicit Q: Add[A]) : Seq[A] = { l.zip(r).map(lr=&gt;Q.add(lr._1, lr._2)) } } it compiles, but if I change all terms to Array: trait subt[A] { def combine(l: Array[A], r:Array[A])(implicit Q: Add[A]) :Array[A] = { l.zip(r).map(lr=&gt;Q.add(lr._1, lr._2)) } } the compiler says that the return type is mutable.ArraySeq[A] instead of Array[A]. If I try to force it with .toArray, I get errors about "No ClassTag available for A" and "not enough arguments for method .toArray", all of which are making me think I'm missing something else simple. Thanks! 
Consider why you even want/need a microservice architecture in the first place. It brings a lot of complexity to the application, given that it's a distributed system. 
Don't worry about that. If there is something in 'scala with cats' book you don't understand, you can refer Odersky's book or ask here.
You should never use `Seq`, even if you assume it's going to be always strict or even always a List, using Sew has nothing but downsides. It's slower than using `List` directly, it contains unsafe methods, it causes more allocations. It's not even easier to use.
I agree, certain vocal people in my company hates Scala with a burning passion and love Go because they think Scala is hard and makes them feel stupid. They refuse to learn it, when actually I find it becomes easier than most other languages once you've actually learned it.
This won't give you everything you need for cats by any means, but if you want a good starting point and to understand IO, I think this article is fantastic: https://typelevel.org/blog/2017/05/02/io-monad-for-cats.html It helped me a lot when I was learning cats/scala on the job. It's not too long either! Especially since towards the end it starts talking about other concepts that you don't necessarily need to learn right now.
Put a network analyzer on that code and you will realize that you are passing the BASIC credentials in plaintext over the wire. First off, if you don't have control over the service, and the service does not support SSL/TLS, and they require you to use BASIC authentication, there isn't much you can do. If you own the service and you can bind it to localhost, there is some hope there. An attacker would have to be able to run something on your server to be able to get to the service or otherwise attack you. Writing your own security layer is fraught with hazards, and almost no one can do it without leaving something wide open. That is why we use standards like SSL/TLS - and even those are updated over time as people figure out different vulnerabilities, which are subsequently patched. So, from the other responses, I would guess the question to you would be ... WHY is https not an option?
I have a method, and I'd like to enforce all invocation to this method to use named arguments. Is there a way to do that? For example, `test(1,2,3)` would fail the compilation, our coder would need to use `test(arg = 1, anotherArg = 2, yetAnotherArg = 3)`. Thank you.
Short answer is yes. However, community and libraries are more about performance and type safety. If you need it and these things will benefit you then yes, sure. Personally prefer Scala over any other production ready language, but I often write Python based services too. If I know that a service I need to write will never encounter thousands requests per second and I know that this service is not mission critical I'll chose Python. In all other cases I'll go with Scala. It is more about time management, you can glue two API endpoint with python in minutes, not really the case with Scala. 
Always return the most specific thing you can and accept the most general thin you can. If you don’t need a something only on List accept a Seq, or an IndexedLike, whatever
Man, Jetbrains' valuation must be extremely high considering they have won the IDE war in many areas. I still can't believe nobody actually competes with them. Building an IDE must be really difficult.
But case classes specifically define `unapply` for the purpose of pattern-matching on record-like data. So yes, they are a data type relatively specialized for pattern-matching.
- What unsafe methods does Seq have that List doesn't? - What extra allocations does it cause? - How is it harder to use?
May I ask why List is the blessed collection data structure of the pure-FP crowd? Why not Vector? The latter will almost always perform better.
I think OP actually implied they want a monolithic application, not microservices.
Yeah, Scala is good for a monolithic webapp. The Play Framework is very high quality and is actively worked on. It's an MVC-style framework with templating, internationalization, and great error handling. If you're looking for a full-service web application, I'd say go for it. One caveat though, it's a little too easy to inject framework dependencies into parts of the application that should have nothing to do with the specific framework. Keep an eye on what dependencies you're pulling in and see if you can keep them split up. Scala's traits are a godsend for this, they let you decouple interfaces and implementations. The so-called 'haxogonal architecture' is a bit harder to accomplish but IMO worth it.
Sorry but–I thought matching on a type parameter was impossible due to erasure. Are you not seeing an erasure warning?
IntelliJ but it's better to have 16 GB of RAM on your machine if possible. 8GB is a bit on the thin side especially with a bigger project.
The `toArray` method requires an implicit `ClassTag[A]` parameter, which you can add to the method signature. def combine(l: Array[A], r:Array[A])(implicit Q: Add[A], CT: ClassTag[A]) :Array[A]
&gt; It's slower than using List directly How is it slower?
- `Seq` isn't great for reasons stated - `List` has meh memory usage (sooo many pointers), meh overall performance (sooo much pointer chasing), and terrible lookup. It's *only* good if you are adding something one at a time, or removing something one at a time, to the same end. - `Vector` has reasonable `O(log n)` asymptotic performance, but pretty bad constant factors all around. In many use cases, it is slower than just concating `Array`s together: the structural sharing is not enough to make up for the awful constant factors, unless you are appending/prepending/inserting/removing elements one at a time. - `Array` has great performance, but is mutable, which is often not what you want. Surprisingly, it is faster than `List` and `Vector` when you are concatenating roughly-equal-sized chunks together, even including all the copying Honestly there are no great solutions; they're all pretty meh. IMO the primary linear data structures should be immutable `Array`s and mutable `ArrayDeque`s, unfortunately neither of them exist in the current version of Scala. Source: http://www.lihaoyi.com/post/BenchmarkingScalaCollections.html
That's a nifty thought. I'm stealing it. But it does introduce a type param, and also the users of the class may not necessarily care about having access to the specific implementation after instantiation.
Twitter heavily uses Scrooge in production. You don’t have to use util and finagle if you don’t want, but you’re not going to find a better tested or faster thrift implementation.
Metals don't provide autocompletion which IMO a must for any IDE. I posted this question in their gitter page and they said supporting auto-complete is not in their roadmap. Hence for the moment Intellij is the best option.
As far as I know Vector has generally terrible performance. Here are some comparisons: https://github.com/fosskers/scala-benchmarks/blob/master/README.org
See the "pattern matching" section here: https://github.com/fosskers/scala-benchmarks/blob/master/README.org
Interesting benchmark, I guess I was going off the [Performance Characteristics](https://docs.scala-lang.org/overviews/collections/performance-characteristics.html) page and just looking at the Big-O performance. And Haoyi seems to more-or-less agree with your benchmarks as well: http://www.lihaoyi.com/post/BenchmarkingScalaCollections.html#vectors-are-ok I guess the takeaway is that Vectors might be a good default, unless you want to iterate over the collection in which case you should _really_ use a List.
Agree. I hope `immutable.ArraySeq` that is part of the new 2.13 collections can solve this.
Thank you ! I think this will be very helpful :)
I wish I could cover all that, but 2 days won’t be long enough ^^
This video takes the principles from the ground up in a really easy to understand and great motivated way: [https://www.youtube.com/watch?v=sxudIMiOo68](https://www.youtube.com/watch?v=sxudIMiOo68)
There's no unsafe match. `a` is being extracted (and will be erased at runtime). `scalac` knows that `A` is the same type as `a` so the workaround is a no-op. IntelliJ probably has a warning since the workaround will look like an unsafe match to it, I didn't check that. 
I would suggest Functional Programming for Mortals. Though it uses scalaz as a "platform", all the concepts are the same in cats. This is a really good book, so I suggest you do give some bucks to the author, even though the book is "Creative Commons" licensed
Have a look at Scala.js + Twirl templating also for FE part and Akka HTTP + Streams for BE. Scala is just great tool for Web, in my opinion.
You can’t. HTTP is plain text, whatever you do will always have some visible side that an attacker can exploit. The reason HTTPS works is because it wraps HTTP and relies on trusted authorities to validate your certificates, and even then there’s loads of ways things can go wrong.
From what i know about comparison between the two, Vector's only advantages come from indexed access (i never once had to do it in Scala) and appending. Meanwhile List sucks at both, but is marginally better at prepending and getting the first element and rest. Which is what people tend to do in Scala in 99% cases. Hence why it us used over Verctor.
I don't think this is possible. The closest you could get would be to take in a vararg of tuples like def test(args: (String, Int)*): Unit test("arg" -&gt; 1, "anotherArg" -&gt; 2, "yetAnotherArg" -&gt; 3) or maybe you could make an enumeration of argument keys, either with sealed trait/case object ADT, or with scala.Enumeration, to get like... sealed trait Argument case object Arg extends Argument case object AnotherArg extends Argument case object YetAnotherArg extends Argument def test(args: (Argument, Int)*) test(Arg -&gt; 1, AnotherArg -&gt; 2) But these approaches won't let you enforce that there is a value for each of your three argument keywords
&gt;Always return the most specific thing you can Depends. Setting the return type to the most specific possible commits you to returning that in the future. Maybe you don't want to be committing in this way because you want to be free to change your implementation later.
In 2.13, Vector concatenation may be getting an order of magnitude faster if we/I can get [this pr](https://github.com/scala/scala/pull/7588) into shape in time.
Colin Woodbiry has an excellent talk on [this](https://www.youtube.com/watch?v=-UEOLfyDi74) and [benchmarks](https://github.com/fosskers/scala-benchmarks/blob/master/README.org) where he got is observations from.
Why do you feel that this is quicker to do in Python than Scala? I feel like this can go pretty quick in Scala if you make use of one of the \`sbt new\` templates.
Thank you. Another way that I can think of is wrapping the value with a case class. At least, it forces users to look at the name of the case class. But all these options seem a little verbose than needed to. I see that scalastyle has this rule, but the rule enforces all method invocations. Maybe the right way is to contribute to scala style. I could imagine that we add a comment or an annotation to enforce this rule only on certain methods.
While there's nothing wrong with Akka-based services on one JVM though there are important restrictions related to this architecture. Isolation (process, limiting resources, deployment cycles, library/version/language dependencies) is crucial for microservices. Microservices and DDD isn't just one kind architecture and approach. It is also defines even your organisation structure.
&gt; It doesn't? That's because List implements Seq I assumed when you said: &gt; It's slower than using List directly, it contains unsafe methods ... that you were comparing it to List. Anyway, I've been meaning to watch the 'How not to write slow Scala' talk for a bit, thanks for the reminder. &gt; Because if you actually want to keep your data type a List if you use Seq In that case you would actually just use List instead of Seq, no?
TIL you can expose type variables with a lower case type variable match ._.
Really? Last I looked Intellij went out of the way to have special highlighting for HLists, shapeless records and Witness.`"String"`.T types.
Let's define a task: call some API which gives back JSON, store result in a database, POST transformed JSON data to another service. Deploy this service to fresh AWS t3.micro with the last LTS Ubuntu version. &amp;#x200B; For both programming environments we need to have the same: web server, web client, data persistence. Say for Scala we will use akka-http with some marshaler/unmarshaler and slick. For Python we will use flask and requests. &amp;#x200B; There is a good chance that I'll write half of the business logic in Python before I could even install all dependencies to AWS box, write .conf files and compile the project for Scala. You see, scaffolding is not an issue whatsoever. E.g. basic thing, how to unmarshal JSON into *some* object in Python? `json.loads(data)`. In Scala? Well, case classes and type level stuff with implicits. Or another trivial thing - persistence. &amp;#x200B; `sbt new template` is not solving any problem because there is no problem to solve, its just a convenience. &amp;#x200B; Note that I'm talking about some minor service which won't take any significant load nor mission-critical. Such a service can not benefit from Scala, however Scala is still completely valid choice.
That’s possible. Mostly i’ve been using lenses, which it doesn’t support.
Software verification example from the Stainless documentation: https://github.com/epfl-lara/stainless/blob/master/core/src/sphinx/intro.rst#software-verification
Have you looked at the documentation published on the Cats-Effect website? - Tutorial: https://typelevel.org/cats-effect/tutorial/tutorial.html - IO: https://typelevel.org/cats-effect/datatypes/io.html You might also want to checkout these presentations: 1. "The making of an IO": https://www.youtube.com/watch?v=g_jP47HFpWA 2. "Monix Task: lazy, async and awesome": https://www.youtube.com/watch?v=X-cEGEJMx_4 3. "Monix Task: lazy, async and awesome": https://monix.io/presentations/2017-task-scaladays.html You can also hop on our Gitter channel: https://gitter.im/typelevel/cats-effect
Yeah it's a bit counterintuitive. Turns out the rule that things starting with upper case are treated as constants and things starting with lower case are extracted variables applies to type parameters as well.
I'm unfamiliar with some of the jargon you used. Is this in the same vein as solidity?
Do I get it right that Stainless uses smt solvers under the hood? If yes this could be a huge step for Scala. If seen smt solvers used in Haskell ([see Liquid Haskell for more details](https://github.com/ucsd-progsys/liquidhaskell)) and really wished that would also exist for Scala.
Minor clarification/addendum to this- It's a great language for everything except where processing speed is critical and/or memory is limited.
Nobody should tmplate using twirl with scala.js. Use laminar or scalatags
Sometimes you need server side templating, don't you? SEO?
Member of the lab here! Happy to see Stainless getting some publicity on reddit, thanks for posting it here /u/yawaramin! I dont have have time to answer questions right now, but I or other members from the lab will be back later tonight. in the meantime, feel free to checkout the HTML version of the documentation, which is more readable amd easy to navigae than .rst files in the repository: https://epfl-lara.github.io/stainless/ (it’s not great on mobile yet though, sorry about that) Main website: http://stainless.epfl.ch
I think not laminar but scalatags definitely lets you run with regular "server" scala. 
Probate it does. Although saying "nobody should" do something, you also should have strong arguments against it - not just because you into templating trying to do HTML completely different. Variety tools for different teams. 
twirl is definitely not the right tool for anybody. It's terrible.
Terrible isn't a good technical category. If you need an efficient HTML/plain text templating for Scala - why is so "terrible"? 
Thanks! I thought there might be a static site somewhere, didn’t occur to check the GitHub Pages URL! I’m exploring it now, looks very interesting. I kind of stumbled upon it while looking for Design-by-Contract resources for various languages. Interestingly, there’s also Imandra for OCaml which seems very similar. Exciting times with formal verification taking steps into the mainstream!
i'm sorry- you're being nice and i'm being vague. but one of the biggest reason is it's weird use of newlines and what not. it's been 3 years but i had trouble with a few things. i think there was something about braces and newline- can't recall but I didn't think the thing was consistent. 
Indeed—it looks like Stainless uses the Inox solver which in turn can use various SMT solvers in a pluggable way. I’ve been looking for something like this too—there’s Imandra for OCaml (as one would expect there to be formal provers in the ML family), but Stainless is exciting because I didn’t actually think that Scala was amenable to these techniques.
They are orthogonal actually. Stainless and other formal provers like it can mathematically prove properties about your functions. It’s like generative property testing except it’s not _actually_ generative, it actually symbolically calculates a universal proof (or refutation). Check out the example in my first comment to get an idea of what it does. Formal provers can of course be used to _verify_ smart contract software, though, which is probably why the crypto community is so interested in them.
No worries. Scalatags is a good (and also efficient btw) tool if you could use completely different syntax for HTML. I used it too. Unfortunately, in some teams it isn't a good option. For example, once I had a web designer who wanted to do some changes in HTML sources himself - he wasn't so happy about twirl actually too, but was completely against anything that different.
Why is https not an option?
&gt; There is a good chance that I'll write half of the business logic in Python before I could even install all dependencies to AWS box, write .conf files and compile the project for Scala. You see, scaffolding is not an issue whatsoever. I'm not sure I understand this. Isn't installing dependencies to an AWS box and writing .conf files things that need done regardless of Python or Scala? I can understand not wanting to wait for compilation. &gt; E.g. basic thing, how to unmarshal JSON into some object in Python? json.loads(data). In Scala? Well, case classes and type level stuff with implicits. Or another trivial thing - persistence. Couldn't you use a reflection based library and get similar results?
Exactly! Stainless depends on [Inox](https://github.com/epfl-lara/inox), a SMT-backed solver for a functional programming language featuring polymorphic recursive functions, lambdas, ADTs, ADTs invariants, quantifiers, and dependent types. &amp;#x200B; What Stainless itself does, is that it hooks into the Scala/Dotty compiler to extract the program after type-checking, and then lowers down a fairly substantial subset of the language down to Inox's input language, by encoding classes, inheritance, mutation, inner classes, etc. into a representation Inox can reason about. By doing so, it can then emit verification conditions that can be checked by Inox and the SMT solvers it rests upon. &amp;#x200B; You can find out more about how the basics of the system by [taking a look at this paper](http://lara.epfl.ch/~kuncak/t/sas.pdf) (what is called Leon in the paper has since been almost entirely rewritten and split into Stainless/Inox). &amp;#x200B; We are very much aware of Liquid Haskell, and there has been some [preliminary work to get refinement types into Dotty](http://lara.epfl.ch/~kuncak/papers/SchmidKuncak16CheckingPredicate.pdf) (the next version of Scala), although it is not clear yet whether that will eventually happen or not. On the other hand, we are now in the process of forking Dotty to [add support for refinement types syntax](https://github.com/epfl-lara/stainless/pull/436), and use it as a frontend for Stainless.
Yep, there is no aprori relation between what are called contracts in the blockchain space, and contracts for verification. On the other hand: &amp;#x200B; &gt;Formal provers can of course be used to verify smart contract software, though, which is probably why the crypto community is so interested in them. Indeed, we are also maintaining a [fork of Stainless](https://github.com/epfl-lara/smart), which allows: &amp;#x200B; a. Writing Solidity contracts in a Scala DSL b. Verifying the correctness of these contracts/programs c. Compile the Scala-based contracts to Solidity
This sounds exciting! I hadn't even realized there was ongoing work in formal proving for Scala. I'll definitely take it for a spin. Liking the fact that even without the formal proof aspect of it, writing in this style automatically conforms to a Design-by-Contract style.
When I find a spare hour, I keep working on the "minimum viable" version of the embeddable mini-IDE for Scala: https://github.com/dotterweide/dotterweide Still looking for contributors to help advance this project!
I don't agree with most sentences that start with "you should always." The only things that are "always" are underlying principles. More to the point, I've had many times that using List as a "contract" created a need for converting between collection types, and using Seq did not create any problems. &amp;#x200B;
Another traits and types question, I don't understand why trait W[A,B] { def f(a: A) : B def f(a: Double) : Double = 0.0 } class WD extends W[Double,Double] {} doesn't compile, with the message: Error:(254, 7) class WD needs to be abstract, since method f in trait W of type (a: Double)Double is not defined (Note that A does not match Double) class WD extends W[Double,Double] {} If I add an "override def f(a: Double):Double = 0.0" in WD, it compiles. My first thought is that this should have been fine because there's already a (a:Double)Double method in W. My second thought is that wait a minute, are there now two methods with the same name and signature? And some experimentations suggest yes: class ZD extends W[Double, Double] { def f(a: Double) : Double = super.f(a) } gives the error : Error:(263, 46) ambiguous reference to overloaded definition, both method f in trait W of type (a: Double)Double and method f in trait W of type (a: Double)Double match argument types (Double) and expected result type Double override def f(a: Double) : Double = super.f(a) Anyway my question is basically where did I go wrong in expecting the original version to work? (I ended up solving the problem with an intermediate trait "trait W0[B] extends W[Double, B] { ... }" that contained the specialization and I'll just inherit from W0[B] whenever I would have inherited from W[Double,B], but that feels a little inelegant)
Thanks for your response really appreciate it. &amp;#x200B; I tried to access the url through https but it did not work which is why Im thinking https is not an option but I will circle back. &amp;#x200B; Thanks again !
Thanks makes sense , and I appreciate it !
Thanks
Although being technically possible, abstract Finagle over an effect type is not something Twitter could be interested in. Unfortunately, I can't really see how this could be driven from the outside either.
I would say that if anything Scala is better for monoliths than microservices, because it's a language that makes it easy to have services that are isolated from each other, or have strictly controlled/limited interaction with each other, without having to separate those services by a network boundary.
You can use Scrooge to generate non-Finagle implementations if you write the templates you need - I have an unmaintaned hacked-up fork that does just enough to write a working server that used ScalaZ Task (I think?) as the carrier instead ( https://github.com/m50d/scrooge ).
I use `Seq` in case classes but I only consider it ok because I ban stdlib `Stream` and friends in my codebases. I don't think you should always have to care about the specific performance characteristics of your sequences (I rarely do) but I do think the difference between a strict and a lazy sequence is too big to hide.
Hmm. I didn't expect your first attempt to work, but I struggle to explain why, so maybe I've just internalised some Scala weirdness. I guess it would seem odd for `f` to override `f`, but the language doesn't disallow "unrelated" methods overriding each other. Appreciate that this isn't really an answer.
Why is this downvoted? Honestly, I don't see why.
Me neither. I actually came from OO background, switched to scala 3 years ago. I remember how hard it was to grasp into advanced functional concepts. And this book gives not just concepts, but how and why implement them in real development. 
No. Blow the $150 on IntelliJ, it’s worth it in so many ways
Please repost this with a title which is: * not clickbait * not inflammatory maybe something like "Eugene Burmako joins Google to work on Swift for Tensorflow"
From my own experience: it is ok to edit code without IDE with complex code, but it's really inconvenient to write something new. You want to have IDE at least for imports management.
The Scala plugin for IntelliJ IDEA is available for the community edition and works marvelously. No need to spend any money. (Of course the Ulitmate or whatever edition has many other things, but it's not necessary for scala development.)
Well. Unfortunately everything negative that I heard on Twitter about this subreddit seems to hold true... This title is neither clickbait, nor inflammatory. It is purely stating a fact... The tweet clearly sates a farewell to Scala. So the part about leaving is completely accurate. &amp;#x200B; How anybody could understand this as clickbait is beyond any understanding! &amp;#x200B; The only part one could argue with is Eugene being a great mind. But considering what he has achieved over the past years this statement doesn't seem too far fetched...
It is clickbait because you didn't even say in the title who you're talking about, what happened, etc. Why can't you just summarize what happened in the title? It is inflammatory because you editorialized it to add your own spin to the situation in order to imply there's a flood of people leaving Scala. You could have chosen to editorialize the title to congratulate Eugene for all the work he's done, or give him a farewell, or indeed not editorialize it at all. Plus, you have no history of posting to this subreddit, so I have no basis on which to assume good faith from you. I'm totally happy to chalk this whole thing up to an honest misunderstanding, and please feel free to repost the post with a different title like stated above!
I use intelliJ for my work, so the community license doesn’t apply to me. Violating the license terms is stealing from other people like me who are programmers, it’s easy to justify spending the 150 bucks given the value
I don't know what OP needs wrt. Editions, but AFAICT the Community Edition is Apache 2.0 licensed? That's what LICENSE.txt in the root of the unpacked distribution says, at least.
Last I checked it was available for commercial use.
Oh, maybe I’m thinking about visual studio, anyway I needed Grails support
It’s not worth the hassle. IntelliJ Community Edition is free (for commercial use) and at work you really don’t want to be worrying about tooling setup, you’ve got other things to worry about. Just spend some money on a beefy machine and you’re good to go. We really need an FAQ for this reddit, there’ve been three different posts with this exact question within the past few weeks.
It's possible, but I don't recommend it. Plenty of people do though. :\ My coworker doesn't use an IDE when working on a complex project. It's affected his code in negative ways. For one, he puts the same zillion wildcard imports at the top of every file. Since it's hard to know what's needed or not without tooling, he just pulls in everything. I've actually had implicit clashes when I've added new imports for the first time in years. Everyone else is right; just use an IDE.
I agree, this does seem possible.
In my (seemingly unpopular) opinion it all boils down to personal preferences rather than an objective reality. I've been working on small-to-big projects with just Spacemacs (no Ensime as I struggle with memory leaks on big projects and no Metals/LSP because I didn't try installing it yet) for years and I am pretty satisfied with it. If anything, I'd like to go back to pure and simple vim on terminal, not an IDE! Despite what many people say, I find my setup perfectly fine to work on projects of any size. I never felt like I was slower than my peers using an IDE (IntelliJ 99.9% of the cases) and I actually found that when doing more advanced typelevel programming Idea slows them down with weird/inconsistent/non-existing errors or warnings that you don't get with \`sbt \~test:compile\`. Sure, it would be nice to have a some auto-complete or auto-imports but I don't find that those are really as important as many people think. Maybe if you are a beginner it is a good idea to start from an IDE but it's only up to you (and to your patience and commitment) to learn to do without.
Working without an IDE is doable - Scala is as concise as many scripting languages, so you're not dependent on autocomplete the same way you would be in e.g. Java. But it's always going to be less effective than working with an IDE. Underlining of implicit conversions, type information on mouseover, and jump-to-definition all help you comprehend code faster. Worksheets and better incremental compilation/testing make your edit-check loop faster. It's never indispensable but it all helps.
&gt; I actually found that when doing more advanced typelevel programming Idea slows them down with weird/inconsistent/non-existing errors or warnings that you don't get with `sbt ~test:compile` That's why I stick to Eclipse/Scala-IDE - the error highlighting and incremental compilation is as reliable or more so than SBT's. &gt; Maybe if you are a beginner it is a good idea to start from an IDE but it's only up to you (and to your patience and commitment) to learn to do without. IMO this is a counterproductive attitude. Yes, one can learn to get by without all these powerful tools, but there's nothing to be gained from avoiding a tool for the sake of it as some sort of demonstration of "patience and commitment". Experts don't make extra work for themselves without good reason.
I'm sorry if I didn't express myself very well but I didn't mean that the whole thing was for the sake of patience and commitment, those are requirements if you want to do the transition, not the end-game. Also, I don't think that I am doing any extra work than other people using a fully fledged IDE: I may miss a few tools in my environment but that's also true about IntelliJ when I compare it with my environment. When I say that it depends on personal preference rather than objective reality I mean that only you know what is more relevant to your style, productiveness or even enjoyment (after all that has an effect on productivity as well). Code intelligence, customisability, keyboard operation, plugin ecosystems, speed, running on a terminal, having the same environment for wildly different languages... everyone will rank these and many other factors in a different way and personally I find that whatever language I am using doesn't really have much bearing on how I rank those (except, as I said, when I am learning something new, in which case I may rank code intelligence higher than I would do otherwise otherwise).
I, and I think a lot of people also, often run sbt shell in IntelliJ with either `~compile` or `~testOnly ...` or `~testQuick`. It's the best of both worlds.
Oh god!
Thank you ! Will go through it.
Whoa, neven knew I will get a reply from on the type level member itself here ! Thank you so much, Alex! Will check them out.
Thank you, Mike! Will check it out.
Thank you ! 
Thank you ! 
Where u can pay $150 on intelliJ? in their site i only found $299.00 if i buy for 3 years.
I'm a huge fan of the HOCON config file format. I wish more projects would adopt it. Spring for one :D
&gt;. Code intelligence, customisability, keyboard operation, plugin ecosystems, speed, running on a terminal, having the same environment for wildly different languages... everyone will rank these and many other factors in a different way and personally I find that whatever langu How to solve the problem with not used imports ? Do you just put all imports without look them?
We working with Atom, VSC and IntelliJ community edition with Maven as build tool, and we are more than happy. Each developer uses the tool he prefers, in my case Atom. I would be more worried on the CI-CD stack than in the IDE. 
You can use instead bloop (a Scala build server) and just export your build with `sbt bloopInstall` and run `bloop compile -w your-project` to have the best experience when developing your code locally. Check out the installation steps and more general info in our website: https://scalacenter.github.io/bloop/
I think this is a pretty simple question. Let's say I've got a trait like this: trait Foo {} and some case classes like so: case class Bar(x: Int) extends Foo case class Baz(x: Int) extends Foo What's the right way to, say, sort on the `x` field for a collection of `Foo`s? val foos: Seq[Foo] = Seq(Bar(1), Baz(3), Bar(0)) foos.sortBy(_.x) // error: value x is not a member of Foo 
There are a few non-English speaking Gitter channels. As for conferences and talks, there's at least [Scala.IO](https://Scala.IO) (French) and Scala Matsuri (Japanese). I'm sure many local meetups exist around the world, but it'll be a lot harder to find content online.
You'll either need to introduce a method to the `Foo` trait that returns the `Int` you want to sort by and call that in `sortBy`, or else use pattern matching to get the correct field from each possible variant.
scalafix to the rescue https://scalacenter.github.io/scalafix/docs/rules/RemoveUnused.html 
&gt; scalafix to the rescue Thank you!!!!
&gt; Do you just put all imports without look them? Yes: &gt;&gt; I copy my most used imports and keep them in the Emacs kill-ring :(
Why not force the project to use `"-Ywarn-self-implicit", // Warn when an implicit resolves to an enclosing self-definition.` `"-Ywarn-unused:implicits", // Warn if an implicit parameter is unused.` `"-Ywarn-unused:imports", // Warn if an import selector is not referenced.` `"-Ywarn-unused:locals", // Warn if a local definition is unused.` in build.sbt Then make those warnings compile errors. The only way you can stop import foo.\_ is via CR, and then its up to you. I have a coworker that only uses vim and he has done so for a while, so while it may take longer (i'm an IDE user) it will remove the crutches...eventually.
Scala was invented by a German, I would think there would be German communities?
There's a Spanish speaking community as well but I believe most of us just hang out in the English speaking chat rooms. There's also an upcoming Scala conference in Uruguay, South America and I'm really happy to see that :) https://twitter.com/SLatam2019
It is completely possible and not that crazy. I use Vim and have never had any issues. Tell your colleague to compile and run some tests before pushing code ;)
Metals supports almost all these features. I'd argue an IDE is no longer needed for those who are experienced with Scala and its perks but for beginners I'd probably recommend Intellij Idea.
&gt; Tell your colleague to compile and run some tests before pushing code ;) Sigh, if only, but that's another issue.
Fetching the primary page data with Ajax and doing the CRUD actions in modals sounds like turning each page into an ad hoc SPA. I'd rather go whole hog in either direction. I also hate modals so I'm biased.
Absolutely valid and often far faster and better results than javascript. 
I think there's nothing wrong with old-fashioned. If it ain't broke, don't fix it. The best possible technology choice for your app is what makes the most sense given all the requirements that you can uncover–requirements like supported browsers, number of people or teams who will be working on this codebase, how many expected users, and so on. That said, one of the requirements is that you are actually comfortable with and have some fun with the technology choice. There's a lot going on nowadays in terms of full-stack development but one of the techniques that I'm keeping my eye on, is server-driven dynamic pages. Think Single Page App but the frontend is a very thin client managed by the backend which pushes tiny DOM updates to it over a websocket connection in response to events. Can't work? There are already prototypes :-) Here's one I'm looking forward to: https://dockyard.com/blog/2018/12/12/phoenix-liveview-interactive-real-time-apps-no-need-to-write-javascript
I would also maybe make `Foo` extend `Ordered` (if it makes sense in the domain)
Here in Russia we have telegram channel t.me/scala_ru and it’s popular
TIL. Always thought Martin Odersky is French. 
Yes, I do use `vim` + `sbt` for smaller projects or quick edits on larger projects. I am looking forward to [metals](https://github.com/scalameta/metals) becoming mature though, so that I can use `vim` more effectively for such tasks.
It's possible and you can be productive, but Scala is a language that definitely benefits from an IDE.
Since you can't guarantee that those are all the subclasses of `Foo` without sealing it, you need to handle cases that aren't `Bar` or `Baz`. Since `Option[Int]` has a defined ordering, it works pretty well, if you just need something ad hoc. foos.sortBy { case Bar(x) =&gt; Some(x) case Baz(x) =&gt; Some(x) case _: Foo =&gt; None } 
I only import the things I need and I use, I avoid wildchar imports and even when using cats or similar libraries I try to import only the bits I need (e.g. \`cats.syntax.functor.\_\` rather than \`cats.implicits.\_\`); I have a comprehensive set of flags on my compiler and `-Ywarn-unused-imports` and fatal warnings are among them. I'd like to try scalafix for this at some point in time but I didn't really feel the need so far.
That's exactly how I do it, plus if you want to disallow `import foo._` you can use scalastyle ;)
Ensime had some maintainer issues so I wouldn't expect to see a lot of active development.
You might like to look at Wicket - I'm a huge fan of its approach to GUI in general (component-based, OO done right, not so page-cycle oriented). It has a nice hybrid approach where all rendering is done on the server, but you can update parts of pages via AJAX (the response includes appropriate rendered fragments). It's Java-first but it's usable from Scala (there's a Wicket-Scala library with some helpful adapters). I would not worry about browser compatibility if you're using any kind of established framework. The days of having to rewrite into static forms for compatibility are long gone. I definitely wouldn't recommend trying to take your own ad-hoc approach to using AJAX for parts of the page without a framework - that sounds far more likely to have compatibility issues. I loved server-side from a coding perspective, but the frustrations that apply in consumer-facing applications are all there for internal systems as well. On an internal network roundtrips are faster, but having to shift between pages is never not going to be annoying. (Though, as another reply said, modals can be equally annoying).
The Italian community has a slack channel: [https://slack.scala-italy.it/](https://slack.scala-italy.it/)
If your list consists of exactly 2 elements, return the head (1st element).
So the function `penultimate` is going to match `ls` against 3 different cases: def penultimate[A](ls: List[A]): A = ls match { case h :: _ :: Nil =&gt; h case _ :: tail =&gt; penultimate(tail) case _ =&gt; throw new NoSuchElementException } The first case it can match against is if you have a list of exactly 2 elements -- `h`, something (`_` is wildcard), and then a `Nil` to denote the end of the list. If the list is of length 2, then you just get whatever's in the first spot. The second case is looking for any value (`_`) and then everything else (`tail`). Because you already skipped over the length-2 case, this case can be handled. It recursively calls penultimate on the tail, which will in turn remove 1 more element from the front of the list. The last case is a default and will just throw a NSEE. Only reason this should happen is when you have &lt;=1 elements in the starting list.
When penultimate gets called with a list it is `pattern matched` to the followings: - `h :: _ :: Nil` - `_ :: tail` - `_` The matching here is simple, :: creates a list thus these match on the following lists that are respectively: - a list containing: an element which will be accessible by the 'variable' `h`; an element which won't be set as any variable; and nothing else ie. `List(1, 2)` - a list containing: one unnamed element, and the rest of the list ie. `List(1, 2, 3, 4)` - everything else ie. `List()` To sum it up an identifier is matched as an element and created as a variable, with the special case being the identifier being the last element in a pattern as then all remaining values will be stored, an underscore is matched as an element but its value is not stored, again a special case here if it's alone in a match it maches everything. Nil is always used as an end of a list being an empty list, that can always match at the end. The matched values then will be usable in the codeblock following the `=&gt;`, which I assume are not the reasons for asking.
Thanks. what should I read to understand List match loop ?
&gt;https://github.com/dotterweide/dotterweide looks interesting. 
I'd agree with the general sentiments here, there's no need to go chasing after the new shiny just because it's a fad. Having said that, as a web developer myself, I like the separation of concerns that a SPA brings (server-side handles data, client-side handles view) for a number of reasons, but most of those are particular to client-facing applications. If you and anybody else who maintains this application are of a like mind, then I'd say do what makes the most sense to the ones who ultimately are responsible for it.
There are lots of Scala programmers in Brazil. There's a slack channel. I'll post the link here once I get to work.
It's basically few things combined: list match { case head :: tail =&gt; // non-empty list case Nil // empty list } (1, 2) match { case (a, _) =&gt; println(a) // _ is used to not bind (ignore) certain arguments to named values } (1, (2, 3)) match { case (a, (b, c)) =&gt; // you can match nested structures } 1 :: 2 :: Nil == 1 :: (2 :: Nil)
No São Paulo, né?
I'm re-posting this from the last quick questions thread as it had already been unstickied and fallen off at the time (apologies if this is rude, and big thanks to /u/md50 for commenting anyway) Another traits and types question, I don't understand why trait W[A,B] { def f(a: A) : B def f(a: Double) : Double = 0.0 } class WD extends W[Double,Double] {} doesn't compile, with the message: Error:(254, 7) class WD needs to be abstract, since method f in trait W of type (a: Double)Double is not defined (Note that A does not match Double) class WD extends W[Double,Double] {} If I add an "override def f(a: Double):Double = 0.0" in WD, it compiles. My first thought is that this should have been fine because there's already a (a:Double)Double method in W. My second thought is that wait a minute, are there now two methods with the same name and signature? And some experimentations suggest yes: class ZD extends W[Double, Double] { def f(a: Double) : Double = super.f(a) } gives the error : Error:(263, 46) ambiguous reference to overloaded definition, both method f in trait W of type (a: Double)Double and method f in trait W of type (a: Double)Double match argument types (Double) and expected result type Double override def f(a: Double) : Double = super.f(a) Anyway my question is basically where did I go wrong in expecting the original version to work? (I ended up solving the problem with an intermediate trait "trait W0[B] extends W[Double, B] { ... }" that contained the specialization and I'll just inherit from W0[B] whenever I would have inherited from W[Double,B], but that feels a little inelegant)
I worked with a guy who wrote scala in vim. I don’t know how he managed but he did. 
Isn't JavaCV just a java-wrapped OpenCV? (I don't do CV on the JVM, but I regularly use OpenCV from the c++ (and sometimes python) sides.) You're not suggesting implementing something from scratch, are you? That would be an enormous amount of work, and if you're really unsatisfied with JavaCV, an easier target might be to just write a scala interface to (basic) OpenCV functionality.
&gt;Isn't JavaCV just a java-wrapped OpenCV? Yes it is, and I feel that is the origin of the problems I had. &gt;an easier target might be to just write a scala interface to (basic) OpenCV functionality. That is another option I am considering.
https://www.scala-lang.org/community/ has links to some non-English chatrooms. (And if anything is missing from that page, please visit https://github.com/scala/scala-lang/blob/master/community/index.md and submit a pull request!)
We have Polish Scala Slack: https://scala-poland.slack.com There is also Polis WIP version of fpmortals: https://leanpub.com/fpmortals-pl I also know that many talks from Scala.io conference are in French: https://scala.io/ You could probably find some talks from Polish Scala meetups in Polish on youtube (there is plenty of such meetups), but these are not very common, people generally try to do that in English to increase the audience.
around 2:30, Bill mentions that a fourth edition of "Programming in Scala" (Odersky/Venners/Spoon) is already in the works and will cover Scala 2.13 👍
A long time ago in a galaxy far, far away....I worked on a Scala project and needed to use image processing libraries. I recall using both Open CV as well as Boof CV. I made this [1] project. I haven't touched in in the last 2-3 years :-/. It's set up as a core library and two sub-libs, one for each of the native computer vision libraries that it binds to. The core lib has some type classes, common functions, etc. There's not a lot there / it might not be very useful to you. I hope it helps in any fashion though! :D [1] https://github.com/malcolmgreaves/scv
**Protenus | Associate Software Engineer | Baltimore, MD, USA | ONSITE | Full Time** Protenus is a leading healthcare company focused on protecting patient privacy and detecting abuses within health systems. Its team of data engineers and data scientists use Scala for high-volume data processing and analytics. We are looking for an early-career software engineer who wants to join our team. Click here to learn more: [https://hire.withgoogle.com/public/jobs/protenuscom/view/P\_AAAAAAFAAE8BpW\_gJ6AkcV?trackingTag=reddit](https://hire.withgoogle.com/public/jobs/protenuscom/view/P_AAAAAAFAAE8BpW_gJ6AkcV?trackingTag=reddit)
M-x sbt-start &gt; console Will get you a repl.
What were the problems you had? Personally I'd steer clear of an underdeveloped Scala library and go to the Java version for my own work if I had to choose. 
users actually started to downvote my post. very welcoming community
thanks, i really wanted to know about how match works. :) 
1. instead of `addCompilerPlugin` you can update the `scalacOptions.in(Compile)` setting to point to a `package` jar. Example: https://github.com/scalamacros/paradise/blob/1d55f7b0902b35d55d6d1dfc94ae8680e622c4da/build.sbt#L76-L88 This works because `scalacOptions` is a task (not setting) so it's recomputed on every `compile`. 2. you can try `symbol.info.annotations` instead of `symbol.annotations` You might be interested in implementing this feature as a Scalafix rewrite rule that automatically inserts the named arguments instead of complaining about it. The Scalafix developer guide has a tutorial on how to implement such a rule https://scalacenter.github.io/scalafix/docs/developers/tutorial.html
*Woah!* It's your **7th Cakeday** johnynek! ^(hug)
Replace get { with post { 
And what path do you want this route to match?
oh ok. Now when I send a post message (using Postman) to localhost:9000 I get a response that i put in the Complete(".."). &amp;#x200B; Now I want to print this POST message in console. What I need to add?
to "hello" I guess :) I edited my question aswell.
please just read the tutorials and documentation and sample projects. I can't sit here and translate it all for you, line by line.
Sure :) &amp;#x200B; This is actually my final step, could you just refer me? the akka tutorial is huge.
You can remove those by going into language preferences for Scala. &amp;#x200B;
IntelliJ. &amp;#x200B; Weird thing is that Ensime+Emacs for Scala is so powerful that you will be left wondering - "why did these people put so much effort into building this ?" and then you will see the answer - "this is more powerful than any IDE I have ever seen, probably they felt restricted by the limitations of all the IDE's and hence went with Ensime+Emacs" &amp;#x200B; This is exactly where things went wrong with Scala IDE things. Yes, those super powerful and fancy features are cool and will help save a lot of time. But what about the community (specially enterprise devs) ? How many of them even use Emacs ? The problem is that many devs can not afford to not use something like IntelliJ, because they also want the same editor/IDE to have 1. a powerful decompiler 2. debugger 3. git-integration 4. support for test-suites 5. support for XML 6. support for HTML 7. support for JavaScript 8. &amp;#x200B; I hope they don't repeat this mistake with choosing VsCode for Scala 3 and make VsCode a cosmic level battleship but just for Scala.
If you are choosing ensime then choose Emacs. Still IntelliJ gives you a lot more than just Scala. &amp;#x200B;
 type Steps = List[Step[Service[_,_]]] you're throwing away type information; if instead you define like so: type Steps[A, B] = List[Step[Service[A, B]]] def combine[A, B](first: Steps[A, B], ...) = first.foldLeft(...) that may not be satisfying, having to specify type params for `Steps` everywhere, but the compiler can only infer `Any` when you chuck away type information right off the bat with existings `type Steps` definition.
I’ve been tracking that concept as well, I think it makes a lot of damn sense 
https://doc.akka.io/docs/akka-http/current/routing-dsl/directives/marshalling-directives/entity.html That page might help. 
I'm not sure I understand the initial point, but yeah, you're right.
Thank you. The `scalacOptions` works really well. The `symbol.info.annotations` doesn't seem to work. I wonder if you know why/how `StaticAnnotation` is treated specially here. &gt; You might be interested in implementing this feature as a Scalafix rewrite rule that automatically inserts the named arguments instead of complaining about it. I'm interested. My plugin seems too narrow to stand on its own. It's probably better if it is a feature. But we would like to only enforce this rule on some methods. We also want programmers to fix it themselves because these methods are likely to be important. I'm not sure if these requirements fit well with scalafix's direction.
&gt; But we would like to only enforce this rule on some methods. You can do that with Scalafix as well, based on annotations or user configuration for 3rd party methods https://scalacenter.github.io/scalafix/docs/developers/tutorial.html#use-withconfiguration-to-make-a-rule-configurable
Thank you. I'll open an issue on the project to discuss more on how the api should look like.
I have even harder task - to find fully remote senior scala job :-)
For internal web application, i'd start with list of browsers I have to support. Many companies still use old computers with limited amount of RAM (even not enough to run SPA) and old IE versions which can heavily influence technology choice. Apart from that, just take what's faster for you to develop. I'd take rest api + typescript/react or angular. 
&gt; My first thought is that this should have been fine because there's already a (a:Double)Double method in W The two `f` methods are different overloads, one abstract, one not. The abstract one must be overridden. &gt; My second thought is that wait a minute, are there now two methods with the same name and signature? You are on the right track here. They do have the same name but different signatures. Because of erasure, the first overload is basically the same as `def f(a: AnyRef): AnyRef` . You can see this if you change `Double` to `AnyRef` - you'll get an error about identical signatures after erasure. Given that there are two overloads, the compiler selects that overload based on what it knows the generic parameters are. Auto (un)boxing can make that selection ambiguous, which is why you cannot call the method through the type `W`. Honestly what surprises me more is that you can create `WD` with that overload at all. I'd expect an ambiguity error there since it could conceivably override both. But maybe the compiler allows it because it realizes you could not call `W.f` due to the ambiguity and sort of overrides both, or perhaps because it assumes inside `WD` that `Double` refers to the matching generic type parameters and so you're overriding the abstract method. &gt; I ended up solving the problem. . .but that feels a little inelegant If you post your full solution and your desired use case we may be able to help figure out something better. It's a little hard to piece together from these snippets.
We're hiring at Abacus ([https://angel.co/abacusfi/jobs/](https://angel.co/abacusfi/jobs/)) -- my email is [ian@abacusfi.com](mailto:ian@abacusfi.com). We're based in SF.
Circe and generic derivation can get you started pretty quickly (just watch out for the compile times!) Still, I know what you mean python's `json.loads(data) and `json.dumps(stuff)` are really neat. I guess you'd have to use JavaScript to get anything similar.
This blog post makes so many wrong assumptions. &gt; The quick FP thinker will implement it with something like `Future.traverse(as)(f).flatMap(_ reduce g)`. I'd argue the quick FP thinker will not use the broken `Future` at all. &gt; If we start defining abstractions over semilattices (or maybe there are ones in cats or scalaz libraries? I really don’t know), The code will become one giant pile of gibberish... They exist in [typelevel/algebra](https://typelevel.org/algebra/typeclasses/overview.html). And honestly, `Semilattices` are really easy to comprehend :) &gt; In the simple FP approach (traverse + reduce). same execution will result with the following computation order... Which means we don’t take advantage of the early completed computations. That simple FPish approach using the `Future` might be just that, too simple. Not necessarily right for the problem you're trying to solve. If using a sane abstraction like monix `Task` or cats-effect `IO` you could accomplish similar but principled behavior using `parTraverse`. &gt; The interface is functional, it is referential transparent (as much as Future is considered referential transparent). It is not. `Future` is broken and it is not referentially transparent. &gt; Perhaps the above is achievable using pure functional style (I would love to know how. really!) It definitely is. When in doubt you can always ask in the Gitter channels, people will help. For example in the [Cats Effect](https://gitter.im/typelevel/cats-effect) one.
Basically is Java is a good environment to use, then Scala will also be good. Many companies use Java for monoliths and microservices. Regarding memory, Spring microservices typically have a large memory footprint anyway.
https://github.com/typelevel/cats/blob/master/core/src/main/scala/cats/CoflatMap.scala basically takes F[A] and F[A] =&gt; B, returns F[B]. I believe this is the flatmap equivalent for comonads
I'm also really looking forward to this! I've been doing scala for a side project for a few months now but beyond my own attempt to learn and optimize what I'm doing, it would be great to be dropped little tidbits about the language and tooling available! (Also, I'm so glad it's available on Google Play, seems like everyone forgot that was a service but I still love it!)
Rather strange way to say "thank you" considering you got a very good answer.
I was wondering wouldn't that be wrong to return a \`NoSuchElementException\` while you say you are returning \[A\] I mean functional programming concepts. 
I was thinking the same. This post as a lot of wrong assumptions, not only about the language and about FP (as you show in the excerpts you took where he assumes Futures are referentially transparent, and where he assumes an FP style solution that no Functional programmer I know would dream of), but also about the Scala community. OP doesn’t seem to be aware that there’s a full FP community uns Scala using cats, cats.effect,etc and using free monad or final tagless style on their day to day jobs. To me, the community being effectively split into a “Scala as better Java” and a “Scala as a poor man’s Haskell” side is the real problem, because these two communities have different views of what they want from the language, and what they would like to see removed. This is also a problem both when hiring or when looking for a job.
How so? I could see you returning a `None` and then somehow flatmapping the recursive elements to be a `Some(h)` instead?
Hi, i am working privately on a project for creating good looking automatic reports (imagine charts and tables...) &amp;#x200B; I am not sure if i make it opensource but will see, but you should be definitely able to deploy it yourself &amp;#x200B; What motivates me is the biggest competitor jasper reports, i was working with jasper in 2 different companies and while its not all bad it still looks the same as \~10 years ago. &amp;#x200B; I think there is a lot of other similar projects (some do look very good) but they often miss some key features jasper has.
&gt; what they want from the language, and what they would like to see removed My impression is, the only community that "wants something removed" is the latter. I am yet to see an OOP-in-Scala acolyte request functional stuff to be abolished but there seems to be quite a lot of noise from the opposite group.
Interesting, I gave the opposite impression. For example FP programmers tend to use implicits to create type class behaviour, but OO developers tend to see it as dangerous (and in an OO context I would agree it is) and often say they should be removed from the language (I wouldn’t mind it myself, if they were replaced with proper type classes).
Scalaz and the surrounding ecosystem are rather top notch. Argonaut is quite possibly the best JSON library in existence. Http4s is rather nice. 
Why choose Scalaz over cats ?
The Cats project seems like it's made a more active effort to provide good documentation. Where as the ScalaZ project assumes (possibly rightfully!) that strong and appropriate types should serve just as well as good documentation. That being said, ScalaZ is probably a bit more expressive, as it's had a longer time to accrue features, and closer to Haskell, if you know what you're looking for. Regardless, functional programming is awesome, and either are really good! 
The divide in the Scala community is often described as FP versus OOP, but I don't think it's true. I think the divide is really along the lines of those who are really into types and lifting as much information as possible into the type signature as possible, versus those who don't. While the demographics of these two groups certainly lean towards FP or OOP, I think it's more the philosophical divide to what are the role of types in the language that cause the most angst. You can even see this divide some what within the FP community over what lengths are appropriate to put things into types. The FP community in Scala I don't really minds using object-oriented type features when they provide guarantees about the correctness of a program, shapeless is a great example of that. Even though I will admit it seems that if there is a problem with an object-oriented feature, the first instinct with some is to abandon it, and solve the problem like how Haskell does it. At the same time, many in the "Scala as a better Java" crowd won't use many of Scala's more advanced object oriented features like path dependent types, refinement abstract type members, or even structural types. I frankly don't quite understand the underlying philosophy this half of the Scala community, so I can't pretend to know what motivates them into thinking a language feature is good or not.
&gt; You are on the right track here Thanks. I think I'm still stuck on a c++ mindset, which I know is incorrect, where I've got the model that these things are just templates and nothing 'really' happens until you try to shove a type in there. The general thing I'm trying to do is write functors (in the math sense) operating on functions f:A=&gt;B, where A and B are numeric types such as Double or Complex (or someday more complicated spaces). And actually as I'm writing this paragraph, I'm realizing that maybe I designed myself into a corner, and I should back up and use what I've learned so far to try something else....
&gt; these things are just templates Generics are related to templates but compared to C++ there is no such thing as "template instantiation" nor are they monomorphized. For instance, `std::vector&lt;T&gt;` creates totally specialized code for `T = int` vs `T = LargeStruct`. In contrast `Vector[T]` has exactly one implementation for all `T`, due to type erasure. As such, generics are purely a compile time type-checking feature; at run time the same code executes in all cases and casts are inserted when needed. This has impact when implementing generic functions in Scala (or Java for that matter). You're essentially operating with a completely opaque and you have to add constraints in order to do anything with them. You cannot assume anything about the type like you could in C++, relying on an error during instantiation to tell you of incompatibilities. As an example, in C++ you could assume default constructable types and write `T t;`, and you can assume that it has some methods available call `t.method()`; all of that is illegal in Scala. You cannot construct a generic type directly and you can only call methods if you explicitly include a bound, e.g. `T &lt;: Trait`, after which you can call methods only of that bound (because that's the only thing you know for sure about the type from within the function). &gt; The general thing I'm trying to do is write functors (in the math sense) operating on functions f:A=&gt;B, where A and B are numeric types such as Double or Complex (or someday more complicated spaces) Have you checked out the [spire](https://github.com/non/spire) library?
Cats started as a fork of Scalaz, so I'd just lump it under "surrounding ecosystem". They do roughly the same thing. 
I'd check out [https://github.com/typelevel/cats-effect/tree/master/benchmarks/shared/src/main/scala/cats/effect/benchmarks](https://github.com/typelevel/cats-effect/tree/master/benchmarks/shared/src/main/scala/cats/effect/benchmarks) &amp;#x200B;
&gt; Why choose Scalaz over cats ? - If you like a more clear lineage to Haskell - If you like your batteries included - If you like the Scalaz community
Yes, tuples are immutable. See also the [doc](https://www.scala-lang.org/api/current/scala/Tuple2.html). Even if there was a sneaky way to change that, you shouldn't do it. Immutability is a core feature of Scala and learning to use it to your advantage is part of learning Scala. 
There are a few questions one could have about your example. For example, why use a `Set` and not a `Map`, since you are indexing by the first element of the tuple? If you had a `Map`, then you could easily iteratively update the `Map` using a `fold`. You could also use a mutable `Map` (Scala has those as well) and update values in that map. Within a single function, mutability is considered acceptable by many in the Scala community.
Not available on TuneIn as of this morning.
&gt; If you like your batteries included What is missing in cats ?
&gt; What is missing in cats ? I believe cats puts into modules/separate projects what is otherwise "in Scalaz".
I've just contacted them, let you know once have updates, thank you :) 
No, thank you!
Cats is quite good as well - It started as a fork of Scalaz after all. They do roughly the same thing with different opinions on how to do it. I suppose it felt better to give a shout out to the library that paved the way rather than the one that copied it. (Ditto for Argonaut vs Circe). 
https://github.com/scopt/scopt It's a library to make it easier to validate and parse command line input. Such as out of order arguments: git merge x --no-ff Or expanded arguments git checkout -t --branch etc etc. There's a whole world of identical libraries for each language: * https://perldoc.perl.org/Getopt/Long.html * https://docs.python.org/2/library/getopt.html * https://en.wikipedia.org/wiki/Getopt The Scala one is especially ugly. However if you want to rewrite a clean, immutable one you eventually converge on exactly what they've done, so whilst it looks bad, by the time you rewrote it, you have nearly the same thing!
Link the to home page [https://scala.love/](https://scala.love/)
I can help! I specialise in Scala recruitment for companies in CA. Is there a good way to get in touch with you?
Reddits abysmal formatting rules requires you to indent code with 4 spaces to be rendered correctly. I would recommend asking on the cats gitter, the response time is fantastic and they're all geniuses
Hey by the way formatting unfortunately doesn't work like that on Reddit, you have to prefix each code line with 4 spaces: import cats.effect._ import cats.effect.concurrent.Ref import cats.implicits._ import org.slf4j.{Logger, LoggerFactory} import retry.{RetryDetails, RetryPolicy} import retry.CatsEffect._ import retry.RetryDetails.{GivingUp, WillDelayAndRetry} import scala.concurrent.duration.FiniteDuration class ResourcePool[T](createOrRefresh: () =&gt; Resource[IO, T])( poolSize: Int, retryPolicy: RetryPolicy[IO])(implicit timer: Timer[IO]) { import ResourcePool._ private val resources: Array[IO[Ref[IO, Resource[IO, T]]]] = Array.fill(poolSize) { Ref.of[IO, Resource[IO, T]](createOrRefresh()) } private val currentRes: IO[Ref[IO, Int]] = Ref.of[IO, Int](0) private def latestRef(indexRef: Ref[IO, Int]): IO[Int] = for { value &lt;- indexRef.get _ &lt;- if (value == poolSize - 1) indexRef.update(_ =&gt; 0) else indexRef.update(_ + 1) } yield value private def failureHandler[A]( ref: Ref[IO, Resource[IO, T]])(in: Either[Throwable, A], details: RetryDetails): IO[Unit] = details match { case WillDelayAndRetry(, _, _) =&gt; ref.update( =&gt; createOrRefresh()) case GivingUp(totalRetries: Int, totalDelay: FiniteDuration) =&gt; IO.pure(in.left.foreach(err =&gt; logger.error("Resource pool failed with error", err))) *&gt; IO .pure { logger.error( s"Failed to access the resource after $totalRetries attempts and $totalDelay duration.") } } private def retryWithPolicy[X]( ref: Ref[IO, Resource[IO, T]]): IO[Either[Throwable, X]] =&gt; IO[Either[Throwable, X]] = retry.retryingM[Either[Throwable, X]](retryPolicy, .isRight, failureHandler[X](ref))() def use[B](f: T =&gt; IO[B]): IO[Either[Throwable, B]] = for { currentRef &lt;- currentRes.flatMap(latestRef) ref &lt;- resources(currentRef) res &lt;- ref.get op &lt;- retryWithPolicy(ref)(res.use(f).attempt) } yield op } object ResourcePool { val logger: Logger = LoggerFactory.getLogger(this.getClass) }
&gt; The Scala one is especially ugly Which one is ugly? I actually think their new functional DSL looks quite nice
Umm... I think it's formatted properly now? The UI shows me the option to use Markdown, and it appears just as the code-block appears for me. But it seems like others see a it differently. &amp;#x200B; Anyway, I've updated the post to use 4 spaces prefix for each line now.
Newbie question about syntactic sugar. Why, if I declare a single-parameter function inside an *object*, e.g. object Obj { def inc(x: Int) = x + 1 ... can't I use it as val x = inc 1 But if I put it inside a subclass (sub-object?), e.g. object Obj { object Math { def inc(x: Int) = x + 1 } ... I can do val x = Math inc 1 ?
Is that `Resource` actually getting closed? Specifically in that `GivingUp` case. Just throwing out ideas, not super familiar with this `Retry` library.
What do you pass as createOrRefresh? What does jvisualvm say the memory is getting spent on?
Hi all. I'm getting yet another approach to learn Scala. Any recommended tutorials, pages you'd think about? I'm looking for more practical approach. Thanks 
Old vs new reddit. Old reddit ([old.reddit.com](https://old.reddit.com)) only supports the four spaces formatting, whereas new reddit supports both that and backticks.
That's because the desugaring rules in Scala are such that `a b c` gets translated to `a.b(c)`. In your first example, `inc 1` does not follow that pattern -- there's no "receiver" of the method in the pattern. But `Math inc 1` does, and so can be translated to `Map.inc(1)`. You also don't need to wrap in an object just to get this syntax, you could also just do `this inc 1`
This free beginner level [course](https://stepik.org/course/16243) in Russian just started.
The \`Par\` function itself will be created whenever \`a\` is referenced in the body of \`fork\`. That returned \`Par\` will need called later, passing an \`ExecutorService\` at that time.
My guess is that it's because of your IO(Refs). Those get re-evaluated every time you dereference them in a flatMap, and because of closure, live until your program exits if you do it the correct way and push unsafeRunSync to the edge of your program. &amp;#x200B; private val currentRes: IO\[Ref\[IO, Int\]\] = Ref.of\[IO, Int\](0) &amp;#x200B; You are constantly creating new Refs whenever use is called, for one thing. Remember that line is semantically equivalent to: &amp;#x200B; IO(() =&gt; Ref(IO(() =&gt;0) &amp;#x200B; So when you reference it in the for comprehension it becomes a flatMap call: &amp;#x200B; IO(() =&gt; Ref(IO(() =&gt;0).flatMap{ currentRef =&gt; ...} &amp;#x200B; Those refs basically live a really long time (especially if you are only calling unsafeRunSync once in your program. &amp;#x200B; Similarly, your resources call initializes a ref every time you access a resource by index. You obviously don't want to be doing that. &amp;#x200B; So, solutions: &amp;#x200B; 1. Don't store an IO ref as a value. Just make use take a Ref\[IO, Int\]. 2. Call sequence after you fill your array of io refs : Array.fill(poolSize) { Ref.of\[IO, Resource\[IO, T\](createOrRefresh()) }.toList.sequence 1. This will return not an Array\[IO\[Ref\[IO, A\]\]\] but a IO\[List\[Ref\[IO, A\]\]\], meaning once you've filled your array you are re-using your refs. 2. You probably do NOT want this to be a class method, you probably want to pass a List\[Ref\[IO, A\]\] into use, and put the pool initialization on the companion object. Then, you can initialize your pool in your caller, and initialize/ get a ref from it, and pass it into use: &amp;#x200B; val x = new ResourcePool(...) for{ pool &lt;- ResourcePool.createResources(poolSize) initialResource &lt;- IO.fromOption(poolRefs.headOption) _ &lt;- x.use(initialResource, pool) } yield () // or whatever &amp;#x200B; Then you won't be putting things in new references all over the place that live for the life of your program due to being in a run loop. &amp;#x200B; However, I question your need to be able to do this. You are aware of Ref, but are you aware of fs2's Queue? You can give it a bounded size and it will do exactly what you are trying to do with your resource pool. Additionally, you can use IO.bracket for resource initialization and cleanup, along with fs2.Stream.retry to handle your retry policies. I highly recommend reusing the existing libraries rather than writing your own. There's also [https://typelevel.org/cats-effect/datatypes/resource.html](https://typelevel.org/cats-effect/datatypes/resource.html) for doing that, when you can use fs2.Stream.retry or this: [https://typelevel.org/cats-effect/datatypes/io.html#example-retrying-with-exponential-backoff](https://typelevel.org/cats-effect/datatypes/io.html#example-retrying-with-exponential-backoff) &amp;#x200B; Anyway, I'd really recommend using fs2.Queue. It's really great for this sort of thing. Again, initialize your queue in your main, then pass the Queue\[IO, A\] around, not the IO\[Queue\[IO, A\]\]. Otherwise you'll end up with infinite queue's eating your memory, too. &amp;#x200B; Anyway, that's all I could glean from your code. Hope this helped.
Why don't we use strictness evaluation here?
Sorry, I don't understand the question. Maybe you can show what you mean with code?
Why don't we use this signature `def fork[A](a: Par[A]): Par[A]` . I think `a: =&gt; Par[A]` and `a: Par[A]` make no difference in this case?
That will depend on the implementation of the method. The main difference is that with the strict form you will always have just that single instance everywhere it's referenced in the implementation, whereas with the lazy version you will get a new function every time the parameter is referenced. Importantly, that instance must be created with the strict version, but with the lazy version it's never created unless it's used, which can be important if it's particularly expensive to create one, and can help reduce overall memory use too.
It should be available on TuneIn within next 24 hours! :)
Having two DSLs in the first place doesn’t really make the documentation nice.
Since I'm learning fs2 these days. I thought this will be a fun exercise to do in fs2. Here is my code using streams. package example import fs2._ import cats._, cats.effect._, cats.implicits._ object Pascal extends IOApp { def run(args: List[String]) = { val size = args(0).toInt.ensuring(_ &gt; 0) Stream.iterate(Stream(1))( Stream(1) ++ _.zipWithNext.collect{case (x, y) if y.isDefined =&gt; x + y.get} ++ Stream(1) ).take(size).map(_.toList).evalTap(e =&gt; Sync[IO].delay(println(e))).compile.drain.map(_ =&gt; ExitCode.Success) } } 
depends on task. You don't need to validate everything. 
And interview being that task, makes validations important.
Nice :) Here's a slightly modified version: ``` Stream.iterate(Stream(1L))( Stream(1L) ++ _.zipWithNext.collect{case (x, Some(y)) =&gt; x + y} ++ Stream(1L) ).take(size) .map(_.toList) .evalTap(e =&gt; IO(println(e))) .compile.drain.as(ExitCode.Success) ```
Interesting, I’m used to ask this problem at interviews. The first thing I would ask you is to compute values over 68, that would need you to rethink all the code. Second to have a solution that is the most efficient as possible in term of computation and memory. If you want to show off I think this is not a good way to do it.
As indicated by the post, the solution I provided was more about being concise, not about being fast or efficient. I find it facinating how much you can do with so few LOC in Scala. Feel free however, to show off your optimized solution here! I am more than happy to learn something new :)
You're right! One should probably also at least mention, that writing some tests for it would make sense ;) Also, figuring out the solution brekas after 68 is kinda hard, if you get this task as a whiteboard coding challenge.
&gt; able to calculate way beyond 68 rows Scala "Int" is only 32 bits, how are you handling overflow? 
I was trying to say that the „algorithm“ isn‘t per se limited to 68 rows. Usign a larger datatype will give you additional rows. I think it‘s rather clear that the largest number in the triangle has to fit the numeric type for it to work...
I was trying to say that the „algorithm“ isn‘t per se limited to 68 rows. Usign a larger datatype will give you additional rows. I think it‘s rather clear that the largest number in the triangle has to fit the numeric type for it to work...
It's not that people change their IO type and more that libraries need to work with more than one IO type.
I think the benefit is about abstracting over the capabilities you need from F[_]. It is easier to understand what is happening, most of the times you realize you don t need IO in most of your functions.
You can print any arbitrary row using the binomial theorem. To print some of the bigger rows you need to do big integer arithmetic so whether this is better or worse is probably subjective, but it does mean that you can get any given row in linear rather than quadratic time.
* because they can * because you can e.g. use Task/IO/Zio on production and something like Coeval in tests (the former handles concurrency, the later is easy to test) * because library writers cannot dictate you your use case (eager evaluation - Future, lazy evaluation - various IO types, tests are easier with something synchronous, etc) * because it is easier to stick to principle of the least power if you have a constraint in the form of a type class, while with using IO directly you will commit to some implementation - in some places it is good idea, in some isn't
Principle of least power.
There are a few reasons for that. Reason 1 would be polymorphism. If you abstract over your effect type, then you can easily replace the implementation by another one (as someone mentioned ZIO, Monix task, etc, all have instances for the cats.effect type classes, so they can easily replace IO) Reason 2, abstracting over the effect type allows you to limit what you know about the effect. This means that you can’t perform the unsafe operations of the effect except “at the end of the world”, since nowhere else you have enough knowledge about your type. M Reason 3, well, this one is not exactly a reason, just a consequence of polymorphism really. You can replace your F during tests. Whoever told you that you should use IO in your tests needs to do their homework. Your tests exist to test your code, not the library code. The library has its own tests. You don’t need to test using IO, and often it makes the test more readable to use Id. Of course, this applies to unit level testing, on other types of tests you might want to check that the whole wiring does what it is expected, in which case you probably want the same effect as your application.
Different responses already hint at this in different ways but for me the most compelling argument for introducing parametric polymorphism is the dramatic impact it has on reasoning. Consider how many ways you can implement a function `Int =&gt; Int` vs how many ways you can implement `A =&gt; A` and have them typecheck. The impact is no less significant for higher kinded types.
[removed]
It's a little different than what you're aiming for, but you might consider Jupyter with a scala kernel (like almond). The nice thing there is that you can write nicely formatted markdown text in between your interpreted scala sections, which I find helpful for learning anyway. &amp;#x200B;
Hi all! I've started to learn Scala and somethings are really easy and somethings straight up confuse me... Could you help me with map of strategies for numeric operations? Such things are really easy in dynamically typed languages e.g. in Python: op_to_strategy = { '+': lambda x, y: y + x, '*': lambda x, y: y * x } op_to_strategy['+'](1, 2) # 3 op_to_strategy['*'](4, 5) # 20 I wanted to do this in Scala, but I can't find a way with function literal... val opToStrategy = Map( //"+" -&gt; (_:Numeric) + (_:Numeric) // &lt;- won't compile "type Numeric takes type parameters" ) I don't want to use e.g. `Double`, but something more generic like `Numeric` (I think). Can you help me? Also when I've checked `Numeric`source I found this `type/val` pattern. type Numeric[T] = scala.math.Numeric[T] val Numeric = scala.math.Numeric Why is it useful?
You could do something like this def operation[A](op: Char)(a: A, b: A)(implicit N: Numeric[A]) = op match { case '+' =&gt; N.plus(a, b) case '-' =&gt; N.minus(a, b) case '*' =&gt; N.times(a, b) case _ =&gt; ??? //Decide how to handle unexpected chars here } println(operation('+')(1, 2)) //3 println(operation('-')(1, 2)) //-1 println(operation('*')(1, 2)) //2 println(operation('+')(1.0, 2)) //3 println(operation('-')(1.1, 2)) //-0.8999 println(operation('*')(1.2, 2)) //2.4 The type/val thing is located in the package object of scala, and thus are available without imports. It's nothing more than aliases for the trait and the object in the \`scala.math.Numeric\` package.
I prefer mxnet over tensorflow, pytorch and all other libraries anytime. 1. Extremely handy and flexible for research due to its imperative nature (like pytorch, even most of the time faster than pytorch), which is so much essential for prototyping and debugging, even for optimization. 2. Insanely faster than any other framework most of the time. Especially when batch size is above 64. 3. Also supports declarative approach (like tensorflow and keras) for light speed execution. 4. It's the only framework that supports data parallelism insanely and easily like no other framework. It's just so so beautiful. 5. And the most important reason why it's the best framework on the planet is that "you can convert your imperative code to declarative" which makes your execution 2x faster. 6. And obviously it has unbeatable aws support So basically the road map is you can debug and prototype in imperative nature which is awesome and very handy, and when you are ready to deploy just convert your code to declarative by hybridising it. The most important challenge for mxnet is tensorflow, which has already captured the market. I used to be a dead tensorflow fan, but since I used mxnet my mind has changed drastically.
Off topic but what does the '~' do in your command `sbt ~test:compile`?
Thanks! I just started reading scala programming guide, but I don't have the intuition yet. For me it looks like Scala should handle this task easily, but then I hit brick wall e.g. `operation('+') _` to apply arguments later...
Could you show where you want to use `operation('+') _`?
This... was very, very helpful! After reading your description I am realising how silly it was of me to write it like this. Thank you very much!
One more question — do you think I can till use the FS2 Queue with retry if I want the resources to be recreated. I am now realising that this is completely wrong, but the initial idea was to have a resource pool of something like an open TCP connection to another service where you might need to close and reconnect when the service may have restarted or moved.
Yes! Refs and queues are mutable in the runtime platform, so if the tcp connection is closed, just open a new one in modify, and put it back on the queue with enque1. You'll probably want a wrapper containing the connection and a connectionid: case class TCPConnection(id:String, connection:ConnectionClassName) Because it's likely that you will want to do some work and shove some packets back onto that connection, and you don't have to pass the connection around to do that, the id and some functions consuming the queue in loops will be better than taking it out of the queue. Then you'll want to consume the stream from the queue forever until you terminate the action you are performing. Check out the concurrency examples for fs2 queue on the microsite. Queue's implementation is also simple enough that you can expand it's architecture to create other data structures.
I've optimized for nothing except making the triangle pretty: ```scala object pascalTriangle { def main(args: Array[String]): Unit = { println("Pascal's Triangle: ") println(makeTriangle(20)) } def pascal(c: Long, r: Long): Long = { if (c == 0 || c == r) 1 else pascal(c - 1, r - 1) + pascal(c, r - 1) } def makeTriangle(rows: Int): String = { (for { row &lt;- 0 to rows col &lt;- 0 to row } yield (col, row)).foldLeft(("", -1)) { case ((str, currentRow), (c, r)) =&gt; val p = pascal(c, r) val v = if (r == currentRow) placeInRow(p, kern = 7) else createNewRow(rows, r, p) (str + v, r) }._1 } def placeInRow(p: Long, kern: Int): String = { val digitLength = p.toString.length " " * Math.max(kern - digitLength, 1) + p } def createNewRow(totalRows: Int, currentRow: Int, p: Long): String = { val offset = 4 "\n" + " " * (offset * totalRows - offset * currentRow) + p } } ``` ![Pascal's Triangle](https://imgur.com/a/sdwbIGq) 
I felt a little dirty after posting this because it occurred to me that the pascal computation was essentially computing in `O(n!)`, and I thought I could do better, since this was taking forever to return at even `rows = 30`. Turned out to be easier than I expected, `scalaz.Memo` to the rescue: val zero = BigInt(0) val pascal: ((BigInt, BigInt)) =&gt; BigInt = Memo.immutableHashMapMemo { case (`zero`, _) =&gt; 1 case (a, b) if a == b =&gt; 1 case (c, r) =&gt; pascal(c - 1, r - 1) + pascal(c, r - 1) } def getNthRow(row: Int): String = { makeTriangle(row, row) } This was virtually a drop-in replacement modulo changing some `Int`s to `BigInt`s. Still a bit magical to me. Even at `rows = 100` returns almost instantly, though now my triangle is effed up, heh.
Among scala's testcases I discovered a way for macros to emit `invokedynamic` instructions, and promptly started writing a library that emulates static `val`s by caching the value of an expression on its first evaluation. Works for implicits too. https://github.com/zygfryd/scala-zygf-cement
That doesn't matter. I just import implicit features and test all ok
I just want to write something like: `val addition = operation('+') _` and use it as a function value later without specifing number types.
Ok so the thing is in Scala it's much more often the case that the type of the number would be exactly statically known, and there isn't currently language support for type-polymorphic function values, which is what you would like to use here. There are some tentative plans / aspirations to support it in 3.0 last time I checked, but not in 2.x. The issue is that once you construct your function value, the type of its arguments is locked in. This is not the case for methods, their type parameter can vary, but if you create a function value out of the method like you're doing (`operation('+') _` will create an instance of `scala.Function2`), then the function is not polymorphic. There are some workaround where you basically disguise an object's method as a function. something like this might be what you want... abstract class Operation { def apply[A](a: A, b: A)(implicit N: Numeric[A]): A } val plus = new Operation { def apply[A](a: A, b: A)(implicit N: Numeric[A]) = N.plus(a, b) } val minus = new Operation { def apply[A](a: A, b: A)(implicit N: Numeric[A]) = N.minus(a, b) } val multiply = new Operation { def apply[A](a: A, b: A)(implicit N: Numeric[A]) = N.times(a, b) } def operation(op: Char): Operation = op match { case '+' =&gt; plus case '-' =&gt; minus case '*' =&gt; multiply case _ =&gt; ??? //Decide how to handle unexpected chars here } println(operation('+')(1, 2)) //3 println(operation('-')(1, 2)) //-1 println(operation('*')(1, 2)) //2 println(operation('+')(1.0, 2)) //3 println(operation('-')(1.1, 2)) //-0.8999 println(operation('*')(1.2, 2)) //2.4 val myOperation = operation('+') val useLaterOnInts = myOperation(1, 2) // 3 val useLaterOnDoubles = myOperation(1d, 2d) // 3.0 If you really want something more dynamic than that, you might want to check out the library Spire, which looks like it supports a dynamic number type, though I've never used it (https://github.com/non/spire/blob/master/GUIDE.md#number)
I was working this week on making small vectors (&lt; 32 elements) more memory-efficient by allocating an array of exactly `size` elements rather than rounding up to size 32. Also, since varargs are immutable in 2.13, one can just reuse the passed-in array from the varargs as the underlying array, when building vectors like `Vector("a", "b", "c", "d")`. This also speeds up `:+` and `+:` for small vectors quite a bit. e.g. `(1 to 32).foldLeft(Vector[Int]())(_ :+ _)` is ~4x as fast as before https://github.com/scala/scala/pull/7743
I took this a few years ago when it was on Coursera and it was pretty good. It does assume you took Odersky's [earlier course](https://courseware.epfl.ch/courses/course-v1:EPFL+progfun1+2018_T1/about) in the series.
Awesome - thanks for the review. Am interested in starting this course but haven't taken Odersky's earlier course. Is it absolutely necessary? (Have a few years Scala experience under my belt)
Scala experience is probably sufficient, since the other course is basically just a class that explains functional programming and some Scala-specific concepts for people who have experience with procedural languages.
By the way if you want to learn more about polymorphic functions you could read about them in cats [here](https://typelevel.org/cats/datatypes/functionk.html#higher-kinds-to-the-rescue)
I finished a clustering sandbox using a hybrid of Akka and Akka Typed. Supports event sourcing, event replay, multi-data centers, auto node discovery with DNS entries, and more: [https://github.com/sniggel/cluster-sandbox](https://github.com/sniggel/cluster-sandbox) &amp;#x200B;
Oh it's a blog post. Silly me for assuming you'd contribute to this community.
Great attitude. If you keep this perspective you'll go far.
Lol...and to think not 6 months ago you were pretending to be an expert...
I just realized this was written by the son of the current Prime Minister of Singapore. Holy shit I've got a new level of respect for him now. 
Just realized this was written by the son of the Prime Minister of Singapore. Holy shit.
I also like the IntelliJ support for Scala. I haven't used it long, but I recall seeing complaints in this sub. Anyone care to elaborate on whats missing?
Intellij is confused by macro/implicit-heavy code, even if you just use a library such as shapeless. Gives a lot of false error complaints. Also in some projects it keeps losing Scala SDK which you basically have to set every time you open the IDE. I haven't figured yet the reason for that and some of my projects do not have this issue.
Ah ok. I’ve only used implicits a little bit here and there, and haven’t used shapeless. 
If I had an akka http application with zero actors, that locks up and refuses to accept new connections under slight load, would that be expected? Because that's what I think I have inherited.
Akka + zero actors sounds impossible. Zero actors = no connections sounds expected. I guess what I'm saying is: I'm truly sorry, but if you have some code maybe people can help.
I just made the the `scala-colog` repository public. It is a Scala port of the Haskell `colog` package, which offers composable loggers. The repo can be found here: https://github.com/alonsodomin/scala-colog
If you look at the first example here on the documentation: https://doc.akka.io/docs/akka-http/current/introduction.html Then imagine that the whole application is just straight up inside the "complete" directive like this: val route = { get { complete { // database calls, no use of ! or ? to dispatch messages to actors. } } } If you scroll down to the second example it shows a function called fetchItem that returns a future called maybeItem. My code isn't written like that. So how is it expected to behave?
The Reactive Programming course from back in the day did not have Akka Typed or Akka Stream so this is fairly new material.
That `bindAndHandle` method will ultimately create at least one actor underneath (older versions used `HttpServiceActor`; the new one uses the akka streams framework which I don't understand but will be doing something similar). The `route` function runs on the akka dispatcher which uses a small number of threads (IIRC 4 by default); code that runs on it is expected to yield rather than block (and it may be further restricted by actor locking). (This doesn't necessarily mean actors per se - one can write good akka-http apps that only use `Future` to perform remote calls / database access). So the short answer to your question is yes, one would expect akka-http code that blocks in the `route` method to run out of akka dispatcher threads under any significant load (e.g. if the code blocks for 1s then once you get above 4 requests/second they will start to queue) and then error (IIRC by default the framework returns an error if your code doesn't handle a request within 30s). You should definitely quote the specific error message when asking for help though, otherwise all of this is speculative.
It is ssssssssslllllllloooooooooooooowwwww
Just to expand on a couple of points: &gt; In contrast Vector[T] has exactly one implementation for all T, due to type erasure. As such, generics are purely a compile time type-checking feature; at run time the same code executes in all cases and casts are inserted when needed. For the record it is possible to use `@specialized`. &gt; You cannot construct a generic type directly and you can only call methods if you explicitly include a bound, e.g. T &lt;: Trait, after which you can call methods only of that bound (because that's the only thing you know for sure about the type from within the function). It's worth mentioning that you can also use the "typeclass pattern" (essentially passing an `implicit methodImplementations: SomeInterface[T]` that contains implementations of some methods for `T`), which is the way of achieving the things you'd do with advanced template specialization in C++. (For the simple cases a bound `T &lt;: Trait` is simpler and more readable).
It seems to easily get confused by higher-kinded types, type members, or combinations of the two. Which things specifically trigger errors change from release to release, but I've never found a version that I could use to work on e.g. [tierney](https://github.com/m50d/tierney) without seeing spurious compilation errors (even in an earlier version that avoided using kind-projector). It's certainly not the case that non-macro code will work correctly (I've got projects with zero macros and zero dependencies that see the issues). (I suspect IntelliJ has some special-case code to handle popular libraries, since I've sometimes seen it error on calls to my own code but handle the exact same construct in ScalaZ or Cats without error)
Can you check if the same code is correctly handled by ScalaIDE? I am not proposing to switch to it, just curious how it behaves in your case. I have found it better in terms of spurious errors but it has a lot of issues of its own, apart from being an abandonware. This IS what I call slow. Also, could somebody elaborate why IntelliJ Scala plugin has to use its own presentation compiler? Isn't it a guaranteed source of discrepancies? There must be some good reason for this.
Yes, correctly handled by ScalaIDE. (There are occasional spurious error underlines but they're shown subtly differently from real ones - the spurious ones don't show up in the Problems tab and have a different icon in the gutter, so they're not a problem). I'm actually still using ScalaIDE for this reason. 
Yeah I apologise. Managed to track down the original error. It was this: Configured registration timeout of 1 second expired, stopping
A big thread pool is a crude solution but it should work (though maximum throughput will not be as high as a properly async solution). `blocking { ... }` around parts that block should also be a big help. The ideal thing is to find the parts that are blocking and (assuming they're blocking on I/O rather than compute) replace them with true async solutions (e.g. akka-http client if the server itself makes http calls, postgresql-async or similar for database access). Profiling your server, or even just running `jstack` while its in the error condition and looking at what it's blocking on, is a good starting point IMO - most code only actually blocks in a couple of places, so better to know what they are and solve them in a targeted way rather than just sticking `Future` on everything.
There are a few on the sidebar that people have found useful. (I can't recommend any personally as I learnt a bit at a time rather than following any particular tutorial).
Care to elaborate?
Try clicking the image for the article that elaborates.
There's really not a lot of meat to the article. Summary: &amp;#x200B; * Await.result blocks an entire thread for as long as it takes for an Asynchronous operation to complete. * There's really no excuse for using it, except for 2 cases that are listed (unit tests and overriding methods you don't control the superclass of). The first is no longer valid as of ScalaTest 3. * It doesn't matter if it's a simple or complex project. Don't use Await.result. &amp;#x200B; I'll add that Await.result causes \*really\* bizarre behavior in Akka clusters. You Await a few times and then the thread the ClusterManger wants to run on is blocked. You miss a heartbeat and the whole world falls apart. And the "wunderkind" that did the Await doesn't understand why his code crashed the cluster. And no, I'm not bitter.
Only the sith deal in absolutes More seriously, there are plenty of use cases where blocking threads is torally fine. High-concurrency akka clusters isn’t one of them, but not everyone is writing high concurrency akka clusters. Even high performance stuff can get by just fine spawning/blocking threads if your concurrency isn’t too high (e.g. a monolithic service with few outgoing RPCs, fast database connection, spending most of its time on CPU work), and the vast bulk of web services with small user bases and moderate traffic numbers need not worry at all.
Theres' a [pull request](https://github.com/rickynils/scalacheck/pull/444) to deprecate shrinking failing tests in [ScalaCheck](https://github.com/rickynils/scalacheck/) when an explicit generator is passed to `forAll`. It makes ScalaCheck a little closer to [QuickCheck](https://github.com/nick8325/quickcheck) in Haskell. I'm not supportive of the motion, but I'm going to try and herd the changes. I understand the frustration with shrinking turned on, but I'd rather see shrinking improved. Nobody is volunteering to fix the latter, myself included, so changing the default behavior seems like the way forward. I've written a [proposal](https://github.com/rickynils/scalacheck/pull/440#issuecomment-464994094) for making the change. The current implementation has existed for a long time. Dropping an `implicit` argument in Scala seems to be tricky. It is going to be difficult to create a good outcome for the library and a smooth upgrade path at the same time. A few weeks ago, I had [proposed a bug-fix release](https://github.com/rickynils/scalacheck/issues/453) of ScalaCheck. It didn't garner much fanfare. If the changes to shrinking are friendly, I guess I'll propose a minor release followed by a major release which would time well with the Scala 2.13 changes.
Saying "just shapeless" is a bit misleading. It's one of the most heavyweight libraries when it comes to implicits, macros and relying on compiler-internals.
Not sure what you meant by this comment. I used "just" to indicate that the macro/implicit-heavy code does not have to be written by yourself, it is sufficient to have it among the dependencies to confuse Idea.
&gt;reactivesystems.eu/2019/0... Yeah, I understand that there are times when Await.result is "totally fine", but why do it in those cases? It seems easy enough to deal with the future directly through map, onComplete, etc.
London has an enviable good FP scene. Congrats its a great meet up and thanks for recording the sessions. 
It's a LOT better than it used to be. I have red very rarely nowadays. I don't have the losing Scala SDK issue. Definitely try the latest version (I even use EAP)
It's a LOT better than it used to be. I have red very rarely nowadays.
I think because it enables them to reuse a lot of their machinery for inspections and refactoring, which was worth it for me even when I used to get red more commonly.
Hey, thanks so much for your feedback! I'm glad you think so, definitely stay posted for more of the videos :)
Could you make the location a bit more prominent in a post like this please? I had to read all the text (location not there), and could only find it in the meetup link. For most readers it’s the first thing they’ll want to know along with the date/time.
This guy *doesn't block.*
Or just discard Future and use [https://github.com/typelevel/cats-effect](https://github.com/typelevel/cats-effect) or [https://github.com/scalaz/scalaz-zio](https://github.com/scalaz/scalaz-zio) or [https://github.com/monix/monix](https://github.com/monix/monix) 
I've been hearing this for years and not noticing much difference. Once I reported a minimal example to JetBrains and it was fixed in the next version, only to be unfixed in the version after that. 
You're going to have to block the main thread at some point, otherwise the JVM will exit. Isn't that the main purpose of Await? 
What version are you on?
&gt; Only the sith deal in absolutes I literally wrote &gt; There is no rule without exceptions. I hope that's sufficient evidence that I'm not a Sith Lord ;) 
Nope, you shouldn't use Await. The JVM will continue chugging along as long as you have actors that are alive and you haven't called ActorSystem.terminate(), as the guardian thread will stay running. &amp;#x200B; If you want to do work after the actor system has terminated, you use registerOnTermination to register a code block to execute after all actors have terminated. &amp;#x200B; One caveat: I've been around long enough to see at least 3 different variants of actor system shutdown. I suspect I've not seen the last, either.
Wasn't specifically referring to actor systems really. Anyway at the very least you will need a way to propogate exceptions from you worker threads to the JVM process, one method is to block the main thread on failure from a worker thread future (or other async construct). Another would be calling sys.exit when a worker thread dies. Anyway, I suppose realistically, most people should be moving towards cats-effect IOApp, or monix TaskApp to the complexities of JVM exit, rather than trying to handle it themselves using Await.result. 
but if there’s not a reason to use it, why would you? It’s not like it’s a huge time saver or anything. 
Scala noob here. The system (among many other things, a Jetty server) I'm working in does a lot of things like: case class Response(foo: Int, bar: Int, baz: Int) // Inside an HTTP request handler val fooDbFuture = dbCall1() val barDbFuture = dbCall2() val bazDbFuture = dbCall3() Response( foo = Await.result(fooDbFuture, timeout), bar = Await.result(barDbFuture, timeout), baz = Await.result(bazDbFuture, timeout) ) According to the post, even doing this is wrong: val futureResponse = for { foo &lt;- fooDbFuture bar &lt;- barDbFuture baz &lt;- bazDbFuture } yield Response(foo, bar, baz) Await.result(futureResponse, timeout) What would be the ideal way?
We've just started using reactive streams at work, and it just feels nice to use on next, onError, onComplete. We definitely have a big learning curve ahead of us, but I'm already enjoying it
Yes, i tried it and realized how I hate not have automatic imports. So I'm back to IntelliJ for now, but it's nice to see alternatives are growing.
 val fooDbFuture = dbCall1() val barDbFuture = dbCall2() val bazDbFuture = dbCall3() val futureResponse = for { foo &lt;- fooDbFuture bar &lt;- barDbFuture baz &lt;- bazDbFuture } yield Response(foo, bar, baz) futureResponse.map(methodTakingResponse) 
This is more of an AND than an OR. The same principles still apply, `Await.result` is just called `unsafeRunSyncʼ. 
Im from Javascript background (8yrs just JS) But I recently started with Scala and GoLang. I can relate my self to Scala due to FP, reminds me of JS. Do you think I can do well if Im not from Java background
Hello, &amp;#x200B; Im doing the manning book and im at chapter3 where I have to write a method that checks if a list is a sublist of another list. &amp;#x200B; This seems to work : def hasSubsequence2\[A\] (sup: List\[A\], sub: List\[A\]): Boolean = (sup, sub) match { case (\_, Nil) =&gt; true case (Nil, \_) =&gt; false case (Cons(x, xs), Cons(y, ys)) =&gt; if (x == y) hasSubsequence2(xs, ys) else hasSubsequence(Cons(x,xs), ys) } &amp;#x200B; def hasSubsequence\[A\](sup: List\[A\], sub: List\[A\]): Boolean = (sup, sub) match { case (\_, Nil) =&gt; false case (Nil, \_) =&gt; false case (sup, sub )) =&gt; hasSubsequence2(sup,sub) } &amp;#x200B; is this a good valid way to solve this or can my code be improved ? &amp;#x200B; &amp;#x200B;
Hey, Of course in future will make sure to mention the location at the start of the post!
Metals provide a far better experience: https://marketplace.visualstudio.com/items?itemName=scalameta.metals#overview
The CFP is open :) [https://www.papercall.io/scalatam2019](https://www.papercall.io/scalatam2019)
So...how is the timeout handled or supported here?
Using akka.after https://nami.me/2015/01/20/scala-futures-with-timeout/ 
Without using Akka
Keeps sbt running and recomiles on every change on the filesystem. I actually run sbt in interactive mode and use this command to be precise: `~;scalafmt;test:scalafmt;scalafmtSbt;test:compile`. This way I reformat everything on and then run compilation every time I save some file in my project. &amp;#x200B;
Ahhhhh okay cool. Out of habit coming from a Maven background originally I've always run SBT in batch mode. I need to get on board with interactive mode. Old habits die hars.
 Hi, I mainly develop in Spark -- never been a big Scala-only project before. I have been going more and more into the Dataset API (caus' types &lt;3). But I find it pretty hard to model everything due to the proliferation of `case class` everywhere in my code. Everytime you need something new (e.g. same information but an attribute is not necessary), I create a class for it. On one hand I could create a `case class` with `Option` attributes but then it means handling all of those options afterwards. Is there a good, clean solution for this ? I know that Clojure intend to implement specs for that purpose for example. TL;DR Is there a way in Scala to avoid proliferation of types when you model a domain precisely ?
I've been annoyed by this a lot too lately, and Rich Hickey's talk a few months ago [Maybe Not](https://www.youtube.com/watch?v=YR5WdGrpoug) really resonated with quite a bit. Nowadays, for domain entities, I lean towards making a big case class where everything is `Option`. This avoids the proliferation of types and is much simpler than using inheritance hierarchies to share the fields that structs have in common (that approach will quickly get complicated with f-bound polymorphism pattern too, if you want to do transformations on objects). Then, in different contexts you can use different subsets of fields, and some places if you are quite certain that the field will be present, you can just `.get`. I know it's not considered good style in Scala but I think a different style is appropriate for "domain entity" code, since the type system does not currently support arbitrary one-off data projections/combinations aka "record". 
Same. Watched Rich Hickey yesterday and I came to realize it was exactly what I was doing (and had a wrong feeling about) f-bound polymorphism : any valuable resource on that ? I'm nowhere near that but using a big `case class` with `Option` everywhere. Wouldn't it be possible to have a maccro (that's the unknown to me) that given field names would return an `Option` of a tuple of those fields ? This seems like going from `Seq[Option]` to an `Option[Seq]` with extra steps.
 &gt;f-bound polymorphism : any valuable resource on that ? This is the go-to explanation of the problem and solutions in Scala: https://tpolecat.github.io/2015/04/29/f-bounds.html &gt; I'm nowhere near that but using a big case class with Option everywhere. Wouldn't it be possible to have a maccro (that's the unknown to me) that given field names would return an Option of a tuple of those fields ? This seems like going from Seq[Option] to an Option[Seq] with extra steps. You don't need a macro for that, you could just use a for comprehension: case class Foo(a: Option[A], b: Option[B], c: Option[C]) def abc(foo: Foo): Option[(A, B, C)] = for { a &lt;- foo.a b &lt;- foo.b c &lt;- foo.c } yield (a, b, c) Or if you like you could use the abstractions given in the pure fp libs like Cats, specifically `Applicative` which will let you do: Applicative[Option].mapN(foo.a, foo.b, foo.c) { (a, b, c) =&gt; ... } https://typelevel.org/cats/typeclasses/applicative.html
Knew about the applicative but was thinking of accessing fields via Strings -- which doesn't make sense at all. Thanks for the info :)
I'm thinking about how to better articulate what sets [React4s](http://www.react4s.org/) apart from other React.js wrappers, apart from the avoidance of implicit conversions and macros - namely (via Attachables) composable lifecycles and (via emit) the lack of need for memoizing callbacks. 
Is Metals the best of the VSCode plugins?
Will the presentations be exclusively in Spanish? There are quite a few companies using Scala in Brazil (including mine) that could do some talks in Portuguese :)
Can LSP even do auto imports I wonder? 
LSP has endpoints to implement IntelliJ-style import management, you can try it out with the TypeScript language server.
https://www.amazon.com/Mastering-Functional-Programming-techniques-programming-ebook/dp/B07DTF8N58
Several over the years, but I had admittedly given up in the last year or so. Will take another look in whatever the latest version is when I get home.
I am 100% doing this the wrong way but want some guidance as I don't know how to proceed. I have made some buttons with javascript and am trying to integrate scala using play. Here is the code. &lt;!DOCTYPE html&gt; &lt;html lang="en"&gt; &lt;input type="text" id="order_display" value="0" /&gt; &lt;div class="order"&gt; &lt;div class="order_keys"&gt; &lt;input type="button" id="order1" onclick="leftClick(this)" value="1"&gt; &lt;input type="button" id="order2" onclick="leftClick(this)" value="5"&gt; &lt;input type="button" id="order3" onclick="leftClick(this)" value="10"&gt; &lt;input type="button" id="order3" onclick="changeSign()" value="changesign"&gt; &lt;button&gt;submit&lt;/button &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;script&gt; var flag = true function leftClick(buttonvalue) { var txt=document.getElementById("order_display").value; txt = Number(txt) buttonvalue = Number(buttonvalue.value) if (txt === 0){ flag = true } if (flag) { txt=txt + buttonvalue; } else { if (txt &lt; buttonvalue) { txt = 0; changeSign() } else { txt = txt - buttonvalue; } } document.getElementById("order_display").value=txt; } function changeSign() { if (flag){ flag = false } else { flag = true } } &lt;/script&gt; &lt;/html&gt; How do I get the value which is in the order_display into a scala function? Is there a better way to do this?
How does this compare to scaladoc? The example is unclear to me.
In order to have a timeout you need some kind of scheduler, whether that's akka, another library, or something you implement yourself. Having a thread enter a blocking sleep for n seconds is the worst possible way to implement a "run this thing after n seconds" scheduler. If you're going to have a blocking thread for each request to provide the timeout for that request, you might as well just write conventional one-thread-per-request code that blocks to access the db - either way you're using the same number of blocking threads, and the code is simpler without the futures.
For the simple cases I tend to use polymorphism (e.g. I might have a type `Group[U] { def users: Seq[U] }` and then my domain type is `Group[User]` while my DTO is `Group[UserId]`). Remember that Scala has HKT so you can do things like `case class UserLike[F[_]](name: F[String], age: F[Int])` and then you can do `type User = UserLike[Id]; type UserUpdate = UserLike[Option]`. For more complex cases, [Shapeless extensible records](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#extensible-records) are kind of a more flexible/dynamic version of `case class`es - just as typesafe but easier to form by adding/removing elements, merging two records etc. Using shapeless higher-rank functions you can do things like forming an update type whose type is "this record, but all the values are options", decode json or whatever to that update type, and then apply an update of that type to a record of that type. You can also convert back and forth between a `case class` and a record-like representation of that case class by using `LabelledGeneric`.
`hasSubsequence(_, Nil)` should be `true` rather than false, shouldn't it? In fact why do you need `hasSubsequence` at all - isn't `hasSubsequence2` doing all of the work? Otherwise looks reasonable enough as far as I can see.
PS: I used [https://textik.com](https://textik.com) for the diagram
Scala is a nice language in many ways, but there are some rough edges that only make sense if you understand the JVM (e.g. needing a `ClassTag` to create an `Array`). If you're willing to put up with those then you should get on fine. You might find Scala.js useful for integrating with JS projects you already have and/or using JS libraries you're familiar with. But it's a bit fiddlier and less well known than JVM Scala, so that might end up outweighing the advantages.
[https://blog.twitter.com/content/dam/blog-twitter/archive/observability\_attwitter95.thumb.1280.1280.png](https://blog.twitter.com/content/dam/blog-twitter/archive/observability_attwitter95.thumb.1280.1280.png)
Nice, I've been looking for something like this for a while now. I don't use my Mac so can't use Monodraw anymore.
It's been gradual but particularly in the last few months. Plus they recently got much improved "ux," e.g. type hints, and showing missing implicit parameter errors inline. I've also begun thinking that it's worth making concessions to it. I recently refactored a library of mine that always threw it off. No fundamental changes, just made the implicit class hierarchy a bit simpler, which has its own benefits of course. Before it was a bit convoluted... Similarly, splitting things up and adding an intermediate explicit type once in a blue moon isn't the end of the world. Nor is breaking things up into smaller files once in a while. A few years ago it was so bad that taking that attitude would be a significant compromise. Now however I would say it's near perfect. So in the few cases it still isn't happy (e.g. certain Slick queries apparently), it either doesn't bother me or is it does I'm happy to simplify the code a bit for it.
If you're an emacs user, `artist-mode` is pretty slick for drawing in ascii!
Never ever build a microservice architecture where services communicate through direct REST-calls. It's basically a distributed monolith with all the bad parts from both domains and very few of the benefits.
Looks to me like it's for when you want to render the code itself too, e.g. for a code example or a tutorial. Although the code is hyperlinked, so I suppose you could use it on a library for an effect similar to the good old sxr (Scala X-Ray). 
&gt; It's basically a distributed monolith with all the bad parts from both domains and very few of the benefits. Nice definition of a microservice
well ideally you would have a 50 / 50 load on events and API call, depending on the nature of your API. But you know, I work in a real world environment :P, so not everything is perfect
Can you elaborate? Whats wrong with using "REST" (did you mean HTTP?) as communication protocol and what you propose to use instead?
What's the alternative?
There is nothing inherently wrong with REST, it just doesn't fit intra-service communication. The culprit is that request-&gt;response-style communication imposes several limitations in a distributed system: * Issues with distributed transactions: Service A calls service B but the request times out. A does not know what the state of B is at this point, and what it should report back to its caller; * Cascading failures: A depends on B (which may depend on other services). If B fails to produce an expected response, any service that depends on A will also be affected by this problem; * Bottlenecks: Even though a single service may be very efficient, it may become a bottleneck if this service is called either directly or indirectly (through a chain of service calls) by a large part of the system as whole. This, in turn, can lead to either of the two aforementioned problems. Breaking a monolith apart into services which communicate synchronously (req-&gt;res) in the majority of cases only impose additional costs. What was before a simple call between two statically typechecked components in the context of a single request is now a network call through several service contexts with all the complexity that goes with it. &amp;#x200B;
Yeah "never ever" is pretty harsh, but I panicked for a while ;) I was earlier employed at a company where great care was placed on designing our infrastructure to encourage and the seniors \*\*still\*\* managed to create tight coupling between services (by using direct service-&gt;service communication through REST), which made the overall architecture very brittle. It eventually dawned on me that the only way to avoid these problems is to have communication be indirect and asynchronous.
A distributed monolith is microservices done wrong. They can also be done correctly, but someone needs to enforce that the design is followed.
Asynchronous message passing with say, Kafka or Pulsar. Even *MQ brokers if you don't need to scale that much, then again you didn't really need microservices in the first place.
Asynchronous communication between services e.g. via message queues. 
Asynchronous communication. Usually each service consumes and produces messages through some message bus. This allows services to be decoupled: they only read what they're interested in and produce what they can, no matter where the message comes from or which services may be interested in their message.
High level overview: you'll need to have your Scala web server running and listening on a port, with a route set up like `GET order/:id`, then add an AJAX call in your script to your Scala webserver at `order/txt`. With the routing all set up correctly Play would invoke your controller with the id sent to it from your JS. I don't see much of a better way. The overall process will be similar regardless of if you use Play or http4s or anything else. I don't really know Play but skimming the documentation it might be a little simpler with something like http4s which is Just Code^(TM) instead of a framework with a bunch of configuration files and such.
[removed]
Recently I've released first version of \[FDB-PubSub\]([https://github.com/pwliwanow/fdb-pubsub](https://github.com/pwliwanow/fdb-pubsub)), it's a Pub/Sub built on top of FoundationDB and Akka Streams. It's inspired by Kafka, so for some developers API can be familiar. &amp;#x200B; I think that it already has some interesting features: \* Support for exactly once processing (thanks to the fact that producing events and committing offsets happens within FoundationDB transaction) \* Support for publishing events within FoundationDB transaction (makes it easy to get events to the Pub/Sub) \* Consumers are exposed as \[substreams\]([https://doc.akka.io/docs/akka/2.5/stream/stream-substream.html](https://doc.akka.io/docs/akka/2.5/stream/stream-substream.html)), so users can benefit from Alpakka to easily integrate with other systems \* Scalable and fault tolerant storage (thanks to FoundationDB) \* Exposed as a library, so if you already operate FoundationDB there is no new stateful component to maintain &amp;#x200B; More details and current limitations are discussed in \[README\]([https://github.com/pwliwanow/fdb-pubsub](https://github.com/pwliwanow/fdb-pubsub)). &amp;#x200B; Any feedback would be greatly appreciated! &amp;#x200B;
I'm not at a computer right now, but if I understand what you're doing here, I would approach this by adding another step in the `for` to translate the `JsResult[JsArray]` into just `JsArray`. And then you can wrap the entire `for-yield` in parens and do a `.map(_.getOrElse(JsArray.empty))`. The `yield` returns a monad in case any step in the `for` fails, you just need to provide the 'default' return value in a failure, an empty `JsArray`. Hope this helps!
I like to read the book with example for both languages. Sometimes it shows how Scala is verbose :)
note that sxr is superseded by metabrowse, https://github.com/scalameta/metabrowse
Well, the project I mentioned is just a sea of red because there's no support for kind-projector's polymorphic lambda feature. Can't really tell past that point, but the version I have to use at work (admittedly a few versions behind) had a bunch of false negatives on the error highlighting only today (which is honestly even more of a problem than false positives).
Hmm I've never heard this before. What is the alternative? All message processing (kafka, etc.)? What do you do when that doesn't make sense? I think there is still value in breaking apart services even if they still have to communicate via req-&gt;res in terms of maintainability, testability.
TrueAccord | Backend Engineer Back-End | San Jose or San Francisco, CA, US | Full Time The technology stack you will be working with includes Scala, Akka, Play!, Spark, and Docker. This a great opportunity to join an engineering team of a successful fast-growing start-up that is transforming the industry, while expanding your Scala and Functional Programming. Responsibilities: * Build highly scalable distributed services that can process millions of events daily as our company on-boards the nation’s largest banks. * Design and implement new components and features in our platform. * Write high-quality unit tested code that will run in our continuous delivery pipeline. * Own workstreams end to end Requirements: * 2+ years of relevant experience * Proficiency with Scala or Java * Insatiable desire for continuous learning and continuous improvement. * Great interpersonal skill and enjoy working with members of other teams. Benefits &amp; Perks: * Work with talented and motivated people in a fast paced environment. * Medical/dental/vision insurance, 401k (with match), flex spending plan, and life insurance * Meaningful equity package * **Unlimited PTO** * Transportation benefits Team lunches and weekly happy hour Positions available in San Jose and San Francisco. DM me or email your resume with a note mentioning this thread to [mtolosa@trueaccord.com](mailto:mtolosa@trueaccord.com) 
In general you cannot change `Future[Option[X]]` to `Future[X]` unless you want a failed future. In order to have a valid `Future[X]`, your X must also be a `Monoid`, ie. something with an identity or "empty" element.
# Code architecture: * Controller / Service / Repository * Controller for handling transport protocol and basic input validation * Service for business logic * Respository for storage Scala: Monix/cats-effect (IOApp), pureconfig, logback, akka-http (I prefer http4s but akka-http is used due to momentum. Everything is in `IO` though right up till the edge where you call `unsafeToFuture`) Other stuff: PostgreSQL, Kamon + Zipkin for distributed tracing Didn't jump on the free monad nor the tagless final train since I never felt like the "benefits" actually outweights the drawbacks, all things considered. Definitely happy with that choice since I find my services are still easy to maintain and test. # System Architecture: * REST for simple request-response flow (e.g. serving frontend) * Events for things that can be eventually consistent Start with stateless monolith and stay that way for as long as possible. Whether you introduce queues or more services depends on the actual problem you're solving.
Nice. Although it's not very mobile friendly. It keeps opening the keyboard and doesn't let me go back
Am I missing something? I don't get hyperlinks.
right-click (or control-click) to get a context menu with "Go to Definition" et al
Pretty much the exact same here, with the addition of metrics via Lightbend Cinnamon. I haven’t used pureconfig, though I’ll have to check it out. Currently using Typesafe config.
Pretty cool, thanks for sharing ! What testing strategy are you using ?
At work, we've been using Scala for 5 years. Nowadays 100% of the code we produce is in Scala. Here's our stack, excluding database stuff: * [React4s](http://www.react4s.org/) and [Router4s](https://github.com/werk/router4s) * [Autowire](https://github.com/lihaoyi/autowire) with [uPickle](https://github.com/lihaoyi/upickle) * [Akka-http](https://doc.akka.io/docs/akka-http/current/) Oh, and of course [Scala.js](https://www.scala-js.org/)! Pros: Typesafe AJAX, typesafe URLs, typesafe components, easy interactive frontend. Cons: We don't take that much advantage of the async features of Akka, since we use some blocking libraries. Also, files using Akka http take forever to compile. 
You can use things like thrift or protobuf for intraservice communication. On a larger scale you can get into things like CQRS where things user Kafka to pass messages around, also using thrift or protobuf etc
Perfect. You've given me a few little leads to track down!
How is react4s working out for you ? We are doing some react for FE but it's not typed and is driving me crazy
Nice list! now, how to solve that: - distributed transactions: -- make API calls idempotent if possible. for example, use POST /resource/humanId to create resource, humanId provided by client, not by server -- classify your errors: "safe to retry" errors and APIs should be separated from "unsafe to retry" and "will fail again if retried" -- if remote call failed and client does not know what to do, it should propagate error to it's caller(that's why RPC rocks, queues sucks), all the way up the remote stacktrace until someone handles that or we tell remote client "unknown error happened, operation state unknown" so it can be noted and resolved either automatically (GET to check current state and retry) or by human - cascading failures: it heavily depends on how clients handle remote failures. well-written system can turn off some functionality instead of total shut down as monolith will. - bottlenecks: Every service within microservices system must be designed to run as a cluster. Every cluster must be monitored for usage. Adding new nodes to any cluster should be fully automated and tested. Microservices architecture is heavy investment of time and money but: - you get truly reusable code - self-documented microservice can be used years after deployment w/o looking at the source - you get reusable, isolated data - currently microservice is the only proper solution to share data 
It's working out great! But then, I am the author :)
We use Twitter Finagle. Despite being very old, it is still the best framework for microservices around, being much more advanced than alternatives. As for other libraries: - JOOQ (the only library I know that allows writing SQL queries with autocomplete in Scala w/o forcing DSL on you AND allows to develop database schema using migration scripts) - typesafe config, still find it better than any alternatives - custom REST router, it can collect API metadata and produce documentation - Jackson for JSON. Sorry Circe guys, but Jackson is FAST and still most feature rich JSON parser out there - JSR303 for rest model validation - Guice for DI - custom path-based authZ engine for REST. I'm not aware for any existing Scala solution. - flyway for database migration scripts
\&gt; Hmm I've never heard this before. What is the alternative? Up to a certain point you can get away with using synchronous communication between services. The risk being that people keep adding additional services in the same manner and end up with a tangled mess of calls which create a tightly coupled architecture. Two services—like what you typically have with a dependency on a third party service—is fine. Three is ok. The higher you go after that you'll start running into problems with infrastructural challenges like discovery, service upgrades (like rolling releases) and the problems listed in the previous post. As long as a service depends on another service to do its job you have tight coupling and the longer the chain of dependencies become the more problems you're likely to encounter. &amp;#x200B; \&gt; All message processing (kafka, etc.)? What do you do when that doesn't make sense? If messaging and eventual consistency doesn't work for your use-case it's either an indication that your service architecture needs to be reworked so that consistency guarantees of a specific feature can be kept in the context of one service, or that the assumptions of the system architecture are wrong to begin with. Microservices require a lot more planning ahead than a monolithic architecture. &amp;#x200B; \&gt; I think there is still value in breaking apart services even if they still have to communicate via req-&gt;res in terms of maintainability, testability. In my experience testing doesn't become any easier by physically separating services. In the best of cases you end up with a monorepo where you can still statically verify that the interface of one service is honoured by a caller, but it doesn't actually improve anything. Maintenance is much the same. &amp;#x200B; Breaking apart a monolith can however expose bad code patterns and buried dependencies, but I wouldn't say it makes anything easier; it just floats the issues to the surface, at which point you either try to push them back down by creating mocks and stubs or bite the bullet and try a more substantial rewrite. &amp;#x200B; In a well-structured monolith testing should be simple, with as few mocks as possible (preferably zero.) These tests can easily be exported if you end up breaking things apart.
Pretty sure you can switch over to pureconfig without touching your actual config files since it uses typesafe config under the hood :) Pretty happy with how our configuration works since we don't need to juggle environment variables. You simply run `sbt run` :) (No we don't commit our secrets)
Good old mocks and stubs (Mokito-scala or handwritten, depending on which is more clear). Also full e2e tests of our system including UI.
&gt; -- make API calls idempotent if possible. for example, use POST /resource/humanId to create resource, humanId provided by client, not by server &gt; -- classify your errors: "safe to retry" errors and APIs should be separated from "unsafe to retry" and "will fail again if retried" Idempotency only ensures that the service being called doesn't create additional resources. You end up with an at-most-once scenario. The calling service eventually needs to update its state and respond, at which point its decided state may be incorrect with regard to the upstream service's. If any service then interacts with both services they will have an inconsistency which needs to be handled in the third service, and so forth. &gt; -- if remote call failed and client does not know what to do, it should propagate error to it's caller(that's why RPC rocks, queues sucks), all the way up the remote stacktrace until someone handles that or we tell remote client "unknown error happened, operation state unknown" so it can be noted and resolved either automatically (GET to check current state and retry) or by human This is the only thing a client **can** do if a remote call fails, not the desired operation of the system. &gt; cascading failures: it heavily depends on how clients handle remote failures. well-written system can turn off some functionality instead of total shut down as monolith will. No matter how a remote call failure is handled it's still a cascading failure if it is done at the request of another service. You'll end up with a chain of service calls that all need to take into account how to respond to an upstream failure. At the end of the chain is the client which needs to know how to respond to a failure particular for the specific service-path the request took. The usual approach being popping up an alert with "Something failed, try again."-style message which does nothing to help.
That applies to systems that *require* distributed transactions. I'd say it should be avoided at all costs due to complexity and performance issues. In reality, well-engineered distributed systems do not end up with broken state if something happens during request processing and do not require strict data consistency, only eventual consistency. It can be achieved by using at-most-once semantics and grouping related data into single microservice/database.
Agreed. It's never as simple as one solution being better than another. That being said, transactions are very simple to deal with a monolith and thus may create a requirement for them. A naïve approach when splitting out services *will* result in a lot of headache down the road if this is not considered.
Defining service boundaries is definitely one of the hardest tasks in microservice design. Martin Fowler sums it up nicely: https://martinfowler.com/bliki/MonolithFirst.html
It is, but still doesn't support ide features like auto-complete.
Nice! We have some talks in english... I suppose portuguese is not that hard to understand for Spanish speakers :) Anyway, get in touch, we can figure out something
Your diagram only seems to work on New Reddit. On the old theme the entire diagram collapses into a single, very long, line.
Hey I love this HAHAAHHAHHAHHAAHHAHHA SCALA HAHAHHAHHAHAYESSSSS YESSSSS MACROS YES SCALA MACROS SHAPELESS I LOVE THIS IM G-
&gt;1 yeah this is kinda complementary to scaladoc or even metabrowse. I find socco useful when I want to describe example code in a step by step manner.
"Scala Native Runtime for AWS Lambda" .... F[Me] I'm sold. Thank you so much for doing this. very very informative. 
I looked at that briefly before and they have very little documentations and examples. I had no idea how to work with it. Hope they can improve on that.
Nordstrom Rack is always hiring Senior Scala Engineers. [https://www.hautecode.io/careers/software-engineer-site](https://www.hautecode.io/careers/software-engineer-site)
Well...... It's not that easy, at least for the European versions, the ones I'm familiar with :) But it's good that you're taking Scala to the wider audience, because it's not always trivial to follow a technical talk in a tongue that's not your native one :)
IIRC I ended up doing one of the Pascal's triangle problems for either the coursera class or some kind of coding practice like Project Euler and at that point I knew it would be a killer language. 3 years later I still feel like an idiot, but it's a fun ride.
I want to get a scala book. From the sidebar there are the basic books and Functional Programming in Scala (Manning). At what point am I ready to get the advanced book?
Because it is a continuation of typeclasses to other ends, and typeclasses are a central design pattern of `cats. Take, say, a definition for a key-value store: : --- trait Store[F[_]] { def put[V: Encoder](key: String, value: V): F[Unit] def get[V: Decoder](key: String: F[Option[V]] } --- In FP-like Scala, this trait is not intended as a common interface (methods) for many objects with same types: it is intended as a declaration of what operations you need to implement to support `Store[F]` on a type `F[_]`. The benefit is that all the flexibility and expressivity that you get to build instances of a type-class, you also get it for a trait like this one. This style, of using traits parameterised on a high-kind type parameter `F`, is usually called _tagless_. This is a homage to some research papers that describe a technique to implement embedded languages in Haskell. The core of the technique is to declare the language's signature as a type-class, and its different interpreters or compilers as instances of that typeclass. 
Author here. I'm a recent convert to Scala and I've become a pretty big fan! &amp;#x200B; Looking to promote Scala, particularly among non-engineers that work with data. E.g., data scientists and analysts. I still do a lot of such work alongside data engineering and I've found data analysis and modeling to be much easier with Scala and Spark. &amp;#x200B; Hoping to create a series of articles that introduce the basics of Scala to non-engineers. My thought process is that they don't need to master the language to start using Scala as a replacement for Python and SQL. Let me know what you think about this tutorial and how we could improve it. I'm open to modifying anything. &amp;#x200B; Also, would like to hear people's thoughts about how best to promote Scala for data scientists, analysts, and other non-engineers. I think we could make a lot of people's work easier and also get more support for this great language. &amp;#x200B; Thanks!
What benefits do you get from using Scala instead of Python or R as a data analyst? 
It looks good! At first I was wary of the use of `var` `Unit` and unit function signatures that don't tell you much like `(Int, Int) -&gt; Int` but then I realized I often use things like this to isolate certain points I'm trying to illustrate when talking with colleagues. Maybe a disclaimer when you violating FP/pure principals in your examples. --- And a bit of meta-critique: Teaching Scala to non-engineers is a great way to turn them into engineers :D You are trying to teach a "Scalable language" via part of the software engineering cannon, SICP. I think the best way to leverage the FP benefits of the language is to create DSLs for them perhaps? There are a few sharp edges off the top of my head (implicits and their resolution, type system) that would be challenging and slower for someone with a python background. Work on ways to make those disappear :)
You should also cross post this to your target communities if you haven't!
Scala made me go from data scientist to engineer.
Honestly I used to work in R and Python and was a real fan, but Scala ruined them to me. Theyre mostly relegated to visualization and academic imprementations or scripting, respectively for me now. 
Thanks for the feedback! I'll look into updating the examples. &amp;#x200B; Like the idea about DSLs on top of Scala. In many ways, I feel the Spark DataFrame does a great job at accomplishing this for data processing and analytics. It does include a bit of syntax sugar for making this legible and concise. &amp;#x200B; Meta: Spark is part of my strategy for promoting Scala. I'm gonna be making more examples of Scala code for data scientists and I hope to have some educational resources (like this post) to share with them.
Doing that with more targeted examples. E.g., doing data science with Spark and Scala. At the end of those articles I link to this one to learn more about Scala.
On a tangentially related note, anyone doing frontend development in React will be better off forgetting their FP ideas of purity. React has its own ways to contain dirty mutable state – components, hooks, redux, etc. Use those. If you want to stick to your FP principles, pick a native Scala UI library that agrees with you on that (e.g. Outwatch). The more foreign-to-Javascript concepts and principles you're using, the worse the impedance mismatch gets with very-Javascripty libraries like React. That goes for performance too. All those React wrappers and compatibility layers aren't free, and when you hit performance problems with them, optimization is much harder than when writing plain Javascript because the performance tax is spread very thinly across your codebase.
Scala is more legible, performant, and the powerful type systems reduce errors. Further, I believe basic Scala is easy and straightforward to learn for anyone familiar with Python, R or SQL. There's also a lot of other nice features about Scala that I've discussed in this [blog post](https://medium.com/@matthagy/initial-impressions-of-scala-from-a-java-and-python-data-engineer-c8e2038aca3d). &amp;#x200B; My long-term bet is that everyone will be more productive in Scala. Additionally, I bet people will like working with Scala more so than Python, R or SQL. Hence, I predict they'll be more development of the Scala ecosystem for working with data. E.g., Spark. I'm doing what I can to accelerate the adoption of Scala amount data scientists, analysts, and other non-engineers.
I'm considering submitting a proposal for a talk on [React4s](http://www.react4s.org/), but I'm a bit confused about the deadlines. On the PaperCall site it says: &gt; CFP closes at May 01 Later it says: &gt; Submissions will be accepted continuously until February 1st 2019, which means submitting early increases your chance of being accepted. And the title of this post says: &gt; Final call for submitting talks to flatMap(Oslo) coming up March 1st! When is the final deadline for proposing talks?
I'd agree that plain React is not very geared towards functional programming, but it's quite possible to extend it towards that. Here's an [example of functional code](http://www.react4s.org/examples/tree-editor) in React4s.
Haha omg, we have such an amazing attention to detail! The deadline is March 1st, I fixed the two mistakes now. Thank you for the feedback, and we're looking forward to hearing about react4s :)
Great :) thank you.
This is wonderful and so timely. I was just recently drawn towards Scala through sheer curiosity and wanting to learn functional programming. 
&gt; `Adjective`'s angle is slightly different, in that it foregoes the ability of compile-time refinement in favor of usability and simplicity. Perhaps write that in your README, instead of: &gt; you should be able to assess the relative similarities, differences, strengths and weaknesses for yourselves Also, you classify your library as "dependent types", but it seems closer to runtime validation. Dependent types would let you use the type system to _prove_ that operations produce valid values of the refined type, whereas you'd have to check the values again at runtime, AFAICT.
Sorry for not helping, just I thought scalafiddle helps for some type of questions like this: [https://scalafiddle.io/sf/8LuNIvU/0](https://scalafiddle.io/sf/8LuNIvU/0)
Thanks for your feedback! This wording is probably better, I'll go ahead and change it. &gt;Also, you classify your library as "dependent types", but it seems closer to runtime validation. Dependent types would let you use the type system to prove that operations produce valid values of the refined type, whereas you'd have to check the values again at runtime, AFAICT. I think you're right that in the strict sense, this isn't "Scala value-dependent types", however the type system does provide a guarantee that an invalid execution path will not be taken - which dances on the line. Also, you could use those guarantees to generate proofs at runtime - a type-system on top of a type-system, so to speak. This is something I intend to do, producing equivalence, tautology and contradiction proofs based on the boolean expression, and possibly on the runtime rules. I understand that by "type system" you meant "type system at compile time", however maybe the definition could ostensibly include things like that. This isn't a statement, just musing.
Check out https://github.com/yawaramin/lightweght-static-capabilities
What I am hoping to try is decouple my codebase from its framework, using 'hexagonal architecture': https://rskupnik.github.io/framework-independence-with-hexagonal-architecture I think Elixir and Phoenix are really a model citizen in this area, they set you up from the beginning with a very cleanly separated project. In Scala where you basically have to manually lay out everything in `src/main/scala`, and potentially implicits can fly around everywhere, it's more difficult to do. But I think interfaces are a key weapon in the fight against coupling.
Thank you for your enthusiasm. I hope you join the course and get value out of your time.
I used the markdown editor, probably that's why
"Types" are properly things associated to terms in a language, so it really only makes sense to talk about the type of a given source code expression rather than of a runtime value.
I don't recommend splitting your UI logic between server-side and client-side. If you want the UI logic on the client side then I'd recommend using a more lightweight REST-oriented framework for the server-side endpoints (e.g. akka-http, which play is built on top of, or http4s) rather than Play; if you just want to use Scala code in your UI logic then you can use Scala.js to compile Scala into Javascript. If you want the UI logic on the server side then I'd recommend doing all the UI the way your framework (Play) encourages, and avoiding writing any Javascript "by hand".
Thanks for writing these. I am coming from Python and SQL so it's just what I need.
&gt; the type system does provide a guarantee that an invalid execution path will not be taken All type systems provide this kind of guarantees. To compare two sound type system, we look at the amount of _spurious_ paths you have to deal with because the type system was not powerful enough to prove they could not happen. Your runtime approach simply means a lot of spurious paths. (Not saying it's a bad thing! Obviously, it's a tradeoff and your solution is just picking a point in the possible design space.) &gt; I understand that by "type system" you meant "type system at compile time", however maybe the definition could ostensibly include things like that Let's not start changing the definitions of things as they are commonly understood ;\^)
I see! Thanks for the clarification, I'm not a compiler-designer, just a user ;)
Thanks for your reply! I don't have a background in compiler- or type-system design, just a user of both. &gt;Your runtime approach simply means a lot of spurious paths What would that mean in practice? &gt;Let's not start changing the definitions of things as they are commonly understood ;^) Sure)))
&gt; What would that mean in practice? &gt; &gt; Say I have a natural number (refinement of, say, `BigInt`). When I increment it with an integer operation, in your system I'll have to check the value again after the increment (or am I missing something? I only looked quickly at the README). In a dependent type system, you'd know the new value still is a natural number, and there would be no need to handle the path where it's not (which is spurious).
That's a really good point; off the top of my head, there'd be no way out of re-running the validation step. Ok, you've convinced me, I'll remove the dependent-typing tag and wording - even to just prevent confusion. 
While learning Scala is it recommend to refer Scala language specifications? How much value it can add?
I've only looked at the language spec when I have a specific question about syntax, like how to use unicode literal characters. For me, examples tend to give me a better idea of how to use the language, whereas the specification would give me a better idea how to write my own parser and implementation.
Author here. This continues my series of teaching Scala to non-engineers, particularly data scientists. I particularly like that the exercises in this article have interesting results. Further, I think people will be motivated to develop their own novel analysis code to compute things they find interesting about Redditors. In general, I hope to do more exercises like this to keep readers interested and engaged. &amp;#x200B; I'd appreciate critical feedback from experienced Scala developers. Is there anything I'm misrepresenting? In general, I'm trying to avoid going into too many details about Scala until people already know the basics well, have done some interesting things with the language, and want to learn more advanced Scala. That does make it challenging because there is so much cool stuff in Scala and fully understanding everything can be helpful. &amp;#x200B; I'm gonna keep doing whatever I can to promote and teach Scala and would appreciate ideas for future exercises that will be interesting to readers. I really wanna do somethings with Spark to process large datasets. My current plan is to create a tutorial for setting up Databricks and hosting some interesting open datasets on a public S3 bucket. Thinking that might be too challenging for many readers and it would require them to spend on the order of $20. So I'd appreciate other exercises for ideas that will interesting to a broad audience of non-engineers. &amp;#x200B; &amp;#x200B;
Thrift and Protobuf are just protocols for RPC right? Isn't that still request-response based?
I'm now using [decline](https://github.com/bkirwi/decline). It's still lacking applicative handling of sum type, I think. But I like it better, as it composes better. Been using scoot in the past.
So, you can use grpc for a client-server architecture but I think the preferred method of typically pubsub using Kafka + a message transport protocol of your choice. This is a really neat article kind of dealing with this type of architecture: [https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/](https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/)
[removed]
The book is nice at orienting you in the scala native world. Also though, if you use scala native I think you are in a bit of an early adopter space and have to apply a little of what Richard calls a DIY spirit to things. 
&gt; While learning Scala is it recommend to refer Scala language specifications? How much value it can add? If you want to get into the nitty gritty of the language mechanics, the Scala spec can be a great resource. Like if you want to know how or why something is parsed or want to know the grammar. But if you're just "learning Scala" like you would any other language, it probably won't help you; I would stay away.
Thanks for the response, that makes sense for the most part. I think what originally tripped me up when applying this thought to my projects own architecture was that I have things like a kafka stream app that is pretty tightly coupled to an API which fronts a database. I struggle to see how I could re-work it to avoid that coupling because it is generally recommended to not share data stores, so having the KStream app access the db directly isn't a great option, not to mention that db needs an API for external apps to get the data as well. So I think some level of coupling is inherent. But I see what you are saying about it basically being a slippery slope.
Let me start by saying I appreciate anyone's efforts to making Scala or type-level things more approachable. That said, the ergonomics feel rough! Just perusing... case class Person (id: DbId.^^) That double up caret thing is crazy. (DbId ?^ 123) Why a symbolic operator? Why not just `isMemberOf` or whatever? Personally, I think symbolic operators are fine, when justified. I don't know that this is one of those times. I guess in your world, the `^^` and `?^` are related? Like `==` and `!=`?
It looks like your main method finishes even before all your queries are run. Try-finally block seems to be useless here. There is nothing wrong with the Slick calls here, they all look reasonable. The synchronous way they are invoked does not. Chain them together using flatMap or for-comprehension. If you wish to work with database in a synchronous manner perhaps Slick is an overkill.
I'm learning it right now by porting a Python app I wrote that scans Google Photos and a given local path and deletes local files that are already backed up on Google Photos (it stores Google Photos paths + md5 checksums in a MySQL DB, and it works off a local mirror of my Google Photos repository, so I'm not hammering the remote service for hundreds of gigabytes of data). I have a table FILES that is roughly represented as: case class FileEntry(id:Option[Int], path:String, checksum:String) trait Db { val db = Database.forConfig(...) } trait FileEntriesTable { this: Db class Entry(tag: Tag) extends Table[FileEntry](tag, "FILES") { def id = column[Int]("id, O.PrimaryKey, O.AutoInc) //other columns defined def * = (id.?, path, checksum) &lt;&gt; (FileEntry.tupled, FileEntry.unapply) } val files = TableQuery[Entry] } class FileEntriesRepository() extends Db with FileEntriesTable { def findBy(id:Int): Future[Seq[FileEntry]] = db.run(files.filter(_.id===id).result) // More operations } and then in my main app I have things like val repo = new FileEntriesRepository() val streamOfLocalFiles: Stream[Path] = ... streamOfLocalFiles.map(handleFile) def handleFile(path:Path) = { repo.findByChecksum(computeChecksum(path)) onComplete { case Success(res) =&gt; handleResult(path, res) ... } } def handleResult(path:Path, rows: Seq[FileEntry]) = { if(rows.length&gt;0) { removeFile(path) } } Point being that all your DB actions are going to be async and returning Futures of things, and for your gets they will return Future[Seq[YourRowType]]. So you have to use async methods unless critically necessary to force sync, surround with Async.ready or Async.result. IN general you don't want to turn async into sync and block a thread, but for instance I have an "index" step that must *fully* complete before executing the "search" step, so I have my indexing function surrounded by Async.ready just to ensure that it's done first. I could split this into two programs where the index runs on a cron job and not need to block anymore, but I'm lazy. Anyway, [here](https://github.com/dnvriend/slick-3.2.0-test/blob/master/src/main/scala/com/github/dnvriend/PersonRepository.scala) is an example of how to do things the "recommended" way.
Hey, I want to create small app which will expose some rpc endpoints + server react front-end. Is there any template or example project for doing that? &amp;#x200B;
Various frameworks have their own examples. If you really mean RPC then I would look at Scrooge (though be aware it's tied into the twitter-scala ecosystem which has a parallel set of libraries/frameworks to "mainstream scala"). If you're willing to (ab)use REST-style endpoints then akka-http (straightforward but less type safe; doesn't really play to the strengths of scala but IMO an easier starting point) or http4s (more functional-purist) are good lightweight options and have some examples on their pages. E.g. https://doc.akka.io/docs/akka-http/current/routing-dsl/index.html#minimal-example has some description and https://github.com/akka/akka-http-quickstart-scala.g8 is a full-application skeleton/example. (Personally I would want to understand each file and put the whole thing together by hand rather than cloning a template, but up to you how you'd rather learn).
Thanks I got it to work based off your explanation.
What alternative to Slick in a synchronous environment would there be?
If I create a class with a @specialized(Byte) T parameter and want to create a member Array[T], how do I ensure that it's properly an Array[Byte]? Is this where I'm supposed to be using Manifests? This array can potentially be quite large and I don't want it ending up eight times bigger than it needs to be.
[anorm](https://github.com/playframework/anorm) or [Doobie](https://github.com/tpolecat/doobie) come to mind. Also see the sidebar in this subreddit for other options. Please don't take these as recommendations, I use Slick myself (although I am not sure if I would choose it again). It all depends on your requirements.
Yes you need to use either ClassTag or Manifest. Which one you use in this case doesn't really matter, tho in general ClassTag is the newer/better thing. Manifest was deprecated a few years ago, then un-deprecated, and probably will be re-deprecated again. &amp;#x200B; `class Foo[T: ClassTag] {` `def newArray(sz: Int): Array[T] = implicitly[ClassTag[T]].newArray(sz)` `}` `println(new Foo[Byte].newArray(10).getClass) // Will print "class [B" which is byte[]` &amp;#x200B; Specialization is kinda orthogonal to this, without @specialized the method after erasure will return Object - but since arrays (even primitive) are objects in java that doesn't matter and no boxing will happen either way.
I welcome any improvements to \`Vector\`! I basically use it instead of \`Seq\` when I want a \`Seq\`, unless I explicitly want the laziness of \`Stream\`. I find myself avoiding \`Seq\`. For some reason people really like \`List\`. Maybe it's a little better for memory usage?
Interesting. In the cost section, could you compare it to the cost of using a regular lazy val? Or in general compare an contrast why you would do def words(text: String): Array[String] = { val sep = cement { Pattern.compile("\\s+") } sep.split(text) } vs def words(text: String): Array[String] = { X.sep.split(text) } object X { val sep = Pattern.compile("\\s+") } vs def words(text: String): Array[String] = { X.sep.split(text) } object X { lazy val sep = Pattern.compile("\\s+") }
I appreciate your feedback! &gt;That said, the ergonomics feel rough! I've gotten that response enough times now that I'll probably go ahead and do a few *human-readable* aliases. &gt;I guess in your world, the `^^` and `?^` are related? Like `==` and `!=` ? Exactly - however it's more like: belongs - `^^` (and `^` also... compiler was confused about naming at one point, so I had to modify - will revisit), does not belong - `~^`, belonging test/general belonging trait - `?^` 
Hi, I'm not really sure I understand - is this a form of proof by induction?
You have to use it in your test suite (or x.await) and you have to use it in console only programs. You don’t have to use it if you have a framework like Play or Akka that’s doing it for you at the out-most layer (which is 90% of the time) but someone somewhere is always doing it. Am I wrong?
You're not wrong, Walter, you're just an asshole.
It is basically the technique you were describing—‘proof at runtime’. Harnessing a combination of runtime checks and type system guarantees to prove something. For example, in the first code sample of the repo I linked to, it did a single runtime check that the list had a suitable length and if so it ‘wrapped’ it in a new type that acted as a ‘guarantee’ that the check had been done. Subsequently any code using values of that wrapped type could access elements at any position within the bounds of the list with no need for additional bounds checking.
Interesting. I wonder how one could abstract over that pattern in Scala - maybe you could POC it, and see how it jives? It's very similar indeed, since we're both using a wrapper-type as "proof" a property holds, however you focus on expanding this by method of induction (if I read it correctly; and it's really cool you would, as it'd dramatically decrease runtime), whereas I haven't even started on this yet - instead focusing on the property-algebra.
Can't wait for the code examples and zio docs to get updated!
Hehe, me too. Long week ahead. :)
[removed]
Damn it! Just when I started to getting a grasp on Tagless Final :) Anyway, some things bother me for some time now (as a FP beginner), especially when I fail to sell the FP idea to other programmers. I've noticed that, in his every talk, John mentions that the benefit of FP (among other things) is treating programs/statements as values, which I understand is one of the cornerstones of FP. While I believe there's a truth and beauty in it, I still fail to see the overall benefits of it (maybe because of simple examples given in talks; I'd like to see a bigger picture). It is cool that you can put a program to a `List`, and that program can be a fiber-level worker so you can replicate it N times with `fill()`. However, can't those things also be done in OOP way? I mean, you would have to create some `Worker/Runnable` that you'd push to thread pool, and if you'd want N of those workers, you could create them by for/while loop looping N times, but it's not THAT different (or am I missing something?). When it comes to "send the program to a function and modify it", I guess that could be coded as a Template Method or some similar pattern? The other thing is Tagless Final which, when I explain it to others, I get the response along the lines "Ok, but what value does it bring to me? I mean, you can have basic interfaces, and if you want different behaviors, you can send them as a method parameter, something like Strategy Pattern". \[as oppose to abstract them in `F[_]` context\] So I want to know does it boil down to "aesthetic" (expressive?) difference or there really is something that Tagless Final (or `ZIO[R, E, A]`) can offer that OOP cannot (or at least that OOP way would be monstrous and you'd have to go against the language to implement the same behavior)? P.S. If this shouldn't be in this thread, I'll move it to separate thread.
A `Runnable` is a program as a value, it's a functional idea poorly implemented in Java. A `Runnable` is literally a thunk John uses in the pedagogical `IO` example, the prime difference being `Runnable` doesn't implement a `map`/`flatMap`.
"What value does tagless-final give me?" is a common question, one I often got while teaching functional Scala. I can enumerate the benefits it provides, but honestly, it's hard to say the benefits outweigh the costs. For many programmers, I don't think they do. Further showing them tagless-final before they've mastered functional effects can really put them off functional Scala. Nonetheless, the arguments for functional effects are very compelling. Beyond just the concurrency, asynchronicity, resource-safety, error management, thread pool management, and other features offered by libraries like ZIO, the paradigm of _programs-as-values_ is _really_ powerful. I'll give you a few nice examples: - a retry combinator that takes _any_ program and retries the first one - a repeat combinator that takes _any_ program and repeats it according to a schedule To achieve these benefits in procedural programming, you have to reinvent the notion of a "program as value" (like a `Worker&lt;A&gt;`), but without the cool functional combinators (`map`, `flatMap`, `zip`, etc.). In addition, programming using expressions is extremely concise and powerful. For example: ```scala IO.forkAll(List.fill(10) { queue.take .flatMap(uploadToS3(_)) .repeatOrElse(Schedule.forever, logError(_)) .forever }) ``` In this tiny amount of code, I created 10 worker fibers, which I can now treat as a single logical fiber, which pull items from a concurrent queue, and upload them to S3 until there's an error (in which case they log them), restarting as necessary. This is a single expression built from program combinators. This level of fluidity and expressiveness cannot be achieved without re-creation of the concept of _program-as-value_. Beyond just these benefits (which I call _abstraction_, because we're using variables and functions to make composable building blocks), we gain the ability to refactor our code without changing its semantics, and of course, with something like ZIO Environment, we can test the code as well. I think not only are functional effects getting better, but as a community, we're getting better _describing_ those benefits, beyond just vague references to "purity". That can only help make a stronger case for Functional Scala.
Thank you for your answers! :) The moment when I fell in love with FP is when I understood the separation of a program description and its interpretation. And to be honest, I found Free much harder to understand than a Tagless Final - TF just clicked to me, I don't find it THAT hard. What I want to say is that TF drawn me to FP, not scared away. Anyway, I think this community needs more examples like one from your post and maybe some OOP counterexample would be nice just for a comparison. And I don't mean that stupid comparisons where someone compares line counts, but the comparison of expressiveness, easier-to-reason-about, easier-to-develop code. Keep up the good work! :) 
A very nice presentation, thank you /u/jdegoes 
Excellent talk! The next bit to fill in is the intriguing promise of incremental migration of Plain Old Scala Programs towards this new world. For example, take your typical web service doing some auth, logging, querying a database, calling some other service, and returning JSON, typically Future-based. For Tagless-Final, not trivial; for ZIO...?
Absolutely kickass presentation. Here is the blog accompanying the talk http://degoes.net/articles/zio-environment
[removed]
In Scala, either your own Adjective library or my Newtype wrapper (https://github.com/yawaramin/newtype) would work. They both capture the key idea of the ‘kernel of trust’ which is carefully managed and the guarantees that it enables to propagate throughout the code once you have the values of the type. And yes—the design space you’re exploring is cool and I think not something I’ve seen outside of things like validation libraries (e.g. ajv in the Node world or Ecto Changesets in the Elixir world). You could build quite complex structured type descriptions with it and it would enable returning useful error messages to an API client. Theoretically it should also be possible to use a macro to compile descriptions using your property algebra down to a single method call which does a hard-coded check for all the given properties in a single pass.
Brilliant. The community needs more jargon free explanations of this stuff like this talk. Keep up the good work
https://youtu.be/eZuPb1Ettow
Being the primary caregiver to a toddler is NOT conducive to watching this video. I'm a smart dude, but I couldn't even make it ten minutes before I felt stupid as hell. I'll be back in a couple years to watch again, smart guy :D
I think you try to pure where you can and where you can’t you can’t. The ReasonReact API is decent enough and is replicated in scalajs-reaction which has shades of react4s in it. japgolly’s scalajs-react library I think tries to be as pure as possible. The probably with going pure scala.js FP is that you can’t leverage the existing react components that are out there. That’s why scalajs-reaction tries to make it easy to interoperate. From what I have encountered, performance issues usually arise from poor designs around state management and other async aspects vs the scala.js interop layer. Having said that, using scala.js SFC’s and hooks, you are pretty much writing javascript and the interop layer is thin based on looking at the output js code. 
/u/jdegoes Hi John ! This does look promising, and in particular wrt the learning curve problematics of Final Tagless (which I do struggle to convey at work). I'm primarily a user of cats-effect, but I'm pretty sure Zio has some similar resource management mechanism. If a module needs to be instantiated in a way that ensures the cleaning of its resources, how would you go at providing/"injecting" it to your ZIO instance ? 
When using akka cluster/persistence, is it common for the data stored in the actors and persistence stores, to also be stored in a regular relational database for other querying purposes? If so, how would you keep them in sync and which would you use as the source of truth? &amp;#x200B;
&gt; it seems that the majority of the Scala community leans towards functional programming I think you might get that impression if you hang out here or on twitter a lot, but in reality I don't think it's true. Besides Scala was never meant to be a 100% functional language. It wouldn't make much sense to do a 180° now as it would be a brand new language, especially when stuff like Eta are in the works as well. That being said, I do think some functional programming concepts need to be baked in Scala, like typeclass for example. Then again Dotty is working on it.
Personally I think that allowing developers to gradually 'roll' into FP is one of the strengths of Scala. Making it a pure functional language would make the learning curve steeper for developers without an FP background, and I think that that is probably still the majority. Now however, adopting Scala from java immediately gets you the improved syntax and reduced boilerplate without forcing you to radically change your programming style. And over time, having spent more time working with Scala, it's likely that people with move to a more pure FP style. I've been at 2 companies now where you can clearly see the evolution in the codebases as devs get more familiar with FP.
Because the majority of the community doesn't heavily lean towards FP. They don't create libraries, presentations, podcasts or Youtube videos. They are the silent majority who are just looking for a much better Java (which Scala is). The sort of developers who follow the Twitter style guide. FP is simply a difficult concept for many developers to get and so it takes time to ease their way in.
How would it be different then from other purely functional languages? Does the world need a second Haskell? Besides, it may still fall short of the really-truly-pure-functional mark because of the tradeoffs imposed by JVM.
&gt; From my experience Out of curiosity: what has your experience been? Where did you get that impression? Personally, I really like that Scala is _not_ opinionated about purity. I very often use impure implementations of pure concepts in local, controlled scopes. As long as the impure stuff does not leak through the interface, you are fine IMHO. (Yes, that requires some discipline. Scala is a language that requires discipline in general, a tradeoff for its great flexibility.) I would never accept a language that _always_ forces me to use needlessly inefficient data structures and algorithms out of pure dogmatism. I noticed that in industry, Scala is being used a lot on things like backends for web services. In that space, IO is probably the biggest bottleneck, and the gist of an application's work is gluing business logic together, with asynchronous access to databases and external microservices. There, it's most certainly fine (and even recommended) to program such everything in pure FP style. But there is much more to programming than that.
Exactly. Scala is a hybrid oo/fp language from beginning and that’s a feature. 
I don't think Eta is going to be a competitor to functional Scala programming on the JVM, because the performance trade off will just be too much. The laziness and garbage generation of Haskell work with a runtime custom-built to alleviate the performance issues they cause, not so much a runtime built for executing imperative code. 
So, as pointed out in other answers, I think there is a selection bias because the FP purists are pretty vocal (and I don't mean that as something negative at all). Most application programmers are probably somewhere in between the axes of FP, OOP and procedural, but you don't hear them as much. Also, even the most FP libraries out there heavily rely on OOP concepts. Looking at Cats for example there is a lot of extension method syntax so you can use your FP concepts in an OOP manner. However a lot of this stuff is marketed as FP, while it truly is a hybrid approach. It is just not the type of OOP people regularly see as OOP, which includes mutable objects and imperative control flow. And I guess that's why it is marketed as FP, because traditional OOP has a bit of a bad name in the FP community.
Scala's success is largely due to its approachability. You can start by writing pseudo-java in Scala, or fall back to it if you don't know how to do it in FP. Without this, many developers who are now able to write good Scala code wouldn't have been able to even get started. In other words, ask yourself why Haskell stays so niche while Scala is widely used in the industry.
I was at this talk the other day and it was excellent! Looking forward to experimenting with this soon. Seems like a very interesting approach. 
This works with a stable team of people that grow together. But as soon as a newbie comes in, then they will found themselves in pure FP land full of monads. So this "feature" is arguably nice but of limited applicability IMHO. In that case, you may as well go full-FP right from the start with a purely functional language. I've been burned by a big project developed by several people with varying skill levels, resulting in a big ball of mud, rendered worse by a mix of imperative, simil-OO and functional styles. Personally I've developed my FP skills/mindset with Java and Clojure, then transitioned to Haskell and functional Scala. 
*Shhh* it's fine, we need *some* way to lure the OOP crowd over to the FP side. :P
Many major headline Scala project such as Kafka, Akka, and Spark don't use Scala in a purely functional way. It's mostly used as a better Java in their codebase. So while I'd say most Scala *enthusiasts* are interested in using it in a purely functional way, there's a huge OOP Scala community.
&gt; could you compare it to the cost of using a regular lazy val I'm not intimately familiar with how those work. I'll keep in mind that I need to do actual benchmarks. &gt; why you would do It just saves you having to go out of your way to add a named variable on a top-level object, really. Cementing automatically generated implicits is the more interesting use case.
Fuck that noise. Go chew on Haskell if you're a purist. I prefer to get things done. 
The stacked module traits in the examples there are reminiscent of the [thin cake pattern](http://www.warski.org/blog/2014/02/using-scala-traits-as-modules-or-the-thin-cake-pattern/).
&gt;why not have a purely functional Scala or perhaps a fork of Scala What features would you expect in such a fork?
I'm getting things done in Haskell everyday thanks 😁
Please put the compensation
Why does Scala not pivot to a purely OO language?
The majority of the Scala community doesn't lean towards functional programming. What you're seeing is an FP community which is far more vocal than the majority. It's not the same thing.
Lol. 
Lots of good answers already. That said this Odersky talk (while old) I does a nice job of addressing the balance he was trying to strike in the language. https://m.youtube.com/watch?v=kkTFx3-duc8 
There is no requirement to have "global" modules. Since `provide` is just an ordinary method on any ZIO effect, you can create "modules" dynamically and effectfully, and use ordinary combinators like `bracket` to ensure they are released: ```scala createModule.bracket(releaseModule(_))(module =&gt; myCode.provide(module)) ``` You'll have to do more work if you want to combine global and local modules: namely you'll have to proxy all the global requirements upward, since right now there is only `provide`, not `provideSome`. Hope that helps! :)
[/u/jdegoes](https://www.reddit.com/u/jdegoes) thanks again for the excellent talk! I am amazed how much cool stuff ZIO has gained in the last year or so. I just have one question left about this pattern. Why do you need to put your service inside another trait (The HasConsole trait), why can't you just pass the Console/Service trait directly instead of wrapping it into another trait with just one field?
I think you should look more into the OOP aspect of Scala, they are publishing articles trivially solving OOP problems in Scala that in other languages took thousands of lines of complicated code.
I use Bazel to build and test Scala at a large company in London. It works brilliantly for us. 
Is any of that stuff getting upstreamed? :)
Are any of those changes getting open sourced?
Yup, ended up pulling it and having a go :) . The lack of composition on modules (at value level) is somewhat annoying, but it seems like you can decrease the pain of proxying by using path-dependant types : ``` trait Logging { def logging: Logging.Service } object Logging { l =&gt; trait Service { def log(s: String): UIO[Unit] } // the boilerplate class Memory(ref: Ref[Chain[String]]) { trait Service extends Logging.Service { def log(s: String) = ref.update(_.append(s)).void } trait Module extends Logging { def logging = new Service {} } } } ``` which lets you do, given `val program : ZIO[Console with Logging, E, A]` ``` Ref.make(Chain.empty[String]).flatMap { ref =&gt; program.provide { val memLog = new Memory(ref) new Console.LiveModule with memLog.Module } } ``` overall, I do enjoy the approach. I'll be looking forward 1.0 ! 
While not eliminating the OOP side by any means, most of the upcoming Scala 3 features are geared toward making functional programming easier: union and intersection types, type lambdas, multiversal equality, null safety, etc.
This may not be so true, ETA does a lot of whole program transformations for your ETA code (which it can do since every call to Java is done by FFI so they can statically verify this), which means it can do a lot more performance tricks then Scala, they are not really bound by an ABI like Scala is. iirc They are already doing this trick in certain spots.
Maybe I'm just not following along properly, but how would this handle type-indexed effects? A simple example of an effect I'm thinking would be having access to a key-value store with polymorphic K, V types. The idea being: the 'generic' portions of the code which don't care about concrete K and V can just operate generically -- maybe an in-memory store doesn't care about the types, or you have code which knows only that K and V implement Monoid (or whatever) -- but for e.g. storage in a database you'd need to have isomorphisms from K and V to strings (or whatever column you you want).
Just curious: Are you working in a mono-repo style? If no, how did you set things up?
Yes, a mono repo, and polyglot too. C++, Scala, Python and other languages. Bazel is ideal for mono repos, but it’s drawbacks (fiddly dependency management) put me off using it at the non-enterprise scale. 
actually it already is a purely OO language. In Scala everything is an object (including functions, typeclasses, etc.).
Third party dependency management pops up pretty regularly on the Bazel mailing list. Google tends to not be very empathetic towards it because internally what they do is very, very different. Accordingly, it doesn't get solved very well.
Thanks! (I've been toying with trying it out, but our shop has different projects for different customers -- which some shared code -- which kind of rules out monorepos, alas.)
I think John already covers it very well. Just adding my two cents on Tagless Final and related stuff. The main purpose of having these techniques IMO is treating programs as a "value" in the same programming environment, so that one would be able to, using the same language, to do what would usually involves meta-programming. For example, free monads is a way to represent a "embedded" program (EDSL), then we can reason about the program itself. In the OOP world, an example can be dependency injection. And as we can see, DI is mainly handled by some "meta" level things, like annotation + reflection, code generation. This is because the language lacks of ways to represent "programs written in the language itself as a value". It's not that OOP cannot do it. But since it's not in the same language/environment anymore (annotation, is another "language"), we don't share the benefits of, such as, type safety, or freedom of creating your own injection pattern for example, instead of waiting for someone to write a DI framework. Think of how big of a project that's going to be in Java. Think all of this as having a "mini" language defined in Scala. Functional language like Scala has the ability to "create" a language inside itself, while traditional OOP language can only work its way out, by adding meta level tools. That's because of the limitation of the language.
No current plans to open source it. No fundamental opposition, but it's pretty messy - the improvements described are a mix of scripts, language-specific improvements, cloud-infrastructure, and other things - and not something that's easy to package up and offer to someone as a coherent open source project
Adding +1 for that feedback :) Scala community has been moving away from this kind of mysterious symbol methods because they're just impossible to read and remember. We can still see them a little bit in SBT, \`%%\` vs \`%%%\`, crazy &amp;#x200B;
The devbox technology is very interesting. Would love to know more about it. It is a pain point for my work, and there is no easy solution for this. 
It's literally an EC2 machine with a very small syncing script to send files over from your laptop automatically when you save, and a very small background daemon that runs `who` every 30 seconds and runs `sudo shutdown -P now` after 90 minutes of inactivity. We use a custom syncer for performance (linked in the blog post) but `watchman`+`rsync` or similar works just as well at the cost of another 1-2s of syncing latency. The main thing that was surprising was how much of an improvement it was: I don't think anyone else expected it to chop all our build times in half and make remote cache downloads effectively instant!
Is there an easy way to view any benchmark results you've done? I didn't see them on the website. Doesn't look too hard to run them locally I was just hoping for an easily consumable format.
I updated my post with the answer 
Great article and unbelievable results! That is really interesting for me because currently I planning to decrease dependency fetching time using [Coursier](https://github.com/coursier/coursier) in our SBT build (in my test it’s from 20m to 4m). And I was thinking about migrating to another build tool (trying [mill](https://github.com/lihaoyi/mill) now), but after this article I definitely want to try Bazel! I have team of 20+ devs and really like remote caching feature. The only thing is our bunch of repos, is blocker for using Bazel? What is the drawbacks of Bazel’s dependency management all people talking about?
I'd love a Scala where you could easily encode more guarantees than are possible today. Like that a function will not throw an application-level exception. Or that a value will definitely not be \`null\`. But beyond that, I definitely think that levels of purity should be up to the team. 
&gt; That is really interesting for me because currently I planning to decrease dependency fetching time using Coursier in our SBT build (in my test it’s from 20m to 8m) &gt; What is the drawbacks of Bazel’s dependency management all people talking about? Third-party dependency resolution is its own monster that we could write a whole another post about. When we started, Bazel provided approximately *zero* support for it, and even now my impression is that the support is rough and experimental. In the end we wrote a lockfile-generator that does resolution once, writes out a lockfile with the resolved versions that you check into the repo, and uploads the resolved artifacts to S3. This ends up working surprisingly well: people don't change the third-party deps *that* often, so most of the time no artifact resolution gets performed at all. The only cost is that if you change a dep, you have to run a resolution script and wait a few minutes for it to re-resolve, and the commit the changed lockfile. For the vast majority of people "just writing code", this turned out to be vastly superior than worrying about SBT re-resolving things over and over. &gt; The only thing is our bunch of repos, is it blocker for using Bazel? Maybe not a blocker, but you definitely lose many of the benefits of using Bazel without the monorepo. Almost the whole point of Bazel is to make living in a monorepo bearable, so you then get all the other monorepo benefits. If you're multi-small-repos to begin with, the value proposition is much less obvious and it might make sense to use a simpler tool like Mill
(This is a copy of my comment/question from another thread on the companion blog post, hopefully it'll get an answer here:) Maybe I'm just not following along properly, but how would this handle type-indexed effects? It seems that there'd be a problem "naming" the member functions/methods if you have multiple instances of the type-indexed effects "in scope" at the same time. A simple example of an effect I'm thinking would be having access to a key-value store with polymorphic K, V types. The idea being: the 'generic' portions of the code which don't care about concrete K and V can just operate generically -- maybe an in-memory store doesn't care about the types, or you have code which knows only that K and V implement Monoid (or whatever) -- but for e.g. storage in a database you'd need to have isomorphisms from K and V to strings (or whatever column you you want). 
Thanks to detailed answer!
Twitter ecosystem is also very heavy on functional aspect of Scala. They just don't emphasise it. &amp;#x200B; And that is exactly the power of Scala. OOPs and FP both have their own strengths and weaknesses. Scala shines by allowing developers to augment both the aspects with situational use of the other. &amp;#x200B; The reason we see all the presentations and talks talking about the functional-features is because these functional-features are actually patterns to solve some general challenges, in a way similar to OOPs patterns such as delegate, facade etc. &amp;#x200B; Its just that these functional-patterns are a bit different from OOPs that they sound alien to people who are new to FP. 
Thank you for your answer! Could you elaborate a bit more on how this relates to e.g. my statement on Strategy Pattern? I follow you on annotation/reflection thing, it is better to have type safety, but with additional parameter, I don't lose type safety. So I guess I argue on difference between sending behavior as additional function parameter vs having it abstracted with HKT. We just shifted it from value level to type level. One could argue that additional parameter interfere with the program logic/algebra, but I'm not sure that alone is strong enough argument for TF.
The value I get from tagless final is being able to keep the effect type abstract in widely-used open source libraries. &amp;#x200B; For example, my team uses [sttp](https://github.com/softwaremill/sttp) and we have our own in-house Task implementation. If sttp had been written to work with a single async type (Future, Monix Task, Cats IO, scalaz Task, scalaz IO, Twitter Future, Trane Future, Trane Task, Stateless Future, or ThoughtWorks future), then we would have had to write a wrapper for the library. In some cases that's awkward to do because libraries returning futures need an execution context passed in and our code usually doesn't have one in scope. &amp;#x200B; So it's nice that sttp keeps that abstract and lets us plug in our own task type.
@Everyone please elaborate. What exactly would be different if Scala became pure?
Haskell is already somewhat slower than Java, and I presume it does mostly the same optimizations as Eta. I'd be surprised if Eta has the manpower to implement substantial optimizations to compensate for the inherent disadvantages of implementing a functional language on the JVM. Optimizations being possible does not mean that they are performed or that they have a large impact. Java JIT compilation has been sold as "theoretically faster than C", but it's still about half as fast as C. Haskell and Scala servers both perform poorly in the TechEmpower benchmarks. Scala already supports using Java libraries and is easier for imperative developers to learn, and if you need speed you can write imperative code and you're not at the mercy of the compiler.
Very interesting ideas. Props for that. But I don't really see how this is significantly more testable than basic `IO`, or as testable as tagless-final. The essential parts of how you would actually test it are also conveniently skipped over: &gt;All we have to do is construct an implementation of the Console.Service interface for testing: &gt; &gt;`object TestConsole extends Console {` &gt;` val console: Console.Service = ...` &gt;`}` &gt; &gt;Now we can run the same program using our test service: &gt; &gt;`val programTest = program.provide(TestConsole)` &gt;`DefaultRuntime.unsafeRun(programTest)` But what is supposed to go on the `...`? Your `programTest` is still a `IO[IOException, String]` which you've just said cannot be tested properly.
&gt;Haskell is already somewhat slower than Java This isn't true &gt;and I presume it does mostly the same optimizations as Eta Some optimizations yes, others no which makes sense because the JVM is quite different compared to the Java runtime &gt;Optimizations being possible does not mean that they are performed or that they have a large impact. Java JIT compilation has been sold as "theoretically faster than C", but it's still about half as fast as C. The point is that Scala has to follow the Java ABI because its designed to interopt nicely with Java. That means there are less optimizations available compared to something like eta. This is much more visible for pure FP code, there are so many optimizations that Scala cannot do because it needs to maintain the Java method contract. If you just use Scala as a better Java, then the difference is less pronounced (but in such a case you should be using Kotlin anyways) Ontop of this, Scala doesn't do purity tracking where as eta does. This means that eta can do things like map fusion (since it knows that the map signature is referentially transparent). Scala doesn't do this, because it doesn't know that \`.map\` is pure. &gt;Haskell and Scala servers both perform poorly in the TechEmpower benchmarks. Scala already supports using Java libraries and is easier for imperative developers to learn, and if you need speed you can write imperative code and you're not at the mercy of the compiler. Right but this is a shallow view of the problem, because for webservers its not just "performance". Its also latency and balancing throughput vs fairness. Its also about what happens when you push 50k requests and how your server will drop requests. There are also other concerns such as streaming vs being strict (i.e. streaming has worse throughput but has much more predictable memory footprint) and whether you use connections pools or not. The thing is that due to ecosystem reasons, people haven't really put the amount of engineering effort required to make ultra high performance webservers into Haskell. Haskell actually has a surprising amount of non-"FP" tools which you can use to microoptimize such things (control over boxing, arrays + mutable collections, strictness controls, references). Its the same deal with OCaml for example, OCaml probably has the best performant runtime of any high level langauge (its one of the only high level languages used in HFT by Jane Street, disregarding Java hacks such as pause the GC and running it at a certain time or just using offheap for everything at which point you should just use C++). However the story for OCaml web servers is not that great because it hasn't been used a lot in that space. People just need to pour man hours into making high performance HTTP servers/clients, its not a trivial thing to do. &amp;#x200B; &amp;#x200B;
Just wanted to ~~plug~~ comment on the Comparison Matrix near the end of the video: the Concision and Type Inference is applicable to Eff monad too, depending on its implementation. - [https://github.com/marcinzh/skutek](skutek) is an implementation of Eff monad, using contravariant parameter + intersection types, in similar way as the `ZIO` type does. This allows Scala compiler to infer effect types and reduces boilerplate typically seen in Eff implementations more closely based on the Haskell's original. - [https://github.com/halcat0x15a/kits-eff-dotty](kits-eff-dotty) is also an implementation of Eff monad, that uses dual, but equally capable variant: covariant parameter + union types. Requires Dotty. 
&gt; Maybe I'm just not following along properly, but how would this handle type-indexed effects? There are 2 solutions to explore (speaking from [this](https://old.reddit.com/r/scala/comments/av27su/the_death_of_final_tagless/ehh11ma/) perspective) 1. Attachment of tags, in spirit of `@@` operator from scalaz 2. Subclassing: case object FooStore extends Store[FooKey, FooVal] case object BarStore extends Store[BarKey, BarVal] type FooBarZIO[A, B] = ZIO[FooStore.type with BarStore.type, A, B] I started with the former (cumbersome implementation), and recently redesigned to the latter (a tiny bit more verbose). Is this applicable to `ZIO`? I ain't gonna lie: might be, or might be not.
But doesn't that still leave one without the ability to address both FooStore and BarStore in a single FoorBarZIO[A,B]? It also seems to lack an ability to access the stores generically. (AFACIT, at least) The methods would 'clash' AFAICT? Or would it be possible to `accessM` either? It seems it could get rather confusing.
You might want to mention what makes Spark a bit different from joins in a RDBMS. Also using the "explain" method and looking through the physical plan can give you some good ideas about what is happening under the hood of the join, and can have massive consequences for runtime. Most instructions that utilize spark's catalyst compiler will be denoted with a "*", and the explain method also highlights what type of join is being done. There is a big difference between a map side join and hash join. 
Thanks for the feedback. Really appreciate it. I am planning to write another part in this series where in I will dig deeper on how catalyst kicks in and performance aspects of joins as well. 
The methods would have to be accessed through the `object`s. That's the "more verbose" bit I mentioned. case object FooStore extends Store[Int, Double] case object BarStore extends Store[String, Double] for { x &lt;- FooStore.lookup(0xF00) _ &lt;- BarStore.update("bar", x) } yield () 
The JVM has an enormous amount of money and work making it run fast, and it is imperative which is what modern hardware is tailored to suit. I doubt that Eta, which is limited to the functionality given to it by the JVM, which is a subset of the functionality available to the Haskell compiler, can be optimized to the level of Haskell. You have a point that it could probably beat pure functional Scala, assuming that the laziness of Eta isn't too big of a hindrance on the JVM. I believe that the cost of laziness is higher on the JVM than in native code. Allocating thunks and checking whether parameters are evaluated adds up. Scala doesn't have that disadvantage, and it will get whole program optimization along with other optimizations in Dotty. I don't doubt that Haskell can have great web server performance, what I do doubt is whether Eta is useful as an alternative to Scala for pure functional programming on the JVM, and whether it is worthwhile to fragment the pure functional JVM developer community further. We already have Scalaz-Cats and who knows how many effect system libraries. One unmatched advantage of Eta is that you can use Haskell libraries with Java libraries, but I don't think that's very useful. 
&gt; The JVM has an enormous amount of money and work making it run fast, and it is imperative which is what modern hardware is tailored to suit. I doubt that Eta, which is limited to the functionality given to it by the JVM, which is a subset of the functionality available to the Haskell compiler, can be optimized to the level of Haskell. I never claimed this, I was responding to the claim the for purely functional programming Scala would be faster then Eta. &gt; You have a point that it could probably beat pure functional Scala, assuming that the laziness of Eta isn't too big of a hindrance on the JVM. Purely functional programming in Scala also persuasively uses lazy, and Haskell (as well as Eta) has many tools to enforce strictness. This is basically optimizing 101 for Haskell/Eta &gt; Allocating thunks and checking whether parameters are evaluated adds up. GHC and Eta both have compiler passes which actually remove the thunks created by lazy evaluation (i.e. if it is safe to do so, it tries to convert lazy to strict). Unfortunately this can be sensitive to the way you write your code since if you write your code in a certain why it can pass through this phase undetected (which is why strictness annotations exist in Haskell/ETA) &gt; I don't doubt that Haskell can have great web server performance given some investment, what I do doubt is whether Eta is useful as an alternative to Scala for pure functional programming on the JVM, and whether it is worthwhile to fragment the pure functional JVM developer community further. iirc, Someone already did a benchmarking suite comparing pure fp in Haskell/Scala and basically Haskell blew Scala out of the water. I think that also Eta was also in these benchmarks, and it was faster than Scala (basically Scala was the worst). &gt; and whether it is worthwhile to fragment the pure functional JVM developer community further. We already have Scalaz-Cats and who knows how many effect system libraries. Scala is first and foremost a hybrid OO/FP language so there is no fragmentation in this regard. Even cats is using some OO concepts (i.e. point style method invocation) in their libraries. The matter of fact is, its always been the fact that if you want pure FP on JVM and don't care that much about Java interopt than ETA will fit you better, both from a ergonomics POV and also a performance POV.
Ah, right. I guess that makes sense.
Awair | Junior Scala Role| SF, CA, USA| Onsite | Full Time| *$80K - 120K* | [Job specs](https://functional.works-hub.com/jobs/Junior-Scala-Engineer-San-Francisco-California-United-States-Aug-2017-14f73?utm_source=reddit&amp;utm_medium=forum&amp;utm_campaign=quan&amp;utm_content=14f73) \-Hands on mentorship from Director of Software Engineering \-IoT product which monitors air quality \-0-2 years with interests in Scala \-$4.5M Series A funding \-Product is commercially implemented in the Salesforce building in SF Contact me at [quan@functionalworks.com](mailto:quan@functionalworks.com) 
This is a really awesome project! A native Scala DB is super sexy! Devil is the details for DBs though, good luck! Happy to chat more about your project if you're interested.
Having only worked with that setup at one place I can't speak to how common it is, but the general idea for event-sourced systems is for your persisted events to be the source of truth, with everything else creatable and recreatable by replaying those events and interpreting them however they want. You keep them in sync by only making authoritative changes through the persistent actors - no ad hoc writes the SQL db for instance - and notifying of updates. Those update don't even have to be super reliable because everything else should be viewed as a cache on top of the authoritative events. If you really need the absolute latest information, invalidate the cache and rebuild it from the events. So you could have your persisted events use a compact binary format and then on any update, after the events are persisted successfully, reindex those events in various other ways. You might want a subset of data in SQL tables for data analytics people and a different subset stored in Redis for web services.
Hey thanks. Yeah my previous experience with cluster was a system that consumed kafka queues. So there was this sense of safety in that we could go back and replay events from the queue. The actor system was never really the source of truth; there was upstream data. With my new hobby project, the cluster gets fed by actual user events posting to endpoints. So there's no initial storage beforehand. And I was stuck on wondering if I should create that simultaneously. But yeah, doesn't sound like that makes sense in the event sourcing model. I suppose you could always create a disaster recovery process by having those events emitted to a warehouse after the akka-persistence step. It sounds like this makes sense for something like populating elasticsearch too - emit to it after persistence. &amp;#x200B;
[https://www.youtube.com/watch?v=1S1fISh-pag](https://www.youtube.com/watch?v=1S1fISh-pag)
Not sure what your imports are (like, these aren't `java.io.File`s but not sure what they are). But I posted some comments here that you could try out https://gist.github.com/joshlemer/ac8cef75047c7bb699b7d75e8d4178d0
Great talk, we actually use this technique (which is called mixin composition) at work to solve other problems as well, including DI (when mixed with lazy implicits to construct your call graph for free). I like the trick that is done with \`ReaderT\` to avoid parameter passing. Like the fact that the video focused on the obvious (but probably not criticized well enough) ergonomic issues being pushed in FP. On another note, I do believe that Dotty is going to give you the ability to abstract over implicit parameters (they call it implicit functions) although I am not entirely sure if they will support currying so you can compose them different implicit parameter lists together.
&gt;iirc, Someone already did a benchmarking suite comparing pure fp in Haskell/Scala and basically Haskell blew Scala out of the water. I think that also Eta was also in these benchmarks, and it was faster than Scala (basically Scala was the worst). I'd be very interested if you could find some kind of link to this. 
I did think of two questions: 1) why call it Sway DB? 2) if internals are non-blocking, do you have a non-blocking API option? understand intent behind reusing Scala collection API, but it's not a good API, only suited for in-memory collections or building execution graph alla Spark.
Great project! Looking forward to following the progress
Just do everything in your foldLeft. Don't zip, use array access. You only actually need the number and a boolean and a bytestring. Use a mutable class rather than tuple2 in your fold left to contain your index number, and your bytestring. Now you are iterating only once, and reusing the instance in your foldLeft. Since you're not exposing the starting value to the outside world, you are still RT.
I believe this is the benchmark GP is referring to: https://github.com/fosskers/scalaz-and-cats/blob/master/README.md#sec-3
I am attending this course and I can say that there are a few problems with the course. In the first assignment they force you to throw an Exception. (NoSuchElementException). I wonder why they choose to teach scala and teach new comers to throw exceptions. That's just bad. Also the course encourages installation of IntelliJ Idea and Oracle JDK. Somewhere in the introduction it says "back pressure" is not one but perhaps the only way to design scalable systems. This is the "blue pill" from the matrix. 
Great! Looking forward to reading that post already. Cheers.
&gt;By default, Bazel spawns N different workers which allow building N targets in parallel (N is often the number of cores available) each of which compiles modules in a single-threaded fashion. Databricks’ Scala Bazel integration instead shares a single multi-threaded JVM worker process that is able to process a number of modules at once in parallel Any further information what this process is, is it is shareable i.e post on GitHub somewhere? &amp;#x200B;
&gt;If you want to stick to your FP principles, pick a native Scala UI library that agrees with you on that (e.g. Outwatch). Are there others?
Yes please, would love to chat more. My email is on my [GitHub profile](https://github.com/simerplaha) or via [Gitter chat](https://gitter.im/SwayDB-chat/Lobby) :) \- **Sway** \- It's called Sway because the internals shift/optimise data in a **smooth** flow(ish) manner and reads and writes move data back and forth without holding onto threads or stepping on each other. So "Sway" kinda sounded like the right one word description of it and to be really honest, it just sounded cool in my head :D \- **Non-blocking API** \- Yes, thinking of going with Monix ([Issue #50](https://github.com/simerplaha/SwayDB/issues/50)). I have not used it before but it compiles to Scala.js and scala-native. So it should be good for non-blocking API. Gotta do some performance benchmarks though, what do you think of using Monix? \- **Scala collection** \- Ah yes, you are totally right. I've been debating that myself for over a year now. The only reason the API is based on Scala collections is because we all (developers) already know them and they are pretty much the same in all programming languages and it makes it easy to think about structuring data using simple case classes, comes natural to us I think. I didn't want to write another query language for us to learn. [Slick](http://slick.lightbend.com/) is a great example on how just using Scala collection we can query complex data structures like in relational databases. The collection API is just a thin layer on top, if we can write a better one, let's build that out too. Having a type [Queue\[T\]](https://github.com/simerplaha/SwayDB/issues/47) is also work in progress. These APIs are just thin layers on top (in [access module](https://github.com/simerplaha/SwayDB/tree/master/access)) so we can build as many as we need. Even a SQL layer is possible. [Scala-graph](https://github.com/scala-graph/scala-graph) is an awesome project that fits into collections and since SwayDB implements collections, we should be able to integrate with it. Let me know what you think.
Thank you, sir! Any feedback is helpful. My email is on [GitHub](https://github.com/simerplaha) as well.
Haha, the only information about this bazel worker infrastructure is a single blog post back in 2015: - https://blog.bazel.build/2015/12/10/java-workers.html And then the source code in https://github.com/bazelbuild/bazel. That's what we ended up reading to reverse-engineer how to use this feature.
Are you Lee Hsien Loong's son??!! I never knew!
Function overloading is a great addition, I can think of a few use cases already for how I can use this. Keep up the great work devs. 
Cheers mate :)
I haven't coded in OOP for a long time, so treat it as a mumble :p By Strategy Pattern, I assume you are talking about DI, just in the case that the dependency is a function. There isn't much difference saying FP can do that OOP cannot, as you said. Since OOP is already equipped with the meta level tools, like DI framework. The pattern in software development hasn't changed much, they just express themselves under different environment. With adding an extra parameter, one issue I can think of is. Let's say we have 10 services/strategies we want to inject. Then we're going to have 10 parameters to every function/class that uses them. With OOP language like Java, there are two things: 1. 10 parameters is too many; 2. passing them all around is not pretty. For #1, in OOP we would wrap these in a class. Now let's consider these 10 services should be grouped in 4 ways, since we want precise control of which class can call which service. Then we'll have 4 wrapper classes. If we add a service, we'll have to change all 4 to see if they needed this extra one. Precise control probably means a lot more than 4 combinations of these services. Tagless final or John's approach with intersection type in the talk is to merge them on the fly. I counted intersection type as a FP feature since no traditional OOP language has that :) This composability of service groups gives the precise control we want. And when we add a service, we only need to add them on the call site. The call site has to be changed anyway, since we add usage to the new service. You can see this can get out of hands pretty quickly with more services, and more grouping. Unless we give up the precise control, then putting everything into one class. The issue becomes anyone can call any service in their code, which is not what we want in many cases. For example, we might want to forbid accidental access to the database in some hot code path. For #2, this passing-around overhead in FP mainly is eliminated by having typeclass/implicit parameters, or in ZIO environment, hidden in a Reader monad enabled by typeclass. In OOP, if we won't want to pass those services on every function call. Note that if we have a wrapper class as in #1, we'll still have to pass things around, we're just passing the group. What we could do instead, is grouping the functions that uses theses services into a class, and inject things as class fields. This is like a combined service module. Then the issue is, say we want to change one of them during the program flow. A dumb example I thought of is, switching from reading cache to reading DB temporarily for some data. Then we'll have to either expose setters from this "composed" service class, which is bad of course. Or having a copy constructor that creates a new combined service and calls its method, which is already very close to what FP does, and not as efficient. FP also does it in a more natural way, because that service protected by lexical scoping and things are properly cleaned up. The above hasn't even talked about the dependencies among those services... Again, just my very superficial two cents. Happy to chat more about this.
Outwatch is _the_ FP UI lib from my impression. From native ones there's also [Binding.scala](https://github.com/ThoughtWorksInc/Binding.scala) (not sure about its FP attitude) and my own [Laminar](https://github.com/raquo/Laminar) (leans towards Scala pragmatism/agnosticism rather than Haskell style FP).
Avoid "functional programming" constructs for perf sensitive code. 
The open standard scala rules has a library for this (there are several workers there: thrift, protobuf, along with scalac), you just extend this interface: https://github.com/bazelbuild/rules_scala/blob/master/src/java/io/bazel/rulesscala/worker/Processor.java Which makes it pretty easy for us with a bit of skylark glue.
Runnable is just a rigid way to say () =&gt; Unit. It's essentially a function type, except it's hardcoded to that particular function type, and it comes with limited things that work with it. I don't know what Worker is but it might be similar, a one-off function type. I mean you can call it whatever you want, but "here is a thing you can call \[with X\] \[to get Y\]" is the point exactly. So it's not really about "programming with values" vs. using Worker/Runnable. It's really (a) how *much* of your program do you want represented as a value, (b) how tedious is it to represent code as a value, and (b) what can you do with those values. In some ways, having special, concise syntax for something doesn't change the thing at all, it just makes it more likely for you to use it more often. On the other hand, when something is more accessible, it can change the way you think about things completely. (Similarly in spoken language, in a language where a concept is easier to express, it will may pervade its speakers' worldview to a greater degree.)
Not using an ide is 2019s equivalent of 'Proudly written in Notepad' on websites of 1998.
Yup thats the one, but I remembered incorrectly. They mentioned eta but didn't benchmark it.
[https://www.reddit.com/r/scala/comments/avbgar/why\_does\_scala\_not\_pivot\_to\_a\_purely\_functional/ehi70ka/](https://www.reddit.com/r/scala/comments/avbgar/why_does_scala_not_pivot_to_a_purely_functional/ehi70ka/) Is the one I was referring to BTW, one of the transformations I was talking about which ETA does that Scala doesn't is that it avoids the use of trampolines by doing program transformation. One of the reasons why \`IO\` is so slow in Scala is that in order to avoid stack overflow issues, they need to use a trampoline which is incredibly slow. Theoritically speaking Scala would not need to do this, however since it is conforming to Java's calling convention with methods its not possible for Scala to perform TCO in such situations (Scala can only do inner method TCO where it gets converted to a loop) 
You can basically use the "same" methods, since the APIs are quite mirrored. 
Oh good to know - cheers
Scala is a mature, widely-used language; realistically it's not in a position to "pivot" in a way that would invalidate existing code. Look at how much trouble the Python 2/3 transition has been; that's a much smaller change than e.g. eliminating mutable state or requiring all I/O to be explicitly sequenced. (Heck, there is already a huge amount of FUD going around about alleged incompatibilities with Scala 3 over changes that are completely trivial by comparison). If a new language were created that was a "purely functional fork of Scala", in what sense would that language be "Scala" rather than being, say, "Haskell"? If you want JVM compatibility, there are a couple of Haskell-on-the-JVM efforts. If you want strict-by-default, Idris offers that. It remains to be seen which if any of these languages will catch on. Scala has already been far more successful than most, and the priority is (rightly IMO) to preserve what has made Scala great so far rather than to make a radical change at this point.
I tried your approach but I get weird compile errors when I combine 2 different \`AsModule\` (as you called it). \`myStuff.AsModule with Clock.Live\` compiles but not \`myStuff.AsModule with another.AsModule\`. Did you run into such issue?
\*dragging self to make human-readable API\* ;) Thank you for the feedback! The goal is for my code to be useful and enjoyable to others :)
Monix is really good, highly recommend it. The creator is crazy about squeezing out as much performance as possible. The only downside is that Monix 3.x series has not been released (yet) but Monix 2.x is still really good
That's great. Should start on Monix API soon. Thanks.
Hi /u/jdegoes ! Thanks for this new approach which seems very promising and simple. I have started to play with these new concepts but I have some trouble to understand how to combine global and local modules and I don't really grasp: &gt;namely you'll have to proxy all the global requirements upward Could you give a code example? Thanks!
I needed some binary calculations recently (addition, counting number of 1s in binary, etc) and was very disappointed I couldn't find a library for that in Scala
If you only need one service, that makes sense and is a nice simplification, but if you need a lot of services, then the potential for name conflicts becomes higher (for example, 2 different config for two separate part of the applications might want a `port` property). Rather than teach people the special case, I wanted to show them the most general case that will work well even if you have lots of services. It will make code maintenance easier in the long run and is really just a small amount of code to push each service into its own namespace.
Benchmark against what? There are lots of benchmarks in the project, and you can find results on various presentations I've done. The other effect monads are somewhat comparable (modulo ZIO-specific features and better type inference) to `ReaderT[R, EitherT[E, F, ?], ?]`. ZIO will be up to 8 times faster than that, with one fewer error channel, great type inference, and no need for lifting operations or type classes. All the modern effect monads are much faster than `Future`. One could compare them with imperative Scala but it's not 1-to-1 because the features in today's effect monads cannot be obtained (at reasonable cost) with ordinary procedural code (interruption, async try/finally, lazy parallelism, etc.).
I don't understand what you mean. I do all those things fairly trivially in Scala with the logical bit operators. Can you elaborate further about your desired API calls? Maybe point our an API for an existing library example from Java, JavaScript, etc.?
Implicit parameter lists won't offer the same benefits as a built-in reader effect, but I'm _super_ excited for true intersection and union types. ZIO Environment is forward-looking to these changes—capabilities will strictly improve and the approach will be even more idiomatic under Scala 3. Also will have a nice announcement regarding Scala 3 and ZIO soon. :smile: 
&gt; Implicit parameter lists won't offer the same benefits as a built-in reader effect Why not? Could you give me examples where a built-in reader is better? &gt; I'm super excited for true intersection and union types. ZIO Environment is forward-looking to these changes—capabilities will strictly improve Interesting! Out of curiosity, do you have examples where true intersection types make a difference? How about union types?
I wasn't asking you to do benchmarks; I've watched a bunch of your presentations and seen that some benchmarks exist in the repo. However that's not really a friendly format for consuming those results compared to a webpage with a table of numbers. Basically for a project which claims "faster" I would expect it to be easy to see how much faster they are and what they're measuring against to make that claim, not just possible to get that information for the motivated individual. As to what to benchmark against, an ideal scenario would include the latest versions of the major players in the ecosystem today for each scenario to which they apply: imperative code, Scala Future, Twitter Future, Monix Task, Cats IO, ScalaZ 7 Task/IO, MTL libraries. An acceptable scenario would be to rely on transitive comparisons and compare ZIO to, say, a version of Monix that compares itself to some others and provides its own results (like [this one](https://alexn.org/blog/2016/08/25/monix-task-performance.html) from a few years ago). The benchmarks don't even have to be completely 1:1. Showing that ZIO gets within some percentage of imperative code for simpler things while also having all the extra stuff is useful. Similarly, showing what it takes to beat ZIO in more complex scenarios is also useful, especially when that comes with error-prone, non-reusable, difficult-to-maintain code. The idea here being to outline the area in which ZIO is a strong choice, along the lines of "if you're getting more complex than retrying 5 times with constant backoff where imperative code may win, ZIO gives you that and all this extra stuff", all the way through to "ZIO gives you all this stuff and gets you to this point, if you need to go faster you have to give up the niceties and work for it".
Of course I could choose a basic type and implement the operations myself but that's what a library is for, no? I would like a binary type with operations I can call reflecting the semantics for example : val bin= Binary.fromString("1001") bin.parity //should return number of 1 bin.next // should return 1010 Etc
You want to avoid all allocations in your inner loop. Currently you have 2 or 3: - The iterator returns a pair - The `xs` value is converted to an array before updating `digest` - You create a ByteString from the digest (maybe `ByteString` is a value class, then this would not count as an allocation. Generally, excessive allocations are what tends to make functional code slow. They can be avoided, e.g. by using a big `fold` as @jackcviers suggests, or by using tail recursive functions. 
I'll write up a post on this. I've got 2-3 things to really explain before things click for people. Too bad I see this only in hindsight. :)
I'll do a head-to-head comparison with IFTs at some point, and yes, fully commutative intersection types help with reader environment composition, while union types can be used for error composition. Beyond that I hope for (but have yet to fully test) better type inference and a need for fewer workarounds. Btw I would kill for `without`. :)
&gt; I'll do a head-to-head comparison with IFTs at some point Looking forward to seeing it. &gt; union types can be used for error composition Note that it [may not work](https://github.com/lampepfl/dotty/issues/5826) in the presence of parametricity in the error types. &gt; I would kill for `without` What's `without`? Something to indicate the absence of a type in an intersection? BTW, in case you missed it, contravariant intersection types have been used for some time now for a variety of things, such as Jonathan's [monadic regions](https://b-studios.de/blog/2019/01/17/even-more-lightweight-monadic-regions/), and contextual types to track variable scopes in my own Squid type-safe metaprogramming framework.
&gt; Note that it may not work in the presence of parametricity in the error types. I only need to _form_ parametric types (`A | B`), not actually match against them; in user-land code, when an error is recovered from, _they_ will know the concrete types. Do you see any issue with this limited form of parametricity? &gt; What's without? Something to indicate the absence of a type in an intersection? Yes, something like a negative constraint in row polymorphism ("does not have"). It's not enough by itself: ```scala def split[A, B, C](b: B)(f: A =&gt; C): (A without B) =&gt; C = ...` ``` ...because you can't implement the type without some notion of proxy. &gt; BTW, in case you missed it, contravariant intersection types have been used for some time now for a variety of things, such as Jonathan's monadic regions, and contextual types to track variable scopes in my own Squid type-safe metaprogramming framework. Thank you! The use of path-dependent types in Jonathan's solution is really interesting. I'll take a look at this and Squid.
Note: I would prefer replies to be in the linked thread rather than here, where applicable and possible, so as not to splinter discussion.
&gt; Do you see any issue with this limited form of parametricity? If users know all the components of the union at error-handling time, it will be fine. But it won't work well if they try to handle the `GenericError` part of `GenericError[A] | B` where `B` is not yet known, for example. &gt; I'll take a look at this and Squid. [Here's](http://epfldata.github.io/squid/tutorial/2-staging.html#first-class-variable-symbols) the relevant section in the doc. It uses the same path-dependent type intersections trick. In both approaches these are phantom types, so they don't actually correspond to runtime instances, which allows us to have strengthening (removing parts of an intersection), which happens in Squid when binding one of the required variables, and in Jonathan's work when handling an effect. But except in non-trivial cases, strengthening requires type annotations.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/linux] [Wayland McWayface (a simple wayland compositor) for the JVM](https://www.reddit.com/r/linux/comments/awx89e/wayland_mcwayface_a_simple_wayland_compositor_for/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
For general binary manipulation, see http://scodec.org.
Sorry for not responding to the comments. I found solution, Since I am using trait I have used `@VisibleForTesting` which helped me to mock trait methods. Thank you guys for your help :)
This is amazing. Offtopic tangent: how effective would this technique be for writing cross-platform 3D graphics applications? Especially for Vulkan / OpenGL APIs.
i'd need to experiment more. as far as I'm aware, jextract makes one binding for one platform, so you'd have to create a new binding for windows.
Future is a fairly simple concept that exists in other popular languages: C# (Task), Java (CompletableFuture), JS (Promise), thus having a much lower barrier to entry compared to IO monads for developers not rooting for pure FP. Tools you mention (Play, Slick, Akka) are also oriented for developers not rooting for pure FP, and Future is the simplest tool that can express asynchrony they require to be reasonably fast. I'd also say IO monads in Scala wouldn't be nearly as convenient to use if the for-comprehension wasn't designed the way it is, and if implicit resolution didn't let you have a crapload of helper methods that are universal but have good type inference. Other languages that majority of the devs in the world use don't have these features. As far as performance goes, you need to be "good enough" and Future is good enough to get started. And I've seen devs quite happy with just following the compiler advice and importing global EC everywhere. It brings you problems down the road, but it's usually way too far ahead to think twice over it when capturing market is more important I've interviewed in couple of shops that migrate from Futures to Monix, and what they do almost unanimously is to leave e.g. Akka as a communication layer, and replace all the guts with Monix Task. For Slick, they just use one of wrapper helpers (`Task.deferFutureAction`) to not propagate the future further. As to alternative systems existing, I'm currently building an app with http4s and Quill, and it's nice and seamless. But your devs need to like pure FP and/or be eager to learn, and if they're new, they would have much lower productivity on things they're used to doing the other way. That's often not a tradeoff a business would be willing to make. Some established businesses will also make a decision based on the fact that Akka/Play/Slick have corporate backing, while Typelevel ecosystem is growing on a sheer force of several key enthusiasts.
If you are just getting started, stay away from sbt. Create a worksheet in an IDE or just open REPL and fiddle with the code. I don't know what tutorial is considered good nowadays but my introduction to Scala was through Odersky's book "Programming in Scala" which I think is sufficient and sufficiently thorough for any beginner.
interesting experiment. I wonder if you'd be able to produce a native image with GraalVM. But I think you'd have to use LLVM bitcode interpreter instead of Panama, though. But I feel like you'd have to rewrite the whole thing. anyway props for this!
Thanks. Currently Im reading Programming with Scala, but so far, finding difficult just to create a Simple REST API
GraalVM aot [already has methods for hooking into native code](https://github.com/oracle/graal/blob/master/substratevm/src/com.oracle.svm.tutorial/src/com/oracle/svm/tutorial/CInterfaceTutorial.java) and I was thinking about using that and creating a GraalVM AOT version of this, but I wanted to demonstrate an actual JVM (as opposed to substratevm) application acting as a compositor first.
Good news today! :) 
Actually these are not "problems" with \`Future\` but are "missing features" and \`Future\` may have been better if the implementation supported all these. Scala Future implementation is already one of the very decent ones. Yes, it is missing these few discussed features but it will be missing 10 other things once these "features" are included.
&gt; Other languages that majority of the devs in the world use don't have these features. Officially :) Javascript's `async` is a for-comprehension in disguise: for { │ (async function() { a &lt;- myFutureA │ const a = await myFutureA; b &lt;- myFutureB │ const b = await myFutureB; } yield (a, b) │ return [a, b]; │ })();
right, that makes sense :) 
Credit Karma | Software and Site Reliability Engineers | London, Leeds, UK. Charlotte, LA, SF, US | Onsite | Full Time Credit Karma helps people make progress on their personal finance. We have over 90M users in the US on our top rated apps in both app stores. We use a ton of scala in our stack for both our ML frameworks and to enable distributed high scale data processing. We also heavily leverage typescript, kotlin, and swift. If interested, especially in the UK, reach out to me: matt (at) creditkarma (dot) com. I’m VP of engineering and currently leading our international expansion to UK :) Thanks!
It's not. Async functions/await (and generators/yield) are sugar for state machines, which makes them unusable for things like List.
I think it is fair to say that monadic \`for\` and \`async\` + \`await\` intersect when it comes down to futures. Monads are more general in the sense of applicable types (instead of the specific implementations), while \`await\`/\`yield\` are for me kind of like a \`generalization\` of Monad.
I'm new to Scala too, so far I've found these to be the most approachable: [https://www.scala-exercises.org/](https://www.scala-exercises.org/) &amp;#x200B; There's also an adapted version of Odersky's coursera class there, and I personally prefer text tutorials to MOOCs (the online class is really good btw if you like that medium). &amp;#x200B; Might not be a popular idea here, but especially when I'm new to a language, I like to fiddle with it using Jupyter notebooks. Beakerx works pretty well for that and is [cross-platform.](https://cross-platform.It) It provides Jupyter kernels for the most popular JVM languages (Kotlin, Groovy, Clojure, Scala and, of course, Java), and built-in plotting and data-manipulation libraries. It also allows you to load your favorite libraries using magics. 
You may be better off downloading the kickstarted projects as a zip file -- they have a packaged sbt and the source code, and so don't require you to have a working JGit. https://www.lightbend.com/scala
I find that they don't intersect much for it to be a useful mental model. Both are sugar for continuations of some sort, and that's where commonality ends. &amp;#x200B; \`await\`/\`yield\` tend to interop with more language control flow constructs, such as conditionals and loops, whereas \`for\`/\`do\` is just a bunch of nested flatmaps and you still need other tools (like \`traverse\`) for common operations. &amp;#x200B; OTOH, I find having a bunch of very general and common operators readily available work much better for me in FP style (compare replacing \`map\` with \`traverse\` to replacing \`map\` with await in a hand-rolled loop... ugh). Well, not like JS promises form a monad anyway. &amp;#x200B; Also if something works for less types, it's the opposite of generalization :) It doesn't have to, btw - I think all you need is to return a new state instead of mutating one internally to make it a monadic syntax, but I haven't seen a single language do this.
So a few things 1. The performance problem of `Future` will be fixed in Scala 2.13. They are introducing a batched executor service which will be used by default which should bring `Future` in line with other "Task"/"IO" type implementations. Scala 2.13 is also adding the ability to run `Future`'s on the current thread with a trampoline to squeeze out more performance if needed. The main cause of slowness is due to the fact that the default `ExecutionContext` uses a `ForkJoinPool` which is heavily optimized for fairness/balance which matters in IO/webserver scenarios, but is absolutely terrible for CPU/synchronous style code. Furthermore the other cause of slowness is the fact that even basic `.map` operations (by default) will also go through a `ForkJoinPool` which is unnecessary. You can find the PR that improves future performance here [https://github.com/scala/scala/pull/7470](https://github.com/scala/scala/pull/7470). This ontop of more control over JVM inlining can make future even faster than current Task/IO types. 2. The issue with `ExecutionContext` polluting the codebase is more of an issue with Scala not treating implicit's as first class citizens in all cases. i.e. its not really possible to abstract over implicit parameter lists, this is however being solved in Scala 3/Dotty with implicit parameter functions. 3. Regarding the existence of an `ExecutionContext`, there are valid reasons for this, its giving you control over **how** you execute a piece of business logic, this scenario comes up in two cases 1. For CPU bound computations, you generally want a `ThreadPool` that just uses native threads or a trampoline on a single thread. For IO bound computations you want a `ForkJoinPool` or something similar (such as an event loop) since you spend most of your team waiting on IO and your tasks execution time is more variable 2. To have control over where you execute something, i.e. for GUI apps. In an old place where I worked, we had 2 execution contexts, one which represented the UI thread and the other which was just generic execution context. The idea is that when you are building GUI apps, you only want to do drawing on the UI thread and do all of the business logic elsewhere. With future this is quite easy since you can do `someFuture.map(result =&gt; drawOnScreen(result))(myUIThread)`. Afaik, this can range from annoying to very hard to do with many lazy Task/IO types due to the lazy nature. There is a `Future` library for Swift where this concept is used quite a lot [https://github.com/Thomvis/BrightFutures#custom-execution-contexts](https://github.com/Thomvis/BrightFutures#custom-execution-contexts) 3. For reliability/resilience. Lets assume that you are listening on some asynchronous events and that processing these as fast as possible is the top priority, even if you don't get a lot of events. In this case you can make 2 execution contexts, one which processes your events (i.e. an execution context that uses a fixed thread. This way you can pin a dedicated thread to process your events, while using a different execution context for your other business logic. This way you are segregating your business logic into different resources. Point #2 is quite a bit disheartening, because of the current ergonomic issues with implicits in Scala people often misuse `Future` by typically mixing and matching different EC's around and then getting weird runtime behaviour/deadlocks in order to avoid boilerplate. i.e. instead of passing the `ExecutionContext` implicitly everywhere (which is what you are meant to do) and only define the source of the `ExecutionContext` where you need to (i.e. in your typical case this is the edge of your app with `import scala.concurrent.implicits.global.ExecutionContext`) unfamiliar people will instead do workarounds. The issues with referential transparency have been debated to death and are largely an orthogonal issues, the only comment I can say here is that Scala is strict by default so there is nothing wrong from a language design POV of `Future` being strict. Instead Scala has language constructs to enforce using `Future` in a lazy manner, i.e. `lazy`, `def`, `() =&gt; Future`. The only abstraction that its missing is a way to enforce that as a parameter the value received is lazy (which is not possible currently). This doesn't just apply to `Future` btw
Rewards Network | Senior Scala Software Engineer (SDET positions also available) | Chicago, IL, USA | Onsite | Full Time &amp;#x200B; Rewards Network powers the leading dining rewards programs in North America including partnerships with major frequent flyer programs, several of the nation's largest bank card issuers, and dozens of national corporations. We are looking to grow our Scala developer teams to scale out and tackle creating the rewards platform of the future by rethinking our existing infrastructure using functional programming principles. &amp;#x200B; We are looking for people who: \* Have experience with Scala (2+ years preferred), or are eager to learn \* Have experience with Docker, Kubernetes, and/or DC/OS \* Have experience working with Akka, Play, Spark, or other Scala-focused frameworks \* Have experience with Spark SQL, Cassandra, Elasticsearch, and/or Kafka (strongly preferred) \* Are familiar with and passionate about functional programming and software testing (ScalaTest, Specs2, Gatling, BDD/TDD in general) \* Are familiar with functional reactive programming (Reactive Manifesto signers get bonus points) &amp;#x200B; We are primarily hiring for senior-level positions as software engineers as well as SDETs, but we encourage anyone with Scala experience to apply. You can private message me for more details and contact information, or comment with general questions.
To be more clear, Scala `for` comprehension is closer to Haskell, i.e. it translates into `map`/`flatMap` calls. This is why it can work on `List` (i.e. if you have a `map`/`flatMap` defined on some type, you can use it in a `for` comprehension)
I found a way to increase the execution speed for-syntax operations on collections. There doesn't seem to be much interest however. [https://users.scala-lang.org/t/30-70-performance-improvements-for-flatmap/4139](https://users.scala-lang.org/t/30-70-performance-improvements-for-flatmap/4139)
Dixa Backend engineer(s) Copenhagen, Denmark We are currently looking for several on-site full-time java/scala engineers. See https://dixa.bamboohr.co.uk/jobs/view.php?id=26 for posting and to apply 
I like Scala, but that has to be the worst description of a language I’ve ever seen.
Something like https://doc.akka.io/docs/akka-http/current/routing-dsl/index.html#long-example might be a good starting point. Scala isn't rally web-first so you generally want a library to the REST part of things, and since JVM systems tend to be big applications there's not always a lot of attention on making it easy to get started. 
&gt; Scala is a machine compiled language whereas Java is Object Oriented.
The fundamental use of `scala.concurrent.Future` is to convert a callback into a pure, side-effect-free value. The next question is: how can a callback be pure? Callback-handlers that back `Future`s fundamentally need to be registered somewhere: - If someone is using `Future`s internally, and they give me a `Future` that lets me register callbacks that they have to run, can I register a bunch of slow callbacks and grind them to a half? That doesn't sound very pure if they can use the value I give them to mess me up. - If I am using a `Future`, pass it to someone else, and they cancel it before it gets triggered, they can mess up my own usage of it. That means the value I passed to them isn't pure either! That gives rise to two conclusions: - Callback-handlers must provide their own thread to run on. You cannot avoid having *some* mutable state to at least register the callback itself, but if that registered handler can immediately pawn off the execution to another thread at least the amount of "additional work" being assigned to whoever triggers the callback will be minimal. This "own thread to run on" is the ExecutionContext, and since we need it everywhere we register a callback-handler we pass it around implicitly. - No cancellation: if I have a future, and am using it, I want to be able to pass it to third-party code without having them cancel the future on me. In a reference counted environment you could perhaps see if `num_cancellations == num_references`, but this is impossible in a GCed environment like the JVM. That is the reason for the `ExecutionContext` and the lack of cancellation. Now of course there are downsides to all this - performance, implicit-ec-pollution, etc. - but at least that is the upside and motivation for doing things in this particular way. Note that the `scala.concurrent.Future.apply{...}` constructor is just a helper function that happens to implement the `Future` API in one particular way; it is not fundamental at all to what `scala.concurrent.Future` is. `Future`s can wrap any and every event-based operation, OS events, network events, callbacks, button-clicks, etc., the vast majority of which do not have a Java thread backing them.
Except it's tied to Promise
The twitter future is can be cancelled and does not take an implicit execution context, all callbacks are executed in the sanne thread, and arguably has a more convenient (if less pedantic) interface (onFailure vs failed.foreach). And it's trivial to convert to/from a Scala Future. 
Babylon Health | Software Engineer, Tech Lead, Staff Engineer, Principal Engineer | London, UK | Onsite\* | Full Time, Contract Babylon Health has got two products. The first is an AI into which patients can enter their symptoms and get back a list of conditions with confidence ratings attached for each. The second is an appointment application where patients can book an appointment and get a consultation with a doctor usually within 30 minutes (no waiting rooms, no leaving the house, etc). Babylon Health is a super fast growing company and a recent article in Financial Times revealed we are looking to secure 400m in funding. We aim to be the best and biggest healthcare technology company in the world. We are currently starting our expansion into the US market. There are three Scala teams at the company. Mostly we are using a mix of the Lightbend (akka http, akka streams) and the Typelevel stack (cats, cats effects, doobie). A staff engineer at the company is an early contributor to Scala-lang. We also have several senior engineers who are very knowledgeable about Scala. We aim for idiomatic Scala and the teams usually like a more functional style of programming. Contact information: send me a PM for more information, or contact directly the company to apply. \*I imagine that if the profile is good enough we can accommodate remote employees, but the company would prefer onsite candidates
Sorry, I don't follow. So you are saying that Future is good "if you want something that can't be cancelled" and something that, once *you* launch it, someone else can't *re-launch* it? Are those the issues people are complaining about? The fact that a Future aren't transparent? 
Excellent summary. Making use of the global EC a trigger for special attention in code reviews is a good practice.
Scala's Futures are more like Java's, which are more like Fibers: Once *have* a future, what you really have is a fiber which you can wait on. It lets you add callbacks to it, but those are just callbacks on the result: maybe the operation is already done, but maybe not. Now Java has Runnable and Callable, which are more akin to Haskell/Cats/Scalaz's IO: you *hold* them and they aren't running when you are holding them. You can then *run* them and get back a pointer (Fiber in cats, ThreadId in Haskell, Future in Java) back. You can use this pointer to wait on the result and even cancel the execution prematurely (though u can mark certain operations uncancable). In Java, you can only get a Future from passing a callable/runnable through an execution service, Scala's Future instead just forces you to pass the ExecutionContext (same idea) around so that whenever you want to make the Future, it starts running right away. Scala's Future seems to be a frankenstein of the two ideas: there is no "runnable" or "callable" or "IO" equivalent in its world, things are just created and ran on the spot. This is the main issue people have with it. Once you do `Future(a)`, `a` is already running and there's not much you can do except just attach callbacks and wait. I feel that the reason why Scala's Future was done this way is that maybe (idk, I haven't done much concurrent Java) most Java devs just create Runnables and Callables and then, right away, pass them into an execution service and continue with the Future. This practice may have influenced Scala's Future design, since that's essentially what it does: take a `=&gt; A`, pass it into a Runnable/Callable, pass it right away into the provided ExecutionContext, and return the "Future". 
That is the most common criticism as far as I know
Well Scala also has Runnable and Callable. They're called called Function0 and Function1.
lol, yeah, in a way. Well `Function0[A]` is like callable, `Function0[Unit]` is like Runnable. 
Sorry, missed an `n't` on that `are`. 
Oh I guess Callable doesn't take parameters then, sorry. Not only are they analogous, in Java 8+ and Scala 2.12+ you can write a function literal wherever such SAM types are expected. &amp;#x200B;
They are similar, but I wouldn't start using Function0 as we would use IO.
A simplistic IO monad is just a wrapper around a Function0
`async/await` can only be used with promises, so it has some limitations. Generators are a better comparison. Basically the same as above but using `yield` instead of `await`. Still doesn't give you a true "Do notation", but works well enough for a lot of cases.
Monix 3.x series is highly recommended. Even though it is still RC-2, it is as good as it gets. We have been using that version in production for months and all is well!
StashAway | Scala Backend Developer | Singapore | Onsite | Full Time &amp;#x200B; StashAway is Singapore's B2C digital wealth management platform that builds personalised investment portfolios to help consumers achieve their financial goals. [https://stashaway-jobs.personio.de/job/107905](https://stashaway-jobs.personio.de/job/107905) &amp;#x200B; &amp;#x200B;
Idiomatic code for option should be foreach or map
Its _simple build tool_, although calling it Scala build tool is acceptable too.
It never been simple.
Also note that one of the reasons why `Future` is slow (which compounds more on `Future` due to it already being slow on typical `.map` operations) is the lack of map fusion. Scala can't do this because it has no concept of purity, however Haskell does it.
To add a bit from the Scala-side: * when it comes to sources about Scala there are [free Underscore books](https://underscore.io/training/) and Sam Halliday's [FP for Mortals with Scala](https://leanpub.com/fpmortals) - much newer sources about knowledge about Scala, * from what I see in job offers while Apache Spark had a huge impact on making Scala famous not that much jobs is about Apache Spark - I would say less that 1/10 of the job offers that I see is about Spark, majority of them target Lightbend ecosystem (Akka, Play) and some adds Cats or Scalaz to them, * Apache Spark is mostly present... on StackOverflow questions about Scala. IMHO it's because other parts of Scala causes less questions and because Spark is used more by Data Scientists, BI and other people who are not necessarily seasoned BE engineers, * I've found Scala offers on sited more concerned about remote work/better information about the job/FP-oriented, e.g. https://nofluffjobs.com/ or https://functional.works-hub.com/ - while not Scala centric there are a lot of Scala offers there * libraries description skipped Scalaz - I am not a Scalaz fanboy (I am pretty far from it) but we should give credit where credit is due and it was Scalaz which brought more FP into Scala. I am sure similar things could be added about Kotlin when it comes to use cases, sources and job offers.
It's simple like the Democratic People's Republic of Korea is democratic. But I have to say it grows on you. 
Well, most general method is fold, but type inference is so bad that sometimes it's simpler to write pattern matching on it.
You are right, i meant that async/await in context of specific data type (for example C# Task) allows one to mimic "monadic" for/do but as you pointed out it can interop with loops etc. (I also agree that explicit traverse combinator is better)
It initially stood for _simple build tool_, by now it's just called sbt.
Hi! (Disclaimer: I am one of the original authors of Scala Future &amp; Promise, as well as maintaining them since being included in the Scala Standard library) Thanks for your question! Let me try to address the complaints one by one below: &amp;#x200B; * referentially opaque * Futures are not intended as a Task-like structure which describes a set of instructions. It is a coordination construction which allows to temporally decouple parts of an application. A Future is a value detached from time. There is nothing preventing or discourages anyone from having Task-like abstractions produce or consume Futures. &amp;#x200B; * non-cancellable * You can read the full explanation of this "omission" here: [https://viktorklang.com/blog/Futures-in-Scala-protips-6](https://viktorklang.com/blog/Futures-in-Scala-protips-6) In short: Having cancellability as a standard feature leads to defensive programming and/or potentially buggy systems. * slow * The notion that Futures are slow many times arises from developers doing transformation in asynchronous steps instead of running synchronous chains of transformation on the Try rather than on the Future. So instead of writing \`future.map(someFun).filter(somePredicate).recover(someRecoveryFun)\` you can instead perform those operations directly on the Try like this—\`future.transform(\_.map(someFun).filter(somePredicate).recover(someRecoveryFun))\`. Read more about that here: [https://viktorklang.com/blog/Futures-in-Scala-protips-5.html](https://viktorklang.com/blog/Futures-in-Scala-protips-5.html) * That said, overall performance is going to be \*a lot\* faster in 2.13.x: [https://github.com/scala/scala/pull/6610](https://github.com/scala/scala/pull/6610), [https://github.com/scala/scala/pull/7470](https://github.com/scala/scala/pull/7470) , [https://github.com/scala/scala/pull/7663](https://github.com/scala/scala/pull/7663) &amp;#x200B; * polluting the codebase with its ExecutionContext. * The reason for having ExecutionContext is to provide determinism as to which threads will be executing what logic since there is a temporal decoupling in Futures. If the logic submitted to a Future transformation (such as, but not limited to: map, flatMap, transform, recover, etc) would be executed by the calling thread, then execution would have to halt for the current thread until the Future had received its value, apply the transformation, and then continuing. Clearly having such blocking constructs would defeat the very purpose of temporal decoupling. The alternative would be to have the transformation executed by the completing thread (the thread which will provide the Future with its value), but that can be highly non-deterministic—sometimes it would have already been completed, in which case the logic would execute on the calling thread, and sometimes it would not have been completed yet, in which case it would execute on the completing thread. Neither the completing nor the current thread might be the most appropriate thread to execute the logic on, as the completing thread will be managed by another section of logic which might be highly sensitive to increased, uncontrolled, workloads—and the current thread is unknown in many cases since it depends on what thread will call it in the first place. This leads us to the current design: where the place to execute the transformation is implicitly injected for each transformation—leading to a much more deterministic reasoning about what executes where. &amp;#x200B; You can read more about strategies to structure logic for ExecutionContext injection here: [https://viktorklang.com/blog/Futures-in-Scala-protips-1.html](https://viktorklang.com/blog/Futures-in-Scala-protips-1.html) &amp;#x200B; * Here are some links for further reading about Scala Futures and Promises, enjoy: * The original SCala proposal for Futures &amp; Promises (SIP-14): [https://docs.scala-lang.org/sips/completed/futures-promises.html](https://docs.scala-lang.org/sips/completed/futures-promises.html) * Futures in for-comprehensions: [https://viktorklang.com/blog/Futures-in-Scala-protips-2.html](https://viktorklang.com/blog/Futures-in-Scala-protips-2.html) * Scala Futures vs C# : [https://medium.com/@viktorklang/hi-eef4acf316a8](https://medium.com/@viktorklang/hi-eef4acf316a8) * Tons of more background &amp; tips ‘n’ tricks: [https://viktorklang.com/blog/](https://viktorklang.com/blog/) Cheers, √
Whoops. Sorry about that. I'll get that fixed.
Scala build tool is fine, it originally stood for simple build tool but that's no longer used on the website https://www.scala-sbt.org/
Thanks. This, as well as other comments in this topic, is exactly the kind of insight I was looking for.
It was a simple build tool in its 0.7.x versions, a long time ago. Then it got redesigned from the ground up and suffered [second system effect](https://en.wikipedia.org/wiki/Second-system_effect)
**Second-system effect** The second-system effect (also known as second-system syndrome) is the tendency of small, elegant, and successful systems, to be succeeded by over-engineered, bloated systems, due to inflated expectations and overconfidence.The phrase was first used by Fred Brooks in his book The Mythical Man-Month, first published in 1975. It described the jump from a set of simple operating systems on the IBM 700/7000 series to OS/360 on the 360 series, which happened in 1964. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
The new implicit keywords make a lot of sense and really help breakdown the different use cases of implicits. I should try them out sometime soon. 
Haskell is constantly changing, too. Haskell is a language mainly designed to contain the latest programming language research results, so it could be changing even faster than Scala. There is a more stable and industry-oriented fork of Haskell, called Eta: [https://eta-lang.org/](https://eta-lang.org/) If you learn Scala first, you will have a more gradual approach. If you learn Haskell or Eta first, you will be forced to start thinking the functional way immediately.
Copying my comment from /r/programming: The original talk referred to by the article is very interesting and I enjoyed watching it. I couldn't get it out of my head, however, that John wasn't being totally honest in his presentation of this new design. The entire idea that it makes TF "dead" is just silly because during the presentation he purposefully tries to make it look as ugly and unusable as possible. In particular the example he gives (as pointed out by this article) where it shows a function that takes in a type parameter that has like 20 different constraints is straight up hyperbolic. Of course, given that John has taught Scala for many years I have no doubt in my mind that he has actually seen code that terrible in person, but as the article states that's just not what you should be using TF for in the first place. I would highly recommend watching the original talk followed by reading this article because it very clearly points out that not everything is a silver bullet. There are lots and lots of ways to design your programs and they all have their pros and cons. ZIO looks great and I'm sure one way or another these developments will have a positive impact on the Scala developer community. I for one am really grateful that there are some great effect libraries out there right now and I appreciate any attempts to simplify writing these programs.
I feel like a good compromise could be to use Tagless Final for the effect wrapper and its true type class instances, and implicit function types (IFT) for the capabilities. Indeed, one can easily abstract over IFTs, so it seems they would solve John's main complaints. All the following works in the latest release of Dotty: def foo0[F[_]: Monad]: given Logging[F] =&gt; given Console[F] =&gt; F[Int] = for { a &lt;- Logging.log; b &lt;- Console.con } yield a + b Abstracted as: type LoggingConsole[F[_],A] = given (Console[F]) =&gt; given (Logging[F]) =&gt; F[A] def foo1[F[_]: Monad]: F LoggingConsole Int = for { a &lt;- Logging.log; b &lt;- Console.con } yield a + b Where I have defined: import cats._ import cats.syntax.MonadOps._ import cats.effect.IO abstract class Logging[F[_]] { def (self:Logging.type) log: F[Int] } object Logging { implicit val IO: Logging[IO] = new { def (self:Logging.type) log: IO[Int] = cats.effect.IO{ 42 } } } abstract class Console[F[_]] { def (self:Console.type) con: F[Int] } object Console { implicit val IO: Console[IO] = new { def (self:Console.type) con: IO[Int] = cats.effect.IO{ 666 } } } Moreover, we can abstract further and have some fun! Let's generalize this abstraction capability to an abitrary type-level list of effects! type ::[H[_[_]], T[_[_],_]] = [F[_],R] =&gt; given H[F] =&gt; T[F,R] type Nil[F[_],R] = F[R] type ![F[_],T[_[_],_]] = [R] =&gt; T[F,R] type |-[F[_],R] = F[R] So we can now write: def foo2[F[_]: Monad]: F ! Console :: Logging :: Nil |- Int = for { a &lt;- Logging.log; b &lt;- Console.con } yield a + b Or, abstracted: type MyEffs[F[_]] = F ! Console :: Logging :: Nil def foo3[F[_]: Monad]: MyEffs[F] |- Int = for { a &lt;- Logging.log; b &lt;- Console.con } yield a + b Of course, IFTs commute (locally), so we can specify them in any differing order, as in: val bar: IO ! Logging :: Console :: Nil |- Int = foo3 
Sorry, just saw the message now. Can you share a full snippet that fails to compile ? 
If you want to be good at Scala obviously you have to learn Scala. It is not a matter of which language to learn, it is a matter of which problems you are going to solve. An abstract question like this does not make much sense. Knowing a programming language is not a badge you can wear on your lapel. As for the length of learning, most people either learn all their lives or move on to entirely different things.
Great update! 
 [cedarlakeventures.com](https://cedarlakeventures.com/) | Full Stack Web Developer | Orono (Minneapolis area), MN, USA | Onsite | Full Time ## About Cedar Lake Ventures, Inc. is a software company based in Orono, Minnesota, that has been delivering innovative online and desktop software products for more than 10 years. Our primary focus is on advanced proprietary image processing algorithms delivered and sold as subscription-based web applications (SaaS). We are a small and nimble team that offers great potential for gaining broad experience and assuming greater responsibilities over time. We offer competitive benefits, paid time off, paid holidays, and a regular 40-hour work week. Employees are not expected (or allowed) to work from home. ## Description We are looking to a hire a Full Stack Web Developer to work out of our Orono office near Lake Minnetonka. Areas of responsibility would include: * Develop and improve existing rich-client web applications using Scala, Play Framework, HTML, Javascript, Typescript, Canvas, Web Sockets, and in-house technologies. Our existing customer-facing web applications are image processing tools, but future work may relate to other topics. Likewise for related APIs. * Develop internal and contractor-facing web applications and management systems to collect and annotate data to train machine learning systems. * Work with UI/UX Designer and Digital Marketing Specialist to publish lightly-interactive and static educational and promotional content. Work with Digital Marketing Specialist to configure Analytics to track marketing campaign efficacy. * Manage AWS resources and production operations: production cluster management, instance reservations, compilation servers, and our deployment system. * Monitor third-party OS &amp; framework dependencies and ensure timely updates. * Develop and improve in-house framework and back-end systems, improving functionality and speed of components while enforcing code and design consistency. Existing components include Scala-based HTML, CSS, and SQL DSLs; a federated user management system shared between our sites; credit card and PayPal-based common billing system with support for one-time and subscription-based payment modalities; and a system to manage seamless deployments of new code with zero downtime. * Manage bug reports and fix actual bugs on all sites. Interface with front-line customer support regarding bug reports and functionality inquiries. Employee will be furnished with an electric sit-stand desk and a high-performance workstation with the employee’s choice of operating system. ## Qualifications * Languages: Scala developer with 3+ years of professional experience. Experience with Java, HTML, Javascript/Typescript/ScalaJS, and SQL also very helpful. * Other technologies: Linux, AWS infrastructure, GIT, Play Framework. * Must be willing to learn new languages and frameworks. * Bachelor’s Degree or higher in relevant field. We are looking for a self-directed self starter who can juggle a number of potentially unrelated concurrent responsibilities, and who will actively seek to take on new tasks as time permits. This is not a good position for someone who requires frequent active management to stay productive. Applicants must be authorized to work in the United States. ## Disclaimer The details above are not set in stone and are not a promise as to the nature or scope of the job. They are intended to help you decide if this sounds like a good fit that warrants submitting your resume for consideration. Job responsibilities are at your manager’s discretion and will probably change over time. ## Contact Please email your resume to [jobs@cedarlakeventures.com](mailto:jobs@cedarlakeventures.com)
Thanks for the great post! &gt;The notion that Futures are slow many times arises from developers doing transformation in asynchronous steps instead of running synchronous chains of transformation on the Try rather than on the Future. I actually wasn't aware of this, but its annoying its designed this way because it doesn't really work in `for` comprehensions, i.e. `for` comprehensions is sugar for `map`/`flatMap` and `map` as you pointed out is the slow operation. I don't think there is any way to fix this cleanly without breaking the `Future` api. 
That's not quite right. Haskell-the-language is defined in the Haskell Report (specifically the 2010 one), which rarely changes. GHC, the most popular compiler, implements Haskell as specified in the report, plus some extra stuff which is hidden away behind compiler flags. Many of these language extensions are widely enough used to be considered more-or-less part of the de facto language, while others are more experimental, and won't be encountered often in the wild. Eta is Haskell for the JVM. As far as I know, they aim to include at least the commonly used extensions in addition to what's specified in the Haskell Report. Eta is still fairly new - barely production-ready.
Java library compatibility?
As an aside, I've written a `dlof` helper function just to reverse the parameters of `fold`, and it works better with type inference! I don't know why it wasn't done this way from the start.
I would say the biggest difference between Scala and Kotlin is not necessary their features, or the exact implementation details, but the spirit of the languages. Kotlin is - always has been - aiming to be a better Java. Whereas Scala is aiming to be the best fusion possible between OOP and FP in a typed setting. &amp;#x200B; I would recommend Kotlin for those who just want to use a better Java and use Java tools. I would recommend Scala for those who like a more functional approach than Java, but don't want to go full Haskell.
IMHO pattern matching is much more readable, especially for not-so-proficient developers.
What is your goal? &gt; want to be good at Scala This isn't specific enough.
I agree
Domain of problems to solve: Big data and distributed computing
want to be good at Scala for Big data and distributed computing
The new implicit design looks great! Is this here to stay or this just in the current release candidate to get some feedback?
&gt; but don't want to go full Haskell Sounds almost like a set phrase. "How's Jack?" - "Forget about him, he's gone full Haskell."
Do I see this correctly that those capabilities might leak (e.g. via a closure)?
This is my favorite competition yet! Keep it coming you all. Good stuff!
All features need to go through the [SIP process](https://docs.scala-lang.org/sips/) before they're officially part of the language, that hasn't happend for the implicit stuff yet, so it's not fully settled and feedback is welcome (preferably on https://contributors.scala-lang.org/), but we'd like to settle on something sooner rather than later.
It depends. Do you want to learn Scala or functional Scala? If you’re interested in Scala because of Akka, Spark, Play, etc, the answer is no. If you’re interested in the more functional side of Scala, while it is possible to learn it without learning Haskell, there’s a benefit to learning some Haskell basics to more easily understand some of the things that may seem a bit awkward done in Scala. I work with functional Scala, and every time I have to help someone learn some of the concepts needed to understand these functional codebases, I invariably end up having to use Haskell examples to help (and these are usually people without Haskell experience) with some of the concepts.
So happy to see top level definitions! Been annoyed by that wart many times. I fully support separating the use cases for implicits into core concepts. Curious how these will be handled during migration. I don't really care for `Eql` as a name but I love opt-in multiversal equality. Dotty just looks better and better to me.
&gt; If you’re interested in Scala because of Akka, Spark, Play, etc, the answer is no Thanks. Why is that? Is functional part not helpful to them?
This might answer your migration question: &gt;[Implementation Status and Timeline](https://dotty.epfl.ch/docs/reference/contextual/relationship-implicits.html#implementation-status-and-timeline) &gt; &gt;The Dotty implementation implements both Scala-2's implicits and the new abstractions. In fact, support for Scala-2's implicits is an essential part of the common language subset between 2.13/2.14 and Dotty. Migration to the new abstractions will be supported by making automatic rewritings available. &gt; &gt;Depending on adoption patterns, old style implicits might start to be deprecated in a version following Scala 3.0. [https://dotty.epfl.ch/docs/reference/contextual/relationship-implicits.html](https://dotty.epfl.ch/docs/reference/contextual/relationship-implicits.html)
Thanks! I figured there'd be something out there about it. I haven't even finished going through the full github issues yet.
You only benefit from knowing Haskell, for learning Scala, if you’re doing things like using higher kinded types, type classes, final tagless or mtl style of coding. When using frameworks like the ones I’ve mentioned above those things are very rarely used.
I’m not sure that is true. What you’ll find is different types of codebases. Some people are using Scala as an OO language, some people use it as a functional language. Some of the people having no understanding of the other paradigm see the approach of using that paradigm as uneeded complexity, but that is not the case, it’s just different ways of doing things. Just pick the one you’re interested in, and stick with it.
(You should state what is your background, and in which context you consider using Scala) Java-style imperative programming has its own artificial complexities. I'm more familiar with Scala FP than with annotation-based Java frameworks, so understanding how the moving parts of the Java Microbenchmark Harness compose is really painful for me (example: the interaction of state and parametrized benchmarks). Some complexities in Scala come from the fusion of OOP and FP. I don't find them irritating, considering I can move freely from imperative to functional constructs in the same codebase (which is important when doing numerics).
&gt; Knowing a programming language is not a badge you can wear on your lapel tell this to recruiters hahaha...
&gt; The entire idea that it makes TF "dead" is just silly because during the presentation he purposefully tries to make it look as ugly and unusable as possible. Are you arguing that your examples are somehow more readable?
This is awesome! I've always wanted to build a chess engine in Scala and never got around to it. I'll just copy yours, haha. So you made the UI in Scala also? Where's the build stuff?
The UI is written in Scala. But since it uses JavaFX, it contains lots of side effects. I didn't use ScalaFX because I'm not so sure of its documentation and if it can keep up with the updates on JavaFX. (And I started this project on a cave, where I couldn't download many third-party plugins.) There are no releases/builds/jars yet. The repository simply houses the source code for now. I'm planning to release soon if time will permit.
My biggest problem with John's solution is that using mixins to create contexts violates the single responsibility principle. `trait FooBarBaz extends Foo with Bar with Baz` means that `FooBarBaz` really ISA `Foo`, and a `Bar`, and a `Baz`, and any such object built out of three independent modules like that will absolutely have terrible cohesion according to the LCOM4 OOP cohesion metric. It is an abuse of inheritance, much more so than tagless final encoding is an abuse of typeclasses. Give me tagless final encoding as is in scala until scala 3, then let me rewrite them in terms of implicit function types, which suffer from neither the abstraction issue nor the cohesion issue.
I meant build like... It's not obvious from GitHub if you used `sbt` or some other build tool.
I didn't use `sbt` because I was mostly offline (and I haven't tried `sbt`'s offline mode). However, I will have internet access again in a few days/weeks, so you can expect it to be added as well.
I'm probably super late to the party but does anyone else not really like the way this looks? ``` implied ListOrd[T] given (ord: Ord[T]) for Ord[List[T]] ``` I don't know that I have an alternative.
Never mind, I got it to work. Thanks anyway!
Professional developer for over 20 years. I came at Scala from having used Haskell on personal projects. Haskell is much, much cleaner and nicer to learn pure FP. There isn't any other way to use it. Scala is really multi-paradigm and you can decide how far down the FP spectrum you wish to end up. If you want to get heavily into FP I would suggest Haskell first, it is easier than Scala since it has a better type system and you will write less boilerplate. Lots of the libraries you will find will have counterparts in the FP Scala world. If you want to use more OO subset of Scala then there isn't much point looking at Haskell first from a pragmatic point of view, you may as well jump in to Scala. 
I agree that the english meaning of `for` seems off 
I had a quick look, and find your code clear and nicely commented. We need more open source examples of that kind!
How so? A is subordinate to B. A for B (‘s context). It’s not *great*but it’s okay.
If you want a job using either language, then in my experience Scala is the one to pick. In the UK neither language is mainstream, but there are far more jobs doing Scala (and if you are desperate you can land a Java or Kotlin role with Scala experience). Learning depends on your past experience. If you come from a C style syntax background (Java, JavaScript, etc) then Scala will be easier to pickup. You can learn the language as a "better/immutable Java" first, then incorporate parts of pure FP.
&gt; The entire idea that it makes TF "dead" is just silly because during the presentation he purposefully tries to make it look as ugly and unusable as possible. I haven't watched John's video, but I've seen that tactic used by so many libraries and languages. e.g. Use technology X in a bad way (that no-one uses) to demonstrate why technology Y is better.
"given" reminds me a lot of "=&gt;" in Haskell. Seems like a good idea.
Yeah that's more or less exactly what happens. To be fair though he's only trying to sell a particular implementation of this idea, not that this idea can only be done by ZIO. He mentions that the techniques they use there are generally applicable and are not some kind of private solution. It's just using the regular features of Scala in a different way than usual (well, not *too* different, something something "cake pattern"...)
The popular chess site Lichess is built in Scala: [https://github.com/ornicar/lila](https://github.com/ornicar/lila) Just the underlying chess engine, no UI is here: [https://github.com/ornicar/scalachess](https://github.com/ornicar/scalachess) . "It is entirely functional, immutable, and free of side effects"
You're not wrong. 
Do you want to get paid, or are you shooting for some "Zen" of Functional Programming? The only reason to learn Haskell is to achieve some personal satisfaction of knowing Haskell and seeing how "pure" your Functional Programming can be. Scala pays. It sucks, but it pays.
I'd recommend looking into Frameless if you are interested in functional programming and big data.
It's not really how I meant it. Scala is relatively pretty close to Haskell, compared to basically every mainstream language. Yet still, it is pretty far away and I think there is a spot for people who want to do mostly functional programming without committing to it fully, what Haskell requires. &amp;#x200B; Personally I choose Scala because it is the only language that can get me a job and where functional programming is not an afterthought, or a second class citizen.
Makes sense, thanks for your answer :)
Wholeheartedly agree that `implied ListOrd[T] extends Ord[List[T]] given (ord: Ord[T])` is much beter and more comprehensive notation.
Trying out Binding.scala and can't seem to find an idiomatic way of binding node to the future that doesn't exist yet. E.g. i have a button and div, button triggers ajax call and result needs to be rendered into div. Two solutions i found were: * have a Var that div's rendering is based on and update its value in OnComplete of Future (really don't like that approach) * do `dom.render(document.getById("divId", renderDiv(doServerCall())))` to forcefully change dom's fragment to new one, but that seem to completely miss the point of having bindings in the first place
&gt; abstract class Logging[F[_]] { def (self:Logging.type) log: F[Int] } Can you explain why the function name and the parameter list are exchanged here? Is this some new syntax in Dotty?
I don't know if the new syntax is a good or bad idea. I admit that the current syntax may look cryptic for those who haven't done some logic programming. The new syntax is clearly more self-explanatory but the subject is inherently hard. In the end I think it is a good idea because it will help beginners to see the purpose of the technique but it may make more difficult realising it's just implicits. 
You can put a for-comprehension inside of your future.transform operation: for { v1 &lt;- Future(1).transform { r =&gt; for { r1 &lt;- r if r1 != 2 } yield r1 } v2 &lt;- Future(v1 + 2) } yield v2
Yes, it's using Dotty's extension method syntax, and [my 'hackoding' of nullary extension methods](https://github.com/lampepfl/dotty/issues/5694) leveraging it :\^D
Yipp, this is a new syntax for dedicated support of extension methods. :) See: [https://dotty.epfl.ch/blog/2019/01/21/12th-dotty-milestone-release.html#whats-new-in-the-0120-rc1-technology-preview](https://dotty.epfl.ch/blog/2019/01/21/12th-dotty-milestone-release.html#whats-new-in-the-0120-rc1-technology-preview)
If he’s new to all of this, I’d be wary to recommend that as then he would be learning Scala, FP, Frameless and Spark all at the same time.
`extends` is an object-oriented term. It doesn't make much sense for type classes. I'd drop `given`, replace `for` with `:`, and change `implied` to `impl`. I'd also replace `extends` (on classes/traits/objects) with `:`, for consistency with how type annotations are written. So: impl ListOrd[T] (ord : Ord[T]) : Ord[List[T]] {…} class ::[T] (val head: T, val tail: List[T]) : List[T] {…} You might also want a shorthand for an anonymous typeclass implementation with no type or value parameters, in which everything up to the colon is omitted: impl Ord[Int] {…}
I think it's cute 
Both, and pay may be more important. Why "it sucks"?
&gt; Tagless Final for the effect wrapper and its true type class instances, and implicit function types Tbh I would still call that Tagless Final. Tagless final means just two things: - Represent your DSL as functions (the language _is_ the interpreter) - Use a type parameter to represent the result You can do it (and it has been done) with typeclasses, records, ML-style modules, or indeed implicit function types
impl can mean both implements and implies, so it's a bit overloaded 
Even with artificial complexity aside, Generics increases the complexity for Scala regardless whether you are coding in OO and/or FP styles. In Java, you can do a lot more before you have to deal with Generics. And it is not helping that Scala supports both use-site and declaration-site variances whereas Java only support use-site variance. Refer [here](https://schneide.blog/2015/05/11/declaration-site-and-use-site-variance-explained/) for clarification if needed. 
Scala syntax is very concise for using it as a better Java. If you write pure functional code in Scala it will most likely end up more cluttered and verbose than the equivalent code in Haskell. Plenty of companies use Scala for their backends, while relatively few use Haskell, meaning that there are more Scala jobs and your skills are easier to transfer.
If that is your use case and you are new to Scala, I would advise you to learn Scala and none of Haskell. That alone takes a lot of time because Scala is very complex. There is a sub-community in the Scala community. They are pure-FP evangelists (beware, this sub-community is somewhat overrepresented in this subreddit). Learning the pure FP style Scala takes even more time and requires you to read like two books more. Stay away from that in the beginning. Learning OOFP Scala: 1-3 months Learning pure-FP Scala: 2-5 months extra What you need for big data / distributed computing is the OOFP part. 
To your point, `impl` already means "implementation" in Java land.
It's a Pingphant! Unofficial Scala mascot :)
Understand you can do this, but its still really unergonomic compared to the typical for comprehension flow, i.e. I am talking about something simple like this ``` for { result &lt;- someHttpRequest() something &lt;- result.split("/") } yield something ``` This all gets resolved to map calls (which are really slow on `Future`)
AFAIK they can "escape" similarly in other approaches to Tagless Final. They can even escape in John's system as far as I can tell, by using ZIO's `provide` method, defined as: sealed trait ZIO[-R, +E, +A] { ... def provide(environment: R): ZIO[Any, E, A] = ... ... } I'm not sure there's any plan for preventing that.
Thanks. What books to read for OOFP part, and what for pure-FP part?
Why the concat(....) spam in the http routes section?
Are we using typed actors, yet?
Wish their samples were in Scala rather than Python https://github.com/Azure/mmlspark/tree/master/notebooks/samples 
First they are artificial complexities, next you can’t live without them. Scala is complex not because of the language itself, Scala is complex because there are many valid patterns available for developers. You can perfectly understand one code base, say Spark, and at the same time don’t understand a thing about another, say Slick. Usually languages have “one preferred way to do something” - not the case with Scala. Scala has 3 major groups of developers: * Scala as a better Java * Scala as a functional language * Everything in between: FP mixed with OOP Therefore required time investments which will get one to the point of fluency can be easily doubled in comparison with some other language.
Pure FP Scala is surely more cluttered than pure FP Haskell, but Scala doesn't always lose on concision, e.g. Haskell's database libraries are very high on boilerplate compared to Scala, no Haskell library comes close to any of Slick, Quill or Doobie.
&gt; Is Haskell better designed and more stable, whereas Scala is constantly changing (remove old features, add new ones or change existing features),and keep borrowing ideas from Haskell? It's precisely the opposite. GHC Haskell is in a weird limbo where at the same time there's no real backwards compatibility and you can expect your programs to stop compiling in 6 months, but at the same time the cruft of a decades old standard library and 100+ language extensions impede progress, learning and simply leave a lot of footguns to be constantly aware of. I'd still advise to learn Haskell first if you want to go the FP route, though.
Hey, thanks for reaching out. Which examples are you interested in using in Scala? Almost all of the code and tests are in scala, you just have to navigate to the right tests folder and you will see many different unit tests which cover a lot of the APIs. Feel free to reach out to our support alias for any questions you might have!
&gt; Which examples are you interested in using in Scala? Any example with DNN (Say Image classification) All using Scala libraries with no python stuff. 
When I look at such programming comparisons which compare programming languages based on jobs, salary, subscribers to reddit, trends on stack overflow... I say to myself there is no lack of charlatans in our industry,. 
&gt; Any example with DNN (Say Image classification). Also the ability to interest with the model using REST (without using any python). That will be an awesome sample to have. Sounds good well try to add some of these for you. 
What an absolutely awful name 
i didn't choose it
Right. In this case, it declares an implementation of a typeclass.
That's intentional. `impl` declares an implementation of a typeclass, which is available implicitly.
Let's put things into context: On my MPB, using the current 2.13.x global EC, I'm getting almost 50 million Future.maps per second—so we're talking 20ns per Future.map? Given that, what does "really slow" mean?
Artificial complexity is introduced by the developer who doesn't follow the principle of least power. http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html 
I agree with you in this part, I mean in the realm of things its not an issue. I am just making two points here 1. The situation is worse when you cascade a lot of maps, I did a trivial example before but if you do a lot of consecutive map operations it adds up. I mean putting into context, this was slow enough that akka-http had to make their own `FastFuture`. Its also caused other "competing" Async/Task/IO types to put `Future` in a bad light (even though I agree they are missing the forest from the trees in such benchmarks and in some cases the people that released such benchmarks are arguably being deceptive). 2. Which then leads to this point, which is simply that it didn't need to be this way.If there was some reason for it being this way then yes it could be justifiable, but there is no reason for `.map` to go through an `ExecutionContext`. Typically the operations that you wan't to go through an `ExecutionContext` are ones that are long lived, variable in execution time and/or non blocking (i.e. http requests, database transactions etc etc). Furthermore for those kinds of operations, they are likely going to be a `Future` (or some equivalent) which means you are going to be composing them with `flatMap` and not `map`. This only leaves slow synchronous computations, and considering that in 95%+ of the cases the `.map` operations are incredibly fast it would have made more sense to have a `.map` that doesn't take `ExecutionContext` and another version of `map` for those corner cases where you would want to pass it into an `ExecutionContext`. 
Hey there I think this must be referring to some other software or service that is also called Scala. This is a subreddit about the programming language called Scala (www.scala-lang.org).
Hey, I am making a small Scala.js program. I need an functional reactive programing(FRP). [ZIO's](https://github.com/scalaz/scalaz-zio) Asynchronicity and Dependency Injection(by cake pattern module provide) may help me. Is it possible to FRP on ZIO?
Doesn't current implementation of `transform` require `ExecutionContext` as well?
Regarding point 1, I think the best course of action, if indeed a driver was to work around Future perf problems would have to be to open an Issue and start collaborating in fixing it. :) I'm glad to see perf in 2.13 having become as good as it has and I see little-to-no opportunity for further optimizations at this point in time. &amp;#x200B; Regarding point 2, I answered that in my reply above: &amp;#x200B; &gt;The reason for having ExecutionContext is to provide determinism as to which threads will be executing what logic since there is a temporal decoupling in Futures. If the logic submitted to a Future transformation (such as, but not limited to: map, flatMap, transform, recover, etc) would be executed by the calling thread, then execution would have to halt for the current thread until the Future had received its value, apply the transformation, and then continuing. Clearly having such blocking constructs would defeat the very purpose of temporal decoupling. The alternative would be to have the transformation executed by the completing thread (the thread which will provide the Future with its value), but that can be highly non-deterministic—sometimes it would have already been completed, in which case the logic would execute on the calling thread, and sometimes it would not have been completed yet, in which case it would execute on the completing thread. Neither the completing nor the current thread might be the most appropriate thread to execute the logic on, as the completing thread will be managed by another section of logic which might be highly sensitive to increased, uncontrolled, workloads—and the current thread is unknown in many cases since it depends on what thread will call it in the first place. This leads us to the current design: where the place to execute the transformation is implicitly injected for each transformation—leading to a much more deterministic reasoning about what executes where. &amp;#x200B;
It does. But Try operations do not.
Requires you to create an account even to read the article. downvote. 
I'm pretty agnostic as far as monads go, but there are def worse choices than Monix (I'm a fan of tagless-final). You can also create your own monad like DBIO. My bigger advice would be to stay unopinionated and don't over commit to any external dependency so that you can pivot the API later.
\&gt; Salary 100K - 110K CHF That's ***insanely*** low per Zurich standards