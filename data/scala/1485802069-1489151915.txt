&gt; Akka is what was used to build spark Spark removed core dependencies on Akka quite a while ago https://issues.apache.org/jira/browse/SPARK-5293 
I just upgraded a scalatra service. It was pretty painless, however we use an unsupported gradle-scoverage plugin, which no longer works.
Which is why I said it has cost me a lot.
spray isn't going to get a 2.12 version. It's a dead project.
While this is definitely an interesting subject, I still can't figure how this would fit in the generic library model. It looks like what you are proposing is more related with the case class(es) and table(s) definition. Something in the lines of: case class Person(name: String, age: Int, eyeColor: EyeColor) case class PersonRecord(id: Int, details: Person) class Persons(tag: slick.lifted.Tag) extends Table[PersonRecord](tag, "PERSON") { def id = column[Int]("ID", O.PrimaryKey, O.AutoInc) def details = column[String]("DATA") def * = (id.?, details) &lt;&gt; ((PersonRecord.apply _).tupled, PersonRecord.unapply) } And then having an implicit Slick converter that would handle the PersonRecord "details" field conversion - from BLOB to type Person and from type Person to BLOB. The definition is pure vanilla Slick. If you find yourself repeating such a pattern a lot, maybe you should abstract this key-value configuration. This would not stop you to use the generic repository library for all the common database operations - save, update, findOne, etc. The (de)serialization would be provided by the vanilla Slick implicit converter. I think that trying to embed such a key-value definition in the generic repository would result in a somehow opinionated model. 
A nice clean example implementation - thank you.
That was an insult and I don't understand what was it for. I know that the development Play 2.6 is cross-compiled to 2.12, we are probably not speaking the same English given that wasn't my advice, but then I don't really care about what those of you with "real jobs" can afford or not.
I love monix and always read anything you write. I find it easier to reason about and use than akka streams, and am trying to get the contractors we use to switch over to using monix instead.
It looks to me that monix and akka-streams are extremely different...
For our purposes (stream processing), nope not really. Have you ever used monix?
Oh yes. My bad. Crazy typo. :)
On a side note, Persistence FSM was recently marked as stable. https://github.com/akka/akka/issues/21190#event-934491973
I will start to work on creating an ADT by using a validation typeclass that is automatically derived with shapeless from (nested) case classes. The ADT should describe, what elements could not be validated and why. The result should then be serialized into json so that a frontend-application can use this information to show the user which fields of some input form have not been filled out correctly. 
Nice you met any cuties into FP on the JVM?
Man there sure seems to be a lot of porn spam on this subreddit lately, not sure if we're being uniquely targeted or if all of reddit is getting spammed, but I'm usually removing two porn spams per day here :-/ 
That's what I meant to say. "match for your complete stmt" == regex that "matches the entire string".
Honestly Finatra is my favorite way to write http endpoints. In any language.
&gt; Another example would be Clojure's transducers, which are a little hard to express as types due to their protocol? Why? Aren't transducers just "here are black-box of operations we can apply to your data"? Something like this: ``` type Transducer[A] = TraversableOnce[A] =&gt; Traversable[A] ``` 
Hi thanks for giving this so much thought! Yes I think you're right that this feature is orthogonal to what your library does, though we may benefit from building it on top of yours in the future to make things simpler.
Pull requests are welcome.
Can you please give an example input and output, where `f` is your desired `String =&gt; String` function? `f( "foo bar" ) === "foobar"` ? Also, why wasn't `String#replace` sufficient?
They've got a lot of interesting technical concepts that have been ported to other languages, and perhaps surprisingly those concepts are more usable in other languages. Which is a shame, but to be expected with Twitter. Honestly, they are not the company you want to emulate when it comes to technical decisions. A huge part of the problem is that they have their own incompatible Futures. If they could just adopt Scala's Future, their adoption would increase several fold. 
OP is probably voicing a frustration held amongst many Play users, and not attacking you and/or your argument wrt to dropping stale libraries. As for Play 2.6 supporting 2.12, great, when 2.6 arrives circa the month of May we can all get on the 2.12 train (6+ months after 2.12.0 release).
&gt; contact library maintainers for libs that aren't available Play, one of the more popular frameworks on the JVM, will not be available for Scala 2.12 until later this spring. Lightbend's implicit response seems to be, "when you get the message, hang up the phone", as Scala has taken a backseat to Java. So yeah, TLS and affiliated libraries stand to gain from Lightbend dropping the ball (see Scala Play and Slick).
 f ("\"$and\" foo bar \"$or\"") === "$and foo bar $or" replace is okay, but it will be called a number of times, which is inefficient I already solved it this way: """"(\$[a-z]*)"""".r.replaceAllIn(s,"""$1""")
In case anyone have fresh macOS: "default" - is default virtualbox/docker vm name brew install docker docker-compose docker-machine create -d virtualbox default eval $(docker-machine env default) VBoxManage controlvm "default" natpf1 "cassandra9042,tcp,127.0.0.1,9042,,9042"; docker-compose up -d sbt run Next time: eval $(docker-machine env default) docker-compose up -d sbt run It does not run port mapping properly on macs https://github.com/boot2docker/boot2docker/blob/master/doc/WORKAROUNDS.md 
They use their bijection library as metaphorical ducktape for the issue internally, I actually like twitters CSP better than Go's, mostly because it takes after concurrent ml. If you want a company to model technical decisions after it's Netflix or Google. The issue with the twitter stack is outside of using finatra and finch, you actually have to know the internals well, which most companies refuse to do. Especially with akka's ecosystem being made directly with adoption in mind. I've been using both akka and finagle a lot lately, I probably know more about akka's ecosystem than I do any other, yet I think twitter's stack is better suited for certain problems. 
&gt; For the Task implementation fusion happens naturally due to its lazy nature and I don't think Haskell does anything special either, though I might be wrong. Haskell is able to track side effects through its enforcement of the IO monad, so I think the compiler is a bit smarter when it comes to IO fusion. Its also true that laziness has something to do with it as well &gt; I've heard about Dotty's compiler attempting to rewrite expressions into more efficient versions when possible, but IMO I don't like the idea. This is usability 101, something I've read about in "The Design of Everyday Things": for objects to be easy to use, users have to form a good mental model of how they work (when buttons are pressed). With fusion that mental model of how something works goes out the window, meaning the principle of least surprise is ruined. Yeah this is what I was talking about. In fact they already have example operations, where multiple `map` operations are fused into a single map operation.
IIRC changes to DI will primarily about removing Guice as the default; if you're already doing compile time DI via something awesome like MacWire, should be good to go. Sure, 2.11 + 2.5 are totally fine, agreed, but given that 2.12 brings a lot to the table, and that the rest of the stack already supports 2.12, it would be great to at least have the choice to make the switch. Since Play is the core, no dice :\
posted here: https://www.reddit.com/r/scala/comments/5oyga4/scala_books_a_list_for_those_wanting_to_learn_the/
Oh man, two things I enjoy that I didn't expect to see together!
Why would I "unwrap it" when I have a typeclass
You can't see it because it's not there, it happens under the hood, which makes it unintuitive. We are basically doing a loop, but given that you can't describe a normal loop that collects a result for async stuff, you end up with recursive calls. In reviews use these: Rule: If it's async, meaning result signaling happens with `callback: A =&gt; Unit`, then that `callback` call must always happen with an asynchronous boundary, always. Reason: if it doesn't have an async boundary, because of its async nature, you can't use it with `for` / `while` loops, so you're going to end up with recursive logic that's not intuitive and that can trigger stack overflows based on the input size. Best practice: if you have a callback to call, you should replace it with a `Promise` and its corresponding `Future`, because they are safe.
Does it work on when using windows cmd terminal or a Cygwin terminal?. My colours seem the same as before. 
This is not a typeclass. This is just implicit functions
Nice article, they don't need monad for : val futureX: Future[Int] = dal.getAsync("x") val futureY: Future[Int] = dal.getAsync("y") for { x: Int &lt;- futureX y: Int &lt;- futureY } yield { x + y // yields Future.value(x + y) or Future.exception(...) } though :D They make a really good point here: &gt; One of the main concerns with Scala is its steep learning curve. However, in highly complex systems where the learning curve of the system is steeper than that of the programming language itself, improving readability and maintainability has a higher priority. In this regard, Scala does a good job, as we'll present in the next section.
When in sbt shell, can you try running "clippyColorsEnabled" instead of e.g. "compile"? That should give you true/false
In addition, typeclasses allow higher order polymophism, that is, you can write it for List instead of List[Int], List [Char], etc. Typeclasses stack, so if you have a typeclass for A, you can a typeclass for F[A] using the Functor on F.
I think you might be mixing concepts - type classes and higher order polymorphism are two different, unrelated things. Some type classes *require* higher-order polymorphism - a functor is a type class on a type constructor, so in Scala it needs to be a higher-order type, but there is no direct relation between the two concepts. And type classes do not stack. Type class instances compose, under certain circumstances, but whatever you mean by "stack", your example is either too vague to incorrect. Say that you have an instance of `Ordering` for type `A`. You don't magically get an `Ordering[List[A]]` because `List` happens to have an instance of functor.
ok done it. https://github.com/softwaremill/scala-clippy/issues/40 
It might be a matter of taste, but I see this as undermining the type checker. When I'm working with an `Int`, I like that it stays an `Int` unless I explicitly do something to it to turn it into another type. To take a more egregious example: implicit def booleanAsInts(b: Boolean): Int = if(b) 1 else 0 1 + true This yields 2. I would prefer this not to compile, ever. I understand this is made popular by weakly-typed languages such as Javascript, but it's just not my cup of tea (note that I do not mean weakly-typed in a derogatory manner - it's the only term I know for a type system where the boundaries between different types aren't strictly enforced). As for how it differs from `Json.toJson(123)` - I don't actually use play, but can I assume that `toJson` expects an implicit instance of some type that knows how to turn a type `A` into a `Json`? If so, it's different from implicit conversion in one critical aspect: the transforming of one type into another is explicit. There is no mechanism that lets the compiler try to guess what you mean and do its best to adapt - you must tell it precisely what you want, and it'll tell you whether it's possible. No magic.
thanks!
What would it take to make it not dead? Particularly given that 2.12 is supposed to be 100% source compatible with 2.11, as someone who likes spray could I just pick it up and make a release?
The post has the usual problem here: it contains an incorrect definition of "referential transparency," and then goes on to choose a stack (Finatra, Guice...) that isn't referentially transparent. NB: I'm not saying that's the wrong choice for them. I'm only observing that they aren't getting the benefits actual referential transparency offers.
The debates here as to what a typeclass is or isn't are among those that motivated the development of [Simulacrum](https://github.com/mpilquist/simulacrum), by the way. Typically brilliant work from Michael Pilquist.
"22 columns ought to be enough for anybody" /u/odersky
&gt; HMRC Is this what you are referring to? https://github.com/hmrc Thanks
I was thinking "talk like a pirate" myself, but that works too...
I really like your generic validate. I will play with this approach, thanks!
The evidence could be wrong. I was actually referring to the relationship between `Int` and `Json` as far as the JSON spec is concerned, but I'm wrong: all `Int`s are, in fact, valid `Json`. So my claim that `Int` implies `Json` is false is false. :-)
I agree that it's a bad idea to implicitly convert between types that don't convey any intent, like Int and Boolean. But if I want to implicitly convert into a class that conveys intent, like Json, then I know what I'm doing. In such a case having an implicit ToJson[Int] (with a method apply(i: Int): Json) doesn't carry any more information than just an implicit Int =&gt; Json. Or if you prefer, Function1[Int, Json].
Not really; an implicit conversion function is still evidence. What's more, with implicit conversions, if there's no evidence you'll get a straightforward type error (expected this, got that) instead of a weird 'could not find implicit evidence of ToJson[Boolean]' (paraphrasing a bit).
* https://github.com/ornicar/lila (lichess.org) * https://github.com/ensime/ensime-server
&gt;because we have come tables that have well over 22 columns. Slick has been able to handle more than 22 columns in a a table for ages.
Finch is really great, strongly recommend. Our Finch services are easily or cleanest and most stable.
~~implicit def parentTraversable[Data, Child, T[_] &lt;: Traversable[_]](p: Parent[Data, T[Child]]): Traversable[Child] = new Traversable[Child] ~~ ~~def foreach[U](f: Child ⇒ U): Unit = p.children.foreach((x: Any) ⇒ f(x.asInstanceOf[Child]))~~ ~~}~~ ~~seems the type is being lost somewhere. But this works.~~ EDIT: implicit def parentTraversable[Data, Child, T[x] &lt;: TraversableLike[x, T[x]]](p: Parent[Data, T[Child]]): Traversable[Child] = new Traversable[Child] { def foreach[U](f: Child ⇒ U): Unit = p.children foreach f }
Some larger seed templates: * https://github.com/KyleU/boilerplay * https://github.com/Daxten/bay-scalajs.g8 Large projects are usually several play applications rather than one single one, for example https://github.com/guardian, and they can be very hard to get your head around. You can look at individual projects like https://github.com/coursera/naptime or https://github.com/delving/culture-hub if you want to see single apps. 
The best way I've found to debug play applications is to write lots of unit and integration tests and target the flow really carefully. That way, once I've fixed the bug, I've got some code I can check in to make sure that bug is squashed for good.
I really wish people would stop calling it “pimping”.
Wow thank you! Can't say I get why it works with TraversableLike but not Traversable, but if it works it works :)
It *does* work with Traversable. The issue lies in `T[_] &lt;: Traversable[_]` which I believe is saying "*T* which has a type parameter is a subtype of *Traversable* which has a type parameter" rather than `T[TT] &lt;: Traversable[TT]`, which reads "T of TT is a subtype of Traversable of TT" TL;DR: The type parameter needs to be locked down.
But wouldn't you be able to do something like this: case class Pen(color: String) case class Paper(lines: Boolean) case class OrderData(val a1: Int, val a2: String, val a3: Double, val a4: List[String], val a5: Int, val a6: String) sealed trait Order { val orderData: OrderData } case class PenOrder(val pens: List[Pen], val orderData: OrderData) extends Order case class PaperOrder(val papers: List[Paper], val orderData: OrderData) extends Order
Write tests and use pprint or debug logging. If you come from rails you should know how to write tests. rspec and Scalatest's WordSpec or FunSpec are very similar. edit: If you write your logic modules in a functional way (data-in data-out), you can use the sbt console and try it out. But imo any testing in the console could be written as a spec as well... 
Let's say you have something like this: implicit def x: Int =&gt; Json = ??? def doSthWithJson(json: Json) = ??? doSthWithJson(5) //compiles If you exchange the implicit conversion method with a typeclass + typeclass instance, the code won't compile anymore *unless* you write `def doSthWithJson(json: Json)(implicit toJson: ToJson[Int]) = ???`. I think this can be considered as a difference, no?
I disagree - when I have a value of type `A`, and some bit of code needs a value of type `B`, I want there to never be a possibility that my `A` is turned into a `B` without my explicit knowledge. The problem with your argument is that you assume you always know when implicit conversions are brought in scope. Frankly, I don't - whenever I import a package, it might bring implicit conversions in scope that I don't know about. Type classes are not about turning a type into another, but about adding behaviours to existing types. If you want to be able to turn your `Int` into a `Json`, my preferred approach would be to enrich `Int` (or any type that has a `JsonEncoder` instance) with `toJson`. And, yes, in order to do that, you do need implicit conversions - but that's ok, the type you convert to is not going to be used by other unexpecting bits of code. I might very well write code that uses `Json`, but I'm very unlikely to ever write code that explicitly needs a value of type `ToJsonEncoderOps`.
Consider these two alternatives: - If the code that's order specific is simple enough, just go with the repetition route, it's simple, works well with the built-in pattern matching, and solves most of your problems - If you really do want generic code like the Validate typeclass, you can use a mixed approach: you write most of your code with a generic order type and typeclasses like I sketched in my other answer, and at the edges of your program, you parse to an ADT (with repetition), call the generic code on each branch, and then send the output back. Something like : case class Pen(color: String, weight: Int) case class Paper(color: String, length: Int, width: Int, lines: Boolean) case class Staple(size: Int, thickness: Int) // generic order case class Order[A](customerId: String, items: List[A]) object Order { implicit def orderTraversable: Traverse[Order] = ??? } //generic code trait Validate[A] { def validate(item: A): ValidationNel[String,A] } object Validate{ def order[A: Validate](v: A): ValidationNel[String,A] = implicitly[Validate[A]].validate(v) implicit def validatePenOrder: Validate[Order[Pen]] = ??? implicit def validatePaperOrder: Validate[Order[Paper]] = ??? implicit def validateStapleOrder: Validate[Order[Staple]] = ??? } // lots of generic code // .... // at the edge of your program sealed trait Items case class Pens(v: Order[Pen) extends Items case class Papers(v: Order[Papers]) extends Items case class Staples(v: Order[Staples] extends Items (yourJson.as[Items] match { case Pens(v) =&gt; v.validate.void case Papers(v) =&gt; v.validate.void case Staples(v) =&gt; v.validate.void }).isSuccess This limits repetition to a minimum, but you should really consider whether it's worth the additional complexity 
In IntelliJ "normal" java debugging works fine, you can use breakpoints, inspect variables, profile, etc.
Is it production ready?
http://tudorzgureanu.com/scala-starter-kit/ Since that the 2nd Edition of Scala for the Impatient was issued. Hopefully I'll get some time to update the starter kit.
Good question! When user opens a page, Korolev assigns deviceId for the browser (via cookie) and generates sessionId for current tab. When new tab is open, deviceId stays the same, but sessionId is new. It means every new tab has a new state both on server and client (client state is just a DOM).
[removed]
&gt; If you already have a Future then there's no point converting it to a Task. Wouldn't it make sense to convert it via `() =&gt; myApi.thatReturnsFuture() ?`
In that case you don't already have a `Future`, you have a method call that returns a `Future` and can indeed convert it like that, like I described in the next sentence.
Ah. Thanks.
100?
you are right. returning `0` is a bug
I understand your preferred approach, and I agree with you that typeclasses in general are not just about conversions. This is why I specified in my post title that a typeclass _with a single method_ is just a function. When you have a single-method typeclass, that usually means you're trying to define a principled way to convert from one type to another. In fact, that's literally what a Scala function is. Hence my assertion that you can think of a `Function1[Int, Json]` as a typeclass instance for converting `Int` to `Json`. Now, you and others have raised the objections that such implicit conversions are dangerous because you might have a method that takes a `Json` but accidentally gets called with an `Int`. I'm saying that I've carefully defined the `Function1[Int, Json]` instance because I _want_ to use it, deliberately. To me, implicits are not accidents waiting to happen _if you define them purposefully on meaningful types._ That's why I'm against implicits like Scala's provided `any2stringadd`, which actually is a bunch of mistakes waiting to happen; but I'm not against conversions between meaningful types that I've defined myself.
A is not 0, the result of the function you called last is 0. A is a valuable and is immutable.
Well I don't get why there is any confusion once I specify in the parameter that children: T[Child] So, shouldn't the compile know now that children is a `Traversable[Child]`?
Has any one else subbed to this after getting frustrated with scala hoping to improve their situation but instead just find instead that they were right to be frustrated?
Definitely seems like a parser bug, leaking scope. Probably an issue in one of the inliner phases. The compiler holds onto the symbols through a few phases before actually resolving them. Forget the name, but there's a phase that unrolls lambdas when possible. 
I'm assuming they didn't write it just like that, but reworked whatever code caused the issue for them until it was as minimal as possible.
Small disclaimer: I treat Scala as a purely functional language, other styles my involve different strategies. Typeclasses are extremely important because they really show how types are not just a tool for catching errors, but a strong design aid. Given that they link behaviour to a specific **type** (as opposed to OO interfaces, where behaviour is linked to a **value**), you can drive behaviour just by specifying the correct type. To get the full benefits, you should embrace modelling your domain with lots of types, which is ok given how syntactically cheap they are. On to your specific question, concerns like serialisation, storage and validation are definitely best modelled with type classes, especially because you can compose them (in the sense that an instance for a composite type can rely on instances for its components). As for your core logic, I tend to model it as several algebras, and each function it's then parameterised by which algebras it's using. Algebras can be encoded with typeclasses as well (often extending Monad or Applicative, in what is known as *mtl style*, or *typed tagless*, or *final tagless*), or as Free Monads. One final trick is to always provide instances for Scalacheck Gen, this not only allows you to write property based tests, but it's very useful when you are experimenting in the repl, since you can summon a value for a given type, no matter how complex (and therefore tedious to produce by hand), just by `arbitrary[MyType].sample.get`
I guess in some way this is similar to what Vaadin has been doing for years. And in other ways it's completely different (such as the immutable and pure part).
Yes. I was debugging some generated code when I stumbled on this one.
No. All it knows is that `T[Child] &lt;: Traversable[_]`, because that's the bound you provided. Notice the "wilcard" type `_` (existential).
&gt; Sure, and you could make the argument that that compilation difference carries the information that you don't want an automatic conversion from Int to Json, you want to do the conversion manually I think this is not a correct description of the intention. I would rather read it like: I don't want to have an automatic conversion and I also don't want to do it manually. But I want to be able to use `Int`s as `Json` *provided* I have know some way how to view `Int`s as `Json`. It might seem like a minor philosophical difference, but it effects are real. Thus, you should not say &gt; a typeclass with a single method is just a function. but rather &gt; Instead of a typeclass with a single method, one can also use implicit conversion *in most cases* which would be okay. But it just isn't *the same*, neither theoretically nor practically.
I think this particular example is nothing to get frustrated about. Despite the phrasing of the post, it's not a riddle like the ones on http://scalapuzzlers.com - it's "just" a bug. That happens. I'm sure other languages' compilers have bugs, too.
Totally good one )
That's why my friends at Verizon Labs wrote [Delorean](https://github.com/verizon/delorean). Honestly, though, at this point I think the scalaz ecosystem is sufficiently rich that you shouldn't be forced to deal with `Future` at this point.
this.
Yea makes sense now. So I want to use pattern matching against the regex like this (taking example from book): val Decimal = """(-)?(\d+)(\.\d*)?""".r val Decimal(sign, integerpart, decimalpart) = "-1.23" match { case Decimal("-","1","23") =&gt; "It's -1.23!" case Decimal(_,_,"20") =&gt; "Some number that ends in .20" case Decimal(_,"2",_) =&gt; "Any number from 2 to 3, or -2 to -3" case Decimal(_,_,_) =&gt; "Anyting" } But this gives me a match error. I'm not sure how to use this syntax
[deleted] ^^^^^^^^^^^^^^^^0.1403 &gt; [What is this?](https://pastebin.com/64GuVi2F/52072)
&gt; sum types Idris has sum types? Would you have a reference to their description? 
Sure, they're simple enough that it doesn't bother to describe them explicitly much. http://docs.idris-lang.org/en/latest/tutorial/typesfuns.html#data-types describes what's clearly sum types (using `|`), but doesn't bother saying anything more than that its types are "similar to Haskell".
Yeah. At the end it's a whole functional interpreter for doing IO. I do agree that some people (like myself) find "top down" learning easier than bottom up, which is the style of The Red Book. I would say that you should definitely supplement reading that with other blog articles and reading through code of various projects, videos etc. The exercises are important, even if you just have to look at the answers sometimes and retype them out I find that it helps solidify the concepts. 
Thanks. I found it to be really theoretical at the beginning and I have a really hard time with theory. Ill give it another chance knowing what you said about supplementing it
You do definitely have to recognize that, e.g. the scalaz ecosystem is one ecosystem; the fs2 ecosystem shares its lineage but is different across versions; the Cats ecosystem is its own ecosystem; and the Monix ecosystem is its own ecosystem. So things that line up well in terms of dependencies from your list, today: http4s -&gt; scalaz-stream -&gt; monocle -&gt; argonaut -&gt; shapeless Cats -&gt; fs2 -&gt; Circe -&gt; shapeless Soon: http4s -&gt; Cats -&gt; fs2 -&gt; Circe -&gt; shapeless Monix is, rightly or wrongly, kind of off to one side in all of this. Note that, especially with http4s, you _can_ use Circe—there's a module for it—but then you really are mixing and matching scalaz and Cats, which I recommend strongly against. fs2 has compatibility modules for both scalaz and Cats, which is great, and as fs2 gains adoption we'll see even more of the "you must choose scalaz or Cats" abate.
() and {} are interchangeable for single-argument argument lists. ( x =&gt; ... ) == { x =&gt; ... } but the lambda block will infer semicolons for every line as they always do. 
Note also that in this case you *must* use braces because the function body has multiple statements. A multi-statement expression (i.e., one with explicit or inferred semicolons) must use block syntax, so in this case `.map(line =&gt; { ... })` or `.map { line =&gt; ... }`. The latter is usually preferred.
Yes it is an anonymous function. You don't have to specify the type of the input variable if it can be inferred. For example: // example 1 val x: Int =&gt; Int = x =&gt; x + 1 // example 2 List(1,2,3).map(x =&gt; x + 1) But there are a lot more cases as well where you don't have to specify it.
[deleted] ^^^^^^^^^^^^^^^^0.1069 &gt; [What is this?](https://pastebin.com/64GuVi2F/19090)
It would be helpful in this case to frame your question in terms of what you're trying to achieve. Listing the methods on an object is called reflection, there are two different reflection APIs you can try to use, the java reflection API and the scala one. Reflection however doesn't lend itself making programs that are easy to reason about. And using reflection as a tool to solve problems is pretty much never necessary,. Attempting to use it is often of the result of a design flaw elsewhere that results in the loss of type information. &gt; s there something similar in Scala? How am I supposed to know what kind of object a function returns and all of the different things the object can do? This sounds more like a question that has to do with polymorphism. This fact, combined with reflection being generally not very useful, is why I ask you to reframe your question.
Thank you. I appreciate the detailed reply.
If you're working from the REPL, unless your version of scala is old, has tab completion. Type scala&gt; sa. then hit tab, a list of methods will be displayed: Welcome to Scala 2.12.1 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112). Type in expressions for evaluation. Or try :help. scala&gt; val sa = Array(1,2,3,4) sa: Array[Int] = Array(1, 2, 3, 4) scala&gt; sa. ++ applyOrElse containsSlice dropWhile fold headOption isTraversableAgain map partition reduceRightOption segmentLength startsWith toBuffer toVector zip ++: array copyToArray elemManifest foldLeft indexOf iterator max patch repr seq stringPrefix toIndexedSeq transform zipAll +: canEqual copyToBuffer elemTag foldRight indexOfSlice last maxBy permutations reverse size sum toIterable transpose zipWithIndex /: clone corresponds endsWith forall indexWhere lastIndexOf min prefixLength reverseIterator slice tail toIterator union :+ collect count exists foreach indices lastIndexOfSlice minBy product reverseMap sliding tails toList unzip :\ collectFirst deep filter genericBuilder init lastIndexWhere mkString reduce runWith sortBy take toMap unzip3 addString combinations diff filterNot groupBy inits lastOption nonEmpty reduceLeft sameElements sortWith takeRight toSeq update aggregate companion distinct find grouped intersect length orElse reduceLeftOption scan sorted takeWhile toSet updated andThen compose drop flatMap hasDefiniteSize isDefinedAt lengthCompare padTo reduceOption scanLeft span to toStream view apply contains dropRight flatten head isEmpty lift par reduceRight scanRight splitAt toArray toTraversable withFilter scala&gt; sa.
&gt; In Python you have the built in function dir, which tells you all of the methods and variables that live on an object. Is there something similar in Scala? How am I supposed to know what kind of object a function returns and all of the different things the object can do? Oh, the irony...
sorry, it wasn't meant as a criticism to you. it is that, python being dynamically typed and scala statically typed, the discoverability problem should be reversed. i, for one, can't find my way with dynamic languages. but, anyway, welcome to scala, i hope you like it. as others have said, hitting tab on the repl, using a good ide (intellij community recommended), and looking at the scaladocs are your best shots. if you are willing to spend the price of a large pizza on software, may i recommend this awesome little utility: https://kapeli.com/dash useful for most major programming languages.
When you complete a method and hit tab, you get a type signature of that method, but thats about really as far the online documentation for the REPL goes. scala&gt; sa.zip override def zip[A1 &gt;: Int, B, That](that: scala.collection.GenIterable[B])(implicit bf: scala.collection.generic.CanBuildFrom[Array[Int],(A1, B),That]): That override def zip[A1 &gt;: Int, B, That](that: scala.collection.GenIterable[B])(implicit bf: scala.collection.generic.CanBuildFrom[scala.collection.mutable.WrappedArray[Int],(A1, B),That]): That
The Scala REPL and the docs can help us to simply explore the available methods just as others said. But when I'm in the REPL and I'm about to explore a *complex* library I used to use [this](http://ideone.com/XfT0We) object and load it with `:load E.scala`. For example `E[String]` prints out all the methods of String. `E.of(obj)` prints out all the methods of 'obj'. `E.where[String]("Case")` prints out all the methods which contains the "Case" character sequence. `E.help` prints out the available helper functions. This method is especially useful when the *library lacks a usable documentation*. This happens when you try to use an old and unmaintained java library.
&gt; How am I supposed to know what kind of object a function returns and all of the different things the object can do? Have you tried looking at the Scaladoc for the code you're working with?
At work we are in a similar situation and this is how we solved the issue * All services should return Task (previously Scalaz, now Monix) * We provide our own wrapper that wraps over Action.async and use it for controller methods instead. So instead of the signature `Request =&gt; Future[Result]` we have `Request =&gt; Task[Result]`. Converting Task to Future is just a simple `runAsync` away. * When migrating services that still uses Futures, we first use Task.deferFuture to wrap the method body, which means we can gradually transition the whole codebase instead of doing it in one big PR.
The guardian makes heavy use of play framework and have used them as a example source: https://github.com/guardian/frontend 
Try InteliJ. Gives you all you need and more.
I actually am using intellij. Do you need a plugin to view the scala docs, or is it a native feature?
This looks like a great quality of life plugin. Great work!
I see that alot of people struggle with REPL :paste mode. Scalafiddle is really good to pin working examples
Yes. Current efforts to support alternative runtime environments are * [Scala.js](https://www.scala-js.org) * [Scala Native](http://www.scala-native.org) 
I hope we are going to see native Scala as soon as possible because JVM imposes several restrictions and a rich language like Scala should become independent of Java libraries and be able to run full speed. From what I have seen Scala Native uses some special primitives so one would not be able to compile a Scala application directly without changes. That should not be the case.
I wonder what causes people to post these things here
These are as much people as I am an astronaut
I think that if one wants to use Scala for functional programming a native compiler makes a difference because it can apply several optimisations which are required for the application to perform well. FP really needs a native compiler to be competitive in terms of performance.
What are the limitations of the JVM for Scala? I actually would argue the opposite end, the JVM is a strength for Scala by giving it a more robust ecosystem, good performance, easy portability, and easy interop with legacy systems. The biggest drawback of the JVM is memory usage. But Scala due to its use cases is pretty much always going to be on big machines, with tons of memory, and it should be a completely non problem. go program Ocaml if you don't want the jvm.
OpenJDK? It's steered by Oracle, but that vm will live open. 
[removed]
JVM and Java libraries are a big part of scala and reason why Scala is used. Hence I still don't see it a negative or share your view on it. Maybe you should look at Haskell as an alternative as it may be a better fit for your needs if you are after a non JVM language that is Functional? 
Yeah, seconded. If the JVM is a downside (and for some use cases, it is!), Scala is simply not for you. 
Oracle employees have a lot of power in it. But they are Oracle employees that would be influential regardless of the company they were at. Oracle, IBM, redhat, Google and academics are all well represented and put large amount of work into openjdk and the JVM.
Haskell doesn't have full erasure, I lot of people assume it does but the GHC runtime does have some limited type information in order for various Haskell language features to work Implementing full blown HKT's with full type erasure is very hard, some kind of boxing + runtime information is needed in various cases, and the people at RUST are having issues implementing a form of HKT that isn't just a very minimalist implementation
$24.99 for a large pizza?
[removed]
[removed]
One of the reasons why scala is so productive is that you can use thousands of Java libraries and still write concise functional code. If you want to stop using the jvm, then use haskell or ocaml instead.
What's holding us back is the lack of the formal Scala API. Waiting for it to get out of RC status before picking it up 
Name one and demonstrate why
Here is the C based gc they use instead of the java GC/runtime: https://github.com/scala-native/scala-native/blob/fba450e75ce47577baf47813bf7e101ae0bb4dbd/nativelib/src/main/resources/gc.c Here is the C interop: https://github.com/scala-native/scala-native/blob/c857f6a48e9b326d3fae6cf3eb81e5e3e4dda905/unit-tests/src/main/scala/scala/scalanative/native/CInteropSuite.scala Here are the definitions that enable stack allocation (line 75): https://github.com/scala-native/scala-native/blob/b47139b31b58596d930d454c0a0b672d380230de/nativelib/src/main/scala/scala/scalanative/native/package.scala If you insist i can show about a dozen tutorials showing how to use type bounds and implicits to create higher kinded types. Not only was it not satire, im fairly sure its all 100% accurate. 
&gt; JVM code is slower largely due to the overhead induced by being executed on a virtual machine runtime Wrong. Code can both be faster and slower. It all comes down to which code is executed in the end -- it's possible to claim that "classic" AOT compilers have an advantage here due to having a basically unlimited amount of time to optimize code compared to JIT compilers (and others claim that JIT has an benefit, because it can look at the program behavior during execution, but then others claim AOT compilers are getting pretty good with PGO, then people respond by pointing out that the JVM can discard and reoptimize code whenever program behavior changes etc.) -- but this is not necessarily related to having a virtual machine runtime or not. &gt; The runtime is not present when compiling with native It is. Pretty much every language has a runtime. It might not come with all the bells-and-whistles of the JVM, but there will be a runtime. &gt; and you can avoid the GC You can't. Scala doesn't work this way. You can be careful and avoid allocations in some places -- either by writing code carefully, using sun.misc.Unsafe on the JVM or Scala-Native's special instructions, but there is no overall guarantee that GC isn't used or that your code is working correctly in the absence of GC. The only thing that might reduce pressure on the GC is the better support for value types, and that's coming with Java 10, too. &gt; if the code it generates is any good, it will likely be measurably faster Maybe, maybe not. &gt; The primary enabler of HKTs in Scala is how it's compiler handles type bounds and implicit parameters. It has nothing to do with erasure. This shows a fundamental misunderstanding of the issue. The GP was clearly referring to the issues with generating code for stuff that uses HKT, not the concepts of Scala-the-language. If you think it's unrelated to erasure, then please share the specific, boxing-free instructions a compiler would generate for code using HKTs. &gt; And to be honest, I would not expect that erasure even is present in scala-native at all. It is. The assumption of erasure is built really hard into the compiler.
See https://www.reddit.com/r/scala/comments/5s0c1c/scala_without_javajvm/ddchefa/.
&gt; JSON serialization JSON serialization is not monadic, JSON deserialization is however. I doubt you're interested in only deserialization or more generally monadic parsing. &gt; Business Rules / Validations Also probably not monadic, could be implemented with the Free Applicative instead. &gt; Remote service access This would actually be great application of the Free monad, I've seen many Free monad tutorials use twitter and github. I've personally implemented an elastic search client recently. &gt; Database access Using Free for database access is not at all different from remote service access. [Doobie](https://github.com/tpolecat/doobie) has already been linked as an example, for working with JDBC specifically.
[removed]
As far as I know Play 1 is only in maintenance mode. There won't be new features or small bug fixes, just security updates. 
Play have their hands full moving to Scala 2.12.x - in scope of PlayFramework 2.6 (and already are planning for 3.0 in 2018), and I don't think there is any future for the old 1.x stream.
I'm waiting for a mature Scala native and real world benchmarks. In the meantime, I'm willing to be that unmodified Scala code will run slower on Scala native. Why? In heavy polymorphic code such as the Scala collection library, the JVM has additional runtime information, and it can inline code very aggressively by assuming the type parameters of a method (and deoptimizing when the assumption is falsified). Unmodified Scala code relies heavily on garbage collection. Scala native uses the Boehm collector, while the JVM has several state-of-the-art algorithms available. Still, I'm very excited by Scala native. Crossing the fingers that the native annotations such as `CStructN` will be transparent for the JVM code. Also hoping for easy access to SIMD intrinsics. Then, the possibility of using Scala to write JNI libraries... For HKTs and erasure, I was referring to the problem of combining reified generics and higher-kinded types. I'm not a specialist, see http://stackoverflow.com/questions/6953909/why-are-reified-generics-hard-to-combine-with-higher-kinded-types
I meant the implementation of higher-kinded types with reified generics (non-reified generics kill memory locality). By erasure, I was referring to the way generics are implemented on the JVM, not that all type information is lost. http://stackoverflow.com/questions/6953909/why-are-reified-generics-hard-to-combine-with-higher-kinded-types 
&gt;but doing a desktop app would probably be way more difficult than in C# or something Why would you think that? There are TONS of java desktop apps. C# might be easier if you're only talking about windows.
Does pushing side effects to the edge of my program help me avoid STDs?
As an aside take a look at crystal and nimrod. I think they both use Boehm and I think they will both beat Java in most general purpose use cases. Their performance is what o ultimately expect from this project with some caveats. 
[removed]
Already found the reason - ill-named source folder in examples project. Looking at examples, idea is pretty interesting for *low-latency networks* e.g. for corporate software. The whole idea of bringing react-like architecture to server-side looks promising from simplicity point of view but it won't do where mobile users usability is important. If you want some advice from my side, think about component as isolated unit that a) contains CSS,HTML and client logic b) can communicate with other components and c) can be hot-reloaded by running application without browser refresh. As for intercomponent communications, here is very inspiring link https://habrahabr.ru/post/235121/. Warning: Bear and balalaika required. Btw, there is no need to implement own HTTP/2 support. It is much better/safer/faster to let nginx transcode protocols for you.
If he doesn't know cats or scalaz he may find doobie to be a bit mysterious. 
Thanks, that is very useful in what makes sense with Free. I agree on Json Serialization (it is serdes, but I was being lazy). Business Rules I agree can be Free Applicative, sometimes Monad if the rule is a single hard stop that prevents further analysis, but in general yes.
Usage of a `ConcurrentHashMap` wouldn't have been against Scala's best practices, given that in this case whatever you do, you're still building an object with state / identity; although it's true that you need to delete old and unused keys, otherwise you can have a memory leak. Use `Array` if you care about performance, not `Vector`. The difference in performance on indexing and traversal is big. Of course `Vector` is persistent and `Array` is not, but for an "*effectively immutable*" and well encapsulated array such as this one, you don't need `Vector` and note that `Vector` having "forced" immutability doesn't help, because when you're this low level, you can get screwed sideways from countless other reasons. The `Atomic.getAndTransform` (and here I'm happily assuming he's using [Monix's Atomic](https://monix.io/docs/2x/execution/atomic.html) :)), is not equivalent to a *lock* - in general locks are evil because Java does real 1:1 kernel multi-threading, the kernel's scheduler can do whatever it wants, including freezing threads that hold locks for whatever reason, which then prevents other threads trying to acquire that lock from having progress. But a `compareAndSet` instruction, even though it might prove unnecessary and expensive (CAS instructions are expensive), cannot block threads from making progress. Speaking of performance, doing `get(key)` twice is not OK, store that in a local value.
I use Scala fulltime for data projects, it's definitely a good fit for those. If you work on data products, that will naturally lead to choices for libraries: * You might find that you need to read/write to databases a lot, which will lead to some choices like Slick or Doobie. * You might find that you have certain types of boilerplate, which will generally lead to ScalaZ or Cats. * If you need to make a web UI for others to interact with that data, that could lead to something like Play. * If you want to do machine learning on large datasets, SparkML is the way to go. * If you want to do more sophisticated statistics on smaller datasets, Twitter's numerical libraries are pretty good. * If you do most of your Spark work on AWS, you might want to look into a Scala wrapper for the AWS SDK, such as https://github.com/seratch/AWScala * If you want to test your code by generating random data, that leads pretty naturally to Scalacheck
Just saying there are likely simpler interfaces for db access if you're not using monads explicitly. 
Let's hope, we'll see. One simple test is to see whether the use of `Option` always allocates on the heap.
The question you should be asking is what problem you want to solve. Choice of libraries will naturally arise from that.
How has type erasure been a limitation for you?
I can say unequivocally that Scalaz is the most common library I use and that I don't ever write scala without it. However, it's important to remember that you don't have to 'learn' a library, you just need to take pieces of it that help solve your problem. My first six months of using Scalaz I just needed OptionT and EitherT, didn't understand 99% of the rest of the code base. Don't be afraid to pick out bits that are useful and ignore the rest. http://eed3si9n.com/learning-scalaz/7.0/
can we have a bi-weekly "how is hiring/wants to be hired?" thread?
Didn't read the article...if I'm tasked with a fronted job, you can rip react and relay out of my cold dead hands.
Thanks for answering this. I've been curious about that too. I think that should be a scala built in
I haven't used http4s, but we use akka-http extensively at work. We're basically all in on akka/akka-streams though, so it makes sense. I also really like the routing directive DSL We write purely backend/restful services though, so we don't have to deal with UIs or cookies or sessions or stuff like that for the most part
[removed]
I've used spray quite a bit. I don't know what to say about my use case beyond "general corporate REST backend". I dislike http4s because it involves writing unsafe-looking `case` constructs, which are something I try to ban from my codebases (e.g. I try to avoid ever writing `match`) because it's too hard to distinguish between safe and unsafe cases.
You only need a plugin to enable Scala support. https://resources.jetbrains.com/storage/products/idea/docs/IntelliJIDEA_ReferenceCard.pdf ctrl+q - shows docs and type signature for selected object.
The big limitation I see is how slow a one-off command line program is to start. E.g. I once wrote a little code snippet to do some refactoring I needed and ran it on about 200 files via `find ... -exec scala ...` and it took a couple of minutes and slowed my system to a crawl with all the memory it was using. Yes, I could have used something like better-files to do the directory traversal in code, but that would have meant adding a dependency and figuring out how to build it and so on (this was prior to the ammonite article) and I already know `find`.
Semantics aside, we agree 😁
Http4s is pure fucked. No documentation and more often than not does not work the way you expect. If you use this heavily in prod you will most likely have to debug through the library to work out how to tune it. Kill it with fire. 
You can connect an IDE debugger to a Play process via JPDA: https://www.playframework.com/documentation/2.5.x/PlayConsole#Debugging
I didn't mention being in rush, thus I'm not in a rush
Ah, thanks for clarifying!
Am using Spray and would prefer migrating to Akka-Http, but couldn't - have to stick to Jvm 1.6. Spray is sometimes a strange tool. In some cases, it's very easy and good looking. Other cases go really hard. Just had a case: needed to write an Unmarshaller, that'd unmarshall the request to a MultipartContent, and then pack it up into my case class. It was very untrivial, as the Unmarshaller in Spray didn't have any composing methods, not map nor flatMap. While Akka-Http's version of such mechanism does support mapping.
If you're using Spark then definitely learn that first, and your first libraries should be those you need to do specific things you have an actual reason to want to do. I'd also recommend learning Shapeless (at least to the extent of being able to use `HList` and do your own typeclass derivation) quite early, as it basically just fills in some generic functionality that should be in the language already, and is relatively self-contained. You will want to learn parts of Cats or ScalaZ sooner or later (which one probably comes down to politics and/or compatibility with other things you're using) - they're libraries that include some very abstract stuff that can be hard to understand outside the context of specific use cases (at least that's my experience), and that have a lot of depth to them - I've been doing Scala for 6+ years and I wouldn't say I understood all of them yet - but it's worth dabbling with the simpler parts and working your way up as and when you see use cases. Finally if you're working with async you will ultimately want something like fs2, but that's very abstract and underdocumented. For all these cases I'd recommend trying to understand the patterns and implementing the pieces you want to use yourself at first - that will give you a good sense for what they do and why they end up working the way they do. (Especially in the case of fs2, the library as written is overwhelming, but the basic essence of it - iteratees - is really simple). Beyond that it's mostly just specific things. If you're looking for a guava-like "general purpose utility library" or "second standard library", Shapeless and Cats/ScalaZ are the ones that fit that bill IME. I've found Spray (officially depreated, I'm looking into whether I can build it for newer dependencies) and Wicket very useful for REST APIs and HTML UIs respectively, but that's specific to those use cases.
Thanks for sharing! You've also led me to discover these ongoing compiler issues: * [unapplySeq silences all exhaustivity warnings](https://issues.scala-lang.org/browse/SI-9232) * [Match Exhaustiveness Testing Ignores Guard Statements](https://issues.scala-lang.org/browse/SI-5365) * [Guard causes pattern matching to not warn of non-exhaustivity](https://issues.scala-lang.org/browse/SI-7631)
Or start adding documentation?
Tbh I don't feel that Scala has the web market cornered. We should check up on Java libs from time to time and see if we're missing anything. E.g. take a look at http://unirest.io/java.html
I think it would have to happen at the language level. What I'd like is for matching a non-sealed type without `@unchecked` to become a warning and perhaps eventually an error, and similarly for refutable destructuring assignments. Internally the compiler knows about the difference between a refutable pattern and an irrefutable one, AIUI. But given the views Odersky expressed in the recent discussion around improving `==` I don't think he values being able to locally distinguish between safe and unsafe code anything like as much as I do, so I don't see much hope in proposing that as a language change.
There's also [OutWatch!](http://outwatch.github.io) *Shameless self plug*
It's not a huge problem. It just makes your code less elegant/messier in certain places.
I don't think everyone here is like that - I'm certainly no Haskell dev. I'm quite new to Scala and FP in general however, so perhaps with time I will become one.
I'm sorry you feel that way -- there's certainly a large community of Scala programmers who aren't functional programmers at Scaladays, and I know that Heather Miller is setting up a community on Discourse based on the success of contributors.scala-lang.org. I think part of the problem is that the programmers who use Scala as a tool are hanging out on r/programming or on the specific tool and infrastructure they use (i.e. Spark, Kafka) rather than on the language itself.
I know Heather Miller is working on a Scala Discourse website for users that is like contributors.scala-lang.org.
This is kind of a niche problem and I don't think introducing a preprocessor is a good general solution. It's a hack.
I see both sides of the coin. Turning more runtime errors into compile-time errors is a good thing, at least for libraries and type-classes and ADTs are strictly better than any Java-like alternatives, but I also think a lot of Scala's potential is wasted if you try to shoe-horn it into purely-functional only.
Feel free to offer non-functional answers here, and bring your friends! There are a lot of ways to do things in Scala. You might also look at the scala/scala gitter channel.
Where did you feel the documentation was the most lacking? What sort of information was missing and what didn't you find that you needed? Most concerning, what behavior acted in a way you did not expect? I see you mention performance tuning, what sort of information would you like to see? I am saddened to hear anyone had such an experience, if I can make sure no one has a bad experience like yours again I will make every effort to do so.
Btw did you consider [finch](https://github.com/finagle/finch)? 
Yeah, things like `State` and `Writer`don't really work in the presence of exceptions because they depend on the computation completing normally. I have passed around a [mutable cell](https://github.com/gemini-hlsw/ocs3/blob/develop/modules/edu.gemini.seqexec.server/src/main/scala/edu/gemini/seqexec/server/TaskRef.scala) (like an `IORef`) for this kind of thing (logging as well) which gives the caller a handle to the state in case the task fails to compute a result. So your new effect type could be `Kleisli[Task, TaskRef[Foo], A]` or something. Kind of janky but it's still pure. It would be more common (for me) to hide this behind `Free` and let the interpreter deal with failure, which is a little easier on the user I think but may be too much engineering if you're already computing in `Task`.
You're not alone, I agree with you and I post here whenever I have time. I will say though, that if you migrate to GoLang hoping it's like Scala or something, you're gonna have a bad time. Go is pretty different, in my opinion, from Scala.
I'm still learning how to do FP. Looks like this problem is an excuse for me to look into learning transformers, thanks for pointing me in this direction. EDIT: Cheers managed to get something working based off this hint.
FWIW, I've settled on [Udash](http://udash.io/) for my personal project.
I like reasoning about referentially transparent functions too. It's hard to like Scala and not appreciate some of the fundamental benefits of functional programming. My contention is that it takes far more effort, often PHD-level effort, to write any non-trivial workhorse system using a type system that tracks effects. Take any "system-level" library that does heavy duty work. Netty 4 is a great example. Just about every library in Scala that does any sort of networking relies on Netty 4 to do packet pushing. I challenge any FP wizard to reimplement Netty 4 using pure FP in a way that does not negatively impact performance or memory pressure. If you try, I look forward to seeing the world's first pure functional implementation of a buddy and slab memory allocator, among many other stateful modules of code. And then on to distributed and database systems like ZooKeeper, Kafka, Cassandra, Spark, Elasticsearch, or an RDBMS. I would faint if I were asked to implement any of those services using pure FP, and yet all have been successfully written in imperative Java or Scala. I remember in grad school reading a research paper from someone who went through the trouble of implementing a very small, toy, lightweight RDBMS in Coq: http://ynot.cs.harvard.edu/papers/popl10.pdf. It's amazing the work he accomplished, but these kinds of techniques are costly, time consuming, and require a highly educated and skilled work force in the programming community that in my experience doesn't exist. Most of my co-workers don't even understand how to correctly use the tiny sliver of functional libraries added to Java 8, let alone basic ML-style programming in Scala. Pure FP might have it's niche in implementation of certain business logic in backend server infrastructure. However, I feel like Scala's best chance of success is to embrace it's ML and imperative backgrounds more, or else it's going to be left in the dust by newer languages such as Swift, Rust, and Golang.
Don't worry. I have a fairly strong distaste for golang. And yet, you have to admire so much of what the community does compared to the Java and Scala communities, part of which is influenced by the simplicity of the language and runtime. Take this post from netflix about switching from the JVM to Node.js: http://blog.builtinnode.com/post/from-java-to-node-the-netflix-story. They claim that one of the benefits of switching is startup time; their Java app was taking 40 minutes to start up! Now, I've actually worked somewhere where startup time is important, and I've compared the startup times of the JVM versus Node.js and Python. It turns out that all of the runtimes are dominated in startup time by time taken to load code. The more code you have, the longer startup time is. It also turns out that, at least with Java 7 bytecode, the JVM is on par with or better than Node.js for startup time. Node.js is fairly slow to start up, about twice as slow as Python 2.7 per line of code. So why did switching to Node.js help with startup time? I can only guess the reasons: * With Node.js they aren't writing a monolothic app containing, including libraries, millions of lines of code. Instead they are probably embracing micro-services or something similarly lightweight. * With Node.js they aren't using a bloated framework like Spring that, among other things, likes to scan the entire classpath multiple times on startup. This gets worse when you have a monolith. * With Node.js they aren't running aspect weavers, which again scan all byte code, as well as modify it. * With Node.js they probably rewrote their app with a better architecture so that they don't do nearly as much work on startup as before. With golang, the language and runtime are so simple that you can't make these kinds of mistakes. There's very little runtime reflection and no classloaders. Everything is statically linked. You simply cannot implement something architecturally complex like load-time weaving, or a a bloated framework like Spring because the runtime and language don't give you the rope to hang yourself with. While I don't like the language, I think the Java and JVM community at large can learn from golang's community, best practices, and libraries. It's possible to be "lightweight" on the JVM if you try.
I think he is concerned more about compatibility with current code rather than locality of equality, i.e. he doesn't want to subtly break a lot of code out there because of changes in equality behaviour
Coyoneda is about "buy now, pay later" : you store a value `F[A] `and if you have to map over it using a function `A =&gt; B`, you just store that function alongside the initial value, and the coyoneda will do "as if it knew how to apply it" and say "well you have a `F[B]` now". 
It's just troll account.
The reason the non-hardcore-FP people are unlikely to be seen here is that we are actually busy doing the actual work, which changes the world. ;) But seriously, I once really had to decide whether I want to continue to learn abstractions and data structures or whether I want to change the world. I decided to do the latter. I spent many years trying to learn as much as possible about Scala and about programming language and software design in general. I learned a lot and it was a fun time to have but interests and liabilities change over time. Nowadays I have to solve problems where no Google and no StackOverflow can help me and especially FP is also not helpful. These things make it easier for me to prototype solutions but in the end they are not the solution. Whether I use Scala or Assembly doesn't change my abilities to solve a problem, they only differ in the amount of time I would need to get there and in the amount of complexity I can keep in my head. Being able to learn about FP during the job is a great opportunity to get better and to enjoy what you do but I don't feel that it would be helpful to me to invest further time in learning even more about FP when the problems I have to solve are still unreachable for FP. One day someone may come and look at my code and decide that it is a disgusting piece of shit with way too much mutable state everywhere. They may find a way to do it more elegantly with FP and they may talk about it. But for that to happen someone has to write the software first and that someone would be me. I do sometimes talk about what I'm doing but usually not here because my main interests moved away from Scala towards other topics. Scala and FP are "just" the tools that I use nowadays to do my work.
Not a bad sentiment, just off topic.
I know, I just feel that the question presents a false duality. It's always good to be aware of the options.
It's awesome that someone actually took the time to explain why they down voted a comment. Thanks
 &gt;to write any non-trivial workhorse system suing a type system to track effects While I don’t think this is true (there are companies using Haskell in the wild for non trivial use cases, and my employer uses Scala that tracks all side effects with Task for a lot of non trivial applications), I’m not sure most people here are advocating for that type of development, all the time. You’re certainly free to use mutation in Scala, and I’d encourage it if you’re actually doing something that requires PhD level understanding of FP otherwise. However, so much of the time it’s not that much work to write a large majority of your application ‘functionally’. I certainly didn’t come from a Haskell background (came from Java/C# land) but can whole heartedly endorse using the FP parts of Scala for getting real work done. Netty works just fine as it is, I don’t see a need to reimplement it to be more functional, but I certainly think ZooKeeper, Kafka, Spark and the like could be written and improved upon using a “pure FP” Scala approach. Is it necessary? No, just like you plenty of people are fine with Redis (written in C) instead of say, Hazelcast. Now, maybe you’re main complaint is that that most of the discussion in /r/scala or on the IRC channel is based around esoteric functional programming concepts, and to that I’d say it’s just the more interesting and *controversial* stuff to dsicuss. While the services I write at work are all “pure functional”, most of the time the code I’m writing probably looks just like yours. It pattern matches on some stuff, it uses some if statements here and there, it returns some new data, lots of Lists, Sets, Maps, lots of foldLeft/Filter/Map etc. The few bits here and there that are “pure functional” might be a bit fancy, but that’s not what I spend 80% of my day doing. It’s just more fun to talk about and the kind of thing I’m more likely to ask for help on. 
http4s 0.16 has fs2 and cats support - which means integrating Monix (Task) is very easy - you just need to implement some fs2 instances. (That's what I'm doing at work)
http4s 0.16 has fs2 and cats support - which means integrating Monix (Task) is very easy - you just need to implement some fs2 instances. (That's what I'm doing at work)
&gt; However, this sub-reddit seems to be filled entirely with people that think the opposite. Nope. I have a six digit LOC code base, and there is not a single cats, scalaz, ... import. Not because I care ideologically, I simply don't have the urge to use these kinds of systems. I would describe my style as hybrid object-functional. 
If you come hang out in the Scala.js gitter chat room you'll find many of the people there are Javascript Refugees rather than Haskellers
The examples tie the action to the domain logic so you can get introduced to Akka-HTTP very easily. In a real app, you want to turn your HTTP request into a case class, and then fire it off to some controller actor, which maps it into a domain and send that to a domain actor, and so on. 
We're building a fairly complex app at work and we decided to go with scalajs-react. The quality of the bindings is tremendous; I can't say enough good things about the type-safety, the expressiveness, the careful and thoughtful way they fit Scala idioms on top of React. Starting up a standard component is not actually that hard either: object MyComponent { case class Props(unwrap: String) extends AnyVal private val component = ReactComponentB[Props]("MyComponent") .stateless .render { props =&gt; &lt;.p()("Hello, " + props.unwrap + "!") } def apply(name: String) = component(Props(name)) } Now put e.g. `MyComponent("Bob")` in some component's `render` function, and you're good to go. That said, a few caveats. First, a generic one: Scala.js JS generation (fastOptJS/fullOptJS) is slow; try to avoid it as long as possible, and instead just compile to make sure it typechecks. That said, check out the 'Reload Workflow' section at https://scalacenter.github.io/scalajs-bundler/reference.html for a neat trick that could speed up JS generation a lot (we haven't tried it yet because we've heavily customised a Webpack config and are doing all sorts of stuff there). Second, don't scatter state across your components. Keep all the state in your toplevel ('home') component and parcel out the 'scope' that scalajs-react gives you as one of the props to any child components that need to manipulate state. In fact the 'scope' objects also support lens-like zooming and functional updates, so you can pass down the _exact_ part of the state (or all of it) that a child component needs to be able to update. Third, I have to say that being able to jump to source and query the type of whatever symbol I'm looking at is a godsend with heavily-typed libraries like sjs-r. I personally use IDEA; ymmv.
Someone once asked something similar about the Haskell subreddit, iirc. After seeing people post mostly about category theory, type families, and other even weirder things, they were left doubting whether Haskell was the language for them. They got a reply: the Haskell subreddit is a place where people come to post their _recreational_ Haskell activities. It's fun for them to pursue those abstract topics, but it doesn't mean it what they do in their day jobs. More likely than not they're just doing bread-and-butter work like plugging together pre-existing Haskell libraries and debugging weird runtime behaviours. Same thing applies here I think--only even more so, since Scala is such a multi-paradigm language. You can do pretty much any kind of programming in it. And fwiw, I personally also don't buy the free monads approach--I think composing effects using [tagless final interpreters](https://youtu.be/qaAKRxO21fU) is simpler and more performant.
Thanks haoyi for your good work. I like your attitude/way towards Scala especially learnt a lot from your blog.
Can you please tell me where i can follow your Scala or other topics talk?
What's the reason people downvote this reply?
My 2 cents: programming in Scala is a fundamentally different mindset from python. The IDE can be WAY more helpful than the python REPL, because python can't give you precise info about how many arguments to pass, or what kinds of data or objects they should be. But you need to fully embrace the IDE. The scala plugin will auto-complete method and function names, show you dropdowns of what's available, let you navigate to other code, etc. It works extremely well. I do a lot of python and ruby for work, and my own experience has been that documentation for python/ruby libraries and frameworks is generally not great, so you get used to digging around to see how stuff works. In contrast, Java/scala stuff is fairly good about its auto-generated documentation, like the one for core Scala here: http://www.scala-lang.org/api/current/ It's a good habit to learn to read these kinds of docs. It teaches you to learn interfaces and how to code to them, rather than being forced to comb through every detail about how a library is implemented in order to use it. This will end up helping you grasp abstraction much better, and reduces the cognitive load of "things you need to know to use X library." 
Care to explain why? 
I always feel like if I'm writing non-functional scala code and mostly just using the language as a "better java" I'd rather just be using Kotlin. I feel like it fit's that niche much better and that Scala only starts to look appealing once you start applying some functional concepts. You don't have to go full functional to get the benefits of the Scala language mind you but if I'm just writing straight imperative code I probably wouldn't chose Scala.
There is a workaround to suppress red marks in IntelliJ when working with Binding.scala . You can define an implicit view: ``` implicit def makeIntellijHappy(x: scala.xml.Node): Binding[org.scalajs.dom.raw.Node] = ??? ``` The function will never be actually called, though it suppresses red marks.
hikaricp for database. For small amount of requests it's fine though. It's actually two APIs on finch. One of them is fast. The second one speaks to first one, and gets those 30req/s under load.
In your mind, does pure functional Scala code means Haskell style Scala?
Also checkout Lagom, which is the Lightbend CQRS implementation.
Can someone explain the issue to me?
What sort of queries?
I use a custom Action that checks whether the user has a given role. def foo = Auth(Roles.Admin).async = ... 
Then I have to say probably only a small percentage of companies using Scala actually do purely functional Scala. However I don't think the rest are not doing right.
Just your not understanding doesnt make an argument. I could say same, OOP codebases look like brainfuck (and work the same).
As a PHP Scala programmer, I thought Scala is a templating language. That's why I created Binding.scala
Monads are not necessarily about tracking side effects or state and in Scala people rarely use the IO type or the State monad. You would go a long way if you tried understanding monads, being just a very abstract and very useful design pattern that shows up even in the Gang of Four - yes, it's there, masked a little and with a different name: the Interpreter pattern from GoF is essentially the Free monad. And Scala is among the handful of languages that can express such patterns in a generic way and so it has a following. I personally don't like the Free monad btw and I have a strong dislike for monad transformers, but don't mind other people feeling enthusiastic about those and who knows, I admit the possibility that I might be wrong. As for this subreddit, I think your assertion is a little mean. The traffic here is low, people rarely post anything and those that do happen to have an interest in functional programming. If you want subjects that interest you then may I suggest that you start such conversations? Nobody is going to mind, quite the contrary, even those of us with a fetish for monads would love seeing more diversity. Or would you prefer that FP subjects wouldn't be discussed at all? But that would be a filtered bubble and what would be the benefit?
Alright, that makes sense. I'll see if I can find some examples of that style of usage. Thanks! Edit: Have had a go with this, but am struggling to find an approach that seems reasonable here. I've seen this post: https://markatta.com/codemonkey/blog/2016/08/03/actor-per-request-with-akka-http/ about per-request actors, and have implemented the first example there, but that just feels wrong. The other two examples in that article seem better, but are not without their own issues; for example, tight coupling again, or more difficult error handling. The last example seems like it could be closest to what I'm looking for, and I've also seen some code from Netflix that uses a very similar approach based on the contents of this article, however it seems that they may have only done that because they moved from Spray. I imagine the advantage of per-request actors comes in when you want to take advantage of an Akka cluster, right?
&gt; With golang, the language and runtime are so simple that you can't make these kinds of mistakes. There's very little runtime reflection and no classloaders. Everything is statically linked. You simply cannot implement something architecturally complex like load-time weaving, or a a bloated framework like Spring because the runtime and language don't give you the rope to hang yourself with. Don't get me wrong, but this is one of those cases where people are missing the forrest from the trees. Bloated solutions like Spring (using runtime reflection, code generation and that bring unimaginable complexity) only exist because Java as a programming language *isn't expressive enough*, so people found workarounds for their common problems. Spring exists because at one point it solved actual problems that couldn't be solved at the language level. You see, Java has a lot in common with Go. Just like Go, the original Java (prior to version 5) didn't have generics and was targeted at average Joe, also built on the idea that programmers shouldn't be given rope to hang themselves. Just like Go, the Java programming language came with improvements for dealing with concurrency and multi-threading - having a cross-platform and sane memory model, along with `synchronize` blocks and all sort of concurrency goodies, powered by an always available garbage collector that eliminated memory management headaches, which was very progressive for its time. Go wouldn't have been popular if it weren't developed by famous people at Google. Java wouldn't have been popular if it weren't developed by famous people at Sun Microsystems. Branding, along with convenience, is what pushed them both into the mainstream. Oh you think Go hasn't developed monsters like Spring yet because it's simpler. But you see, that's the problem that gives birth to monsters, because simplicity in the language implementation actually leads to complexity for the developer. As a former Python developer I could rant all day about how Python is actually not a TOOWTDI platform, in its ecosystem having multiple solutions for every problem, all of them slightly broken, with hacks piled on top of other hacks, powered by its non-orthogonal features, many of which evolved because Python lacks multi-line anonymous functions or because Python never escaped its past, such as its GIL or the reference counting it does in place of a real GC. People bitching about Scala not solving some implicit at compile time should try how it feels seeing Python's native MySQL client not playing well with [Gevent](http://www.gevent.org/)'s monkey patching, praying it doesn't break production and wondering if maybe they should have chosen [Eventlet](http://eventlet.net/) instead. As a side-note, when I hear about other languages being *pythonic*, I run away. As for why languages like Go (or Python, or Java) lead to complexity, that's because they aren't designed for growth and then when growth inevitably happens it gets really messy, but here I think Guy Steele's presentation does a better job at explaining it, see [Growing a Language](https://www.youtube.com/watch?v=_ahvzDzKdB0). Also relevant here is [Simple Made Easy](https://www.infoq.com/presentations/Simple-Made-Easy), a wonderful presentation by Rich Hickey. Or in other words, it's true what they say, *those that don't remember the past are condemned to repeat it*. Go is still young, give it time, it'll get there and then we'll switch to something new that repeats all those design mistakes all over again ;-) It's all an illusion anyway: https://news.ycombinator.com/item?id=9267211
because he makes statements like these: &gt; earn abstractions and data structures or whether I want to change the world they are not exclusive. Actually with attitude like that he's more likely to achieve none.
Users in the database have roles. Those roles get loaded when the user info is looked up from the session key, although you could just look them up when the user logs in and store them in a JWT.
Could you show some example of how you're combining Akka HTTP handlers with Free and Task?
play2 auth implements exactly this pretty nicely: https://github.com/t2v/play2-auth
Sorry, you are right. I just don't like when people ingrain this stuff, and I had experience multitude of people claiming "FP is bullshit for academics, OOP is real world" before chuging into their reflectiony-dependency-xml-generated-codebase that took them ages to solve simplest of problems (and usually, after "sprints" of investigations, coming up with similiar solution as to what FP offers by default - separate data from behaviour, deffer execution from model/computation description, etc...) How about we stop pretending FP is hard, and take a real good honest look at OOP, if it, by any means, is not harder, more complicated. Nah, I'm sure we're just all not following Clean Coder properly /sacrasm
I fear it might make the place too commercial. We've got the link to scalajobs.
I would stay away from akka-actor unless you're certain you need it. The great advantage of akka-http etc. is that your routing is just plain old code. So you can factor it according to the normal rules of code. I would certainly extract out parts of a route as and when it seemed necessary - e.g. if there is business logic in a given route I would want to be able to unit test that business logic separately from any routing/http-related concerns, so I would move the business logic into a service class that could be unit tested and have the route call that. On the other hand, if a given route just returns something simple (e.g. a healthcheck) there's no value in the ceremony of placing that in a separate service class. Since everything is just plain code there's no need for a one-size-fits-all layering model of router/controller/... or what have you - just treat your routing as first-class code and make it good code. My general rule of thumb is that a class should fit on a single page, and when a class gets longer than that it's time to split it up into smaller classes.
I've been consistently using Scala as an OO/FP language for 4 years. In fact, I'd say it's impossible not to use Scala as an OO/FP language. Then there is the matter of style. And that can vary among Scala developers, given the un-opinionated nature of Scala --- which, frankly, causes a fair amount of angst in the community. So, perhaps, CT, or category theory, not FP, is a more accurate label for those who revel in it. Most Scala developers simply don't have much use for raw api-level CT, because they're too busy building apps not libraries. The bigger challenge for the CT crowd is that their CT libraries either don't work well or at all in Intellij, Eclipse or Ensime. If they did, I suspect more Scala developers would use CT libraries, and thus grow to appreciate CT. I find CT interesting and occasionally useful. As a consultant, though, I can't espouse any CT library unless it has a high usability factor and just flat-out works.
&gt; that kind of toxic community This is what the people who do a lot of functional programming have written in this thread: &gt; "Java without semicolons," and if people choose to leave it at that, no harm, no foul. and &gt; Feel free to offer non-functional answers here, and bring your friends! There are a lot of ways to do things in Scala. You might also look at the scala/scala gitter channel. And &gt; If you want subjects that interest you then may I suggest that you start such conversations? Nobody is going to mind, quite the contrary, even those of us with a fetish for monads would love seeing more diversity. This is what you are saying: &gt; But the cork sniffing from these FP's is turning a lot of people away. I wish they'd stop forking scala and posting their shapeless and scalaz forks, they're just confusing the entire world and nothing will come out of that crap. Some of the shapeless examples I've seen read worse than Brainfuck. and &gt; The FP crap on this sub and &gt; if you don't like it, you can go fk yourself. People are absolutely *fine* with you wanting to write scala in whatever way you want. They might tell you it's not they way they do it, or that they don't think it's the best way to do it, but they're not going to tell you to go fuck yourself if you do. You on the other hand are acting exclusive, and anyone who doesn't subscribe to your opinion is a "cork sniffer" who should stop doing what they are doing and fuck off this subreddit.
&gt; How about we stop pretending FP is hard From my perspective FP *is* hard. That's because programming is hard. FP gives you the tools to get something right, where you have a great amount of confidence it is right, if you can write it at all. OOP gives you the tools to get something compiling, and iterate towards the goal of getting it right, or at least right enough. OOP gives me easier tools to work towards getting it right, at the cost of less confidence it's right when I'm done, and something that's more difficult to maintain. It might be what I'm used to, but let's not pretend that FP is *easy*. It's not, at least, not to a very large group of people.
Throw jprofiler at it. Seriously, I wouldn't even bother trying to think unless that doesn't work.
That seems like a reasonable approach in regards to akka-actor. I like the sound of the rest too. I suppose if you do separate out that business logic from the routing / service handling "controller-like" code then you aren't all that tightly coupled after all. (Besides which, I suppose some other frameworks tightly couple you to their implementation of a controller, if we're talking about MVC frameworks in other languages). I'll see where I get to with this, I did want to try split things up into a more modular structure, and then compose them back together. That should be fine I think, I will just have to get used to approaching this with a bit of a different perspective.
I am curious if there are examples out there that can help. Like a real end-to-end typical business application (which a ton of developers do). I have spent a lot of time getting my head wrapped around Monads and such (to the point that I actually understand what they do), mtl, type classes and the like. There is just this gap in "here is how you actually put them all together. Part of me wants to write a blog series on the matter, but I want to believe something like this exists.
I would hope so: option, list, future, either.. all in scala and all monads.
I think the problem is that while many companies are cool with open sourcing libraries, they are not cool doing the same for applications (which is understandable tbf). I honestly just don't think there is an easy recipe to follow to _design_ applications (in OOP as well), which incidentally is also why I think big frameworks are fundamentally flawed. Imho the best way to learn how to write a whole application using pure FP is start writing one, and ask for help when you get stuck _on a specific problem_, exactly like you did here. A few iterations of this, and you'll be able to do it yourself just fine, and reap the (imo significant) benefits. The only problem then is how to explain it to others :)
Yes but how do you connect these roles to the methods ? `f:(Role,Method)-&gt;Boolean` 
I fully agree you. In full honesty, i wanted to write "how about we stop pretending ONLY fp is hard". I didn't know anyone will care for that rant anyways. Thanks for pointing it out.
Asbtract : &gt; Performance critical software is almost always developed in C, as programmers do not trust high-level languages to deliver the same reliable performance. This is bad because low-level code in unsafe languages attracts security vulnerabilities and because development is far less productive, with PL advances mostly lost on programmers operating under tight performance constraints. High- level languages provide memory safety out of the box, but they are deemed too slow and unpredictable for serious system software. &gt; Recent years have seen a surge in staging and generative pro- gramming: the key idea is to use high-level languages and their ab- straction power as glorified macro systems to compose code frag- ments in first-order, potentially domain-specific, intermediate lan- guages, from which fast C can be emitted. But what about secu- rity? Since the end result is still C code, the safety guarantees of the high-level host language are lost. &gt; In this paper, we extend this generative approach to emit ACSL specifications along with C code. We demonstrate that staging achieves “abstraction without regret” for verification: we show how high-level programming models, in particular higher-order composable contracts from dynamic languages, can be used at generation time to compose and generate first-order specifications that can be statically checked by existing tools. We also show how type classes can automatically attach invariants to data types, reducing the need for repetitive manual annotations. &gt; We evaluate our system on several case studies that varyingly exercise verification of memory safety, overflow safety, and func- tional correctness. We feature an HTTP parser that is (1) fast (2) high-level: implemented using staged parser combinators (3) se- cure:withverifiedmemorysafety.Thisresultissignificant,asinput parsing is a key attack vector, and vulnerabilities related to HTTP parsing have been documented in all widely-used web servers.
I'll do that. I just couldnt have imagined that CRUD app, that doesnt do nearly anything can have slowdown on their code instead of on one of external dependencies (or barriers between).
CRUD. Some windowed/aggregated stuff across thousands rows
Wow, this is really interesting, where is this wisdom coming from ? Is this Haskell wisdom ? Is there a webpage / book where I can read about this ? Some things I don't understand: **1)** &gt; I'd have the return value of each method include a value indicating what kind of authorization it requires ... Does this monad value contain a function/action plus a value that describes authorization requrements for executing that action ? Do I get it right ? **2)** I don't quite get what you mean by "middle" layer. **3)** &gt; "get the value out" how about modifying some value ? Is this approach also good for securing CREATE/UPDATE operations too ? I guess yes, if my understanding given in point 1) is correct. 
It could be anywhere. 9/10 times that kind of performance problem isn't "core" at all, just a silly configuration-like error, either in the library or your code (e.g. I once had a spray webapp with an average response time of about 300-400ms - turns out I'd written the route such that it was doing all the DB healthchecks, serially, on every request before running the actual route. Once I fixed that it dropped down to 4ms).
I see, so you wrap functions into `Auth(Roles.Admin)(...)` ? Is the type `Auth(...)(...)` has something like `Role-&gt;(T-&gt;P)-&gt;(T-&gt;P)` ? Where `T` is the input argument type of the wrapped function and `P` is the output argument type.
Good and relevant point, though it is not really an answer to the original question.
This is a non-answer: there's also an alternative to monad transformers in the form of [effects](https://github.com/atnos-org/eff).
In my experience `Free` is useful for a few reasons. - It has no inherent meaning; it's just structure. You ascribe meaning later via an interpreter, which is nice because you can have multiple interpreters (a real one and a test one for instance). This also means that you can start writing programs before you're sure how to interpret them, and when you try to write an interpreter you will learn whether or not your user language is expressive enough. So for me it separates the top-down and bottom-up design concerns in a nice away. - It has a nice compositional model, as /u/paultypes noted. - It allows you to hide data and operations that would be exposed if you used something like `State` or `Reader` for instance. User code can only express computations in terms of the operations you provide, and your interpreter can deal with effects that you don't want user code to observe. This lets you specify your API in a very narrow way. doobie uses this to ensure resource safety and prevent leakage of lifetime-managed database objects, for instance. - Using transformers in Scala is a pain in the ass because inference is so limited. With `Free` you can interpret *into* a stack and then immediately run it, so you get the expressiveness of stacked effects without requiring your users to write programs using these awful types. So it's not a silver bullet and not without its costs, but it's a nice way to implement expressive little embedded languages (which is often a useful thing to do). 
So I will be howled down for this but: I write directly in terms of a monad I call Process. It squashes together what would be a stack of effects including Task. However, Process contains no run method nor any implementation. Process is just a family of case classes. The interpretation is provided in a separate run operation. There can be more than one interpretation simply by providing more run implementations or by parameterizing the run operation. This approach is not modular in that one monad describes all the effects. But I believe there is less boxing (maybe it is more efficient) and I know it is simple to use. The set of useful effects seems quite small and stable anyway. There are new error handling possibilities. Errors can be handled now be handled "out of band" and the error response can include recovery actions. I need to write a blog post to explain this. 
&gt; Wow, this is really interesting, where is this wisdom coming from ? Is this Haskell wisdom ? Is there a webpage / book where I can read about this ? Just my experience, which I'm afraid was all private (and I was lucky enough to program alongside some very talented people early on). Very little documentation I'm aware of (hence the Haskell link - I don't actually write Haskell myself but they often have better writeups of the concepts). I do see a lot of it as just pushing widely accepted concepts of good programming - separation of concerns, use of the type system - a little further. &gt; Does this monad value contain a function/action plus a value that describes authorization requrements for executing that action ? Do I get it right ? Yeah, the implementation would most likely be something like that. I try to avoid thinking too much about when a value is calculated - it shouldn't matter (Haskell won't even let you know the difference), and for the cases where it does matter when evaluation happens (such as database transactions) it's worth separating out that operation as a value that you can explicitly execute. &gt; I don't quite get what you mean by "middle" layer. Imagine something like this: case class Secured[A](...) trait UserService[F[_]] { def load(id: UserId): F[User] } class RealUserService extends UserService[Secured] { def load(id: UserId): Secured[User] = Secured(ViewUser(id), ...) } abstract class FriendsService[F[_]: Monad](userService: UserService[F]) { def friends(id: UserId): F[Vector[User]] = for { user &lt;- userService.load(id) friends &lt;- user.friendIds traverse userService.load } yield friends } class FriendsRoute(service: FriendsService[Secured]) { val route = path("friends") &amp; get &amp; param[UserId]('userId) { userId =&gt; secured { service.friends(userId) } } } `RealUserService` (the bottom layer) has to be written in terms of `Secured`, because it has logic that deals concretely with security (it checks the permissions). `FriendsRoute` (the top layer) has to be written in terms of `Secured`, because it too has logic that deals with security (I'm imagining `secured` as some directive-like method for applying security to a Spray route - this is the point where I should warn you that Spray is officially deprecated, but I haven't migrated yet (and may never) so I'm afraid my example will use it). But `FriendsService` can be written generically, and you could unit test it by passing in a stub `UserService[Id]` - you can be confident it propagates the security information properly because it has no way to not do so (assuming the monad is correct), it's written in terms of generic `F[_]: Monad` so it must only ever use monad operations and you don't have to test that aspect of it. &gt; how about modifying some value ? Is this approach also good for securing CREATE/UPDATE operations too ? I guess yes, if my understanding given in point 1) is correct. Yes, with the slight wrinkle that (as I alluded to earlier) you probably want to represent those operations as a value too (using something like doobie). At which point you're faced with the "open research area" of composing monads, for which your main options are monad transformers (workable, particularly if you're willing to define one or two fixed "stacks" that you want to use in your application and write helper methods to work with them, but cumbersome, and you lose the ability to write `F[_]`-generic middle layers like I described), "lazy monad transformers" via "mtl-style" typeclasses (workable and lets you write `F[_]`-generic code again, but aesthetically and theoretically unsatisfying and still cumbersome in places), and various coproduct/effect approaches (elegant, theoretically satisfying, but less mature and there are a bunch of competing libraries at the moment). I don't really have a good answer for you here (though I'm hoping there'll be a community consensus in a year or two); you might want to avoid using more than one monad at once unless and until you find the advantages compelling enough to be worth the downsides of combining them.
I like this perspective. I don't necessarily believe FP is a panacea but I do think there is a lot of really good, practical stuff to learn. Scala sort of eases the transition making it possible to Get Stuff Done while leaving the window open for improvement in ideas, process, and implementation. I'm very much in the same boat, however. I'm stuck between reading theory and applying it, landing somewhere in the middle that doesn't quite seem right (or useful, at times). But... I do think this struggle is key to progressing as a programmer, and really as a person. We should always question ourselves and each other and be open to this questioning. I'm somewhat surprised that this post in particular has been so productive-- it's something you might not find in a lot of other programming communities. Kudos, /r/scala.
Could you provide some simple example of such family of effects and an example of run operation on that family? And how do you represent the result of a function that requires a couple of effects simultaneously?
I feel compelled to answer this one ... First, let me link to a related Stack Overflow answer: http://stackoverflow.com/a/29446936/1829647 scalac, and dottyc, are large codebases. And neither has really been developed in an effort to abstract it away from the platforms. This means that there is a non-trivial amount of code that (sometimes implicitly) relies on JDK APIs that are not available on Scala.js. The most obvious part being the file system, but also reading .zip files (.jar files), generating .class files (with a third-party Java library ASM), etc. I do not think scalac's or dottyc's codebase are any more difficult than each other to abstract away enough so that bootstrapping would be possible. It is a significant amount of relatively dumb work, though. And the reward is relatively low (who wants a Scala compiler that is 3x even slowlier than the existing one?). That means there is virtually no incentive to put manpower on this task. That's the only real reason it hasn't been done yet: not enough manpower to allocated to a large, dumb-ish task, with virtually no reward. If you want to be the poor soul to work on it, be my guest :P
I was talking about Odersky and http://www.scala-lang.org/blog/2016/05/06/multiversal-equality.html . It sounds like your position (whoever you are?) is the same as mine.
FYI, a try towards composable free monad with no boiler plate to manage effects in scala: https://github.com/ISCPIF/freedsl
I don't think Free buys you enough if you can do something simple with a transformer stack. I'll use free when I'm writing a DSL for a side effect heavy API, but if I'm doing some web service requests and/or comprehending on some already functional API, I just use a transformer stack. Usually StateT[Task, B, A] is enough for most use cases since I get asynchronicity, fail fast, and state...
it helped. Too many queries to db for data that doesnt change. Small cache fixed it. Thanks.
I'd like to read that blog post.
So, for example asynchronous execution with success/failure would be written def p1(i: Int, j: Int): Process[Int] = process( if(j == 0 ) fail("can't divide by 0") else stop(i/j)) where def process[U]( step: =&gt; Process[U]): Process[U] // a process that will execute a step asynchonously def fail(message: String): Process[Nothing] // a process that never returns but may cause a side effect when run to handle the failure def stop[U](u: U): Process[U] // a process that has a constant result (ie 'pure' or 'return') 
Not if you declare it as a val
Also relevant here: [Simple Isn't Easy](http://underscore.io/blog/posts/2015/07/21/simple-isnt-easy.html)
Thanks for the code example and for the detailed answer, it clarifies things a lot. Few questions: **1)** &gt; you probably want to represent those operations as a value too (using something like doobie) For example as in [IO + Maybe] (http://stackoverflow.com/questions/32579133/simplest-non-trivial-monad-transformer-example-for-dummies-iomaybe) ? Where IO plays the role of the operation and Maybe plays the role of the Security monad ? **2)** Why is it a good idea to represent the operations as a Monad ? To create a custom vocabulary of operations that can be interpreted as needed ? Sort of like an [interpreter pattern](http://softwareengineering.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern) ? You mean that ? **3)** &gt; ` Secured(ViewUser(id), ...)` What is `ViewUser(id)` here ? Is it a value that describes if user with `id` has rights to execute user view actions ? Or is it an action that describes how to get the user object for a given user `id` ? I guess it is the second but I am not sure.
I see a lot of people conflating ordinary `Free` with the combination of free and coproducts, and I think it's important to separate these notions. People can and do use `Free` with conventional transformer stacks. In terms of Free versus custom monads: `Free` is almost always less good than a domain-specific monad where you can actually define a canonical representation and equivalence of two different instances. But it's cheaper to implement and often good enough. In terms of transformer stacks versus free coproducts, transformer stacks are more mature (with more consensus about which libraries to use) and *might* be more efficient in practice (even though their asymptotics are the same or worse), at least in current implementations. But they're theoretically unsatisfying, introduce extra ceremony (there are multiple ways to represent equivalent stacks, and ways to represent "obviously wrong" stacks), and I haven't seen any case where they have a "fundamental" advantage at all. So I think free coproducts are probably the future, but not necessarily the present.
In principle I could see the argument for using "better java" language if it had the tool/library ecosystem of Scala. But I don't think any of them other than Java itself do. As long as you have the discipline to stay away from bad libraries (something you're always going to need on the JVM), the extra capabilities of Scala don't cost you anything if you're not using them. (Kotlin specifically has enough horrible edge cases (special-cased null-handling, "non-denotable platform types") that I would never use it)
I wouldn't necessarily agree with this. At university where I went, we had an (almost) equal weighting for FP versus OOP/imperative, so at least anecdotally I have been exposed to both. First year we had to write a compiler for a subset of Haskell, and in 2nd/3rd year we had courses in Haskell/Scala/Idris/Coq (latter two are mainly used in theorem proving courses) FP is only easy if your problem is trivial in design. Once you get to building really complex products with a lot of different types of state and business requirements, than FP also starts showing its shortcomings, particularly where it comes to interfaces and modules that need changes in runtime behaviour (note that this is the reason why Scala shines, its because it allows you to cherry pick from OOP these features which is what OOP is really good at) Heck at work here we have a Haskell meetup group and someone spent a huge amount of time essentially fighting the compiler (i.e. getting something to compile by getting the types to work) for an incredibly trivial task, something that would have taken a few minutes in your typical OOP/Imperative style code. Pure FP style code (at least if you do pure FP) is incredibly generic and abstract, and so it becomes difficult for the similar reasons that subjects like pure mathematics/physics is difficult for a very large section of the population. OOP/Imperative programming is definitely easer to understand if you are dealing with non trivial problems, the downside is they push the difficulty into a different area (now your difficulty is in dealing with state and debugging, rather than understanding formal concepts). At the end of the day, everything is a tool. There is a lot of things that pure FP code is good at, but there are things that its not very good at.
Creating purely functional datastructures that have the same asymptotic runtime characteristics as their mutable OO counterparts is IMO significantly harder. How much time would it take you to create a mutable OO AVL tree from scratch? How much for the purely functional version of it?
No it does. The `%` in Java/Scala is not modulus but remainder: http://stackoverflow.com/questions/5385024/mod-in-java-produces-negative-numbers
Excellent. I had no idea PolyKinds were even on the radar; having spent a while writing junk like https://github.com/m50d/tierney/blob/master/core/src/main/scala/tierney/core/FunctorKK.scala this would make my life a lot easier. Now if only there were a way to use it with maven and scala-ide...
Notice I never brought performances in the discussion - if the goal is performance, then let's all get back to assembly! (yes, I'm being an ass. I do realise that there's a middle ground between pure FP and assembly). I'll assume, given your example, that we're both talking about FP as _immutable values_. Correct me if I'm wrong, because if not the rest of my post is going to be rather off topic. To take your example and bring it into my argument: immutable data structures are easier to reason about than mutable ones. If you have a cons list, you know for a fact that it's not going to change under your feet and you can just work with it. If what you have is a mutable list, you have no such guarantee and must either think hard about what to put behind a lock (complex to reason about) or just lock every operation (simple, but high chance of a deadlock that's going to be a right pain to debug later). And that's on top of actually trying to do what you need to do to the list. Is the mutable list faster? most likely, at least for a subset of its operations. We don't disagree there. Is the mutable list easier to write? Having written implementations of both mutable and immutable lists (or trees or stacks or most common data structures), I don't think so. I find mutable collections to be a massive pain to write, because I either need to work out just the right locks, or ignore the problem entirely and let users deal with the complexity I couldn't handle. So, yes. Mutable collections do tend to be faster, you're right. But I do find them harder to both write and reason about.
Having immutable data structures certainly is nice, and modern computers are certainly fast. So in many cases taking a large constant factor on the chin is an excellent trade off. That's not what I'm talking about. Nor am I talking about a cons list. Not that there is anything wrong with a cons list, it's an extremely useful and versatile thing to have, and most of the time it's just as easy to work with, if not easier, than something mutable. But sometimes you need something fancier. You need a priority queue. Or you need a balanced search tree. Or you need whatever else you need. So you need to implement one. And there is no way you're convincing me that implementing, say, a functional Red-Black tree that provides the same asymptomatic run time complexity as its mutable counterpart - never mind the constant factors - is easier. FP makes an excellent case for itself, and its strong parts are amazingly strong. But some things are certainly harder. If we want to sell the strong parts, we shouldn't lie about the hard part's.
In essence this just forks the language unless we know for sure all features are going to be accepted to the mainline compiler. Not sure that's something we want...
Unfortunately MTL style doesn't work very well with the typeclass encoding we're using in scalaz and Cats right now. You very quickly run into ambiguous instances (look for MTL in cats/scalaz issues for several discussions).
Agree completely with paragraph #3. I'm curious what you see as the biggest distinction between free w/ coproduct vs without (if I'm understanding you correctly.) In production use I have only used free w/ coproduct, but I would think you basically lose composability of interpreters and organization of code (bloated dsl) if you stick with a simple type constructor for your free programs. Also would love to understand better what you mean by &gt; define a canonical representation and equivalence of two different instances Is free less good in a case where you are talking non-stacked target monad vs free over simple (non coproduct) type constructor? Or any case?
Its mostly stuff that gets imported from csv files. Ends up in db. Never changes and is under constant id. Change in template =&gt; new id (=cache key) in db. Yet old users need to access old template.
I think you need to see requiring the implicit instances of things like MonadError and MonadReader in the same way as you require an implicit parameter for your DSL when using the inject approach in Cats right now. You require that the F[\_] you have must be able to do at least what you require, for example it must be able to at least handle errors at the level of your program. And if that is what you want from your F[\_] then I don't think you are coupling too much and I don't see why it would be a leaky abstraction. Free also allows you to do anything within your interpreter. If you make a DSL for State-like operations then it is not necessarily doing anything similar to what MonadState is doing. it can do whatever it wants, it could never store the things you put and return random values from get. So I think it is a matter of choosing what you want, do you choose for the freedom to do whatever you want in your interpreter. Or do you want to have the additional structure and be ensured that the instances you receive will abide by certain things (well technically you could still create unlawful instances if you really wanted, but I hope it's clear).
Ah, that's weird. Thanks!
&gt; ways to represent "obviously wrong" stacks In what way would a monad stack give you ways to represent an obviously wrong stack in which a coproduct would prevent it?
Doesn't it seem a bit unfair that we are comparing things to a sort of limited approach to what can actually be done with transformer stacks? I mean from a usability point of view (the nested lifting, having to choose the stack when defining functions), because there are still actual differences between both approaches of course.
&gt; having an interpreter that does not properly use the target monad I'm not talking about improper use of a target monad (not entirely sure what that would entail though). I'm talking about the fact that a Free monad can be interpreted into any target monad. We could interpret our State-like free monad into Id, or a fancy monad stack without any StateT in it. If we take MonadState as a requirement, our stack must contain a monad which can at least provide the State operations for us.
wrapping in a future won't do much for you if the function blocks internally. there is no magic. if all it takes to produce async code is to turn f(x) into future(f(x)), life would be much easier for all of us.
I might be off base here, but I feel you're not answering the right question. The way I understand OP's question is, for example, why do client HTTP library return Future[Response] rather than just Response - and it's a valid question, the call is blocking on a remote resource, just wrap it in a Future and work on the eventual result. You're talking about architecturing your code in a way that asynchronicity flows naturally - and you're right, of course, I just don't feel that's what OP is asking.
Because the http client has better async implementation than locking the thread. For example it may have an event based concurrency implementation instead of just stupidly waiting the thread until the connections finish
Yeah, the Okasaki Red Black tree implementation has the same time complexity as the mutable counterpart. I believe all data structures in his Purely Functional Datastructures do as well. That was heralded as a massive achievement, and anything but simple.
Great post, thanks! (reading through the co-effects post now)
Look at the red-black tree code though. It *is* simple. I mean the pattern matching in the balancing function is verbose, but its mutable equivalent is hardly better.
This is old news. Why post it again? Edit: Totally misread the post. Apologies!
Hmm. ID lookups in a database really ought to be fast - like, single-digit milliseconds at worst (if it's mysql you can avoid the query parsing overhead by using the memcached protocol). Assuming it's too much data to fit into memory you shouldn't be able to do much better than a database (and if it does fit into memory, maybe just read the files into memory on application start?). Appreciate that I don't know your use case and am probably horribly overthinking anyway.
The only difference between synchronous calls and synchronous calls wrapped in a future is the return type and that when wrapped, the execution happens on a different thread from the caller. It's still synchronous within that other thread, and that thread still comes from the same pool as the calling thread's (I'm over simplifying) The problem with synchronous IO is there's a thread that is blocked while it's waiting for the call to return. This means that thread isn't doing anything during that time. If your ExecutionContext uses a thread pool (which it does if you're using the standard global context), there are a maximum number of threads in that pool, which means if you are doing enough IO, you'll starve the rest of the application of threads, preventing the application from doing other stuff while it waits for a thread. Libraries that use futures are likely NOT blocking the calling thread, and are also most likely using their own ExecutionContext, meaning their own thread pool. This has the effect of not taking up a thread from your application. 
I don't see any leak? `def foo[F[_]: MonadError[E, ?]]: F[Foo]` and `def foo: FreeStack[Error[E] :+: CNil, Foo]` seem equally generic and equally coupled - the `F[_]` is maybe more cumbersome to work with, but I don't think that's a fundamental/theoretical gap.
For reference [here](https://github.com/typelevel/scala#relationship-with-lightbend-scala) is the current policy re: Lightbend Scala. 
Don't think Future is technically a monad, neither is Either (at least until 2.12). Future.failed(new Exception("blah")).flatMap(_ =&gt; Future.failed(new Exception("boo")) != Future.failed(new Exception("boo")).flatMap(_ =&gt; Future.failed(new Exception("blah")) 
Some more complex examples: * https://scalafiddle.io/sf/y0ixoUP/0 * https://github.com/walfie/gbf-raidfinder For transactions, simply put a large `case class` into a `Var`.
There is magic: https://github.com/ThoughtWorksInc/each
[deleted] ^^^^^^^^^^^^^^^^0.1148 &gt; [What is this?](https://pastebin.com/64GuVi2F/79191)
[deleted] ^^^^^^^^^^^^^^^^0.8161 &gt; [What is this?](https://pastebin.com/64GuVi2F/51887)
Real world application usually creates coarse grained `Var` instead of pre field `Var`, see https://github.com/walfie/gbf-raidfinder/blob/master/client/src/main/scala/walfie/gbf/raidfinder/client/ViewModel.scala#L72
To add some discussion: While I like types, I find them sometimes a wrong abstraction. Just like classes can be a wrong abstraction in some cases (if you are a FP-er you would say many cases). Types are in C a way to define how data is stored in memory. This allows C to be very efficient in data usage, and use weird operating-system data (virtual memory tables etc). Java does the same, but with less freedom and adds classes (data+ method-table). Pascal adds named types, and ranges, but somehow that did not make it popular. In Haskell and other type-theory languages, types become something more abstract. Types can now also tell how the data should be used. With Futures, we define data that is not available immediately. With Maybe we define data that is maybe not there, etc. In Rust we can manage the life-time of data. But in usage, I notice that Rust becomes much more complex due to life-time management. Also for cases that are very simple. So using types can sometimes be a wrong abstraction. It can make things more complex than simpler. Its added complexity is currently the reason why many people are not picking up Rust. So adding all kinds of type systems can be very interesting, but it can also make things too complex. That depends of course on the problems that you want to solve. Anyway, great post. I am learning a lot from it.
Will these features be backported to Typelevel Scala 2.11.9?
&gt; *Types are in C a way to define how data is stored in memory. This allows C to be very efficient in data usage ... Java does the same, but with less freedom and adds classes (data+ method-table).* That's actually not true of Java. If you'll take a look at the generated bytecode, there's not much type info specified, the actual objects being untyped, the place where types truly happen being at method call sites, the JVM needing to know the interface for which you want to call that method (e.g. Iterator.next). And it needs to know that interface in order to solve the method single dispatching it does at runtime, according to Java's call site rules. But even though the JVM was built for Java, the underlying engine in OpenJDK is based on an older Smalltalk runtime that Sun bought, by the name of [Strongtalk](http://www.strongtalk.org/) and its core is completely dynamic. This is how [invokeDynamic](https://docs.oracle.com/javase/8/docs/technotes/guides/vm/multiple-language-support.html) was possible, with language designers having the possibility to override how the JVM solves method calls and thus the name of the interface encoded in the bytecode for call sites is no longer needed. &gt; *But in usage, I notice that Rust becomes much more complex due to life-time management.* Actually it is C that is the most complex mainstream language of them all, because C has virtually no memory safety features. People view it as a simple and honest language and it may very well be a good choice for the Linux kernel, however working with C isn't doable for mere humans. C is just portable assembly and shouldn't even be considered in the league of high level languages. And every time you have a change of heart, remember [Heartbleed](https://en.wikipedia.org/wiki/Heartbleed) or any other major vulnerability in the wild that you can think of and blame C for it. Now think of how much responsibility we actually have, given that software controls everything these days, including medical devices on which people literally depend on for their lives. A language like Rust does nothing more than to make you pay attention to memory management in an environment that doesn't have a GC. It's only complex because memory management is an incredibly complex issue and ignorance is not bliss.
I specialise in [language rants](https://www.reddit.com/r/scala/comments/5sfciw/where_do_the_nonhaskell_scala_programmers_hang_out/ddfpo85/), name a language and I can say mean things about it 😝😆 &gt; FSharp is interesting because it eschewed some of the things that makes OCaml great because they also make it more confusing to newcomers...things like GADTs and Functors. But FSharp has led the way with other contributions I don't necessarily agree about those features being eschewed because of generated confusion. FSharp is the way it is because of compatibility with .NET and C#, sharing that trait with Scala, except that it started from another direction. IMO Scala achieves a better symbiosis with the host because it started from being a better OOP language, evolving to an FP one and also because the JVM is a more flexible target for language designers. FSharp actually has a complex type system, in the true meaning of the word, because on one hand you have the Hindley–Milner system imported from ML, giving you type inference, but on the other hand you have .NET's OOP with *subtyping* and they mix like oil and water. For beginners it's actually really confusing to understand when type inference works and when it doesn't, or when operator overloading works or when it doesn't. OCaml on the other hand has *row polymorphism* for its OOP, which does a better job at being mixed with Hindley–Milner. Also F# much like OCaml has no means for ad-hoc polymorphism, except that in OCaml this need is alleviated by usage of functors. And it's not like F# doesn't need that, but much like Golang, these are special features of the compiler that are not accessible for developers. Witness the type of these functions: &gt; let max a b = if a &lt; b then b else a;; val max : a:'a -&gt; b:'a -&gt; 'a when 'a : comparison &gt; let areEqual a b = a = b;; val areEqual : a:'a -&gt; b:'a -&gt; bool when 'a : equality Those special `comparison` or `equality` restrictions you see in there is special compiler magic that you can't touch. It's also confusing when operator overloading works or not. OCaml is ugly in that it defines different operators for different types, but at least it isn't confusing: &gt; let sum a b = a + b;; val sum : a:int -&gt; b:int -&gt; int This should have worked for `float` as well, because in F# that `+` is also defined for `float`. But to make it even more confusing, F# introduces the notion of member restrictions, achieving a sort of structural typing, but only for `inline` functions: &gt; let inline sum a b = a + b;; val inline sum : a: ^a -&gt; b: ^b -&gt; ^c when ( ^a or ^b) : (static member ( + ) : ^a * ^b -&gt; ^c) And the problem with this approach is that `inline` functions are not first class, meaning that you can't just have a reference to it, it's not a "value", because it's not a type encoding that the platform supports and as soon as you try, those generics get erased: &gt; let value = sum;; val value : (int -&gt; int -&gt; int) &gt; let sumList seed list = List.fold sum seed list;; val sumList : seed:int -&gt; list:int list -&gt; int It's interesting here that F# doesn't get rid of the notion of "*static member*", also borrowed from C#, which if we are honest, makes no sense for an OOP language because it's not polymorphic and besides some visibility rules being just a plain old function, but here it is, an old relic of C++ perhaps. In Scala static members are just an optimisation trick, but F# exposes it in the type system and keeps operators (like `+`) as static methods, which was a bad idea for an OOP language. Not to be misunderstood, I like things about F# and I'm playing with it, trying to learn it. You already mentioned some improvements, like "units of measure" or "type providers" and I like those. What I don't like about F#'s approach though, which also happens with C#, is that its designers *keep adding special features for limited use-cases to the compiler*. On one hand this is cool, because those features are polished. On the other hand by introducing features for limited use-cases, you keep adding complexity to the language, solutions that might not be relevant in 5 or 10 years from now because we can always move to better ways for solving those problems, or because those problems are no longer relevant. To give an example F#'s code quotations are limited. Scala's macros are more powerful, yet very complex and buggy. In Scala we could describe [scala-async](https://github.com/scala/async) as a library, which seemed really cool at first, but then many of us started to outgrow it because it's not friendly to code filled with lambda expressions, because that's just the compiler/macro rewriting your code and not actual first class continuations. If we had `scala-async` in the language, now we would be stuck with it, but we don't, so it's a matter of choice if you want it or not. And Scala macros will eventually be user friendly, given enough iterations, though macros in non-Lisp languages are always terrible and meant only for library authors as an alternative to forking the compiler. F# is *pythonic* of course and that's not necessarily a good thing ;-) Python being a language that refused to add multi-line anonymous functions and got like a dozen features to replace the need for it. Now Python got await/async too, which is good for now, at least people no longer have to abuse its generators, although I have the feeling that the monkey patching of the socket module will continue. Must say I'm not writing this to turn people off from F#. I think **F# is fun** and personally I like that it has the best bindings available for UI toolkits, due to Xamarin's work, so there are some great benefits for it running on .NET as well.
well that's 2 new terms I've just had to google from reading scala reddit (what is ML), and what is a Valleygirl :)
You can convert any asynchronous API into a synchronous one by blocking threads. You can't go in the other direction - if you have an API that blocks threads, you can only control which threads get blocked and sometimes not even that. But you can't get rid of threads being blocked and those threads being blocked is expensive because those are kernel threads. Blocking threads is also very error prone because you have to know and control the configuration of the underlying thread-pool. For example even Scala’s `ExecutionContext.Implicits.global` has an upper limit to the number of threads spawned, which means that you can end up in a dead-lock, because all of your threads can end up blocked, with no threads available in the pool to finish the required callbacks. - See the best practice advice I've given here: https://monix.io/docs/2x/best-practices/blocking.html - See my recent article on asynchrony: https://alexn.org/blog/2017/01/30/asynchronous-programming-scala.html#h2 Having asynchrony being the default is healthy. Blocking threads being the default is why many people have migrated away from Java, because it's a game of pretend that doesn't scale well. And I could tell you horror stories about what happens as a result, like [Slowloris](https://goo.gl/iagIZY). And now this issue is more relevant than ever with [Scala.js](http://www.scala-js.org/), because Javascript runtimes can't block threads, as they don't have 1:1 multi-threading.
Wrapping a blocking API into a `Future` turns the API into an async one from the *caller's point of view*. Threads still get blocked, but then you can choose what threads get blocked (e.g. having a separate thread-pool for blocking I/O) and then after retrieving the result, the consumers can do their own thing on the default thread-pool. On the other hand threads still get blocked in your app and that's not good.
Parent is talking about blocking threads. You can't convert an `FileInputStream.read()` into something that doesn't block threads, no matter what you do. And the solution you're linking to doesn't work for code making use of lambda expressions, which is how we do things in Scala, because that's just a compiler trick and not first class continuations, which would require support from the runtime. I used to like this approach, but it has so many limitations that you might as well just work with `map` and `flatMap`.
If you want to use an async call synchronously (blocking a thread), you can always do so using `Await`. If you want to use a sync call without blocking a thread you can't. So if the library wants the call to be usable without expending a thread, it has to offer the async API.
Huh? `StateT[Task, B, A]` *is* a transformer stack (if a short one).
As I understand your comment, all Pull Requests (PR) opened against Typelevel Scala will be also be opened against Lightbend Scala. However, are all of Lightbend Scala's PRs merged into Typelevel Scala? 
I guess it's an...improvement but damn if it doesn't just make sense to use either Task or EitherT, as you'd get similar semantics but it'd work in for comprehension too...
Types have always been about helping the developer in correctness rather than "simplicity". Of course having a huge sophisticated type system will make things more complex for the developper. Because learning it's rules creates a steep learning curve. And the more rules, the steeper the learning curve. Weaker type system are easier to understand and use but they are easier because they come with less guarantees. The more guarantees the type system needs to make. The more complex it needs to be. Types in C exist, as you said, to help use memory layout consistently and helpfully correctly. But it says nothing about runtime safety. Rust on the other hand guarantees memory safety and thread safety without runtime cost. But the cost is in the type system rules. They are much more sophisticated than C, and therefore harder to learn. Other languages like idris guarantees all your functions are total function and as such every code path is intentionally thought out. But again, it comes with an additional cost: learning to handle dependent types and proofs. So yes. Typesystem are complex. But they are as complex as the guarantees they provide. I disagree with the idea that they are "the wrong abstraction". They are a tool to provide guarantees and sometimes the guarantee are very complex and hard to make. In that sense it's like saying flying a spaceship manually is too complex because the interface is the wrong abstraction to pilot the ship. 
I love this and the philosophy behind Typelevel Scalac! They've basically taken it upon themselves to be the experimental branch of scalac. I have faith that the SIPs will be accepted as they demonstrate value in some of the more adventurous parts of the Scala ecosystem. The Predef override seems like it will have the most immediate impact on everyday Scala. I'm looking forward to being able to make the jump to 2.12 to take advantage of faster implicit derivation, too. One question about Predef -- how will IDEs statically know where to look for symbols? Searching the build for the compiler option?
Haha, ok! To be fair, I'm looking forward to being able to make the jump, too. But not rushed, either, due to the fact that it doesn't offer any killer features for me.
Yep. You can't reason about `Future` or `Try` so the whole setup gives me the heebiejeebies.
I agree with you, I found the first course to be extremely abstract and complex. Assignments were hard even if I had to program them in an imperative language I already knew. Those assignments looked like desgined for an advanced algorithms course.
Publish it. Even if it gets trashed, it may lead to good discussion, and newbs like me can stalk and learn
I asked this question after dealing with a frustrating library that returned Futures for its completely cpu bound calculations. I see now why it makes sense for io bound applications. 
[deleted] ^^^^^^^^^^^^^^^^0.9641 &gt; [What is this?](https://pastebin.com/64GuVi2F/40882)
That suggests the wrong scala-library ... are you definitely on SBT 0.13.13 or later? Can you point me at a github repo? Best bet would be to hop on the gitter channel: https://gitter.im/typelevel/scala
Literal types look interesting. I'm not sure exactly how they work, but the residue example sure looks intriguing. If someone would make a tutorial or provide additional use cases, I'd greatly enjoy that. Also unimporting Predef by importing a member is very sensible. Not that I ever wanted to yet. 
I actually liked the trait/class distinction, since it meant it was always unambiguous what the "parent constructor" was. IME most of the problems with multiple inheritance are about initialisation. So trait parameters make me nervous. (Of course unfortunately traits still have initialization problems with `val`s in the current scala - if early initializers are gone then does that mean those are now resolved?) I was also fond of `forSome`-style existentials - they seem like a necessary converse to generics (`forAll`). I appreciate they didn't play well with inference etc. though. I feel like there should be an elegant model here, but that's easy to say and hard to do. Similarly I'd really like to see a first-class product or union type - a dual to `case class`es - and that feels like a simplification (compared to the `sealed trait`) encoding even though in a sense it's a new concept. Maybe talking about simplicity isn't actually helping and my views are a bit more ad-hoc. But I think duals need to exist even if it's not as immediately obvious why they're useful (which I guess contradicts my opposition to coeffects - but ) &gt; null vs Option. Java interoperability again. Just don't use null. It would be nice to track null in the types, even though this will make the language and the programs more complex. If you're going down that route I'd love to see checked exceptions captured as values (like what `catching[SomeException]` either javaMethod` does, but automatic based on what `javaMethod` says it `throws`). Product or union types would probably be necessary for that though. &gt; Dynamic: how would you express that with existing features? The few times I've seen it used, the requirement would be better expressed with a macro. Mostly I just don't see it used often enough to justify how many language rules it breaks/changes, IMO. Maybe there are important use cases I'm not aware of. &gt; control flow keywords. Interestingly, early versions of Scala had while and do as methods, not control constructs. They got turned into keywords because that was easier for syntax highlighting and syntactic error recovery. In retrospect I think it was no big deal either way. They're rare enough that it's hard to remember what they do without being able to click through to the source (at least for me), and they make shifting to monadic versions look more complex than it is. I'm not sure what the right way to do syntax highlighting is, but to my mind making it easier to highlight `while` when it's still hard to highlight `ifM` isn't useful enough to be important. (I know `while` gets used for low-level "java in Scala" style code, but I don't think that's worth complicating the language proper for - apart from anything else Scala has excellent FFI into Java and it would be fine IMO to write core "hot loops" in Java). &gt; structural types - that's actually a bit ironic. If you want to be fundamental then structural types come first. Every foundational type system I know of has structural types. Nominal types always have this awkwardness that they mix user-defined definitions with actual structure. I would claim that DOT is the first foundational type system that gives a satisfactory account of nominal types as abstract types. So Scala made a bet on structural types because they are a fundamental language construct. It turned out it did not have many uses. If Scala was designed from scratch today it might well not have structural types. But in a sense that would make the language more ad-hoc, not less so. You're right, at the foundational level. I guess my view is that nominal types are important enough to programming practice (at least given that our type definitions don't include laws) that our theory needs to capture them. In current Scala structural types break some of the rules by allowing a function to have specific behaviour in terms of my type without having any explicit link to it - if my type defines `florble` then I'd expect anything that calls `florble` to have a reference to my type, even if indirectly via a typeclass etc. (Similarly I'd like `for`/`yield` to desugar to methods on a defined interface rather than ). But I don't know if that generalises to anything useful. Vaguely relatedly, I'd like to see a way to define general types that don't implement all of `AnyRef`/`Object`. Particularly with Scala no longer being tied to the JVM, it would be great to be able to define types that simply don't implement e.g. `equals`. &gt; implicit parameters: That got me really puzzled. They are the essence of Scala, including it's claim to minimality. How would you make them redundant without introducing something much more complex? Heh, I was thinking back to our discussion about implicit parameters to functions (I go by lmm some places), and also possibly just getting a bit confused. Making functions and methods less different/more unified is a great thing. I'd love to see that taken further - I'd like to be able to write generic (type-parameterized) functions (which probably requires polymorphic values), and ideally functions that take named arguments. I'd also like to be able to write functions that take by-name parameters, perhaps just by making `=&gt; Foo` an ordinary type that values can have. (Perhaps that could be a solution to the problem of collection views / fusion / ... - `List[A]#map` is (and IMO should remain) strict but `(=&gt; List[A])#map` can be lazy. The conversion from `=&gt; A` to `A` could remain but become an implicit conversion, so that it's possible to define higher-priority variants. That'd probably be a very radical change though). At the same time I think implicits are maybe too general for their use cases. Maybe this is a point where the language should be complex to match how it's used. I see reasonable uses of implicits as extension methods, typeclasses and type level functions, passing parameters down, and the magnet pattern - maybe those should be split. Extension methods are largely fine as `implicit class` (and that construct is rarely used for anything else) - maybe those should be separated from general-purpose implicits. I'd like to see some slightly more direct support for typeclasses and type-level functions - at least to the extent of the compiler knowing that `MyTypeClass[A].B` is the same type as itself without having to pass `=:=` around everywhere, and some way to reduce the boilerplate for custom typeclasses even if it's just syntax sugar. Passing parameters down is error-prone because it can very easily fail silently (e.g. for an `implicit ExecutionContext` it's very easy to forget to declare it at one of the methods along the way but pick one up from the containing class, or the global one, by accident). I really don't find implicit parameters a great fit for this (though I know you disagree) and would rather use an explicit parameter or a reader monad - or just some slightly more explicit syntax for calling with an implicit parameter, like the `&lt;-`/`=` distinction you get with monads. But most of all I was just concerned about the whole effects concept in that discussion; I think that the monads-and `for`/`yield` encoding of effects, while it has its issues and compiles to something inefficient, actually works very well. So for any "lightweight effects" implementation (and an `implicit` parameter is on some level just a very lightweight effect) I'm very anxious that it interoperate with what we already have, ideally by being explicitly defined as syntax sugar for `for`/`yield` etc. (even if that means having to reconstruct optimization opportunities that we throw away via that encoding). If nothing else this is showing me that language design is hard; as always, thanks for Scala. That said I would try to emphasise that as a user of the language (who doesn't/can't watch videos) what I'm most anxious about is having no idea when anything new will arrive. I have no sense of when (if?) Dotty is going to arrive or even much new functionality in Scala (I think the roadmap that was previously published has disappeared from the website?). I'm now using kind-projector in my projects and seriously considering trying to use typelevel Scala; I don't want to burden potential contributors with the complexity of setting up a different toolchain etc., and a lot of the fixes feel very ad-hoc, but I can't justify ignoring them in favour of more general solutions that are on the way if I don't have any timeline for those being actually delivered.
Sure, I don't disagree with any of this. My main point is, that the specific vulnerability in heartbleed wouldn't have been addressed by using RUST because its highly likely this the specific part of code which caused heartbleed would have been written either in C/ASM anyways. The biggest issue with the heartbleed vulernability was more the quality of the code which is something that LibreSSL is attempting to solve
&gt; structural types - that's actually a bit ironic. If you want to be fundamental then structural types come first. Fully agreed. In fact, structural types are essential if one wants true modularity, something that up until now one could only achieve via the structural matching of ML style modules (and I say this as a hardcore Haskell guy). I think one of the reasons why structural types don't see much use in Scala is that they're implemented through reflection, so people tend to avoid them because of the possible performance implications
Thank you SO much for writing these. I've read through the first three, and those have made the most sense to me out of anything else I've read. I really like how you get into what's going on at a lower level, especially the decompiled java bytecode. It really helps to identify the differences between scala and other languages. Keep up the good work!
If running tests the default configuration for sbt, just start sbt with the -debug flag, passing in a port number for intellij to connect to with it's remote debugger. If you fork tests into their own VM when run, you'll have to add jvm flags to turn on JDWP in the `javaOptions` settings in `Test` scope. EDIT: the flag is `-jvm-debug` not `-debug`, see `sbt -help` for online assistance.
You explained the issue with [Future](https://www.reddit.com/r/scala/comments/3zofjl/why_is_future_totally_unusable/cyns21h/). However, what's the problem when reasoning about `Try`? You're talking about https://issues.scala-lang.org/browse/SI-6284?
What's the value in `Future[A] =&gt; Future[Try[A]]`? `Future[Either[A, B]]` appears more valuable to me than `Future[Try[A]]` since the `Left` can indicate an application-specific error. Whereas, `Try`'s `Failure` simply holds the `Throwable` available in the failed `Future`, I'd expect. However, I'm doubtful of my doubts of `Future[Try[A]]` since you find value in it.
Sure, but your integration test is much simpler now because you only need to test the mechanics of sending [any] email. Your business logic ("did this action generate the intent to send an email with the address/content, etc.") is now a normal functional unit test. So the concerns are now separated.
Could you please give an example? I don't follow the "non-termination" piece.
Good to see people using scala on Android 
Exceptions are outside of what you choose to express since they can always get thrown. Converting a Future[A] to a Future[Try[A]] makes sense because Try[A] is used internally to store results and unexpected errors, and by converting it you can expose it for processing by means of map and flatMap. To answer your question more directly: Try because it is a Try that you get in onComplete and IMO there's no point in making matters more complicated. You can always convert Try into Either.
Yes, I got the idea much better after reading http://rea.tech/to-kill-a-mockingtest/ and seeing the example where he pushes the effectful part of the code to the outer layer. Where I work now people are doing unit tests and what they call functional/behaviour tests which is driving me nuts. The code is very messy with effects (auditing, logging) mixed with "business logic". I need to study Free and start putting this into practice.
Functions throwing and catching exceptions yield non-deterministic results. Often exceptions get thrown because of problems outside the program's control, like bugs, going out of memory, networking issues, hardware problems, etc. Try isn't pure because it deals with exceptions. Try isn't a monad because this law fails: Try(expr) flatMap f != f(expr) BUT that's only if functions throwing exceptions can be considered *mathematical functions*. I for one do not - at best you can consider them partial functions throwing exceptions for invalid input. That's because exceptions are doing stack unwinding, triggering essentially an alternative exit point for your program, invalidating the rules of *structured programming* as well. Exceptions are basically an alt-truth 😜 Note that the monad law above is broken for all Task and Future implementations as well. And everything that ends up doing asynchronous execution actually. Because with async execution, if you're not catching exceptions, they'll blow the stack of some lonely thread somewhere and nobody will hear it. In other words Try isn't a true monad or functor, if functions throwing exceptions are fair game, but can be considered one for practical purposes.
It's a subtle point, I guess, but: the actual reason element access in Scala collections looks like function application is because it is. In other words: &gt; scala&gt; Map(1 -&gt; "foo", 2 -&gt; "bar", 3 -&gt; "baz") res0: scala.collection.immutable.Map[Int,String] = Map(1 -&gt; foo, 2 -&gt; bar, 3 -&gt; baz) scala&gt; List(3, 2, 1).map(res0) res1: List[String] = List(baz, bar, foo) works.
There is some subtlety to this. Equational reasoning says that if you perform a legal substitution the new program and the original will compute the same value *if they both terminate*. However a legal substitution can result in non-termination ... given a lawful functor `fa.map(f andThen g)` might work while `fa.map(f).map(g)` exhausts memory for instance, but if they both work they will give the same answer. However this is not the case with `Try` because it *detects the non-terminating case and uses it to compute a value*. So in the example above the substitution would result in a *different* program: one that computes a `Failure` rather than `Success`, which means the substitution was not in fact legal. This can also happen with substitutions inside a `Try` block or within the body of a method passed to `map` or `flatMap`. As for types like `IO` and `Task`, these treat exception trapping as an effect that you can *talk about*, but nothing actually "happens" until you `.unsafePerformIO`, which is a side-effect, but it's outside the scope of your program. It's up to you whether you think this is a problem. I can barely remember one set of rules so I stay away from things like this.
This is what I do
I think good maintained typescript to scalajs generator would greatly help scalajs ecosystem. Threre could be some bot that watches new typescript definitions and automatically compiles and publishes scala wrappers...
&gt; Would I just make a class that contains those functions with a companion object? Sound good. Though you may not need a class at all, just an object if all your methods are static. Something like object MyMath { def someFancyFunction(x: Int): Int = ... } &gt;How would I test the functions with a singleton object? Would I make a new instance of the class and test the functions through that object? You don't need to instantiate anything when you use a companion object. Just call it like MyMath.someFancyFunction(12) in your tests. 
So, you reinvented part of scalaz? Not to say it's always a bad idea but it seems like it is often not the best use of time...
Neat, good luck! Sounds incredibly useful.
Thanks for the clarification.
I have a library called Liberator (https://github.com/aecor/liberator), it looks very similar to yours :)
This is why I've written authorization and authentication implementations on every project I've worked on the last ten years. The business rules are notably different in every single case, which is why I've not seen a implementation that works well with all of them. When deferring to a framework like Sprint Security previously, I found myself eventually overriding the majority of the default logic in the framework. Also, keep in mind that for many apps, it's not just a matter of "can the principal invoke the given function", but also, can they see all of the results. Perhaps User X can only see equities A, B, and C. In that case, you'll need to filter the results of said function to elide equities D and E. Of course you could just push down the filtering logic to the repo level, but then you'll fail to contain the A&amp;A logic at the controller layer, vastly increasing the A&amp;A surface area. A&amp;A sucks, no wonder startups ignore it. YMMV, but it depends how customized you business stakeholders require your application A&amp;A to be.
I think that's a lot more difficult than it looks. The likes of e.g. collection APIs (and really, any APIs) look very different when they're designed to be used without GC - indeed most APIs change, because there's a whole new set of concerns around managing ownership of things that are just irrelevant in a GCed language. I've worked in mixed Python/C++ projects (using swig) and the difficult part isn't the simple ABI/function-call -level compatibility, it's handing over ownership management to/from a language that isn't designed for it.
I'm pretty sure Processing is a separate language, maybe it has some bindings for Scala? As for Swing, it was supposed to be replaced with JavaFX, so you could just use http://www.scalafx.org/. Still, it's more of a general UI library. For games you might want something like https://www.lwjgl.org/ or https://libgdx.badlogicgames.com/.
&gt; Sure it's true that I've only written ~700 lines of Go total Of which half were `if err != nil` I actually love Go for really basic, small apps that can benefit from its concurrency model. Plus it has a really great stdlib for the modern web. Writing anything non-trivial in it, however, is just torture, though. Boilerplate galore, shitty type system, etc.
Neither is particularly suited for game development. Use libgdx, it's fast, well-maintained and documented: https://github.com/libgdx/libgdx/wiki/Using-libgdx-with-Scala 
Nitpicks on methods: - methods are not functions, because they imply an implicit `this` argument and because of the single-dispatching done, the actual implementation for a call-site not being known at compile time - methods aren't first class, i.e. they aren't values, although you can create a function that invokes a method (by obviously making `this` explicit or capturing it from the context) - methods attached to top-level objects are not methods; for one, because they are actually compiled as static methods, which is a misnomer as static methods aren't methods Also, objects with an `apply` method aren't necessarily functions. Even though they look and smell like one, it's just syntactic sugar, as you can't pass them as parameters when actual functions are expected, functions being objects inheriting from `Function0`, `Function1`, etc. The author's `MyFunction` trait, even though it's good for didactic purposes, doesn't describe Scala functions, although conversions are possible.
FTR: turns out using it with maven is totally easy. PRed to update the documentation.
Lol I was recently in a phase where I was trying to make my code as functionally pure as possible, I stopped because it cost me in my productivity because I spent too much time thinking about what I would do in advanced and the resulting over engineering, also the smugness that started creeping in kind of bothered me. In the long run it did improve my coding style, to where my productivity is boosted, the quality of the code itself improved, and the maintainability was greatly improved. So I think all scala developers should get really into haskell or any ML or Lisp. But wanting scala to be haskell is asinine. 
Processing is a "language" that's really just a library for Scala (the "embedded DSL" style), AIUI. It's very elegant if what you want to do fits within it, but I wouldn't try to use it for something that extends beyond data visualization.
Shorthand `enum Color { case Red, Green, Blue }` syntax might make it in as well, which would be nearly as terse as ML based languages for simple ADT definitions. Regardless, scrapping the sealed class/trait + case class/object ADT boilerplate will be a welcome addition to the language.
In what sense are they dual?
He's probably thinking of the way an enum implements a coproduct over the enum case types ('i am any one of these') and case classes are isomorphic to products over their key types ('i need one of each of these'). Categorically, these are dual concepts. That's my guess anyway.
ah I see. I thought he meant representation via sealed traits (case classes) hierarchy to be dual to enum in some sense.
Not Scala specifically, but I believe Spark uses [univocity's CSV parser](https://github.com/uniVocity/univocity-parsers) by default. Univocity have also created [a comparison](https://github.com/uniVocity/csv-parsers-comparison) between Java compatible CSV parsers.
Have you seen [featherbed](https://github.com/finagle/featherbed)? It's quite awesome and wraps Finagle HTTP clients (shameless self-referencing: take a look at [this blog](http://vkostyukov.net/posts/finagle-101/) to see what Finagle clients can do) with some type-level constructs.
First impression: overwhelmingly favorable. I think using `enum` for paramaterized GADTs is almost abuse of the term... but ingenious abuse. The majority of the boilerplate I write in scala is ADT definitions, which this will largely simplify- and in doing so, improve readability. I think it's also a great name for what is basically syntactic sugar from a *marketing* perspective. Now I don't have to start off explaining the virtues of ADTs; I can simply say "look at how much better Scala's enums are". This is true enough to be useful, and un-intimidating, which is a good thing. Also important- use of the (intentionally, justly) scarce resource of syntactic sugar on ADTs signals to newcomers what the language is "really about" more effectively than any amount of explanation or persuasion. Making ADTs painless convinces people that they are doing the right thing as they fumble along the path to Scala proficiency. Basically: yay!
But not really consistent with Java enums. I guess another factor is that `enum` is less likely to have been used as an identifier (e.g. a variable name) in existing code, than say `data` (a la Haskell). I guess over time `enum` will shed its semantic baggage in much the same way as say `static` has in Java. 
There is a library to enable Go style concurrency for Akka: https://github.com/qifun/stateless-future-akka I created this library 3 years ago, and I hope I could turn the project to a typelevel member. Leave some feedback at https://github.com/typelevel/general/issues/58 if the idea interest you.
Those are not well regarded stable projects...
I'm strongly in favor of this. I only [spent +50 hours hacking through this problem space](http://stackoverflow.com/a/25923651/501113) a little over 2 years ago trying to get exhaustive pattern matching working. I got the abstract case class idea from Odersky himself as I was trying to sort it out. Whoohoo! I _really_ want this to make it into the language. 
I had not used it before; thought those were just some extension methods. But now I see it can also replace return and parameter types of facades. That empty trait is a clever trick... My mistake, I should have read the article more closely. But this is not yet mentioned in the relevant docs page: [Write facade types for JS APIs](https://www.scala-js.org/doc/interoperability/facade-types.html)
I'm the author of [kantan.csv](http://nrinaudo.github.io/kantan.csv/), a mildly popular CSV parsing / decoding library. It comes with an acceptable parser, both from the point of view of features and [performances](http://nrinaudo.github.io/kantan.csv/tut/benchmarks.html), but your question is about the *best* parser. Having implemented connectors for most major libraries (when I could, some popular ones could not be supported because they failed to support features I consider requirements), I've come to the following conclusion: - [jackons csv](https://github.com/FasterXML/jackson-dataformat-csv) is just incredible. Not quite as aggressively maintained as I'd like, but the lack of updates might just come from the fact that it already does everything it could reasonably be expected to do. - [univocity](https://github.com/uniVocity/univocity-parsers#reading-csv) is a close second in terms performances (for the benchmarked use-case), and much more frequently updated. - anything else just lags far behind, either in terms of performances or features. Now, I'm assuming here that what you mean by best is: fastest and / or weird CSV corner-cases support. If you mean easy to use in Scala, then my answer is quite different, and you should look into one of the type classes based libraries.
I like [tototoshi/scala-csv](https://github.com/tototoshi/scala-csv). This library is very simple, easy to use and practical enough.
The notion is that `enum` types are defined by an enumeration of their constructors. Constructors can be values or they can be parameterized. 
You should probably read [destroy all ifs](http://degoes.net/articles/destroy-all-ifs) 
Now, if I get a way to write extension methods without going through implicit class ClassNameIllNeverNeed (val something: Something) extends AnyVal {def foo = ... } I'll be very happy indeed.
Oh I used the pattern only in this post. in reality that trues and falses are driving by business rules. they don't really follow the triangle pattern. sorry. I just wanted to beautify the post.
akka-stream with `Framing.delimiter(ByteString("\n"), maximumFrameLength = 4096, allowTruncation = true)`
Thanks... this is really a good suggestion. Let me try this out and see if my code improves.
Processing is a Java library. It also has a JavaScript version, Processing.js.
Not really a trick, but I would capture these patterns in `Criteria` methods and then interrogate them: case class Criteria(b1: Boolean, ..., b6: Boolean) { def isFoo: Boolean = Array(b1, ..., b6) forall identity def isBar: Boolean = Array(!b1, ..., b6) forall identity ... } ... if (criteria.isFoo) // do something else if (criteria.isBar) // do something ... This has the advantage that the business logic is encapsulated and not exposed. True, it's not pattern matching, but in any case you weren't getting the full benefit of pattern matching--exhaustiveness checking--because you don't care about all possible combinations of these booleans because they're not all valid. That's actually the root of the issue--all these booleans allow many nonsensical states. It suggests a refactor, as Yaron Minsky says, [make illegal states unrepresentable](https://youtu.be/-J8YyfrSwTk?t=17m59s).
canada, most probably
nice.
I dont see it as big problem tbh., imho you shouldnt need to write many extension methods so it doesnt warrant new language construct
Concur. This is what I use to accept a streaming upload of CSV data since it has a streaming parser.
You could set some defaults to the parameters (which also requires you to name your parameters) in your Criteria class constructor to reduce the number of booleans you need to pass in (at the expense of being more verbose by also providing a parameter name) [Here's a link that explains things](http://docs.scala-lang.org/sips/completed/named-and-default-arguments.html#default-arguments-1) Here's an idea of how that would work applied to your example: criteria match { case Criteria(b0=true, b1=true, b2=true, b3=true, b4=true, b5=true) =&gt; // do something case Criteria(b1=true, b2=true, b3=true, b4=true, b5=true) =&gt; // do something case Criteria(b2=true, b3=true, b4=true, b5=true) =&gt; // do something case Criteria(b3=true, b4=true, b5=true) =&gt; // do something case Criteria(b4=true, b5=true) =&gt; // do something case Criteria(b5=true) =&gt; // do something case _ =&gt; // do something } You could also invert the default to true, as opposed to false, depending on if you tend to be passing more trues or falses
`IO` gives you composition and equational reasoning, which is a worthwhile improvement over side-effecting code. The smaller your IO primitives the more apparent this will be; wrapping your entire side-effecting program in `IO { ... }` buys you nothing of course. A better approach I think is to use the free monad to define a language for your domain and then interpret it into `IO` or `Task` or whatever. This gives you a way to speak in terms of only the types and operations that make sense in your domain, rather than speaking in terms of `IO` which is entirely unrestricted in what it can do. There are many talks and blog posts on this approach. For implicit ordering of effects you can introduce a data dependency (the preparation action returns a value that is necessary for the processing action); or provide languages that nest inside one another (the preparation action takes a processing program as an argument, and they have distinct effect types); or just have an imperative structure in your monadic code and write tests to ensure it's in the right order. It will depend on the probability of making an error and the difficulty of detecting it. As for resource safety look at the exception-handling combinators on `IO`. There are many and they perform the bracketing operations you need. These are lacking on `Task` but all can be expressed in terms of `.attempt` and `.fail`. Hope this helps. These are all good questions. 
Monads capturing side-effects such as `IO` and `Task` will have a lazy `map` operation, so a good approximation of `delay` is this: def delay[F[_], A](f: =&gt; A)(implicit F: Applicative[F]): F[A] = F.pure(()).map(_ =&gt; f) 
akka streams is your friend
Option 1: criteria match { case c: Criteria if allTrue(c) =&gt; //feel free to use more meaningful name than "allTrue" case c: Criteria if allTrueButFirst(c) =&gt; ...... } option 2: custom objects with unapply(c: Criteria): Boolean criteria match { case AllTrue() =&gt; case AllTrueButFirst() =&gt; ...... } 
Couldn't you use unapply creatively to give the rules / masks you are matching readable names? http://docs.scala-lang.org/tutorials/tour/extractor-objects.html So you could create extractors named after the business logic that they need to follow. I'm extremely inexperienced in Scala so apologies if this is unconventional...
How many times do you need to read the file? `io.Source.fromFile(file).getLines` returns a lazy iterator into the file (so you're not loading 10s of GBs of data into memory), but because it's an iterator it can only be read from once. Then it's just something like io.Source.fromFile(file).getLines.foldLeft(0){ (count, o) =&gt; if (meetsCondition(o)) count + 1 else count } to do simple counting of the objects that meet your condition. I wouldn't call this super *functional*... I mean, it's lazily reading the file, so IO, IO everywhere. There's work to be done to make things nicer/safer, but it might be closer to what you're looking for. I found websites like: http://eed3si9n.com/herding-cats/ http://eed3si9n.com/learning-scalaz/ http://learnyouahaskell.com/ To be helpful in thinking about things functionally and being aware of tools/abstractions available out there. 
 $ amm Loading... Compiling replBridge.sc Compiling interpBridge.sc Compiling HardcodedPredef.sc Compiling ArgsPredef.sc Compiling predef.sc Compiling SharedPredef.sc Compiling LoadedPredef.sc Welcome to the Ammonite Repl 0.8.2 (Scala 2.12.1 Java 1.8.0_112) @ import $ivy.`co.fs2::fs2-io:0.9.4`, fs2._ :: loading settings :: url = jar:file:/usr/local/bin/amm!/org/apache/ivy/core/settings/ivysettings.xml :: resolving dependencies :: co.fs2#fs2-io_2.12-caller;working confs: [default] found co.fs2#fs2-io_2.12;0.9.4 in central found org.scala-lang#scala-library;2.12.0 in chain-resolver [2.12.0] org.scala-lang#scala-library;2.12.0 found co.fs2#fs2-core_2.12;0.9.4 in central downloading http://repo1.maven.org/maven2/co/fs2/fs2-io_2.12/0.9.4/fs2-io_2.12-0.9.4-sources.jar ... [SUCCESSFUL ] co.fs2#fs2-io_2.12;0.9.4!fs2-io_2.12.jar(source) (136ms) downloading http://repo1.maven.org/maven2/co/fs2/fs2-io_2.12/0.9.4/fs2-io_2.12-0.9.4.jar ... [SUCCESSFUL ] co.fs2#fs2-io_2.12;0.9.4!fs2-io_2.12.jar (207ms) [SUCCESSFUL ] co.fs2#fs2-io_2.12;0.9.4!fs2-io_2.12.jar(javadoc) (278ms) downloading http://repo1.maven.org/maven2/co/fs2/fs2-core_2.12/0.9.4/fs2-core_2.12-0.9.4.jar ... [SUCCESSFUL ] co.fs2#fs2-core_2.12;0.9.4!fs2-core_2.12.jar (163ms) import $ivy.$ , fs2._ @ import fs2.{io, text, Task} import fs2.{io, text, Task} @ import java.nio.file.Paths import java.nio.file.Paths @ io.file.readAll[Task](Paths.get("bigpic.md"), 4096).through(text.utf8Decode).through(text.lines).scan(0)((old, v) =&gt; if (condition(v)) old + 1 else old) This gives you a `Stream` of `Int`, the last of which will be the number of lines satisfying `condition`, so you'll want to `runLast` the `Stream` to get the (`Either` of the `Option` of the) result. Also, I've taken the opportunity to show off just how nice and easy [Ammonite](http://www.lihaoyi.com/Ammonite/) makes doing this kind of exploratory fiddling. :-)
I think it's a mistake to try to finesse this operation … people would be tempted to use this with `Future` for instance. This is a very special things and it needs to be crystal-clear what you're doing.
&gt; but Java parallel streams and .Net PLINQ do not have that problem. What problem?
I wrote this library and I'd be grateful for any feedback you might have. Thank you!
Even more succinctly: io.Source.fromFile(file).getLines.count(meetsCondition) 
Good question - the syntax would definitely be closer to plain React. However, you'd probably lose autocompletion and compile time errors. It's worth a shot - would you be interested in implementing this?
I've researched the topic a little more, and it seems that XML literals will be removed in Dotty, which will probably become Scala 3.0 eventually: http://dotty.epfl.ch/#why-dotty
Java streams only let you visit each element once, is hardly comparable with a random access array
I can see how our backend guys would like this and that's why I'm not going to share :D My life would get so much more miserable if I had to write my frontends in Scala. It's a nice proof of concept though. And I'm sure that there are projects where it's going to be useful. Nice job!
Thank you. I was really hoping it would :)
Haha, thanks! Yeah, Scala.js compile times are a pain sometimes :) Still, it's nice to be able to share types between the client and the server.
Something specific you don't like about https://github.com/japgolly/scalajs-react?
Excellent question - I wrote this after using scalajs-react, and the syntax for building VirtualDOM is inspired by it. There are some things that bother me about that library: - Components aren't classes. Instead there's a builder system with very complicated types and macros. - You can't use ordinary functions and methods. You have to use scalajs-react specific function types: `Callback` and `~=&gt;`. - You have to memoize callbacks in order to make shouldComponentUpdate() work. - It uses non-alphanumeric identifiers for non-operators: `$`, `&lt;` and `^`. - It rejects the built-in equality in favour of a macros-and-implicits based equality type class. - It monkey-patches base types with lots of methods so you can write "2 px", instead of taking the straightforward approach "Px(2)". - It's awkward to access state and props, and you're encouraged to use monadic style. As a result of the above, the scalajs-react code I write feels very out of place, and looks nothing like ordinary Scala code to me. Macros and heavy usage of implicits also lead to really bad error messages like "ambiguous implicit values: both &lt;unrelated implicit 1&gt; and &lt;unrelated implicit 2&gt; match &lt;your type&gt;". I wrote React4s partly to get rid of these problems as I perceive them, and partly to expose a simpler API for React that's tailored to Scala rather than to JavaScript. 
This is correct, but the idea is that an `xml"&lt;foo/&gt;"` interpolator can be implemented. So using embedded XML syntax is not necessarily a no-go even with Dotty.
What about regular running jobs? They often run on their own platform so they don't block something else when they consume all cpu power.
No prob. Enjoy Scala! The standard lib collections have a very useful set of methods. (`Iterator` has the same API as a collection, basically) You might be interested in the [Scaladoc for TraversableOnce](http://www.scala-lang.org/api/2.11.8/#scala.collection.TraversableOnce), which `Iterator` inherits from.
Yeah, I think this speaks to how I'm probably still mostly thinking about my program in imperative terms. For my specific case, I feel as though I am doing a bunch of specific one-shot operations that don't make a ton of sense to generalize or compose. Perhaps I am wrong but I want to try and learn functional styles so I can at least make a proper judgment on whether I find it useful or not. In a more general sense. I think I have an okay understanding of many of these concepts in a vacuum, but I'm totally lost when it comes to actually putting it all together and building an application. Like I know what the words mean but can't make a sentence.
For anyone interested, I've been using https://github.com/tpolecat/tuco as a learning base for the Free monad.
Hi there, friend! I'm the author of scalajs-react here do that annoying, boring thing that library authors do when they see their library mentioned and reply to each point in excruciating detail that no one will read. I think there are a few (understandable) misunderstandings I'd like to clarify. Firstly though, allow me to say that I've been working on a [rewrite (which will be 1.0)](https://github.com/japgolly/scalajs-react/tree/topic/neo) for a while now that will simplify usage and address some of the things that bothered you. I'm nearly done and it should be out this month. You know, the current version started as a proof-of-concept looking similar to your React4s, then over the years I kept bolting ad-hoc features on. The more serious my usage the more I needed from the library, the more React-related bugs I discovered and so beefed up the types to prevent, the more support I added for making React performant, testing it, etc. A word of advice: be aware if you plan to use it for big/serious projects that it might take a lot of time and effort discovering everything you need but didn't know you needed, then writing it and getting it right. I wouldn't have believed how much work I'd pour into scalajs-react when I first started it. Anyway: the excruciating point-by-point response I promised: * ATM you can't create a component directly by extending a `class` - true. it shouldn't be hard in the new version for someone to contribute such an abstract class and I'm hoping someone will because that will mean the end of me having to hear about how I don't have it. :D I personally prefer passing all the interfacing functions (that you'd specify as methods in a class) to a builder better because it adds more power like having external, reusable features that you tell the builder to use; in such a scenario it becomes a one-liner and always works where as trying to do that through a class mechanism it's 1-n lines and it doesn't always work. * Absolutely agree that the types became too complicated and spilled over into users' code. This has been significantly improved in the new version. There are still complicated types under the covers but they're much more hidden, and the types you use as a user are much simpler. * Yes and no. You do need to wrap your callbacks in `Callback`, it's just a wrapper and gives you compile-time protection against subtle bugs I've hit in the past. I find it incredibly useful and it's here to stay so I understand people who dislike it moving to Sri or react4s. Regarding `~=&gt;` howewer, you don't *have* to use `A ~=&gt; B`, that's an advanced feature for when you're improving performance; normal `A =&gt; B` functions are fine in their place. * The `&lt;`, `^` are optional and there is an import that you can use that doesn't use any symbols. See [with](https://github.com/japgolly/scalajs-react/blob/topic/neo/core/src/test/scala/japgolly/scalajs/react/vdom/PrefixedTest.scala) / [without](https://github.com/japgolly/scalajs-react/blob/topic/neo/core/src/test/scala/japgolly/scalajs/react/vdom/UnprefixedTest.scala). I like the symbols because they prevent me always accidentally shadowing vdom with local variable &amp; function names. (Eg. `val id = 3` prevents you from using the `id` vdom attribute unless namespaced.) Regarding `$`, yeah, that's a silly habit of mine with my lambda args `$ =&gt; $.blah($.props)` when `$` is a component. It's not part of the library (at least not in the rewrite). * Yeah, waaaay too much monkey-patching &amp; implicits before. It's decreased by 95% in the new version. That being said I still have, and personally prefer `2.px` instead of `Px(2)`. * scalajs-react favours strong compile-time bug-prevention (ok fine, "type-safety"!) and FP. I try not to ram it down anyone's throat and most of it is optional but it does appear in some foundational places where you can't avoid it. I think it's a very valuable tradeoff but I understand not everyone agrees. Best of luck and have fun!
Name a use-case, I'm genuinely interested.
Imagine you have a few lists of complex json blobs in your DB(s) and you want to regulary create a cache-like json blob in another table for which you have to analyse and merge the other json blobs together - which takes a long time. The faster you do it, the faster your users get to see the "fresh" data.
Heh I think you linked me the repo when I asked for Free help on gitter/reddit. I'd be interested in a blog post summarizing the changes to the machinery in doobie ;)
Miserable in beginnings. Then love.
&gt; then real DOM nodes which has nothing to do with HTML I think you're overlooking the fact that people have been writing these bijections by hand for decades.
Scala native seems to be coming along, tonight I updated my [sglgears](https://github.com/Milyardo/sglgears) project, a port of freedesktops.org's gears.c to scala-native, against the newest version of scala-native, and now have a working OpenGL application on OSX with scala-native. I look forward to creating more bindings for OpenGL in the near future, probably as a dedicated project.
Non-related to the topic: could you recommend some literature about concurrency/parallelism/asynchrony?
Java `enum`s are surprisingly powerful - in particular you can have an abstract method on the enum that's implemented inline in each case. To C++ users this seemed like a huge extra complication. So expanding `enum` is already a proud Java/JVM tradition.
I'm starting to think `implicit` is overly powerful for what it does - it's basically got three legitimate-ish use cases[1]: extension methods, typeclasses/type-level functions, and a pseudo reader monad. Some of the problems with e.g. compile times come because these things all share the same namespace, even though you basically never want them to overlap. Maybe introducing dedicated constructs for these three things and then deprecating "general" implicits in their favour would improve the language. [1] Four if you count the magnet pattern, but its biggest advocate has moved away from it because the error reporting for it is so bad. Possibly the magnet pattern should also be added back in with a dedicated construct that could have reasonable error reporting in the compiler.
If you're doing a bunch of CPU-bound work it can make sense. I was once doing some matrix algebra for statistical modeling (via breeze or some such so that the actual heavy lifting was native fortran blas/lapack, but that shouldn't matter) where the calculation took a minute or so. It was worth a couple of lines to make it 6x faster; it wouldn't've been worth my time to port it to a GPU or anything like that.
This is similar to what I am doing (although your code is a little more expressive). I feel similar to you - not doing things 'functionally' just for the sake, but I don't want to fall into bad habits when just starting a new programming style. Thanks for the info :)
Bingo! I appreciate the search term...sometimes knowing is the secret. I also own that book but have put it on pause while I do the Coursera course in Scala. Another commenter pointed me to FS2 which makes me confident that it is a good idea. Thank you.
What are akka streams? I have heard a lot of things about akka but am unsure what it is in kind of "ELI5" terms.
&gt; I'm nearly done and it should be out this month All I wanted to hear :P 
Fun note: You can hijack scala.xml: https://github.com/OlivierBlanvillain/monadic-html/blob/master/monadic-html/src/main/scala/scala/xml/xml.scala#L1
I think this particular plugin was attempted to be made with Scala macros - but he wasn't successful. To be honest I'm not sure.
Id be happy if it wasnt used a bit for everything and then have separate constructs in language. As long as we are just not piling new features that overlap and dont play togetger nicely
True. But it's one of the arguments given in the original article. I do not spend much time writing sealed traits either.
FWIW I think ScalaTags syntax is superior to either of these: `div(cls := "bar", a.bar)`
If you import things unqualified, I think the fair comparison would be: div(className("bar"), a.bar) div(cls := "bar", a.bar) The `:=` syntax is a bit closer to the HTML notation for attributes, but it comes with its own set of tradeoffs, such as worse autocompletion and worse error messages. Here's the signature of `:=` from ScalaTags: def :=[T](v: T)(implicit ev: AttrValue[Builder, T]) And here's the signature of `className` from React4s: def className(value : String*) 
cmd+shift+4, OSX does it when you take a screenshot of a window.
So what about "easy to use in Scala" - when I'm not concerned with performance and/or corner cases - just simple parsing. What should I use for that? I usually just read the lines from a file myself and then use the `split` method, and split by commas.
Hello! I'm trying to learn Scala, and I'm using "Functional Programming in Scala" to do so. I'm a bit confused on something: Scala has classes for Tuples and Functions, but only up to 22 parameters? Is there any reason for this? And is it not possible to go beyond 22? I've tried searching Google for a straight answer but nothing really comes up.
Yeah, ScalaTags has weird types with Builder et al because it supports multiple output targets (e.g. it can generate straight native DOM nodes or straight HTML or any custom stuff if you provide an interface for it). If I recall correctly, it also doesn't actually check types for each particular attribute, so you could easily pass e.g. a boolean or a number as a class name. I forgot that I'm using my own variation of it where the signature of `:=` is simpler, and it is typed: trait Key[V, S &lt;: Setter[_, V, S]] { def := (value: V): S } class Attr[V] (val key: String) extends Key[V, AttrSetter[V]] { override def := (value: V): AttrSetter[V] = new AttrSetter[V](this, value) } I didn't have problems with autocompletion, there's really nothing to autocomplete, the method name is two characters and the parameter is of an obvious type. --- To clarify, I don't have anything against React4s API. Its design goals are respectable and it looks great.
I haven't tried these, but Shapeless HLists and nested tuples should be able to solve this: http://underscore.io/blog/posts/2016/10/11/twenty-two.html There's also this ticket: https://issues.scala-lang.org/browse/SI-7099 There's a chance there's more up to date info, but I only found these. 
Hi, first off the API looks really nice. You have a good eye for capturing the important bits of React. Having components 'emit' messages is especially nice and very reminiscent of Elm. Now, a couple of questions: how do I get messages out from a contained component and into my container component? E.g. suppose I have a `Card` component which contains a `Calendar` component, and I want to get and display the date in my `Card` and obviously have it be updated whenever the date changes. How should my `Card` component grab the date update messages from the `Calendar` component? Also, why do I have to `emit` messages manually from the event handlers? I.e. why isn't a message emission the default thing that happens? It would be nice to just do: `E.button(Text("OK"), A.onClick(_ =&gt; true))`. Cheers!
I just wanted to know why these limitations existed in the first place. 
The repl comes with built command for loading files. Just type in `:load &lt;filename&gt;` into the repl. If you want to run a script directly from your shell you can use SBT's script runner `scalas`. There's also ammonite which promises less overhead while scripting.
raquo is right that the Scalatags weird types are there to support multiple output targets. The `:=` syntax is kind of arbitrary. I could have gone with `cls("bar")` just as easily, with it's `def apply` method having exactly the same type signature. Same with the qualified/unqualified thing: you can import tags and attributes partially-qualified in Scalatags too, it's just an example in the middle of the docs rather than at the top. The weird implicits are what lets you assign `onclick := {() =&gt; println("foo")}` when running on Scala.js and generating DOM elements, while prohibiting you from doing that when running on Scala-JVM generating text. It also lets you write code to generate templates on *both* platforms with the common subset of the API; type-safe "isomorphic" code, if you will. I could probably replace the implicits with virtual-classes and method-overloading, but that's just a different kind of icky =P Scalatags' user base is almost 50:50 split between the Dom and Text backends, at least according to github search, so for that library it's unavoidable. I personally use the ability to write isomorphic templates pretty heavily (along with the rest of my isomorphic Scala).
Ideally you shouldn't need to use compiler plugins to do metaprogramming. Currently you need compiler plugins to do things like adding additional compiler passes frequently needed for things like optimizing mutual recursions, stream fusion, minibxing, etc. Scala macros + Macroparadise is an API for transforming abstract syntax trees representing a Scala program into another Scala program. You can't add new language features with macros, macros still have to type check after transformation. The primary problem is scala macros as is, they're basically just the internals of the compiler exposed for public use. It makes evolving the compiler hard. It makes using the AST hard because there are a lot of parts to the AST that only really matter to the compiler. scala.meta is a user more user friendly version of Scala macros. The AST isn't cluttered with things that only the compiler would care about, or things that could create invalid programs. It's also decoupled completely from the compiler, this lets third parties use the library(like IDEs for example) reason about Scala sources without having to depend on the Scala compiler and typecheck programs in order to generate code(this allows scalafmt to be fast). There are more compile time tools in dotty, specifically the dotty linker. The dotty linker allows for link time rewrites of Scala programs providers a cleaner API to implement some optimizations like stream fusion, so you don't have to write a compiler plugin. The linker is also exciting as it may allow platform specific rewrites, lowering the barrier to entry adding new platforms to Scala and for library authors to add their own domain specific optimizations/rewrites.
You want background workers on a separate machine for this.
I fly out for several days tomorrow, but if you remind me mid-next-week I'll try to craft a small example. But the tl;dr is trivially easy: read `A =&gt; B` as "`A` implies `B`," or, slightly more concretely, "∀`a`:`A`∃`b`:`B`". When you define an algebra and derive its free monad, this is what you are saying for every operation in your algebra, and it's at the heart of what "typed, total, referentially transparent programming" means. The thing you're wrapping this way may or may not reflect this, and if it doesn't, there are various strategies you can use to deal with it, but there comes a point at which you end up sacrificing so much in terms of API coverage and/or type safety that it becomes questionable as to whether it's worth it. Maybe the easiest example of this sort of thing showing up in practice is in the various [Definitely Typed](https://github.com/DefinitelyTyped/DefinitelyTyped) TypeScript wrappers of popular JavaScript APIs, and how often they have to resort to typing things as `any`. Even that doesn't completely capture the problem, because `null` is still a possible result. The Scala equivalent would be `Nothing`, which literally (via the Curry-Howard Isomorphism) represents logical inconsistency (non-termination, crashing, throwing an exception...) While I'm away, maybe [/u/tpolecat](https://www.reddit.com/user/tpolecat) can elaborate, but I shouldn't put him on the spot like this. :-)
A quick and dirty method I've used is to simply pipe the file to `sbt console`: ``` $ cat myscript.scala | sbt console ```
Yeah, exactly. This is why you can use all the CPU on this separate machine and it doesn't matter because your realtime critical stuff runs on a different machine.
Thanks! The message system is indeed inspired by Elm :) Here's some example code to capture the update messages from `Calendar` in `Card`: Component(Calendar).withHandler(onEmitFromCalendar) If you check [Main.scala](https://github.com/Ahnfelt/react4s/blob/master/src/main/scala/dk/ahnfelt/react4s/Main.scala), there's a full example. You have to `emit` messages explicitly, because you might want to handle the event within the current component instead of emitting a message. I hope that clears it up! Otherwise, feel free to ask again. Edit: To clearify, messages are never consumed by the component that emits them, but only by parent components. If the component needs to handle an event, it can just do so directly.
I wonder, why is this done before tailrec optimisation? Shouldn't this apply when tailrec fails? Or would tailrec mess with this new mutualc feature? Also, pretty cool.
It was an arbitrary limit. The code was generated by some kind of template/script. I don't know why the number 22 specifically was chosen. (It's a design mistake really - tuples should just be represented the same way that `HList`s are - but no language is perfect and Scala puts a high value on backward compatibility. Some of the restrictions are removed in 2.12 (I believe you can now define larger tuples subject to certain restrictions), and there are plans to unrestrict more functionality in future versions).
or the efficient way ;-) $ sbt console &lt; myscript.scala
Which (if any) of these things are released/mature? Is there anything I can use today where I can expect backwards compatibility to be maintained for a couple of years?
Thanks, but this is not what I'm looking for. I'd like to run script files on demand, not each time I start the console.
None, sadly. Compiler plugins are part of the compiler, not the language, so AFAIK there are no guarantees there. Scala macros are part of the standard library and stable-ish, but are still marked experimental and have no official stability guarantee. It's also slowly being deprecated as scala.meta catches up. Scala.meta is the long-term API and on track towards stabilization, but I don't believe they've committed to stability yet. Scala.meta macros are also currently limited to annotations, there isn't an API yet for plain "def macros" (that look like regular method calls).
Alright, thanks so much!
Have you tried using the TracingFutures? Curious how that went.
&gt; there are various strategies you can use to deal with it, but there comes a point at which you end up sacrificing so much in terms of API coverage and/or type safety that it becomes questionable as to whether it's worth it If this is the gist of it, then I think I get it. Do you think this is a flaw in the underlying API or the type system in use? (or maybe a bit of both)
http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/ Found them very useful for large files like you are describing. Allows you to use your resources very efficiently. 
Previous discussion: https://www.reddit.com/r/programming/comments/5st3dy/all_of_twitters_mobile_web_traffic_thats_like_a/.
If I understand correctly, they introduced the Node.js layer as a light-weight layer which would be easier to use from the front-end: https://www.reddit.com/r/programming/comments/5st3dy/all_of_twitters_mobile_web_traffic_thats_like_a/ddi91jv/ (see also [the parent comment](https://www.reddit.com/r/programming/comments/5st3dy/all_of_twitters_mobile_web_traffic_thats_like_a/ddhute8/)).
Im happy to hear that
Have you tried not using SBT.
What are you doing that makes you hate SBT so much? We use SBT for all our Scala projects and it's quite simple even when we have ConductR configuration in the same file for our distributed applications.
We use sbt for all of our scala projects and never had any problems.
OK let's assume, for the sake of argument, that sbt is as bad as you make it sound. The main theme of your post is plain factually wrong: no Ph.D. has ever worked on sbt. sbt was born in the industry, has grown in the industry, and is still developed in the industry. Saying it's academic ivory tower blah blah is factually wrong. Scala the language--which apparently you like--was born in academia, and for a very long time was developed in the academia, however. Now it's hard to take you seriously when the whole of your post is ungrounded ranting about academics.
It's hard to take him seriously when he seems to be copy and pasting quotes from this post on the [mailing list](https://groups.google.com/forum/#!topic/scala-user/iI4u3Xqs5K0).
What specific problems are you having with it? I dont think it gives most of the community many issues. What build tools have you used that you find better than SBT and why?
[removed]
Awesome, thanks! I'll take it for a spin 😊
There's nearly no reason to complain. You can use maven or gradle. Or hell, make.
Just because the argument was presented poorly doesn't mean it's wrong. Things can always get better. Maybe you don't have to take this post too seriously but maybe there's a good point or two in there?
Do you have a specific point in mind, or are you just going to continue to equivocate?
Thanks for the link, frameless looks very interesting. 
I never had problems with SBT. Sure it has flaws. We had two java guys join team, who never wrote scala and never seen SBT (maven only). After 1 week one of them wrote plugin to spin up docker containers for integration tests and shut down after tests are finished. The other guy never complained about SBT and his PR's modifying build process were on-spot. So I guess we hired two secret PhDs.
I'd recommend you check out the Essential SBT guide. https://www.scalawilliam.com/essential-sbt/
...so you don't care to make any specific point then.
Learning a well structured language need skill and is not accessible to everyone, so they prefer to stay with a poorly designed language written for front. In order to compensate the shame, node.js developers had to invent name that boost their ego
Programming languages are tools, not sports teams and the market isn't a zero sum game. And there's no such thing as the "*next super language*". Java and Javascript have already won and if there's ever going to be a next big language, it's going to be just a rebranded Java, making the same choices and design mistakes, because the game isn't about empowering people to do more with fewer resources, but about luring beginners and managers, with the set of traits required for that being entirely different. &gt; *If not for these ivory tower nerds and they're religion of functional programming.* Anti-intellectualism. Maybe it's time to have a talk about that.
This isn't even good performance art.
I get shivers when I remember the dark ages when Scala didn't have SBT yet. Coming of SBT was without a doubt an improvement of Scala world. However, I really wish Scala had build tool that: - Doesn't take over command line Because with SBT I can't use Ctrl+C, IO redirects, file tab completion, etc. - Doesn't take 10 seconds to boot Because of that I can't use `$ sbt run` to just run a project, so I end up having to set classpaths by hand. You may downplay importance of stuff like this, but I'm certain people coming to Scala from any other language will experience this as instant disappointment. I don't **really** know SBT. I understand the simple parts of it. For advanced stuff (like SBT plugin) I have to resort to basing upon other people's sbt files and tweaking them to work without real understanding. **Learning SBT is like order of magnitude harder than learning Scala, the language**. It's amazing. I wondered why it is so: - Maybe it's just me being dumb and/or lazy. - Maybe it's because SBT documentation implicitly assumes prior knowledge of concepts from Java ecosystem (like terminology, common practices, etc.), which it then puts behind Scala abstractions. Hence: - if you're coming to Scala from Java, you have difficulty with the abstractions - if you're not coming to Scala from Java, you have difficulty with the Ant/Maven roots of SBT - Maybe SBT documentation is just not learning friendly. For example, at some point, instead of listing and describing available options, it tells you can "discover" them yourself by using `inspect` command. 
Second this, never used SBT, always used Maven/Gradle for the production environment. Never have had a chance to regret this, peeking over the cubicle into the meetings of another team, that had hard time fighting some critical issue with the SBT build not picking up some libraries or something like that.
Yes, they're SICK people. They need HELP before they hurt themselves and Scala.
Even using no build tool is been better than SBT. SBT is purpose built to waste your time by people who want to destroy Scala. We need to do something before time runs out! # #MAKESCALAGREATAGAIN
&gt; So I guess my question to you is, what would you want to fix? Delete SBT. Have anyone who ever worked on SBT banned from ever writing Scala again.
SBT is going to DESTROY Scala. We need a crusade to save it from the religion of FP.
What is the "Bring Your Own Concurrency Monad" pattern?
Scalaz is welcoming new contributors, especially since we have some big stuff in the pipeline! Happy to answer any further questions. 
I thought your quest was to push the bounds of hyperbole.
I'm in academia and I code for my job. I'm in bioinformatics and I work on vertebrate evolution btw. Anything else?
There is, there are three excellent books on this subject; One is from Odersky and it's called simply "Programming Scala", the other two are from O'Reilly Media. "Learning Scala" "Scala Programming".
I started with Scala for the Impatient. It was excellent and I highly recommend it. 
Given your big data background, what do you think about integrating http://getquill.io and SparkSQL? :)
Java is battle tested, but it is old. We need new language for smart people, we need Scala, but with REAL tooling.
You seem quite upset. Whats really bothering you?
I know Akka is always looking for hAkkers :-)
I have to deal with this pain every day and I developed a complete full deep hatred for it also. I believe it's the single worst and most annoying pain point in the scala world. It's slow. Has too many things and options available completly hard and impossible to discover as there's almost no documentation of why things exist or their use case and you are left scratching your head when you see them for the first time. A build tool should be something so simple, fast and transparent that you shouldn't even ever notice it exists. Why the hell do we have a build tool that compiles itself in multiple recursive stages to find definitions and plugins and whatever else can exist under the sun... I don't even know. At least the other day I found out about coursier, it helped a little. We need a tool as simple and efficient as Cargo. 
Have you seen CBT?
Depends: what level are you on the beginner-expert spectrum, and how much time do you have? Beginner, not a lot of time: Bruce Eckel's Atomic Scala Mid-level, a little more time: Scala for the Impatient Mid-expert, even more time: Functional Programming in Scala Beginner-mid, even more time: Odersky et al.'s Programming in Scala
I became frustrated with the idea of maintaining two packages of the same library to support both users of cats and scalaz. An example of this in the wild is [Argonaut](https://github.com/argonaut-io/argonaut). So I made this.
How does this differ from https://github.com/djspiewak/shims ?
The Coursera courses are great but mostly focus on the functional programming part of Scala. If you want to learn something more practical, try Play! There are lots of books available about Play! and you don't need a lot of knowledge about the functional stuff to get started.
maybe you should take your medicine friendo
Note in the following text I'm using the royal you - I don't know you and have no idea what you did, this being a generic response for the rants I'm seeing. You might dislike SBT, but it's not the only option available, you can use Maven, Gradle, Ant or even Make. I have experience with about a dozen ecosystems and SBT has its fair share of problems, but it's much better than alternatives. Either way I respect your dislike. What I don't respect is the hatred. I get it, seriously, we put our heart into what we do, and experience pain from using tools. And I'm all for constructive criticism - I respect constructive criticism. However some people worked really hard on building and improving SBT, released it as open source and you probably haven't paid them a single cent and you probably haven't contributed a single line of code. Oh, what, do you think authors should be considering themselves privileged because you're using their work for free, while bad mouthing it on public forums? You see, there's no "we" in this conversation, there's just you along with those that feel self entitled without any contributions attached to their name. And that's not a good company to be in ;-)
Well, that's a huge correction right there! Thanks for your contribution :-)
I read a "Programming Scala" book from Odersky and made a project using Scala and Akka. Coursera course looks to basic for me.
There are some libraries for Play (https://www.playframework.com/documentation/2.5.x/ModuleDirectory#Authentication-(Login-&amp;-Registration)-and-Authorization-(Restricted-Access)), but none of them has convinced me to be honest. I've used Spring in java and from what I remember, it was way easier and cleaner to implement authentication (email/password, or oauth) and authorization 
You should take a look at Play Silhouette https://www.silhouette.rocks. It's the number 1 reason I use Play for my REST API frontend implementations. It has a very simple approach for Authorization filters as well. Take a look at it https://www.silhouette.rocks/docs/authorization. I believe the author wants to eventually make it framework agnostic and make it available for Akka HTTP etc.
Can't we just choose one and stick with it as a community already? It makes everything easier. The way I see it, Cats is the new (and improved?) Scalaz. It's probably not better every possible way, and perhaps less featureful, but to me at least it's more appealing overall. Cats goes better with the scala standard library - ie. using scala's Either, Option, etc instead of reinventing everything. I think that alone makes it more fit to be The lingua franca of the scala-FP world.
cbt, sbt, ensime
&gt; You should take a look at Play Silhouette https://www.silhouette.rocks. It's the number 1 reason I use Play for my REST API frontend implementations. It's really disappointing that there does not seem to be an alternative if one does not want to use Play yet... I hope it will be made framework agnostic.
From what I've seen that's the direction things are going although even if people were to fully agree on it we can't transition overnight.
What do people think of MVC? There are lots of people (at my workplace) that say it's dead, everything has to be cleanly separated, pure frontend with backend... and I tried this for most of the time. But last days I've been playing with Play MVC and I find it vastly superiour to be able to fast iterate and quickly create new features. It doesn't feel that nice, passing html instead of json over wire (almost feels like calling function which gives you string instead of data repr.), but still. So what are your thoughts, experiences?
Perfectly reasonable solution to the entirely wrong problem. Also: https://xkcd.com/927/
[Image](http://imgs.xkcd.com/comics/standards.png) [Mobile](https://m.xkcd.com/927/) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini\-USB\. Or is it micro\-USB? Shit\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/927#Explanation) **Stats:** This comic has been referenced 4244 times, representing 2.8447% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_ddy2mwq)
I came across this the other day, no idea what its like thought I share in case you find it useful: https://bigdatauniversity.com/courses/introduction-to-scala/
What if that person is writing BuckleScript (OCaml) to deploy to node?
F# looks beautiful and allows a conciseness Scala can't quite match, probably due to the ML heritage of F#. Stuff like algebraic data types being "clean" to write, pipeline operators present by default, etc. 
I'm playing with F#. It has some features that I like, like type providers and units of measure. The syntax is a little cleaner, you'll miss it when declaring ADTs. You'll probably miss the H-M type inference as well, although in F# it's not so great either, compared with OCaml or Haskell, because it doesn't mix well with OOP subtyping. Scala has a more powerful type system, having higher kinded types and ability to encode type classes and thus ad-hoc polymorphism. The OOP feels better as well, due to singleton objects, traits, f-bounded polymorphism, etc. And in these languages you do need a lot of OOP. Overall Scala has more expressive power due to its type system. Most Haskell abstractions are usable in Scala, which is not something that you can say for F#. See the libraries from [typelevel.org](http://typelevel.org) for sampling. In terms of platforms you'll also deal with JVM vs .NET. IMO the Java ecosystem is vastly better for server side, whereas .NET is better for the client side, especially due to Xamarin's work on UI bindings. Scala also has a potent JavaScript compiler btw, it's pretty awesome, with lots of pure Scala libraries supported, see [Scala.js](http://www.scala-js.org). Also see my [previous rant](https://www.reddit.com/r/scala/comments/5sv1w5/the_divergence_of_strongly_typed_programming/ddiuz8b/) on F# 🤓
&gt; It's probably not better every possible way, and perhaps less featureful, but to me at least it's more appealing overall. Unfortunately, I've already run into "less featureful," and it makes my already-extant concern with Cats' maturity worse. In some sense it's a self-solving problem—just give it time. But when you're making decisions today, there it is. &gt; Cats goes better with the scala standard library - ie. using scala's Either, Option, etc instead of reinventing everything. Cats works with the standard library's `Either` and `Option` exactly the same way scalaz does: by providing typeclass instances for them. If anything, Cats' continuing to make API-incompatible changes at a breakneck pace is probably the biggest red flag for someone making production decisions today, even if you agree (as I do) with the assessment that it's ultimately a desirable destination.
It's really not clear which one is better, and that's not even considering Scalaz 8. &gt; using scala's Either, Option, etc instead of reinventing everything. This just isn't true. There's no difference between cats and scalaz in that regard. cats used to have `Xor`, then removed it, now it has instances for Either, breaking every single user's code in the process. Scalaz still has `\/`, but also has had instances for Either for basically the entire history of ScalaZ 7. Scalaz infact has [more instances](https://github.com/scalaz/scalaz/tree/series/7.3.x/core/src/main/scala/scalaz/std) for the types in the standard library than [cats](https://github.com/typelevel/cats/tree/master/core/src/main/scala/cats/instances) does right now. Also, neither library ever replaced Option.
Hmm. I don't know how prevalent its usage is, but Scalaz does indeed seem to have its own version of `Option`(??): https://oss.sonatype.org/service/local/repositories/releases/archive/org/scalaz/scalaz_2.11/7.3.0-M1/scalaz_2.11-7.3.0-M1-javadoc.jar/!/index.html#scalaz.Optional And the `\/` vs using the already built-in `Either` type was precisely what I'm referring to. Yes, cats invented its own Either type `Xor`, but "came to their senses" last year :p It's just a lot more comfortable to (re)use Scala's own types in your code, because you know they're not going away anytime soon. Cat simply adds some sugar ontop of the plain old either. That said, I fully understand why Scalaz and Cats made "a better either" in the first place; but now that 2.12 fixed Either, it makes sense to just use that.
Hey @jagpolly, I'm thrilled to hear about the new version coming. Can't wait to try it out.
I'm not completely sure, but from I what read [squants](https://github.com/typelevel/squants) seems to "give" you units of measure. Although it is restricted to the units they defined.
&gt;Are these still happening? Anything else I should be aware of? Any particular area where contributions are especially welcome? Thanks. Yup, those are the bigger changes and all still happening at various speeds (and in various repos/forks). Lenses is something I'm definitely spending some time on, but that is a *deep*, deep topic so it's slow going. Also some Free improvements and not just to the free monad. Also, a lot more macros (internally) for perf. reasons. 
Me. I learnt F# for a couple of months and then I moved to Scala. As a language F# is beautiful. However the ecosystem sucks big time. I was struggling with Nuget which is a relic of 1990s and nothing as compared to Gradle, SBT or even Maven. Nuget was a disaster. and if you complain against its primitive nature then they immediately ask you to use Visual Studio which is another relic of 1990s. I started learning Scala because I was working on a few big data technologies like Hadoop and Spark. and while there were 100s of frameworks on the JVM to write a hadoop map reduce job.... there were 1 or 2 ways in .NET
you might like the [From F# to Scala](http://theburningmonk.com/from-fsharp-to-scala/) series from Yan Cui (theburningmonk.com) 
what are your company using scala for?
Do you want to be the client or the server? OAuth 1 or 2?
ooo that's definitely up my Alley! I've started reading the documentation for it yesterday, hopefully will be able to start helping out soon :D. One question: When a project is as established and complex as Akka, how do you start contributing in the middle? Do you just start out by helping with documentation or are there "starter tasks" to help with?
Silhouette does authentication, Deadbolt 2 does authorization. 
I don't know enough about it to say whether this is viable or not, but I'd like to see an AMQP or Redis transport for Akka. Direct TCP is not allowed on Heroku. It looks like there was one some time ago that was abandoned. 
Would this syntax support the use of parameters in the enum constructor?
[removed]
Thanks for that extensive comparison:-)
Thanks 
Pac4j is quite good.
SBT is kinda democracy - best of the worst. Encrypted operators. Do you know them all? Hard-to understand "non-dynamic tasks" concept undermined by Def.taskDyn. Compex type system. Slow start. Scala 2.10 (they promise to cross-build SBT somewhen thought). Horrible user support. Since they started providing commercial support it is really hard to get help on advanced features (and advanced bugs). The only damn way to *really* learn SBT is to dive into its hard, functional, badly-documented sources, looking at official docs as the reference. Can't say I'm unhappy about official sbt documentation, but it gives mostly recipes but not understanding of the engine. Still, there is no real alternative if you need most wanted sbt features: fast recompilation, sbt-revolver and tons of other useful sbt plugins.
This really depends on your background. There are two main sides to scala 1) The object oriented side 2) The functional side Traditional java programmers may find it difficult to understand the functional side and that is due to the complexity of functional programming and not due to the fact that it is scala. My advise is to start with the object oriented side first and to that I have to really recommend my own tutorial series here - https://madusudanan.com/tags/#Scala I first explore the OOP side to somewhat depth so that you are comfortable with the ecosystem coming from java. I have written only one article just edging towards the functional side but there is a lot more ground to cover. But it is really worth it since it teaches several programming concepts which can be reused across many languages. Also I would highly recommend watching the video tutorial series from Mark Lewis here - https://www.youtube.com/playlist?list=PLLMXbkbDbVt8JLumqKj-3BlHmEXPIfR42 He explains it in the simplest way possible IMO. All the best :) 
Wicket is the best web development experience I've had, by a long way. I don't mind passing HTML over the wire when it's just an input/output format; having my business logic split across two places connected by weakly typed JSON is far worse. Sadly I don't think there are any jobs doing it these days.
&gt; That said, I fully understand why Scalaz and Cats made "a better either" in the first place; but now that 2.12 fixed Either, it makes sense to just use that. Agreed, and I assume ScalaZ may well do so sooner or later. That they didn't "burn it all down" immediately and remove `\/` in the next minor release, leaving existing users high and dry, as Cats did, counts as a point in ScalaZ' favour IMO.
I still agree with separating the UI from the implementation - even with Wicket I would build the frontend and backend as separate maven modules, with a small, explicitly designed API/language bridging between them. But being able to write that API in Scala is very nice.
What I hate is the way SBT is presented as the only or best option, particularly to newcomers to the language. I think this drives people away from using Scala, ultimately leading to software being written in worse languages, being buggier and less maintainable, and the world becoming a worse place. It's particularly frustrating as I've never seen anyone make the case for SBT over Maven (or any other alternative) on any kind of logical grounds, it's always a purely tribal "SBT is the scala build tool, the scala community uses SBT". Often with an undercurrent of "only corporate drones who are attached to XML and boilerplate use Maven". I try to stay positive. I try to be the change I want to see, telling people about the advantages of Maven and that the option of using it exists. I've made [a couple of](https://github.com/typelevel/scala/pull/133) [documentation contributions](https://github.com/non/kind-projector/pull/46) of that nature and will always try to contribute more, though I have limited time and sometimes other open-source projects take priority. But honestly I don't blame the people who get angry about SBT, because a lot of the time they're responding on the same level that it was sold to them. There's been a lot of bad behaviour from SBT advocates (who of course don't necessarily have any connection to actual contributors to SBT) over the years, and some of the angry responses we're seeing now is the result of that very aggressive and emotional salesmanship coming home to roost.
I would be cautious about defining a set of "rules", as they can often end up more cumbersome than just allowing a user of the library to define a plain-Scala function. I'd recommend implementing a real app or two that uses these constructs first, and then try to pull out the authorization as a separate thing. If you find enough common ground to be a useful library even when the apps have e.g. different representations of their users (which won't necessarily always be `Int`) then great. But you may well find that it's easier for each app to just express its own definition of authorization directly, and there's not actually anything that can usefully be pulled out in common (except for parts like the concept of a monad itself).
How's the runtime performance?
It still stays a very opinionated framework from my perspective. This line is proof: `.enablePlugins(PlayScala)`. This not necessary a bad thing, but it has some serious implications and drawbacks.
I guess you mean recompilation and not recomplication? :P
Why should choose this over reStart? 
I found the type inference and IDE support really good for F#. Using collections in Scala is a bit smoother thanks to CanBuildFrom.
I see what you mean. I sometimes also try to do that, however lines get really really long every now and then. I have an ultra-wide monitor and sometimes a line takes my entire screen when i try that(I do have font size 16, so rather big)! 
You don't have to declare them as lambdas as in for instance python. For instance you can do myList.map( λ =&gt; λ*λ ) myList.map( dankMeme =&gt; dankMeme*dankMeme ) or in this case the compiler can infer that there is only one interesesting variable so you can use an underscore: ~~myList.map(\_*\_)~~ edit, Daxten is correct. A case where underscore is correct would be myList.map(println(_))
sure but in case of sbt there is no much difference)
Thank you. It looks like I can follow it for free.
last one would be equivalent to myList.map((a, b) =&gt; a*b) never to myList.map(a =&gt; a*a)
I guess back then (2014) FAKE wasn't popular then... when I looked around visual studio was the unanimous approach on how to build projects and manage dependencies. For me that was a completely unsatisfactory and ugly answer. I hated hated the fact that I need to download a 9GB IDE (which didn't allow you to skip visual basic install) to build F# projects. Also, there were very few community projects and for everything we had to resort to use the old and ugly (imperative) .NET API... which killed the joy of using a functional language.
Thanks Mate!
You're welcome.
I didn't know about Shims, but a quick read says that Harmony is for users of a library written for Cats or Scalaz, whereas Shims is for library authors who want to support both. In particular, the premise of Harmony is that library authors should not go to the extra work of supporting both Cats and Scalaz. Instead, clients should use a converter if they dislike the library's choice. &gt; [Shims] ... is explicitly targeted at upstream library authors, not downstream It appears that using Shims to support both is not any easier for the library user. &gt; From a user standpoint, they must add a single additional import which would not be required if you wrote specifically against cats or scalaz.
This is not an appropriate use of that comic. I am not providing a competing library.
Sry for the late reply and thanks a lot for checking out my work. This was a initial attempt at making and learning about distributed storage. The goal was to create a working key/value store and focusing more on the distributed/high availability side of things (s3, dropbox). The next feature I would actually like to add is encryption. So I would say that the best use for this at the moment is if a single person/machine is trying to manage some files and wants high availability and security (after the encryption feature). 
No, this is not even a competing interface. It's something you add to your imports, and it provides conversions from one interface to another.
What are the "serious implications and drawbacks" you refer to?
With compiler plugins you not only need to know how the compiler works but also the plugin works to understand whats going on. Take the play `routes` file as a specific example. It is actually not typesafe *unless* you change the compiler and precompile it, making it typesafe. However, you then additionally need to know how the routes files syntax and semantics works, what and where to it generates files and so on. You also lose IDE support unless your IDE also offers a corresponding plugin. Frameworks can be seen as their own languages, built from another language. This is what distinguishes them from libraries.
You shouldn't have to use long lines to achieve it if you don't want to - it's also possible to break lines after opening brackets, or between a function and its argument, so you could do something along the lines of: (myList(...) zip (mySecondList(...).map( _.toString )).toMap (the slightly counterintuitive part is the need to write `a zip\n b` rather than `a\n zip b`; it's possible to use an `a\n.zip b` style but this can create subtle issues and is probably best avoided)
what can be done to stop the sex dating site spam in this sub?
I really like your idea, I'll try to do it soon. Thanks.
You'll be happy to know that they are working on replacing it with Aeron for more performance. Check out Akka Artery for more details 
Awesome!!!
What I'd like to fix is: 1. frequent incompatible changes to what constitutes a valid build definition 2. poor searchability (and, historically at least, poor documentation - allegedly improved recently) of build definitions 3. multiple ways to do the same thing 4. arbitrary code freely mixed into build definitions 5. lack of bidirectional integration with eclipse. My use cases are: 1. build a 2-year-old project with the latest version of the build tool 2. be able to rapidly go from a specific part of a build definition to knowing what it means, via my usual tools for finding things out 3. be confident that if two builds look different then they are different 4. be confident that all the logic in my system has gone through the normal code review process, been checked for test coverage etc. 5. be able to add a dependency via the eclipse GUI and have it applied immediately without a manual regeneration step; be able to add a dependency by editing the text of the build definition in eclipse and again have it applied immediately without a manual regeneration step. 1 and 3 are in tension with each other, and 2 is probably unfixable without making them worse - because fundamentally SBT was pushed too hard while too immature. Last time I had to get an SBT 0.7 project building I found it easier to run `sbt make-pom` and convert it to maven than to try to bring it up to date with a current SBT. I would definitely remove the ability to configure an old SBT version in a properties file - I think that's inimical to both 1 and 3, and dooms SBT build configurations to permanent unmaintainability. I would also replace all the symbolic parts of SBT definitions with word-based versions that are amenable to google etc., which would be a huge incompatible change. Fixing 4 would require removing the ability to embed arbitrary Scala expressions and replacing `build.sbt` with some kind of data-only format, again a huge incompatible change. Fundamentally I think fixing SBT would be a lot more work than figuring out how to implement its only vital feature, cross-building, in a better build system. (I'm looking into ways to do cross-building in Maven when I have time; I'd be very interested in any existing work in that direction). I think it's only a matter of time before it collapses under its own weight, and we will look back on SBT with the same "what were we all thinking?" sense that we look back on the cake pattern with. But I've never been able to understand how people ever got so enthusiastic about it in the first place.
Nice!
Does it mean in your mind, Scala for the Impatient is bit of advanced than Programming in Scala? Thanks 
Thanks for your good articles :-) I plan to introduce them to my colleagues who are new to Scala
Your use of the `λ` character as an identifier might be a bit confusing, since in the lambda calculus, it isn't an identifier, but more of a keyword to introduce a new function, and is then followed by the names of variables to be bound to the function arguments. That's exactly its purpose in Python, too. In Scala, the infix `=&gt;` syntax serves that purpose. I'd only use `λ` as an identifier if it made sense in context (like operating on a collection of wavelengths?).
Thanks for trying that. It worked for me, just counter intuitive.
Martin Odersky's Scala developer levels give a good guide on your current Scala level and what you need to learn to "level up". His levels are tough... I don't know too many people that are higher than A2/L1.
The syntax for an anonymous function express is: x =&gt; y // or (x1, x2) =&gt; y // or ... // or (x1, x2, ..., x22) =&gt; y As you can see, there is no keyword used. What you name the anonymous function expression is, of course, up to you, but I would recommend not naming it `lambda` unless that specifically has a meaning in your domain. Much better to name it something meaningful, like `bmiOfWeightHeight`.
In F# check out https://github.com/SwensenSoftware/unquote . Tough to get that in Scala.
I miss the beautiful ML syntax and top-of-the-line type inference in Scala, and I really miss higher-kinded types in F#.
But are you _right_-biased?
This came under some criticism when it came out (it's quite old) because the categorization is arbitrary in a lot of ways, and it gives the impression that some fundamental things like folds are "advanced" which might cause people to think they should be avoided. So take it with a grain of salt.
&gt; Is that right? Yes. RDD is Spark 0.1, still there for legacy reasons. Dataset in the newest API. The two are completely independent.
I have been on and off at reading it, so I don't really know. It is a really challenging book, and you'll have to practice its examples to make it work. It takes a long time to read it (at least one month). It require a lot of effort, but you'll get a lot of insight for those efforts. You know high involvement, high reward and all that stuff.
Please help, Scala gurus! I'm lost among the waves. The problem seems to be that JsonPath.query.right.get.toArray produces an Array[Any] which doesn't allow a .circular. How do I then map the Any to Map[String, Any] for use with a Gatling Feeder?
I don't understand the downvotes. What is incorrect about these statements? I don't have a dog in this fight.
Your website design (the left hand hav thingy) makes it impossible to zoom the content or rotate on an iPad.
Oh yeah, I think I deleted the actual answer while editing my reply (great editing). Oops. Datasets still use RDDs under the hood. Its API is focused on preparing the query execution plan. LogicalRDDs carry the reference to the RDD underpinning everything. https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala#L131
Thank you for reporting this. This is a Jekyll theme called Hyde. I didn't like the fixed side bar as well, I'll switch to a theme where it's not fixed You can also file an issue here - https://github.com/poole/hyde so that the author can fix it
I think these things are fun and useful as long as one doesn't take them too seriously. E.g. I find that I'm pretty clearly around A2/L1 on this scale, or "Fire Lubline" on the John de Goes scale, and this helps give me a sense of what concepts might be useful as a next step vs. which ones are likely to be out of reach for now.
Basically all projects are candidates here. The highest priority are projects which might benefit from being compiled and tested with Typelevel Scala or which might be good test cases for Typelevel Scala.
So going through packages directly under Typelevel, and then moving to any that rely on Typelevel and are on SBT 0.13.13+ are candidates? Sounds like a small adventure in and of itself. EDIT: Just saw the list grew significantly - that should give me a little more direction.
&gt; Datasets are also built on top of RDDs This is incorrect. Here is an hypothetical stacktrace for `Dataset#select`: https://github.com/apache/spark/blob/1a45d2b2cc6466841fb73da21a61b61f14a5d5fb/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1139 https://github.com/apache/spark/blob/1a45d2b2cc6466841fb73da21a61b61f14a5d5fb/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L177 https://github.com/apache/spark/blob/1a45d2b2cc6466841fb73da21a61b61f14a5d5fb/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L192 https://github.com/apache/spark/blob/1a45d2b2cc6466841fb73da21a61b61f14a5d5fb/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala#L63 https://github.com/apache/spark/blob/1a45d2b2cc6466841fb73da21a61b61f14a5d5fb/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleExecutor.scala#L71 And so on all the way thought Catalyst internals. As you can see none if this code is `RDD` related. Some operations do fallback to `RDD`, (`foreach` is an example), but that not the same thing as "being built on top of RDDs" (that is, the `Dataset` is *converted* to an RDD, and the operation is then run on this new collection).
Good to know. Anything like this for maven?
We've been working on the infrastructure for that together, but @olafurpg made the final push: https://github.com/scalameta/paradise/pull/171 :)
Cool, I look forward to giving it another look next time I take my kid to basketball practice.
I don't see how Dataset is built on top of RDD. Dataset and RDD are not subclasses or superclasses of each other, they are not subclasses of a common ancestor (except for Object), and they are not, from what I can tell, composites or containers of each other. Can you explain in what sense you mean that Dataset is built on top of RDD?
FWIW, I've updated the question with a solution. I hope is helps someone in the future. Thanks, all!
Thanks both for the hard work, can we compile/execute runtime generated quasiquotes yet?
Would this work for you? Check it out: https://github.com/nulab/scala-oauth2-provider I tried playing around with this and implemented it with Play and Slick.
Can it be used without play?
Its pre1.0 thus api breaking changes should be expectes imo. People should aknowlege that and use at their own risk
Thanks, will take a look.
[removed]
Why learn Scala? 3 reasons why I started learning Scala
I'd love to give it a try. It looks clean an straight forward. When are you planning to publish the library?
I'd say that employability is a good enough argument. Definitely much better than writing toy projects in some obscure language and hoping that this alone will make you "a better programmer" one day.
Greetings to those lonesome mathematicians. Well done.
[removed]
totally agree. All Scala devs i know are gods in human flesh
I'm more or less in the same boat of OP. But I want to be the client, using OAuth 2. Does anyone have any suggestion?
Great :) In a way it's already public - on GitHub with an MIT license. But I suppose you mean - when will I release it to the package repository? I will give it a go this weekend.
Do you have the results from that benchmark by chance?
For me it looks more like a `typeclass`, that is basically the same idea - but might be more flexible at some cases (you don't need to specify a bound - simply to add some implicit methods in scope that can deal with the abstract operation)
I started learning Scala because it was the first language I found that fulfilled the following conditions: * expressive; * both functional and imperative; * can access a rich library ecosystem; * isn't some amateur basement project that will get abandoned as soon as the main dev gets bored. Initially, the language caused some annoyances, but then 2.8 came out and fixed them.
I've seen a few too many deceptive names in my time.
I would argue that the implicit parameter group is more easily understood than an implicitly defined apply method, but might just be my inner noob talking.
Something that doesn't yet exist :) In Javascript (bless its heart), you can do something like: ``` const { value2 } = { value1: "apple", value2: "banana" } console.log(value2) ``` There's a really concise shortcut syntax to bind an object property to the same name in the current scope. I'm rather obsessed with the ability to bind names from `case class` extractors by keyword. Having pattern matching only work positionally creates a coupling between class definitions and usage that I'd like to get rid of. In my applications, I seem to only rarely construct objects, but I deconstruct them quite frequently. When I modify my domain objects, I have to change all of my pattern matches.
Right now Typeclass design is more of an art than science. There's libraries like [simulacrum](https://github.com/mpilquist/simulacrum) and [machinist](https://github.com/typelevel/machinist), and I think there's been some effort in getting first-class support in mainline Scala. At the very least there should be more discussion like this.
it indeed works, however with a few trials and failures about the Android SDK and intellij. 47deg's "nine cards" is a more mature example code that could be followed https://github.com/47deg/nine-cards-v2
Purescript :)
I think it depends on what you wanna achieve. Today if I need to get something done, I'd pick **Typescript**. Having used Scala.js/Diode/scalajs-react, I still have my reservations regarding Scala.js as a FE alternative - namely the relatively slow iteration speed, ecosystem and debuggability. However I think these problems will eventually be resolved and learning Scala is a very good way to expand your horizons. **Elm** does one thing and one thing well, but I can't help but feel like it's a dead-end language like Go. It's fantastic for projects that are, business-logic wise, simple and is easy to pick up, but I'm not sure about its long term viability as a investment of your time. **Purescript** is interesting - With its aim of running on more than JS (native C++) it can be a good alternative to Scala/Haskell for many tasks. The learning curve will be higher though you're not already experienced with pure FP. Another option is **Ocaml** which you have js_of_ocaml or bucklescript for your JS code gen. You get good compilation speed and if you use bucklescript - very readable JS output.
The post starts with "The problem with overloading": def combine(x: Int, y: Int): Int = x + y def combine(x: String, y: String): String = x + y &gt; Since duplication is bad, we ask ourselves: can we get rid of it? And here is the final solution (may I remind that the problem is duplication): trait Adder[A] { def add(x: A, y: A): A } object Adder { def apply[A: Adder]: Adder[A] = implicitly } implicit object IntAdder extends Adder[Int] { override def add(x: Int, y: Int)= x + y } implicit object StringAdder extends Adder[String] { override def add(x: String, y: String)= x + y } def combine[A: Adder](x: A, y: A): A = Adder[A].add(x, y) Indeed, we just wrote one implementation of `combine` that works for two types. Problem solved!
it doesnt really cover functional well either IMO. Yeah it does show some recursion, but doesnt go into the details or why it is better. It is like I am missing another class covering basic recursion. 
Fantastic reply. Thank you. 
Scala.js no doubt. At least, from my personal experience with it I really can't recommend it enough. I guess Clojurescript would come in at second; although not a close second. I guess clojurescript, elm &amp; co allows you to get "up and running quickly", so it's great if you just want to make a tiny script/app. But for anything mid/large, scala's static type system blows everything else out of the water imo. And Scala(.js) is a _perfect_ fit for the "JS platform" if you ask me. It's as good of a fit for the JS-platform as it is for the JVM-platform. + It compiles (a lot) faster than for the JVM (thank god!). The only drawbacks I can see with scala.js: It's (still!) kind of a hassle to start a new scala(.js) project - you have to add a build.sbt, projects/plugins.sbt, projects/build.properties, then you need to add the painfully cumbersome maven directory structure etc. That's no fault of the language scala though; it's just the over-enigeering inherited from the maven convention. I guess it's possible to deviate from that, but if you want intellij to work flawlessly, it's probably a good idea to just stick with the conventional directory structure. ...But once you've set things up, developing with Scala.js is amazing! :D Some very much recommended libraries: - utest (for testing) - Scala.rx, Scalatag &amp; Scalatags-rx (which gives you DOM that auto updates whenever your Rxs ("model") changes. It's much superior to the way angular &amp; react does it if you ask me. Functional reactive programming is _the_ way to deal with things that change over time (like UXs)) - autowire (allows you to typesafely send/retrieve data between your client &amp; server code; REST is abstracted away, and you can simply "call" methods in your backend from your frontend, and it automatically serializes (via json) the way you expect it. Really removes a ton of boilerplate while simultaneously being way more (type)safe. Highly recommended, but not by any means required if you're making a frontend-heavy app) - there's probably a few more I can't think of right now, but really, all you need is the Scala.rx + Scalatags + Scalatags-rx combo, and you can pretty much make as infinitely complex frontend applications as you want).
:)
I'm using Scala JS on a recent project with a certain amount of complexity on the client side, and I couldn't be happier about it. Using the same typesafe code on both client and server has removed all the time and pain in having to make sure APIs, data structures, constants, enums, defaults, etc... are in sync between server and client. I'm also using ScalaJS React (along with autowire to make API calls), and I've gotten to the point where as long as the code compiles I'm pretty close to being sure the code is working as intended, instead of having to test the client thoroughly looking for bugs due to differences in data structures / APIs between server and client. And to boot, this also buys me being able to do server-side / isomorphic rendering of the pages (using Nashorn), so I can send them either to search engines that don't speak JS, or as an initial render to the client while the browser loads and processes the JS libs (a very nice feature of React: once the React JS libs finish loading on the client, they simply "attach" to the server-rendered DOM and continue from there).
I agree that type classes introduce a specific way to handle themselves, but duplication is not the problem they solve, it already exists in `String` and `Int`, you can only remove it with some kind of duck typing. `Dollar` example changes nothing: new type - new implicit val.
Elm.
Mirc? Now that is not a name I've heard in a long time 
When I was first learning Scala, I dug into SBT's documentation within my first week becuase it's important for me to understand my build tool. Sorry, but.. even as a Scala novice.. I got it. It's not *that* complicated as a user, though I cannot speak to some of the edge case behaviors you refer to such as its ability to be intelligent about re-compilation, and I can agree that "it" is slow, but I'm not sure that's SBT so much as scalac. Honestly? I *like* SBT. I love its recursive nature, its DSL, and even features such as scopes make perfect sense to me. I've found some warts, sure, but of all things I might complain about re: Scala, SBT just isn't on my list. I know I may be in the minority, though.
where does it show how to set up Scala for Android Studio? 
it's http://scala-android.org/ IIRC. If you are interested, I had a hello world project https://github.com/ntu-sec/sbt-android-hw
That is what I'm doing now, I use Jackson to parse so I leave it as an `ObjectNode`. It has been a bit annoying to use due to the underlying types being abstracted as `JsonNode`, so I was wondering if there was a better way to deal with this issue. Could be that a different library is the answer.
Yeah there is a wealth of handy Scala JSON libraries, I'd recommend just keeping the `JsObject` or equivalent of whatever library you choose, but it's hard to say without knowing what you have to do with that nested object. 
Do you really share a lot between server and client?
A lot of stuff: for data manipulation streams, for machine learning models and large scale data processing in Spark, and we are also soon planning to start moving our backend APIs to Scala that are currently written in Node.
I prefer using Finch for all my server side code. It supports OAuth2, if that's any help for you. Check it out: https://github.com/finagle/finch/blob/master/docs/cookbook.md#oauth2
Is this docker for integration tests plugin that you're talking about open source? If yes, can you please share?
Its not, sorry. And i no longer work there. There are similar (google sbt docker ... sbt docker compose) and just adjust to your orga needs
&gt; Dollar example changes nothing: new type - new implicit val. Well, that's true, but still. Now we get also `Euro`, `Yen`, `Yuan` and these store more than just a value field. And then we also get a class like `Salary`, storing one more multiple of the ones above. Now, all of these should be made "addable", but making them addable is much more complicated than just creating an `Adder.by(_.value)` or `Adder.by(_.money)` (for salary). We can even derive these instances by using typelevel programming (see shapeless), so that I don't even have to add a single line of code for my `Dollar` class as long as it contains only one field for that we know it is addable - at compiletime without that we would have to write a macro for that or rely on reflection or other runtime mechanisms. You can look for examples in the `circe` library that allows you to serialize e.g. an instance of `case class User(name: String, age: Int)` without having to write a function that describes how to serialize a `User` - again, the circe contributers have not written a macro to do that and they have also not used any kind of reflection.
Congratulations! Your comment used every letter in the English alphabet! To celebrate the occasion, [here's some free reddit silver!](http://i.imgur.com/l7LP33z.jpg)
If you set up the project correctly, yes. We have a `domain` project where we keep our types and JSON serialisation logic. It gets shared perfectly between the backend and frontend.
&gt; please help us increase that number Is quantity more important than quality?
Why assume it's a tradeoff?
How is that comment disrespectful?
[removed]
I don't know, Typescript is the PHP of statically typed compiled-to-JS languages. The most popular, but also the least safe / sound. Very little type inference, many surprises when incorrect code compiles with no warnings, doesn't even handle variance properly, etc. Just very frustrating in general. If you want typed Javascript, might as well go with Facebook Flow.
&gt; should I switch to IntelliJ version control GUI? Definitely give it a shot. Reviewing changes and resolving conflicts is particularly convenient. Having uncommitted changes color-coded on the edge of the editor window is another simple but super useful feature, don't know how I lived without it.
If it's a very standard SBT project it works well. If the build is a bit special, like multiple projects in one build, your milage may vary.
thx
Hobby programmer here. I would probably just foldLeft into a Tuple2 with the left result being myPeaks and the right result being aggregated. Whenever the boundary sensitivity is hit, append the left of the tuple to the right, and reset the left with an empty HashSet. With the result of the fold, return the right side with one final append of the left. Tail recursion with the partition method might also work. Keep partitioning until it can no longer be partitioned.
Or they could extend the deadline by a few days if necessary...
You could simply use a reduce, with an object containing your four values. BTW it looks like your code is creating myPeaks each time though the loop.
&gt; "standard" functional style I THINK it's due to point-free style, which people are familiar with from haskell. 
[removed]
Unfortunately there doesn't seem to be any full example for using Silhouette with external authentication providers. (Like google or facebook login) If anyone knows of a tutorial that shows how to wire up all the components together, that would be great! It's so modular that I don't even know where to start! :D :(
Saw it in "This week in Scala", adding it here.
Thanks you very much. I watched your commits very closely ;p I'll give it a try during the week.
No, but it does help to know the lower level goings-on of the jvm. Knowing commonly used Java libs would help as well, but only insofar as you use them. Honestly, it would help more to know functional programming principals; that would even help you understand how scala uses the jvm. 
It doesn't seem to have been in a problem in the code my colleagues and I have written, but I suppose if you have reason to be concerned about it, [Matryoshka](https://github.com/slamdata/matryoshka) is a helpful thing to look at.
No.
I'm mostly interested in the syntax for applicative functors, which often appear in regular Scala code. For example, when reading/writing JSON in play. An `apfor` would make the syntax much nicer there. For comonads, I don't have an immediate use case. But see http://softwareengineering.stackexchange.com/questions/322431/what-is-a-comonad-and-how-are-they-useful , especially the last answer in the context of reactive frameworks.
Weak stuff, but seems like the average quality for the fora where one might find this particular format: * Several items, each with highly emotional and exaggerated language. Each ending with a single-word "this is the emotion you should be thinking of" statement. Format popularized by the current US president and the related subreddit. * "BTFO". Feels like I'm browsing a *chan /pol/, /g/ board, or any of the other boards where a decent chunk of the content is flame-bait threads * The claim that SBT is inaccessible to business or not what an "experienced business developer" wants makes me think that the poster's focusing on the Enterprise COBOL environment and claiming that's the top priority for the language. * "in such a bad state that it isn't even at 1.0" showing that again, this flamebait attempt would fare better in forums with more folks that aren't too aware of how the versioning history of many open source progresses. Over all, I'd say this is the average quality of a /g/ board bait post, and one need only skim a few of those to recognize the mechanism of action and avoid the usual discussion failure modes. Obligatory ad-hom: the format makes sense for the demographics of the boards where it thrives - teenagers, NEETs and other folk that are far from the "right tool for the job" end of the spectrum, and closer to the "my pet language exists in a reality where there is no justifiable need for trade-offs" end. I'd say there are a few points that can be discussed in the post's content (the old FP vs "pragmatic" crowd chestnut), so if OP were to adjust their presentation under a new account, they might have a better chance at inciting a flame war. Apologies for the r/iamverysmart tone above, English is not my first language and I'm way too lazy to edit this throwaway post to sound less pompous/smug/like academic wankery. To the OP: haha, made me post.
Exactly - what I've learnt using Scala.js at work and for all my recent personal projects is that building a good FE experience is hard. I just cannot hand-to-heart suggest scala.js when there are superior tools now to get the job done. Not to be harsh on the scala.js guys tho - they have done a tremendous job and it's up to the ecosystem to develop its killer app.
Might it make sense for someone to make a starter scalajs giter8 template? E.g. for ScalaFX I can just do [sbt new](http://www.scala-sbt.org/0.13/docs/sbt-new-and-Templates.html) (given sbt 0.13.13+): `$ sbt new scalafx/scalafx.g8` There's also a list of some giter8 templates: https://github.com/foundweekends/giter8/wiki/giter8-templates EDIT: one downside is that (at least in my experience) `sbt new` leaves an unsightly no-longer-necessary `target` directory in the current dir after execution. I find that a smaller downside than having to remove the `.git` dir from a template after doing `git clone` at the moment, but YMMV.
&gt; It's worth noting that Elm also has type system features for interacting with JSON in a way that’s far simpler than what you have in Scala. How could be easier than Scala? In scala you can just create case classes and deserialize into those, and you've got type safety, or even access the json with Dynamic.
Matryoshka and similar libraries have nontermination issues - if anything they make them more acute. This is why there isn't a recursion-schemes library for Idris. In Scala it's possibly less of an issue than in Haskell becaues things aren't lazy-by-default (so one gets an out-of-memory when one tries to form or unfold an infinite structure rather than nontermination when one tries to consume it), but you tend to advocate a style with heavy use of `Task` etc. that seems like it would have these problems. I agree that it's not much of a problem in practice, but there are people who would say the same about most of the problems that strict FP style avoids.
We're right in the middle of delivering over &gt; 100 tons of fertilizer to smallholder farmers in Kenya on credit. It feels incredibly good to have written software that I'm getting to see directly help some of the poorest people in the world. Turning the power of Scala towards good has been pretty great.
Sorry! I meant to say "with JS objects", not "with JSON". It has [records](http://elm-lang.org/docs/records), which [correspond to JS objects](https://guide.elm-lang.org/interop/javascript.html) and are kind of like Scala case classes, but support structural typing, where it looks to me like Scala.js [doesn't](https://github.com/scala-js/scala-js/issues/956). e.g. you can write a function that expects any object having a particular set of fields, and do something with it. Furthermore, if a JS function passes an incorrectly structured object to your Elm code, the exception is raised on the JS side, not on the Elm side, so you don't need to do any special error handling in Elm. I don't think Scala.js addresses this, so you would have to handle missing/mistyped fields using Scala code. If I'm wrong, someone please point me in the right direction!
Could someone explain why `cofor` results in a function, rather than being an expression that takes an arbitrary number of comonadic values, like `for` does? 
If the people they're teaching can't handle installing an IDE and a plugin, what hope do they have of learning Scala?
Modest Scala devs :)
I have to agree with the parent. Installing scala is by far the easiest part of learning. Installing it into intellij is already a simple process. Making it easier gets almost no gain. Scala is not the language that targets people that can't fight their way through installing an ide.
The point is that installing something is not a high bar at all. The only esoteric thing there is setting up an SDK; IntelliJ should probably default to the bundled JDK. Complaining that you have to go through several pages to get to the download is laying it on a bit thick. Just link to [the download page](https://www.jetbrains.com/idea/download/), or even link to the downloads for [macOS](https://www.jetbrains.com/idea/download/download-thanks.html?platform=mac&amp;code=IIC), [Windows](https://www.jetbrains.com/idea/download/download-thanks.html?platform=windows&amp;code=IIC) and [Linux](https://www.jetbrains.com/idea/download/download-thanks.html?platform=linux&amp;code=IIC). The second complaint is that you have to click through a "wizard" (ooh, scare quotes!). Are they targeting people who have never installed a software package before? They do the standard thing on each platform: Windows, go through an installer wizard; macOS, drag the package to Applications; Linux, untar a tarball. 
I'm the author of ScalaKata. I created this project in 2012. I was one of those who: &gt; can't handle installing an IDE and a plugin Since then, a lot has improved in developer's tools. For example, you can now import sbt projects into Intellij IDEA. Let's not accept the status quo and keep improving our toolset.
[removed]
I'm working on an example application for [React4s](https://github.com/Ahnfelt/react4s) using the Spotify API: https://github.com/Ahnfelt/react4s-example The example works, but I should probably add some comments :)
I'm working on a library for the Tak board game. http://github.com/daenyth/taklib
&gt; More generally this is useful if you want to stack a bunch of "zipper"-like operations - e.g. the time-step operation for a game or simulation, where what happens at each point depends on what happens at its neighbours and it's very hard to get the sequencing right if you do this with conventional functions. Can you elaborate on this or do you have any links? I'm actually working on a turn based game library right now and was already looking at a zipper for the the event history, I'm interested to see what more may be useful
I use the GUI for git blame in the gutter but anything to do with commit crafting or push and pull on the CLI. IMO the CLI is better, I've seen people who don't completely grok git make some messes via the UI
I just converted a single project to multi project and after a restart it did fine
No, I've never got the chance to go deeply into this stuff. A quick look found http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html , but I don't know the territory well enough to tell you whether that's a good or bad example.
That sounds awesome - is it open source?
Please, people, don't just add a "+1" comment on the issue tracker. If you want to vote for it, vote by clicking the thumbs-up in the right margin. This happens every single time anybody submits a link to any issue tracker in any programming-related subreddit. 
No, we're trying to build a company around the process. We will definitely be open sourcing some stuff as we go along though.
I don't think there's any way to downvote in YouTrack. I don't mind this thing existing, especially since it would be aside IntelliJ proper, but I'd rather the JetBrains team not spend their precious time on it. I have three open code highlighting issues and perennial red squigglies in my editor. Community Edition is already open-source. This seems like a perfect example where the community should step up and do the work, with the assistance of the JetBrains team where needed. But that's just my opinion.
I guess you can compare them from TodoMVC examples: * [Scala.js + Binding.scala ](http://todomvc.com/examples/binding-scala/) * [Scala.js + React](http://todomvc.com/examples/scalajs-react/) * [Elm](http://todomvc.com/examples/elm/) * No Clojurescript example found...
I created a link-shortener for fun, it's built with http4s, argonaut and doobie: https://github.com/timo-schmid/0x7e.xyz
If performance at that level is relevant to your use case then you need to figure out how to test it. If it's not relevant then don't worry about it. My guess would be that a tuple is just a class and will behave as one, but it's just a guess.
Learning a language is hard enough. Learning the tools at the same time can be mission impossible. Personally when I start learning a new language, I keep it simple and at first I just open some text editor that does syntax highlighting and work with the REPL. Worked for me thus far. Some people feel the need for an IDE from the start. IntelliJ IDEA fits that bill, but it's also very complex, as it supports many languages and tools. I can see beginners feeling overwhelmed when trying to setup a learning environment. Having an IntelliJ IDEA educational bundle for Scala sounds good.
I am working on a conference paper that details my work on bridging non-real-time and real-time signal processing in the context of the sonification platform [SysSon](https://github.com/iem-projects/sysson). Namely, I have taken an experiment with an image processing blob detection algorithm and refactored it into two smaller unit-generators [Blobs2D](https://github.com/Sciss/FScape-next/blob/ef107772154687547263490cd49dded295fbd2de/core/src/main/scala/de/sciss/fscape/graph/Blobs2D.scala#L39) and [BlobVoices](https://github.com/iem-projects/sysson/blob/046fda7009fc1758ffc0ba5eab7c76c0774d1e40/src/main/scala/at/iem/sysson/fscape/graph/BlobVoices.scala#L68) that can now be used from within our own SysSon IDE. The real-time sound synthesis process, based on ScalaCollider, can now include elements that refer to output generated by an automatically cached pre-processing stage using these offline UGens. The DSL looks very similar to ScalaCollider. FScape (non-real-time): val mIn = Matrix("anom") val d1 = Dim(mIn, "time") val d2 = Dim(mIn, "altitude") ... val taLo = 0.0 val taHi = 3.5 val win1 = win0.max(taLo).min(taHi) / taHi val win = Gate(win1, !win1.isNaN) val blobs = Blobs2D(in = win, width = width, height = height, thresh = 0.4, pad = 1) ... val mOut = BlobVoices(in = BufferDisk(win), width = width, height = height, numBlobs = blobs.numBlobs, bounds = blobs.bounds, numVertices = blobs.numVertices, vertices = blobs.vertices, minWidth = 10.0, minHeight = 4.0, voices = 4) val frames = MkMatrix("out", specOut, mOut) ScalaCollider (real-time): val mIn = Var("anom") val vr = Matrix("blobs") val dTime = Dim(mIn, "time") val dAlti = Dim(mIn, "altitude") val speed = UserValue.kr("speed", 6) val tp = dTime.play(speed) val timeTr = Impulse.ar(speed) val vp = vr.play(tp, interp = 1) ... output := mix The UGen implementation is still very involved. The streaming framework (Akka back-end) is simple enough to use for fixed-size buffer signals, but it's very difficult to represent structural data, such as the hierarchical blob structure, and my layer on top it (FScape) is not yet optimised for non-equidistant low-frequency signals and inlets/outlets of diverging rates. So while the result is very promising, there is a ton of new questions that came up, and a lot of work to do to make it pleasant to design these UGens.
Bintray?
We're building a news service webapp. Everything backend is Scala, and all new frontend development is in Scala.js. We're also looking to build a Mobile App in Scala.js, probably with React, eventually.
Oh crap, I had no idea that was a thing. Thanks! 
At the end of the article &gt; A few topics still need further exploration: &gt; Currently there is no support for specialization of collections. It would be nice to allow this in the new design if we can do it without too much of an impact on the majority of non-specialized collections.
I think it would be nice if the alternative iterator design mentioned [here](https://alexn.org/blog/2017/01/16/iterator.html) were included as part of the rework. Specifically, I think the `moveNext`/`current` pair is better than the `hasNext`/`next` pair or the `hasNext`/`next`/`head` triple from `BufferedIterator`, for the reasons mentioned in the blog post. I wouldn't mind a conversion to work as a Java iterator.
I wonder whether it will have some performance impact to bar anything on views. If you chain multiple transformations you might end up with excessive method call overheads per element. In the old architecture you had intermediate collections. Both approaches perform differently depending on the context. Somehow I am leang towards the old approach, but don't know why.
Feedback on writing style: - you have pieces of text that are underlined and are blue, just like regular links (e.g. "*Avoiding infix in suffix notation methods*") which is bad design. Remember, design isn't just about making things look pretty, but also about making it easier to use and read - you're making too much use of vertical whitespace. Paragraphs are about separating main ideas, allowing people to read on a diagonal, but if you make each sentence a paragraph, then it defeats the purpose. - code samples should be meant for the REPL, not for `*.scala` files, so you don't need to `object Runnable extends App`, which just adds noise
If you don't think this post has any quality then downvote it
Also Sonatype, there since 2012: http://blog.sonatype.com/2012/02/scala-artifacts-now-on-central/ - you have to create a 'ticket' to get access, but that's a one-off action, afterwards you can publish any number of open source artifacts with your group-id. They will be automatically found by sbt.
Here's an early experiment: https://github.com/scala/collection-strawman/pull/2 Another option would be to just let the compiler specialize everything. Once we have a more complete implementation we need to benchmark this.
Doing F# now after a few years of scala. The ecosystem is tiny compared to jvm. I hate visual studio. People complain about the scala compiler being slow but using sbt is lightning fast compared to waiting around for whatever VS is doing. I actually don't much like the type inference in F# in the sense of not usually having to specify the type of function parameters. That combined with easiness of using point-free style and no apparent culture of annotating function return types makes it much harder to just scan new code and get a sense of whats going on. The computation expression/lets not say monad thing is a bit odd. On the one hand they try and avoid using that terminology but then it uses return and bind anyway so why bother. Its not like "computation expression" tells you anything anyway. I prefer case classes to F# records for simple data containers, since you can just name the thing you are creating, aiding readability, and case classes don't have the naming collision problem. And being explicitly a class they fit with the rest of the system. The |&gt; in F# is ok but I find scala's OO to often be more concise. You just write x.map(y) instead of x |&gt; List.map y for example. Maybe my brain is just too settled on scala.
I think you make valid points. Thanks. 
I really wish we had the equivalent of Cargo/npm—those are the gold standards in package distribution and communities. (Thoughts scattered right now, sorry.)
Tuples are [case classes](https://github.com/scala/scala/blob/v2.12.1/src/library/scala/Tuple2.scala#L1).
Now, it is impossible to implement a `SpecMap[@specialized K, @specialized V]` such that `SpecMap[Int, AnyRef]` has primitive keys (try it! specialization works only for value-value or anyref-anyref pairs). I would not hold my breath for specialization in Scala collections, not without a bigger change like moving to miniboxing. I have an implementation of fast, specialized, array-back collections [scala-metal](http://www.github.com/denisrosset/metal) , but it required a bit of macro machinery, and the usage is not discoverable through IDEs/scaladoc. Edit: corrected link
Let's backup First you said this is self promotion,to which I have already given an explanation Next you say that this is not quality content. If you think it is not then you are most welcome to give feedback on improving this. Just by saying "hey your blog post sucks" is not going to help in any way and sounds more like a rant than feedback. And now you are saying something else like don't continuously post links here because it is annoying. You do realise that each post has its own topic and content right? Some people might already know about the topic and do not wish to see it again and for some it's new and interesting. I don't think people will be annoyed in that case. They will just feel like l, hey I know about this so I am not going to read about it again and I'll skip it. From your comments I take that you haven't read my blog post at all and you seem to hate for whatever reason. At this point you just seem like a troll rather than being useful. 
Thank you for the detailed feedback. I agree with everything you have said. Will work towards improving it 
This is something I've been working on; while normally it wouldn't be of much interest to people in a language-specific subreddit, in this case the application is built entirely in Scala, with Scala.js for the front-end. e.g. all the UI you see in the demo site https://demo.fluentcode.com/ is Scala.js. If anyone's ever wondered what a Scala.js website looks in practice, well here it is
That's amazing, keep up the good work haoyi! :) 
I use command line most of the time and rarely use IDE GIT features. For some reason I have seen issues importing a GIT project from file system vs. the Intellij import from a remote repository feature. I should caveat that I generally have minor issues with importing projects in to IntelliJ, once we're up and running it's all good, but I can have issues getting settled when opening a project for the first time.
It looks very similar to [Scalaz](https://github.com/scalaz/scalaz) + [ThoughtWorks Each](https://github.com/ThoughtWorksInc/each) Goggles: get"$myBakery.cakes*.toppings[0].cherries" Scalaz + ThoughtWorks Each: monadic[List]{ myBakery.cakes.each.toppings(0).cherries } Goggles: set"$myBakery.cakes*.toppings[0].cherries" := 7 Scalaz + ThoughtWorks Each: monadic[List]{ myBakery.cakes.each.toppings(0).cherries = 7 } 
The metaphors are multiplying!
What's the full stack of it ? Backend / frontend libraries? Thanks :) awesome work
Hm, looks nice, but I don't quite like using macros - unless it's a really huge boost of productivity. I really like the shapeless lenses, but traversels are indeed a good argument for something like Goggles.
Hi Yang Bo, good to hear from you! These similarities certainly are interesting, thanks for pointing them out! If we look closely though, we'll see that they are quite different. The Monocle code being generated by Goggles is something similar to: `Getter[Bakery,List[Cake]](_.cakes).` `composeTraversal(Each.each).` `composeGetter(Getter(_.toppings)).` `composeOptional(Index.index(0)).` `composeGetter(Getter(_.cherries)).get(myBakery)` If I read it right, the Each code would rewrite as: `myBakery.cakes.map(_.toppings(0).cherries)` `apply(0)` is a partial function that will throw an exception for empty toppings; however, the Monocle version above is completely safe, because the Optional index projects over the possible value. It doesn't quite translate to a monadic thing, because Option and List are different monads. We could convert the option to a 0|1 length list, but it would be a bit clumsy. Lenses/Optics are the best solution for this kind of problem, monadic code on its own doesn't quite give you what you want. Also, IIRC, "each" is some kind of magic keyword inside the Each macro, whereas the "Each.each" generated by the * symbol in Goggles is just a standard Monocle Traversal. It's much less complicated than what Each is doing. 
More of a general question, but are lenses doing anything more efficient than the copy syntax (besides the obvious niceties)
I don't understand the question, sorry
Hey sorry /u/DeontologicalEthics, no hard feelings but I think I'll have to remove this post for being offtopic
everything you make it pretty awesome. damn. 
I know a lot of fellow engineers who aren't tech savvy at all, yet manage to write code that solves very hard problems. While I agree that installing an ide should be a walk in the park, I realize that simply the *inconvenience* of it may accidentally exclude potential future contributors. 
I saw Autowire signatures in the api :-). And I guess akka-http.
To me Scala as a better Java was a cool trick to get a Java team to use it, but not really the end game. Just as a better Java, it isn't different enough. You need to call all in Scala to see the difference.
See also: https://www.reddit.com/r/programming/comments/5ue632/the_myth_of_using_scala_as_a_better_java/.
I also agree with the overall sentiment, but I don't think it's a "myth." You might start out using scala as just a better java, but then you start seeing these things and using them. It is still just a "better java" at this point, because you're not really leaning into any of the powerful Functional Programming principles. So I don't think it's a myth, but I think there's a spectrum of myth-levels at which point it's bad to keep doing it the "java way" and you should embrace some of the better points of the scala way.
I'm using Avro, and it works fine when I do explicit conversion to BytesWritable before saving. I want the serialization to be implicit in Hadoop's system, since that's a standard process and I want my main method to stick to clean, communicative types. But everything I've tried causes serialization issues either in Spark or Hadoop. I'm sure if I explore further I can make it work. However, that's not what is really making me upset. I had to do something funky with Avro since it's API is very Java as well. But I could easily encapsulate that mess and my Java Avro object is only referenced in a single Serializer class. Further, the Serializer API is easily plugged into Scala transform methods making it highly readable and coherent with the rest of the Scala code we are running in Spark. Hadoop's APIs seem designed to frustrate anyone that relies on a type system. Even the RDD.saveAsHadoopFile() method uses runtime type checking, and now I'm back to using a MultipleSequence OutputFormat class that uses [Text, BytesWritable] rather than my actual data objects. It's infuriating to me that I can't seem to encapsulate this functionality somewhere that hides these nasty bits.
From what I've seen, it's the way to go for "better Java". Scala is something that's worth doing for its own sake. It may as well be the "better Ruby" inasmuch as its a "better Java", from a language perspective. Although, I do think it's a fair point that if you have a bunch of legacy Java code to preserve, but you want to eventually leave Java behind, Scala could provide a reasonable migration path.
[removed]
I'm using Avro, and it works fine when I do explicit conversion to BytesWritable before saving. I want the serialization to be implicit in Hadoop's system, since that's a standard process and I want my main method to stick to clean, communicative types. But everything I've tried causes serialization issues either in Spark or Hadoop. I'm sure if I explore further I can make it work. However, that's not what is really making me upset. I had to do something funky with Avro since its API is very Java as well. But I could easily encapsulate that mess and my Java Avro object is only referenced in a single Serializer class. Further, the Serializer API is easily plugged into Scala transform methods making it highly readable and coherent with the rest of the Scala code we are running in Spark. Hadoop's APIs seem designed to frustrate anyone that relies on a type system. Even the RDD.saveAsHadoopFile() method uses runtime type checking, and now I'm back to using a MultipleSequence OutputFormat class that uses [Text, BytesWritable] rather than my actual data objects. It's infuriating to me that I can't seem to encapsulate this functionality somewhere that hides these nasty bits.
That's what I thought too. I just started learning scala, and before I did too much I was like "Oh cool, runs in the jvm, I can easily port this code!" Then came FP... Scala just incorporates sooooo many different paradigms. It's a different world than Java, and should really be recognized as such.
kotlin is scala--
It's not open source, but it's free to use for small teams of people. [Try it out](http://install.fluentcode.com/) =)
I'm pretty sure that structural types are no longer using reflection as of 2.12, but I can't seem to find that announcement anywhere
seriously? Didn't find any language is a better Scala and Kotlin is not exception. I could express it as better Android dev language.
The additions of Kotlin-the-language each solve particular problems, for example the "?" business to avoid dereferencing of null. Each of those additions has corresponding new syntax and new rules for checking code soundness. Given some Java code to refactor in Kotlin, the path ahead is clear and brings immediate benefits. The result will still be close to the Java code, in terms of abstraction. (Google "Design patterns in Kotlin" if you don't believe me. They are pretty much the same in Kotlin and Java now). You can probably take a Kotlin code base and translate it back to Java without changing the structure much, at the price of adding a little boilerplate. Scala-the-language is more orthogonal than Kotlin and Java; Martin often brags, and he's right, that the language spec/grammar of Scala is shorter than both. (That said, if you don't consider two particular hacks to enable better performance on the JVM: specialization and value classes. Properly specified, they would make the language spec explode.) But the flexibility allows many different programming styles; in that respect, it is like C++ with less pitfalls and saner syntax.
Interesting article. I would be interested in thought on why orm is bad. I know that from a functional programming point of view, modifying objects and then saving them is considered impure. But the main reason I started programming in Scala was that it offers you the powerful functional programming features but does not enforce them. Using orm in my opinion, speeds up the development process since you don't have to write the same sql statements for saving stuff into the database or reading from it. 
2009 I sucked it up too. "Hey Java is pretty much shit and here is this new better language called Scala, which you can use with your Java ecosystem!" And I was hyped! But, well then I saw the actual Java ecosystem... Java itself is a pretty clunky language, especially for people like me, who did scripting language programming all the time and are probably not considered Real-Developers-TM to the Java folk. So the ideas of Scala resonated with me pretty much. Anyway, after trying to configure stuff like Kafka, Zookeeper and Maven and setting up a SOAP API, I saw that the problem with Java isn't the language. Yes it's ugly, but JavaScript is ugly too, well even Python has its ugly parts. The problem also isn't performance, like so many C devs cry about daily. The problem seems to be that Java devs like to create things more complex than they need to be. Conglomerates of XML files, layers on layers on layers... I just don't know anymore. I didn't use Scala for long and it wasn't because it failed me, it did, it brought its own complexity with implicit conversions and overloaded operators etc. but that wasn't the reason, it was the ecosystem which usage Scala enabled, the ecosystem that felt like it was still stuck in pre 2000. :\
&gt; For a beginner? No chance. Can you explain how you've set up the expectation that Generic programming, or even type level programming is a beginner topic? &gt; My point is, given what's being actually being accomplished (namely, defining a mapping of type-level metadata to run-time values), a lot of machinery has to be thrown at the problem. You'll have to be more specific to make a coherent point instead of handwaving, calling things you don't like machinery. What doesn't belong? 3 type parameters? Witnesses? The inductive base case? Even in a language like Idris, where the type level and value language is the same language(not practically a second one like in Scala), you only save on verbosity.
I'm quite surprised at how quickly Http4s has come along. Too bad it's still not as complete as play or akka-http. I can't live without websockets. 
The Techempower Json Serialization [2016.11.16] benchmark is a pretty fair baseline test: https://www.techempower.com/benchmarks/#section=data-r13&amp;hw=ph&amp;test=json Apparently, Finatra rules among Scala Http frameworks. Akka-http is 'supposed' to be faster than Spray. Play does fairly well. And http4s beats akka-http. The next round of tests should prove interesting.
Holy shit this is awesome. So...can we get this in Scala yet? 
You certainly can use Scala as a "better Java", I've worked at 2 companies that have done that. Yes, it requires some different thinking, and a lot of learning on the part of Java programmers. The examples you list are minor quibbling: Loops - while loops are identical; for loops are only really different when you are doing a for-comprehension. Return - this is only a problem when returning from a nested lambda. The rest of the time, it functions identically. In any case, companies often adopt a "don't use return statements" policy. Checked exceptions - Scala does not use them, however, you can add checked exceptions via the "throws[T]" annotation. We do this on occasion, when Java compatibility requires them. I really don't see your point though; as a Java programmer, I avoided checked exceptions whenever possible. Try with resources - this is a relatively new feature in Java; you can do the equivalent thing using higher-order functions in Scala. "Loaner" anti-pattern: I've never even seen this in code. Scala has call-by-name and laziness; while useful, they can bite you in the ass, so intelligence is required... Type inference: types are optional, meaning you can add the types anywhere you are confused. A Java programmer can just put type annotations everywhere if they want.
ELIsomewhatfamiliarwithscala?
Play has its own performance benchmarking tests at https://playframework.github.io/prune/ 
Unlikely, see ezyang's [comment](https://news.ycombinator.com/item?id=13785395) on HN. Dotty is actually more closely aligned with Rossberg's 1ML (Odersky actually refers to 1ML in his ScalaExchange talk on Dotty a couple of months ago). Don't think we'll ever see ML type inference in Scala, but would be nice to see the thesis implemented in a new ML or somehow integrated into an existing ML like OCaml or Haskell; then there'd be more motivation to switch away from Scala ;-)
Lolwut, everything a singleton? I think you guys are using guice wrong. We use guice in scala and it's made our lives so much easier by being able to swap things in at test time and. H being able to easily refactor and inject concrete types without needing to use bindings. I guess to each their own 
What? finatra? we are reading same benchmark here? Colossus is very high and some s-server (is new for me). 
I'm familiar with all of that. Also, the loan pattern (mentioned in Odersky's book) isn't really considered an anti-pattern. &gt; Yes, it requires some different thinking, and a lot of learning on the part of Java programmers. That's my point! Selling it as "a better Java" may suggest that Java programmers can be immediately productive in Scala instead of having to learn a whole new language. This would obviously be a problem.
That's great news, even if leads to difficult choices for projects (should `Order` of `cats-kernel` be coherent or not?, and `Signed` of spire?)
Isn't this just getting rid of a compile time error and adding the possibility of a run time one? What happens if I'm using a library that exposes a coherent trait? How di I know it's coherent and that I'm obeying the rules?
In my experience, no one has ever said you can drop Java programmers into a Scala team and be immediately productive. Some knowledge does transfer: the JVM is the same, there is a high degree of interoperability between Java and Scala, you can use your familiar Java libraries (although over time, people will switch to Scala equivalents, where available). Scala is also in the family of C-style languages, so it isn't totally alien like LISP or Haskell. Finally, you don't have to leap into 100% functional programming like certain languages. You can slowly pick up functional ideas at your own pace, or ignore them entirely. As a former Java programmer, Scala was very attractive, for the above reasons.
The point is you don't need to know. Since instances of a coherent tagged type must be equivalent the compiler can safely disambiguate them, unlike in present day Scala.
`Order`: definitely not. In fact, the biggest worry about the proposal for some of us is that libraries will be overzealous in their use of `Coherent` which would shut out alternative implementations. 
I hope it's not just you, because that's awesome.
As tpolecat [points out](https://www.reddit.com/r/scala/comments/5xg747/allow_typeclasses_to_declare_themselves_coherent/dei9t8n/) this approach will breakdown with `Order`ings of things and the like. But otherwise it allows users to solve the problem of ambiguous implicits a la carte rather than being forced to manually supply a specific instance (of candidates that all supply the same behavior). As a first stab it certainly beats the status quo...
[removed]
I like the idea of Option, but I'm not aware of any JVM language that's been able to pull it off successfully for reasons like the one OP stated. That and the fact that in Scala 2.12.1 you can still do: scala&gt; val wat: Option[String] = null wat: Option[String] = null scala&gt; wat.isEmpty java.lang.NullPointerException I'd rather see a JVM language handle null safety directly since nulls still exist on the JVM. Groovy's way could have been decent if it enforced null checks. Kotlin [does this and then some](https://kotlinlang.org/docs/reference/null-safety.html), so it looks like it would be pretty safe. Ceylon [takes a similar approach](https://blog.jooq.org/2016/03/15/ceylon-might-just-be-the-only-language-that-got-nulls-right/) (warning: slightly trollish post), too. That being said, as a Scala and Groovy developer, I've had time to experience the pain of both of their approaches while I can only go on the hype for Kotlin and Ceylon's take on null safety. They could end up just having a different set of issues for all I know.
That never happened to me during me 2years of scala though... not that your points are not valid... 
While possible, I haven't seen anyone go to the lengths of abusing Option like that. There's always wartremover (https://github.com/wartremover/wartremover) to help identify nasty behaviors like that.
The reason it doesn't happen is because you'd usually wrap `wat` with the `Option` constructor: scala&gt; Option(null) res0: Option[Null] = None This produces the behavior you would want. I do agree, though, that the use of null is very dangerous and breaks a lot of contracts.
I've used a couple setups in the past - my current company uses Nexus, but at my previous company we used Artifactory, which was a massive pain to set up. Eventually we just set up a Maven repository on top of Amazon S3, which took a lot of initial effort but worked really well thereafter. Nexus has been much easier to set up comparatively and I've liked it so far.
I have trouble seeing the issue. `Some(null)` is a perfectly fine value of some `Option[A]`. Whether `A` can be `null` is solely `A`s business, not `Option`s.
Our setup is fairly simple: artifacts are hosted in an Ivy repository, which is basically a directory. Publishing scripts need it mounted locally (to work around an SBT bug where maven metadata is not updated on non-local files), building scripts just declare it as a webdav repository. It's worked for us for a long while. I'm starting to have second thoughts though, since we now use gitlab-ci for CI, and it's proving *really !@# hard* to mount our repository locally from within docker.
Have your tried configuring it as a volume in the runner config file?
for OSS I've used bintray once (https://bintray.com/) for private/internal Amazon S3 has been working for me until now
But by doing so it breaks the functor laws
Thanks for clarifying
`Some(null)` is kind of defeating the purpose of `Option[A]`. In this particular example it is the `Map[A, B]` which wraps the value `B` in an `Option[A]`, so you'd expect it to handle the case where the value is `null`, but it can also be unexpected in some cases. It boils down to library authors having to make a choice, and they choose to let that be a valid return value.
The Coursera class linked in the sidebar, "Functional Programming Principles in Scala" is a very good introduction to Scala and functional programming, taught by the creator of the language. I'm also working through the book "Functional Programming in Scala." ~~I'm not sure this is linked~~ It's linked, published by Manning, and it's a very good resource as well - the approach it takes, at least for the first five chapters that I've made it through, is to have you implement standard library features.
[removed]
I've typically seen it happen when working with a method from a Java library that unexpectedly returns null. I would expect the Kotlin compiler to enforce such calls to always handle the possibility of a null, although if that's not true, it wouldn't be any better than Scala's approach.
It's certainly not a prerequisite, but if you're already comfortable with Java it might help to ease you into a more functional way of thinking. Then, when you're familiar with both, you can decide for yourself which you like more.
If `null` is a valid value in the domain of some `A`, it's perfectly correct for `Option[A]` to carry it.
No, it's not. If `null` is a valid value of some `A`, `Option` has no business in messing with it. Don't want `null` to be a valid value of `A`? Make `A` a non-nullable type.
Koltin/Ceylon don't handle `null`s coming from Java, so back to square one.
From TFA: &gt; *Some(null) is never EVER what you wanted* That's false. As an anecdote I wanted `Some(null)`. As long as the type system allows `null` values for `A &lt;: AnyRef`, then `Some(null)` is equivalent with `Some(None)`, which is obviously different from `None`. And you might end up with `Some(null)` for the same reason that `null` is still useful even in Scala for as long as the type system allows it, besides Java interoperability: to avoid the boxing that happens with `Option`, which is kind of expensive when doing high performance work, stressing the garbage collector, so for properly encapsulated pieces of code, as long as `null` usage isn't exposed in public APIs and as long as you know what you're doing, it's perfectly OK to use it. &gt; But what happens if null creeps in as a value? What would you expect the following to return? &gt; &gt; val map: Map[String, String] = Map("bar" -&gt; null) &gt; map.get("bar") &gt; &gt; If you guessed None, you’re wrong No, I did not guess `None`. Why would anybody guess that? The key `"bar"` is part of that map, so it has to return *something* different than `None`, which is reserved for *missing* keys. Returning `None` does not make sense. In fact this is the perfect example for why `Some(null)` is useful.
Sure, asumming null is actually a valid value. Considering that null is "we didn't put a value here" I have a hard time seeing it as a valid value. Hence it throwing an exception if you try to use it.
Nexus all the way, word of warning though... They are on the cusp of 2.x to 3.x releases and migrating up from 2.x is a feature they have promised with no delivery for almost a year now... So use 3.x if you are doing a new install.
Java's `Optional` and Scala's `Option` both model type that support absent values (as an alternative to using nulls). If you want a type that can be an error then you would use Scala's `Try` type, or it's `Either` type if you want control over the error type. Care to explain why you think `Option` and `Optional` are so different?
I'm new to Scala and finding that compile times are slow (sometime very slow) compared to Java. Is this the norm? Is there anything that can be done to mitigate this?
In Scala, errors are handled with types like `Option`, `Either`, `Try`. They are not handling `null` in any special way, evidenced by the fact that all of them allow expressing "there was a value, and that value was null". These types are _not_ an alternative to `null`s (as `null` can be a perfectly fine result value), they are an alternative way to handle errors. In Java, errors are handled by returning `null`. `Optional` only makes this more explicit, but `Optional.empty` is literally representing the case of "this was null".
I once saw a video which explained why monads can be seen as monoids in the category of endofunctors but I've since lost it and it doesn't click. I've forgotten which video it was. Any ideas?
&gt; As long as the type system allows null values for `A &lt;: AnyRef`, then `Some(null)` is equivalent with `Some(None)`, which is obviously different from None. I agree that it is different from `None`, but I don't see how that equivalency holds. &gt; And you might end up with Some(null) for the same reason that null is still useful even in Scala for as long as the type system allows it I'd argue that the type system is flexible and allows many things. Should we use them all? Not sure. &gt; as long as `null` usage isn't exposed in public APIs and as long as you know what you're doing But that's the thing, this is a public API. Should the API return `None` for `null` values? I don't know, and I stressed that in the post. Like everything, it has pros and cons, but perhaps making things clear via documentation would make people more causes when dealing with values that are potentially `null`. It is misleading. &gt; No, I did not guess None. Why would anybody guess that? Why? For the same reason `Option(null)` returns `None`. Don't forget that the API itself wraps a value of type `B` in an `Option`, the type of `B` isn't an `Option[B]`. It is completely logical to assume nullness of a value would produce `None`. &gt; The key "bar" is part of that map, so it has to return something different than None, which is reserved for missing keys. That is a great explanation, and I think the docs should definitely add clarification.
Last lecture in [this series](https://www.youtube.com/watch?v=I8LbkfSSR58&amp;list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_) by Martosz Milewski. But watch all 20 ;-)
Note that these are not the same thing. The type of `Some(a)` is more precise, which is usually *not* what you want (you would never have a parameter of type `Some[Int]` for instance). So this can be the basis of an argument for you. scala&gt; import cats.implicits._ import cats.implicits._ scala&gt; 1.some res0: Option[Int] = Some(1) scala&gt; Some(1) res3: Some[Int] = Some(1) scala&gt; List(Some(1), Some(2)) res4: List[Some[Int]] = List(Some(1), Some(2)) scala&gt; List(1.some, 2.some) res5: List[Option[Int]] = List(Some(1), Some(2))
Thanks. I was aware of this distinction but haven't figured out a non-convoluted case where you actually run into problems with `Some(x)` (as my `x`-es will generally be non-nullable so I suppose it's some covariance/contravariance cases where this may matter).
If your container is invariant you'll have to "cast" Some(x) up to Option through type annotation, otherwise your code won't compile
Although you can just use the `Option(1)` constructor which will return Options
Can someone explain what kind of dark magic is this? This is fine scala&gt; val m = Map.empty[Int, Int]; m += (1 -&gt; 2); &lt;console&gt;:13: error: value += is not a member of scala.collection.immutable.Map[Int,Int] m += (1 -&gt; 2); ^ This took me by surprise: scala&gt; var mm = Map.empty[Int, Int]; mm += (1 -&gt; 2); mm: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 2) scala&gt; mm res12: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 2) scala&gt; mm += 3 -&gt; 4 scala&gt; mm res13: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 2, 3 -&gt; 4) ... I guess it desugars to + and =, but... I find that unfortunate
`Option.of(null)` isn't actually possible because it will throw an exception at runtime. That is part of the reason `Optional.map` has to break the functor laws.
Does anyone else feel horribly unproductive in Scala? I have programmed in Python &amp; Java previously, but have been playing with Scala over the last 2 years. Don't get me wrong, I love Scala. The power of the language (powerful type system, macros, OOP), all its intricacies (read: hacks) and the ecosystem of functional programming have seriously broadened my perspective on programming languages. But there are so many ways to do things I am never sure if I'm doing it the right way. I'll be using `Future` for some trivial thing and feel icky for not using a proper(but completely unnecessary for trivial use) abstraction like `Task` so my `apply()` wont be side-effecting and my `map` will be trampolined. It just feels like there a lot more of these tiny decisions to be made in Scala than Java, and its fatiguing. There are a hundred firehoses to drink from - new libraries doing things i did not even know were possible, increasingly advanced FP magic etc. Does anyone else struggle with this? Is there some secret art to reading &amp; writing Scala *quickly/fluently*?
Sbt supports GitHub dependencies. 
Yeah it's a struggle but eventually you are just so productive with refactoring and not having bugs that it doesn't matter as much if you spend 4x as long thinking about how to approach the problem than you would have in Java. I spend one tenth of the time as I do debugging when using Scala so still a net win. I'd say you need to be confident in the ability to refactor, so when you're under the gun to get something done or shipped you just do it and make it better later as you learn new techniques. 
Thanks for sharing!
&gt; there are so many ways to do things I am never sure if I'm doing it the right way There are so many ways to do things I've generally stopped worrying about the "right way". Instead I advocate for finding a "good enough" way that meets your immediate needs but which doesn't prevent you from making improvements in the future. I think that generally means you strive for a strong API by thinking through that (whether at the REST level or the library level or whatever) and letting the implementation details work themselves out later. Take the "easy wins" where you can get them, e.g. using a sealed trait for exhaustive pattern matching or designing the code for simple testing, but otherwise stick with what you know and what you can glean from any feedback you get from team members. Over time what you know will evolve so you'll see issues in advance and consequently get more "easy wins" sooner, both for yourself and for any teammates you have. You'll start to recognize when using `Future` is sufficient, when the benefits of switching to `Task` are worthwhile, and then again when you actually need `Future` instead of `Task`. You'll also recognize when you can improve on something you did in the past, but if you've thought through the API and don't have to change it then you're left with a relatively localized refactor, which is much simpler to handle. There's a bit of an art to it and the insights definitely come from experience. There is no one size fits all right way - there are always trade-offs. The tricky part is to learn how those trade-offs are likely to affect you and those who depend on you, when their effects are likely to happen, then balance the costs of addressing any issues now against the costs of dealing with them later. Definitely don't get caught up with dogma. Yeah, there are new libraries doing neat FP things. That doesn't mean they're "proper" or right for you. Not everything needs to be pure and trampolined. FP is not the one true way. OOP is not the one true way. There can be real benefits on both sides. I suggest starting with whatever is simple for you and introducing complexity only as you need it. &gt; Does anyone else struggle with [feeling horribly unproductive in Scala]? Is there some secret art to reading &amp; writing Scala quickly/fluently? I'd argue that writing "quickly/fluently" is less valuable and less productive in the long term compared to writing "correctly/reliably". I propose worrying less about how fast you reach functionality and more about how well you meet your needs, how consistently it behaves, and how easy it is to change if necessary. Your own productivity is not the only thing that matters; the productivity of those who come after you can be even more valuable.
IMO it's too suprising for a reader relative to how much value it provides. `m = m + (3 -&gt; 4)` is not much more to read, and is clearer.
Play is... special. You might have more luck asking /r/play as this is likely to be some play-specific thing.
Figure out how much you actually care, and how much he does. Pick your battles. Find a code standard both of you can agree on, and accept that that will involve both of you doing some things that you don't prefer. (IMO `x.some` is not better by enough to be worth requiring a library and an implicit. Certainly some libraries like to have no dependencies, so you will never get to a point where all the code you have to read uses `x.some`, so I'd prefer to always use `Some(x)`)
The problem is how you get to there from here. `Optional` is a key tool for eliminating `null` from your codebase - but the Java version is written so that you can only use it if you've already eliminated `null` from your codebase :(.
Some early Scala libraries used to do that kind of thing. If you're working with old libraries (perhaps because you have to interoperate with old technology stacks e.g. WS-*) you'll occasionally see it.
Use `Optional.ofNullable` to map nulls into `Optional.empty` as soon as they hit your code?
I just wish i could do this without the performance overhead: case class DemoValueType(demoType:String) { def apply(demoType: (String) =&gt; String = (x) =&gt; x) : DemoValueType = DemoValueType(demoType(this.demoType)) } case class DemoValue(value: Int, valueType: DemoValueType) { def apply( value: (Int) =&gt; Int = (x) =&gt; x, valueType: (DemoValueType) =&gt; DemoValueType = (x) =&gt; x):DemoValue = DemoValue(value(this.value), valueType(this.valueType)) } case class Demo(name: String, value: DemoValue) { def apply( name: (String) =&gt; String = (x) =&gt; x, value: (DemoValue) =&gt; DemoValue = (x) =&gt; x):Demo = Demo(name(this.name), value(this.value)) } val dmo = Demo("hai", DemoValue(3, DemoValueType("number"))) //This usage is clearer val changed = dmo(value = _(valueType = _(demoType = _ =&gt; "int"))) //But you don't need to name all the variables val changed = dmo(value = _(valueType = _(_ =&gt; "int"))) of course, the method could use the name "copy" but that would make it more verbose I really like this, since it does not introduce any new syntax, and feels intuitive.
&gt; "loops" don't work the same way Ordinary `for` works the exact same way. Scala offers `for`/`yield` if you want it, but if you're using Scala as a better Java you just ignore that it exists. &gt; return statements don't behave the same Only in terms of how they interact with fancy constructs. If you're using Scala as a better Java you can use `return` and it will behave as expected. &gt; error handling patterns are different A lot of popular Java libraries use only unchecked exceptions; any serious Java developer is capable of working with such libraries, whether they agree with that design or not. &gt; resource management patterns are different True, but new in Java too - I guess a more precise phrase would be that one can use Scala as "a better Java 6". But again any serious Java developer will be capable of working with the older style, at least for the time being. &gt; type inference can mean the onus is on the developer to know to follow the types. If you're using Scala as a better java you just always include the types.
We use Nexus at work. I'm not sure how difficult it is to setup since I wasn't involved. * Dependency management works well and is generally painless * UI feels a bit clunky, but relatively useable and intuitive * We publish using sbt-aether-deploy plugin which writes maven metadat - this allows us to use sbt-updates plugin to check what internal dependencies can be updated :)
That point isn't entirely correct, and it was demonstrated why in the replies of the thread. By entirely correct, I mean if you use the assumption that an `ExecutionContext` works as expected, else there isn't any real difference in regards to referential transparency. And as pointed out in a different thread, both `Task` and `Future` aren't referentially transparent when it comes to either abstraction handling exceptions that happen to be thrown in `Task` or `Future` Also eager evaluation and referential transparency are somewhat orthogonal concepts, just because something happens to be lazy doesn't make it referentially transparent.
It can get pretty cumbersome - rather than `myOption.map(myObj::myMethod)` you have to write something on the lines of `myOption.flatMap({x =&gt; Optional.ofNullable(myObj.myMethod(x))})`. And you can never know when that might be a semantic change that will break your program. Ultimately you're working towards a world where `null` never hits your code and you only check at the boundaries, but it's really important to be able to get there piecemeal.
In some cases, `Future` has better performance and in some cases `Task` has better performance. For the use case that `Future` was designed (handling a lot of async concurrent tasks that are bound by things like network/database IO), it will have overall better performance than `Task` because of the lazy handling that `Task` has. On the other hand, if your concurrency involves jumping around on different cores `Task` will have better performance, because it runs the `Task` in a tight loop and it won't need to jump to a different `ExecutionContext` every time (which hurts performance and cache locality for `Future`) Note that if you are writing code which is only run concurrently over cores and you care about performance, then you will always get better results in using threads/locks/mutex's although this is a hard style to code in.
Yes modify and modifyF do something slightly better than `set . f . get`. It is based on the Van Laarhoven encoding of Lenses, the idea is to avoid reading several times the same value (in your example, you do game.currentLevel and game.currentLevel.player). Here is the code: https://github.com/julien-truffaut/Monocle/blob/master/core/shared/src/main/scala/monocle/Lens.scala#L130 Even though this implementation is better than `set . get`, Lenses still cause a performance overhead around 2x comparing to hand written copy (as measured with jmh) 
Provides and singleton are not paired together. One is a marker to indicate how to create something, the other is a lifecycle marker. Most things in guice are not singletons 
replacing a `map` with a `flatMap` that avoids NPE might be a semantic change. In a realistic large codebase you will sometimes have e.g. code paths that always NPE and this is caught by some retry logic or some such. Yes, it shouldn't happen, and should be cleaned up when it does. But the ability to be absolutely certain that a given refactor isn't going to change behaviour, even in the presence of terrible code above and below in the call stack, is really important.
&gt; So invariance is maintained under the substitution model of evaluation? I believe the standard opinion of my echo chamber is that this is only true for Future with pure computations. So there are real differences. What do you mean by inpure computations? You can actually model inpure computations in `Future` such that they still follow the laws of substitution as long as you reflect this gets reflected in the type system (with the caveat I mentioned before about `ExecutionContext`) &gt; Which is a lot fairer than some random "Future is broken" propaganda. It is what it is. Agreed, there are many legitimate uses of `Future`, and actually any sane `Task` implementation uses `Future` somewhere under the hood because certain operations require it for sane performance symantics
But according to the issue, in the mergeable `TreeSet` case you should be using a scheme similar to ML modules. &gt; For priority queues, Chris Okasaki's scads is a nice solution, which generalizes readily. In essence, all that's needed is to move the type parameter T one level further out However, I'm wondering how that works out for operations like `map` that change the element type (scads does not seem to have it).
I'm trying to use scala.util.parsing.combinator.RegexParsers and am running into something I don't quite understand. In particular, the differences between these: def BC:Parser[C] = BTYPE | CTYPE def CB:Parser[C] = CTYPE | BTYPE When I have input that matches CTYPE, BC fails to match but CB succeeds in the match. I'm stumped as to what's going on, the only complication I can think of is that BTYPE and CTYPE both start off similarly but CTYPE continues further.... Here's the full code. import scala.util.parsing.combinator.RegexParsers case class C(ab: String, index: Int, col: Option[Int]) object abc extends RegexParsers { def INDEX: Parser[Int] = "[01]?\\d".r ^^ {_.toInt} def BTYPE: Parser[C] = ("B" ~ INDEX) ^^ {case rt ~ rx =&gt; C(rt, rx, None)} def CTYPE: Parser[C] = ("B" ~ INDEX ~ "." ~ INDEX) ^^ { case rt ~ rx ~ dot ~ rc =&gt; C(rt, rx, Some(rc)) } def BC:Parser[C] = BTYPE | CTYPE def CB:Parser[C] = CTYPE | BTYPE def main(args: Array[String]): Unit = { println(parseAll(BC, "B1")) println(parseAll(BC, "B04")) println(parseAll(BC, "B1.0")) // no println(parseAll(BC, "B10.3")) // no println(parseAll(CB, "B1")) println(parseAll(CB, "B04")) println(parseAll(CB, "B1.0")) // yes println(parseAll(CB, "B10.3")) // yes } }
Hey, you might get more answers in the ask-anything thread, since this is more a question than a project.
Awesome summary, thanks!
&gt; For the use case that Future was designed (handling a lot of async concurrent tasks that are bound by things like network/database IO), it will have overall better performance than Task because of the lazy handling that Task has. That's not true, performance is similar and can be better for Task (when compared strictly with Scala's Future). This is because with Future you're forced to do one or two CAS operations on an atomic reference for each Future involved, whereas for Task you can pick and choose from whatever synchronization model you want, depending on use-case. Note that Task.gather in Scalaz has poor performance but it doesn't have anything to do with the Task evaluation model.
I understand the semantics of Some.apply vs Option.apply. The question that should be asked is "Which of these is suitable for this particular use case?", i'd say both candidates are applicable, which one should be selected will tend to go to phylosophical more than practical
&gt; That's not true, performance is similar and can be better for Task (when compared strictly with Scala's Future). This is because with Future you're forced to do one or two CAS operations on an atomic reference for each Future involved, whereas for Task you can pick and choose from whatever synchronization model you want, depending on use-case. I think we have a misunderstanding here. If you are using `Future` for basically multiplexing asynchronous operations not on the CPU (i.o. composing network requests), then it will have better performance (your own benchmark tests actually show this). What you are talking about is doing a lot of `.map` operations on a `Future` (in which case `Task` does have better performance). It can however be argued that this is a more fundamental issue of Scala not yet having IO fusion, and actually there is work on Dotty which is addressing this. 
I think your opinion is valid, I too thought the same thing for the first months of learning scala, but as more time passes I got use to it, and honestly, now it feels like a chore to code in any other language. 
Oh no problem! After I answered I went to check your answer and I linked because I found it was better written than mine. 
&gt; On the other hand, if your concurrency involves jumping around on different cores Task will have better performance, because it runs the Task in a tight loop and it won't need to jump to a different ExecutionContext every time Please note that this new Future implementation diverges from the Scala Future considerably regarding the execution model. It *does not* use a separate thread pool or executor unless the user explicitly wraps a computation with a `FuturePool`. The readme has more information about it. Given this different execution model, it tends to have higher throughput and lower memory footprint than `Task` for cpu-bound compositions.
Oh I apologize, I didn't notice that you are not OP. Yes, your opinion is a 100% valid. And my opinion may change if I inherited a code base. 
I have not seen a satisfactory answer to this question, other than global coherence for all type classes. Which is how I do things.
Fix the language.
Yes, and this is the same flawed reasoning that we went through the other thread. `val` is not the same thing as `def` in Scala, and in this case `Future` is actually mimicking the same semantics that design Scala. Yes its true that `val x = Future { // something }` is not pure, however its for the same reason that `val x = { // something }` is not pure. Since `val` is strict, it gets executed immediately, and unless you use some monad transformer/eff to abstract over the side effect (you can do this with `Future` just as you can with anything else), then yes it will leak side effects. This is what I meant when I said &gt; You can actually model inpure computations in Future such that they still follow the laws of substitution as long as you reflect this gets reflected in the type system (with the caveat I mentioned before about ExecutionContext) There are two main points which I am trying to make though 1. The real difference between `Future` and `Task` is that one is lazy, and one is strict. The other differences are more subtle and differ between implementations. 2. `Task` has some "better" handling for side effects, however this can easily be simulated with `Future` by using the same abstractions when we deal with non async code 3. Both `Task` (the common implementations) and `Future` are not referentially transparent when it comes to dealing with exceptions. In this case, trying to use `Task` as a guarantee of RT is a bit of a red herring, particularly since common use of `Task` does tend to wrap/abstract over code which is not uncommon to throw exceptions
I've googled a bit but couldn't find much information about the non-allocation model. Does it mean that it does only stack allocation? Is it built on top of green threads? Sorry, I'm not familiar with Rust :)
Actually, the Scala Future seems to be the exception regarding scheduling. Twitter's Futures (that were the inspiration for Scala's) have a similar model that reuses the current thread but in a stack-safe way, and Java's CompletableFuture has the non-*Async methods that also reuse the current thread.
Hmm, I wonder if the Goggles macro could fuse `foo.bar.baz` chains into manual copy calls in the future..... we can safely do all kinds of violence to the implementation behind the macro syntax.
Suffice to say, I disagree on "flawed reasoning". Other than that, interested people should do some further research.
OK so looking at this a bunch more, I think I'm in something like this situation: http://stackoverflow.com/questions/7812610/non-greedy-matching-in-scala-regexparsers and so BTYPE is a "successful" match for the "B10" part of "B10.3" but fails overall because there's still the ".3" part left over, but since BTYPE succeeded it doesn't go back and try CTYPE. I could try implementing the solution from the stackoverflow post, but to be honest I'm pretty disgruntled that it's necessary at all, and a lot of the recent wisdom out there is of the form 'don't use the built-in parser anyway', so I'm considering just punting on a bunch of work and trying fastparse....
yea, so much this
Seconded. I know how much time good documentation can take (well, no, honestly I know how much time mediocre documentation can take). So I really appreciate seeing something like this! 
I like the coursera course on [Functional Programming in Scala](https://www.coursera.org/learn/progfun1/home) quite a bit.
Indeed, the underscore books are excellent but pricey. For a newbie, Programming in Scala is a good start. Then go from there. 
FP in Scala is a very good book in my opinion. I have owned it for a few years now, and I really like the constructive approach taken there.
I don't agree with recommending FP in Scala for a newbie to learn Scala. The premise of the book is to teach FP principals and it just uses Scala as the vehicle to get there. I will say though it is an awesome book once you are more familiar with Scala. Programming in Scala is good, and I've also heard good things about Scala for the Impatient.
There's been a lot of posts lately about this lately. Search first please. I got my foot in the door with Scala for the Impatient. Gets you to a point where you can at least write a script. From there I recommend Functional Programming in Scala. 
I quite recommend Programming in Scala 3rd and Scala for impatient 2nd. Better not jump to FP in Scala before you read either of above books.
Scala for the Impatient book.
Yeah, it can take a while to find your groove. I try to understand what idiomatic Scala is by trying out a lot of different things and figuring out what 'feels right'. But also you have to look at a lot of other people's code to see what they think the idioms are. For me idiomatic Scala seems to be somewhere between OOP and FP, with OOP useful for the structuring and FP useful for the behavioural and operational aspects of the code.
Here's an explanation from Martin Odersky http://bruceeckel.github.io/2014/12/30/operator-underloading-in-scala/#comment-1767719959
I'd recommend using https://github.com/playframework/play-ws over Gigahorse right now, as it uses AHC 2.x and has caching built into it.
Programming in Scala and then FP in Scala + the 47 Degrees scala exercises. From there you should be able to work with just about any scala out there. 
With all due respect to the document, how do they help here? "Optionally returns a value" doesn't really say much. Do all APIs that return None when something is `null` specify that explicitly? Not sure. 
[removed]
Thank you. That was very inspiring. While the reasoning is easy to understand, I never thought about it. 
That was actually quite... off-putting. I mean, when I start dabbling in the language, I certainly don't think about saving all of humanity. You should seriously work on your sales pitch, man. 
Scala School then learn Haskell or Ocaml and practice.
&gt; "Optionally returns a value" doesn't really say much. Please don't quote it wrongly! It says "returns **the** value" and it refers directly to the key. So, if you ask for the value of the key "xyz" then you want to get **the** value without any transformations. If it would really say "a value" then you would have a point here, because it could be any value (but how would that information be usefull?!). &gt; Do all APIs that return None when something is null specify that explicitly? Not sure. I'm also not sure. But I would at least expect, that if they are just storing values or provide me values (e.g. as a proxy) then they better tell me if they manipulate these values before they pass them to me when I just ask them go give me the value. If not, I would open an issue.
Thanks for writhing this up
Interesting read. It's begining to get even more confusing to me now on how to write efficient scala, as a relative newcomer to the language. 
Quoting was a mistake as I did it from the top of my head, wasn't intentional. I still think distinguishing between *the* and *a* isn't descriptive enough, as I think you'll agree.
You don't have to use the constructs you don't understand. Really, it is 100% possible to use Scala as a better Java, spending less than half an hour learning anything Scala-specific and remaining as productive as you were in Java. Write Java-like code with slightly nicer syntax, ignore everything that wasn't in Java (including libraries, just use Java libraries or libraries that offer a Java interface - even Java collections initially). I have direct personal experience of this.
It's a lot of code overhead for not much benefit. It can be hard (and counterintuitive) to convince the compiler that your types are correct. As MasGui points out, trait initialization is unsafe: you get no warning if you access a field before it's initialized.
I'd like a good way to build native code from Scala (i.e. something like LLVM bindings, or better still an actual-scala implementation of the same thing). Though that's probably pretty niche.
Oh ick. Universal equality strikes again.
To add to this, one can use Scrooge to produce reasonably idiomatic Scala classes for Thrift definitions.
&gt; Suffice to say, I disagree on "flawed reasoning". Other than that, interested people should do some further research. Sure thing. Maybe "flawed" isn't the right work, but the point I am making is that `Task` (at least the scalaz `Task` that most people are familiar with), isn't just a pure `Task` in the sense that `Future` is, i.e. it also handles a lot of extra other stuff, which includes side effect capturing. The argument I am making is that side effect capturing is orthogonal, and it can be expressed via other abstractions (such as Monad transformers) which can easily be used in conjunction with `Future`
Yeah this is interesting, I am not sure why Scala Future uses this model as it does make Scala's Future have worse performance unless you have some sought of fusion
Disclaimer: I am more into distributed systems in of themselves than data analytics. First off did you try map(x =&gt; whatever(x)).collect then writing to a sequence file. When I have to deal with hadoop, I use Scalding, I never used it side by side with spark, but I think it's doable, they also have spark-scalding which is deprecated.
If you're talking about the benchmarks from my blog, those are out of date. I don't believe in "operator fusion" by means of compiler tricks. I think I mentioned this to you before. It just makes benchmarks look good. Fusion happens naturally by laziness, no compiler tricks needed and if you want fusion, then laziness is better than compiler tricks that only work for really simple use-cases.
&gt; inscrutable is a bit of hyperbole implicit def cconsIsEnum[K &lt;: Symbol, H &lt;: Product, T &lt;: Coproduct](implicit witK: Witness.Aux[K], witH: Witness.Aux[H], gen: Generic.Aux[H, HNil], tie: IsEnum[T] ): IsEnum[FieldType[K, H] :+: T] = new IsEnum[FieldType[K, H] :+: T] { def to(c: FieldType[K, H] :+: T): String = c match { case Inl(h) =&gt; witK.value.name case Inr(t) =&gt; tie.to(t) } def from(s: String): Option[FieldType[K, H] :+: T] = if (s == witK.value.name) Some(Inl(field[K](witH.value))) else tie.from(s).map(Inr(_)) }
programming in scala is good (odersky). fp scala book and programming scala (wampler) are books to look at next if your ready for a challenge,. use the gitter channel for any questions you have. also knowing how to import programs into the REPL is very useful (then you can play around with code easily). intellij with scala plugin is the best editor i think (for browsing code). read the sbt with scala files example - i prefer compiling with sbt in a console to using intellij. 
&gt; The body of this if-statement is just Unit methods, so I'm not sure why I would use fold. `fold` is a way to have different code paths for different variants of an ADT. It's like a safer version of a pattern match. (You could also just do an actual pattern match with `case Nil` and `case _`) &gt; I only want to run doSomething() if the list is empty, and only run it one time. The snippet I posted should do that. &gt; I want a method that will only act on an empty list and/or None elements. So the method will do nothing if the list is non-empty. I really think the clearest way to express those semantics to readers of your code is with an `if`. If you really want a method then all I can suggest is implementing your own and pimping it on.
I created a small, simple API with Finch. Maybe it will be helpful. https://github.com/adrice727/interactive-classroom/tree/develop/server
I'm under the impression that implicit conversions are mostly useful in interop situations: interop with Java libraries, scala-native, scalajs, etc. And this is because the constructs in those scenarios are not as powerful. For example, nscala-time has implicit conversions because the base types cannot be extended except through kludgy inheritance. However, a library written in scala is more likely to use better abstractions that have easier-yet-safer extensibility. If I'm not mischaracterizing, then wouldn't this be a question of whether or not the future of scala depends on interoperability? How much do java/javascript/C libraries infect the average scala program? As a tangent, is there any semantic difference between (implicit def + class) and implicit class? 
My experience with good Java and Ruby people moving to Scala is that they enjoyed the book and learned a great deal from it. It provided a foundation for libraries like the Type Level libs. Perhaps for less experienced developers you are right that the other books you mention are probably better to begin with.
Great work! I was contemplating doing the same thing, but didn't follow through because of the impossibility to share code between identical instantiations. I could see this being useful in more than one situation. For example, if one doesn't want to deal with the complicated types of the standard collections, one can write: @template def sequence(xs: Any) = if (xs exists(_ isEmpty)) None else Some(xs map (_ get)) No need for `CanBuildFrom` magic and crazy genericity tricks :\^)
How would any of these fair to a general beginner? Understanding basic concepts of programming with JavaScript, but havent really had the full experience or exposure of it (mostly been doing HTML, CSS and some JS)
[removed]
Have you checked the [Cookbook](http://finagle.github.io/finch/cookbook.html) and [Best Practices](http://finagle.github.io/finch/cookbook.html)? Lots of good tips there. If you're looking for something like an end-to-end app, see [Finch's own examples](https://github.com/finagle/finch/tree/master/examples/src/main/scala/io/finch).
Yes. Note that you can restrict `xs` to `Seq[_]` instead of `Any` if you don't want accidentally pass an `Option[_]` in.
These are a couple of patterns I have seen with implicit conversions - @ implicit def IntToFunction(n: Int): Int =&gt; String = { m: Int =&gt; (m + n).toString } defined function IntToFunction @ 5(3) res1: String = "8" @ implicit def OptionToString(o: Option[String]): String = o.getOrElse("") defined function OptionToString @ implicit def StringToOption(str: String): Option[String] = if (str != "") Some(str) else None @ val test: Option[String] = Some("my string").toUpperCase test: Option[String] = Some("MY STRING") Even just writing this small example I had this issue - @ implicit def StringToOption(str: String): Option[String] = if (str.nonEmpty) Some(str) else None cmd3.sc:2: type mismatch; found : str.type (with underlying type String) required: ?{def nonEmpty: ?} Note that implicit conversions are not applicable because they are ambiguous: 
https://github.com/scala-native/scala-native
You kind of do when you can't use patterns and constructs you are used to. This can lead people who are used to Java but have to work with Scala to be frustrated. I'm not denying that somebody can write Java-like code, but I don't like it when people gloss over the implications of it being a whole separate language. 
&gt; Both Task (the common implementations) and Future are not referentially transparent when it comes to dealing with exceptions That's not true in my opinion. You can only talk of referential transparency when you talk of [mathematical functions](https://goo.gl/7Xwt4U). Functions throwing exceptions and blowing up the stack are not mathematical - or lets say that they are at best *partial functions* that throw exceptions on invalid input. Such functions are not even [structured routines](https://en.wikipedia.org/wiki/Structured_programming), because throwing an exception is basically creating an alternative exit point. But to talk in terms of laws, what doesn't work is the monadic "*left identity*": pure(a).flatMap(f) == f(a) This can't possibly work in the presence of exceptions, because due to the evaluation model, if an exception gets thrown on some thread somewhere, it can't be seen from the current thread. And to make matters worse, if you don't deal with it in the `Future` or `Task` implementation, then it will be silent, maybe with an error logged on stderr by the thread-pool implementation, as a best case scenario on which you can't rely. From an engineering perspective, by saying that this law should work in the presence of exceptions is to say that *all evaluation* should happen immediately on the *current thread*. That's not an acceptable restriction. Therefore invoking this as proof that `Task` and `Future` aren't referentially transparent doesn't cut it. Because in the presence of exceptions being thrown, all bets are off because those are no longer functions and heck, exception instances themselves aren't values.
Yes.
Yes, a lot actually :-)
How you maintain your microservices, services discovery, kubernetes, ect? How your microservices communicate? Which REST API are you using play2? Thanks
Pretty cool. Would it be possible to extend this to support decltype and constexpr similar constructs?
Lots of people are. Here are some approaches you can take: - Use Lagom, an opinionated framework from Lightbend for microservices - Use Play -- you can use the same tool as for other kinds of webapps - Use Unfiltered, akka-http, finagle, finch, or http4s, all of which are libraries you can just call from your main method to listen as a server Anyway it would be good if you asked a more specific question. ;) 
&gt; You kind of do when you can't use patterns and constructs you are used to. What patterns/constructs though? try-with-resources I'll grant you, maybe these days there are Java developers that rely on having that available. But other than that what's missing from Scala that one uses in a typical Java codebase? Checked exceptions are gone but they were banned in many codebases anyway. `break`/`continue` are missing but how often do you actually use them? Java-style use-site variance is available if you want it. All libraries still work the same way.
If you're building internal microservices, try using a serialization protocol + RPC framework like Thrift or gRPC. For Thrift, [Scrooge](https://github.com/twitter/scrooge) generates Finagle server stubs and client bindings, while [ScalaPB](https://scalapb.github.io/grpc.html) generates gRPC service and client bindings.
I've written some microservices using spray where HTTP/JSON was specified and thrift (via scrooge) where it wasn't. Various approaches to service discovery - zookeper, multicast - Scala is pretty agnostic and integrates with most standards. I've never seen a need for containers - the maven shade plugin lets you produce a single executable jar which is most of the advantage a container gives you. I've used a variety of tools for deployment - rundeck, fabric, vagrant - it doesn't really matter which because all the tool has to do is install a JVM, download the jar from nexus, and run `java -jar ...` Scala for microservices works, and it's probably better than any other language, but I find using network interfaces means giving up a lot of Scala's advantages. If you instead use maven modules, you can enforce most of the strict separation advantages you get from microservices, but use much more richly typed interfaces that play to Scala's strengths.
See https://www.reddit.com/r/scala/comments/5szdwz/futuretransform_in_scala_212/ddl6ddo/ and https://www.reddit.com/r/scala/comments/5szdwz/futuretransform_in_scala_212/ddoq0a5/ There are some subtleties which not even `Task` can handle in regards to treating exceptions
You can use any service discovery mechanism you like, it's pretty language-agnostic AFAICT. http://microservices.io/ has a lot of information about microservices (including scala code)
Seconding this and adding some info about my personal experiences with each of these approaches: Bintray is very easy to use and has nice integration with sbt via sbt-bintray (https://github.com/sbt/sbt-bintray). It is especially nice if at some point in the future you want to open source / promote a once internal artifact to JCenter or Maven Central. If you go the Amazon S3 route, I've used frugalmechanic's sbt plugin to do the heavy lifting for a number of small projects (https://github.com/frugalmechanic/fm-sbt-s3-resolver). Overall, the Bintray experience is a bit smoother and more similar to a traditional Nexus integration overall, but alternatives exist if you just would like to use an S3 bucket as your artifact store, for instance. 
Would you recommended to use Lagom?
Which service discovery are you using?
It does. [The difference is only in the front end.](https://github.com/lampepfl/dotty/pull/2060#issuecomment-284717821)
what's the different from play-ws?
Well phooey. Thanks for the answer tho!
If this is for a commercial project you sound like you're out of your depth. Each point you've written is basically a books worth of material. If this is just for personal learning then I'd recommend googling these terms and following the top links and doing some research.
Thanks, i want to develop for react but plain js makes me sick!
Shameless self promotion: you could have a look at [scala-expect](https://github.com/Lasering/scala-expect) which is a Scala implementation of a very small subset of the widely known TCL expect. It is somewhat documented, could have some more scaladocs and a much better readme.md but I think it should be understandable. As far as being beyond entry level Scala, it uses variances (have a look at the [Action class](https://github.com/Lasering/scala-expect/blob/master/src/main/scala/work/martins/simon/expect/core/actions/Action.scala)), it uses higher kinded types (again look at the Action class or the [When class](https://github.com/Lasering/scala-expect/blob/master/src/main/scala/work/martins/simon/expect/core/Whens.scala)). It has a grade of A in [Codacy](https://www.codacy.com/app/IST-DSI/scala-expect/dashboard), 100% code coverage and a compliance of 7/10 in [Better Code Hub](https://bettercodehub.com/results/Lasering/scala-expect).
This looks so much better than 0.x; I shared many of the same concerns that the [React4s author expressed previously](https://www.reddit.com/r/scala/comments/5u9tf0/react4s_straightforward_component_based_webapps/ddsl7eq/). So *thank you* for this. I'm especially glad that you opted for more verbose names this time. Also sort of regarding readability, I avoid using scalaz, preferring [cats](https://github.com/typelevel/cats) instead. It seems to me that others might share my preference, as well, looking at examples like [doobie](https://github.com/tpolecat/doobie) supporting both, and [http4s adding cats support](https://github.com/http4s/http4s/projects/3). Is there any plan so far to add a cats module? 
Sorry I don't know anything about Expect or TCL, can you explain why this returns a Future containing 6? import work.martins.simon.fluent._ import scala.concurrent.ExecutionContext.Implicits.global val e = new Expect("bc -i", defaultValue = 5) { expect .when("For details type `warranty'.") .sendln("1 + 2") expect .when("""\n(\d+)\n""".r) .sendln { m =&gt; val previousAnswer = m.group(1) println(s"Got $previousAnswer") s"$previousAnswer + 3" } //This is a shortcut. It works just like the previous expect block. expect("""\n(\d+)\n""".r) .returning(_.group(1).toInt) } e.run() //Returns 6 inside a Future[Int] Not sure where the 6 is coming from.. 
Hi! You can get a guidance to learn Scala here: https://www.alejandrolujan.com/blog/scalabooks
Sounds quite complex. I don't really understand what the change is compared to the other proposal and what the rationale for the difference is - I can see the examples for what desugars for what but I don't have a sense of what the overall principle is. I'm also a bit concerned that the type parameters are a bit magic. Concretely, could I implement e.g. [paperdoll Queue](https://github.com/m50d/paperdoll/blob/master/queue/src/main/scala/paperdoll/queue/Queue.scala) as an enum, given how the type parameters of the three cases (`Empty`, `One` and `Node`) don't always align with those of the common `Queue` interface? It sounds like this works as long as the names line up correctly, but e.g. how does that work for `Empty[..., A]` which is a `Queue[..., A, A]`?
doobie's code isn't all that fun to read, and it might be a bit much if you're not used to monadic programming. But I'm happy to answer questions about it. I recommend everyone read [spire](https://github.com/non/spire) and [shapeless](https://github.com/milessabin/shapeless).
So, what exactly is Dotty? New compiler? Or something like Scala 3.0?
It's a new compiler for what will be the next version of Scala, and I'm guessing it will be called Scala 3 when it's ready. 
Agreed. I've defaulted to Typescript because the various scala bindings to react have proven to be unstable/fragile so far. I'd love to see a compelling scala-first framework. I've seen a few ideas but they all seem so incomplete. 
I'm not doing microservices currently
&gt; Is the type B = A style something supported by Dotty in general? &gt; can I write e.g. &gt; class MyClass extends TraversableLike { type X = ... }? AFAIK that's how it's represented internally, but it does not work in the frontend. I tried this: class X[A] class Y extends X { type A = Int } And for `Y` it produces after typing: class Y() extends X[Nothing^]() { type A = Int }
Thanks. By the way, is there any estimates anywhere, when will it be ready?
Those snippets are precisely examples of bad implicit conversions, which should be disallowed. You're making a point in favor of the proposal, here ;)
As a language, of course. In terms of *everything else that matters*, it's not even close. 
Yes, this is what Lagom is all about: https://www.lightbend.com/platform/development/lagom-framework
I don't think there's any kind of hard estimate but it is supposed to be pretty far off. Mind you you the next major release of Scala 2 is scheduled for [1 year from now](http://www.scala-lang.org/news/roadmap-2.13.html), so it certainly won't be before that. 
What are people experiences with this?
&gt; There were too many symbols, inconsistencies in the API, single-letter suffixes all over the place, [...] &gt; &gt; All (ok, 98%) of this has been removed and replaced with a clearer API. Thanks for this change. It was my biggest issue with the library. 
Out of curiosity, has anybody tried swapping in InfernoJS with scalajs-react? 
I seem to be doing a bad job at communicating here. The examples that I gave we're meant to be examples of the misuse of implicit conversions. In a particular codebase I was working on, I removed several unconstrained implicit conversions since they were getting in the way and also making the code harder to read in places. I'm in favour of this proposal.
From what I hear, Erik Osheim's code is elegant and performant. Check out jawn, cats and spire.
Doesnt scalajs compiler remove unused/uncalled methods from output ?
I've used Vert.x since 2015. It's very good but with some quirks that they are now addressing (JSON to Object binding for example). In general I like the framework but if you're used to writing code in a synchronous manner there is a learning curve. It took me a bit to adapt to the Vert.x model. 
I am not sure. This vs Akka?
How about adding it to https://en.wikipedia.org/wiki/Expect#Alternatives
Spire has interesting design constraints. - We have to balance performance and purity. - We deal with approximate types (primitive integers, floating-point) and strive to prove correctness. - We use the type system to encode mathematical properties, but e.g. division by zero breaks totality. - We have mostly idiomatic, typelevel-style interfaces with mutable state, arrays and for loops under the hood. - We harness mathematical structures for soundness (ex: the `Ring` hierarchy), but (try to) avoid jargon. - Oh, and we bang our head against specialization limitations all the time.
It is worth to learn scala for getting job in the future.
Es, but don't just learn Scala. Learn other functional, OOP, and imperative languages so you can compare and contrast and have a well-rounded view of programming.
But good language should always make it much harder to shoot yourself in the foot.
Any books how to start lagom?
I really like its http module. It's very non-magical and feels as straight forward as Sinatra in ruby or express in node js. Except you get to use it with typed languages. 
If you want to work in an actor style, then akka really is better, but if you want to work on an http service with a Sinatra or node js/express style, then vertx is nicer to work with than spray with futures IMO. 
[removed]
&gt; That's not true in my opinion. You can only talk of referential transparency when you talk of mathematical functions. &gt; Functions throwing exceptions and blowing up the stack are not mathematical - or lets say that they are at best partial functions that throw exceptions on invalid input. Such functions are not even structured routines, because throwing an exception is basically creating an alternative exit point. Thats what I precisely meant, because we have exceptions in Scala (that are thrown quite commonly, especially in the type of code that `Task` and `Future` wrap around) it doesn't play well with maintaining RT. My main point is that arguing `Task` provides "better" RT is a bit of a misnomer
I am running two akka-http based services and the only thing that links them to actors are that you have to spin up an actor system.
Rust is a nice and novel language, but the programming style is quite different from Scala, since you have to be careful about ownerships and borrowing. The Rust compiler checks these things for you, but when transitioning from Scala you may feel quite restricted in terms of the kind of programs and functional patterns that you can write. This is not to say that Rust is bad - it's a good language - it's just about a different set of tradeoffs.
That's a very good answer. I was just going to say scp. Good luck op. May your back ends deploy smoothly.
Then pick up a copy of Chiusano &amp; Bjarnason's _Functional Programming in Scala;_ it will teach what you need to know. Note that you will need a supplementary reference to look up Scala syntax; FPiS doesn't really teach a lot of syntax.
A good language should make it hard for users to misuse a language feature, but still allow experts to leverage it when they know what they're doing (see the use cases in Scala.js in the github thread). I actually think the current approach of hiding implicit conversions behind a flag is a good compromise: you can easily monitor their usage (or ban them) in your team, but they're there in case you need them.
Hi, You could take a look to this blog post, has a good survey of Scala books: https://www.alejandrolujan.com/blog/scalabooks
Hi Rafael, take a look to this blog post has a good survey of Scala books: https://www.alejandrolujan.com/blog/scalabooks Will help you to start on the right direction
Yes, there's a series of mini books out by O'Reilly: http://www.oreilly.com/programming/free/reactive-microservices-architecture-orm.csp for example.
Hi, You can find more information about Scala libraries on this blog post has a good survey of Scala books: https://www.alejandrolujan.com/blog/scalabooks
but I can not see any code!!
Not in that book. There's a whole series of them. If you want to see code, look at the examples: http://www.lagomframework.com/documentation/1.3.x/scala/LagomExamples.html
[removed]
You probably want https://github.com/scala-android/sbt-android 
should have tagged it NSFCs. NOT SORRY, enjoy the redpill.
Yes, sbt is recommended for people starting out with Scala, it's the tool used in almost every Scala project, so it's good to get familiar with it. You can find sbt plugin that will help with Android development
&gt; Is Sbt recommended as a default build tool for Scala projects, and in particular Scala for Android SBT at the moment is pretty much the defacto build tool for Scala. Here's a pretty helpful quickstart on Scala and [android](http://scala-android.org/) with SBT. &gt; What are the downsides of Gradle and Maven? &gt; What are the downsides of Sbt? Why the emphasis on the negatives? I'm tempted to interpret this as an intentional attempt to start a flame war.
Tfw we use maven w Scala 
SBT is way better. Add the [Coursier](https://github.com/alexarchambault/coursier) plugin for faster library downloading.
Downside of Gradle: it's a fucking mess. Its main configuration file is a mutable DSL, which pretty much guarantees nasty surprises. Some rather important settings (notably the project name) have to be configured in a separate properties file, because reasons. Avoid this monstrosity. Downsides of Maven: * The XML schema is too rigid. Any setting that isn't defined by the schema has to go inside a plugin configuration section, even if multiple plugins need it. * The XML schema is too verbose. Dependency group/artifact/version identifiers are written as elements, not attributes. * The plugin API has that awful dependency injection thing. Dunno about SBT. I'm sure there's plenty wrong with it, but I haven't looked at it in some time, so I couldn't tell you.
Hmmm. Methinks you don't fully grock gradle. The project name does not have to be configured in a separate file, why do you believe this? I don't really understand your "mutable DSL" assertion either. You change the SBT configuration by mutating one or more configuration files as well, no? When all things are equal, I reach for gradle first.
I mean, I like the principle of "who cares what someone believes" when it comes to who can and cannot contribute to a project, but there *must* always be the caveat of "as long as they are respectful to everyone else". The funny thing here is, even if this FAQ was created with good intentions, it goes too far, making the whole thing a petty, rambling rant trying to justify itself. Whether the entire thing is true or false is now irrelevant. The FAQ has become the very thing it spends so much time denouncing.
&gt; The project name does not have to be configured in a separate file, why do you believe this? Because it's a read-only property of the `Project` object. Googling revealed that the only way to set it is in a separate file. Maybe this was remedied at some point, but I wouldn't know. When I last went back to the Gradle web site to find out, I was greeted with a deceptive advertisement. They've gone shady. Not touching Gradle with a ten-foot pole now. &gt; I don't really understand your "mutable DSL" assertion either. Then you aren't qualified to participate in this discussion.
Blank lines no longer matter in build files. The static checks that Scala and SBT perform does take time to run and a dynamic language would be able to bypass that. Insert standard pro/con discussion of upfront static analysis vs deferred dynamic analysis. I've never had a problem with the build time personally, though I don't know if that's just a difference in expectations for compiled languages or an issue with build configuration.
A little bit of context, from what I could scrap together: https://www.reddit.com/r/scala/comments/4x4wa9/is_it_a_shame_to_use_scalaz/ https://news.ycombinator.com/item?id=12365490 https://groups.google.com/forum/#!topic/scalaz/9X_putSGoCY https://github.com/typelevel/cats/commit/9d1837e20a89c99c174723a2f112d84507b9ebdb http://archive.is/4SuE0 https://github.com/tonymorris/course My own impression is that Tony Morris is a(n) (extremely?) gifted developer who genuinely cares a lot, especially about his students and their learning, but is not always that strategic or cunning in regards to certain topics as far as I can tell, and has in the past been considerably less than polite in his messages about what he considered as "bullshit" or unfair or insincere behaviour and actions. Later, events happened which included what I believe was the "capture" of github.com/scalaz, of which Tony Morris might be the (co-)founder?, and removed him from the project?. He was also banned elsewhere (mailing lists?), and a number of other people seemed to actively go after him, such as in this notable comment from the Hacker News thread I link: "Calling Tony Morris a rapist was probably the highlight of the affair: https://twitter.com/shajra/status/544712807220908032". He seems to have moved on since then to Haskell. I don't know why he seemingly have been targeted; but he has seemingly been at least very impolite, possibly unstrategic in regards to topics such as infiltration and subversion, bold, and seemingly a very strong proponent of free speech, and also someone who seemingly genuinely cared and contributed a very large amount in several ways (http://blog.tmorris.net/posts/scalaz/index.html and https://github.com/scalaz/scalaz/graphs/contributors). In short, relatively easy target with large consequences and effects (such as being able to capture a project?). I find it a fair bit disconcerting to look into, to be honest. It reminds me of a number of other cases in software development, such as with the forced(?) departure of Brendan Eich from Mozilla (https://en.wikipedia.org/wiki/Brendan_Eich#Mozilla). I somewhat get the impression that the abuse against Tony Morris may be almost loosely organized. I find this a fair bit bitter - reminds me of a number of people that argued for free speech who were attacked in various ways, such as [Finn Noergaard who was murdered when participating in a meeting regarding free speech](https://en.wikipedia.org/wiki/Finn_N%C3%B8rgaard#Death). With [600-700 people later participating in favor of the murderer at the murderer's funeral](https://en.wikipedia.org/wiki/2015_Copenhagen_shootings#Aftermath). There does not seem to be a general consensus about either the events regarding Tony Morris and Scalaz, nor the evaluation of those events. To get more context, it is probably a very good idea to investigate yourself - data and sources can be picked and chosen from, and it is difficult to determine who or what can be trusted or not, and in which ways.
See this comment: https://www.reddit.com/r/scala/comments/5yhazc/scalaz_conduct_faq/deqj8os/.
I believe Linus would be sympathetic to that opinion, and the development process in Linux seems extremely effective. Which is very, very good given the importance of the Linux kernel.
I have written some context in this comment: https://www.reddit.com/r/scala/comments/5yhazc/scalaz_conduct_faq/deqj8os/. I believe this link is relevant: http://archive.is/4SuE0.
I presume any JVM framework would be fine. Have not used it specifically, but http://wix.github.io/petri/ comes to mind.
looks like i am io bound because there are 30 million rows in the cassandra table and the for loop executes for 90+ minutes. there is no processing but just to check one of the fields in the record extracted. So definitely IO bound. so how do I reduce the 90 minute iteration on the 30 million records.
Yes I am familiar with this whole saga, but I am not sure what prompted Tony to write what he did now. The whole unpleasant business should have blown over but clearly feelings are still raw. Which is the point. Whilst he states that insults etc. are acceptable and should be a part of a project, Tony is vey thin-skinned and can't take what he dishes out. Which is perhaps why he should consider his position. None of this is helpful.
Where can I find scala doc for vertx?
&gt; There does not seem to be a general consensus about either the events regarding Tony Morris and Scalaz, nor the evaluation of those events. As much as I try to avoid getting involved in these kind of discussions, I need to add something in here. There is a consensus. This Scalaz debacle is the result of 2 different views of building a community. On one side you have a group of people that, as this 'code of conduct' shows, accept insults, conflict, and aggressive behaviour toward newcomers as natural and acceptable. Under the moniker 'free speech' they accept white supremacists, as the lambdaconf controversy showed (see [here](https://modelviewculture.com/news/lambda-conf-fuckery-white-supremacy-under-the-guise-of-inclusion), and [here](http://degoes.net/articles/lambdaconf-conclusion) for 2 views on this). And don't get it wrong, this LambdaConf controversy is just another side of this issue, at its core. On the other side you have people who reject toxic behaviour in a community to the point of making it an exclusion trigger (if you are toxic you are not welcome in here), and don't want to separate your behaviour as white nationalist from your contributions to the community (rather have less quality than a bigot). Those are really the 2 sides of the issue. Either you don't mind who's in the community and how do they behave even if that excludes/alienates people, or you care about the community as a whole and protect it to keep it like that. I know which community I want to be part of. I am not sure many people enjoyed the drama days of Debian and their flame wars; I am not sure newcomers like being told off in quite nasty ways when they ask for knowledge; I am not sure minorities in the IT community feel welcome when sharing a conference space with a well know racist and misogynist. I know that I've worked with people who were great geniuses on the art of coding but given the conflict they brought to the environment (and how unpleasant they made it) I rather work elsewhere in a worse job than try again. There was, recently, this article in [hacker news](https://news.ycombinator.com/item?id=13655554). "It is entirely possible to be extremely passionate (and even brilliant) without being a jerk". It is possible. I know plenty of people like that, some of them involved in this Scalaz controversy. The difference between them and what Tony was doing at that time is that they empower newcomers. The community grows, the language gets wider adoption, the open source projects get more contributors. Your time invested in this community pays back as it thrives and expands, there are more jobs that demand your skills as it spreads across the tech world. Everybody wins. But if you create a toxic environment, only toxic people or people who don't care much will stay around. If the community grows, all the expansion will be permeated by that toxicity, even your workplaces. If you think this is madness, maybe you should read again about Uber. Toxicity brews toxicity, bad environments grow and fester. Also, and probably shouldn't add this, I find something very funny about this FAQ and Tony's attitude in general. If he's really as smart and he declares and Scala is so useless as he estates, why did it take him so long to realise? Like, how many years? Under that prism, and knowing Tony IS smart (he's proven that), the only conclusion I can make is that he's throwing a tantrum like a child with this kind of posts. And, yeah, not interested. 
Are there any cases where implicit conversions can't be replaced by explicit ones?
Sbt: de-facto standard, useful plugins, most powerful, but very complex and poorly documented Gradle: modern alternative to Maven in JVM world, easier to understand but you'll lose lots of useful sbt plugins since Scala devs tend to write plugins for sbt (always) and Maven (for popular projects). Maven: battle-proved, understood by everyone, but verbose and does not support automatic recompilation/restart on source change which is very useful. 
Language feature or application feature? Language features I prefer to use the same language for my whole application. Application features I prefer to control in code. Why is it you find yourself wanting a feature flag/toggle? If it's about being able to make changes quickly, that might be better addressed by having a quicker release/deploy process.
SBT is "recommended" but that seems to be purely a matter of fashion (but that does mean documentation for some projects assumes you're using it, unfortunately). I find it horrendously complicated for very little benefit. SBT makes sense if you're writing a library that you need to cross-build for multiple Scala versions, or maybe if you write code in a text editor and rely on the build tool for incremental compilation (I use an IDE so I can't speak to that use case). But for an ordinary application maven is more consistent, better documented (and much easier to *search* for documentation since it uses words rather than symbol salad), and better integrated with IDEs. It's very much "there is one way to do it", whereas SBT and Gradle allow random code in your build definitions. Gradle is mostly the worst of both worlds (it's inconsistent and allows arbitrary code in the build, and it's not particularly popular for Scala either, nor does it have good support for closs-building), and performs poorly to boot (build definitions are an undocumented scripting language that has to be parsed and run every time you do anything), but it does at least have slightly more readable and less symbol-heavy build definitions than SBT.
I trust you've run a profiler before trying to get clever about performance? If not, do that first. You probably don't need to parallelise (and if you do it's probably better to do it with multiple queries to Cassandra in parallel rather than trying to share a single Iterator) - more likely you need to figure out what you're doing that's taking so long. But yes, FS2 is the best way to do this kind of thing, and can let you do things like batching up entries coming out of an iterator and processing batches in parallel. But it will be a big step up and a lot of complexity, especially if you're not familiar with the functional style already. Don't try it until you've exhausted the simpler options.
Disclaimer: I'm a maintainer of Spire, based on algebra, based on cats-kernel (so scalaz "competitor"). Many people here observed that this "conduct FAQ" is sloppily written. It does not reflect the Scalaz organisation as a whole (I hope), nor (to my knowledge) has it been approved by a majority of Scalaz maintainers. This document is clearly a mistake that will hurt the Scalaz project. In the last year, Tony is not even in the top 5 of contributors according to Github. For better or worse, scalaz is part of our common ecosystem. Let's find a way to cohabitate if we cannot collaborate. Advertising this rambling FAQ (see! how scalaz is toxic! I told you so!) is just rubbing salt into old wounds.
I don't disagree with you. However, I don't like the blanket labelling of people as "toxic", like there are "good people" and "toxic people" (of course, I am on the right side, see how welcoming it feels here). This disregards two underlying issues: some toxic behavior can come simply from having a different cultural background (do you reject a bigot when he was raised in the Westboro Baptist Church, and needs time to make his own mind, for example?). Some unacceptable behavior also comes from genuine mental disabilities (interestingly, those would be protected under most code of conducts). 
I have a blocking code( eg: file uploads) which has been wrapped inside a Try. I would like to make it as async computation by using Future. I have a method in Future library (Future.fromTry) but would that be wise to use it? 
&gt; It does not reflect the Scalaz organisation as a whole It's in the Scalaz Github organisation. The evident implication is that it applies to the Scalaz organisation as a whole. What other conclusion could a reasonable person draw? &gt; Advertising this rambling FAQ (see! how scalaz is toxic! I told you so!) is just rubbing salt into old wounds. If Scalaz want to avoid a reputation of being toxic, perhaps they should not put up FAQs like that. It's not been taken down. The evident implication is that it is not so objectionable to the rest of the Scalaz project that it needs action. What other conclusion could a reasonable person draw?
Annoying thing is the play ws requires an actor system.
How did a "rambling vandal" get hold of permissions to post a FAQ in the scalaz organisation? And why have the "majority of maintainers" not immediately taken down this "defaced" page that they have not approved? You're being disingenuous. Our beloved "rambling vandal" can do this because his behaviour is not seen as a sufficient problem by the other maintainers, and he has the power to do so. Therefore, the Scalaz project as a whole is culpable. &gt; Let's not advertise this drama, and discuss this further in PM if you want. I'll advertise it all I like. It's important that people understand the nature of the Scalaz project, as advertised by the project themselves.
http://vertx.io/docs/#core
Application features can be nicely controlled by configuration and Guice injection. In a nutshell: have two or more implementations of some trait/interface which injections are controlled by configuration.
I wouldn't necessarily trust ThoughtWorks' judgement. IMO if you put feature flags in you're basically just emulating feature branches "in userspace". Occasionally that's useful for being able to e.g. get early warning that two features conflict, but you can achieve that with branches and builds as well. To my mind you're better off letting the VCS do what it's good at - bisect becomes a lot harder when your system is full of feature flags. &gt; Another use case for feature toggles is A/B tests or things that kind of need to be tested in production. For this kind of use case I don't think they merit any kind of special treatment. They're just another input into your code, handle them in normal business logic.