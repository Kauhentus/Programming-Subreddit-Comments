That should be no problem, you can preserve the state across MEX invocations, see the `Gaumixmod` class example over here: http://www.nr.com/nr3_matlab.html#wft // just in case, here's the header file: http://www.nr.com/nr3matlab.h However, make sure to switch to C++11's random number generators if available (if not, it's worth updating compiler just for that), the old ones are of a *very* poor quality (and the implementation-specific "guarantees" are non-portable). See: http://isocpp.org/blog/2013/03/n3551-random-number-generation http://en.cppreference.com/w/cpp/numeric/random // note that these are ordinary classes, so I gather you can preserve the state similarly to the example class above;
Thank you. I like how clever and simple this is.
I didn't know that. This is my first time working with MEX invocations. All of these resources look very helpful. Thank you.
Instead of the loop iteration count, you can also use a random number for the seed.
&gt;However, we already get this for free—thanks to structured lifetimes, the called function’s lifetime is a strict subset of the calling function’s call expression. Even if we passed the shared_ptr by reference, our function would as good as hold a strong refcount because the caller already has one—he passed us the shared_ptr in the first place, and won’t release it until we return. This isn't technically true due to aliasing and can burn you in subtle ways. For example if you have a vector v of shared_ptrs, and you pass an element p of the vector into a function f, it's possible that f will in turn call a function that erases p from v. This might happen indirectly and in such a way that it's not even obvious or general. In that case p will no longer be valid and you'll likely have your program crash or even worse. This code is a very simple demonstration of how this can happen. class C { std::vector&lt;std::shared_ptr&lt;int&gt;&gt; v; void f(const std::shared_ptr&lt;int&gt;&amp; p) { auto i = std::find(v.begin(), v.end(), p); v.erase(i); ++*p; } }; ... C c; c.v.push_back(std::make_shared&lt;int&gt;(5)); c.f(c.v.front());
One should note, however, that this is a general problem of aliasing, not a `shared_ptr` specific one.
Randomness is a bitch, and trying to explain it is even worse. Reseeding breaks the randomness chain. Seeing as the generator most commonly used in the standard C library rand() implementation (a linear congruential generator) is flawed to begin with, this does not matter much though. Seeding with the loop iterator would give you the same random numbers for every run of the program. Not very good if you need good randomness. To improve it, you could for instance add some milli- or microsecond timestamp to the iterator. OR. Couldn't you just let MATLAB generate random numbers you could pass?
There already is `std::common_type`: http://en.cppreference.com/w/cpp/types/common_type &gt; For arithmetic types, the common type may also be viewed as the type of the (possibly mixed-mode) arithmetic expression such as `T0() + T1() + ... + Tn()`. 
Although the article and code generator provide C++11 solutions, they do not mention the most general C++11 tie idiom for implementing a strict weak ordering for custom types, e.g. (if you want to apply an inline lambda): std::sort(std::begin(v), std::end(v), [](const point&amp; lhs, const point&amp; rhs) { return std::tie(lhs.x, lhs.y, lhs.z) &lt; std::tie(rhs.x, rhs.y, rhs.z); }); This is by no means a new trick, so what does the article and its code generator provide what std::tie doesn't already cover (except that you have to use C++11 for std::tie)?
Exactly. I had a whole 1000-word section on pass-smart-ptr-by-const&amp; aliasing and the dual problems (modifying the parameter, or modifying the thing it aliases) with examples, and ripped it out because it's essentially the same as aliasing any other object.
Right, I was specifically addressing your addition-result-type question (as in "Should `AdditionReturnValueType` for `Add(char, short)` be `char`, `short`, or `int`?") and the lack of need for `AdditionReturnValueType` in this context, not commenting on the concept-like return vs. inferred result-type.
I'd strongly recommend getting the slides over watching the video. The video is filled with lag time, then often the presenter skips right past the content or just reads the slides. When he says more it's quite poorly explained.
Is it actually about VC++, or just C++ in general?
It's C++ in general, but there are some notes on VC-specific compiler flags and tips on intellisense and Visual Studio UI.
Well the first problem is that, that's not a T-Rex riding the shark...
It also uses silverlight, which is unavailable on Linux computers (to the extent of my knowledge). 
In this release, I think what looks the most interesting in future.then and the ability for boost asio to return a future
Hello, This is a very interesting and technically correct remark. From a theoretical and technical point of view, the tie() template function already does most of the job that my wizard does, as long as you can afford being depended to C++11 or Boost. Hence, in that point of view, my wizard can be considered more or less obsolete. (sadly) However, from a more practical point of view, we have to consider the fact that countless C++ developers still cannot use C++11 conforming compilers. And it is logical to assume that financial implications, licensing implications (e.g. new gcc compilers moved to GPL3) and other factors will prevent many C++ developers from using such advanced compilers for many more years. Furthermore, I think that for several years most library or framework developers will be very reluctant to break the compatibility of their code with all the compilers which does not conform with C++11. Consequently, I believe that my wizard can still help many C++ developers for at least a few more years. (hopefuly) Having said the above however, I certainly recognize that, as the time goes by the adoption of the C++11 standard will become wider and wider and will make the usefulness of this wizard increasingly smaller. Obviously, this doesn't make me feel very happy about the research and planning I have done before implementing the wizard. Needless to say, that I will try to mention the tie() solution in the article during the next few days. Best Regards, Jim Xochellis HomePage: http://sites.google.com/site/xochellis/ 
Yes, I guess what I'm trying to point out is that this statement is false: &gt;thanks to structured lifetimes, the called function’s lifetime is a strict subset of the calling function’s call expression. C++ does not have structured lifetimes due to aliasing. It doesn't apply just to shared_ptrs but you have to remember a lot of guys using C++ don't know all of the quirks and subtleties in which things can fail, so they may read a general statement about structured lifetimes and then get the wrong impression. I also don't think this detail is nitpicking either, after all the whole point of having a shared_ptr is to deal with issues involving lifetimes.
That was actually *really* helpful for me. The recommendations on not passing smart pointers by reference (unless it's the pointer itself which is of intereset) really struck home for me. I've been using this anti-pattern for some time now. Especially with *std::unique_ptr&lt;T&gt; const&amp;* It always felt kind of strange, but I could never put my finger on why. When you think about it, it's painfully obvious of course (as so often). Apparently it *does* take a Herb Sutter to point this out to me, though. Good article.
In the past, I've had intermittent success with [moonlight](http://www.go-mono.com/moonlight/).
`Platypus agentp = std::make_shared&lt;Platypus&gt;("Perry");` How does that even work? (it doesn't for me)
http://imgur.com/fKRFLkl THIS. Pet peeve of mine. 
I think that move semantics are AWESOME, but it just seems like C++11 has created a paradigm of "good coding practices" that are HORRIBLE in C++98. Fortunately, some of them (like the "return stream" example shown here) won't compile in C++98, but other will just silently give horrible performance and that seems a little scary to me.
So maybe you shouldn't use 15 year old standards with modern practices, seems like common sense to me. 
i like to think of move as resource passing. instead of create, copy and destroy like in c++03 things are just passed around and handed off with c++-11. As with anything new it takes some time to figure out how to appropriately use the features as opposed to abusing them. Last week I went back in and ripped out an api I had made middle of last year that used rvalue refs and switched them back to pass by reference.
I think many people here may find the Chinese documentation a little challenging.
It would be: std::shared_ptr&lt;Platypus&gt; agentp = std::make_shared&lt;Platypus&gt;("Perry"); But I wouldn't repeat the definition, so: auto agentp = std::make_shared&lt;Platypus&gt;("Perry");
It is just the way it is it seems, c++11 will have to be a very explicit choice 
&gt; maybe you shouldn't use 15 year old standards with modern practices Some of us work on much older code base and have to support compilers that don't even implement C++ 98 properly.
I wrote a rant about this issue when VS2013 was announced here: http://www.reddit.com/r/programming/comments/1fl22c/visual_studio_2013_brian_harrys_blog/cabbtej This is honestly very very poor on the part of Microsoft. To have the once largest software company in the world make such a big deal about how they would be at the forefront of native code and keep VS2012 up-to-date by releasing quarterly updates adding new C++11 features, only to fail so miserably and be unable to keep up with any of their competitors is shameful. The one company that made all this noise about C++11 ends up being the one company that can't even deliver some pretty basic stuff.
I think its safe to say that no non-chinese speaking person will ever use this. There are so many RPCs out there, most of them very well documented and with tons of clients for other languages - I dont think anybody will invest the hours into translating / understanding the code.
That, and when VS2012 came out how they were communicating that the C++11 support was *complete*, later adjusted to C++11 *standard library* support (which was also not true due to the lack of variadic template support). It was already behind the competitors when it came out, and stayed there. The IDE is a complete mess, very slow and features very unusual aesthetic choices (indistinguishable monochrome icons?). Not their best release, that's for sure.
VS seems to be doing everything they can to make the as yet unreleased JetBrains c++/c IDE dominant.
Looks really interested. Things I like about it: 1) C++11, Header only, works on Windows, and Boost license. Will definitely download and try it out
Sure: * MsgPack Rpc (https://github.com/msgpack/msgpack-rpc) * Apache Thrift (http://thrift.apache.org/) * Remote Call Framework (http://www.deltavsoft.com/) * Xml RPC (http://xmlrpc-c.sourceforge.net/) * PT (Another Xml Rpc with bundled with a framework) (http://www.pt-framework.org/htdocs/namespaces.html) * Simple Rpc (https://code.google.com/p/simple-rpc-cpp/) All the Protobuf C++ RPC Implementations: * http://zeroc.com/ice.html (Multiple languages) * http://protorpc.lekebilen.com/ (Qt/C++, Java, Python) * http://code.google.com/p/server1/ (C++) * http://deltavsoft.com/RcfUserGuide/Protobufs (C++) * http://code.google.com/p/casocklib/ (C++) * http://code.google.com/p/protobuf-remote/ (C++/C#) * https://bitbucket.org/chai2010/protorpc/ (Go/C++) Those are just a few of those that are available. In the broader sense of the word RPC you can also count Dbus and an rpc service though its a bit bloated IMHO. I remember seeing some live-rpc implementations bundled with a webinterface for real-time broker-based systems but I cant find the link. Not all of them are well documented but they all have english comments in their code.
Don't forget the current XBox debacle, oh and the NSA thing. Yeah.
I don't usually talk about what I do all day, but it's not secret - it's just that people are usually more interested in hearing about the results than the process. You are correct - VC's C++ Standard Library implementation has been licensed from Dinkumware since around 1996 (which was before my time; I joined VC in Jan 2007), as you can tell by looking at the bottom of any STL header. Dinkumware is responsible for writing all of the TR1/C++0x/C++11 features we've shipped over the years. On Microsoft's side, I review updates generated from Dinkumware's master sources before checking them into VC's source control (now Team Foundation Server). For example, I have to keep all of the Microsoft-specific stuff working, like support for /clr and custom calling conventions. I also analyze bug reports (all Microsoft Connect bug reports against the STL go through me) and route them to Dinkumware, myself, or occasionally the compiler team as appropriate. VC's C Standard Library implementation is different - it was written by Microsoft back in the 1980s.
To be honest... I remember Herb Sutter saying that by the time VS2012 is released that it will have a complete implementation of the standard library. That statement was made without qualification. All the official Microsoft marketing on their website also said, without qualification, that VS2012 would deliver a complete implementation of the standard library. It wasn't until I read your blog that I found the big disclaimer being that only parts of the standard library that don't depend unimplemented C++ features will be added and also the table of what those features were. If they had made that clear to everyone from the beginning that would be fair and honest. In other words, to a large group of customers who are not Microsoft insiders, the impression given to us by Microsoft was that a compiler was going to be released with a complete implementation of the standard library and updates would be made quarterly to add bug fixes and additional language features. This led many to believe that enough of the compiler would be ready to at least fully implement the standard library. It's only when you go over blog postings made by individual employees and read comments made on random MSDN blogs that you realize how superficial a lot of the official statements made were. I mean honestly, Herb Sutter went to the Build Conference, stood up to an audience and declared boldly that VS2012 was going to have variadic templates among other features added in soon to be released service pack, and that they were working on adding even more stuff down the road. And then when Update 2 was released, not a word was mentioned about C++. I downloaded it and was scratching my head wondering why none of the new functionality was delivered. I thought maybe I installed it incorrectly or something. Not everyone of us has the time or the investigative energy to dig through every MS employee's blogs or comments to find out what the real matter of fact story is.
Please tell me this exists.
So, we have to go through the whole 'no support XP' thing again?
https://www.jetbrains.com/objc/features/cpp.html
Thanks. I can't wait to play with it. I use AppCode as a C++ IDE on my Mac and -- while not being perfect -- it's really good.
I'm not going to say that you're wrong about what Herb might've said but the slides I have from Herb's "The Future of C++" talk only mention variadic templates wrt the Nov 2012 CTP, not in any production release. Other than that, I'm completely on board with your frustration.
XP support is **confirmed** for VS 2013. The comment permalink doesn't seem to work, but see [Brian Harry's post](http://blogs.msdn.com/b/bharry/archive/2013/06/03/visual-studio-2013.aspx) then search for this: &gt; Steve Teixeira, MSFT &gt; @Azarien: VS2013 will continue to support targeting down-level to Windows XP from C++.
Any idea what they are going to use for indexing the C++ code? Something clang based, maybe? Edit: Nevermind - I see they are mentioning Clang Analyzer.
This would be really awesome and pretty much required to allow us to upgrade (still most people will hate the look of the IDE). Also, on a somewhat related noted: Please don't take the criticism leveled at Microsoft personally. It's just that -- critique on a company level, not on individuals. I highly respect the work you do, especially the work you don't have to do like the videos you made for Channel 9.
I bought ten VS2012 licenses for my [team](http://www.re-lion.com) on the promise (of Herb Sutter, mostly), of a more feature complete C++11 compiler in the first half of 2013. I would have been happy with the CTP changes realized. Variadic templates and initializer lists are big features after all. Now I will have to buy updates all around once again? Just for some C++11 features? I don't think so. I've finally been pushed over the edge to another compiler. Which would open a door for us to develop on another (free!) platform. Which might lead to us not using MS software anymore. Which might lead to our clients not using MS software anymore. See where this is going? 
Bah.. C++ devs are irrelevant to MS anyway. Writing C++ code means people can at-least think of porting code to other platforms. They just want to drive people to Microsoft owned/controlled languages where they can keep you running on a constant update treadmill. VS2008 was the last IDE that I used from MS for personal projects and I really liked using it. Moving to GCC + GDB was definitely hard for me but thankfully I'm free from MS shackles now.
Stephan, I don't know how many people from Microsoft are aware of the frustration from C++ devs about only having one C++11 update for VS2012 or having to purchase another version of Visual Studio in order to get more updates. Could you please make sure that some product managers at MS see some of these threads at reddit? (And perhaps also from other discussion forums you may happen to visit.) I understand that some of the claims by people are incorrect, but I do believe the frustration is genuine. I really, really like Visual Studio as a development platform. But I too am personally frustrated by the lack of C++11 support in Visual Studio -- so much so that most of my new personal projects I have started using GCC with the netbeans IDE. Thanks for answering questions around here; for handling yourself in a professional manner when people misconstrue what you've said; for all the videos and everything else you do to help other C++ devs out; and (as far as I can tell) just being a really good and genuine guy!
constexpr is the only one that's really disappointing for me.
In the right direction to be frank.
I thought the deal was that the IDE and the compiler were going to be decoupled from each other so that the compiler can just be updated quickly to cover new features? This was Sutter's line at *C++ and Beyond 2012* anyway.
&gt; `in["elem1"](a);` I would have preferred in["elem1"]= a; 
Well that's disappointing.
I second the kudos for what STL does, I've learned a lot (especially about regex!) from his work and comments.
Turns out good compiler engineers are really hard to find. Even mediocre ones. One compiler engineer I know that works for nvidia drives a fucking Ferrari. I can tell you for sure that if I had to do it all over again, I would have focused on compiler technology instead of game engines.
People still use XML?
And this is why I use MinGW.
Yeah, compiler front-end development is incredibly specialized. For example, I read the C++ Standard for a living, and (conceptually speaking) I work on the first thing the compiler compiles after itself. Yet compared to a compiler dev, I may as well be a pizza delivery guy. Interestingly, the compiler back-end (whose devs are terrifyingly smart in a completely different way) is even further away from the FE. The FE and the STL share a common interest in C++ arcana, but the BE essentially consumes C.
I could really do without the operator overloading like this, it doesn't help to clarify (at least for me) what's going on. Thankfully though all that gets tucked away into a single file and all anyone else sees in my projects is LOG(severity) &lt;&lt; "Message";
&gt; Could you please make sure that some product managers at MS see some of these threads at reddit? I've already brought one of these threads to Herb's attention (not this one here, at least not yet). &gt; I understand that some of the claims by people are incorrect, but I do believe the frustration is genuine. I believe it too. I think we've made a lot of progress (just compare VS 2012 to 2008 RTM, which was pre-TR1), but programmers who depend on our tools are entirely justified in wanting even more progress. I have drafted a VCBlog post explaining (in my trademark excruciating detail) the STL changes in the VS 2013 Preview. We'll publish it shortly after the Build conference, when Preview bits will be available. &gt; Thanks for answering questions around here [...] That's really kind of you to say!
Is the compiled lib still ~150mb?
But then, would people buy the new versions of the IDE? Especially when the UI choices are discutable.
It sounds to me like you're looking for an excuse, can't do without variadic templates? Really? If you want to port to GNU, that's cool, but don't trot out brain dead reasons like you can't implement your business features due to a lack of a feature complete C++ 11 compiler. Meanwhile in the real world...
No, I was not referring to *your* post. You can keep taking pride in writing clearly.
&gt; whose devs are terrifyingly smart in a completely different way Could you elaborate on that? I'm curious about the differences in skills required between BE/FE development.
Don't over-focus on one aspect and then try to staw-man me into a position. There is a big list of not (never?) mplemented C++11 features that would help us be more effective. That's what this is all about. MS gave us some, we were so happy it looked like Stockholm syndrome. I also never said we couldn't implement our business features due to a lack of a C++11 compiler. Again, it's the effectiveness we're after.
In the real world, lack of C++ 11 support is already becoming a bit painful. One of the advantages of C++ is that there are lots of libraries and code examples available for reuse or adaptation. Increasingly these make use of C++ 11 features. I develop for embedded systems and my toolchain doesn't support C++ 11, and I'm already starting to find this slows my team down. 
Isn't Clang an option? Or does that also need mingw?
Not to be flippant, but why? Do you think quality of work or effectiveness can be better using GCC or Clang or whatever, or is your opinion more ideologically motivated?
The idea is to harmonize input and output with a uniform syntax, this is the reason the function call operator was used. in ["elem1"](a); out["elem1"](a); I was wondering if it's a good idea to go even further and combine in/output with templates. But I found both cases to have *slightly* different requirements, e.g. input often requires config migration or special error handling. So I kept the input/output proxy classes despite the near-identical code one has to write in the standard cases.
Um.. yeah.. thats totally believable.. **MICROSOFT** - The richest software company in the entire world cannot get staff to implement a bunch of features that have already been implemented in competing compilers. 
AFAIK They are already decoupled in VS. The advantage (atleast from my POV) is that VS has simply the best C++ debugging experience on any platform (esp w.r.t game dev) which makes it a very powerful IDE (discounting all the retarded crap they keep adding for the pointy-head idiots).
Clang on windows is much less polished than gcc on windows, and by itself wouldn't solve any issues with mingw.
C++11 is a standard, standards are written so that people working on a variety of platforms all share a common reference point. When one company decides to forgo respecting those standards it creates real world engineering problems. In the real world a lot of people are writing third party libraries that make use of C++11, and those libraries don't work in Visual Studio. In the real world a lot of people write software that is meant to work on multiple platforms, like Linux, OSX, and Windows. And it's a huge problem when you have to basically code against only the features supported by Visual Studio because it's the only compiler with major bugs and missing C++11 functionality. It's not just variadic templates that are an issue, it's that VS2012 has a lot of conformance related bugs in it that cause subtle differences in behavior when running in Windows vs. running on other platforms.
They don't actually pay that well. Young talent in the area usually end up at better paying jobs for smaller companies. And, not everybody wants to work for Microsoft.
I don't believe that Microsoft would have any problem if they were serious about hiring people to work on their compiler. Also in this particular case, young talent (I assume you mean fresh grads here) is to be explicitly avoided because you want people for whom compiler design is 'old hand'. 
I was patient with VS2012, hoping that the incremental compiler updates would get it to full C++11, but when I saw where it was going I jumped ship to GCC. The additional surprise came when I ported a few libraries over and found quite a bit of bugs that MSVC generously hid from me, since a few things that have **undefined** behavior in the C++ standard are accepted by MSVC and work just fine. Ok, where is the problem in that? The problem is when you want to port the code over to another platform since, surprise surprise, other compiles will exhibit the undefined behavior leading you to a chicken chase for bugs that shouldn't have been there in the first place. The fixed code works just fine in MSVC... Have plans to eventually jump ship to clang, but need to get to linux first. Much easier to port linux C++ code to windows if you are using portable frameworks for your GUI. Edit: One example of the undefined behaviour accepted by MSVC: http://stackoverflow.com/questions/16575566/deleting-an-array-of-objects-upcasted-to-base-pointers There were a few subtle thing as well, but sad to say I don't have them documented anywhere. 
How so? Its very reasonable under Eclipse CDT.
Intel has to be the best compiler out there for windows. Probably tied with gcc on linux. But it has a longer compile times.
Wow, thank you for your insightful reply.
If you think you want to migrate, try to experiment with other toolchains first. Mingw is a bit bitchy to setup on Windows but works lovely once you have it up and running, Clang support is kinda poor. You might also want to check this out, if you won't like Eclipse CDT or QT Developer IDEs: http://www.jetbrains.com/objc/features/cpp.html 
Yeah, that's a good question. Let's see if I can explain this as an outside observer (since I watch FE development fairly closely, and I do interact with the BE from time to time). FE devs need to know all the dark corners of C++ in the area they're working in - and the Core Language is highly interconnected. They have to be prepared to parse the crazy stuff that STL and Boost devs write, and we stress the hell out of the language because we wish we lived 10 years in the future. This is further amplified by users using libraries in unusual ways. For example, when working on variadic templates, we discovered users (including the compiler sources themselves) saying list&lt;PrivateNestedStruct&gt;. Our STL implementation itself doesn't use a lot of nested classes, especially private ones, but users like this. It is totally permitted by the language, as long as the user has access to PrivateNestedStruct at the place where they form the list. However, as list instantiated various variadic templates, the compiler got confused, and thought that list's implementation didn't have the right to say PrivateNestedStruct (we wouldn't if we tried to name it directly, but when passed as a template argument it's okay). This sort of thing was sufficiently subtle that it got past the FE testers (and I don't blame them, only real-world use would find this particular one). There are some dark corners that library devs can just avoid (e.g. I don't have to worry about virtual inheritance except in a couple of places), but FE devs don't have that choice. They always have to be worrying about, "what if it's a bitfield? what if it's a function type? what if it's a cv-qualified member function type?" There's also a certain point at which ordinary users (including library devs) can just expect stuff to work, while FE devs need to actually make it work. For example, the rules for matching explicit and partial specializations, although subtle, have intuitive effects almost all of the time. I can look at any system of specializations and figure out whether it's unambiguous or potentially ambiguous (and if it's scary, rewrite it so it's clearly unambiguous). But I don't actually need to do the matching. The Standard specifies a very complicated game, and the compiler has to implement it exactly. (What's specified is that the compiler has to form imaginary function templates, and then match them against each other - it's *really* clever and beautiful when you see how it works, but it's got to be a lot of work to get right.) And of course FE devs need to be familiar with classic data structures and algorithms, as taught in college compiler courses (none of which I took, heh), plus quirks specific to their codebase (VC's FE, C1XX, is especially quirky). As for BE devs, they don't need to worry about the C++ Core Language and crazy STL/Boost code. Stuff like overload resolution and templates simply doesn't matter to them. As far as they're concerned, they're compiling plain old C (the details of C++ do occasionally matter but very rarely). For example, when I reported an autovectorization issue with the STL, the first thing I did was to reduce it to a plain old function taking raw pointers, since the BE doesn't "see" vector iterators. In exchange, they need to be extensively familiar with assembly and optimization algorithms. VC currently targets three platforms (x86, x64, ARM) and while the FE and libraries almost never worry about architecture details (sizeof(void *) is almost the only issue), the BE has separate codepaths for each platform. In fact, it has to understand the kinds of processors that code will actually be executed on, so that it can be tuned appropriately. As for optimization, that's a whole research area by itself, with many sub-specialties. The BE has to handle inlining (it has complicated heuristics for deciding how aggressively to inline stuff), classic register coloring, autovectorization, constant propagation, alias analysis (eek), etc. Again, this is all stuff that the FE and libs are vaguely aware of, and may rely on their existence (e.g. the STL expects aggressive inlining of small wrapper functions), but we don't care about how it works. The BE also really cares about benchmarks, while the FE has essentially no impact on the speed of generated code. So BE devs monitor benchmark programs (e.g. spec2k6) and they can notice very small effects - for them 1% is huge. In the STL, I don't have nearly as sensitive measurements - I begin to be concerned around 10%. Also, library performance issues are localized - e.g. if we're doing too many divisions in deque, only deque users are affected. The BE affects everything that's compiled with it, so optimization quality immediately affects customers like Windows, Office, SQL, Windows, and Windows.
To add a few illustrative examples to the STL's fantastic answer (let me use this opportunity to thank you for everything you do, Stephan!), see also: * The Glasgow Haskell Compiler (GHC): http://www.aosabook.org/en/ghc.html In particular, in "5.2. High-Level Structure", three distinct chunks are introduced: `compiler`, `libraries`, and `rts`. (RTS being The Runtime System) Note that these components are distinct enough (with distinct goals and constraints) to the point they can even be developed in another language (similarly to C-style backend), compare "How to Keep On Refactoring" with "Developing the RTS" in "5.6. Developing GHC" to see how this results in a vastly different development practice. *// Also note how the points in "Developing the RTS" parallel Stephan's experience of how the "BE affects everything that's compiled with it."* In particular: &gt; 1. Every Haskell program spends a lot of time executing code in the RTS: 20--30% is typical, but characteristics of Haskell programs vary a lot and so figures greater or less than this range are also common. Every cycle saved by optimising the RTS is multiplied many times over, so it is worth spending a lot of time and effort to save those cycles. &gt; 2. The runtime system is statically linked into every Haskell program (that is, unless dynamic linking is being used), so there is an incentive to keep it small. * LLVM: http://www.aosabook.org/en/llvm.html In "11.1. A Quick Introduction to Classical Compiler Design": &gt; The most popular design for a traditional static compiler (like most C compilers) is the three phase design whose major components are the front end, the optimizer and the back end (Figure 11.1). The front end parses source code, checking it for errors, and builds a language-specific Abstract Syntax Tree (AST) to represent the input code. The AST is optionally converted to a new representation for optimization, and the optimizer and back end are run on the code. &gt; The optimizer is responsible for doing a broad variety of transformations to try to improve the code's running time, such as eliminating redundant computations, and is usually more or less independent of language and target. The back end (also known as the code generator) then maps the code onto the target instruction set. In addition to making correct code, it is responsible for generating good code that takes advantage of unusual features of the supported architecture. Common parts of a compiler back end include instruction selection, register allocation, and instruction scheduling. &gt; This model applies equally well to interpreters and JIT compilers. The Java Virtual Machine (JVM) is also an implementation of this model, which uses Java bytecode as the interface between the front end and optimizer. &gt; A final major win of the three-phase design is that **the skills required to implement a front end are different than those required for the optimizer and back end. Separating these makes it easier for a "front-end person" to enhance and maintain their part of the compiler. While this is a social issue, not a technical one, it matters a lot in practice, particularly for open source projects that want to reduce the barrier to contributing as much as possible.** *// emphasis mine* To see an example of the necessity of this separation in practice, see "11.5. Design of the Retargetable LLVM Code Generator": &gt; Similar to the approach in the optimizer, LLVM's code generator splits the code generation problem into individual passes—instruction selection, register allocation, scheduling, code layout optimization, and assembly emission—and provides many builtin passes that are run by default. The target author is then given the opportunity to choose among the default passes, override the defaults and implement completely custom target-specific passes as required. For example, the x86 back end uses a register-pressure-reducing scheduler since it has very few registers, but the PowerPC back end uses a latency optimizing scheduler since it has many of them. The x86 back end uses a custom pass to handle the x87 floating point stack, and the ARM back end uses a custom pass to place constant pool islands inside functions where needed. **This flexibility allows target authors to produce great code without having to write an entire code generator from scratch for their target.** *// emphasis mine* Perhaps this is also worth noting (from "11.7. Retrospective and Future Directions"): &gt; Another major aspect of LLVM remaining nimble (and a controversial topic with clients of the libraries) is our willingness to reconsider previous decisions and make widespread changes to APIs without worrying about backwards compatibility. Invasive changes to LLVM IR itself, for example, require updating all of the optimization passes and cause substantial churn to the C++ APIs. We've done this on several occasions, and though it causes pain for clients, it is the right thing to do to maintain rapid forward progress. * The Dynamic Language Runtime and the Iron Languages: http://www.aosabook.org/en/ironlang.html &gt; IronPython and IronRuby are good examples of how to build a language on top of the DLR. The implementations are very similar because they were developed at the same time by close teams, yet they still have significant differences in implementation. Having multiple different languages co-developed (IronPython, IronRuby, a prototype JavaScript, and the mysterious VBx—a fully dynamic version of VB), along with C#'s and VB's dynamic features, made sure that the DLR design got plenty of testing during development. &gt; The actual development of IronPython, IronRuby, and the DLR was handled very differently than most projects within Microsoft at the time—it was a very agile, iterative development model with continuous integration running from day one. This enabled them to change very quickly when they had to, which was good because the DLR became tied into C#'s dynamic features early in its development. While the DLR tests are very quick, only taking a dozen seconds or so, the language tests take far too long to run (the IronPython test suite takes about 45 minutes, even with parallel execution); improving this would have improved the iteration speed. Ultimately, these iterations converged on the current DLR design, which seems overly complicated in parts but fits together quite nicely in total. 
My understanding is that front end developers have to be know how to take a high-level language down to an intermediate representation which is sort of like a generic higher-level assembly language, whereas back end developers have to know how to take the intermediate representation and convert it down to code targeted for a particular architecture (x86, ARM, SPARC, etc...)
How does it compare with [pugixml](http://pugixml.org/)? Since you mention performance as one of your goals in the rationale, have you considered performing the benchmark against the existing libraries as in [here](http://pugixml.org/benchmark/)? *// benchmark sources and data files link at the bottom;*
&gt; In the real world a lot of people write software that is meant to work on multiple platforms, like Linux, OSX, and Windows. And it's a huge problem when you have to basically code against only the features supported by Visual Studio because it's the only compiler with major bugs and missing C++11 functionality. I am one of these people, but I have to also support Oracle Solaris Studio, and therefore Microsoft's compiler is the least of my problems.
That's a great answer. Thank you.
These are informative quotes! As an aside, although optimization and codegen are distinct, VC groups them together. If you look at our binaries, cl.exe (the compiler driver) loads c1xx.dll (the C++ front-end) and c2.dll (which does both optimization and codegen).
This is why porting code to multiple platforms can actually be a healthy endeavor (if painful in the short term). There isn't a C++ compiler in this world that doesn't have some dark nasty corners. Here's one for GCC that I just ran across (the hard way): http://gcc.gnu.org/bugzilla/show_bug.cgi?id=31368 
&amp; Windows Phone
Why not? What’s the problem with typedef’ing pointer types? I do that routinely, and in fact the standard library also does it, there are plenty of `someclass::pointer` typedefs.
I've read elsewhere that Windows uses a different compiler for various historical reasons. Is that no longer the case, or is it just a different frontend (or was what I read just totally wrong)?
Apple is quite a bit richer these days. Money can't solve all problems. I wouldn't be shocked if the number of top-notch compiler devs in the world was in the double-digits, and some of them would not work for MS for any amount of money. Some would, but only for far more money than it's worth.
In hindsight yes, I totally agree! Best to use as many toolchains as possible, or the "worst" case of at least running automated testing on multiple toolchains. *sigh* starting to develop a new found respect for devs of multiplatform libraries.
It is an option recently. C++Builder is one way (not free) to use clang 3.1 on Windows.
AFAIK, the implementation is free to define undefined behavior. Most probably, they didn't even try to define it, it works because of how they've implemented it. In any case, UB means that ANYTHING can happen, including doing what you think it should be doing.
Wow, pugi outperforms rapidxml - this is news to me!
Real-world engineering problems occur when your product is a library that you deliver to a customer which is a huge organization who for logistical reasons cannot easily switch to a newer compiler. So you're stuck on VS2008 and gcc 4.2 for a couple of years. 
It's probably worth pointing out that it only supports a very limited subset of XML.
Can I ask what is the benefit of this technique? What is obtained by avoiding virtual functions? I guess I have to elaborate for the polite denizens of r/cpp: Why is it a good idea to pass objects with internal pointers between address spaces? Is this a C++ article, or a "here's a technique that worked on my architecture, with my compiler, with my runtime, with my ABI" article? It doesn't seem portable, nor safe, nor generally a good idea, nor advantageous to portable, safe, general solutions like serialization. I think it's a great case of "just because you can do it, doesn't mean you should".
Yes.
You might want to reread it in *entirety* then.
I don't know how I can live anymore.
Ok, now what?
thanks for finally getting 64bit offset support into iostreams in 2012. so now you need to address why iostreams is in my experience about 4-5x slower than GCC/Linux. with disk io I do understand you have no control over the poor windows io in general but the iostreams definately makes it even worse.
as much as I dislike ms products I have to point out that different compilers all point out different bugs. its best to always work with at least 2 compilers if you can. gcc, clang msvc (express) are usually a decent mix. I'm sad Borland is so far behind
Unless I am mistaken, it will only work if the consumer processes are identical otherwise the function offsets will differ.
Someone else also pointed that out, and in retrospect yes, both are absolutely correct. Need to see if I can get an automated build server up that builds with multiple compilers are runs basic tests on the executables. Oh joy! :D
ASLR is already here, even for Windows. I know it's not as reliable as Linux's randomization though but you're exactly right that this technique is not something to rely on, especially with future Windows versions (who knows, it's probably broken already on Windows Server OSes).
Sometimes I think c++ was a big mistake to begin with. Or maybe it's just references that were the big mistake. Something's amiss though. Anybody else feel that way? 
Ah then, refuted! 100% of the commenters disagree!
Facebook does. Not sure if that's type of thing you mean?
&gt;written in C++ Its written in php afaik. Their 'to C++ compiler' could be written in C++, but i dont think this is what OP means.
QtCreator + mingw is quite easy to grasp if you come from VS and you can keep compiling with whatever VS compiler you use as well. Just use different kits.
Well, I fixed one of the Game_Music_Emu ones. I'll have to let the original author figure out how he wants the other one fixed, if at all.
It's true that Facebook's UI is written in PHP. But their backend services are written in many different languages, including C++ and Java. They created Thrift explicitly in order to simplify the RPC between different languages.
It is, there's an interesting video showing how the Qt guys are going about it: https://www.youtube.com/watch?v=sajBj_eiH10
&gt; thanks for finally getting 64bit offset support into iostreams in 2012. Sorry it took so long. I thought we'd fixed it in 2010, but I missed truncation in a macro. &gt; so now you need to address why iostreams is in my experience about 4-5x slower than GCC/Linux. GCC/MinGW would be a fair comparison - we don't control the OS. (I haven't run the numbers lately.) The problem with iostreams is that their design is aggressively hostile to efficiency, and they're so complicated and fragile that improving performance is difficult. This is on my radar, but it's below higher priority things that I know I can do something about.
Yup
I think a ton of Yahoo stuff was. Might still be.
It would still work though since the pointers are stored as RVAs. Windows ASLR only rebases whole modules so it will be ok. If it moved the sections around inside it would totally break the PE format. However calling GetModuleHandle( NULL ); will not return the right module if this code was built into a DLL. A better way would be to use GetModuleHandleEx with GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS
Can you explain what you mean by "type tags", or provide a link? I haven't heard of them before.
[Lumatic](http://www.lumatic.com)'s routing engine! Beautiful C++ through and through. Although I doubt it meets the qualification of simple..
I've been moving sensitive code away from iostreams in general and using simple self cooked classes to still have a unified interface using files or memory, etc (where is a good 3rd party c++(11) iostreams alternative anyways?). I see a 4-5x speedup on windows vs 1.5x or so on Linux gcc. I need to check the sate of kings. I must have 64bit and support for reliable threading only available with vista and later.
Okay, so it does actually have virtual functions (virtual destructor) and it's just a templated wrapper for a function pointer? Correct me if I'm wrong.
You can say `a = a + b;` or `a += b;` which is equivalent.
Yes, this has been my go-to reference for regex both personally and people on the same team. Several pitfalls safely avoided as a direct result of seeing the material (making good use of regex_iterator vs regex_search). Thanks again!
i dont think i can do that with what im working with. heres my code: int number; cout &lt;&lt; "Enter a number: "; cin &gt;&gt; number; while(number &lt; 100) { cout &lt;&lt; "Add another number: "; cin &gt;&gt; number; number = number + number; } cout &lt;&lt; "The number has reached 100";
you could also call this remap, I'm not sure this is optimal. float zeroToOne = (blend - Y0) / Y1; return zeroToOne * Y3 + Y2; I don't know why the above function is so complicated
This looks a heck of a lot like cubic spline interpolation. The function turns into: result = Y0*(-B^3 + 2B^2 - B) + Y1*(B^3 - 2B^2 + 1) + Y2*(-B^3 - B^2 + B) + Y3*(B^3 - B^2) I hope this is helpful in some way.
Looking at its form, it's a cubic (third order polynomial) in Blend. It's likely meant to "smoothly interpolate" between values Y0, Y1, Y2, and Y3. It's easy to see that evaluating when Blend = 0.0 returns Y1. With a bit more work you can see that evaluating when Blend = 1.0 returns Y2. With some more work you could figure out which values of Blend produce Y0 and Y3 precisely, but that's probably not necessary. The assumption is likely that Y0 &lt; Y1 &lt; Y2 &lt; Y3, and you'll only ever call this function with values of Blend between 0.0 and 1.0 to smoothly interpolate between two values in, say, a long lookup table in such a way that you don't suddenly change direction when you pass through a given point. It was likely derived by fitting a cubic to 4 data points (e.g. (X0,Y0), (0.0,Y1), (1.0,Y2), (X3,Y3), for some X0 and X3 that we could figure out with some effort) and then evaluating that cubic for X=Blend. 
CppCMS is really nice!
According to the [GCC FAQ](http://gcc.gnu.org/faq.html#dso), GCC since 3.0 does a simple pointer comparison on the `type_info` objects associated with the classes involved in the `dynamic_cast`. I would be very surprised if Microsoft has not made a similar optimization in VC++.
&gt; And given the size of the C++ compiler team at Microsoft (at least several years ago I knew it was very small) Isn't this one of the major problems though -- that Microsoft's C++ compiler team is so small?
Thanks, that was very helpful :)
So... he invented a technique to replace the virtual function table with a self-crafted virtual function table, that instead of O(1) space in the object takes O(n) space in the object and more maintenance. The function table he invents has the advantage of being serializable at the expense of being really fragile as it requires the exact same build of software to run everywhere. Networking is pretty much the antipattern of that, as your software will update differently in places. So I see a pile of complex constructions that construct a half-arsed implementation of polymorphism and serialization. Can I suggest using actual polymorphism with virtual functions, and actual serialization instead (using /u/ickysticky 's type tags to send the actual type along) ? That's faster, simpler and less fragile than this solution.
I believe VC++ still uses string compares in order to work correctly over DLL boundaries. 
I stopped reading at the point that he uses a ...function pointer in order to ,,,avoid function pointers in vtables. What a lame article...
It's cubic interpolation. You supply blend between 0 and 1, and two points before and after the value you wish to interpolate. It fits a cubic approximation to the four points supplied and returns a value from that cubic polynomial between Y1 and Y2. I.e. * Blend = 0 -&gt; Returns value from cubic polynomial at Y1 * Blend = 0.5 -&gt; Returns value from poly halfway between Y1 and Y2 * Blend = 1.0 -&gt; Returns value from poly at Y2 I use it when resampling audio data - to play something slower we need to generate more audio samples than the original audio file - so as we move forward we are moving non-integer steps and need some way to interpolate the samples in between the ones we know.
https://github.com/toffaletti/rl-proxy
Thanks for the audio example.
This formular does not assume equidistant points: E.g. take the polynomial p(x) := (1 - x)·(x - 2)·(x - 3) it has p(0) = 6, p(1) = p(2) = p(3) = 0. Lerp(6, 0, 0, 0, 0.5) gives -3/4 but p(1.5) is -3/8
Thanks. This is a good example of a service that is fully dependent on a fast backend (and a lot of precomputations). I wonder what kind of shortest-path algorithm they use. http://en.wikipedia.org/wiki/Shortest_path_problem#Road_networks
In that form there is a pole (undefined or near infinite at or around Y1=0). The form above uses multiplications which don't have this issue. Not sure if I follow your simplification which produced the division either. 
Thanks for the references. After reading through them I feel that my request was somewhat misguided. I think that when a decision is made to create a C++ backend it is made because of requirements for not only speed, but for the flexibility and scalability delivered by C++ as an OOP language. I guess this is the reason that C++ is used mainly for larger systems/frameworks, and not small web libs. I have some experience in both backend and frontend programming and this argument makes sense to me. Please share your views on it. Edit: it is always interesting to see why and where different programming languages are used. There is learning in that:)
What would you say is a viable alternative to iostreams? Boost? EDIT: sorry, saw that bnolsen also asked the same question.
yeah .. that's why it is called INTERPOLATION and not exact calculation ...
I can't believe I got this fooled by Microsoft. I only bought VC2012 after seeing Nov CTP which STL (nice MS guy) promoted on his C++ channel. C++ 11 support in VC2012 is a joke. Without variadic templates and updated standard lib it has almost no relevant updates since VC2010. Last time I paid MS for anything. 
&gt; lerp(a,b,c) = a + (b-a)*c Thank you so much for explaining how Lerp works, that equation makes perfect sense.
... and GCC has had considerable problems with that implementation across shared library boundaries. They've since changed it back to strcmp(): http://gcc.gnu.org/bugzilla/show_bug.cgi?id=47960 GCC has had multiple issues over the years relating to STL performance hacks depending on static object addresses which inevitably blow up in the presence of shared libraries. It just doesn't work.
OkCupid serves their page and runs their backend in purely C++
Djikstra's with an A* heuristic applied. Turns out that's plenty fast enough when 99.9% of the paths you generate are within the same city! Source: I built lots of it.
of course .. for interpolation you have some points as your data and relating function values, nothing more. And your task is, to estimate the value of the function between the points (under given assumptions)
The problem with iostreams is that nothing has superseded it. I'll fall back to stdio if performance is critical, but it's dangerous to use.
Of course every cubic interpolation will naturally give back accurately any value of a 3rd order polyom. This is because the interpolating polynom is unique: https://en.wikipedia.org/wiki/Polynomial_interpolation#Uniqueness_of_the_interpolating_polynomial "Lerp" doesn't do this which is likely due to Y0, Y1, Y2, Y3 corresponding to x-values with different distance. Maybe they take the nulls of the chebyshev polynomial of 4th order...
Well you learn something new every day! I doff and waggle my hat in your direction and bow to your superior mathematics knowledge. I'm looking at that page right now and trying to figure out why it might not apply to this particular case.
How about fixing Qt so it doesn't need to use a dreaded pre compiler?
That is really interesting. I may be pushing it here, but can you tell us more about it? E.g. what data structures did you use?
Okay, I know where this formular is coming from: It's not a cubic interpolation that looks for a polynomial of degree &lt;= 3 to intersect all four points, but rather just the middle two + a requirement on the derivative at those two points. It's essentially the approach of: http://www.paulinternet.nl/?page=bicubic With *one difference*: The formular from "Lerp" used f'(x1) = Y2 - Y0 and f'(x2) = Y3 - Y1. This seems to be incorrect as they forget the division by 2.
Ah, well spotted! I was plunging through the [Polynomial interpolators for high quality audio](http://yehar.com/blog/wp-content/uploads/2009/08/deip-original.pdf) paper by Olli Niemitalo trying to find a match for it :-) Nice job. At a guess it's a "common optimisation" - there's loads of places over the web where it's given as "the cubic interpolation" function without deriving it.
thx for clearing it up .. i missunderstood you
You can make .txt files and then probably just turn that into a batch file. Have you tried looking up libraries for this?
Yeah, this is easy to do. The simplest thing to do is just use cout &lt;&lt; "some text" statements to build a file, and then insert a variable that you've read from input whenever you want to. A slightly more sophisticated approach is to have a template file that gets variables inserted into it via user input. E.g. the template file might have variables like "$x", and then you can copy the file, replacing each instance of "$x" with something from user input. Either way, fairly straightforward. I'm used to unix scripting, not batch files, but it might also be possible to do this in the batch file itself. That's what the batch scripts should be for, anyway. Also, if your week number just goes up sequentially, you could write a program that detects it automatically based on the date or passed history. You could write a C++ program that figures it out, then runs the batch script, or try building it into the batch script itself.
I have tried a little bit, I'd say I'm a novice with programming and was just trying to see if I could make my life a little easier here. Ill start looking around more online, I have found that I will probably have to do what you've said and just change the extension.
Thanks, yeah basically I want a user input say "Enter Week Number: " then have the user input the week, say 43, then have the program create a batch file for all the various things it needs to do, essentially copying a boat load of files from one directory to another. Just every week I need to edit this batch file and it takes a good while. Thanks for the tips tho! Much appreciated.
I think notepad might be in C++.
I wonder if it's possible to have a moc with less limitations (templates, nested classes, typedefs working correctly, virtual inheritance, ...) using this work.
 error: expected ‘(’ before ‘else’
Just confirming that it does indeed still perform string comparisons. And really it is not too slow for occasional use, out of a loop etc. 
Sure, I'd love to! In general, our data structures just build on STL stuff. Here's some code lifted from graph.h. It contains all of the basic data structure for a traversable graph. We feed this in to Djikstra's. template&lt;class NodeData = void, class EdgeData = void&gt; struct graph { typedef unsigned gid_t; struct edge; struct node { gid_t id; NodeData data; std::vector&lt;edge *&gt; edges_out, edges_in; ... Constructor bool has_link_to(const node *other_node) const { for(const auto e : edges_out) if(e-&gt;to == other_node) return true; return false; } ... Some other convenience methods } struct edge { gid_t id; node * from, * to; edge * reverse; EdgeData data; ... Constructor } std::unordered_map&lt;gid_t, node*&gt; nodes; std::unordered_map&lt;gid_t, edge*&gt; edges; gid_t highest_id; gid_t get_next_id() { return highest_id++; } .. Functions that deal with making new nodes/edges and linking them up. }; We load up a snapshot from OpenStreetMap in to this graph structure and have a function (follow_edge(edge * e)) that spits out the cost of following an edge. In the Djikstra's implementation, a priority queue keeps track of the next 'best' edge to follow. Since we're very geo-centric, we organize all of our data spatially. For spatial indexing, we use a home implementation of a KD-Tree in either two or three dimensions depending on the area we're covering (stuff starts to get weird when you have to take the curvature of the earth in to account). Spatial lookups are blazingly fast. For fuzzy landmark name seach, we use a data structure which I call a word tree for lack of a better name. It's loosely based on a BK-Tree. Basically each word of your query 'votes' on which landmark you're looking for and the landmarks with the most votes are pushed up to the top. Mis-spellings make votes count slightly less. So, if a user searches for "wendy's", we would serve up the results "Wendy's" and "Wendy's Old Fashion Hamburgers" on exactly the same footing. Both the KD &amp; Word trees (and really any other thing we develop) are built on plain old STL containers, except for our unicode codepoint string data type, which is just typedef'd to std::basic_string&lt;int32_t&gt;. That's kind of a smattering of some of the stuff we work with. Happy to answer specific questions.
That's not quite the same thing. In any case, clang happens to use a hand-built parser rather than a yacc/bison generated parser, so I guess that's handled and now Qt needs to do away with its pre-compiler?
Not really an issue for me, since having several incompatible VS2012 releases does not seem like a good position to be in. Now, if VS2013 does not manage to finalize the C++ 11 support either, that would be quite pathetic.
why not use environment variables? you know, the %MYVARIABLE% thingies? So, in your bat file replace every occurrence of week number with a variable, %WEEKNUM% for example. and then -- &gt; set WEEKNUM=42 &gt; mybatfile.bat I bet some bat file wizard (which I am definitely not) can use the date and time (or something else) commands to generate the current weeknumber for you at runtime. 
i think "used to be" is of note here...
It looks very nice r-lyeh, thanks for your job!
How does it compare to JSONcpp?
That's what I was wondering at the time. For ~2 hours. And that day I decided to leave my laptop in the hotel room...
Do you return keys in the order added?
 // Parse string or stream Object o; assert(o.parse(teststr)); // Same than above istringstream input(teststr); assert(Object::parse(input,o)); Why do you need these two different ways to do the same thing? What's the state of o before parse? Why not let Object take the input string in its constructor or let parse return an Object? This would be simpler API: Object o = json::parse(input); Is an object always at the top level of a json string or can it be something else? If so, what happens in the code above?
Where exactly are those thoughts?
It's time for smaller more concentrated Qt that doesn't rely on external tools.
Needs more enterprise. I suggest some form of web app written in J2EE with start/stop buttons and a management console with pretty graphs. Store the xsl in an object versioning database and write the output back there too under a design-by-committee filing plan that takes three weeks to get all the stake holders to agree. Best guess - about 1.2 Mill EUR. And we'll be late.
What is the batch file? You could do that in Python in 5 minutes, no need for a complicated solution in C++.
That is a very good question. As it stands, the input could be null.
Just to keep compatibility intact against original hjiang's design. I'm currently simplifying API whenever possible and extending functionality as a contributor :D ::Object() is used to construct a key-value pair instead. ::parse() method could return another Object indeed, but that would be costly if the Object is big enough, unless we are using C++11 move semantics. the library is legacy C++ friendly.
From what I can see jsonxx is much smaller, allows relaxed parsing (useful when you are hand-editing your local JSON files) and can convert JSON documents to lossless XML files. By the other hand jsoncpp handles JSON comments in a better way, and the library is older too, so I might think it has been tested more deeply.
Object is mapped to a std::map, so after some processing Object is sorted by key name. A small source modification could return it back in original state, however I cannot think of any usage of this. Any idea? Thanks!
Yep, should change it for 101% safety. Next rev maybe.
Pointers are a 'nullable' type, and References are not nullable. It means that with a reference you can guarantee that there is something 'being pointed to', while a pointer being null and you don't account for it can cause your program to die a horrible death.
both are equivalent in this case. however, pointers can point both to valid and invalid places. references always "point" to valid places. pointers are handy sometimes, but they may be more dangerous like zzing said. check answer at http://stackoverflow.com/questions/4364536/c-null-reference
So references would be preferred to pointers? Or just check if the pointer is null? 
But if we do it my way - _We'll Be Millionaires!_
original author wants to support legacy C++ too. issue is fixed anyways now :D thanks!
I truly appreciate the effort that goes into these presentations but I just can't get past it: streaming video is *not* a good medium for presenting this type of information. PPT slides are better. IMHO YMMV
STL frequents this subreddit, maybe he would be willing to put together presentations for distribution with his videos? Assuming he hasn't already :) 
Aha, some comments make sense. Will do a configurable boolean jsonxx::Setting::SortedKeys in next revision. Thanks for the tip!
Some people really like video and others really like written material (slides, blog posts, books). I am actually in the latter camp myself, since I read way faster than anyone can talk and I retain information better when I've seen it. Still, lots of people seem to be in the former camp, so I keep filming these videos. I'd like to prepare companion slides, but writing them takes me forever. I wrote slides for my GoingNative 2012 presentation, [STL11: Magic &amp;&amp; Secrets](http://channel9.msdn.com/Events/GoingNative/GoingNative-2012/STL11-Magic-Secrets), but that took me a solid week (all slides were fresh except for parts of the shared_ptr diagram that I copied from one of my previous presentations). If my job consisted solely of preparing presentations, I'd totally have time, but my primary job is working on the STL.
if it checks for null, there should be a preprocessor NOSAFETY #define. It's a good thing to be safe, but I hate it when libraries don't allow me to compile with no safety nets. If I make a guarantee that my code is valid, the library should be able to accommodate and not uselessly double-check. 
Did you compare performance with json spirit? 
Alright. Return value optimization was available long before C++11. Enjoy!
Pure texts may be good for learning something quickly, but watching videos with high-quality content like these is learning + entertainment, a double-win!
http://youtu.be/SLLOSAm-OS0?t=26m14s "any of you under 40 that are coding in C++ really want to rethink your decision" *Uh-oh...* ;D
The difference is that on the one hand a preprocessor takes input it mostly doesn't understand and it just makes a few modifications to the bits it thinks it knows and passes most of the input through, and on the other hand a compiler takes a program in a defined language which the compiler understands and transforms it into another language which is also understood. Of course there's a gradient and yacc/bison are not at the extreme 'compiler' end, but the more you move toward the preprocessor end of the spectrum where neither the input nor the output are well understood by the processor, the greater the likelihood for problems that the processor will create or won't be able to help you fix.
I don't find a documentation link.
I actually like video talks a lot better, because I can easily consume it while I do something else. Like browse Reddit...or do work.
I was actually wondering the day before yesterday when another Core C++ video was going to come out. Yay! Can anyone give any insight into why .\* and -&gt;\* are lower precedence than ()? I never understood this and it seems highly unnatural given the way .\* and -&gt;\* are used.
Why? It was merely an example of a language that compiled to a different language instead of machine code directly, there are *many* other examples, many of which are held in renown. E.g. haxe compiles not even to C, but to C++. CoffeeScript to JavaScript, Vala and Genie to C, RPython can be compiled to C. And so on. My point is that people complain about using a frontend as if that's not a normal development technique. Domain-specific languages have a bright future; just look at some Boost code if you don't believe me. And before you note that Boost requires no pre-processor, go further to note how poor the syntax usually ends up, especially for things like Boost Spirit.
One regex wouldn't be enough. Several maybe. The [URI grammar](https://www.ietf.org/rfc/rfc3986.txt) is surprisingly arcane. Certainly complex enough to warrant using a real parser framework.
I just turned 30, and my decision to learn C++ when I was 18 was the best decision I've ever made. (I decided to learn C when I graduated high school, then spent a year and a half struggling before I finally hit a wall with memory management and reconsidered my ESR-influenced rejection of C++ as too complicated.)
I'm myself a cpp developer, but you're right I didn't look for the c subreddit. Regarding using a regex, the grammar is not that simple like noted. Furthermore I'm not aware of a lightweight (both in terms of API, dependencies, size and complexity/performance) library that would do the job. What library would you use?
At the moment the documentation is part of the header (and VERY minimal :)). 
I hate to be negative, but this is yet another json library that uses recursive descent parsing and as such is vulnerable to stack overflows (probably exploitable). This means you should never use this library on user submitted json. A first step to mitigating this would be to implement a nesting depth limit.
&gt; Furthermore I'm not aware of a lightweight (both in terms of API, dependencies, size and complexity/performance) library that would do the job. What library would you use? The standard library.
How can you exploit a stack overflow?
Ehm nope, wanna create a few benchs in spirit and send them to me?
Do consider alternative algorithms, as provided by Bison/Yacc and similar. For a grammar as simple as JSON, it's not difficult to drop in there, and they might even improve performance. :)
That was part of Stanley Lippman's talk. He sounded seriously burned out during the talk.
&gt;Our first brown bag will be on the great new C++11 features, THIS WEDNESDAY (6-12-203) @ NOON (12:00 PM) In what time zone for us international folks?
Edit: After downloading the Citrix GoToMeeting software, I'm able to confirm that it is indeed at 12:00pm CDT. (GMT -6, or GMT -5 for those who dont want to do the CDT conversion).
Citrix GoToMeeting doesn't support Ubuntu.
+1 for providing hope :-)
Yeah, there was a lot of... *intriguing* stuff in that one :-
Sure. in a few weeks though. I'll keep you posted
Oops sorry about that, I typed that out too quickly. I guess it is only vulnerable to a denial of service.
Off topic, but fun fact: I once had a look at the leaked Windows source code and most built-in programs were actually written in C.
More videos are being added to http://www.youtube.com/user/BoostCon/
A system("pause") at the end of main() instead of a break-point would simply avoid the console/ide windows switching back and forth...
This is something I've been looking for.
Could you post an example of a list and an example of the values you want. In the base case you posted and the way the users write.
Perhaps nested (first level: separator "`,`"; second level: separator "`/`") invocations of [Boost.Tokenizer](//boost.org/libs/tokenizer/) would do?
I wish it was homework, thanks I'll take a look into it. 
Can't say I really understand the problem with the original snippet of code. void f() { Thing t; work_on(t); } How is that any different than: void f() { Thing t; work_on(std::move(t)); } If t's constructor does happen to make use of the 'this' pointer to address itself, for example add itself to a list of all Things, well then when t get moved the move constructor will have to do whatever work needed to update itself, such as removing the old instance of t and adding a new one.
The move constructor should not deal with removing the moved-object address from the list, in that scenario that will be the role of the destructor. However, consider the case where Thing has a private attribute buffer which is dynamically allocated during construction. class Thing { char * buffer; ... }; Thing::Thing(): buffer( new char[50] ) { } Now the move semantics lets Thing get built without reallocating that array: Thing::Things( Things &amp;&amp; moved ) :buffer( moved.buffer ) { moved.buffer = nullptr; } It comes very handy when buffer is some very heavy structure.
It's still not clear what the problem is. I'm not trying to argue that I'm right on this issue but the article fails to explain what will actually, matter of factly, go wrong if such an optimization were performed and that's why I don't understand it. Basically what would fail or go wrong if the optimization were performed?
Perhaps you'll find this relevant: * http://www.reddit.com/r/cpp/comments/1g9i1g/coming_soon_gradle_build_tool_support_for_cc/ See also: * http://www.reddit.com/r/cpp/comments/wvnko/build_systems_for_cc_projects/ * http://www.reddit.com/r/cpp/comments/17mhxx/managing_modular_software_for_your_nuget_c_and/
If the compiler was allowed to optimize this snippet, there could be a race condition: void f() { Thing t; work_on(t); } In this snippet, the moment where the destructor of T is called is precisely known: it happens after work_on() is done executing, when f() returns. Suppose that right after work_on() finishes, and before f() returns, another thread decides to use the list of Thing you were mentioning (I am assuming that the thread are synchronized). As t is still a live object when work_on() returns, using it is legal, so everything is fine. void f() { Thing t; work_on(std::move(t)); } Now in this snippet, when work_on() has returned, the state of t is unknown. On some implementation, it could be in an usable state, and on some other it might not. So if another thread tries to get access to t, it creates a race condition (and undefined behavior). I hope this helps
Hmm... if this is strictly a matter of multithreading I mean maybe you're right, hard to know exactly. I feel as though if this issue was one involving multithreading the article should have probably at the very least hinted at it, if not just explicitly stated that the problem involves using t in multiple threads. Something tells me the author meant that there is an issue with the code even in a single threaded context, and that's what I'm scratching my head about. If Thing defines a no-throw move constructor, then work_on(std::move(t)) should always be a valid operation. And if it is a valid operation then there should be no difference between those two snippets of code, regardless of whether Thing's constructor takes a pointer to itself or not. If it does take a pointer to itself then the move constructor should factor that in when performing the move operation, otherwise the move constructor has a bug in it.
Pretty cool stuff, and the diagram is really helpful. Reading your previous posts :)
If there a code for your "Designing a flexible GPU abstraction (C++/OpenGL)" post? Especially that it explicitly mentions C++ and OpenGL ;)
Interesting, thanks for the explanation.
Good to hear it's useful to you!
Thanks!
My team uses CruiseControl.NET.
[Teamcity](http://www.jetbrains.com/teamcity/)
Jenkins is written in Java, but it works great as a C++ development tool, we use it extensively. We use distributed builds so that we can build on Windows and Linux. It does automatic building when we push to source control (git in our case) Don't write it off because of Java!
TeamCity is great.
dynarray has a *fixed* but dynamic size. That is, once it's constructed, its size cannot change. That's why you only need the option to pass in an allocator at construction. 
Well, chances are you arent allowed to do what you do in signal handler anywyas (iirc pretty much only global atomics are ok to touch from there). Some more info [here](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3633.html).
+1 for Jenkins. 
 #include &lt;boost/spirit/include/qi.hpp&gt; #include &lt;iostream&gt; #include &lt;vector&gt; #include &lt;string&gt; int main() { std::vector&lt;std::string&gt; colours; std::string s="green,red/black,blue,orange/purple"; using boost::spirit::qi::char_; using boost::spirit::lit; boost::spirit::qi::parse( begin(s), end(s), +char_("a-Za-z") % (lit(",") | "/"), colours ); for(const auto&amp; s : colours) std::cout &lt;&lt; s &lt;&lt; '\n'; } Of course you can make that more complex as you like.
I have a question, just to reaffirm my knowledge of C++, doing: work_on( Thing() ) would never be as desirable as doing: work_on( &amp;t ) correct?
Actually, the point of this article is that doing work_on(Thing()) lets the compiler perform optimizations that would be otherwise forbidden. So yes, work_on(Thing()) can yield better results.
I guess I could do that. I hate seeing that in examples, though.
Because the comma operator has the lowest precedence of all, `i = ++i, i++, ++i;` is parsed as `(i = ++i), i++, ++i;`. Then `i = ++i` is executed first, but that violates the Multiple Modification Rule I talked about - it attempts to increment `i` and assign to `i` without any "sequence points" in between. Therefore, it's undefined behavior.
I suspect that only Stroustrup can answer this question - I don't recall seeing it mentioned in D&amp;E. I might ask him at the next Committee meeting.
I thought the article was more about sending over a copy of t. Just to be specific, I am referring to a pointer to t...would that really not be more performant, than work_on(Thing()) assuming the constructor is small? Just double checking
Yeah, that's right. Of course, anyone writing such code is a monster.
Hm, it seems to me all those ruminations in the article apply equally well to the move-construction case: A move-constructor could equally take the "this" pointer and mess with it. Maybe his example is just bad. There are at least other reasons why the compiler should not implicitly move. E.g. In the example the "Thing t" is never used again after the function call. Let's assume there were an implicit move in this case: From the programmer's point of view, as well as from a future C++ compiler view, this could be seen as equal to a temporary. But if the programmer added new code later that uses "t" after the call, one would expect this to deactivate the implicit move. These are pretty awkward semantics: adding code should not change the meaning of code written above.
Here's a more obvious argument against implicit move in the example: The lifetime of "Thing t" is expected to last until after the function call. The compiler should not be allowed to implicitly move, which would limit the lifetime to a point inside the function and before it returns.
'They just don't care' does not equal 'legitimate' though. 
thanks! now i'm rethinking whether I should be investing my time in qt...
It does in this case. Standard legitimizes such behavior.
Cool! I'm using JUCE to write an audio application, and man, the code is nice. I'm thinking about building an iPhone/Android game using it next. I know cocos2d-x would technically be a better fit, but the code looks pretty grim next to JUCE code. 
If undefined behavior occurs the compiler is allowed to do whatever it wants, including the exact same thing as it would if undefined behavior hadn't occurred. Thus, it's allowed to ignore any cases in which undefined behavior would occur.
But in this case we don't have undefined behavior. We have an address addition that might result in wrap around.
I don't think the standard does so. We don't have undefined behavior here: wrap around is not undefined behavior. 
We have jenkins building and packaging 21 projects Visual Studio 2010 C++ projects every night. It works perfectly. You might still need a few scripts here and there - but even just managing all the scripts the OP has through jenkins will still give great benefits, as jenkins take care of all the plumbing, makes it easy to archive stuff, makes it easy to schedule builds, or trigger builds from changes to the source repository and so on. 
Its undefined to evaluate expression that produces a pointer not pointing into original array (or one past it). Thats exactly what happens with the check: &gt;ptr + len &lt; ptr It checks whether 'ptr + len' is before beggining of an array. Producing such value is undefined behavior (which compiler can assume doesnt happen) thus expression gets removed entirely. 5.7.5 talks about this: &gt;When an expression that has integral type is added to or subtracted from a pointer, the result has the type of the pointer operand. If the pointer operand points to an element of an array object, and the array is large enough, the result points to an element offset from the original element such that the difference of the subscripts of the resulting and original array elements equals the integral expression. In other words, if the expression P points to the i-th element of an array object, the expressions (P)+N (equivalently, N+(P)) and (P)-N (where N has the value n) point to, respectively, the i + n-th and i − n-th elements of the array object, provided they exist. Moreover, if the expression P points to the last element of an array object, the expression (P)+1 points one past the last element of the array object, and if the expression Q points one past the last element of an array object, the expression (Q)-1 points to the last element of the array object. **If both the pointer operand and the result point to elements of the same array object, or one past the last element of the array object, the evaluation shall not produce an overflow; otherwise, the behavior is undefined.**
From the JUCE documentation: &gt; An instance of this class is used to specify initialisation and shutdown code for the application. &gt; An application that wants to run in the JUCE framework needs to declare a subclass of JUCEApplication and implement its various pure virtual methods. &gt; It then needs to use the START_JUCE_APPLICATION macro somewhere in a cpp file to declare an instance of this class and generate a suitable platform-specific main() function. From here: http://www.juce.com/api/classJUCEApplication.html The above is better than say, Qt? I don't think so. Furthermore, JUCE doesn't have signals and slots, it has listeners, which is a subpar way of handling changes. JUCE doesn't have an ownership memory management model like Qt, from what I see from the examples (those deleteAndZero calls are ugly). It uses command ids, like MFC. A major problem. It doesn't have a clearly defined layout mechanism. I see methods like 'parentResized' and classes like 'StretchableLayoutManager' that don't inherit from any class, meaning no clearly defined layout mechanism. For menus, JUCE doesn't treat menu commands as objects, it uses ids, as evident in TreeViewDemo.cpp. In other words, it seems not that good as Qt. Kudos to the writer though, it's impressive work for a one-man team. 
The result of pointer math that results in wrapping around *is* undefined behavior. Legal pointer math is required to never result in an overflow (which can be achieved by just not using the last byte of the address space).
Obligatory: http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html
Becoming handy with Qt is good use of your time. EDIT: *much later thought* I in no way meant to suggest you should not also learn or use JUCE if it's more appropriate for your project. I only meant to say that you wouldn't want to consider it an alternative to Qt. I can see both projects being very valuable additions to a developers toolkit.
For #1, I'd go with [adjoin](http://clhs.lisp.se/Body/f_adjoin.htm).
It's great if your UI requirements are not heavy or non-standard. The media functionality is excellent. And it's so damn easy to drop in to a project - that makes a big difference for certain tasks. As far as I can tell JUCE is aimed at audio applications - especially VST plugins for audio software. I've used it before where all I needed was a few buttons/sliders for UI but heavy requirements for audio and video (support for low latency IO on lots of systems.) I'd certainly pick it over QT in some cases (I've used both.) Also I feel that QT signals and slots are overkill now we have C++11. std::function and bind make the whole listener mechanism for changes work just fine. The java world gets by with listener based approach OK because anonymous classes are easy to define. I agree that doesn't help you if you need a more general messaging system - but often that is not needed.
I don't really like the idea of using auto everywhere, at best you gain nothing in readability and at worst you make it difficult to distinguish variables that might take different types depending on the call. Auto seems best used to indicate that either the type of the variable might not always be the same or that the explicit type isn't hugely relevant, like in `for(auto&amp; foo : bar){baz(foo);}` where auto just means "type of thing in bar". Writing `auto i = 75_u32t;` is really just saying "look what I can do guys!" It doesn't really hurt, but it also doesn't make your code any more readable. ~~Off topic: is there a way to insert code formatting inline? Because little snippets like this look awkward.~~ Thanks vanhellion.
&gt; By "totally awesome", you mean "ugly hack", right? Yes :o!
You can hit these same issues with iterators, since the iterators for `std::array`, `std::vector` and `std::deque` are very thin layers on top of pointer arithmetic.
I don't know if this is still on the JUCE website, but it used to say something like "the current number of known bugs in JUCE is always zero". The lack of wisdom reflected in that statement was almost enough on its own to convince me I'd be a fool if I made this the central, core framework that my application and plugin depended on. (And I don't mean to say the developer was being cocky, but perhaps if you don't have any known bugs it means your software isn't tested well enough.)
Manually manage memory? Because you want to implement a container object that has different performance characteristics than the standard objects. Because your application needs specific string behavior that std::string does not provide. Because you are writing the STL. Because you want to learn how to write the STL. Because you are [EA](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2271.html). Because this is C++, and you are allowed to manage memory yourself.
* To add to that, even if you exclusively use standard library, like `std::advance` with iterators instead of pointer arithmetic, you can still run into analogous cases of undefined behavior: &lt;http://stackoverflow.com/questions/1057724/what-if-i-increment-an-iterator-by-2-when-it-points-onto-the-last-element-of-a-v&gt; * Same goes for comparisons: http://stackoverflow.com/questions/4657513/comparing-iterators-from-different-containers
&gt; Grave accents (`). `This is code.` == This is code. Awesome, thanks. Auto can also cause real errors when you use it with certain kinds of code. If I have some vector x in the Eigen library and I try to use it like: `auto v = {some reduction on x}` I won't get a `bool` or a `double`, I'll get the type of the template expression for that reduction, which last time I tried it with Eigen(admittedly about half a year ago) would fail to resolve and cause a compiler error. I feel like the real strength of auto is not in its ability to deduce a type but simply in that it tells the reader that such a deduction is taking place at all.
While there are certainly times where it's just another way to shoot yourself (and everyone who reads your code) in the foot, there are valid reasons to prefer it, as Herb illustrates. Somebody who is just learning to code would completely miss something like `int a = b + c;` being in danger of narrowing (to say nothing of overflow, which auto still won't save you from). That isn't stuff they teach in intro programming courses (at least not the ones I took), you learn about those when your code change takes down a mission critical server or causes a reliable crash in your employer's flagship product. Not that I would know from personal experience or anything. ^^_whistles_ I think the hope was that `auto` would be more of a help than a hindrance, which I think _is_ the case. But like everything in C++, developers have to know where it breaks down, and they need to know when it makes the code harder to read instead of more robust/portable. That can be a hard line to find even after you've been coding for many years.
I've been looking through the JUCE documentation. It looks like just another bog standard GUI/media library like a zillion others. But I also noticed that there's no breakdown into modules: just one big list of classes and functions, with no indication of which module any particular one belongs to. So putting the "core module" under a more open license is a bit of a cheat, since there doesn't seem to be any way to tell whether any particular feature belongs to the core module or one of the other GPL modules. Also, a thread in the forum revealed that JUCE is only usable for GUI apps, not command line apps, so I'm afraid it would be no use to me anyway, regardless of what other features or licenses it has. 
I assume that the core probably includes the reinvented array, hash table, string, etc, which doenst work with for loops(or BOOST_FOREACH) or c++ algorithms. Im not sure what else it includes, but i dont see what advantage it would have over using c++/boost/qt.
Out of pure curiosity: 8-year old original Dev-C++, wxDev-C++ or (unofficial) Orwell Dev-C++? Don't know how to help you thought. Did you try asking on Stackoverflow?
I got it from [bloodshed.net](http://www.bloodshed.net/dev/devcpp.html). I believe it's the original but I'm not sure. Honestly, I'm studying sound design for video games, and I've been told that C++ will be used in the Unreal 4 engine, so I decided that I need to learn it. I have no previous programming knowledge though, so Stackoverflow is new to me. I'll try posting there, thanks.
Upgrade ASAP, here's why and how: http://clicktobegin.net/programming/why-you-shouldnt-use-dev-c/ // BTW, if you're planning to do some video games on your own, SFML (Simple and Fast Multimedia Library) will be *much* more useful to you: http://www.sfml-dev.org/ -- it's a relatively popular library, well documented (tutorials: http://www.sfml-dev.org/tutorials.php -- always use the newest version), and easy to install (binaries available right out of the box: http://www.sfml-dev.org/download.php) -- feel free to also ask on http://en.sfml-dev.org/forums/ if you need help or anything :-) In general, there are a lot of resources if you need help with C++ (a very popular language, after all) and related aspects -- so much, in fact, that you've got to separate the wheat from the chaff; here's an overview: http://www.reddit.com/r/cpp/comments/1786vs/what_are_some_good_sites_for_solutions_to/c8341aq I can also recommend the following reference: http://en.cppreference.com/w/cpp // It may also be worth to drop by http://gamedev.net/ and ask gamedev-specific questions on http://www.gamedev.net/index -- although there's also a [Game Development Stack Exchange](http://gamedev.stackexchange.com/) it's not frequented that much at the moment Note that things like {Code::Blocks, Dev-C++, Orwell Dev-C++, wxDev-C++, and Microsoft Visual Studio} are [integrated development environments (IDEs)](http://en.wikipedia.org/wiki/Integrated_development_environment) -- while things like {GCC, Clang, Microsoft Visual C++ compiler} are compilers. The *integrated* word in "integrated development environments" means that a lot more than a mere compiler is included -- e.g., in addition to compiler, you also get an editor, a debugger, often also a profiler, help system, etc. For instance, "Microsoft Visual C++ compiler" (as the name suggests, a compiler) is included in "Microsoft Visual Studio" (an IDE). Similarly, some version of GCC (compiler -- or rather compilers, see below) is included included in *Dev-C++ (IDE). I said "some version", since GCC (where "GCC" stands for the "GNU Compiler Collection" and includes compilers like `gcc` for C and `g++` for C++) on Windows is again available through multiple choices, like MinGW (this is one way to get GCC on Windows) or MinGW-w64 (this is also a way to get GCC on Windows). See: http://mingw.org/ ; http://mingw-w64.sf.net/ ; http://nuwen.net/mingw.html ; http://josuegomes.com/mingw.php In general, there are a lot of choices, see http://isocpp.org/get-started -- this can be overwhelming at first, but it can also be a good thing (some healthy "competition", like that between GCC and Clang, leads to improvements like better quality, friendlier error messages, etc.). Some available choices to just get FLTK on Windows up and running quickly: * Note that you can use Visual Studio 2010 on Windows XP -- although admittedly this is NOT the newest or really recommended edition (to be clear, however: it's absolutely, definitely, without a shadow of a doubt a superior choice to the out-of-date and un-maintained BloodShed's Dev-C++, which is even older and worse in every conceivable aspect). Here's a freely available Express Edition: http://www.microsoft.com/visualstudio/eng/products/visual-studio-2010-express If you decide to use that, follow this: http://stackoverflow.com/questions/11418721/fltk-1-1-10-with-microsoft-visual-c-2010-express * [Code::Blocks](http://www.codeblocks.org/) -- follow these notes: https://sites.google.com/site/bauyt008/code--blocks-tips--tricks-and-tutorials/c-fltk * MinGW-w64 -- http://mingw-w64.sourceforge.net/ -- you can try using binaries from here: http://www.drangon.org/mingw/ * You may also consider using binaries from here -- http://code.google.com/p/fltkwinbin/ -- Stroustrup's book uses FLTK-1.1.9, so it shouldn't be dramatically different.
Thank you. All of that was really useful and informative. I'm more focused on making games as a part of a team. My school had a presentation from a lead sound designer who had worked at Lucasarts before it was disbanded, and he told me that sound engineers were the most in demand for the gaming industry. My thought is that since we work in Unreal here (UDK actually since we don't pay for the licenses), C++ is a good one to start with. I'm glad that I can use Visual Studio because I wanted to stay as close to Stroustrop's book as possible. Thanks again, it's nice to see the cpp subreddit community helping me already :)
It's JUCE, not JUICE. It's Qt, not QT. QT is for QuickTime. Signals And Slots are not the same thing as std::function and bind. Please get your facts straight and then we can talk. 
No, pointer math that wraps around is not undefined behavior. A 32-bit integer with the value of 0xFFFFFFFF, when added 1, will become 0. it's not undefined at all. 
To consider producing an oveflow 'undefined behavior' and then removing such an expression is just stupid. Overflowing an integer is a completely defined operation. 'Undefined behavior' should mean that something cannot be defined, not that something 'should not be defined.' 
And pointer value isn't an integer somehow?
No... The library's very explicitly modularised, and if you browse the source-code and see how it's laid out that's very obvious. I guess that if you only look at the online doxygen-generated pages that's not so clear, as those files were just generated by scanning everything. And yes, of course you can write command-line apps with it! There's even at least one command-line app in the examples. It's because of requests from people who want to use the core module for BSD-licensed server-side work that I made this licensing change!
No, those things are "Pre-invented" rather than "re-invented". Juce was started back in 2001-ish, before boost was a thing, and when even the basic standard c++ classes weren't something you could trust for cross-platform work. So just like the Qt guys, I had to invent some basic wheels to get things rolling. If I sat down to write a library today, obviously I'd use all c++11 containers, but there's no sense in criticising a mature project for keeping old classes that its users rely on! Anyway.. the core module actually contains quite a lot of stuff, not just a few container classes. It has threading, locking, atomics, XML/JSON parsing, cross-platform timers, process management, socket and pipe abstractions, file and networking utilities, zip compression, etc.
Not going to argue about all these points, but really need to correct you on the memory management point: The library does of course use modern C++ ownership patterns, and I bang-on about RAII and good practice endlessly to anyone who'll listen.. Years ago, the GUI generation tool did emit those awful "deleteAndZero" calls, but that's long-since deprecated, and if any instances of it are still hanging around in the demo code, my apologies for giving the wrong impression! I'm very aware that the example code is really old and badly needs rewriting! EDIT: Oh.. and re: signals/slots: as soon as enough of my users have moved to c++11 compilers and I can start using lambdas, that'll be on my to-do-list.
If you're doing audio in C++, you really should be familiar with www.juce.com
Doesn't work with for loops? All it needs is a iterator begin() and iterator end() implementation.
Yes, all the container classes have standard iterators. You can use them in c++11 range-based-for loops, etc.
&gt; The reason I chose dev-c++ is my OS is Windows XP, so Microsoft Visual Studio is not compatible with my computer. I found dev-c++ on Stroustrop's site, but I'm not attached to this particular compiler if there's another one that may work better on XP. I strongly recommend you get some new tools. You don't have to pay for anything; take a look at a linux distro that can be run from a live disk or USB drive. At least this way your not going to be fighting an uphill battle to get things to work on an 12year old OS. Furthermore you can use a compiler that is actually user friendly like a modern version of gcc or clang++.
So... g++ version 4.6.3 as a front-end, with dragonegg and LLVM version 3.1 needs more work on its -O3 optimizations? Fair enough. Although I'm pretty sure this was known already.
Can someone clarify what I can do with the "core module" and what can/can't be done without the "other modules"? 
Quickest way to see what the core module contains is just to get the tree and look inside juce/modules/juce_core. It's all nicely laid-out and easy to browse around and see what's going on. Or open any of the example projects in your favourite IDE and look in the "Juce Modules/juce_core" group. The juce_core module has no dependencies on other modules. (All the other modules do depend on juce_core though). So basically anything inside juce_core is ISC licensed now. 
Correct; it's a pointer, not an integer.
What undefined behavior "should mean" to you doesn't really matter. What matter is what undefined behavior is specified as. What's more, if you look at the specification for 3.4.3, it points out exactly the case you are arguing against: 3.4.3 1 undeﬁned behavior behavior, upon use of a nonportable or erroneous program construct or of erroneous data, for which this International Standard imposes no requirements 2 NOTE Possible undeﬁned behavior ranges from ignoring the situation completely with unpredictable results, to behaving during translation or program execution in a documented manner characteristic of the environment (with or without the issuance of a diagnostic message), to terminating a translation or execution (with the issuance of a diagnostic message). 3 EXAMPLE An example of undeﬁned behavior is the behavior on integer overﬂow.
This doesn't seem obvious from the documentation.
&gt; No, those things are "Pre-invented" rather than "re-invented". Juce was started back in 2001-ish, before boost was a thing. There was boost in 2001, which already had smart pointer, bind, function, regex, iterator adaptors, and much more. &gt; the basic standard c++ classes weren't something you could trust for cross-platform work. So just like the Qt guys, I had to invent some basic wheels to get things rolling. You could of implemented at least most of the c++ classes instead of rolling your own. &gt; If I sat down to write a library today, obviously I'd use all c++11 containers, but there's no sense in criticising a mature project for keeping old classes that its users rely on! But its not just that. It appears in the docs that theses classes don't work with C++ for loops and C++ algorithms. &gt; Anyway.. the core module actually contains quite a lot of stuff, not just a few container classes. It has threading, locking, atomics, XML/JSON parsing, cross-platform timers, process management, socket and pipe abstractions, file and networking utilities, zip compression, etc. Which is not clear in the docs. Also, I can get this same functionality in C++/boost. In many ways it seems this is better. Just briefly looking at juces, the thread class requires creating a class rather than just instantiate an object, to start a thread. The timer class is the same way, I need to create a class rather than just an object. Also the thread pool requires heap allocation to start a new job. The stream classes, don't seem to work with C++ iostreams(or C++ iterators either); they dont use the stream operators, and they use non-polymorhpic overloads as well(such as writeFloat, writeDouble, instead of just write). The sockets can't be used as a C++ iostream, and it doesn't provide async io. This is what it appears from briefly looking at the docs. I could be wrong
If you really want to discuss nitty-gritty like that, the juce forum would be the place to do it. I don't have time to defend all these points here, I'm afraid..
WTF license model are you talking about? a 32-bit integer with the value FFFFFFFF, when added 1, will result in 0, period. 
Is it my fault that the standard is wrong? If this is undefined behavior: ptr + index &lt; ptr then this should also be undefined behavior: *ptr = 5; Because if ptr is FFFFFFFF and index is 1, in both cases, the behavior is 'undefined', for the canonical definition of undefined. But the compiler only takes out the first case and not the second, which shows how much wrong the definition of 'undefined' is. 
No, a pointer is an integer that represents a memory address. It's still an integer. 
Well ok, fair enough, but your code will still lag behind Qt in usability. Your library has a lot of unnecessary complexity. 
&gt; WTF license model are you talking about? RTFA: &gt; the total license model allows compiler writers to treat any possible undefined behavior as a “can’t-happen” condition, permitting aggressive optimizations. &gt; Compilers that implement the total license model are becoming increasingly common; such compilers have the latitude to change how they compile undefined behaviors at any time, and they frequently do so to further optimize code. Consequently, the results produced by code that depends on undefined behaviors can change arbitrarily without notice. As for: &gt; a 32-bit integer with the value FFFFFFFF, when added 1, will result in 0, period. No, it won't for signed integers. Including 32-bit signed integers. "Wrapping around" means you're assuming two's complement. Newsflash: as far as standard C or standard C++ are concerned, you're alone in your assumption. See, e.g., GCC. Or, for that matter, see all of the other examples in my last comment. Or, for that matter, see the standards: "In the C99 standard this is in section 6.5. In the C++98 standard it is in section 5 [expr], paragraph 5." No one on the planet cares a tiniest bit about what you (wrongly) think happens on your particular compiler or what you (wrongly) think may happen. It's completely irrelevant to this discussion, in fact it's so completely unrelated I'm not sure why are you even posting this. All of the examples of everyone else's experience show your "experience" is isolated to you (and is incorrect anyway). Period. 
Show me a modern mainstream cpu that does not have integers that are two's complement. 
The problem is manyfold: 1) firstly, the definition of 'undefined behavIor': we know very well what happens when an integer overflows, either in two's or one's complement model. So, it's not undefined, in any definition of undefined. 2) secondly, the definition of 'undefined' is completely arbitrary. For example, the following code: *(unsigned long *)0xffffffff = 5; is not considered undefined behavior, although it shares many problems with integer overflow. 3) in any case, removing code without notifying the user because it is illegal is extremely stupid. Illegal code should be an error, not silently removed. 
I think you forgot to add "IMHO" at the end of that comment.
I'm not even going to address the typo - from context it's pretty easy to work out! In my comment I literally explained that I _don't_ think signals and slots are the same as listeners and std::function - read the last sentence. We could probably have a decent conversation about this if you were capable of reading a viewpoint which contradicts your own without getting all neckbeardy. 
I am relativly sure that the second is in fact undefined behaviour too. Even if it isn't, dereferencing ptr after that is.
CUDA vs OpenCL choice is simple: If you are doing it for yourself/your company (and you can run CUDA), or if you are providing the full solution (such as the machines to run the system, etc) - Use CUDA. It's nicer, easier and slightly faster, especially for non-common problems. If you are writing something for wider use, you don't want to limit yourself to NVIDIA. Use OpenCL.
C++11 added shrink_to_fit(), replacing the obnoxious C++98/03 idiom here.
No problem with that, shoot. 
No, it's not a matter of opinion: your library has a lot of unecessary complexity. My apologies if that hurts your feelings. If it makes you feel better, full kudos from me on the effort. 
I recommend you to forget DevCpp and go with Qt Creator, it's free and it works in all 3 main SOs (and more)
will the slides be available at one point?
I really love Bjarne's talks. I think he is a very good speaker and obviously he is very knowledgeable.
That's not SQL. More akin to LINQ really, but not that either.
Wait. Wasn't it SUtter who a couple weeks ago was telling me that no, C++11 is not the future after all, and that I should instead wait for C++14? Or was it C++17. I've lost track. But then I do GLEEFULLY admit that I no longer care. 
Why would anyone care about C++ when you have golang?
Because Go throws out the window lots of nice features of modern languages, including exceptions, strong type enums, generic programming, ...
If you don't care about C++ then why the fuck are you subscribed to this subreddit, troll?
I agree--but I find this really frustrating too! He really knows his language, and it makes me wonder if he knows how "average joe" programmers feel about it? Like whether he understands and can empathize with our complaints about C++? He just seems to be so many levels higher than me.
I think it's hard to be him when it comes to C++. There's a whole committee which needs to be satisfied with what is being made with the language and coming from Denmark I bet he finds all the bureaucracy tiresome and hard to navigate. 
Is this the release with full C++11 support?
The comittee isn't half the burden as is the volumes of existing C++ code that they don't want demolished via language incompatibilities. I think the current comittee is extremely proactive at this point ( after a bit of a nap through the late 90s and early 00s ). They are willing to make major changes, but only with minimal chance to kill existing code. 
At the language-level, yes; the compiler is fully C++11 compliant. However, my understanding is that the standard is not yet C++11 complete.
[libc++](http://libcxx.llvm.org/) is 100% C++11 compliant, on OS X if not Linux. libstdc++ is not yet complete.
On the web site it says: libc++ is a 100% complete C++11 implementation on Apple's OS X. No current information about it for linux and windows. The logs about their test suites are both &lt; clang-3.2 and are missing dates. 
If I was you, I'd ask me to explain which bits have unnecessary complexity. But you most probably consider yourself an internet star, and you would not talk to a mr unknown, would you? After all, you created the JUCE library, how dare I criticize it. 
I get criticised most days by a whole bunch of smart people, and I love it. Without criticism I wouldn't improve my work, learn new things, or make any progress. One of my favourite things is to find something in the library that needs cleaning-up or refactoring! You're clearly clever, and maybe you've got some good insights to share. But if you just fire off a load of arrogant and sneery posts, it doesn't make me want to pursue a conversation with you. I know many people who are both smart *and* friendly, and I'd rather get my advice from them.
I'm always keen to add to the docs where they're lacking... Perhaps if I added a note about this to the main class description for classes like Array? Would that have helped you find it?
What exactly did I say that wasn't friendly? you attacked me first. I didn't say anything unfriendly prior to your very offensive reply.
Nice! I'm especially eager to play with the Android support included in this release.
This isn't a very good list. * WTF is K&amp;R doing there?? * POSIX Threads are only used by some OS's. C++11 threads have a better interface, are portable, and there's a boost version that's as complete as can be with C++03. * Bjarne's books aren't really that great. Frankly I've not really looked at TC++PL since I tried to learn from it; something that didn't go well BTW. * Where are the Exceptional books? * Modern C++ Design? TMP? * The standard is more useful, at least to me, than Design and Evolution. I've not read the book so not sure why one would think it's a must have. Inside the C++ Object Model would probably be more useful. * C++ Templates? I could go on and on probably. This list doesn't impress me as a good one.
I need Qt Quick Controls so bad. I have a 5.0 project I'm sitting on waiting for the official 5.1 release so I can replace all the rect buttons and other nonsense with the controls versions so I get native looking apps finally. Qml is great up until you realize your program looks nothing like native OS the way you expect a traditional widget app to look.
Still no information about modules support, although one of the WWDC 2013 talks was about them in the upcoming XCode 5. Still waiting for the videos to become available for download.
Does Android support means being able to deploy C++ native apps to Google Play Store ? Does they run through the Dalvik VM ? 
Maybe you thought you were being friendly, but unfortunately your posts all just sound offhand and condescending to me.
I am fully aware of that, as I follow the public discussion of them. That said, they aren't experimental in what concerns C and Objective-C support, because they were announced as **done** at WWDC 2013 last week. Since I lack a Mac OS X environment, I am still waiting for the videos to become available for download. If you have access to Mac OS X, you can watch the videos already via Safari.
Does LLVM even work in windows? Last time I looked it seemed to a 'would be nice' feature.
From the video pjmlp referenced the feature is going to be a part of Objective-C, supported by the next version of the OS X and iOS SDKs. So on that front at least modules are beyond the 'proof-of-concept' phase. I'll be interested to see how the C++ standardization of modules interacts with modules as implemented and deployed for C and Objective-C in clang. Will the C++ committee want to add features that make C++ modules unusable as a language feature in C? Will the C committee accept a compatible modules feature? Would clang implement two incompatible module features?
Or just use a vector of smart pointers :)
http://qt-project.org/wiki/QtDesktopComponents QtQuick Desktop components have been around for over 2 years now, and works all the way back in QtQuick 1.0. They are really neat in that the custom QStyleItem used internally paints using the default system qt styles. Overriding the behaviour of desktop components is a cinch as well (i.e. making spinboxes more touch friendly etc.).
"Hallelujah" is a little strong, but the new edition of Stroustrup is very good.
Of the countless opportunities for library vendors to be annoying/unhelpful (intentionally or otherwise) in their choice of implementation while still following the strict definition of the standard, this particular occasion is probably not one that you should spend too much time worrying about.
&gt; because they were announced as **done** at WWDC 2013 last week. It was snuck in at the end with no elaboration that developers won't actually be able to create modules, only use the system frameworks that are shipping as modules as such. So presumably they're not really viewed as stable yet.
Delphi... LMAO
Ah, many thanks for the clarification.
Sweet, I wonder if I can get a build system using qtcreator, qmake and llvm that works everywhere since thats my setup on linux atm which is pretty solid so far.
Would you mind putting a list up that you think would be right?
Yes. Got negged and now I'm butt-hurt. Don't really feel like going out of my way giving an opinion if people are just going to be dicks about it. Maybe I'll blog about it later. In the meantime you can already see quite a few in my post above.
I believe it uses the NDK but I'm not sure what further is needed for integration with Google Play Store.
Based on the implementation available in clang I'd guess that this is due to the fact that the current implementation uses what Doug Gregor has termed the 'transitional' model. Users will be able to create modules using the same 'transitional' module.map files but that users won't be _expected_ to do this extra work. Eventually I'm sure clang will implement some version of what Doug called the 'futuristic' version of modules and then they'll tell users and third parties that new frameworks should be modularized. You can find Doug's presentation on C++ modules [on the llvm site](http://llvm.org/devmtg/2012-11/)
"C Cruft Cruft" :)
Maybe your both wrong? ( 3rd party not taking sides)
I build and run clang on Windows using cygwin. Works great.
I am no C++ expert, but I know the language since the CFront days and after a year of C around 1993, jumped straight into C++. So maybe I am biased, but my issue is that most of the "average joe" programmers I know can't even deal properly with languages like Java.
You really need me to help you with that?? Ok.. &gt; No, it's not a matter of opinion: your library has a lot of unecessary complexity. &gt; My apologies if that hurts your feelings. If it makes you feel better, full kudos from me on the effort. I love it! My apologies if this hurts your feelings, but that post really is pure Sheldon Cooper gold! And if you genuinely can't understand why anyone would find it annoying (and hilarious!) to be spoken to in that way, then the Sheldon Cooper comparison is even more appropriate!
Not sure it's the sentence structure as much as aimless wondering around the subject. I read the article and still have no idea what the intent was - A review/critique of the book? Was it a rant? A musing on the complex topic of C++? I just don't know
That reads like a good list. What do you think is the impact of c++11 on these books, do you think they will lose relevance or is most teached in theses books version independent?
It's written in a very conversational, very British style. I quite enjoyed it.
I can't seem to be able to extract anything meaningful out of this talk except that maybe Lippman has gotten senile and bitter.
I genuinely apologized for hurting your feelings regarding my negative comments on your library. I don't see why you find that annoying. In fact, I said that out of being kind to you, because I posted a lot of negative comments on your library. Care to explain why you find it annoying then? 
Sigh.. Enough now, this thread is getting silly! I'm off to do something more useful with my time, and so should you!
Of course, in the case of using smart pointers both smart pointers and the objects they point are preferably in the CPUs cache, which means less cache space for objects. Though this is only a significant problem when the objects are relatively small, no?
Probably all that you have listed and more.
I know that, but that doesn't really answer my question ;)
I think 19 years with a language would qualify you as an expert. Though on your latter problem, there are 2 completely different aspects to "dealing" with a language - understanding the syntax and eccentricities (like constructor initialization being in the order variables are declared) or being able to comprehend complex software interactions (this function does what? Why is this import here, what is this object being passed around for, how does this container work?). The latter is software design, and even in the world of Java the design of some template libraries could make little sense. For example of a design barrier in C++, consider std::hash. A novice assumes a hash *function* is implemented as a template specialization of such a function, but in practice std::hash is a class template with operator() overloaded to return the hash value, so you have to declare a hash object of the form std::hash&lt;T&gt; hasher after having defined the template specialization of std::hash for that type. You ask "why a class / struct rather than a function template?" and compound that with a few layers of complexity and a novice can get lost, I expect even in Java (especially in Java, where all your preferred-function templates have to be class templates). That, versus say, understanding RAII, or that you need to end your class definitions with a semicolon, which is just language knowledge.
Yes! More information please!
C++ native apps are allowed on Google Play Store, they were never forbiden as long as you use what the NDK offers and not your own hacks. Yes, they are compiled into shared objects that are loaded via Dalvik. There is a Java Activity that wraps them and uses UNIX pipes for IO events. There are no pure executables in Android.
Agreed, tho I did notice the unordered_*&lt;&gt; types have no shrink_to_fit() equivalent. Unshrunken (sp?) hash tables *have* bitten me in the past.
&gt; Isn't a shared_ptr 8 bytes (16 in the case of 64bits). Yes. &gt; The reference counters are shared between all instances of shared_ptr, so is it fair to count them to the actual size of shared_ptr? Yes. We're comparing a vector&lt;T&gt; versus vector&lt;shared_ptr&lt;T&gt;&gt; where the shared_ptrs aren't copies.
I think this article was more referring to libraries that encapsulate the raw CUDA and OpenCL calls, making both easier to work without...without introducing performance degradation. 
True, because Android API is pure Java, as such unless you are doing a game, you need to be doing JNI all the time if you want ot interact with Android APIs, which leads to performance drop due to marshalling/unmarshalling of data between Java and C land. Even the new game APIs announced at Google IO 2013 are Java only and they clearly stated it is up for developers themselves or third parties to wrap them for NDK based applications. http://www.youtube.com/watch?v=L9vI0w263Xk&amp;list=PLWz5rJ2EKKc9WGUwq2gQ-coU3fSyexgOx&amp;index=29 As I mentioned in the previous post, &gt; The only issue is that Google sees it as a way to port legacy code, optimize certain hot paths or for games. Contrary to what people think, C++ does not enjoy the same level of support on Android as it does on iOS, WP8, Blackberry.
Thanks for the tip! Expression SFINAE seems to cover all of my use cases and indeed works better (at all) when confronted with overloaded/templated functions. Though it may force implementation details to leak in the expression inside of the `decltype`'s context. I'll update the answer when I get home, thanks again. edit: edited, I think it's more correct now. You're also right about lambdas being convertible to plain function pointers in VC11, I had my test project opened in VS2010 mode in my VS2012. That being said `if([]{})` still results in an error (both with and without the CTP), but I'm not sure whether gcc's or VC's behaviour is the correct one (or maybe they both are)?.
Have you looked at Boost.Dispatch there seems to be a lot of overlap here.
Boost has an answer to this question, the new TTI (type traits introspection) library, part of the upcoming Boost 1.54 Here's the (beta) docs: http://www.boost.org/doc/libs/1_54_0_beta1/libs/tti/doc/html/index.html
Thanks, although it (by design, not a mistake, I assume) [doesn't return what I want](https://gist.github.com/KrzaQ/5821926) when inheritance is involved. Or I might've misread the docs.
That doesn't compile because of VC's support for non-Standard calling conventions. We made lambdas "omniconvertible" to function pointers with arbitrary calling conventions. That's desirable when passing a lambda to, say, a Windows function that takes a stdcall function pointer - but it breaks the case you're trying. The Standard says that directly testing a lambda should perform a UDC to function pointer, then a standard conversion to bool. In VC, it's ambiguous because lambdas have multiple UDCs to function pointers.
Why would one want to test a lambda for "non-null"? Are there any valid use cases? 
Our scope guard implementation was written with `boost::function&lt;void()&gt;` in mind, which is testable - and it was tested before execution (think `if(this-&gt;isValid &amp;&amp; this-&gt;f) this-&gt;f();`). The class itself is a template and pretty much everything we threw at it worked - that is until we tried compiling with VC10. 
In retrospect, the language probably would've been far better off if *implicit* was the C++ keyword rather than *explicit*. Oh well, you live and you learn...
Thanks for explaining the reason behind that. Can we expect this to change in VC 12 (or however the compiler is called in VS2013)?
I had totally forgotten about AMP. Does anyone know how well Intel's clang implementation of it is working? I'd prefer full C++11 support while I mess around with it.
I always did it to avoid implicit conversions. Although I never found issues with explicit, I surely had issues with other features across compilers.
Being unable to boolean-test lambdas is a compiler bug, but it's incredibly minor. This would rank as extremely low priority to fix.
Function pointers and std::functions are testable, but most functors are not (e.g. std::less is not testable).
Could somebody explain why `constexpr` is needed in the first place? Couldn't the compiler deduce that information by itself? Or is that one of those cases where the compiler could do it but there is no proper module system to keep track of that information?
I also consider this to be suboptimal. Id rather have a feature that requests computation to be performed in compile time at evaluation place, producing ill formed program if the compiler finds that rules now required for constexpr function do not hold for the expression (and its components). Eg sth like: int foo() { return 4; } int bar() { return rand(); } int arr0[foo()]; //error, not ICE int arr1[constexpr(foo())]; //ok int arr2[constexpr(bar())]; //error, bar() cant be constexpr But im sure there are drawbacks to this too.
you're pretty much bang on its not needed as such however, its a nice maintainable way of expressing compile time constants using expressive type rich c++ semantics normally reserved for regular code without any run-time overhead.
That was the original idea. From [N2235](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2235.pdf) &amp;sect;4.5: &gt; The original proposal [DR03] for generalizing constant expressions did not introduce a new keyword to distinguish constant-expression functions from others. That proposal relied on the compiler recognizing such functions being simple enough for use in constant expression. However, during discussions in the Evolution Groupat the Kona meeting (October 2003), the consensus was that we needed syntactic marker. Given that (our proposed `constexpr`), a programmer can state that a function is intended to be used in a constant expression and the compiler can diagnose mistakes. We considered this in conjunction with the user-defined literal and initializer-list proposals [Str03, SDR05]. At the Mont Tremblant meeting (October 2005), the Evolution Group agreed on the new declaration specifier `constexpr`, for defining constant-expression functions and constants of user-defined types. I guess the idea was that with plain `const` if you're not careful, you might get unintended behavior. In a world without explicit `constexpr`, you can't look at this and know for sure that the value is computed at compile time: const int foo = bar(); If `bar()` is not a constant-expression function, then this is dynamic initialization which takes place at process startup alongside static object construction. Presumably you might have reasons for wanting that to happen at compile time (performance, read only memory, etc.) and you'd want an error telling you that you need to look at `bar()` and fix it to be a constant expression. With: constexpr int foo = bar(); ...you get that enforcement: the right hand side must be a compile-time constant expression, and if it's not for some reason you get an error. 
The redundant work in the constructor combined with the non-scalable mutex around the global type_info table makes me think this design needs a little work. Also, the type hierarchy had better not be too deep nor wide or you'll encounter integer overflow problems (never say never). In the end, if the performance of dynamic_cast&lt;&gt; were the limiting factor in my design (which I *really* hope is not ever the case), I think I would look for a way to avoid polymorphic casting in the first place before looking for a faster dynamic_cast&lt;&gt;.
Wow a *reinterpreter_cast&lt;&gt;*, what about multiple inheritance, virtual inheritance?
I think I'm a bit late to the party here, but I've seen this come up pretty often after looking for a solution myself a few weeks ago. I posted this: http://stackoverflow.com/questions/16991942/how-do-i-make-a-class-that-only-compiles-when-its-type-has-a-certain-member-func/16993794#16993794 as a possible solution. The thesis is that while googling will turn up several ways of doing this, they're not particularly clean or simple. If the problem is approached from a different angle, it can be quite easy: define any special cases for which a class does not have a member function and then just use normal templating to assume it does. The compiler will throw an error if it's missing.
Very good use of user-defined literals
The keyword is required to allow separating the declaration and definition of a function because the compiler sees only one file at a time.
That's totally irrelevant as one of the requirements of a constant-expression function is that its definition must have already been seen in the current translation unit. There is no such thing as a separately-compiled constant-expression function; they are implicitly inline. 
Anyone has a link to the slides?
Not yet, but watch this space: http://nwcpp.org/june-2013.html :-)
Thanks.
*nitpick mode on*: &gt; The (somewhat unusual) rules of C++ dictate that when picking a member function on a temporary class object, the non-const is preferred to the const one =&gt; Not sure what is unusual about it. A temporary may or may not be const. If it's not, C++ better pick the non-const overload. &gt; I said that the rules of best function matching are unusual, because they are quite the opposite when we are picking the best non-member function for a temporary object. In that case we prefer const lvalue reference to non-const one: [...] Therefore the second, non-const overload is picked in line 6. =&gt; First it's claimed the const lvalue would be taken, but the very next example picks the non-const non-member function. Also the non-member function is not called on a temporary but on a global variable.
Nothing that actually makes sense (you can put `has_size_method` as `B`'s nested type, but obviously it's not a viable solution). Though I'm not sure whether there actually is a use-case that would require it.
Thanks, though not creating hard errors was one of the goals here - it's always easier to throw a `static_assert` or `enable_if` on our type trait than recover from/avoid a hard error.
Good article but &gt; Keeping units straight is something that we shouldn’t have to worry about anymore in 2013, but for some reason [it’s still a source of very real and very serious bugs in even the most carefully crafted systems](http://en.wikipedia.org/wiki/Mars_Climate_Orbiter#Cause_of_failure). No no no, the problem here, while yes units related, wouldn't have been solved by this.The Mars surveyor problem was related to separate teams being overworked, and a major miscommunication. Please sell your work on it's own merits.
I ran into this once with an internal project; my solution was to cache the dynamic_cast results by a vtable to vtable key in a concurrent map. Worked out well enough, no change to existing classes needed.
The Boost units library was made for this exact reason. Hell of a learning curve but worth it.
You're actually right. It wouldn't work any other way. I checked the numeric_limits header in Xcode 5 the other day to see if they added `constexpr` support and believed to remember there were only function declarations.
Passing by const-ref for values that fit into a register can be optimized. I don't think there's any real reason to prefer copy over const-ref.
Are you sure? I've always been told to pass small objects (including primals) by value. 
How widely used is this? Looks interesting, kindof a hodgepodge of stuff... like a gnu science library with some more modern topics.
exactly. I like its licensing (boost software license), but for most of its uses I already have a good library I can use. Also not using boost (smart ptr f.e.) makes me wonder. Their who uses dlib page is a joke: http://sourceforge.net/apps/mediawiki/dclib/index.php?title=Known_users 2 companies and a student? orly?
I'm the author. I would say that adding a dependency on boost just to get something as minor as smart_ptr is kinda obnoxious :) Yes, the "who uses this?" page is pretty neglected. However, there are lots of [publications that cite dlib](http://scholar.google.com/scholar?q=%22dlib.net%22+OR+%22dlib-ml%22+OR+%22dclib.sourceforge.net%22). I also know about a bunch of commercial projects that use it but I don't feel that it's my place to add their names to the wiki.
That's awesome.
I'd like to see some comparison of pass-by-reference vs value when calling a function that can't/hasn't been inlined (eg where its in another translation unit); that would give a better indication on the impact of this choice in a larger project. Nice work anyway, I've always wanted to try making some "unit" classes.
Looks nice. Looks like the idea from Bjarnes keynote on C++11. If you don't mind me asking, what does constexpr Meter operator"" do?
It's a new C++11 feature which is described here: http://en.cppreference.com/w/cpp/language/user_literal --- So, if I create a function: class Object; constexpr Object operator"" _obj(long double deg); Then I could use it like this: Object x = 90.0_deg; Which would call the above function. Unit types is really the most obvious use case for this. It allows you to create things like `5_kilometers` and `1000_meters` and easily convert between them with some type safety.
That's exactly where I got the idea! And it's the new C++11 literal operator. Here's a good explanation of what it does: http://akrzemi1.wordpress.com/2012/08/12/user-defined-literals-part-i/
Relevant: [Chandler Carruth: Optimizing the Emergent Structures of C++](http://www.youtube.com/watch?v=eR34r7HOU14&amp;feature=player_detailpage#t=2922s) The link above jumps to the quick summary of the talk; but you probably should watch the whole thing if you're interested in it.
Yeah. The author posted a [followup](http://www.reddit.com/r/cpp/comments/1gvai9/optimizing_away_small_classes_units_of_measure/) today confirming.
I just made a test for this. If a function with a const reference to float parameter has no calls which could potentially modify the referred to parameter, g++ dereferences the parameter's address once and keeps the result on a register when using it multiple times, incurring minimal cost. I could not detect any difference by timing a large number of function calls. However, passing a float by const reference instead of by value increased the size of the timing loop calling the function by 5 bytes and the called function by 1 byte, which might add up to some cache misses in a real program. I used g++ 4.7.2 20121109 (Red Hat 4.7.2-8) on x86_64 Linux with -O3 for this test.
You should watch the whole thing if you're ever going to write CPP code. If you're not interested in watching it **and** you're going to write CPP code, you should think about your life for a while.
I'll look into it. I tried earlier with a simple system of my own, but I didn't know of std::ratio then. Thanks for the advice
It's kind of difficult actually. Well not so much difficult as complicated. Should you be able to add millimeters to Kilometers? what should be the result type? millimeters, kilometers, meters? Multiplication and division aren't so bad. I already do the relevant calculations for the multiplications of dimensions. Millimeters divided by kiloseconds is easy: the result type can just be millimeters per kilosecond. How should conversion between measurements of the same unit but different prefix happen, automatically, or explicitly? The questions aren't rhetorical, I'd really appreciate some advice/suggestions, as I had a rudimentary system for this earlier, but I couldn't come to decisions on that, so I scrapped it.
No worries, I didn't interpret it that way :) I would accept a patch from someone that did anything as long as the new feature was a nice modular well thought out thing, whatever that may be. So the scope is any set of C++ tools that are modular, well documented (in the contract programming style), portable, useful, and do not complicate the build process (e.g. you shouldn't have to configure or install anything to use dlib). But that said, I like scientific computing and statistical machine learning in particular. So as long as I'm the main contributor you will continue to see more things along those lines appearing in dlib in the future.
So I admit I haven't looked at your code, but from a purely theoretical standpoint, I think I'd have a "length" object, store the value internally however makes sense to you, and then have it be able to output in km, m, mm, etc. to either an arbitrary precision (eg. "three decimal places") or a "best-effort" precision (ie. as much precision as you can fit in a float, or a double, or whatever you choose to yet). But, again, that's just my 2 cents, and I admit I'm not 100% sober at the moment...
I've tried using boost::units several times and I always run up against the issues of trying to multiply a vector (say accn) by a scalar (say mass) which seems like a natural thing to want to do.
Hi! Someone is currently asking for a downloadable file on the official website, perhaps it can be of help: &lt;http://isocpp.org/blog/2013/06/c11-the-future-is-here-bjarne-stroustrup&gt; Meanwhile, the slides are available separately: * [Slides, PDF] http://accu.org/content/conf2013/Bjarnes_Keynote.pdf This talk was also given again a couple days later at another venue, perhaps this will help: * [Video, Info] http://www.swansea.ac.uk/compsci/distinguishedlectures/bjarnestroustrup/ * [Slides, PDF] http://www.swansea.ac.uk/media/C++11%20Swansea.pdf HTH! :-)
I'd go for avoiding floating point whereever possible. Internally storing as an integer. That means you pick the internal unit based on the resolution desired. You might pick a 64-bit nanometer as your unit, that'll make everything go pretty fast, and give you enough resolution for anything physical you're likely to work with.
You have the type system why not use it, is "90.0_rad" really that bad? It also requires less documentation on the functions that recieve the parameters.
I could do that actually. I'd like to do that in the near future. The only thing I'm worried about is how to convert between the two systems, or if it should be allowed at all.
How is a singleton class better than a namespace with functions? Also, what is a "scoped singleton"?
The initialisation order of global objects is non-deterministic. That would include globals in a namespace. The usual solution to that is an initialise-on-demand singleton object. This is good when you have multiple singleton globals that are interdepedent. The initialise-on-demand system will ensure that they will self-order.
You seem confused. A singleton doesn't have a public constructor. It has a static instance() method that returns the one and only instance of itself which, by nature, has global scope. What on earth is your understanding of a singleton?
That's quite better than my original counter-example, actually, that would be a perfect solution if you could command destruction of the instance.
A scoped singleton uses the [RAII](http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization) idiom. You do not have to take care of cleaning up your singletons manually by calling *destroy*. In the sample code the singletons are freed automatically when the receipt goes out of scope (leaves the code block) guaranteeing no memory leaks. This also gives you full control when your singletons are generated and destroyed. For example, for some fictitious game engine you could do something like this: std::vector&lt;receipt_ptr&gt; my_game_engine; my_game_engine.push_back(ilogger::generate&lt;logger_network&gt;()); my_game_engine.push_back(igraphics::generate&lt;graphics_opengl&gt;()); my_game_engine.push_back(iphysics::generate&lt;physics_box2d&gt;()); my_game_engine.push_back(iaudio::generate&lt;audio_miles&gt;()); When the **my_game_engine** vector goes out of scope, the whole engine gets shutdown.
What rightfold probably wanted to say was that a “scoped singleton” makes no sense – if it’s destroyed at the end of the scope, it’s not a singleton, by definition (because you can have more than one instance).
Hm, I can’t see how being destroyed at the end of the scope means it is not a singleton?
This singleton is neither simple in implementation nor in use. A better implementation would make at least usage simple. A great implementation would make both simple. I think you can do better.
That is technically true. However, my sample still behaves like a singleton such that it only allows you to have one instance of a certain type. You could change the underlying implementation. e.g. auto r = igraphics::generate&lt;graphics_opengl&gt;(); igraphics::replace&lt;graphics_dx11&gt;(r); But you still will ever have only one instance of *igraphics*. 
If you don't actually need replaceability, you can make it a fair bit simpler, no? Just construct the object in automatic storage, no need to play a `shared_ptr` dance. template &lt;class T&gt; class singleton { public: static T&amp; get() { assert(s_inst); return *s_inst; } protected: singleton() { assert(!s_inst); s_inst = static_cast&lt;T*&gt;(this); } private: static T* s_inst; }; template &lt;class T&gt; T* singleton&lt;T&gt;::s_inst;
I agree that the definition is debatable but scoped singletons would allow several instances, and that’s no longer a singleton: { auto a = my_singleton::generate(); } { auto b = my_singleton::generate(); } But actually the problem in OP’s code isn’t even scope. The following would also be allowed: { auto a = my_singleton::generate(); auto b = my_singleton::generate(); } Again, that’s not a singleton.
What prevents you from calling `igraphics::generate` twice?
You are correct in this. The sample does not correct for that case. my_singleton::get() Will always return the instance of the last called *generate*. However, you will always have this issue with any singleton class that allows for on demand initialization (unless you enforce by asserting on multiple generates). What I was exploring is a pattern that: * behaves like a singleton * allows for deterministic **initialization** and **destruction** * gives the option for replaceability * uses RAII methods for automatic cleanup 
I believe this problem can be addressed if my_singleton returns a shared reference to a common object, probably using a shared_ptr, as nexuapex suggested in another comment.
Ah, but nothing prevents you from caching the previous instance: struct x : public singleton&lt;x&gt; { int n; void print() const { std::cout &lt;&lt; n &lt;&lt; '\n'; } }; { auto a = x::generate&lt;x&gt;(); auto&amp; ac = x::get(); ac.n = 1; auto b = x::generate&lt;x&gt;(); auto&amp; bc = x::get(); bc.n = 2; ac.print(); bc.print(); } will print `1 2`. This is really something the `singleton` base should prevent, otherwise it has no advantage, and lots of disadvantages, over using the objects directly. In fact, your code, as it is at the moment, is a fancy wrapper around the following code: x a{1}; x b{2}; a.print(); b.print();
This is another version with support to variadic parameters, so you can do, although it's not scoped... Foo::getInstance("hello world"); // and then Foo::getInstance(); https://gist.github.com/bianjiang/5846512#file-singleton-hpp
WTF would you do with that vector afterwards? There is nothing you can do its contents because `receipt` does not provide any functionality. And additionally, what is wrong with creating two loggers? Or two physics simulators? Or three or seven? Does that immediately produce invalid program state? I don't showing something no one that thinks would ever do is a convincing use case.
I would just use a basic singleton pattern if it wasn't for the requirements i would like to have: * behaves like a singleton * allows for deterministic **initialization** and **destruction** * gives the option for replaceability * uses RAII methods for automatic cleanup So I guess I really shouldn't call the sample a Singleton. However, I like your version with variadic parameters. Unfortunately, only one of the two compilers I use supports it (XCode yes, Visual Studio 2012 no).
in that tiny example, the purpose of the vector is to just keep a strong reference to those objects so they are not destroyed. You would not actually use that vector for anything other that lifetime management. after you add those items to the vector, just use the static get method like a regular singleton e.g.: iaudio::get().play_sound("some_sfx.ogg"); ilogger::get().output("log message"); You are guaranteed those objects while the vector stays in scope. 
The people running this site seem to have confused having a rudimentary understanding of compiler design with a deep understanding of its programming language. I don't think this "Grandmaster Certification" is worth the cost of admission.
Wait a second, is he actually taking that certificate seriously?
I found that talk very rewarding, even though I had problems with the language since I'm not a native speaker. Are there any other good talks about C++ optimization someone wishes to share? They're not that easy to come by in my experiance.
Guh. This post is so silly. It's not really "why to quit" but rather "why not to start." All of these things were not just apparent but explicitly stated before the course started. Sorry, bro, but you're just a quitter. You went up against the grandmaster challenge and you lost. If you had problems with it that weren't foreseeable, that'd be a bit different (and would at least merit some discussion). But no, you just quit for the same reasons most people didn't start: it was too big a challenge and had no direct value. Don't try to justify your failure as an issue with the system. You just [lost](http://youtube.com/watch?v=VDW0ZnZxjn4).
Regexes don't really help very much with writing a compiler. You can't parse C++ with regexes, so really the only use is as part of a tokenizer.... which is trivial to do without regexes.
I wish he'd talk more at school ;__; Or that I could get a class from him...or that he'd accept me for PhD work...I got one handshake out of the guy, and it was at a recruiting thing for A&amp;M to make people happy. Though he did acknowledge the necessity of proprietary software which was really cool...
&gt; The latest publicly available working draft of the C++11 standard is a whopping 1314 pages long. Yes, but over half of that is the Standard Library. Clauses 1 [intro] through 16 [cpp] inclusive are "just" 407 pages. That said, C++ front-ends are still some of humanity's most complex machines. &gt; I feel like writing a compiler without some form of regular expressions I'm not an FE dev, but I can confidently state that parsing C++ is way beyond regex's capabilities. &gt; If half of all the effort spent towards this course was directed at improving clang’s documentation, or writing desperately-needed modern C++ development tools Yeah, this was my original reaction. There are so many better things to do with your time (and that's not even counting the sketchiness of the people behind this "course", who still haven't named themselves - they certainly aren't part of the C++ Implementer Conspiracy because nobody's seen them at our meetings).
It does not. VC's STL destroys them front-to-back, occasionally surprising users who expect back-to-front. It's a little weird that the STL doesn't specify reverse order like the Core Language does, but consider deque and list - they can be built up through push_front() or push_back(), so neither forward nor reverse destruction is inherently more natural than the other one.
You can see the assignments on the website posted. Plus you can look at the form which has a stackexchange type answer and question here: http://forum.cppgm.org/questions/ 
true, there is opportunity cost: participants *could* be doing something else (anything else) instead, and that would be a more worthwhile choice.
So is anyone still going?
&gt; My real pain point was not being able to use external libraries Yeah. Even given unlimited access to libraries like Boost.Spirit, writing an FE is *hard*.
I admit I probably overstated the importance of regex to the front-end. The parts I was using it for were for tokenizing and parsing literal expressions. I rewrote it without using std::regex fairly easily and eded up with what was essentially my own really limited, buggy regex implementation. It's not that things like this are hard, it's just stupid to have to redo them because Ubuntu 12.10 AMD64 (what the grading servers use) ships with an old version of libstdc++.
What would switching to C++ allow you to do in terms if memory management that C did not? 
I think OP does not have the most healthy attitude about quiting, but the CPPGM guys do not seem legit. I read the FAQ, They forbid release of the code. They set deadlines for what will be a secondary goal for most people (jobs or business I think will be most peoples goals) and they have no concept of scope. About the scope of the project. They recommend 2 years of C++... I have been coding in C++ on and off for 14 years and very in depth the past 5. I could implement most of the containers in the standard library, I might even be able to parse large swaths of the syntax. I do not think there is a single person who could actually create a standard complaint compiler in less time than it takes to make a new standard. The size of the work and the level of precision required is too great. C++ seem to be on the scale of the most complex and difficult to create things humans have ever devised. They must be taking shortcuts of some kind, perhaps not thoroughly testing completed compilers. The premise of this test is nice, but it is implemented in an idiotic fashion. I think they should structure it more like project euler. With Project Euler you get to share you code, you learn and you can do it while you have a job or run a business.
Yes that would help, but why does the begin and end functions not show up in the docs? Or perhaps im missing something?
I can't bear to read most (real word) C code because so much of it would be redundant with std::vector, list, etc. and exceptions. I really can't see the "C is simpler" argument - why not use C with containers and exceptions for errors you're probably not dealing with properly anyway?
Not having RAII made you hit a wall? I guess I just don't get it.
For one, it allows you to be able to avoid writing [code with cruft](http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/drivers/media/pci/ttpci/av7110.c?id=HEAD#n2370) like this: static int av7110_attach(struct saa7146_dev* dev, struct saa7146_pci_extension_data *pci_ext) { // ... // more cruft removed, apparently reddit doesn't like it either: "this is too long (max: 10000)" // ... tasklet_init (&amp;av7110-&gt;debi_tasklet, debiirq, (unsigned long) av7110); tasklet_init (&amp;av7110-&gt;gpio_tasklet, gpioirq, (unsigned long) av7110); mutex_init(&amp;av7110-&gt;pid_mutex); /* locks for data transfers from/to AV7110 */ spin_lock_init(&amp;av7110-&gt;debilock); mutex_init(&amp;av7110-&gt;dcomlock); av7110-&gt;debitype = -1; /* default OSD window */ av7110-&gt;osdwin = 1; mutex_init(&amp;av7110-&gt;osd_mutex); /* TV standard */ av7110-&gt;vidmode = tv_standard == 1 ? AV7110_VIDEO_MODE_NTSC : AV7110_VIDEO_MODE_PAL; /* ARM "watchdog" */ init_waitqueue_head(&amp;av7110-&gt;arm_wait); av7110-&gt;arm_thread = NULL; /* allocate and init buffers */ av7110-&gt;debi_virt = pci_alloc_consistent(pdev, 8192, &amp;av7110-&gt;debi_bus); if (!av7110-&gt;debi_virt) goto err_saa71466_vfree_4; av7110-&gt;iobuf = vmalloc(AVOUTLEN+AOUTLEN+BMPLEN+4*IPACKS); if (!av7110-&gt;iobuf) goto err_pci_free_5; ret = av7110_av_init(av7110); if (ret &lt; 0) goto err_iobuf_vfree_6; /* init BMP buffer */ av7110-&gt;bmpbuf = av7110-&gt;iobuf+AVOUTLEN+AOUTLEN; init_waitqueue_head(&amp;av7110-&gt;bmpq); ret = av7110_ca_init(av7110); if (ret &lt; 0) goto err_av7110_av_exit_7; /* load firmware into AV7110 cards */ ret = av7110_bootarm(av7110); if (ret &lt; 0) goto err_av7110_ca_exit_8; ret = av7110_firmversion(av7110); if (ret &lt; 0) goto err_stop_arm_9; if (FW_VERSION(av7110-&gt;arm_app)&lt;0x2501) printk ("dvb-ttpci: Warning, firmware version 0x%04x is too old. " "System might be unstable!\n", FW_VERSION(av7110-&gt;arm_app)); thread = kthread_run(arm_thread, (void *) av7110, "arm_mon"); if (IS_ERR(thread)) { ret = PTR_ERR(thread); goto err_stop_arm_9; } av7110-&gt;arm_thread = thread; /* set initial volume in mixer struct */ av7110-&gt;mixer.volume_left = volume; av7110-&gt;mixer.volume_right = volume; ret = av7110_register(av7110); if (ret &lt; 0) goto err_arm_thread_stop_10; init_av7110_av(av7110); /* special case DVB-C: these cards have an analog tuner plus need some special handling, so we have separate saa7146_ext_vv data for these... */ ret = av7110_init_v4l(av7110); if (ret &lt; 0) goto err_av7110_unregister_11; av7110-&gt;dvb_adapter.priv = av7110; ret = frontend_init(av7110); if (ret &lt; 0) goto err_av7110_exit_v4l_12; mutex_init(&amp;av7110-&gt;ioctl_mutex); #if IS_ENABLED(CONFIG_INPUT_EVDEV) av7110_ir_init(av7110); #endif printk(KERN_INFO "dvb-ttpci: found av7110-%d.\n", av7110_num); av7110_num++; out: return ret; err_av7110_exit_v4l_12: av7110_exit_v4l(av7110); err_av7110_unregister_11: dvb_unregister(av7110); err_arm_thread_stop_10: av7110_arm_sync(av7110); err_stop_arm_9: /* Nothing to do. Rejoice. */ err_av7110_ca_exit_8: av7110_ca_exit(av7110); err_av7110_av_exit_7: av7110_av_exit(av7110); err_iobuf_vfree_6: vfree(av7110-&gt;iobuf); err_pci_free_5: pci_free_consistent(pdev, 8192, av7110-&gt;debi_virt, av7110-&gt;debi_bus); err_saa71466_vfree_4: if (av7110-&gt;grabbing) saa7146_vfree_destroy_pgtable(pdev, av7110-&gt;grabbing, &amp;av7110-&gt;pt); err_i2c_del_3: i2c_del_adapter(&amp;av7110-&gt;i2c_adap); err_dvb_unregister_adapter_2: dvb_unregister_adapter(&amp;av7110-&gt;dvb_adapter); err_put_firmware_1: put_firmware(av7110); err_kfree_0: kfree(av7110); goto out; } 
a little bit off topic: george perec wrote a novel without the letter "e". the original french novel is called "La Disparition" and there's also a translation into english titled "A Void". how fitting...
Not a talk, but worth reading: http://www.agner.org/optimize/
Seems simple enough to do in C: { unsigned char v[n]; // allocate n bytes } // deallocate n bytes Oh wait... I forgot that Microsoft still doesn't have a C99 compliant compiler.
What cruft are you referring to? The C++ version would look almost the exact same except it would have a crap-load of try/catches instead. That code looks like it's low-level driver code where the last thing you want is to have some uncaught exception thrown accidentally or have some kind of invisible behavior happening that is hard to predict because it's hiding behind 5 layers of abstraction that only kicks in when some esoteric situation listed as a disclaimer on the bottom of page 796 of the C++ standard is triggered.
No, good use of RAII lets you ignore the goto nightmare epilogue of the function. Then you throw _one_ exception, or, if you'd like, return an error code. The important bit is that all of that cleanup is no longer explicitly needed once you write some destructors. It's not like using error codes instead of exceptions somehow makes error handling go away. I'd argue that forgetting to catch an error code is _much, much worse_, because now instead of crashing with knowledge of where and what went wrong, you stumble along in this zombie state with an undetected failure for a bit before crashing a few dozen lines down the road, leaving you, the dev who has to debug this crap, confused as all helll.
Honestly it's just a large copy/paste of what looks like low-level driver code. I mean sure we can have a language war over this particular snippet of code and pretend like C++ has absolutely no ugly looking snippets, or we can acknowledge that a lot of low-level systems level code is written in C because despite the syntactic warts the language has, it's still preferred due to having incredibly simple semantics that makes it easier to predict exactly what's going to happen. Or maybe all these guys are just idiots and they should come to learn RAII and SFINAE and C++'s implicit and invisible conversion rules and the subtleties of using vector&lt;bool&gt; and in addition to the language specific cruft they should also understand the compiler specific subtleties like how MSVC and GCC produce entirely different instantiations given the exact same template source-code because of differences in how Microsoft and GCC interpret a certain section of the standard about name look ups within a template's namespace. Or maybe they decide they just don't want to deal with that, that no language is perfect and that perhaps, just perhaps the trade-off of using goto to handle errors is acceptable in when dealing with low-level systems code. For every abstraction that C++ introduces that makes life easier for many domains, it also introduces pages worth of disclaimers and and exceptions and tiny gotchas that in other domains are just simply not worth taking a risk on. And I say this as someone who develops almost exclusively in C++.
That's what the author was referencing, yeah.
&gt; I really can't see the "C is simpler" argument In the sense that a pedal car is simpler than a Ferrari. This is actually perfectly on topic: making a C++ compiler is hard.
FWIW an up to date version of libstdc++ wouldn't help in this case. 4.8 still doesn't have much of &lt;regex&gt;.
&gt; C++'s implicit and invisible conversion rules Actually, the worst thing about C++'s implicit conversions is C's usual arithmetic conversions. &gt; differences in how Microsoft and GCC interpret a certain section of the standard about name look ups within a template's namespace. If Standardese leaves "room for interpretation", that's a defect. (For example, the Standardese once specified range-for's lookup in a sufficiently vague manner that it could be interpreted as either "ordinary ADL" or "pure ADL", when the intention was "pure ADL".) The Standard contains defects, but in this case you appear to be referring to ordinary compiler bugs in VC.
You can't use lex &amp; yacc? is the cpu allowed to be in protected mode??
There's like two lines in there that actually seem to deal with memory management. So, again, what we're really talking about here is RAII. The original post referred to "hitting a wall" in regards to memory management. To me, means more than "C++ has some features that make it easier, assuming the libraries you're using takes advantage of them as well". Apparently that's what he actually meant though.
He might've meant a wall of text ;) There's nothing in C++ that's *impossible* to do in C, but in the same vein, there's nothing in C that couldn't be done with raw assembly. Why stop there, write machine code directly like a real man. In fact, skip assembler, open up your hex editor and write your own PE's. C++ offers nice and tidy abstraction that doesn't (have to) incur any performance cost - you only pay for what you (sometimes not enough) explicitly ask for.
&gt; CPPGM You realize this whole thing was a joke, right?
I am actually trying to avoid singleton as much as possible. Nothing against it, just I don't like to deal with locks in threading... I don't know if VS 2012 is c++11 feature complete... 
&gt; Actually, the worst thing about C++'s implicit conversions is C's usual arithmetic conversions. Amen
What is.... CSS?
&gt;Yes, but over half of that is the Standard Library. IIRC, the course includes writing the entire standard library as well.
Isn't this just a matter of reading the documentation?
well, last time, when only the first lesson was available, there were a few complaints... ;)
Sort of disappointing. He's mostly reviewing GUI front ends, and mostly front-ends for gdb, rather than 13 different debuggers. Also he doesn't really look at advanced debugger features; about the most advanced debugger feature looked at is custom display of variables. There's no evaluation of things like how scriptable the debugger is or how good the expression parser is. I want to know things like how well the debuggers support evaluating complex statements in the program's context; not just if the debugger supports setting variable values, but can it call functions, does it support operator overloads, can it execute arbitrary C++ code? Another feature that'd be good to evaluate is scriptablity; for example, can scripts be attached to breakpoints? That's been important to me ever since I saw a fairly impressive demo where someone showed how scripts can vastly increase the ease and power of debugging. In the demo, after demonstrating the bug, they wrote a script that would linearly search every node for the entry that wasn't being found, and after verifying that the node existed they extended the script to print out the path to that node. Then they set a breakpoint in the broken search function and attached another script which would compare the path the search function was taking to the path that was known to lead to the desired node, and as long as the paths matched up it would automatically continue. The result was a breakpoint in a recursive function that stopped execution at exactly the right level of recursion, without any counting, or recompiling, or repetition on the part of the engineer. That's the sort of feature that makes software developers' lives easier. edit: Here's a written version of the demo I saw, for anyone that's interested: [Using Scripting and Python to Debug in LLDB](http://lldb.llvm.org/scripting.html)
If we are talking about gdb GUI improvements, then [Monodevelop](http://monodevelop.com/) should be one of them IMHO. I am very happy with that IDE at the moment.
Agreed, all these front ends to gdb are a bit samey. I also don't share his fascination with whether I have user dockable windows. I'm not sure whether it is the frontends themselves that are lacking, although I suspect it is the gdb itself which is the problem, but Visual Studio is still a much better product for debugging. It's probably the only area where it is far ahead as a development environment but it is not a trivial one. Personally I am still holding out hope for the more flexible llvm tools to get integrated into more IDEs.
It's Dr. Dobbs man. Their credibility evaporated around the same time people stopped thinking dependency injection was novel.
Great idea, but why not just keep with the lowercase naming convention that pervades posix? There's something about camelcase and posix that doesn't seem right.
Not a pretty picture. I'm surprised the addition of Python support to gdb hasn't helped a lot in this area. What possible reason could there be for a gdb frontend to be unable to display standard containers today?
I've often wondered why C++ wrappers don't exist for POSIX functions. It's quite the conundrum: Use a non-standardized C++ wrapper and alienate possible contributors or lose some low-level use case, or use posix functions directly and lose all of the numerous benefits of C++(11). :/ It looks like this one has a particularly complete API, though.
ddd is truly awful. The tcl/tk interface is really dated and adds very little to help visualize anything. The graph view just complicates structure layout and doesn't really compare to the watch window in VS. It also seems like every version that I've used pukes when you're debugging a large binary. I found it easier to just stick with gdb. 
&gt; The top type of bug that /analyze finds is format string errors – mismatches between printf-style format strings and the corresponding arguments. This is C++. Why are you using printf? Even iostreams is deficient for modern output. You really need to use a type safe and localization friendly output mechanism instead. This isn't hard. &gt; it is 100% guaranteed to cause crashes when compiled with gcc or clang. 100%? Unlikely. I wish it was 100%. &gt; The size_t and ptrdiff_t types are trickier because there is no standard C++ printf format specifier for these types. Just stop already. You're only hurting yourself (and your coworkers). &gt;&gt; if ( SomeFunction() || SomeOtherFunction()-&gt;GetFlags() | kFlagValue ) &gt; The code above is an expensive and misleading way to go “if ( true )”. It's actually if ( x() || (y() | kFlagValue) ), since | has precedence over ||. That doesn't always evaluate to true. &gt;&gt; result = max( 0, a – b ) &gt; This code would have been fine if both a and b were signed Be very careful when mixing signed and unsigned variables. In my opinion, unsigned is the one true integer type, and signed is the exceptional case meant for difftype and user data. &gt; I highly recommend creating and using template based intermediaries that can take a reference to an array and can infer the array size. This trick is also helpful. It was originally from the Windows header, though I can't find the source now. template &lt;class T, size_t n&gt; size_t lengthof (T (&amp;) [n]) { return n; } &gt; Annotations &gt; int V_sprintf_count( OUT_Z_CAP(maxLenInChars) char *pDest, int maxLenInChars, PRINTF_FORMAT_STRING const char *pFormat, … ) FMTFUNCTION( 3, 4 ); &gt; template &lt;size_t maxLenInChars&gt; int V_sprintf_safe( OUT_Z_ARRAY char (&amp;pDest)[maxLenInChars], PRINTF_FORMAT_STRING const char *pFormat, … ) FMTFUNCTION( 2, 3 ); Seeing that code, you need to ask yourself, "Is this making my code better or worse?" In my opinion, it's far worse. Readability is critical, and he's thrown it right out the window.
&gt; This is C++. Why are you using printf? Even iostreams is deficient for modern output. You really need to use a type safe and localization friendly output mechanism instead. This isn't hard. Care to elaborate? :) I thought iostreams were type safe.
But the begin()/end() functions *are* there in the docs.. e.g. http://www.juce.com/api/classArray.html#a5aeb6bdbca7a057c6fe483f2a9c89ef5 
I've never used a publicly available library. Generally the %1 %2 can be reordered in the specifier string, and the parameters maintain their order. Thus you can localize by changing the specifier strings (which you retrieve from some other file) and you don't have to change the code at all.
I have never needed any of those, but here's a few I know of: [`boost::format`](http://www.boost.org/doc/libs/1_53_0/libs/format/doc/format.html), [`QString::arg`](http://qt-project.org/doc/qt-5.0/qtcore/qstring.html#arg), POSIX [extensions](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fprintf.html) to ISO C (`"%n$"`).
iostreams suck for readability and - as you mention - localiztion. printf and printf-style formatting can be made type safe. 
Completely agree, both the stdlib and posix use all lowers, this library fits with neither. 
Whilst on the whole I like the idea, the naming convention is an odd one (as someone else has already mentioned) and the other thing you have reinvented the C++ abstraction to threads that C++11 and boost provide, but not really as well. (the .run() java way is rather un C++ and casting to `void*` is pretty hideous too).
We use printf because we have legacy code and because it actually works quite well. gcc and clang intentionally crash if you pass a non-trivially-copyable object to a varargs function. So yes, 100%. Double-check your logic on the || (|) calculation. Yes, it is always true. If you keep ORing then you just need one true expression. I disagree with you about annotations. Yes, it makes the function declarations less readable, but how often do you look at the declaration for such functions? Those annotations catch many bugs. And, they ensure that the declaration fully specifies the API -- previously some of our wchar_t functions had an ambiguous 'count' variable, and now byte-count versus char-count is clear. The template trick you list is used in the Windows macro ARRAYSIZE, and elsewhere. It is a great way of reliably getting the size of the array. However, as my strncpy post says, that is a poor replacement for functions that use template array references to infer the array size themselves. Passing the array size, no matter how elegantly calculated, is error prone. 
basically, you use variadic templates template &lt;typename... T&gt; int safe_printf(char const * fmtstr, T... params) and can check for each element that the parameter type matches the format specifier, e.g. template &lt;typename TArg&gt; struct FmtArg { string ToString(TArg arg, char const * fmtspec) { throw "error"; } }; template&lt;&gt; struct FmtArg&lt;char const *&gt; { string ToString(char const * arg, char const * fmtspec) { ... e.g. check that fmtspec is a "%s", throw if not, otherwise, apply format } }; // specialization for all other argument types allowed This is very rough, you have to deal with varargs type promotions etc. Pre-C++0x you can use overloads with 1, 2, ...N arguments; in this case you can also only "inject" a parameter check and if successful call the original actual formatter method. You still do have to parse the format string, but that's not too hard. 
I've used codeblocks a few years ago. It was simple enough to get something done quick enough. It's multiplaform. But of course nothing beats the terminal, tmux, clang and vim for me.
On Windows, I prefer using a simple text editor with ported gcc compiler, invoking the latter through the DOS command line. Text editor is [Sublime Text](http://www.sublimetext.com/). GCC on Windows is [MinGW](http://sourceforge.net/projects/mingw/files/Installer/mingw-get-inst/).
I used CodeBlock for some time but I prefer Qt Creator so much better, give it a try
Yes, because unfortunately libstdc++ doesn't seem to be getting quite the same amount of attention as GCC (not to diminish the efforts of any involved).
The course dictates you are not allowed to use any generated code unless you write &amp; provide the generator yourself. So, no, lex &amp; yacc are not allowed (unless you wrote them yourself).
Same here. I used CodeBlocks for a few years, and its had its time to shine. But Qt Creator development is much fastern, and for me its the clearly better IDE now.
I like it. It's my goto when ever I need an IDE on platform other than Windows.
The problem with python support in gdb seems to be that it's first of all not well known and that you have to manually set up gdb to support it. Even on Debian or Ubuntu you have to install the `dbg` package for `libstdc++` and manually set your `.gdbinit`. 
As you said those are just gdb frontends. So the points you are talking about are not specific to them but specific to gdb.
Link to download?
Well, gdb supports advanced features so GUI front-ends could be compared on whether they provide access to those features. And more, although the products in reality are just frontends, he's reviewing them as though they're independent debuggers. As such it doesn't matter what 'debugger engine' is used behind the scenes; either a feature is there or it isn't, and it doesn't matter whether that's because the front-end doesn't support it or because the 'engine' doesn't support it.
The phrase "auto abuse" comes to mind...
Visual studio needs a console that it can attach a program's standard io to.
This code really just me goofing around with functional programming in C++. I'm using it to learn. I'm not seriously proposing it for anything real.
Better folks than I recommend almost always using auto: http://herbsutter.com/2013/06/13/gotw-94-special-edition-aaa-style-almost-always-auto/. Not sure I agree with that, but it's certainly useful in places.
What's the elevator pitch for having monads in C++?
I'm doubtful that anybody (even Herb) has enough worldly experience with auto idioms to say definitively how much they should be used. auto definitely improves "write-ability", but it only occasionally improves *readability*. This is not one of those occasions, IMO.
There are other reasons to use it. In this case, the reason is performance. The type of the object returned is complicated to the point of being unutterable. Besides auto, the only managable way to let people utter the type is to erase it, similar to `std::function`. That would require an indirection at the very least, and probably a dynamic allocation.
Andrei Alexandrescu gave a talk at Going Native 2012 titled [Variadic Templates are Funadic](http://channel9.msdn.com/Events/GoingNative/GoingNative-2012/Variadic-Templates-are-Funadic) that demonstrates the technique. But it requires C++11, so if you're forced to use older tools it's not an option. (It might be possible to approximate in C++03 using some extremely ugly macro programming to provide a definition for every arity up to some fixed argument number limit, akin to what the VC++ standard library does for things like `initializer_list` or `tuple` to compensate for not having variadic templates in their front-end. But it will be extremely ugly and unless you're a masochist you will not want to implement it.)
Disclamer: I am not actually well versed in functional programming With that said. I would think one main benefit would be that you could add const to all the functions and thus know that they will not give side effects which will make debugging much easier (in theory).
I agree with you on every point; I should have explained why casting works for me: I mostly work on games, and printf-type functions are usually used to format what user sees. If he sees truncated long to integer, or float to integer - that is not a big problem, and certainly not a problem which would cause game crash; and that is what is most important to me. Otherwise, if I would try to print double with %d, or integer with %lf, that can cause stack corruption and game crash. And in games, I often have to change variable types from integer to double, or even to some custom number class. But I completely agree, that is not a proper substitution for static analysis, particularly if it is important to you that output correctly displays non-truncated value; but it still helped with a lot of bugs (for me).
It's OK if you're not familiar with the usual *nix tools, the configuration is a little annoying (dependencies seem to have to be set up in multiple places). The best GUI I used was NetBeans with the project set up in CMake.
The OP is looking for opinion on CodeBlocks; not asking you what prefer you use. If you have never used it you can simply refrain from providing a response
Yes...
Read the OP again. It specifically asks: "What are some other IDEs/compilers that are good?"
I love Code::Blocks, but I don't actually use it much. I prefer to use [Kate](http://kate-editor.org/) as my text editor with [Cmake](http://www.cmake.org/) for creating cross-platform makefiles and [GCC](http://gcc.gnu.org/) for compiling.
What does the "with Components" referring too? Browsing through the first 7 lessons didn't give me an answer. 
Looks very nice. Thanks for the link!
How do you localize using printf? Are you only using printf for debug/logging strings, or are you formatting strings for the user to see? Regarding the OR example, the if statement evaluates to false if all values are false. I'm scratching my head trying to understand your statement.
Text editors are not IDEs
Thanks, and no problem! Not sure if it's ready for release any time soon, but I've certainly enjoyed working on it. Like you, I was mainly goofing around for fun to begin with. Then, at some point, it started to look like something that might actually be useful to someone (at some unspecified time in the future when it's nice and stable).
And yet, as seen in this thread, many people use a collection of independent tools for the exact same purpose as an IDE, they just aren't all in an official "package" together. Because these collections of tools have the same purpose as an IDE, I think they're fair game for this thread. After all, OP just wanted to hear what other people thought was good.
I poked at it when it was first linked here, and "potentially useful in the future" seems like a good way to put it. There's a lot of bits that look very nice on the surface. No MSVC support stops me from trying it in anything practical, but hopefully MSVC support will become more reasonable in a few days when the 2013 RC is released.
One clue about the 'or' statement is in the warning. As it says, "tested expression is constant and non-zero". In other words, always true. Another clue is the name kFlagValue which implies (to me anyway) a bit flag, such as 1, 2, 4, 8, etc. Since kFlagValue is non-zero it generates a non-zero result when bitwised ORed with GetFlags(). Since the right side of the logical OR is true the entire expression is true. I hope that makes sense. I was being intentionally terse about some of the warnings in order to see if people could see the problems on their own. In the case of that particular warning the compiler was entirely correct -- the intent was to use '&amp;' instead of '|' because the code was supposed to be testing a flag. In short, it's an incorrect operator bug, not a precedence bug. And, most of our use of printf-style format strings is for debug/asserts/logging. 
&gt; Google C++ Style Guide Not commenting to take any sides in this particular squabble ;-) but just to mention that this style guide is somewhat... controversial (politely speaking, "opinions are divided") in the C++ community and is mostly highly optimized for (tuned to) Google's very specific constraints (of course, this quite possibly includes your "we have reasons" disclaimer): * http://stackoverflow.com/questions/11239253/googles-c-style-guides-file-naming-rules/11239299#11239299 (although personally I wouldn't necessarily go that far, e.g., I found the "General Naming Rules" section quite good). * it's worth noting that even in their own explanations they remind the readers of how some of the rules evolved primarily due to Google-specific codebase history: http://stackoverflow.com/questions/5184115/google-c-style-guides-no-exceptions-rule-stl On a side note, what are your thoughts on [FastFormat](http://fastformat.sourceforge.net/) in this context? Claims to be safer ("more robust") than IOStreams and faster/lighter than printf, benchmarks: http://fastformat.sourceforge.net/performance.html Found this -- http://stackoverflow.com/questions/446276/is-there-a-catch-with-fastformat -- but nothing sounds too dramatic.
&gt; VC++ really needs to add a /analyze:fast mode that just does the quick types of static analysis. I was glad to read this, as my first thought when reading the article was 'Why are format warnings being found by static analysis? That should be a normal warning.' &gt; max() was a macro and clang normally avoids peering too deeply into macros Another reason not to use macros. Besides, it's illegal to name a macro `max` in any code that #includes any standard header. If you do this then your program has undefined behavior. &gt; I highly recommend creating and using template based intermediaries that can take a reference to an array and can infer the array size. Another solution is to use `std::array`. Raw arrays have more bad behavior than just forgetting their type; they cannot be passed by value, they cannot be assigned, etc. And `std::array` has zero overhead vs. equivalent usage of raw arrays. About the only advantage raw arrays have is deducing their length from an initializer. Using `auto` with a `make_array` utility fixes that.
http://bartoszmilewski.com/2011/01/09/monads-for-the-curious-programmer-part-1/ http://bartoszmilewski.com/2011/07/11/monads-in-c/
learn to google.
Yes, max absolutely should not be a macro. We're gradually expunging that horror. I've found that a template max, due to enforcing that the two types match and avoiding double-evaluation, either gives identical or better code than a macro max. I'll look at std::array, although we are only slowly moving to C++ 11.
Agree. The "auto"s make the code look like Javascript: var a = someFunc(); var b = a * 5;
A page of code, no explanation. Link to github doesn't explain anything either. I think this needs a downvote.
Them's fightin' words. ;-) Unlike Javascript, C++ is statically typed, even if you use auto.
Yeah, I didn't really quite grok monads until I read his blog.
Yes, I know "auto" in C++ is strong and statically typed. But from the readability side, "var" and "auto" are the same to me, because I can't deduce the type from "auto" at first glance, though C++ compiler can.
Why not try the following: . eclipse . XCode . EMACS / AQUMACS &amp; all the other variants. . Visual Studio 
I used to use Code::Blocks, it's still an OK ide. But now, I usually use Eclipse, I find that it works much better than all of the IDE's I've previously used. I never really liked Qt Creator, but that's just me.
IMO it's not as nice as some of the alternatives. Whenever I work on large projects, I usually use KDevelop or Qt Creator (depending on the task) just because they offer more features and better completion (I'm a lazy fuck).
I don't use QT Creator as a Goto, I prefer to use While coding or For coding.
FastFormat looks interesting. It looks like it might require memory allocations for all print operations, although I can't be sure. If so that would be a nuisance, especially since it makes logging in failure conditions more troublesome. I'd also be worried about compile times -- template solutions tend to make them worse. But, I have no specific reason to believe that it would be a significant problem.
It's a design pattern. This is a *very* rough sketch, and apologies to all the category theorist I'm about to offend, but imagine if C++ let you overload the semicolon operator so that you could specify some custom code to run between statements, grabbing the result of the previous statement, munging it in some way, and then passing the result on to the next. That's a monad. Roughly. Different monads give you different behaviors by, er, overloading the semi to mean different things. The Haskell equivalent of the overloaded semi is called "bind" and is spelled `&gt;&gt;=`. There's a monad kind of like `boost::optional` called `Maybe`, one kind of like exceptions called `Either`, one that gives you something like stateful computations called `State`, and a whole zoo of others. They all share the same interface, so there's a whole other zoo of algorithms that can work with them in interesting ways. In Haskell, there's even a special syntax for working with them so you don't have to sprinkle `&gt;&gt;=` everywhere, but it's just sugar. It's really just overloaded semis (and may Curry Haskell and Alonzo Church have mercy on my soul).
I guess it's possible to decipher the API with the example, but I agree. A concise list of the available macros and a short explanation of the syntax used in the example would have gone a long way.
Here you have the link: http://goo.gl/zqCvg You can download "full" Qt which also has Qt Creator or download the IDE separately. I am using it for a non-Qt project right now and it's great :)
I have been using vim for years. It is wonderful but no integrated debugger, and no real intellisence. Plug ins help but trying to rape it into being an IDE doesn't really feel productive to me. 
Do you use it on windows? Also do you not like auto completion? I really have enjoyed sublimetext2 but as a novice having auto complete is a very handy tool. I know their is a plugin, however it requires clang and I'm not familiar enough to know how to install that on windows.
I mainly program on Windows, so Visual Studio wins all the prizes. Code::Blocks is uggly, and I hate uggly.
Ok, I'd have to check HashMap, it might just have been overlooked. It's not a very commonly used class, as most people will use std library containers for their maps. String is different though - it's not trying to be a standard container. Instead, it can represent itself as various UTF-encoded classes that are specially designed for iterating in a more stringy way, and doing string-type ops.
CodeBlocks is a very nice IDE and I use it in all my personal projects on Windows. This does not mean Qt Creator or Eclipse might not better, I have not tried them.
Very nice. A simple header file library that makes it easy to incorporate. Thanks for sharing.
I think there's a plugin that makes vi use clang for code completion. Same for emacs.
If you like autocomplete, it's hard to beat visual studio. I haven't found anything that comes close to that speed and accuracy. That plugin you're talking about slows down on bigger projects, so I just don't bother with it. I develop on osx and build on linux.
It would be nice if you could change the link title to say Visual Studio 2013 **Preview**. I have a big VCBlog post drafted about the STL features in 2013 Preview; I'll try to get it published as soon as possible.
Well, you've just met another (over the internet, for what it's worth). Ignoring localization, iostreams are good for everything they do. I don't even find the `&lt;&lt;` and `&gt;&gt;` overloaded operators too bad. They are good for everything, except formatting. Formatting is shit using iostreams, and OK-ish for printf. Here is a tiny formatting library, which wraps iostreams in a printf-like interface. I'm sure there are loads others. https://github.com/c42f/tinyformat
I have always had a weather eye on functional programming and played with the castor library for an intro (it was very good for many things). This seems very intresting, first off great effort and well documented, I would be interested if you have ever approached boost with this or had folk review that know much more than me (such as Bartosz). Eric, it would be great to hear what you say about the lib as a recodnisable name in the industry with more experiance than me in both c++ and fucntional.
I am curious to see if there is going to be any statement related to the broken promises of continuous updates targeting Visual Studio 2012 for C++11 updates, and instead releasing Visual Studio 2013.
From the link: &gt;Note this is a Microsoft conference, so the talk is specifically about the future of the Visual C++ product, [...] and the bulk of the talk will be an update on standards conformance in Visual C++ So more shilling from Herb. Oh how the mighty have fallen!
I agree that this is an annoying change. There is a way to turn it off, though. http://stackoverflow.com/questions/10859173/how-to-disable-all-caps-menu-titles-in-visual-studio-2012-rc Disclaimer: I work for Microsoft (not on visual studio), opinions are my own, etc.
I'm glad they are making OK progress. If I were to prioritize the C++-11 features I care about I think they've made good choices (one caveat: I do wish they included non-static data member initializers, but maybe I won't miss them so bad now that I can use delegating constructors.) I have a cross platform project (win/mac/linux). Sometimes I fantasize about using clang on windows (or dropping windows support) so I can use all of C++. If modules (and hopefully speedier compiles) ever get stable in clang it's going to be *really* tempting to drop windows.
"To empower developers and enable synergies between C++11, WinRT and Windows 8 regarding the creation of state of the art software as well as the leverage of up-to-date methodologies, processes and tools combined with the changed paradigm of Metro and Microsoft's commitment to a high quality app ecosystem have forced Microsoft to implement enhanced support for C++11 only in Visual Studio 2013."
I would guess the answer is no, I'm afraid. And I know what you mean about lower cased acronyms, the standard C# style does that too and it looks weird (e.g. XmlWriter and HttpRequest).
I forgot to include Preview and unfortunately reddit does not allow editing titles. :( Looking forward to your post about STL features. Quick look around looks like true variadic tuples and initializer_list support for containers.
It's really hard to tell whether this is an actual quote or a mockery of the overuse of buzzwords by big marketing departments.
Yeah, I have seen it as well. Somehow I am happy that my employer kept using VS 2010 instead of shelling out useless money for VS 2012, specially if they will require a full pay after less than one year. I am starting to think Apple devices are not that expensive if you consider that the development tools are available for free and there is no feature segmentation.
This is the thing dreams are made of. That said, I always have troubles trusting C and C++ refactoring tools, macros mean doing a 100% job, especially on cross platform codebases, is incredibly difficult. 
Yep, also worth to say that GCC/libstdc++ does have a `&lt;regex&gt;` header and code will compile just fine with it, it will however give you random errors or just fail to do the right thing as it's not actually implemented. I wish they would have put some better error messages in there to indicate that it's not done yet, would have saved a lot of people a lot of trouble figuring out why their regex aren't working.
Yeah, I hope to be able to add MSVC as supported at some point. Unfortunately, I don't currently have access to a Windows box to try the new preview of VS2013. Of course, if someone else tried compiling e.g. the tests, I'd love to hear back about the results.
404s for me.
No, I haven't approached boost about it, but I'm flattered you think FTL might possibly be a candidate for it. Maybe I'll consider doing that at some point. However, it really isn't ready for wide spread use for a while yet. At the very least, I need to write tests for all remaining modules and even then, there are probably some rough edges and maybe even a few logic errors left to polish away before I consider an "official" release of any kind. Until then, I welcome any critique, feature suggestions, bug reports, and even just straight up opinions with open arms.
I've been playing with the preview and it looks like somebody has been veeeeeery busy ;)
Is there an implementation of Reaps I could borrow?
I took a brief poke at compiling with the VS2013 preview and it's still missing a lot of stuff: constexpr, `using` type aliases, and in-class member initializers in the first 100 errors.
Do they finally have a good MDI system / setup for multiple monitors?
Great! There are some very interesting topics on schedule. Looking forward to seeing the videos.
Agreed, I'd be more comfortable with their tools if they just built atop Clang (like Xcode ... which unfortunately has its own share of issues).
Broaches differences in philosophy between Stepanov and Meyers in Lecture 2, Part 2.
At least `std::regex` works. I'm looking at you, gcc.
Does clang not work on windows at all?
The compiler team has a list of what remains to be implemented, and they're prioritizing features according to user demand (including library demand) and implementation cost.
Yep, plus more.
Finally! We almost never get credit for shipping Dinkumware's regex implementation back in 2008 SP1.
where can we find your blog?
They should call the C++ version "RePlusser". Or "RePlusPlusser". [Edit: Fixed typo.]
I'll admit, tuple was the first thing I checked after firing the preview up. Jaw dropped, it doesn't even look the same. I'm glad the future is arriving! Kudos, I bet it feels great getting this release out the door.
I will post on the [Visual C++ Team Blog](http://blogs.msdn.com/b/vcblog/), aka VCBlog.
Yeah, I am *so* happy that I'll never have to look at another faux-variadic-template macro again.
It works fine here, strange.
Dunno. Didn't write this article, just thought it worthy of sharing when I ran into it.
There are still some issues with exception's support and libc++.
magic!
re-increment?
&gt; to get people to buy your product. What's wrong with that?
Where I can vote for constexpr? Good work!
Definitely true inside the same translation unit, but what about across multiple TUs.
&gt; Your product has been superseded by llvm and gcc. Hahahaha not even close
FastFormat does not compile on OS X, because STLSoft, a dependency of FastFormat, does not compile on OS X (at least the last time I tried). One of STLSoft's headers issued a #error upon compilation, and the fix is not trivial. Edit: FastFormat will now compile on OS X with some manual tweaking. See [here](http://stackoverflow.com/questions/13717936/fastformat-on-os-x) for more information.
Wt has a list of [publicly available Wt applications](http://redmine.webtoolkit.eu/projects/wt/wiki/Sample_Wt_Applications)
PVS-Studio vs Clang: http://www.viva64.com/en/b/0108/
* [Zoltan Porkolab: Debugging and Profiling C++ Template Metaprograms](http://www.youtube.com/watch?v=i0bzwHBiNc4) * [Vassil Vassilev: Interactive, Introspected C++ at CERN](http://www.youtube.com/watch?v=K2KqEV866Ro) * [Sumant Tambe: Standardizing the Data Distribution Service (DDS) API for Modern C++](http://www.youtube.com/watch?v=GUtPutqVgaw) * [Andrew Sutton: Concepts Lite: Constraining Templates with Predicates] (http://www.youtube.com/watch?v=o1lNd12uYjE) * [Boris Schaling: Boost.Graph for Beginners](http://www.youtube.com/watch?v=uYvBH7TZlFk) * [Rob Stewart:Multi-Threading With C++11 and Boost](http://www.youtube.com/watch?v=53_kveAvcKM) * [Sebastian Redl: Overloading the Member Access Operator](http://www.youtube.com/watch?v=Gy9ITl1AWRY) * [Alisdair Meredith: Allocators in C++11](http://www.youtube.com/watch?v=v7B_8IbHjxA) * [Lelbach &amp; Habraken: Boost.Asio and Boost.Serialization: Design Patterns for Object Transmission](http://www.youtube.com/watch?v=8XHxGWacbdM) * [Boris Kolpackov: ODB, an ORM for C++(11)](http://www.youtube.com/watch?v=B1vgKT-NdRg) * [Alex Fabijanic: Look ma, "update DB to HTML5 using C++", no hands!](http://www.youtube.com/watch?v=p959njgdlKk) * [Alex Fabijanic: Dynamic C++](http://www.youtube.com/watch?v=QySTK4cSq7o) * [Marshall Clow: Fun with Tuples](http://www.youtube.com/watch?v=3tipURTxEUw) * [Michael Caisse: Solving World Problems with Fusion](http://www.youtube.com/watch?v=6V73Q7ULFi0) * [Julian Storer: The Projucer: Live coding with C++ and the LLVM JIT engine](http://www.youtube.com/watch?v=Uj94TYQ6LN0) * [Sounders and Jefferey: Dynamic, Recursive, Heterogeneous Types in Statically-Typed Languages](http://www.youtube.com/watch?v=W3TsQtnMtqg) * [Bart Janssens: Building finite-element matrix expressions with Boost Proto and the Eigen library](http://www.youtube.com/watch?v=pDTQlwXkjvU) * [Louis Dionne: A system for resource deadlock prevention using intrusive dynamic analysis](http://www.youtube.com/watch?v=Re67U4zAN-M) 
I didn't even realize there's a link to download PVS Studio on that page until your comment pointed it out.
&gt;Herb Sutter: "Bullshitting and trolling VC++ users" FTFY
There are currently only 2 videos available: * http://www.infoq.com/presentations/programming-for-life * http://www.infoq.com/presentations/Cplusplus-11-Bjarne-Stroustrup More should appear at http://accu.org/index.php/conferences/accu_conference_2013/accu2013_schedule (search for 'video')
Recently played a little with Eclipse CDT: * Fast and accurate Intelisense * Easy to use/config * Tons of plugins IMO its better than Qt Creator and CodeBlocks.
Eclipse CDT has a really good autocomplete.
Depends on how expressive it is. Does it, e.g., explain the intent (good) or is, e.g., just a mere alternative spelling (bad). In other words, `NumberOfSupportedPlatforms` may be more informative than `unsigned int`, while `uint` isn't. (Arguably, even a strong typedef could make sense here -- e.g., it doesn't make sense to, say, multiply a variable of type `NumberOfSupportedPlatforms` by a negative `int`). Similarly, `Speed` brings additional information compared to `double`, while in contrast `MyFloat` or `OrganizationNameFloat` are just evil. See type-rich programming: http://ecn.channel9.msdn.com/events/GoingNative12/GN12Cpp11Style.pdf (e.g., slide 18). Another example from http://www.stroustrup.com/Software-for-infrastructure.pdf `// construct a rectangle:` * uninformative interface: `Rectangle(int x, int y, int h, int w); // use: Rectangle (100,200,50,100)` * type-rich interfaces: + `Rectangle(Point xy, Area hv); // use: Rectangle(Point(100,200), Area(50,100));` + `Rectangle(Point top_left, Point bottom_right); // Rectangle(Point(100,200), Point(150,300));` 
The bjarne talk was good but I couldnt bring me to listen to the end of the first guys talk. He seemed to be talking about everything except programming. Typical agile consultant then.
I realise it would probably be harder to test, but I would be very interested in comparing Clang to VC12
JackTrue_burn_
I was under the impression c++ did not have strong typedefs? Am I wrong? I think part of the reason for this is that in the days of yore compiler support for data types was more varied than it is now, so you may have had LPCSTR defined as a different size pointer depending on your CPU. Even today you see some of this with things like libraries doing explicit vectorization. the DirectXMath header has a type that is defined to _m128 or the NEON equivalent. tl;dr yes.. this is really annoying and it is one of those things that people see in their system headers and think "oh I should do that to!" without realizing that these headers are decades old.
Interesting, those errors sound like they could actually be worked around. Most of the `using` instances could be changed to type defs (and the others, while nice, aren't _vital_), the in-class initialisers could simply be moved to constructors, and even `constexpr` can be worked around for the cases where it's necessary. When I eventually get VS2013 at work (might unfortunately take a while), I'll give it a shot.
So, he posts links to his own blog, I don't see a problem with besides it's got useful information. Or did you not notice it's a link to an article?
It would be best to compare C99 or C++11 code.
Vote minus and go read other articles. If my links are bad, they go into a minus. No Problem. &gt; There's even a static analyzer built into Visual Studio, for free. Did you know that? * [Comparing the general static analysis in Visual Studio 2010 and PVS-Studio by examples of errors detected in five open source projects](http://www.viva64.com/en/a/0073/) * [Comparing static analysis in Visual Studio 2012 (Visual C++ 2012) and PVS-Studio](http://www.viva64.com/en/b/0151/) * [Errors detected in the Visual C++ 2012 libraries](http://www.viva64.com/en/b/0163/) 
FYI, this is not my upcoming VCBlog post that I've been promising around here (that'll be published after Herb's talk on Friday, possibly on the same day if I can get it blessed quickly enough). This is a direct copy of what will be published on MSDN soon, listing compiler/library/IDE features for Preview only. The STL section was directly derived from my upcoming post, where I will explain that stuff in far greater detail.
My Karma: 4 268 Your Karma: 21 It is not necessary to discuss who publishes interesting and useful links. You're just jealous. :) 
Yeah, it appears to be an article with some possibly useful info (I haven't had the time to go over in detail yet), and I think I like the blog, and possibly the product as well — it is the first time I see it, so I don't know yet. It is a free world and and a free site that has a great feature that allows you to ignore posts that you don't like, that is if you aren't capable of self-control.
I think that is not the case. Especially his analysis of open source software for bugs such as Qt or boost was quite interesting to read. Still, problem with most tool vendors is, that they are a bit to self centered. 
I very much doubt. There are only two of my colleagues. Especially, reddit does not allow the vote all the time for one person. So do not worry. :)
...and good job on including `std::make_unique`! :-) // I gather you *must* have had some assistance due to being *in close communication* with the proposal's author ;-)
Sure thang, mate!
Does that mean your post will have information on future plans?
The future of C is death, if MS has its way. The only C99 features MSVC has are the ones that accidentally showed up when they were making C++11 work. They are making my life harder, and I am not happy.
&gt; I was under the impression c++ did not have strong typedefs? AFAIK, il doesn't. I believe `mttd` was lamenting that fact. &gt; compiler support for data types was more varied than it is now The situation still isn't simple: 32-bit system: 32-bit `long`, 32-bit pointer. 64-bit Windows: 32-bit `long`, 64-bit pointer. 64-bit Unix: 64-bit `long`, 64-bit pointer. 
The article made it pretty clear that you need to test your implementation in a full app to understand performance. That's pretty much the rule in all performance work.
Don't most platforms support int32_t, uint32_t, et al for when you need that level or precision?
Is this preview stable enough for real use, or should I wait for something more complete?
While building a large (100+ c++ projects) solution the build seems to run into race conditions leading to thousands of linker errors that resolves itself by just stopping the build and trying it again. Doesn't seem very stable to me at the moment.
Oh, yeah. That's gotten worse in every version of VS ever since 2008. It's a bug with their PCH system. The solution (and it's ugly but it works) is to disable ASLR using the Microsoft mitigation tool. It really works quite well.
I'll look into that, thanks.
&gt; Is something like this considered 'good practice?' No.
Bummed that `std::optional` didn't get in (like `std::make_unique` from the C++14 CD did). Especially since it would be a library-only change. But, it's great to see most of the VS2012 CTP features finally making it into a "soon"-to-be supported version of the compiler.
For how long can I use this "Preview" software?
I assume `std::make_unique` is only in because STL had to implement it as part of writing the proposal, so shipping it (hopefully) didn't require much extra effort.
You can do a reasonable job of faking them. See http://www.boost.org/doc/libs/1_53_0/boost/strong_typedef.hpp and the Units of Measure posts from a few days ago for examples.
Heh. It was actually more than 5 seconds of copy-and-paste work, since I had to reformat my code to DinkumStyle, and I decided to use existing type traits instead of introducing a new one (which I did in the proposal for clarity).
Why are you bummed you can literally implement that yourself in 15-30 minutes. 
Ordinarily, talking about the future is above my pay grade. However, my post will talk about concrete changes between Preview and RTM, and I'll link to Herb's Build presentation (summarizing parts of it) which will talk about the future in great detail.
Somasegar (DevDiv's Corp VP) [announced](http://blogs.msdn.com/b/somasegar/archive/2013/06/26/visual-studio-2013-preview.aspx): "Visual Studio 2013 Preview and .NET 4.5.1 Preview are now available for download, and as “go-live” releases." "Go-live" means "you can build stuff with this and ship it to end users". In my opinion, the C++ compiler and Standard Library significantly supersede 2012, i.e. things are _more_ stable when considering 2012's feature set. Obviously, the new features in 2013 Preview are not yet RTM-quality, but they are way better than the CTP/Alpha. In particular, they are stable enough for the STL's use, which stresses many features beyond what ordinary users do.
The EULA presented during installation should answer this. (I haven't seen it myself, since I haven't installed Preview - I get my day-to-day work done by building the compiler and libraries without installing the whole product in a VM which takes way longer.)
Thanks, I ended up downloading the installer, and it says it's good until January 15, 2014.
I am never sure to be excited or depressed for new things. At work I am stuck on VC6, and will be for at least another year or 2. I want fancy toys. 
&gt; From your description, it looks like someone went overboard with the idea. I can't see that from his description. Let's recap: - It is about "a few header" that "mostly declare typedefs": I don't see any problem here. Why should the typedefs need to be mixed into many other headers that do have classes for example? Both are perfectly good approached and depending on the context one or the other might be better. - "In many cases" they typedef from other typedefs (which come from unsigned int). This is also perfectly nice. For example consider a codebase where numbers are used as identifiers for many different kinds of objects. Then it makes perfect sense to have a typedef going from a built-in type to a general identifier typedef and go on from there to whatever other typedefs are used directly in code. Now if you decide that "unsigned int" has too many or too few bits at some point in the future, you could change that with a single line. It might even make sense to nest this further, because you can already see that you will maybe change the built-in type used for a subset of those identifiers, because some of them might grow beyond the some limit in the future, others won't. - "It's not so bad since i found Sublime Text's CTags plugin [...]" Well, it is quite common to have a codebase that can only efficiently worked with, when you have a sufficiently powerful development environment. The complexity of code can rise with the tools used, otherwise your shiny tools will bring you limited or no advantage. Don't expect you can just print out some files of a large codebase that is unknown to you and work yourself through it with a pencil. - "I'm finding this over zealous use of typedefs quite distracting to my primary goal." There might or might not be good enough reasons for the typedefs, but I can't see evidence for either from the description. I can only see that someone is frustrated with the fact that he has to work himself through a codebase that he doesn't know.
Test against icc if you want a real challenge.
Yikes. MS doesn't support VC6 anymore, it's 15 years old.
codelite multiplatform, programmed in c++, wxwidgets. Great codecompletion, great evolution, small memory, fast Qtcreator. multiplattform Great IDE. The best for qt applications kdevelop fantastic IDE, ¿windows?
Another note on the headers that declare typedefs. One thing that these headers do is forward declare almost every class in the code base (when I say almost I'm going to estimate about 90% or so) and then typedef a pointer to them to another name. example: class Foo; typedef Foo* IPFoo; then throughout the code base you'll find people using both Foo* and IPFoo all over the place, seemingly at random. Yes, it's frustrating going through a new codebase that I don't know and have no documentation about nor easy access to the original developers. I would just like to know if this is something I should fix, or if it's me who needs fixing.
It's like a goldmine of informations. I watched the talk about allocators in STL containers and was something that nobody else has talked about. The only problem is that video quality is bad
This is the best NoScript page I have ever seen, you can't even read the text.
I would say skip it and use boost::asio - no need to add another API layer
`cpp-netlib` seems to mostly provide HTTP support, which `boost::asio` does not seem to offer.
Ah, yes - there's a lot of lifting to do to get the HTTP protocol right with boost::asio. In the http case then I wouldn't got with asio
AFAIK its based on asio. So you don't need to do any lifting and get http supported. Also this is the branch which aims at standardizing network operations for an upcoming C++ Standard, so its worth taking a look at it for sure.
&gt; Find me an article that doesn't try to sell you their product http://www.viva64.com/en/b/0129/ :-)
??? I modified the registry key as the stackoverflow link describes, and I have "SQL" (all caps) right between "Tools" and "Test" (not all caps). I assume the actual text is already correctly cased, but the default menu style uses an all caps font.
&gt; At work I am stuck on VC6 Wow... Ten years ago, I would have pitied you for using a pre-C++99 compiler. Today, my brain can't even comprehend the hole you're in.
They just announced (6/28) that RTM will have non-static data member initializers! My wish came true!
POCO has a nice http library as well.
When I promised to publish a big post, I meant *big*: 40 KB of plain text.
Wonderful and exhaustive, and most of all it's great to see that MSVC is catching up, albeit slowly. 
&gt; * &lt;atomic&gt;, whose entire purpose in life is blazing speed, contained unnecessarily slow implementations for many functions. YIKES!
So I noticed you were talking about how you couldn't release "C++11 features" in VS updates. Does that mean that the C++11 features you mentioned are the only ones that will be implemented in VS2013?
Sort of amusing to see C++98 conformance after a bunch of C++14 stuff, but I suppose it is something that I care less about than pretty much everything else on the roadmap. Bit sad to see how far away some stuff is, but I guess the overall priorities are decent.
No move semantics. This is for me the most impactful improvement in c++11... And Microsoft still don't have it. Sigh. Thank gods for GCC, clang and mingw.
I love how * You must #include &lt;algorithm&gt; when calling std::min() or std::max(). is a breaking change, not a bug fix.
The domain died.
That broke *so much* code. STL changes are consumed immediately by the VS build as a whole, so when we make changes like this, I have to fix all misuse throughout VS. However, it wasn't an STL bugfix - I just took the opportunity to make this change when `min`/`max` overloads for `initializer_list` were added. STL headers are allowed to include each other in unspecified ways, so it was perfectly conformant for the STL to define `min`/`max` in a centralized place, as long as they were available through &lt;algorithm&gt;. It wasn't conformant for users to depend on that, though. So when I moved `min`/`max` into &lt;algorithm&gt; itself, that broke nonconformant user code.
It used to be even worse. &lt;atomic&gt;'s implementation was *separately compiled* (preventing inlining) throughout 2012's development. I was able to make it header-only just before 2012 RTM, in one of the biggest overhauls I've ever done.
Heh. I did that just in case we weren't able to get variadic templates implemented quickly enough. We nearly ran out of time, but we ultimately made it.
Ha! The VS 2013 RTM finally implements the C++11 features I need, Alias Templates make my life easier &gt;)~~~~
So a new implementation of &lt;tuple&gt; is somehow the reason for not keeping the promise of "out of band" compiler updates in VC2012? I do not get this at all, just make an update to the&lt;tuple&gt; (and any other broken header for that matter) and update compiler to the one from Nov CTP (but fixed) as you promised. Don't call it an "update" if this somehow makes you feel uneasy :/ Please stop with the BS.
So, what's good about this library as opposed to the *many* other options?
What is Agda, and should I spend 83 minutes watching a video about it?
but it takes so long for some tools work with the new visual studios, I'm not looking forward to this :( still waiting on nvidia Nsight to come to VS 2012
The description on the YouTube page for the video at least partly answers that question.
I really hope for a sane module-system in C++-17 that will fix this chaos… Warnings about unused imports and errors on missing imports instead of unreliable, internal inclusion of standard-headers; wouldn't that be great?
It's called the C++ Standard Library. "STL" was a library developed in the early 1990s by Alexander Stepanov and Meng Lee. This then became part of the C++ Standard Library with C++98. While some parts of the Standard Library can still be identified as derived from STL, it is _not_ STL and the use of this term is unhelpful. Especially when non-STL-derived parts of the Standard Library are referred to as "STL". (e.g. 'shared\_ptr' was not in STL, it's Boost-derived, so it's _wrong_ to refer to it as an "STL Feature"). This widespread misuse of terminology is responsible for so much misunderstanding. There are still people (even C++ programmers) who think that "STL" is some optional add-on for C++ that they can't rely on the presence of and may even have to download separately from their compiler! Note that the term "STL" does not appear anywhere in the C++ standard (at least not in the publicly-available draft from 2012-01-06). Also, why on earth is the VS team working on C++14 features when they C++11 implementation is still far from complete!?
From the linked article: &gt; **Frequently Asked Questions** &gt; Q1: Why are you implementing C++14 Standard Library features when you haven't finished the C++11 Core Language yet? &gt; A1: That's a good question with a simple answer. Our compiler team is well aware of the C++11 Core Language features that remain to be implemented. What we've implemented here are C++14 Standard Library features. Compiler devs and library devs are not interchangeable - I couldn't implement major compiler features if my life depended on it (even static_assert would take me months to figure out), and I like to think the reverse is true, although rocket scientists are probably better at pizza delivery than pizza deliverers are at rocket science. 
&gt; a bit unrelated and I know its not your area ... but any news on C++AMP? * http://blogs.msdn.com/b/nativeconcurrency/archive/2013/06/28/c-amp-highlighted-in-build-2013-keynote.aspx * http://blogs.msdn.com/b/nativeconcurrency/archive/2013/06/28/what-s-new-for-c-amp-in-visual-studio-2013.aspx * http://blogs.msdn.com/b/nativeconcurrency/archive/2013/06/28/simultaneous-cpu-gpu-debugging-in-visual-studio-2013.aspx 
Here's an example of how it would look in C++14: namespace detail { template&lt;std::size_t ...Is, typename F, typename Tuple&gt; decltype(auto) apply(std::index_sequence&lt;Is...&gt;, F &amp;&amp;f, Tuple &amp;&amp;args) { return std::forward&lt;F&gt;(f)(std::get&lt;Is&gt;(std::forward&lt;Tuple&gt;(args))...); } } // namespace detail template&lt;typename F, typename Tuple&gt; decltype(auto) apply(F &amp;&amp;f, Tuple &amp;&amp;args) { return detail::apply( std::make_index_sequence&lt;std::tuple_size&lt;Tuple&gt;::value&gt;{}, std::forward&lt;F&gt;(f), std::forward&lt;Tuple&gt;(args) ); }
Adga is a dependently typed programming language / theorem prover. In a dependent type system, types can depend on values - for example, your list type can know how long it is. How is Agda a theorem prover? It's well known in the functional programming community that certain type systems are isomorphic to constructive systems of logic - that "a proof is a program, the formula it proves is a type for the program". For example, the simply typed lambda calculus is equivalent to first order intuitionistic propositional logic. Most theorem provers have type systems that are equivalent to intuitionistic predicate calculus. There's a technique championed primarily by Conal Elliot called denotative design. Essentially, you think about what the most general way to implement something is in math, and then you implement something based off of that in your language. Conal Elliot both thinks and writes Haskell - he writes something semantically nice but slow, and then tries to make it faster without compromising on the semantics. David Sankel thinks in Agda and writes C++. That is to say, he'll start off thinking of what something means, writes down a type in Agda which captures that meaning, and then implements it in C++. As you might guess, he makes heavy use of templates and boost.
That's good then. Move semantics are my favourite.
Very nice! Concerning coding in C++, things are getting better and better! Nicely, common useful aspects of C++ template metaprogramming (TMP) are also being identified resulting in constructs being placed in the language so everyday programmers won't have to rely on C++ TMP to do needed tasks without any added/unnecessary run-time overhead.
One can hope! Supposed to be out later this year. Maybe this will be good for vs tools. Maybe more of them will be designed as standalone with additional VS integration, thus not requiring a year long rewrite to catch up.
There's a whole bunch of features that I'd like to see added to C++ someday, but I'm not the standards committee, it's not up to me what goes into the language.
Even if tuple change would break binary compatibility (which I don't think would happen because it would still be exact same data structures with same sizes and data layouts) not adding a single feature from Nov CTP is a disgrace. Not even "Raw string literals"... C'mon... It was a pure business decision.
I understood that. I guess implementing features from future standards looks better on the feature list than a finished implementation of the current standard. I mean, which of these would make you more likely to buy the product: * 100% complete C++11 compliance or * C++11 compliant! * Key C++14 features! Yeah, the second sounds better, while the first is _actually_ better.
Setting up a C++11 development environment ~~on Linux with Clang and Emacs~~ with Linux, Clang, Boost, Eigen, TBB and a shitload of other stuff we liked
&gt; Cant be easy there being only one person at MS implementing STL. Dinkumware implements stuff, whereas I mostly fix bugs. Aside from my own C++14 features, it's very rare for me to (re)implement anything from scratch (there are maybe a half-dozen places where I've done this).
And it would be wonderful to not have to make the STL defend itself from forbidden macroized keywords and other evil things.
I have a serious question for you, slimshader: do you favor latency over bandwidth? That is, would you prefer our compiler team to spend time backporting features to 2012, instead of implementing more features in 2013 and beyond? This is indeed an either-or decision: time spent (carefully!) backporting code cannot be spent elsewhere. Even something relatively simple like raw string literals takes time and carries risks (e.g. macros beginning with LR were handled incorrectly as the parser got confused while looking for wide raw string literals).
No-one seems to have anything to say about this, so I'll begin. I love to see Herbs talks, and this was no exception. I know there is a lot of bitterness about MSVC's broken promises, but personally, I'm happy with the excuses given. As a multi-platform developer, I can use an awful lot more of C++11 while still targeting XP than I can targeting Android 2.2; an OS released *after* Vista. To developers interested in supporting a broad range of clients, these things are a big deal.
I would evaluate the product as a programmer, not as a marketing droid. You can bet that I'd rather have actual useful features from C++14 than 100% of C++11. Who in their right mind really cares about 100% compliance? By that metric virtually no compiler is ever good enough. MSVC certainly never hit 100% of C++98, since they didn't support export templates. Nobody did, not gcc, not clang, except for a few masochists at EDG, because everyone knew it was a bad idea. (And EDG admitted after the fact that they wished they'd never bothered, as it was a huge waste of time and money.) And I'd bet there are still other C++98 edge cases that aren't quite fully supported in MSVC, gcc, or clang. I'd much rather have my compiler vendor working on useful features than pedantically filling in checkboxes that nobody cares about.
If they really do some real C99 work, I will actually be pretty happy.
Why did he separate the X, Y, Z locations for the Nbody example, instead of separating the Pos, Vel, Acc instead? He's always loading X, Y, and, Z for each body anyway, so it doesn't make sense to split the data on X, Y, and Z. It seems the real problem was that he's caching Vel and Acc values that he doesn't use in the inner loop.
He is a great communicator, I also love his talks. I was a bit pissed off and voiced my opinion about it in a few places, but after reading Herb's comments to the overall situation, I do reckon this was all a big miscommunication issue. If the overall situation was clarified on time, instead of waiting one year for the BUILD announcement, maybe the reaction would have been smoother to the whole out-of-band updates theme.
Visual Studio has an *old* bug where (IIRC) the PCH output files *must* load at a specific address. VS works hard to *attempt* to load it into the right address, but you'll often see failures. Solutions with 100+ projects seem to *mostly* fail once you've got 16 cores. I've known about this bug for ages, and someone recently mentioned they ran into it. If you disable ASLR for Visual Studio (cl.exe, link.exe, etc.) using the Microsoft EMET tool, this bug vanishes. This is a PSA, and I apologize if I got any of the details wrong. I no longer use that many projects in a solution so I can't reproduce the bug anymore. Those of you who do see these random build problems, especially if they vanish after you clean your output directory, need to give this a try. It's a 100% fix, so you should see a difference immediately. Be sure to clean your output directory before testing though.
The example makes perfect sense. Acceleration is calculated based on the distance between every pair of bodies, as shown in the equations, hence the nested loop and O(n^2 ) complexity. Once acceleration is calculated for each body, then it can be integrated into the velocities and positions with O(n) complexity. Edit. I see what you mean. There's no point in storing the acceleration outside of the outer loop if you're only going to use accelerations to updates the positions, and you're only interested in plotting positions.
ok, but where did you put the script that does all this automatically?
Because of the streaming loads. When they are laid out like this, he can load 8 values at once into a single register with a single instruction and immediately do the calculations on it. Otherwise he would load three x and y and two z values in one instructions and to do the calculations he would need to shuffle them around first. The performance difference might not be so big anymore, but if you already start reorganizing the memory layout it is not a big change. And it will significantly reduce the code size.
http://bikeshed.com
ASLR = Address Space Layout Randomization, EMET = Enhanced Mitigation Experience Toolkit. The more you know.
And I do have a very serious answer: I favor sticking to my promises. You are missing the point Stephan, I would never ask my customer such question. "Yes we promised you some additional features that will come with updates but we decided not to do it but to charge you for them instead." That is how I would loose a customer. But you don't have a competitor on your platform so you do not care (well I know you do care but that is how it looks like). I will not help you justify your actions by picking the one you obviously want me to pick. "Latency over bandwidth" is not our (customer) problem. And how is it our fault that "parser gets confused"? Why do we have to pay for this? And you keep doing this kind of stuff over and over. You introduce broken compiler and library. We are then forced to create non-standard non-portable code (because you at MS always know better what committee will vote for) and then you have an excuse to release new version of VS because "fixing current release would break existing code". I just bough VC2012 2 months ago. I was convinced it was time to finally switch to C++11 compiler after Nov CTP and your video. As a long term VC++ user I never bough VC2010 because I know it would only get me in trouble later. VC2008 is pretty decent C++03 compiler OTOH so I waited for a decent C++11 one. First thing I did was trying to port async/future/packaged_task code from boost:: to std:: and bam! Unit tests fail, quick googling and of course "This is an issue with VS2012. It's not the only issue either --- their implementation of the C++11 thread library has several bugs." (http://stackoverflow.com/questions/12774838/stdfuture-still-deferred-when-using-stdpackaged-task-vs11) I now know that I will be not porting anything else until Update 8. But that is not so bad, at least soon I will be able to make my classes prettier and safer with delegating c-tors and initializer lists. I will fianlly be able to clean my unnecessary overloads thanks to variadics, and remove those nasty safe-bool idioms (that are unexplainable to newcomers) thanks to explicit cast operators. Except that no, I will not be doing any of those. Unless I pay again. I will not be using "auto" everywhere (I am with Mayers on that not Sutter). Lambdas are cool and everything but until they are polymorphic, boost::phoenix does much more (and in C++03). So yeah, I am very unhappy with how things went. I care 1000 more about compiler updates than library updates (especially when you invent your own ones). To be fair I ported my Expected&lt;T&gt; to use std exception stuff from boost and that is probably about everything I got from moving from VC2008 to VC2012 code-wise. My point is, please please focus more on VC2012 instead of just creating new buggy VC2013. I know wit will be buggy because that is the businesses we are in and I perfectly accept that. Please stick to the holy standard first and only then when all is properly implemented only then guess what C++14 will bring. I beg you. I know what you are going to say "we never promised anything". Herb says the same now. "I chose my words carefully" he says but that does not change anything. You knew what we would think when you promised "out-of-band" updates. "Updates" are just renamed service packs. We knew what "service packs" are, we expected more from "updates". Since it is way longer post then I wanted (altho only briefly presents my griefs) I just want to add that I think you are great guy personally and I appreciate all your work. I like your videos very much and often direct people to them, I am only unhappy with how things went with VC++2012. High regards.
Herb Sutter himself! I am honored :) I was preparing to reply under your latest post as I already put my griefs there too (Szymon Gatner). I do understand your points, I really do. TBH I am in big part so frustrated because I am a long term user of VC ++ (since VS6) and since I started I constantly have to deal with differences between what is in compiler and what is in the standard. And believe me, it is not an easy way to learn C++ (your books are always near my desk tho). It becomes even harder to be optimistic when things like that happen for VC2012 while my work colleagues look at me from above happily coding using every single C++11 feature using certain free compiler starting with the "C". It is painful to read your suggestions of implementing make_unique&lt;&gt; in overloads instead of using variadics.... Eh, I will buy VC2013, I need those features and you are not giving me any other option to get them. Big fan of both of you guys, so it is very hard for me to still be angry now that you both replied to me. Still unhappy to! ;)
field a bug report?
I noticed that repository, too. And it should be an olive green.
https://en.wikipedia.org/wiki/Parkinson's_law_of_triviality "Parkinson's law of triviality, also known as bikeshedding or the bicycle-shed example,...
Thanks. Re promises, FYI here's the video clip of what was promised -- that we wouldn't wait for the then-current 2-3 year release cycles to ship conformance features, and I think we did what we said here: http://channel9.msdn.com/Events/GoingNative/GoingNative-2012/C-11-VC-11-and-Beyond#time=9m19s . Will continue trying to be clear in the future -- the VS release cadence and structure has changed a lot in the last year and we're all (internally and externally) learning the new release cadence and what the new expectations will be, so we're in a transition period learning it together. I'm glad to see VS as a whole is shipping faster, so that we didn't have to do a separate VC++-only RTM release to keep the promise to deliver faster than the previous 2-3 year band. C++ continues to heat up, and seeing compilers compete to implement the standard is good. :)
Sure, n/p! I found C++ AMP pretty cool :-) Much cleaner than, say, C for CUDA -- and the `restrict` keyword seems interesting and possibly with some further potential (Ada's [`pragma Profile`](http://gcc.gnu.org/onlinedocs/gnat_rm/Pragma-Profile-_0028Ravenscar_0029.html)?) -- although I haven't made up my mind on this direction completely... BTW, things may get interesting if some day some of these get into the standard: * http://isocpp.org/blog/2013/03/n3554-a-parallel-algorithms-library-nvidia-microsoft-intel * http://isocpp.org/blog/2013/03/n3556-thread-local-storage-in-x-parallel-computations I imagine these may be fun to implement! ;-) 
You obviously haven't attempted to implement an `optional` (or `nullable`, if that suits your naming tastes) container before. A 15-30 minute implementation will get you lots of "gotchas". I'm already using `boost::optional` at work, but we generally prefer things from `std::` over `boost::` if available.
Completely understandable. I noticed most of the C++14 features in the VS2013 Preview had your name on the original proposals ;-) No hard feelings, it just would have been a welcome addition. I guess one can still hope for the RTM, but I won't hold my breath.
Looking at https://github.com/cplusplus/LWG/blob/master/src/date.cpp; I'm curious why they use an iterative solution to compute a gregorian date from a julian, when there are well-known non-iterative/branching solutions available, such as the functions at the US Naval Observatory: http://aa.usno.navy.mil/faq/docs/JD_Formula.php Granted, the examples are in Fortran, but it's trivial to port to C, C++, C#, Python, Java, etc.
&gt; updated on a subscription basis. Software vendors *really* want you to pay every month.
wtf dude
FWIW the [Zeus IDE](http://www.zeusedit.com/ccpp.html) has some awareness of the C++ language. It does the standard syntax highlighting, smart indenting, code folding, code templates etc. But it can also be configured to compile, link and navigate errors from inside the editor. It has some support for gdb and the automatic ctags.exe drives a limited form of intellisense, but is generally only good for code navigation. NOTE: Zeus is shareware, runs natively on the Windows platform and runs on Linux using Wine. *Jussi Jumppanen* *Author: Zeus IDE* 
Yes, but I'd have to ask the compiler team for that, and I'm already asking them for tons of stuff. Also, the rule is somewhat subtle - macroizing keywords is forbidden if and only if any Standard headers are included in that translation unit (before or after). So the compiler would also have to be able to recognize Standard headers.
Thanks man. The best thing you can do right now is try 2013 Preview on your codebase and report any bugs you encounter with nice self-contained repros. We have a limited amount of time to fix bugs before they start locking down the product for RTM, and we never really talk about the lockdown's progress because we're cagey with precise release dates. If you wait too long to report bugs, we'll have to postpone fixing them until the version after 2013, which doesn't make anybody happy.
Blue would be more friendly, and obviously, blue paint is much more weather resistant. 
Thank you for the great update there, it's very much appreciated as is the hard work from the compiler and library teams. There is a lot of good stuff here and I can see you do care :) I have to add my voice to those a little upset that it's going to be at very least 18 months or so until the release after vs2013 before we can expect full c++ language compliance and even then there are no promises. But I guess this is close enough to get real work done using the majority of the useful c++11 features. It's getting a little embarrassing though to have 2 free compilers essentially more feature complete than one with an actual developer budget though.
I'm curious why that whole file is littered with C-style casts...
The /r/cpp_questions subreddit would have been a better fit. In any case, could you format your code with proper indentations? It's really hard to read like this.
1) after cin &gt;&gt; human; add something like: &gt;while(cin.fail() || human &gt; 3 || human &lt;= 0) // it means that input was not a number or a bad number &gt;{ &gt; cout &lt;&lt; "wrong input, type 1 or 2 or 3" &lt;&lt; endl; &gt; cin &gt;&gt; human; &gt;} 2) this line &gt;if(3 &gt;= human &gt; 0) does not work as you want in c++, you must do 2 checks: if(human &lt;= 3 &amp;&amp; human &gt; 0) 3) move your game logic from constructor(game()) to some class function, constructors should be used only for initialization
Is [that](http://stackoverflow.com/questions/10828937/how-to-make-cin-to-take-only-numbers) what you looking for?
Others already provided some help, so I will just add that you should throw away the class and use plain functions. You don't need to complicate such a short piece of code with OO.
Eventually you will have to add in more and more checks to establish the data is accurate / crash proof. If you are looking to do user input and definitely want to implement it yourself I would recommend creating a class that does it for you. Makes it OO and simpler for main. It is a good starting place and it is fun creating a class you actually have a use for. Unit testing makes a lot of sense for input classes.
Except if they are trying to learn OO, in which case they have a few issues with their design (constructor does more than constructing)
Yeah its good to point out that C++ isn't really an OO programming langauge, its more like its original name would suggest, C with Classes. Often times a simple function is better than creating a whole class of objects and usually it's easier than trying to shoehorn your design into OO.
He *does* need to split it into two different statements, but the "Yoda Syntax" works just fine in most cases, and is actually used to prevent assignment errors (where someone would accidentally type `human = 3` instead of `human == 3`).
Compilers warn about that anyway; and I think it's better to have the code more readable. Unfortunately Yoda conditions are less readable.
&gt;... when the basic monthly wage is 600 Euro... Are you from Greece? 
&gt; Thank you for the great update there You're welcome! &gt; I have to add my voice to those a little upset Yeah, but there's a limit to how fast we can move in a single year.
Woo! Now to see if my code breaks when using Boost.Log instead of the svn one. Hopefully not to many things:) Great to see the library added to Boost though.
I tried it a while ago and I like it a lot better than log4cxx. I ended up going with glog (google logging), but it depends on what you need. glog is light-weight, and boost log will do anything anyone could ever imagine wanting to do, ever, really.
Its stupidly flexible (in a good way). I haven't even begun to use all that it offers. Though it does go a bit far with "syntax abuse" :) A matter of taste though.
https://gist.github.com/ is an excellent way to post code that is more than a few lines. Formatting code for reddit is just a pain.
Do go on, you had me at nightmare.
sorry, formatting was driving me nuts include &lt;iostream&gt; include &lt;string&gt; using namespace std; class game{ public: game(){ while(total &lt; 20){ cout &lt;&lt; "Your turn" &lt;&lt; endl; cin &gt;&gt; human; if(3 &gt;= human &gt; 0){ if(total &lt; 20){ total = total + human; cout &lt;&lt; "You choose " &lt;&lt; human &lt;&lt; endl; cout &lt;&lt; "The total is " &lt;&lt; total &lt;&lt; endl; cpu = 4 - human; cout &lt;&lt; "Opponent chooses " &lt;&lt; cpu &lt;&lt; endl; total = total + cpu; cout &lt;&lt; "The total is " &lt;&lt; total &lt;&lt; endl; } if(total &gt;= 20){ cout &lt;&lt; "You will never win!" &lt;&lt; endl; } } else{ cout &lt;&lt; "Please enter 1, 2 or 3!" &lt;&lt; endl; } } } private: int human; int cpu; int total = 0; }; int main(){ string yorn; int played = 0; cout &lt;&lt; "Enter 1, 2 or 3 and first one to 21 loses" &lt;&lt; endl &lt;&lt; endl; cout &lt;&lt; "Do you want to play?" &lt;&lt; endl; yorn = "yes"; while(yorn == "yes"){ played = 0; cin &gt;&gt; yorn; if(yorn == "yes"){ while(played &lt; 1){ cout &lt;&lt; endl &lt;&lt; "Cool" &lt;&lt; endl &lt;&lt; endl; game play; played++; } cout &lt;&lt; "Do you want to play again?" &lt;&lt; endl; } } return 0; }
Syntax abuse + Macro abuse From the doc: BOOST_LOG_TRIVIAL(trace) &lt;&lt; "A trace severity message"; Why a log module needs macro like that? 
Eh you can always macro it away to a single keyword :P /s He does explain in the Log tutorials why this is. You can either go the long way, which is ~5 commands (setup log message, get handle to internal logger, see if logger instance is OK and then commiting the log message to the back end)
p.s I know it does nothing, that's what i'm going for.
*facepalm* I stared at this for like ten minutes and didn't see it. Thank you 
Sorry about this, but i just put the semicolon on to it and it still gives me the error.
The next error message gives you a hint: expected ; before system. Plus, you have misspelled the function name (you're missing a 'c').
I love you. 
Why don't you use a better IDE like [Code::Blocks](http://www.codeblocks.org/)? Dev-C++ is pretty outdated.
If that's what they suggested then they have no idea what they're talking about, not to mention that online C++ tutorials are notorious for being complete garbage. Get a reputable C++ beginners book, [C++ Primer](http://www.amazon.com/dp/0321714113) (NOT primer plus) if you want to learn C++ well.
Thanks for the advice, i would do that but i don't have the money.
http://bookos.org/ is your friend.
Please, if you want to learn C++ pick up a good book, like the C++ Primer.
I am guessing that by using a macro you can control logging levels with 0 runtime cost while still using a hassle free syntax. Using they syntax above without macros would result in evaluating the message construction expression regardless of log level.
If i only could replace vs2012 compiler with clang. Recompiling clang is not an easy task.
Well there is advice from a wise man: https://www.youtube.com/watch?feature=player_detailpage&amp;v=4YsVtwOyWmo#t=2436s
Rvalue references v3 is tentatively planned for a post-2013-RTM alpha/CTP release, as Herb announced.
Branch it, fix it, and file a pull request!
my complain is not about "yoda syntax" if(3 &gt;= human &gt; 0) is not the same as: if(3 &gt;= human &amp;&amp; human &gt; 0) in first you compare 3 with `human` and after you compare bool result with 1 gcc -Wall shows it 
Right, that's why I said he still needs to split the statements. Sorry if that didn't seem clear.
The interface should be inherited from using virtual inheritance in case it is implemented by more than one class in the hierarchy. That said it's really unfortunate that a video game is used as the example, as modern video games eschew the complex class hierarchies that come with a single class for each enemy type etc and instead use component-based designs. And finally: Why does the post only talk about explicit interfaces based on run-time polymorphy? This is C++ after all, interfaces may well be implicit (at least until concepts are added to C++ to make them explicit) and used with compile-time polymorphy, i.e. templates.
I think pretty much everything about his example is how *not* to use interfaces in C++. Correct interfaces in C++ should probably just contain pure virtual methods. Of course, there's a time and a place for classes to have pure virtual methods, and whether or not these are really interfaces is up for debate. If you want to see examples of how to do interfaces right, a good place to look would be at some of the various component frameworks out there, like COM, XPCOM, etc. Also, I think the way he's using inheritance to model his monster class is pretty shady. A better way would probably be to compose monster behavior, instead of using inheritance. 
"GoingNative" could also be the slogan of a nudist festival.
Everyone seems to have a different idea of what modern C++ is. It's one of the problems with C++, there are a million different ways to do things and everyone claims their way is the correct and modern approach to it now that C++11 is out. I would never design a class hierarchy the way OP did. I mean maybe it's nitpicking the intention of the post, but for me there's no reason to even have inheritance for what he's doing, just define one single monster class and have its constructor take in parameters that specialize its behavior. That way I can play around with all kinds of funky monster behavior just by playing around with parameters, I could even do this at runtime while playing my game without having to write different classes for them and recompile things. I pretty much only use inheritance when I really, really have to and even when I do have to I limit the scope of my base classes to very narrow domains.
&gt; void main() ...
Have you tried compiling it with GCC (from e.g. MinGW)? At least with the versions between 4.3 and 4.8 I tried there where no issues.
very interesting article!
I would recommend Visual Studio, QtCreator or Eclipse. (For some incomprehensible reason Dev-C++ attracts newbies like moths to a flame.)
&gt; All problems in computer science can be solved by another level of indirection, except for the problem of too many layers of indirection. &gt; --David J. Wheeler
&gt; If different parameters allow the code to call very different functions, it will become a mess quite quickly. I think Kranar is alluding to dependency injection, or policy-based design (whether compile-time or run-time). For example, struct Monster { Monster(AttackPolicy&amp; attackPlyc, DefencePolicy&amp; defencePlyc); }; [...] Monster myMonster{getFireAttackPolicy(), getIronDefencePolicy()}; Now the monster will attack with fire and use some iron thing to defend itself. You can start making concrete kinds of monsters by providing factory types that inject preset groups of policies. [Disclaimer: I'm not very good at D&amp;D analogies]
I don't think you're missing anything. Anyone who has even heard of Scott Meyers or the GoF probably already knows what the Template Pattern is (which he calls NVI for some reason). The author of the article missed the memo that inheritance hierarchies are a terrible way to implement game objects, though.
Entity-Component systems. It's basically using the Composite design pattern for all your game objects.
I believe he is implying component based design is better suited, may be wrong. If that is his intention is it indeed preferred for game objects? I must admit I haven't been exposed to this and nearly do everything via oo inheritance. Hobgoblin being derived from Goblin does seem pretty silly to me. Is anyone able to explain also why the goblin class in that example doesn't implement all the virtual functions? I wasn't quite clear on that. 
Does C++ really allow overriding of private virtual member functions in non-friend subclasses? That's a bit strange because it implies that private members aren't, in fact, private and an implementation detail.
[Direct link to the PDF](http://accu.org/var/uploads/journals/Overload115.pdf). I was a little disappointed that the article cut short right when it got interesting. Don't get me wrong, it's a great read, especially for those that haven't had much exposure to `auto` yet. But with that headline I would have expected actual discussion on the evilness of the keyword, not just an introduction to its usage and semantics. I'm looking forward to part 2.
spoiler: the answer is yes
Exactly; a much better title would have been "Driving around C++11 in our shiny new `auto`".
Yes, but there's only one level of inheritance. Inheritance isn't bad, but you have to make sure to keep things simple. Components give you a way to only have one or two levels of inheritance.
You seem to be missing the point, Boost.Asio is a lower level library than netlib. Asio was design so that people can implement whatever top level protocol that want; netlib is the http api. In a manner of speaking netlib is to asio what clang is to llvm.
it's the most std c++ / boost like. So feels more at home in a code base that uses them (pretty much everything uses std:: right?) Furthermore, is that I believe that it is targeted at inclusion into boost, so in the future you would have one less dependency to manage.
Why not just link directly to the [book's page](http://pragprog.com/book/lotdd/modern-c-programming-with-test-driven-development)?
you are correct
I guess I was too optimistic after reading lots of articles about component design being superior to inheritance in games.
Also it's easy to mix and match like when you want a door (decoration, can be opened/closed...) that can also attack (enemy): Give it a suitable interaction component plus an attack behavior component instead of creating diamonds in your class hierarchy.
Only really simple-minded people use concepts such as "evil" (or "good" for that matter). But FTR "auto" isn't "evil." Same goes for the C preprocessor (it's just horribly underpowered).
Bought the beta. I am enjoying reading this book. Very practical. Modern C++ based (C++11). 
&gt;If you program in C++ you’ve been neglected But I haven't? There are at least half a dozen unittest frameworks for C++ out there, from UnitTest++ to boost. And even more articles.
The problem is the proliferation of attributes in game objects. Some objects are not movable, some don't get drawn to the screen, some aren't affected by physics, etc. Entity-component systems aren't without their problems, but I believe they are now the "accepted" method for programming game objects. ^^Disclaimer: ^^I'm ^^not ^^a ^^game ^^programmer ^^nor ^^have ^^I ^^programmed ^^a ^^game.
This is an awesome book. I am a big TDD proponent and I have changed the way that I write tests. Great practical advice, great topics, and even a chapter on working with legacy code. I cant recommend this book highly enough.
Reference-to-const, yes. I wasn't pouring too much thought into my example, sorry. Taking them by value would not permit run-time polymorphism and would really defeat the purpose of them being injected in the first place. Alternatively, you could use static polymorphism and inject the policy types through template parameters to the struct; then you could pass the policies by value. This would really be a performance consideration though, so I wouldn't default to it. **Edit:** Another run-time alternative would be to inject factories by value. External clients would then seed the factory with the parameters that will factory the right policy type.
Excited about this one, which brings experimental Qt Quick to Android. New Qt creator bundled too!
Template Method addresses a language-independent OO concern - how to add customization points to a general algorithm. NVI, though structurally similar, is a C++-specific technique with a different emphasis, namely to separate the public interface from the derivation interface. To see the difference, NVI proponents advocate the public-nonvirtual-calls-private-virtual structure even when the 'algorithm' is trivial, and the nonvirtual function consists solely of a call to the virtual. I think the term NVI is due to Herb Sutter.
Right. Sorry. Redditting from mobile has its drawbacks :(
Pass by ref is a double edged sword. It does allow multiple threads to use the same immutable input data without incurring the cost of copies. But yes, screwing up by passing a reference to a loop temporary is a sure way to spend a few hours debugging.
TDD isn't about unit testing, or at least isn't just about unit testing. The book "Growing Object Oriented Software, Guided by Tests" really opened my eyes to that. And to the fact that working with C++ for a number of years meant I'd been living under a rock with respect to modern approaches to testing :) Unit testing was originally mostly to support refactoring - cleaning up code to repay tech debt while preserving behaviour. M Mock tests, which are used quite a bit in TDD, are good for system testing to verify that the behaviour is correct (rather than unchanged), and when done well are fairly easy to maintain and give early and decent feedback of various design smells in APIs that you're writing. Until recently there wasn't a good mock test framework for C++. Now there's GMock and Turtle. [This](http://games.greggman.com/game/using-a-mock-library-to-make-unit-testing-easier-in-c/) is a nice into to GMock, although Growing Object Oriented Software is still highly recommended (and perhaps this book as well or instead - we'll see).
Not to mention QtQuick Desktop Controls and QtQuick Layouts making it easier to write cross platform desktop UIs. And by default they are all accessible by default so people with accessibility issues should be exited about this release as well.
They blah blah about developing for iOS and android. I cannot find a single bit on how to do this on a Mac. Can it be done on a mac? 
Also, why header only? A lot of the code can be moved to .cpp files. You single include version contains more than 8000 LOC, which is too much. The smaller the header, the better.
I believe I made my [presentation](http://www.youtube.com/watch?v=_6_F6Kpjd-Q) on a mac 
IIRC, this also applies to std::async. Just mentioning.
Header-only code is extremely easy to build.
&gt; So, remember it. If you want to pass something by reference to a thread, you need to use std::ref(). If you want to pass something by const reference to a thread, you need to use std::cref(). But yeah, don't do this! The API is designed the way it is precisely because keeping cross-thread references is really unsafe — you *want* the thread to own the memory and resources passed in through its arguments, whether moved or copied there. *If* you know what you are doing, and are 100% certain that the calling thread will not access the memory until after `thread.join()` has been called, or all access to the object is protected with a mutex, and copying is too expensive, and moving is not an option, you can potentially use `std::ref` and `std::cref` — but only then. :)
Actually the same separation of implementation and interface is there (except for templates, of course). The "implementation" part is only compiled into one translation unit (the one you #define CATCH_CONFIG_MAIN or CATCH_CONFIG_RUNNER in). The other TUs must still run the whole file through the preprocessor but that's really fast and adds no noticible overhead (at least last time I profiled it). Because all the implementation (what would be in .cpp files) is compiled into a single TU this can actually be *faster* than separately compiling a set of .cpp files (cf. "Unity Builds") - although, in practice, if that was an issue you'd just build a separate lib, of course (you can still do that, if you'd like). The point is that there is not really any compile time overhead here due to being header only.
Header-only libs are trivial to include in any build environment - #include "header.h". Unlike linking logic, inclusions, indirect dependencies and so on that plague other libraries and inconsistent inclusion between different platforms. In particular if your library somewhat fits in a single header, you should definitely use one header.
1. Unlike the "header only" question, using templates does add a bit of overhead, yes. Most, if not all, test frameworks that evaluate lhs &amp; rhs (or actual &amp; expected) - whether through expression templates or the traditional family of assertion macros - use a similar amount of templates, though - along with template tricks to determine whether a type is streamable into an ostream. I don't believe Catch adds any appreciable overhead compared to those - but I have not benchmarked that. I use Catch daily against a 1m LOC quant library (not full coverage, but there are hundreds of test files) and have not found it to be a bottleneck. It's not using any of the really expensive template techniques, which typically involve recursion - faux variadics being one of the prime culprits. 2. Depends how you define stable :-) Does it have bugs? Yes - see the issue tracker on GitHub. A lot of people - including several banks - use it for serious work and are very happy with it, though. Is the public interface set in stone? I can't rule out any further changes that may be breaking in specialised cases (e.g. when interacting with the Catch runner programmatically) - but the whole point of declaring it (finally!) 1.0 was that I now believe those interfaces *are* stable enough. It also means that I'm turning my attention more fully to stamping out those remaining bugs. I feel a follow-up blog post coming on.
Thanks for the clarify. I guessed the library doesn't need recursive template so the compiling time should be OK. Despite of that I dislike the "header only" approach, I delicioused your project and may try it when I have time. I'm not very satisfied with the unit test framework used in my library. Again, I like the auto deducing of lhs and rhs, that's so smart! Thanks for your work. 
Thanks. Due credit for the lhs/ rhs deduction goes to Kevlin Henney, though
Why not just? public: virtual void receive_damage(double damage) = 0; virtual void interact_with_chainsaw() = 0; virtual std::string name() const = 0; I don't see the point in having public wrappers that just call private virtuals. Why the protected constructor? You can't construct a class with pure-virtual anyway. Why delete the copy constructor? Again, you can't construct a class with pure-virtual. Why delete the assignment operator? You will only have references or pointers of the interface type, so if you've gotten to the point of invoking the assignment operator you've already derailed. I'm not convinced this is "done right".
http://solarianprogrammer.com/2011/11/01/cpp-11-lambda-tutorial/ Simple and to the point lambda example.
I would first consider using `std::shared_ptr&lt;&gt;` since that's what its intended for.
Added a Synopsis to the [Readme](https://github.com/martinmoene/lest/blob/master/README.md) on GitHub . Thanks for pointing out.
I actually find this behaviour more correct? Just define a friendly swap function and on move construction just call | swap(*this, rhs) and you are done. If you have pointers then use class member initialization to set them to nullptr, everything else should be a non-issue, ensuring that swapping won't leave the temporary object with random pointers. Plus, this has the advantage that it has the strong exception guaranty, since swapping and destructing _should_ be safe operations. If they are not, something is probably wrong in their implementation. Edit: I see he does mention this toward the end. Good man! :)
Rant: the naming convention smells like teen Java :P It really could be more C++ kosher, like STL/Boost. ~~ The lower-case namespace naming crusader ;-)
One of the consequences is that (some) movable objects pretty much _need_ to have invalid/default state in which destruction does nothing. This can be solved elegantly for some resources (like pointers, just =0 them) but for other it may mean boolean 'valid' state or 'invalid handle', which is ugly as night. &gt;It is always possible to use a copy operation in place of a move operation; i.e., to implement move as copy. &gt;It is always possible to use a swap operation in place of a move operation' i.e., to implement move as swap. Im not sure i follow this, but this doesnt seem to be true. Standard containers have more requirements on moving and can not be implemented with simple swaps, nor copies. 
Yeah it's a weird thing to say... you can move a unique_ptr but you can't copy a unique_ptr, and hence it is not true that "It is always possible to use a copy operation in place of a move operation; i.e., to implement move as a copy."
Yes, just wanted to say that in a way move() is conceptually closer to swap()!
That'd be lots of work for little value, and it might break stuff. Experience has taught me that doing exactly what the Standard says is usually safe, while increasing cleverness is increasingly dangerous. Some of the dangers that come to mind: Argument-Dependent Lookup cares deeply about the true names of things, which namespace surgery will interfere with. Consistently using the same _Detail namespace might avoid that, but ADL should be treated with respect and fear. More obscurely, introducing a _Detail namespace would increase the lengths of our mangled names, and annoying things happen when they get too long (the details are mind-numbing).
Interesting, I had come up with some of those reason, the other, although related somewhat to the symbol names, is that errors will get more cryptic. Still it would be nice for writing better code (or at least more portable). Maybe modules will help us with this in the future.
Where can I read how lhs/rhs are extracted?
&gt; This can be solved elegantly for some resources (like pointers, just =0 them) but for other it may mean boolean 'valid' state or 'invalid handle', which is ugly as night. Fortunately, I have so far found in practice that if a resource is movable then the reference to the resources is most likely nullable. &gt; Im not sure i follow this, but this doesnt seem to be true. Standard containers have more requirements on moving and can not be implemented with simple swaps, nor copies. I think that his point was just that copy and swap obey the most basic requirements of a move operation, not that they are valid in all circumstances.
As I just replied to the parent, I am pretty sure that his point was not that copy and swap are always options that one has available but rather that when they are available they are perfectly reasonable implementations of the move operations as they obey the necessary semantics. Put another way, there is nothing that says that you *have* to implement a move operation by implementing an invalid value for your type.
In the code (Look for ExpressionDecomposer and follow the trails ;) See also [Kevlin Henney on Rethinking Unit Testing in C++ (Video)](http://skillsmatter.com/podcast/agile-testing/kevlin-henney-rethinking-unit-testing-in-c-plus-plus).
Nice, gentle introduction, but nothing surprising. 
[xkcd: Convincing](http://xkcd.com/833/)
I don't think that we are actually in disagreement. I was not trying to imply that a null value counts as a valid value to which one can validly apply operations, only that the existence of a null value gives us a mechanism by which we can flag a resource as being invalid without having to create an additional boolean variable for this purpose; upon destruction an object will check to see if the resource is null, and if so then it doesn't do anything, rather than passing an invalid value to a close function.
 *(new ... is suspicious. You create something that you will throw away without being able to delete it later. Also addFace(Face face) is strange because you're not using a reference and get a copy instead.
 addFace(Face(NULL, vert, parts.size() - 1)); and void addFace(Face&amp; face) { you should also improve your formatting, standardize your indexing (are you missing one vert[] in the for loop?) and generally think about that piece of code.
I am not surprised that you age getting segfaults. That code is very. ermm, c like. Anyway, build with high debug levels and no optimisations then run it in a debugger like gdb and backtrace from the segf. EDIT WHOA: also this: currentObj-&gt;addFace(*(new Face(NULL, vert, parts.size() - 1))); specifically: *(new Face(NULL, vert, parts.size() - 1)) is very bad indeed. You have created a leak right here as you pass it to addFace. This would be better. addFace( Face(NULL, vert, parts.size() -1) );
Actually, that is very good, if he knows what he is doing. you need a copy any way for the vector. He should go on to do: addFace(Face face) { faces.push_back(std::move(face)); } This way you have a fairly optimal `addFace` member function for both the copy and move case.
Where does your program crash? There are lots of things to improve here, but I can’t see anything that would make it crash here. If you can’t get access to a debugger, then add some asserts: assert( parts.size() &gt; 0 ); assert( currentObj != 0 ); assert( parts[i].c_str() != 0 );
Wait, it actually did that!
&gt; I don't think that we are actually in disagreement. We probably are not ;) My entire point is really that 'before moving' cleanup code could be very elegant, because it knew that resource is always there (due to RAII) when you are hitting dtor. This invariant (existing object has valid resource) is no longer true with 'moving'.
Thanks for pointing out that. I am not missing a vertex because the first element is not a vertext. This is part of code to parse a WaveFront Obj file. I changed the code to this and it worked. GLint* vert = new GLint[parts.size() - 1]; for(int i = 1; i &lt; parts.size(); i++) vert[i-1] = atoi(parts[i].c_str()); currentObj-&gt;addFace(*new Face(NULL, vert, parts.size() - 1)); void addFace(const Face&amp; face) { this-&gt;faces.push_back(face); };
But now you are leaking a "Face" object in every loop iteration. Remove the "*new " part.
Don't see why everything can not be elegantly moved. If you have default initialization which puts the class into some sort of valid (but actually invalid/uninitialized) state, then you just swap the instances and let the destructor do its thing. If some sort of default instantiation is not possible, then maybe its a heads up for bad class design? Remember, the default constructor can be declared private and only be called when move constructor is invoked, making sure no one can make invalid instances of your class. But class member default initialization would be the most sure way to go and can avoid the default constructor or even better, declare it default.
That is a very valid point.
If we are gonna nitpick, it should be addFace(Face face) { faces.emplace_back(std::move(face)); } This should ensure perfect forwarding is the function gets inlined?
&gt; If you have default initialization which puts the class into some sort of valid (but actually invalid/uninitialized) state, then you just swap the instances and let the destructor do its thing. The whole point of RAII is to not have this invalid/uninitialized state. &gt; Remember, the default constructor can be declared private and only be called when move constructor is invoked It doesn't change a thing, really. You still will be able to make those invalid objects by moving object to some place else. A a(stuff); A b(std::move(a)); //a is zombie 
Not at all. There is an r-value overload for `push_back` for exactly this reason. http://en.cppreference.com/w/cpp/container/vector/push_back The general mantra (I think should be) is if you are copying or moving into a vector use `push_back`, if you are calling a non-replicating constructor emplace. (we really need Scott or Herb to weigh in here). `emplace_back` is for inplace construction; it's arguable move construction, sure, but that's not really what emplace is for; all you are doing is increasing compile time and making an error messages worse by using the complex `emplace_back`. The inlining thing doesn't hold water either, given that `std::vector` like all std:: containers are header only template classes anyway. TLDR. I would be very surprised if emplace_back produced better code that push_back, and I would not be surprised if push_back was quite a bit faster to compile.
No, you need a template to get perfect forwarding: template &lt;typename F&gt; enable_if_t&lt;is_same&lt;decay_t&lt;F&gt;, Face&gt;::value&gt; addFace(F &amp;&amp;face) { faces.push_back(forward&lt;F&gt;(face)); } But the right thing to do here is to perfect forward the arguments so you can directly emplace into the vector instead of newing a pointer, dereferencing it, and copying the contents: template &lt;typename ...Args&gt; void addFace(Args &amp;&amp;...args) { faces.emplace_back(forward&lt;Args&gt;(args)...); } which is faster, more correct (doesn't leak memory), and better looking. 
except now you have to put addFace in the header. which has drawbacks for such a minor optimisation over: void addFace(Face f) { faces.push_back(std::move(f)); } Moves are cheap, rebuilds of code bases aren't. I would only do what you show in a hotspot after profiling or in a library.
OK so I tried it. compile times slight favour `push_back`. But it's really very minor. At -O3 (gcc 4.8) doing this experiment with `std::string` `push_back` was marginly (ever so slightly) faster than `emplace_back`. (I suspect whatever difference was dwarfed by the cost of allocating those strings).
Moves aren't always cheap, especially with large types.
Here's an advice: http://klmr.me/slides/modern-cpp/
Qt Creator has grown an enormous memory leak since the late 2.6.X releases. I've been meaning to solve it, but I loathe Visual C++'s free memory debugging tools. Otherwise a great full featured IDE, that memory leak is killer. 
If it helps. Here is a repo with all the source code. https://bitbucket.org/novasharper/daedalus/src/ The code that is giving me problems is in the Obj.cpp/Obj.h file. The code is in the Game_C++/src folder
Someone pointed me at this talk from Alexandrescu, where he presents such a scopeguard solution in C++11 working with lambdas: http://channel9.msdn.com/Shows/Going+Deep/C-and-Beyond-2012-Andrei-Alexandrescu-Systematic-Error-Handling-in-C
Ah, so stupid of me to forget that. Will ponder a bit if there is a good way to combat this.
I switched to using std::move, but I get this problem. It happens at the end of the cycle of the loop. Thread [1] 0 (Suspended : Signal : SIGSEGV:Segmentation fault) libstdc++-6!_ZN9__gnu_cxx18__exchange_and_addEPVii() at 0x6fc5b93f libstdc++-6!_ZN9__gnu_cxx9free_list8_M_clearEv() at 0x6fc60eb6 libstdc++-6!_ZNSs4_Rep10_M_disposeERKSaIcE() at 0x6fc89d16 libstdc++-6!_ZNSsD1Ev() at 0x6fc8c7e3 std::_Destroy&lt;std::string&gt;() at stl_construct.h:95 0x40f11a std::_Destroy_aux&lt;false&gt;::__destroy&lt;std::string*&gt;() at stl_construct.h:105 0x40c6f7 std::_Destroy&lt;std::string*&gt;() at stl_construct.h:128 0x40f0b4 std::_Destroy&lt;std::string*, std::string&gt;() at stl_construct.h:155 0x40f0d0 std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt;::~vector() at stl_vector.h:403 0x40d826 _fu2___ZTIi() at Obj.cpp:116 0x40219a Game::Game() at Game.cpp:13 0x402408 _fu1___ZSt4cout() at main.cpp:29 0x401810 
What's it for, exactly?
I do have some expierence with it, I'm not on smarterer, neither trust them, could you post the questions?
I am not a blackberry dev but I got the sample question right ;)
Smarterer tests are for users to test their knowledge and find out what skill level they are on in a variety of topics. We want to get as many testers possible for this new test (and receive feedback) to make sure the questions are appropriate for Blackberry Dev. 
Unfortunately, I can't post the questions here as it would defeat the purpose of 'testing' users' ability on Blackberry Dev if they can see the questions online. 
You probably also want to disable copying/assigning if you're going to use RAII styled destructors to do the clean up.
Okay, so using a combination of things, I fixed all the bugs. The latest commit is in the repository.
yes, ofc a real implementation would be more complex than just having a constructor and a destructor. But that is left to the reader to implement. For me boost::scope_exit works fine.
shared_ptr doesn't really help with that very much. shared_ptr is just to stop an object being destroyed until all references to it are destroyed. In this case it can ensure that the object isn't destroyed while its still in use in another thread but you still have all the race conditions from the shared memory. You can often ensure that the object isn't destroyed otherways too, just keeping it in scope until all the threads have joined for example. 
Even hearing the name brings back bad memories. My condolences to any programmer that has to support it. I am so glad that I don't have to use VC6 ever again (I do however miss VB6. There has never in my mind been a real replacement that did it justice).
Why would anyone choose to be a Blackberry 10 developer?
Compared to vc7 through vc11, It compiles fftw with modest but significant improvements some range of vector sizes that we deal with. We had a customer file a bug based on a performance loss from our compiler upgrade. Now for performance reasons, we have a few tiny pieces of our lib built with basically every version of msvc. 
Why is the `BOOST_SCOPE_EXIT` not immediately after the the `beginInsertRows`? The vector push_back can throw, which would result in `endInsertRows` not getting called.
true. thats a good one :) fixed.
This has to be possibly the greatest waste of Internet bandwidth ever! Nobody in their right mind will bother with BB10 thus this idea that someone would sign up for this program would immediately bias the test result based on the high proportion of nut cases taking the test. CPR works on humans but has little positive impact on dying companies. 
&gt; Even hearing the name brings back bad memories Seriously, why? When it was released (1998?) it was one of the best compilers out there even if you don't count the IDE. Sure, using it today makes zero sense, but do you even remember gcc before 3.0?
I did survive the egcs wars! :)
This article is all over the place, I'm confused about what his point is...
the point is, to make sure that a certain part of function is executed, even if a exception is thrown in the code which comes logically before. mutex.lock/unlock is a common example.
It was great when it came out in 1998. By 1999 and 2000 people were using more advanced C++. I was somewhat involved with boost during that time period. Supporting Visual C++ 6, was often a major effort since it did not support partial template specialization and a lot of its advanced template support that it was supposed to have was buggy. Go to the boost group http://tech.groups.yahoo.com/group/boost/ and do a search for VC6 and you get an idea. It wasn't until Visual C++ 7.1 in 2003 (5 years) that Visual C++ finally got partial template specialization. 
I needed to do an app for work, it's actually a very nice platform at this point to develop for. The documentation is the only shit bit.
The VS2013 Preview brings lots of C++ goodness to improve our productivity. There are more than enough C++11 language features as well as IDE enhancements keep us busy learning through the preview! C++11 introduced Lambdas to the C++ language, and many compilers have provided support for several years (i.e. VC2010). Come learn why C++ Lambdas enable you to easily do things that were previously either difficult or practically impossible! (we will even take a peak at how C++14 promises to take Lambdas to the next level) 
&gt; Seriously, why? Because VC++6 has terrible C++98 ISO compliance let alone C++03. 
To call arbitrary function `f` on scope exit, simply use the `shared_ptr` custom deleter. shared_ptr&lt;void&gt; guard(nullptr, bind(f)); 
I don't see a reason for an argument: "Parkinson's law of triviality" (abbreviated "PLoT") is as unambiguous and formal as one can get, thus clearly superior in the context of a Standard, where unambiguity is the highest of virtues. It also has this "just google it if you haven't heard of it" ring to it - a quality which the obscure, injoke-ish "bikeshed" (which BTW isn't even a word one'd find in the [Oxford Dictionaries](http://oxforddictionaries.com/spellcheck/english/?q=bikeshed)) can only envy.
Ah, VC6 ... this brings memories. I had to save my allowance to buy it because I couldn't get a pirated version anywhere. (Pre-P2P and DSL Germany). The first software I bought :) 
http://www.reddit.com/r/cpp/comments/1hkb8z/qt_51_released/cavq61h
I've had 6 (or will have had 6, once I receive my replacement for bricking a device for applying an OS "upgrade") blackberrys. I can honestly say, as a user, that each successive blackberry has been worse than the previous. There is no way in hell I would actively choose to develop an application for this cursed ship. It's sinking. Go android, go iOS, skip blackberry. My very first one worked the best. Sure, I had to dock it to my PC to get address book updates, but it sent and received email like a champ. Now, I can't even scroll through a single email, or see who it was sent to without the device hanging. If I was writing a review for a consumer, it would read: "Do. Not. Buy."
I've never used GTK+, but I have used Qt fairly extensively. We ported a large number of old CDE/Motif panels to Qt a few years ago. Just looking at the first two charts in that story, it seems to me that even simple things like creating a new widget is overly complicated. Is GTK itself this complicated, and GTK+ just carries it along?
While I appreciate the ingenuity of this solution; I would think unique_ptr would be more efficient (no reference counting and extraneous allocations). And, additionally, this is not an "obvious" solution.
GTK is not overly complicated given its constraints. Most of the complexity you see here stems from the fact it's written in C. Because C has no native object model they have to emulate it through GObject. Conceptually it's no more complex than Qt and its MOC system, but the problem is the programmer has to solve many of the problems (over and over again) that the C++ compiler and the MOC preprocessor solve for you if you're using Qt.
Are there no decent C++ GUI toolkits available? GTK is C based, and Qt uses moc to make you write your code in a language almost but not quite c++. Are there no C++ toolkits?
I just added expression decomposition to [lest, a tiny C++11 test helper](https://github.com/martinmoene/lest) using the same technique as CATCH, but in a very basic form.
I don't understand what do you mean by reverse the actual regex. Can you give some examples?
&gt;CPR works on humans but has little positive impact on dying companies. The same could be said about C++
I could see `decltype(auto)` being used a lot. They should probably provide a shorter keyword to avoid the appearance of verboseness. EDIT: I know, I know. Any new keyword could break a lot of existing codes.
Writing mocks in C++ in any framework can be quite painful if not impossible at all.
Why not just use plain "auto" instead of "decltype(auto)" in the example he used? I must be missing something here.
auto drops const / volatile qualifiers, so to get the full auto behavior when you have something like const ComplicatedType GetImmutableCrap() you have to know its const and type const auto variable = GetImmutableCrap(); or use decltype(GetImmutableCrap()) variable = GetImmutableCrap(); In general decltype() has more complex deduction methods that auto does not.
Also, `auto` intentionally drops references. As Scott Meyers' article explained, `auto` follows the same rules as deducing `T` for `foo(T)`. Given `const int&amp; bar()`, calling `foo(bar())` instantiates `foo(int)`, so `auto x = bar()` is equivalent to `int x = bar()`. `decltype` isn't really "more complex", it's just different.
It was quite useful to me to understand how it works under the hood, hopefully it will be useful for you too
I've been using HippoMocks together with CATCH for a few months to pretty good success; though, the framework is pretty "crazy" (you really have to learn its nuances). I've been wanting to switch to Turtle Mocks, but it doesn't seem to handle move-only types just yet. Here's a unit test I wrote yesterday in CATCH with HippoMocks: https://gist.github.com/bkuhns/8904eb5d6052fe066556 The `SETUP_EXPORTER_AND_MOCKS` macro^[1] sets up a handful of mocks to be used in each of my unit tests. Below that you can see me resetting the default expectations on one of the mocks and setting a new explicit expectation for the test. **Update (2013-07-11):** The Turtle Mocks dev seems to be working on the issue of move-only types and has fixed part of the problem already^[2]. [1] *I generally hate macros, but this one simply calls a single setup function and provides a few lines of variable declarations with a call to std::tie() since the setup function returns a tuple of pointers to mock objects created by HippoMocks.* [2] https://sourceforge.net/p/turtle/tickets/15/?limit=10&amp;page=1#0357
Note x64 is very different, but this is still a great article. Was very handy for me once when I had the (mis)fortune of writing a custom debugger. 
From what I remember, x64 handling is mostly like DWARF exception handling except with the ever-loving MS twist.
It is an error to feed 0 to FreeLibrary.
Hmm, can't tell if sarcasm...
If true, a simple check in the constructor body for a nullptr and throwing an exception seems suitable?
No. That's not good enough - ~~default constructed~~ moved from one will still have zero pointer. A lambda with check against bad handle needs to be used instead of FreeLibrary and forward there only if the handle is OK.
Hah, this is good to know. Thanks for taking the time and benchmarking it.
Considering that MBCS was really only ever in place for the ANSI platforms (non-NT), this should not come as a shock. If you're going to globalize your application in some form or fashion, use Unicode instead of MBCS. However, I do feel for anyone who has to port from MBCS to Unicode as it may not be straight-forward for every situation.
I haven't touched MFC since VC6. I remember it gave me nightmares and spurred me to write my own WinAPI wrappers. Has it improved since then?
&gt;Sometimes people say that the unspecified order of evaluation permits the generation of more efficient code but it is very hard to believe that this effect is ever usefully exploited. Calling sort on a vector before doing a find on it will result in faster code than the other way around. Not sure why you'd ever pass the results into a function, but I just wanted to provide an example of code performance changing without the end result changing.
I don't understand your example. Can you show the (simplified) code?
[Here](http://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-an-unsorted-array) is a good thing to know about. Essentially, sorting an array before processing it allows the compiler to improve branch predictions. Sorting before doing a find should theoretically allow the compiler to use a binary search instead of a linear search. If you intend to have a sorted list in the end, you are better off sorting before doing anything else.
It's mostly just gotten more features.
Any good articles on that that I can read?
&gt; Essentially, sorting an array before processing it allows the compiler to improve branch predictions. Yes (sometimes), but: &gt; Sorting before doing a find should theoretically allow the compiler to use a binary search instead of a linear search. No. Compilers in the real world don't perform such algorithmic transformations.
I am not very knowledgeable with the STL, but won't find use slightly different logic if the vector is sorted?
The compiler isn't aware that it is sorted. 
Yeah. The library definitely doesn't have such knowledge (we do lots of tricky things in the STL, but determining the sortedness of input is a linear time operation). In theory, the compiler has TU-wide knowledge (program-wide, in the case of LTCG), but in practice compilers will not notice "oh, you're sorting X, then searching X, I can use binary search". They aren't Skynet. Also, the processor does branch prediction, although compilers try to arrange code to make it easier.
You have to manually call std::binary_search instead of std::find, then it will be faster.
&gt; In a sensible world, left-to-right evaluation would be specified by the standards. Failing that, individual compilers should specify this. I disagree. At least with the second statement. It would mean that a valid gcc program is not necessarily a valid clang program. Efforts were made to remove gcc extensions. I remember for example that the following syntax was valid: int foo(int y) return x=1 { if(y&gt;10) x=y; } This was very handy, because it is more concise than either of the standard alternatives. Now that is one vendor specific defined behaviour that can be changed: programs using this will just not compile with modern gcc (or other compilers for that matter). But what if you write a program that relies on vendor specific behaviour, that can't be checked easily at ~~runtime~~ compile time? I would argue that evaluation ordering is one of them. Nowadays you have to be aware that the ordering is undefined, which is ok, because at least you don't consciously introduce these problems. But when people start writing vendor-specific code that can't be checked at compile time, we are really in trouble. The reason btw. why the standard doesn't define left-to-right is because it has the potential to make the emitted code more complex than needed: It's always more natural to evaluate arguments in the ordering that they are to be put on the stack. And that happens to be right-to-left by default on many architectures as far as I am aware, and for a good reason: consider varargs functions like printf(). If you push the left argument to the stack last, the called function can expect it at a fixed known stack pointer offset on entry. Otherwise the function would have to have some more information about how many arguments have been passed. So if anything, the standard behaviour should be right-to-left evaluation for technical reasons. But I can understand why this hasn't been defined yet: it looks unnatural and it is opposite to the evaluation guarantees of the comma operator as well as the logical operators.
No problem. :)
Seconded.
About writing a debugger? It's relatively straightforward but you have to deal with reading and writing memory in another process so it gets confusing quickly. The msdn articles and sample projects with Debugging Tools for Windows cover most everything, along with articles like these (and another describing the type_info RTTI structures for getting types of exceptions etc). The docs were sparse last I looked. 
&gt;Sorting before doing a find should theoretically allow the compiler to use a binary search instead of a linear search. I find that really hard to believe. Could you provide any evidence that a compiler does this or has ever done this? i.e. supplies different library implementations depending on its ability to predict branches. AFAIK the implementation of the STL is never modified by the compiler _at source_. At the hardware/asm level the compiler does all sorts of magic to make code run faster, but even there, I have never heard of a compiler changing sort algorithms. edit: NVM saw other reply.
I was thinking more along the lines of an internal flag of vector being set and then a specific search implementation then being used, but I suppose that is not what is happening.
Except it doesn't work for all patterns. Ex. "([\^,]+,[\^)]+)"
They aren't Skynet, yet. Though this is a pretty obscure optimization. I'm not that familiar with compilers, but it should be possible for a compiler to drag along additional metadata about object state over the compilation? This would require additional metadata about object implementation and function side effects, but this seems like a perfect application of generalized attributes. Or am I grossly underestimating the problem?