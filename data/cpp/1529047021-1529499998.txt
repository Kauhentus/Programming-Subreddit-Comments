I've never heard about std2 namespace, I'm just happy that this crazy idea was abandoned.
Honestly: the idea to be able to finally make some breaking changes and fix mistakes made 20+ years ago seems quite compelling to me.
Could you elaborate?
The latter also makes my eyes bleed.
There are breaking changes (see auto_ptr just to quote one) and C++ committee has agreed to do more in the few years http://shape-of-code.coding-guidelines.com/2018/04/14/the-c-committee-has-taken-off-its-ball-and-chain. Having a std2 would be ugly as hell, people will start to guess “where is the thing I want to use? ah yes, it's in std2” and then we will have perfect valid useful modules in std and brand new ones in std2 so there at a time you have non-deprecated features that is still in std instead of std2. There is already std::experimental or tr1 for things that are still not enough polished to be included in std.
It is an argument for not bothering with trying to standardize `#pragma once`, but still keep doing whatever you were doing and that works until and much better replacement to either option include guards or pragma is out.
&gt; However when I used Java, you really had to have your project set up in this specific way or there's pretty much no way to build and run your application. Sure, but we are not talking about designing a new language here but adding things to a language that is dozens of years old, has billions of lines of code written (compiled on single machines for embedded targets as well as on distributed compute clusters), almost no universally agreed upon common best practice and that is very keen on not breaking any existing code. I really can't see how you would be able to put a required project structure into the standard.
... just import the perfectly valid modules in namespace std2?
using libraries on windows is easy if you use vcpkg and cmake. But there are always people who have to be different.
Actually depends, for large stuff and C++ it's very ugly, but feasable. LaTeX and similar "clearly" structured stuff, can easily be written via vim (works great on the phone) with ssh or something.
Alluding to a different current discussion. Changing size_t from unsigned to signed would be possible by moving to std2.
I don't think anyone was talking about deprecating std or making std2 a other form of std::experimental. But the current design of the standard library largely comes from more than 20 years ago. In that time, the language has changed, the applications have changed, the platforms have changed and people have gained a lot of experience. Any other library out there would have at least gotten 1-2 major revisions to adapt to that changing landscape that go beyond removing a handful of rarely used types for which there exist a drop in replacement. There is just so much you can do within the confines of the existing API design and stay consistent.
Thanks both! I didn't even know about `inner_product` ! I was trying to come up with a decent example to show lambdas, and I'll have to think of a new one now! Or at least show the `inner_product` version too!
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
Not really in faviour of this paper, especially changing dereference. It would only recreate the problem of C syntax order, not solve it.
So you want to break user code using std::string instead of std2::string?
1. Prefer combined as I group header and source files by "module" into subdirectories. I do have an `include` folder for public headers if I am developing a library. 2. Having a src/include split per module is just too unwieldy. 
The problem here is that it's the standard library, so you can't just opt to stay on an older version. If you could choose library versions independently of language or compiler versions it would be much easier to make breaking changes.
What kind of mistakes are there that need fixing? Just curious since i'm not keeping up with all that stuff.
&gt; auto minAllowedIndex = max(0, targetIndex - 3); This doesn't compile because of mismatched types (int vs size_t) :)
And even if we do hit that limit, assuming Moore's law having it be unsigned will save us for like 2 years.
How would you fix this pragma once problem? By renaming the header file? That works only if the header isn't part of your public interface. If the "unresolvable header" (for lack of a better word) is supposed to be deployed in a user's `/usr/include` suddenly, renaming the header is a breaking change. &amp;nbsp; Header guards have a similar problem - two distinct header files have accidentally wound up having the same include guard. Here, the solution is to rename the header guard macro, not the file. Since that macro is never part of a library's public interface - this isn't a breaking change. The "optimization" that `pragma once` introduces over include guards leads to lowered disk I/O, which is expensive and so on... we all know the story, right? Well, GCC and Clang both have had that same optimization for a long while. I don't know about MSVC, but I wouldn't be surprised if it excercised the same optimization with header guards.
Reverse iteration of arrays: for(size_t i=a.size()-1; i&gt;=0; i--) do something with a[i]; 
Which is exactly the reason to introduce std2, where you can put "new" version of standard library types without removing the old ones.
What do you mean exactly?
Link invalid
... why would it break anything ? I'm just saying to do : std2_string.hpp: #include &lt;string&gt; namespace st2 { using string = std::string; }
I thought this is just name reservation. To avoid problems with having std::string and std2::string, std2 can be imported to std namespace to replace current imports. So, the is a way to migrate with a style. : )
The problem is transition: You have a huge codebase using std:: and want to transition it to std2::. You start by selectively changing code and end up with a mixture of std:: and std2::. But as the two standard libraries will have different types, you get incompatibilities: std2::optional&lt;T&gt; is not the same as std::optional&lt;T&gt;, so you have to manually translate. std2::vector&lt;T&gt; is not the same as std::vector&lt;T&gt;, so you have to manually translate. This makes it very hard to actually transition existing code bases, which is why it was dropped. (AFAIK, I wasn't there either, please correct me if I'm wrong)
Wow this looks awesome!
Would love to read some document/paper about this. But I also think having two namespaces is probably a mistake. Where's the thing I want to use? In `std::` or `std2::`? What will be where? Presumably not the whole `std::` will be deprecated, so there will always be up-to-date things there too? It's also less of a mental hurdle for people to use it. Even if it's stupid, I'm going to think twice before using `std2::` - it just sounds weird already, and like it will not be supported on many platforms at first. Using `std::` is what we're used to and much less of a mental hurdle (note that this is not a rational argument, just how some people will think). If the new stuff that's added is so great and can replace old stuff, then the old stuff in `std::` can slowly be deprecated. I don't know a good example but maybe this is one: Ranges can replace iterators? Then iterators could be marked deprecated in C++23 so that you get a compiler warning when using them. Then at some point in the far future (maybe C++26?), they could maybe even be removed. Note that this is 8+ years into the future, so I do think that would be potentially possible.
The worst part of C - lack of templated algorithms and data structures.
there's also this thread from last week : https://www.reddit.com/r/cpp/comments/8pqjiz/transwarp_16_released_a_headeronly_c_library_for/
&gt; I think techniques like this are an idea whose time has come, if you do computer music this time had come for you since the mid-80s with stuff like Max :p https://www.google.com/search?q=max/msp+7&amp;tbm=isch
Why on earth should a size be signed? "This type is -2 bytes large?"
Yea me too - took me a few seconds to guess that the first "s" stands for "signed", and I still can't be 100% sure without checking.
at least the vector&lt;bool&gt; specialization comes into my mind.
Sounds like a good reason to use rbegin and rend not to change the type.
It does behave exactly like Natural Numbers though. It's like arguing that float doesn't behave like numbers because it cannot represent the square root of -1.
With how much its changed, why not just branch the standard into C+++?
Ranges don't replace iterators, but the range algorithms (somewhat) replace the iterator version. Iterators themselves are still useful.
Excellent README, that's the kind of presentation that makes one want to use a project :D
because when you make size signed you need extra work at the caller site to ensure that you aren't passing wrong values. With unsigned and not enough coffee - or just not taking care: std::size_t index = vec.size() - whatever_computation; if(index &gt;= 0) { // does not make sense but remember, it's 23:45 and the release has to ship vec[index] = "foo"; // boom, because for some reason whatever_computation was &lt; 0 hence index is 434635453746537645 } so you must do this painful little dance for each index substraction to be safe : if(whatever_computation &gt; vec.size()) { std::size_t index = vec.size() - whatever_computation; if(index &gt;= 0) { vec[index] = "foo"; } } With signed, the check which is mathematically logic, actually is useful int index = vec.size() - whatever_computation; if(index &gt;= 0) { // ok: index is necessarily between 0 and vec.size(); vec[index] = "foo"; } and even if we forget to do the check, a nice vector implementation could do T&amp; operator[](ptrdiff_t v) { assert(v &gt;=0); // not possible with unsigned assert(v &lt; size()); // ok today ... } Thankfully recently clang has gained the ability to check for unsigned number wraparound in ubsan, even if it is technically legal behaviour I can guarantee you that I got a few additional white hairs when I enabled it on my codebase ; except for some hash functions every unsigned wrap was a bug. 
because your comparitor types also needs to be unsigned to avoid compiler warnings. As well as: auto a = vec.size(); auto b = a-500; // Does b contain a sane value?
The machine representations we use are all approximations of the notions we try to express. And you are right I agree the situation is similar to float and sqrt(). The difference is that using floats is a much better model for numbers than unsigned for non-negative integers. Using unsigned you far more frequently end up with situations where they don't behave like people would expect a number to behave. Unsigned ints model modular arithmetic, not non-negative integers,
Because "unsigned considered harmful". There are many good reasons for avoiding unsigned - it's easy to write a loop that looks correct but won't terminate with unsigned, overflow (and underflow) is defined so certain compiler optimisations are suppressed, and it's mostly unnecessary anyway - you're not likely to have a &gt;2 GB object on a 32-bit system (2-4 GB usable address space). And definitely won't on a 64-bit system. Added to that, ptr_diff_t is signed, which is used for getting the difference between two pointers into the same block of allocated memory. So if an allocated block is &gt;2GB, the difference calculated as a ptr_diff_t between its start and end won't be correct anyway.
If you're typing a report or something, you don't need to use special characters as often with LaTeX. With C++ you're in for a ride.
&gt; Since that macro is never part of a library's public interface - this isn't a breaking change. of course it is. The number of times I had to debug two files having header guards such as TIMER_H or UTIL_HPP ...
&gt; pragma is out. pragma will *never* be out. Most of the code that I include has copyright header that ends in the 90's.
The problem with unsigned types is that they don't really behave like normal numbers, but they have modular behaviour. uint8_t x = 0; --x; `x` is well defined: it will be 255. Similarly: uint8_t x = 255; ++x; `x` is certainly 0. This can also cause you crazier problems: int a = -1; unsigned int b = 1; assert(a &lt; b); // FAILS! `a` gets converted to an unsigned int, which then gets wrapped around to `UNSIGNED_INT_MAX - 1`, which is much bigger than 1. Sure, occasionally you want this modular behaviour, but most of the time, I'd suggest, you don't. For signed integers, underflow and overflow is undefined behaviour. This enables certain optimisations: return (a+1) &gt; a; can be optimised to: return true; if `a` is signed, but not if `a` is unsigned. A more extreme example: return (a * 14) / 7; can be optimised to: return a * 2; but not if `a` is an unsigned int. (Consider: `uint8_t a = 20`: 20x14 gives 280, which wraps around to 25. 25/7 is 3 (using integer division), rather than 40, which we probably expect.) The other thing is that overflow / underflow is *normally* a bug. Also, most quantities (and `size_t` is typically a quantity) are small - that to say, have values near 0. So that buggy behaviour of underflow is nearby *and* undetectable, because if you underflow you get well defined behaviour making the size appear to be very large. However, with a signed size type that buggy behaviour is much further away, and you'll notice straight away if your size has gone negative. The bug can be picked up immediately, instead of where it causes the code to crash down the line. [[Pretty much all this information comes from two lighting talks. Please see [here](https://www.youtube.com/watch?v=wvtFGa6XJDU) and [here](https://www.youtube.com/watch?v=equbUrX-ZWQ&amp;list=PLHTh1InhhwT55y4fRRTBIelxnRSZ8G5yg&amp;index=27&amp;t=0s)]].
That's what I meant. I wouldn't do C++ on my phone. "Writing code on a phone [...]" is a little too general imo though.
&gt; If itemCount were unsigned, I wouldn't need the first part of the check. except that you would have to add checks where you compute your `i` because if your signed `i` can go under zero, then your unsigned `i` will at some point end up being 2^64-something
AkTually, `unsigned` does work like numbers - but it's ℤ/nℤ (n = 2^(k)) instead of the common ℤ. Question is rather whether such ensemble is suitable to represent sizes (it's not).
[unsigned int vs signed int](https://github.com/ericniebler/stl2/issues/182)
Please note that the value-based and TMP-based papers are not really in opposition. Both agree on the scope and semantics of the reflection facilities. That is why the CNS-papers form the basis of the TS, it is the giant on whose shoulders the other papers stand. the value based approach is a different means of exposing the same information. We did already vote in SG7 (in an evening session with large attendance) that we prefer the value based approach. We are still going forward with the TS as it is, to explore if it exposes the right data for the envisioned use cases in a way that is practically implementable. When the value based metaprogramming facilities arrive the TS will use those as their front end.
Your 2 posts are opposite of each others: one is "Unsigned doesn't behave like numbers" (and /u/robthablob was right to correct it, even it's a technality that doesn't change the matter at hand), the other recognize that the ℤ/nℤ is a valid set of number
A single character difference between signed and unsigned, where that character is just "doubling down", seems just like a typo enabler. Remember the `=` vs `==` in conditions? Where compilers have "imposed" using `()` around assignment do "confirm" that it's really assignment you want and not equality? It's a usability nightmare to differentiate two things by a single repeated character :(
Awesome. I use TBB flow graph a lot, so I think I’ll give this a try and compare. One thing that’s come to bite me is task isolation in TBB. For instance, if I have 2 nested parallel loops and the inner is behind a mutex, a context switch can have me jump from the inner loop with the mutex to the outer loop and then deadlock, when the thread tries to reaquire the mutex. TBB mitigates this with task isolation or task arenas. Does this library have a similar mechanism?
Is there any type safe parameter passing between tasks?
https://cmake.org/cmake/help/latest/command/project.html You can set the version using the project() command. 
Sorry I absolutely stand by "unsigned doesn't behave like numbers" as "numbers" in informal discussion pretty much always means integers. And "behave" typically means the normal binary operations. Neither N nor Z/nZ behave at all like Z. I mean, subtraction isn't even well-defined for N, how can it behave like Z? I'd say floats do behave like "numbers" since all common operations they are defined and behave as you'd expect.
Turning std::bad_alloc into std::terminate is the biggest breaking change imaginable.
Actually, to be pendantic, signed integer operations have UB on overflow. All the above only works by fluke. 
I think it's a good post, hopefully many C programmers will read :) This part I had to laugh out loud, but not because of the blog post, just in general: &gt; std::string offers most functionality you’d expect, like concatenation (as shown above). I guess that really depends on the perspective. From a C programmers perspective, maybe :) From a year 2018 perspective (for example comparing to Python), it's rather laughable, no tokenization, finding some letters in a string is cumbersome syntax, etc. So many string facilities that are missing and could really make their way over from Boost or Python into standard C++.
I agree that "numbers" in an informal discussion and "numbers" for mathematicians don't have the exact same meaning, although as we're dealing about a technical enough subject, using the mathematical definitions and reasoning is more suitable. Otherwise, we would ignore lots of pre-existing work. &gt; Neither N nor Z/nZ behave at all like Z. That's kinda our point: unsigned being Z/nZ, it's a poor choice for a size type, despite being a valid mathematical set. &gt; I'd say floats do behave like "numbers" since all common operations they are defined and behave as you'd expect. I disagree with that - float have many surprising edge (and not-so-edge) cases (NaN, etc.)
&gt; then your unsigned i will at some point end up being 2 And thus less than any reasonable count of whatever.
Oh dear god yes please.
I suspect you are right. What I don't see though is why we can't have a `--stdlib=` switch to match `--std=`, or simply to introduce breaking changes with the next `--std`. Upgrading to a new standard is likely as big an effort as swapping to a new standard library. So just update the stuff already.
I feel like this argument strand conveniently ignores the myriad pitfalls when dealing with signed integers and their corner cases. As far as I can tell, dealing with signed integers is strictly more complex overall than unsigned. Once again, this seems like a byproduct of how other languages deal with integers, and the habits that creates in programmers.
&gt; That's kinda our point: unsigned being Z/nZ, it's a poor choice for a size type, despite being a valid mathematical set. Fully agree with that statement! &gt; I disagree with that - float have many surprising edge (and not-so-edge) cases (NaN, etc.) Agree there are many edge cases, but it's the best we have for now. With sizes though though we can do better than unsigned.
I think most people mean integral numbers when they talk about "natural" numbers.
There are all kinds of solutions to this. The fact is that those kind of bugs (usually in less obvious situations) occur in real code base, because even programmers that in theory know what is happening when pointed to it tend to overlook such details while writing and refactoring code.
&gt; Unsigned ints doesn't model non-negative numbers ?
[Part 2](https://codingnest.com/basic-cmake-part-2/) is about creating an installable CMake library, including versioning and declaring version compatibility.
IIRC I ran into trouble with plain target name when I wanted to pass the path to the binary to an external script.
No sane dev team would choose anything other than #pragma once. Everyone had this fight 15 years ago when gcc deprecated #pragma once. The fight is loooooong since over. The only strange edge case is linked header files, but I wouldn't be surprised if most compilers can handle that too. I've never heard of anyone running into the problem unless they're trying to run into the problem.
I suspect he was flooded with criticism and took it all down.
why not move the old unwanted stuff into a std::deprecated?
The link doesn't work anymore. Can someone provide a summary?
One of the things I hate about using signed ints is instead of having this: if(i &gt;= max_value) You end up having to do: if(i &lt; 0 &amp;&amp; &gt;= max_value) I just got tired of seeing `&lt; 0` everywhere honestly. I'm not particularly convinced that signed integers causes less bugs. `a[-1]` isn't likely to be spotted for example, as that is likely valid memory, `a[4294967295]` isn't and will just crash right away.
Any news on new containers? Flat maps, colony?
This. It would probably be much more progressive if we could drop backwards compat and make major changes in a new branch.
&gt; In practice, the convention is that we use `struct`s only to bundle data together, and a `struct` generally doesn’t have an interface with methods and everything. So technically, you can replace `struct` with `class` in all that follows, but this does not follow the convention of `struct` and `class` (which everyone should follow). Uh.. no. Just no.
Bad code justifies nothing; write better code. for(auto i=size(); i--;)
Try abs(vec0.size() - vec1.size())
Packaging doesn't care about how you organize your code. It cares about telling your build system things like exactly which compiler and options to use in order that the results are useable in the project being built. It cares about telling the build system where exactly the results should be installed. It needs to be ask how to use the results, if there are particular flags that must be used. That you have your cpp files and headers all in src/ is irrelevant. 
Can you elaborate? I'm sure users of your library aren't supposed to check your header guards. In which case does changing an include guard break user code? As for `TIMER_H` and `UTIL_HPP`, using such generic macros is just asking for trouble. For such short names it's not a bad idea to use a few randomly typed characters (`TIMER_H_ASFI` for example). Granted, this practice isn't seen often in the wild.
Well then let us use floats. Just write good code: for (float f=size(); f &gt; std::numeric_limits&lt;float&gt;::epsilon(); f -= 1.0)
Its ugly to have it written everywhere in your code.
Both are ugly and must be supersede by a `std::text` which handles encoding properly.
Trust me when I say that I am aware of what you are not allowed to do with unsigned numbers. I got a codebase with half a million loc that's full of unsigned math. But the fact that bad stuff can happen when not being careful with unsigned numbers is not an argument but to use them. Bad things can happen when you are not careful with pointers and other language features as well.
&gt; Incredible how most people here seem to think index types should be signed, with the main argument being "it prevents bugs". Huh? &gt; If you don't understand how unsigned vs signed integers behave [...] _ad hominem_ 
Yes because if (i &gt; (std::numeric_limits&lt;unsigned int&gt;::max_value &gt;&gt; 1) || i &gt;= max_value) is so much better and clearer than if(i &lt; 0 || i &gt;= max_value)
Yes, but you balance the risks with the benefits. And this post's comments lack actual pro arguments for std::size_t. So less risks for no downsides sounds like a good decision...
I'm not really much of C expert, I work mainly with hardware. But I have to say I don't really understand your reasoning here. Why does it matter if unsigned behavior is 'well-defined', and signed over/under-flow is UB? Your CPU - basically *any* current CPU/uC/anything - doesn't handle signed or unsigned addition/subtraction differently, it's all the same to the CPU. So even if the C standard says signed over/underflow is UB, the actual result will be just as wrong (but completely expected on 2s-complement architecture) as with unsigned numbers. For example: int8_t a, b; a = SCHAR_MAX; b = a+1; printf("a=%d b=%d\n", a, b); // 'unexpected' result b = (a*14)/7; printf("a=%d b=%d\n", a, b); // 'unexpected' result The result will be: a=127 b=-128 a=127 b=-2 Which is clearly wrong. I fail to see what is different/better than with unsigned numbers? I think the takeaway is that the programmer should be wary of the limits of any variables he's working with, irregardless of unsigned / signed type. There's no way around it when you work with fixed bit size numbers. Or maybe C should allow the programmer to inspect CPU flags, like under/over-flow, carry, etc ;) 
I messed up my sentence, I meant to say that once there is a good replacement, there is no need for include guards or pragmas.
While waiting for C++ 20, I emulate Designated Initialization with lambdas: struct MyStruct { int i = 0; int j = 0; std::string str; template&lt;typename ...Args&gt; MyStruct(Args&amp;&amp;... iArgs) { (iArgs(*this), ...); } }; namespace attr { auto I(int i) { return [&amp;] (MyStruct&amp; iStruct) { iStruct.i = i; }; } auto J(int j) { return [&amp;] (MyStruct&amp; iStruct) { iStruct.j = j; }; } auto Str(std::string str) { return [&amp;] (MyStruct&amp; iStruct) { iStruct.str = std::move(str); }; } } int main() { MyStruct s { attr::J(15), attr::Str("test") }; return 0; }
There was another suggestion of doing namespace std{ namespace _1998{ } } http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n2013.html
If you can drop backward compat, you may go to another language, like Rust or go. The main advantage of C++20 over C+++/Rust/go is his backward compatibility. And if you look at the proposals and the evolution of the language, more stuff have been deprecated and remove in c++17 than c++14, and more from c++20 than c++17. It's moving forward.
&gt;I'm sure users of your library aren't supposed to check your header guards. In which case does changing an include guard break user code? sure, but it ends up in their global namespace all the same, whether they want it or not. `#pragma once` does not. &gt; As for TIMER_H and UTIL_HPP, using such generic macros is just asking for trouble. yes, but mistakes happen and programming language should strive to offer ways that prevent the mistake from happening in the first place, especially when it is something so common that it has to be repeated for 99.9% of your .hpp files. Another very common mistake is starting a new file through a copy of an existing one and forgetting to change the include guard - I can't count the number of times I or a coworker of mine did this until I started to enforce `#pragma once`. 
I've got nothing more to add to this. I think we've gone through all the pros and cons of both approaches. The extreme case is worse for `#pragma once`, but is much rarer that issues caused by header guards. In the end, use what works for you - I am not going to preach one way over the other.
You are correct in that case you need to use a generator expression to get the correct path. CMake will only deduce the path when the COMMAND is only an executable created by CMake.
None of the operations described by jcelerier use signed overflow.
That sums it up perfectly.
And believe it or not, [sometimes it is a good thing to have UB](https://www.youtube.com/watch?v=yG1OZ69H_-o).
No, you don't need that normally. `i &gt;= v.size()` is sufficient to check if i is outside the range of valid indices for v. The equivalent with signed values is `i &lt; 0 || i &gt;= v.size()`.
Capturing locals by reference is a sure recipe for UB. ;-] But the resulting syntax is decently palatable.
Don't we still need iterators to deal with partial containers (for example search the first half of a vector with find_if)
That, is simply untrue. Backwards compatibility is arguably a disadvantage for many people (such as myself), since it means a lot of libraries have outdated dependencies. One of the main advantages of Rust/Go is that when you download a library you have ~95% certainty it will compile without wasting an hour of your day having to dig through **** to figure out how ot make it work on your system. Many people prefer C++ over Rust/Go/D because it: -&gt; Speed (can even be faster than C due to some std components that are Fortran inspired) -&gt; It doesn't get in your way (i.e. it doesn't prevent you from shooting yourself by replacing the trigger with a rubix cube) -&gt; It's the most advanced and easy to use language in terms of CTP -&gt; It doesn't enforce a programming style, even though it has it roots in the disgusting OOP space, you can pretty much adopt any modern style of code and painlessly write a C++ program following it. So, no, backwards compatibility isn't what's keeping everyone using C++, it's arguably what's driving most people away.
Well, I guess you don't store your include files on an NFS server, then remount it in the middle of compiling (generating new inode #s in the process). Apparently, somebody did that once and now we can't have nice things.
How do you imagine this to work? Any ideas? How would you make a support of something application-specific to be implemented in library, I mean. Templated tasks?
You're right, I was thinking of if you wanted to do something different if it's over or under, but if you don't care which way it's out it can be done with one check with unsigned.
You're right. Normally it's captured by value with move semantic ;) 
I was recently surprised that KDevelop marked a constant conversation as a warning by default. It does so through clang so I'm sure there is a flag to protect from this.
I've never worked in a proffesional setting - why would a manager/someone from coprorate be against _developer_'s decision to use something like `std2::` or even notice it. Do they... review code? 
Thanks!
 #include &lt;Windows.h&gt;
I think the issue he’s talking about is how difficult it would be to mix old `std::` structures and functions with new `std2::` ones. Even aside from any interoperability issues, I’d imagine it would be visually confusing to use similarly named classes (e.g. `std::vector` and `std2::vector`) with very different APIs.
So, if you have software older than 5 years old, you're on your own due to "outdated dependencies"?
Better use the 'towards' operator for extra verboseness! for(auto i=size(); i --&gt; 0;)
&gt; With signed, the check which is mathematically logic, actually is useful &gt; &gt; int index = vec.size() - whatever_computation; &gt; if(index &gt;= 0) { // ok: index is necessarily between 0 and vec.size(); That's not right, if ` whatever_computation` is signed (which you assume in the previous example), `index` can be larger than ` vec.size()` or the calculation can overflow. If it's unsigned the calculation can overflow as well. For unsigned `whatever_computation`, the example with unsigned `size_t` is much nicer: if(whatever_computation &lt;= vec.size()) { std::size_t index = vec.size() - whatever_computation; vec[index] = "foo"; }
C+=2
&gt; Why does it matter if unsigned behavior is 'well-defined', and signed over/under-flow is UB? There are a couple of reasons the difference is important. The first is compiler optimisations. If you look [here](https://godbolt.org/g/YPTBP7) you can see the stark difference in assembly between using an unsigned int and a signed int. While I know benchmarking isn't as easy as "count the asm instructions" any more, the performance difference is very clear. &gt; The result will be: a=127 b=-128 a=127 b=-2 Which is clearly wrong. The result *might* be that, but you've invoked undefined behaviour, so anything's legal really. The other thing pertinent to this is that there is tooling to detect UB, which will catch these errors for signed ints but not for unsigned ints. With unsigned ints you might never be able to reasonably catch these errors at runtime: size_type distance_to_end(size_type index, const Container&amp; c) { return index - c.size(); // OOPS! } int main() { Container c{/*Initialize/*}; size_type i = get_random_element_index(c); size_type distance = distance_to_end(i,c); assert(/* ??? */); } If `size_type` is signed, I can use `assert(distance &gt;= 0 &amp;&amp; distance &lt; c.size());` But with `size_type` being unsigned, I have no such option. If `i` is small, and `c.size()` it is possible to even have a result such that `assert(distance &lt; c.size());` doesn't fire, even though the result is clearly wrong. And because behaviour is well defined, my UBSan tool won't help me either. But finally, I think [examples such as this](http://cpp.sh/6rvsu) show quite clearly why an unsigned `size_t` type is less preferable that a signed `size_t` type.
&gt; It does behave exactly like Natural Numbers though. Not at all. In the natural numbers, `2 - 3` is not even an allowable operation. In C++, `unsigned(2) - unsigned(3)` is `4294967295`. &gt; It's like arguing that float doesn't behave like numbers because it cannot represent the square root of -1. Not a good argument because `float` represents _real_ numbers, which also cannot represent the square root of -1.
&gt; Do they... review code? Yes! Technical managers set the tone and make the big technology decisions because they have a long-term impact on future revenue.
You don't strictly speaking need it (you can usually express things like that as subranges, but I would agree, that sometimes iterators are more convenient. Anyway. Removing iterators completely would be such a massive, breaking change it will never ever happen.
I'm agreeing with your argument in general - just a tiny quibble here: &gt; subtraction isn't even well-defined for N, Subtraction is well-defined for _some_ values in `N ⨉ N` - perhaps it's more accurate to say that subtraction is a partial function on `N ⨉ N`.
Compilers certainty don't think so :) int a = -10; unsigned int b = 5; auto c = a - b What do you expect the value of c to be?
I'm only experienced at startups, but you don't really work in a vacuum. You could try to sneakily add code from std2 and hope no one notices, but the second you have major issues or a code review that results in something along the lines of an argument, you're going to have to say something about it to your PM, who may have code competency and have concerns, even if your fellow developers are on board (which they might not be). Additionally, when you're upgrading the compiler in the first place, you'll often need to argue that it makes business sense. ("it will result in fewer bugs", "development time will decrease", etc.) One of your arguments will likely include std2 if it's a major feature, and you will have to justify it at that point. It's possible (though I find it extremely unlikely) that a manager might look at that and say "std has been working for us so far, and std2 represents risk" and accept the compiler upgrade but ban std2. It seems more likely to me you'll have a stick-in-the-mud coworker who doesn't want to use it for technical reasons (they don't understand it/want to learn it/think it's a good idea for the reasons others in this thread are saying). All that aside, I think it's pretty unlikely management will let you upgrade but won't let you use major new features like std2. 
My way (more ugly, but concise): template &lt;typename T, typename Fun&gt; T lambdaConstruct(Fun fun) { T ret {}; fun(ret); return ret; } #define CONSTRUCT(Class, Code) lambdaConstruct&lt;Class&gt;([&amp;] (Class&amp; c) { Code }) //... MyStruct s = CONSTRUCT(MyStruct, c.j = 15, c.str = "test"; );
There have been a lot of successful graph based domain specific programs like Max and Touch Designer (realtime), shake, nuke, digital fusion (compositing), houdini (3D). The big difference here is that it is meant for general purpose software while maintaining lock free concurrency. The nodes are written in C++ and work on arbitrary datatypes, but the biggest difference is integrating nodes that are made to keep state with nodes that just do transformations. This ends up merging a message passing structure with a more functional structure so that high level control and state are done with message passing nodes and complex transformations are done with data flow.
If backwards compatibility is a disadvantage, why did we see so many "C++ killers" that simply failed to replace C++? All the other things you mention could be implemented in another language in the same way in which they are implemented in C++.
I think this is a really odd point. Specifically, you're acting like backwards compatibility is some kind of "feature" of a language and when you program you're continuously evaluating which language you want to use at all times. Granted, your points make some sense when it comes to starting a new project (though I do think it's ignoring the fact that you can still use libraries written in the 80s if you really need to, which is a benefit). But if you already have a project in C++14 and it's huge, the main reason to stay on C++ *is* backwards compatibility because if C++20+ (or whatnot) becomes heavily backwards compatible, you may just not be able to upgrade. And I'd say the vast majority of people using C++ on a daily basis are people who already have projects in C++ that were started years ago and simply don't find it worth it to rewrite. I think that's what it is meant when one says "backwards compatibility is the reason most people use C++". 
You are certainly correct that two comparisons i) are longer and ii) possibly reduce performance. If performance is really the issue here then you should be really happy, since if this is your bottleneck, you are blessed by already very good code :-)
Templates, yeah. I've been toying with similar ideas but it gets really complex really fast
I believe the better way to handle the `vec.size() - x` situation is the first example you show, not the second one and calling for a `signed` index or size type is unjustified. Let me explain. In general, it is better to avoid going down a known-bad code path and backtracking later when a subsequent error is detected. For example I assume you would agree that delaying the error handling even further by writing `try {` [`vec.at`](https://vec.at)`(index) = "foo" } ...` would be even worse. (Avoiding exceptions for control flow is following the same principle. It gains you efficiency not just because C++ exception handling is not zero cost but because the range check is made either way but in one case there are unnecessary calculations made before that.) Now I'm in the camp that says the problem domain should be modeled as closely by the type system as possible and therefore in this case `index = vec.size() - x` is already a calculation that has a nonsensical result the *exact* same way as `vec[index] = "foo"` does. Therefore this should be handled the same way, by **avoiding** the calculation and not by checking the result for errors afterwards. I agree that regular old `unsigned long` is an imperfect type for the \[index into an array\] domain because of the wrapping behavior, but the regular old `index &lt; vec.size()` check \[which you forgot in your code BTW\] would catch out of range access just as well. But signed types are worse because it distances the code from the problem domain and this is already visible by having to make sanity checks` (0 &lt;= ind`ex) that should already be captured by the invariant of the type used. If you ask me, first stay with regular unsigned for compatibility reasons. If you're omitting `index &lt; size` checks, that's on you, but if you don't then it will catch out-of-range errors, including those by unsigned underflow. If you make a computation error that results in a wrong-by-program-logic but in-range-by-accident index, then you need an index type much smarter then `signed int` to save your backside.
can you not argue against ideas you just learned about please `std2` would be nowhere near as ugly as maintaining `std`
nothing would break
[it hasn't been given up on, that i'm aware of](https://github.com/boostcon/cppnow_presentations_2017/blob/master/std2_workshop/cpp20_language_features_for_std2__alisdair_meredith__cppnow_05-18-2017.pdf)
there's no problem here leave old code old. write new code newly
but when you do this, you have to have a mental model of how modular arithmetic works - and you assume that the readers of your code do. This should not be something that is required to do whenever you want to do something as simple as accessing a vector. 
&gt; do the distributed workers need to communicate with each other or is the work they do independent? This is the most important question to answer. If you just want to distribute the calls to different servers depending on the current load/availability, you will need a Broker type application to do it. Technology to do it already exist in the sphere of services-based architecture and Cloud computing. On the other hands, if you want to distribute the load of a single calculation to multiple servers to increase performance through parallelism, you will have to rewrite your application to allow for that. Fortunately, technology that do exactly that already exist also in the form of MPI, with popular implementation from Intel, Microsoft and there even exist open source ones like OpenMPI. 
To clarify: this is not my guess, this is my recollection of what people who where there told me why it was dropped. But I might have misrembered it, that's why I added the disclaimer. And you can't just write new code using std2 because it might have to interact with old code using std. This means copying elements between different containers etc.
I modified it to be more generic with a use o an ugly macro. #include &lt;string&gt; #include &lt;iostream&gt; #define CAT(a, ...) PRIMITIVE_CAT(a, __VA_ARGS__) #define PRIMITIVE_CAT(a, ...) a ## __VA_ARGS__ #define A(fieldname, value) ([&amp;] (auto&amp; obj) { obj. fieldname = std::forward&lt;decltype(value)&gt;(value); }) struct MyStruct { int i = 0; int j = 0; std::string str; template&lt;typename ...Args&gt; MyStruct(Args&amp;&amp;... iArgs) { (iArgs(*this), ...); } }; int main() { MyStruct s { A(j, 15), A(str, "test"), A(i, 12) }; std::cout &lt;&lt; s.i &lt;&lt; ' ' &lt;&lt; s.j &lt;&lt; ' ' &lt;&lt; s.str; return 0; } but something is telling me that this is bad
Personally, I think sizes should be unsigned, but it should also be possible to detect and handle overflow correctly. Similarly, I think most automatic coercion of types is bad - as they lead to common errors.
There are plenty of C++98 libraries still out there...
&gt; but the regular old index &lt; vec.size() check [which you forgot in your code BTW] well, no, I did not forgot it (though I implicitely assumed without thinking too much about it that `whatever_computation` was unsigned too since it's a case that I often have - gonna add it to the post). Given two positives numbers a and b, either a - b is in [0; a] or it is negative. The problem is that the vector index type really does not look like something in Z. &gt; For example I assume you would agree that delaying the error handling even further by writing [...] would be even worse indeed, I just want a big assert because it's almost 99% certainly a logic error somewhere. Crash early, crash often is the pathway to a bugfree software. But I really disagree with having the error checking up-front. Either it's provided by dependent typing which is fine but still farily impractical at this point, either it's in the vector API, but I *don't* want it in my code - I am of the opinion that the ideal code is to just have the correct path and have problems detected at the boundaries of the system.
While I at least somewhat agree with your points (though not entirely -- I hit the `=` vs `==` thing *one time* almost two decades ago now, turned on warnings, and don't think I've made that mistake again), I still think `ssize_t` is a much better name than `ptrdiff_t`, and I've failed to think of one I think is better without getting unacceptably terse. (Maybe `s_size_t`. You could sell me on that, though maybe not on the utility of it above using the semi-existing `ssize_t`. I *will* say that if we have a time machine and could go back in time to when `size_t` was created and make *it* signed, that would be better of course, but I'm sort of in a "if the STL were written today what type should you use" mode. And I think standardizing the existing common `ssize_t` and using it is the right way forward in that sense.) But `ptrdiff_t` is just *wrong*. Not only is it not descriptive of what it holds, it's also lying to you. You could make a case for the contiguous containers, though I think even there it's kind of poor, but think about any discontiguous container. What pointers are being differenced to give you the `size()` of a `map` or `list`? If I say `my_dequeue[i]`, what pointer is `i` being added to to get the element?
If this were to happen, I would use only one of them per file, then have one file that converts std &lt;-&gt; std2 although this cannot be universally applied since converting entire dses all the time is probably pretty slow.
&gt; How would you fix this pragma once problem? ... Header guards have a similar problem - two distinct header files have accidentally wound up having the same include guard. Here, the solution is to rename the header guard macro, not the file. If you run into that problem, the solution then would be to change from `#pragma once` to guards. Header guards don't stop working just because you standardize `#pragma once`. 
Have you reported it?
I am not aware of it being dropped, and people are still talking about it like it isn't dropped. &gt; And you can't just write new code using std2 because it might have to interact with old code using std. This means copying elements between different containers etc. no it doesn't. 😂
I think backwards compatibility is huge, but it's not the only reason other languages fail to displace C++ - the tools ecosystem of C++ is substantial, with at least 4 different compilers that are hardened by many years of refinement and multiple IDEs that integrate completion and debugging. 
&gt; Your CPU - basically any current CPU/uC/anything - doesn't handle signed or unsigned addition/subtraction differently, it's all the same to the CPU. Multiplication and division is handled differently. Not the least comparisons. Yes, it's the same CMP instruction, but, depending on whether a conditional uses signed or unsigned types, jump instructions inspecting different flags are generated.
That's if you want the _exact_ same behavior, yes, but that's not my experience at all. There was this particular old codebase who had _a lot_ of it because of static-sized arrays, and they had a specific max for a bunch of things, which was checked everywhere, _along_ with checking for negative values. There were some bugs where negative checks weren't done. &gt; If you remove the less than 0 check and don't introduce the much more unwieldy check, you've possibly introduced a bug. The less than 0 is more likely to get forgotten as it doesn't make much logical sense... whereas checking for the maximum amount of something is a lot more common.
Then, for your information: it is dead. &gt;&gt; And you can't just write new code using std2 because it might have to interact with old code using std. This means copying elements between different containers etc. &gt; &gt;no it doesn't. 😂 Thank you for your counterargument, you clearly know how a discussion works. &gt;in the meantime, by this logic, most of today's standard library shouldn't exist either And now you have a completely unfounded reductio ad absurdum. 
Fair enough. For some reason I had `#pragma once` completely separated from header guards in my head.
Why don't you update to 2017?
I will, as soon as I get access to my Microsoft account.
That depends on your perspective. The Linux kernel is compiled with no UB on signed overflow.
&gt; Then, for your information: it is dead. The link I gave, from a standards body member, seems to show them feeling differently less than 12 months ago. Like I asked you earlier: why do you believe this? . &gt; &gt; &gt; And you can't just write new code using std2 because it might have to interact with old code using std. This means copying elements between different containers etc. &gt; &gt; no it doesn't. 😂 &gt; Thank you for your counterargument, you clearly know how a discussion works. Hitchens' razor: that which is asserted without evidence may be rejected without evidence. . &gt; &gt; in the meantime, by this logic, most of today's standard library shouldn't exist either &gt; And now you have a completely unfounded reductio ad absurdum. Reductio ad absurdum means "I will show your position to be incorrect by running with it until it reaches an impossible endpoint." Even if you take for granted that I was mistaken, what I did in no way resembles that. Generally speaking, even when you're getting the fallacy correct, which you didn't here, [the use of fallacies to fight doesn't actually work very well](https://laurencetennant.com/bonds/bdksucks.html). In the meantime, we've had waves of "you can't do anything new because working with the old versions at the same time would be mildly inconvenient" many times in this language. It has never been taken seriously. You can use `std` containers with `std2`, many of them can just use the same memory space with shared allocators, many of them can masquerade, etc. Weirdly, everyone seems to use `vector` as their example, like it's somehow difficult to understand what to do there. I would be more sympathetic with node-allocated structures, because they at least seem difficult on casual inspection if you don't know container allocation well
++++C
For some unfortunate bureaucratic-organizational reasons that are not entirely clear to me.
Not to take away from your point, but that spew of instructions replaces one `idiv` instruction because division is really slow. FWIW, you also didn't need to comment out `signed` if you wanted it there to make the difference clear, since `signed int` is legal and equivalent to `int`.
&gt; The link I gave, from a standards body member, seems to show them feeling differently less than 12 months ago. Like I asked you earlier: why do you believe this? I believe this because I had a discussion with members from the standards committee, including the chair of the Library Evolution Working Group, where they said that they dropped it in the Jacksonville meeting earlier this year (IIRC, could have been Albuquerque as well), and gave the reasons for it. Let me just elaborate what I meant with "And you can't just write new code using std2 because it might have to interact with old code using std." because you don't seem to understand it (which is my fault, I could have elaborated it more). Suppose you have old code with a function `process_elements()` taking `const std::vector&lt;T&gt;&amp;`: void process_elements(const std::vector&lt;T&gt;&amp; elements); And now you're working on new code using `std2`. Furthermore, suppose there is a new container `std2::array&lt;T&gt;` which is "in spirit" like `std::vector&lt;T&gt;` but has a better name, implementation etc., so it is a completely distinct type. This code will not work: std2::array&lt;T&gt; my_elements = …; process_elements(my_elements); // error! std2::array&lt;T&gt; is not std::vector&lt;T&gt; So you have to convert your `std2::array&lt;T&gt;` to `std::vector&lt;T&gt;` or just use `std::vector&lt;T&gt;` directly. `std::vector` is just one example, there are many types that could have been done better, so would be different in `std2`. You would either end up copying a lot of data at the boundaries or just continue using the `std` types, which defeats the purpose of `std2`. The transition story is just unfortunate. Using `std2` is only really possible if you're starting from scratch.
Please elaborate since I kinda agree with the original statement.
Hmm, a talk from a full year ago that starts out with "Does *not* represent ISO committee positions in any way". This post is talking about recent discussion and about ISO committee positions. --- For the record, I saw the talk a year ago and it was pretty interesting.
It uses clang 6. It can check if even std=c++17 is possible. And I'm surprised that std::byte is not available. Can be that I got the include headers wrong. I have to check.
Symlinks are not the only edge case, particularly in these days of containers. Symlinks can be worked around (non-portably) by using inodes, but layered filesystems and bind mounts used by containers can (and do) change the inode number, giving no reliable way to definitive determine file "identity".
\++C --&gt; moving the standard forward without an extra copy
&gt; I believe this because I had a discussion with members from the standards committee, including the chair of the Library Evolution Working Group, where they said that they dropped it in the Jacksonville meeting earlier this year (IIRC, could have been Albuquerque as well), and gave the reasons for it. well this is rather different than "what people told me why" . &gt; Let me just elaborate what I meant with ... because you don't seem to understand it oh good, a programmer has a zen master on the mountain act that he wants to show off pro tip: 90% of the people who do this aren't worth listening to . &gt; &gt; `me` many of them can just use the same memory space with shared allocators &gt; `you` So you have to convert your std2::array&lt;T&gt; to std::vector&lt;T&gt; Yeah, or you can just do it with no conversion *shrug* . &gt; &gt; `me` You can use std containers with std2, &gt; `you` or just use std::vector&lt;T&gt; directly. Is there some reason you keep telling me things I already said? . &gt; &gt; leave old code old. write new code newly &gt; Using std2 is only really possible if you're starting from scratch. Is there some reason you keep telling me things I already said? . There are actually lots of other options besides those two also But generally I'd just share the memory. Zero overhead perfect solution
This has come up a few times before: https://www.reddit.com/r/cpp/comments/4cjjwe/come_on_guys_put_pragma_once_in_the_standard/ https://www.reddit.com/r/cpp/comments/8devw1/can_anyone_actually_name_a_compiler_that_doesnt/dxmpt6m The short answer is that symlinks, hardlinks, bind mounts, etc. make it such that reliably implementing the feature on a variety of platforms is infeasible. It can work in specific environments, but the problems make it unsuited for standardization.
This has come up a few times before: https://www.reddit.com/r/cpp/comments/4cjjwe/come_on_guys_put_pragma_once_in_the_standard/ https://www.reddit.com/r/cpp/comments/8devw1/can_anyone_actually_name_a_compiler_that_doesnt/dxmpt6m The short answer is that symlinks, hardlinks, bind mounts, etc. make it such that reliably implementing the feature on a variety of platforms is infeasible. It can work in specific environments, but the problems make it unsuited for standardization.
&gt; Thankfully recently clang has gained the ability to check for unsigned number wraparound in ubsan, even if it is technically legal behaviour I can guarantee you that I got a few additional white hairs when I enabled it on my codebase ; except for some hash functions every unsigned wrap was a bug. That's why the solution is to check for errors, not use a different type that happens to prevent *some* bugs but is also a magnet for others (undefined overflow). &gt; If we really wanted to avert bugs, the "correct" type for indices would be a type that cannot ever become negative, e.g. std::safe_index(2 - 3) would throw Yes! Please, can we have namespace std { template&lt;typename Rep&gt; class sane_int { // ... } } Overflow throws. Narrowing conversion throws if it overflows the target range. Unchecked, wrapping and saturating arithmetic provided as member or free functions. Y'all can still use raw integers for performance-critical code. PS: using sane_uint64_t = sane_int&lt;uint64_t&gt;; etc because batteries should be included.
They don't support it "just fine". They begrudgingly support it so that it works in simple cases. But the cases which preclude it from standardization aren't simple. This has come up a few times before: https://www.reddit.com/r/cpp/comments/4cjjwe/come_on_guys_put_pragma_once_in_the_standard/ https://www.reddit.com/r/cpp/comments/8devw1/can_anyone_actually_name_a_compiler_that_doesnt/dxmpt6m The short answer is that symlinks, hardlinks, bind mounts, etc. make it such that reliably implementing the feature on a variety of platforms is infeasible. It can work in specific environments, but the problems make it unsuited for standardization. 
Because you may want to deprecate `std::vector&lt;bool&gt;`, which would silently change the behaviour of programs. Having a sane `std2::vector&lt;bool&gt;` would be an explicit opt-in by the user into the changed behaviour. The aim is not deprecating old stuff for removal, but fixing mistakes in the current implementations, so you want to be explicit, that stuff changed.
ok, well, if you want to show me a recent discussion that says otherwise, etc.
Right, valId point.
&gt; well this is rather different than "what people told me why" Yes, with people I meant "people with authority on the subject" (because why would I mention it otherwise?!). &gt;oh good, a programmer has a zen master on the mountain act that he wants to show off &gt; &gt; pro tip: 90% of the people who do this aren't worth listening to I don't want to show off, I stated it was my fault that you didn't understand because I didn't elaborate it. &gt;&gt;&gt;me many of them can just use the same memory space with shared allocators &gt;&gt; you So you have to convert your std2::array&lt;T&gt; to std::vector&lt;T&gt; &gt; Yeah, or you can just do it with no conversion shrug Now I don't understand: how would you, for example, convert a `std2::array&lt;T&gt;` with small buffer optimization to a `std::vector&lt;T&gt;` without changing `std::vector&lt;T&gt;` in an API or ABI breaking way *and* without touching the existing code *at all* (because otherwise you can't do a step by step transition). Please explain it to me. &gt; leave old code old. write new code newly Yes, but as there is a lot more old code than new code, is it really worth investing time into something 90%+ of C++ code will not be able to use? 
This can be solved by keeping `index_type` as unsigned, and creating a separate signed type, `index_delta`, to express difference between two indices. This is basically what is done by pointer arithmetic, here we do as the pointers do. The following operations is permitted: index_type + index_delta -&gt; index_type index_delta + index_type -&gt; index_type index_delta + index_delta -&gt; index_delta index_type - index_type -&gt; index_delta explicit index_type(size_t) index_delta(int) // implicit constructor operator size_t(index_type) index_type &lt;=&gt; index_type index_delta &lt;=&gt; index_delta index_type &lt;=&gt; index_delta = deleted The implicit conversion to index_type to unsigned is to allow index_type to be used for indexing into standard containers and compared with integers. This can be omitted at the cost of explicit casts. The implicit constructor of index_delta allows code like `index_type i; auto j = i + 3;`. 
Oodles of discussion over at this post from yesterday https://www.reddit.com/r/cpp/comments/8r5b3k/this_from_gslspan_is_a_good_example_of_why_the_c/ (as well as many other historical ones, I'm sure). Here's [my comment](https://www.reddit.com/r/cpp/comments/8r5b3k/this_from_gslspan_is_a_good_example_of_why_the_c/e0op2vm/) about it.
&gt; There are a couple of reasons the difference is important. The first is compiler optimisations. If you ask me, that seem like a strange optimization. The result will only be correct *sometimes*, and it is (at least to me) harder to reason about what ranges the x variable can take without possibly overflowing. &gt; The result might be that, but you've invoked undefined behaviour, so anything's legal really. The result *will* be that, since that is what the CPU will calculate, unless the compiler for some crazy reason inserts some additional instructions. But why would it? Just to let the programmer know that this is UB? &gt; But finally, I think examples such as this show ... Maybe a better solution would be to remove implicit casts from C? I get your point, I do. But I just don't buy that changing size_t to signed type would magically solve all problems the programmer might have, because it won't. There is no perfect solution with numbers represented in a limited number of bits.
You can have a type like `index_delta` which is difference of two `index`, similar to how `int` is the difference of two pointers. I explained it in a bit more detail [here](https://www.reddit.com/r/cpp/comments/8r5b3k/this_from_gslspan_is_a_good_example_of_why_the_c/e0pzhab/).
Or, you know, network file systems, bind mounts (containers anyone?) etc.
I've seen a lot of people (including compiler devs) claim that having an object that takes up more than half the address space is undefined behaviour, specifically because `ptrdiff_t` can't measure the difference between its start and end.
&gt; doesn't work the same on all of those compilers. That's exactly my point, it would work the same if the committee standardized it. Or at least, as much "the same" as include guards are specified to work the same everywhere. The fact that there are discrepancies here across compilers is *the committee's fault*.
The `unsigned` set is ℤ /n ℤ and not N : `a - b` is always well-defined, and gives a result by "looping" around if need be. It might be ok to use elements in N as container size, but ℤ /n ℤ (which has a "circular" structure) is a poor fit, even if n is large. Keep in mind signed/unsigned is not only about the possible valeus, it's also about how operations behaves, and what is the resulting type -and thus possibles values- of those operations. The main issue (imho) is the minus operator between `unsigned` returning an unsigned value, instead of a signed one (although I guess there were many good reasons, like performance, to make it so)
&gt; Yes, with people I meant "people with authority on the subject" (because why would I mention it otherwise?!). one, you didn't say "with authority" two, on reddit, this usually means Stan down the hall . &gt; I don't want to show off . &gt; I stated it was my fault that you didn't understand except that i did, perfectly fine, obviously. which is why all your rejoinders were in text i already wrote. . &gt; Now I don't understand oh. . &gt; how would you, for example, convert a std2::array&lt;T&gt; with small buffer optimization to a std::vector&lt;T&gt; without changing std::vector&lt;T&gt; in an API or ABI breaking way and without touching the existing code at all (because otherwise you can't do a step by step transition) is this one of those things where i say "it's usually okay to replace ac delco parts with after market parts" and you respond with a series of completely useless challenges that suggest you haven't even tried to come up with an answer, relying on a combination of brandolini's law and a salted earth social approach to make me not want to participate? if you know what the abi is, you know that a programmer can't make abi guarantees. i don't know why you're asking trick questions, but it's gross, and i don't want to be involved. . &gt; Yes, but as there is a lot more old code than new code, "Don't introduce new tools! You'll make the legacy goblins unhappy!" Okay, enjoy your SQL71
Thank you! That seems to be promising, especially `cmake_paths` generator as we don't always use Conan. 
I'm not talking about the optimization in compile time - that hasn't been true for years (decades?). I'm talking about more stating what you mean, reducing macros/global symbol names and more convenience for the programmer. About how to fix it: If you really belong to the one in a million group and have a complicated enough build setup, that pragma once fails you, you just run a script, that will add the header guards.
&gt; Upgrading to a new standard is likely as big an effort as swapping to a new standard library Noooo way, not if the library swap has breaking changes and as compared to current standards. My company just switched from a code base that was compiled all the way back to GCC 4.2, so couldn't use anything from C+11 except a couple long-common extensions like `long long` (no pun intended), to building with C++17. That actually *was* a significant effort for us, but it was almost all because we were changing not only that but also changing what complier we use on non-Windows platforms (Clang instead of GCC) and also *where it even gets the compiler from* -- it previously used whatever the system happened to have, and now we have a custom build (or *many* custom builds, really) of Clang. I wasn't fully involved in the process, but over the last couple of years *have* done a lot of work in getting a large swath of our code base to compile with Clang instead of GCC (fixing Clang-only warnings, etc.). I can only think of a couple changes necessary to handle the upgrade to the *standard*: * We had a bunch of things like `printf("Something: "PRId64, my_u64)` where the `PRId64` now meets the syntactic criteria for a user-defined literal * There were a few isolated uses of the `register` qualifier on a variable * We had two uses of `auto_ptr` I'm probably missing a few, but all of those changes were easy peasy. And that's jumping forward from C++03 to C++17, a pretty big leap. That's because upgrading to a new standard doesn't actually mean you have to change anything really. You don't have to change all your iterator-based loops to range-for. Everything you have will continue to work. But replacement for the standard library types is a wholly different story. If there are two vectors (*whatever* they are called, be it `std::fixed_vector` or `std2::vector` or whatever), you'll very quickly start running into problems, like "I am writing new code, have a `std2::vector`, and now need to call `foo(std::vector const &amp; v)`; crap." You either have to keep using the old stuff in new code, or change the old code to use the new stuff, or copy one to the other (expensive), or something like that. It's just a mess. I don't know what the solution here is. This would be a far more painful transition than anything that has come before, but at the same time if you can't change anything you can't fix anything. My best idea would be to try to design `std2::container` so that it has a better API or whatever you want, that you can efficiently move from `std::container` to `std2::container` and back. But that would likely not help in many cases.
&gt; The main issue (imho) is the minus operator between unsigned returning an unsigned value, instead of a signed one (although I guess there were many good reasons, like performance, to make it so) There are a lot of reasons other than performance. For example, `--u` should be the same as `u = u - 1`... but the latter results in signed? So there's an implicit cast back to unsigned. Which you could do, but it basically wipes out the advantage of having `u - 1` result in signed. So do you disallow `u--` for unsigned? That's certainly odd. I'm not saying `unsigned - unsigned` yielding signed wouldn't have been better, but I *do* think it is not a clear win and wouldn't have gone very far towards solving the problems with unsigned size types.
Rust and Go are too different, IMO. A C++ fork thats still recognizably C++ and works more or less the same way you expect it too would be better than a new language that is entirely different, like Rust and Go. C++ without the warts, but still C++. Break backwards compat, but you don't have to go all in and make it a entirely different language.
Ask /u/kalmoc. I haven't been involved with this.
https://m.youtube.com/watch?v=944GjKxwMBo I think it was briefly mentioned after minute 4
&gt; If you ask me, that seem like a strange optimization. The result will only be correct sometimes, and it is (at least to me) harder to reason about what ranges the x variable can take without possibly overflowing. What's strange about it? It takes a multiply and a divide, and replaces it with a single add instruction. That's clearly faster and cleaner? Why does it make it harder to reason about "possibly overflowing"? The maths is *exactly* the same as the unsigned case, just with a different upper limit. The compiler isn't allowed to make non-overflowing code overflow, although it is allowed to make overflowing code not overflow. &gt; unless the compiler for some crazy reason inserts some additional instructions. But why would it? Just to let the programmer know that this is UB? Yep. GCC and Clang both have a UB sanitizers that look for this kind of thing. [Check it out here.](https://godbolt.org/g/R7xFZD) Another option the compiler has (although none of the compilers on Godbolt seemed to do it) was to decide that a function unconditionally invokes UB when called, and strip it out, as well as any code paths that call it. (It would be an aggressive, but legal optimisation.) This is the thing: you're reasoning about the behaviour assuming the compiler is going to be nice about UB, and compile it naively. That's not necessarily the case. &gt; Maybe a better solution would be to remove implicit casts from C? Got a better idea to accurately do a signed-unsigned comparison? The casts allow it to be done in one instruction. (I guess "promote both to a larger, signed type" would work. I don't *think* it would break anything.) &gt; But I just don't buy that changing size_t to signed type would magically solve all problems the programmer might have, because it won't. There is no perfect solution with numbers represented in a limited number of bits. Getting a little bit meta-debatey, here, but I don't think I ever claimed this. I think this may be slipping into the [Nirvana fallacy](https://en.wikipedia.org/wiki/Nirvana_fallacy) (which is, I concede, easy to do without realising it. I don't think you've argued in bad faith.)
Note: I'm not saying "unsigned - unsigned should have been signed", more like "if we still decided to keep unsigned as a size value, then unsigned - unsigned (or rather, unsigned - signed) should be signed" I know such changes would have impacted/be impacted by many things, including issues like the binary representation of signed and unsigned or C-compatibility or indeed `--u`, so this proposition is not very serious. 
I didn't realize you understand me because your answers didn't make it look that way, but probably just because I didn't understand you. So just to make it perfectly clear, let's rewind: `std2` might add an improved vector with small buffer optimization, let's call it `std2::array&lt;T&gt;`. It is not layout compatible with `std::vector&lt;T&gt;`. Now if you have legacy code using `std::vector&lt;T&gt;` it is not compatible with code using `std2::array&lt;T&gt;` and thus requires conversion. In order to make it compatible one either has to break the API or ABI of `std::vector&lt;T&gt;` which one can't do. (And yes, a programmer can ensure that the ABI is not broken, there are rules for ensuring ABI compatibility) (You mentioned some way it could be done without conversions using shared memory allocators. I don't really understand that, please elaborate) Because the two types are not compatible, you can't really introduce std2 gradually in an existing code base, you have to start from scratch for that. This means that only new projects can use std2, the old ones have to continue using std. Now what this means is that the committee has to invest a lot of time to both a) design std2 and b) continue maintaining std, because this is the library used by a majority of projects. This is a lot of work that will slow down C++ at a whole with only a benefit for a minority of users. So they decided to kill std2.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8rc9t9/could_someone_help_me_opening_open_source_project/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Yes, it is a wrapper, but [sol2](https://github.com/ThePhD/sol2) have made Lua it way easier to work with, in my own engine. I seldom have to touch the Lua API anymore, and it have significantly increased my development speed. But it does make the code a bit slower, which is probably the opposite of what you want.
&gt; I already have to cut and paste the symbols for degrees and omega when I need them One hope might be that "you need to support this" pressure from the language would lead to better ways to enter those symbols in your text editor. The best-in-concept thing I've seen along that line was an X input method or something that would recognize TeX sequences as symbols. So if you wanted an omega, you'd type `\omega ` and boom, Ω. That said, I think the better approach is probably the other way around -- use a more "standard" name for the keyword, and then have the editor replace that keyword with something nicer visually. [Here's an example](http://ergoemacs.org/emacs/i/emacs_pretty_lambda_26110.png) of Emacs displaying `(lambda ...)` as `(λ ...)`, using `prettify-symbols-mode`. &gt; Another option is to declaring compliance with new keywords: [[standard:c++20]] (or something similar), to turn them on conditionally. I actually... really really like this idea. Kind of a `"use strict";` thing, and you could scope it similarly. The one thing that I think I'm most concerned about is it'd be nice to put it at the top of a header file and have it apply to just that header; that'd be a bit unusual in that `#include` would function differently from a straight textual replace. Still, `__FILE__` and `__LINE__` has to be tracked anyway, so it seems plausible that the standard already requires implementations to do everything you'd need to do to make that work.
There is a finite amount of 'general' idiomatic algorithms. There is an infinite amount of loops. It works out in the long run. 
You'll be hard-pressed to find a modern optimizing compiler that doesn't inline the templates and lambda to generate the same assembly as the raw loop. And this example is Parent's advice taken to the extreme, and IMO too far. When he talks about "no raw loops", he's usually referring to much higher level operations than this.
My point was not to introduce `std2::` at all. The compiler would use one standard library by default, and then wholesale switch to the new one with the same `std::` namespace if you told it to. No problems with conflicting types, but a lot of problems similar to exactly what you described in your post if you use deprecated or changed functionality.
It was basically a complaint that index_type in `gsl::span` (and in proposed `std::span`) is signed, which is inconsistent with the rest of the standard library.
Is this really a fair comparison? You seem to be fine with moving to c++17 but (presumably) nothanding over your whole memory management to smart pointers. Instead you are probably starting to use smart pointers in new code step by step and (maybe) have to do a handover at some points, because some old interface still expects an owning raw pointer. However, employing the same strategy for a hypothetical std2::vector is suddenly not acceptable? Not all usage of vector is involving it being passed around. And not all interfaces that are used with a std::vector have a const std::vector&amp; as a parameter (they might also accept pointer+length, templates or a span like type.
&gt; You'll be hard-pressed to find a modern optimizing compiler that doesn't inline the templates and lambda to generate the same assembly as the raw loop... which was Matt's whole point here. But most of my time is not spent with optimized code, it is spent at -O0 or -Og. My build folder in debug mode is ~20gigabytes, for only 400 megabytes in release mode.
&gt; (You mentioned some way it could be done without conversions using shared memory allocators. I don't really understand that, please elaborate) there's tons of approaches, but by example, they can both allocate with placement new then you just give them the same location
Well need modules for that. With headers, you'll have to write each header two times: one for the new standard, and one for the old. You won't be allowed to mix the two! With modules, that could become a possibility. Each module interfaces could be implemented using different standards, or even different languages if your implementation permits it. Since they are not transitive like headers, only the compiled binary interface and the compiled object file matters, not the language.
&gt; You seem to be fine with moving to c++17 but (presumably) nothanding over your whole memory management to smart pointers. Instead you are probably starting to use smart pointers in new code step by step and (maybe) have to do a handover at some points, because some old interface still expects an owning raw pointer. So two points. First, for the specific case of smart pointers, we've long been using our own and will continue to (mostly) do so, ignoring the smart pointers. There are good, or at least very reasonable, reasons for that. Second, smart pointers in general don't have this problem. If you need to pass a smart pointer to an old interface taking an owning raw pointer, no problem -- that's what `release()` is for. If you get an owning smart pointer from one, no problem -- you can create a smart pointer from it. `vector` was actually probably not a good example because I suspect that moving from `vector` to `vector2` would also be fast and efficient, but imagine that the committee took Chandler Carruth's criticism of `unordered_*` seriously and `std2::unordered_map` had a completely different layout from `std::unordered_map` -- you can't efficiently go from one to the other.
&gt; I really can't see how you would be able to put a required project structure into the standard. Haha, then I think we agree. I see what you're saying, we probably just misunderstood each other. Have a good day.
&gt; With C++ it looks different: platform matters. This is true for those who either write platform dependent code(win32 API, etc) or don't know how to write C++. &gt; I realized that if I want to do anything that involves user interaction, I will have to tie myself to whatever APIs Windows offers. Most languages use some API, java hides it and C++ does not. &gt; How does real world C++ look like compared to Java, Javascript, and Python? Real world C++ is more pragmatic than any language. Unlike Java, we don't always create factories for everything. Unlike python, variable type, scope, return value matter a lot. 
What if i want to actually use my sizes like numbers? Size1 - Size2 could reasonably be negative. except it would actually be ~eighteen quintillion(18,446,744,073,709,551,615) and difficult if not impossible to check for because unsigned.
I don't think Rust is all that different from what a C++ untethered from backward compatibility might look like.
As for dependencies - you have the same issues in Rust &amp; Go where half your dependencies are written in C, and another 5% are written in C++.
This just isn't useful feedback, without giving some reason as to why. `class` and `struct` are identical modulo defaults which are easily overridden, so there's a kind of logic in re-purposing them towards this convention. Your answer makes it seem like this is some far out there idea of the author's, when in fact, it's sufficiently common and uncontroversial to be in the core guidelines: &gt; C.2: Use class if the class has an invariant; use struct if the data members can vary independently Bottom line this is a perfectly fine convention with some moderate benefit, and definitely the most common convention surrounding when to use class, and when to use struct.
Or just... use them? All 3 major compilers (I believe, much less familiar with MSVC), support designated initializers. In gcc and clang they compile *by default*, you have to specifically turn on pedantic or something like that to make them a compilation error. Our codebase uses clang and gcc, I use it to make our code more readable and there's been zero downside so far.
Yeah, I've gone pretty far out of my way to do any non-trivial text work in C++. I'd much rather pre-process text/configuration stuff in python, do as much as possible there, and pass the "bottom line" into C++.
How's it worse?
I have used the lldb shipped alongside Xcode and it also reports wrong values. You may be right that the bug lies in the generation of the debug info. Wherever it is, fixing it would prove most useful.
But built-in arithmetic types in C++ don't work like people's numbers - and this is where all these arguments about signed cardinal entities go down in flames. Unsigned types in C++ (and arithmetic in processors) act mathematically correct on Galois fields and can be perfectly reasoned about because of that, thereas signed types in C++ (and their treatment by compilers) are a made-up thing with no solid mathematical backing and different rules than their hardware implementation.
Ok, it's straightforward for linear queue. But what about gathering? A, B, C fall into D, D accepts double, for example. What are the rules then? One and only one predecessor should give the double future?
The example from their front page &gt; fn main() { let greetings = ["Hello", "Hola", "Bonjour", "Ciao", "こんにちは", "안녕하세요", "Cześć", "Olá", "Здравствуйте", "Chào bạn", "您好", "Hallo", "Hej", "Ahoj", "سلام","สวัสดี"]; for (num, greeting) in greetings.iter().enumerate() { print!("{} : ", greeting); match num { 0 =&gt; println!("This code is editable and runnable!"), 1 =&gt; println!("¡Este código es editable y ejecutable!"), 2 =&gt; println!("Ce code est modifiable et exécutable !"), 3 =&gt; println!("Questo codice è modificabile ed eseguibile!"), 4 =&gt; println!("このコードは編集して実行出来ます！"), 5 =&gt; println!("여기에서 코드를 수정하고 실행할 수 있습니다!"), 6 =&gt; println!("Ten kod można edytować oraz uruchomić!"), 7 =&gt; println!("Este código é editável e executável!"), 8 =&gt; println!("Этот код можно отредактировать и запустить!"), 9 =&gt; println!("Bạn có thể edit và run code trực tiếp!"), 10 =&gt; println!("这段代码是可以编辑并且能够运行的！"), 11 =&gt; println!("Dieser Code kann bearbeitet und ausgeführt werden!"), 12 =&gt; println!("Den här koden kan redigeras och köras!"), 13 =&gt; println!("Tento kód můžete upravit a spustit"), 14 =&gt; println!("این کد قابلیت ویرایش و اجرا دارد!"), 15 =&gt; println!("โค้ดนี้สามารถแก้ไขได้และรันได้"), _ =&gt; {}, } } } that seems pretty extremely different from C++ to me. it doesn't look like C++ at all. And everything is a different name! call a switch statement a switch statement. it doesn't need to have fallthrough. And wtf is =&gt;? I don't want to use two symbols like that. also no types? and `greetings.iter().enumerate()` is basically python.
Afaik, there are already some proposals for new hash tables due to the inefficient design of std::unordered_map. What's the difference between them and a (imaginary) std2::unordered_map? And, my other argument still stands: How often do you actually have those types in your function interfaces? How often would new code have to pass them through such an interface? I also don't know, why the discussions always reduce to standard library containers. The only things that have (afaik) ever been proposed for std2 is the content of the ranges TS which now will be put into std::ranges (I believe)
[Meson](https://mesonbuild.com)
@MorrisonLevi, Thanks! I think the distributed works will not communicate with each other. They just run some fine-grained job independently.
I don't believe the spec states that. My understanding is the opposite - attempting to use ptrdiff_t to store the difference between pointers that are more than that distance apart incurs undefined behavior. 5.7 #6: &gt; When two pointers to elements of the same array object are subtracted, the result is the diﬀerence of the subscripts of the two array elements. The type of the result is an implementation-deﬁned signed integral type; this type shall be the same type that is deﬁned as std::ptrdiff_t in the &lt;cstddef&gt; header (18.2). As with any other arithmetic overﬂow, if the result does not ﬁt in the space provided, the behavior is undeﬁned. In other words, if the expressions P and Q point to, respectively, the i-th and j-th elements of an array object, the expression (P)-(Q) has the value i−j provided the value ﬁts in an object of type std::ptrdiff_t. Moreover, if the expression P points either to an element of an array object or one past the last element of an array object, and the expression Q points to the last element of the same array object, the expression ((Q)+1)-(P) has the same value as ((Q)-(P))+1 and as -((P)-((Q)+1)), and has the value zero if the expression P points one past the last element of the array object, even though the expression (Q)+1 does not point to an element of the array object. Unless both pointers point to elements of the same array object, or one past the last element of the array object, the behavior is undeﬁned. 18.3.2.2 #5: &gt; The type ptrdiff_t is an implementation-deﬁned signed integer type that can hold the diﬀerence of two subscripts in an array object, as described in 5.7. So, it's a little ambiguous, but it does state that if your result would overflow, it's undefined to use the ptrdiff_t, effectively. Though it's possible to read it differently. However, using `ptrdiff_t` here is a bad idea *simply because it's different than the rest of the C++ standard library*, and thus represents an incompatibility.
@biliwald,Thanks! I divide a large job into some independent small jobs. Right now these small jobs are running on different thread. Can MPI manage and schedule the jobs automatically when running on different machine?
I don't understand your point. The only argument you have is that it's "very good code." I'm not convinced.
Some stuff is similar, some stuff is very different. &gt; call a switch statement a switch statement. Match has significantly different powers than `switch`. They're not really shown off in this example. (other than exhastiveness, which is implied but not called out specifically well. This example is... well we've tried to find new ones but have never been happy.) &gt; also no types? Rust has strong type inference; everything is statically typed here.
More like - actually returning something new.
(++C)++
match is not a switch statement, though it can function as one. It's much more powerful. &gt;And wtf is =&gt;? It's an arrow. Maybe you've heard of them. They point. Could potentially be used in C++ terse lambda syntax. &gt;also no types And? have you heard of `auto`. You can easily write the code in modern C++ that would look like this: int main() { auto greetings = {"Hello", "Hola", "Bonjour", "Ciao", "こんにちは", "안녕하세요", "Cześć", "Olá", "Здравствуйте", "Chào bạn", "您好", "Hallo", "Hej", "Ahoj", "سلام","สวัสดี"}; for (auto &amp;[num, greeting] : enumerate(greetings)) { print("{} : ", greeting); switch (num) { case 0: println("This code is editable and runnable!"); break; case 1: println("¡Este código es editable y ejecutable!"); break; case 2: println("Ce code est modifiable et exécutable !"); break; case 3: println("Questo codice è modificabile ed eseguibile!"); break; case 4: println("このコードは編集して実行出来ます！"); break; case 5: println("여기에서 코드를 수정하고 실행할 수 있습니다!"); break; case 6: println("Ten kod można edytować oraz uruchomić!"); break; case 7: println("Este código é editável e executável!"); break; case 8: println("Этот код можно отредактировать и запустить!"); break; case 9: println("Bạn có thể edit và run code trực tiếp!"); break; case 10: println("这段代码是可以编辑并且能够运行的！"); break; case 11: println("Dieser Code kann bearbeitet und ausgeführt werden!"); break; case 12: println("Den här koden kan redigeras och köras!"); break; case 13: println("Tento kód můžete upravit a spustit"); break; case 14: println("این کد قابلیت ویرایش و اجرا دارد!"); break; case 15: println("โค้ดนี้สามารถแก้ไขได้และรันได้"); break; } } }
We could fork it to ditch the backwards compatibility in-language, but also adjust it to have something like `include_c`/`import_c`, so we could include/import C/C++ headers and use them (and they would be decomposed by the compiler so their symbols/rules could be used by (++C)++)
Why does what those libraries are written in matter? Your dependencies are functionally separate code. If you have to include a header, we can implement an `import_c`/`include_c` header which will let you import headers from C or C++ so your new code can consume the symbols.
That's why you have std1_5 which can intrinsically interop!
Most compilers do describe what exactly they do with bitfields, though, so you can mitigate that pretty easily. GCC and Clang have flags to control their behavior, as well.
Yeah, TBB templates the graph nodes on input and output types.
There are no data types that work like numbers, so how is that helpful?
Eh. Not THAT breaking. The % of code that'd benefit from it is magnitudes higher than than % fo code that actually catches bad_alloc and does something meaningful with it (and can't be easily rewritten).
C+∞
In Sean's "C++ seasoning" talk where introduces the guideline, his definition of raw loop specifically excluded very short loops like that. Still though, the named algorithm is particularly weak for `std::accumulate` because it's too generic. IMO it's much more worthwhile for algorithms with descriptive names like `std::all_of` and `std::any_of`.
&gt; Unsigned ints model modular arithmetic, not non-negative integers, And signed ints don't model any arithmetic. That unsigned has a property that makes it useful for some modular arithmetic is rather unrelated to its usefulness to represent non-negative integers, since signed int clearly also isn't a perfect fit.
some work more like numbers than others
You might be interested in comparing against the technique that Ben Deane described in his [recent C++Now talk](https://youtu.be/2ouxETt75R4?t=2452).
 #define NOMINMAX #include &lt;windows.h&gt; Turn that frown upside down! :p
I used it in my most recent talk, and I already have it on the wall by my desk, where I keep all my Bjarne "+1"s and a "Good rule of thumb" that I got a few years ago. I refer to them when people at work disagree with me :-) (sure, it's "appeal to authority", but it makes my workday easier sometimes)
&gt; ∞ C∞ That looks profane. ∫C (Integral C) C±1 √C (Radical C) C… (C and so on) C! ∀C (C for everyone) σC (C Select)
&gt; And? have you heard of auto. Yes? rust seems to be always use auto, and then if you need you can tack types on at the end like `let x: i32 = 0` which seems like a tacked on feature to me.
Don't forget about `#define WIN32_LEAN_AND_MEAN`! I even learend about another similar one recently but am blanking on what it is. Though that being said, I've made a commit with the following commit log: &gt; I should maybe redirect some of my `#define max` scorn from `&lt;windows.h&gt;` and redirect it at the numerous macros that POSIX headers typically `#define` that don't follow the "macros in uppercase" rule either. &gt; Rename `si_value` to compensate for this.
Sure, if you try to fuck it up.
I saw your talk, but we need to see your wall :)
It was impossible for about 30 years to create a directory in C++ (because directories also "work in specific environments") but now filesystem is a thing. Who knows, maybe #pragma once will make it to C++47. 
Why? Modules actually solves the problem and doesn't require an every growing list of hacks.
No. The include guards have a deterministic easily defined behaviour. pragma once is nearly impossible to define in the standard in a way more usable than it already is. I.e. the most standard could do is just say "#pragma once - does something implementation defined, usually as an alternative to include guards", welp thats already the case and didn't help anything.
No only. There are also edge cases for network drives or for build systems that move/copy the code around.
Depends: On linux, new afaik never throws an exception anyway (maybe if you try to allocate the entire address space) and very few code bases actually try to recover from it when it occurs anyway. 
That would be a breaking change, and we know how the committee thinks about those (it si one thing to remove rarely used types that have a drop-in replacement, but a totally different one to e.g. remove std::string).
Nice!
Whoa, didn't even know that was a thing...
&gt; call a switch statement a switch statement That's not a switch statement, it's pattern matching. They just happen to only use one very specific type of pattern here so it could be replaced with a switch in this case. But general, compile-time pattern matching has been proposed for C++. Consider his implementation of inspect_s here: http://davidsankel.com/uncategorized/c-language-support-for-pattern-matching-and-variants/ So, yeah. Seeing something like that in a reworked C++ is very believable. &gt; And wtf is =&gt;? That is the operator that separates the pattern from the executed statement. The author at that link also proposes using precisely the same syntax for C++. &gt; I don't want to use two symbols like that. You mean like -&gt; ? &gt; greetings.iter().enumerate() is basically python. I have seen virtually identical syntax in C++ countless times. While it's true that we're currently heading more toward a | syntax (we don't have an enumerate yet, so it would probably be zip(ints,greetings) ) that's a pretty tiny change. Anyhow, wasn't thinking so much about the sugar. I was more along the lines of how one thinks about the code.
And now you need to write `arr[i - 1]` everywhere! Or add another variable.
Things from the top of my head: signed vs unsigned sizes, unicode support in `std::string`, conceptified algorithms, a different allocator model for containers. 
Again, that is simply untrue, for Go you indeed depend on some amount of C, but Go isn't a system programming language the way C/C++ are, so sometimes you need the efficiency of C/C++. But Go doesn't even link against libc or requires gcc to compile.. For Rust, however, there's an entire kernel + user friendly shell (Redox) written with 0 lines of C. Any Rust program can run with zero signs of C, not even libc (https://github.com/alexcrichton/rlibc) and libc is essentially 80% assembler and kernel specific calls anyway... not much actual C in there. Other than that... what popular Rust programs require loads of C ? I mean, it compiles to LLVM bytecode and most of the toolchain used after that is C/C++, that's about it. Other than that most Rust programs and libraries that I know require little to no C/C++ code, the C/C++ programs they require are related to interacting with the kernel (libpthread, libdl... etc) and can be replaced by rust libraries if so desired. Obviously, most machine outside the embeded domain run linux and that's essentially huge dependency written in C, but system languages which aren't C/C++ have only started popping up their head in the last few years, and only recently gained enough steam to become main players. Writing a kernel is a task that takes a lot of time, and I'm sure Redux and fuchsia are just the prelude to some amazing kernels to be written in C++/Rust/D or even Go. The question for C++ is if it wants to become the new C or become a modern programming language which is used to solve actual problems, not maintain legacy systems. Anyway, those last 2 paragraphs are my biased opinion, but claiming Rust/Go code usually relies on C libraries is just false.
...[Wut?](https://wandbox.org/permlink/ZOZTHBAp2C58NSs9)
I don't know all the details because I only ever worked with it in the context of a college class, but the objective of MPI is exactly to abstract that part for you. With MPI, you will write a single program, and all machine will run the same program. MPI exposes to you functions to communicate between all of them. Example of such function are MPI_Scatter/MPI_Gather to scatter and gather the values of your working set, like a matrix for example. If you are interested, I could share an example of the MPI program that I wrote for college. The objective of the exercise was to implement matrix inversion using parallel Gaussian elimination. 
First of all, thanks for bringing some hard information on the topic. But I don't agree with that explanation: - First of all, std2 would not only be about containers (actually, so far the ranges library was the only thing proposed for the std2 namespace) - How often have you actually hard-coded std::vector&amp; into important API functions? Often people use templates, ptr + size, span-like types or custom wrappers in the interface. An efficient move from one version to the other should probably be also possible. - If you are working in a code base where you really can't switch to `std2::vector`, because `std::vector&amp;` is used everywhere, then just don't - Are people also opposed to the introduction of more efficient hash tables just because they could not be used as a drop-in replacement for `std::unorderend_map`? 
There's a giant list of special macros for the `windows.h` header to make leaner, meaner, and less problematic than its default. I can't remember them all and I can never find a good list online, but if you search for `#ifdef NO` in `windows.h` I think you find them all pretty easily. I typically have a `WindowsKit.h` header that defines the umpteen billion `NOFOO` macros available, and then even explicitly `#undef` some of the goofy ANSI/UNICODE macros that nobody should ever need (call the correct version explicitly!). That header is then only ever even included from the platform-specific files I have, with the utmost care taken to minimize those as much as possible.
Either I miss something, either on each `set_req_field` he do copy of all Builder (`request_t`) with data...
I don't see overflow in these examples... Unless you talking about huge inputs. The problem with unsigned is wrapping -1 to 2^64-1 isn't what you want most times.
Here is my draft version, using his builder-fluent style technique. If I understood it correctly [http://coliru.stacked-crooked.com/a/f58190d022c8d133](http://coliru.stacked-crooked.com/a/f58190d022c8d133) . I think it's a little bit clearer.... And I think it is possible to make it a little shorter.
Yeah having now looked at those passages I agree with you. The GCC discussions I was remembering mostly talk about their implementation wishing to disallow such objects, which would also be compliant. https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67999 https://gcc.gnu.org/bugzilla/show_bug.cgi?id=63303#c5 I also agree that regardless, having a signed `size_type` is bad just due to inconsistency.
Thanks. Was that a genuine post from james McNellis? If he is still employed by MS I would think that some of his collegues wouldn't be too happy about that statement about the coreguidelines and span. Maybe thats why the tweet is removed 
You can use `find_path()` to find include paths and `find_library()` to find libraries directly. No need for `find_package()` or having a recursive build structure.
If the calculation can already be broken down into independent jobs that don't require communication between workers then you've already solved the hardest part. I would keep your program free of the concerns of distributed processing. Defer that instead to a higher level. Write your program so that it can process one chunk and use something like GNU parallel to distribute the work across multiple machines.
In some code bases, yes, they do. ``` #ifndef XYZZY_COMPONENT_INCLUDED #include &lt;xyzzy_component.h&gt; #endif ``` Up until pretty recently, this was a reasonable performance win keeping the preprocessor from having to search open and parse the header. 
The ones that usually wrap around but are allowed to summon demons in the process or the ones that just wrap around?
[It was genuine](https://twitter.com/JamesMcNellis/status/1007343322174775296). 
I prefer the ones with demons near the numbers I never go close to, than the ones that don't have demons, but cause other demons to appear, around the numbers I use all the time. Let me more fully quote my email: For example, any code that compares adjacent elements can fall prey to unsigned (without compiler warning) // assumes sorted // returns whether each element is far from the next bool is_spacious(std::vector&lt;int&gt; const &amp; v, int space) { for (std::size_t i = 0; i &lt; v.size() -1; i++) if (v[i+1] - v[i] &lt; space) return false; return true; } We had a bug like this in our code. And when it is in the middle of a bigger function, it is harder to see. One example doesn't mean anything, obviously, but it is just one of many. Once upon a time I thought unsigned made more sense; experience has taught me otherwise. Sure, maybe I've only worked with bad programmers. Or, just people that expect numbers to work like numbers.
I admit I understand his sentiment. Much as I agree that indexes/sizes should be signed, it is really annoying to have such a tpye in a code base, where every size variable is unsigned.
Always something worth getting to know or refresh memory about - STL. I also found this interesting blog about few of the algorithms worth checking - http://www.fluentcpp.com/STL
Wow, I've missed that you've put `;` at different place. That makes me look bad, but at the same time kinda shows how this code would look to an average (I hope I'm at least average) programmer on a first glance.
=&gt; is used in dart
The "postscript" type definition is not a tacked on feature but a conscious decision shared by many new languages (as well as the old ones). One reason for it is because indeed, in most cases you don't need to write the type, and as this built-in from the get go (and not "tucked on" like `auto`) it was just more ergonomic choice. Also, it is much less ambiguous (compared to C and C++ most vexing parse and other, although It's possible to design a C-like system without such flaws). Anyway, it still strange to give weight to this difference. It's no more important than sane type names like i32 vs int and is not even present in the example above.
&gt; The include guards have a deterministic easily defined behaviour. pragma once is nearly impossible to define Nonsense. A perfectly fine definition would be "behave as if I had written an include guard manually, which is unique to this file". Any question about "what if there are symlinks, or hard links, etc." can be resolved by saying "what if I had written an include guard manually, what would then happen according to the standard". And if the standard gives that situation well-defined behavior, great, and if it gives it implementation-defined behavior, that's fine too. The definition of `pragma once` does not need to be more rigorous than the spec of include guards. Pragma once cannot be "impossible to define", that's simply nonsense. It's only "impossible to define" in the same way that "the compiler may search an implementation-defined set of paths" etc. etc. when resolving includes. The standard already has a notion of "include paths" leading to "textual substitution" after an `#include` directive is expanded. It is a legal fiction to pretend that the notion of a file is somehow missing here and impossible to make sense of. This is being driven by a bunch of lazy compiler writers who want to use the most optimized API for their OS's file system etc. etc. and just don't want to have to change to someone else's way, so they prefer to pretend that this is impossible to standardize. It is amazing to me the number of otherwise intelligent people that say stuff like "pragma once is nearly impossible to define". 
Well I certainly wouldn't format it that way, all lumped together; I just mimicked the formatting in the GP.
No, it should not. Software breaks. Software needs to be maintained. The act of compiling, linking and packaging a program should not be software.
C64 (Commodore 64) 
C64 (C for Commodore 64) 
Well I don't know. Please correct me. 1) `request_t` may have only 7 requested fields. 2) If we have 7 requested fields its 2\^7=128 `request_t` class versions. How this compile-time-wise? 3) It may be not that clear what happened... For me it was not... 4) I think you can do more then just builder, with "check-list". You still need to group tags, before calling dependent command, but you do control **when** actually calls happens. Not everything can be build from scratch with builder.
&gt;"behave as if I had written an include guard manually, which is unique to this file". Because C++ doesn't really know what is a file and more importantly what is a unique file. Is it the file with the same name? Same size? Same content? Same filepath? What if it's the same file but different macros are defined which may influence it? &gt;can be resolved by saying "what if I had written an include guard manually, what would then happen according to the standard" No it can't. That's the point. If the compiler sees "include guard" (basically just a macro) which it's already seen, it just skips the file. It doesn't care about if it's really the same file or where it's located. It can't reliably do so for #pragma once in a cross-platform/portable way.
&gt; with at least 4 different compilers Which all fucking sucked until about 4 or 5 years ago.
I work as a software architect. Part of my job is figuring out what techniques, libraries, and systems we'll use to build our software. I absolutely *have* banned parts of a standard library. Java's AWT is forbidden. I don't think I'd ban `std2` by name. That's some pointy-haired bullshit. But I certainly might ban use of some features, such as the proposed graphics API (I would prescribe our in-house graphics stack).
C++ but the operator++ is overloaded to increment an unknown amount
&gt; Because C++ doesn't really know what is a file and more importantly what is a unique file. This is a legal fiction. The compiler is reading the files from disk, it knows what they are, there is a notion of filenames being resolved to content in the course of an include being expanded. &gt; If the compiler sees "include guard" (basically just a macro) which it's already seen, it just skips the file. It doesn't care about if it's really the same file or where it's located Yes, we all know the mechanics of include guards. &gt; It can't reliably do so for #pragma once in a cross-platform/portable way. This is obviously untrue. The only thing the committee needs to do here is pick any of the numerous reasonable solutions to this trivial problem and standardize it.
@biliwald Thank you so much! Could you send me the example? 
@delarhi, Thanks!
&gt; You mean like -&gt; ? does anybody like that either? theres a reason most IDEs let you use dot in either case, changing it to `-&gt;` if its a pointer.
..whats dart and why is it relevant?
&gt; This is a legal fiction. The compiler is reading the files from disk, it knows what they are, there is a notion of filenames being resolved to content in the course of an include being expanded. No. It doesn't know what they are. It doesn't always read them from disk. There is no proper performant portable way for it to distinguish between logically the "same" files accessed by different paths. That's been discussed so many times already and I really don't want to reiterate. &gt;This is obviously untrue. The only thing the committee needs to do here is pick any of the numerous reasonable solutions to this trivial problem and standardize it. There are several cross-platform IDEs that support the generation of include guards and do so without problems. That'd make most compilers non-conforming since pragma once functions differently on most implementations. Also, IDE solution is just "good enough". It works in 99% cases, but just as pragma once - sometimes it doesn't. For different IDEs it'd be different cases. Some just use the simple "generate include guard based on filename" approach. Surely that's not ideal for standard implementation (but good enough to save you some boilerplate). I was pro pragma once standardization at first, but really, now, I don't see the point. I don't see what it'd bring/fix. pragmas are already imlementation defined and it's already supported on all sane compilers in whatever capacity they'ce decided on.
Your mic feels like god is talking to me
The point the grand parent is making is about the "Natural numbers" i.e. 0,1,2,3,4,... . You are making a point about the integers. ... -5,-4,-3,-2,-1,0,1,2,3... . I know that some people include 0 in the natural numbers and some don't.
Yea, Jo Boccara's blog is really good!
For header-only libraries something like this: include/foo &lt;public headers go here&gt; detail/ &lt;private headers go here, usually under namespace foo::detail&gt; For projects: include/bar &lt;common headers go here&gt; detail/ &lt;implementation headers go here&gt; src/ &lt;system agnostic sources go here&gt; linux/ &lt;source only compiled on linux&gt; win32/ &lt;source only compiled on windows&gt; ... depend/ &lt;subtrees ans submodules&gt; For larger projects, I sometime pull headers for subsystems under include, so that from the main source files it looks like `#include &lt;audio/mixer.hpp&gt;` or whatever.
followed by std3 to fix all the whacked stuff in std2..
The syntax is.
That's because they wanted to use std++:: instead.
&gt; The problem is transition Or in other words: inertia 
&gt; uncontroversial really? `std::complex&lt;float&gt;` is a `struct` per core guidelines (its real and imag parts can vary independently), but according to OP its a class (it has an interface with methods)
Welll,,, you changed more than half od the tokens so by these metrics it's fairly different
&gt; Because the two types are not compatible, you can't really introduce std2 gradually in an existing code base, you have to start from scratch for that. This means that only new projects can use std2, the old ones have to continue using std. Sorry, but that is nonsense. It's like saying, we can't introduce std::map in our code base, because it isn't compatible with std::vector. You talk like once you use a certain container in a function interface any other container in your program has to be able to be passed through that interface even if it doesn't have anything to do with it. If what you are actually meaning is "you can't gradually replace `std::vector` with `std2::array`", then the first question is why would you want that and the second that your codebase is a mess if you are not able to change any type of any variable without that change affecting all the rest of your code base (and as I mentioned in a comment further up, there are a variety of interface designs that would accept multiple different container types like templates, pointer+length, span/array_view etc.)
I'm not 100% sure if I understand you correctly, but that seems to be utter nonsense or could you provide a code snippet demonstrating what you mean.
When you are programming in c or c++ you aren't targetting a normal x86 or arm cpu but the C abstract machine which behaves fairly differently at times.
One the one hand, this looks really nice, on the other I can't but wonder, if a simple runtime check wouldn't work just as well.
404
Thanks, fixed and reposted https://www.reddit.com/r/cpp/comments/8rht1b/ieee_754_floating_point_type_overengineering/
I only use -O2 really. (should I use -O3 with GCC?). I guess you use a lot of gdb and optimizations make you step over lots of code? I'm not sure if you really needs it.
Hi, this is my first technical article and I'm sure that there are things to improve. I would love know know if you have any feedback. Thanks.
&gt; there's a kind of logic in re-purposing them towards this convention. I disagree with this mindset. It's a language, with a syntax, a grammar, rules... **Nothing** more. Any "convention" relative to usage of certain features in a certain context will be broken at some point by someone and this will lead to lot of wasted time arguing just for the sake of it. The only reason to choose class over struct or conversely is to reduce the amount of times you write `public` or `private`.
If the names are hard to remember, you can opt to use union operators instead. https://codingtidbit.com/2018/01/08/pythonic-operators-on-stl-set-algorithms/
&gt; If what you are actually meaning is "you can't gradually replace std::vector with std2::array" Yes. &gt; then the first question is why would you want that Because you'd like to transition away from `std`. &gt; the second that your codebase is a mess if you are not able to change any type of any variable without that change affecting all the rest of your code base You can of course change it. But you can't do it gradually, you have to refactor a lot of code at once. This was deemed problematic.
&gt; First of all, std2 would not only be about containers (actually, so far the ranges library was the only thing proposed for the std2 namespace) I would imagine that they want to do a redesign of everything for std2, at least a sane `std2::vector&lt;bool&gt;`, for example. &gt; How often have you actually hard-coded std::vector&amp; into important API functions? Often people use templates, ptr + size, span-like types or custom wrappers in the interface. An efficient move from one version to the other should probably be also possible. Yes, but what if you change `std::optional` which is designed for these kinds of uses? &gt; If you are working in a code base where you really can't switch to std2::vector, because std::vector&amp; is used everywhere, then just don't But then the committee has to maintain two standard libraries, which is a huge time sink. `std2` only works if `std` can be dropped (i.e. not being maintained by the committee), otherwise it is a community split.
&gt; (i &gt;= 0 &amp;&amp; i &lt; itemCount) `static_cast&lt;unsigned&gt;(i) &lt; itemCount` will be cheaper and give the same result. If `i &lt; 0` then the cast to unsigned will wrap-around and exceed `itemCount`.
Every time I wanted to debug past -O1 it has been a waste of time. In addition I generally use address sanitizer and it is mostly relevant when building at -O0 since past this some bugs could hide behind compiler optimizations (up to the day they don't anymore of course).
It is really good. Great explanations, good topics. It's difficult to make stuff so digestible.
Are you promoting your own YouTube channel? It looks like you submit a lot of links to this particular account.
First one small thing, you should add a link to the official standard or at least to the wiki, so a reader can do a quick lookup. I really like your article. The flow and length are very nice, but you ask for some constructive feedback so i would like to see a little bit more motivation, as to why we need to resolve this ambiguity and for what purpose, so the reader sees the value of the tests more clearly :-)
I have e.g. a 2D/3D library with geometric shapes. Them having e.g. a radius of Positive&lt;double&gt;, heights of Positive, distance(a,b) returning NonNegative etc. . Having to add checks everywhere seems more cumbersome to me. But sure, if you don't like the library or style, no problem. I don't expect everyone to like it ;)
Or, you know, we could just add more checking to `size_t` that warn when we over/underflow and would have to be explicitly silenced in the very few rare cases that you might actually want it rather than making it nonsensically signed. If the problem is with wonky maths, fix the maths, not the entire world around it to make it look right.
You're completely wrong
It depends on what is doing malloc and free. The Rust folk are realising that OOM -&gt; kill wasn't a great idea.
&gt; I would imagine that they want to do a redesign of everything for std2, at least a sane std2::vector&lt;bool&gt;, for example. That may or may not be the case (a complete duplication of anything in the STL would imho be a complete waste of time an resources and I don't see who'd write the proposals for that), but my point is that anything beyond the ranges TS is pure speculation at this point. &gt; Yes, but what if you change std::optional which is designed for these kinds of uses? Do we need a std2::optional? What problems of std::optional need to be solved? Even if we would get a new one, I can think of very few examples, where an optional is just passed through (in particular not by reference). Usually an object gets wrapped into an optional at one side of the API and unwrapped at the other side. And even if a new optional type would get introduced and even if you had to convert from one type to the other - Is that really a problem? I can imagine why people wouldn't want to copy container types but I'm hard pressed to think of examples where that would introduce significant overhead with optional. &gt; But then the committee has to maintain two standard libraries, which is a huge time sink. std2 only works if std can be dropped (i.e. not being maintained by the committee), otherwise it is a community split. I've already voiced my scepticism about the assumption that a std2 would be a complete duplication of std. More importantly though, I'd like to see some evidence or better explanation why std would result in more maintenance effort. The ranges-TS still gets merged into the standard, a (partial) replacement for IO-streams is also proposed, new hash table is also in the making.The standard library is growing either way. And types that are not broken in std (unique_ptr) can just be imported if you really want to have everything in std2. Also, the whole point of std2 was to be able to make breaking changes without breaking the world - the premise for that is that the old types continue to exist. I don't see std being removed in the next two or decades even if there is a std2/3/4
I agree with this. If the standard specifies something, then we follow that universally, because otherwise our code isn't going to work. If they don't specify something, then that is better left for a style guide specific to the project/work place/etc. to specify that. While the current `class`/`struct` convention is fairly common, there may be codebases out there that achieve better use of the division with different rule for their own purposes, and there's nothing wrong with that. Readability and clarity of code are sometimes fairly subjective.
The latter is usually preferred, e.g. std::string That way you don’t pollute the scope and make things more explicit 
This should [more or less cover it](https://stackoverflow.com/questions/1452721/why-is-using-namespace-std-considered-bad-practice)
There's a couple of things to keep in mind here. You should never use "using namespace" inside a header file because it carries over to any other file that includes that header and could cause namespace collisions for someone else just by including your header file. While that's likely never going to happen for the small code you're writing while you're learning I believe habits are important and this is an easy bad habit to develop. This goes for any namespace. Namespace std:: also has some functions which, if you use namespace std:: can cause namespace collision with some other standard libraries from the C library. If the function you want isn't the one that the compiler picks you've got problems, so I never use namespace std even in files where I would use other namespaces. After debugging enough problems from namespace collisions I tend to pick out the things I want to import into the namespace individually because if I do a Using namespace boost; I have no idea what I've just brought into my namespace and what collisions this caused or might cause when some header in a dependency changes to include a new boost library. While writing this makes me feel like I should be outside yelling at clouds I hope you find it useful. 
I stick with "std::" as I prefer to be explicit
The point was that it's not _that_ different.
&gt; After debugging enough problems from namespace collisions I tend to pick out the things I want to import into the namespace individually Since OP is learning, what is meant here is something like #include &lt;iostream&gt; #include &lt;string&gt; using std::cout; int main() { std::string str{ "Hi" }; cout &lt;&lt; str &lt;&lt; endl; }
Well, if you can guarantee that runtime checks will occur at startup - then yes. But, what if your runtime checks in conditional branch? What if they will trigger only sometimes?
It's not about tokens, but the structure. If C++ was designed "from grounds-up" without backwards-compatibility with old C/C++ in mind, they'd have no reason to keep the old tokens where it doesn't make sense. But that is not really important. If the same code can be written in both languages with only significant difference being a few replaced names, it doesn't count as "extremely different". You can rewrite this in haskell and that'd be rather different (though this example is a bit too simple to show all the differences). One of the points I think ShhTinyBanjoPurr was making is that most visible differences just come from some design decisions that would most likely also be present in "modern redesign" of C++ (but not necessarily).
I'm certain if I dump a few thousands random C++ projects and count those that catch bad_alloc, their % would be in single digits You don't really need it that often (and it's not always possible to do) and not many 3rd party libs handle it gracefully anyway. And when you really need to handle the OoM, there is a high chance you don't use many STL containers anyway. The remaining % is rather small. Also, afaik bad_alloc is a bit broken anyway, i.e. there exist situation that it can fail.
Except it's 2x longer and doesn't increase readability at all.
Oh, I do agree that `ptrdiff_t` is far from an ideal name and that I wish `size_t` was signed instead. I just think `ssize_t` is too error-prone, and the worst is that unlike `=` vs `==` code is likely to compile, and even work somewhat, when mistaking `ssize_t` for `size_t` (or vice-versa) making finding the problem tricky :(
My feedback would be that IEEE 754 types aren't good, if you want conformance let Fortran or Matlab do it, if you want precision and speed you better relax the requirements. One of the main issues you get with strict conformance is time loss on every function checking for NaN and the like.
&gt; Let’s say we want to use IEEE 754 32/64bit floating point types in C++, then there is float and double right? Unfortunately, C++ standard guarantees almost nothing about the built-in floating point types. Huh?!? static constexpr bool is_iec559; The value of std::numeric_limits&lt;T&gt;::is_iec559 is true for all floating-point types T which fulfill the requirements of IEC 559 (IEEE 754) standard. If std::numeric_limits&lt;T&gt;::is_iec559 is true, then std::numeric_limits&lt;T&gt;::has_infinity, std::numeric_limits&lt;T&gt;::has_quiet_NaN, and std::numeric_limits&lt;T&gt;::has_signaling_NaN are also true.
You assume the need for exact floats is for self contained numerical computation, what if you need to deal with data from another source without mangling it?
Just make it simple and don't use `class` keyword, ever.
Not claiming that runtime checks are as good, just wondering if they would be good enough (didn't use the builder pattern too much so far and usually not a cross complex control flow/function boundaries.
&gt; You can of course change it. But you can't do it gradually, you have to refactor a lot of code at once. This was deemed problematic. And I'm saying your codebase is a mess if that is the case. Of course it always depends on the details and what you consider "a lot of code".
The syntax is, if reasonable (parsable, convenient), arguably one of the least interesting characteristics of a PL. The thing is that C++ syntax is crap; writing a parser of it is an insane amount of work. Hey, even a programmer reading it sometimes can't distinguish between a declaration and a function call without great efforts. Even the semi modern features have aspects that are insane because of syntactical causes and the interaction with legacy, when you dig in the details: like {} initialization. So the syntax of C++ is part of C++ problems; using another one, if sane (and the one of Rust is, at least more than C++) *must* be done if you can tolerate breaking backward compat. I doubt you will be able to map a C++ syntactical style to a cleaner underlying design. Same is true for pretty much *all* C++ features. Smart pointers and move semantics are somewhat nice in the C++ context, but in the absolute they are extremely broken. Because what you really want is enforcement of *real* lifetime checks and guarantees, and move destruction, not moved-from unspecified but destructible states in some cases and empty states in others, nor mere RAII delete (or RC) while raw pointers are still available. This was just an extra example but the list could go on. C++ is fundamentally broken if you want some *serious* help from the compiler, and if you insist of keeping the language too much the same there is no reason this will ever be able to change. And obviously the syntax is strongly linked with the semantics, the two have co-evolved. So for nearly all practical purposes, we can consider Rust being better at doing what C++ wants to do, if unconstrained by legacy compat. One of the few areas where Rust has problems is the lack of a (semi-)stable ABI on some Unix like systems (GNU/Linux, maybe some BSD), at de-facto platform level. There never have been one on some other systems for C++, so in some cases it is not even a problem to compete. Go however, is not in the same category. It uses a GC, so it is unsuitable as a replacement in some cases. 
And? That just proves the quote. If standard guaranteed something about floating point, there wouldn't be any facilities to check whether they actually adhere to some standard.
You mean simply store the data? You probably don't need the whole shebang for that. Unless you absolutely need to handle NaNs.
&gt;On linux, new afaik never throws an exception anyway (maybe if you try to allocate the entire address space) and very few code bases actually try to recover from it when it occurs anyway. This idea is only true enough to be dangerous, and I'm very afraid if it causes people who don't know enough to "specify" yet another case of UB in the standard (like it was presented the last time I read the Zero-overhead deterministic exceptions proposal). That would be retarded. Under Linux If you try to allocate way too much memory (like: more than your total amount of RAM+swap) under any configuration, or any amount of not-available memory on a system where overcommit is not allowed, new will throw. The fact that OOM killing exist is unimportant at language level. It is not more important as the fact the the process can always be asynchronously killed by external forces, for all kind of reasons. On some systems under some configurations, some resource exhaustion is one of those reasons (but on those systems even the concept of which process made a "too large" allocation, and which process shall be killed as a result to try to recover some resources, is not completely deterministic, and there probably are no way to render it deterministic). It's none of the PL business. 
C++ is fine and it is getting better all the time. You are a drama queen. 
To store data for yourself no, to store data accessed by (or access data stored by) other applications yes.
The total scope for something like `[[standard:...]]` would be up for debate, of course. It could also offer a route towards finally deprecating some core features we all hate, without immediately having to rewrite massive amounts of code. I know it has been discussed, and apparently Stroustrup doesn't like it. I'm not sure why exactly, but perhaps he wants to limit the possibility of dialects of C++ as much as possible. Still, as a safe way to introduce new keywords it seems worth exploring. 
C++ is getting better, I agree with that. Fine is likely in the eye of the beholder, and depends on the application. For the last part, I'm sorry I won't be able to participate in trolling requests. So thinking slightly more about it at a meta level, maybe you are actually sort of right. Bye. 
i'm pretty tired of you insulting me, downvoting me, then asking me for things go read a junior c++ programmer's textbook. placement new is not complicated
C♯ !
What's the fucking point of C++'s namespaces, it doesn't even have private symbols. What a joke. 
You basically want to avoid polluting the namespaces for other programmers. So don't use them in global scope when you're writing a header file, and generally prefer using individual objects or functions instead of the entirety of the namespace. Makes it easier for maintainers to see where everything comes from. There are tons of [stupid namespace tricks](https://blogs.msdn.microsoft.com/oldnewthing/20180516-00/?p=98765) that you might want to review at some point so you know what's possible. Sometimes maybe you want to import some objects from another namespace into one of your own. Personally though, I usually just type them out in headers so that I know where everything is coming from. Maintenance is usually a lot more effort than the original programming, so I've developed a lot of my habits around supporting maintenance. Personally, if I could give myself one piece of advice a couple of decades ago, it would be test early and often. Get comfortable with one of the unit test libraries for the language I'm working with, and write unit tests as you develop each class. I haven't quite drunk the test-driven-development kool-aid yet, but it's [worth reading up on.](http://wiki.c2.com/?TestDrivenDevelopment) Having a solid suite of unit tests can save you a lot of debugging effort when you need it the most.
It's true that the keyword should've been implicit, instead of explicit. However, I think we still would need an implicit (bool) in that case, to have some conditional control over it.
Great, I assume you're ok with naming variables `liKeThis_RighT`? After all, as far as the language syntax and grammar rules go, that's a perfectly legal variable name. There's a valid case to be made against this class vs struct convention, but you're not making it. Conventions are everywhere, in every language, and simply saying "the convention isn't compiler enforced and therefore it will be broken and lead to arguing" isn't a useful argument; if the rules are clearly stated, easy to follow and spot, and enforced in code review, and have some actual utility, they can be worth it. You haven't made any arguments about any of these points.
I think, I have good example for you. SVG Path: namespace painter_interface{ class Path{ public: Path(const FillRule&amp; = FillRule::non_zero); void move_to(float x, float y); void line_to(float x, float y); void quad_to(float cx, float cy, float x, float y); }; } `line_to` and `quad_to` must be called only after `move_to`. And `move_to` is definitely not part of constructor/builder. class Path{ public: Path(const FillRule&amp; = FillRule::non_zero); struct Moved : Applied&lt;Moved, Path&gt;; Moved move_to(float x, float y); struct Lined : Applied&lt;Lined , Path&gt;; Lined line_to(const Moved&amp;, float x, float y); struct Quaded: Applied&lt;Quaded, Path&gt;; Quaded quad_to(const Moved&amp;, float cx, float cy, float x, float y); }; With this interface you simply can't make that mistake. And you can't have something like SubPath for free (when you exit SubPath - you need to restore your previous Pen position, and to do this you need to store last\_pos for each operation) \[may be or may be not costly - but diffenetley not no-op\].
I don't think you really agree with the original point; the original point argues that *any* convention is stupid, you're arguing that it's ok for individual codebases to have their own convention. FWIW I do agree with your point in principle, I just think a codebase/project would need a fairly compelling reason to diverge from the norm, and I personally haven't been involved in such a project, but I don't deny it could exist.
I'm wondering on what kind of hardware this actually is useful, that is, what is the intersection of hardware which : * has #include &lt;limits&gt; and a compiler capable from targetting it * does not have IEEE754 fp. I tested all the options provided in godbolt, x86, ARM, MIPS, PPC, PPC64 and I could not find one which did had `is_iec559` at false. `is_iec559` is set, in libstdc++, to : `__FLT_HAS_INFINITY__ &amp;&amp; __FLT_HAS_QUIET_NAN__ &amp;&amp; __FLT_HAS_DENORM__`. I could not find a *single* instance of either of these three being set to zero : * https://www.google.com/search?q=+"__FLT_HAS_INFINITY__+0" * https://www.google.com/search?q=+"__FLT_HAS_QUIET_NAN__+0" * https://www.google.com/search?q=+"__FLT_HAS_DENORM__+0" 
IBM?
Uncontroversial doesn't mean 100% total acceptance in all cases. It just means that the fraction of people objecting to the practice was judged to be small enough that the core guidelines thought it was ok to accept as a guideline. In the case of complex, my guess is that it's a case (relatively rare) where the primary purpose of data encapsulation is to hide the representation. the accessors have to return a T, but they may not have wanted to constrain it to store two T's. Note that none of the functions (that I can see) return T&amp;, only T by value, which means that the structure doesn't have to store two T's at all. With a generic class, that the user is allowed to specialize, data hiding increases its utility. I think the guideline could be stated a bit better; class/struct should actually be all private vs all public data. Private vs public data in turn is closely tied to invariants, but there are other reasons (again, relatively rare) why you might make data private.
&gt; Great, I assume you're ok with naming variables liKeThis_RighT? After all, as far as the language syntax and grammar rules go, that's a perfectly legal variable name. ... yes, I am 100% ok with it if it makes sense in a particular point of a project. I never understood all the fuzz about naming in the C++ community. 
&gt; the other recognize that the ℤ/nℤ is a valid set of number ℤ/nℤ is a valid set. It's not "numbers" as commonly understood by regular humans by any means
The one thing that kills meson for me is the explicit lack of support for wildcards: I don't see any kind of issue with wating even half a second at the start of a build to compare the filesystem to the cache (and frankly: if it takes half a second something is VERY wrong), given that most builds will take much longer to begin with. The argument is especially moot given that there is no reason why speed should be degraded for those who don't use wildcards. If he changed that policy, I'd probably switch immediately. 
https://en.wikipedia.org/wiki/IBM_Floating_Point_Architecture
It is misleading to say never use “using namespace xxx” in the headers. Considering the wider usage of templates and more and more header only libraries nowadays, it would cause great pain for the lib programmers to do as what you say. I think the rule should be “Never use it in the global scope in the header”?
You do have to use macros to lift template or overloaded functions, unless they were declared with [`BOOST_HOF_STATIC_FUNCTION`](http://boost-hof.readthedocs.io/en/latest/include/boost/hof/function.html).
yes, I hope other programmers don't have to look at this to understand it because it's a bit more cryptic?
So you're saying that there are projects that deal with it. Even the Rust folk are thinking about reverting its behaviour on OOM: https://internals.rust-lang.org/t/could-we-support-unwinding-from-oom-at-least-for-collections/3673/32
Good point, I just added a link to the standard text. :) You're absolutely right on that I haven't provided compelling cases where this can be useful. Well actually, for me it was more for fun rather than a practical usage. I think I should have communicated that clearly in the beginning. Now I've read it again, I've realized that it's very poor in that respect to the point that it's confusing. I'll think about how to revise it. Thanks for the good feedback!
What about valarrays? I've never seen one used. 
If you just started learning C++, I would recommend you to check out and subscribe to r/cpp_questions In general, that sub is much more suitable for newbie questions. r/cpp is for generally more advanced topics.
It does not have IEEE and it has (I presume) a modern C++ compiler. So I guess it would qualify?
No. Rust thread is not about "reverting its behaviour on OOM". It's about being able to optionally do sensible things then OOM is detected (and detect it in the first place). That's different. Also, I don't see how it relates to your broad "completely wrong" statement. I never implied there are no projects that need to handle OOM gracefully.
the article says that it has IEEE since 1998
Well, yes. But can you imagine IBM not supporting their own format on their own hardware? I have difficulties imagining an IBM C++ compiler on IBM hardware not supporting double and float with IBM representation.
I disagree, it's a set of numbers. The operations are not the most usual ones, but it's still numbers, just like the primes numbers, etc. As we had issues with modeling numbers, using as much as possible mathematical notion over the every-day life ones will yield much better results.
&gt; using as much as possible mathematical notion over the every-day life ones will yield much better results. I don't know who you are programming with but I'd wager a good 70% of programmers don't have any formal math education
I was talking about language designers, and maybe library writers (if the library theme is relevant). The need for the regular dev to know that is much lower. 
&gt; However when I used Java, you really had to have your project set up in this specific way or there's pretty much no way to build and run your application. This is completely false buddy, `javac` does not dictate any project structure, using the common maven conventions `src/(main|test)/java` means that build tools and IDEs work with 0 lines of configuration, but you can easily change these if you are so inclined.
&gt; When I was a beginner 2 years ago 22 years later and I am still a beginner, you must be a fast learner.
I agree but we need to keep pushing fordward
Basic computer programming lessons in high school also helped a bit . Get started with a project. You wont regret it.
whats your definition of a beginner?
A person who still hasn't passed the stage of loops , types and variables and is unable to form a structure a project in mind before implementing it.
I feel like, with a language as complex as C++, it doesn't go from beginner to expert, it goes from beginner to expert to master to super user to TMP pro etc. There's so much complexity that it's easy to always learn something, but you can still be productive within a few months of using it, like any other language.
I would say the new conditional explicit keyword is one of those new features which won't be of any benefit to 95% programmers. I know there is a paper from STL implementors I know they argue that conditional explicit will simplify library code for std::variant or std::optional but meh this is not the kind of problem developers other than STL writers meet regularly.
*Only* 22 years? Pssht, I've been programming in C++ for 31 years and I'm *still* a beginner
My first introduction to computer science was through C where I learned all about malloc, free, pointers and low level stuff. In some ways it was frustrating, I spent many a late night debugging cryptic memory errors, but I'm glad I did it. I feel like jumping in the deep end with a language that doesn't hold your hand as much makes moving to more modern languages so much easier than it would've been vice versa.
I'm with you my friend. I'm 24 years into c++, and it is one of those languages that I will continue to learn forever.
That's not what he meant. He meant he still *feels* like a beginner, even though he's been using C++ for 22 years.
Whoosh. The commenter is saying that 22 years hasn't taught them enough to call themselves not a beginner. I think you missed the sarcasm. C++ is an extremely complex language, and two years is nowhere near enough time to call yourself an expert in it. That being said, it's a really rewarding language if you dig in to it, and it's good to hear new users joining the community!
I feel like I'll always be a beginner at programming x.x 
&gt; The pros of building desktop apps with C++ far outweigh the cons. What pros does a native/non-managed code base have in particular for desktop apps?
&gt;javac does not dictate any project structure Everything I could find told me that you'd have to be absolutely insane to use javac to build an application. &gt;using the common maven conventions src/(main|test)/java means that build tools and IDEs work with 0 lines of configuration, but you can easily change these if you are so inclined. That's good to know. I was using Gradle to build my Java applications, and spent many hours trying to get it working with my layout, before eventually giving up. 
I was a little proud for myself to been programming in C++ for about a decade... But I 100% agree, it's a forever learning, which is fun!
&gt;22 years later and I am still a beginner, you must be a fast learner. True, very true especially when u start attending CppCon
It makes me a little uncomfortable having that on the form. Can you instead just add a sentence to your description stating that your company has a diverse and welcoming environment, and people of all genders/races/backgrounds are welcome to apply?
Yea nobody uses javac directly, just saying that there is no mandated project structure for java at the core. However Gradle with the config language being groovy script is a terrible tool if you don’t know it very well, too many ways to shoot yourself in the foot. I have also given up on it many times.
The facilities however give a rather wide ranging guarantee with a true result and many programmers don't care about platforms that return false. 
&gt;too many ways to shoot yourself in the foot. Sounds about right. I just got it working """good enough""" to get my assignments done and started to realize how much I liked C++.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8ript5/learning_c/e0spcqe/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8r6yog/what_should_be_keep_in_mind_when_developing_a/e0spj0r/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
That has very little to do with them though. They don't provide anything, just help write portable code.
My heart goes out to those affected by execution order. I once troubleshooted for hours about why the hell things would come out in reverse order after being serialized then unserialized. 
Actually what I am trying to say is that maybe the value that is supposed to bring is only really valuable in certain narrow contexts. I do not mean the library is uselss.
IBM seems to have made their floats rather complicated (for historic reasons I guess), but yes, that z10 seems to have those hardware FP-units. In the nineties, I worked for a company (in finance), that were running an s36 (a.o. for accounting), replacing that large freezer size thing with a (rather sturdy) desktop size box AS400 was a god-sent. I think (and you basically implied that) it's fair to say that the code in the article is not a comprehensive solution for IBM mainframes.
I like this layout. You cover subtrees/modules as well as the cross platform problem. I just did some research on this a few weeks ago and many recommend the directory layout solution like this vs. indefatigable everywhere. Do you have any example projects like this up on a public repo site?
I once worked with IBM floating point values because I had to convert GRIB data which uses the IBM format. I actually did not find the format difficult to understand but was bitten by the fact that you apparently do not need to normalise the value as you do with IEEE. 
True. On the other hand application development in Modern C++ is easy peasy, like Python with a more verbose syntax while library development of generic components is black magic that very few people master.
You better ask Blaise Pascal.... http://quoteinvestigator.com/2012/04/28/shorter-letter/
Writing sloppy hard-to-follow content is ready. Editing it down for length and clarity takes time, effort, and skill. I think it was unironic. See also: “Beware of bugs in the above code; I have only proven it correct, not tried it.”
https://quoteinvestigator.com/2012/04/28/shorter-letter/
I am not claiming these with respect to the content of the book itself, but of the language in general
&gt; The need for the regular dev to know that is much lower. well, no, the regular dev has to know that because the regular dev uses `std::vector::operator[](std::size_t)`
Since your whatever-it's-called lists "Embedded" -- wouldn't signed sizes have a negative impact on performance in the embedded space? E.g. on Atmel 8-bit processors that one bit matters.
If the signed/unsigned warning is on (and it should be), and if the API was correctly implemented (thus the need for select lib writers to know the mathematical notion behind), then the dev would get a nice warning. No need to go any further, even if could be a plus.
It's called "pithy".
It takes time to create a good summary. He is saying that he didn't have the time to condense the information down. Just like code, you write it and then refactor it down.
A proposal for a large library like this would do far better if it were to come up through Boost. That gives it extra review rounds, implementation experience, and actual in the field experience, before ever coming to the point where the committee needs to worry about it. 
I think (correct me if I'm wrong) the proposal was not to make allocation failure UB, but rather to terminate the program. I'm not entirely ok with that, but I see a certain tariff involved: If allocations are noexcept that can make some code more efficient. Considering that those exceptions are a) very rare (or don't ever happen in a large class of environments) and b) are often not dealt gracefully with (better than terminate but still it often will just crash the program) it might be worth it. On the other hand, I certainly want to handle allocation failures from e.g. a local memory pool or when I try to allocate a large buffer on a resource constraint system. Personally the reasoning in the paper looks to me more like - "Allocation failure is rare and hard to handle so let's pretend it doesn't exist at all" , which might be a convenient approach, but not necessarily good engineering. 
Ok, great. Here's a suggestion for Herb: why not add new functions that are called `..._or_die` and leave the current set alone? *Why is it necessary to so blatantly break compatibility here?* 
I work with multiple gigabyte datasets. Normally they are processed in batches, but recently a customer decided to try processing an entire dataset in one go. It ran out of memory, calmly announced the problem, backed out of that particular sub-operation, and finished everything else. Notice how this is different from "it aborted without so much as a notification to the customer", which would have resulted in a non-compliance report to me. So what do you reckon is a good approach here? I don't know how big a dataset is until I try loading it (it's a packed format, and the on-disk size can be very different from the in-memory size). So should I just load it and allow the application to crash if it happens to be too much? How do you even find out how much 'too much' is? Will Windows tell you? How about Linux? How will I know the value I obtain from the OS will still be valid by the time I need to use that memory (race condition!)? Let's be clear here: if STL starts aborting on OOM, we will stop using STL. If C++ starts aborting on OOM, I don't see us doing any new work in C++ anymore.
&gt;can you clarify for me your concern? 1. Need to rewrite millions of lines of code to use new functions. 2. C++ no longer acceptable for mission-critical roles where just aborting is not an option. 3. Blatant compatibility break. 
The proposal explicitly forbids that.
Exception safety is not all that hard; apply RAII carefully and you're basically already there. And while it may still have little things that break, it certainly breaks a hell of a lot less than when the language breaks it on principle. 
I can't see how signedness could have a negative impact on performance here. The processor treats them equally anyway, and if your code is well thought out and all preconditions are met, compilers can't take advantage of UB to 'offer' an alleged performance benefit.
I just added gcc/g++ 7 headers which include std::byte. Also the web client runs with -std=c++17 now. See [here](https://github.com/andreasfertig/cppinsights/issues/36) 
Doesn't clarify the question, though. Pascal wrote it sincerely.
please upstream the zapcc features to clang proper
I'm not Herb (or anyone's) personal secretary. If you want your position presented write a paper. If you are unable or unwilling to use the machinery available to you beyond that, I will personally propose and present it for you.
The behavior you rely on is not portable; depending on the kernel you may observe error long after allocation (overcommit), unusable thrashing (virtual memory), or terminate on OOM (OOM killer). &gt; So what do you reckon is a good approach here? Use `std::set_new_handler` to "calmly announced the problem, backed out of that particular sub-operation" and terminate. No one suggested "abort[ing] without so much as a notification to the customer". In terms of making your program more robust, maybe the format can be modified to indicate its own size? This is a robustness feature that would make sense regardless of how OOM is handled.
Well I think your right in the context of std containers, since iirc standard requires them to be able to address all memory locations. That said, on an Atmel 328p using signed ints as indices on arrays is quite wasteful since you in effect throw away half the addressable range. To wit, maximum array size becomes 128 rather than 256. To use signed ints you'll have to resort more often to using 16 bit (signed) ints which on a 8 bit machine will require synthesized arithmetic and additional memory loads, versus simply using a register. Also wraparound on 8 bit machine can be nice for ring buffers 😉
You'll need to actually read the paper to learn why.
1. There are techniques that are not "millions of lines"; and you can always stay on C++20 2. Failure to handle OOM is not a C++ issue 3. That "compatibility" was not cross platform to begin with 
&gt;// Copyright (c) 2014-2017 Ceemple Software Ltd. All rights Reserved. What is the license of this code? Just cramming one line copyright notice into new files is quite unhelpful.
It says in README that &gt; This open source release is licensed under the LLVM Release License (University of Illinois/NCSA). It is also stated in LICENSE.txt file I am not a lawyer but I assume the same license applies for all the source code in the repository, unless otherwise stated.
This appears to be original llvm readme. I wouldn't take this code if it was up to me.
&gt;If the option std::embed_options::null_terminated is specified, then the expressions *v.end() and *(v.data() + v.size()) will evaluate to and compare equal to the value 0. Why do you even need null terminator? That seems so arbitrary.
This is something I've wished for a long time. Mainly because using vendor api for embedding is often traumatic. 
&gt; Every C and C++ programmer -- at some point -- attempts to #include large chunks of non-C++ source into their code. Of course, #include expects the format of the data to be source code, and thusly the program fails with spectacular lexer errors. Lol, what!?
Note the you have the C++ calling convention playing against you. If you return a struct by value, if it is bigger than 128 bit, it will be passed by reference instead.
I think of it as working on poems.
Hopefully most of those names would only come into play once C++ has better unicode and text encoding support. I'd hate to use a programming language that isn't reliably able to support it's own name!
There are xxd and bin2c and tons of other tools though.
Forgetting to convert it to XPM first is a classic newbie mistake.
&gt; Because you'd like to transition away from &gt; std &gt; . Again, why? I can understand that I want to transition from a particular type to another type if that offers particular advantages or the original type has particular problems. A lot of things in `std` are not broken and imho don't require transitioning away. 
Please use a variable-width font for paragraphs. "Handle" usually means a reference to a resource owned by the OS. (One semi-exception: Classic Mac OS had generic "handles" which were just memory blobs that the OS could relocate to combat fragmentation.) Using indexes instead of pointers is a useful technique, but catch-all arrays aren't a good programming solution. This is something that C++ allocators were supposed to solve (i.e. encapsulate), but failed.
Sorry, apparently I gave the wrong impression: I'm not necessarily in favor to call `std::terminate` if `new` fails (the reasoning I read sounded mostly like "it makes thigns easier for people that don't care" but what is with the people that do). I'm just saying, that changing `std::bad_alloc` into `std::terminate` isn't necessarily that big of a breaking change for many code bases. Imho there is a large class of code bases, where it is absolutely a breaking change.
Can you show an example that contains an actual error? Without that it is hard to say, whether dynamic checking would actually be a problem or if the kind of error is likely enough to justify the complexity and additional dependency. Again, I like your solution and see its advantages. I'd just would like to see some convincing examples of bugs this is solving (otherwise it tends to look like a solution that looks for a problem)
&gt; Where's the thing I want to use? In std:: or std2::? If you can't kkeep track of the types in the c++ standard library I really wonder how you manage in a project that has a few more dependencies (e.g. boost) or a language with an actually large standard library like c# or java. How do you even know how the thing you want to use is called?
Shader files come to mind. I've even seen some [engines write them as c++ code](https://github.com/cocos2d/cocos2d-x/blob/v3/cocos/renderer/ccShader_PositionTexture_uColor.frag) and then [include them all at once](https://github.com/cocos2d/cocos2d-x/blob/v3/cocos/renderer/ccShaders.cpp). 
Maybe in another two decades, why not? 
I'd wager most people doing UI work or games did
Parsing of such preprocessed multi-Mb files can become a noticeable portion in compile time. And I've seen some old IDEs stumble if you try to open such file for editing.
This is basically the conceit to the Qt resource system, so that developers don't have to figure out how to make this work. So Qt packages it for you and you don't see the #include, but you're still doing it behind the scenes.
Handles are just a generalization of pointers. Probably safer than passing a bunch of raw pointers around, but almost certainly not safer or faster than correctly using modern C++ smart pointers. I can see a scenario where you needed a higher level abstraction of a pointer that references some object that may or may not exist in system memory at any given time, or that needs to be movable, but other than that it seems like asserting that they're always better than pointers is just absurd.
You mean the readme that starts off "zapcc is a caching C++ compiler based on clang"
(Author here) Some APIs require null-terminated sequences of bytes. I added it since some people pointed out it would be useful. There's room for other options.
No, i meant readme.txt, i missed the license in *.md one. Having reference to it in source would still be nice.
(I'm the Author) The updated version of the paper lives here: https://rawgit.com/ThePhD/embed/master/papers/d1040%20-%20embed.html Prominent changes feature a Guy Davidson quote, woo! (Used with permission, of course.) There are many improvements. I plan to submit r1 to the post-Rapperswil mailing list, since LEWG got to look at it. I am also getting close to finishing my implementation (and then will start working on the implementation of the Future Direction portion, which includes compile-time aliasing to other trivial types).
This is exactly what I am trying to avoid. And as /u/carrottread accurately points out, larger files make for quite a bit of problems.
As a side note, incbin -- and the github project that wraps it -- are not portable. See the end of incbin's readme about VC++ or read the updated version of the paper, linked elsewhere in a top-level comment on this thread.
I've encountered this comment on hackernews: https://news.ycombinator.com/item?id=17332899 &gt; I evaluated zapcc at one point. Its a fork of clang because the changes are very intrusive to clang, and probably would have had a hard time getting accepted in to trunk without a ton of debate. I believe it actually daemonizes in to a long running server process (zapccs) that then holds the instantiated templates in memory, communicating with the zapcc compiler process over IPC. I think that most clang maintainers would have found that radical of a rearchitecting kind of controversial especially from an outsider to the project.
Yes, this is one of the many use cases I aim to make vastly simpler. Butchering your shaders into C++ code (and losing any ability to edit them on an independent level) is frustrating.
QT’s doesnt totally make me hate myself 
Maybe not "meow.jpg", but "test.py" or "server.lua"? Absolutely.
Yes, if you're making a Qt application that's acceptable. Otherwise dragging a 10+ Mb library along only to embed resources is a pain. 
Yeah, the QT environment integration is what makes it nice
The important part are imho not the handle vs pointer but the customized allocation strategy and relocatibility.
Good related [article about handle_maps](https://www.gamedev.net/articles/programming/general-and-gameplay-programming/game-engine-containers-handle_map-r4495/) by Jeff Kiah.
That was literally unreadable. I have no idea how OP achieved this effect, I am used to reading monospace in terminal but that was just letter salad.
that takes me back to resource files when using the classic WinAPI... I like the idea of a cross platform resource file inside the exe. I can also think of a few nice to have additional features like encryption etc. There are also a few open questions, like how do the library functions find the data in the generated exe. At the moment I imagine that the compiler generates some kind of magic adresses for begin and end of the (contigous?) data and also magic values representing size etc. But now I get some security concerns and questions: * What happens if somebody manipulates this magic values? * How can I be sure that the security of my application is not compromised, because the magic values have been changed from outside? * What happens if somebody from outside changed the data inside the exe file? * Do we need some kind of security mechanisms to protect the data... I do not know whether this concerns are valid and if we are able to shift them off to the compiler implementer... Can you mitigate these concerns or tell me why they are invalid?
The way data is embedded is implementation-defined: this function exists **purely** at compile time. Using the function at runtime is going to be ill-formed when I write the formal standardese for this feature. Data staying in the binary is -- by Quality of Implementation -- something I want to avoid if it is used entirely at compile-time only. If someone can compromise your executable and start re-writing bytes, you have MUCH bigger problems than "how safe is my std::embed data". Adding such checks are outside the scope of this proposal and likely outside the scope of the entire C++ standard right now in general.
ok than I have error in reasoning somewhere, if i understand you right, the data is only there during compilation, that means it is **not** part of the final executable (only its effects during compilation are)?
Gimp can save images as C header files :)
TIL that some people have trouble reading monospaced text? In the DOS days that's all there was. 
Did you actually read the article?
Doesn't this violate the principle of separating a program from its data?
I wonder if the speedup is beneficial to the tooling side e.g. would Qt Creator's libclang parser be faster?
When you have real utf8 support, string array access no longer makes sense. Not sure how people will take that.
Shaders must be shipped as SPIR-V in Vulkan and can be shipped as bytecode in D3D11. OpenGL is a mess.
Will it detect if the data is already null terminated?
Code is data.
Zlib compression with constexpr decompression. :)
My streaming asynch renderers do this. It plays nicely with instanced systems and SOA semantics.
Rolling your own is not so bad, [it takes 50+ lines of Python](https://github.com/jpakkane/monocoque). That being said having this builtin to the compiler is awesome and something we should totally get done.
Hello, I don't understand why it does make you uncomfortable ? Can you please elaborate ?
What about people who do not identify with either gender? Why call attention to this at all?
Usually (ok... often, but certainly not always...), types and functions are in namespaces that make sense - like that's the point of namespaces. To group things into groups that make sense and give those groups sensible (ideally) names. What we're talking about here is first of total non-meaningless namespace names (`std`, `std2`), and then a sort-of non-logical division between stuff that's in the former and latter - like you might have an `std::sort` that works on iterators, and an `std2::sort` that works on ranges. And then you have one of them that supports an execution policy as first argument, while the latter one doesn't. Yeah. Surely you can argue "Ranges are new, you would now that, and all new stuff is in `std2`". But I'm pretty sure that wouldn't hold up long. Like imagine half of boost was in `boost::` and the other half in `boost2::`. Where is `program_options`? In the former or latter? Where is `lexical_cast`? Where is ...xyz...? I am sure you could not remember all of them and having to look that up each other time sucks.
I love fast IPC, but why not just use files? They would be cached in memory or written to an SSD anyway.
&gt;(should I use -O3 with GCC?). [Yes.](https://youtu.be/w5Z4JlMJ1VQ) tl:dr: "O3 breaks your code" is FUD unless your program leans on undefined behavior, in which case you should stop doing that anyways. Even `-ffast-math` might be your friend on modern hardware. 
&gt;3.1.1. Pre-processors and **Massages** What a treat!
You know, all my examples that I can only imagine solve "MUST\_USE at least once before calling this" problem. And all of them can be solved dynamically.... But you probably ask where it reasonable to use.... To be true, the only *altered* form of this technique, that I really used is [https://www.reddit.com/r/cpp/comments/8qvoll/access\_control\_for\_field\_mutation/](https://www.reddit.com/r/cpp/comments/8qvoll/access_control_for_field_mutation/) *(Mutators passkey composition replaced with "check-list")*. And I think its pretty powerful - you can sleep at night, without doubts that you accidentally, somewhere, somehow touched field from wrong thread at the wrong moon phase; update field from inherited class and just forget that you also must update 40 other values, that was based on it. You clearly **see** where, when and what bits you can update. *With something like check-lists you can have AccessGroups.* If simplify to the max, and convert to callbacks: struct Document{ std::function&lt;MutName::Mutated(Document&amp;, MutName)&gt; rename_impl; Renamed rename(){ rename_impl(*this, {}); return {}; } std::function&lt;ApplyList&lt;Renamed, Saved&gt;(Document&amp; doc)&gt; rename_and_save_impl = [](Document&amp; doc){ return doc.rename() | doc.save();} RenamedAndSaved rename_and_save(){ rename_and_save_impl(*this); return{}; } }; Think of this, as a static contract... \-------- Some thoughts about application: // Complex state is really easy to describe, and track in this way. template&lt;class State = ApplyList&lt;&gt;&gt; struct SDL_Surface_Session{ void* surface = nullptr; template&lt;class Other&gt; SDL_Surface_Session(SDL_Surface_Session&lt;Other&gt;&amp;&amp; other){ std::swap(surface, other.surface); } struct Locked : Applied&lt;Locked&gt;; auto lock(){ static_assert(!State.contains(Locked())); SDL_LockSurface(surface); // not sure clang-tidy will track this move. See version below. return decltype(State() + Locked()){std::move(*this)}; } auto unlock(){ static_assert(State.contains(Locked())); SDL_UnlockSurface(surface); return decltype(State() - Locked()){std::move(*this)}; } ~SDL_Surface_Session(){ /* unlock if locked */ } }; SDL_Surface_Session sf(surface_front); // With clang-tidy use after move is error you will never // misuse, twice lock or unlock auto sf1 = sf.lock(); if(cond){ auto inter = sf1.unlock(); sf1 = inter.lock(); // same type (reinit moved value [llvm should understand this according to doc]) } auto sf2 = sf1.unlock(); Probably more clang-tidy friendly version: // This should be conditional friendly // Rely on llvm/clang ability of detecting move in branch. struct SDL_Surface{ void* surface; struct Locked : Applied&lt;Locked&gt;; static std::pair&lt;SDL_Surface, Locked&gt; build(void* surface){ return {{surface},{}}; } template&lt;class State&gt; auto lock(State&amp;&amp;){ static_assert(!State.contains(Locked())); SDL_LockSurface(surface); return State() + Locked(); } template&lt;class State&gt; auto unlock(State&amp;&amp;){ static_assert(State.contains(Locked())); SDL_UnlockSurface(surface); return State() - Locked(); } }; [sf, state_initial] = SDL_Surface::build(surface_front); auto state1 = sf.lock(std::move(state_initial)); if(cond){ auto intermediate_state = state1.unlock(std::move(state1)); state1 = sf.lock(std::move(intermediate_state)); // ok - same type } auto state2 = state1.unlock(std::move(state1));
If you concatenate a `.zip` at runtime, you get the best of both worlds: one-file distribution, but easily changeable. (and also doesn't completely kill your build times, which is the main reason to never do this)
From the README: &gt; When was the source last merged with LLVM trunk? &gt; This open-source release was last merged with LLVM r307021 on 2017-07-03. That's almost a year ago. Cool stuff, but not very promising, unless the maintainers can demonstrate quick/fast rebasing.
If you have Firefox try their 'Reader View' feature. It strips most of the formatting of the page providing a very clean view of the content.
No because of poor compiler support.
I think it's that specific monospaced font. Those typewriter fonts are wide with narrow strokes. Other monospaced fonts exist, like Adobe Source Code Pro. It is also probably the colour scheme: dark, not enough contrast, making the narrow strokes even harder to focus on than normal. I guess that's why typewriter fonts were like that - intended for black and white printing on paper.
You should post this WindowsKit.h somewhere. I have given myself countless headaches when `windows.h` brings in some name clashing problems and I get a cryptic error later on, getting to see all the things would be nice since I've blown more hours than I'd like to admit when I included it in the middle of a project and things started breaking everywhere.
The thing I just realized by reading your comment is how much mangled C++ names don't phase me anymore. I guess special thanks to having to dumpster dive the stl implementation and after seeing that gigantic clusterfuck... it has changed me for the better (or worse!)
There is a binary and a string format. For how to handle the null terminator, it's probably going to change since bikeshedding will happen.
Some data needs to be `constexpr` for best performance. For example, let's say you want to perform 64x64 DCTs. You can hardcode the parameters in your .cpp file, but it will be big and ugly. Being able to include a binary file with all these parameters can improve the compile times (no need to parse your huge array declaration), and will provide the same binary in the end. If you load the parameters at run time, you're going to lose a lot on the optimizations. I'm not sure if in this specific case it's faster to load literals or use an array, but there are definitely cases where having literals in your code makes it faster.
You can totally dump it in the executable though. You can make a const array that has this data.
You cannot specify between binary and text. The previous proposal mentioned in Prior Art tried to do that, and it made that part of the proposal go right to hell: binary/bytes only, and you have the freedom to cast the `std::byte` into whatever you deem necessary. We will not detect if the data is already null-terminated. If you're including it, you should know what kind of binary you're putting in your executable. 2 bytes of null termination as opposed to 1 in the rare case you forget your file might have one hardly sounds like the worst thing to happen.
Good article but couldn't go by my day without saying this sorry. Wow, that's a surprisingly ugly font. Good find.
I'm pretty sure the paper left throwing or recoverable memory allocation options in. There's still `malloc` after all. I think the idea is to make the default `new` non throwing and make it terminate the program on failure, but let people make a throwing version if they want. Containers would be `noexcept` if their allocator uses a `noexcept` new.
Some good thoughts, but ultimately I feel like the author conflates the type system with keywords and operators and instantiated objects. Forgive the aggressive shorthand below, but what is the type? To my way of thinking *MyObject* is the type and *m* is just a pointer to an object of type MyObject. Similarly *n* is just a constant of type *MyObject*. class MyObject {}; auto* m = new MyObject(); const MyObject n; I think if we take the article to its logical conclusion, we would end up saying that a lot of C++ features are little more than syntax sugar because the compiler turns things into pointers. Maybe it is true that, ultimately, these features are syntax sugar for developers, but they are important sugar nonetheless because they allow the programmer to communicate intent. They also allow us to increase memory safety or have RAII, which aren't really related to the type system in my opinion. Personally, I view the type system in the narrowest, most axiomatic way possible and prefer not to clutter it up with matters of notation: ex: pointer, reference, value.
Oh, I'm sorry - there are _techniques_ to modify millions of lines of existing source code to magically rewrite themselves to use new function calls if they want to remain safe in the future? Pray tell, what are those _techniques_? Does it involve an editor and man-years of typing and testing? We are discussing how C++ handles OOM. How can you possibly state that it is not relevant to the language? Who's talking about cross platform support? So far, C++ was a good investment because if you wrote a large application in it, you would know it still compiles on the next C++ version. With this change, that will no longer be true. And we are not talking about some minor thing you could fix with search and replace either, we are talking about the very heart of the language. "Stay on C++20" is not an acceptable answer. We have no desire to work with a dead language that will slowly lose compiler support, and for which there will be fewer and fewer external libraries. 
On Linux it's fairly easy to generate an ELF object file containing data from an arbitrary binary file using "objcopy --input binary". 
You are conflating what one specific OS does in some configurations with how the language should work across all platforms. The OOM killer can be disabled, it is not present on every platform (such as non-linux), doesn't trigger on large allocations, and doesn't trigger if a single process runs out of addresses (32-bit, it happens) while the system still has plenty of free memory. In the proposal it specifcally says that set_new_handler is only ever allowed to abort. It may very specifically not back out. Have you actually read it before becoming its champion here on reddit? There is no need to modify the program, because it is _already_ robust. The thing that is happening right now is that the underlying language is being modified so we lose that robustness. 
I like this proposal (after a quick scan). To me this should be part of the 2D graphics library. SFML f.e. provides machinery to load sound and graphics, easily.
You should try to get your company to sponsor you into a voting meeting. That out-from-under-everyone change sounds irresponsible to me. 
Trying to stop people from altering your binary is a useless fight. If there is something to "crack", then there will be plenty of people who will find great joy in creating the tools to circumvent your security (and it won't take long).
&gt; Please use a variable-width font for paragraphs. actually the problem is probably the kerning settings set in css, and a poor choice of letter spacing
&gt;Zapcc server has a memory limit and will automatically reset after reaching it, restarting with an empty cache and low memory usage. I'm somewhat surprised it doesn't use a least recently used cache here, seems that could provide a better usage of the cache.
Every programmer? I never did...
That sounds reasonable. Thanks!
Thanks, could you please fix the formatting.
Did you read [Herbceptions](https://wg21.link/p0709)? I think it was pretty interesting at how it's possible to make C++ faster, since not having to throw exceptions as much and being able to make almost everything `noexcept` reduces the boilerplate coming from that.
I've read it, but skimmed some parts and didn't remember all the details of others. It is interesting and probably the right way for c++ to go, but just like meta classes far too far out to be of big interest to me at the moment.
&gt; Having a std2 would be ugly as hell, people will start to guess “where is the thing I want to use? It's no worse than guessing whether a particular function is in `&lt;algorithm&gt;` or `&lt;numerics&gt;` library. Good documentation with a decent search capability makes this a non-issue. 
The code is impressive, but please, don't put code into a header file.
I made this change a while back with my renderer, lets you some neat stuff. You can pack your handles for all your graphics state into an integer key. You do this for every draw call and when everything's submitted, you sort and merge the keys. Makes it much, much easier to batch draw calls and do threaded rendering since you order doesn't matter while submitting a draw call.
This looks like a great concept and I think much like the author that almost anyone had a need for this at some point. However, I do not see yet how this can be achieved purely by a magic compiler extension without causing some major issues with existing build systems. Questions such as: "How do we provide the search path for the files in a cross platform manner? How should the build system know which files to monitor in order to trigger a rebuild?" and many more that I have not thought of will surely arise. The issue being, that there is no standard for the complete toolchain. In the end this will be just one more special case to support in the tooling without providing substantial improvements over some custom build step. Sadly I think this feature is currently out of scope for the standard, eventhough it is highly desirable.
I'm pretty sure he's pushing for it now already, there's no dependency on other features like metaclasses. I think you can read his committee report where he'll talk all about what other people thought of it.
If SFML or any other graphics library of choice already supports loading from a pointer+length, or an istream, then it can load from a `std::embed`ded resource. But this proposal contains things that are useful to have but not particularly useful for graphics, such as embedded data being available in a constexpr context.
Whenever I accidentally end up in an STL/boost header I think of the poor (cruel?) people working on those things.
I like this proposal as well. I do some Qt applications and I just love the resource system.
There is a story for us Java old timers that Gosling went around Sun offices asking about unsigned arithmetic and given that almost everyone got it wrong, he was convinced to have only signed numbers on Java. Not sure how much of it is true, as I don't remember where I got it from.
Oh. I was under the impression that header files are used to segment the code into many files, and we can then include those in the main cpp file. How should it properly be done?
Oh, sorry
Superfluous
I like this article. The blog is well-written and the author has a good sense of what constitutes interesting and noteworthy content. Anyway, I don't like the idea of using "size" for things that are counted unless it's in bytes or maybe things like shoe sizes. Size is almost like the billion dolllar error part 2. So I propose "count". It seems like a great idea to just use count for things that are counted. Size is bad because size sometimes means count. But size might also mean size in bytes. Which can be a count, but not always. And size isn't always the actual size, with offsets and padding and platform and compiler idiosyncrasies and all that.
You split the code into multiple .cpp files. Header files just allow .cpp files to communicate. Thus, declarations (of classes and functions) go into headers and definitions go into .cpp files (with exception of templates which have to be inside headers).
Just keep in mind that, as `/u/matthieum` tangentially mentions, `try_` methods aren't really a panacea. For anecdotal evidence, Rust started where C++ is going (heap exhaustion aborts the process) and more recently it has gotten configurable heap exhaustion semantics (`oom=panic`/`oom=abort`), and `try_` methods (`Vec::try_push`). In practice, none of this really works when overcommit is enabled (most Linux default configurations, MacOSX, *BSDs, ...). Basically, either you have a system without overcommit (Windows, embedded, ...), or you better design your application to never attempt to exhaust the heap. 
thanks, I'll watch it. 
God knows we have the time for getting a massage whilst waiting for the pre-processor.
Had the same thought. Guess they don't have the time to invest in doing something better here. Question is how often you hit this limit.
The worst part of C - lack of RAiI
Clang has had C++ modules since 2010, and its module system is less intrusive (it allows incremental adoption in big projects) and supports more features (like exporting macros).
Don't forget there's room between `variant` and `any`. With something like `dyno`, you can have a non-template that can store something based on its interface. This lets you have, say, a container of objects that can be any type as long as they can be used the same way.
Much fun I had using C++
I'm not sure whether C++20 allows the declaration of `size` in the article. From [\[namespace.std\]/7](http://eel.is/c++draft/namespace.std#7): &gt; Other than in namespace `std` or in a namespace within namespace `std`, a program may provide an overload for any library function template designated as a customization point, provided that (a) the overload's declaration depends on at least one user-defined type and (b) the overload meets the standard library requirements for the customization point. `size` is a designated customization point ([\[iterator.range\]/1](http://eel.is/c++draft/iterator.range#1.sentence-2)). In the article, the function template `size`: - does not depends on any user-defined type - does not seem to meet the standard library requirements (because it does not always return the same type as `std::size` does) Thus such declaration might be not allowed by the standard. But maybe I'm just misunderstanding [namespace.std]/7.
And then you add error checking, fix some bugs, add some features FUCK IT ITS COMPLICATED NOW RECODE
Same here - it's especially fun when everything just works on the compiler you've used for months, and then breaks in the most hilarious ways once you switch compilers :) 
A simpler fix that hasn't been mentioned is to make the global `size` a function object, which kills ADL. Another way to kill ADL at a call site is to parenthesize the function name. (Also, the `&lt;::` and `&gt;&gt;` fixes are only superficially similar. `&lt;::` is fixed during translation phase 3 as a special exception to maximal munch. `&gt;&gt;` is fixed during translation phase 7 by reinterpreting a `&gt;&gt;` token as two `&gt;` tokens in certain contexts.)
(Just bought it so I do not have full picture yet, or any in fact) I agree with you about this challenge, yet I would rather say that the author should have provided more ways of solving this problem, starting with above basic one and then give example using ranges and compile time solution. Then it would have been complete, [C Templates Complete Guide 2nd](https://www.amazon.com/C-Templates-Complete-Guide-2nd/dp/0321714121) are wrote in that manner and it works perfectly well. But still this is just one challenge out of 100, and as someone else mentioned first from the list. In my opinion it's not a best choice to review the book on just single example, but might bring some concerns. 
FYI I got asked at Rapperswil to make P1031 *Low level file i/o* (https://wg21.link/P1031) constexpr, so at constexpr time, one would be able arbitrarily read and write files on the filesystem. The compiler vendors are fairly appalled, obviously, but out of the many stakeholders I've asked about this, nobody can think of any good technical reason to not allow it, and there is a long list of useful use cases. Curiously, memory mapped file i/o is a far bigger ask than constexpr file i/o, believe it or not. It would actually be less objectionable, currently, for all-constexpr memory mapped file i/o than runtime memory mapped file i/o. This doesn't affect your proposed `std::embed`, other than to say that yours could be a convenience high level API written on top of low level constexpr file i/o, if P1031 makes it into C++ 23. Thanks for bringing the proposal!
is there anything proposing dyno for std:: ?
Not directly at this time. There was [virtual concepts](https://github.com/andyprowl/virtual-concepts), but I think Andy got busy. You can make do with reflection to make the library easier to use, and easier yet with metaclasses. Louis did mention his thoughts on the underlying machinery this library uses being standardized in his C++Now Back to the Basics talk, and I think his thoughts on dyno itself as well.
Pff, don't return bare pointers...
I also met this issue and wrote such a piece of library (explained in README.md): [https://github.com/jm4R/explicit\_tuple](https://github.com/jm4R/explicit_tuple)
I totally agree. It's better to be specific when naming. Another example: windowSize() vs windowCount() 
I agree 100&amp;#37; with Bjarne's comments. While the enthusiasm for C++ is fun to be a part of, the lack of focus on proposals (and instead the focus on adding features) is a problem. As an example, the graphics proposal. IMHO this should not be a part of the language (of course!) and probably should not even be part of the standard library. Instead, we need Modules and a Package Manager so that 5 or 6 different graphics libraries can be made available to any who choose to import them. This is akin to Python having Matplotlib, Bokeh, Seaborn, etc. I hope that the standards committee picks up what Bjarne is layin' down, cuz he's right.
`using namespace std;` and even `using std::size` are traps. Type out `std::size` if that is what you mean to call, it isn't that long. Making C++ code read like Python is a non-goal.
I agree, the main thing missing from `any` is a standard way to imbue it with an interface. I use (https://github.com/tzlaine/type_erasure), to achieve something like that (less fancy than `dyno` though).
&gt; As an example, the graphics proposal. IMHO this should not be a part of the language (of course!) and probably should not even be part of the standard library. Instead, we need Modules and a Package Manager so that 5 or 6 different graphics libraries can be made available to any who choose to import them. This is akin to Python having Matplotlib, Bokeh, Seaborn, etc. Great summary!
There is a misspelling in your title. It should read "Anything that you need to know about std any".
I don't think there is anything wrong with a graphics proposal in itself but damn it'd be nice to be able to make a socket first.
&gt; Individually, many proposals make sense. Together they are insanity to the point of endangering the future of C++. Perl had the same problem, when version 6 was planned. A lot of proposals were placed that were too narrow in focus. Thanks to strong development team, they picked the features that made sense to bigger groups of people, instead of small groups of interest. I would assume c++ is managed well enough to facilitate the same thing.
I respectfully disagree. Paths are defined by the implementation, and are expected to be configured in the same way as include paths (because there is no other sane alternative). Because this feature is only callable at compile-time, the implementation has full access to the `resource_identifier` string_view that denotes where the resource comes from. After that, it is a matter of adding caching on top of that. At this point, the problem becomes the same as `#include`, except that the contents of `std::embed` can't be affected by preprocessor-macros and other strange things, which means the answer is cache-able *across translation units*. Note that in the implementation I'm working on, I get this for essentially free using Clang's `SourceManager` and `FileManager`, which is what is already used for opening and reading source files during compilation. I have not yet implemented dependency tracking, but that's why I'm going to submit my implementation to the cfe-dev mailing list when it's not Hot Garbage™. I expect the answer for it will be "use what is already used for include files". In other words: this is an entirely solved problem, with entirely reasonable solutions already at hand.
The code in the picture makes me want to close that window and quit my job.
I'm not yet really into C++, planning to properly learn it over this summer. I agree though, importing modules for more specialist applications is a far better approach. 
All these concerns are invalid. Remember that you are compiling C++ source code into executable code. In what scenario do you have a build tree where C++ source can be trusted but static data files cannot?
If you can modify the content of an executable, you can modify the content of the executable. Your attack scenario is too overcomplicated. Replace the executable contents with \`VIRUS.EXE\`.
&gt; IMHO this should not be a part of the language (of course!) and probably should not even be part of the standard library. Instead, we need Modules and a Package Manager so that 5 or 6 different graphics libraries can be made available to any who choose to import them I second this *a lot*. Thinking of `std::` as a way to make easy for people to access third party libraries is a symptom of severe issues going on.
For me personally package management has been solved with CMake+Hunter. And when you use CMake targets/interfaces with Hunter's namespacing and dependency management you are effectively use libraries as Modules. None the less I can see the benefit of having it standardized for performance reasons. But why does package management need to be in the standard? Different people have different requirements and goals here and it doesn't look to me like a settled/solved problem 
Funnily enough, it seemed to me that Bjarne is very much in favour of the graphics proposal making it into the standard.
I'm reading this just fine...
Cool, thanks for sharing this - even though it looks like Schrödinger's cat danced over my keyboard. I'll probably never check this out, though, because I firmly stand in the west.
I was also used to west for years, but now this approach seems more reasonable to me :)
As a Californian, Left/west const is the best const.
ENTERPRISE_CPP_FACTORY_FACTORY
He's very keen on *some* graphics proposal making it into the standard.
That 's why c++ need a package management. If c++ has its own "pip" and we can easily install and use boost using pip install boost::lib_name, why bother add everything into c++ standard ?
&gt; But why does package management need to be in the standard? For the same reason headers are &gt; But why does package management need to be in the standard? So you don't need 2 or 3 external tools to include a file into your program. &gt; Different people have different requirements and goals here and it doesn't look to me like a settled/solved problem The corner cases of package management more settled/solved than the corner cases of headers for the vast majority. It's the same as how &lt;thread&gt; is good enough for &gt;95% of people and the other 5% of use cases are doing something you could never write a standard to cover all of.
this is the answer. 
&gt; As an example, the graphics proposal. IMHO this should not be a part of the language (of course!) and probably should not even be part of the standard library Sure. And network also shouldn't be in either. And neither should threading, or strings. Let's go back to the time where there was all those marvelous competing ways of doing string manipulation, it was such a joy, and I mam missing it. /s &gt; Instead, we need Modules and a Package Manager Newsflash: we won't be getting a standard package manager anytime soon. &gt; I hope that the standards committee picks up what Bjarne is layin' down, cuz he's right. He actually wanted the graphic library in the standard.
Ineteresting choice of examples: for (int&amp; x : v) ++x; // increment each element of the container v vs. for (int i=0; i&lt;MAX; i++) ++v[i]; // increment each element of the array v While I agree that the first is the way to go, it suffers from some slight issues: for (int x : v) ++x; // increment each element of the container v or the more subtle version of: for (auto x : v ) result += x.compute_stuff(); that lurks in so many codebases. There is also another point that the second one is more general than the first, for instance if you need to do it only on the elements multiple of 3, or, more classic, if you need to do it on two vectors, like: for (int i=0; i&lt;MAX; i++) f( v[i], w[i] ); or when you have you own indexed objects: for (int i=0; i&lt;object.max(); i++) object.get(i).do_something(); (because not everyone is able to implement iterators) So the range loop make for much cleaner code, but I am not sure it makes the language simpler.
But what if you _don't_ mean to call `std::size`?
&gt; Newsflash: we won't be getting a standard package manager anytime soon. Nor modules xD
The old clang modules are not modules, it's about generating a PCH for a single header and consuming that PCH, so parsing gets faster. Their `import` directive is a preprocessor directive and get translated to including a PCH. These are headers disguised as modules, but not modules. Clang modules supports less features because they are not modules. True, it does support macros, just like headers, but also come with all the problems we had before, because they are still headers. Their modules are still transitive in nature. That means of I change a line in a header somewhere, every file that include out it (or "import" it) will have to be recompiled, even if you don't use it directly, if you included a module that include a module that include that header, you will need to be recompiled. In contrast, the module TS is not transitive. In a module interface, you can change the implementation of inline function as much as you want, and it's likely to not cause recompilation of direct importers. Indeed, for a module interface, you could ask a compiler if a BMI has any change that will semantically affect importers. So you can add imports, change the implementation of a function, change the return type of a private function (I'm less sure for this one) or you can add as much non-exported code in your interface and it won't cause recompilation. Clang also have less features because as far as I know, you cannot have private stuff. You're still left with `detail` namespaces everywhere, just like headers. Also, when they're are implementation detail in a header, and changing something there also causes recompilation every time. &gt; macros do not play well with -fmodules-ts (e.g. &lt;cassert&gt;, ASSERT, __CALLING_FUNCTION__, ..., macros for supporting attributes in different compilers, etc.), These are all supported in the module TS. `__PRETTY_FUNCTION__` is still defined in any functions, and modules won't change that. You still have access to all compile definitions and all compiler generated macros. For ASSERT and cassert, they will still work with the module TS. Simply include the header `&lt;cassert&gt;` in the file where you need it and you're good to go! including a header somewhere don't break any of the goals of the module TS. Even if the `&lt;cassert&gt;` is edited, somehow, it will only cause recompilation for direct includers. This is something clang module did not achieved yet (as far as I know) because a macro can change the meaning of the whole code. Also, the beautiful thing about including in a modular world is that you know that you used the preprocessor in that file, so you know you may have macros. In fact, the great thing about that is compile definition still work as before, including a header still work as before and you still can use macros, as long as you put them in headers. You will say to me that including a lot of headers might slowdown compilation when you include the same thing everywhere. I deal with this problem below. &gt; fmodules-ts doesn't automatically modularzie already modules code using #include There's no need for that in the module TS, since headers and module interfaces behave in a completely different way that is not 1:1. &gt; you cannot externally modularize dependencies easily That don't belong in a compiler nor in a module proposal. This belong in *tooling*. I. The module TS, you can create a module interface that basically export everything for an external library. Of course, for a large amount of code and library, it's tedious and repetitive. This is why we need a tool! A code generator that take all the declaration in a header, or a group of headers, and generate a module TS interface. Optionally, we could also generate a header for exported macros. That code generator can be invoked by the build system, and can even be invoked when a code tries to import the legacy library. This would work with any compiler, and don't require to transform modules into headers by adding macros to them. With this tool, you won't need for the compiler to automagically transform headers to import, since a separated tool will exist for that purpose. It could be used for C headers, external libraries and you're own code, not rewritten in modules yet. That tool could be done with clang tooling, it seem already pretty good at that kind of stuff! We could reuse part of their old module implementation to help modularize the world! For the exported macros header, if your library has done things correctly and undef every macros that are used only for implementation detail, that small header might be faster than exported macro by clang modules. That header will contain the minimum amount of preporcessor code. We can do that by computing the difference of the preprocessor state between before including the library and all it's dependencies and after. And at the end, macros that are transitive between modules cannot exist. If you want them with modules, `import` must be a preprocessor directive, and you're pretty much left with headers at that point. Clang mixes the preprocessor step and the C++ step, but the two are separate in nature. With the module TS, `import` is a C++ statement. A C++ statement cannot change the preprocessor state because the preprocessor happened *before* any C++ parsing. There is also a very interesting paper [p1052r0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1052r0.html) that explain a small part of the problem.
=&gt; is used in C#
That causes the entire program to be a single compilation unit. There are pretty significant trade-offs involved, in particular there's scalability issues. But if you make a separated project as these folks are recommending and later decide you'd _like_ to do a unity build, there are tools to do that. You don't have to design for it upfront. That said, designing for it up-front has some tooling advantages, in particular you don't have to worry about build systems that want to you register every new source file by hand (a pet peeve of mine). It also allows you to be generic even when unnecessary, with auto params and all that, if that's a direction you want to go (allows a weird kind of decoupling). But... that's an unpopular style for a reason.
Did you [adapt](https://en.wiktionary.org/wiki/adapt) rather then [adopt](https://en.wiktionary.org/wiki/adopt)?
I wholeheartedly agree with this article. Half the problem is how C++ is taught, the other half is how C++ makes certain simple things very difficult for new programmers. 
&gt; At this point, the problem becomes the same as #include So I don't say this with any kind of confidence, but isn't it the same thing as *dynamically-generated* includes? Which a lot of popular build systems currently punt on?
SG15 is looking at package management, but it is highly unlikely that it will be in the standard. It's much more likely to be documents about how package management interacts with builds, modules, etc. 
I guess both terms have intersecting meaning. What exactly do you mean?
&gt; The compiler vendors are fairly appalled, obviously, but out of the many stakeholders I've asked about this, nobody can think of any good technical reason to not allow it, and there is a long list of useful use cases. What about the security argument? That even *compiling* a program is now something you can't do? I'd hope at least that vendors would require (and the standard would allow them to require) an explicit flag to opt into allowing this feature for a specific compilation.
Why not that: #if defined(QL_USE_STD_UNIQUE_PTR) template&lt;typename T&gt; using selected_ptr = std::unique_ptr&lt;T&gt;; #else template&lt;typename T&gt; using selected_ptr = std::auto_ptr&lt;T&gt;; #endif
I'd like C++ to focus on lessening its increasing verbosity. "Modern" C++ is starting to look frighteningly arcane, verbose, and it's gradually becoming a write-only language. For instance - the various iteration algorithms. Not only does C++ now have like 4 or 5 ways to iterate over a container (a problem shared with Java), but it lacks truly terse syntax to use things like `std::foreach` sanely. In another thread, people were half-joking about forking C++... but that's slowly becoming a reasonable course of action. I'm relatively disillusioned with committee-driven design, and dislike the fallacy of the mean that it encourages. C++ still lacks many core things other languages have, lacks standardized packaging or building, lacks modules, proper designated initializers, built-in auto-parallelization without reliance on algorithms and lambdas without arcane syntax, and more. The committee also seems to *really* like jamming things that should be implemented at the language level into the stdlib with templates, making the code even *more* verbose and awkward. Why not, instead of `shared_ptr` and `unique_ptr`, use *^* to designate a refcnted shared pointer like in C++/CX, and say a `__unique` modifier or something for unique? Why are attributes wrapped in [[attr]] instead of the far more terse @attr? Why can we not define custom annotations like in Java? Why doesn't C++ add a preprocessor extension so we can namespace-scope macros? And so on. Instead we are talking about a graphics proposal. We don't even have networking, proper multithreading capabilities (though it has gotten better), proper portability for packages and builds... hell, we don't have sane string handling or the ability to determine if an argument is `constexpr`... but we're worried about graphics.
The standard is concerned with portable behavior. You seem to be very confused about what I'm trying to do, thinking I'm here to convince you of something. I'm trying to convince *myself* of something. If *you* are trying to do what I think you are, your effort is better spent writing a paper. If you're unable or unwilling to present it, I will gladly do that for you. I will gladly "champion" it at committee.
&gt; Das Allgemeine Gleichbehandlungsgesetz (AGG) verbietet die direkte oder indirekte Diskriminierung von Bewerberinnen und Bewerbern durch die Formulierung der Stellenanzeige oder Ausschreibung. Die Anforderungen der Stelle sind ethnisch neutral, geschlechts- und altersneutral zu beschreiben, es sei denn, die Besetzung durch eine Person mit einem bestimmten Profil ist sachlich erforderlich (z. B. weibliche Models für Präsentation von Damenmoden). Translated this means roughly: Direct or indirect discrimination of applicants in formulating the job advertisement is illegal. The requirements have to be i.a. gender neutral if this is not an explicit requirement for the job. So, attention is called to this because it is a legal requirement in Germany. Also you are not exempt from this law on the internet. Now is a job advertisement gender neutral enough in English? Probably, but some just want to be safe. 
Downvoting because of the pejorative words at the end. Also, you could make an argument about out of index range bugs that occur with array subscripting.
This reply is unnecessarily argumentative. There are tools, like altering your input format or using custom allocators, that you could use instead of `try_`, and probably should use `bad_alloc` doesn't throw on many systems. Until you're able to improve the robustness and portability of your important code, you will have a very very very long time with C++20 compatible compilers at your disposal.
You missed the past where I offered to present a counter paper on his behalf.
The caching problem becomes the same as #include, not the whole feature. What's the difference between a dynamically-generated include and a non-dynamically-generated include? I type in my editor and hit save, then re-compile: it has to rebuild everything related to what I just saved. This is really no different. There's no way around it: if you change the file, you need to rebuild. And since your build system doesn't need to kick in for this one, you can do minimal recompilations of only what's affected, rather than having your build system do whatever Ad Hoc Insanity™ it settled on for what happens when you regenerate an asset.
I think "is there pretty much one settled API and or implementation?" is a good litmus test for whether it's worth standardizing. We've really only ever had sockets for networking, and they've worked fine forever. If you could specify a graphics API that was a few pages long, fast, and made most people happy, you'd have a successful proposal.
And you think that tooling won't be plagued with an invasion of the same idealistic people who ruined graphics...
tbh I was hoping for more technical description
We need a common, standardized HLIL to make distributing packages easier. If MSVC, GCC, and Clang can all put out the same HLIL that represents the code itself (but before target compilation/optimization), we can distribute rough binaries instead of target-specific binaries or raw code. Higher-level languages already have this. There's no fundamental reason it cannot be implemented for C++ other than an unwillingness to do it. Couple it with modules/interface definitions, and you now have a modularized packaging system.
&gt; Thanks to strong development team, they picked the features that made sense to bigger groups of people, or adjusted the proposed ones so that they would benefit more people and rejected everything else. When the group of people the committee represents is as broad as the C++ committee, this is just the golden mean fallacy. The middle-ground doesn't always make sense nor is it optimal or even good when you're deciding between proposals that benefit desktop users, mobile users, embedded users, and ones running industrial control systems from the 1970's.
Can we please make this not yet another thread discussion if graphics should or should not be part of the standard library? BTW.: You are aware, that you and Bjarne disagree on wether a graphics library should be part of the standard right?
&gt;While I agree that the first is the way to go, it suffers from some slight issues: &gt; &gt;for (int x : v) ++x; // increment each element of the container v i don't know what you mean here - but this code is just wrong (with the comment); or maybe that was your point? How easy it is to make a mistake like this?
&gt; for (int x : v) ++x; // increment each element of the container v A smart enough compiler should be able to warn about this being an operation without side-effects. If C++ would adopt const-default semantics (it never will, but I can dream) then it wouldn't be a problem. &gt; for (auto x : v ) result += x.compute_stuff(); I think that we can still effectively do without even requiring `auto`. Yes, it technically declares the variable, but Java and such have eliminate the need to declare variables in all contexts which makes the code more readable/terse. In a situation like this, the meaning should probably be obvious, and without `auto` or an explicit definition, should probably default to `(const T &amp;)` or `(const T &amp; __restrict__)`. Also, we need to add `__restrict__` to the standard, even though every compiler supports it in some form (MSVC has `__restrict`, GCC/Clang have `__restrict__`, and such). &gt; (because not everyone is able to implement iterators) If you can implement an incrementing for-loop, you can implement iterators in some form. Even if you are relegated to using `std::begin` and `std::end`. Or you will just end up with something like `for (int i : make_iterable(object)) ...`. However, you shouldn't be designing your APIs around something like `::get(int)` anyways, and past that, as said, if you have a `::get(int)`, it is trivial to *also* implement iterators within that class.
What would be the advantage of such a HLIL compared to raw source code? Please don't forget, that thanks to things like macros &amp; conditional compilation, varying sizes of fundamental types and the myrad of compiler flags, there isn't all that much you can precompile without knowing the target platform your code is going to run on and the flags the user wants to use
The security aspects were not the compiler vendor's main concern by any length (build systems already have huge scope to modify the system, the compiler being also able to do so is no big thing). They were far more concerned about race conditions, so if in a parallel build each compilation unit constexpr reads and writes a shared file, each affecting the other, that would produce lots of bug reports against the compiler which are not bugs in the compiler. P1031 does have byte range lock facilities, so concurrent users can coordinate, but I get their concern - this is a big brave new world we're considering here, with lots of unforeseen consequences and impacts. The whole idea of metaprogramming would gain a new level because you now effectively are adding constexpr multiprocessing with shared memory based IPC, with the ability for constexpr code to self generate new C++, or self rewrite, as it compiles. That's a huge sweeping change, as big as Metaclasses, maybe bigger. Precisely why some like the idea so much, of course.
C++'s problem is that there is no "development team" at all. Just lots and lots of individuals and companies that want to scratch an itch or tailor c++ to their particular needs. What we get in the end are a lot of compromises, and we know " A good compromise is achieved when both sides are unhappy with the result"
Faster to compile/include, and it would be agnostic of compiler extensions/whatnot. That is - regardless of the form of the language used by the compiler, *or even the language itself*, your compiler could accept the HLIL binary. Raw source code, as much as we like to tell ourselves is portable, is not about 99% of the time. If the HLIL is entirely standardized, it can be entirely portable. The HLIL wouldn't care about things like type sizes and such. It would be a representation of overall logic rather than low-level operations. The issue with macros does exist, yes. In that case, you need to make sure that macros *aren't expected to alter the behavior of distributed modules*, which seems sane to me. Should probably include a separate configuration mechanism for HLIL so when the compiler recompiles it, it can apply customization. *Technically*, you could embed macro references as well if you wanted to, as well as preprocessor ops. HLIL at this level is functionally a 'universal, partially-precompiled programming language', which can come from anywhere and be used anywhere. Thinking slightly more on macros, generally that is for low-level configuration of libraries. So, as said, you have three choices: 1. Embed macros/cpp concepts into the HLIL itself. 2. Provide an alternate configuration input for HLIL imports. 3. Allow the user to distribute multiple flavors of HLIL binaries (akin to multilib).
&gt; A smart enough compiler should be able to warn about this being an operation without side-effects. [Nope](https://godbolt.org/g/MX3dTV). Neither GCC 8.1, nor Clang 7.0.0 warns about it, even with -Wall and -Wextra. Haven't tested with MSVC.
Even without The return type of my function can depend on the size of a fundamental type, overload resolution can depend on it. The standard is full of Implementation defined behavior. The semantics of a program can (unfortunately) change depending on the standard version it is compiled with. I'm not saying there isn't room for a more efficient platform agnostic distribution format, but. I believe, the gains will be much smaller than you expect 
That's, um, an [interesting name](https://en.wiktionary.org/wiki/sod#Noun_2) for a library. Do they use [git](https://en.wiktionary.org/wiki/git#Etymology_1), I wonder?
IMO there are enough skilled people in the world where we can focus on multiple issues at once
Standardization is the solution to the problem of competing solutions most of the time, I feel like your litmus test is putting the cart before the horse? Although I completely agree with a KISS graphics API
Apparently not, the 2D Graphics TS failed and the Networking TS is most likely to not land until 2023. The focus needs to be on implementing a TS not creating proposals.
Reason is overrated, my gut tells me otherwise.
If there is "pretty much one settled API and or implementation", there is no point in standardizing it, is there? That API or implementation is a de facto standard. Traditionally, you standardize when there are several possible solutions, to select the one possible solution the industry can converge upon, ensuring interoperatibility and compatibility. From this perspective, graphics API is ripe for standardization.
I started learning C++ at my old university. One thing that I found frustrating is that not only were we not taught about the newer features of the language (C++11 on), but we also weren't allowed to use it at all. The result was that I'd find a lot of intriguing things I'd like to play with but couldn't justify spending time with due to assignment-related time restraints. Due to health issues, I'm starting at another school now. I decided only to transfer my non-CS credits as I'm quite rusty due to time away to deal with health issues. Now I have to learn Java. I'm looking forward to that even less.
I would be happy with the alternative simplified graphics proposal to be standardized. Give me *some* standard way to put pixels on the screen.
Like many things being made possible at compile time, the implications feel a mix of horrifying and incredibly exciting.
Maybe that should read "API-style"? Most people agree Cairo and Skia (and Direct2D...?) are similar APIs. The problem is Cairo/Skia style API has a large surface area, doesn't match modern hardware well, and doesn't meet enough use cases.
&gt; If there is "pretty much one settled API and or implementation", there is no point in standardizing it, is there? Sure there is -- to go from "pretty much" to "precisely so" -- that's why parties can converge on it: the loss in productivity moving to the standard is outweighed by the ability to rely on it once you're there. If the standard didn't even meet your use cases, then it would be impossible to adopt.
&gt; HLIL Does this mean high-level interface language? That's my best guess. There's no definition for it on Google.
Wall Wextra is nowhere near all warnings. Need to try with Weverything
yea, the point is that omiting a simple &amp; will make code invalid. the next example is even worse, as it will work, but with the cost of an extra copy.
Ok got you
Because the point of using `auto_ptr` is to support C++98/03, and alias templates aren't legal C++98/03.
Perl 6 is pretty dead though, and its hard to tell what exactly killed it so drawing any conclusions from what they did seems strange, to me.
My policy on pejorative words is to add them after people have started to downvote because RES told them I posted something. Maybe you think I am the problem, but from my point of view, this sub is mostly a safe space where people please themselves with consensual arguments (module: good, graphics: bad, cmake: good, range: good), and little to no in-depth disucssions, apart from the random insightful post of stl or some other highly technical minutiae. I prefer range loops over indexes, but there are real pitfalls in using them and one cannot avoid learning indexes too, which makes ranges them not as beginner-friendly as one may think (you have to learn them ON TOP of other stuff). On your argument of out-of-range: * I see the curse of the missing ‘&amp;’ all the time. I rarely see indexes error on trivial loops. Many non trivial loops require indexes anyway (or std::transform, std::accumulate, and plenty of lambdas — hardly beginner friendly). * Fixing out-of-bounds accesses would be a great beginner friendly C++ feature (optional, of course)
why is it dead though?
I was guessing high level intermediate language. 
windowHorizontalPixelCount /s
My policy on pejorative words is to add them after people have started to downvote because RES told them I posted something. Maybe you think I am the problem, but from my point of view, this sub is mostly a safe space where people please themselves with consensual arguments (module: good, graphics: bad, cmake: good, range: good), and little to no in-depth disucssions, apart from the random insightful post of stl or some other highly technical minutiae. I prefer range loops over indexes, but there are real pitfalls in using them and one cannot avoid learning indexes too, which makes ranges them not as beginner-friendly as one may think (you have to learn them ON TOP of other stuff). On your argument of out-of-range: * I see the curse of the missing ‘&amp;’ all the time. I rarely see indexes error on trivial loops. Many non trivial loops require indexes anyway (or std::transform, std::accumulate, and plenty of lambdas — hardly beginner friendly). * Fixing out-of-bounds accesses would be a great beginner friendly C++ feature (optional, of course)
Im not sure. A lot of it seems sound, its probably compatibility break that killed it and people moved to alternatives. It will be very hard for it to gain any traction now.
i'd still pick perl over anything else when it comes to extracting data from A and putting it into B, where A and B are virtually anything and you have 15 minutes to hack up a script.
Tried. Same result. ¯\\\_(ツ)\_/¯
It's `const west`.
Fixing out of bound accesses or invalid pointer access is not more trivial than fixing a missing &amp; in the declarator in a range for-loop.
They're just imaginary internet points, my dude.
Excuse my ignorance but what kind of special support does CUDA need?
Parsing and build tools. Old CLion doesn't support CUDA at all and it seems the new one doesn't support it as well. KDevelop has many other issues...
HLIL is not required. stable cross-platform ABI however is. But it's unlikely this will ever happen.
IIRC there is one more committee meeting before major feature freeze for c++20. I just don't see something fundamental like that going through standardization that fast (it might happen, but it seems very unlikely). So if we are lucky (coroutines where originally also believed to land in c++17, now we might not get them in c++20), we will get it for c++23. When will c++23 support be widespread enough so we can use it outside our private toy projects (in particular, in the embedded domain adaption of new standards is happening very slow)? In 5+ years, rust might or might not turn out to be the superior language and we might stop using c++ as much as possible.
They're referring to this where you meant adapt but wrote adopt. &gt;But you will probably have to adopt it 
Whenever I see east/west const discussions I imagine Alli G. https://youtu.be/oesOC7JvcwQ
The next logical step, of course, is for constexpr code to be able to load in shared libraries and call them during constexpr. Like libclang, so one can rewrite the AST of one's own source file. I had a lunch at Rapperswil where that was actively discussed, basically we'd simply constexprify P0275 and leave the userbase build out interesting applications of it, see what to standardise down the line. Let me put this another way: this is one route to "True Modules", rather than "Precompileds" which is the more appropriate name for Modules v1. And I particularly like it because it's library based, no need to fiddle with the language.
Thx, I have checked it again in another dictionary and it was more specific. Fixed it!
Have you tried [juCi++](https://gitlab.com/cppit/jucipp)? We added CUDA and OpenCL support a while back, but the IDE is not optimised for very large projects. Still, worth a try. If you run into problems, create an issue and let me know.
An HLIL can eliminate the need for the stable ABI, as it would be encoded at a higher level.
I've always wondered, if ADL was introduced to make things like `std::cout &lt;&lt; 42` work, why its scope wasn't restricted to `operator %whatever%()` only?
whats that have to do with whatever dart is?
If you want to know what Dart is you can google it; my point was C# is a better known language that also uses the same operator
Did ... did you just call a destructor manually? *For shame!*
Putting pixels on screen is an abstraction that doesn't allow for efficient usage of graphics hardware
&gt; build systems already have huge scope to modify the system, the compiler being also able to do so is no big thing There are a number of contexts where you might want to compile something outside of a trusted build system though, e.g. someone on the internet saying "why doesn't this code compile?" I don't want to say there's nothing to that argument and the following does seem a bit subjective, but at the same time it seems like just compiling code shouldn't be a security risk absent compiler bugs, just like looking at code in your editor shouldn't be able to do anything malicious. Though maybe [Matt Godbolt's experience](https://youtu.be/bSkpMdDe4g4?t=3192) says that you just shouldn't trust compilation to be safe anyway (though I'm not sure how much of that applies to the cases where this'd be important).
Really? What unit testing tells you that you forgot a ‘&amp;’ in: for (auto x : v ) result += x.compute_stuff(); creating tons of copies of x? Well, unit tests don’t show you that. I fixed hundred of those. And valgrind actually tells you about out of bound accesses, so you have your whole argument backward. And, yes, that range loop with extra copies was an example in my original post.
How would *you* destroy something instantiated with placement new?
Catch. That's a test framework, however.
Thanks! I'm not sure if it will offer us all the features we need, but we will give it a shot. :)
&gt; but catch-all arrays aren't a good programming solution What does this have to do with the article? He is talking about wrapping indexes into class, gain locality of reference, and using extra bits to solve dangling pointers problems and type mismatch. (And to nitpick, MacOS handles were not only to combat fragmentation. They could also be automatically purged, and/or backed by resource data)
Just use the eclipse variant that comes with the CUDA SDK?
&gt; The old clang modules are not modules, it's about generating a PCH for a single header and consuming that PCH, so parsing gets faster. T This is incorrect. Clang modules != PCH
Well, I guess I'm the only one who actually laughed and +1'd this, despite everyone apparently -1'ing it.
Eh, I'm using "file system" in a very abstract sense. A file system is just hierarchically organized buckets of bytes with a particular I/O API (that you can sort of ignore if you mmap the file and interact with it as if it were memory). An object store is not much different; it's just that it may not be hierarchically organized, and maybe it doesn't support things like random access or dynamic resizing...or maybe it does. My point stands, I think: you need some way to organize your buckets of bytes, and hierarchical directories in a file system are a well-known, well-supported way to do that, and don't impede on the data path once you've identified which bucket you want to read/write. Hence: I don't expect them to disappear, and there's actually no a priori reason they need to. One may not like the filesystem's API, and one might want to change it to organize things in a different way, but that's actually an orthogonal question.
I don't think everything would go through, but some parts could make it. We got contracts already, and this is a logical extension.
You get more locality by making the array more specific to a data structure, and less like a general free store. Maybe I missed something, but I didn’t notice coverage of this in the article. Trying to make use of extra bits, without an ironclad guarantee of how many are there, is a well-known pitfall. Yes, now I remember PurgeHandle. As for resources, they’re squarely in the OS-owned regime. (To be fair, purgeable handles were seldom used outside the Resource Manager, which amounted to a software virtual memory paging scheme.)
This is exactly the thing that could have made some embedded work I did \~7 years ago far less painful.
The same way I destroy something in a `union`, of course. Duh!
Phew, where to begin with this one. &gt; For instance - the various iteration algorithms. Not only does C++ now have like 4 or 5 ways to iterate over a container (a problem shared with Java), but it lacks truly terse syntax to use things like std::foreach sanely. `std::for_each` is not a great example, because it rarely gains you anything over a range-for loop. But something like `sort(vec)` will (finally) be possible when we get the Ranges TS merged in C++20. In the mean time you can use an implementation such as [Range-V3](https://github.com/ericniebler/range-v3) or (&lt;plug&gt;)[mine](https://github.com/tcbrindle/NanoRange)(&lt;/plug&gt;). &gt; The committee also seems to really like jamming things that should be implemented at the language level into the stdlib with templates This is a variation on "why doesn't C++ have a built-in `string` type like every other language?". The key to C++ since the earliest days is that it gives you generic tools to build these things yourself. We could easily have had a language-level `__unique` annotation to make unique pointers; instead (for better or worse) we got move semantics, which allowed `unique_ptr` and much more besides. &gt; Why are attributes wrapped in [[attr]] instead of the far more terse @attr? One reason might be because `@` isn't part of the basic C++ character set, as bizarre as that sounds. But I hardly think three characters is "far more terse", particularly when attributes aren't actually used all that often. &gt; Why can we not define custom annotations like in Java? Why doesn't C++ add a preprocessor extension so we can namespace-scope macros? Why can't I get the type or name of the current class generically at the non-member scope? You want reflection. I want reflection. Everyone wants reflection. People are working on it. &gt; Why is RTTI still insanely inefficient performance-wise, almost always relying on string operations, And so on. C++'s RTTI is far too little for some people (who want full runtime reflection) and far too much for other people (who never use `dynamic_cast` or exceptions). I'm not sure of your assertion that it uses string operations: I think this may only be on the Windows platform. &gt; We don't even have networking, proper multithreading capabilities (though it has gotten better) The Networking TS is currently held up by the Executors stuff, which will also improve the multithreading story &gt; proper portability for packages and builds You mean modules? &gt; placement new and calling the destructor don't have symmetrical equivalents Placement new is not a beginner-level facility. If you know what placement new does, you can cope with psuedo-destructor calls. &gt; the C Embedded Specification still hasn't been brought into C++ making Harvard MCU development with C++ painful I don't know what the Harvard MCU spec is, but I don't see what C Embedded has to do with C++. &gt; I can't memory map a file or make a true ring buffer without OS APIs Memory mapped files would be nice. Why can't you make a ring buffer? &gt; we don't have standardized signal handling We have what C has, which people have coped with for 40 years, as horrible as the interface is. &gt; exceptions are still implemented stupidly, with no sane equivalent to Java's 'throws' Are you (re-) proposing checked exceptions?! I... I just... &gt; move constructors not being noexcept struct S { const std::vector&lt;int&gt; vec; }; cannot have an automatically-generated `noexcept` move constructor, alas. &gt; we don't have sane string handling `std::string` is far too big as it is. I'm hopeful for [Boost.Text](https://github.com/tzlaine/text) personally. &gt; the ability to determine if an argument is constexpr Literal types as non-type template arguments are coming in '20. This will provide most of what people want when they talk about `constexpr` function arguments. 
&gt; I'd like C++ to focus on lessening its increasing verbosity. "Modern" C++ is starting to look frighteningly arcane, verbose, and it's gradually becoming a write-only language. ... In another thread, people were half-joking about forking C++... but that's slowly becoming a reasonable course of action. I'm relatively disillusioned with committee-driven design, and dislike the fallacy of the mean that it encourages. "Within C++, there is a much smaller and cleaner language struggling to get out... and it's called Rust."
Article should be subtitled *Or, The Modern Prometheus.*
Because it was also made to make things like `swap(x, y)`, `begin(r)`, and `end(r)` work.
&gt; This is incorrect. Clang modules != PCH Thanks for clarifying. I thought it was the case since clang BMI were pch files. Maybe I'm wrong. &gt; Clang modules does not have an import directive Maybe I'm messing up with what they proposed and the original clang modules. In the early google proposal, there where proposing an `import` statement that would either work backward in time (a C++ statement changing the preprocessor state) or being itself a preprocessor directive instead (what it seem clang implemented in their module-ts implementation). &gt; Clang modules support more features. If it doesn't allow me to put *all* my code into module interface without slow down, allow to compile different modules as different language version or allow me to compile a module in an entirely different language (I don't see why a special C compiler wouldn't be able to output a BMI), then I guess Clang modules has some feature difference (does clang module inter-operate with Obj-C modules?) If yes, then clang indeed support macros, with all their drawback (not having language level module) &gt; Have you ever used clang modules? They are basically Objective C++ modules backported to C++. All iOS software pre Swift uses them. I knew there were related to Obj-C modules, but I didn't used them for long. I did have to support an iOS app for a small amount of time, and the "header not found" errors were quite confusing to me at that time since I thought they were more traditional module, not a mapping over headers. I think clang modules are quite nice as a compiler extension, and indeed can be useful today without being intrusive in the code. However, this is not a match against true *language* level modules (not preprocessor level). Having clear semantics that are implementable by every compilers and feel well integrated to the language is something C++ must have to stay relevant.
&gt; and even `using std::size;` are traps. This is completely missing the point of ADL – that's exactly how it's _meant_ to be used, just like `using std::swap;` or `using std::begin;`...
Didn't know about the British meaning but SOD is the acronym of Symisc Object Detection.
`std::launder`? :)
It won't, since it'd be hardly any different from just shipping sources.
There was a proposal from STL to remove the need for auto &amp;&amp; in ranges for lips, but the committee didn't like it.
Does the proposal even target c++20 at all (I vaguely remember that someone said the proposal want discussed in full, because the committee wanted to focus on c++20 proposals. Anyway I'll just wait and see, but I've stopped some time ago expecting that a certain feature makes it into the standard before it has actually been merged.
It looks like he tried to define a destructor but forgot the definition. Because pedantry is fun. 
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8s4702/why_cant_i_access_superclass_variable/e0wexq4/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
The issue is that unqualified name lookup doesn't reach into dependent base classes. In plainer terms, when you need to access a base class's members (data members or member functions) and that base class is templated, you need to say `this-&gt;`.
I agree, but to loosely quote John Kalb: Consistency is why we can't have nice things in c++ :(.
What in the world?
&gt; And network also shouldn't be in either. And neither should threading, or strings. Disagreeing with the graphics proposals is hardly the same as wanting to roll back std::string. Stuff like std::string and std::thread are extremely useful in writing graphics apps in the real world, while the std graphics proposal wouldn't be. String and Thread are also useful for many non-graphics applications, while the graphics proposal couldn't possibly be. See how there's some differences there? And that rejecting one proposal doesn't put us on some inexplicable slippery slope to removing half the language? 
What *does* the committee like?
TMP, verbosity, backwards compatibility, bike-shedding, library- instead of language features ;) Admittedly, the committee is a very diverse group with very diverse opinions on most matters but the above has been my overall impression so far.
Smart people have bad ideas all the time.
They're failing because we need neither in the language.
Can you implement a linked list in Rust yet?
Networking was planned for C++14. Then C++17. Then C++21. Probably never gonna come.
It's not dead but it's not getting any traction to speak of it usage-wise.
What is C++21? There is no C++21.
Like OpenGL for C++ Standard?
Drawing pixels is useless in any sort of real life case except a "Hello World" or maybe a super inefficient roguelike.
Well considering how much everyone hates exceptions in some way, it's really easy to get traction for a proposal like this. Deterministic exceptions is something people have been wanting forever.
&gt; Let's go back to the time where there was all those marvelous competing ways of doing string manipulation Do you mean now? String manipulation is still a mess.
Perl was doing already, Python (for all its faults) has been superior for a while.
Those are a lot of work. It shouldn't take a paper to defend the status quo. Why don't you propose a new noexcept STL that uses a new, noexcept new? 
What could re-doing the type system look like, and how could you do it without breaking everyone's code? Perhaps things like 'unsigned char' vs 'signed char' vs 'char', but, who's suffering under that? Anyone know what Bjarne means? 
Yes
I have no need for such a thing is why.
seems like wherever you go, you can't escape node and electron.
Unit test will fail and will tell you where it failed. No need to fire up valgrind in the first place.
That was not your initial point. You talked about modifying the elements of v. Now you're talking about expensive copies, which is a different point. I'm not buying inefficiency as a bug in the logic of the program.
This is a dangerously offhand dismissal of Rust based on a silly meme. It's still a young language and you may not personally like it, but there's good design at it's core and it's worthwhile to learn from it, especially since its target audience is similar to C++'s. C++ obviously shouldn't _become_ Rust and it's not going to be replaced by it anytime soon, but it has to learn from modern language design if it wants to improve.
Is `std::size` actually intended to be used with ADL? Sure, `std::swap` and ADL are idiomatic, but few other things are. I think code that attempts to call `std::begin` or `std::size` on non `std::` types through ADL is walking down the wrong path. Those APIs have clear behaviors that non-`std` types can hook into (e.g. having reasonable `begin()` or `size()` methods. ADL is certainly a confusing mess. Best to minimize its use to a small vocabulary such as operators, `std::size`, and other APIs clearly intended to be used through ADL. I don't think generic code should feel it has any license to call whatever function it wants through ADL and call it good if it happens to work.
I don't think the point is the inefficiency. If you make work on a copy, but think you are modifying the object, that is more problematic. Right ? Not sure if it is what was meant, but it is true that you can be surprised by the syntax. Not that hard to figure out, but a potential bug.
I assume he means, at minimum, type safety and true strong typing. So no implicit conversions, no type punning, etc. For example: // Unions are a candidate for type punning - banned! union U { int x; float y; }; // No implicit conversions int x = 4; short y = x; // Oh no, implicit narrowing! short z = static_cast&lt;short&gt;(x); // This is fine, explicit cast char c = 1; // Not allowed! char is for characters, not numbers. Maybe have a different byte type for 8-bit integers. Parent *p = new Child(); // Maybe ban implicit pointer downcasting? If we're optimistic, we can go a bit further than just type safety and hope for proper algebraic type support with proposals like `lvariant`. Maybe we'll even get language-level tuples some day ¯\\\_(ツ)_/¯. As for how to do this without breaking backwards compatibility, the best option I can think of is to start by requiring warnings for this "banned" behavior and then maybe deprecate it some day in the distant future. 
Given the 2 minus points, I have to conclude I probably expressed myself badly. I meant to say that the 2D library being discussed for inclusion in the standard goes hand-in-hand with this proposal. This proposal should cover that bit (some of what I f.e. currently find in SFML) as well. I did not intend to say: "We don't need this, because we already have SFML".
No; you are being incredibly obtuse. Your argument revolves, again and again, around rewriting software that already works fine, and that has seen massive investment in the past. How can I make you understand that that is just not an acceptable solution? If we are going to rewrite, we might as well do it in another language. It would be a good time to evaluate what is on the market, and see if there are languages out there that are maybe more pleasant to work with than C++. At least the great argument for C++, the guarantee that the language won't change out from under you, will be gone - so why stick its lousy syntax and its masses of UB? So what if "some systems" don't throw bad_alloc? We sell _applications_, and our customers want those to run correctly _on the platforms we support_. We are, in short, _your_ customer: people actually using the language to actually solve real problems in the actual real world out there. Do you want us to remain on board or would you prefer that C++ becomes a language only used by academics for the purpose of writing libraries? Might as well scratch `main()` from the language then, nobody will be needing it anymore... 
Attempting to engage here was a mistake. My apologies.
[Actual link](https://www.theregister.co.uk/2018/06/18/bjarne_stroustrup_c_plus_plus)
Thanks, I messed it up. I'm deleting it!
A package manager (even with modules) will only solve 10% of the problem. The bigger problem is that there's no true base class library, coming from a centralized body with actual useful standards, and containing useful abstractions and a good set of interface guidelines and examples. e.g. Great you just "pip" installed 3 different libs, but they each take a random assortment of raw, smart, and other pointers in their interfaces. One package depends on jsonlibx++ but too bad the other 2 don't -- they each want their own json package. And that one interface returning a wstring, well now you need to convert to string if you want to use that result with the other 2. One uses boost exceptions to attach a helpful stacktrace object to them so exceptions are actually useful but the other 2 don't -- 1 of the them just throws ints as an error code mechanism and the other just flat out std::aborts... yay package manager tho! The STL model doesn't really cut it. Just check any q/a site (heck even here sometimes) where seemingly simple questions get asked and you'll get umpteen different ways to skin it using the STL (which while nice, is not a very pleasurable environment to work in day in day out)
I think std is a good base class library. std::algorithm has treated me well. Yeah there's no std::json, and perhaps there should be, so I'll give you that. For me, modules are basically the single missing feature in c++. You claim that it's 10% of your problems with c++. You've outlined the lack of std::json as a problem for you, can you provide a second clear example of a problem and also propose a solution?
Thanks! I'd take language tuples, for sure. 
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8s68vr/travis_for_c_help/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Sorry for the downvotes you're getting. I've read the Effective C++ books and I feel like "Proper" C++ leads you to rust anyway. The way you're supposed to put everything in a unique_ptr and carefully manage your shared_ptrs is like rust's memory management, and they way everything should be "const correct" is on by default with rust's use of "mut". I get this is a C++ sub but geez
What do you mean we don't have sane string handling? What do you mean we don't have proper multithreading support? That's quite a statement, I don't think you can drop that without elaborating, or rather you should know that your not helpful when you drop an abstract, meaningless statement like that. Let's be precise here. Let's be verbose. Let's focus. Built in "auto parallelization without the use of arcane syntax" is a subjectively qualified statement that doesn't hold up in court to me. I can point at your solution and call it arcane, as well. Also a huge ton of the verbosity is eliminated by use of "auto". I don't find c++ exceptionally verbose. I find the lambda syntax to be very good. "We don't even have networking" I'm really confused by this. C++ was the language I used in my networking class and I currently am working on a networked system in c++. In regards to "I can't do this without OS Apis", that's not a very good argument to bringing something into the language spec. You have to prove that it's fundamental enough to warrant it's existence in the spec of the language. Or else your just bloating the thing up. "Standardized signal handling" I don't think we should have that, that seems like a clear "user" problem that should be solved by the user. If we go down this path your going to want dependency injection next....
I'm always trying to open makefiles in Photoshop and can never understand why it won't build my projects.
&gt; Those APIs have clear behaviors that non-`std` types can hook into (e.g. having reasonable `begin()` or `size()` methods. I don't know the proposal that introduced the non-member functions offhand and don't want to go searching for it, so I don't know for sure, but I'm pretty sure non-intrusive support for such types was a key design point. (Why force forking/modifying a 3rd party / legacy library to make it play nice..?) It's worth noting that the Ranges TS (so presumably C++20) treats `size`, `data`, and `empty` as "customization points", along with things like `swap`, `iter_move`, `iter_swap`, `begin`, `end`, etc., and that said customization points are ADL-driven.
Look into heaptrack
Of course that was my initial point. Read again (or, more accurately, read it): &gt; or the more subtle version of: &gt; for (auto x : v ) result += x.compute_stuff(); &gt; that lurks in so many codebases. Wtf did you think I was talking about? My first example was about modifying, my second about copies, my third about the need to switch to indexing when iterating over pairs of vectors and my fourth about the need to implement you own iterators if you want to keep that scheme consistent in your codebase. That is not even difficult concepts to grasp. &gt; I'm not buying inefficiency as a bug in the logic of a program. Lol.
That point was inefficiency. I’ve seen many many instances where one is iterating over a collection of containers, casually making copies of the contained containers and their content. In quite a few cases the one doing this iteration was me :-( My original super-downvoted-sub-hurting-feelings-cause-it-is-not-100%-aligned-with-bjarne comment is just about the fact that range iteration is not as beginner friendly as it looks.
&gt; In regards to "I can't do this without OS Apis", that's not a very good argument to bringing something into the language spec. You have to prove that it's fundamental enough to warrant it's existence in the spec of the language. Or else your just bloating the thing up. Not your parent, but I personally think in terms of importance, networking is above file system stuff. (Not `fstream`, but `std::filesystem`)
it's very practical at what it is designed to do - Practical Extraction (and) Reporting Language. not that much of it is user-facing, though.
Conan seems way better to me. It can be used with any build system, including CMake, which is nice. 
Do you really want a single vendor to control the whole library echo system? And `std:: thread` is actually a horrible abstraction and shouldn't be used by 95% of the people at all.
The best technical solution doesn't help you, if no one uses it. Turned around: c++ is really not a good technical solution but used quite a lot.
The use case I had in mind was more graphs and grids. I have very often felt the need to show graphs as output, and grids of data in some cases. The alternate proposal would solve this need quite nicely. It needs blitting support, I forget if the alternative proposal includes that. 
I agree, but the use cases I had in mind for a cross platform graphics API doesn't need very efficient use of graphics hardware. It was more to display graphs and grids, maybe a button or two. The standard library facilities are written to cover common use cases, not specialized ones. I wouldn't use std::string if I was writing a text editor, nor will I use std::unordered_map if I needed some serious use of hash tables. Lot of software uses jemalloc instead of the standard library facilities. Similarly, I wouldn't use the standard graphics API to write a game engine, there are more specialized tools for that.
East-const is forbidden by the CppCoreGuidelines.
&gt; What do you mean we don't have sane string handling? C++ has no real concept of UTF8. &gt; What do you mean we don't have proper multithreading support? That's quite a statement, I don't think you can drop that without elaborating, or rather you should know that your not helpful when you drop an abstract, meaningless statement like that. Let's be precise here. Let's be verbose. Let's focus. Limited multithreading constructs, fairly limited asynchronous constructs. You cannot promote a shared lock to a unique lock. Threading overall still feels 'latched on' to the language rather than being a core part of it. &gt; Built in "auto parallelization without the use of arcane syntax" is a subjectively qualified statement that doesn't hold up in court to me. I can point at your solution and call it arcane, as well. I'm not sure how auto-derivation of parallelizable constructs is arcane. There's no reason to require `std::foreach` or some other algorithm to be used in lieu of a range-for in order to determine if it's parallelizable. &gt; Also a huge ton of the verbosity is eliminated by use of "auto". And a huge amount of verbosity is *added* by constantly jamming everything into the `std` namespace with templates. &gt; I'm really confused by this. C++ was the language I used in my networking class and I currently am working on a networked system in c++. I can use OpenGL with C++ as well. OpenGL is not part of C++. There is no standardized networking API for C++. &gt; Isn't that just an array where you access it using a cursor and a modulo? A true ring buffer is two logical memory segments directly adjacent that point to the same physical memory. A 4096B ring buffer, in that case, writing to buffer+4097 writes to buffer+1. You can implement them using OS constructs, but there's no standardized mechanism for it. &gt; I don't think we should have that, that seems like a clear "user" problem that should be solved by the user. If we go down this path your going to want dependency injection next.... Not sure how those are related. I have some algorithms which require signal interception in order to detect invalid writes to memory in order to trigger behavior, like selective decompression of data. Right now, the code for that is completely different on Windows (with VEH) and Linux (with rudimentary signals).
Almost your whole reply read as either "that is the way c++ works" or "this might get fixed some time in the future". That doesn't mean that the current state of c++ sucks. And c++ isn't the only language that allows you to write a string as a library type, but it is one of the few where the compiler front end has no Idea what a standard string is and instead let's it rediscover and re-analyse it's definition over and over again in each translation unit. Allocation elision and optimization in general would be so much easier, if the compiler knew that the value of a string is a sequence of chars and not three random pointers to memory. Similar things can be said about other fundamental vocabulary types like std::variant, std::optional, tuple etc. There are even people/companies actively avoiding any standard library type that has a direct or indirect dependency in `std::string` because it's header is too heavy weight. Admittedly, that is only partially the languages fault
How about "static_foo" or "stack_foo"?
You'd have to ask him, but I got the impression he wants a teaching API. Like LOGO for C++. I'd support that happily.
&gt; In regards to "I can't do this without OS Apis", that's not a very good argument to bringing something into the language spec. Actually, that should be the main reason to standardize something. The purpose of any standard is that you can create one solution that works on/with all platforms instead of having to use a separate, proprietary interface for each new system you want to run on/connect to. That is why we standardize internet protocols, that is why we standardize things like USB or pcie and that is why there are standardized screw sizes.
Shooting down hopes and dreams.
`switch (auto ch = getnext(); ch) {` `// case statements as needed` `}` You don't need C++17 for this. This was possible since C++98 and it's also shorter: `switch (auto ch = getnext()) {` `// case statements as needed` `}`
Simple simpler simple simpleton McSimple™
&gt; As long as care is taken to keep C++ sane and coherent, It could be argued that that isn't the case anyway
&gt; sort(vec) That's not an iteration algorithm. Also still unsure why it requires the ranges TS to implement. &gt; This is a variation on "why doesn't C++ have a built-in string type like every other language?". The key to C++ since the earliest days is that it gives you generic tools to build these things yourself. All right, then give me a utf8string class that handles it for me so not everyone has to embed a different utf8 library. There are common use cases which *should* be handled. &gt; One reason might be because @ isn't part of the basic C++ character set, as bizarre as that sounds. But I hardly think three characters is "far more terse", particularly when attributes aren't actually used all that often. [[N]] looks out of place, makes code look more verbose, and is distracting. We'd use attributes *more often* if they were more obvious to use and we actually had useful attributes given to us. Especially if we had custom annotations. &gt; You want reflection. I want reflection. Everyone wants reflection. People are working on it. I am doubting their ability to do it, since they've been 'working on it' for a very long time. Meanwhile, implementing something as simple as 'this_class' should be incredibly trivial. &gt; The Networking TS is currently held up by the Executors stuff, which will also improve the multithreading story IMO, features are constantly 'held up' by other features because they'd prefer to use those features, so C++ always seems to have chains of features pending other features. Maybe that's indicative of a problem of procedure? &gt; You mean modules? Modules are certainly one aspect of package management. &gt; Placement new is not a beginner-level facility. If you know what placement new does, you can cope with psuedo-destructor calls. I wasn't aware that we were designing languages solely to be friendly to beginners. And yes, I can cope with a lot of things. That doesn't mean we should have to. It *would not be hard* to add standard `placement_new` and `placement_delete` operators. But I suppose that it doesn't have that sexy veneer of a new feature that the committee loves? &gt; I don't know what the Harvard MCU spec is, but I don't see what C Embedded has to do with C++. If you aren't familiar with what an MCU is, or what a Harvard architecture is, then you probably shouldn't be discussing ISO/IEC TR 18037 nor how it would be relevant to C++ and using it for development on Harvard-architecture MCUs. &gt; Memory mapped files would be nice. Indeed. And specifying them would be trivial. The language is missing a lot of trivial things. &gt; Why can't you make a ring buffer? You try remapping a logical area of memory after itself without the assistance of OS APIs. You can make a purely-logical ring buffer, but for proper performance benefits, you need to be able to remap the pages after themselves so you have a contiguous address space. &gt; Are you (re-) proposing checked exceptions?! I... I just... No? I want functions to be able to annotate what exceptions they will return so we can do compile-time validation of exception handling. Surprisingly, other languages like Java actually do this sanely. This may shock you, but C++ exception specifications aren't the only way to do things, nor are they good in any fashion. &gt; cannot have an automatically-generated noexcept move constructor, alas. IIRC, that code cannot have an automatically-generated move constructor of any kind. As it is a `const` member, the move constructor will be automatically deleted, and move-constructor overload resolution will use the copy-constructor instead. Which isn't a good example. &gt; Literal types as non-type template arguments are coming in '20. This will provide most of what people want when they talk about constexpr function arguments. I have other reasons for wanting actual `constexpr` function arguments. Like the ability to optimize functions within themselves based upon the known values of arguments. Or specifying specific behavior if an [N] array passed down is `constexpr` or not.
Did you try WeverythingAndThisTimeIReallyMeanIt? 
&gt;I had a very bad experience with Lugano interviews. I hope things have changed since the last time I interviewed, they didn't even know the position I had applied for (onsite).
[Modules](https://i.imgur.com/lgMDYrN.jpg).
As I've been saying for many years now, the syntax [x, y, z] should be made to represent the literal form of a tuple. 
 for (const auto&amp;[key, value] : mymap) { Are key/value's references? Or does the &amp; on the left mean both parts of the pair are also references?
 for (auto[iss, name] = pair(istringstream(head), string {}); getline(iss, name); ) { // Process name } Sorry, but this is pure insanity. "Template Argument Deduction" has existed since forever. The "class templates" part of the name is important.
In non-generic code one can always call swap by the fully qualified name and in generic it could have been achieved in another way, e.g. specialisation. However, [time was running short, a solution was available, and no one else had any ideas for a better solution](http://www.drdobbs.com/cpp/a-personal-note-about-argument-dependent/232901443).
key and value ar both const&amp; in this case (pretty much like with normal auto). Be aware if you don't specify the reference qualifier, decltype rules apply for structured bindings (so everything keeps it's qualifiers, they are not decayed).
&gt; You don't need C++17 for this. This was possible since C++98 and it's also shorter: Yes, this is a bad example. A better one would be: if(auto it = some_map.find(value); it != some_map.end()) { }
Am I the only one thinking that this: auto [itelem, success] = mymap.insert(std::pair(’a’, 100)); If (!success) { // Insert failure } is more readable than this: if (auto [itelem, success] = mymap.insert(std::pair(‘a’, 100)); success) { // Insert success } I mean you gain nothing besides one line less line of code, right?
I don't understand, does this guy get paid to write posts with inaccuracies in them?
Yeah agreed it's the **class** template argument deduction guidelines that make it work. The article makes it sound like it now works by default for all templated classes. Which is only true for the stl types like std::pair because the class template argument deduction guidelines have been added there ... I also think it's even easier to write (now that we have CTAD): //auto [itelem, success] = mymap.insert(std::pair(’a’, 100)); auto [itelem, success] = mymap.emplace(’a’, 100);
you gain the scope of iteLem/success being limited to the body of the if statement.
C++21 happens when a bunch of us get together as a shadow committee, and put out a new spec on our own.
[Modules](https://i.imgur.com/lgMDYrN.jpg).
I know this is an exploratory project, bit personally I'm questioning the sanity of spupporting all standards from 98 till 17/20. At some point, a new library version that only supports c++11 upwards seems the more reasonable choice. If people still need to compile with c++98, they can use the old version and I'd much rather Backport Bugfixes than adding a whole lot of conditional compilation into my code (or try to understand/trust other people's code that have all of that complexity.
Sure, but does it worth the readability? 
I agree. Honestly I still don't understand why the committee thought packing everything in if statement is a win. Sure you create a scope for the variable but readability suffers:/ Most controversial addition to c++17 imho
I think his arguments for naming are good, "fixed\_capacity\_foo" is very descriptive and to the point for containers. "*This one in particular: In particular, "static\_foo f" looks a heck of a lot like "static foo f", and I can imagine people getting very confused about the meaning of "thread\_local static\_foo f" "*
I think that's true for other offices as well.
I would say it definitely depends on the case. If the line you get is too long, you should definitely break this into two statements for readability.
 #define let const auto
You are making assumptions that (1) the committee thinks that "packing \*\*everything\*\* in if statement" is a good thing and (2) that readability \*\*\[always\]\*\* suffers. Neither of these is true. You should stray away from drifting into extremes because you will always be wrong.
The trick with turning things into a pair is a bad trick; I think we'd all rather have `for (auto x = 2, y = 'z'; ...` and have type deduction work like it's meant to. Also the whole business of `if (foo(); bar)` is ugly and unidiomatic; I can see the intention in restricting calls to a particular scope, but this is a really clumsy way of doing it.
Fantastic!
or format your code differently.
The delay is mainly due to Executors, as it is felt by a majority (currently) that the subset of Networking not requiring Executors cannot be shipped immediately. I personally think they should revisit that decision, ship the thing in two tranches same as Modules, Concepts, Ranges etc, but it's not my proposal.
Yes I agree, this is too much. I'm okay and I do some initializations in map but only if the code stay clean like this: ```cpp if (auto it = map.find("foo"); it != map.end()) it-&gt;second(); ```
&gt; &gt; Memory mapped files would be nice. &gt; Indeed. And specifying them would be trivial. The language is missing a lot of trivial things. This is incorrect. Memory mapped files are actually extremely hard to incorporate into the C++ standard. Several very influential members of WG21 have the "over my dead body" opinion regarding them ever being supported at all in the C++ language. Now, I hope to persuade them otherwise eventually, but some of the things we need to address in the language before we can enable memory maps would be: - Object lifetimes exceeding process lifetimes - Objects having more than one address in memory - Visibility of modifications having multiple tiers of propagation - Whether we standardise on vptrs, and permit them to be blessed and cursed. And none of the above touches *shared* memory maps, which are a whole additional bundle of fun because now we need to incorporate the idea of there being more than one C++ program running at once into the standard, which is currently undefined behaviour. I appreciate the frustration here. "Just get on with it" is what other languages such as Rust and Swift do. But they take on technical debt when they do this, forcing a sooner rather than later requirement to fork out the technical debt, as Swift has done several times now. C++ has not forked since the early 1990s. My current contract has me working on two million lines of 1994 era C++ codebase. Still compiles and works in C++ 17. That's very valuable to the big multinationals who mainly support WG21 work.
Didn't know C++98 had Auto type deduction ;)
&gt; Instead we are talking about a graphics proposal. We don't even have networking, proper multithreading capabilities (though it has gotten better), proper portability for packages and builds, placement new and calling the destructor don't have symmetrical equivalents, allocators lack realloc equivalents, we also have no try_realloc, the C Embedded Specification still hasn't been brought into C++ making Harvard MCU development with C++ painful, I can't memory map a file or make a true ring buffer without OS APIs, we don't have standardized signal handling, exceptions are still implemented stupidly, with no sane equivalent to Java's 'throws', and with move constructors not being noexcept... hell, we don't have sane string handling or the ability to determine if an argument is &gt; constexpr &gt; ... but we're worried about graphics. I feel I need to point out something which nobody else has pointed out yet, and that is that every single one of your items bar the standardised signal handling has a paper already before WG21 on track for either C++ 20 or C++ 23. Graphics merely made a whole lot of noise so it sticks out, but all the other proposals have been comparatively well received and given time, will probably make it. Now, as to the standardised signal handling, I have a paper in the works. I only just finished the reference implementation a few days ago, you can see its documentation at https://ned14.github.io/quickcpplib/namespacequickcpplib_1_1__xxx_1_1signal__guard.html. I'm deploying it right now into the reference implementation for P1031 *Low level file i/o*, and indeed you will see that the documentation for `map_handle::write()` now indicates that signals raised during the write will be trapped and an error code comparing equal to `errc::no_space_on_device` will be returned (https://ned14.github.io/afio/classafio__v2__xxx_1_1map__handle.html#a00b7e1e99089c2641c45ee824811ebc1). This is not a complete standardised signal handling implementation for C++, it's just the lowest common denominator between POSIX and Windows. Indeed I can already foresee that WG21 will want more meat on the bones, that what I'll be proposing is too meagre. But that's okay. So tl;dr; we're on to everything you raised. It takes the time it takes. Donations of community time in helping us to make things happen faster are always warmly received.
Does iostreams and stdio allow for efficient usage of filesystems?
I would rather not have to depend on Python to use it. I love that my Java and .NET building tools don't require tools written in other languages.
I was very lucky to have gotten access to the manuals of Turbo C 2.0 and Turbo C++ 1.0 almost at the same time, followed by acquiring C++ARM book. So I ended up learning C++ as Bjarne and Kate Gregory advise to. Of course, after going through the ritual of writing my own vector and string classes. 
;)
There are languages with modules which also support macros and conditional compilation.
I occasionally do encounter Java and .NET codebases of similar vintage, yet with all their slowness they still advance faster. My pet peeves were type inference, AOT compilation and better support for value types and GC free algorithms. Apparently they might get their improvements faster than modules or executors will arrive.
I do not see the point of having if-initializers and aggregate-class initializer in C++17, they are nice to have features which complicate the language with their caveats.
huh! so that means this works: if (auto [thing1, thing2] = std::tuple(get_optional_1(), get_optional_2()); thing1 &amp;&amp; thing2) { std::cout &lt;&lt; *thing1 &lt;&lt; *thing2 &lt;&lt; '\n'; bit on the verbose side, but at least it works!
I just saw the new post about destructuring, apparently if (auto [thing1, thing2] = std::tuple(get_optional_1(), get_optional_2()); thing1 &amp;&amp; thing2) { std::cout &lt;&lt; *thing1 &lt;&lt; *thing2 &lt;&lt; '\n'; this works!
Qt Creator's Clang code model takes the CUDA option into account (\`-x cuda\` and \`--cuda-path=...\`) if it's defined in the project config (e.g. for qmake: QMAKE\_CXXFLAGS += -xcuda).
Thing is, there are serious issues with both Modules as proposed and Executors as proposed. I myself voted neutral on the former at Rapperswil, and weakly against on the latter. In my opinion, both suffer from hideous complexity, and neither have sufficient or any (respectively) user experience to back them being standardised in their hideously complex form. In particular, I take real issue with standardising big complex things which are not standard practice somewhere for many years. Standards innovation ought to be avoided much more than we currently do. I think standards innovation can be acceptable for small items, or simple items, but big "brave new world" items? Sorry, no.
`std::inplace_foo`is just about the worst possible name for a type that doesn't allocate, because most of the `std::inplace_bar` *algorithms* are the ones which *do* allocate! Consider: std::any a = 3; // potentially allocates std::merge(...); // doesn't allocate std::inplace_any a = 3; // doesn't allocate std::inplace_merge(...); // potentially allocates 
What platforms are supported?
Go has something similar. I've gotten more used to it over time. I just tend to glance at the end if the if condition before reading it normally now. Though not having parentheses helps a tiny bit for readability too.
Pretty sure it's targeted at graphing applications. Rudementry graphics support is pretty alright for that since you're generally not redrawing them in realtime. 
&gt; key and value ar both const&amp; in this case Structured bindings function like aliases, but technically they are not references, and it's possible to tell the difference with `decltype`
Finally, that part twisted my mind. 
Interesting. Could you name one so I can look up their semantics? Just to be clear: I'm not saying modules are not helpful (I really hope we get them in c++20) I just don't know if they are useful as platform agnostic distribution format.
What do you gain with such a change? My guess...nothing.
{ // let the compiler do the job auto \[itelem, success\] = mymap.insert(std::pair(’a’, 100)); if (!success) { // Insert failure } }
People keep screaming "we need a package manager" and ignore we already have several. Modulo bugs and some usability improvements this is as good as it is gonna get. People (authors as well as consumers) just have to use them.
I decided against it b/c it seemed redundant. My CMakeLists.txt is already describing my dependency tree with it's targets and interfaces. Hunter simply complements it. Why would I want to re-specify that in a Conan file and then maintain that? I don't really want to deal with any build system outside of CMake itself.. CMake is already a handful. Also the ability of having my toolchain file just forwarding immediately and seamlessly to all my dependencies so everything is compiled with the same setting is just fantastic. Crosscompiling and messing with compiler flags is now super easy
Could you explain why are you not using c++ &lt;regex&gt; ?
I understand that position, but there many like myself that have long left as C++ daily driver, we only keep using it because our daily tools still need some C++ help every now and then. However 5 years is a very long time for the tooling to improve, thus allowing for some catch up to happen. The performance additions in C# 7.x or the new HPC# compiler in Unity to incrementally replace C++ on their code base are examples of such improvements. I am not optimistic that by lets say 2030, there will be many new greenfield projects taking advantage of C++.
What were the innaccuracies in the post?
I thought the same thing at first, but I realized it could be useful in some cases. switch (auto ch = getnext(); ch) { case 'a': case 'b': case 'c': string += ch; break; } 
Hi! I have installed the juCi++ and it looks nice. However, I have a couple of things to mention: * CUDA device code was not recognized * the code coloring isn't as nice as in KDevelop * when I hover with a mouse over an object, it would be nice to get a context, instead of the type only * some namespaces couldn't be resolved Here is an example: [https://ibb.co/kekHKd](https://ibb.co/kekHKd) Everything else seem nice and fine. :)
Being \_fluent\_ in a language doesn't mean you're not saying gibberish :)
&gt; which is a much nicer syntax and is also consistent with modern C++ style using auto almost whenever possible. I am not sure that the move to have auto everywhere is a really great idea. I think it can sometimes hurt readability.
Some examples that come to my mind. Turbo Pascal supported conditional compilation since version 4.0 for MS-DOS. .NET does support conditional compilation. Dylan did support macros and conditional compilation. OCaml has modules and a few conditional compilation libraries via ppx (extension points). Rust has macros and modules, although both are still bit WIP. Swift has modules and conditional compilation, should be reaching ABI maturity with version 5.0, if they keep their plan. As platform agnostic format I am with you. It would require standardizing it across compilers, which many don't want to do. However it would already be an huge improvement if could at least be compiler specific, while improving on the overall PCH experience. This has been done in the past in Lucid C++ and Visual Age for C++ v4, which made use of a database based repository for C++ ASTs, providing an almost Lisp like development experience, of course they required crazy hardware resources for their time and ended up being a commercial failure.
Do you confuse CppCoreGuidelines with Guideline Support Library?
I used to think that, but for me at least, using auto pretty much everywhere has really reduced the cognitive load of both reading and writing code. Where things start to get really interesting is deciding to use ```auto```, ```auto&amp;``` or ```auto&amp;&amp;```.
&gt; range iteration is not as beginner friendly as it looks. Well, I think we can agree with that then.
Maybe? I thought the CppCoreGuidelines where the compiler checks implementing a "safe" C++ sub-set. Some of the checks are available in clang-tidy but it seems that MSVC supports many more checks. The Guideline Support Library is a library component required by some of the checks and IIUC some of the library utilities are implemented with compiler support to make the CppCoreGuidelines check work.
It's long been the case that new codebases don't choose C++ unless they are replacing an existing codebase in another language where the previous codebase had severe issues where a rewrite into C++ made sense to everybody. I fully expect that to continue. You don't make C++ your first choice if you want to iterate quickly, keep up to date with hardware advances, keep your wage bill low, or don't mind vendor lock in. You do choose C++ where other approaches have failed, you expect to be shipping the product for at least ten years or more, need to support multiple platforms, and stability is very important to you. And I think that's okay ultimately. C++ almost always isn't the right choice for startups, or anything which wants to be nimble. It's a good fit for large multinationals where nobody would dare consider running on anything newer than half a decade e.g. my development Linux kernel is 2.6.32, and we'll be on that until 2019 on the current roadmap when we plan to move to Linux kernel 3.10 for our primary software development. As much as it sucks for people on /r/cpp, there are tens of thousands of C++ developers employed by multinationals where how C++ is presently advanced suits them perfectly. Indeed, some feel that progress is far too quick and reckless, and WG21 needs to dial it back considerably. I noticed there was a sizeable minority at Rapperswil who voted against all the big changes, and made passionate arguments against change before it was ready in general. I am not unsympathetic - `std::variant&lt;&gt;` was shipped too early for example, and we need to not repeat such mistakes.
Maybe because `&lt;regex&gt;` has incompatibilities across standard library implementations (and is slow as heck.)
It's less error prone, moving a runtime error into compile time. if(auto ptr = someFunc(); ptr != nullptr) { // can safely use ptr } // attempt to use ptr here will result in compile time error as opposed to: auto ptr = someFunc(); if(ptr != nullptr) { } // attempt to use ptr here will result in a runtime error The example above is most certainly trivial but in bigger bodies of code that a) you didn't write and b) have much more going on it may not be immediately apparent.
You should write up a proposal!
Just for my enlightenment: what are executors, exactly? I've tried searching, but the only papers I found didn't attempt to explain the basics...
He's fairly receptive to feedback, from what I've seen in the past.
It seems exactly right: stick it in Boost first. If it passes reviews there and gets enough usage (hard to measure, maybe), then it's time to start discussing standardisation.
See [CppCoreGuidelines](https://github.com/isocpp/CppCoreGuidelines). In short: * The C++ Core Guidelines are a set of rules to write good programs * The Guideline Support Library (GSL) contains functions and types that are suggested for use by the C++ Core Guidelines maintained by the Standard C++ Foundation. There exists an implementation from Micosoft (see the link above) * Visual Studio contains a Core Guidelines checker that can be used to check your code. AFAIK no other compiler implements such a checker yet.
Any benchmark with Clang LTO?
The core guidelines are a set of guidelines for good coding practices. I know clang-tidy give warnings about violating the guidelines but I don't remember if any of the compilers do. The GSL is a library implementing some of the guidelines which you can use in your code.
You get downvoted, but the question is reasonable: why should a package manager be in the standard at all? The answer is something like this: - because other languages have demonstrated how useful such a thing is to have. - because downloading and compiling libraries for use in a C++ project can be extremely painful, with some libraries requiring very specific compilers and refusing to work with others. If you agree with these points, then the final point is this: - because we don't have a solution yet. Apparently nobody out there is working on one, or if they are, there is either too much fragmentation, or they are simply not getting enough traction. And that's something where standardisation could help a lot.
No, I completely with you. In almost all situations where I applied it the readability dropped (at least for me). I fear that this is one of the features that will get overused in the short term.
But that wouldn't be the case. The standard just describes what a package looks like; any vendor could write tools for processing (downloading, compiling, installing, updating, ...) packages. Maybe Microsoft Package Manager has a slick storefront interface. Maybe the Mac version is totally automatic. Maybe the Linux version requires you to type 6 lines of command line line noise to get a package. The point is: it would all work with the same underlying format. 
How about this: if (auto [itelem, success] = mymap.insert(std::pair(‘a’, 100)); success) { // Insert success } Very similar to the first "more readable" one, but with the scoping advantage of the second one.
IIRC the pair is what is `const &amp;` and the key/value are references to the values in the pair, which are implicitly const because the pair is.
`static_foo` is addressed in the article, and I'd add one more -- it suggests to me that it uses static data, which it doesn't.
Yeah, they've kinda morphed and evolved over time, plus there are two competing proposals, both called "Executors". And various C++ conference talks are about someone's pet Executors. It's definitely confusing. Anyway, all forms are a mechanism for distributing work over available compute resources. The simplest is a basic CPU thread pool, but they also cover multiple machines, and GPUs. In theory they *ought* to support 1 million thread compute resources, so imagine a few thousand GPUs, each running a few thousand threads. I find all that laudable, but unachievable without risk of a severe design mistake before C++ 20 ships, which is the San Diego meeting for something of this size. Me personally, I'd split off the parts of Networking which do the synchronous socket i/o, and ship that now. Add the asynchronous socket i/o next C++ standard once Executors are done.
Faire enough, as mentioned it was more a kind of sideline remark given that I am not really the ideal customer target of ISO's work. Even so, let me just thank you and others on ISO, as the work isn't easy and should be appreciated, regardless how the community might feel about the outcome. 
The point, though, is if the HLIL is actually followed, it eliminates language and dialect differences. Compiler extensions would be eliminated at that level, and there's no reason you couldn't trivially import C, C++, D, Rust, or Brainfuck binaries as they could use a common HLIL.
It's really hard to write anything about C++ because almost anything you say about the language is likely to be inaccurate to some degree with respect to some minutiae about the standard. Even the big name pros get details about the language wrong from time to time (remember universal references?). Even compiler implementers themselves don't fully agree about how things about C++ is supposed to work. So either no one writes anything about C++ because there's always going to be commenters pointing out irrelevant minutiae and do so in a very condescending manner; or we make the trade-off that it's better to know about these new features at some high level that satisfies 99% of use cases and if you're an expert in C++ and really need to understand that remaining 1%, you likely are not the target audience.
&gt;The article makes it sound like it now works by default for all templated classes. Which is only true for the stl types like std::pair because the class template argument deduction guidelines have been added there ... It does work by default for templates where the class templates can be deduced from the constructor, for example this works: template&lt;typename T&gt; struct X { X(T&amp;&amp; v) {} }; int main() { auto x = X(5); } Even though no deduction guidelines were added.
With the caveat that I've not done much C++17 programming, this is my impression too. Structured bindings are a somewhat leaky abstraction, so it seems to me that it's good to have in mind what it desugars to. I'm not totally confident on this, but what I *think* that is is: auto [x, y] = foo(); // to auto __pair = foo(); auto &amp; x = __pair.first; auto &amp; x = __pair.second; and auto &amp; [x, y] = foo(); // to auto &amp; __pair = foo(); auto &amp; x = __pair.first; auto &amp; x = __pair.second; and auto const &amp; [x, y] = foo(); // to auto const &amp; __pair = foo(); auto const &amp; x = __pair.first; auto const &amp; x = __pair.second; and I'm not sure about `&amp;&amp;` on the pair. Please, someone correct me if I'm wrong.
That's how I understand it as well.
Yes, Clang code model works with cmake and qbs also.
You have the binding of `__pair` to `foo()` correct, but the binding of `x` and `y` is equivalent to `auto&amp;&amp;`, always (`auto&amp;&amp; x = __pair.first;` and `auto&amp;&amp; y = __pair.second;`). See 'case 2' at http://en.cppreference.com/w/cpp/language/structured_binding.
I honestly don't see why that matters so much to people. Conan is great and it works very well. If you're a programmer, there's a good chance you've got Python installed anyway.
it is kind of interesting on a cultural perspective how it makes you (and most likely others too) uncomfortabel to have it clearly stated, while it would make every German uncomfortable to have it not stated :) I can just delete it if you want
So I've got a couple responses. First, case 2 says this: &gt; For each identifier, a variable whose type is "reference to std::tuple_element&lt;i, E&gt;::type" is introduced: *lvalue reference if its corresponding initializer is an lvalue, rvalue reference otherwise*. (emph mine) which matches the way I said it. But at the same time, it's equivalent, right? Like the reference collapsing rules will collapse the non-`&amp;&amp;` cases into `&amp;` anyway, right? 
Clang-tidy also checks core guidelines rules, though only has around 20 checks in this category.
Meh, still not convinced. Data visualization is a big field; you can definitely hack your own visualization tool but unless you're an expert, not using something like matplotlib etc is a bad idea. It's like brewing your own ML algorithms. Doable but not necessarily a good idea. Moreover if you want data visualization, maybe that should be part of the stdlib (I would disagree again) and not drawing pixels. Adding 'drawing pixel' seems like an XY problem in stdlib, what do you *ACTUALLY* want? Drawing pixels is not a useless operation for *anything*.
In particular it would be nice to represent argument packs as [args...] and be able to do decltype([args...]) and get back Args... 
Depending on what you mean by "static", it does use static data (rather than dynamic).
It takes some effort to startup a CUDA compilation environment, especially for beginners.
Yeah, resource manager was a poor-man's mmap. Fun fact #1: like in the post proposal, high bits of pointers were used for metadata (hence the limitation of RAM to 8 MB for a long time in Macs, because only 23 bits were avalaible for the address : 8 for flags, the rest for address, with a bit for hardware vs RAM). Btw, it wasn't PurgeHandle, but HPurge. Fun fact #2: resources were limited to 32Kb because of a bug in the original ROM ResourceManager's WriteResource, where a TST.L was replaced by a TST, which tested only the lower 16 bits of the size to see if it was non negative. The original technical note (tn54) has since be wiped out from the planet.
Right, I'm just saying don't overcomplicate it – the fewer moving parts the easier to understand, and there are fewer moving parts than you imply. :-] The "declarations" of `x` and `y` are identical regardless of the cv-ref applied to the structured binding, which really only applies to `__pair` (as opposed to its members).
I am a programmer and have zero use for Python on Windows, thus one more thing to install.
Please no
It's interesting to watch a C++ conference video where the presenter asks the committee members in the audience a question and still gets conflicting answers.
&gt; Right, I'm just saying don't overcomplicate it – the fewer moving parts the easier to understand, and there are fewer moving parts than you imply. :-] I agree with the first part, but I think the reference collapsing is a moving part that you don't have to worry about with my way. :-) I think it all depends on how comfortable you are with that set of rules. If you're comfortable, probably your way is simpler. If you still have to think through "ok, so the thing its binding to is an lvalue, so that means that it collapses to an lvalue ref" explicitly, I think mine is simpler.
Background: https://llvm.org/devmtg/2016-03/#presentation7 &gt; The static binary optimization technology we've developed, uses profile data generated in multi-threaded production environment, and is applicable to any binary compiled from well-formed C/C++ and even assembly. At the moment we use it on a 140MB of X86 binary code compiled from C/C++. The input binary has to be un-stripped and does not have any special requirements for compiler or compiler options. In our current implementation we were able to improve I-cache misses by 7% on top of a linker script for HHVM binary. Branch mis-predictions were improved by 5%. 
It doesn't use static memory unless the object itself is statically allocated. The actual storage follows the storage of the object. If you say `new static_foo`, it won't use static memory. If you make `static_foo f;` as a local variable in a function, it won't use static memory.
Thank you for testing and giving feedback on our CUDA integration. I have barely touched CUDA so any help is welcome! Regarding the unresolved symbols (CUDA device code and namespaces), I noticed that newer LLVM/clang reports a lot of warnings and errors when parsing CUDA headers. When using the following CMakeLists.txt in my own CUDA example, some parsing issues were resolved: ```cmake cmake_minimum_required(VERSION 3.5.1) project(cuda) # Sometimes error-limit is exceeded when CUDA headers are parsed with libclang set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -ferror-limit=1000") # Disable frequent warnings generated by libclang when parsing CUDA headers add_compile_options("-Wno-ignored-attributes") add_definitions(-D__CUDACC__) include_directories(/opt/cuda/include) add_executable(cuda src/main.cc src/gpu.cu) ``` Note that the `error-limit` flag is for clang and parsing only. In GCC this is called `max-errors` I think, that that flag would be unrecognised by libclang... I would perhaps instead look into the warnings and errors generated by newer clang when parsing the CUDA headers and see if that can be fixed. Maybe this is a known issue in the CUDA project? For simplicity, we decided against showing the same file in two buffers. However, we will consider the other features you mention in the future. 
Use smart pointer (if you can, exclusively). Wrap everything with a lifetime (resource created or initialized by a function, used by other functions and then cleaned up by another function) in a class. When you write any class answer these questions: 1) Is this class copyable? 2) Is it moveable? Either implement copy and move ctors and operators or mark them = delete. Either make dtors virtual OR mark the class final.
Good luck with that.
Nah, it's fine. As you said, it's just a matter of cultural perspective.
As a junior dev, I love reading these blogs. Even if there are some inaccuracies, there is a reddit post with comments from senior devs pointing out the exceptions and errors. Between the two, it's the best resource that I've found for modern C++.
Without looking too much into algorithm in re.h but double as positions everywhere and char as bool already seems a bit off.
`pair` actually does have a guide to get decay right.
I see the terminology is very overloaded here. To clarify, the way I see `static_foo` is: the whole object lives inside foo (does not dynamically/ad-hoc access remote memory or resource), the memory it lives in does not change and its size is known at compile-time. foo can be reasoned about "statically" (akin to `static_assert`). I would like to see a new, better word be introduced for this, but alternatives look very unnatural and unappealing.
Thanks for that correction.
It won't eliminate compiler differences/extensions unless you get to the lever there ABI starts to matter. LLVM IR is probably the closest thing to what you are talking about but it still "suffers" from most same thing simple source distributions would (well, it doesn't because no one really tries to use it that way, but still). It's just very simple. No matter what IR/IL you try to use, if one language uses GC, others uses unmanaged memory and the third one relies on a strong static analysis and a type system to manage life times, you won't get a seamless interaction between them. But this not even the point. One really easy example, that is impossible in your HLIL model: imagine a C++ program that has conditional compilation depending on a result of some "compiler extension" (it can be even the standard one). No, you need a way to preserve this dependency in you HLIL. But you can't, since it's all should be eliminated at that point according to you.
Graphs need labels. Labels need text rendering.
&gt; &gt; Visual Studio contains a Core Guidelines checker that can be used to check your code. AFAIK no other compiler implements such a checker yet. Qt Creator integrates with clang-tidy which also checks the core guidelines.
&gt; &gt; "This one in particular: In particular, "static_foo f" looks a heck of a lot like "static foo f", and I can imagine people getting very confused about the meaning of "thread_local static_foo f" " I've used `static_*` containers and I don't remember being ever confused.
Yes. Yes. Yes. I know this is heresy, but I love working with tuples in C# (when I get the change which is not common.) I'm happy to have read this today. Yes.
I am not so experienced in it so maybe I have a lame question: what is the difference between BOLT optimization and optimization provided by -O2/-O3/-Os? Simple response should be enough for me. :-)
The compiler has no idea how your code is actually going to run, so it effectively makes arbitrary decisions about how to lay out the instructions in memory. This can cause poor cache locality. BOLT uses profiling data taken from previous runs of the program to figure out what code is accessed the most, and tries to place it together in memory for better locality and thus performance.
Well I think you may also be biased as the syntax is not familiar. If you show a lambda to some people with no c++11 background they could be scared.
It still can, afaik.
Unless it's templated or you want the compiler to be able to inline it for optimization. There are certainly important cases for both putting implementation in a header vs source file. Once Modules are introduced, this might be a more hard and fast rule.
&gt; This solution is not perfect for one simple reason: the friend declaration is proportional to the number of template parameters of `Base` class template. It might get quite long if you add more parameters. Doesn't have to be proportional. You could use the *injected-class-name*: friend class Base&lt;Derived&gt;; // article friend class Base; // works here friend class Derived::Base; // works even if Base is dependent
C++ developers came to use east-const not from logical arguments but from personal preference, therefore you can't make them change their mind using logical arguments.
His final note regarding inlining should not be a problem anymore. Most of my experiments with godbolt (latest GCC, clang) completely inlined the function call.
Doesn't look like a proper argument. - If you use const-West your rule will be: const applies to what is on the left, but in case nothing is on the left, it applies to what is on the right. - If you use East-const it changes to: const applies to what is on the left. So we have compressed our representation technique and designed simpler principles. We have reduced the entropy. That is a good logical thing. If we are conscious species, then we should do what is optimal... 
Ooh an open source IDE. I've been looking for a replacement to codeblocks for a very long time, MSVC is too buggy and obtuse, and lots of other IDE's are sufficiently slow or hard to use that I'm not a fan. It'd be completely fine if the code completion is better, but sadly general support is a bit lacking these days How large is large?
There are inaccuracies because Jonathan Boccara writes his blog as he learns new things about C++. But apparently this is not even his writing, but some kind of guest post.
Sorry, I wrote it backwards. :) I edited my post.
 Would it be UB to simply upcast DerivedT to accessor&amp; instead of calling do\_foo() indirectly?
If you wrote code explicitly so it cannot be encoded as HLIL, then HLIL compilation would fail.
"If you wrote code explicitly so it couldn't be encoded as portable ISO C++, then C++ compilation would fail.". And here we are back to "HLIL provides little to no no advantage to just distributing the source code".
MSVC enforces the code guidelines at compile time, and does something like Rust lifetime analysis when using the GSL types.
My questions still stand: * Is there a tutorial to write memory safe C++ using the core guidelines checkers? * Is there a code base or examples somewhere that uses them to write memory safe C++? My initial aim is to try something simple, like writing a memory safe `std::array` or `std::vector`, and moving up to `std::list` from there.
This is interesting, I wonder if profile guided optimizations had ever planned to rearrange instructions like this. What I would love even more would be a way of cutting down binary size. I just can't accept that binaries need to balloon in size instead of using only what they need out of libraries, templates, etc.
What do you mean with unsafe? Because `std::vector` is unimplementable in standard C++; you have to rely on (possibly unportable) constructs, which in my world are "unsafe" because they don't work everywhere (I know that gcc and clang support them - don't know about the others).
Compiler (at least most support it) can also do PGO. How does it compare to that?
"inplace_foo" maybe?
Isn't that the first or second paragraph of the article?
If you need 'curiously' to describe what you are doing, take a moment and think about how you should really be designing your program.
CRTP is useful for "static" polymorphism, without the cost of virtual function calls. So what is your argument here? 
&gt; Because std::vector is unimplementable in standard C++; Which part of `std::vector` is unimplementable in standard C++? I've implemented it many times just fine. &gt; What do you mean with unsafe? I mean static guarantees that the code is safe. Like MISRA-C, CERT, Rust, ...
I can't tell if you're intentionally being this way? Plenty of compilers have language level extensions. Like GCC with ranged cases, that aren't portable but can be completely eliminated by an intermediate portability language. Can you seriously not envision any situation where it wouldn't be ISO C++ but still completely decomposable to an IL?
Thank you! I presume you will be at the shadow convention in **REDACTED**?
The axes aren't labeled. What do they mean? This looks very neat. 
&gt; This heat map shows that the frequently executed code is isolated in a much smaller region than the original binary. It's a distribution of where the most accessed code exists in the binary, where the first shows it's spread throughout, and the second shows that it's now consolidated after being run through Bolt
what are the inaccuracies you are referring to other than mentioned in the top comments?
&gt; Is there a tutorial to write memory safe C++ No manual memory allocation at all. No raw pointers. Every allocation must be cleaned up by RAII (even if stuff throws). No shared pointers (or at least think very carefully before you use them). No raw loops. If you use Visual Studio and turn every warning and the guidelines warnings on, you should have something memory safe unless you use a sneaky trick. Also, by the way if you manage to make an unsafe `std::array`, I'd have to say kudos because it's a stack array.
Indeed it is. Reddit instills a habit of reading comments before article ;P
Such extensions can be decomposed into regular C++ in most cases the same way. No need for HLIL. The way you've described it, the only purpose it'd serve is fracturing the language by encouraging the use of non-standard extensions. Implementations would have little reason to converge. HLIL won't be able to provide any compatibility between different target architectures, stdlibs or even compilers.
No worries, we're all guilty of it at some point
Whats the difference from this and Profile Guided Optimization? [MSVC](https://docs.microsoft.com/en-us/cpp/build/reference/profile-guided-optimizations), GCC, and [Clang](https://clang.llvm.org/docs/UsersManual.html#profile-guided-optimization) support it natively already.
Raster fonts are a thing.
&gt;I thought the CppCoreGuidelines where the compiler checks implementing a "safe" C++ sub-set. This is not the case. It was originally advertised as such based on the premise that a yet-to-be-implemented static checker would ensure that all the "unsafe-by-default" C++ elements would be used in a safe manner. I'm afraid this was a flawed premise. It's been a few years now and the necessary static checker functionality has not materialized. Some are [certain](https://robert.ocallahan.org/2016/06/safe-c-subset-is-vapourware.html) that the required functionality will never arrive. A *"safe" C++ sub-set* does [exist](https://github.com/duneroadrunner/SaferCPlusPlus) (shameless plug), but unlike the Core Guidelines, it's premised on an acceptance that in many cases either run-time checks or certain compile-time restrictions are necessary to ensure against use-after-free bugs. As for "safely" implementing a vector, as another commentor noted, basically you can't. Someone [asked](https://users.rust-lang.org/t/why-is-vec-implemented-un-safely/1580) about doing it in the Rust language, and the response would basically apply to a safe C++ subset as well. But given a safe vector and a safe shared/reference counting pointer, I think you can build a safe implementation of pretty much any other data structure.
The curious bit is how often the pattern recurs, not the pattern itself
I find auto makes it easier to read my own code but more difficult to read others code
Luckily the AAA style never really got a foothold. Auto id like variable names, it depends on the context, but blindingly using it everywhere is like using 3 letter abbreviated variable names everywhere.
This is verbose and ugly
It's sort of consistent with for statements
Thanks for the explanation! I do wonder how many people have problems that require thousands of GPUs (or even two in two different machines, really), and if that should really be part of the standard. Wouldn't you need networking for the cross-machine part? How will code be compiled for GPUs, are GPU vendors on-board with this? I certainly see why it might take a while before this becomes a thing... 
&gt; safe unless you use a sneaky trick. You'd have to add a few more to that list. * Don't use `std::&lt;algorithm&gt;` without a wrapper that takes a *container* -- otherwise you might pass an iterator range that's not an iterator range. (BTW, I think there's a case to be made here that the current `std::` algorithms are more error prone in this respect than raw loops.) * Write a check for `clang-tidy` or something that you never create it iterator except in one of those wrapper functions or something else that you audit very closely * No raw references either * Never use `[]` on native arrays, `std::array`, `std::vector`, or `std::deque` * Never call a C API that takes or receives a pointer * ... Basically I list these things to say that there are degrees of memory safety. You're not going to program memory-safe C++; it's just not a feasible option currently. You can make things better, and the story for safer C++ is a lot better than it was 15 years ago. But at the same time, I think it's important to not delude yourself into thinking that you can't -- or even that it's hard -- to make memory errors even if you're adhering to a modern style.
&gt; I see the terminology is very overloaded here. Which is why I (and the author) think it's a bad name. It doesn't correspond to any of the current uses of static, except as... &gt; and its size is known at compile-time ...an abbreviation of this -- `static_foo` really means `statically_bounded_foo`. If you're talking about a `static_vector`, a container that [exists in boost under that name](https://www.boost.org/doc/libs/1_57_0/doc/html/boost/container/static_vector.html) (so there is precedence for that name even if I think it's not a good name), even calling it "static" is misleading -- it's only the *storage* that's static. Right there in the description: "A *variable-size* [emph mine] array container with fixed capacity." Even if I bought your argument that "static" to describe the storage was good, it's still not a `static_vector` in my mind -- it's a `static_storage_vector`. And of could I'll reiterate the `static static_foo` != `static_foo` argument. In short, I don't think it's the worst name in the world, but I think it could be a lot *better*. 
Transwarp implements that: [https://github.com/bloomen/transwarp](https://github.com/bloomen/transwarp) Essentially, there are two ways of passing parameters: Either the shared\_futures directly or their results. Maybe taskflow can get some inspirations from there :)
Good points, raw references are as dangerous as pointers if you don't know where they're from. Good points for iterators, I've only used sane ones but I guess with `span` some bad things will happen. Avoiding raw loops tends to limit the need for indexing, but it was indeed an oversight. I'd say avoiding indexing is even more important than avoiding raw loops.
this looks like more lower level, operating on machine instruction-like IR to do code layout reordering. it crosses function/program boundary as it can operate on instructions.
Non-reallocating `push_back` + `v.first() + i` must be valid (array semantics). You can't have the latter if you have the former and vice versa. The latter requires that you create a new array every time you push back, but the former doesn't let you reallocate if you reserved enough memory beforehand.
I thought thats what the compiler profile based optimizers did? Use the profile data to reoptimize the code based on usage. Put frequent stuff together and whatnot
When comparing Taskflow to transwarp ([https://github.com/bloomen/transwarp](https://github.com/bloomen/transwarp)) I noticed a few features that Taskflow seems to lack: \- Support for custom executors (e.g. running tasks in a custom thread pool) \- Canceling tasks (both before started and while running) \- Passing results between tasks \- Task priorities \- An event system to be notified when a task reaches certain points \- Support for Visual Studio Are there plans for adding these features? I think particularly the first three would be nice to have :)
Familiarity is certainly a factor, but longer lines are objectively harder to read.
And to add onto what you said, Java's NIO is poorly documented and very easy to hang yourself without knowing the very intimate details that it tries to hide from you. I definitely would not want to see a repeat of Java's mistakes in C++.
juCi++ is optimised for a git workflow where rebasing and switching branches might happen frequently. We do cache parsed files, but we do not preparse the whole project like many other solutions do, as this could lead to unnecessary resource use. Neither do we yet cache file content, but we might store some file content in the future. Currently, if you do a recursive grep in your project, and it does not take more than 5-10 seconds, I would say that juCi++ should handle it well. But it of course also depends on your machine and what dependencies you are using. The OS you are using might also affect parsing speed, and native Linux seems to yield the best performance as far as I've seen, though at the cost of increased battery use.
[AMD used to have a GPGPU library called Bolt](https://github.com/HSA-Libraries/Bolt). Do people even bother checking for pre-existing projects when naming things?
by mistake this is [linked list data structure] operation.
Then I really don't want to work with you sir.
That doesn't prevent this AFAICT: auto v = std::vector{2,3,4,5}; auto&amp; v0 = v[0]; for (auto i: v) { v.push_back(v0 * i); } The memory safe C++ subset in the CppCoreGuideline checker prevents that.
Editing my example from above, that is not enough to prevent UB here: auto v = std::vector{2,3,4,5}; auto v0 = v.begin(); // this is an std::vector::iterator for (auto i: v) { v.push_back((*v0) * i); } // UB But the memory safe C++ subset in the CppCoreGuideline checker prevents that.
Yes a memory safe subset of C++ does exist. You can find more information about it here: https://www.rust-lang.org
It's a linter. It'll point out areas of your code where you may have made a mistake (or you may not). If you follow the core guidelines you're less likely to make mistakes that cause memory issues. It's not, not by a long way, a memory-safe subset of the language.
Did you read the article? &gt;In practice, however, the PGO approach faces a number of limitations, both intrinsic and implementation-specific. Without the source code, no compiler has control over the code coming from assembly or from third-party libraries. It is also difficult to obtain and then apply the profile accurately during compilation.
Indeed, because argument packs are tuples anyway. As structs are.
That takes time and effort. I want free cake and people to recognize my smartness ASAP.
Nah, c++ has too many proposals already. If anyone thinks this is important, feel free to make it a proposal.
There are attempts, I can't vouch if they do indeed succeed or not. http://ithare.com/a-usable-c-dialect-that-is-safe-against-memory-corruption/ /u/duneroadrunner 's https://github.com/duneroadrunner/SaferCPlusPlus And while everyone is downvoting the sibling comment about rust, that might be what you want.
There are two kinds of programming languages: those everybody complains about, and those nobody really uses. Rust is among the latter.
Their argument is "I know literally nothing about the topic of discussion, but I find the name given to the pattern to be alarming!"
An alternative that you may find useful are the Clang Power Tools for MSVC: [https://caphyon.github.io/clang-power-tools/](https://caphyon.github.io/clang-power-tools/). From the FAQ: &gt;Does Clang Power Tools support automatic checking of CppCoreGuidelines? &gt; Yes. By leveraging clang-tidy support for checking [CppCoreGuidelines](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md). You can use the cppcoreguidelines-\* filter from Clang Power Tools *settings*, to select CppCoreGuidelines from the available clang-tidy [checks](https://clang.llvm.org/extra/clang-tidy/checks/list.html).
As for the Core Guidelines checker, it seems to be part of Visual Studio by default now, [see here](https://docs.microsoft.com/en-au/visualstudio/code-quality/using-the-cpp-core-guidelines-checkers). The problem is that C++ is memory unsafe at its core, that was how it was designed to be to give that low-level power to the programmer. I think having some tool which is basically a meta-C++ checking literally everything for unsafe code would be a huge project that maybe we don't have the technology for right now. Luckily, we already have a high-level analytical tool for making code safe - programmers. Which is what the Core Guidelines and GSL target. It's relatively easy, and hopefully effective enough, to try to teach porgrammers how to *use* it in a more safe, modern manner.
If even google doesn't check, why bother?
I have read this book, IMO the only problem is that he is mixing much of data structures in this book, and doesn't focus a lot on the language itself. Instead, I suggest you to checkout the C++ Primer by [Stanley B. Lippman](https://www.amazon.com/Stanley-B.-Lippman/e/B000APLJ0A/ref=dp_byline_cont_book_1). 
Oh okay. Perfect for beginners too? 
&gt;Bonus &gt; &gt;`[]&lt;&gt;(){}` is now valid syntax omg :)
&gt;Absolute C++ surely it is!
It's a firing offense on at least one AAA game currently in development. Too many juniors caused too many problems using it as a crutch for not understanding what actually was being worked on.
Hehe
I meant the book you suggested. Unless that's what you meant also? Lol 
Thanks, but I was asking about C++.
clang-tidy barely has a couple of the checks implemented from the 100s that are in the CppCoreGuidelines.
It's not. The template parameter list can't be empty.
You can help them out in that area :)
I thought that such a checker has existed for almost two years now, which is what the talks were conveying. I am ok with the answer "the checker doesn't exist, and what exist doesn't work".
beautiful, we need more like this
Can you elaborate? You can just implement `std::vector&lt;T&gt;` to store `optional&lt;T&gt;` instead, which allows you to have all elements between `size()` and `capacity()` safely uninitialized, and then to become `std` compliant you replace the `optional&lt;T&gt;` with an `union`, and use the `size()` to know which elements are initialized or not.
`[[nodiscard]]` is C++17. 
&gt; P.S. Please don't #define implicit explicit(false). Just Don't. Do Not. Not Do. Don't even Try. Why not?
&gt;**Note** &gt; &gt;The spaceship operator resembles a TIE **bomber** &lt;=&gt;, not a TIE **fighter** |=| Even better :-)
It's more that the correct design - which is the whole reason that C++ is evolved through committees of experts pondering over years rather than going with "whatever is expedient now" - is one which scales without issue to millions of threads. I think everybody agrees on that, and finds it useful to think about the problem in those terms. What is much harder is doing something about it which isn't user hostile. For example, you kinda have to ditch thread local storage, because how `thread_local` is implemented in current compilers assumes an implementation model which can't scale to millions of threads. And you might say, well that's okay, I can live with that. But then you realise that `errno` is thread locally stored, and suddenly that now means you can't call any C functions, including any POSIX functions or cmath functions. And that would be a showstopper for almost everybody. There are various ways round. One is to special case `errno` by breaking ABI. Another is to write exception throwing C++ wrappers for all the C functions which return `errno`. Another is TLS partitioning, whereby only the fragment you actually use of TLS goes with each thread. Each has very serious problems. The first breaks all binary compatibility. The second is a "how long is a piece of string" solution. The third violates the C++ memory model, because all objects must be equally reachable by all threads, and each object must have exactly one unique address. These are but some of the reasons why Executors is taking so long, and is doubtful for C++ 20. Thus if you choose to be dependent on Executors, you're blocked out of C++ 20 as well. I think there is a very obvious practical solution there, but I've repeated myself many times now, so I'll not do it again. I also think that the million thread scalability problem is mostly hard because C++ is trying to solve this within C++. I think if C fixed itself, our problem goes away. And you would be surprised at how amenable WG14 are to helping solve these problems, they've been very helpful with deterministic exceptions so far at least. I would also mention that POSIX 2018 now explicitly tracks the C standard, so if WG14 changes say how `errno` works, POSIX tracks that into every POSIX implementation out there. That's my opinion on how best to solve this, but mine is not a popular one at WG21 no doubt.
&gt; As for "safely" implementing a vector, I explained here how I intended to implement it with such a checker: https://www.reddit.com/r/cpp/comments/8s8l15/questions_about_using_the_cppcoreguidelines/e0zb5rg/ Without the checker, I think I can still implement it by returning special reference and pointer types from the vector. Basically, any mutating operation would internally take a memory lock (just like Rust RefCell), and if you try to take to take two locks (that is, perform two mutating operations at the same time), or if you try to read from the vector while something has the lock, then you emit a run-time error.
I will admit to not yet reading the article. That quote doesn't seem to make much sense to me, though. doesn't seem to answer my question? For one, it acts like PGO is a different approach. So how does this differ? If it's just another implementation, it has the same issues, "both intrinsic and implementation-specific" whatever that means Why wouldnt the compiler have the source code? &gt; no compiler has control over the code coming [...] from third-party libraries isnt that true regardless? &gt; It is also difficult to obtain and then apply the profile accurately during compilation. Compared to?.. reordering the instructions? I don't see a reason that a compiler PGO implementation can't do that, if they don't already. they can both optimize code differently and optimize binary layout? Not to mention the compiler is the one who puts the instructions there in the first place, can't get much lower level than that. and going around changing third party binaries just seems like it's asking for trouble. 
Who the fuck wrote that code? They even messed up null termination
I'll settle for \`\[\](){}();\` it's still enough to scare away C++ neophytes
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
We don't need many new features. We need to improve current features and add missing parts of standard library (networking, new file I/O, text operations, concurrency)
`decltype(auto)`
and in C++20 it's going to be applied to `empty()` member functions of the standard library containers e.g.
Your approach would require illegal type aliasing – `vector::data()` must return a `T*`, which you don't actually have.
 [](){[](){[](){[](){[](){[](){[](){[](){}();}();}();}();}();}();}();}();
 #define MACROS_ARE_BAD 1
:( but every library I use uses them.
For with init: You have 2 table columns: "C++" and "C++20". Make it "C++11" and "C++20"
no one is saint, but if we didn't have moral code, we would be completely covered in macros.
Hello Lisp, we meet again!
Will Pope rms write us a static code analyser to check for moral code? 
... yes ? 
so would writing a compiler plug-in that adds implicit as a keyword would be fine ? because that's what is going to happen one day to circumvent bullshit rules like these.
so would writing a compiler plug-in that adds implicit as a keyword would be fine ? because that's what is going to happen one day to circumvent bullshit rules like these. 
`__implicit`
Also `#define yesexcept noexcept(false)`
or `throwable`. inspired somewhat by Java.
Oh sorry, I mean the c++ prier is perfect for beginners too!
To build CUDA code it can still be done in CLion by setting up the CMakeLists file correctly. There is no code inspection on .cu files, but to get some syntax highlighting, keywords can be manually defined in thr file type settings. Maybe it is best to keep kernel code in separate files from host C or C++ code, like with OpenCL.
I noticed this as well, I only use it now in very specific cases so the code is clear.
Indeed. So I was googling about the rules of pointers to an array of unions, for example, if I have `type U = union { T, T }` and then create an array of `U`s, `[U; N]`, can I cast an `U*` to a `T*` to read from the array or is it UB? AFAICT as you say it is UB because U and T are not the same type, and this violates type aliasing. But since both `U` and `T` have the same size, I'd expect a way to make this work. It seems that such a way does not exist though. Note that type punning using memcpy does not work here :/
Although many would probably not like it here, the memory safe C++ subset is called rust. That also has it downsides being new and very strict, but if you are looking for such safety it might be a good alternative.
I am writing this comment in Firefox. Hence I am using Rust.
It’s forbidden for two of the oddest (and weakest) reasons. “Conventional notation is more familiar to more programmers.” This is analogous to stating that T* is preferred to {unique,shared}_ptr&lt;T&gt; “Consistency in large code bases.” If I start a new code base and becomes quite large in the future, I don’t see why we need to enforce this as a guideline.
That generally works without the `; ptr!=nullptr` compnent.
Also the pattern itself has curious repeats. The name is layered.
RTTI, that is all.
Doesn't seem similar to lisp at all. Lisp doesn't have a notation for function call, since every juxtaposition is a function call by default.
Someone please propose the X-Wing operator `&gt;=&lt;`.
&gt; if your little python project grows, it can be tricky to manage True. Because python is dynamically typed, in small projects it is very prolific (write a bit of code, get a lot going). In larger projects (whether that is something that starts to be large from the first design, or something that grows past it's initial requirements) the lack of static checks starts to be felt. This is why for larger python projects unit testing is critical. If you end up with a (large-ish) python project with no unit tests (especially if you are not part of the original development team) your best bet (in order not to sink the project) may be to add unit tests. 