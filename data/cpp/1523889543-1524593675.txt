got it wrt ccache for windows - check https://github.com/frerich/clcache, "works for me" VS2015
I'm not sure what the question is: is it about why there are so many language servers or why are all language servers based on clang? I guess both are pretty easy. As for the first one: the idea of language servers is appealing; providing decent abstractions for editors would be great. Why are they all Clang-based? LLVM and Clang is an active project with tons of people working on it, one could also get some assistance when needed. Implementing yet another parser is a tremendous amount of work and it's honestly not worth the effort. Clang has a decent API, too. I mean, what else could be used? GCC is not really an option when it comes to reusing the codebase and building something on top of it. At the same time, there's everything one needs in the Clang project: libClang for stable stuff, libTooling &amp; friends for cutting-edge. It's also cross-platform and pretty easy to setup, so this is an obvious choice for many tools.
Ah, okay; I didn't update the page and ended up not seeing your update. Well, the same can be asked about most tools out there: text editors, IDEs, OSs. The best reason is typical that these tools are not useful by design (there are obviously many other reasons too). Reviving clangd idea was proposed because rtags and cquery could not meet (by design) few needs people wanted badly. Also, having clangd in the LLVM repos is pretty good because the developers can get necessary changes in Clang and LLVM painlessly and also quick, which is also important.
&gt; That takes away a huge amount of the mental load off the programmer I disagree. I program C++ day to day and very little of my mental load is about resource management. Basically it works most of the time. Note that in a GC language you can't really ignore ownership and management either because if you do you wind up with unnecessary references to memory which is essentially a leak. Mostly in C++, I create a resource (file, vector, image, lock, etc) and never have to worry about it again. Rust is somewhat similar except it's much more, uh, pedantic. &gt; That's the thing though. Despite being in the works for so long, and now over 3 years post 1.0, Rust still has not seen the kind of adoption they'd hoped for. I don't know what they did hope for. I mean Rust is aimed very squarely at the same space as C++, which in practice means large, high performance things. That's something of a niche, and there are never going to be that many projects (though they may be important) that fit the niche. &gt; No doubt they're considering this option to make it more palatable to the lay programmer, and thus increase adoption. Sounds more like they're putting it in so it can interface cleanly with GC languages, like JavaScript, which they kind of need in their browser. GC comes with bit penalties. One of the main points about Rust is to have a program without the penalties of GC, but without the risk of segfaults you can get in C++. Doing day to day rust with a GC seems pointless. In terms of achievements, I remain cautiously optimistic. The mozilla team have delivered competitive browser with a team of about half the size of the competition. They've also produced the only one with fine grained multiprocessing. That's no mean feat and shows that Rust does to some extent achieve what it set out to do. I don't know if there are enough projects in that niche to make it really succeed. And until it gets nontype template parameters, I'm going to observe from the sidelines! 
&gt;I must say that I have never understood the reasoning of such questions on this subreddit. It's kinda annoying and it is putting people back from publicizing their work because it feels they have to explain themselves for doing it. Explaining yourself is not an attack or a form of diminishment... if you want people to appreciate your work, then you need to appreciate their time. It's a form of mutual respect where I respect your work by reviewing it and giving you feedback, and you respect my time by showing you took the basic effort to research this particular area and how your work builds upon it, improves on it, or differentiates itself from it. That's how publicizing works and if you don't want to create that kind of mutual level of respect between you and the audience, then why bother publishing your work? You say explaining yourself in this way is annoying for you... let's consider the converse position. Imagine if everyone who worked on a vim plugin (or any tool) posted it without any regard to any other similar plugin, and expected everyone to have to go over all of its features one by one from the ground up... Nothing would ever get accomplished and it would be a monumental waste of productivity for all people involved, including you. At any rate, yes, kudos to you for putting in the effort to make this plugin as in my opinion vim is desperately in need of tools for C++, but also be appreciative when someone asks you to compare your work to other well established and existing tools. That person isn't attacking you, they are giving you an opportunity to showcase yourself in a way that you haven't yet done so. Respect that opportunity and address it appropriately.
what the hell, talking about c++ here automatically gets you downvoted???
Dude, we get downvoted here in a c++ reddit for talking about c++. Guess who do that??
practically? I don't know, although it was great practice! Realistically, it tests the effectiveness of (abstracted-)threading in a language. You're supposed to create 503 threads, take a token (integer), give the token to the first thread, which then decrements the token and if not zero, passes it to the next thread which repeats this procedure. The last thread is linked to the first thread. The answer is which thread ends up with the token once it reaches zero. You could just do this in O(1) but the requirement is to write the code expressed as above (tasks that can be picked up by threads). You are allowed to use a task-queue to share the work between the 503 threads (that was the previous solution) so that is what I did. This is just a ~100 lines of code mini-test but it shows the performance of running a large amount of pre-emptive threads in te language, to first order, and how much work it is to write. In this case, it shows that c++ is quite fast, and you can write your own task-queue (well, circular buffer) with only the standard library in less than 100 lines. But compared to Haskell it's still a lot of work for little gain.
Your comment is just uninteresting
it was (shutting down now) exactly what you see. It's not that grounbreaking or anything. It's a bit of fun to compare languages and see nice performance tricks/code snippets. At least that's what I use it for. Results were also posted on /r/rust quite a few times. These micro benchmarks are useful for new languages as they offer a small, varied, set of problems to compare expressiveness and performance between languages/compilers.
There isn't even a c++ solution for the matmul one. Perhaps it would be worth it to make a Github organization with a problem-set repo, benchmarking repo, and a template for languages to follow. Then each language/person could create their own repo to test against the 'standardized' benchmarks. 
Imagine an api where the result is a collection of binary blobs best represented as a tree. Add them to blob_tree then serialize it to a buffer.. you can then return it to caller (perhaps in http response).. the caller can then use blob_tree to turn the buffer back into a tree.
Without looking into it, it probably is relevant, yes. IIRC that bug also affects Ubuntu, because its glibc is not built with the unwind tables that makes it work for Fedora x86_64 and similar targets.
&gt; shutting down now In transition — the URLs you posted are actually the new location. Perhaps in a couple of months programs will be accepted for measurement once again — but right now we're busy trying to figure out how to archive some of the stuff from the old project.
https://www.reddit.com/r/cpp/comments/8ch6x8/benchmarksgame_is_no_longer_accepting_submission/dxgllhx/
&gt; Something like … would be nice. But not true.
I can't speak to how much effort would be involved, not sure if it's trivial. &gt; I'm not sure why single TU/project-wide makes a difference for the speed of completion and error checking. Both are usually done in context of a single TU, unless you want to provide some kind of auto-import, which these tools probably don't. In theory, with perfect design and sufficient man hours, yes, it shouldn't matter. In practice, going project wide adds a lot of complexity, and needs an improved design and more work, both of which are often hard to come up with. When you do one TU, the language server really only needs two states (per TU): ready, or not ready. When you do project wide, let's say the user changes a single header file. That leads to tons and tons of re-indexing. But you still want to re-provide error checking completion in that header right away, on the assumption the user is still working in that file. That in itself is just a guess, ideally the frontend would track which file the user in, so you can prioritize that for re-indexing... etc etc. Project wide just requires a much smarter, more asynchronous, more "fail gracefully" kind of design, I think. Global refactoring can be done by leveraging the clang-rename tool, and goto definition might really be goto definition, or declaration if unavailable. It's hard to say. IMHO find references is the canonical project-wide feature, if that's not provided it certainly seems like something critical is missing. I probably will switch, but I'm one of the rare C++ developers I know that has a perfect functioning setup at the moment. My error checking, goto definition, auto completion, find references, and a few other things, all work 100% of the time in my IDE (spacemacs). It took a decent amount of work, so I'm just not rushing to be an early adopter of cquery. But it's definitely going to happen (cquery may also provide syntax highlighting, the feature is already there in a sort of beta... very exciting stuff).
No, the point is that it is not LGPL. LGPL is fine for preventing "closed source fork overtakes", but does not force developers to put their code under a license with a fundamental idealogy that they don't share. Simple as that. If one creates a library and want copyleft, which is fine, its their code and their right to believe in that, then it should be LGPL. Otherwise its just not going to get used that much.
&gt;Windows only how about no
&gt; The point I was trying to make is that it seems vastly unfeasible for anyone to be familiar with all of the alternative options. That seems vastly more feasible than writing something nontrivial from scratch without knowing what already exists in that category of software. Would you really have written this if some existing LSP daemon already did 100% of what you want..?
&gt; Explaining yourself is not an attack or a form of diminishment... if you want people to appreciate your work, then you need to appreciate their time. Exactly. That's what the documentation I have written was intended to. To not to try to waste someone's else time and try to depict what the project is about in a clear and concise way. Or is it not enough detailed? &gt; and you respect my time by showing you took the basic effort to research this particular area and how your work builds upon it, improves on it, or differentiates itself from it. Can you point me to the same kind of research done by `cquery` or `rtags` developers? I don't remember these projects have been criticized for that very point. &gt; That's how publicizing ones work goes and if you don't want to create that kind of mutual level of respect between you and the audience, then why bother publishing your work? Well, I've gone a long way creating this product and documentation for it so I believe that I've shown enough respect to (potential) users. &gt; You say explaining yourself in this way is annoying for you... let's consider the converse position. Imagine if everyone who worked on a vim plugin (or any tool) posted it without any regard to any other similar plugin, and expected everyone to have to go over all of its features one by one from the ground up... Nothing would ever get accomplished and it would be a monumental waste of productivity for all people involved, including you. I'm sure that's how every documentation on numerous Vim plugins for let's say Git interface is done? Each and every Git-related Vim plugin does a comparative study w.r.t. other similar plugins? I wouldn't say so ... I am not opposing to what you're saying but you're talking about utopia. People do these kind of projects in their _spare_ time and there is usually a single work-force behind them and not any commercial value nor company standing behind them. You can't really expect from such projects to have all the information you would ever want. &gt; putting in the effort to make this plugin It's not about the plugin. It's about the language server which this plugin only builds upon. Vim is completely irrelevant here. It just happens that I've made a frontend for Vim because I am a Vim user myself. &gt; Respect that opportunity and address it appropriately. I will. That's the reason I've posted it here because I would like to hear valuable input from the community. 
That is part of the auto-complete feature which I am afraid is currently not available. But yes, I hear you because I had the same expectations when I first tried out `YCM` :)
So you want feedback, but if you don't like the feedback it's suddenly "toxic". Amazing. I hope you know how meaningless people like you have made that word.
&gt; Hey OP, this is great, I have been following your progress on yavide for a while. Thanks for the kind words. Glad you like it. &gt; I think it might be time to try the switch from vim. Well ... yes. I've found that I was really pushing Vim limits with this plugin. Especially on the rendering end. &gt; Are you aware of any caveats, especially when working on large projects with possibly large files? I see there might be slowdowns when scrolling with cxxd-vim. Given that the semantic syntax highlighting service is enabled, Vim will start to choke on a large files. There are too many syntax rules (given the introduction of semantics) so rendering engine is simply not capable of processing those timely. A fix would be to apply that many syntax rules as there are visible number of lines in the viewport but that would require us to hook onto the some kind of a scrolling event in Vim which unfortunately doesn't exist :) The size of the project would only impact the time indexer takes to finish. Although indexer is implemented to take advantage of all available CPU cores in the system so it should be as fast as it can be.
alright, in this case it would probably be handy to be able to toggle semantic syntax highlighting/standard syntax highlighting with a vim command (mappable to key combination) keep up the good work!
*only* supporting Vulkan is a no-go for the stated target audience (games). Supporting it *as well* would be nice though, certainly! You mention "embedded" a couple times, but from the FAQ: "Ultralight is intended to be used for rendering HTML UI within games and desktop apps." As to why only having Vulkan support wouldn't work: Vulkan doesn't work for game developers targeting high-end development on the XBox or the PlayStation or the Switch. Since they can't put the proprietary API code in a public repo for NDA reasons, that at the very least necessitates a render API abstraction and not being hard-coded to Vulkan. (The Switch does support Vulkan, but that's only used by simpler indie games; the big titles use NVN to get full hardware features/performance. Same as the hacked-up GLES support on the PlayStation 3, which went basically unused by everyone in favor of PS's more capable proprietary API.) For mobile development, Vulkan only works on Android 7+. A very sizable segment of Android users are running Android 6 or lower. Being limited to Vulkan support would significantly cut down the potential market of a game. Also, even with Android 7, there's also the issue of hardware support; recall that Vulkan is designed specifically for currently modern hardware, and a number of older mobile GPUs or low-end budget devices that might support GLES just fine don't have Vulkan support. Older and budget devices are especially prevalent in certain very large and potentially lucrative emerging markets. Vulkan is also not a great choice as an "only" API for Windows. Games and even other dekstop apps - especially "high-end" ones - are still typically written in D3D. Given that the app is already thus integrating heavily with D3D debugging/profiling tools, developers *really* want their UI library doing the same. If an app is using Vulkan then the developer would certainly want the UI library to use Vulkan, sure, but most (big) apps on Windows do not use Vulkan. Regarding Apple platforms, I'd imagine that part of it is that MoltenVK being freely available is a fairly new development and might not have been the case when the Ultralight developer started on Metal support. The TL;DR is that Vulkan only supports a small subset of what game developers and the Ultralight target audience actually need. All that said, the abstraction is the key part. Many significant renderers already have their own low-level API abstraction. The application/framework in place might even have its own debugging/profiling tools (e.g., we have a light GPU counter/profiler system used for in-game statistics/graphs in internal development builds, similar to those found in mainstream engines like Unreal or Unity). Developers want their UI renderer to route through their render abstraction, not hardcoded to just Vulkan or anything like that.
XD is a fantastic text emoji swimming in a world of ugly yellow emoji. Admittedly, I would reserve the capital 'X' for more powerful emotions, and stick with xD. xP and x3 are also great, and though I would prefer to go back to the ways of :P and :3 (thank you Reddit) ... I now have to avoid them in fear of evoking un-fittingly boring and bothersome yellow images.
Exactly what I don't like? What kind of feedback? _How does it compare to xyz?_ is not a feedback which you can particularly like or not. It's pointless and not constructive at all but at the same time probably the easiest thing you can ask for. I'm sure you do the same when downloading a new TODO app on Google PlayStore. You don't really go downloading it and actually trying it before the author makes a comparative study with all of the other TODO apps out there. Sorry Miss.
&gt; alright, in this case it would probably be handy to be able to toggle semantic syntax highlighting/standard syntax highlighting with a vim command (mappable to key combination) [It's already possible!](https://github.com/JBakamovic/cxxd-vim/blob/master/plugin/cxxd.vim#L40) Actually it is possible to be done for any other service as well. Seems like a good candidate for a new section in documentation. &gt; keep up the good work! Thanks. 
It appears to also deadlock on Fedora27
Can't tell if serious or troll.
That's silly. "Not going to get used much" is just the usual astroturfing by closed software vendors. It's the "GPL is cancer" argument again made by MS when they felt threatened by Linux back in the day before they lost There is plenty of GPL software used. 
https://upload.wikimedia.org/wikipedia/commons/d/db/Icono_xD_para_wikipedia.png
I would just ignore this thread for the most part. I think you doing a comparison between other tools would be a luxury but not an expectation other people should hold. I also think you've become the victim of a reddit pile on, which don't often get salvaged, even with decent explanations. 
DX
Yes, there is a lot of GPL _software_ used. But most useful libraries are LGPL. Big difference in usability. And no, this does not only affect closed source bullshit, it also makes it unusable for other open source projects that use incompatible licenses. GPL is not the only kind of valid open source license, as much as some people would want it to be. Licensing libraries as GPL damages the open source ecosystem more than it helps it. This does not apply to tools or programs, this is about libraries that you have to link to in order to use them.
Unfortunately, thread has already gone too far with the offtopic. And yes, it won't get any better no matter what kind of explanations one provides. It's actually sad and I see it far too often around here. Easier to dismiss someone's work I guess than having a try being constructive.
&gt; That's what the documentation I have written was intended to. Extensive documentation is greatly appreciated, though a friendly summary in one or two sentences is an even better way to attract new users. I'm one of YCM/ycmd developers and if you were to ask me why one should/shouldn't use ycmd the answer would probably be "it's well tested, stable, had a lot of real world mileage, its completion is really fast, supports quite a bit of editors... though it doesn't (yet) have features that require project indexing". &amp;nbsp; From your readme: &gt; cxxd is a C/C++ language server which offers rich support for features that aid the process of source code navigation, editing, automatic formatting, static analysis etc. One can utilize it, for example, to bring IDE-like features to an editor of your choice. Great! Though the description fits any of the C/C++ language servers, so, as a potential new user, I am still not sure "why cxxd". Then a big feature list catches my attention (still being a potential new user), I go carefully through the list and can only see that cxxd is missing completions. Not a big deal, contributing wouldn't be a problem if I get really interested. However, the supported features don't tell me where they thrive. &gt; Can you point me to the same kind of research done by cquery or rtags developers? I don't remember these projects have been criticized for that very point. Can't say anything about `rtags` because I've never come close to that project. As for cquery, I've personally asked the developers to explain tons of decisions they made about the project ("why this?", "how do you do that?", "do you know about that thing?", but also "do you know that cquery could easily break given this simple input?"). My questions have always been welcomed and answered in the best possible way. Sometimes "the best way" is saying "We don't know", which is perfectly fine, as, like you said it yourself, one developer/team can't know everything. &amp;nbsp; If you want people to read your docs instead of explaining things over and over again, write an FAQ and, when asked, link the FAQ to potential future users. &gt; Well, I've gone a long way creating this product and documentation for it so I believe that I've shown enough respect to (potential) users. Excuse me, but you've been pretty hostile towards people in this thread, so your words here are contradicting. &gt; ... plugins doing research ... YouCompleteMe has certainly done its research and states in the README.md why one should consider it, which stuff it does better and which plugins it can replace. My point here is that you shouldn't neglect those that do the research. Cquery, once again, has done its research - it was started to make up for the slow development of clangd. &gt; People do these kind of projects in their spare time I couldn't agree more. &gt; It's not about the plugin. It's about the language server which this plugin only builds upon. I'd say they are equally important, otherwise you would have presented only the server. &gt; Respect Your respect has not been shown in this thread. I hope I've managed to explain why these questions that you find annoying have merits. *_* All that wall of text aside, I've inspected your code.You make *a lot* of assumptions, thus restricting your server to a very specific use case - your own. Even if you do only care about linux and no other, you should be explicit about what is supported. Also, because of your assumptions, your server has a fairly long list of dependencies that the user needs to take care of - again, an explicit list is needed. Lastly, a thing even cquery has troubles with, your compiler flag handling is **very** brittle. Here's a simple example that works with gcc, could end up in your compilation database, but would break your server: `arm-none-eabi-g++ -march=armv7-a -o foo.c -c foo.cpp`. Here you would strip everything you usually strip and would be left with just `-march=armv7-a` and the result would be an AST deserialisation error. Also, `-Xclang` flags cause AST deserialisation errors as well.
&gt; A fix would be to apply that many syntax rules as there are visible number of lines in the viewport but that would require us to hook onto the some kind of a scrolling event in Vim which unfortunately doesn't exist :) You can use `CursorMoved` for that. That's still hacky, but doable.
I don't think anyone is really trying to dismiss it, I think people see you say a question that they might have is silly and the mob swings in one direction. 
Why is language server protocol json based. Seems like it would benefit from being super lightweight. Ex protobuf 
What's the advantage of the Language Server Protocol as opposed to just using the underlying library? Or is the main advantage just to allow you to access the functionality without having to link against a C-library, which could be annoying for some languages/editors/etc.?
The idea behind the LSP is exactly as you pointed out -- to eliminate the need for linking to the underlying library or having to invoke a tool every time you want to do perform an operation. The protocol allows the client to then reach out to the daemon for code completion/linting/whatever in a language-abstract manner. Basically LSP makes it much easier for text editors to provide code analyzers in a much easier way than what exists today.
&gt; Extensive documentation is greatly appreciated, though a friendly summary in one or two sentences is an even better way to attract new users. I'm one of YCM/ycmd developers and if you were to ask me why one should/shouldn't use ycmd the answer would probably be "it's well tested, stable, had a lot of real world mileage, its completion is really fast, supports quite a bit of editors... though it doesn't (yet) have features that require project indexing". Sure. And I still don't see what I have done wrong. This kind of summary _is_ in the documentation. The thing is that some people would expect even more apart from that. And that's where we start to disagree ... &gt; Great! Though the description fits any of the C/C++ language servers, so, as a potential new user, I am still not sure "why cxxd". Then a big feature list catches my attention (still being a potential new user), I go carefully through the list and can only see that cxxd is missing completions. Not a big deal, contributing wouldn't be a problem if I get really interested. Great! I'm sure if others went through the documentation just like you did, we wouldn't end up having this conversation. &gt; However, the supported features don't tell me where they thrive. I'm not sure if I am following you on this one? &gt; Can't say anything about rtags because I've never come close to that project. As for cquery, I've personally asked the developers to explain tons of decisions they made about the project ("why this?", "how do you do that?", "do you know about that thing?", but also "do you know that cquery could easily break given this simple input?"). My questions have always been welcomed and answered in the best possible way. Sometimes "the best way" is saying "We don't know", which is perfectly fine, as, like you said it yourself, one developer/team can't know everything. Do you see me not politely answering the questions around here? The issue I see here is that there is this same kind of a question repetitively being asked whenever someone posts something here on this subreddit. Guess which question I am talking about. I've expressed my feelings about it, however, I still tried to stay on-topic and I have provided with the detailed answer which can't be told for many other replies in this thread. &gt; Excuse me, but you've been pretty hostile towards people in this thread, so your words here are contradicting. Are you kidding, me being hostile towards people? Did you read what people had to say in their comments and in a what manner they've done so? Luckily these are only few individuals. Others I have a normal conversation with. &gt; YouCompleteMe has certainly done its research and states in the README.md why one should consider it, which stuff it does better and which plugins it can replace. My point here is that you shouldn't neglect those that do the research. Cquery, once again, has done its research - it was started to make up for the slow development of clangd. So you feel there is not enough content in my documentation to persuade and attract potential new users to the project? I think I've done a fair share of writing the good documentation. Comparison tables to other tools I just don't find as necessity. Reason of project existence is clearly a lack of good C++ developer tools being capable to keep up with the latest C++ standards. This is what all of these tools have in common and I didn't think it had to be stated explicitly to be understood. &gt; Your respect has not been shown in this thread. I hope I've managed to explain why these questions that you find annoying have merits. Being disrespectful to someone usually becomes a two-way street. &gt; You make a lot of assumptions Which ones? &gt; thus restricting your server to a very specific use case - your own. I am not sure if I understand what use case are you thinking of? &gt; Even if you do only care about linux and no other, you should be explicit about what is supported. Yes, Linux is my main development and target platform. However, I am not sure of the issue here. No mention of platforms supported? But I agree that this could be part of the documentation. Officially Linux is supported only but given the implementation is done in Python, it should be doable for other platforms as well. There are perhaps few technical debts in the code in few places like using hard-coded temporary directory paths and path separators but we are really talking about a couple of instances which could be fixed in no time. &gt; your server has a fairly long list of dependencies that the user needs to take care of Such as Git, Python, libclang and Python bindings for libclang. I wouldn't call this list fairly long. Other than libclang others are fairly standard packages installed on development machines. &gt; again, an explicit list is needed. Will add it to the documentation. &gt; Lastly, a thing even cquery has troubles with, your compiler flag handling is very brittle. Here's a simple example that works with gcc, could end up in your compilation database, but would break your server: arm-none-eabi-g++ -march=armv7-a -o foo.c -c foo.cpp. Here you would strip everything you usually strip and would be left with just -march=armv7-a and the result would be an AST deserialisation error. Also, -Xclang flags cause AST deserialisation errors as well. Yes, eating compiler options mechanism is plain dumb and is definitely a subject to improvement.
Probably flexibility. Json is far easier to work with across a range of platforms and languages than protobuf.
IIRC I think I've already tried with it and it didn't work quite well because it doesn't get triggered always when one would expect. I.e. * when you're moving from the last line in the viewport to the next one * when moving with PageUp/PageDown * probably something more which I forgot
Happy cakeday!
What they said. ^but ^against
Why? IDEs operate on millisecond timescales, and I think that means the overhead of JSON shouldn't be noticeable.
Depends how much json needs to go through, I guess. I've had plenty of workloads where serialising and deserialising json has taken &gt;10s, and the actual computation has taken tens or hundreds of milliseconds. It can be dog slow. That said, it's widespread and the lack of a need to do codegen or communicate a schema is a big win for day-1 productivity.
I used to use ycmd and now use cquery. Even in its admittedly alpha state it compares well to ycmd and feels way faster. It also does completion better in my opinion because its whole project. Its basically rtags and ycmd combined. The downside is the index it builds takes a lot of memory. 
Especially with libraries like RapidJSON, you can get some absurdly fast code.
Printing doubles in particular is not fast.
&gt; Overall allocator support in a std::function like wrapper has shown to be implementable as mentioned by /u/BillyONeal Actually not; we implemented support but the support doesn't do everything related to the allocator model. It requires inspecting information about the allocator's *type* to do that, and the whole point of function is that it erases the type. What our implementation does is we use the provided allocator in that constructor call of std::function. If you assign over the std::function with a different callable, the allocator is not (and cannot) be used. This means that several constructors in the standard, like `function(allocator_arg_t, Alloc&amp;)` don't make any sense.
json is the most portable format I know every single language has a parser for it. json does have it's issues, but interms of portability and cross language support, it's great.
Are you saying that all tools will be written in C++?
What is the purpose of this location compared to all the other seemingly identical places to do the same thing? Presumably there should be some explicit need that is unfulfilled to offset the cost of further fragmenting the community. There are already IRC channels, slack channels, and discord channels with places for beginners. Also, you should spend the 5 seconds it takes to spell check your post.. it doesn't lend credibility to have typos. 
Nice, I hope she liked her first exposure to modal editors.
Things like autocomplete and syntax highlighting feel much better when they're low latency, so quite possibly? I don't know how much overhead the current RPC mechanism imposes, but I would hope that they bothered to measure it before deciding to put in a lot of extra work to replace it.
&gt; when they're low latency Of course, but the feel between 0.001 and 0.0001 seconds (for example) isn't noticeable and I really wonder if this type of integration is really capable of a significant enough improvement in latency to warrant the integration. 
If you actually look at the protocol for LSP, it transfers very little data back and forth.
LSP was originally made by the team that made TypeScript, so... that's one that thrives. Rust's language server use LSP, and it's coming along pretty nicely. Go has a reasonably nice LSP implementation. I'm sure there are others!
1) Never heard of a company „reinventing the wheel“ just because dependency management is hard. I mean it’s inconvenient, yes, but economically speaking it would make no sense to roll your own implementations unless you have solid reasons to do so. (Like most of the game industry) 2) I agree, error messages, especially template-related, are unnecessarily verbose and complex sometimes. Static analysis tools can help with this. Also last time I checked MSVC reported dangling references/pointers to stack memory. So it depends on the compiler.
Json parsing is really fast, so it hardly matters. See [Raph Levien's talk on the Xi editor](https://youtu.be/4FbQre9VQLI?t=14m19s) for example for some performance analysis on how much it takes compared to everything else.
This keeps getting repeated but isn't remotely true. There are many situations where you can get large numbers of results, e. g. code completion in boost. You will notice the lag parsing 300 MB of JSON.
It is not fast compared to ANY binary format. It literally benchmarks 100x worse than capnproto or flat buffers.
That's called premature optimization. Being 100x as slow as a binary format means absolutely nothing when it's still 100 times faster than all the other stuff happening. 
The description said effectively the same thing as you, but you placed the emphasis in a way that conveys the point better (the point being that the implementation complexity for autocomplete is NxM where N is the number of editors and M is the number of languages).
This is brain dead. I can cast 4 bytes to an integer in 0 cycles (that's literally all the deserializing work the modern libraries have to do). If they can't benchmark that as faster they are doing something seriously wrong. Also 300 MB per second means you will get a 1 second lag in a large code completion.
OP, A human moderator (u/blelbach) has marked your post for deletion because it is not appropriate for r/cpp. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8cuk2n/bug_in_gcc_and_msvc/dxhy5ss/,%20was%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
File a bug report against one of the compilers, or ask on slack.
That comparison is unfair. SQLite stores arbitrary JSON, including the names of each field. When the schema is known ahead of time, binary encodings can be much more efficient.
But the real question is what other kinds of analysis or visualization or services would become feasible if it were 10 or 100 times faster/smaller.
Awful article
Wait until it's used in projects with more than 100k source files
The first one mentioned should work - if it doesn't report a bug. The second one doesn't work because that is not considered "a motion".
Without knowing what the numbers are it’s hard to say, but when you get unto the realm of 1ms to 10ms, you start to get into noticeable territory. You start missing V-syncs, or you have 3/4 serial RPCS, (which takes 30-40ms instead of 3-4ms) 
Yes, it probably is a bug but anyhow it would be only a half-baked solution given that it doesn't catch other "non-motions". I remember I was looking at contributing the proper scrolling event to Vim codebase but looked too tedious at the time.
&gt; Sure. And I still don't see what I have done wrong. This kind of summary is in the documentation. Think like a sales person. Yes, all the info is there if one looks hard enough, but people want to know up front if "it" could be interesting to them (whatever "it" may be), so "why this over that" is a selling point, even if you're showcasing your pet project. I hope I was clearer this time. &gt;&gt; However, the supported features don't tell me where they thrive. &gt; I'm not sure if I am following you on this one? Does your features do something different compared to similar projects? For example is jumping to the definition faster than with others? (Just an example question, not a concrete one.) Things like that. &gt; So you feel there is not enough content in my documentation to persuade and attract potential new users to the project? Not at all. I just think the info might not get to the user before the same user loses attention - the sales thing again. Note that I'm definitely not the one who would know how to write it good, after all I'm not in sales. &gt; Did you read what people had to say in their comments and in a what manner they've done so? Yes, there have been harsh words from both sides, now let's get back to the technical stuff and stay on topic. &gt;&gt; You make a lot of assumptions &gt; Which ones? You are expecting that the user has `g++` available so that you can discover the system header paths. And even if the user does have `g++` in their `$PATH`, your approach will fail if the user is using `--gcc-toolchain` or `--systoot` flags. You don't respect Windows style flags if clang is invoked in CL driver mode. I *think* your server depends on some LLVM/clang python bindings? Consuming `-c` and `-o` is the right thing to do, but the way you do it constrains the user's flags in a very strict form, while the command line allows users to get pretty... "creative". Think of the following flags: - `g++ -c -DFOO -DBAR foo.cpp` - user left out the output - `g++ -DFOO -DBAR foo.cpp` - user realised `-o` and `-c` are useless. - `g++ -DFOO -DBAR` - user also realised `foo.cpp` means nothing to you. - `-DFOO -DBAR -c -o foo.o` - user cleaned some flags. - `-DFOO -DBAR` - flags cleaned up by the user. - `g++ -o foo.o -c -DFOO -DBAR` - user is being weird... but this is valid on the command line. All of these should amount to the absolute same behaviour as far as the user of your server is concerned. Now for the CL driver: - `clang-cl /DFOO /DBAR /c foo.cpp /o foo.o` - `clang-cl -DFOO /DBAR -c foo.cpp -o foo.o` - `clang++ --driver-mode=cl -c foo.cpp /DFOO /DBAR` - `clang++ --driver-mode=cl -c foo.cpp /DFOO --driver-mode=gcc` - this one is bad, because it is not actually in CL mode. - `C:\Path\To\cl.exe /DFOO /DBAR` - works for libclang on linux. Note that your way of getting the system header paths will fail on non-Windows systems in CL mode. &gt; No mention of platforms supported? That really should be mentioned somewhere. &gt; Git, Python, libclang and Python bindings for libclang. Git is obvious, but which version of python works? Do I need a compiler? If I do, which one works? "Long" may have been a wrong word, but some dependencies are not obvious. &gt; eating compiler options mechanism is plain dumb Not as dumb as you may think. Consider the following flags: `ccache g++ -c foo.cpp -o foo.o`. Libclang will choke on this (AST again). Your code will consume the first flag and actually make this work. Now tell me what happens when a user gets creative and starts using multiple compiler drivers? (`ccache goma g++ -c foo.cpp -o foo.o`). &amp;nbsp; Parsing the flags is really hard. You'll find tons of complaints in any libclang based project's bug tracker. I believe that my [`clang-flag-sanitiser`](https://github.com/bstaletic/clang-flag-sanitiser) would be interesting to you.
I like the docs, how did you made them?
&gt; This is just a proof-of-concept implementation programmed in the editor. I am working on a library that contains a real implementation, except it soon™ I suppose this should be "..., expect it soon™".
What would this kind of thing be used for?
**Company:** [**nuTonomy:**](https://www.nutonomy.com/) **Type:** Full time **Description:** nuTonomy aims to be the first company in the world to launch an autonomous taxi system, and we are building up an awesome team to make this goal a reality. We are developing the first-of-its-kind complete solution for providing point-to-point mobility via large fleets of autonomous vehicles. This includes software for autonomous vehicles, smartphone-based ride hailing, fleet management, and teleoperation. The company's software has been tested extensively on public roads in the U.S. and Singapore. We offer a unique opportunity to work closely with experts from a wide array of backgrounds, to create ground-breaking technology with potential for huge impact. Our C++ engineers would potentially be working in several different teams; either pure software, or hybrid software + researcher teams. Depending on the team, you would be building software applications, or contributing to design/architecture, and even refactoring our old code. **Location:** Boston, USA OR Singapore **Remote:** No **Visa Sponsorship:** No for USA, Yes for Singapore **Technologies:** We use C++11/14 on Linux mainly. Any additional experience in any of the following will be beneficial: * GPU Programming/CUDA etc * Map related software/API (ArcGIS, Google Maps, Tomtom etc) * Robotics/motion control software * Radar/Camera related software/library **Contact:** If interested, you may email CV/questions to eugene@nutonomy.com, or apply via https://www.nutonomy.com/careers/
Spacetime inversions. 
For example, registering a bunch of classes somewhere - I've seen lots of boilerplate code for these purposes
But we shouldn't be using swap pattern anymore with C++11 move semantics /s
&gt; Does your features do something different compared to similar projects? For example is jumping to the definition faster than with others? (Just an example question, not a concrete one.) Things like that. &gt; You are expecting that the user has g++ available so that you can discover the system header paths I think that is a pretty reasonable thing to assume since we are talking about C++ _development_ environment. &gt; And even if the user does have g++ in their $PATH Fix is an easy one. Make path to the toolchain be configurable. Expose it in the protocol. &gt; You don't respect Windows style flags if clang is invoked in CL driver mode. I haven't mentioned Windows support anywhere in the docs. ~~Probably a wise thing to do.~~ Done. And I don't think it is actually relevant. See below. &gt; I think your server depends on some LLVM/clang python bindings? Yes, it does. It utilizes `libclang` Python bindings to implement source code aware features. One open question still is if `cxxd` should bring this dependency in by wrapping it into the repository itself and hard-coding to the specific version of it or should it depend on the version that end development machine provides. Former one probably brings more stability and latter brings more and/or improved existing features. &gt; Consuming -c and -o is the right thing to do, but the way you do it constrains the user's flags in a very strict form, while the command line allows users to get pretty... "creative". I don't see how the command line use-case affects `cxxd`? All the project-specific configuration can be done either with: 1. `compile_commands.json` in which case one can get such a "creative" compile command entry only if CMakeLists.txt contains such an entry (it's probably possible). *or* 2. plain txt file in which case user is responsible for providing sane compiler flags entries. Parsing and extracting appropriate compiler flags takes place only in the former case. &gt; Note that your way of getting the system header paths will fail on non-Windows systems in CL mode. I really don't see why? [G++ is used to extract system headers](https://github.com/JBakamovic/cxxd/blob/master/parser/compiler_args.py#L133-L138). Clang is not used at all in this equation so I am not sure how CL mode would have an impact here. Using G++ always is obviously sub-optimal when compiling projects with clang specific bits (e.g. different standard library used). &gt; That really should be mentioned somewhere. Done. &gt; Git is obvious, but which version of python works? Do I need a compiler? If I do, which one works? The only real dependencies are Python2.x and libclang (including Python bindings) and there's really nothing more to it. Having a compiler installed on the _development_ system doesn't seem like a dependency one expects to be explicitly stated. Nothing gets compiled in the `cxxd`. Putting a compiler dependency on a list is a no-brainer but not big of an issue IMO. &gt; Not as dumb as you may think. I was referring to the implementation which `cxxd` provides and not to the given problem. It is the implementation which is plain simple and dumb. I simply didn't try too much in that respect and I am totally aware there's a lot of space of improvement in this regard. It just happens that it _worked_ quite well on every project I have tried it on (e.g. non-trivial and complex code bases such as ChaiScript, Adobe Source Libraries, json, cppcheck, clang, etc.) so I haven't invested much time in getting back to it. &gt; I believe that my clang-flag-sanitiser would be interesting to you. No documentation or comparisons to other similar tools? Just joking ... It looks promising but is it already integrated somewhere such as `ycmd` or any other project? What sort of toolchains are supported? Cheers, Adi
That would still work with `std::string_view`literals like this: std::string s("this is a string"sv);
[Numerals in Unicode](https://en.wikipedia.org/wiki/Numerals_in_Unicode#Roman_numerals) &gt; The Unicode standard, however, includes special Roman numeral code points for compatibility only, stating that "[f]or most purposes, it is preferable to compose the Roman numerals from sequences of the appropriate Latin letters".
I need to test, but I'm pretty sure that you don't need the `for_values` with c++17. I think like this might works (I not sure about the constexpr/auto/auto&amp;&amp; part): ``` for (constexpr auto it: {2, 8,16}) { std::array&lt;int, X&gt; a; something(a); } ```
Meh. Why do at runtime what you can do at compile time. `-fno-exceptions`
Very cool.
&gt; the STL part (following a functional programming paradigm) Not really. A big part of functional programming is immutability, whereas most STL algorithms mutate something.
How can you be screwed parsing JSON regardless of format? JSON is a format. If you mean 300MB will be slow no matter what, you are incorrect. JSON is about 100x slower than modern binary formats. If you can do 300MB/s, you can parse the same amount of binary data in 1/100th of a second. The user then won't notice any input lag.
I can't help but feel the language is getting overly-verbose and hard to read.
How would achieve the same constructs in a better way?
Any known issues running this with neovim?
We’re talking IPC, messages between processes should be short. In the context of LSP, all the server needs is a “user updated these lines” message so that it can update it’s own internal representation of the syntax tree - which _is_ fast. This is all within the context of the Language Server Protocol. If I’m building something else this might not apply. But yeah, JSON parsing can be slow in certain contexts. Watch the video I linked above to get an idea of how I mean - sure we could use a faster format, but overall we’re optimizing a code path that won’t really yield much faster code overall. Plus we lose having a message format that is human readable. But yes, you are right that JSON parsing is slower.
Very nice! Would it be possible to add something like unique_function_view that has a type-erased move() method that returns an owning unique_function? I'd love a way to take a type-erased view that does not allocations or copy/moves if invoked immediately but still has the ability to stash an owning copy of the callback somewhere if I need to schedule it for later invocation. If this is too abstract, I can provide a concrete example.
If I understand correctly the conversation, you may have extremely big database, with really huge number of possible completion (eg. millions lines codebase with extensive use of boost), but this doesn't mean the amount of data exchanged between the server and the editor will be huge. You only need to send the current line/function from the editor to the server, and the server will reply with a list of 1-100 items that can be used for completion.
Okay, I get that aspect of it -- it would be nice to write only one plugin per editor for all the languages supported by the language server. However, that doesn't quite answer my question (or maybe my question wasn't clear enough) -- what's the advantage of having it be a server as opposed to a .so/.dll/etc. that you link your editor against to get all the languages? I guess a server would make implementing said plugin easier as you wouldn't need to deal with various platform vagaries surrounding how to load a library dynamically.
Thank you, I compiled and ran your code.
&gt; FP What's that? FunctionalProgramming?
You also can post your positions to the Meeting C++ Job board: https://meetingcpp.com/mcpp/jobsubmission/
I imagine you'd have language-level constructs instead of relying heavily on templates and lambdas. This is how most languages solve this. The issue is C++ tries to maintain backwards compatibility with itself for the most part, while also trying to please the disparate committee members, so we don't usually get radical language changes, for better or for worse. And I get the downvotes. I use C++ *every day*, including in embedded work. But a lot of people *do* think the language has a verbosity/clarity problem, and it is getting worse. Code in some cases is seriously starting to look downright *arcane*.
Good stuff. In many cases, if you want a function object that behaves polymorphically you can probably use a `std::function` instead and avoid much of the boilerplate: using Base = std::function&lt;int(int)&gt;; void f(Base const&amp; base) { std::vector&lt;int&gt; v = {1, 2, 3}; std::transform(begin(v), end(v), begin(v), base); } int main() { auto d = [](int){std::cout &lt;&lt; "Derived class called.\n"; return 42; }; f(d); }
It would be great if function2 would support C++17 template deduction guides.
Any plans to add C++17 template deduction guides support? 
Could you give me a short example how an intended use case would look like?
See the duplicated question below
The former, though now I’m curious what a fat pointer may be...
Thanks, I exactly know what you mean and I thought about it as an `acquire` method which could return an owning callable wrapper as you described. This would be highly beneficial especially if you have to decide dynamically whether to store the callable or not. Sadly I decided to ship a "light" `function_view` because currently my time is really limited. However, this feature could be added into the existing codebase (not with ease but it's feasible). Maybe I will extend this in the future. A contribution into this direction would be highly welcomed.
&gt; There’s something we can learn about this mistake: The need for a new container. A container that is like std::vector&lt;T&gt; but doesn’t provide ordering or an array access operator, it just has insert(element) and erase(iter), both are O(1). std::unordered_multiset
 auto lambda = []() { return 42; }; std::function fun = lambda; Like this for example. 
From what I read, function pointers in C++ are implemented as "far pointers", though I don't know much beyond that. The combination of Functional Programming and C++ is more baffling to me though. I'm a person that is interested in trying out some Functional-like programming in C, but still to think that someone treats that as something serious.
Ah ok, yes you are right MSVC specifies template deduction guids for this. But I see a major issue with this. `std::function` only supports one `operator()` overload without any qualifiers, thus the signature can be deduced perfectly. But function2 supports multiple signatures which can't be deduced. Additionally the qualifiers of the wrapper should relect its usage and not the qualifiers of the type (opinion based). But you are right, one could think about adding support for this. If you would like to create a PR for this I would merge it.
&gt; I think that is a pretty reasonable thing to assume since we are talking about C++ development environment. Not if your user is on Mac or *BSD. They use clang by default and usually avoid GPL published software as much as possible. &gt; if cxxd should bring this dependency in by wrapping it into the repository itself and hard-coding to the specific version Probably. It can cause less headaches that way. &gt; I don't see how the command line use-case affects cxxd? The second use case. Why can't user have weird and unexpected flags in the plain text file? In fact, the user would assume they are free to do with the flags anything they can do on the command line. That's where the cli weirdness comes to play. &gt; G++ is used to extract system headers. Fair enough, though ycmd only assumes the user has libclang installed - since the compiler isn't a dependency of ours, it might not be there. &gt; The only real dependencies are Python2.x Now I've learned that cxxd doesn't work on Python 3. Thank you. &gt; Having a compiler installed on the development system doesn't seem like a dependency one expects to be explicitly stated. Sure, but cxxd depends on `g++`, no other compiler works. That's what I was getting at. Depending on *a compiler* isn't a big deal. Depending on *one specific compiler* should be mentioned. &gt; I was referring to the implementation which cxxd provides and not to the given problem. It is the implementation which is plain simple and dumb. Ideally, compiler should be determined from the flags, but that's terribly hard. &gt;&gt; my clang-flag-sanitise &gt; No documentation or comparisons to other similar tools? Hah! I liked that, regardless of whether you were joking. &gt; is it already integrated somewhere such as ycmd or any other project? The idea is for Cquery and ycmd to both use it once it is ready, so both projects can benefit from what each of them find regarding exotic flag configurations. Of course, other projects are welcome to use it. Contributing wouldn't be bad as well. It still has a few rough edges which need a little polishing, but it is almost on par with ycmd's sanitising (I believe it is better than what Cquery currently uses). For example, it needs some unit tests and shouldn't be so aggressive towards "stray paths". I was considering writing python bindings for it, but I think it is fine to let downstream projects write those as they see fit. &gt; What sort of toolchains are supported? Compiling requires C++11. Runtime assumes you won't feed it anything MSVC, gcc (g++), or clang (clang++) don't understand. Other toolchains may see some support, but I won't go out of my way to basically write a &lt;random compiler&gt; driver for libclang. For example, I would consider taking care of ICC's `-xHOST`, but I wouldn't replace its `-qopenmp` with the appropriate flag
Shouldn’t Fmt&amp;&amp; support an lvalue due to universal references? 
Thanks!
`std::forward` was not constexpr in C++11, so you need to target C++14 for that, _but_... You don't want forwarding here, so it's a moot point – you're calling static member functions so there are no values to forward, and involving references here at all is counterproductive.
Avoid much of the boilerplate _and_ the possibility of inlining. Sometimes this matters, sometimes it doesn't.
[Proof:](https://godbolt.org/g/gPCEf5) #include &lt;type_traits&gt; using HelloT = std::remove_reference_t&lt; decltype("Hello, world!")&gt;; static_assert( std::is_array&lt;HelloT&gt;{}, "String literals ARE arrays"); static_assert( std::is_same&lt;HelloT, const char[14]&gt;{}, "String literals ARE `const char[N]`"); static_assert( !std::is_pointer&lt;HelloT&gt;{}, "String literals are NOT pointers"); static_assert( !std::is_same&lt;HelloT, const char*&gt;{}, "String literals are NOT `const char*`");
I didn't think the virtual member function in the polymorphic class could be inlined either? 
I use [`plf::colony`](http://plflib.org/colony.htm) for this purpose. The standard proposal is here: https://wg21.link/p0447
That depends on the linkage or finality of said class – if it has internal linkage or is `final` then it will almost certainly be devirtualized with optimizations enabled.
Sorry, you're right; I wasn't looking at your snippet in the context of what you're replacing.
&gt; What incentive does a compiler vendor have to release a version of their compiler that will likely break existing code? I don't understand the subset of the C++ community that clings on to the strict bastion of backwards compatibility. Or rather, I understand it, but I feel like the objections raised are myopic and are made with incorrect assumptions. Other languages, software, libraries, etc break compatibility. The rate of change may be fast or slow, and they have different mechanisms for doing so. C++ is one of the few languages that strives hard to *never* break compatibility at pretty much any velocity. We have a `[deprecated]` decorator we can use which occasionally gets employed on certain functions but there isn't really a scheme for deprecating syntax, or other core aspects of the language. Between the extremes of never breaking compatibility, and breaking it every other minute, there's probably a more appropriate cadence for C++ (and yes, I'm saying this in spite of the middle-ground fallacy since I still think it's true even after taking it into consideration). These days, compiler vendors should (and sometimes do) update to stay competitive with competing platforms that provide better/faster implementations. Better tooling for your platform should, in principle, result in better applications, faster development turnaround times, and ultimately a better user experience. Without having any deprecation policy whatsoever (even on the cadence of years), we choose between either accruing technical debt in a monotonically increasing fashion, or eventually freezing all progress. Extrapolating a bit, after 100 years of continued evolution of the C++ standard with no deprecation, how many corner cases would we expect an average developer to need to know to be effective? How many compiler flag workarounds will we need? How many feature detections will every library need to determine how it should build? In my opinion, if we as a community decide something is objectively bad (or at least, there's a *decidedly better* way to do it), we should pick a reasonable time nobody should use it any more, and figure out an optimal period through which to phase it out. In maintaining a large codebase, experienced developers know that *deleting code* is often more important than creating it. The standard should be no different, and I am personally glad it is exploring more heavy-handed means of proceeding forward.
Saying it's just "beginner unfriendly" is too dismissive. Rust tackles all the same problems of being low-level (and more, since it adds a borrow checker), yet has very few serious criticisms of its quality as a language. You really need to note the difference between the way a screwdriver is a bad hammer, and the way a bad hammer is a bad hammer. Python may be a screwdriver to your nail for a large enterprise project, but even in situations where C++ is the *only* reasonable choice of language, C++ is still a bad hammer. It is important to note that my reason for holding the position is not because it is actionable, but because I consider it important to hold positions honestly and accurately. I maintain that Bjarne's dismissals of criticism does not seem to be commensurate with a healthy respect for epistemology. I would have made the same points even if I did not see a way for the language to improve itself on the back of it. Nonetheless, I do think that accurate judgements here can help looking forward. First of all, C++ is still making new features. [As I discussed earlier,](https://www.reddit.com/r/cpp/comments/8bouu7/why_is_c_so_hated/dxetmiy/) many of the criticisms I have are comparatively new. There is an opportunity to do the right thing looking forward, as long as the C++ community figures out why the previous proposals suffered as they did and makes genuine attempts to change the process moving forward. It has many strong examples to choose from. Secondly, many issues can be fixed without any kind of backwards incompatible break. `&lt;random&gt;` is a really poor library, but there is no in principle reason to think it is unsalvageable. Figuring out how to put the pieces together would be a large undertaking, but I see no intrinsic barrier to huge improvements in the standard library over the course of a few standard revisions. Even things that look very hard to make way on, like aspects intrinsic to the languages itself, are not immune to improvement; metaclasses are a good example of how additions to the language can target even the most established constructs. How metaclasses themselves will pan out I cannot say, but the important point for this argument is to show that at least in principle there seems to be some kind of in.
[Seems Googleable.](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0684r2.pdf)
You can still do pointer arithmetic with `T * const p`... *(p + 1)
shows up here as well: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018
Also trigraphs, which I regularly use(d) as a code review detector. 
I don't really have a problem with deprecating parts of the language, but it has to be really slow. I have to support some compilers, that have barely C++11 support, so if a feature gets deprecated and removed in the future I need at least 10 years of compatibility. So the replacement has to be available 10 years, before anything can get removed. Otherwise I need to fall back to a lot of #ifdefs, to support new and old versions and that would be a nightmare to maintain. As most compilers provide flags for compatibility, I don't see it as too much of an issue, but it's something to be aware of.
In terms of breaking backward compatibility, one defacto way compiler vendors have done it is with aggressive optimization that takes advantage of undefined behavior. In the past, in C and C++ you could get away with undefined behavior. Now, you can't.
When trying to find an actual paper, wg21.link is awesome. In this case wg21.link/P0684R2 takes you to the rigt paper..
&gt; if we as a community decide something is objectively bad I'm not really sure I believe there's a unified C++ community voice, or if there is that the majority of people who use C++ would agree with it. I will note that while the temptation to break backwards compat is strong, it has to be done carefully or you end up with a Python 2 vs 3 situation, which would be an absolute disaster. I suspect this means that it's hard or impossible to remove anything that's widely used, no matter how much some may dislike it. &gt; How many compiler flag workarounds will we need It's probably worth keeping in mind that there's very little chance that compilers will remove both remove a feature from the language, and remove the ability to compile code that was written with a previous version of the standard. That would pin a lot of people to old versions of compilers, since many people who are compiling code only do it incidentally as part of package management on unix boxes and the like. So, the situation with compiler flags will likely remain complex indefinitely.
Do you mind substantiating why 10 years is a requirement for you? To me it seems hyperbolic and more a consequence of downstream vendors never needing to upgrade.
&gt; Can you point out which ones? eg. `&lt;random&gt;`. I'm not going to make another list. &gt; Isn't this exactly what is being done with many of the upcoming proposals? I'm not holding my breath, but if C++'s new proposals fix the issues then all the better. From what I have seen of concepts and ranges, I am not all that impressed, but I have not tried to keep up to date. &gt; There are plenty of criticisms of Rust's quality including questions about many of its fundamental decisions. Do you have any evidence for this claim? I have heard people say that the borrow checker is not an appropriate trade off for their use-case, but I have heard extremely few claims that Rust is *bad*. 
I think you are effectively summarizing the position I am opposed to. We are all aware with the Python 2/3 fiasco. Ruby had a similar problem on a lesser scale with Ruby 1.8.7 to Ruby 2. Using such egregious examples of seismic shifts on a short timescale doesn't really do much to substantiate the claim that no deprecation should be supported. &gt; It's probably worth keeping in mind that there's very little chance that compilers will remove both remove a feature from the language, and remove the ability to compile code that was written with a previous version of the standard. That would pin a lot of people to old versions of compilers, since many people who are compiling code only do it incidentally as part of package management on unix boxes and the like. The standard != compilers. If you as a compiler vendor believe you're better off supporting old features so as not to alienate developers, so be it. It should be understood though, that doing this at the cost of not conforming to the standard could be crippling. I've already completely ditched Apple as a platform for my machine learning projects because of their awful C++ compliance and absence of GPU vendor drivers that give me access to needed extensions. The point of removing something from the standard is that it at least gives the compiler author a choice. For a period of time, usage of the feature should warn (loudly) that something will go away in X timeframe. And then, the compiler author should remove it, and spend his or her time thinking about more important problems than trying to support a nest of flags.
Not the OP, but for me it's government systems stuck on old (but still supported) versions of Red Hat
I suspect there's an odd chicken and egg problem here. As I was alluding to in my comment about "downstream vendors," slow-moving entities like the government won't change without some external impetus. Having the lights go out 5 years from now is a good reason to upgrade within 5 years.
Wow... the C part might be messy. I can understand both sides of that argument
&gt; I think you are effectively summarizing the position I am opposed to. Eh, I don't think I am. I'm encouraging caution, not saying it can't be done. I don't really know what feature you could find that would actually be worth removing. I guess stuff like di/trigraphs, the non-symbol version of operators, etc would probably be uncontroversial. I don't know if you could remove anything much bigger than that though. &gt; Using such egregious examples of seismic shifts on a short timescale doesn't really do much to substantiate the claim that no deprecation should be supported That's probably because I don't hold that opinion, I just think that if anything is going to be deprecated, it needs to be: 1. Actually worth removing, as mentioned. 2. Done carefully. The py2v3 situation is a worst case scenario, but one that needs to avoided. There's often a temptation to add multiple breaking features in one go, now that you're no longer maintaining strict compatibility, but this would be a big problem. Taking out something too large or widely used would also be a big problem. &gt; The point of removing something from the standard is that it at least gives the compiler author a choice. It depends on the compiler. That might be true for compilers that only support the most recent version of the language, but it's very common for compilers to accept a flag that specifies which version of the language to compile against. For compilers with those flags (which includes all major open source ones, many proprietary ones, but not e.g. MSVC), it's not realistic to think they'd stop supporting the old standards in this case. Doing so would be very unpopular and cause a lot of mailing list drama for the open source ones, and likely cause forks. As someone who has worked on compilers before (not for C++, though), you're also probably overestimating how much effort it takes to support various language features -- Certainly some are a lot of work, but as I mentioned, I doubt anything really large would realistically ever get deprecated (nor do I think it should). &gt; It should be understood though, that doing this at the cost of not conforming to the standard could be crippling I'm not saying it wouldn't support the standard. I'm just saying you won't ever do away with flags like `-std=c++98` or whatever, no matter how much compatibility breaks in the most recent version. To be clear, I was replying to your comment about compiler flags, and not about the standard. &gt; I've already completely ditched Apple as a platform for my machine learning projects because of their awful C++ compliance and absence of GPU vendor drivers FWIW Apple ships with Clang, which has always been one of the most compliant C++ compilers, so I dunno what you're talking about (if you think clang is bad, you must not have to use MSVC++ often -- how long did it take them to get constexpr working again?). You are right that they are behind on graphics driver support though, and they have been for as long as I can remember. It's completely unrelated to this, but I agree it is a real problem.
&gt; For compilers with those flags (which includes all major open source ones, many proprietary ones, but not e.g. MSVC) MSVC [now supports](https://blogs.msdn.microsoft.com/vcblog/2016/06/07/standards-version-switches-in-the-compiler/) `/std:c++14`, `/std:c++17`, and `/std:c++latest`. 
Thanks, edited.
It used to be nothing. Now 10 years is roughly 3 new standards, and the major compilers are mostly caught up each time.
If they’re going to do this they need to do it all at once since you can’t do it again for a long time. Rip it off like a band aid, then push for vendors and compilers to update. I hope this isn’t a half ass refresh of the language that is only a partial cleanup.
Apple ships an old version of clang. I've worked on plenty of my own compilers so let's not poison the well unnecessarily. MSVC has better support than Apple by a huge margin! I work on Windows, Mac, and Linux *extensively* and was one of those people that tore out my hair for years over a lack of thread_local support on OSX for example. As for just "pulling the plug" on things, Apple seems to routinely do it to *an extreme degree* and get away with it (Carbon to Cocoa for example).
You et. al. have been doing standup work on Visual Studio and Visual C++ in particular. I correct this misconception any time it comes up in conversation as I think it's noteworthy and has made me support and use Windows more aggressively than I have in the past (formerly, I developed on Linux almost exclusively). I think people have a really tough time periodically reevaluating facts that may have been true in a bygone era.
I remember the manuals that came with Borland C++. It was a nice shelf full of the IDE, C++, and OWL manuals; maybe more but I cannot remember. It was a nice christmas gift from the parents back in the day
The US government can and will pay Red Hat or whoever to maintain a fork of GCC or whatever compiler in order to maintain support for an old version of the C++ standard. They've done this sort of thing before with extending the life of Windows XP long after Microsoft killed it for everybody else.
&gt; I think you are effectively summarizing the position I am opposed to. &gt; Eventually if C++ doesn't adapt to new technologies it will be phased out for other programming languages. I'm actually hopefully that will not happen as the last few revision seem to indicate a greater desire to move the language forward. At some point the C++ community will be blocked with moving forward if backward compatibility is made mandatory. &gt; &gt; &gt; We are all aware with the Python 2/3 fiasco. The only fiasco there was a few belligerent developers that didn't want to move forward. As far as I can see the Python community did exactly what was required to correct the mistakes of the past. &gt;Ruby had a similar problem on a lesser scale with Ruby 1.8.7 to Ruby 2. Using such egregious examples of seismic shifts on a short timescale doesn't really do much to substantiate the claim that no deprecation should be supported. I would hope that seismic shifts can be avoided. In any event I suspect that most of Ruby's problems these days is due to Ruby. Python on the other hand migrated the transition extremely well and as a result is one of the most popular if not the most popular scripting languages out there and likely will remain the scripting language of choice for years to come. A lot of that is due to Python 3. &gt; &gt; &gt; &gt; It's probably worth keeping in mind that there's very little chance that compilers will remove both remove a feature from the language, and remove the ability to compile code that was written with a previous version of the standard. That would pin a lot of people to old versions of compilers, since many people who are compiling code only do it incidentally as part of package management on unix boxes and the like. &gt; &gt; &gt; &gt; The standard != compilers. If you as a compiler vendor believe you're better off supporting old features so as not to alienate developers, so be it. It should be understood though, that doing this at the cost of not conforming to the standard could be crippling. I've already completely ditched Apple as a platform for my machine learning projects because of their awful C++ compliance and absence of GPU vendor drivers that give me access to needed extensions. The point of removing something from the standard is that it at least gives the compiler author a choice. For a period of time, usage of the feature should warn (loudly) that something will go away in X timeframe. And then, the compiler author should remove it, and spend his or her time thinking about more important problems than trying to support a nest of flags. I have to agree fair warning is important. I do suspct that you are a bit confused with respect to standards. Last I knew C++ from the LLVM/Clang tool chain is very complaint. As for machine learning there isn't much that is standardized. This brings us back to backward compatibility in C++ as I eventually see support having to be built into C++ to support ML technologies, may impact existing code.
&gt;Other languages, software, libraries, etc break compatibility. The rate of change may be fast or slow, and they have different mechanisms for doing so. And how many of those languages were able to stick around for 30, 40, 50 years? How many "C++ killers" have come and gone over the decades? Are there any other languages from the 80s that are as widely used as C++? If anything, the enduring success of C and C++ proves that long-term reliability is the most important feature that a language can have.
I mentioned in the other comment but Apple LLVM != LLVM. I suspect people that aren't aware of this don't actually develop on Apple.
Not OP, but stuck with the same problem. We have one customer that sticks with gcc 4.1.something and won't upgrade or explain why they are not upgrading. 
Validated systems, such as aircraft flight control systems or medical devices, need a very stable code base. Updating validated systems is a very expensive activity.
&gt; If they’re going to do this they need to do it all at once since That's the route Python went and while my personal experience with the language is quite limited my understanding is that most people in that community were not very happy about the whole Python 2/3 saga, so I'm not sure the "band aid" approach is ideal as it may initially seem.
Because it's unnessicarily complex. The decisions made during it's design, like templates didn't pay any attention to compilation time, they just wanted the easiest and most flexible way to have it all work, in short, it was designed top down, not bottom up engineered.
It's unfortunately not hyperbolic, imo. For instance, if you want to support all actively supported Ubuntu distros, that puts you back to 14.04. that means you're stuck with gcc 4.8 by default, or gcc 4.9, if the user installs the package. This means no C++14, and IIRC, there was a big STL ABI change from gcc 4.x to 5+. Not only this, but if you're shipping a closed source application, you have no choice but to require not only a standard compliant STL implementation, but also an implementation which is ABI compatible with the one you chose at link time (libstdc++ and libc++ aren't 100% compatible iirc). That, or you statically link in an STL, which isn't even recommended with some implementations, e.g. LLVM.
Every instance of backwards incompatibility will cause a schism. Thte python 2 vs 3 split didn't happen because they chose to enact several backwards incompatible changes at once, it's because they chose to do them at all
Does it really? Has the removal of `std::auto_ptr` causes such a split? I argue that if the changes are small enough, spread out enough and (as others have said) has a clear replacement feature - then they should be removed. Removing things shouldn't be removed from the table completely, but it shouldn't be taken lightly. 
That's a solved problems RHEL6 and onwards. Just use devtoolset and be done with it. You're clients won't even notice
Just look at the Python 2 to Python 3 migration. It's almost a decade later and they still haven't finished ripping off the bandaid yet. Be careful what you wish for.
This would be a lot easier if there were some kind of build and package system which incorporated versions. That way if someone absolutely must have compatibility with a deprecated feature, they can get it easily.
Also the whole backwards compatibility thing isn't really working anyway. Old code will frequently stop working with a more modern compiler, as the newer compiler tends to be stricter and more correct in implementing the language (e.g. `iostream.h` or the `for` scoping of MSVC6). Compilers and source code that is 100% conform to the standard just doesn't really exist in the real world. What I don't understand is why they don't just go the route of GLSL and introduce a `#version` instruction to mark the source code with the version of the standard it's intended to be compiled with. Thus you could have new features without breaking compatibility and it would be an explicit feature of the source code instead of fumbling around with `-std=c++xy` compiler flags. Compatibility is important, but the important part is that you can have old and new code interact (i.e. to avoid the Python3 problem), not that the code can coexist in the same source file without changes.
The key should be required source to source translation. It is fine to make breaking changes iff you understand the breakages well enough to automatically transform C++x to C++x+3.
Of course not, we'll have AI written in C++ writing C++ for us!
+1 for [deprecated], but actually it does work on newer compilers, not older ones. I want to explain what is happening for Java. In September 2017 they released Java 9 with new module-oriented organization, including major breaking changes with old code. In march 2018 they released Java 10 and by default marked Java 9 *out of support*. In September 2018, Java 11 will be there and even Java 10 will be outdated, bringing so many breaking changes (2 above all: no more JavaFX and Java Enterprise Code shipped in the jdk). In my opinion, looking at the Java world, that's an awful way to bring new features by breaking what was there before in so large sets. If we want to break stuff, please do it slower (2-3 years) and not so fast
here the simplest possible argument, imho: * photoshop: 28 years old * mysql: 24 years old * maya 3d: 20 years old * winamp: 21 years old * microsoft office: 29 years old * unreal engine: 20 years old * chromium: 10 years old i'm far from being a c++ veteran, but i believe none of that would be possible with a single major breaking change in the language. 
Depends how you implement it. The important part is to preserve interoperability between old and new, not to allow old and new to coexist in the same source file. That's where Python failed, it provided no compatibility between Python2 and Python3. The 2to3 source code converter they had never worked 100% and required manual intervention. With C++ you could avoid all that simply staying ABI compatible, that's essentially how C++ can still call old C code, despite both languages not being 100% compatible anymore. The syntax of the source file doesn't really matter as long as the compiled object file can still be read (#include files provide a bit of a challenge here, but that should be solvable for most part). 
The only thing I'm worried about regarding breaking changes is that you wouldn't be able to use older libraries (or even C libraries if compatibility with C is broken). How would we handle that?
But why can't you just keep using the compiler you had at the beginning ? I mean, some folks are still using GCC 2.95.
Is there a reason you have to build with a mismatch of standards and can't just.. use the lowest common denominator all your compilers support? And if new standards break backwards compat, theres no reason to use them. Compilers will supply flags for older standards if theres a need, wont they? Or failing that, use an older compiler version. It works right now, it should continue to work later.
Apple ship a crippled version of clang. It still doesn't support &lt;optional&gt; and they actively **disable** thread_local support.
People wouldn't be happy either way though, and the python 3 switch solved a few issues. It took awhile for everyone to catch up, but these days most things support Python 3, and theres automated tooling to help in porting/supporting both versions. It may not have been ideal, but there may also not *be* an ideal. People won't like it either way, better to have them only have to not like it once IMO. Complain once, grumble, but eventually upgrade, instead of doing it for a few versions one at a time.
You can't break comparability with something that was never compatible.
&gt;I'm not really sure I believe there's a unified C++ community voice, or if there is that the majority of people who use C++ would agree with it. I think this is C++'s biggest problem, along with the obsession over legacy support. C++ is a language you can write in many ways, want to write all procedural? Go ahead. Prefer OOP? go ahead. Want to write C? Go ahead etc. The problem with this is they all have their own different design philosophies and patterns. Which leads to a lot of inconsistency in design, not only in third party libraries but in the built in standard libraries. This gets compounded when you throw an over commitment to legacy support into the mix. Oh that old library isn't designed very well? Can't do fuck about it because some people may have used it in the last decade. It leads to C++ feeling wildly inconsistent when comparing it to languages more focused in their goal. I love C++ but it can be a nightmare to work in due to this inconsistency. I work professionally in C# and the unified consistent design is a dream. When I use a library I'm not trying to figure out what way the creator of it codes so I can figure out how their library works. C++ is already difficult due to a lot of freedom combined with a very complex compiler that has lots of tricks to learn. Add to that the inconsistency of design because they want to support 4-5 different coding paradigms. The amount you need to learn to become truly good at C++ is quickly going to become unfeasible for many newer programmers. I am certainly no expert at either so I may be wrong, this has been my observation so far though.
Backwards compatibility is very important, but how they handle it is at very least odd. They should avoid only extreme breakage (like python2-&gt;python3). They also should break compatibility where needed while preserving a way to target older version. So if we build code with `-std=c++11` then code will compile even on new compilers. However if we switch to `-std=c++14` we should be fine to go through code base and fix things, granted compiler informs us of breakage at compile time. I may be naive and this may be harder to do in reality, but i feel that this would allow language to get into better shape.
Just mark it as so in the spec and have compilers issue warnings about it. And make a new syntax that isn't so awful. i.e. make this crap: TimeKeeper time_keeper(Timer()); return time_keeper.get_time(); into something like this: TimeKeeper time_keeper = keyword Timer(); return time_keeper.get_time(); where keyword is anything that isn't `new`, e.g. `raii`, `local`, `stack,` or for the most fun: `auto`.
Watch also Java 8/9/10/11 burning on these screens soon
The great thing about Java's breaking changes is that a large and prominent user of Java are Android apps, and the Android tools are currently limited to "Java 7 + most of 8".
**Company:** [Gameloft] **Type:** [Full time] **Description:** [A leading digital and social game publisher, Gameloft® has established itself as one of the top innovators in its field since 2000. Gameloft creates games for all digital platforms and with an audience of 157 million monthly users offers via Gameloft Advertising Solutions a unique level of visibility and involvement to advertisers. Gameloft operates its own established franchises such as Asphalt®, Order &amp; Chaos, Modern Combat and Dungeon Hunter and also partners with major rights holders including Universal, Illumination Entertainment, Disney®, Marvel®, Hasbro®, Fox Digital Entertainment, Mattel® and Ferrari®. Gameloft distributes its games in over 100 countries and employs 6,000 people worldwide. Gameloft is a Vivendi company. We are looking for a C++ Game Programmer who is self-motivated, goal-orientated and a strong team player. C++ Game Programmers are involved in designing and implementing 3D engines and gameplay mechanics on Apple, Android and Windows platforms, developing tools for 3D video games creation and participating in the creation process along with the game design and art teams. ** Daily challenges** include: -Develop the game according to the game design document and the programming rules already in place; -Integrate all elements of the product, including engine, user interface, graphics/animations and sound (if applicable); -Work with the team to improve the production process and help in establishing best-practices; -Document the code and design and share the information with the team. **Why join Gameloft?** Excellent Social Benefits (Medical insurance package, Gym facilities and swimming pool, free for employees, on site cervical massages on a regular basis) The offer includes relocation assistance, a merit based salary progression system and a competitive initial salary package Career development opportunities An exceptional, dynamic, demanding and motivating working environment within a fast growing company Challenging and rewarding work on the next great frontiers in games An international environment which offers daily contact with other countries &amp; cultures. **Why Join Gameloft Cluj?** Cluj is “the Heart of Transylvania”: a cosmopolitan European city and an important destination for business in South-Eastern Europe, with a remarkable potential for foreign investments It is one of the most welcoming cities in Eastern Europe Cluj succeeds in combining its history and architecture with the fresh and young atmosphere created by the city’s over 80.000 students Our studio is very ambitious and fast growing - it opened in 2010 and we are now proud to have over 500 employees Quality comes first: the perfect place for you to make a difference. ] **Location:** [Cluj Napoca, Romania] **Remote:** [No] **Visa Sponsorship:** [No] **Technologies:** [ C++, graphic libraries (OpenGLES 2.0/3.0/3.1, Metal, Direct 3D, 3D mathematics (Linear algebra, arrays, vectors) Considered a plus: -knowledge of current game engines and tools (Unity/Unreal/Crytek/etc) or physics engines (Bullet/Havok/etc) Knowledge of the major 3D modeling and animation software;] **Contact:** [How do you want to be contacted? Email, reddit PM, telepathy, secret code hidden in the C++ draft?]
Get off of _your_ high horse. Some of us have spent our careers doing our best to avoid UB, even with people smugly saying how pointless and "inefficient" that was. I can't count the number of times over the last 20 years that I've been chastised for avoiding illegal `reinterpret_cast` or union-based punning violations in my code – code that now works with `-fstrict-aliasing` all this time later, untouched. Now that's paying off, and frankly, you're reaping what you've sewn; lashing out at other people like you have some objective high-ground here is laughable.
No, I'm not looking for the paper. The paper is linked in the blog post as well. I'm looking for the source of what the blog post is talking about, what the committee discussed about (and potentially directions they decided) **with regards to** that paper.
No, I'm not looking for the paper. The paper is linked in the blog post as well. I'm looking for the source of what the blog post is talking about, what the committee discussed about (and potentially directions they decided) **with regards to** that paper.
Author here. This is part show-and-tell, part code review request. Bob Nystrom, author of [Game Design Patterns](http://gameprogrammingpatterns.com/), has been writing another book called [Crafting Interpreters](http://www.craftinginterpreters.com/). The code in that book is all Java, so I decided to follow along and write it in C++. I did this partly for fun and partly for practice. I like C++, and I'm slowly trying to shift into a C++ dev role, but for now I don't get to do any C++ at work.
Well, programming languages have gone a long way in the last 29 years. All of them, not just C++. Very different techniques are prevalent nowadays, other design patterns, etc. Some of them because the world has just found out in the meantime how to do things in a better way, some of them because the underlying hardware and software changed a lot, so the software we write for it has to change too. These codebases might be 20-29 years old but any codebase that doesn't evolve is acquiring serious technical debt and is eventually going to die. Somewhat related: "Live at head" talk from Titus Winters (you can find it on YouTube).
In addition to code review, if anyone would like to help with a few specific items, that would be awesome. 1. I can go to a physical Win/Linux/Mac machine and compile this project with MSVC/GCC/Clang respectively, but the [Travis job with GCC on Mac](https://travis-ci.org/Jeff-Mott-OR/cpplox) consistently fails because "Undefined symbols for architecture x86_64", even though the exact same build steps and flags work fine on Clang/Mac and GCC/Linux. 2. The language being implemented in Crafting Interpreters is a garbage collected language. I initially used Herb Sutter's deferred_ptr ([introduced at CppCon](https://youtu.be/JfmTagWcqoE?t=1h1m55s)), which worked correctly and was super simple, but it turned out to be slow. Switching from deferred_ptr to shared_ptr yielded an order of magnitude speedup. In Herb's defense, he does clearly state it's an experiment and not a production quality library. So I was hoping someone wanted to try making Herb's library production quality, which would be useful to the community in general and not just to my little pet project. In the meantime, I'll have to look into different GC solutions, since shared_ptr doesn't detect cycles and currently leaks.
Are there any lessons to be learned from languages that do break compatibility? (Python comes to mind; they have language versions.)
no way would i argue with most of what you say. my point is, you can't update all parts of a software all at once. the codebases of the programs i listed must be insane: a disgusting mix of precisely the change in styles and patterns that you describe. to me, that's how reality is. currently i'm working on a very odd tool where i compile without clib and stdlib, but i still need maths function like sin/cos/.... for this i'm using moshiers cephes library from the year 2000. that's 18 years now. i'm grateful he made that thing, it perfectly suits my needs, but i doubt a bit that there would be much incentive to maintain compatibility to multiple versions of c++ .... **if** it were a c++ library, it's actually c. this makes my argument funny, i hope my point comes across anyways :) i get some changes (like the current discussions about the signed/unsigned comparisons), but i am quite scared of hasty updates. i wasn't using it then, but apparently c++ did fine from 1998 to 2011, when no updates where made. 
Screw backwards compatibility. Everybody's always talking about backwards compatibility. What about forward\-compatibility? C\+\+ needs to be a language compatible with the future. That means mature, well\-considered features as ease of use. I urge the committee: Take some initiative, the Rust folks are miles ahead of you on virtually every front.
Alternatively, layer feature deprecation. Mandate compiler warnings initially, and then remove them entirely in later versions. That way, you provide developers with a decent window to refactor their code and give them less of a reason to stick with older compilers.
I think what you are looking for is auto time_keeper = TimeKeeper(Timer()); That works fine today. I've transitioned to declaring most variables like that. Is there any reason it doesn't meet your needs? 
You're right about Python 2.7 sticking around, with some people never able to upgrade, all because they broke backwards compatibility.
Things I hope will happen in C++ - removing all C functions, - removing C arrays.
Keep using your old compiler or a compatible compiler until your software is EoL'd. 
Wow cool I need this. 
We have some internal libraries we use in almost all of our projects. As we have some applications, that are plugins that have to link with some binary libraries, we can't increase the Visual Studio version there (binary compatibility). So we still have to support VS2010 this year. We mostly dropped VS2005 an VS2008 support this year. Yes, compiler support is getting better, but if you have to support software for at least 5 years and you also want to support newer releases for new products, you need the deprecation period to be longer than your support period. As you can see from the above example VS2005 to the year 2018 is actually longer than 10 years, but as a developer I would be glad, if we could move to newer standards faster, but as a user I would hate having to upgrade every 3 years, especially if I have some tooling that may need to be migrated. So having the newer standard available in every compiler at release is nice, but Microsoft used to change their ABI with every major compiler release and I believe they want to do that again with their next major update. On Linux that is a bit easier, as you can generally get ABI compatibility between major versions to work. If C++ had a stable ABI, that would be easier, but that isn't going to happen in the foreseeable future.
&gt; Is there any reason it doesn't meet your needs? Because I had no idea you could do that!! I've never seen RAII objects initialised that way before! It's always done using the unreadable `type variable_name(ctor args)` way.
I still occasionally maintain some 20 year old Borland C++ 5.01 code for legacy utilities. While the IDE wouldn't run on Vista, 7 or 8, it does run on the latest Windows 10 in compatibility mode, albeit with a 16 bit module load popup error when starting that doesn't seem to matter. No more do I need an old VMWare XP image just for BC5. I do all my editing in VSCode and only use the BC5 IDE for compiling and debugging. It's painful compared to the dev tools we have today - but back then it was state of the art.
That's.. surprising. It's been relatively widely known [since 2013](https://herbsutter.com/2013/08/12/gotw-94-solution-aaa-style-almost-always-auto/).
Pretty much every language to date can link with C libraries. You don't need to be C compatible for such to be possible.
c++ has been a language compatible with the future for decades now. what are you talking about? rust looks great, but it has no yet stood the test of time. 
no idea now.
[P0037](https://wg21.link/p0037) is relevant, but I don't think it has any direct relationship with N3871.
C++ really isn't compatible with the future. Its "modern" features are awkward to use as a result of attempting to tiptoe around legacy syntax.
But can that be done without special bindings? For example, I've had to use dllexport to use a C library with C# and JNI for Java is a whole different beast. With C++ you can just include the headers you need and link the binaries and then you can just access the whole interface of the library without exporting the functions first in some fancy way. If C++ compatibility with C is broken, just including C headers won't work. Now personally, I would still be down with scrapping C compatibility to make the language more modern, as long as we at least have some way to link to C libraries, even if it would require some kind of special bindings.
&gt; shared_ptr ... currently leaks It does? You mean due to cycles? Or it just doesn't delete stuff even when cycles aren't present?
&gt; removing all C functions I'm not sure what this means, but at least `memcpy` must stay. &gt; removing C arrays C arrays are needed to implement `std::array` and a few more cool stuff, though I hope it be made Regular (i.e. copyable, default constructible, and equality comparable).
In Herb Sutter's [Guru of the Week 94](https://herbsutter.com/2013/08/12/gotw-94-solution-aaa-style-almost-always-auto/) post where he evangelizes the "Almost Always Auto" policy, he suggests that we should almost always use one of these two forms for initializing variables: auto x = init; when you don't want to commit to a specific type, or auto x = type{ init }: when you do want to commit to a specific named type. Typical of his GotW posts, it is quite a long article because he is trying to provide an authoritative, persuasive case for AAA. But for that very reason, I think he succeeds at his task. I have started using this style myself. Note that u/redbear0531 used a different variation of this syntax, using parentheses instead of brackets: auto x = type(init); One of the guidelines in GotW 94 is to use {} instead of () unless you want explicit narrowing. The {} form does not allow accidental implicit narrowing conversions. 
That works for a single release, but not for library code, that is used in multiple projects using different compilers.
I have seen these questions about state of different proposals appear here and on Slack. Is there any centralized place to see it for all of them?
Yes, if every compiler supports version flags, that would work. MSVC doesn't support version flags and while I think, they would support allowing older code to compile with newer releases, I don't want to bet on it. I would prefer it, if replacements are simply available long ahead of the removal of the original functionality. 
Right, but /u/m_ninepoints was asking why deprecation on a 10 year scale matters, not why new features might.
That doesn't work for library code, that is used in multiple projects. We use the same helper libraries (string splitting, etc) across multiple projects. Some depend on VS2005 for binary compatibility, some on VS2015. Having to ifdef the library for each compiler and maybe even having to provide different interfaces is a lot of extra maintenance cost. While we mostly dropped VS2008 and lower, we still actively use VS2010, which only has barebones support for C++11. It would be easier, if we could implement the library using the same code for projects spanning the last 10 years of compiler releases. If the replacement functionality is made available 10 years before the deprecated functionality may be removed, I can switch to the new way of doing things and have it supported in all the platforms I have to support, before the code no longer compiles.
I don't think this is an insurmountable issue. Firstly linking isn't part of the languages domain, that's the build tool's problem. All the language would need to standardize is how to import the function signatures. Currently that's done through including the header as you said. It's important to remember though that the preprocessor (of which #include is a part of) is part of C compatibility. The much appraised modules TS would replace the need for #include at which point I feel it would be preferable to have a C++ module for that library anyway. Much akin to how Python handles importing C libraries. 
shared_ptr not detecting cycles is intentional and a big part of why it's so fast. Best to make your peace and re-organize how you manage memory.
Hehe yep certainly agree with you as well. In that situation I would strongly encourage my employer though to gradually update this odd tool so that it always compiles with the most recent compilers. I am sure you would discover quite a few bugs actually by just doing that. And then gradually some updates to the code can be performed. If the tool is supposed to last another few years, maybe even 5+, you know - at some point that 18 year old C library might stop working for good.
Because you might work in a mixed environment. I have some code that is used by other teams besides mine. Even if my team is using C++17, *some* of the code needs to compile with C++11, some even earlier. So, same code, different compilers. The other example is when you have LOTS of code. You want to use new C++ features in new code, but don't have time to upgrade ALL your code to the latest features. You hope that old code, that was written against C++11 (or worse) will still just compile. There is a limit to what you should expect to be compatible (for the most part, C++11 is a big dividing line, since so much changed). As above, ~10 years seems like the limit. But I hope that explains why you can't just keep using an old compiler.
thread_local has been working on Apple's version of clang since Xcode 8 beta (that's 2016). Optional is still in &lt;experimental/&gt;. Yes.
Nothing new, I've never seen so many inexperienced peoples like in game dev anywhere else.
Great news! &gt;Perhaps, in a couple of months, programs will be accepted for measurement once again I'll sit on this one for a bit then, maybe I'll even try a [few](https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/fannkuchredux.html) [more](https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/revcomp.html) to help celebrate a 'relaunch' ;) p.s. would something like recursive_func(n){ constexpr auto compile_time_constants[N] = {constexpr_func&lt;0&gt;(), ...,constexpr_func&lt;N-1&gt;()} if( n &lt; N ){ return compile_time_constants[n] return something*recursive_func(n-1); } else { } } Count as optimizing out the work/algorithm? On the one hand, aside from the if statement it's 
I'm not sure "passed" is the right term for a "meta" proposal that doesn't actually change the working paper draft of the standard. It was "well received". Expect a follow up paper, and eventually we might see small changes in how the committee does things.
Yes but the point was that compilers support new features much faster these days so things trickle down to the corporate users faster than ever
i have no employer, i'm putting this on myself. i'm embedding a tiny tiny clang + lld in my application so i can write audiofilters in realtime. (it is far out on the experimental side of things i have to admit). 
MSVC supports version flags.
Only if it's done in a way that makes moving past the breaking feature impractical/insurmountable. Deprecagion warnings/feature flags can make up for that.
&gt; Not if your user is on Mac or *BSD. They use clang by default ... Right. Setting up a CI server might be the next good thing to do. That should make sure that stuff is functional across different platforms. &gt; and usually avoid GPL published software as much as possible. What do you mean by that? `cxxd` and `cxxd-vim` are released under GPLv3 license. That would really put people off? &gt; The second use case. Why can't user have weird and unexpected flags in the plain text file? Sure it can ... One can write even something which is totally unrelated to the any compiler flag existing. I think bigger responsibility here is on the user side and main focus is really on the auto-generated `compile_commands.json` file. Plain txt file is really a fallback solution that might serve as a first-aid solution when starting-up the project. Having `cxxd` fail miserably in such cases by employing non-trivial compiler-flag parser seems like an overkill to me at the moment. It is a goodie but perhaps not the very first thing I would do next. I am not saying it's not important though ... I know how frustrating is not to have the tool running as documented. &gt; Now I've learned that cxxd doesn't work on Python 3. Thank you. Whole Python and broken backwards compatibility is a really big mess. Now put Vim Python (python2 or python3 or both? Different distros deploying different Vim configurations ...) support into the mix and you can start with losing your hair ... I wish I haven't started with Python at all but I should have stayed with C++ ... I guess I wanted to dig more into another programming language. &gt; Sure, but cxxd depends on g++, no other compiler works. That's what I was getting at. Depending on a compiler isn't a big deal. Depending on one specific compiler should be mentioned. It will be added to the dependency list I'll create. &gt; Ideally, compiler should be determined from the flags, but that's terribly hard. I wouldn't really try to hard with plain txt configuration and start guessing all of the possible combinations and whatever the user can provide as an input. I would make `compile_commands.json` more recommended way of using the software and put more focus on bringing up the new features and/or improving existing ones. &gt; Hah! I liked that, regardless of whether you were joking. Yes, now imagine you invested literally 2 years of your life into creating something this complex and you believe you've gotten to the alpha state so you wanted to share it with others ... and you've done your homework of writing an extensive documentation on the topic with all sorts of diagrams, API's, GIFs whatever not ... and what do you get? A salvage like this on Reddit :/ It doesn't seem fair at all and totally demotivating. &gt; I was considering writing python bindings for it, but I think it is fine to let downstream projects write those as they see fit. I believe this is a feature that would enable a bigger impact on the size of your users. This is what Python is really good at. &gt; ... you won't feed it anything MSVC, gcc (g++), or clang (clang++) don't understand ... My suggestion would be to make distinctions between different toolchains be explicit in the code. It will make it easier to extend the functionality to other toolchains you want to support. 
In what world does Chromium not have breaking changes?
&gt; Because you might work in a mixed environment. I have some code that is used by other teams besides mine. Even if my team is using C++17, some of the code needs to compile with C++11, some even earlier. Then you compile the C++17 files with a C++17 compiler and the C++11 files with a C++11 compiler. Just treat them like different languages. There's no problem calling FORTRAN, Rust or C code from C++, so why should there be problems calling "older-C++" code ? &gt; The other example is when you have LOTS of code. You want to use new C++ features in new code, but don't have time to upgrade ALL your code to the latest features. You hope that old code, that was written against C++11 (or worse) will still just compile. Or you apply the same as before. It's a one-liner in CMake, there's no excuse for not doing it. 
Wish I had known about that sooner. Thanks a lot :)
Would you rather the microscopic percentage of compiler writers have an easier time, or that the community doesn't fragment and all c++ search results are always valid all the time without looking for date/compiler/version.
No code you've written in the last 20 years is going to be moved to a new version of c++ unless you were the only developer working on it. You empirically have wasted your time.
&gt; C arrays are needed to implement std::array and a few more cool stuff, though I hope it be made Regular (i.e. copyable, default constructible, and equality comparable). Yes and no, in terms of basic implementation yes it needs a C syntax. But a lot of stuff in std:: is already tied to the compiler internals. Including std::initializer_list (you can't create a function with initializer syntax without that) and many of type traits. So I think std::array could be compiler dependant :)
Removing all type keywords and get them into std could be nice too. Example: `std::bool`, `std::int`, ...
What we really need are *feature* flags. If we deprecate/remove/add breaking language features, we need to be able to toggle them. A standard ABI would be nice, too. Or a standardized dynamic library "definition" file so the ABI can be determined at runtime and patched.
Well, in my case it works, if I just have long enough to migrate away from old features to newer replacements. Having to check each and every interaction of every feature seems to be hard. It doesn't work that well in OpenGL, but it seems to work well enough in Haskell. Having a standard ABI would be nice, but that would basically mean providing multiple symbols for bounds checked containers and every flag that interacts with the ABI or change the way inlining works.
Of 
You're never satisfied, anyway.
&gt;Yes but the point was that ... Whose point? I was responding to the comment implying 10 years might be too slow for feature deprecation/replacement.
&gt; It doesn't work that well in OpenGL Does anything? &gt; Having a standard ABI would be nice, but that would basically mean providing multiple symbols for bounds checked containers and every flag that interacts with the ABI or change the way inlining works. Eh. I'm thinking more in terms of being able to make the calls and the basic ABI. That can differ from library to library. For the actual behavior of structures and such, one shouldn't be relying on full definitions, but rather thin view types which can have a defined, fixed layout. Dynamic linking doesn't really inline - I suppose you could in very, very limited circumstances, though.
The most important fix is not decaying to pointers because it opens a hole in the type system. Regular would depend on whether the held type is regular as well. 
I disagree. IMO, Feature flags would just unnecessarily complicate things. Your code should be designed to support a standard, in it's entirety. You shouldn't mix and match features. If C++N adds a breaking feature, you're under no obligation to switch to it, and theres no reason to have a flag so some people can write "C++N but not" If you want to use C++N, you should adapt your code to support C++N, not adapt C++N to support your code.
A better analogy would be the compilers increased traffic for the road, since nobody should be crossing it, it's perfectly fine for more cars to drive through it. It's nobodies fault but your own if you use UB and it doesn't work.
How does `std::shuffle` enable race conditions?
That hardly seems realistic. I'm not sure I believe that any large C++ codebases that don't use either of those exist.
Some whining based on some vague subjective opinions? Great constructive post. Thank you
Pretty much accurate from my experience. I would say STL also suffered from bad rep from early on. Back in 2006 I was taught that gamedev's hardly ever use the STL because of it's general nature making it slower and lack of control over allocations. GameDev also tends to have absolutely massive code bases (gigabytes of code), so adoption of newer versions of the language (and even compilers..) was really slow. You can't just up and lift your 8 year old code base to the new version of the MS compiler easily, or even update the project. A lot of this is dependencies on older versions of external libraries, which you always build along side your code. Good practices and precompiled headers can reduce compile times, but linking times on large products were a massive time sink too. Most of my time waiting for an incremental local build was linking times. GameDevs in my experience are also all within their own little ecosystem of development. You have the engineers swapping jobs all around in the industry and trends tend to stick as people continue to do and use technologies that they know, which can include continuing to use outdated techniques. That said, some of the smartest engineers I've ever worked with were in GameDev. When performance is such a critical aspect of your job, you tend to get people very familiar w/ the hardware they develop on.
&gt; It leads to C++ feeling wildly inconsistent when comparing it to languages more focused in their goal It happens. The only way to avoid it, I think, is either be new, be written by one author with a clear vision, or have some math-esque system underlying it. I'd say we are doing better than R, which has 3 or 4 different object systems, functional programming features, and dataset operations that are more reminiscent of SAS all bolted on top of what started as an SPSS clone. And you know what? R is doing OK in popularity. So I'm not sure what exactly about the diversity of C++ users' needs, opinions, and future visions makes it feel like "C++'s biggest problem" for you.
I agree, flexibility of c++ is its biggest strength. There is a difference between breaking something because a change is needed to implement a new feature (which even the post admits you never know will actually be good until after) and just breaking something for no other reason than it does not conform to some peoples views on programming dogma.
This is exactly what LLVM library is supposed to be used for, so I'd not think of it as an experimental approach - of course your code may be of experimental quality, but it's a solid design. You can forward C library calls to the host, and easily at that!
I don't think that was the point of this talk at all. Rather than being about the inexperience of game devs, this was about the workarounds they use to find optimisations that the STL and Boost don't provide them with. EA developed their own copy of the STL with more dynamic allocators, Unreal uses a custom template language which a pre-processor turns into reflective code that modifies values at run-time. This is not in-experience, this is generating specialised tools that work for game development. In fact I'd say it probably takes a lot of experience to understand the kind of optimisations they're making (I certainly do not).
doesn't work well for templates, for example. Or any C++ that passes around C++, instead of sticking to C interfaces. It is easy to call C from anywhere. It is not easy to pass a vector to a function compiled with a different compiler, and expect it to be the same vector.
You should add your implementation to the [wiki](https://github.com/munificent/craftinginterpreters/wiki/Lox-implementations).
&gt; I fail to see the problem here. Well that is your problem. Fact is that noobs will c/p the example from cppreference that works fine since RNG is in main, but if you c/p that code and put it in a function it is either constructing rng every call(expensive) or if you make it static it is race condition. Nice!
What would be some advantages of doing this?
&gt; C++ gives the programmer incredible power at the cost of needing to actually understand what's going on. If this seems too complicated for you, maybe python is more your style. Typical snobbish answer. The fact you need to know what threadlocal is to shuffle a vector means that std lib is poorly designed. And the fact you wrote a wrapper proves my point, not yours. If std was designed properly wrappers would not be needed, except for specific use cases.
My observation is that it's those who strictly adhere to avoiding UB who are smug about it and believe themselves to be on a moral high horse. The overwhelming majority of C and C++ developers are not at all strict or ideological about undefined behavior, which is why you can't find any non-trivial project that doesn't make use of it. The expression "get off your high horse" is intended to express the notion that an individual, or minority of people, see their actions as being superior and righteous compared to the majority of people. The expression doesn't work if you're in the minority criticizing the majority. Now how can we assess that it's the majority if C and C++ developers who make use of undefined behavior in their program? We can look at popular projects and libraries, so let's do that. Any project that makes use of POSIX right off the bat is relying on undefined behavior since the POSIX API requires its use for function pointers to work properly as well as details involving shared objects. If you use boost, cryptopp, Protobuf, MySQL, Postgres, Boehm GC, Qt or Folly, your application is using undefined behavior because all those libraries consciously make use of undefined behavior. The Linux kernel makes liberal use of undefined behavior as does Chromium and Python. I'm looking over the top trending C++ projects and once again they make use of undefined behavior. Most of them assume a flat memory model where pointers of any kind can be compared to one another which is technically undefined behavior but supported by basically any platform developed in the past 20 years. Then there's the strict aliasing rule which hardly anyone can properly adhere to and so it's common to disable it. As a final point since you talk about lashing... I think it's worthwhile to point out that people don't make use of undefined behavior on moral grounds or because of ideology, they make use of it because it provides measurable benefits to the end-user/customer and those benefits outweigh the downside of writing ideologically pure code. On the other hand, I hear very little other than moral grand standing from people who believe that their code should be strictly free of any and all UB. My stance is pretty reasonable... not all UB is the same. Some of it must be avoided as much as possible because they are inherently a program defect, such as dangling pointers, array out of bounds access, reading uninitialized data... Some UB is benign, like storing function pointers in a `void*`, doing certain kinds of pointer comparisons, certain arithmetic operations that are well defined like signed arithmetic shifting. As engineers it's our job to evaluate the pros and cons of the options available to us.
i suggest you try a boehm type gc
I wonder if finance really wants a floating point type with decimal radix, or if a fixed point type would equally (or better) serve their needs. One thing that might be worth putting in the standard library is a generic fixed point type of arbitrary radix, similar to the `std::chrono` duration types. So finance could have a fixed point type that's exact to 1/100th or 1/1000th, with well defined rounding. Or maybe this should be part of a generic units library.
And you think std::rand, which `std::random_shuffle` uses, is thread safe? C11 7.22.2.1 "The rand function"/2 says rand is not thread safe. It certainly hasn't ever been thread safe on our implementation.
And variant and a number of other things. You can't use c++17 (it has to be 1z), etc.
Nah, the radix conversion causes rounding issues. Gotta have a radix that's a multiple of ten, or you're gonna have trouble representing 10¢
you use unsigned ints everywhere? or build all your code with -fwrapv? because, technically, int overflow is undefined behaviour.
Looks like our rand uses thread local state. But it also can't return values greater than 2^16 and thus can't shuffle arrays longer than 2^16 in random_shuffle.
&gt; Looks like our rand uses thread local state. Ah so I was right, random_shuffle in practice is thread safe. &gt; But it also can't return values greater than 216 and thus can't shuffle arrays longer than 216 in random_shuffle. There is this [magical trick](https://msdn.microsoft.com/en-us/library/336xbhcz.aspx) known to experts. ;) I mean it is possible that random_shuffle in MSVC is THAT bad, but I really doubt STL would let that trash ship. :) 
&gt;There is this magical trick known to experts. ;) A magical trick that `std::random_shuffle` has no means of doing, because we don't know there anything about what the max value your RNG can return actually is. This is precisely why `std::random_shuffle` got deprecated; because URBGs can communicate the number of bits they actually generate to the library to do the right thing, and the rand function concept `std::random_shuffle` uses cannot be fixed up that way. &gt;I mean it is possible that random_shuffle in MSVC is THAT bad That's what we ship. And what we are still going to ship, effectively forever. https://imgur.com/a/oINR2
&gt; FWIW Apple ships with Clang, which has always been one of the most compliant C++ compilers Apple ships with AppleClang, which is a fork based on an older clang version (I think 4.0 currently). The libstdc++ and libc++ are also both quite dated. C++14 works reasonably well but nothing newer works, not even `&lt;optional&gt;` (yes it has `&lt;experimental/optional&gt;` but it's non-standard conforming and broken). It's around a year or more behind other platforms/compilers. Even MSVC is a lot better/newer than that.
&gt; Optional is still in &lt;experimental/optional&gt;. Yes. And non-standard conforming. (I believe it doesn't have `.value()`, among other things =&gt; makes it completely useless for cross-platform projects). `&lt;variant&gt;` is missing completely.
&gt; Then you compile the C++17 files with a C++17 compiler and the C++11 files with a C++11 compiler. Just treat them like different languages because that's what they are. There's no problem calling FORTRAN, Rust or C code from C++, so why should there be problems calling "older-C++" code ? Would you mind elaborating a bit how you're doing this? I think they are ABI incompatible, at least the standard libraries are. If I compile one of my libraries with C++17 (because it e.g. uses `&lt;optional&gt;`) and then want to use this e.g. in an Unreal Engine 4 app (which only supports C++14 and the build system isn't really configurable), then it throws lots of errors (don't remember whether it was compiler/linker/run-time).
It depends where. In practice: you are writing non-portable code on purpose by making that assumption.
&gt; A magical trick that std::random_shuffle has no means of doing, because we don't know there anything about what the max value your RNG can return actually is. Sure, but we are not talking about the same thing. You are talking about random_shuffle(begin, end, rng), I am talking about random_shuffle(begin, end) where you (implementers) provide the rng. &gt; https://imgur.com/a/oINR2 https://imgur.com/lcnHGKi 
Dunno about UDK's compiler but gcc for instance allows to choose which library ABI yoi want.
http://www.hyrumslaw.com/ 
By arbitrary radix I mean that you'd specify the radix. So if you want something that can exactly represent hundredths of dollars you'd specify that, e.g. similar to the way `std::chrono` allows you to have durations that _exactly_ represent tenths, or sixtieths, or any fraction of a second; no rounding or approximation.
I think there's an alternate solution here. I know its horrifying but javascript actually seems to have a pretty good approach here with 'use strict'. Newer code generally seems to be written in strict mode, and older code still runs just fine even under a new standards version (as far as I'm aware, my JS skills aren't amazing but that isn't the point) With C++ perhaps there could be a similar mechanism. Stick 'use strict' or 'use c++23' in a scope and you get the backwards compat breaking changes. This would make it very easy to introduce fairly significant changes without breaking any existing code, as it'd explicitly be opt in The exact mechanism is obviously a strawman and you'd need some way to deal with STL headers etc, but in principle it seems like a reasonable idea to me 
If that was the direction taken, I think it'd be best to have a dedicated `std::decimal` type anyway, as an alias for `std::fp&lt;10&gt;` or whatever (or maybe `std::fp&lt;30&gt;` so that 1/3 has a terminating representation too (although I don't know if the different rounding from base10 math would make base30 math unacceptable to the finance guys (also also 30 is closer to 32 than 10 is to 16, so base-30 is more memory efficient--but I don't know if this practically matters, especially given that if you were to have five-bit digits, you'd either have digits spanning words or you'd have bits of words just dangling off the end; not part of any digit, not big enough to use, just sitting there and wasting space)).
I don't see your point. You are in fact relying on something that is not promised to you, and that is even explicitly documented as being not guaranteed. My point is: the implementers don't have to care about your assumptions if they decide to make a change. In other words, your code is brittle.
It's MSVC (2017).
&gt; My point is: the implementers don't have to care about your assumptions if they decide to make a change. You are wrong. World would fall apart if rand became thread unsafe. And even more minor things are fixed. MSVC can not change RAND_MAX because it would break too much code.
We support mixing 14 and 17 TUs.
It's undefined if it happens, not if it might happen.
I think it's less that it would break too much code and more that people shouldn't use `rand()` for anything ever. There is nothing that can be done to `rand()`'s API to make it good. It's almost like `gets()`. If you want to do modelling or similar, you need more control over seed material than rand can give you, and doing a TLS read on every generation is cost prohibitive. If you need cryptographic security, rand() only accepting a size_t of state prevents that (as you would want to use a block cipher or cryptographic hash to power that which need at least 128 or 256 bits of seed state).
Breaking compatibility means rewrite of the old code. Rewrite costs money and does not add any product value (that you can explain to the management). In practice it will mean sticking with the old compiler and not getting any new shiny stuff at all, or, with sufficient demand, appearance of non-standard backward compatible compilers.
People who were using `gets` said the same thing.
&gt; Rewrite costs money and does not add any product value (that you can explain to the management) The fact that you think a break in compatibility requires a rewrite tells me a bit about why you'd have trouble communicating to your manager.
Just checked my logs - the error I got was a run-time error, when calling a function that uses `std::optional`, I got: &gt; "Microsoft C++ exception: std::bad_optional_access at memory location 0x00000097C08CAFE8." The scenario was that I compiled a library into a .lib file with `/std:c++17`, and then had another project, a Unreal Engine 4 project, that linked against this lib and used a function from it. Unfortunately I can't dig further into this as I don't have it set up locally anymore. Maybe it was just a mistake on my side. In any case UE4 was such a big pain... primarily their build system...
I'm not 100% sure what you're talking about; this is about a numerical type representing a real number with an internal representation that uses the requested radix to store the digits. 
it's not about chromium having breaking changes, it's about the foundation that it's relying on (c++ stdlib in this context). 
Why would you want that over a scaled/fixed point implementation like the one I proposed?
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;My, how fiendishly clever! I never thought one could exploit templates and function overloading quite like that; neat! 
Because neither 1/10 nor 1/3 are exactly representable in binary
i'm taking the project very seriously, but the lack of knowledge has certainly caused me some pain. on the upside -- i'm learning a lot :) on the off chance, do you have any good resources on the topic? i'm curious how other people approach this but i'm having trouble finding articles. 
We need "= delete if constexpr " for special member functions in the language :P 
&gt; smartest &gt; very familiar w/ the hardware they develop on From my experience in gamedev, hands-down the latter is true, but... do you really think those two things are so closely related? Many of the smartest engineers I've worked with care about their hardware only as much as is strictly required for the long-term performance of their job.
That's mandated to be thrown when you call `value()` on an empty `optional`. Sounds like you just had a bug in your code.
We have concepts. That allows you to just do: template &lt;typename T&gt; struct optional { optional(optional const&amp; rhs) requires std::is_copy_constructible_v&lt;T&gt; { ... } }; You don't need the `= delete`, since there just won't be another copy constructor. 
To be honest, the std::shuffle overload function from the Library Fundamentals TS v2 *could* have been considered for inclusion in the IS before totally getting rid of std::random_shuffle. It is more beginner-friendly than the current std::shuffle and guaranteed to be thread-safe, which is generally more than enough for small test programs (the ones you write N times a day to check semi-simple stuff before deleting them).
I didn't know that you can specialize common_type for your own types.
[removed]
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
EASTL was a good thing, and had increased stl in gamedev usage. There is nothing magical about the STL though, and even today I can see instances where someone might want finer control over implementation details in a container, etc.
We're dealing with a ton of that right now on a codebase we inhereted - the previous company let loose their new grads and interns to add a ton of cool stuff they must have recently heard about... which didn't solve any existing problems and added no meaningful value but did add a metric crap-ton of interlocking dependent code.
`s/default_deleter/default_delete/` EBO is described incorrectly in the slide. You can't "construct [an empty object] whenever you need it" because you don't know how to construct one. `common_type_t&lt;T&gt;` is not identity. It's `decay` if not specialized. Template argument lists like`is_same_v&lt;iterator_traits&lt;Iter&gt;::iterator_category, ...&gt;` is not a context where `typename` can be omitted. 
Is it different from the one done at CppCon that had the same title?
You can't construct something that has no state because it makes no sense. There's no way for a constructor to do anything outside of messing with globals.
You forgot enterprise, where the legacy grows faster than ever. 
I currently need to write an interpreter at work for a custom scripting language we defined. I am using boost spirit as a base and have gotten it working but I feel like it’s messy. Would you say this would be easier to implement than using boost spirit? I need something that is allowed in commercial applications. 
Well I think it would be lying if I said I understood everything, I remember mostly that this shit is much harder than it looks. Far from a solved problem, lots of compromises, etc.
I think the takeaway is that we could make it much easier with reflection or a dedicated language feature.
The syntax is correct, that's why it compiles. But this is a good example of undefined behavior. 
What are libclang and clangd?
&gt; But this is a good example of undefined behavior. What's the undefined behaviour here? Isn't this just equivalent to: if (a &lt; 100) { a = 0; } else { a = 1; }
 if (a &lt; 100) what is the value of `a` here?
Variables are actually in scope in their initialization. If you attempt to use the value before the initialization is complete you get undefined behavior of course. But being in scope allows you to take their address, which is okay, or to use them in unevaluated contexts (like `sizeof` or `decltype`). I recall using this with `decltype` once but can't remember what it was good for. What you _can't_ do is auto a = 100 &gt; a ? 0 : 1; ---- error: variable 'a' declared with deduced type 'auto' cannot appear in its own initializer
I think it's pretty clear that -Werror doesn't count for answering a "Will it compile?" trivia question.
It's harder to implement stuff when you need the committee to agree on how.
`scope_success` and `scope_failure` relies on exception count of *current thread* to distinguish stack unwinding by exception from "success" path. It assumes that scope is executed *fully* on *one* thread (without hopping between threads or breaking up scope in smaller part). Coroutines provide a way to execute part of the scope, pause and then resume scope execution later (and maybe even on different thread). It is obvious that this two things is incompatible even without early destruction of coroutines. IMO if we try to enable `scope_success` and `scope_failure` in coroutines we should make a step back and consider rebuilding `scope_success` and `scope_failure` from the ground up. `scope_success` and `scope_failure` require a language support. Now this support is provided through `std::unhandled_exceptions` which does not make sense in coroutones. IMO unhandled exception count is the root of the problem and third reason for stack unwinding is not. Maybe we need more direct solution to scope_success/failure guards in the language?
No, it doesn't compile. `a` is not declared. A better equivalent (and equivalently UB): if (int a; a &lt; 100)
Not even indeterminate value. `a` is not declared...
Regarding [`common_type`](http://eel.is/c++draft/meta.trans.other#3.2): &gt; If sizeof...(T) is one, let T0 denote the sole type constituting the pack T. The member typedef-name type shall denote the same type, if any, as common_­type_­t&lt;T0, T0&gt;; otherwise there shall be no member type.
I didn't mean to imply that I respected them so much just for their knowledge of the hardware. That was just a part of it, they knew that much because it was part of their job. These guys work on massive projects touched by tons of different engineers over the project lifetime and need to do so without breaking anything existing. On top of that, the ones I respected really cared about the product and would go out of their way to find and fix things. Since the projects needed to remain maintainable, there was a sort of discipline I picked up when it came to implementing features or fixing bugs. You needed to be able to justify every single change you made and the side affects it might cause. Sitting down and analyzing metrics frame by frame, to identify where and why millisecond hitches were happening and being able to explain exactly how and why your fix worked was required. At the time, I might fumble around until I found a symptom of a bug and patch that but that wasn't good enough. I needed to explain exactly why that symptom was there, when it started showing up, what caused it and then justify my fix. It taught me discipline and the guys I respected could do it all in a fraction of the time it took me. They almost never made a mistake, it was eye opening.
In a nutshell, they let you state the syntactic (not semantic) requirements of the types you use in a template. For example, `std::sort` requires its iterators to be random-access iterators. This provides an easy means to define what it means for a type to be a random-access iterator and then `std::sort` can require this and have the compiler check each requirement when the function is given an iterator type. If you've specified preconditions before, it's like those for types.
C != C++, so you cannot "Go ahead and write C!" Try casting to a void* in C++, and you'll see what I mean.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
I am aware of that, but you can write mostly C style if you want to.
&gt; EBO is described incorrectly in the slide. You can't "construct [an empty object] whenever you need it" because you don't know how to construct one. I saw a need hack somewhere that leverages the fact that empty objects have standard layout and the common initial sequence rules of union to get an object of an empty type without invoking the constructor.
Theres no intent for optimizations to break your broken code either, so whats your point? optimizations let your code be faster(more cars). If those extra cars happen to hit you, thats your fault for being in the street.
"If it's not implemented yet, it never will be" - great logic there. 
You could encounter that in just about any code, or find the opposite . I've encountered containers that were superior to the STL equivalent ( super configurable, order of magnitude better to debug, move support before the language had it, etc ) and those that had everyone going "oh hell no!" from the moment they saw them. :)
With Xcode 9 you can use c++ 17 (-std=c++17), but as mentioned elsewhere, optional is missing value();
Structured bindings work. 
Hmm, right. Yes, that's certainly not impossible. It is weird though, since when I switched to Akrzemi1/Optional, it worked right away - without changing any of the underlying code or algorithm - solely by changing the optional type. So it's weird that there would be an empty `optional` in one case but not in the other. Anyway, it's very helpful to know that mixing 14 and 17 TUs is supported, including mixing the two standard library versions. Thank you very much! :-)
&gt;It looks like a conscious decision of a satirical title Ironic shitposting is *still* shitposting.
Satire is not just irony, and it's definitely not shitposting.
&gt; Right. Setting up a CI server might be the next good thing to do. That should make sure that stuff is functional across different platforms. Setting up a CI is annoying, but proves invaluable in the long run. &gt; What do you mean by that? cxxd and cxxd-vim are released under GPLv3 license. That would really put people off? You have no idea. Even linux folks aren't as fond of GPL as you think. I have personally been bitten by GPL3 being too restrictive, so many don't like GPL and *especially* GPL3. &gt; Sure it can ... One can write even something which is totally unrelated to the any compiler flag existing. True, but the server should be capable of handling the same flags the project uses for compiling, thus things get hairy. &gt; I think bigger responsibility here is on the user side and main focus is really on the auto-generated compile_commands.json file Coming from ycmd, which had to implement something before compilation databases even existed, I see it differently. Ycmd doesn't have plain text as a fall back - it has "extra conf" written in python that can be much more powerful than the compilation database. Therefore, both options (compilation database and an alternative) need just as much care. &gt; you can start with losing your hair ... I understand. I guess I've been lucky that I've become a part of ycmd team *aftee* they have dealt with the compatibility nightmare. &gt; I wish I haven't started with Python at all but I should have stayed with C++ ... I guess I wanted to dig more into another programming language. Learning a new language would hardly be a waste of time. &gt; I would make compile_commands.json more recommended way Fair enough. I'm still having the point of view where the alternative to the compilation database is potentially more powerful than the database. &gt; It doesn't seem fair at all and totally demotivating. I actually know what you mean. YCM receives a lot of hate on #vim, because people don't always read the docs and then go on random places to bother people who are not maintainers and what do we get at the end? YCM on the list of softwares #vim loathes. &gt; I believe this is a feature that would enable a bigger impact on the size of your users. That's true, but... Which library should be used for python bindings? Boost.Python? Pybind11? It would force a dependency on everyone and potentially could cause compatibility issues when the sanitiser uses one library for the bindings and the downstream project uses another. &gt; My suggestion would be to make distinctions between different toolchains be explicit in the code. It will make it easier to extend the functionality to other toolchains you want to support. There's not much point for that. The sanitiser is still preparing the flags for libclang. Gcc/clang support overlaps completely and the MSVC still requires the same sanitising.
&gt; Any way, there's enough stuff missing that it's completely useless. That's just not true. Those missing things can be backported, as long as the compiler itself is solid. And I take a compliant compiler with some experimental parts of libc++ missing to non compliant compiler any day. You can also build your own version of libstdc++ if you're not happy with the provided one.
It's better to pick up an area of what you like and see into it. It will keep you involved in learning and analyzing it, cause you could really use it. There is no point to study an abstract structure that does crazy stuff that you won't even understand but other people think it's a masterpiece. Pick one. Stick to it. Spend some time on it and after that you'll see what's good and bad about it that particular context. Remember there are many contexts and what considered good in one of them would be terrible in others.
You're talking syntax. What I'm simply stating is that C is not C++. I recently started a C-project, after doing C++ for 5/6 years. I was rather surprised of all the stuff that popped up, being different from C++, the void* cast was just one of them.
I completely agree with you. The STL is not the be all and end all. Especially when we start talking about specific implementations. I've found game development to be pretty good about using custom data structures when there is a performance advantage to doing so. Where I get suspicious is where some wants a bog standard double linked list and doesn't want to give it the same interface as std::list. Also it's really nice when the custom data structure gives you iterators which inherit from the std random/forward/whatever iterator types.
Apple usually updates c++ toolchain with new XCode versions, announced at WWDC. So it's extremely likely that update is coming in June (or later beta versions). I agree that Apple is focusing on C++ way less than Microsoft does lately, but than again, Microsoft does has have a lot to make up for. And while Apple is somewhat neglecting c++, they're definitely not abandoning it - i.e. c++ refactoring introduced in Xcode 9, recent announcement that they plan to use clangd for tooling, etc. Lot of Apple frameworks are build in c++ even if exposed api is ObjectiveC (CoreAnimation, AVFoundation, etc).
We're dealing with the exact opposite issue. Our codebase uses `std::string` and `int` for multiple ID types - strong typedefs would improve safety and readability immensely and also allow easy overloading/metaprogramming
\&gt; Apple usually updates c\+\+ toolchain with new XCode versions More or less nothing has happened from 9.0 to 9.3. If not nothing at all. And just updating the C\+\+ toolchain every 1.5 years \(at best\) is just not acceptable anymore nowadays. Btw. I think in some other /r/cpp comment recently someone posted that Apple pulled most of its employees from working on the clang/LLVM C\+\+ side, their contributions went down massively.
I'd like the language allow an optional `unwind_reason ` param in destructor, e.g. enum class unwind_reason { normal, exceptional, forced }; ~Transaction(std::unwind_reason reason) { if (reason != unwind_reason::normal) Rollback(); }
I think the previous comment meant representation like n * (1/30) where n in binary number. 
Having written rather too much financial software I can say for sure that fixed point is far more popular than floating. In particular, the issue with floating point, decimal or not, is that if you exceed your precision in intermediate calculations, then it will just throw away the right-most digits silently, and you'll never know that you were off by a few cents - and it's extremely hard to detect that this will happen. If you use fixed point numbers, it's significantly easier to detect that your next operation will overflow - more, if you do overflow, most of the errors will be gross mistakes (often adding two positives and getting a negative) which hopefully your other safeguards will catch.
&gt;If you didn't even make the effort to try to distinguish it, that's your problem. So, what was the clue? If buzzfeed started saying that their titles are actually satire, would that suddenly be not clickbait-anymore? &gt;that title was just poking fun at those kind of titles Well it's failing hard.
The clue is that it's the title of a talk about a programming language, and not a chopped up list article. That's a pretty big clue. I think it's clear it's you who is failing basic deductive reasoning from simple observations.
I guess this would be a prime example for where meta-classes would make sense.
What do you think, would decimal_for_cpp be too simpistic for financial stuff? https://github.com/vpiotr/decimal_for_cpp "Values are stored internally using 64-bit integer, so maximum number of digits is 18. Precision is user-defined, so you can use this data type for currency rates."
That's not interesting to anyone but you - save it for your diary. 
**Company**: Epic Games Montreal **Type**: Full time **Description**: For over 25 years, Epic Games has been making award winning games and game engine technology that empowers others to make visually stunning games and 3D content that brings environments to life like never before. Epic’s award\-winning Unreal Engine technology not only provides game developers the ability to build high\-fidelity, interactive experiences for PC, console, mobile, and VR, it is also a tool being embraced by content creators across a variety of industries such as media and entertainment, automotive, and architectural design. As we continue to build our Engine technology and develop remarkable games, we strive to build teams of world\-class talent. Epic Games Montreal focuses on the Enterprise side of Unreal Engine. We develop tools, pipelines and special projects for many industries: movies, television, architecture, automobile, aeronautics, ... We are growing fast and we need to hire strong C\+\+ devs, tech artists &amp; QA analysts. If you are passionate about rendering, tools, pipelines, and game engines, you won't find a better place! We have many interesting projects in * Virtual Production pipelines for broadcast and movies \(compositing, keying, tracking, previz, ...\) * Conversion and optimization framework for CAD and DCC models * Distributed multi screen powerwalls and caves * Augmented and virtual reality * ... The **ideal** candidate will have a mix of the qualifications below: * Excellent C\+\+ skills * Experience in many fields of game engine technologies \(core, tools, rendering, ...\) on AAA games * Excellent optimization and debugging skills * Well versed in software engineering principles * Experience writing UIs with a toolkit such as WPF, QT, Slate \(UE4 UI system\) * Familiarity with multiple platforms and/or Unreal Engine 4 is a plus * Experience developing in a large codebase; ideally experience designing and implementing engine features from the ground up * Demonstrated ability to communicate fluently in French and English \(written and verbal\) **Location**: We are currently located near Parc Jarry in Montreal but will soon move to our brand new offices near McGill station in downtown Montreal. **Technologies**: We develop for Windows, Mac, Linux, Android &amp; iOS using C\+\+11. We develop some tools in Python. **Contact**: See all our current openings [here](https://epicgames.avature.net/careers/SearchJobs?3_5_3=1479). Please apply thru our website. I can answer questions here on reddit.
And only breaks that don't cause subtle bugs which could very well kill someone or cause some sort of disaster. Such as, not changing integer division to result in floating point. _shakes head_. If its something which will cause a compilation error that's a better place to start. 
Wouldn't this be better suited for a lint tool rather than inside compilers?
Here i'm suggesting that eg, within that scope signed overflow would become well defined, which would definitely need to live within a compiler
The real question is: can we generate a better pun on std future?
The empty object needs to be trivially constructible for that to be an actual instance of the object. Otherwise it just looks like UB with some wallpaper on it.
Isn't common initial sequence a pure c-thing?
[Nope](http://eel.is/c++draft/class.mem#21).
Speed is relative, but not having to parse text is absolute :-).
Well, not really: If you have a union of type `T` and `U` where `T` and `U` are standard layout and `T` is currently the active member of the union, [class.mem]/24 allows reading members of `U` as long as the members are port of the "common initial sequence". The "common initial sequence" is defined as sequence of members where they have pair-wise layout-compatible types. Two types are layout compatible if they are - among other things - layout compatible classes, which, in turn are classes where the common initial sequence compromises all members. This means this is legal: struct foo { int i; foo(int i) : i(i) { std::cout &lt;&lt; i &lt;&lt; '\n'; } }; // non trivial constructor struct bar { int j; }; union { foo f; bar b; } u{bar{42}}; u.f.i; // common initial sequence Now if we assume that `struct empty {};` is layout compatible with any other empty type, we can do the same trick.
Couldn't this be mostly accomplished with template wizardry? Something along the lines of: `using PersonId = newtype&lt;int, disable_arithmetic, disable_math, comparison&gt;;` Where it'd generate a struct which had mirrored all of the operators then disables them, and provides an `operator T` to convert back, and adds some custom comparison operators? Might be a way to do it nicely and succinct enough you could use it without much hassle. 
I thought the point was that not _all_ empty objects are standard-layout, but I could be wrong.
Dear Diary, Today a person on reddit took time out of their extremely busy and interesting life to reprimand me for making a one sentence comment over a week ago that didn't interest them. I was totally crushed. I can't imagine what an exciting life they must lead that such a tiny inconvenience required such a stinging rebuke. Also, Dirk asked me to go to the prom with him! 
Well they are actually working on it and started a TS now.
But the permission to read members is useless here because an empty class has no members.
Yes
I hate code-related videos. Everything is a video these days.
I'd rather have `#include_once`, or `#import`. That way, it avoids file io.
Just go with it, we use it extensively. My disk hasn't been formatted from UB yet.
There have been standard proposals, even recently, to have #pragma once in the standard in some form but they have been rejected. Apparently specifying the specifics in term of the abstract C++ machine is too hard, and modules are on their way so the committee doesn't want to delve into the details of a feature that will mostly be unneeded once modules are there. In the meantime you can safely use it in more places than recent standardised features, so it doesn't really matter much.
Yes, I think so, so is #prama once. That's why /u/Ameisen said #include_once, which is used in the "host" file (which uses the include file).
But can it compile range-v3? :)
Every compiler implements pragma once in a subtly different way. It turns out that determining the identity of a header is tricky. One compiler identifies whether two headers are "the same" by comparing the path. Another compiler identifies whether two headers are "the same" by comparing the file inode. Yet another compiler identifies whether two headers are "the same" by comparing the file contents. All of these approaches have flaws. If you want this standardized, you'd have to "pick a winner" or make up something new.
I generally learn by imagining what I want, then looking at the code. LLVM comes with some documentation. The rest is pretty much playing with handwritten IR and getting it to do what you want, then figuring out how to get the whole stack do it. I'd suggest writing some IR that calls the sin function, and then linking it to the address of the C function in the runtime library (literally `&amp;sin`) - it should work. For C functions, you could reuse the host's runtime completely.
That was literally released a decade ago. Also, earlier versions supported it but gave a warning.
Interesting, how did you check? Or just file cache takes effect?
Interesting, how did you check? Or just file cache takes effect?
It's now indeed basically everywhere, but rather recently for some compiler: `#pragma once`was added in C++Builder (ex-Borland) in their 2010 release; and some people are likely stuck with a previous version. That's said, it's likely the #1 exception to the "Prefer using the standard" rule.
I asked one of the Standard committee members about that. The answer was twofold: * Yes, the basic implementation is present in every frontend and works (based on the file path, probably) * However, it is impossible to guarantee the correctness of the feature when the underlying filesystem has special features such as symlinks and hardlinks (the same file is reachable via several distinct paths that)
Cool talk! Are the slides available somewhere?
MSVC &gt; Apple. That was the only point I was trying to make as far as the current state of affairs with standards compliance and the Apple armchair programmers disagreed.
Would the standard be required to specify how the compiler identifies that headers are the same? What if they just specified the expected behavior when using #pragma once, without the implementation details. As long as all the compilers have some common ground of the end result, wouldn't that work?
You wouldn't store individual digits, though. This isn't like binary coded decimal. You store a sign, mantissa and exponent just like you normally would for floating point representations. In traditional floating point, the exponent is presumed to have a base of 2. With decimal floating point, the base would be 10. This obviously requires rekajiggering the floating point arithmetic that's used, but that's how you do it. For fixed point, which is what ReversedGif is actually talking about, you have a defined quanta, and you can represent any number that is a multiple of that quanta (barring overflow). The quanta could be anything: you just define it to be so. The only issue comes in when you convert it to a native type, but that's not usually necessary.
If I understood correctly, it's only for backward compatibility to include non-module based files.
The results differ based on which one you choose. You could contrive situations where you'll get errors with any one of these cases but not in another one.
Not everybody has the resources to do that and the fact they does not invalidate their criticisms.
I just create a `template &lt;typename T, typename Tag&gt; class Id;` and then instantiate it with: using PersonId = Id&lt;int, struct PersonTag&gt;; And I'm done.
So just continue to use it. What's the point of it being in the standard then?
That isn't a criticism, though. &gt; I only fear that error message will probably be crap This isn't even based on actual data, just pessimistic speculation. 
Then address it as such :)
We were examining the effects of external guards (surrounding the #include), but some compilers document how they work, too: [clang](http://clang.llvm.org/docs/InternalsManual.html#multipleincludeopt), [gcc](https://gcc.gnu.org/onlinedocs/gcc-2.95.3/cpp_1.html#SEC8)
That's why my reply was structured like that. Baseless speculation isn't constructive. So many of their comments that I see take a dump on C++ without offering anything constructive. This one finally pushed me to offer a way to get something constructive out of it. This topic is especially sensitive considering [past interactions with Andrew Sutton](https://www.reddit.com/r/cpp/comments/7jxq8r/does_anybody_know_why_requires_requires_was_not/drdj3y3/?context=10000), the person behind the TS and GCC implementation. When I see a bunch of unconstructive ranty posts and comments, then this, then a bunch more, there's only so much I can pass by before I say something.
Still clunky compared to something the language could have directly built into it is where I was going with that thought. It probably show that I sometimes work with code older than half of the posters here... :)
https://en.wikipedia.org/wiki/Pragma_once#Portability
But that wasn't what we were talking about there. When you're laying out exact address ranges that data get's DMA'd into and out of and someone adds code without even understanding what it generates, links in, and dynamically accesses, and then doubles down on how "correct" they are and how everyone else isn't... A lot of the newer/younger people we encounter today when interviewing seem afraid of raw memory or understanding how the hardware truly works. For a lot of jobs, it's probably better that they don't need to know, but for some things that level of depth is required. 
Good to know the documents, thanks.
Knowing that it'll always be there helps some.
Some of them, e.g. hashing/equality for maps.
One thing to consider is the AlienTube extension, which shows Reddit comments on the YouTube page. It's handy having each thread on its respective video page instead of having the megathread be inaccessible from YouTube.
My first impressions (sorry if they're overly critical): Why would I do these instead of hackerrank questions? You only have very easy and easy questions. None of the questions build on C++ concepts (unless I missed them) like pointers, typedef, etc I like the premise, I want to practice my c++ now that don't use it daily anymore. But your offering of assignments is for things I'd never use c++ for in the first place. 
I see a lot of the challenges as being good use cases for ranges which, I believe, makes a better choice than if you were to use raw pointers or even the standard library by itself. (not to be snarky) I would definitely go through these to familiarize myself with rangev3 or similar libraries.
I assume you are talking about class statics. Yes. They work with everything. They use the same underlying mechanism that statics of class templates use.
Can a kind soul list the 10 things? I started watching, I saw the first trick was EBCO, stopped watching. Respectfully, I hardly think that this is a trick that only library implementors know, so I'm curious to know what the rest are.
&gt; So just continue to use it. What's the point of it being in the standard then? because every other year some people send pull requests to clang or GCC to remove them because it's not in the standard. It was almost removed from GCC once.
Only until it doesn't work because "implementation defined" (for everything, including implementation and what it does), and now it's no better than it was. Only more people relying (because now it's in the standard) on it reducing the portability.
I've hit the problem once, with a custom build system that symlinks the header files into the separate include directory, and thus each header file becomes accessible through two separate paths.
Seriously? I surprised they haven't went completely closed-source/subscription-based due to the massive amounts of non-standard extensions those compilers already have! They must receive thousands of PRs to remove those (though surprisingly the number is only increasing)! I can't even imagine the pressure they are under!
Nah. Separate posts make searching (and commenting) nicer. One of the mods adding a flair that can be filtered out like they did for cppcon videos, now...
Yep, I really have **no-- interest in doing these sort of things in C++ unless I can use C++14+ + range-v3 (+ Boost for some things).
The "obvious" choice seems to be comparing file content. Its doable on all plateform without any issue (except maybe differing end of lines? -- at this point whatever) Of course some code bases developed by complete psychopaths will break, except the feature would not have the syntax pragma, so *no* code base will actually break. So at this point this is only looking for excuses to not have it standardized. And the module excuse is *very* quite poor one actually: usually what is practice should be preferred to a superior but mostly theoretical solution. 
Our local ACCU group is doing an ACCU 2018 retrospective next week, I'll see if I can post a summary...
Not to detract from your work at all, but I think it worth mentioning that Boost.Test has been significantly lighter-weight since its major overhaul in Boost 1.59. (And don't forget [Boost.Core.LightweightTest](https://www.boost.org/doc/libs/release/libs/core/doc/html/core/lightweight_test.html) – 'lightweight' here is not an understatement.)
It helps me to understand how is it spread the data, what makes me feel I am not going to do mistakes. Is the 'something is going on indicator' described [johannes1971] what express exactly this, he even smelled I avoid references just for being able to visualize `&amp;`. I do not know if this makes sense, but maybe could be said are two techniques? one decide what to feed the functions, and the other are the functions what decides how to get feed? I also have tendency to redesign the data flow when I am able to see some kind of possible gain (mostly runtime speed, some times trying to simplify), and I detect it when I see patterns in the syntax indicators. I solve it creating the data closer to the root of the tree of functions calls, or modifying or creating new structures for to make the data more correlative/consecutive, etc. Things that emerges with code grow time. Probably I can modify or make a plugin for interact with the static analyzer for visualize deeper unrolling information at first sight in the editor, and see how it goes. 
xlc didn't have it for a long time, did they add it?
That struct Ovld variadic template aggregate base class thing doesn't seem to work on MSVC yet
http://pubs.cray.com/content/S-2179/8.6/cray-c-and-c++-reference-manual-s-2179-86/miscellaneous-directives
&gt;\[Why testing self\-driving cars in SF is challenging but necessary\] \( &gt; &gt;https://medium.com/kylevogt/why\-testing\-self\-driving\-cars\-in\-sf\-is\-challenging\-but\-necessary\-1f3f7ccd08db &gt; &gt;\) \&gt; **The author deleted this Medium story** :\-\(
Have you yried tye latest preview? I think that added supoort for class template argument deduction and parameter packs for usibg which are the 2 tgibgs necessary for the overload trick.
Yeah. Saw "return last element of array," was like "it's gonna be a C array." Yep, it was a C array. Get lost with that shit. A C++ version could be something like template&lt;typename Container&gt; decltype(auto) get_last_element(Container &amp;&amp; c) { return *std::prev(std::end(c)); }
True. But it won't suddenly all go away if the standard just added `#pragma once - may do an implementation defined thing in an implementation defined way`.
All the major compilers read the include file only the first time, both in the case of `#pragma once` and include guards. 
I don't think anybody is asking for standardizing unspecified behavior.
`#pragma` already means that.
Is it as lightweight as this unnamed test framework from Type.Erasure, which consists of only 21 lines? https://github.com/boost-experimental/te/blob/master/test/common/test.hpp And here's how it's used: https://github.com/boost-experimental/te/blob/master/test/te.cpp 
And that’s OK. Those of us who keep are file layout sane can use it. Those who don’t can use include guards. (Which means if you make a library, you’ll have to use include guards since you can’t dictate what your users do.) It would still be good if it were in the standard &amp; the limitations spelled out.
The thing I find most annoying about that second point is that this could easily be circumvented by an optional identifyer #pragma once MY_LIB_HEADER_NAME In the mythical 99% of the cases, `#pragma once` should work just fine (it is not like this hasn't been used for decades in huge production code based without issues, not to mention the millions small and/or teaching projects out there) and in the remaining 1%can use the explicit identifyer.
Watcom supports it. [cguide.pdf, 32-bit pragmas, pg 218]
`priority_queue` was listed in the "new containers" list when talking about the EASTL around 9 minutes, but doesn't that [already exist](http://en.cppreference.com/w/cpp/container/priority_queue) in the Standard Library?
I was replying to the post that essentially boiled down to "I don't care about all the different existing implementations, it works for my project in the compilers that I use!".
The challenge list is hard to look at. Having the problem list consist of just the title, or the title and a specifically created subtitle (for explanation) could improve it.
If you have to do that, you've already lost most of the value of `#pragma once`. Those 1% that can't use `#pragma once` should probably just use include guards.
Unfortunately, as much as the situation you've described sounds absurd, it is becoming ever more common. In my experience, I would say that attention-seeking blog writers on the C++ community are the main culprits with this disconnection between actual code and pie-in-the-sky coding style. Recently there was a blog post that suggested that trailing return types is the true and pure syntax. This kind of non-objective analysis is seen over and over again, while they all pat themselves in their backs while writing their next C++ talk. Take for instance pointers... There a stupid presentation that goes around in this message board called something like "Never use pointers". Unfortunately, despite of the good intentions of that presentation, it causes more harm than good, because it tries to dissuade people from writing pointer spaghetti code, without really explaining what would be the valid cases for the use of a raw pointer. It is the absolute stance that doesn't hold a ground on reality. I lead a team, and we have a few policies that every now and then are attacked by this kind of "wisdom". For instance, we by default disallow copy and assignment on types, turning them on as we need them. Every now and then, someone comes along and says "if you are doing this, you are not doing correct type semantics". I not even know where to start with this kind of jerk. 
That's a reasonable point. Consider writing a WG-21 proposal: https://isocpp.org/std/submit-a-proposal
Unfortunately that does not work as the component author has no way of knowing that his module/component has been picked up by several other users in the uber build tree and the build folks are playing tricks in order to solve some unrelated issues...
I disagree. `#pragma once` adds no benefit other than being less verbose. Include guards, on the other hand, have precisely defined semantics, as opposed to `#pragma once`.
The committee still doesn't agree about exporting macros, which might be a blocking point, but that's pretty much the only one. I don't think that the committee will want to work on #pragma once to solve it considering that macros are often considered evil.
The main benefit is that you don't have to maintain a unique symbol for each header. When I converted some large, old codebases to `#pragma once`, I discovered a few copy/paste errors in the include guards. Updating the guard is extra work to do whenever you rename a file, and people sometimes forget to do it.
Agreed. When *using* c++, I would include all the basic libraries like ranges v3 and Boost, but also debugging techniques and common errors. But when I'm evaluating something to help me practice c++, I want to practice common C++ problems, not how to write a function that adds two numbers together. For OP, some other suggestions: I want to see the assignment when I'm writing code. There's no reason not to have that available. I answered one of the questions and it gave me a clang error. Your error messages need to be better (clearer), your environment needs to be more robust (I shouldn't be seeing your server's directory tree, nor should I have anywhere near this kind of access to it).
Even if they did, modern OS's cache the file in VM.
&gt; the input data has to be regular and valid So it's a [spherical cow](https://en.wikipedia.org/wiki/Spherical_cow) solution. 
Comparing file content seems like the worst choice to me. Imagine accidentally getting into a situation where you have two copies of the same file and they're both being included. Initially, it compiles and links fine because they are the same content, so the new guard works no problem). Later on, you change some whitespace in one copy of the file (maybe you realize there's a trailing space on a line that didn't need to be there). Now all of the sudden you're getting multiple definition errors at link time. It would be confusing as hell to change whitespace in a file and have that break a C++ build, but that's what would happen. The path and inode solutions would break when you want them (as soon as possible after the actual problem is introduced) by giving you the multiple definition errors as soon as the copy pasted file is included. However, using the path wouldn't work correctly with symlinks. Using inodes seems like the "most correct" thing to me, since that's closest to actually identifying a file, but that can't really be standardized since inodes are not a universal thing for all file systems. I'd guess nuances like that are why there really is no "obvious" choice, and is probably why it hasn't been standardized.
Not having to use the `#endif` at the end of your file is nice.
How about you compare them with whitespace removed? It's trivial to remove whitespace.
&gt; It is objectively and undeniably better than include guards. Well, we measured our build at work both ways. #pragmas was significantly slower, so we switched back to include guards. This was a couple years ago, maybe things have changed.
And, includes will still be used anyway for libs that "export" macros or for source code configuration.
Spherical... cows?
I'm so glad you're here to punish people! What would we do without you? 
Anyone remember when gcc spent years complaining about #pragma once and threatening to remove it real soon now? It was more than a decade ago but it's one of the reasons I don't like gcc to this day.
Bizarre. What sort of implementation would have #pragma once *slower* than #include guards?
have you heard of [doctest](https://github.com/onqtam/doctest) - "The fastest feature-rich C++98/C++11 single-header testing framework"?
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8dkydt/boost_spirit_parser_questions/dxo23kv/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
No. We talk about this every year. If I feel inspired tomorrow, I'll add flair and a filter for ACCU.
Oh, great! Another community raising awareness for diversity. So needed and so helpful! Thanks!
This is awesome; thanks for this!
Oh I definitely agree. I really enjoy taking advantage of types and typeclasses when using haskell. It would be cool to see how far you could go with just templates, then it could inform empirically what would be a good design for language level. It sounds like also an exercise in madness to make it work well too though haha. 
Of density 1 
It actually doesn't include them by default - the [benchmarks page](https://github.com/onqtam/doctest/blob/master/doc/markdown/benchmarks.md) is a testament for that. ```&lt;iosfwd&gt;``` is included only in the case of clang with libc++. 
In vacuum
Ah, sorry about that! It looks like we just [changed the link](https://medium.com/kylevogt/why-testing-self-driving-cars-in-sf-is-challenging-but-necessary-77dbe8345927). 
Everybody keeps saying you should use them in classes, but why wouldn't you use inline global variables? It saves a double declaration (once normal, once extern). 
It isn't a macro - in fact it would be a replacement for a macro.
&gt; you can't create a function with initializer syntax without that template&lt;class T, std::size_t N&gt; T max_(const T (&amp;arr)[N]); auto v = max_({1,2}); :)
Here is a great SO [answer](https://stackoverflow.com/a/34884735/797744) on why you should never use `#pragma once`:
You might consider a minor change: #ifndef H_$uuid #define H_$uuid #endif // defined H_$uuid It makes it easier to know what that trailing #endif is in reference to down the road when the header file gets more complicated.
Pretty much nothing. I tested it a while back and separating the common path into separate file has almost no impact. OTOH if you have CATCH_CONFIG_MAIN in every cpp file, you are doing it really, really wrong.
It was a part of FoundationDB, so you'll find it in its github: https://github.com/apple/foundationdb/tree/master/flow
Thank you!
Thanks. I didn't look carefully at the code. It seems that they [use a simple C# transpiler to modify CFG](https://github.com/apple/foundationdb/blob/master/flow/actorcompiler/ActorCompiler.cs#L138) and implement the major parts in C++. 
Done.
Ah, sorry, I misunderstood you. The counterargument would be that it will take quite some time until we live in a modules world. But I agree, that the committee will most likely just say "Sorry, not an issue worth fixing anymore".
Well, it's more that they won't want to take committee time for #pragma once that they could use for modules while they can trust the compilers to still provide a good enough solution in the meantime :)
I use auto-generated random strings with at least 128 bits of entropy. Why code the filename into the `#define`? It just leads to the problems you describe.
That's exactly what is being presented in the talk...
Yes, as I said, less verbose.
Very curious what you mean by Id&lt;T&gt;, how do implement this? I'd be very grateful for an example. I'd like to do something similar in our codebase
Would you mind sharing an example of how you would use this? I'd like to use more strongly typed identifiers
And trigraphs!
Looking up header files based on an `#include "string"` is an inherently implementation defined process. I'm not really sure
For anyone that hasn't read the article: this quote comes from the author talking about the problem with his earlier solution. This article is literally about trying to add validation while maintaining good performance.
It's better if commentary is kept per thread, per video
C\+\+ implementations have lots of \*de facto\* standards not standardised by the C\+\+ standard, for various reasons. A lot of technically UB is implemented consistently across all the major compilers, for example. And I'm fine with that. De facto standards in many ways are least priority for the C\+\+ committee. They cause least trouble for end users. The committee should focus on the stuff which cause headaches and problems for the majority of C\+\+ users. pragma once is not one of those.
The gist of it is : template&lt;typename T&gt; struct Id { public: explicit Id(int); // your basic opaque typedef implementation bool operator==(Id&lt;T&gt; other) const; // etc... private: int m_val{}; }; template&lt;typename T&gt; class Identifiable { public: Identifiable(Id&lt;T&gt;); Id&lt;T&gt; id() const; private: Id&lt;T&gt; m_id; }; and then class MyDomainObject : Identifiable&lt;MyObject&gt; { }; 
[Already posted](https://www.reddit.com/r/cpp/comments/8d5pu9/the_badlands_of_c_professional_game_development/), no karma for your own video. ;-]
Really interesting article (still reading), but in the real world that use case is I/O bound...
yes, if you have that kind of control over the format of your input you might as well be using a binary format that wouldn't require any parsing
Where I work, our data format is universally .tsv so that data can be examined in e.g. Excel. When half your team is data scientists, parsing known-valid-data is just reality.
I'm glad people liked it because it was challenging to do. I had another talk planned for the conference that didn't make it, so I foolishy submitted this one as a 20 min short one. I had to cut down significantly from more material I had. Like the disclamer says you can see the more in\-depth one at [https://skillsmatter.com/skillscasts/10231\-c\-plus\-plus\-july\-meetup](https://skillsmatter.com/skillscasts/10231-c-plus-plus-july-meetup) This is a very fascinating subject and looking back at the longer talk I would do it differently now \- it's a very shifting sands industry and I keep discovering new stuff daily.
None of [Texas Instrument's compilers](http://processors.wiki.ti.com/index.php/TI_Compiler_Information#Production_Compiler_Releases) (except those based on GCC) support `\#pragma once`.
Already seen that answer a while ago and it's absolutely nonsensical. He references a bug nearly two decades old. What does some bug have to do with 1 line vs 3? Bugs happen, they get fixed, and that's it. At the end of the day, you are much less likely to make mistakes using #pragma once than header guards.
If you read the comments on that answer you will see that said bug is still not fixed. &gt; Bugs happen, they get fixed, and that's it. Eventually, maybe.
There is good reason why https://ned14.github.io/afio/ does not have a directory monitor implementation yet. They are *very* hard to implement correctly. And by *very*, I mean that I consider implementing a correct one of those which performs well one of the very hardest of all the filesystem algorithms which one can implement. They are fiendishly hard to get right, and keep linear complexity. Your implementation suffers initially mainly from far too much malloc. That might be okay for a few dozen files, but not so well on a directory with a million or ten million files (before anyone comments, those work perfectly well with AFIO on all major platforms). You need to eliminate entirely all use of the memory allocator. Your second issue is that you trust the operating system to not lie to you. I can tell you straight out that when the kernel gets overloaded with too much change happening in a directory, it'll start corrupting and dropping events randomly on almost all the major platforms. I vaguely remember that the Windows API even has a special error code coming out of the change monitor saying "I give up, please reopen me". Your third issue is TOCTOU races whereby you appear to do nothing about the changes which can occur in between one filesystem syscall and another. And finally your fourth issue is that you are not implementing fallback. For example, inotify will fail with some filing systems on Linux, or not work right. In those circumstances, you need to fall back to a complete directory scan on a timer. Please do however be encouraged to invest the significant effort in writing a correct one of these. I'd gladly take it as a donation to AFIO, and from there I'd propose it for C++ standardisation, so work done here would have potential long term lasting effects. My criteria for acceptance as as follows: 1. Scales no worse than O(N log N) to items in directory and changed items. I *believe* O(N) is achievable, but I have not implemented one myself yet. 2. Works with directories constantly randomly relocating on the filesystem. 3. Works with directories whose contents constantly randomly resize between zero and ten million items. 4. Does not consume more than 85% of a single CPU on a top end Intel CPU with ten million items no matter the change load. 5. Detects all individual changes in items without dropping, nor misrepresenting, changes on all possible filing systems and configurations. This includes inode number changes where metadata is otherwise identical. Note that renames are particularly hard to always get right, I would accept a statistical percentage of likelihood that a rename has been correctly identified. 6. Marks changes with whether they derive from the same syscall snapshot and therefore are self-referentially consistent, or are from differing snapshots and therefore are not consistent with one another. Obviously, a suite of unit tests to prove the above are correct in any contributed implementation would be required. AFIO comes with a rich set of multi-process and multi-threaded conformance testing infrastructure which can be reused. Good luck, and thanks for sharing your code at /r/cpp!
I would get rid of static factory for such a simplistic case - I want to write file_monitor{}. Also I would get rid of shared_ptr in this code. It's not what I want to use by default. It would be cool not to enforce the class user to instantiate object on the heap. Why not: using monitor = xxx_monitor ? Using virtual member functions for a compile-time case? If you want to have a base interface for all 3 implementations, consider using CRTP. I don't like it this way. I would also try to move things like "#include "windows.h" to the .cpp file. It's not worth polluting the user's scope for just few symbols.
Lol
The problem is that your two suggestions don't work very well together. In the current version, the platform specific headers do not get installed, and they are doubling as the compiler firewall you are proposing. This way, the abstract base class serves as the compiler firewall and I do not need a PIMPL to implement the platform specific functionality.
When I write code I like to consider the interface the outside programer sees as the most important part, to me its what sets apart great code from good code. To that end, i feel like you have redundant apis, why does your make monitor, not take a string and behave like start? In code you’re just going to call both anyway. I also don’t like having to poll objects. I made one of these recently my self as well, check oht the interface i used: https://github.com/ThomasMonkman/filewatch
It is not an rvalue ref, it is a universal reference.
You asked for feedback. Don't get defensive about it, just take it in stride and move along
Why use 'stackful fibers' and not just use classes that hold their state and implement operator() ? 
Because stackful fibers are a negative overhead abstraction and you don't need to explicitly manage state i.e. your code is exactly the same whether it's running synchronously or asynchronously. This is a big win for cognitive complexity. Another way of looking at it: your code should focus on what it needs to do, not how it should be executed. Putting all your state into classes and implementing `operator()` is a crappy idea for synchronous code, why should it suddenly be great for asynchronous code?
A lot of them miss changes depending on the OSand underlying filesystem or network share and need a way to poll periodically or upon request
Nice. I like the way you templated the filename type! Well the polling is a necessary evil to get the results in your thread and not worry about locking anything. We were debating about offering the background-thread solution you are doing as an option, especially since mac does that anyways. But the "polling but no thread-sync" won out for now. I'll keep it in mind for future development though.
The interface seems convenient and implemention seems clean. But I'd hate to include boost for something simple as watch directory/files. The boost include, in my opinion, makes this less lean as it could and should be. 
I haven't read this properly, and I'm not sure I totally understand the stuff being parsed, but here's my initial thought. Parsing a 4-digit span can be done with PEXT-sub-multiply, and range checking requires an extra few instructions. initial: 0 0 0 0 3 4 1 2 (characters) pext: 0 3 0 4 0 1 0 2 (characters) sub: 0 3 0 4 0 1 0 2 multiply: 3412 412 12 2 Values below `'0'` will set high bits in the "sub" phase. Do another "sub" to check for values above `'9'`, and branch. Use of PEXT means potentially very fast handling of separators. Everything can be branchless. Might not be as fast as SIMD but it should be better than a typical scalar approach. 
Yep. It’s pretty simple. And likely totally unnecessary with coroutine support recently added to C++. Theirs was a workaround for the lack of `co_yield`. 
How does it compare to [fswatch](https://github.com/emcrisostomo/fswatch)?
The scope is different. Fswatch seems to be a more full-fledged and complete solution while this focuses more on being small and easy to integate for the hotloading case.
Thanks. Yea, I agree with you there. Hence the comment in the readme that we want to get rid of that once std's &lt;filesystem&gt; adoption is more prevalent.
See https://github.com/electronicarts/EASTL/blob/84127fd4ac234b24a40ea7911d05c85e7ba75120/include/EASTL/priority\_queue.h#L6 for differences
Isn't that a bit of overkill for something "primarily meant for asset hotloading in games and 3D engines."?
Excellent points \- if I ever do this talk again I will take them into account
I agree. Unfortunately, our company still sticks on gcc-4.8 (previously using gcc-3.4 :( ), and given that we have to wait for years before full C++17 support, I think it is still worthwhile to try it in our projects.
Anybody can hack together something quick which works most of the time in a very specific use case. I assumed, perhaps wrongly, that the OP posted his code here for feedback on how to improve it. Improving it means making it generic, reusable, scalable, perhaps even worthy for standardisation into the standard library. It's no easy thing. Neither the Linux nor Windows kernel folk have got their implementations correct. FreeBSD's implementation is close to correct, scales beautifully to load, but it will silently drop events if they are occurring faster than they are drained. Still, I'd just love if Windows and Linux did whatever FreeBSD is doing. Same goes for FreeBSD's implementation of byte range locks, phenomenal quality of implementation, I'd happily buy a beer for whomever wrote it. And Linux + Windows kernel folk reading: please improve your byte range lock implementation!
How is it even different? You are running a function that has a persistent state. 
What do you think about FUSE etc? It would certainly be a lot more reliable to detect changes if you are in control. 
Such systems are not generally updated to new compiler even to backward compatible one
If you use Boost anyway, you should use Asio to get rid of the polling API. Asio can read from inotify asynchronously and I am sure it can also help you with overlapped IO.
And frictionless
0K
Then I'd just use asio's polling API instead, or am I missing something?
I appreciate the "good enough" argument. Indeed, I often advocate it to argue against the overengineering we are all tempted to do. But your library is demonstrably buggy and incorrect. It will fail to function correctly in various situations. For me, "good enough" here means slapping warnings all over it saying "this won't work all of the time, do not write code assuming it will", ideally documenting all the known cases of where it can fail to work right. That's my personal opinion anyway, but I know that others disagree with this position. For them, if it works fine on their personal computer in the limited cases they test, they consider it "good enough". And I can appreciate that, though really they should say "good enough *for me*, might be useful to others"
Fibers can be written more naturally as the execution state in its entirety is saved, rather than just the state of a class (which would functionally be equivalent to a stackless coroutine). It's easier to work with stackful coroutines, aka fibers. Fibers also have performance advantages over large numbers of threads. I often use fibers as well. You *can* get better performance out of managing state yourself in a class object or such (presuming your saved state is smaller than the saved execution state), and treating it as a somewhat-stateful stackless coroutine. Your code is going to be messier and harder to understand, however, and that approach does shoehorn you into some unusual or less-than-optimal design choices.
There is no use case, or at least, not yet. It's for Monte Carlo testing that your implementation is correct under a 99.999% load. We like five sigma correctness soak testing. All the other bits of AFIO are tested to a similar degree. It's why it takes so long to implement the reference library. 
Ah, I see. 
https://bitbucket.org/lingfors/strongtype But it's opt-in rather than opt-out, and it depends on Boost which might be a show-stopper for some.
This was already included in the existing proposal http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0538r0.html
Does anyone call that by its official name?
Well I do for sure because it's a tiny bit more descriptive than just forwarding reference.
I get it. For standardizing such things, you would want a design to be able to handle much more than what can be imagined for "regular" use. As you said said previously, it's hard to get right. But then, is it even possible for any standard, cross platform, library to meet those critieria if operating systems themselves don't (can't?) provide the most accurate and fast API just for their file systems?
r/SIMD would like this :)
Not mentioning or remembering cvs and rcs made me feel real old :)
"Forwarding reference" _is_ the official name; "universal reference" was a placeholder name coined by Meyers until the standard formally named it in C++14.
&gt; But, apparently everybody else likes it that way. ¯\(ツ)/¯ Not necessarily, but in any case flair+filter would solve everything.
&gt; What is a file, something that can be referenced in an include directive ? 
&gt; Now all of the sudden you're getting multiple definition errors at link time. It would be confusing as hell to change whitespace in a file and have that break a C++ build, but that's what would happen. It would be confusing for a whopping two seconds, then you would read the error message saying that symbol A is defined in path1/foo.h and was previously defined in path1/foo.h and you would fix your buggy codebase. Having twice the same file is 100% a bug.
and then someone just copies your whole file as a base into another file, changes it, forgets to changed the include guard, no one sees it during code review and six month later, BOOM
&gt; stackful fibers Pardon my ignorance but what do you mean by 'stackful fibers'?
Not OP but I wrote my own file monitoring library because of the fswatch GPL license.
&gt; Having twice the same file is 100% a bug. Exactly. That's why my point is that you should get the error when you include the copy-pasted file. If you're comparing file contents, you don't get the error until you edit one of the two files. The jump from "I just copy pasted some files and now I'm getting a multiple definition error" is a lot shorter than "I just changed some whitespace and now I'm getting a multiple definition error". Granted, they're both better than include guards where whichever is included second just silently gets ignored forever, but I'm saying that between comparing paths, inodes, and file content, doing it by file content is probably the worst.
Again, I'm wondering what the technical difference is, just like the other poster, you are saying what you use them for and making claims about performance. I just don't see the difference except that one is syntactic sugar to take the variables you declare in a function from the stack and put them into the persistent state of a class. 
I ended up writing something similar but haven't open sourced it yet because what I wrote is a bit too new. In case it's useful, here are a few design choices I made: In my case, the interface that I settled on was a bit different. Instead of pumping an event loop, I have my `AssetManager` run its own thread and have other people query it for `Asset`s and subscribe to changes on individual assets. It handles thread safety internally, since invariably, any significant use of it will require responding to file changes from multiple threads. The `AssetManager` also supplies a `barrier` function that lets any `thread` block until a file is available for reading (it accepts a `vector` of file names as an argument). This is useful for game engines that have some sort of multistep pipeline (e.g. texture compression, shader compilation, etc). The last piece of the abstraction is an `Updateable` base class that other objects can inherit from. Given a path to watch in its constructor, it `subscribe`s to the `Asset` and `unsubscribe`s on destruction in RAII fashion. These `Updateable` objects have a virtual `perform_update` which is called when the underlying `Asset` changes. They are also chainable (call `chain` and supply the parent updateable) to perform a dependency graph. For example, shader is edited, modifies a shader module abstraction, which then notifies a graphics pipeline to update itself, which then causes the pipeline manager to attempt to destroy the previous pipeline on the next frame. The interface is general enough that update signals do not have to come from `Asset`s. In practice, this is working pretty well for me and pretty much everything in my engine can change on the fly, and it's trivial for me to add new hooks for various files (json, images, shaders) while supporting non-trivial dependencies using a combination of chained updateables and barriers. The only thing I have to be careful about is that the virtual `perform_update` function occurs on a different thread (the asset thread) than the thing accessing/controlling the object listening for the update, but this is probably a concern no matter how you do it unless everything runs on the same thread.
r/cpp_questions something something `std::minmax_element`
But, once stacks are *no longer rare*/unique, the fact that they take many-MB of state starts to suck. I guess on a 64 bit system with a full memory space you can give each stack a GB and fit 16 billion of them. But what you have seem to have done is reintroduced memory space contention just when we finally stopped having it... 
Except you have permission to *read the members* of the type. There are no members. Having permission to read no members is permission to do nothing. Starting with your example: union{ foo f; bar b; } u2{ foo{42} } }; bar* pbar = &amp;u2.bar; // pbar does not point to a bar: this is illegal int* pint = &amp;u2.bar.j; // legal, common initial sequence, there is a j there. And invoking any method on an object that isn't there is undefined behavior. 
You don't see the difference between needing to save specific state yourself, and thus requiring the coroutine to be in a *very* specific state when you return from the coroutine, versus a stackful coroutine that can 'return' at basically any point as the full state is saved as it was?
Why don't they just say `#pragma once` is equivalent to putting include guards in the file, with an "anonymous" preprocessor symbol associated. Done. Why is this so hard? Why do they feel the need to define in terms of files / symlink / etc.? Just do the thing that is excessively verbose and error prone that I used to do, and give me a simpler and safer syntax instead that is standard please, thank you.
The Agency got him! Nooo!
However, I wasn't sure why. I suspect because from an r-value array you can get the element as an r-value, while from an l-value array you get it as a reference?
Yes.
http://reddit.com/r/cpp_questions
In this case a file is something uniquely identifiable by either the inode or path. That's why linking the file or one of its parent directories is implementation defined. The implementation would have the choice to use the path or inode to generate an id or some other operating system dependent way to identify a file. Specifying `pragma once` would bring many code bases closer to the standard. While `pragma once` is supported almost anywhere, it's still technically not part of the standard. This leads to a lot of discussions, some linters warn about it and if almost every every implementation supports it, why shouldn't it be part of the standard? Why should I depend on behaviour, that is not guaranteed on other platfroms. I know, that links are not an issue for me, because I have a sane build system, that doesn't depend on symlinking the same headers under different names and so translation units can't include the same file. So `pragma once` doesn't have any issues for me. Also if it were part of the standard, that would force build systems to avoid synlinking the same file and to work woth pragma once, while currently some build systems don't support it, as it is a compiler extension. We have so much vaguely specified behaviour in the standard, why is pragma once a problem?
Well that was unpleasant to read. The author shows this: #include &lt;stdio.h&gt; #include &lt;map&gt; #include &lt;string&gt; int main() { std::map&lt;std::string, std::string&gt; x; int i = 0x5001; x["foo"] = i; printf("%s", x["foo"].c_str()); return 0; } and then says... well, I'm not sure what they say. &gt; If you're relying on some kind of warning or error, it's not good enough for this situation. It's good enough for me. All three compilers I use warn for this. The rest of the (short) article doesn't add anything but more ranting: &gt; Incidentally, citing one of my posts and saying I'm "not a real C++ programmer" is just amazing. Who cares what you think? I think I'll ignore further articles from them.
Not only fibers will save the state for you. There are more advantages over operator(). For example, you can suspend/resume and remember the reenter point at wake up time. This simplifies a lot the code flow for while loops and ithers where you can break in the middle. With just a stateful class you are going to have a shitshow of ifs to figure out where to continue the next time you have to enter operator(). Believe me, it is a true story.
More than one comment here is "show me an example implementation of a strong Id". So instead of replying to each: https://github.com/tvaneerd/code/blob/master/StrongId.h 
&gt; If you're relying on some kind of warning or error, it's not good enough for this situation. Well... % g++ -O -Wall -Wextra -Wconversion foo.cc foo.cc: In function 'int main()': foo.cc:9:14: warning: conversion to 'char' from 'int' may alter its value [-Wconversion] x["foo"] = i; ^ You can certainly get a warning about it; it's just not one of the usual ones and you have to turn it on specifically with gcc and clang. You should also be at least marginally aware of the various overloads of the various methods of a class that you're using. 
For those as horribly confused as me, the issue is not `map` but `string::operator=`: There is `basic_string&amp; operator= (CharT ch)`, for some obscure reason. The existence of that is in fact not really obvious, especially since the ctor is `basic_string( size_type count, CharT ch)`, so std::string valid; valid = ' '; compiles, while std::string invalid = ' '; does not since the ctor would be std::string ctor (1, ' '); I agree, fuck that.
I for one just want decent backends for LLVM from them, plus a few additions to a standard library.
You don't have to use both inline and constexpr. Functions with constexpr attribute are evaluated before code generation. 
I don't think you meant to reply to me?
I did, as my comment is only relevant I. The context of yours 
constexpr implies inline, I might have accidently added an inline somewhere.
ah yes I read it too fast!
http://learncpp.com is a good intro. I doubt you need to buy anything to get proficient in using it.
If the compiler deduces an l-value ref, it will collapse into an l-value ref. If the container is empty, the behavior is undefined.
I suggest that you go to isocpp.org and see the "Getting Started" section. Read the FAQ. Get a book. Have fun! :-)
I mean, #pragma once UNIQUE_IDENTIFIER // ... is just a less verbose equivalent of #ifndef UNIQUE_IDENTIFIER #define UNIQUE_IDENTIFIER // ... #endif The form without an identifier is different, of course.
If I put all the variables I'm using in the function in the class and don't put any on the stack inside the operator() function, the only difference is the point in the function the execution ended on, which can be taken care of with a switch statement.
I would use a switch statement, possibly with some __LINE__ macros, but I understand what you are saying, thanks.
Thank you! Deleted it - didn't see the original one
Hey, I have small comment about chosen interface. Combination of poll() and the way how callback is used is a bit "unusual", at least for me. You either register listeners with callback because "callback will be called later" somehow - kind of asynchronous/delayed processing. In this case poll() is just a way for user/client to give you some time on some thread to update your state. Or either you don't have callback and poll() simply returns collected data back to the caller without inversion of control thru callback. Looking into code, I think, second case is best match for you. What I mean is: file_list_t poll() = 0; Callback is not needed and unnecessary there. It may be helpful if you choose 1st described interface, something like: void add_listener(change_event_t) = 0; void poll() = 0; This way you can support multiple listeners, if you need. This way, for example, client can register callback and just poll you on separate, dedicated thread. 
Not the OP, but I'm assuming they mean a cooperatively-scheduled thread (the fiber part) that has a stack associated with it (so it doesn't have async/await-style syntax)
Trailing return types is a stylistic choice like brace positions and hardly an "academic vs real code" thing. Which presentation are you referring to? The popular one that everyone references is Sean Parents, and he absolutely come across as someone who writes lots of code in the trenches. Please explain, I'm genuinely curious, why do you disable copy by default? To prevent accidental deep copies?
I think one of the best ways to approach C++ is not to consider it a low-level language. Rather think of it just like you think of the Python/C99 language pair. You have a high-level that you write most of your program in, and use the low-level when you must. The bulk of your Modern C++ code should feel as high-level as Python. The nice thing about C++ is that when you need low-level control, you don't have to change to a different language (for example Python -&gt; C99) you can still use C++. When you are learning C++ and using C++ libraries, if you find yourself thinking that you are writing code at a lower level of abstraction than python, that is a sign that the library is designed incorrectly or that the teaching material is teaching you outdated C++. So if you see code that is using a bunch of pointers and doing manual memory management, your warning light should go off. Some examples of C++ libraries that enable writing this kind of high-level code: https://github.com/nlohmann/json http://fmtlib.net/latest/index.html https://github.com/ericniebler/range-v3 
This is a small etude I wrote a few nights ago that ended up using features like constexpr if, constexpr, and template variables to good effect in a real world setting. This was more intended for graphics engine programmers (who affiliate themselves primarily with C++) but I thought the C++ community might be interested too.
&gt; If I put all the variables I'm using in the function in the class and don't put any on the stack inside the operator() function, the only difference is the point in the function the execution ended on, which can be taken care of with a switch statement. You seem to be confused as to how this works. Stackfull coroutines (fibers) are *kernel level objects*. *You* aren't managing the stack. The stack exists as the stack normally exists. For them, *literally* the only thing you are doing is switching between fibers. There is no other state management involved. Working with the fibers is like working with a normal callstack, just with the ability to switch fibers (and thus switch to the original 'controlling' fiber) at will, and return back at will. Stackless, you have to actually construct the logic to restore state, and you have to construct very specific points you can 'return' from as all the state that is needed upon restoration of the coroutine needs to be available at that time to continue execution. This is automatic with stackfull coroutines. You aren't "putting the stack in the `operator()`". All the `operator()` function in this case is doing is calling `SwitchToFiber` (or your system's equivalent), which makes the fiber's execution context current. There's no real manual state management. Stackless coroutines are *entirely* manual, *and* you have to handle all of the return and restoration logic, which is implicit with fibers.
The other answers were nice but this one was neat. One if the things I'm most dusty about is libraries, with python I just use the package a manager but with c++ I just don't know what the actual procedure is today. Any links in that? I work in osx/gnu but also got win boxes, but Linux is just fine. What would you use for http request? Range, fmt and json are pretty nice libs. I will start using them soon :) Thank you 
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8dpu64/getting_into_c_coming_from_a_c99python_background/dxph69n/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8dp93o/anyone_ever_used_cppmicroservices_library_for/dxph7ot/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I've seen w bunch of type safe wrappers around C/C++ graphics APIs, but they rarely turn out nearly so nice as this. Bravo! I also like the technique of using `std::tuple_element` of a `std::tuple` do some of the metaprogramming heavy lifting, it's a nice trick. Does Vulkan insist that the specialisation data is densely packed? Do GPUs not suffer from any performance penalty for misaligned accesses?
Thanks :) Yes the specialization data must be densely packed and there are performance penalties. In this case it's a bit different since the data is embedded in the shader module itself (again, think of it like a preprocessor define, only compiled at runtime). If you read the specifications, there are very specific alignment rules for different types of buffers. There are also edge cases. For example, if you have an array of structs like so: layout (binding = 0) uniform UBO { Stuff stuff[3]; }; then `Stuff` must be `vec4` aligned. However, this requirement relaxes if you did something like layout (binding = 0) uniform UBO { Stuff stuff0; Stuff stuff1; Stuff stuff2; } 
&gt; for (int n : numbers) &gt; [...] Copying elements is a bad habit. I'm sorry, but are you saying that in this situation const int&amp; should be used ? Isn't such case like iterating over array of integral type without applying const ref standard practice ? We do pass integers to functions by value. What is different in case for range-based for loop ?
Why don't you just cross-compile your tests?
Haha! My first one was cvs. I don't miss it at all :P. The Borland one was PVCS btw
&gt; are you saying that in this situation const int&amp; should be used ? `int&amp;`, `const int&amp;`, `auto&amp;`, or `const auto&amp;`. (Or `auto&amp;&amp;` in highly generic code.) &gt; Isn't such case like iterating over array of integral type without applying const ref standard practice ? Classic index, pointer, and iterator loops don't copy elements. All of `for (size_t i = 0; i &lt; v.size(); ++i) { do stuff with v[i] }`, `for (int * p = v.data(); p != v.data() + v.size(); ++p) { do stuff with *p }`, and `for (auto it = v.begin(); it != v.end(); ++it) { do stuff with *it }` work in-place. &gt; We do pass integers to functions by value. What is different in case for range-based for loop ? The difference is that when calling a function, programmers are accustomed to creating different objects when passing arguments by value. When iterating, there is a strong assumption that iteration is in-place, and range-for's syntax is hiding the copying (performed by `int n = *internal_iterator;`), so it's easy to miss the mistake. The consequences are: * Reduced efficiency for expensive types (e.g. `string`) * Won't compile for move-only types; a potential stealth build break in generic code * Lost modifications if the variable is assigned to * Dangling pointers/references if the address of the variable is stored somewhere long-lived Copying elements is badness all around. Never do it implicitly. 
That's just the "many changing files in a directory" use case, not the "accurately monitoring" use case. The files are transient results, and the directory changes are already known to the only program that actually uses it. There's no need to monitor that stuff, since it is the one generating it. And I really doubt any compilation process would need to generate tens of millions of transient files. For high volume processing, you would probably want an actual database of sorts which only uses its own indexing and "monitoring" which is much more efficient (and thus accurate) than monitoring a directory. In which case, it would be more important to have good byte range level file locking, as was the other point that /u/14ned made, than directory monitoring.
I was expected Spectre to affect some of the really over-the-top C++ code, not the line of code they specified. Still, the code is moderately crazy enough to where it becomes an edge case enough times
I think you said "cross product" when you meant "Cartesian product."
Well, yes, actually you can use a switch but the point still holds: it is difficult to read actually... though better than ifs.
Well, i is an index input, and the caller can train branch prediction for that function. That's all it really takes.
Probably better off looking in r/cppquestions. Also there is a maintained list on stack overflow for c++ books if you don’t mind reading. Just google “c++ books stackoverflow”. 
Because sometimes, things like the build setup are out of your control, so you can't take the most sensible option.
Thank you.
Install lsp-ui too, to get nice peeking ui for references and overlays for fixits and stuff.
thanks for that!
I was expecting a link to to a code is with C++17 features and Modules TS.
This is offtopic for /r/cpp, check out r/cpp_questions 
Yeah, I remember this Polytron system haunting all of the dev team on my first job in 1990: the host was running MSDOS &amp; Novell, but the actual development was done on an in-house self-constructed 68020 machine running 'primitive dos' (the os rose to its name). Some PC development was done on Turbo-Pascal and Turbo-C++, too. All this swearing from the other team members when the one on holidays has forgotten to check-in his changes before leaving and all of the files were locked.
on a mac in chrome the code font is difficult to clearly read. a backslash is nearly vertical. also once you've gotten a successful answer, there doesn't seem to be any way to go back and alter it. But really, this isn't practicing what people need to practice about C++. Much better questions would be things like "write a function which manages a memory structure without using new/delete" or "implement a template which handles all types except X in one way and handles X in another way". It seems like all the things you can do are just generic exercises that would be better done in another language than C++ in the first place.
Just for clarification ... 
Hey, What kind of positions you have in Pune, India? I am an Indian working in Germany in the field of robotics/computer vision/drones. Mostly use c++ everyday at work. Use of Tx1 on a regular basis. About 1 and half year experience after a Master's degree from a reputed university. 
I stranded my laptop at a friend's house, so flair isn't happening today, as I don't want to edit the subreddit internals from a phone. Hopefully tmrw.
Not bad, looks vaguely similar to Boost.Serialization (the last time I tried it, which was very long ago). Of course, all problems of C++ implementations of serializations are here: the need to repeat the name of the serialized object several times, the need to register types somewhere, etc., but until we have meta-classes or at least static reflection, those can't be really overcame. I dislike the need to write the constructor for all members the most: it means repeating every name thrice (once in parameters, and once as this.name(param_name)). On the other hand, of course, it provides the opportunity to finish the object deserialization by calculating the dependent members. The other approach is to register the class members as pointers-to-members in the serializer is some way, which removes the need for repeating their names so often. I usually go this way when I need to write a (de)serializer myself.
string_view is not 'considered harmful', stop with the hyperbole. It's one the best additions to the STL in recent times.
Can you post a screenshot of the hard to read text?
You are right about the similaties, I based my framework on existing ones, while making sure the performance is the best, and that it is easy to read and use. I didn't understand what you meant by writing constructor for all members, the only requirement is to have a default constructor, it doesn't have to do anything or be publicly accessible. Regarding registration - right now no registration is performed except for polymorphic serialization. If you have an idea to use pointer to members to improve performance i'd like to hear it. Regarding repetition - repeating the names of members is only requires on the serialize member function. Thanks for your comment.
I am very happy about the change. I will not look back I think :)
I find the sideline a bit too much too.. Haven't looked into if it's easily configurable. I use company too, with company-lsp. I'd like to figure out how to do client-side fuzzy matching and filtering, like how vs code does it. But I haven't dug into it..
Things started looking arcane with C++11...
So this inspired me to try it with VSCode tonight, and setup seemed relatively easy, but unfortunately it doesn't appear to want to provide anything. I've looked at their generated cache and it seems like it doesn't want to go through every file, it just stopped on one of my dependencies and didn't continue on. Even if I go into one of the cpp files in those dependencies it did cache, it's not giving me any auto complete or goto definition. Unsure what I'm doing wrong :( But still, I'm excited for the project, I'm sure I've probably just done something wrong.
https://i.imgur.com/VPd4nMJ.png
It's really hard to tell that this is a backslash not a pipe: https://i.imgur.com/PN591nI.png
It is a very useful addition, but IMO it *is* important to educate people about the ways you can run into problems with it.
Would be nice, if that operator would be deprecated.
I mentioned it in the post. It is not working totally ok for VSCode for some reason. You need the cquery plugin and such but I think there seems to be some problem with the includes. In emacs it works perfect.
&gt; I mentioned it in the post. It is not working totally ok for VSCode for some reason. Yeah, I had noticed, but I had been meaning to give it a go for the last month or so. Does the include issue cause errors or anything? I don't really get anything at all besides the following: https://i.imgur.com/XrGiefP.png No errors or anything, just those and nothing else. I'll just try it again in a month or so, see if it's been resolved or more instructions are released :)
Whay do you mean?
`...` folds can improve both size and offset calcs. Size obviously; offset by doing a size calc with a multiplication. return ((sizeof(element_type&lt;Is&gt;)*(Is&lt;Target))+...); where `Is...` is an `index_sequence`, and `element_type&lt;I&gt;` does the tuple element dance. Getting rid of compile time recursion can increase compile speed and lower memory use. 
I haven't looked at it myself, but for anything resulting in as system call, I would not worry about the overhead a virtual function call unless there is evidence to the contrary
Before you state a list of demands, should you not first all, if the author is interested in donating his library to AFIO in the first place?
What demands? They are criteria for approaching correctness. Feedback, as the OP asked for. The OP can use or ignore them. If he uses them, I'd gladly take ownership of a copy into AFIO. Maintenance costs much more than implementation, I'm still supporting some of the open source I released twenty years ago!
Wow this looks very promising. Anyone has experience using this with vim? How does it compare to YouCompleteMe?
I don't have any data regarding how much of the percentage of desktop apps use boost. It wasn't part of any desktop app I ever developed as part of work. 
Thats some good feedback! Thanks for that. I'll look at it more closely later. 
IIUC clangd is more of a single-TU thing. For completion, code fixits, etc., it's probably 90% of what you want. cquery processes the whole codebase so additionally offers xref-find-references and things. But it does come at a bit of a cost since it's keeping so much more in memory (and on disk). The comparisons I've seen suggest that cquery has support for more features.
Sorry, that should have been "requirements" not "demands". 
How is emacs a non fancy editor? It is more like an OS...
Most likely the completion will be no different since they're both using the same underlying code. (cquery has slightly different symbol lookup than rtags. Even when it's well-defined what "foo" means in a particular place, cquery may find a definition of a different one whereas rtags is rather more particular. The difference is a deliberate choice, and neither seems to me necessarily better.) I've also previously used rtags; cquery's significant benefit is that it seems lots easier to get all of its features working.
OK, so substituting those words, the answer is that I judged that people interested in solving this long standing problem would read this topic, and it was worth more to the community to comment on properly solving the problem in the absolute case, rather than relative to the OP's code. I can see some felt it came across as rude. That often happens if you rush an answer because you have a meeting in five minutes, it happens sometimes. I am sorry if I offended, no doubt wording could have been better, but I stand behind the intent and content of the post.
I was looking for an ideas how to implement polymorphism for my serializer [bitsery](https://github.com/fraillt/bitsery) and I liked your idea to hash polymorphic type name and take 8 bytes as type identity, but your implementation is still missing multiple things to full support pointers. Btw, why you write `class polymorphic { public: virtual ~polymorphic() = 0; }; inline polymorphic::~polymorphic() = default; ` instead of `class polymorphic { public: virtual ~polymorphic() = default; };` ? 
I could not change it after the fact: replace editor by IDE. Sorry for that ;)
From the point of view of how it works in emacs, I can say that cquery/lsp is both easier to set up and accurate with code completion.
He posted the same thing a while ago. Left me confused since 99% of the code looked like „c with classes“ and not really C++17. Still the game looks cool and all, but I don’t see how it belongs here. Especially since he’s not even showing any module-related code.
I intentionally don't support pointers as it increases the complexity of the library by a significant amount, that in my opinion is not justified. Regarding your question, the code you suggested makes polymorphic non-abstract, which is why I prefer to have the destructor pure virtual.
Hi! I fixed all of the issues you and /u/dioafire brought up: - Fixed the CMake file - Fixed the VC++ extensions usage - Added a Travis build - Added a coveralls coverage report - Improved the coverage by writing more tests - that caught some bugs that I fixed. - Improved the README - Code now compiles cleanly in GCC, Clang &amp; MSVC! Thanks for all your help and tips :)
I use RTags, Irony-mode, and company-irony and it works quite well
Yes I did! Thank you.
The game is closed sourced, I've only ever posted the game loop, and yes it had some legacy code, but also it only used modules and maybe one header if I remember correctly. I didn't show code here because I notice everyone has their own ideas of how things should be written, and my point rather is to show people how modules-ts can benift a project rather than argue about code.
Yes I should have written up a small blog to show how and why I use this development environment. I'll see about writing one up this morning.
Yes! its time to switch! 
Yes, in the sense that a rope is a bunch of individual pieces, but no in that the rope owns the pieces whereas a multi chunk string view would be a bunch of references.
In theory very well. It was designed for that. But it will use more memory.
and some IDEs do much more than some OSes
&gt; What's the use case of next_permutation? https://www.reddit.com/r/cpp/comments/8b3gml/has_anyone_actually_used_next_permutation/
I used [this benchmark](https://github.com/thekvs/cpp-serializers) to compare [my serialization library](https://github.com/Ebenezer-group/onwards) to some others. Have you done anything like that? My approach [doesn't require default constructors](https://github.com/Ebenezer-group/onwards/blob/master/tiers/account.hh).
Thanks for your comment. I made a decision not to use macros in this library, which I think is a great feature for some of the C++ community. I can certainly picture users wrap this piece of code with a macro: ```cpp friend zpp::serializer::access; template &lt;typename Archive, typename Self&gt; static void serialize(Archive &amp; archive, Self &amp; self) { archive(self.object_1, self.object_2, ...); } ```
No idea there. My experience was non-resolved includes in VSCode but not #pragma problems.
Meanwhile Eclipse just deployed new version with support for Intellisense and all Visual Studio Code plugins. I still prefer it's own parser to Intellisense, but I'm looking forward to the future
Is there anything practical we can do with this information? I.e. does the warning call for rewriting the section of code involved, and if so, in what way? 
Was distracted by the speaker's pronunciation of 'idempotence', with such slight syllabic stress that it sank in the sentential stream (so sounding more like the word my spell-checker is suggesting). A more usual British pronunciation has stresses like EY dm PO tnc. Enjoyed the talk, despite the distraction. Disclaimer: I once attended one of Mr Matthews Advanced C++ courses.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8dxnwj/weighted_graph/dxqt6z5/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
How does it relate to [cereal](https://github.com/USCiLab/cereal)?
No tested with vim but I used ycmd on Emacs. cquery provides better completion support because it parse all translation units (ie compile everything). So you can find references of a symbol, ..., but it is way slower especially if you have lot of template code than ycmd but faster than rtags which was not usable in my case. FYI, there is also another tool: clangd. There is less features than cquery but Apple said that they will now work on this. References: https://github.com/cquery-project/cquery/issues/63 http://lists.llvm.org/pipermail/cfe-dev/2018-April/057668.html
I use it on different subsets of a large codebase, depending on the subsystem I'm working on at the time. 1MLOC is probably the largest I've tried, takes about 20min to index (time to grab a coffee or check reddit) but afterwards it works just great. Xref is where cquery really shines, very accurate and almost instantaneous.
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;You could also use [C++17's `std::pmr::polymorphic_allocator`](http://en.cppreference.com/w/cpp/memory/polymorphic_allocator) for this, correct? 
Thnx for a pull request to https://github.com/fraillt/cpp_serializers_benchmark You're right pointer support adds a lot of complexity, you can see at https://github.com/fraillt/bitsery/blob/master/examples/raw_pointers.cpp how bitsery handles it. Although it might seem complex at first, but it is actually a bare minimum, because pointer can have unique or shared ownership, or have no ownership and point to any other type T&amp;, T* or even to wrapper type like optional&lt;T&gt; or shared_ptr&lt;T&gt;, it can also be null or point to object that you already had serialized before! The main problem becomes that users loose this elegant serialization syntax, because they need explicitly define ownership and also handle pointer linking context across multiple serialization calls, and no one likes complexity especially when you can avoid it...
Yes! I actually tried using folds for a bit but had some difficulty getting it to compile on a couple platforms. This is actually what I tried doing initially, and I should probably revisit it. In terms of compile times, there are usually no more than "a few" constants so the recursive approach isn't a killer, but folds would definitely be preferable.
Thanks, sounds like it may at least be usable then. I don’t mind waiting for the indexing to complete, rtags would just go away and never come back. Going to try setting this up tonight.
How exactly is `#pragma once` supposed to work as you described? How do you select this anonymous identifier?
[This has been proposed, but I'm not sure we ever discussed it.](https://wg21.link/p0538).
The same way anonymous identifiers are selected for lambdas and unnamed namespaces?
thanks for this library, looks nice. my feedback: the cmake scripts could be improved, e.g. use namespaces for includes and flags \(target\_include\_directories instead of include\_directories\) ref [http://mariobadr.com/creating\-a\-header\-only\-library\-with\-cmake.html](http://mariobadr.com/creating-a-header-only-library-with-cmake.html)
For if you don't want the kitchen sink, here is something very minimal: https://github.com/ahupowerdns/hello-dns/blob/master/tdns/nenum.hh Sample usage: ``` // enums enum class RCode { Noerror = 0, Servfail = 2, Nxdomain = 3, Notimp = 4, Refused = 5, Badvers=16 }; // this makes enums printable, which is nice SMARTENUMSTART(RCode) SENUM6(RCode, Noerror, Servfail, Nxdomain, Notimp, Refused, Badvers) SMARTENUMEND(RCode); ```
My attempt was to create a simple and fast library, I was taking cereal and more serialization libraries into account when designing my own. You can see for yourself in the following benchmarks: https://github.com/thekvs/cpp-serializers/pull/24 https://github.com/fraillt/cpp_serializers_benchmark/pull/2
I think that without context on how awful this problem is, that might be the case. Having dealt with this more times than I'd like, I read it as: &gt; This problem is awful. Beware beware beware. But if you *do* beware, let me know, and I'll work with you to roll it into my proposed standardization for this absolutely terrible problem; you will have a meaningful impact on the standard and language if you do. I sympathize with /u/14ned, though. I've had to explain this to at least 3 different teams, and only 1 followed my recommendations. The other 2 spent 4-8 months chasing bug after bug and inevitably falling back to the only safe option: a slow scan of the directories and eventual consistency. 
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Add [Mach-O](https://en.wikipedia.org/wiki/Mach-O) to that list of executable formats (Apple's Darwin platforms&amp;thinsp;—&amp;thinsp;macOS, iOS, tvOS, and watchOS&amp;thinsp;—&amp;thinsp;use it, if you'll recall. 
Thank you for taking the time and describing the process, I had something similar in mind, I'm not sure if I'm ready to take this step. Adding this means encouraging more overhead, complex use cases, whereas my initial goal was to provide a simple yet elegant solution for a fairly simple problem.
`to_string()` pls. Much better name than the current.
`?:` operator is a big pile of backwards compability and corner-case language rules - especially value types. [cppreference](http://en.cppreference.com/w/cpp/language/operator_other#Conditional_operator) lists like 20 points of rules. I would not use it unless it's obvious it will work and second and third operand are the same type.
I'd argue, that if we are doing this, we might as well go all the way, and use a bit more template/macro magic to make the defining syntax nicer. I'd feel confident saying that this could be defined with a single function macro call, maybe for a finite number of elements because of manual FOR_EACH macro typing, but still. Also, without looking into it, i assume the cstring stuff could be worked around using templates under the hood. Happy to be proven wrong though!
&gt; how modules-ts can benift a project But you don't show that in any way at all, whatsoever. You just show what you're working on; you're clicking a keyboard and swiping a screen. There's no explanation of what modules have added or how they've made things better. There's no mention of anything at all. 
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;There's also [P0907, 'Signed Integers are Two's Complement,' by J. F. Bastien](http://wg21.link/p0907) to consider… 
&gt; So, what was the clue? The authors being known authorities on the subject matter speaking at one of the bigger conferences on the topic.
Yeah, why the hell do they even bother to record the talks at a conference? Nobody wants to watch people talk about stuff, ever, after all.
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;As I remember things, list initialization via braced-init-lists is different form list initialization using `std::initializer_list`s; [cppreference has a discussion of this](http://en.cppreference.com/w/cpp/language/list_initialization). That is to say: all `std::initializer_list`s reify braced-init-lists, but not all braced-init-lists are `std::initializer_list`s. 
&gt; I think that without context on how awful this problem is, that might be the case. People consistently underestimate how hard this problem is. Everybody naturally gets that implementing a lock free, interruption safe, high performance B+ tree is wicked hard and they respect it accordingly. Yet directory change monitoring? Surely that's first year algorithmic complexity? And yes it is. Doesn't mean it isn't fiendishly hard! It's the tradeoffs y'see, you can go for perfect change reporting, but that'll be slow. You can go for low latency, but changes reported will be inaccurate. You will trade off somewhere in between, but start running into curious corner cases and quirks. It's all surprisingly non-deterministic, and lumpy to boot. Your solution will work 98-99% of the time, and then go pathological just occasionally enough to be maddening and become a showstopper. It's a surprisingly challenging problem, yet one a first year compsci could theoretically solve. &gt; Having dealt with this more times than I'd like, I read it as: &gt; &gt; &gt; This problem is awful. Beware beware beware. But if you do beware, let me know, and I'll work with you to roll it into my proposed standardization for this absolutely terrible problem; you will have a meaningful impact on the standard and language if you do. Yep, that's how I meant it. Dragons abound here. Thank you. &gt; I sympathize with /u/14ned, though. I've had to explain this to at least 3 different teams, and only 1 followed my recommendations. The other 2 spent 4-8 months chasing bug after bug and inevitably falling back to the only safe option: a slow scan of the directories and eventual consistency. My proposed standard low level file i/o library reimplements directory enumeration over std::filesystem. I already know I'll face a storm of calls at WG21 to remove it due to duplication. Yet std::filesystem's directory enumeration is utterly broken, wrong even. It can never work right, not ever. My directory enumeration however, is as unracy as is possible for the given platform, and it'll enumerate 4-5 million entries per second. Those, like you, who have experience with the evils of directory change monitoring will see immediately why I went to town on redoing directory enumeration. It's exactly as you say: the only sane solution is a full scan of the entire directory as a single snapshot enumeration, and then running a O(N) delta generation algorithm on the two snapshots, a non trivial thing to write as well incidentally. And also exactly as you say, you aim to arrive at eventual consistency. Thank you very much for your post. And I send commiserations to you for having to deal with that problem, I've made three (lifetime) attempts now at a correct solution, none lived up to it. So I feel you. It's hard.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8dztca/whats_the_best_way_to_learn_c/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
The stack-per-fiber cost can be amortised because you cache stacks and reuse them so the cost of creating a fiber is essentially zero and the cost of task switching is essentially the same as a function call (stack push/pop).
1. [Empty Base Class Optimization](https://youtu.be/wYd2V4nPn0E?t=157) 2. [Constrain "Greedy Templates" using SFINAE](https://youtu.be/wYd2V4nPn0E?t=714) 3. [Use `common_type_t&lt;T&gt;` as an identity meta-function](https://youtu.be/wYd2V4nPn0E?t=1148) 4. [Conditionally Deleted Special Members](https://youtu.be/wYd2V4nPn0E?t=1991) 5. [Conditionally Explicit Constructors](https://youtu.be/wYd2V4nPn0E?t=2710) 6. [Use `unique_ptr` For Exception Safety](https://youtu.be/wYd2V4nPn0E?t=3090) 7. [Customizing `allocator&lt;T&gt;::construct`/`destruct` Behavior](https://youtu.be/wYd2V4nPn0E?t=3591) 8. [Use Tag Dispatching To Exploit Data Characteristics](https://youtu.be/wYd2V4nPn0E?t=3896)
So you're agreeing with him then..?
"So if you’re in C++98 how do you do to shuffle a collection without introducing technical debt?" By using random_shuffle. There is nothing wrong with it or rand() for most of the use cases. 
Remote?
It seems that they do not give the same results. I have exactly the same experience as you with RTags.
[But it's not that simple - how do you decide when two headers are the same?](https://www.reddit.com/r/cpp/comments/8devw1/can_anyone_actually_name_a_compiler_that_doesnt/dxmn5yo/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=cpp)
Whoa, you really went the extra mile! Thanks!
So, should I call the variable something like this: std::vector&lt;std::vector&lt;int&gt;&gt;vertex??
No, since and adjacency matric holds edge weights.
The same way `std::filesystem::equivalent
FYI, the fix has been merged to NamedType.
If they aren't effectively infinite, you have the problem that code in a Fibre is no longer like code outside a Fibre. Which is the point of a Fibre compared to a stackless coroutine. If the platform kept this in mind and stacks where paged with links between them, then I'd be a bit more comphy with them I guess.
Repeating RCode twice, need to specify number of enumerations, looks ugly.
I will look into that, thanks.
I like camel casing better. I could be an option through. :)
Love/hate the only game in town, it is a bit like javascript
Doing a file ID comparison may be fairly expensive. At least, it is on Windows.
I admit the declaration syntax is not exactly beautiful. But I think you are right, the _DECL macro can be moved into the _VALS macro. It maybe just needs to be turned into a callback, like the values. The values still need to be a series of callbacks, to support unlimited number of them. Anything that is declared as a data list has this annoying limit of 64 or 256 items. The cstring stuff exists because strings are variable length, so when they are stored in a single array, we need an extra level of indexing nodes to store the lengths and offsets. Also, the getter function usually returns a reference to const data, but for strings, it needs to be a const char* - a const char&amp; is not a string. 
There's a wikipedia page on it.. did you even google before posting?
If this is a time-stepping simulation, then make sure you're aware of [Kahan summation](https://en.wikipedia.org/wiki/Kahan_summation_algorithm).
Just a word of warning: numerical analysis can be a big field! I'm no expert on it myself, but I do keep a copy of Acton's "Real Computing Made Real" on my bookshelf as a reference. Anyway, you might consider fixed point arithmetic. If you take the solar system as about 244 A.U. in diameter and quantize each dimension with that using a 64-bit integer, you get a precision slightly better than 2 micrometers. You might also want to look into [double-double arithmetic](https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format#Double-double_arithmetic). This uses pairs of standard double-precision values to emulate something that's fairly close to quad-precision arithmetic. It's not quite as precise, but it's fairly close and still reasonably quick. (Certainly much quicker than arbitrary precision arithmetic). If you just want to track an estimate of your error, [interval arithmetic](https://en.wikipedia.org/wiki/Interval_arithmetic) is pretty straightforward to implement though some operations can blow up the error estimate (e.g., division where the divisor includes zero in the interval). To really do it right you also need to be careful about setting the rounding mode for computing each end of the interval. There are other approaches that refine the idea to give tighter bounds, but interval arithmetic is the basic one. In terms of 3D in particular, just using the right coordinate frame can go along way. Matt Pharr had a neat little blog post recently on [rendering in camera space](http://pharr.org/matt/blog/2018/03/02/rendering-in-camera-space.html) on the topic. Good luck!
I disagree. If OP is trying to simulate orbital physics then fixed point will either give insufficient precision at small scales or waste too much information at large scales.
Following /u/MillerTheDeer 's comment, found [this one](https://github.com/eteran/cpp-utilities/blob/master/Fixed.h). From my reading of the code, this is simple and give a better precision than floating-point. Just gonna need to compare performance. What do you think of it? Thanks for your suggestions! I might even buy the book since I kinda like the research I'm doing now!
I like camelCase too but consistency is a strong argument for sticking to ISO C++ naming conventions. Why be deliberately inconsistent in a new library?
Could you please elaborate? From what I'm planning, "small scale" should be driven by Bullet physics which should be precise enough that a ~10km radius would look good. Outside of physics range, my code will take over and drive everything using orbital formulas (KSP is doing it like that, more or less).
From a quick look at the code the algorithms look a bit primitive. In particular, division does one bit at a time instead of using reciprocal estimation or a half-width division, and neither multiplication nor division are rounded. You may get better precision with this over double but the accuracy would be worse. 
Thanks, I'll take a look at your code and think about it. The most important thing for me is as I've said to fraillt, not to encourage over complicated use cases, and not having any more overhead than necessary for other use cases that are already covered. Have you read fraillt's notes about the ownership and my reply?
This isn't exactly relevant as an answer to your broad-scope architectural question, but [Herbie](https://herbie.uwplse.org/) is a very cool tool that can rewrite floating-point expressions to improve their numerical accuracy. Its companion tool, Herbgrind, detects floating point expressions in running programs that could be rewritten.
Thanks for sharing! Adding that to my toolbox will help optimize in the future!!
Noted. I'll need to find/make something better. Thanks!
Honestly for CV or robotics in India, I won't know of any openings personally - but if you send an email, we'll look over it and ask around, and let you know if we have anything that would be a good fit.
yeah... but it is so simple &amp; no freak show. If you realize a class ENUM is almost there already the need for a lot of magic goes away. 
Yes, using `std::pmr::polymorphic_allocator` should be supported, however the allocators template argument will be rebound to an unknown type. As mentioned earlier `propagate_on_container_copy_assignment` and friends aren't supported as of now, and this will cause the allocator to be propagated on container move and copy.
One thing to keep in mind with floating point is that half of the entire floating point precision is between -1 and 1. Thus, the scale that you choose for your units is very important, and you also want to make sure you are centered near the origin.
The relative error in the representation is the same wether you are between -1/+1 or -2/+2 or whatever. Scaling your data doesn't but you anything here.
The suggestion isn't to scale all your data to be within -/+ 1 unit in value, but to scale all values such that the bounded range is -1.0/+1.0.
Welcome to the study of numerical analysis! It's a very interesting field and well worth the study in my opinion. Some things you'll want to understand: precision gets you only so far. Once you can describe the position of an entity to 3 or 4 significant figures you won't see much benefit from greater precision. More important is convergence, and phase. Convergence means that errors decrease over time instead of accumulating. If the simulation is not stable, then no amount of precision will help. The reason why super high precision numbers aren't common is that typically, if you can't do it in a double, you can't do it in a long double. Again, if you're numerical scheme doesn't work with a double, it likely won't work with a long double. You may need the precision just due to the scale of the system, but you *cannot* use precision to solve numerical integration issues. Typically you need a stable system to work best, but you're in pseudo-periodic system, so errors *will* hang around as time goes on if you're integrating the forces. One error to pay attention to is phase, if you're simulating N-body Newtonian physics, the phase will drift from true over time resulting in fast or slow orbits. KSP runs it's planets on patched conics, meaning that the planet's/moon's orbits are described by ellipses (or hyperbolas for escape trajectories). These are stable as the parameters don't change over time. The down side is that only 2-body systems can be described. The up side is that most systems are basically 2-body systems. The ISS isn't much affected by the moon. If you're interested in playing with Lagrange points, they won't exist in such system, but much of your numerical issues will disappear. If you're not going to the planets, consider having a local inertial frame for action which itself orbits a body so that locally things are responsive and accurate even if the orbits are a bit out. Check out wikipedia ([this bit in particular](https://en.wikipedia.org/wiki/N-body_problem#Simulation)) and that might give you a bit of vocabulary to start with.
Fixed point is terrible for most problems. When doing you're physics you're stuck to using known exponents for all parts of the calculation. So you will need to know a-priori the exponent for the position, the velocity, the acceleration, the square of all these (for normalising vectors), the forces, and the masses. Often, a fixed precision number won't work anyway. If the distribution of numbers isn't uniform you will *lose* precision as some numbers will be filled by zeros. Now consider that you will have numbers, and their squares (and cubes), they can't all be well distributed in the range. Few people use fixed point for a reason, it's less accurate. A floating point can use the whole mantissa, the whole time.
&gt; For any other position this could be negotiated. Is location/remote-ness also negotiable for such position?
Just for clarity: is Vulkan specialization data similar to uniform buffers in OpenGL?
What platforms did you struggle with? I Successfully compiled and used fold expressions on latest GCC, Clang and MSVC.
I stumbled over this a little late, but I would really be interested to take a look at what you wrote. Did you in fact open\-source it? Especially how you tackled modules support would interest me, as it's something I yet have to add to my own build system.
Nice article! I'd just like to point out that because your specialisation data is densely packed, the elements might not be correctly aligned, so you may get a hardware exception if you access the memory directly through `reinterpret_cast`. To get around this you want to use `memcpy` to copy into and out of `m_data`, i.e. replace get &amp; set with: template &lt;size_t N&gt; auto get() const { type&lt;N&gt; value; std::memcpy(&amp;value, m_data.data() + offset_of&lt;N&gt;, sizeof(type&lt;N&gt;)); return value; } template &lt;size_t N&gt; void set(type&lt;N&gt; value) { std::memcpy(m_data.data() + offset_of&lt;N&gt;, &amp;value, sizeof(type&lt;N&gt;)); } 
I couldn't disagree more... Writing a simple little build system is incredibly easy, up until the point that you want to support complex projects such as QT, which require extra steps due to stuff that's not part of the C\+\+ standard. Aside from this point, making it easier to use than CMake is even trivial.
Here's a neat trick: Defer the division. Assuming all you need is just rational numbers, you can do all your calculations with integers (just operations on nominators and denominators). Consider encapsulating it in a class, and overriding the arithmetic operators. Just be aware of integer overflow. Consider normalizing the fraction periodically. Also integer operations are generally faster than floating point. Converting your fraction to a float where needed should be trivial.
This is the correct response. You can use all the tricks you want, but you're limited by the accuracy of your discrete model of a continuous system. The key word here is: &gt; if you're numerical scheme doesn't work with a double, it likely won't work with a long double.
Is there a single modern system that doesn't use IEEE floats? GPUs are _slightly_ non-conformant but still IEEE mostly.
Boost.Rational springs to mind...
So you're saying it's a realistic scenario to add a folder to your source tree but forget to commit it due to some ignore? Or to accidentally add a file too many, even though it actually breaks your build \(meaning you made your changes without ever checking if they're fine\)? How are your source trees structured that this kind of thing happens to you? I'm genuinely interested to know how such things would happen. *Note: I'm assuming we're talking about reasonable globbing, not CMake's way of caching and thus ignoring successive changes to the source tree.*
Sounds good! I have contributed on a repository which introduced catch tests, and not being on a good computer, the RAM usage is too much. It is pleasing to find this on /r/cpp frontpage in complete coincidence. The unit tests need some tiny adjustments, it's a shame not to have kept the same parameters for the macros.
Just use `double`, like Orbiter.
To be honest, any enum implementation starting with classes is already going down the wrong path. There's no data to encapsulate, there's no polymorphism, there's no reason to use a wrapping class. Especially since said class scan never behave like a real enum, you can't template on its value. I haven't publicized it much but I've written a reflective enum that has advantages over anything I've seen. It's probably missing some of what your described but thanks to now it's written it's extremely easy to extend. I'm planning to submit to boost when I have more time. https://github.com/quicknir/wise_enum. Please let me know what shortcoming you feel this has; it seems to touch most of your boxes and if you have additional shortcomings I'm willing to try to address them.
Lots of good tricks in the comments. This is a cool overview of how Intel achieves low errors in its math library, albeit at a low level. https://youtu.be/KH5fCmOVglc Numerics can be hard to get right. Especially for a game engine, where real time performance is key, you should consider which modelling simplifications and approximations you are willing to make for speed. Best of luck! 
Yeah, I thought about that. I'll look into it, thanks!
Thank you! I understand now! Someone suggested fractional values ans converting to `float` when needed. I'm gonna look into that!
That belong to [stackoverflow](https://stackoverflow.com/) or [Qt forums](https://forum.qt.io/).
Qt does not have a compiler. Qt is a framework, that provides libraries for a whole range of things. Do you have a compiler installed? Your Qt creator also gives you a warning at the top. 
It's common knowledge that uniform initialization failed. It's jokingly called unicorn initialization. Because it doesn't exist. Some people find it somehow reasonable to use combination of AAA style with parenthesis since C++17 because of guaranteed copy elision: auto v0 = std::vector&lt;int&gt;(3, 1); auto v1 = std::vector&lt;int&gt;({1, 2, 3}); With C++20 i think we will get aggregate initialization with parenthesis, so it's gonna be even better. You would be able to use such style for value initialization of old raw pointers when we will get observer_ptr. And if committee somehow will reconsider auto type deduction for nsdmi it'd be the one true uniform initialization, imo.
Do you have a specific example that you're struggling with? Brace initialization should generally be preferred to the old syntax. Advantages: List initialization does not allow narrowing (§iso.8.5.4). That is: An integer cannot be converted to another integer that cannot hold its value. For example, char to int is allowed, but not int to char. A floating-point value cannot be converted to another floating-point type that cannot hold its value. For example, float to double is allowed, but not double to float. A floating-point value cannot be converted to an integer type. An integer value cannot be converted to a floating-point type. 
im kinda new to this c++, i installed from official website.. everything was working fine for past week. now its not working, the run button doesnt work anymore..
I'll most certainly not bother with the whole N-body thing since my mathematical understanding of it is poor at best. What do you mean by the "local inertia frame"? Something like KSP's physics range but that sphere is actually in orbit? And I understand your other point about precision : I need to stabilize (ie. extensively test and debug) my orbital formulas. But don't I lose precision the larger then numbers get? If an orbit gets to 200 AU (29 919 570 000 000 meters), what would be my best approach?
Best way to avoid rounding errors is to not use large numbers. You want physics for solar system. So create a physics world where 1 unit is 0.1 AU or similar. Then entire solar system width will be pretty small and you get no rounding errors. Naturally you would like to have a spaceship in your scene, but spaceship does not have to be simulated in same physics world or rendered in same scene. Simulate/render scales separately. First galactic, then planetary, then local. One gets rendered on top of previous. Hope you get an idea.
I like the refusal of narrowing part. But between the weird things for say std::vector like std::vector&lt;int&gt;{2, 1} resulting in {2, 1} rather than {1, 1}, or for some certain cases with classes with std::initializer_list constructors I have seen some things which resolve differently between different compilers. Then I have to remember how many { to use which still is unintuitive to me. I have many personal examples but they're relegated to memory at this point :) where things just didn't work when they should have. Now, maybe I was wrong, but initializing something shouldn't require so much arcane knowledge so I think the initializing style is the one that is wrong. 
Most vexing parse can be avoided with 'auto o = Object();' syntax.
Brace for list_initalizer constructors and thats it IMO
I think the main reason uniform initialization failed is because of `std::initializer_list`. I have a feeling that if `initializer_list` has had its own unambiguous syntax (e.g. double braces vector&lt;int&gt;{3, 3}; // [3, 3, 3] vector&lt;int&gt;{{3, 3}}; // [3, 3] ) then everything would have been fine... 
If you're simulating space things you're going to hit *serious* precision issues due to the large distances involved. I played around with a custom numeric type (representing meters) that keeps the integer portion of the value in a 64-bit int and the fractional part in a double, which gives me sub-millimeter precision within a 2-light-year-diameter sphere.
&gt; I wish we could somehow have nice looking initializer_list&lt;T&gt; initialization for std::array&lt;T, N&gt;. We have that with brace elision and CTAD: https://godbolt.org/g/jZxKz3
Wrong thread?
bad bot
"std::vector&lt;int&gt;{2, 1}" being {1, 1} would be very weird to me. I don't think of this initialization as calling a constructor for std::vector's case, I think of it as giving it the elements it wants. Just like for every other container, when you give it a list of elements of the container's type.
No uniform buffers also exist in Vulkan and there is no analog that I'm aware of for specialization constants in OpenGL. See [here](https://github.com/KhronosGroup/GLSL/blob/master/extensions/khr/GL_KHR_vulkan_glsl.txt#L131) for the specification
`std::array&lt;int, 5&gt; a = {1,2,3,4,5}` works, though `std::array&lt;std::pair&lt;int, int&gt;, 5&gt; b = {{1,1}, {2,2}, {3,3}, {4,4}, {5,5}};` does not (needs to be `std::array&lt;std::pair&lt;int, int&gt;, 5&gt; b = {{{1,1}, {2,2}, {3,3}, {4,4}, {5,5}}};`).
Look at it's name. Block it and report it to the admins.
Bad bot.
Would love for a language lawyer to comment on that - because that's my impression too: `initializer_list` failed, not uniform initialization. At least, that's what I got away with from Scott Myers treatise on it: the remaining edge cases would be bearable if not for the interactions with initializer_list.
Oh I see. So like constants that you can „inject“ in the shader module upon pipeline creation. Neat! Especially in conjunction with `GL_KHR_Vulkan`.
Oh I see. So like constants that you can „inject“ in the shader module upon pipeline creation. Neat! Especially in conjunction with `GL_KHR_Vulkan`.
I guess, yeah. A better example would have been `{2}` but still same idea. It's just one thing, but doesn't seem to make sense to me at least for initialization this way. It'd make sense for: `std::vector&lt;int&gt; m = {2, 1}` But not: `std::vector&lt;int&gt; m{2, 1}` It's subjective but just how I see it. 
Numerical errors aren’t all rounding errors, and this is hardcore numerical analysis — and unrelated to programming languages. Understanding IEEE-754 is a prerequisite, but this area is a rather big art and the requirements on numerical precision should be an outcome of your analysis and understanding of the problem. You’re focused on data types without knowing what is the error behavior of your integration, and what precision it requires in the first place. You’re also claiming that the errors are “unacceptable” down the road: that’s not useful since you have seemingly no idea as to what’s acceptable. First you must define what is acceptable, and it won’t be just one scenario. Think in practical terms: say you run a copy of our solar system with 128 most massive objects included. What state vector errors do you accept for each object after a year? A thousand years? Do you want your game ship to stay put in a stable Lagrange point for a year? A million years? What do you care about, and what do you not? You should know the what the gameplay requires, then you need to recast that narrative into quantitative requirements, and that will drive your integration scheme selection, the data types, the computational requirements (will it run on an integrated GPU or need four Titans at full tilt?). Step away from C++ and IEEE-754. Do the numerical analysis of the problem first. 
I’m saying that globbing requires a clean source tree and that’s not feasible while you develop. So the scenario that’s most common to you as a human: that of development — is handled the worst. I also presume that you use the source control aggressively while developing. It doesn’t handle directory contents unless you specifically unigore everything in folders subject to globbing. Perhaps that’s the workaround. 
You likely need to add the file to the projects .pro file. Alternatively, start at qt creator and hit file&gt; new&gt; and play with the new creation options. When in doubt and starting out, always let QT create the file and itll add it to the project correctly for you. It is usually the simplest/best way of going about things. 
I agree here. The difference between making an array with 2 elements versus a single element with the number two in it is stark. The fact that these two consequences are so closely tied has caused me many a debugging nightmare when I was learning the syntax.
What exactly do you want, exact physical correct solutions, or just visible correct ones? Which time frames do you want to simulate? If you just want a visual correct representation, there are energy constant integration algorithms. So planets do not change orbit. And decreasing the time step reduces error, but not if you make it to small. Otherwise you could use error controlling, to keep the error in each time step in acceptable ranges. As you still have rounding errors with the max 16 digits precision of double, you will hit a limit with all of them. Then the only solution is to use/ programm a library to use big numbers. But for those you need multiple processor calculations for each calculation you do, so you take a huge performance hit, of course depending on the used precision of your big number. As this is only a list of possible solutions, if one sounds like it could help you ask for it and I will explain it/ link you something.
I'm still confused by your arguments. The only way you'd end up with fibers that have many-MiB stacks is if you let the stacks in the fibers grow that large. That is a design problem, and one you'd have with threads, as well. It's a convoluted situation that I simply don't see occurring in a reasonable situation. If you actually have that much state that you need to maintain, then I'd imagine you'd have similar problems with stackless coroutines. By default, fibers have relatively small stacks, and you can control the initially-allocated space. They will grow just like any stack if you tell them to, but you have a serious design problem if you are legitimately ending up with *huge* stacks in your fibers, while also having *massive* numbers of fibers. The point of a stackfull coroutine compared to a stackless coroutine is that stackfull coroutines are *vastly* simpler to implement, and are in some cases faster to switch to (and in some cases slower, it depends). I am also curious where and why you'd be creating 16 billion fibers. That... seems like a terrible idea. 
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8e4l5b/computer_systems_a_programmers_perspective_book/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Thanks! About the parameters for the macros, catch-mini only supports the minimum features needed for running basic tests, so Catch features like test case groups and sections are not supported. 
I use this in my lunar gravitational model at work, and results were good enough to warrant it. Thanks to the way that particular model is implemented, I was actually able to even use AVX2 intrinsics to accelerate it a bit. Consider ways you can reduce error as well, OP: working in body fixed coordinates when doing force calculations, for example, and making sure to use coordinate frames that let you keep the magnitude of your actual numbers lower also make sure to do things like rendering relative-to-eye, when you start rendering actual things too
My rule of then is to use `{}` only for default construction and `()` otherwise in generic code. 
&gt;auto i{5}; Well, you shouldn't do just that, the type of i are going to be deduced to some weird std::initializer_list that way. You are supposed to use either `int i{5};` or `auto i = 5;`. The former has the advantage of preventing possible precision loss, the latter has the advantage to be able to omit repetitive or cumbersome type declaration.
&gt; Well, you shouldn't do just that, the type of i are going to be deduced to some weird std::initializer_list that way. I think it's possible in C++17, but like I said, I doubt I'll ever use it. &gt; `int i{5};` &gt; The former has the advantage of preventing possible precision loss I think precision loss could be handled with a compiler warning instead of changing the syntax for how variables are initialized. &gt; `auto i = 5;` &gt; the latter has the advantage to be able to omit repetitive or cumbersome type declaration. I don't think hiding the type like that creates readable code. To know the type of `i`you have to know what the standard says about default types, when it's just as easy (in this case even easier) to just put `int` instead of `auto`. Auto makes sense when the type is present on the right side of =, or if the variable is temporary like an iterator or something.
Since C++17 `auto i{5}` is deduced as int - http://en.cppreference.com/w/cpp/language/auto
AFAIK, the only reason for `{}` syntax is to be uniform with arrays, i.e. `int arr[] = {1, 2, 3};`. Otherwise something like `[1; 2; 3]` /`[1, 2, 3]` would work just as well.
First, your wise_enum declares enum values as parameters to a macro function, and uses BOOST_PP_SEQ_* to iterate the values. Then you are limited to 64 maybe 256 values per enum. Not for serious use. You are not alone though, almost every other "smart enum" I have found has this problem. Xenum has no such limit since it defines values through a sequence of macro callbacks. Second, classes are wrong? I want methods on my enums, not global methods. What you propose is to define the enum and all supporting data and functions as globals (in caller's namespace). It pollutes caller's namespace, and easily extensible just means more pollution (classes would encapsulate that mess). When you are in another namespace, you need to call back to some::other::name::space::wise_enum_to_string(enumValue). With xenum you just do enumValue.getIdentifier(), no matter what namespace you are in. Plain OO. In most cases it is not a problem to use a wrapper object instead of a native enum value, it only has advantages (everything is available through the object). There are few cases where you really need the native enum values, like switch(), and as you mention, template parameters can not be objects. In those cases you can just use the native enum, it is easily accesible (constexpr) in the wrapper classes. If Fruits is a native enum, you could say: template&lt;Fruits fruit&gt; class Pie { ... }; Pie&lt;Fruits::apple&gt; applePie; When Fruits is an xenum, you instead say: template&lt;Fruits::_Enum fruit&gt; class Pie { ... }; Pie&lt;Fruits::apple()&gt; applePie; Admittedly not exactly the same, but close enough for me. Sure, I would like enums to become reflective (by getting methods on them), and still behave exactly like old enums so no code ever needs to change. But only a new C++ standard can fix that. Until then there is a price to pay, you propose namespace pollution, I propose just a slight bit of semantic ugliness. 
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8e1pf9/reducing_rounding_errors_in_floatingpoint/dxsja61/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
It's even got backported to C++11 btw
I use `{}` when the type is know and don't use `std::initializer_list`. In generic code, unless the unknown type is supposed to be a struct or a specific concept, I tend to use `()`.
&gt; First, your wiseenum declares enum values as parameters to a macro function, and uses BOOST_PP_SEQ* to iterate the values. Then you are limited to 64 maybe 256 values per enum. Not for serious use. Depends what you mean by "serious". Usually the whole point of enums is that there is some reasonable number of them. If your enum has that many levels, one could argue it's really data more than code and maybe it shouldn't really be defined inline with source, though obviously it's not that simple. I would be open to changing the implementation to simply use a python script to generate what I needed and surpass whatever limit. Btw, do you know how to check the limit? I can't find any clear docs on it. &gt; Second, classes are wrong? I want methods on my enums, not global methods. That may be your preference, but that's not idiomatic C++. In Java or Python, fine. In C++, free functions (global methods are a very weird way to refer to them) are preferred for very good reasons, and ADL helps keeps them from being too verbose. And indeed, one of the reasons why free functions are preferred is on display here: lacking extension methods, there's no way in C++ to add member functions to something existing. So you have to totally replace enum classes (which has a lot of issues, see below) to add methods. Free functions are non-intrusive. &gt; What you propose is to define the enum and all supporting data and functions as globals (in caller's namespace) That's not correct. Only a couple of helper functions are in the caller's namespace, and they take (with one exception) a type that is an implementation detail of the library, so it can't interfere with the user. Almost all the main functionality (and almost any future extension) is in the library namespace. It uses ADL to find the macro generated supporting utility function. `to_string` is generated in the user's namespace taking the enum class so that ADL works, for convenience. &gt; When you are in another namespace, you need to call back to some::other::name::space::wise_enum_to_string(enumValue) Again, this is wrong. If you have an instance variable, you can simply call `to_string(enumValue)` and it just works (ADL lookup). Most of the other interesting things you do with enums, don't actually involve an enum instance, like iterating over the values. In that case, you do `::wise_enum&lt;MyEnum&gt;::range`; as opposed to `MyEnum::range`. It's not really that big a difference, and that works in all namespaces (I didn't understand your example). &gt; In most cases it is not a problem to use a wrapper object instead of a native enum value, it only has advantages (everything is available through the object). it can't be used in switch case, and it can't be templated on directly, which is two of the most useful things to do with enums. And the fact that you can't do these things means that switching to this library is a) a breaking change, b) code involving these enums and those use cases, even though it has nothing to do with reflection, will look totally foreign to people. That's a major unnecessary burden to put on people. We haven't even begun to get into the fun corner cases; what if I naively pass this enum to another library, that uses `std::is_enum` as an implementation detail? What about `std::is_integral`? In a language as complex as C++, I think it's incredibly hard to justify not layering things on top of a vanilla enum so you have basic assurances that it will behave the same way in existing use cases. &gt; you propose namespace pollution, I propose just a slight bit of semantic ugliness. The namespace pollution is very slight as I mentioned. Your approach introduces ugliness into some of the main uses cases of enums, but also is a breaking change, and potentially exposes users to very confusing things with third party libraries. I can believe that your solution is best for you and it's always good to put things out there to help (no downside), but IMHO my library avoids a few major pitfalls and is a better choice for a third party user, by trying to keep the reflective enums as "vanilla" and as idiomatic C++ as possible.
&gt; Why would auto i{5};be better C++ than int i = 5; int i{5}; 
...but we could have just made arrays work with the alternative syntax. E.g. `int arr[]{{1,2,3}}`
&gt; The former has the advantage of preventing possible precision loss Not even that, at least for constants. Uniform initialization allows constants of different types to be converted, even when that means loss of precision. E.g. `float f{3.14}`, where `3.14` is of type `double`. More on it: https://stackoverflow.com/questions/48141187/why-does-list-initialization-allow-conversion-from-double-to-float-values
It looks like cmake-ide is emacs specific, and combines various tools. cquery works with any editor with a language-server implementation and cquery implements the features provided by cmake-ide directly instead of running other tools
With C++20 the following should already be possible: auto v = std::vector&lt;int&gt;({.size = 3, .value = 1}); I'd prefer that or a tag type like: auto v = std::vector&lt;int&gt;( count{3}, 1);
which proposal allows that?
Let me help you out.. "DAE &lt;ANYTHING&gt;" The answer is always yes.
I prefer the good old `Object o;` syntax, I just want an object, why are you asking me for any more info, just give it to me.
&gt; I think precision loss could be handled with a compiler warning instead of changing the syntax for how variables are initialized. Well, `T()` is equivalent to a C-style-cast. Warning on a cast that it is a cast is weird. OTOH: template&lt;typename Integer&gt; std::size_t to_size(Integer i) { return size_t(i); } int main() { std::cout &lt;&lt; to_size("YOLO!") &lt;&lt; '\n'; } Yes, this compiles. And it is the main reason, why I want to see `T()` dead.
That doesn't exhibit the most vexing parse, so how does it relate?
`auto i = int{5};`
For my own project where I serialized classes full of pointers on various tables, I made a custom stack allocator and I flushed it to the file when it was done. It worked well even with nested pointers, but a lot of boilerplate (that can't really be avoided when you have C-style arrays where the size is in a different variable).
I think about this in terms of if the constructor is bijective, if it is, it matches how the c-style of brace syntax and so that's what should be used, otherwise use parenthesis. With this rule, braces means that you can guarantee that what you put in, can be extracted out exactly.
I think it tells you that the mitigation is inserted here so it will affect performance. If you know you can vet your inputs and this function is safe and performance-critical, it allows you to choose to skip the mitigation. In some cases, you can rewrite your function to avoid the need for the mitigation and the associated performance loss.
Or the fact that you can't use move-only types with it. I love `std::unique_ptr` but sometimes it is a big pain.
That's the point, I don't want the vexing parse, and this doesn't avoids it.
You prefer the approach that doesn't avoid something you don't want..? Er, okay...
`Object o;` performs [default initialization](http://en.cppreference.com/w/cpp/language/default_initialization). `Object o();` performs [value initialization](http://en.cppreference.com/w/cpp/language/value_initialization). Replacing `Object o();` with `Object o;` is going to give you different behavior in some cases. Replacing it with `auto o = Object();` preserves the value initialization behavior.
Author of lsp-ui here :), I don't see the previous comment but yes you can easily configure the package with `[M-x] customize-group [RET] lsp-ui [RET]` and disable a feature you don't like.
That's what we do at my workplace specifically because of the ambiguity
Yeah, I meant it would be possible to implement. As long as we don't switch to std2, I don't think the interface will change.
I highly recommend [EurKEY layout](https://eurkey.steffen.bruentjen.eu/start.html). You get all the good of US/UK layout without losing access to symbols of european languages.
This seems an odd choice, particularly given that we should be trying to avoid using c style arrays anyway
Maybe we could have gone the route to make a distinction between ... a{1,2,3} // calls the constructor with 3 parameters and ... a = {1,2,3} // calls with std::initializer_list Yes, that would be different than it used to be but at least there is one guaranteed way to call the constructor and one to use an initializer_list
nice abuse of rules
What stopped C++ from adopting `&lt;optional&gt;` back in C++98 days? If the STL returned `std::nullopt`instead of `container.cend()` that'd be nice.
It's pretty much I/O bounds when it comes to performance. Though you get close to the best you can by making a single write to the file once you have everything in a contiguous area of memory. It was mostly picture data, so for example you'd allocate `m_height*m_width` on your allocator (very stupid that simply changes where it points with a check for overflow). It worked well because it was easy to guestimate the worst case for the size you'd need and allocate that and it was never so big it'd be an issue. I implemented with a template function with 2 parameters (original type and exported type, as many types had useless data I didn't need to save), which defaults to `memcpy` if not specialized. The tricky part was the weak references between objects that I had to preserve, for this I used a map that mapped the former address of the object with the new I gave it. Note that this only worked well because the objects were already sorted (a bit like a linked list) and never had references to objects I hadn't gone through yet. If they did I would have had to use more trickery. In total, that's ~200 lines of code to deal with the serialization of this object and its nested parts. Hard to compare performance because it was made for this thing only. Overhead should be really minimal, time is spent mostly memcpy'ing things around and each object is only copied once in the big buffer then the bugger is flushed out. Hard to be faster than that unless you start going async but that's opening the possibility for bugs to creep in and I didn't want that.
Honestly? I'm pretty sure it was delayed due to arguing over whether optional references should be supported (and what the semantics of assignment would be if they were) and other technicalities like that. It's remarkably hard to design something that's going to be set pretty much in stone.
Interesting article. I think you should use if statement with initialization, like if (auto retV4 = CheckSelectionVer4(sel); retV4.has_value()) std::cout &lt;&lt; "ok...\n"; to not pollute the scope of your function with retV4.
📅 2018-04-23 ⏰ 09:52:48 [(UTC)](https://www.timeanddate.com/worldclock/converter.html?iso=20180423T095248&amp;p1=1440) &gt;Do you have C++ knowledge to share? Speaker applications are now open for our October conference in Sydney, Australia. For more information or to apply now, please visit our speaker portal: https://speaker.pacificplusplus.com/ \#cpp \#cplusplus \#pacificplusplus &gt;— Pacific++ ([@pacificplusplus](https://twitter.com/pacificplusplus)) &gt;🔁️ 0 💟 0 &amp;nbsp; ^(I'm a bot and this action was done automatically)
Good point. So it's good to use optionals only when having no value is natural. Not as an error handling
I would argue that `optional` should be used in functions that take optional parameters, kinda like: `Bar foo(std::optional&lt;Baz&gt; baz, int x)` As you said, returning an optional takes away the error information, instead you should probably be returning something like [boost.outcome](https://ned14.github.io/outcome/) (or `std::variant&lt;T, Error&gt;)`. I'm really sorry to be linking Rust code in the CPP sub, but [this Builder](https://doc.rust-lang.org/beta/src/std/thread/mod.rs.html#259-264) kinda exemplifies what I meant. 
It'd be great to have reflectable attributes in C++. C# implements them without any headache, and it's been incredibly useful there.
Eurkey is also available on all Linux distros that I tried. Just run `setxkbmap eu`. Sadly that only works on X11, I have yet to finda way to use it with Gnome on Wayland.
Haha, sorry, it's an area I'm pretty familiar with and I can get carried away. Have fun with it, get your hands dirty, and be prepared to make changes as you go. Don't take my word for it, start with Euler's method in 2D and see what happens to your planets. Keep your simulation loosely coupled to your display and what not and you should be able to switch out as you learn!
You are an imbecile
Nice. By the way when I was asking about performance I meant of the part that doesn't write to the file. If I end up implementing pointers I'll have that in mind.
And then there's also this defect in C++17 to add to the list: enum class Shape {square, circle}; enum class Color {red, blue}; Shape shape{Shape::square}; Color color1{shape}; // Compiles fine. Color color2(shape); // Fails. Color color3 = shape; // Fails. (Found by [Ólafur Waage](https://twitter.com/olafurw/status/964970638053138432))
Note that I'm not working with file =]
You are right that `has_value()` is redundant. However, the syntax is if (auto retV4 = CheckSelectionVer4(sel); retV4) std::cout &lt;&lt; "ok...\n";
I tend to do this: optional&lt;type&gt; find(container, criteria) // not finding an item is "normal" (e.g. depends on the validity of external data) versus type get(container, criteria) // not finding an item is not normal; exception is thrown if not found (`find` versus a "commanding" `get`)
That's syntax added in C++17, yes, but what /u/konanTheBarbar posted has worked since C++98 (`auto` aside).
One of the goals of attributes is that a program should still be valid when they're ignored, which sounds like something quite difficult if they become reflectable.
good points. my convincing argument is the implementation of arithmetic operators which I think is unique among the libraries I know. It makes the matrix arithmetic very fast especially for large matrices. I am planning to move this to C++17 implementation, especially using futures and promises for time consuming operations. 
Then that goal should be discarded. Or these new reflectable attribute-like things should be something that isn't specifically an "attribute" as C++ understands it, so that they're never ignored. Whatever it takes...
I used `expected` at my last job, and it was helpful. It's sad to see it didn't make it into C++17. Any idea what happened? I don't see a push for it in C++20. v_v
Why would not finding an item be normal in `std::optional`'s case? I've used it quite a bit, and it is rather nice anywhere a null might pop up. It's nicer than throwing an exception because I can tell what the function is doing from it's declaration. I don't have to do into the .cpp file to figure out the intended behavior.
&gt; the implementation of arithmetic operators Not sure what you mean with that. Most (all?) serious matrix libraries out there, including Eigen, use expression templates, avoid temporaries, and allow things like `A = B * C + D;`. A better readme would certainly help (I'd encourage anyone to do that _before_ posting to /r/cpp). Also, cmake files too, if you'd like other people to potentially use it. (and get rid of that `build_all.sh` and `Makefile.Linux.GCC64` cruft stuff). Also, what's "DMScu"? And is your library header-only? Does it build on all platforms (VS2017, Xcode 9.2, gcc/clang)? Etc. - many basic things missing before I would post anything to /r/cpp.
You need to normalise vectors which requires irrationals.
&gt; One of the goals of attributes is that a program should still be valid when they're ignored, which sounds like something quite difficult if they become reflectable. Right, but one of the points of the article is that this seems like a poor goal, particularly for user defined attributes. If the user wants to rely on reflectable attributes, then what does it hurt?
&gt; I suppose there are some hardware for which [[caries_dependencies]] makes no sense, but on such hardware, std::memory_order wouldn’t make sense either, making the point moot. Sorry but this is just false.
Thanks for the overview, and thanks for maintaining it! I will certainly try it out!
It builds on a bunch of other tools namely CMake, to make Emacs a bit more C/C++ "code aware". Better explanation here: https://github.com/atilaneves/cmake-ide/blob/master/README.md
Yes, I opened [this ticket](https://svn.boost.org/trac10/ticket/1094) with some proof of concept code over 11 years ago! Usual story for this sort of thing: Boost.Build/bjam was such an impenetrable mess that I couldn't make any further progress without help, and the Boost developers don't seem to care much about making it usable downstream. Whatever the reasons and technical difficulties, there has been no concerted effort to make this stuff work, and that's a real shame. The work I've done on the cmake side is purely mitigation for the lack of upstream support. People might complain about the CMake support being out of date for each Boost release, but I do the best I can to keep up with the changes, and it's work which a CMake `Find` module should never need to do; it's by far one of the most complex. I took that burden on so that imported targets and such could be used, despite it being hugely painful. Ultimately, I'm just another end user of this stuff who wants to be able to link with Boost libraries and export the dependencies properly. Sad to say, but I'm reducing my Boost usage vastly because of this and other problems (the painful MPL dependencies and other things which should be replaced by C++11 variadic templates but are still ancient and barely maintained). When I've finally ditched it all, `FindBoost` will likely need a new maintainer. Today, I'm dealing with horrible breakage upgrading to 1.67 due to multi_array exceeding the MPL limits. Who knows why, it's been working fine for years until now.
I meant a queue, not a file there. My English is not perfect. Obviously it's probably a vector underneath, but the concept is what matters most when making the algorithm. Especially since you'll be popping it all at once, a vector with a guestimated size will be the fastest.
Generally when I've seen [Option types](https://en.wikipedia.org/wiki/Option_type) in other languages they've been paired with a [Result type](https://en.wikipedia.org/wiki/Result_type) of some sort, for precisely this distinction.
That would be annotations and would probably be a rather similar yet different feature v0v
Have there been any papers on it? Are we really going to just create more syntax for something that really could just co-opt what we already have? 
`expected` and `outcome` are still being discussed in SG14, and /u/14ned is re-implementing Boost's AFIO using ... one of them, I forget which, in order to help others get more real-world experience dealing with it. For reasons I couldn't quite follow, only one of `expected` or `outcome` should be standardized.
C# implements them with plenty of headache; their use of runtime reflection has led to some of the leaky-abstraction-iest, spooky-action-at-a-distance-iest code I've ever seen. Compile-time reflection would be interesting though.
Note that I know of. I kind of like the guarantees of attributes: hints for the compiler so that it might optimize pieces of code when possible, and let a valid code even if the compiler isn't able to optimize them to its fullest; as a library writer it's handy to provide optimizations when possible without having to put checks everywhere. Annotations would be super useful too, but semantically it's a different kind of entity (a new syntax would be easy to find and remember if the committee accepted to extend the standard character set to include @, but it isn't likely to happen).
Stacks in most implementations of C++ require contiguous memory space. That memory doesn't have to be physical, but the address space has to be reserved for use by that stack. Running out of stack address space is one of the few uncatchable errors a C++ program can run into. So you want that space to be pretty large. In a program where threads are relatively rare and expensive (where they, say, represent actual threads of execution) the address space requirements on a 64 bit system are trivial. Coroutines, however, have no reason to be rare and expensive. Except if they are Fibers, one of the things that remains expensive is having a decent amount of stack address space for each of them. Stackless coroutines don't require that stack address space to be reserved, rendering them cheaper along one more axis. No physical memory is used by this reservation, just a note that the memory space isn't supposed to be used by anyone else. If fibers permit reallocation of the stack memory space, it requires that everything in it can be reallocated to a new address space. And if you are trying to pretend this is just normal automatic storage, that doesn't work. C++2a coroutines are not "user-encapsulated" as much as compiler-encapsulated. The state is bounded by what the method uses, and the compiler stores it for the code. In place of a stack frame, it has a coroutine frame. And because the coroutine frame cannot grow without bound (like a stack), this reserves no more memory than the method tries to use in automatic storage. Switching to the coroutine does not require "saving" and "restoring" state any more than calling a method on an object requires "saving" and "restoring" state. Suspending and resuming and awaiting are operations roughly as expensive as calling a function or returning from one (sometimes with some extra overhead to allocate the memory space required for the coroutine). 
My homebrew range-based find returns `optional&lt;iterator&gt;`, because I sometimes want the iterator and sometimes I just don't care. I rarely want to copy implicitly. 
Thanks, I agree. The thing is that performance wise it feels to me that going over the data to accumulate the size is going to be slower than exponentially grow the vector as serialization takes place.
Yeah that's exactly what I meant. You don't have to manually add the "; retV4" this check will be added anyways by the compiler.
A `serializable` meta class + reflectable attributes would be amazing. Manual serialization / macro magic / custom parser + code generator just to turn your class into a json object is getting old.
&gt; C# implements them with plenty of headache; their use of runtime reflection has led to some of the leaky-abstraction-iest, spooky-action-at-a-distance-iest code I've ever seen. It may be a headache for an implementor, but for an user it really allows to have great frameworks. 
Author is. So, my question is : why ?
&gt; I'm speaking from the perspective of having used several of those frameworks (specifically WPF and Entity Framework); they were worse than just hand-coding the stuff they were abstracting over. Sorry but 100% no. I'd ten thousand times rather do WPF than raw windows API. These abstractions are *good*. They allowed people I know who were honestly not that great programmers to ship working apps fairly quickly which is all that matters.
Has an attribute like [[no_ignore_return]] been discussed, or maybe it exists already? Basicly an attribute that tells that a return value must not be ignored. This seems on first glance like a low-hanging-fruit where it can really help writing good/safe code that is self-documented.
Hey, Shakti213, just a quick heads-up: **basicly** is actually spelled **basically**. You can remember it by **ends with -ally**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
[`[[nodiscard]]`](http://en.cppreference.com/w/cpp/language/attributes#Standard_attributes) was added in C++17.
I think it would be good to put more on the readme than a single line repeating the title. Also you might want to show benchmarks comparing to Eigen (big template expressions) and Intel's Embree matrix library (straight forward but should use SIMD intrinsics). Benchmarks for memory or compile times could also be worthwhile or just shown instead, anything that shows an advantage.
Look for [[nodiscard]] in http://en.cppreference.com/w/cpp/language/attributes
Ah sweet! Good to know!
Hehe true, that hit me in the past as well. But usually you can catch these by either just not using `auto` with Eigen (when you want to store something/keep something as a matrix or vector), and/or having tests and running in Debug/RelWithDebugInfo mode.
Well, they're different things aren't they? Attributes are information about the program; annotations that may be used to improve performance, or to make intention clear to analysers, but not to fundamentally change the meaning of the program. If you want that you need a keyword. If adding keywords to C++ was not so incredibly difficult we wouldn't have this discussion, since the distinction between the two is quite clear. But because adding new keywords is such a complete pain, and because sticking them between [[]] seems like such a nice, easy way out, we have this problem. For what it's worth, I feel new keywords should be spelled as normal English words, not adorned with underscores, namespace qualifiers, or brackets (because it is `std::[[__Ugly__]]`). If we need a mechanism to avoid clashes with existing identifiers beyond context-sensitive keywords, let it come in the form of a switch - something to communicate to the compiler that this translation unit is using new keywords and does, in fact, intend for them to be keywords and not identifiers[.] (https://www.youtube.com/watch?v=x0fBiCNiR1E)
 ↑
&gt; in absence of strong guidelines, use attributes to shovel more keywords into the language, since adding attributes is easier than adding keywords: I've got one: [[variant]] union v { }; Pattern matching there u go. Another one: [[metaclass]] class s { }; Hence this: &gt; And letting attributes be everything might make the language an ungodly soup of attributes.
There's no such thing as an irrational on hardware. You'll lose the precision anyway.
Protobuf mostly solves these problems for primarily data classes.
&gt; hardly an "academic vs real code" thing My comment made no mention about this... You seem to eager to pick up a fight.
I'm pretty sure that the c++03 syntax is deprecated. You should either use `if ((type name = expression)) { /* block */ }` or `if (type name = expression; name) { /* block */ }`
I don't get the point of this post. The standard doesn't mandatel
Good points , thanks
I've always found it funny that *everyone* uses underscores to avoid name clashes.
**Company:** [Quantlab] (http://www.quantlab.com) **Type:** Full time **Description:** Quantlab is a dynamic, technology-driven firm supporting a large-scale quantitative trading operation across a wide range of global financial markets. Founded in 1998, Quantlab is an established presence and one of the pioneers in quantitative investment management with a track record of consistent profitability under varying market conditions. We support a very successful proprietary trading organization (similar to a hedge fund, but no outside investors, so we are much more stable). We invest heavily in the latest technologies, seeking to optimize performance and minimize latency throughout our trading systems. As the member of a software project team, you will be doing work that directly affects our ability to trade every day. You will work with a strong team of developers &amp; analysts that will create, maintain and support the tools and software that make up our mission-critical trading applications. On any given day you may find yourself doing the following: * Writing low-latency, high-throughput C++ routines and libraries * Building applications that leverage internal data to meet various reporting requirements * Analyzing enormous amounts of data * Learning how exchanges work; learning how trading works; learning how Quantlab operates. **Our ideal candidates bring many of the following attributes to the table.** * You’re up to date on modern C++ standards and have been using C++11/14 professionally or in your personal projects * Many years of experience optimizing low latency code for nanosecond latencies * Have worked on the full stack including high speed network design, firmware, kernel modification, compiler modification, and distributed system design * Several years of experience writing code using BDD/TDD processes * Thorough understanding of and experience with Continuous Delivery and Deployment * Are able to teach and elevate the team with existing knowledge * Experience in the automated trading domain **Location:** Amsterdam NL **Remote:** No **Visa Sponsorship:** Yes **Technologies:** C++11, C++14, Linux, Boost experience a plus **Contact:** If interested, apply [here] (https://www.quantlab.com/careers?p=job%2FoSny6fwD)
This would be the first I've heard of it being deprecated. Your first option doesn't compile and the second contains a redundant respelling of the name.
Interesting, do you have link about that `std::error`?
https://ned14.github.io/status-code/ Specifically https://ned14.github.io/status-code/doc_error.html#standardese-system_error2__error I should stress that SG14 have neither reviewed nor approved this to go to WG21 yet.
Thanks!
Does Pacific\+\+ have student / volunteer tickets available?
/r/cpp_questions
Why should I use this instead of `#pragma once`?
Good. `#pragma once` helped you notice that the build system is shit.
Why is it confusing? Because people coming from other languages confuse it with init lists?
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8eddkn/question_over_comparing_nodes_in_linked_list/dxuc5h6/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Do you think you could extrapolate on this a bit? Is this true even if the data we are packing is restricted to scalar types? The only UB I was aware of was hardware-dependent struct/class padding.
Could you explain why you think that a whitepaper about using Rust at Chucklefish is on-topic on r/cpp? A whitepaper is usually destined to C-suits, and thus pretty light on technical details, so there's not much material in there. The [AMA in r/rust](https://www.reddit.com/r/rust/comments/78bowa/hey_this_is_kyren_from_chucklefish_we_make_and/) from 6 months ago might be more interesting; though the comments from kyren are quite scattered given the format.
It takes two integers that mean completely different things and with no obvious order. Even if you know that the size+value constructor exists and aren't intending to use some other one, it's very easy to accidentally swap the values.
But one is a size_t and the other is a val&amp;. Are people still using vim or something? 
I think there is a lot of information in this thread comments to help you decide which method works best for you. As for me personally, it's very simple. * Standard compliance * Name clash avoidance 
I hear him mentioning shared and unique ptr? Did you miss that, or are you talking about something else?
&gt;The one thing I'd criticize is that Eigen is becoming a dinosaur, hanging on to C++98/03 and not willing to progressively move forward towards C++14 Have you tried Peter Gottschling’s [MTL4](http://www.simunova.com/#en-mtl4-index-html)?
I've never found that rtags completion nor error checking were as good as ycmd. So I used a combination of both. They both run off the same compile_commands.json so it's not much extra trouble. I do plan to switch to cquery at some point, but right now everything is working perfectly for me so I'll probably wait a while until everything is super mature. Rtags does have some super cool features probably not in cquery, like expandable reference trees and inheritance hierarchies. On the other hand cquery is supposed to make real syntax highlighting possible soon, so win some, lose some, I suppose.
There's at least one codebase that's multiple tens or hundreds of millions of lines that's been using modules already. Of course the ability for such large existing codebases to incrementally adopt modules depends on some functionality that hasn't been put into the modules TS yet. I think widespread adoption will depend on features like those outlined in P0273 _Proposed modules changes from implementation and deployment experience_.
If you are talking about Google, that is certainly non- representative (consider that some companies only now switch to c++11) and do you actually know, if they got rid of include guards at all? To the best of my knowledge, their version of modules is essentially pre-compiled headers on steroids, but has little in common with modules as a language concept.
&gt; and do you actually know, if they got rid of include guards at all? I don't believe they did, and that's a pretty important capability for incrementally adopting modules: code has to work as both a module and as a header for different parts of the codebase while things are moved over. This is one of the reason P0273 is so important. &gt; To the best of my knowledge, their version of modules is essentially pre-compiled headers on steroids, but has little in common with modules as a language concept. I think the modules-ts could fairly be described as 'pre-compiled headers on steroids' and there's a great deal the modules-ts has in common with the earlier work on clang modules.
What does expected have that variant does not?
`auto` and lambdas.. don't forget lambdas! accumulate( v.begin(), v.end(), Eigen::Vector3d(), []( const Eigen::Vector3d &amp; l, const Eigen::Vector3d &amp; r) { return l+r; //kaboom } );
I just had a quick look at it, thank you very much. I was getting very high hopes after seeing that shiny website and reading about that it uses "most modern programming techniques". Unfortunately it went on a steep downhill slope from there. * Is the library header-only? Hard to find out. * The non-standard license (even if supposedly open-source) is a big problem for companies - nobody will use my project if I include this. We need MIT, BSD, Apache or MPL for wide adoption. Actually I can't even find out what license it is (https://svn.simunova.com/svn/mtl4/trunk/). * It is hosted on SVN? Okay, fair enough, maybe it's the author's preference... But not having the library on GitHub (or GitLab/BitBucket) severely inhibits other contributions. Nobody will contribute to this so you'll always be at the mercy of the one (or maybe two) authors. Also the license will probably inhibit contributors even more. * Also I don't really get it, the headers are inside a boost directory? (https://svn.simunova.com/svn/mtl4/trunk/boost/numeric/mtl/) * "most modern programming techniques", but the library supports VS 2005, according to their readme? And g++-4.1.3? Sorry but there is no way this would ever work. Back-porting C++14/17 to VS 2005? Yea sure, welcome to #define-hell. * The documentation looks horrible and was last updated in 2014 So if I was being a tiny bit harsh, I'd say: Very shiny website but completely unusable, wouldn't even come close to choosing this. Or am I missing something? Also totally unrelated but one other problem would be that the integration with Ceres and pybind11 will not be as good though or rather non-existant. With Eigen, both of these work perfectly out of the box, while for another matrix library, there will be some serious amount of work involved. 
Really cool thanks
It's explicit in what it is, which is a good practice to follow. Also, we didn't have variant then, so expected was the only option. Today, I'd use std::variant if I had to.
Yes it still matters, as the compiler will generate code assuming correctly aligned data to access your misaligned data. I'm going to limit my argument to an x64 machine, because it's the one I'm most familiar with. Imagine your specialisation data was a char and a double. So you're going to pack the data like this: 0 1 2 3 4 5 6 7 8 | c | d | d | d | d | d | d | d | d | Where the numbers are the byte offsets, `c` is a char, and `d` is a double. What's important is the byte offsets and alignments of the two data elements. The char is at the start of the buffer so has the same alignment as the buffer (`alignof(m_data)`), the double is offset by one, so has the alignment 1st element of the buffer (`alignof(m_data[1])`). So your `m_data` is a `std::array&lt;uint8_t&gt;`, and as such is (unlikely) to have any alignment restrictions, so will be aligned to an 8-bit boundary** (i.e. no alignment). Therefore we can only guarantee that `m_data[1]` is going to be aligned to an 8-bit boundary. This is a problem because `double` typically has much higher alignment requirements (on x64 it needs to be 64-bit aligned). By using `reinterpret_cast`, you are promising that the pointer being cast satisfies both [space and alignment requirements](http://en.cppreference.com/w/cpp/language/reinterpret_cast), which is a promise you cannot keep (you've invoked underfined behaviour). Now x64 is generally pretty forgiving with regard to misaligned memory access, so it will generally just work (although it can be slow). However, to correctly handle misaligned data in the general case, you should use [`std::memcpy`](http://en.cppreference.com/w/cpp/string/byte/memcpy), as it is capable of copying memory from two arbitrarily aligned buffers and [will likely be optimised into a single memory operation](https://godbolt.org/g/SdZQFW). For more information there is a (better) [explanation of unaligned memory access in the linux kernel](https://www.kernel.org/doc/Documentation/unaligned-memory-access.txt) which is well worth a read. Otherwise, I can answer any more questions where I can :) ** if the `std::array&lt;uint8_t&gt;` was allocated on the heap it would have an alignment of `alignof(std::max_align_t)` [which is typically 64-bits or 128-bits](http://en.cppreference.com/w/cpp/types/max_align_t). However, this doesn't help in our case as a one byte offset from the 64- or 128-bit boundaries is only 8-bit aligned.
Portability would be one reason. Compilers keep providing their own extensions via their own special syntax. Such code compiles with the compiler it was written for, but won't compile with one that doesn't understand these extensions. If the syntax is standardized, then compilers will at least know that what parts of the source code can be ignored. This only makes sense if these "ignorable" parts can indeed be ignored without changing the meaning of the program.
Protobuf is very useful, but the type system they have is extremely limited and they made it *more* braindead in protobuf3.
Lightweight exceptions??
.... You don't check iterator validity before using them??
Do you just need to specify return type to fix that?
Oh that's sinister. Thanks for the pointer, I hadn't considered that.
It's so harmful to have another special case that you can't easily grep for, though. It's okay to do X unless Y or Z, but we'll never see it be a problem in normal development conditions and there's no reliable way to locate it after it goes into master. I really want to make a clang plugin to track these offenses down, but I haven't had the time.
To make things more interesting, there exists `std::valarray&lt;int&gt;(3, 1)` which creates a valarray holding one element of value `3`.
In my case, I have a way of telling the size beforehand. In the general case, it's hard to estimate the size.
Good point. What I meant should be an extra indirection.
Still very much a work in progress, but I'd appreciate any feedback/suggestions to make the tool more usable.
Another library you can try is GLM. It's header only, easy to use and has overloaded operators. The compiler can eliminate unused code so adding a full library shouldn't be an issue.
I've written raw win32 GUI apps, and yeah they were less buggy, faster, and easier to write and understand than both of the WPF apps I've worked on. I certainly wouldn't want to do it; Qt is substantially better than both. But my experience with WPF isn't just "oh it's not fast enough" or "it doesn't have enough flexibility"; it's been full of bugs and bizarre workarounds for bizarre issues. And terrible documentation. We wanted to have a combo box that would display a "please select a thing" message; we couldn't figure out a way of doing it that didn't involve writing out that behaviour by hand everywhere we use it. Inheritance doesn't seem to be a thing. Objects would often get properties set while in strange partially-constructed states - most notably in-between two different setups of the same widget. WPF's property magic sidesteps C# properties - via runtime reflection - resulting in code duplication and weird contortions to make stuff like validation work. And a lot of this is because the "magic" that drives everything leaks like a sieve. It's really hard to write interfaces based on runtime reflection that don't do bizarre things because of the way they reach into object internals.
What didn't you like about Eigen? It's header-only, portable, and if you just go [`#include &lt;Eigen/Core&gt;`](https://eigen.tuxfamily.org/dox/group__Core__Module.html) you pretty much only get matrix functionality, and utility stuff for converting from C arrays and whatnot. It supports [vectorization](http://eigen.tuxfamily.org/index.php?title=FAQ#Vectorization) out of the box. It took a little getting used to its conventions but overall I'm pretty happy with it. Another such library I know of is [MathGeoLib](http://clb.demon.fi/MathGeoLib/nightly/), but while it's fairly lightweight it's not header-only, and back when I gave it a try I eventually ran into some buggy code when I was playing around with vectorization.
I have not so cool computer. When I include Eigen/Core my clion is slowing down a lot
It's going to be hard to find a C++ math library that doesn't rely on templates (which are somewhat slow to compile). I recommend setting up a precompiled header. CLion [seems to have some support](https://blog.jetbrains.com/clion/2017/02/clion-2017-1-eap-pch/). I personally found Eigen's compile-time overhead to be negligible compared to other template-heavy libraries I have to use.
How does this compare to CMake's dependency graph: https://cmake.org/Wiki/CMake:For_CMake_Hackers?
A non-issue for most people, but Eigen is CamelCase.
*PascalCase
Right. Even worse. ;)
I thought about another ide. Tried KDevelop, eclipse cdt but couldn't get used to. Does qt creator have plugin for intellij idea keybindings and generate functions (Constructor/getters/setters/override/etc..)?
As I said, "It will take quite some time until we live in a modules world [where we can get rid of include guards]" Also, I would dispute that the modules TS is anything like precompileed headers. PCH are an optimization technique for compile-times using caching and doesn't touch the language. The modules TS is about changing the compilation model on a language level and about what information gets exchanged between compilation units. Sure that simplifies caching (and that may be one of its primary purposes). But that is a separate discussion. Again, the point of my previous argument was that I don't believe that some form of header guards will become unnecessary in the near future and whatever happens at Google is not representative for the industry at large, because few companies have - such a massive codebase - so good support for making large scale changes - so many people working on tooling
https://i.imgur.com/TUTDVf0.jpg Real men draws dependency graphs by hand :P
In that case you may be better off looking for something that isn't header only! Header only makes things really easy to include but you actually pay a price at compile time and during general use because the whole lib is being constantly evaluated. If so.ething is compiled as it's own translation unit, and you link to it in some way then you're only seeing whatever is visible in the interface from the rest of your code!
For me it is a net win. I do not mean it is better at everything though. But just having code completion working reliably... and if you add lsp-ui, it shows documentation so the context becomes very visible very quickly. I can also navigate and find references... I think the feature set I get is more robust and just what I need.
In his variant and inheritance comparishion he insisted that you have to store your objects as a raw pointers, if you want to put them to vector, and that you have to naked new/delete those pointers
[Direct link](https://speaker.pacificplusplus.com/)
Wondering why you would think qt would have intellij bindings. Intellij keybindings aren't a standard or anything. I'd bet just about everything has vim bindings on the other hand. 
Not all structs, but aggregates – this is what powers the Magic Get library (now named [Boost.PFR](https://github.com/apolukhin/magic_get)).
Yep, they're not even proposed yet. Preproposed. They would be fully deterministic in time and space, unlike current exceptions.
&gt; You have no idea. Even linux folks aren't as fond of GPL as you think. I have personally been bitten by GPL3 being too restrictive, so many don't like GPL and especially GPL3. Wow. &gt; Learning a new language would hardly be a waste of time. It's not that it was waste of time because it wasn't but it brought me more trouble than I would expect. Testing code written in a dynamically typed language is a nightmare (I suppose I should have gone with Python3 in the first place and use its type hints). But not only that. Python will change the semantics of (operating system) process for you. If you think that each process deserves its own address space that no other process has direct access to (e.g. data) then Python (and its broken modules implementation) will "fix" that for you. Have a happy debugging session with that. It's absurd that it breaks such well-established contracts. Bottom line is IMO: writing scripts in Python? Sure, but something more than that? No, thanks. There are a lot of promises around the language which do not necessarily hold. I will lose less hair with statically typed languages. 
Yes, vim bindings are the best of all but they are the best only in vim itself :) Other implementations like vscode-vim, idea-vim are more worse
Good question. Yes but how that will work is not decided. Please email the conference on the email on the website if you are interested. 
&gt; GPL3 What would you do if you spent some time and effort to create something just to realise you have accidentally violated GPL3 and now have to throw everything in the trash and start from scratch. What's more, for the rewrite you need to "dark room" the code now. &amp;nbsp; F\*\*\* you, GPL3! &gt; Python You've summed up my experience with javashit.
I believe I've managed to emulate these events with `CursorHold` and `CursorHoldI` events. I am very satisfied with how it works. It catches basically any kind of movement just like you would expect when using either mouse or keyboard. [Have a look](https://imgur.com/a/dQzvqWv).
You are of course correct. I just never use non-aggregates :)
Looks like Eigen has internal issues. I can't compile it. It always fails with `error: template declaration of ‘const Eigen::internal::FixedInt&lt;N&gt; Eigen::fix’ static const internal::FixedInt&lt;N&gt; fix{};`
Yes, he was right. It doesn't compile with c++14 and 17 only on 11 for me
Scrolling slowdown should be fixed now. Incremental sematic syntax highlighting is now in place so give it a go of you feel like.
Doesn't this apply to literally any library that uses expression templates? Is there any way to make "auto" deduce the result type instead of the template type?
So you're spending a huge amount of effort keeping them all rationals (when trying to approximate irrationals) for what benefit then? Why not go the faster performing route?
It looks a *lot* like cpp-dependencies, specifically the options --graph, --graph-target (although that does not get you your callers, but does get you the full tree for that target) and --graph-cycles. How do you do the d3 output?
CMake outputs the graph that you input into cmake through cmakefiles. This (and similar tools) do not use or look at CMakeFiles, so they output correct information, instead of reflecting back to you what you entered.
Nice work! I see it has a considerable delay, but that is to be expected.
&gt;I’ve seen custom containers taking a size_t in their constructor, but that did a different thing, such as allocating a memory buffer to be able to store that many elements without additional allocation. Said differently, this parameter in this class’s constructor had a semantics of a capacity, whereas the one in std::vector has the semantics of a size. Not following this norm creates confusion. All of what you write is good stuff, except the above I think. A calling function might expect push_back/emplace_back, following your "rule" makes that unnecessarily inefficient, just for the rules-sake.
Oh man, that's indeed really bad - does that compile fine, but then explode at run time?
If you read the article carefully I make the distinction between * Standard attributes ( are ignorable, I argue the should not be) * Vendor-defined attribute (the compiler ignore those if it doesn't know them, that's great) * User-defined attributes (reflection, should always be exposed)
@Dascandy is right, this doesn't rely on the user using a particular build/meta build tool. In terms of the output it generates, my vision is to generate rich, interactive diagrams (which allow filtering of extraneous data so you can focus only on the details you want) rather than static ones.
I agree! Wouldn't it be nice if all projects were so simple and elegant? Alas, in my experience I've had to fight and refactor legacy code structured like spaghetti.
You know you need tooling when graphviz tells you it had to downscale the graph because the PNG format won't support that size.
Yes, protobuf is braindead. Take a look at Bond: https://microsoft.github.io/bond/
I explicitly did mention that my tool is inspired by your work on cpp-dependencies :) The d3 output is based on a force directed graph layout, I've kept the *view* code completely separate so the graph can be rendered using any tool/framework (like graphviz, d3.js, graphml etc.) but I favour d3.js the most because of how powerful/flexible it is. Using Ruby to generate JS-compatible output, that's a fair question. Don't get me wrong, I love C++ but as a language nothing makes me happier than writing code in Ruby (it was designed for programmer happiness after all). Besides, I code in C/C++ all the time so where's the fun in that? I have not bothered to optimise performance for really large projects just yet, but that note was more for *dot* visualisations which can take notoriously long to render. d3.js on the other hand renders hundreds (perhaps even thousands) of nodes/connections fairly effortlessly without any noticeable delay. Cycle detection, yea again I was following the old adage - *make it work, make it right then make it fast*. I'd like to think I am still in the *make it work* phase. This was just a fun project for me to work on during a recent holiday. I am happy to help you enhance cpp-dependencies too :)
&gt; If we need a mechanism to avoid clashes with existing identifiers beyond context-sensitive keywords, let it come in the form of a switch - something to communicate to the compiler that this translation unit is using new keywords and does, in fact, intend for them to be keywords and not identifiers. Don't we already have this switch in the form of `-std=c++17`? If you're explicitly specifying that you're using a standard that introduces new keywords, surely you'd go through the trouble of fixing any errors it would cause in the codebase?
I've also got a rather ambitious TODO list here, we can definitely work together to make these tools better! https://github.com/shreyasbharath/cpp_dependency_graph/blob/master/TODO.md
Delay is most probably due to the client-server communication overhead even though it's based on IPC queue message passing. TU is cached on the server side so no reparsing does not take place. Caching a TU on client side could probably reduce this effect.
Guilty here, but I've only used gem once before. So in Ubutuntu 17.10 I did `apt install ruby-dev` which installed `ruby 2.3.3p222 (2016-11-21) [x86_64-linux-gnu]` but the gem requires 2.4.0. Is this version a strict requirement?
which compiler / eigen version ? You can include it in godbolt for all the gcc and clang versions, and it works just fine on them all.
No 2.4/2.5 is not strictly required but I am using a couple of APIs that are only available in Ruby &gt;= 2.4. It's easy enough to not rely on these and support 2.3 too, I'll make the change soon.
You have to put the READ_TL(registry) inside a header file, which included in a precompiled header. Then the `registry` list is empty.
I see you also have a clone of a Obj-C dependency checker - cpp-dependencies also does Obj-C (and obj-c++). No swift yet - maybe something to add? Don't know anybody who does swift though...
I did `rvm install 2.4.1` and `rvm use 2.4.1` and everything seems to work. Being cutting edge (if Ruby 2.4 even is) is okay. It will stable soon. 
Yes, but it's very easy to miss that.
Compiles fine, explodes at runtime if you are very lucky. When this happened to me I got a garbage result and it took quite a while to spot the problem.
I was going to make a joke about using free functions everywhere instead, but apparently it's actually a thing...
Yes to the former, no to the latter. There was [this paper](https://wg21.link/n4035) but AFAICT it never went anywhere.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8ejz2f/any_tips_for_a_noob_to_get_a_real_understanding/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
When is cell-by-cell multiplication useful?
Tried it on linux and windows with ruby 2.5.0 - both times got a traceback ending with: &gt; lib/ruby/gems/2.5.0/gems/cpp_dependency_graph-0.1.2/lib/cpp_dependency_graph/source_file.rb:44:in `scan': invalid byte sequence in UTF-8 (ArgumentError) Installed via `gem install`. Invoked like so: &gt; cpp_dependency_graph visualise -r project -o graph.html Directory exists and contains my project. Full path also does not work.
http://reddit.com/r/cpp_questions
How do we know the output is correct? How do they know which paths to scan for headers?
Yep, same for [std::begin()](http://en.cppreference.com/w/cpp/iterator/begin) and [std::end()](http://en.cppreference.com/w/cpp/iterator/end), which have already been available since C++11.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8eka46/good_design_strategies_and_programming/dxvy5td/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
You wrote your response in C, not C++. That was probably the issue.
Where in the CPP core guidelines is it mentioned to not use uniform initialization? 
Thankyou for your comment. If it was for a C. How would you rate my answer on scale of 10. What are the blunders that I made?
It's only a problem when your expression type is not memory safe. Eigen is relying on an implicit type conversion to wrap up temporary references before the assignment takes place. Auto doesn't give you that conversion because it deduces the literal type of the expression (not the type that library authors expect you to assign to). Not all uses of expression templates will suffer from this same ailment, however it is very difficult to detect if a particular use does.
Can't say I understand the motivation. It must make template meta programming better or something.
&gt; I have been seeing few success stories on Rust and Go Presumably you meant "a few", although... ;) 
1\- Ok, I understand and maybe agree 2 \- Yes. Agree too, nothing new or special. But if i stop because it's already exist, i will do nothing about nothing. And other resource wasn't clear enough for me, so it's my try. 3\- Not understood you last point \(i'm wrong, okay, but i don't understand what you said, a link maybe ?\) I will learn from your comment, but i strongly disagree with point 2 : everything is already done \(or almost\).
OP is only looking for 4x4 and 4x3 (3D graphics) matrices. GLM is designed around that. Eigen would be a Swiss Army Chainsaw for this situation.
You are right in one way: you should reinvent the wheel, if you study some basics or if you have an idea how to craft the best wheel ever. I think neither one is true for you 😉
uh? Microsoft, Google, Facebook, Bloomberg etc. and thousands more? My guess is all of these have transitioned to at-least C++14. Take a sampling of speakers in CppCon and you'll see that a good chunk of them (and their companies) use Modern C++ in their day to day work.
https://angel.co/stellar-development-foundation?utm_campaign=platform-newsletter&amp;utm_medium=email&amp;utm_source=platform-newsletter C++ and an awesome project
Thanks. Looks interesting.
As far as I remember, the top 4 you mentioned have mostly talked about custom implementations of their data structures or libraries for their particular use cases, which is fine ofcourse. I am looking for comments more like that of /u/jcgretton 
Glad I could help! 
r/cpp is so random... every now and then they choose someone to downvote to oblivion for no apparent reason.. usually they do this to u/Z01dbrg, but today they've chosen you. Interesting...
&gt; PCH are an optimization technique for compile-times using caching and doesn't touch the language. I think syntax is a superficial aspect of modules. It's true that clang modules don't have any c++ syntax so if that's what you're looking at they seem more like PCH than the modules TS. But the important thing is the semantics. &gt; Again, the point of my previous argument was that I don't believe that some form of header guards will become unnecessary in the near future and whatever happens at Google is not representative for the industry at large, because few companies have My point was that proving modules out on such a large codebase shows that it's even easier on smaller codebases. And with P0273 changes can be incremental rather than large-scale, and don't rely on tooling. As long as P0273 is listened to we really can get modules more widely deployed than you might think. 
No sir, that's when you need to modularise your architecture.
FWIW I do not understand his post either. Why do you need reverse_iterators if you do not provide rbegin/rend? Other than that OP is generally right, but also wrong. a) you should be rarely writing your own containers b) refactoring them to work properly is not that hard, biggest problem is lying, aka claiming you have RA iterators so template code blows up after 7 layers of poop and tag dispatch.
The truth is that SOLID is a shit grade acronym composed of shit grade names for concepts far too valuable for such a disgusting cheap way to force their memorization. Those concepts should be understood at at deep level, not memorized to repeat them like parrots. 
You could say the project that I work on at work is a success story, but it would probably be a violation of my employment contract to talk about it without permission. C++ is in a very different position from Rust and Go, in that people actually use C++. There is really no marketing need for articles evangelizing the benefits of C++. It is already an acceptable language to use in most of the domains someone would consider using it. If you work at a company (that is not Google or Mozilla) and you chose to start a new project in Rust or Go it is highly likely that you will not be allowed to. At the very least some people will object and when you are finished or about to release you will be asked to show why it was a good decision to use a minor programming language. This type of article is meant to answer these sort of questions. The reason you do not see this type of article about C++ is no one is being asked to justify using the latest version of a top 5 language.
As mentioned in the `Live at head` video from cppcon, Google is currently transitioning over quarter of a billion lines of code to modern C++. That's probably the biggest refactor in human history. If that is not a success story, I don't know what is.
Good answer. But I do not agree completely with: &gt; ...and you chose to start a new project in Rust or Go it is highly likely that you will not be allowed to. I think we still have a long way to go from being an expert friendly language. Also, there are still a lot of misconceptions about the language amongst the C folks and also spread throughout web which makes its use in newer projects questionable w.r.t Go/Rust as they being new languages have correctly positioned themselves for the domains they are intended to be used.
Looks pretty awesome. I wonder how virtual function calls and templates are handled.
For virtual function calls there are override edges to each function in a derived class, so you can see which functions are potentially called where a vtable lookup occurs. For templates there is a different node for each implicit or explicit specialisation, so you can see all specializations of std::shared\_ptr and all the places where e.g. std::shared\_ptr\&lt;SomeType\&gt; is used.
&gt; The reason you do not see this type of article about C++ is no one is being asked to justify using the latest version of a top 5 language. but you have to consider the position of the "tech" manager who used to code 15 years ago and keeps following "latest tech trends". When he looks at hackernews or r/programming, what does he see ? Rust, Go, Haskell, node.js, Scala, Clojure, etc etc. And so he concludes that C++ is "dead" or at least dying and looks towards transitioning his team. Just look at the number of desktop software which started including node.js for some parts : Visual Studio, Max/MSP, various Adobe software etc etc. In order to touch these guys, it is necesssary to have a regular flow of interesting blog articles which show how to leverage the language to do cool stuff, or alleviate existing pains.
Haaahahahahaha... What do you think a modular architecture looks like?
Not a company, but I've been working for the last three years on a [full-fledged DAW (digital audio workstation)](https://github.com/OSSIA/score) -- still in alpha/beta depending on the parts -- which packs a lot of stuff not available in most commercial-grade music apps. Absolutely couldn't have done it without modern C++, especially for the low-latency audio graph processing - I'm able to handle thousands of synchronous nodes executing in less than 1 ms on a single core... and multi-core is supported.
They think of all paths as possible, and leave out those that are ambiguous. Cpp-dependencies can tell you which it did that for (with --ambiguous). Ambiguous ones cause under-reporting, so you will have missed dependencies if you have ambiguous includes. If you have no ambiguous includes, everything has exactly zero or one option. How do we know the output is correct? Hard to answer honestly. We've compared it to the handwritten cmakelists and in the 20 cases people brought forward where they disagreed, 19 were actually the user being wrong and 1 was a bug (which we subsequently fixed). That gives me a lot of confidence.
The autonomous Staaker drone externsively uses C++14 features in its autopilot software: https://staaker.com/ Developing on an embedded platform in C++ with modern features has benefited our team massively compared to using just C which has previously been the industry standard for embedded development.
Thanks. Really nice to know. Is there any place (tech blog post or something) where I can see what libraries and design choices you guys have taken ?
I suspected it was something like that. It is far more important in code to communicate to the human reading it than the machine that is going to run it. To do this you have to follow standard practices and use normal idioms. This pattern you are using is just something that you thought up, because it makes sense to you. This is not the way that the majority of C++ developers understand the language. When you write code in this manner you are not communicating well. Pointer and reference parameters in C++ have a very specific meaning and that meaning has nothing to do with changing data. If I declare a function like this one void foo(A *a, B &amp;b) what I am saying is you have to give me a valid value for b, but a can be null and I will do something reasonable (by reasonable I mean you would be justified in being angry with me if the first line of foo is assert(a)). C++ has a feature that means what you are trying to use pointer for. That feature is the const keyword. If I declare void foo(const A *a, const B &amp;b) that is what means that I will not change either a or b. The call statement foo(&amp;a) simply does not mean what you want it to mean. The value of a might change or it might not. That is all you will be able to infer from the call site. If you want to know more you have to look at the function prototype. Now, there is another question. Should there be a way to tell at the call site if a function will change the value of one of its parameters? Maybe, however, I can tell you that the C/C++ languages have been this way for quite a long time. People that have been working in these languages for a long time are used to the idea that the only source of truth for this information is the function prototype. Most people have built into their workflow the ability to get to the function prototype very quickly when they need to. Because of that it seems unlikely that most people are going to see a lot of benefit in this suggestion. 
If authors read this I had this usability problem: I installed boost, it did not work, after raging and googling I saw I only installed 32 bit version. There should be some "relationship" between packages so that you get offered 32/64 version when you install 64/32b version.