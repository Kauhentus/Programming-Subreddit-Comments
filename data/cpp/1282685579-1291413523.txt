as alternative for your solution consider to try inline comments int idx = 65; func(idx, true /\* use feature_x */); with a nice syntax highlighting it's quite readable but more important not misleading to the reader. 
Probably not a reason: By using this kind of specialization, you can chose the constant to use by a type and thus at compile time like this. struct period_tag {}; template&lt;&gt; inline constant&lt;int32_t, period_tag&gt;::operator int32_t() const { return static_cast&lt;int32_t&gt;( static_cast&lt;unsigned char&gt;('.') ); } struct comma_tag {}; template&lt;&gt; inline constant&lt;int32_t, comma_tag&gt;::operator int32_t() const { return static_cast&lt;int32_t&gt;( static_cast&lt;unsigned char&gt;(',') ); } template &lt;typename T&gt; void foo(T const &amp; tag) { constant&lt;int32_t, tag&gt; const x; std::cout &lt;&lt; x &lt;&lt; std::endl; }
Possibly the code was intended to be portable to crazy non-ASCII character sets?
Nope: Unicode only. 
Non-ASCII is ever less crazy. There are many people who develop a product for the Americas or Western Europe only to find that they now want to mine an untapped market in Asia.
If you mix up the order of the parameters, I think this solution is worse than no solution. Consider: void f(bool useY, bool useX); f(true /\*use X\*/, false /\*don't use Y\*/); Also, this is a typical case where someone will change an argument and forget to update the comments. I am probably a bigger fan of comments than many these days, but this is a place I wouldn't use them.
Doing it with classes is very difficult, cumbersome and time consuming.
has the various boost functional utilities covered all of this? It looks older than boost and was just wondering if it is still being maintained, etc. I would definitely use it over the equivalent in boost if it is still functional (no pun intended)
I came to the same conclusion a few months back. Having named parameters like this would hopefully be trivial to back fit without forcing everyone to do this. For a new language it might be nice to use this technique to eliminate required function parameter ordering...to the compiler it's just a key/value pair that can be optimally organized into an argument function list during compilation stage.
Assuming you're looking at the same docs I am: someone seems to have had the bright idea of putting a fixed size in the img tags that is slightly smaller than the full image size, resulting in lots of fuzzyness...
Have you ever seen perl code?
Yes. Perl 6 code isn't that bad actually.
Regarding the '=' vs '==' issue, why the hell did they choose to make assignments expressions? assignments are clearly statements and should not be mixed with expressions. 
I've used the Active Object pattern in my latest c++ project (90Kloc) and I found it to be the best solution for threading. My app has lots of custom threads, because it must communicate with lots of external devices over serial and ethernet ports, and each device has its own logic to be handled. By using the Active Object pattern, I didn't have any issues with threads. Yeap, you read that right: I had zero issues. Not only did I have one active object per communication thread, I also had objects shared between threads to be active objects themselves. The thing worked like a charm, with the additional benefit that the GUI was always responsive, no matter what the active objects did in the background. Another benefit of this approach was the automatic performance increase when the app runs in multiprocessor systems. I noticed that in a system with 8 processors, the app was much smoother than in my development system with 2 processors and consumed much less CPU time overall. This is a natural consequence of using threads - the O/S will schedule your threads between the available CPUs, and therefore you have true parallelism. The biggest benefit though is the programming enjoyment. With the Active Object pattern, I didn't have to think about synchronization issues. I just used the objects as if there was no threads at all.
Regarding the class 'map', the features he wants can be implemented via an intermediate type. The expression m[5] could return this intermediate type instead of creating an object, and the assignment to this intermediate type would do the actual insertion. Example: template &lt;class K, class V&gt; class map_entry { public: //getting the value out of the map checks for the element operator const V &amp;() const { if (_it == _map-&gt;end()) throw exception(); return _it-&gt;second; } //assignment map_entry&lt;K, V&gt; &amp;operator = (const V &amp;value) { _map-&gt;insert(_key, value); } private: map&lt;K, V&gt; *_map; K _key; map&lt;K, V&gt;::iterator _it; }; template &lt;class K, class V&gt; class map { public: map_entry&lt;K, V&gt; operator [](const K &amp;key) { return map_entry&lt;K, V&gt;(*this, key, find(key)); } }; So when using the code: //will throw if m[5] does not exist std::string s = m[5]; //creates/replaces m[5] m[5] = "foo"; 
good references, thanks! :)
The draft standard which is available from the internet for free in PDF format is pretty useful.
Seems like a cool idea, but I'm guessing that this is a non-standard extension and therefore has limited or no re-usability outside of this specific compiler.
Use a spellchecker ffs.
No. Streams encode numbers in ascii. Basically he's talking about a text-based format.
If you ask me, RAII is *The* one thing every C++ programmer need to understand and use. Its also *The* thing that makes C++ better (as in safer) than other popular languages (that lack destructors). Beacuse of RAII, I could not imagine using a language without destructors. Edit: Hi Jack!ing my own comment here. *What does the downvotes (to the link) mean? Are people downvoting the subject of RAII, or the writers take on it?*
I have a problem with RAII. You are putting too much responsibility on the "}". With not enough programmers understanding what is RAII, using it is very dangerous and error prone.
I strongly disagree, destruction at "}" is very intiutive. After all thats where all the values of built in value types get destructed. "}" is where the visibility of a variable name goes away. Its a compilation error to try and use a variable name after that point. What is so clever with RAII, is that the visibility of an object is exactly the same as its lifetime. This is very intuitive. All the built in value types behave like this, its only pointers and values of user defined types that might not. With RAII, you can make sure that even these types behave as intuitively as the ints do (If in doubt "do as the ints do"). If you dont use RAII, then you have to force your users to remember to manually call some release method of your class. This is hard to do correctly for your users if there are multiple exit paths in his code, and even if he only has one return statement he will still have multiple exit paths if he is calling any function that might throw (not to mention what will happen if users try and use your object after it has been released but before its name has gone out of scope). Do your users (and yourself) a favor and make the lifetime of your objects as intuitive and predictable as that of the built in ints. Edit: I'll give you an inch though.. with something very time critical like a mutex, you might want to add an extra subscope (a "{" and "}" pair) to make sure the mutex is released as soon as possible. That might look abit weird to users not familiar with RAII.
Honestly, if your programmers can't handle RAII, you need to reconsider either your choice of programmers or your choice of language. C++ isn't a language that's friendly to developers who don't know what they're doing.
Sadly they weren't my programmers... I was just one of the crowd. And that was one of the reasons why I left.
The difference between int and a RAII class is that int doesn't effect other values in his life time (span?) (I'm referring to ScopedGuard and ScopedMutex and etc). It's not enough to create a class that will function as a RAII, *you need to educate your colleagues. *You need to name your class and instance with a proper "warning" name. *You need to give the class 2 interfaces one for RAII and the other for normal usage. *And... usually a big comment to warn people. I agree that: "RAII is The one thing every C++ programmer need to understand and use", but as long as not every C++ programmer knows what RAII is, than it is very dangerous and error prone.
Well, I dont really agree with the one class two interfaces idea. That Would be confusing. Just have one class accuire a mutex in constructor and release in the destructor. And if your users, for some strange reason, dont want this easy to follow one step usage. You can give them another class that does it the old error prone manual four step way: construct, accuire, release, destruct. What you dont want to do is combine these interfaces into one class. Because then by the time your destructor is called, your user might allready have released the lock manually. And you would have to keep some internal flag for this (and your accuire func would most likely do nothing).. but worse it would be confusing to your users. RAII isnt that hard to explain to users. Its: call constructor, that it. And maybe add that if lifetime is very time critical make sure the users enclosing scope "{}" is as short as possible (which is a good idea anyway). Edit: From the point of view of a user. The interface of an built in value type like an int and a RAII class that wraps some resource that has to be accuired and released (be it a pointer to dynamic memory, a lock, a file handle, a databse connection, etc) is very much the same. The user define a value of the type and uses it. No accuring or releasing neccessary. Two step construction is a bad idea. You dont want any init/accuire or destroy/release type functions other than your constructor and destructor. Because that would mean it is possible for your object to sometimes be in an invalid state (after construction but before initializiation, or after release but before destruction). With RAII such situations are impossible. (Actually int falls abit short of proper RAII, as you can define an int without immediately initializing it. I would be happy if the standard made such things illegal ... but that would break C compability.)
&gt; It's not enough to create a class that will function as a RAII, you need to educate your colleagues. You need to name your class and instance with a proper "warning" name. You need to give the class 2 interfaces one for RAII and the other for normal usage. Wow, holy shit. What? No, dude. RAII is [one of if not] _the_ fundamental idiom in C++. Giving a class a RAII and non-RAII interface is frankly dumber and worse than writing C++ as if it were C-with-classes. (Reading below) if your colleagues couldn't handle that, man; true pity to you, sir.
Not sure I can agree that it is more "error prone" than not using it. If you really have a problem you can play games with "new" to discourage bad behavior. I worked on a project once where we wanted to use exceptions. I asked someone to demonstrate how to make some code exception safe. When they failed to use RAII, we dropped exceptions, but they all got to learn RAII anyway. Exceptions were to difficult to learn quickly, but RAII was not.
2 classes? How about one class: //knows RAII { RAIIClass foo; } //Doesn't know RAII { RAIIClass* foo = new RAIIClass(); delete foo; }
Looks neat. Now I just need something to use it for...
If you can write a program in a language like Python, meet all of your requirements and still achieve satisfactory performance, by all means do so. That said, it doesn't matter one bit what scripting languages you prefer if your choices are C or C++, which is oftentimes the case.
Can't say for sure; maybe the developer had already been using types to represent each token for other reasons, but needed a way to convert back to integral constants? One possible reason is that it allows you to overload a function based on the type of the token. e.g. struct period_tag {}; struct comma_tag {}; void do_something(period_tag) { std::cout &lt;&lt; "got a period\n"; } void do_something(comma_tag) { std::cout &lt;&lt; "got a comma\n"; } Without more context, though, it's hard to call the code in the OP anything but peculiar.
That was a really poor article, by an author who seems to have been forced to write an article by their editor, on a topic they are not too familiar with. Cringe worthy.
Understanding what "scope" means is fundamental to the langauge. If a programmer doesn't understand that then they shouldn't be using c++.
I think it's pretty damning of C++ itself as a language that people writing about it are able to get away with rehashing the same content other people were writing 10-20 years ago. Compare the C++ articles that are posted on reddit as brand new and then see how many identical articles you can find in old trade magazines from the 90s. Hell, look at the argument this article alone has generated in the rest of the comments. Why are the merits of RAII even up for debate anymore? This isn't a contentious topic or a purely philosophical matter--it should be taught to kids in their fucking first semester and should only take a couple minutes to explain. And the really sad thing is, I know plenty of C++ developers who have been programming for 20 years and still don't use these concepts because spending 5 seconds to understand them is too much effort.
Another redditor posted the best possible summary in a comment slightly above yours - *if a programmer doesn't understand scope, they shouldn't be using C++*. This isn't a problem with the language per se, it's just a fundamental component of a stack based language. Until someone figures out how to make a data flow based language, we're stuck with a stack based language, and at this point in time, there is nothing more powerful (speed, memory utilisation) than C++.
Quite so, the transaction-analogy doesn't really hold up well... You can extend the RAII principle to cover transactions, but that's a further step. RAII is mainly about automatic clean-up, which is of course included in a transaction that does a rollback, but a transaction really is more.
Like I said... 1 of the reasons i quit that place.
Yes, I agree. RAII is the most basic (and important) concept in C++. Stroustup came up eith it way back when? The reason I linked this article at all is that there are still frustratingly many c++ programmers who dont get it. I can tell by reading the comments of (other) r/cpp threads, looking at c++ code in the wild, and worst interviewers never ask about it! While not the best article, the guy who wrote this article usually catches peoples attention. By dumbing things down, using a confronting tone, and hyperboling things quite a bit. So I figured it might get to Those people. The wikipage on RAII isnt all that good either. For a better explanation of RAII any Scott Meyers or Herb Sutter book would be better. *Im confused about what the downvotes to this link mean, though. Is it that people think the author sucks, or that the topic sucks? As in is it people ignorant about RAII or people who understand it, that are downvoting?*
Yes, the transaction analogy (commit 'n rollback).. is more appropriate for strongly exception safe code, where you do things that can throw to the side, to say a pointer and commit by swapping pointers (that never throws). Its hard, if not practically impossible, to make object strongly exception safe, without RAII.
Oh, please no! Hide operator new, and when someone looks at the header file wondering why they cannot 'new' an instance of the class, they can read your comment about RAII which happens to reference http://en.wikipedia.org/wiki/RAII . Anyone who allows an RAII class to be heap allocated is just inviting lots of problems!
Heap allocation of RAII objects is part of what one needs to do. That said, you *can* force the use of smart pointers. The example above is just demonstrating that there is no reason an RAII class need appear surprising to someone who doesn't know RAII.
&gt; The example above is just demonstrating that there is no reason an RAII class need appear surprising to someone who doesn't know RAII. This I agree with. The concept is simple. Many developers I work with didn't know the name RAII, but have seen and understood the concept. When presented with the name, almost every one of them said something along the lines of "oh, there's a name for that?" &gt; Heap allocation of RAII objects is part of what one needs to do. Please supply me with an example. I have not yet seen a situation where one needs to heap allocate RAII objects. &gt; That said, you can force the use of smart pointers. How exactly can you force this so that you get compilation errors when the rule is violated? Or were you saying you can enforce a coding style?
&gt; This I agree with. The concept is simple. It's not even that. Even if they don't grok it for a second, the old approach of using bare pointers and explicit deletes works fine with classes that are RAII. &gt; Please supply me with an example. I have not yet seen a situation where one needs to heap allocate RAII objects. std::vector&lt;RAIIClass&gt; raiiList; &gt; How exactly can you force this so that you get compilation errors when the rule is violated? #include &lt;memory&gt; #include &lt;cstdlib&gt; class RAIITest { public: RAIITest() {} ~RAIITest() {} static std::auto_ptr&lt;RAIITest&gt; create() { return std::auto_ptr&lt;RAIITest&gt;(new RAIITest()); } friend class std::auto_ptr&lt;RAIITest&gt;; private: static void* operator new(size_t size) { return malloc(size); } static void operator delete(void* p) { if (p != NULL) { free(p); } } }; int main(int argc, char** argv) { RAIITest test; std::auto_ptr&lt;RAIITest&gt; test_heap = RAIITest::create(); RAIITest* fatChance = new RAIITest(); //won't compile return 0; } Note: I'm not recommending this approach, just pointing out that it can be done.
is this a one man project that might die off when the dev lost interest or it is funded for commercial purpose?
Yay. If it actually works as advertised we should get some nice language bindings for C++ libraries. As it is, that is the stumbling block - C++ is too hard to parse. A lot of libraries, especially template heavy ones, just don't bind well. \*edit\* never mind it's already abandoned.
He's already given up. Read the end of the thread where he realizes he just cannot compete with clang: [http://old.nabble.com/Scalpel%3A-a-Spirit-Wave-powered-C%2B%2B-source-code-analysis-library-td29612157.html](http://old.nabble.com/Scalpel%3A-a-Spirit-Wave-powered-C%2B%2B-source-code-analysis-library-td29612157.html) Here's the final nail in its coffin from the author: [http://groups.google.com/group/scalpel-users/browse_thread/thread/1231dcdd7d951009](http://groups.google.com/group/scalpel-users/browse_thread/thread/1231dcdd7d951009)
My first question was going to be, why didn't he just contribute to Clang? :p He must be talented to have worked on this though, he should simply join their efforts.
Soooooo.......... obfuscated code will be understandable?
Cute. Just to see if I understand this correctly. So the private RAIITest::new is there to hide it from outside code.. and the friend declaration is there to unhide the private RAIITest::delete, so that std::auto_ptr&lt;RAIITest&gt; can delete its wrapped RAIITest pointer?
Good read, but I would also love to see the same type of review/coverage of the non-concurrency related changes from the meeting.
The closest would be Herb Sutters [trip report](http://herbsutter.com/2010/08/28/trip-report-august-2010-iso-c-standards-meeting/) from the meeting.
I though the issue with trying to keep the sytnax for atomic types compatible with the upcoming C1x was a bit funny. Let me hyperbole: *we tried to keep civil with the C folks, we really did, but those bastards just would not play nice.*
Yes, that is it. Obviously you want to be more flexible than this and allow different kinds of smart pointers to RAIITest, and there are ways of grabbing the pointer from an std::auto_ptr&lt;&gt;, but you get the gist of what I'm saying.
Actually [this](http://onlamp.com/pub/a/onlamp/2006/05/04/smart-pointers.html?page=1) is a better article about smart pointers.
Thanks. I didn't know part 4 was out yet. Looking forward to watching it.
Part 1 and 2 where kindof basic, but this one was on another level. The traits template meta programming technique was very interesting. Also the discussion about smart pointer implementations was worth the view.
http://learncpp.com/ helped me get started programming in C++. The tutorial there teaches you not only what, but how and why. It will go over what the compiler does when you write code in C++. If you're on Windows, you can download the Code::Blocks IDE with the MinGW compiler. It has everything you need to get started.
true but the article is too short.
Using a profiler sure, looking at assembled code means that you are probably taking so fucking long new cpu's will be out by the time your done nowadays. Edit: His example was just inlining functions in a inner loop, which is done by compiler optimization in a lot of cases, and any good developer will know that function calls are very expensive. You don't need assembler to find that out, the profiler will tell you StupidCall() is using 40% of your execution time.
So, if you don't use exceptions, there is no problem?
No, it just means there are slightly fewer ways you can leak. Exceptions or no, you should be using smart pointers everywhere you use dynamically-allocated memory.
there's more to memory management than smart pointers
Keep in mind this was written in 2006. TR1 was just a draft proposal and `std::auto_ptr` (deprecated in C++0x) was state-of-the-art.
I meant if you use the headline "memory management in c++" then the article should mention memory pooling, small object allocation, garbage collection etc etc. Otherwise just call it "smart pointers in c++". Well, never mind - that's just what I am interested in. BTW, does anyone know a free implementation of a small memory allocator? The Loki version does not compile for me (and Alexandresku admits himself it is not that great). Boos::Pool also sucks imho. Is there no free solution that I can just snap in?
No, I was coding in 2006 and even then I knew there were large problems with std::auto_ptr. The problem was there was no standard replacement, you basically had to steal a working smart pointer from Boost.
So (nearly) all of Qt is doing it wrong? This is an overly narrow and simplistic view that misses the central point. The real point is that anytime you dynamically allocate memory you should have clear and well-understood ownership semantics for it. You shouldn't have to guess about what is responsible for deallocating that memory. Smart-pointers provide this, but there are other equally-valid solutions. Saying that *only* smart-pointers fulfill this role is cargo-cult bullshit.
What *"other equally-valid solutions"* are you referring too here? Naked pointers to dynamically allocated memory are unowned resources, so you better take ownership by wrapping them up with smart pointers or with your own RAII object.
Fair enough. I chose the more general title as a bait, I admit. Whats wrong with boost::pool though? Interface looks good to me.
For instance, Qt uses object-trees with well-understood parent-child relationships where parent objects own their children. It's a fairly common idiom. Parent-deletion is guaranteed to delete child objects. Users specify a parent object on construction of a child (or defer parent-assignment for later, prior to which the object is unmanaged). You can even mix and match this strategy with smart-pointers. The parent object provides RAII but it's not strictly a smart-pointer. Clients use raw-pointers for the most part but the ownership and management semantics are well defined without requiring smart pointers everywhere. Sure it makes sense to use smart-pointers internally for parent-child relationships but that's an implementation detail not exposed to clients. Client code only (usually) ever deals with raw pointers. The point I'm making is that there are other sane, viable management mechanisms besides smart-pointers. Smart-pointers explicitly annotate a specific storage location with a memory management policy. It's equally valid to have an implied global memory management policy for certain objects. Hell, a GC is just an implied global memory management policy. Likewise, it's possible to drop in a GC (on supported platforms) in C++ on a per-allocation basis. This is could be super-handy if you're using a throughly-borked 3rd-party library that leaks like a sieve. I'm not arguing against smart-pointers. I use them frequently and they do a great job of solving the problems they were designed to solve. But heralding them as the *only* way is bunk. 
In my opinion part 3 was best.
last time I checked, the pool doubled its size each time it is full. There was no way to change this without hacking, and this behaviour sucks when you have gigabite size pools. 
std::auto\_ptr was part of the 1998 standard; it was not "state-of-the-art" in 2006. scoped\_ptr, shared\_ptr, and weak\_ptr were are pointers being considered for TR1 in 2006.
There are places in Qt that could definitely benefit from RAII. setOverrideCursor / restoreOverrideCursor is one of bthem. We created our own RAII object to wrap the override cursor functionality, but that's something that should be handled by Qt.
link's already dead. [new link](http://software.intel.com/en-us/blogs/2010/09/03/simd-parallelism-using-array-notation/) 
I am using Qt and shared pointers extensively, but I do not see what the advantage of the Qt memory management model is. Shared pointers is a better and more generic solution. I too wished Qt used shared pointers. 
I also mix Qt memory management with shared pointers. And I think that is one of its strengths it's flexible enough that you can mix it with other memory management policies. One possible advantage is that you can force-delete something in the Qt model. Which isn't very easy with vanilla smart-pointers. For instance, even though object A's lifetime is managed by object B I can explicitly delete object A and object B will release its ownership. Also, I consider it a minor syntactic win to not have everything littered with smart-pointer types. There are also disadvantages vs. smart-pointers, I'm not arguing that the Qt system is better, just different, and equally viable. 
Yes, I also use often custom RAII objects with Qt. For example, when direct painting and you inevitably want to save/restore the painter state. It's much more natural to have an RAII object take care of the saving/restoring for you, and then put all your drawing examples in a scope with the RAII object. I'm sure there are many places where little RAII additions could improve Qt. But I'm specifically talking about their memory management implementation, which does provide RAII (although it doesn't force it).
&gt; One possible advantage is that you can force-delete something in the Qt model. Not that I really disagree with your opinion, but, why would you do that if you used smart pointers? isn't it redundant to force deletion when the smart pointers will do the job? Otherwise, if you are sure that you want to delete something, using just raw ptrs will do. 
Yeah, I haven't had serious problem with Qt memory leaks. I do have a lot of complaints about the design of Qt (moc files, the SIGNAL and SLOT macros [heaven forbid you make a typo], ....). Qt was originally designed a long time ago, so many of the decisions are forgiveable. It's much better than MFC on Windows, is cross-platform, and is more complete than competing frameworks, so it is still my preferred framework. It still feels a little crusty at times though. Oh well!
Wow, my head hurts abit after reading that. Amazing what you can do with C++. I just wonder what the error messages are going to look like for the poor user who makes a syntax error in the domain language. "*What is this? I dont even...*"
The next article will be about syntax checking and error reporting. Proto make it very easy for a DSEL author to give their users meaningful and *short* error messages.
Cool, cant wait. Keep up the good work!
Great series of articles. 
&gt; So (nearly) all of Qt is doing it wrong? Yes.
What are the errors?
via: http://gael-varoquaux.info/blog/?p=141 Multitouch with VTK (and MedINRIA and Mayavi)
This part and the one before seem to be more about Nurikabe than STL. Maybe it would have been better showing off STL with many smaller examples than with one big program. Just my two cents..
Fuck yeah, hello new holiday project.
thanks this helped me out too
Perhaps a few smaller ones as well would have been nice to cover as well, but the size of the Nurikabe example is ok. The main problem, for me at least, is that I feel like I'm lost in all the domain details, having never even heard of Nurikabe before. Wikipedia provides a good overview, but it's hard to get a good feel for the rules without actually playing through a few games. A more well known game like Sudoku would probably have been more instructive.
Don't know anything about Bada (never heard of it till now), but with HTTP the client is usually interested in the response. Just from looking at the API quickly I'm guessing what you're interested in is the function Osp::Net::Http::HttpMessage::ReadBodyN(). Don't forget to check the HTTP status code to make sure the message the server sent you is really content and not an error.
In my opinion, [C++ template errors suck but not because of templates](http://thegreatlambda.blogspot.com/2010/09/c-template-errors-suck-but-because-of.html). 
Thanks for the reply. I'm going to check the function out now. I think I had before but I'm a little more educated now from trial an error. I am a Java developer assigned to making a Bada application in C++ :s Also, I have been checking the status code and I think it is coming back ok. Gonna go test this now. Thanks again
Sometimes I regret abandoning C++ for the C/Python combo. This is not one of those times.
I'm not saying the error messages are justified but this is just pushing it expression = term &gt;&gt; *( ( '+' &gt;&gt; term ) | ( '-' &gt;&gt; term ) ) ; term = factor &gt;&gt; *( ( '*' &gt;&gt; ~factor ) | ( '/' &gt;&gt; factor ) ) ; factor = uint_ | '(' &gt;&gt; expression &gt;&gt; ')' | '-' &gt;&gt; factor ; ಠ_ಠ
You can do a bit better than that with C++0X. `std::unique_ptr` and `std::make_shared` are quite an improvement, thanks to the new Movable concept.
blasphemy! 
To be honest, two "+"s can be arranged (with some smooth deformation) into one "#" already.
You should post this to /r/cpppp
I prefer to think of the "#" as a not-at-all-equal-to sign i.e. two "≠"s.
No, it's not. C# is a half-step above C, music wise, and that's where the inspiration for the name comes from: http://en.wikipedia.org/wiki/C_Sharp_(programming_language)#Language_name What you see is merely a coincidence.
Fun *humanity is doomed* message of the day: This is the top voted submission in the last 4 months for this C++ group (or was a few seconds ago)
I wonder how many hits http://www.reddit.com/r/cpppp got today because of your comment.
*"Like all the standard type traits, we will derive our class from std::true_type if the type T meets our requirements, and from std::false_type if it doesn't. [...] The primary template will take care of the case where is_class_type is False; if is_class_type is True we fall through to the specialization"* I dont get it. I see no class deriving from true_type.
so c &lt; c# &lt; c++ ?
The article seems to disagree (after it agrees). &gt; By coincidence, the sharp symbol resembles four conjoined plus signs. This reiterates Rick Mascitti's tongue-in-cheek use of '++' when naming 'C++': where C was enhanced to create C++, C++ was enhanced to create C++++ (that is, C#).[citation needed]
i click because of yours
The example code can be downloaded from the paperclip icon at the allegro.cc thread.
What errors are you getting?
'Osp::System' has not been declared Also, I know I have essentially the same thing written 3 times, it's just to show my attempts.
According to [the docs](http://static.bada.com/contents/docs/apis/bada-V1.0.0a3/framework/classOsp_1_1System_1_1SystemTime.html) you need to `#include &lt;FSysSystemTime.h&gt;`
Wow! That works perfectly. Thank you. I have a working example that doesn't include that so I would have never come to that conclusion :) Thank you &lt;3
You lost me at List l = List(); 
I thought it was C--------
I don't understand the usefulness of it. Pretty much I hold the position with the first guy who posted in that thread, though it does seem like a cool concept if nothing else.
First of all, I'm a huge fan of the pimpl idiom. Qt uses it very cleanly, but doesn't prevent you from accessing the internal implementation (see Qt's [QDomNode](http://doc.qt.nokia.com/4.6/qdomnode.html) from their XML subsystem). QDomNode is a value type, and the biggest advantage over pointers or references is that you can chain operations: QDomNode someChild = someDocument.firstChild().namedSibling("name").firstChild(); This example is a bit contrived, but if namedSibling() cannot find the node it returns a "null" QDomNode that can still be chained. Instead of checking the result of each function call a single check on the result is sufficient. I think this saves a lot of time and demonstrates the power of the pimpl idiom. However, the forum post linked here is a bad idea. Instead of making things easier it forces the programmer to add another layer of encapsulation around basic data types and adds complication without benefit. Inheriting from the base and storing an internal "OBJECT" forces each function call to cast it into the correct type. See String.cpp: //add a string String &amp;String::operator += (const String &amp;str) { static_cast&lt;STRING *&gt;(impl())-&gt;m_string += static_cast&lt;STRING *&gt;(str.impl())-&gt;m_string; return *this; } The pimpl idiom is not designed for inheritance.
I don't get 'pimpl'. It hides the implementation. Good. It speeds compilation time. Good. It slows down runtime performance since you dereference the pimpl per function call. It's an extra redirection. Very bad :( If you really, really have to hide the source, sure, but otherwise it seems like a waste. It makes the language slow to benefit the developer. That's what java was invented for.
It really helps compile time (2x or more -- I've tried it on my own code). So, faster builds -&gt; more productivity -&gt; more time to improve performance where it really matters.
How about a c++ like SDK for java?
This suggestion from someone commenting on the original site is also good: &gt; I guess its ok. You would probably want some way to create java-style objects as well. class MyFoo: public Object { }; ... MyFoo foo = new MyFoo(); Everything requires an unnecessary heap allocation and an overloaded assignment operator that has to somehow know when it should and shouldn't delete the argument -- thanks for the awesome idea, Java dudes! I'll go implement that right after I decide to get fired.
&gt; The pimpl idiom is not designed for inheritance. Why not? you say it's bad but you don't give any reasons why it's bad.
If the implementation is hidden in an internally allocated struct, there is no need to use 'new'. So the above code is a way to declare variables. It could also be this: List l; 
The real benefit is programmer productivity. Just like when you use higher level languages, with such a library you could go low level if you wish, but you wouldn't need different tools, you would do it from the same IDE and language. 
Well, its not a bad idea and I would claim that many projects end up with something similar in their design. The issue is (and this is the real power of C++) that you will end up using this approach in a localized area of the project. If you want something more robust, that could be applied to a whole project, look at COM (you can do very neat and polished re-implementations of COM) or pure OOP approaches like message passing.
Yeah, I know. I meant that this line calls a constructor and then a copy-constructor which is inefficient and unneeded. (maybe a clever compiler will optimize it out, but why base on it).
Yeah, sure. It's more inefficient that this: List l; but it's not that inefficient, since what it does is a shallow copy.
Still an unneeded inefficiency with no real benefit.
&gt; Why is doing it with exceptions better because it's the C++ way of doing it and you're writing C++. Returning -1 also forces you to write more code (the nothrow and the if test).
It's situational but exceptions are generally cleaner. As a generic answer, I'd say handling bad_alloc on your own forfeits the benefits of exceptions such as stack unwinding and resource releasing through destructors. Or at least puts the burden of those things on you. There's also an old argument about whether exceptions hit performance but that's mostly compiler specific. Plus if you're calling new() enough times in rapid succession to warrant performance considerations then you should probably rethink your memory management.
If memory allocation fails, you've got serious problems. In a linked list demo app, crashing is probably exactly what you want to do. The only time it's worthwhile to check returns from allocation is when you can use that knowledge to free up some memory from somewhere else in your program and try again. And even then, in C++, you'd probably use set_new_handler to do whatever it is you want to do, anyway. And even on top of that, returning -1 is almost certainly wrong, too. As Shmurk said, C++ doesn't use these opaque numbers to indicate status, like C does. It uses explicit status (bool, enum) if it's warranted; or, more likely, it uses exceptions to indicate exceptional status. And having memory allocation fail is textbook "exceptional status". In short, your friend is wrong on most/all counts.
If his linked list is an object design then he might be allocating a new node in a constructor. In which case he can't return a status flag and can only either throw an exception or set a status flag in the object (std::istream and std::ostream do the latter.) If an exception is thrown while allocating a node then care must be taken to deallocate all the nodes that have already been allocated, otherwise you'll have a memory leak. Writing exception safe code is non trivial. While it's a valuable exercise to implement your own linked list, many commercial developers would use std::list, which by default will throw an exception if it fails to allocate memory, but will not leak memory if an exception happens. 
There is no simple answer to your question (see, for instance: http://www.codeproject.com/KB/cpp/cppexceptionsproetcontra.aspx ) but in your specific case (a C++ class), I would say: use exceptions.
The runtime overhead for the calls themselves is not that much actually. It's comparable to a virtual function call. You are more likely to take a performance hit because every instance of a pimpl class requires one additional memory allocation. Additionally the compiler can't do function inlining on the methods of a pimpl class. On container classes etc. this can make a _big_ difference. If OTOH maximum performance is not critically important, i.e. the I-have-to-get-extremely-clever-because-virtual-calls-are-too-slow level of performance, then the benefits of pimpl should outweigh the downsides most of the time. What I don't quite get is, how pimpl is better than an abstract base class, but I haven't spent much time on the subject so maybe someone will enlighten me. :)
The operator+= demonstrates that pretty well. String uses the impl member variable of **Object**, an **OBJECT\***. At each access of String data it has to cast **OBJECT\*** to **STRING\***, which is quite adsurd.
My lazy and "professional" opinion is that if a small memory allocation fails you're so fucked for other reasons it's not really worth handling the error with anything but a crash (crashes like that are easy to find in a debugger!). I'd probably change my mind for some application where you HAD to allocate a lot of memory and knew that running out was a very real possibility, but in the kind of general application development I've done this has never been the case.
what would be the purpose of this? If you wan't something like java then why not just use java??
But the cast is static. It compiles to nothing.
You don't have to mess with indirections and heap allocation.
The correct thing to do really depends on the situation. Even in your specific circumstance (as described above) I can't say what is expected, but I think either is acceptable. Normally the characteristics of the project would determine which was correct. These deciding factors include (but are probably not limited to) the following: * Is the project even using exceptions? If not then you'll be using the no throw version of new, so this discussion is moot. Projects can decide not to use exceptions for performance reasons or because it is so difficult to write exception safe code. * Is the platform memory limited? If so you won't be treating new as "infinite" and therefore failure will not be an "exception" it will be a standard control path. Personally I have not worked with an "exception happy" c++ project or library. Most code written in c++ is meant to be performance critical and just *enabling* exceptions will significantly increase the size of your executable, and for this reason it usually gets turned off. Ultimately, for the amount of overhead exceptions incur it's just not worth it. The only projects that I see them used are small.. well, except the STL.
Exceptions can be turned off with most C++ compilers and it used to be more common to do this to avoid space and time overheads. There were also some platforms like Windows CE that didn't support exceptions.
Okay the heap allocation thing makes sense. What do you mean by indirections?
Exceptions travel throughout the stack until they are caught or the program shuts down (starting from the place where it is generated and outwards until it hits the main function). So he could just wrap the entire thing in one try-catch block. That way he would not have to keep polluting everything with if (x == null), and could try to free up memory or something else in the catch or restart everything.
Looks like an interesting replacement for my use of DTL, although I kind of wonder how much it would affect already overlong compilation times.
With pointers, one needs to constantly write -&gt; and *. With values, you only need the dot. Example: Foo *foo = new Foo; foo-&gt;action(); (*foo)[5] = 3; vs Foo foo; foo.action(); foo[5] = 3; The latter is much more readable than the former.
The time it takes to generate the database support code is negligible since it is just parsing of the headers and writing of the C++ code. The generated code itself is quite small and simple so it shouldn't take long to compile. The core database API uses some templates but those are just simple wrappers that call the generated code. The object query language uses templates more heavily thought. I guess the only way to know for sure is to try it out on some sample project. There are some examples and tests that come with ODB that can give you a rough idea. 
This is awesome. The lack of a decent ORM system for C++ (that doesn't require XML, for goodness sake...) has been a thorn in my side for a long, long time. Now all I need to do is figure out how to use this to do automatic database migrations across application versions and I'll be all set. Lately I've gotten into the horrible habit of bundling python or ruby with my programs just to take care of miscellaneous database management stuffs. I pray for the day when it will be so easy in C++, that that will no longer be a temptation.
Secure iterators were enabled in Release mode until VS2010, but they are turned off by default now.
&gt; Now all I need to do is figure out how to use this to do automatic database migrations across application versions and I'll be all set. We are actually planning to support this. I.e., some sort of class data versioning from which the ODB compiler will generate database migration code. Something along these lines (very rough sketch): #pragma db object class person { ... std::string first_; std::string last_; #pragma db version (2) std::string middle_; }; Would this be something that could help you? 
gcc only?
No: *The ODB compiler uses the GCC compiler frontend for C++ parsing and is implemented using the new GCC plugin architecture. While ODB uses GCC internally, its output is standard C++ which means that you can use any C++ compiler to build your application.* And: *ODB is written in portable C++ and you should be able to use it with any modern C++ compiler. In particular, we have tested this release on GNU/Linux (x86/x86-64), Windows (x86/x86-64), Mac OS X, and Solaris (x86/x86-64/SPARC) with GNU g++ 4.2.x-4.5.x, MS Visual C++ 2008 and 2010, and Sun Studio 12. The dependency-free ODB compiler binaries are available for all of the above platforms.*
This is the confusing part for me. I need to use gcc to parse my code in order to generate the ODB code? I have no exp on how the GCC 4.5 plug in system works. is ODB compiler deisgned to work as a standalone tool as some point or I must have gcc around?
Yes, it is a standalone tool. It has the compiler driver which invokes GCC under the hood, loads the plugin etc, etc. All you have to do is: odb -d mysql -I path/to/some/lib/that/header/includes header.h Furthermore, the pre-built binaries for the ODB compiler include a private copy of GCC binaries. So if, for example, you are on Windows, all you have to do is download the binary for Windows, unpack it, and you are ready to run the compiler. Binaries are provided for GNU/Linux, Windows, Mac OS X, and Solaris (both x86 and SPARC). The only reason I mentioned GCC at all is because when someone claims that their product includes their own, special-purpose C++ compiler, I personally feel very skeptical. So I wanted to show that there is no magic behind ODB's ability to parse any standard C++.
Extension through pragmas is a novel concept. In a way it reminds me of attributes on C# code, such as when you do serialization. I'll give it a shot. It looks like a lot less upfront work than database template library and I'm already used to doing this kind of stuff with cmake and Qt moc. Looks pretty clean. Thanks for the info!
Good read, thanks. It's great to see STL becoming more and more usable in the real world. Having only ever messed about with writing games in a limited subset of C++ (ie treating it as C with OOP), this and other recent articles on STL have really made me think about having a serious go at doing it again "properly" with STL and templates.
We used STL* in ~2002 for Stranger's Wrath. It worked great. The only concession we made was building our own stl::vector for our very specific needs and conforming to the standard. * STLPort
Better and better! Christmas has come early this year! Seriously, though, keep up the good work - you guys are amazing. (On a related note: *Is* it "you guys," plural? Or is CodeSynthesis a one-man-shop?) First off, a bit of a correction. Apparently my terminilogy is mistaken. &gt; ... a database supports *schema evolution* if it permits modification of the schema without the loss of extant data; in addition, it supports *schema versioning* if it allows the querying of all data through user-definable version interfaces. ... we will consider the schema evolution as a special case of the schema versioning where only the current schema version is retained. [[1](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.7241&amp;rep=rep1&amp;type=pdf)] So, what we are referring to is *schema evolution*. ---- I guess what I had in mind was not necessarily a full-blown schema evolution system, but rather a set of "helper" classes and functions that would let me manually write upgrade/downgrade scripts in C++ rather than in SQL, making it completely independent of any specific backend database. See here for example [[2](http://api.rubyonrails.org/classes/ActiveRecord/Migration.html)]. ---- If you do end up going down the rabbit-hole, then at a minimum, whatever versioning system you end up developing needs to be able to support the following operations: * Add column * Drop column * Change column attributes * Change column name * Add class * Drop class * Change class name Simply being able to add columns to existing tables is not enough. It's essential that any versioning system support *all* of the above. If it doesn't, then the first time I need to perform a modification to the database that the ODB versioning doesn't support I'll be unable to use the ODB versioning system from that point forward since the physical database and the ODB generated model of the database will be in an irreconcilable state on client's machines. To complicate matters, there's no elegant solution to delete columns/tables using "#pragma" directives withought adding quite a bit of unsightly cruft to my header files, since even "deleted" columns would need to remain in my header indefinitely with a "#pragma" above them indicated that they, in fact, no longer exist. A possible workaround would be for ODB to automagically create (at user's request) and maintain a "version history" header that keeps track of all changes made to a schema each time ODB is run. This could detect additions/removals from the schema, modifications to column types, and changes to the names of tables/columns via the `db column('foo')` and `db object table('bar')` pragmas. When I say auto**magically**... I'm not kidding about the magic part. What form that "version history" header file would take is a bit of a mystery to me, but you might be able to use the semantic approach described in [[1](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.7241&amp;rep=rep1&amp;type=pdf)]. ---- \[1\] [A semantic approach for schema evolution and versioning in object-oriented databases](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.7241&amp;rep=rep1&amp;type=pdf) \[2\] [Active Record Migrations](http://api.rubyonrails.org/classes/ActiveRecord/Migration.html)
How about table (object) association support?
Coming in version 1.1.0. For now you could do that manually; see the comments to the blog post for details.
Thanks for the thoughts and the links. I think it will be better to take this discussion to the odb-users mailing. I took the liberty and re-posted your comment there. I hope you don't mind. I am also going to reply to it there after I give it some more thought. If anyone else is interested in this, they are welcome to subscribe to [odb-users](http://codesynthesis.com/mailman/listinfo/odb-users) or follow [this discussion](http://www.codesynthesis.com/pipermail/odb-users/2010-October/000001.html) in the odb-users archives. 
Qt is metaphorically equivalent to the Java API/SDK and handles many of the annoying things about C++ and cross-platform dev
&gt; C++ isn't a language that's friendly to developers who don't know what they're doing. Or, quite frankly, to those who do know what they are doing.
MSDN – If there is insufficient memory for the allocation request, operator new returns NULL or throws an exception (see The new and delete Operators for more information). If you follow the link you will find the following statement: Beginning in Visual C++ .NET 2002, the CRT's new function (in libc.lib, libcd.lib, libcmt.lib, libcmtd.lib, msvcrt.lib, and msvcrtd.lib) will continue to return NULL if memory allocation fails. However, the new function in the Standard C++ Library (in libcp.lib, libcpd.lib, libcpmt.lib, libcpmtd.lib, msvcprt.lib, and msvcprtd.lib) will support the behavior specified in the C++ standard, which is to throw a std::bad_alloc exception if the memory allocation fails. Guidelines: *Always check new for null. Running out of memory can and does happen, but not often. It's exceptional; hence exceptions. *Returning negative is usually not standard (0 for ok. &gt;0 for error) Chances are that you've not included the proper libraries for a throwing new. I wouldn't agree with ptrb's explanation that "your friend is wrong on most/all counts". Why make shitty code just because it probably won't happen. If you were making a real linked list api, you better be damn sure it's not going to crash if you run out of memory.
I don't think you can make it more than a partial order: c &lt; c# c &lt; c++ but c# ??? c++ 
If I were interested in contributing Sqlite support, would the mailing list be a good place to discuss that?
Sounds good. I've subscribed.
Whatever floats your boat
Yes, definitely. SQLile support is on our agenda, along with PostgreSQL.
Note that this is a PDF.
&gt; Why make shitty code just because it probably won't happen. If you were making a real linked list api, you better be damn sure it's not going to crash if you run out of memory. Crashing when you run out of memory is almost always the correct behavior. A "real linked list api" is [probably] no exception.
Problem?
Nope.
No, but it's polite to note it in the submission title.
Note taken
Here you go. [https://docs.google.com/viewer?url=http://www.research.att.com/~bs/terminology.pdf](https://docs.google.com/viewer?url=http://www.research.att.com/~bs/terminology.pdf) No need for plugins (:
Someone already tried: http://acdk.sourceforge.net/ 
Dear mr Stroustrup, We c++ programmers eternally thank you for eternally confusing us with your terminology. Most c++ programmers simply read 'lvalue' and 'rvalue' as 'left-side value' and 'right-side value' respectively.
'left-side value' and 'right-side value' are not enough when move semantics and perfect forwarding comes into play. The left and Right are not enough today because it's stopped being about what side of the '=' sign they could appear. 
Exactly.
I see no scenario where a crash is the optimal path. Take for example a document editor. Would you want to lose all the information you worked so hard to enter because you ran out of memory? No... you would want it to exit as gracefully as possible. Possibly allow the user to close other applications to free up memory for a save. I can think of so many other scenarios where exiting cleanly is the way to go. Can you enlighten me on a scenario where a crash is "the best route"? It certainly is the laziest.
Is no one going to summarize this? I normally love reading nonsense like this (no, really, I'm sorry if that sounded sarcastic &lt;/homer&gt;), but my real life is rather hectic right now and I don't have time to parse Stroustrup's wordiness.
&gt; Take for example a document editor. Would you want to lose all the information you worked so hard to enter because you ran out of memory? No... you would want it to exit as gracefully as possible. Possibly allow the user to close other applications to free up memory for a save. How do you pop up the dialog to allow a user to save, when there's not even enough memory to allocate another node in your undo stack (or whatever)? It's a serious question, and the answer is you generally can't. Unless you're _ulimit -v_-ing all your processes, when you run out of memory in an allocation, it means your entire system is already paging like crazy, and every other application is basically frozen. If this is happening on a server, all your other services have ceased responding to new requests. By any measure, that's an _Oh Shit_ scenario. And since it was caused by a bug in _your_ application, it's _your_ application that needs to get the fuck out of the way and let the computer continue running. If you crash with a coredump/stacktrace, you have a decent possibility of figuring out where the problem is. I mean sure, there are exceptions to this rule. Maybe if you run a server that periodically allocates a huge chunk of memory temporarily, (like say if you're copying some buffers to flush to disk or something) then you might expect that to fail and would have some backup plan: chop it into pieces, or blow away some old data in a ringbuffer, or something. But unless you're _absolutely sure_ you know what to do when you hit that ENOMEM, crashing is your best bet.
The C++ designers have painted themselves into a tiny corner, where even they can't understand there own terminology anymore. Exercise: Apply the new terminology to the reference collapsing rules in template deduction. Does this result in any new insights?
Calm down. Stroustrup brought clarity to a messy problem. This is the best possible solution that is the most backward compatible with old terminology. To your question.. lvalues and rvalues, are old well established terms. You cant change the meaning of those. They are opposites. lvalues is all the values that have identity and cant move, just like before (your good old named values). All other values are, rvalues. Rvalues are all the values that can move, just like before (when the only ones that could move was unnamed temporaries). However with C++0x we have two different types of rvalues. We have values that dont have identity and can be moved (unnamed temporaries) and we have this new type of value that has identity and can be moved (any type with move constructor). The first rvale is called xvalue and the second is called prvalue. Rvalue is just a general name for the two specific types xvalue and prvalue that can move. Then we have another general name, glvalue. glvalue is a general name for the two specific types that has identity, lvalues and xvalues. The more you think about it the more you undestand that this is the best categorization of the terminology. I would have preferred ivalue to glvalue though. As in (general) identity value. As xvalues have both the property of identity i and movability m, they belong to both the abstract groups glvalues (i) and (m) rvalues! Having it belong to something named ivalues and rvalues instead seem less confusing. Anyone agree about ivalues?
Isn't that a oneliner for ruby/perl? ruby -ne 'puts /(.{1,80}\W)/.match($_)' &lt;file_to_cut EDIT: I didn't understand the problem correctly. The right "oneliner" would be ["fold -s"](http://unixhelp.ed.ac.uk/CGI/man-cgi?fold)
Eh, why not just use `cut`?
It's actually different--cut discards the remaining columns, whereas col puts them on the next line. I have attempted to be more specific about the difference in the latest README, which is pending commit.
I wouldn't know, that looks like a bit of gibberish to me, but does that preserve the entire content of the file?
No, it doesn't. I didn't understand the purpose of your program. I thought you want to throw away what was left on the line. For your problem, there is [fold](http://unixhelp.ed.ac.uk/CGI/man-cgi?fold). fold -s should do what your program does. EDIT: I forgot to explain the oneliner: It uses a regular expression, that matches as many characters as possible as long as they end with a non-character before reaching 81 characters and then outputs the match.
Yes, fold has been pointed out as well. I've decided that's cool and all, but I mostly just wanted to hack on something anyway. And since I already made it, most changes will be trivial. Right? Anyway, I did learn something really cool today, so I'm glad I'm keeping the project.
My favorite part: &gt; ODB is not a framework. It does not dictate how you should write your application. Rather, it is designed to fit into your style and architecture by only handling C++ object persistence and not interfering with any other functionality.
Here is a diagram that might be useful when reading ... for example you can see from it that xvalues belong to two general categories, glvalues and rvalues. http://i.imgur.com/g32Nq.png expression ╱ ╲ ╱ ╲ glvalue rvalue ╱ ╲ ╱ ╲ ╱ ╲ ╱ ╲ lvalue xvalue prvalue
Very nice. What kind of performance can we expect?
The Active Object pattern is very useful and can solve lots of concurrency/parallelism issues. It's a shame it's not used more widely.
Is exception handling on windows supported yet?
Here is some more information: http://news.ycombinator.com/item?id=1762335
Thanks!
So they implemented export keyword? I can't find it mentioned anywhere.
Shh!
I'd love to see a comparison of the binaries performance as well
Yes: honestly the compile speed doesn't matter to me that much... the speed and correctness of the generated code is what I really care about.
Plus a Clang vs. GCC 4.5 since g++ 4.5 is supposed to compile much faster than previous versions.
&gt; You start asking why gcc is really slow compared to clang in the things that a compiler is supposed to do fast — which is compile programs. Why is a compiler supposed to be fast? All other things being equal (which they aren't), I'd expect a slower compiler to perform more optimizations, resulting in higher code quality. The performance numbers given are pretty much useless, seeing as they measure compile time, and not the speed of the generated code. Downvoted.
At first this seemed no longer necessary now that we'll have lambdas in C++0X, but that it allows you to *transform* the code is exciting. Now if I just could think of a good reason to use this is in my code :-)
Maybe because slow compilation speed is usually the biggest reason (in many cases the only one) why C++ is ditched in favor of C, C# or Java.
I've never heard this before, that **compilation speed** cases people to ditch C++ in favor of managed languages. Are you serious?
Yes, I'm dead serious.
&amp;#3232;_&amp;#3232; 
Very much one of the many reasons why a return to understandable code and provably correct C code is more important than ever. This kinda language feature is just stupid. There's no analysis at all of how and why these sorts of features and behaviors improve the language in a practical way. 
This kind of problem demonstrates clearly what pure functional programming supporters have been saying all along: destructive state updating can create lots of undesirable side effects. Incidentally, while I think destructive updating is necessary for performance reasons (for example, in-place algorithms), I always thought that move constructors will come bite you at some time. And the time has come. 
You mean people don't care about C++' insane semantics but they do care about 100ms more or less in compile speed? Citation required.
Yes, if you talk to retards they will complain about C++ semantics, low portability, low performance, whatever... I'm sorry but I don't count retards. And even though I'm a huge advocate of C++ it's definitely not 100ms but more like 1s..5min per .cpp file.
Understandable code? You can write illegible code in any language, regardless of features.
It's pretty demonstrable that most C/C++ programmers have a far easier time with simple C code than with complicated C++ code. It's not about illegible code. It's about understanding how code behaves. C++ has tons of little weasly idiosyncratic gotcha's that make it a total PITA for doing real development. Many teams eschew a lot of C++ features for precisely this reason. And those features add no actual value to C++. 
What makes C more provably correct than C++? These articles are exactly the type of analysis that you're saying isn't happening, as it says on the very first line: &gt; The bulk of this article is going to be a C++ Standards Committee document Are you talking out of your ass?
&gt;These articles are exactly the type of analysis that you're saying isn't happening, as it says on the very first line: The bulk of this article is going to be a C++ Standards Committee document Yeah, and when was the last time you saw a reference from the C++ standards committee as to why the language required a feature because you couldn't get something done without it. Never. There's a reason why all the most used pieces of software are written in fairly straightforward C. 
&gt;What makes C more provably correct than C++? Neither C nor C++ have any formal semantics, so in principle neither can be analyzed in a purely formal fashion. They are both defined informally through their respective "Standards" specification. However, it's no secret that C's standard specification is far less ambiguous than C++'s, to the point where there exist many different interpretations and implementations of C++ that do wildly different things. Examples of this include SFINAE, where it's not exactly clear what class of errors can be properly omitted by a compiler when performing a template substitution. In addition the definition of volatile is different in C than in C++, with there being an ambiguity in C++'s definition that results in many compilers using it in different ways. And I haven't even started discussing ambiguities in the grammar of the language itself. Because of this, it's very common for organizations to restrict the use of C++ to a very small subset of unambiguous, well understood cases. This subset is very close to C with classes, namespaces and type-safe macros (templates used in a very restricted manner, similar to generics in Java/C#). Something tells me that while move constructors might be of some performance benefit in a variety of cases, many organizations will opt to play it safe and avoid their usage for many, many, years to come.
You can get anything done in assembly too... why don't you write in pure assembly? Hell, write in pure binary -- I've had to do that too, it can be done. Higher abstractions make it possible to solve more complex problems.
&gt; Higher abstractions make it possible to solve more complex problems. You can design higher abstractions in any language. Did you read this article? It's not even about higher abstractions. It's about almost-never-used stupid features and stupid problems created by said stupid features, and changes to little-used said features which break what little code already uses those stupid features. We'd all probably be a lot better off as consumers of C++ if the standards committee spent their time doing nothing, rather than focusing on this sort of silliness. 
At the same time, I'd say that most C/C++ programmers have a much easier time with simple C++ code than with complicated C code. While many teams avoid several of C++'s features, there are also undoubtedly many that use those features as well. Except that shit with inheritance and constructors/destructors. That's fucking balls.
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2002/n1377.htm#Motivation &gt; Move semantics is mostly about performance optimization: the ability to move an expensive object from one address in memory to another, while pilfering resources of the source in order to construct the target with minimum expense.
Yes you can use higher abstractions but you cannot enforce them. Btw. C++ is actually lower level then C when comparing current stable standards. Calling r-value references and move semantics something that is stupid and almost never used IS stupid. This will be the single most used feature in new C++. The article simple proves that implicitly generated code or semantics are never good. In any language (that includes C).
C++ standard isn't ambiguous. Yes it is complex, hard to read, heavily cross-references but definitely not ambiguous. But I guess if you want to call the fact that C++ has incompatible features with C "ambiguity" then OK, C++ standard is ambiguous. I'm too lazy to look into the standard but I'm convinced that the template instantiation lookup is actually well defined. Yes many companies use only a subset of C++, but most of this companies also sill use Visual Studio 6.0. And yes, because their developers were retarded and had zero idea what code quality is they now can't switch to newer versions, because the new versions of MSVC are actually starting to compile C++ and not "something that looks like C++".
I definitely have much bigger problems with C code, because in C you need to think about low level semantics at all abstraction levels. You can't create abstractions handling low level problems without suffering insane performance penalties or writing unmaintainable (or unportable/invalid) code.
I did read the article. I wasn't responding to a point in the article, I was responding to your previous comment that called C more straightfoward and claimed that the new features of the language weren't necessary. (you didn't say which ones, so I assumed that you meant most) Constant claims, such as yours, that C is a better language due to its simplicity usually come from people who don't like abstractions and that's usually the subtext behind their statement. Perhaps I assumed more than you meant, but there is a lot of value in the changes coming in C++0x, and I am very anxiously awaiting its standardization.
&gt;C++ standard isn't ambiguous. Something can be both well defined, and ambiguous. For example a basic context free grammar such as: E -&gt; E + E | E - E | e for arithmetic is both well defined, and contains ambiguities. All an ambiguity means is that there are two or more unequal interpretations of a statement. It does NOT mean that it contains an error or something false. Also, when it comes to the examples I provided, namely SFINAE and volatile, it's well documented that ambiguities exist with those two aspects of the language. In SFINAE's case you can have a look at Boost where it's littered with #ifdefs for various compilers and interpretations of what constitutes a substitution failure, and what constitutes a full blown error. For volatile, consider that Intel, g++, MSVC, Sun's and HP's C++ compiler all have different interpretations of volatile. HP's C++ compiler doesn't even do anything for volatile. Read the following for how volatile is incredibly messed up in C++, owing to the differences in both languages on how l-values get converted into r-values and what the effect of that is on the volatile keyword: http://blog.regehr.org/archives/41 I can give you a bunch of other examples and turn this post into a wall of text, but if you're interested... ask yourself this simple question and answer it using the C++ standard specification. Does the following code result in undefined behavior? int* a = new int(5); delete a; int* b = a; Keep in mind section 3.7.3.2.4. C is very clear about whether that code, (using malloc and free instead of new/delete), results in undefined behavior or not. C++ is not.
I would say that the standard is very clear about this: &gt; If the allocated type is a non-array type, the allocation function’s name is operator new and the deallocation function’s name is operator delete. &gt; If the allocated type is an array type, the allocation function’s name is operator new[] and the deallocation function’s name is operator delete[]. And yes, everything not defined in the standard is undefined.
Assuming TNode is declared something like this: class TNode { public: TNode* left_link; TNode* right_link; int value; }; You probably want a function declared as TNode* copyInOrder (TNode* tree); The rest is up to you, because you shouldn't post homework assignments. 
You're confusing my use of new(initial _ value) with the use of new[number_ of _elements]. To make it simpler just consider this code: int* a = new int; delete a; int* b = a; The real question is whether the copy constructor is valid if the value being copied contains an invalid pointer value.
Move semantics is not an "abstraction". 
As I said, I was responding to your comment, not the article, and assumed that you were referring to all of the changes in C++0x, many of which ARE abstractions.
OK, I can help. Change majors.
It's not homework....
Okay.
use a stack
stackoverflow.com might be a better place to post such questions?
Copy constructor? There is no copy constructor involved. Oh and sorry, for some reason I total saw int[5] there.
If there is no meaningful default value, then as a general principle you shouldn’t provide a default constructor. Can you give a specific example where not having one is causing you a problem?
If you are talking about the default allocators for STL types, well, that is because they are default and therefore handle the most common case.
Agreed. I think the poster might be talking about std::map in particular, because that class has the (perhaps unusual) behavior of creating a default instance of an object when operator[] is called with a non-existent key. Because of this one corner case, the std::map class requires member types to have a default constructor. I've found this obnoxious a couple times in the past. One workaround is to allocate on the heap and store pointers to objects instead of the objects themselves. It seems like the most flexible way to put things in STL containers, although sometimes it means you have to provide custom sort methods. It's not so much a C++ flaw as an STL issue. The STL is an ancient interface, and it shows.
Because there's no other way to represent an "empty" object while satisfying class invariants. Consider the following: Foo x[10]; // Foo is a class of some sort What are the elements of x initialized to? The compiler can't just fill the array with zeros because that's very likely not a valid state for object Foo. And obviously they can't just be left uninitialized either. In order to satisfy class invariants, the elements of x must be constructed. That's what a constructor does: it builds a valid object out of dust. Since you can't specify which constructor to call, the default constructor is necessary to initialize the objects. In containers, it's not uncommon to allocate an array of some type T. And if that's the case, then that type T needs to be default constructible, which is why it's so common. There's no way around that without resorting to very ugly hacks (eg. placement new).
&gt; I think the poster might be talking about std::map in particular, because that class has the (perhaps unusual) behavior of creating a default instance of an object when operator[] is called with a non-existent key. Because of this one corner case, the std::map class requires member types to have a default constructor. Does it really? You certainly need a default constructor if you’re going to use `operator[]`, given that the semantics of that operator are defined based on one. However, I’m not aware of any general restriction on the target type of a `std::map`, and you do have the option of using the `insert` member functions to add keys with pre-built values directly.
You're 100% right. I've apparently misremembered this.
This is the sort of thing that happens when you have both value and reference semantics in a language. If, like in many OO languages, you could only have handles to objects rather than holding them by value, the problem goes away. An early design goal in C++ was to make it possible for programmers to make classes which do everything the primitive types do. This triggered default construction, copy construction, object assignment (with the possibility of object slicing), destruction on leaving a scope, etc. It's consistent in principle but so tricky in practice that the language has become a leviathan. 
The main part to remember is that when you have a class template, and you instantiate it, you don't automatically instantiate all the member functions, but only those that are actually needed. So if a function in a class template calls some function on the template parameter, but you don't use that function of the class template, you can use a template parameter that doesn't have the other function. Hm, I hope this is understandable ;-)
I guess it's just coincidence that you posted a [question about induction on binary trees](http://www.reddit.com/r/cheatatmathhomework/comments/dpxzk/structural_induction/) to /r/cheatatmathhomework. By the way, the answer to the induction question is really simple. Just think about what happens when you insert a node into a binary tree. You're replacing a `NULL` pointer with...
The std::map class should not create an object on operator [], it should create a promise to create an object. In this way, if the operator's [] return promise is not assigned, no object would be created, and if assigned, an object would be created. If the key specified as parameter to [] already has a value, then the promise would return the actual value. 
&gt; This will be the single most used feature in new C++. Why do you say that? in which situation performance cannot be improved without move semantics? personally, I don't see one.
Yes it is optimization, but actually its more about the ability to pass a reference to r-value. For example you can't pass an anonymous object as reference, you only can pass it as const reference. This is huge pain in code forcing you to create temporary variables and causing huge performance issues.
Any combination of STL container and dynamically allocated memory (generally any occurence of dynamically allocated memory). Plus passing r-values as references is something that I need to use a lot.
A simple example would be QtConcurrent. To use QtConcurrent::run your return type needs a default constructor. To use the parallel map, fold, filter,... versions your container element type needs a default constructor. The latter is a requirement of the QVector used internally even when the input container is a QLists (which does not have that requirement). A lot of other places in the Qt libraries also force you to provide a default constructor. I agree that it is not an issue of the language so much as it is one of the culture around the language, i.e. lack of thought about the problems caused by requiring one.
Your and my definition of 'valid' are obviously slightly different. My definition considers constructed objects with nonsensical values to be just as useless as those not constructed at all, or actually more harmful because now all the other places in the code using objects of this type need to be prepared for nonsensical values too.
You don't use constructors?! 
Google.com is a better place, I'm not cynical just search "binary tree algorithms" or something and you will find any algorithm that you ever thought of with source code for any language in any spoken language. I used to do homework for some Uni students... easiest money ever. 
Of course I use them! But coupled with multiple inheritance, constructors and destructors become some of the most confusing commonly used features.
&gt; Any combination of STL container and dynamically allocated memory (generally any occurence of dynamically allocated memory) Have you ever returned a complex object via the return statement? personally, I have not. When I need to fill an STL container, I pass it as a parameter. So move is not necessary in this case. &gt; Plus passing r-values as references is something that I need to use a lot. How so? please give me an example. 
In many cases boost::optional&lt;T&gt; can be used instead of creation of meaningless default constructor.
In your definition of "valid" what exactly would you place in Sc4Freak's array? Bear in mind it has to satisfy the type system. You're griping about this but I don't really think you fully grasp the issue. It isn't a cultural issue (for the most part) it's a language issue. 
It seems perfectly rational to me. If you *don't* have a default constructor for your object and you have something that requires one, that generally means you should be passing in a reference of some kind (typically a pointer) rather than the value type itself.
How would that solve the problem? Then you would still have to deal with the invalid value of a NULL pointer everywhere.
I would use something like boost::optional&lt;Foo&gt; (similar to OCaml's option or Haskell's Maybe).
At this point I have to question your knowledge of C++. int* b = a; The above line of code invokes the copy constructor. When initializing a variable, even if you use the assignment operator, the copy constructor is invoked. In other words the following two statements are semantically equivalent, and they both invoke the copy constructor. int* b = a; int* b(a); You're right about one thing... C++ is a VERY hard to understand language with incredibly unintuitive semantics.
&gt; There's no way around that without resorting to very ugly hacks (eg. placement new). If you're writing a container library, it's your job to resort to ugly hacks so the users of your library don't have to. And as ugly hacks go, placement new is pretty tame.
If, for example, you have a container that requires the ability to have an object which has no value (which is effectively what a default constructor creates), and yet you have an object type that must have a value for all of its instances, the container logically ought to be storing pointers to the type, rather than the type itself. NULL is the reasonable way to represent that case where you have no value.
Oh, yes, I agree, the container should be implemented in such a way that it should not require a change in the type it stores to meet the container's needs. It should simply extend the type to include a value signifying "this position is empty" internally. That way other uses of the type are unaffected.
Sure, but then you have boost::optional&lt;Foo&gt; x[10] which is completely different. The point is default-construction is necessary to support all value-based cases. You can call it an oversight that a value-based option type wasn't included in the language at the beginning but it is what it is. 
I meant the internal storage should use such a datatype, instead of leaking the requirement of an additional value for purposes of determining an empty element through the container abstraction to the outside.
That sounds more like an issue of Qt than the language or its culture. Also there's nothing stopping you from returning a different type that wraps the one you need a complex constructor for. template &lt;class T&gt; T foo() { return T(); } struct A { const int _x; A(int x) : _x(x) {} }; // no default constructor struct B { A _a; B() : _a(5) {} }; int bar() { return foo&lt;B&gt;()._a._x + 10; } If you really want to get complicated (e.g. if you need to pass values into the wrapper instead of knowing them in advance) you can throw some Boost.Binds and overloaded/conversion operators in there for good measure.
Who cares? If Foo were some primitive type the values in the array would be undefined. Also, you CAN specify which constructor to call. http://codepad.org/hmQMfbpz 
That is actually pretty much what I was doing in the last few days to work around these issues.
You bring up a good point. I also dislike the fact that most containers require default constructors for the containable objects. Its basically the bad old two stage construction. Where the lifetime of the object starts before the object can be initialized to a valid value. Its better to delay the lifetime of an object until it can be initialized to a valid value. Same should go for the objects in containers. Like vector::resize.. it defaults to default construct objects. So that is one reason why vector needs its stored objects to have default constructors, maybe the only. In that case its a big price for users to pay for a convenience function the user could just as well do himself. Then we have fixed size boost array that also requires default constructible objects. The case for it might seem more obvious, as thats what C arrays require. But if an array had to be fully initialized with values where it was defined.. then there would be no need for default constructors there either. Edit: about vector resize.. I forgot template instantiations only occur if the template method is actually used. So there is no "price" for user to pay in general.
Here's a related thread on `comp.lang.c++.moderated`: http://groups.google.com/group/comp.lang.c++.moderated/browse_thread/thread/f932becf97b7e08b/c946827a71a8ff2f I still haven't got a good answer to the default constructor/no default constructor dilemma.
If the problem goes away by using handles to objects in other languages, then the problem would just as well go away with using pointers to object in C++. But it doesnt. A null pointer is much the same as a default value. It will have to be initialized to some valid value later. That forces other code to check whether it has been initialized yet, before trying to use it. And that is ugly. The problem is that the visibility of the name is not tied to its lifetime. One should seek to delay defining a object until one can initialize it. Sometimes that is hard to do though. But that is the only solution I see. Edit, im abit ignorant about Java. But you might have a point. The handles in java are probably like references in C++. And thats much better than pointers, as references have to be initialized where they are defined. So yes, that makes sure every object whose name is visible has been initialized to a valid value. So yes, you do have a point. I yield.
Oh yes, those two statements are semantically equivalent. But OK I guess you can call this invocation of copy constructor (even though there is no copy constructor involved, because pointers are POD types). Still I don't know why a pointer assignment should lead to undefined behavior (unless you want to call the fact that the pointer will be pointing to a semi-random value undefined).
&gt; Have you ever returned This is about the memory inside the objects. Consider std::vector for example, each push_back can invoke copying and destruction of all objects inside the vector. If your objects are using dynamically allocated memory then they can move very fast. &gt; How so? please give me an example. This can't be done in old C++ (you need to use useless temporary variable): sometype somemethod(myclass&amp; stuff) { bla_bla; } somemethod(myclass(bla)); Now possible using r_value reference: sometype somemethod(myclass&amp;&amp; stuff) { bla_bla; } somemethod(myclass(bla));
&gt;even though there is no copy constructor involved, because pointers are POD types PODs do have copy constructors, assignment operators, destructors, initializers... etc just like any other object in C++. The only restriction is that they are not user defined, they use the default/trivial implementations (Section 9.0.4). &gt;Still I don't know why a pointer assignment should lead to undefined behavior I pointed out the relevant section of the standard. I will also point out that the C99 standard explicitly states that such an assignment is undefined behavior. The reason you don't know why it might lead to undefined behavior in C++ is because the C++ standard is ambiguous on this issue.
Crashing is still often the best route. You need to save internal backups often anyways (in case you crash for some other reason), so you just restore the backup the next time the application is run. If you want doubly extra bonus points you *could* let the exception propagate up, and during stack unwinding attempt to save a new backup file if anything has changed since the last one (crashing if it fails - which it probably will). I see this as a nice thing, but not necessary in most cases - and if your user wont notice, who cares.
Template instantiations only occur if required.
In your imaginary model what would be the semantics of: Foo x[10]; x[0].bar(); if you haven't assigned to x[0] and Foo doesn't provide a default constructor? Undefined behavior? Exception? Segfault? It seems like you're still left with the issue of checking all the time whether you've got a valid Foo or not. 
That's basically how it works though. Containers are designed to store value types that behave like native types. Part of the criteria of "behaves like a native type" is the default constructor. If your new type doesn't work that way, the only way you can use that container is by using a pointer, which is a native type, with the container. It's actually remarkably consistent.
At least in my version of the standard there is no 9.0.4. But this is an inicializer issue 8.5.14 &gt;Otherwise, the initial value of the object being initialized is the (possibly converted) value of the initializer &gt;expression. Standard conversions (clause 4) will be used, if necessary, to convert the initializer &gt; expression to the cv-unqualified version of the destination type; no user-defined conversions are considered. &gt; If the conversion cannot be done, the initialization is ill-formed.
I think C++ is simply following its C roots here. In C, assignment works with storage cells -- rvalues vs lvalues. It follows then that in C++, for deference to occur properly, the result of dereferencing with op[] must be some sort of lvalue. Contrast with property assignment in C#, or Python. The form "X = Y" takes the property named by the form on the left and invokes the setter function with the expression on the right. It does restrict the number of permissible syntactic forms to just properties and property like things (eg, "f() = z" can be legal in C++, but not in C# or Python), but does lend itself better to avoiding default values in these cases.
No, you only have the issue in the context of using this particular container, not everywhere Foo is used (e.g. all parameters or return values of type Foo). I think returning some kind of object that allows you to check whether something was actually stored at that position would be best here, probably another boost::optional&lt;Foo&gt;. A default constructed value usually does not have that property unless you explicitly add a member keeping track of when the object actually contains something valid.
&gt; If your objects are using dynamically allocated memory then they can move very fast. Same thing could be achieved using reference counting, without the problems of losing control of what is moved. Many libraries already do that. For example, QString is very efficiently stored in std::vector. &gt; This can't be done in old C++ (you need to use useless temporary variable): The question is why do you need that. Up until now, a non-const reference meant you have to pass a variable, because it would be modified. Have you ever actually needed that? I haven't, in all my 15 years of c++ programming. Please show me a real use case. 
We're talking about a flat array here though. That completely breaks the semantics of arrays and makes C++ arrays incompatible with C arrays (in the case of items with no default-constructor). The size of the type of the array reported by the type-system is no longer compatible with the size of the actual element in the array. In the case of an arbitrary container you're also asking all users of the container to take a performance hit just to support the fairly uncommon case of a type with no default-constructor. Unless you want two implementation variants of every container, one with no indirection, and one with indirection. Sure container authors shouldn't require more out of their contained types than is absolutely necessary. In that spirit pretty much all of the STL containers can be used with objects that are not default-constructible, provided you use the right interface (i.e. don't call methods on the container that require default construction). I've run into this annoyance several times but I ultimately think it's a non-issue. 
Happy 25th birthday.
&gt; fairly uncommon case of a type with no default-constructor. About 90% of my types I might actually store in containers (i.e. those modeling some kind of domain entity) do not have one. There are very, very few types in most domains where you can produce a default value that makes sense in my experience.
You guys keep quoting the C++ standard as if you've read over it. I'm fairly new to C++, but I'm currently learning by reading a book and have never heard anyone specifically refer to it in discussion as you guys have (eg. citing specific sections). Why are you guys so familiar with it? Is that a viable learning approach or... I'm wondering why you guys seem so familiar with it; why did you read it in the first place? 
The C++ standard can be purchased from either ANSI or ISO and the drafts are freely available although may contain errors and are incomplete. As for using it as a way to learn the language, the standard is written not to be instructional but instead written for those who might have to implement the language one day. I would not recommend reading it to learn the language as it is very complex. My personal familiarity comes from having written compilers, specifically working on the .NET team at Microsoft where familiarity with many language specifications is a must. The C++ specification is by far the most convoluted and complex language specification there is, and also one of the longest. Reading it will give you insight into the finer aspects of the language, but in my opinion one of the most important qualities a programmer should have is creativity and ingenuity and that won't come from reading the standard. Being able to quote some obscure text from the standard about how invoking the copy constructor on an invalid pointer value can lead to undefined behavior will not help you become a better problem solver or write more maintainable code. Not to mention that most of what is in the standard can be searched for in Google and explained in a much more clear manner when that obscure detail becomes an issue.
That's pretty much what I thought, was mostly curious how you guys knew it so well. Good info though, thanks man.
So, for clarification, this assignment is over and you just want to learn from your errors? Good man.
Starts debugging... ... What the hell is `stl_merge`? What do `s` and `r` and `m` mean? Explain to us what you're doing and and you're 90% to finding a solution on your own. 
Has anyone tried the GCC 4.6 (svn) for the speedup, in their software, yet? 
Aka livre sur le terrorisme
In all my years of programming and helping people online I have never seen a more clear cut case of 'homework assignment' :-)
Crazy... This is crazy.. Happy 25th C++
i wouldnt worry about it. im sure `stl_merge` is recursive, in which case its pretty clear at this point that this person has no idea what is being asked of him. the thought of that is pretty hilarious, in fact. god, i hope `stl_merge` is a recursive function. that would make the outcome so sweet.
In my experience it is an uncommon case. I've found it's seldom that a sensible and meaningful default object can't be constructed. And in those cases that one can't create a meaningful default it's simple enough to work around with boost::optional and other utilities. Further, it hasn't been my experience that a default constructor is a common requirement for template parameters. Copy-constructor and assignment-operator, yes, default-constructor, no. It's not a requirement of STL parameters in general (it is in a few places but those are easy to work around). In fact, I would also say that it's a fairly rare case that any container requires its items to by default-constructible. Regardless of how common it is, you haven't addressed any of my points for why trying to "fix" it in your prescribed manner is problematic/a bad idea. How long have you been using C++? 
I have been using C++ for about 3 years now. I was not talking about types where I could create a default object for the language's sake (e.g. names as empty string,..), those are very often possible. I was talking about types where the default actually makes some sense in the domain.
I've often wanted to disable using the default constructor by placing it in the private section. But too many times, I have to make it public again because say I use a custom iterator for my own container... I may not need it right away, but suppose I need to use it in two different scopes. Inside an inner loop as well as after the outer loop. I need to declare the variable at the highest scope so that both scopes have access to it (instead of declaring it when it is first needed). If you're talking about iterator operations not checking for NULL's or error states, then I'm also fine with that. I'm sure there are ways to wrap your iterators to be safe_iterators. 
Right you are. While its not much of an issue as one can avoid map::[] or vector::resize. It is a general requirement for types contained in boost::array and (i think) boost::pool to be default constructible. Which is less than ideal.
So, what would happen if you try to test the thing returned? if (vec[5] == 7) {...} What should this code do if the element doesnt (yet) exist?
throw an exception.
There are other ways to use the pimpl idiom, albeit harder, that would allow the compiler to perform some inlining i think.
Yeah, Qt even has what they call java like containers because they are easier for people to use. I have to admit though, once you are used to using the STL, it's not hard to use.
Yeah, what he said!
Both matter to me. There's no reason I can't do iterative rebuilds in Clang and if A) it proves to be less performant and B) that's important, then I can switch to some other compiler for a final build. Add in the fact that it would be possible to create faster and more powerful intellisense-like databases, and get far better error reports, probably even better debugging, and I'd still think it's a win overall. But yeah, if it generated more correct, faster executables, I'd use it exclusively.
COW leads to lower performance on multi-threaded aware code because you have to have a synchronization primitive to account for multiple potential readers/writers.
I agree with you, I think that the majority of these techniques are reservered for library creation only.
I really don't see the "shift" that's mentioned at the beginning of the article. How are things like CALL and floating point not just "features for an algorithm" like his later examples. Last I checked, pretty much every language uses both of the above and yes specialized languages crop up to take advantage of vectorization/parallelization, but the same concept was true back in the day as well.
While he does not spell it out in so many word. The way I read it, the shift is multicore. Its not that programmers like to to program in languages where program flow is split into many parallel units, that cpu vendors are forced to create multiple cores. Its the other way around. We are getting more cores (instead of faster cores) weather we like it or not, and now we need programming languages that make it easy to make use of these multiple cores. 
Word up to the Alma mater
Is it just me or is VS 2008 ignoring the lack of a typename when using a type?
what is the best C++ book for beginners? On a Mac, please =)
You could put the `string` into a `stringstream` and read the `int`s from there (pretty much as you're doing it), but that's kinda lame. To do this slightly more efficiently, you should use `istream::peek`. That is, ifstream infile("input.ini"); int N1, N2; if(infile.peek() == '*') { ifstream &gt;&gt; N1 &gt;&gt; N2; } Less data copying than reading things into strings etc, though obviously less powerful - you only get to peek one character into the stream etc. A word of advice: If you're doing this more than once, be careful around whitespace. If there's a newline after those two ints, and you want to do this again, your peek won't return the first character on the next line, it'll return a CR or a LF or something. (I think. Test it yourself if you're not sure.) Also: You should probably post this sort of thing to StackOverflow. I'm not sure this is on-topic here.
fantastic, thank you very much. I'll refer to stack overflow in the future.
/r/learnprogramming is probably more on-topic if you wanted to stay on reddit.
If this is a personal project, and you can bring in additional dependencies, I would suggest looking at boost and the lexical cast class. 
Shit! This was for a homework assignment, actually, and it appears my instructor reads reddit. Hey John, it won't happen again -Taylor http://imgur.com/7pGll
"The C++ Programming Language" by Bjarne Stroustrup
Busted! But seriously, I never really understood why professors get upset when students utilize the largest body of knowledge ever collected (aka The Internet) to solve problems. edit: I should say, to **help** solve problems. Gimli illustrates this very well below.
while i can agree that you do learn things by using the internet to solve problems, its not the same as figuring things out and putting things together yourself.
Yeah, what do they think people in the real world do when they run into problems? They could sit there and waste time trying to nut it out, or wait until a few hours or days later when they can get into contact with their supervisors/mentors, or they can jump on the Internet and get the answer in minutes. 
There are three kinds of people in r/askscience: those who ask, those who answer, and those who think they know they answer because they heard about it from some guy one time. Your professors want you to be some mix of one and two, not one and three.
Homework is about learning to apply concepts that you have learned in class. You have to understand how to take a bunch of identities and weave them together to make a proof. So asking the internet to do your homework for you is a disservice to yourself; the professor is really acting as a traffic cop. Note that this doesn't mean you shouldn't ask about a smaller bit if you get stuck. If the assignment is "plot out the shortest drive from New York to Los Angeles" then asking folks "how do I get from NY to LA" is off limits, but "I'm stuck in Des Moines; what's the best way to head west from here?" is not. (As a wry side note, very often in programming fora someone will ask how to do something, and the response is "why are you trying to do that?" Many people get frustrated at this response, but there's a reason for it. While I have this analogy in place, it would be like showing up in a US maps forum and asking "How can I drive a car from New York to London?")
he didn't seem that angry.
I write java code for a living, I know the concepts and everything, blah blah blah, but I still need to google mundane details sometimes, rather than spend time digging through their source. Google is a programmer's best reference, IMO. 
I disagree. Homework is a means for you to learn how to solve a problem. I think that if there are resources, they should be used. The idea isn't that you learn the *mode* of coding but rather the *means* of figuring out how to do it. Teachers should be providing a way for students *to* learn, not specifically teaching them. The old adage about giving a man a fish vs. teaching him to fish is applicable here I feel. If you're tasking a student with bringing back a fish and they realize they can just go to a market to get a fish, then by all means encourage it. You encourage lateral thinking that way. Let students solve problems in the way they figure to solve problems. 
Because, cut and paste programming is not learning. It is, get it to work and forget it. When I was in school, one of my professors did his labs in a small room, a few students at a time. One of the tasks was writing a FreeBSD device driver. Now we could have easily searched how to do it online or downloaded a book. He put us in a room, gave us 24 hrs and said finish it. To this day, I can remember how to do it. The struggle it took to get it done with limited research really set a foundation. modular programming where you search online for a previous solution may get the job done, but that is not creativity. 
Lets just hope your IRL name is not really Taylor R. You dun goofed. Props to your professor, He's a great detective. 
I disagree with your disagreeing. The point of homework (this in particular) is *not* to learn how to solve a problem. It's to gain practice using methods you've already learned.
And how do you teach a man to fish? You explain him the basics and then let him practice for a few days, then explain the next steps. It doesn't help if he lets a stranger do his practicing.
This applies to a lot of other fields, not just programming.
Good teachers are trying to provide a way for students to learn. The derivative of x^2 is 2x. Anyone who's taken first year calculus knows that to find the derivative, you move the exponent to the coefficient and decrease the exponent by one. However, anyone who really paid attention and did the work in first year calculus understands that the derivative is the slope of a curve at any point, found by calculating the slope using (y2-y1)/(x2-x1) as the distance between the two points approaches zero. I learned that in 1984. I still remember it. Even though I've forgotten most "rule of thumb" formulas for derivatives, I could figure them out because I understand the fundamentals. If the teacher had assigned for homework "what is the derivative of x^2?" and I just looked up "2x" on the internet, where would all that knowledge be today? I believe we learn by figuring things out - by forging the pathways in the brain through slogging our way from point A to point B with axe in hand. Having "eureka!" moments are what truly help us learn new concepts. Copying formulae from the internet does not. 
Let him practice on his own time. It's his impetus to learn, so it's on him to want to practice. It's not the job of a teacher to coddle along students at a set pace when there are those that might need less or more practice than others. When you set the pace for everyone you stunt those who should be moving forward.
Yes but learning to fish in this day and age is a skill not really useful. Let the kid know *how* but don't expect him to practice a skill when he can just go to a fishmarket. By letting him go buy the fish, he gains practice and insight in to learning how to best go about buying a fish (i.e. how to figure out the answer to a problem he cant reason out on his own).
I'm just curious as to what his Google Reader was set to pick up...
But it's homework. It *is* on his own time.
That's what finals and midterms are for -- to test the abilities of the student. The means they go by to learn how to solve those problems should be up to them. As long as the assumption and agreement between teacher and student is that the student must know the material at the end of the term then it is simply presumptuous to tell them *how* to learn.
He _can_ reason it out on his own, he just needs more practice at it. And he's paying for his education, I would hope he gets something better out of it than learning how to ask for stuff on Reddit.
Tell that to all the teachers who gave me zeros for not turning things in 'on time'.
I have apparently misunderstood the meaning of the word "teach"
Learning how to ask is an important part of learning. A very crucial part of learning too. 
maybe reddit
"On your own time" means not during classroom hours. It does not mean "when you get around to it". Just because you are lazy or choose not to prioritize your time properly it is not the fault of the instructor. FFS, you spent *thousands* of dollars to take classes, and you treat it like it's a tedious waste of your time. You really need to reassess your priorities and responsibility for your own growth.
Definitely. But not to the point that you should skip doing your homework to gain another asking opportunity.
Which I think is cool. Hopefully, by not reading him the riot act the student won't automatically revert to rebel mode and realize that learning the concepts and applying them is in his own best interests.
No, you just understand it in the analog sense. The way you think of teaching is fine for factory-style education, but to truly educate nowadays you have to encourage and reward lateral thinking, not simply rote memorization of material.
&gt; I disagree. Homework is a means for you to learn how to solve a problem. Not really. In this case, the goal of the exercise is not the answer, it's the practice of certain concepts. &gt; The old adage about giving a man a fish vs. teaching him to fish is applicable here I feel. And in this case, the goal is to teach the student how to fish, not to obtain a fish. When the student begs for fishes on the market, how does that improve his fishing skills? 
I certainly treat some classes as a tedious waste of time. Others, however, make that tedium worth it. Did you attend ever social gathering or activity that your college provided? You were paying for it after all, and it does teach and provide practice for social skills. If you didn't attend every one, why not? Edit: And "on your own time" to you is simply an extension of the classroom. When I speak of "on his own time" mean that as an "on his own terms". I'm not *faulting* the teacher for anything. If I choose not to do labs because I feel them a waste of time then that is because *I have made a value judgement about the worth of my time* vis-a-vis learning, and it is terribly presumptuous of you to assume you know best how I should manage my time or even how to learn.
Right, but this student doesn't code for a living. He is a student and is supposed to be learning the concepts and everything, blah blah blah. You going to the internet for answers can save time and benefit your client/employer. The student going to the internet for answers is denying himself the knowledge needed to benefit himself and future clients/employers. I don't think anyone is saying that looking to the internet for answers is always wrong, just not the right choice in this scenario. 
If we don't learn how to problem solve on our own, how will any of us survive post-apocalyptic times? 
You're stuck thinking of learning in linear, analog terms. That's perfectly fine for factory-style schools. I disagree with that teaching style. It discourages lateral, creative thinking.
School should be a place to learn to use the tools available to you, and how to use them better; instead of a place to memorize crap for a test/project, and then forget it once you're done with that class. Just because you ace projects/tests in school does not mean you will do well in an environment where you're solving new problems daily (sometimes). I can probably guarantee that some people that had a high GPA in college, aren't doing as well as others with lower GPAs in the real world.
Exactly. It's not about learning, it's about learning how to learn.
Wait... I'm arguing that a teacher needs to encourage students to figure out how to solve a problem on their own - that a major part of learning is making breakthroughs and applying the knowledge learned in class. As far as I can tell, the other guy is saying it's no business of the teacher's how the student gets the answer - if he or she just looks up the answer on the internet and copies it down without even understanding it, that's just fine. and you're saying *I* am encouraging rote memorization? 
&gt; Did you attend ever social gathering or activity that your college provided? You were paying for it after all, and it does teach and provide practice for social skills. If you didn't attend every one, why not? Just stop the bullshit right now, man. There is a difference betwen a social mixer and coursework which you registered for and which provides the marked progression towards your degree, and you fucking well know it. Stop wasting *my* time with this cockgargle. And FYI, I also chose to decline the "student activities" fee, which I could do at my University. So, no, I *wasn't* paying for those mixers or cheap tickets and whatnot. &gt; Edit: And "on your own time" to you is simply an extension of the classroom. When I speak of "on his own time" mean that as an "on his own terms". No. On your own time means, precisely and simply, outside of the classroom hours. You are spreading bullshit like butter to try to justify your own laziness and poor work ethic. You fail to impress.
I could agree with your point of view if the student came up with a novel way to solve the problem using different coding techniques. He didn't though, he simply asked for the answer. That is worse than simple memorization, in this scenario he would not only lack the knowledge of how to apply concepts in novel ways, but also the basic concepts themselves. He is in a programming class to learn how to program, not how to learn novel ways of cheating.
Well, we found John's account name.
&gt; Let students solve problems in the way they figure to solve problems. Okay - let's say Jack is learning to build bridges. Using your logic, he gets his coursework done by hiring Jill to do it for him. When Jack completes his education and is hired based on that degree, the company hiring him quickly learns he doesn't know how to do anything. (They may want to hire Jill.) The company also learns that the degree from University Fuckstain (founded by "manfrin") gives out degrees to idiots. Or are you suggesting that Jack continues what he learned at your university after school and just hires Jill indefinitely to do his work? How does he verify that what Jill submits to him is correct? Would you like to use a bridge built by someone from your institute of higher learning? sincerely, Richard Nixon
The kid isn't taking a course in buying fish; he's taking a course in fishing. You're missing the point. sincerely, Richard Nixon
Not if it's for a grade.
I agree to an extent. I just don't think the tools available to him should include asking the internet for answers. I fail to see how he is going to learn more doing that rather than figuring it out for himself. I think the more he can learn to handle on his own the less he will need to rely on others and ultimately the better programmer he will be.
How long does it take to teach someone to program? How long does it take to teach someone how to look for programming answers on google? If you're going to spend 4 years learning how to do something I think you should focus on the part that a senior programmer can't explain to you by saying, "Well uh... just google it if you aren't sure." 
Incorrect. It's on his own time, not just *for his own benefit*, which is what I think you're mistaking this for. On your own time means on time not regulated by others, e.g. in time over which you have control of your focus.
Agreed -- asking the question rather than looking for ways to solve it is the wrong way to go about it. This would be the same as asking coworkers how to do something when you could just as easily google around for the answer.
rss feed of new posts in /r/cpp
So we shouldn't teach addition, because everyone has calculators?
Dude, I just googled it: http://www.cplusplus.com/reference/clibrary/cstdlib/atoi/
AND MY AXE
I think you just miss the goal. Creative solutions are fine, as long as you keep the goal in sight. Here, the goal is not the answer, it's the practice. So a creative solution that makes you practice your programming skills would be fine. But when your creative solution brings you the answer without the practice, you missed the point.
You have a lot to learn. I hope that someday you do. In the meantime, you should leave school now and make a lot of money while you still know everything. 
bullshit; pedagogy (step-by-step, linear learning) is an excellent way to obtain a base of knowledge on a new skill or topic. I agree that *adults* don't advance very far with pedagogical techniques, and should be taught with androgogical techniques once they have the basic skills. However, the sort of "free exploration" you have been recommending is most valuable one someone has a working base knowledge.
Especially for *homework*. It's not like he was taking a test, and used reddit on his android phone to solve a problem at the last minute. WTF, John.
God it is *so* hard to read thoughtful, insightful responses from you, and then hit "Sincerely, Richard Nixon" - it feels like I missed some subtle sarcasm somewhere...
Well, I certainly applaud anyone wanting to do their homework by themselves, but take it from this old studyhall rat, I've spent my entire adult life in a university, and a program like this one can do more harm than good. If you only train one part of your problem-solving arsenal (and that's all a single mental exercise like learning-on-your-own is going to do for you), you're setting yourself up for failures down the road. I've seen it a hundred times. Solving in isolation basically only trains you to not ask for help, and to some extent, to check the back of the book for answers. What you really want to do is train your group solving skills, all the major interpersonal skills (leading, following, collaboration, respectful disagreeing) at the same time, over the course of a semester. And don't forget your TAs office! I'm proud of you guys wanting to do this. A-triple-plus! Falling in love with problem solving, analytical thinking, etc., is one of the greatest things you can do for yourself. And you WILL fall in love with it if you can just force yourself to stick with it a year or two and experience the amazing grades you'll make. But do it right, okay? My advice, find any coffeeshop, with free wi-fi and friendly baristas who won't rat you out (especially in the beginning, until they get to know you better) and will make you a double-tall during your quest for better cheating methods. One or two hours a day, three days a week, is all you'll ever need to surf (I refuse to believe anyone is so google-inept that he or she cannot solve any assignment in that time, especially considering how free college schedules are). And don't worry about being embarrassed or not being google-fu enough the first time you hand in a scammed assignment. You have to start somewhere and almost every one of us were there ourselves at one time. So no one will say anything to you and very, very quickly, you will progress way beyond that stage anyway. Now get out there and get As! :-)
&gt;The way you think of teaching is fine for factory-style education, but to truly educate nowadays you have to encourage and reward lateral thinking, not simply rote memorization of material. That's really funny! Gimli suggested that simply memorizing the answer or even memorizing the trick about moving the coefficient and decreasing the exponent for the question "What's the derivative of x^2 ?" would not be helpful, that really understanding the relationships between functions and their derivatives was far more powerful, and would enable the student to independently derive the answer not only for the given question, but for many others that might not seem immediately related. This is the anti-thesis of factory-style education, where rote application of algorithm and procedure substitutes for understanding. Asking someone on the internet "What procedure can I use to perform this task?" instead of thinking about the essential nature of the problem and independently figuring out a solution is not divergent or lateral thinking - it is the refusal to do any real thinking on your own. It is the pinnacle (or nadir) of factory-style education. The instructor isn't looking for code that will do this: he already knows how to solve this problem. The purpose of taking a programming class is for the student to develop skills in analyzing a problem, reducing it to its critical components, and taking a limited set of clearly defined operations and applying them in novel ways to perform complex actions reliably to solve the problem. The point is to learn how to think like a programmer, not just copy code another programmer wrote. Shakespeare's sonnets are available copyright-free online. Would you really consider that you had learned how to effectively compose a sonnet if you just copy-pasted from Shakespeare and turned it in in response to the assignment "Compose a sonnet" ?
[bestof'd](http://www.reddit.com/r/bestof/comments/dugkr/student_asks_programming_question_on_reddit_and/)
A person interested in learning will use his time teaching himself regardless of the environment, in academia gpa for employment is created by answers not education.
How did you...
So what you are saying is that programming is not really a useful skill in this day and age, because all the good code has already been written and you can just go buy it or snag it off the net? I can tell you that, as much as collaboration and people skills are important in the workplace, if I have a choice between hiring a programmer who only knows how to ask his friends and hiring one of his friends who actually knows how to write effective code, the friend who can really code will get the job 100% of the time. 
&gt;the assumption and agreement between teacher and student is that the student must know the material No it isn't - it is to understand the material. If it was just to know, then the student would be unable to answer any question he had not seen before, exactly the same.
&gt;treat it like it's a tedious waste of your time. Often times it is. Schools make people take classes just to fill up their time quite often, electives and other such classes for example. 
&gt; it feels like I missed some subtle sarcasm somewhere... Maybe you did... sincerely, Richard Nixon
Just stay off of the road?
congratulations, you found a bad idea!
"University Fuckstain". I think I'm going to start referring to my alma mater that way. It seems like that was its operational mantra.
You sir, are my hero. The process of teaching involves, not just the ends, but by what means the ends are met. One cannot perceive a destination until they experience the journey. Your trip analogy is spot on and is the basis by which all professionals should teach.
"Mr. Hand, I've been thinkin'; since I'm here, *and* you're here, doesn't that make it our time?"
Except that the student in question is solving an inconsequential problem now at the expense of being able to solve consequential problems later.
http://amzn.com/B001U3YQ0Q
I took programming languages, risk analysis, and data management courses for my degree electives because I wanted to round out my technical knowledge. I took art, film, and music appreciation classes to fill my humanities electives, because I wanted some context to work with, and to appreciate art with some comprehension of the media and some "tools" at my disposal. If you find yourself in Basketry 101, you only have yourself to blame. College is not (solely) about turning you into a cog for the machine, it's an opportunity to learn *and* grow.
Thanks, but the homework was due last night (which is why I turned to reddit). I'm about to go to class, I'll post the whole story when I get back.
I'm going to class in 45m, I'll post the whole backstory when I get back.
I just want to say, I upvoted all of manfrin's posts in this thread, not because I agree with him (I don't), but because he helped create a worthwhile dialogue. Redditors shouldn't be downvoted just for being wrong.
I call bullshit. Whenever I felt the need to ask the internet it was because the concept was poorly explained (because the teacher was unable to view the problem from the point of view of someone unfamiliar with it, thus not correctly highlighting the importance/point of the problem). Anyone visiting reddit is nerdy enough to actually feel motivated to learn this stuff, so I say the teacher failed badly at "communicating the concepts" that were so important to him. Take that, teacher! I don't fear you on the internets!
"...and what's wrong with a little feast on *our* time?" You are my new hero.
He damn well better know how to fish after the zombie apocalypse. 
You call your instructor by his first name? Interesting.
&gt;redditor for 5 months You fucking delinquent, you've made 4 comments in 5 months? You've been summoned almost everytime Gimli comments! That's like ... millions.
Interstate 80 West. Just be sure not to go south on accident at the I-35/80 junction, or you'll end up KC.
&gt;Just because you ace projects/tests in school does not mean you will do well in an environment where you're solving new problems daily (sometimes). I can probably guarantee that some people that had a high GPA in college, aren't doing as well as others with lower GPAs in the real world. Sometimes this is because they got their high GPA's by asking the internet to do their homework for them. Also, upvoted for using the expression "probably guarantee".
Many people learn better in a structured environment. There's a *lot* of information out there, some good, some bad, and some good but disorganized. Education and teachers exist in part as a filtering and rephrasing mechanism.
Lots of profs go by first names. Generally the cooler ones.
at my uni, all profs in all departments are referred to by their first names. 
&gt;As a wry side note, very often in programming fora someone will ask how to do something, and the response is "why are you trying to do that?" I ran into this earlier, actually. Fixing a couple small issues with a nice web app I built for internal use. We're all IE internally (shoot me), so I'm using a few ActiveX tricks here and there to make things easier.. such as exporting documents/spreadsheets directly into Word/Excel rather than making the user download a file. Nice trick. So, the text being generated has double spaces after each sentence. A formality thing, I guess. The double spaces simply refuse to export. So I go looking online. From when the same question was asked back in 2003: "Why are you doing that? That hasn't been a typography standard since the '50's." Yes, but I need to do it. It's not your position to tell me that I shouldn't have to do my job; it's MY job. They need something, I do it. If they want double spaces after their sentences, they want to get them, they don't care if it's "standard" or not by your definition. If you can't answer the question, don't waste my time by telling me my problem shouldn't even exist. It does, now help or gtfo.
&gt;But seriously, I never really understood why professors get upset when students utilize the largest body of knowledge ever collected (aka The Internet) to solve problems. Yeah seriously, I mean what's the point of learning anything at all when you can just google it? Who needs to be presented with historically tested methods of understanding ideas which in turn train the brain how to not DERP all damned day. Not to be rude, but crowd-sourcing and the mob democracy of the internet are not to be wholly praised without understanding what they UN-do. School is a place of intellectual experimentation (among other things), not an answer-generating institution. **TL:WRWP — School is questions, not answers.** 
Using C functions is generally frowned upon in a C++ class.
How can you so stubbornly defend the fact that it's part of learning? He doesn't learn to apply anything by simply asking. Learning where to find an answer isn't nearly as productive as learning to come to it yourself. Frankly, it doesn't take years of education to jump onto a computer and post to a couple forums. If everybody asked everybody else instead of learning nothing would ever get done. 
...WOUND
There's a continuum of "why are you trying to do that?" As you point out, in this case it's a stylistic issue - my approach would be that I wouldn't reply unless I could actually help on the technical issue. And then I would ask the question first, just in case you weren't aware that single space after a full stop was now the norm; but I would also go on to answer your question. Then there are times where a question is strongly indicative of a bad architecture decision. I guess a real-life example would be someone asking "what's the best food to pack if I want to hike the Appalachian Trail?" Note that this is an incredibly valid question on its face. But an expert hiker would have alarm bells going off, because anyone planning to actually hike the entire Appalachian Trail should *know* what food to pack... Finally, there are questions that simply indicate something very bad is about to happen. The epitome of this one would be "How can I automate Word on a web server to convert documents for a Facebook app I'm building?" Yes, it's technically possible, and there is a technical answer. But you shouldn't be doing this for any reason whatsoever. 
You could only BE so lucky to end up in KC.
But "class" is a much larger concept these days. A homeless person with only an iPad stealing WiFi from Starbucks can, in enough time, complete a Master's in just about anything that doesn't require hands-on testing with a cadaver.
Yeah, but you have to drive through Missouri to get there.
Because they didn't go to school for so many years to teach students to google answers. If that was the case then good luck finding a job with a meaningless degree. I wouldn't even give you a dollar for a degree that you essentially googled to get.
Thank you for taking the time with that troll. While I'm normally against feeding the trolls that kind of entrenched "I shouldn't have to demonstrate *knowledge* just the ability to produce results" attitude needs to be exposed as you have done. People who learn using manfrin's philosophy are the ones who kill others with their poor designs.
Do you wonder *how* a professor knows when it's one of his students? After all, thousands of classes across the nation are covering similar material. It's not when a student asks for some nitpicky aspect of a problem. I also believe most profs won't intervene if they saw "Hey, this is my homework problem, and here's how I'm trying to solve it, but I keep getting stuck - can someone help?" or "Does this answer look right?" The issue is when a student posts a homework problem virtually verbatim, asking for a solution, and doesn't even identify themself as a student or that it's homework. That's when it becomes "teach a man to fish, but then he goes to the store and buys a fish."
*Gimli is considering changing his username to 'muffdyvrsHero'*
Now ***knock that off***
I always called all my teachers by their first name and so did anyone around me until I went to an English speaking college. That was a weird culture clash to me. I live in Quebec.
Unless he's been living an elaborate double life, Gimli\_The\_Dwarf is not a college professor teaching programming at this time. Browse through his comment history...
Whethr it's a single book, a friend or the largest body of knowledge ever assembled is irrelevant - they are still not thinking for themselves.
Fair enough, but if someone's going to keep writing you a nice check to keep working on that Facebook conversion that you should never be doing, you'll keep doing it.. unless you've got another job lined up. I almost had it happen a couple days ago, regarding a particle engine I'm in the process of "finishing". The 3D is pure trickery. The points are being calculated in 3D space, but the drawing is by no means "proper" 3D. I "should" be using rotation matrices, but I'm not, because that's not how I wanted to pursue this. I don't want you and your seventeen proggiting brothers to tell me I should be using a rotation matrix, I want you to help with the one small issue that I have. If I wanted to do it the "right" way, I'd just be using DirectX. Not every situation needs to be completely practical. I keep doing stupid things over and over again at work because they keep paying me to do them. I'm building a 3D particle engine using faux-3D because it still accomplishes the final goal, and thus proves that the idea I had *does* in fact work. Feel free to ask *why* I'm doing something a certain way, but if I've made the point that I'm doing it this way *because I want to*, don't tell me it's wrong. I probably already know this.
Maybe the concepts "presented in class" were insufficient and the redditor just wanted to be efficient so he could get his homework done AND he could go get some tail.
You forgot to say something. ಠ_ಠ 
But some questions are "I need to pound a nail, what do I use -- a glass bottle, or an old shoe?"
There's a big difference between being able to cut and paste and being able to understand the underlying concepts. In engineering you CAN look at what has been done before and apply it to your own situation, if you're lucky it will work out, but there's a big difference between being lucky and being good.
I'm sorry, but I don't see the reason. What is it?
3 comments*
The I-35/80 junction is confusing. For a while I ended up taking 235 to go around it. I think they recently (in the past few years) changed it so it's less confusing, though. Either that or my GPS is just taking me an easier way now.
That is a very good way to put it.
&gt; "I need to pound a *screw*, what do I use -- a glass bottle, or an old shoe?" I believe this is what you meant to say, sir or madam. Sometimes it's a rivet instead of a screw, then you're really in trouble, and should probably just sit back and watch the show.
Agreed, it is very hard to find code that you can copy and paste and it just works without any changes. Usually I find basic examples of something I want to do and get it working, but then I have some feature or twist that I'm implementing that requires reworking most of the example. I don't like to reinvent the wheel, I don't have time!
Or ask the question on Something Awful where (some) forums aren't visible to unregistered users.
Absolutely. He's nothing but a contrarian waste of time. I've had to fire people like him, and they try to alternate-universe-logic their way out of the situation, when the point actually is, hey, *you're fired*.
Uh oh. As a relative novice to C++, I've always been using ato[i|f] because that was the first thing I could find on Google, just like the gp post. What should I be using instead?
If you've tried and tried but still keep failing, it's likely you are missing some key piece of information necessary to accomplish your task. This might be a result of not paying attention in class, no bothering to read your book, a professor or book assuming you have knowledge that you don't (and glossing over key topics), or simply that you're not seeing how things are connected. I see no difference in asking reddit vs. asking a classmate, TA, or professor for help in pointing out what you're missing. The point of HW is obviously to try things on your own, apply concepts from class, and to practice, but also to identify what it is that you don't understand from lecture or the book. At some point, no matter how much you try, you need help. As someone who has taught programming (as well as many other engineering classes), I know first hand that I often glossed over things because they seemed obvious to me...but I have many years of experience, and in most cases, my students have none. Sometimes this meant that students were totally lost and couldn't make the connection from A to C, because they didn't fully grasp B and its relation to A (speaking in abstract terms). In these cases, I didn't just tell my students to suck it up and figure it out on their own (or make them feel bad for trying to use the internet to learn), instead I spent time to better explain the concept so that they could learn (because you know, it's my job to teach to them). I tried very hard to be approachable, so that students would feel comfortable asking questions. I had professors and TAs in college that were such jackasses...they tried to make you feel stupid if you asked a question and acted as though showing up to their office hours was wasting their time. I tried hard to not be one of those people. With that said, I always refused to help students in my classes that hadn't put any effort forth to at least try to solve a problem. I've definitely had students that were just lazy and wanted to be given the answers or told how to solve the problems, without ever trying on their own (and in some cases, students showed up to office hours without having even read the problem). I specifically wrote tests that would be very difficult if you just copied the hw, to reward students that really put the effort in. If the OP had just posted "how do I do this" with no code to show they had thought about it and attempted it, I might agree they were just lazy and wanted someone to do their hw. But that doesn't seem to be the case. Instead of complaining to the class listserv, maybe the prof needs to spend a little more time clarifying the concepts he is teaching. He might need to be a little introspective and think "how do I treat students that ask for help? Are my TAs helpful?" Or maybe the student is just lazy, as they seem to have waited until the last minute to do their HW.
I am *not* upset that he hasn't been following me around. 
They're lies, I tell you - all lies! No, wait...
atoi doesn't include any error handling. You can use strtol or sscanf as an alternative.
&gt; I call bullshit. Whenever I felt the need to ask the internet it was because the concept was poorly explained Good for you. But that certainly isn't the behaviour of everyone on programming fora. The original poster is better than most in this case. He actually tried to think of an answer on his own and said what it was. But I will typically see questions that are just cut-and-pasted from homework with no effort whatsoever on the student's part.
Professor's preference. 
Point being those people probably also have requisite knowledge on HOW to 'nut it out' if they have to, where the person simply googling all the answers won't necessarily learn shit from it and, in the absence of the internet, remain a complete waste of space as they have no inherent knowledge on the subject at hand. I can watch a video of how to play a certain song on a piano, it will never translate into me knowing how to 'play the piano' in a general sense.
&gt;AND JOHN'S AXE! FTFY
You mean lateral thinking like rote memorization of material you got from the internet? 
But I am.
Is your iPhone jailbroken? I'm guessing the -92 is the db level of the signal, but what does the 29MB represent?
&gt;Asking someone on the internet "What procedure can I use to perform this task?" instead of thinking about the essential nature of the problem and independently figuring out a solution is not divergent or lateral thinking - it is the refusal to do any real thinking on your own. It is the pinnacle (or nadir) of factory-style education. The instructor isn't looking for code that will do this: he already knows how to solve this problem. The purpose of taking a programming class is for the student to develop skills in analyzing a problem, reducing it to its critical components, and taking a limited set of clearly defined operations and applying them in novel ways to perform complex actions reliably to solve the problem. The point is to learn how to think like a programmer, not just copy code another programmer wrote. This.
You smell funny.
TLDR; *"Conclusions. To have move useful, it must be implicitly generated in many cases. To minimize surprises and bugs, no move operations should be generated for classes with a user-specified copy, move, or destructor. To keep the rules consistent, the generation of copy operations should be deprecated for classes with a user-specified copy, move, or destructor."* 
I assumed he would reply and didn't want to edit my comment. I'm doing the opposite of ret-con, IRL.
free RAM
I admit, I only read over the op's question quickly but it reminded me of typical beginner's problems that have more to do with arbitrary quirks of how a programming language handles strings/arrays rather than logical thinking/learning. It's like asking for a historic date in a history paper. Worst case for the *student:* He didn't listen when it was explained, in which case asking someone else (or the internet) is the best fix imaginable. 
Probably had google alerts setup for "read a line convert to int" or something similar from the assignment. 
What sucks is, if anyone takes your comment seriously, they're already a participant in number 3. That's the problem with following logic through. Consider all the variables. Not just the ones you care about. 
And except for the unlikely event that the person asking really is just that dumb, chances are it's a corporate question, and the only tools provided to the builders are beer bottles and shoes. Happens to me all the time.
How is this for an subtle all or nothing ultimatum; *"I would prefer to see move become as common and as well understood as copy. If we cannot achieve that, we should consider eliminating move semantics from the standard (which would imply a most undesirable delay and loss of functionality)."* Imagine that, move being removed from C++0x just like concepts was. But it seems a good solution was found. That means only the C++98 code that "should" be broken, breaks. On another note, I love reading these inside papers. What other languages have such an open and transparent evolution process? Also its nice to see there is not a whole lot of politics going on. The best solution for the good of the language will win out, as always.
| I never really understood why professors get upset when students utilize the largest body of knowledge ever collected (aka The Internet) to solve problems. **(1)** The professor doesn't give a fuck about the solution to the problem, he already *knows* that. The purpose of the exercise is to make *you*, the student, think about and apply the concepts you've been studying, so you learn and retain them. Learning how to shop work out to third parties via the Internet may be a good exercise for a business class, but it's worthless for a *programming* class. **(2)** Getting a stranger to write a 20 line solution to a homework problem is easy. People like to show off and/or they consider it good practice (the entire point of *you* doing it yourself), and it only takes a few minutes for someone who has already internalized the concepts you're supposed to be learning. But getting that person to then *do your job* for you, writing thousands of lines of codes over the course of weeks and months, is an entirely different proposition. If you can't figure out these problems when they are spelled out for you in a homework assignment, you're going to be extra fucked when it comes to figuring them out in real world code.
But if you just look it up on google, you're not learning it. You aren't knowledgeable if you *have to* look it up on google to solve it. That is to say, if you are completely incapable of solving the problem on your own. The point of school is to give you the tools to solve these problems on your own, in case you need them. You might not, because you will sometimes be able to google it.
Heh, I can't help but side with the students in these cases. The only ones I met *truly* lazy enough to copy an answer without using it to understand the underlying material are the ones who failed classes. I wish all teachers had an approach as liberal as yours, but the "ask the internet == cheating" one is so common, it always gets me a bit angry when I see it being abused for some power games. A bit too tired to check what exactly is going on here, but I've seen it before.
 &gt;Yeah seriously, I mean what's the point of learning anything at all when you can just google it? Totally. Taken further, we should just revamp universities so the only degree you get is Bachelors of Googling. That ought to give everyone the skills they need to be a professional anything! 
Especially when it comes to programming... googling for answers to simple conceptual problems will not benefit the student. I speak from experience and I can tell you that when I used to get stuck and just google for answers or look at past semester's solution (that's WORSE!!!), I would get a good grade for the assignment, but flunk the midterm. Then when I took a class that involved coding in assembly, I did everything myself and the end result was a good grade, deep understanding, satisfaction and confidence in myself. Google can't give you that, especially the latter part. I can also probably guarantee that some people that had high GPA in college are doing better than others with lower GPAs in the real world... 
True, but unlike the C++ function calls, you actually know exactly what the person is trying to do. I've programmed in C++ since the ARM, but to this day can not see how anyone considered stream notation readable. Hungarian notation or similar becomes not just a convenience, but a necessity so you can make your code readable. 
| Every way leading to a solution of the problem is a good way. The solution is irrelevant, ultimately discarded. The end goal is not a solution, it's the *understanding* and *knowledge retention* which results from trying to work out the solution.
I'm not sure I understand what you mean, could you be more explicit?
That does *not* look like a C++ instructor.
That would have been ok in 1971, or in 1989 when it was first added as part of the standard but without your own error checking you didn't know if you did atoi("nonumbers") or atoi("0"). strtol was added later and is much better, or of course you can use streams.
&gt; students were totally lost and couldn't make the connection from A to C, because they didn't fully grasp B and its relation to A They should really learn the alphabet before trying programming. /trollface.
...and the response is "why are you trying to do that?... I HATE HATE HATE this. I hate it because I choose my questions carefully with the knowledge that the particular constraints I'm under are forcing down the present path. Once I explain why, most often they agree that my approach is sensible, meanwhile, I've wasted my time with giving explanations when I could have been getting answers. And if I truly am misguided, one should say something along the lines of, "You could do it that way, but a better way is X" - and I guarantee you, if one puts it that way, I'll think hard about what you're saying instead of blowing you off. Take note! Your breath is not wasted if you just answer the question!
Unless he's going to stick with pure theory forever, learning how to deal with the quirks of languages is just as important as the theoretical concepts being taught. Knowing these concepts in an ideal setting without being able to apply them in the real world isn't useful and there are no "how to deal with all the different problems you'll face when trying to actually apply all we've taught" classes.
Doesn't seem like this guy is teaching a c++ class. 
&gt; As a wry side note, very often in programming fora someone will ask how to do something, and the response is "why are you trying to do that?" Many people get frustrated at this response So true.
We had newsgroups for this! (Seriously, the professors recommended it, and the TAs monitored it to answer questions)
Not him. He's not a professor listed on my school's website, I've done the google search and he's not easy to find. 
UT?
That's the amount of RAM available.
you go to Sac City
Aren't you supposed to pass the exams exceptionally well to get high GPA? Typical grading scheme 40% homework 60% exam. So if you copy-pasted your homework from the net, you will fail miserably on exam. Cramming stuff before exam won't work either (if you study CS). In my college some of the professors used to give exam questions with similar wording as sample papers, but with few catches, so if you just stupidly tried to put answers together based on your home "solutions" you won't get great grade -&gt; no high GPA.
Probably streams, but atoi is considered deprecated by strtol if you want to go a more c route. 
C++ isn't my forte, so I'm not an expert but in perl I'd just encapsulate it in an eval() in that case. How hard is error checking in c++? Also, it's a freaking school assignment, how much error checking does he need? It'll do what he wants done. 
Nope.
Indeed.
Oh you ninja. Have a pineapple.
No, jackass, I found a perfectly workable solution. Thanks for playing!
&gt; there are questions that simply indicate something very bad is about to happen. The epitome of this one would be "How can I automate Word on a web server to convert documents for a Facebook app I'm building?" Yes, it's technically possible, and there is a technical answer. But **if you have to ask** you shouldn't be doing this for any reason whatsoever. FTFY. There are reasons for doing things like this, and if you've got the team, knowledge and budget it can be done. However it's a fucking nightmare to deal with so you better have explored every alternative architecture decision before you even *think* of going down this path.
College is about learning how to find the answer, not necessarily knowing the answer. Seems to me that this student is just doing what college students need to do, find the answer. 
I think it's totally valid to ask "Why are you trying to do that?". At work someone is always asking one of the senior developers for help. Sometimes (not all the time but enough to be noticeable) the person asking for help has started heading down a track and kept following it deeper and deeper into the forest, and can no longer see the wood for the trees. A fresh set of eyes and the question "Why are you trying to do that?" often leads to a totally different way of looking at the problem, and a much simpler approach that completely avoids the specific issue the junior developer is trying to solve. Another point: As I tell my occasionally ungrateful kids, don't look a gift horse in the mouth. If someone is asking someone else for help, they shouldn't criticize the helper. The helper may be asking that question with the best of intentions and shouldn't get slammed for it. Even if the helper is being snarky, the person asking for help shouldn't complain. If you ask for help you're looking for a favour and the people you ask are under no obligation to help or even be civil.
If college is just about "ask around until you find someone who knows how to do it for you" then we have some problems...
Actually I picked that particular example for a reason - don't automate Word on a server. Ever. It's not supported, it's not licensed, and stuff is just gonna keep breaking. :)
I had a minstrel once following me around. But we ate him. 
Hi John!
http://en.wikipedia.org/wiki/Atoi#Deficiencies &gt; It is impossible to tell whether the string holds valid sequence of digits that represents the number 0 or invalid number as the function returns 0 in both cases. The newer function strtol does not have this deficiency. ... &gt; atoi is considered to be deprecated by strtol.
Personally, I wouldn't consider this as the type of thing you would openly say on a website since you've already been busted by your professor - a prof who not only reads reddit, but subscribes to it on google reader - for seeking help with your assignment. He might get a bit freaked out and who knows what would happen then...
yaaay.
or r/domyhomeworkformeplz
Well if you put 40 odd "slave" XP VMs on a server, make sure to reboot them weekly, write your own server/client software while isolating XP from the internet and use the standard Word desktop APIs while praying like hell it can be done.
&gt;I think it's totally valid to ask "Why are you trying to do that?". I completely agree. Where I have a problem is when people reply, explicitly refusing to help, because they don't approve of my problem. That's not help, that's "fuck you" in the form of pixels. The moment you tell me I shouldn't do what my boss says, you've lost all credibility. I have a job to do, and if it involves doing something esoteric because the business needs it, then guess what? It's my job to do it, regardless of if it's "standard" or not.
[Guitar Solo]
I can see why you don't want anyone to make a village, you must be the idiot. :o
Mr.Fujiwhereisthebunter i assure you, that indeed i am the idiot...and i am sure that mouth stays open for all the penises from my village.
Because you'll NEVER do that as a professional developer!
I like you, you're silly.
In this world you have to be...i not only sit on my ass, am one too.
There is a function that handles it for you, why reinvent the wheel?
How much your GPA reflects your exam performance really depends on how much work the professor wants to put into grading, and in some cases, how well the rest of the class is doing. In some classes, the homework/exam ratio is considerably higher than 2:3, and good exam grades may or may not reflect mastery of the material. I know of a well-respected, highly selective state university where the entire engineering physics sequence was (I don't know if this is still the case) graded on a strict curve. There have been many semesters where a 50% on their exams got people a solid "C", and a 70% was an "A". It frightens me that people with degrees from that school go on to design bridges and aircraft, but they do. 
How did he keep you there for 24 hours? Were there beds available? Food and water? Or are you being metaphorical? 
I have a secret: The professor isn't running a sweat-shop for homework answers. He doesn't want a solution. Repeat after me: He doesn't give a fuck about the assignment problem. Giving him an answer is useless horseshit. He wants you to know the subject and to prove that you know it. That's all.
This.
There where sign up sheets for each day and 4-6 students registered each day. You got the assignment in an email that morning and from that point you had 24 hours to submit it via email. You could leave whenever you wanted. You could bring your laptop and outside material, but he asked us polity not to. You where allowed to go to eat and do whatever else you had, but you did have a 24 hr limit on the project. You could even help each other as a team with generalities if you wanted. So in no way where you trapped. You where just given a time limit. It was by far one of the most educational classes I have ever taken. 
Those five digit patterns are giveaways. Or else, I've just been here too long.
Some kid was asking for help on a homework assignment Monday and I looked at it because I had some knowledge on the subject, then I realized it was the homework for a class I grade. I asked him if he did go to that school but he never responded. I'm not the professor so I don't care how they get the answers.
Homeless person with an ipad, lol. I can't afford a fucking ipad, they can't afford rent.
This is exactly right. If someone is coming to a job with a degree in hand, then I assume that they know the how's, what's, and why's that were supposedly taught to them in their four years. That's why they get more money, etc. If you're just a googlin' fool, then you're no better than a sharp HS student, and I might as well save a few thousand $$$$ and hire them.
I think the teachers should ask harder questions...questions that they don't even know the answer to....questions that no one knows the answer to. And then you have to try to find out the answer by asking anyone/everyone you can. And if you get it, awesome, you just helped the entire world learn a little bit more. A+. If you don't get it, they could grade you on the amount/quality of work you did it in attempting answer it. So you'd keep a detailed research trail the whole time. And the teacher would grade you on whether you followed a certain train of thought to it's end. And your creativity in taking different lines of thought. Then, kids could ask Reddit, Wikipedia, their uncle, the cute girl sitting in the front of the room, the lunch lady, members of Congress, NASA engineers, etc. But they can't stop there...they have to keep following the evidence until they come to a dead end. And then try a new one. Do this until the assignment is due. Then hand in your answer or the research trail. I think the world would be greatly benefited by this. 
Yay, more complexity for C++...
shoe obviously
Probably this: http://www.reddit.com/r/cpp.xml
Something something something Denmark.
...bravely ran away, away....
Well yeah. Everyone knows the bottle is only for *removing* them.
Holy shit, I know her..
How did he find out it was you? Was it the inclusion of your last name in your user name or was it when you submitted the homework.
Well if someone showed him how to do it and he learned from it. What is the harm. Its like one of those books that come with not only the answer to complicated math problems but the steps to solve the problem so you learn how to do it.
&gt;but if someone's going to keep writing you a nice check to keep working on that Facebook conversion that you should never be doing Well if that's the case do you plan on sharing that nice check with the people who kindly take time out of there day to help you with the task you shouldn't be doing in the first place?
Tell his old outdated ass that you're being resourceful. Don't let these naysayers tell you that you are hurting yourself, you're helping yourself. All the time you are putting into finding out the answer by researching it out on reddit, I'm sure it will have lasting impact on your brain. That's called being resourceful, a real world tool that you will need later on in life. You know how to think for yourself, outside the box, bravo. You are after all working with computers...
I AM the person that writes the answer these people google. How did I get so awesome good? Googling the answers, then reading a bit more around it by googling. Most students though don't get past the first half of the exercise, and that's where they're letting themselves down.
&gt; What other languages have such an open and transparent evolution process? Rhetorical? If not, Python's PEPs seem fairly open, I think I see a dozen or so Haskell language extension papers a year, and Scheme is presently in the process of revising the errors of its most recent report. Go's creators seem to be very active in the official forums -- or at least proggit leads me to believe that. I'm with you though -- it's an exciting time for programming language evolution.
&gt;TaylorR137 [S] 11 points 9 hours ago[-] ಠ_ಠ 
just ask on 4chan
*Shrugs* I'm not saying you can't learn from google I'm just saying that if you're going to bother being in a traditional educational system you might as well make your professors (especially this guy, he sounds chill) earn their paychecks. 
brave sir robin... danged angry birds FTW!
It is blocked at work?¿. DERP
Fair enough. Go, though. Its still very much a proprietary language with a "benevolent dictator". But with C++, anyone can join the meetings and have a vote, and Stroustrup only has one vote just like everyone else. I'm not familiar with the python or haskell process, but I would be happy to learn they work the same?
He means that YOU are just "some guy", also. Anyone taking your comment seriously, and as gospel, is akin to what you describe in your 3rd kind of person. Realistically, in science, you wouldn't want to be any one of those three - or any combination of those three. What you'd want to be is a continual loop of all three.
Isn't it different? "scripted" languages and "compiled" languages? no dis-respecting to python and bros, but... isn't it different?
How about swapping the value to be moved with a default constructed value, instead of moving? that will maintain the invariants, since a properly designed class always sets its invariant in the default constructor. 
They aren't the ones getting paid to do the task they shouldn't be doing. Hell, if I'm being "proper" about asking my questions, then you'll just be helping me with the occasional hiccup I run into along the way. I help with people from time to time myself, as internet communities helped me get where I am today. 
Why would it be? The language grammar needs to be proplerly designed either way. Otherwise you get php.
This is exactly what is wrong with education. It should be broad rather than about specifics. A good solution that goes outside the established norms should actually be given more credit. In the UK it has historically been that if you get the answer right then the path of getting there is irrelevant. Unfortunately I think this standard has been abandoned in favour of 'no credit unless you do it the right way' which is utter bollocks.
This is good, common sense stuff. Now if only there were a follow up article on how to land a developer job at a company that has this sort of common sense. Or, you know, thinks about architecture for maybe a few goddamn minutes before anyone sits down to code. Ooh - or, I know - doesn't put important logic for a server piece inside of a FUCKING MFC GENERATED VIEW CLASS. Okay, back to work. FML.
The one lecturer we had who was known for this basically had an underground email class note sharing ring based around his courses. He was renown for coming in, drawing a line down the middle of a board, writing his notes onto the board verbatim and then pissing off before anyone could ask any questions. Every year an archive with the collected notes from several previous years are emailed out to the new people in the 3rd semester. At that point you may as well not turn up because the notes are as good, if not better, than the lecturer. I actually wonder if this guy was malicious or if he was one of those who only wants to do research and has been forced to take up a teaching post.
He is doing CS. He will have to go far to get some tail. In my year we had 3 girls in a class of 180. We were put in the same building as maths, physics and engineering just to exacerbate the problem.
But, python 3 isn't back compatible with 2, Haskell made some changes through the years, And about scheme I don't know but you cant "revise the errors of its most recent report" without breaking some eggs. ;) C++ standard is compatible after 25 years (at least the standard), that's the difference between scripted and compiled I'm trying convey.
He gives good advice, but seriously, not use Qt? Qt triples one's productivity, especially for the UI part. 
You're still missing the point. Do you feel that paying someone to do your homework for you is an acceptable way to reach the answer? 
I'm a firm believer in building our codebase on as many platforms and as many C++ compilers as possible: every compiler nitpicks on different things, and it is well worth the effort (IMHO.) The issue I run into here is that, while we deploy our software on Linux servers, the majority of developers prefer developing in Visual Studio on Windows. So the VS solution serves as the "master" for the Unix build environment, which is this hacked together set of Ant (!!!) build scripts and a custom Ant task that pulls stuff from the Solutions/Projects. People get addicted to their IDEs, and the last time we looked Eclipse CDT wasn't all that good. Perhaps it's better now. Myself: I just want my Emacs and Makefiles and tags and I'm good to go... :) 
No. There is a tradeoff here. You are effectively screwing the students who will do things properly in order to protect the ones trying to cheat. The only people losing out in one circumstance are the ones cheating. They don't learn and their lack of knowledge will be caught pretty quickly out in the real world. By trying to stop it you are hurting the ones who will do things properly.
I guess you mean that instead of risking leaving an moved from object in an invariant breaking state. You would reconstruct the moved from object with its default constructor, even if most of the time it would not be necessary. If that was free, then sure. But default construction is just as expensive as copy construction. And that would defeat the whole performance purpose of moving instead of copying. Also if would mean that objects without default constructors couldnt be moved and would weaken the exception guarantees of move. The proposed solution is better though. If you have an invariant, you will have defined custom copy constructor, assignment operator and/or destructor to protect the invariant. If none of those have been defined the compiler can assume there is no invariant and do moves instead of copies. If there still was an implicit invariant, then the code was pretty much broken anyway. 
Second that. #5 and #7 are nonsense: http://en.wikipedia.org/wiki/Not_Invented_Here
&gt; But default construction is just as expensive as copy construction How come? most STL containers, for example, simply initialize their members to NULL/0 on their default constructor. &gt; Also if would mean that objects without default constructors couldnt be moved Yes, but it's very nice a trade off: the compiler will complain if there is no default constructor, but it can't check the invariants. &gt; and would weaken the exception guarantees of move. Actually, [it will keep them](http://cpp-next.com/archive/2010/10/implicit-move-must-go/comment-page-1/#comment-1019). &gt; The proposed solution is better though. If you have an invariant, you will have defined custom assignment operator or destructor to protect the invariant. If none of those have been defined the compiler can assume there is no invariant and moves instead of copies. If there still was an implicit invariant, then the code was pretty much broken anyway. Are there many classes that don't have a constructor or destructor? I don't think so. So for the majority of cases, perhaps over 90%, the move will have to be explicit. 
Having worked on several cross-platforms jobs, I think their ideas are mostly pretty good. They seem like they might advise against this, but if you are developing in C++, using the Boost libraries is a good idea, IMO. * It's very modular, so you can use only the parts you need. * They work hard at testing on all the major platforms. * Many of them are active on the C++ standards development, and a lot of their code is getting included in the standard C++ libraries. * They use magic to enable functional programming techniques for C++, like anonymous fuctions, functors, etc. 
Why not try to get your people to use Qt Creator? It's the first IDE I actually like and I am a vim user.
Ok, you are partly right on the first point that default construction might be much cheaper than a copy construction. I did not consider the cases where copy constructor has to allocate memory on the heap, while the default constructor doesnt. But a user defiend type might aswell do very expensive things in the default constructor. It might be just as expensive as a copy constructor. There is no way to know. Even in the best case default construction is not free. Move on the other hand is almost free. It would be a shame to slow it down. Unless I'm misstaken move should not throw, user defined default construction might throw (if allocating memory, or acuiring other resources). Making move do default allocation therefore would weaken the exception guarantees. But most importantly, there are lots of user defined types that dont have default constructors. We still want to be able to move these. I consider default construction a bit ugly in general. Often it implies a two stage initialization. The object has to have internal flags to check if its a proper object or if its just a dummy, and so might its users. Its better not to create an object before you can initialize it to its proper value. There are exceptions though.. like containers with dynamic size. Edit, I guess the main point though is. Moved from objects are dead (much like deleted pointers). They just haven't been buried yet (scoped out). One shouldn't waste time by trying to get the dead to look alive for their funeral (by default reconstructing). Some objects are so sick that they look dead (objects with invariants but without operators to protect the invariants). But its okay to kill them (break their invariants).
I'm not reinventing anything, my original point was google finds an acceptable answer for a c++ 101 course like the OP is in, why does he bother reddit with something that's googleable? I'm not saying write a new function from scratch but combine the things you've already learned and mix them together, exactly what he should be doing. 
What is the difference, exactly, between paying people to do your homework and getting them to do it for you for free (which is what was requested here), other than that the people doing the homework don't get fairly compensated for their time?
How so? If you're a friend of hers I probably have heard of you.
Why should I care about being anonymous on the internets if I'm not doing anything provocative? You found my facebook, great! Lets be friends, fellow redditor. (Note: I fully support the right to remain anonymous online, its just that some of the stuff I do online I actually want associated with myself)
Oddly enough, they don't confiscate your electronics when you lose your job.
Sorry, it was just not worth posting. He didn't really care, and he didn't even notice that I had used my real name until I approached him before class (and got a good laugh out of a lot of fellow redditors in class). I'm a physics major. Its a class in scientific and statistical computing, introducing C++ and Fortran 90. One of the reasons it is important to use the material from in class is the need for high efficiency on supercomputers. It just wasn't a big deal. I wouldn't of done it if I hadn't procrastinated until the night before it was due, but being a senior in physics, getting ready for graduate school, things are pretty hectic right now... so yeah...
When I ask "Why are you doing it that way?", I don't mean the question as a criticism - I am assuming you have a valid reason (or what you believe to be a valid reason) to approach the problem that way, and I want to know what it is so I can help you with *relevant* suggestions. And if it turns out that your reason doesn't constrain you in the way you think it does, I assume you would welcome that discovery, not get angry about it. 
&gt; Rule #3: Use standard ‘C’ types, not platform specific types But doesn't that kinda defeat the purpose of the typedefs in the first place? Presumably the platform-specific typedefs are there to protect your code against changes in the library. If you use the standard 'C' types and the library changes the underlying type, won't your code break? I seem to recall that POSIX did this in a few cases - the underlying definitions of some POSIX typedef's changed over the years.
To expand on this, I have my own set of libraries: * Cookie - Relativily small audio library that outputs using MMSYSTEM. Grouped audio, wav loading and 3D sounds. Working on streaming audio. * TinyImageLoader - It loads images. And that's it. Needed for a project where FreeImage was too bloated. * Papyrus - Logging library. Output to console, file, GUI, whatever. It's very nice, but perhaps a bit overkill. * Toolbox - Generic stuff. Stuff like math, vectors, matrices, colors (oh colors can get very complex), timer, thread class and input (with key definitions for Win32, SDL and GLUT). I've been porting my projects to 64-bits, and every single one of these damn libraries has been giving me trouble. In the end it was easier to rip out TinyImageLoader and use FreeImage innstead, for instance. But, I must say I disagree with some of his points. **Rule #4: Use only built in #ifdef compiler flags, do not invent your own** What? No! For God's sake use your own! You have *no* idea how shitty these flags get across shitty platforms. Here's some platform detection code: [YGPlatform.h](http://yoghurtgum.googlecode.com/svn/trunk/SDK/headers/YGPlatform.h) (Which reminds me, I have some tabs to turn into spaces. &gt;:\) **Rule #3: Use standard ‘C’ types, not platform specific types** Actually, I would say: "make new platform-independent types". Ogre does this. It doesn't use float's, it uses Ogre::Real's. Then, with a simple typedef, every floating point operation can be doubled or halved (theoretically) in precision. Secondly, pointers can be set to a fixed size, depending on platform and configuration. Some platforms have more useful typedefs than others, this way you can ensure they can all benefit. Here's how I do it in YoghurtGum: [YGSettings.h](http://yoghurtgum.googlecode.com/svn/trunk/SDK/headers/YGSettings.h)
Rule #10 is a little harsh. Is it really reasonable to expect every programmer to know all the little idiosyncrasies of each compiler you're building on?
Thirded that. Qt is da-bomb.
#4 is potentially a problem. For example #ifdef __WIN32 code around bug #endif What if the bug you are coding around gets removed in a later version of the compiler or a later version of the operating system? That is why autoconf style THIS_COMPILER_HAS_THIS_BUG is much better. Although everything else about autoconf sux.
&gt; Rule #3: Use standard ‘C’ types, not platform specific types I don't think the rule disallows typedefs. Its really asking not to use things like HRESULT, DWORD, sem_t, etc....
&gt; Rule #3: Use standard ‘C’ types, not platform specific types I don't think the rule disallows typedefs. Its really asking not to use things like HRESULT, DWORD, sem_t, etc....
When I was working on the Android build of my engine, I would code in Visual Studio on my laptop, commit it and compile it on my Linux desktop. :P
In #7 he is contradicting himself. He says that it is easier to learn 3 different GUI frameworks, than 1 GUI framework.
Patterns! We use patterns *everyfuckingwhere*. We can't spell facade but we do have a foofasade class that implements the same interface as Foo and only passes the request on. WTF man!
How big is the project? Did you maintain the makefiles / solutions manually? I have a codebase that spans multiple SVN repositories and has 8-12 different (VS) projects.
The project is an open source cross platform 2D engine for mobile devices in C++. All the codes are on [Google Code](http://code.google.com/p/yoghurtgum/). Yes, I did maintain the vcproj and makefile manually.
Good to hear you survived. It could've turned out much worse. Reminds me of when I took a graduate level algorithms class. We were allowed to use whatever resources were available. It was the first time I googled something and got 0 results.
Actually - I do welcome the ah-ha moment, when it occurs. But in my couple years of experience, the question is almost never asked in that spirit. Rather, it's something more along the lines of: "I can't *believe* your deep, deep ignorance - that you would even attempt to solve the problem in that way... just quit now because you are a mental midget and do not deserve the brilliance and perfection of Visual Studio".
A comment on the [preview price of his book](http://www.manning.com/williams/) (reachable from the posted link): $45, for something incomplete and, wot, $70 for it and the physical book when it comes out? For something that will cost, on Amazon, $23 or so the day it ships? (Judging by others). What are they smoking? 
&gt; But a user defiend type might aswell do very expensive things in the default constructor. It might be just as expensive as a copy constructor. There is no way to know Minimizing the dangers of moving is more important than the overhead few classes might have in their default constructor. Furthermore, a class that acquires some resource in its default constructor is wrongly designed: there is no way to communicate the error to the caller, since constructors must not throw and they also don't have return values. The most sane approach for default constructors is to simply initialize their state to null/0. &gt; Even in the best case default construction is not free. Move on the other hand is almost free. It would be a shame to slow it down. If you put the swap algorithm side by side with the move algorithm, you'll see that swapping is almost as efficient as moving, with the important property that it maintains invariants, if the class is coded in a sane way. &gt; user defined default construction might throw Which is not good design, because it might leave an object in a half-constructed state, since its destructor is not invoked. &gt; there are lots of user defined types that dont have default constructors. Clearly a design error. Not providing a default constructor also prohibits types from being used in statically allocated arrays. There are no cases that I know of that a class must absolutely not have a default constructor. 
Not funny
no u
What does STL do?
Just in case you're actually wondering: [Standard Template Library](http://en.wikipedia.org/wiki/Standard_Template_Library). Also, I highly recommend [this series of video lectures](http://blogs.msdn.com/b/vcblog/archive/2010/10/25/video-introduction-to-the-stl-parts-6-and-7.aspx) by Stephan T. Lavavej who works on the version of STL that ships with MSVC++. Parts 1 - 3 are especially great, and I have high hopes for parts 6 and 7, though I've not yet watched them.
Honestly, I didn't immediately recognize the acronym either... I always think of it as std :) Also, I wanted to point out that the lecturer you pointed out... his initials are STL o.O
A very important read for performance-critical programming. 
It's not STD anymore it's STI, infection, because majority are curable.
doing some more reading on EASTL I'm actually quite excited and may try using this in a future project : http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2271.html On some operations they have increased the performance over the native stl by 100% in some cases. Especially looking at the performance improvements on their gcc platforms (Mac especially), it makes a compelling argument to at least give it a try. And if they have found a decent way to debug stl containers that would be a godsend!
if you are looking for performance you can check out this cool stuff: http://msinilo.pl/blog/?p=675&amp;cpage=1 RDESTL is said to be even faster than EASTL, though it's not as complete. the hash_map implementation is the best, blows everything out there out of water..
Parts 1-3 are great, the guy definitely knows his stuff. I thought it was funny that his initials are STL, I'm assuming that's really his name. I saw parts 6-7 recently appear here, but I don't recall seeing 4 &amp; 5. Lots of stuff to catch up on....
My initial reaction reading n2271 is skepticism. Premature optimization is the root of all evil after all. Did they really use standard STL and boost libs to begin with and then by profiling find out that its performance was unacceptable? Was there no way around it? Was this a problem searching for a solution, or the other way around? Couldn't they just have extended the stl, and boost libraries (boost::pool, boost::shared_ptr, boost::unordered_map etc) instead of reinventing the wheel like this. This must have taken a huge amount of time. Was it really necessary? Maybe there was no way around the lack of alignment support, and the broken allocators. It would have been better though if this effort could have been channeled into improving the current boost libraries. Also i have some skepticism about the performance gain. Leading computer scientists have spent decades on optimizing STL. Is it really that easy for one man (or team?) to replace and improve? And now with the improvements in c++0x (alignment support, move constructors and rvalue references, etc), I dont think there will be much need for this lib. Just my 2 cents. 
&gt;Furthermore, a class that acquires some resource in its default constructor is wrongly designed: there is no way to communicate the error to the caller, since constructors must not throw and they also don't have return values. The most sane approach for default constructors is to simply initialize their state to null/0. I dont mean to offend you. But it seems you are not familiar with the most important and most fundamental idiom in C++. RAII. In short constructors are *The* place to throw. One might almost say that throwing anywhere else is bad design. You should acquire all resources in the constructor (containers are an exception). And obviously acquiring resources might throw. The point here is that the constructor will either succeed and the user will see a valid object from "day one". Or constructor will throw and from the users point of view the object never came alive and into scope at all. That means that no user code ever has to check if an object is valid. If the name is in scope it is valid. I suggest you read Scott Meyers, and Herb Sutters books for more info on RAII and exception safety. On the another point, you say that swap and move are about the same cost. Yes almost, but we are not even comparing that. We are comparing the cost of a move to the cost of a swap+default construction. The point is that default construction can be expensive. 
Your post really doesn't make much sense. STL isn't an implementation, it's part of the C++ specification. The STL itself is neither optimized or unoptimized and the specification only provides some upper bound guarantees about the time complexity. There are many existing implementations of the STL that address various requirements and use cases. Also saying leading computer scientists have spent decades on the STL is as absurd as saying you have decades worth of experience programming in .NET. The STL itself hasn't even been around for more than a single decade and its use was incredibly limited for a good chunk of time due to limitations in compilers. The various mainstream implementations include the original one developed by HP, there's SGI's implementation, there's the open source libstd++, there's Microsoft's implementation which is a heavily modified version of the original one from HP, there's STLPort and the list goes on and on. Each implementation of the STL aims to address a different issue, whether it's being open source, or being compatible with as many compilers as possible, or being easy to debug. EA is providing an implementation that is best suited for development on video game consoles, such as the XBox 360.
Okay. The current popular implementations of STL doe not have decades, but *man* decades, of effort behind them. This is not an implementation of the STL though, as its not compliant with the standard. It breaks it in many fundamental way. This is a replacement of the STL standard, and of any current implementations. Its widespread adoption would mean some serious fragmentation. I hold my position, that extending upon the standard and boost libraries would have been preferable. 
First of all, if constructors are allowed to throw, then what happens with already constructed objects? they are not destroyed. There is a huge source of leaks right there. Secondly, RAII does not necessarily mean that it's the default constructor that must acquire resources. I agree that it's a good technique to see a valid object from day one, but I disagree that this should be done from the default constructor. Furthermore, in practice, it's very rare for a constructor without parameters to acquire resources. In most cases that I've seen, acquiring resources requires parameters, and so the resources are acquired in constructors with parameters. Additionally, expensive default constructors do not allow me to build cheap aggregates of objects. Finally, failure to provide cheap default constructors goes against the language's mentality ("don't pay for what you don't use"). For example, without cheap default constructors, I cannot create a statically allocated array of objects. Regarding the swap, one my assumptions is that default constructors are cheap. In practice, this is valid. After having used all major libraries, 99% of classes provide a cheap default constructor. Even if default construction is expensive and cannot be avoided, it is much better to have less performance than to introduce bugs that are difficult to trace. Software engineers' time is way more costly than hardware. 
The need to use standard C types is in conflict with the need to use native GUIs factored out. On Windows you need to do the TCHAR/TEXT dance. If you want portability you'll need to create your own character handling macros that put TCHAR/TEXT in for Windows and do something that is actually sane elsewhere. If you want to target windows you must give consideration to this. You can convert each and every string to UTF-16 but that would be unnecessarily expensive.
It is not just you. MS compilers do that.
&gt;First of all, if constructors are allowed to throw, then what happens with already constructed objects? they are not destroyed. There is a huge source of leaks right there. The lifetime of an object is from the time its sucessfully constructed until it goes out of scope, and the destructor is automatically called. Its the destructors job to deallocate live objects. Destructors wont deallocate object that where never alive. Objects that throw in the constructor never came alive. Say you do two new's in the constructor. The first one suceeeds, second one does not. What you do is you catch the exception, you delete the first pointer, so that there will be no leaks. And then you rethrow the exception to signal your users that construction failed. That might sound complicated, and it is needlessly complicated because we are using pointers and are not following strict RAII. Its better to use a smart pointer, like autoptr. It has value semantics, and will scope out and deallocate any successfully created objects that would otherwize have to be dealloacted manually. No need to do anything. No catch, no throws, and no leaks. &gt;Furthermore, in practice, it's very rare for a constructor without parameters to acquire resources. In most cases that I've seen, acquiring resources requires parameters, and so the resources are acquired in constructors with parameters. I dont think its that rare for default constructors to be expensive. Some classes only have default constructors. Take a mutex wrapper. It acquires a mutex in the constructor, and releases it in the destructor. Even some containers do it. boost::array does not have any constructors except for its implicit default constructor that allocates all the storage. (The array size is a compile time template value.) &gt;Finally, failure to provide cheap default constructors goes against the language's mentality ("don't pay for what you don't use") You dont pay for what you dont use, sure. But you might as well pay now for what you must use later. As in, imagine that you have a ugly two stage construction. First a default constrcution and then waiting for some last parameter sent in to some init func. Its still better to do as much allocation work as possible in your constructor. You want any allocation failures to happen in the constructor and not later in a some func. Dont get too stuck up on containers. The wast majority of user defined classes are not containers. A lot of them will lack default constructors, and some of them will only have default constructors, and some will have both that have much the same cost.
&gt; The lifetime of an object is from the time its sucessfully constructed until it goes out of scope, and the destructor is automatically called. True, but it doesn't take the default constructor to achieve that. &gt; What you do is you catch the exception, you delete the first pointer, so that there will be no leaks You can't do that externally (i.e. outside of the constructor) if the member pointers are private, which is 99.99999% of all classes out there (if not all). You can't also do that internally if you use initialization lists. &gt; Its better to use a smart pointer, like autoptr. Since we are using smart pointers, then the default constructor can successfully initialize the pointers to null. &gt; Take a mutex wrapper. It acquires a mutex in the constructor, and releases it in the destructor. Nothing stops the class to provide a default constructor that doesn't take a parameter. &gt; boost::array does not have any constructors except for its implicit default constructor that allocates all the storage. (The array size is a compile time template value.) Then the class boost::array is not designed properly. If it allocates the data dynamically, then it should do it lazily. If it just contains the number of elements specified as a template parameter as members, then it already provides the appropriate default constructor, which is expensive, but that's by design. But it is suitable for moving, because it maintains the invariant. You are talking about few classes though. Perhaps 1% of all classes. &gt; But you might as well pay now for what you must use later. Why do that? it might be that what must use later may never come. &gt; As in, imagine that you have a ugly two stage construction. First a default constrcution and then waiting for some last parameter sent in to some init func. You don't have to do that, and that's wrong design. You provide the lazy initialization only when required, inside the class, or in a parametrized constructor. &gt; A lot of them will lack default constructors, and some of them will only have default constructors, and some will have both that have much the same cost. In which major library most classes don't have a default constructor? I don't know of such a library. Perhaps you do. 
It seems we are talking in circles here. So Ill stop. I recommend you read the Effective C++ and Exceptional C++ books. Many of these topics are better explained there.
They don't even bother to do the empty base class optimization for their container allocators... They even know about it and left a comment to "fixme" but they couldn't spend 5 minutes actually implementing it...
Do I have to agree with those books?
DAE read this as "Everything you've ever wanted to know about swapping genders"?
No, and the actual content of the article was far cooler. 
Well duh, C++&gt;Gender Swapping any day of the week.
I've read this site before. Optimisation is useless without covering profiling. Heck it doesn't even mention what profiling tools are available, let along how to use them. I could optimise in my sleep, but profiling can be tough.
So, while I agree with the upshot, this article could have stopped immediately at the -O2 stage. If you have to work to make your code not optimize to demonstrate a point, that point doesn't need demonstration.
I think you will find them interesting, and they bring clarity to lots of topics. They are best practice type books full with do's and dont's. Much about RAII and exception safety. They are probably the most recommend series of books in the C++ community. If you are serious about C++ you will come to read them sooner or later anyways. You dont have to agree with me. The books though are hard to disagree with.
&gt; Method A &gt; &gt; { // Limit the scope of t &gt; int t = x; &gt; x = y; &gt; y = t; &gt; } &gt; &gt; Method E &gt; &gt; std::swap(x, y); &gt; &gt; I'm inclined to call Method A the best because unlike Method E it explicitly shows you what it's doing I disagree. I don't care how it's done. std::swap makes it abundantly clear what's happening to x and y. It's concise, optimised, and general. Anybody can see immediately what std::swap(x,y) does, whereas you actually have to spend brain cycles parsing the 5-line solution in Method A.
What profiling tools do you use? I typically use a lightweight timing code to determine the execution time of specific functions, but have never used a dedicated profiler
It does mention profiling as part of the optimization life cycle in section two. It seems it's assuming you already know where the bottlenecks are and now you're trying to optimize. There are other places that talk a little more about profiling and the tools [[1]](http://en.wikibooks.org/wiki/Optimizing_Code_for_Speed#Profiling) [[2]](http://en.wikibooks.org/wiki/C%2B%2B_Programming/Optimization#Profiling), though there still are no examples, and it definitely needs work.
Oh, xixor, sarcasm is such a hard thing to convey in text. I'll try to send you towards a positive karma with my +1. 
Something smells wrong after I click that link. 
Yes - there are a couple of cursory mentions and a couple of links to wikipedia
valgrind is quite useful in combination with kcachegrind - the text output could be formatted better. gprof is good and easy to use. However it does not work well for multiple processes and threads. Intel's vtune is a really nice sampling profiler, but it is quite expensive.
You're getting voted down? I've done both, and I can tell you that this comparison is very hard to call...
Haha, I suppose the downvotes may be from people sensing a bit of homophobia, though I assure you that's not how it's meant. It's simply one of those geekisms&gt;everything else type things, haha. Oh well.
 __asm("xchg %1, %0" : "+r"(x), "+r"(y)); Are we done with this non-sense already?
I've also found that oprofile is quite good, once you've learned how to use it.
The funny thing is that a lot of time passed from when I read those books. Perhaps I need to read them again ;-). 
Maybe the downvotes are coming from the Anti-C++ factions of reddit. They might actually prefer to change their gender.
Look, I agree with the conclusion, but it's frustrating to continuously see assertions about "taking more CPU time" without any reference to CPU model or even family. The assertion is false on many architectures. That doesn't mean you should use the XOR trick of course.
I think the problem was how it was framed to begin with because, while it's common to swap two variables, you're usually doing it with variables whose values are not known at compile time. The function should not have been `main()` but: `void swaptest (int&amp; x, int&amp; y)` That way, it's not the result of the swap that matters, but that they were actually swapped. EDIT: Here are my results using `-O2`: Method A and E (identical, since `std::swap` gets inlined): movl (%edi), %edx movl (%esi), %eax movl %eax, (%edi) movl %edx, (%esi) Method B (using XOR): movl (%edi), %eax xorl (%esi), %eax movl %eax, (%edi) xorl (%esi), %eax movl %eax, (%esi) xorl %eax, (%edi) Method D (using add and subtract): movl (%edi), %eax addl (%esi), %eax movl %eax, (%edi) subl (%esi), %eax movl %eax, (%esi) subl %eax, (%edi)
Yes, also the first one is imperative (describes the steps but not the intent) and the second is declarative (describes the intent but not the steps). Declarative style is better as it means loose coupling between the programmer and the compiler. The programmer should tell the compiler what to do, not micromanage how. That way the compiler will be free to do things in the most efficient way. While the declarative style doesn't make make much difference in this case. It becomes extremely important as many core goes mainstream. For instance loops are inherently sequential and the compiler cant parallelize except in special cases. Using something like for_each instead and the compiler can do it in general. I think many core will be the end of application development in languages like C with a strong imperative style. Except maybe for small self contained single threaded apps.
Did you actually RTFA?
The whole article? Certainly not. The subject matter doesn't deserve such logorrhea. Spewing random asm dumps is no justification for wasting the time of your readers. If for some really weird reason you dearly care about such trivial problem, batch+vectorize+parallelize and forget about it. So, i'm doing others the service of answering the wrong question with the right answer in one line. 
So you didn't even Ctrl-F before being an ass… &gt; So now that we've beaten this code to death, I have one final tangent: there is an assembly instruction called XCHG (or EXCH, or something like that, depending on what architecture you're on) which swaps the values of two words. Unfortunately, it's generally a bad idea to use it because if you use it on a location in memory, the processor considers it a memory fence and messes up all sorts of pipelining and slows down all the code around it. So the only time you should consider using XCHG is if you want to swap two registers, and in that case, it's probably better just to remap the registers in the rest of the program (like in the assembly example I showed for the NontrivialFunction version), rather than explicitly swapping them.
That xchg up there being between registers is no memory barrier or serialization; again the right answer to the wrong question in one line. Beats reading pages after pages and being none the wiser. 
I understand your point, but it is really hard to demonstrate real-life complier output in a trivial sample. Compilers are just too damn smart these days and will optimize away your whole program if they notice it is not doing anything.
I think this is great- While I don't like the idea of fractured STL versions, by opensourcing this code the concepts &amp; code (license allowing) can be used to improve the other OpenSource implementations. I won't be switching to this anytime soon, but hopefully it will lead to speed improvements in mainstream STL Libraries which have a reputation for being slow.
You're right, but the problem goes slightly deeper... You shouldn't be writing a function to do this, even though a function is an excellent way to investigate what the compiler is doing. If you add the overhead of a function call for every swap, you're doing it wrong. Additionally, the number of instructions doesn't necessarily indicate the speed of execution. There's renaming and reordering to be taken into account. In the end, measuring real usage is the only way to know how to optimize correctly. That being said: The conclusion of the article had it right. Write your code to be read. Don't be unnecessarily clever.
You're absolutely right, but I feel the right way to approach this is to either compile into a library, or take a real program, and look at its performance and compiler output. That way, any focus on optimization can take into account the real characteristics of machine &amp; compiler behavior. When approaching performance below the level of algorithmic complexity, architecture becomes necessarily intertwined. Demonstrations explaining that can teach people both the more basic truths seen here and the more nuanced truths of the real world.
Great article, one of the best explanation on how to use TMP 
I agree. I have never used Haskell. But I caught myself staring at the same 3 Haskell lines for at least 5 minutes. Thinking, *Wow*. Its terseness is quite beautiful. The C++ TMP syntax is a bit ugly to the eye, but the functional concepts beneath are just as pretty.
Down for me. Here's the actual article: http://bartoszmilewski.wordpress.com/2009/10/21/what-does-haskell-have-to-do-with-c/ And here's the google cache of the linked page: &gt;C++Next Suggests: What Does Haskell Have To Do With C++? Here at C++Next, we’re big advocates of knowing many programming languages and many paradigms. Often, they intersect in interesting ways. Here is a choice post by Bartosz Milewski that shows the interesting overlap between C++ Template Metaprogramming and Functional Programming by comparing compile-time computation in C++ with Haskell, a purely functional programming language. &gt;What Does Haskell Have To Do With C++? &gt;This should be of particular interest to those who have followed David Sankel’s series on algebraic data types in C++, and my series on embedded domain-specific programming languages. &gt;Enjoy!
Who can deny breasts?!
The slamming on 'd' was appropriate. My thought about 'd' is typically "15+ years after c++ and this is all you can come up with?" It's a step forward, sideways and backwards (smart pointers beat out forced GC, I haven't seen issues with memory leaks in c++ code bases I've worked on for the past 5 years). I'm familar with lisp, ruby, IO but never looked into haskell. It seems interesting but the code I was looking at seemed to mock stupid template tricks. I guess I should look more into it.
yhe, but after 3 years in c++ you can look at STL implementation and understand it and then you see the beauty. wow geek and poke was right, Stockholm syndrome. 
Not sure I understand the reference. But code sure can be beautiful, if it is simple yet powerful, for much the same reasons mathematical proofs can be. I consider Haskell to be half way between programming and maths. Not sure about you but I got maths envy. I think the STL implementations are pretty straight forward compared to all the TMP magic going on in boost land.
I am an electrical engineer by degree, doing work as a C++ programmer. I would say that once you get past the beginning, you have to learn the caveats and pitfalls of C++ in practice. The best books for that are any of the ones that start with the words "Effective" and "Exceptional" (a few different books by Herb Sutter and Scott Meyers). Also, understanding how C++ got the way it did has been useful for me. Stroustrup's "Design and Evolution of C++" and Lippman's "Inside the C++ Object Model" are interesting books (although very detailed and tedious at points, but the overview is helpful). Personally, I'm fascinated by C++ template magic, and there are also some good books on that (like Alexandrescu's "Modern C++ Design" and Abrahams' "C++ Template Metaprogramming"). C++ is a big language. Enjoy it!
How about learning by doing? Start some simple/intermediate project, preferably something you actually want to do. You can only learn so much from books and tutorials alone. Also, if you have not yet, take a look at [Boost](http://boost.org), a collection of handy general purpose C++ libraries, augmenting the standard libraries.
Contains some pretty serious errors. Edited &gt; Define the destructor as virtual if and only if the class contains at least one other virtual member function. to &gt; Define the destructor as virtual if and only if the class contains at least one other virtual member function or if the class might be derived from. 
You could try getting a datastructures textbook in C++. The one I'm using in class called "Data Structures and algorithms in C++" by Adam Drozdek actually starts with a quick review of inheritance. I'm slogging through it in class but I probably only have about a third of the math background that you have (former Economics major). 
This. You can be knee-deep in books and tutorials all you want, but you aren't going to actually learn anything until you start writing programs.
Embedded programming. I know this is off-topic, and more C then C++, but as an electrical engineer you'll appreciate it. Get yourself a microcontroller kit, or try putting uClinux on a router.
http://geekandpoke.typepad.com/.a/6a00d8341d3df553ef0133f54ae12f970b-pi
Everyone learns in their own way I suppose. But I still strongly stress to anybody learning that they need to write, write, write. Books are great sources of information, but generally I've found that you won't "get it" until you actually do it.
On the other hand, thats one year of bad code that you wont write on your next project. Would you have been in a position to appreciate those books when you began the work? Or now that you've got a 3d engine?
Thank you. I have just ordered those books from the library. They look very good and I look forward to reading them!
Then how could this ever work: vec[5] = 7; It's the exact same member function (operator[] overload).
http://linuxtopia.org/online_books/programming_books/thinking_in_c++/index.html one of my favorite books. There are two books in the series, both great. And the bible: http://www.parashift.com/c++-faq-lite/index.html
A) C++ has its uses, but they are very specific. Think about switching to another language, unless you're working with one of the following: computer games, real-time audio or video, complex mathematical algorithms. Basically, if speed is your PRIMARY concern, choose C++ (or C). I don't have anything against C++, I quite like it, but I find working in C# or Java makes me a more efficient programmer. I build real-time audio effect processors (VST plugins) as a hobby, and I do it all in Java. It's quite fast. I've implemented the same algorithms in C++ and they run about 20% faster, but that 20% is not worth the extra 50% development time it takes to work in C++. Balance your requirements and see if C++ is a good fit. B) If you do stick with C++, realize that C++ on its own is completely powerless. there is no GUI, no streams apart from raw text or binary, no built-in methods for specific tasks of any kind. Learning C++ is about learning how to use libraries like Boost, OpenGL, MFC, ... (this can be hard to understand for people who first learned languages like Java, .NET or Python) PS: also an EE ;)
I agree with the other posters about the importance of both reading books and writing code. Personally I also like reading blogs, but most of the active C++ blogs I found are targeted towards more experienced C++ developers. You can find my beginner/intermediate blog at http://blog.knatten.org. I would also appreciate recommendations on any other intermediate level C++ blogs!
The returned object should be coded like this: template &lt;class K, class V&gt; class map_entry { public: map_entry&lt;K, V&gt; &amp;operator = (const V &amp;value) { owner_map-&gt;insert(make_pair(key, value)); } bool operator == (const V &amp;value) const { map&lt;K, V&gt;::iterator it = owner_map-&gt;find(key); if (it == owner_map-&gt;end()) throw std::logic_error(); return it-&gt;second == value; } private: map&lt;K, V&gt; *owner_map; const K key; }; 
I don't think anyone disagrees that reading books is good. But reading without writing programs will not improve your skills very much. You must implement to really "get it."
Actually, C++ is used more and more in embedded programming. I don't know if you can find a microcontroller with a C++ compiler -- but if you could, that would be great! Oh, and I agree that actual programming is the way to make you a better programmer. Don't stop reading books, but you need to practice actually writing code.
Ah, use a proxy object. Of course :) I definitely do not like the std::map interface; it's a bit cumbersome to use. Perhaps something like this would've made it much easier to use.
Showing example code (in 3 separate links, no less) for all 3 is hardly a comparison. How about some analysis or even just a superficial article describing the differences?
I also thought, that "example" would have been more appropriate than "comparison". These examples are useful for me, when I have to set up a multithreading program and I thought I'd share these.
You need to do something like this: #define private public #include &lt;header.hpp&gt; In C++, an implementation file is just another user of the header file, indistinguishable from a file just using the API. There is no sane, general way to relax vsibility checks for implementation files beyond what is provided by `friend`s. Some programming styles provide a way around this. For instance, the PIMPL (pointer to incomplete implementation) idiom allows you to make the actual implementation a `struct` with public fields.
As you seem to know, the right thing to do is to put it in the header file with the rest of the class declaration. However, given the constraints of your assignment I think the best answer is: just don't do it. I'm assuming that the teacher has provided the header files as an interface and your job is to provide an implementation. If the assignment is well thought out, you probably don't need to add anything to the class. Reconsider your basic strategy. But you don't provide many details about the assignment or the reason you want to add anything to the class, so maybe I've missed the boat. You might be able to accomplish what you want by declaring your own subclass that inherits from the class in the forbidden header file and making your additions in the subclass. Whether this will work depends on a lot of details you didn't provide. 
Is figuring this out specific to the assignment meaning it is part of the puzzle you are trying to figure out for the assignment? Or are you trying to do this a specific way on your own. 
If you're trying to circumvent a `private` declaration on a header file that was provided as part of a homework assignment, it sounds like you're going about the assignment the wrong way. That's a major reason why we use the `private` keyword: It's a hint to other programmers that, if you're trying to access something marked `private`, you're not using the class correctly. If you can accomplish what you want to do using the class's public methods, then you can write stand-alone functions that take an instance as an argument and do the work using the public API. If that doesn't work, you might need to give a bit more detail about the problem.
C++ does not support the concept of "categories" or "extension methods". However, it should be noted that most of the awesome implementations of these concepts from other languages would still bar you from accessing the instance variable that the other code insisted was private from its context: doing otherwise undermines the point of even implementing a "private" feature.
Never do this! Ever! People who write classes intended for inheritance use private for good reason. If you're doing stuff like this, you might as well just go and write in C; you've just broken one of the fundamental aspects of C++.
You can't. If you have a helper function that you'd like to write, write it as a regular-old function and pass in a reference to the member that you'd like to manipulate when you call it. You shouldn't really take this as a shortcoming of the language per se; in a real-world scenario, if you are accessing private members, then you'd be able to edit the header and add those private/protected helper functions. The contract of "don't edit the header file!" is purely something that the prof is enforcing upon you.
It sounds to me like the prof has given him a header file (or files) and said "now implement the class that this header file describes", in which case, accessing private/protected members is perfectly fine, since he is *implementing the class*. Perhaps I am misinterpreting, however.
&gt; #define private public &gt; #include &lt;header.hpp&gt; Wow, this is maybe the worst thing I've ever seen on the subject of C++. Emphatically do not do this.
This is not good practice, but if you have no other choice: in your .cpp file, create a method that takes whichever bits of the class' internal state you want to modify as references, then call that method from within the class. Something like: void baz(int&amp; x, int&amp; y) { x = x + y; } class Foo { int x; int y; public: void bar() { baz(x, y); } }; Again, that's how you might do it, but don't.
Whoa, you guys rock! Thank you so much for the informative replies, even those that fall under "never ever do this," it's a good learning opportunity for me. This homework's nothing special—part of it is implementing a linked list, which I can do in my sleep. I just wanted a helper method to return the memory address of the last item in the linked list, but knowing what I know now about C++'s conventions, it looks like it's not worth it. Thanks again, redditors, sincerely.
Oh yeah, the restriction is obvious. It occurs to me that I could indeed do a plain-old function like you described, except for the fact that the linked list (see my [previous comment](http://www.reddit.com/r/cpp/comments/e08jn/c_learning_question_how_to_write_an_instance/c1497zg)) uses a private struct for its list entries. Thanks for the ideas and insight regardless.
Subclassing wouldn't have been a bad idea either, but I have a feeling it would be beside the point of the assignment (le sigh). It looks like I'll just place this code inside the block it actually gets used in. This whole question stemmed from my usual attempts to keep my code DRY and such, but this instance, especially with the constraint of not being able to touch the header file, is clearly exceptional and irregular. Thank you for the help!
No, that's exactly right. I tend to break up my code into reusable bits as much as possible, though, so this question came up when I wrote a routine I thought I might use elsewhere. Looks like I'll just write it in one of the already-declared methods at this point.
Ah, that makes more sense. In that case, you've got two practical options without resorting to `friend` declarations: 1. Write it into the already declared methods that you're implementing. 2. Write static functions that take as arguments whatever private variables they need to know about, and call them as appropriate from your methods.
No, RDESTL simply costs too much memory, which makes it not suitable when memory is critical. It is not hard to trade memory for speed, but what we really need for a generic library is a balance. I would discourage to use the hash table implementation in RDESTL.
What a coincidence! I was just looking at the worst hack I've ever seen in the wxWidgets code. Macros, auto_ptr hacks, stdlib (not a good version either) on top of classes, on top of classes, ugh. If you need to use a malloc and a delete function, why wouldn't they just rewrite those instead of trying to combine namespaces? His comment made me laugh though: /* WARNING: the copy ctor and assignment operators change the passed in object even although it is declared as "const", so: a) it shouldn't be really const b) you shouldn't use it afterwards (or know that it was reset) This is very ugly but is unfortunately needed to make the normal use of classname buffer objects possible and is very similar to what std::auto_ptr&lt;&gt; does (as if it were an excuse...) */ */ because of the remark above, release() is declared const even if it isn't really const */ It is harder to paste code than I thought. Full link here: http://interreality.org/bzroot/voss5-wxwidgets/include/wx/buffer.h Somebody please explain to me what the hell is going on (specially @ line 114's macro) and why 5 compilers shit themselves (so far, except VC of course)? 
You could create a new class that was structurally the same as the one you're trying to access. You could then add whatever members you wanted. People do this all the time in game engines with things like Vector3 classes and such. A big game engine will often times have multiple implementations of a Vector3 because of integration with middleware, it's often useful to do something like: my::Vector3&amp; vector = (their::Vector3&amp;)a; 
foo.h: class MyFooPrivate; // No implementation yet class MyFoo { public: MyFoo(); private: friend MyFooPrivate; MyFooPrivate *d; // The "d-ptr" idiom }; foo.cpp: // Has implementations for private methods on MyFoo. Has full access to MyFoo // thanks to the "friend" declaration in the .h class MyFooPrivate { MyFoo *m_parent; public: MyFooPrivate(MyFoo *parent) : m_parent(parent) { } // Add methods here... }; MyFoo::MyFoo() : d(new MyFooPrivate(this)) { // Need a new private method? Add it to MyFooPrivate without changing the .h! }
Man, tell me if I'm way off-track, but aside from the fact that this isn't really "the C++ way", isn't that very close to what a closure's supposed to provide from an encapsulation perspective?
Haha, oh God no, a closure is something else completely. This is actually a well-known design pattern, known in KDE programs as the "D-pointer pattern" but known to the rest of the world as the Pimpl design pattern. It's probably better known in C even. Closures are a way of trapping local state in a function and then passing that function around. Even Java doesn't fully implement them to my knowledge, although I've learned not to put anything past C++ programmers with enough time... And don't make comments about things that are "The C++ Way" or not. One of the most famous programming books about C++, Scott Meyers's "Effective C++" is where most C++ programmers learn this technique from. C++ gives you the tools. Use them, or don't. Follow good coding conventions, but don't shy away from new coding conventions.
Gotcha. Thanks for the correction and thanks for the book to look up. That's what I get for having closures on the brain from reading [JavaScript: The Good Parts](http://oreilly.com/catalog/9780596517748).
This is also known as the "pimpl" idiom (pointer-to-implementation), check out Herb Sutter's old GOTW articles on http://gotw.ca/publications/index.htm.
Thanks for the Linuxtopia website, that's a hell of a site! A lot of great information.
Not even a single mention of MPI? ಠ_ಠ
Is it just me or everyone is getting a "500 internal server error" on any page of this site (cpp-next.com)
It gave me 500 - Internal Server Error.
I've reported the problem.
I've put a copy here while they work on the site: [http://boost-sandbox.sourceforge.net/libs/proto/doc/expressive-cpp/06-html/expressive-c-function-composition.htm](http://boost-sandbox.sourceforge.net/libs/proto/doc/expressive-cpp/06-html/expressive-c-function-composition.htm) 
WTF! Couldn't that giant macro just be implemented as a class template?
Lately I've been working with JavaScript for the GUI, which is not my cup of tea but hey "It's a living". What I do love about JavaScript (I'm guessing it's an attribute of a scripting language) is how easy it is to extend the language (JQuery and plugins, YUI, backbone and etc) which are not the same as libraries it's extension of the language it self. What I'm trying to say is that it seems that this attribute will come to C++ with the help of Boost. (Hopefully it'll be more ... readable but hey... A girl can only dream)
If u want balance just stay with standard STL. For Xbox game development memory might be scarce, but it's not true for 99% real world applications. Most systems are 64-bit and memory is so cheap nowadays, space is not what people care usually. Especially in finance industry speed is the only thing that matters. Some dumb ass even intentionally leak memory to save latency that freeing memory might cause, u got the idea...
That did not look like fun. Goes to show, you can do anything in C++, but it will require 8 different objects, a very good understanding of template rules, two hours of web research, and at least 600 lines of code. It will take several minutes to compile and if there is an error, the error message will at no point refer to the line of code that actually contains a mistake.
I think they wanted it to compile on a few compilers and not just VC++. And to maintain type-safety at runtime. I think it compiles correctly on about 5 compilers, lol. All that work for nothing. Boost plus native windowing APIs FTW.
"Finally, after dozens of lines of code (and many more hidden away in boost::result_of) we have matched what Haskell did in 5 lines. &lt;sigh&gt;"
Good stuff. Always learning something new with your posts, even though they hurt a bit. I will read it again, tomorrow, to try and get that recursion you did.
 read_reddit: if ( lang == "C++" ) printf("%s", canned_response); else goto read_reddit; 
I'm actually a 13-year C++ pro. The post just touched off my biggest gripes with the language. It's like using STL algorithms. Sure I could do that, but just iterating through the structure is so much shorter, easier to validate, and all the code is located in the same place. Writing one functor moves the real work code out of sight, and once you have to write two, you can't even see all of the real work at the same time. C++1x will help considerably, though it may be another 5 years before it is actually approved for use at work. 
Sure, it's not a good idea in C++. But taking a step back, it's really a bit odd that C++ ties visibility so directly to the class concept. It seems to be an inevitable result of the compilation model, but it does encourage programmers to put more implementation details into header files than necessary.
Indeed, you can't expect all your coworkers can use these code happily. But I feel fun at least, just like playing Lego.
Doesn't solve MPI a different problem tough? Cilk and OpenMP do parallelism using shared memory (good for current multi-core CPUs) whereas MPI does parallelism using communicating processes (good for CPU clusters, like those used in high performance computing).
Sure it does, but that's not to say that MPI can't be used on shared memory platforms too. You just don't get the advantage of sharing memory, but on the other hand, the communication overhead is entirely negligible. These days, clusters typically consist of interconnected multi-core CPUs, so you'd essentially need an OpenMP / MPI hybrid. This takes a lot of technical coding, which can be done quite simply by only using MPI and taking a slight memory hit.
You're correct. I just don't think that's what intel is after with Cilk, so they don't mention MPI here.
I guess you are compiling for 64bit: the array[] is decaying at the point of call to int\*. sizeof(int\*) in 64bit is 8. EDIT reddit eats asterisks... :) how do you escape it for display?
\\\* I belive.
OK, that works. Thx.
[My work here is done.](http://www.youtube.com/watch?v=6J03g8iS3PA#t=1m4s)
Arrays decay to pointers the first chance they get. In displayArray, the type of the variable "array" is int* (meaning pointer-to-int). in main, the type of "array" is int[3], meaning an array of ints 3 elements big. When you pass the array to displayArray, the array decays to a pointer, which is why the sizeof returns the size of a pointer.
Oh thank you I understand! Is there a way to circumvent that (ie get the size of an array from within a function)?
Thank you for your explanation!
Not really. :( You could use std::vector&lt;int&gt; instead. or you need to pass the size as another argument...
Oh ok... I guess this isn't much of a limitation anyway.
Never use sizeof() to find out the length of an array. Either use another variable to keep the size or use NULL to indicate the end of an array that you're looping through (that of course won't be a good idea for an integer array).
You can cheat with templates or macros if the call site has the actual array, but if you do that, do it ASAP and store the size.
As others have pointed out, this happens because the array decays to a pointer at any function call. However, there is a tidy way to deal with this: #include &lt;iostream&gt; namespace { template &lt;typename T, size_t n&gt; void displayArray(T (&amp;array)[n]) { std::cout &lt;&lt; "n: " &lt;&lt; n &lt;&lt; std::endl; for (size_t i = 0 ; i &lt; n; ++i) { std::cout &lt;&lt; array[i] &lt;&lt; std::endl; } } } int main() { int array[3] = {1, 2, 3}; displayArray(array); } Here the function is templated on the size of the array (and the type of the elements), and is passed by reference so the type information persists. In very complicated scenarios this isn't possible (due to some of the limitations of template type/value inference), and you have to fall back to using a `std::vector` or similar.
It is actually possible, please see [my post in this page](http://www.reddit.com/r/cpp/comments/e2lp3/could_somebody_please_explain_this_behavior_to_me/c14td73).
Yeah I never used that method anyway, I just found odd that it wasn't working.
It's funny you posted that, I was about to review templates (I haven't done C++ in a long time and need to relearn it for a new job). I'll definitely try it!
There is more than just templates going on here. For example, if the function was declared template &lt;typename T&gt; void displayArray(T (&amp;array)[2]) the program would fail to compile, as no match would be found which accepted an `int` array with 3 elements. The point is, C++ allows propagation of the length of arrays if you pass them by reference with this seemingly wacky type declaration. **Edited** to add further explanation.
I think using sizeof() to find the size of an array is just fine (using the idiomatic (sizeof(foo))/(sizeof(foo[0])) The problem is when the array has decayed into a pointer, for example across a function call. Something a lot of people don't realize is that the [] is essentially meaningless in a function parameter argument list, except for triggering the pointer decay. For example, in the function prototype, the OP wrote: void displayArray(int[], int); but he could have just as easily written: void displayArray(int[99], int); and there wouldn't have been so much as a warning, even though an array with only 3 elements was passed. Recommended reading: [Peter van der Linden's "Expert C Programming"](http://www.amazon.com/Expert-Programming-Peter-van-Linden/dp/0131774298)
frutiger - I'm not a C++ templates expert, so forgive me if this is a stupid question, but if you called that templated function 100 times with 100 different array sizes, wouldn't it result in 100 different instantiations of the code?
Seems like a good book, I'll get my hands on it ($10.90 used seems like a good deal!) Thanks
Yes, that is correct. Your program's text (or code) segment would be larger. But it saves yourself having to type out the different functions every time you wanted to use a new type/different length. The compiler only generates instantiations as you call them elsewhere in the code (i.e. it is lazy). Besides, C++ is a deep and complicated language. Usually, the phrase "no question is too stupid" is patronising, but I think it's very relevant with regards to C++. No one really knows all of it.
FWIW try using a vector&lt;int&gt; instead of an array and passing it by const ref. it would be much safer and more convinient
excellent question!
The terminology that everyone is using is "decaying", in that the array decayed into a pointer in the function call, and this is the **correct** terminology, but it becomes easier to understand if you instead consider what is happening here: What is the type of array in the parameter `int array[]` of the function? You didn't pass the parameter as `int array[3]`, but instead passed it as `int array[]`, which is similar in almost all ways to `int* array`. This is the actual reason: By passing the variable of type `int[3]` to a function looking for `int[]`, you performed a cast.
or even int*, int
This bug crops up from time to time. If you see a string truncated to four characters it's usually the same reason. Somone takes a chunk of code, refactors into a function, forgets to change sizeof, etc etc.
Well then the only time sizeof will work the way the op is expecting it to is either a) when the array is defined within the same function as the sizeof call or b) if the array is defined globally. Any time you pass the array to another function sizeof won't work and any time you create an array with a call to malloc sizeof won't work. In the cases where sizeof will work, wouldn't it be better just to use some sort of constant to store the length of the array? Using a constant certainly seems more readable than having to write "sizeof(array)/sizeof(array's datatype)"
Even with all the shit phoronix gets from Reddit, I think it still is an important, though not completely professional, resource to see where we stand from time to time.
Off the top of my head, here are some possible solutions: * make array_info a pointer to an array of info structs * use your current definition and overlay the struct onto your packet (C style arrays are inherently unsafe so there is no bounds checking done, though gcc/visual studio may have rudimentary bounds checking in Debug builds) The later should work I think (C style pseudocode): char* bytes = socket-&gt;Read(); struct packet_type\* packet = reinterpret_cast&lt;struct packet_type \*&gt;(bytes); packet-&gt;array_info[x] where x 0..n and n is: (size of packet - sizeof(int) * 2) / sizeof(struct info) 
Is the zero length array a requirement of the course? Or can you change the implementation and just don't want to? I guess that's a silly question... Is it meant to enforce that the packet type specific data immediately follow the packet "header"? Edit: Yeah, berium pretty much covered it. This little snippet looks more like c than c++ by the way.
Zero-sized array in a class type (which is, BTW, a non-standard extension) is just a convenient name for data. Its size is 0. In other words, sizeof (packet_type) will be the size of two int's. So to allocate an instance of this struct with some number of entries in array_info, you do the following: void* mem = operator new (sizeof (packet_type) + sizeof (info) * SIZE_I_WANT); packet_type* pt = static_cast&lt;packet_type*&gt; (mem); pt-&gt;num_info = SIZE_I_WANT; 
Is this C or C++? In C++ this would usually be wrapped in a class, but if you can't change the definition of the struct, use something like: int number_of_infos = 15; packet_type *p = reinterpret_cast&lt;packet_type*&gt;(new unsigned char[sizeof(packet_type) + (number_of_infos * sizeof(info))]); p-&gt;id = 5; p-&gt;num_info = number_of_infos; for (size_t i = 0; i &lt; number_of_infos; ++i) { strcpy( p-&gt;array_info[i].info_name, "cheese" ); } The important part is the line with the new statement. This allocates enough memory to hold both the header (id and num_info) and the variable length array in one block. The reinterpret_cast is there to tell the compiler to treat it as a packet_type pointer so you can store it is a variable of that type. When you are finished with the packet_type, remember to delete it with: delete[] p; If you are using C++ then you should definitely use a smart pointer to hold the pointer to avoid leaking in case of exceptions, etc. 
Ahh, that makes perfect sense. Thanks for your help! 
I would have declared it like so: struct packet_type { int id; int num_info; struct info* array_info; }; Then initialize it like so: struct packet_type* packet = new struct packet_type; packet-&gt;array_info = new struct info[SIZE_I_WANT]; packet-&gt;num_info = SIZE_I_WANT; Then later, when you're done: delete[] packet-&gt;array_info; delete packet; 
The downside of this is that when you send it out over the network you would then need to allocate another buffer of sizeof(packet_type) + sizeof(info[SIZE_I_WANT]) and then copy the data over whereas the method he is using he can just send the data straight through.
This is a C idiom, and a hacky one at that. You really shouldn't be doing this in C++.
By the way, arrays shouldn't be of zero length (although some compilers will allow it). If you care about that, use a length of one and compensate when you do the size calculation: size = sizeof(struct packet_type) - sizeof(info) + sizeof(info) * desired_length;
Seems your question has been answered already, and besides pointing out again that this is a pretty hackey thing to do, I noticed this: &gt; Thus, why i'm using structs instead of classes. Just as an FYI, a struct is essentially the same thing as a class in C++. The only difference is the default visibility of members - with structs they are public by default, and with classes they are private by default. So these two things are equivalent: struct S { int i; }; class C { public: int i; }; And these are also equivalent: struct S { private: int i; }; class C { int i; }; So it's mostly a style choice. You can place member functions, constructors, and even overloaded operators in a struct. I personally use struct when I have *just* data members, or when I'm making a function object with only an operator() method. Also, one last small piece of advice: If this is for a class project, they're teaching you wrong. Learn to use the standard library, and pick up a good book if you're really interested in C++. Typical crappy college programming courses - you'll spend years unlearning all of the bad stuff they taught you.
No. The problem here really is a deficiency in both C/C++ at expressing memory layout, which is best addressed using that zero-sized-array extension. There's a variable amount of data preceded by a header: this is a contiguous block, with precise alignment specifications; describing it any other way would be an error prone mess. Case in point most other posts got it wrong. 
As berium mentions, zero-sized arrays are a non-standard extension. An instructor would be correct to say `array_info[0]` is not valid C++.
When you say "without altering the definition of the struct" do you mean it's C++ representation or it's memory layout? If it's the latter, you can use templates: template &lt;int s&gt; struct packet_type { packet_type() : num_info(s) {} int id; int num_info; struct info array_info[s]; }; then: struct packet_type&lt;5&gt; packet; will allocate space for 5 "info" objects. If you really can't alter the struct you can always use the template to allocate your object then cast it to packet_type Note: This won't work if the number of objects is not known at compile time. 
Reading through the document, I think they hit the nail on the head with the allocators, and streams. The allocator implementation really does not give you much context and streams are usually fine, unless you do a lot of I/O. I tried parsing a huge text file using streams and it became so slow I had to go back to fscanf. I do have to say though that a lot of the functionality that they are talking about does not really require them to fork the STL. They could have just as easily donated the code to Boost, and made the code external to STL. 
&gt; In other words, sizeof (packet_type) will be the size of two int's. Not necessarily, if the word size is larger than sizeof(int) and you don't give instructions to tightly align the data, sizeof(packet_type) might be larger than 2*sizeof(int). Apart from that, you're right.
Another way to do this is to make it an array of size 1 and then have a separate header struct like so: struct packet_header { int id; int num_info; }; struct packet_type { struct packet_header header; struct info array_info[1]; }; Then you can use berium's code to allocate the block just replacing sizeof(packet_type) with sizeof(packet_header). The advantage of doing this is that you have just one continuous block of memory for all your data so it is much easier to delete or serialize or what have you. And I'm pretty sure this is standards compliant if you do it this way.
&gt; And I'm pretty sure this is standards compliant if you do it this way. No. Indexing an array beyond its last element makes you eligible for nasal daemons. 
 struct info { char info_name[32]; }; struct packet_type { int id; int num_info; struct info array_info[0]; }; template &lt;size_t SIZE&gt; union PacketType { packet_type pt; struct { int id; int num_info; struct info array_info[SIZE]; }; }; int main() { packet_type *pt = &amp;(new PacketType&lt;20&gt;())-&gt;pt; return 0; } 
The following works without changing the initial definition: template &lt;int s&gt; struct packet_type_allocator : packet_type { packet_type_allocator() { num_info = s; } struct info array_info[s]; }; static const int SIZE_I_WANT=20; packet_type_allocator&lt;SIZE_I_WANT&gt; packet; // on stack packet_type_allocator&lt;SIZE_I_WANT&gt; *ppacket = new packet_type_allocator&lt;SIZE_I_WANT&gt;; // on heap Again, for non const values of SIZE_I_WANT, you need to use other techniques described by others.
I would say that the Game Programming Gems series is an overlooked by non-game programmers source for great C++ programming design advice
watched it.. pretty good, I learned quite a bit
Are there any more videos by this guy? I like his style. Very fast talker, but very clear good explanations
I knew someone would pick on that ;-). That's why I program in C++ instead of Java...
Well, as I said in the comment on the blog, [auto-dependency generation](http://mad-scientist.net/make/autodep.html) is a way more useful.
Touching a source file just because a header that it includes has changed -- that's a new one.
If your professor is anything like mine, they'd give you credit for ingenuity, but attach a note saying "if you ever do this again, I'm giving you a zero".
Don't reinvent the wheel and use Boost or POCO libraries. 
The first book you need is [*Advanced Programming in the UNIX Environment*](http://www.amazon.com/Programming-Environment-Addison-Wesley-Professional-Computing/dp/0321525949/ref=sr_1_1?ie=UTF8&amp;qid=1289592368&amp;sr=8-1). This is the bible for socket programming (among other things). This book is based on C. A good resource is [*The Indispensable Guide to C*](http://www.amazon.com/Indispensible-Guide-Engineering-Applications/dp/0201624389/ref=sr_1_3?ie=UTF8&amp;qid=1289592581&amp;sr=8-3-spell). Hope this helps. 
http://www.linuxhowtos.org/C_C++/socket.htm 
part of some code I wrote this week for testing a wifi driver, pardon my lack of formatting im half copypasta #include &lt;unistd.h&gt; #include &lt;sys/socket.h&gt; #include &lt;stdio.h&gt; #include &lt;netdb.h&gt; #include &lt;stdlib.h&gt; #include &lt;errno.h&gt; int makeSocket(int port) // if port &gt; 0, socket = listening(server) socket { struct sockaddr_in sa; int sock = socket(AF_INET, SOCK_STREAM, 0); if(sock == -1) { fprintf("%s/0 OHSH-.\n", strerror(errno)); return -1; } sa.sin_family = AF_INET; sa.sin_port = (port &gt; 0) ? htons(port) : 0; // if this is a listening socket, pick the port to bind to, which is how servers work, otherwise just use the default allocated sa.sin_addr.s_addr = 0; if(bind(sock, (struct sockaddr*)&amp;sa, sizeof(sa)) &lt; 0) { close(sock); fprintf(stderr, "%s happens.\n", strerror(errno)); return -1; } if(port &gt; 0) { if(listen(sock, 5)) { close(sock); fprintf(stderr, "wtf? %s\n", strerror(errno)); return -1; } return sock; } doConnect(int sock, char *host, int port) { struct sockaddr_in sa; sa.sin_family = AF_INET; sa.sin_port = htons(port); sa.sin_addr.s_addr(inet_addr(host)); if(connect(sock, (struct sockaddr*)&amp;sa, sizeof(struct sockaddr_in)) { fprintf(stderr, "insert generic obscenity here %s.\n", strerror(errno)); return -1; } return 0; } This will get you started, the rest is much easier, almost file io like read and write calls. You'll need select in there, but the connect call is nearly trivial. 2 final things: 1. if you're actually doing this to learn and not just do your CS 221(i think) h/w tonight, I'll walk you through it, but honestly you can copy/pasta this into your work pretty easily and use it the rest of your life. Otherwise, I hope your professor is a redditor (Hi Mom!). 2. The main thing here is you have to create the socket (which is a file descriptor that can be io'd remotely), bind it, ie set the port and other flags in the sockaddr_in struct (the structure that defines inet socket parameters), and then either listen or connect. Also, select is important after this point, but that'll wait for the next lesson. You've taken your first step into a larger world... *edit oh my god the formatumanity! editted to try to fix that. *edit again, I would laugh like mad if this compiles.
Its a group project I'm working on this semester, not so much an assignment. We're just exploring sockets and the demo we would like to create is just a simple tcp/udp chatroom. It's turned out to be a bit more complicated than we anticipated seeing as how we don't have much C/C++ experience. Thank you for this :)
Yeah I've read over that and run the example code. It works fantastic, but all of their examples are C and not C++. I'm very new to this, is there a way to easily transition C to C++. From my understanding the libraries are pretty different.
Awesome thanks I hadn't heard of either. They seem to have some very useful modules for socket stuff.
90% of that code can be reused, its more a unix style thing than c. If its really c++, I'd create a class for client connections, and a class for the server socket, then just have each of the client sockets have the logic it needs to talk to the server and its fellow clients. Oh you need to read about the accept call also (man -s2 accept I believe), which spits out a client socket every time you call it. Just have the client class have a constructor that takes a socket as an argument, and new() one every time accept returns. It's not an easy thing to learn, but you need to understand the metaphor first, I'd look at pipes (man -s2 pipe), they're generally easier to understand, and a socket is just that with an end on each machine. As Einstein so famously said: &gt;"You see, wire telegraph is a kind of a very, very long cat. You pull his tail in New York and his head is meowing in Los Angeles. Do you understand this? And radio operates exactly the same way: you send signals here, they receive them there. The only difference is that there is no cat. " Truer words were never spoken.
http://beej.us/guide/bgnet/ This is a good start to at least understand socket concepts. The author writes in plain easy to understand english. What kind of issues are you having with C++ in general? 
Really it's just a lack of understanding the language. It's difficult to go from never having really used the language to developing a client/server chat program. So far the best /most understandable example i've seen is: http://www.linuxhowtos.org/C_C++/socket.htm But it's in C which doesn't help us out very much. Usually I do all C classwork on a unix machine with gcc available, but using visual studio is a new experience. 
As you probably know, C++ is very close to C. Some of the biggest differences are the use of new, classes and being able to declare variables after the beginning. Although, that is far from complete. If you ever need to get a lot deeper into C++ take a look at Dietel &amp; Dietel's C++: How to Program. It goes over C++ and the STL in lots of easy to understand examples. 
Thanks. This stuff does not hold your hand like python.
There's not really a native C++ interface to *nix sockets.
You should pick up a copy of "Accelerated C++" by Koenig and Moo since you already know how to code. And *don't* jump straight down the rabbit-hole of using Boost for everything. Parts of Boost are great, other parts are awful. Their socket stuff might be OK, I haven't checked.
Just to throw a helpful hint, use `new` as sparingly as possible. C++ object lifetime guarantees are very powerful. You'd be surprised at how much you can achieve using just stack values assuming you have encapsulated the correct thing into an object. In fact, I'd go so far as to say: "you know you have your encapsulations roughly correct if you have minimal `new`s and `delete`s." Of course, all rules can be bent, and even broken, given the right circumstances. (And if you use some kind of managed pointer, you should never have to `delete`).
Programming basic Berkeley-style sockets is so easy that for a simple app, it's not really worth it to learn external libraries. Boost is great for big, cross-platform projects, but is overkill for this. Sockets have been around so long that I find it hard to believe anyone would have problems finding complete examples, which are probably only a few screens of code.
[Accelerated C++](http://www.amazon.com/Accelerated-C-Practical-Programming-Example/dp/020170353X/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1289668397&amp;sr=8-1) is an excellent recommendation. I would suggest quite the same thing, learn the basics before jumping into Boost. Accelerated C++ offers good, in-depth explanations of what's going on in the code, but not too mind numbingly deep that you get bored or can't understand it. [Boost::Asio](http://www.boost.org/doc/libs/1_39_0/doc/html/boost_asio.html) is pretty good as far as a fairly mature socket and I/O library goes. 
Wow, this is horrifying.
Yeay, a synchronized queue of length=1. Congrats!!! :-)
via: http://baus.net/statically-linking-libstdc++-more.html
FYI there is a way to do this without any locking internally. The same concept for a non locking circular queue can be simplified for a 1-item queue (MVar as you say) :) Some info in this [SO thread](http://stackoverflow.com/questions/871234/circular-lock-free-buffer)
Thanks for the link. I'm not so proficient in using lock-free structures, so I cannot say right now, whether lock-free can benefit MVar (or a 1-item queue as you say) :-). I mostly wanted to show the abstraction. The implementation can probably be made more efficient.
I've seen the article in reddit Programing and I couldn't understand why is everybody so worked up... Synchronizing\Passing data between threads?
I realize that this might already have been answered (it says there are 11 comments, yet I only see one), but is this a limitation of the GNU implementation of C++? In other words, if I switch over to clang and friends, will this no longer plague me when I go to distribute applications to various Linux boxen? As it stands right now, I've just been packaging up the whole slew of shared objects (including stdc++, ld-linux, libc, etc) into a "libs/" directory and pointing to it via `-rpath` and `-dynamic-linker`, so I can, for example, copy my executable from Ubuntu 10.04 to Debian Lenny without worrying about whether or not a certain library of a certain version is installed. (It's ugly as hell, yes, and it nukes many of the advantages of dynamic linking, but wow does it save time. If anyone has a better solution, **please** let me know. :D)
A way to deal with this is: #include &lt;iostream&gt; // This function is so short that it will usually be inlined. template &lt;size_t n&gt; void displayArray(int (&amp;array)[n]) { displayArray(array, n); } // The function that does the real work only has one definition void displayArray(int array[], size_t n) { std::cout &lt;&lt; "n: " &lt;&lt; n &lt;&lt; std::endl; for (size_t i = 0 ; i &lt; n; ++i) { std::cout &lt;&lt; array[i] &lt;&lt; std::endl; } int main() { int array[3] = {1, 2, 3}; displayArray(array); } 
The 100 year language will be a cross of Haskell and C++!!!
Horizontal scroll bars for code snippets? Are you kidding me? At 64 characters wide? No thank you.
The bible for socket programming is [Unix Network Programming](http://www.amazon.com/UNIX-Network-Programming-Networking-Sockets/dp/013490012X/ref=sr_1_1?ie=UTF8&amp;qid=1290015292&amp;sr=8-1), also by stevens.
Somebody voted this down? Seriously? Richard Stallman, is that you?
Has anyone here tried this?
*Silent applause*
at last. Boost.Interprocess didn't work with C++ 0x
We don't use C++ because we like it. We use C++ because nothing else lets us get the job done. I've heard claims that a good question to ask language enthusiasts is "name ten large real world applications written in $blah". If people immediately start trying to give examples, the language isn't suitable for large real world applications. If people give you a "are you seriously asking that?" look, it probably is.
Moral of this story: give the "are you seriously asking that?" look regardless of whether ten large real world applications written in $blah actually exist. (Side note: it's interesting that Python is now actually in the "are you seriously asking that?" category.)
My primary language (though less so with my current systems focus) is C++, and there is no language that comes close to giving you the flexibility, scalability, and use-anywhere-for-anything ness of C++. If you do it right you can impl anything from device drivers (IoKit), embedded systems, web and media browsers, high performance codecs, all the way up to enterprise applications, though java is really more at home for that. Mostly, you get to pull in the infinity of c code for free, while still being able to object orient some abstractions to make life easier and more automagic. If they had more jazelle support for x86 (why tf don't they btw intel/amd? ppl don't want java accel?) java might make more sense, but the freedom to throw down from raw memory mapping and io all the way up to high level applications, with everywhere else in between is unmatched. It's C, but without all the work. The downside is, 90% of the programmers I've met abuse C++, because it's like heroin, it doesn't solve all your problems, but you pretend it does as long as you can... C++ deserves its own class(or 2) in CS, mostly "DONT DO THIS!!!" kind of warnings, because it is too easy to design unsupportable and unworkable structures in C++ when you get lazy.
and c++0x will make c++ a much ^2 nicer language. 
I'm a bit of a C++ hater (why am I subscribed to this subreddit you might ask). I mostly program in Java and Javascript. I've always felt that C is a better language than C++, that you if you are forced to use a C++ environment, just program in C. Maybe its because I had a heavy dose of the horrors of C++ when it was taught to me: I had to build a BigNum implementation that used operators, templates and friends. What the teacher did not explain to us is how to deal with the ridiculous compiler errors that g++ produced. What the does "ISO C++ forbids declaration of 'operator' with no type" mean? I eventually figured out the trick: arrange the function definitions in a silly and counter-intuitive way. None of this was obvious from what the teacher taught me or any information online; I just had to try different things until it worked. I was one of the few in my class who succeeded. I would like C++ a lot better if compilers could accept definitions in any order, like Java, or if they had reasonable and sensible error messages, like Java, or that the language specification was clear enough to be able to know why something is failing, like Java. Not saying that Java is all that, but at least it has a compiler that helps development rather than frustrates. Also, when I asked on IRC, what does "ISO C++ forbids declaration of 'operator' with no type" mean, people said "exactly what it sounds like".
I end up using c++ for two reasons: 1) available libraries and tools. 2) performance For reason 1, there's tons of c and c++ libraries out there that work well in c++. I don't need an FFI, i dont lose any of the api design in the translation, etc. For reason 2, c++ is probably the fastest there is. I program in ocaml, haskell, etc. and while it's easier to express algorithms in those languages, c++ really does have the fastest compilers out there right now. 
Well, this looks like a lot of fixes, finally. I like it.
Are you seriously asking that?
I tried to reproduce your error, and I managed this frutiger [0] ~ $ gcc -x c++ - class A { operator!() { return A(); } }; ^D &lt;stdin&gt;:2:12: error: ISO C++ forbids declaration of ‘operator!’ with no type This, on the other hand, compiled (and linked) fine: frutiger [0] ~ $ gcc -x c++ - class A { A operator!() { return A(); } }; int main() {} In C++, user-defined operators are very similar to functions -- they require return types.
It sounds like you're complaining because you misunderstood C++'s scoping rules, which for the most part, are exactly the same as C's scoping rules.
Well, to be fair, it is exactly what is sounds like.
I've seen countless people claim "you don't know C++, you only think you know C++" . what they mean is, because you're not a template generic programming super-wizard, you're somehow not a good programmer, and C++ is too much for you, and you should just stick with Visual Basic. What people need to realize is that you can use C++ just like you use Java, or C, and the code can be almost identical. C++ has everything, but nobody says you need to use all the tricks in the toolbag. Trying to be effective in every aspect of C++ is like learning 4 different languages at the same time. This creates frustration and a feeling of overcomplexity. Personally, I don't like to use operator overloading or templates except in the rare occasion that it will make my code considerably more readable (notice that I say readable, not faster!). I'm very capable of using these things if I need them, but they are uncomfortable to use (yes, they are) That said people trying to tell me to use C instead of C++ is a bit like telling someone to only use one hand to type the keyboard. You don't have to use the other hand if you don't want to, but it doesn't hurt to have it there if you need it. Don't chop it off. Write you code in C++ but use a C programming style, just like I prefer a Java programming style (yet I still work in C++)
C++ compilers do have a tendency to generate error messages that seem cryptic at first, but this is mitigated (to the point of being a non-issue in most cases) by actually having a working knowledge of the language. It is not the compiler's job to teach you the correct syntax for declaring an overloaded operator, for instance. 
C++ allows programmers to be more productive. That's what C++ has over C. The two languages are well known and offer very similar performance. Just using the basics of object oriented programming and bits of the standard library gives many people a better option than C. That's why the gaming industry, much of desktop software, the military and others use C++ . C still has a place, but there are good reasons people choose C++ . It's interesting to compare the number of C/C++/Objective-C applications against other languages. People are not silly, there are reasons why most desktop apps are in these languages. 
&gt; What the does "ISO C++ forbids declaration of 'operator' with no type" mean? When you mentioned errors, I thought we were going to be privy to a nice, ridiculously obscure template error. This one is pretty straight forward and so common that the first hit on an exact string match in google turned up a threat that exactly explains [the error, when it occurs, and the fix](http://gcc.gnu.org/ml/gcc-help/2009-01/msg00259.html).
You may not understand what this saying implies. Unless the person saying it is a complete jackass, it doesn't mean you're a bad programmer. It means you don't fully know C++. Fortunately, you don't need to know every part of C++ to be a good programmer if your projects don't utilize that aspect of c++.
This was a few years ago, I don't remember the details. I did search for that exact message and turned up nothing of help.
unless the operator is *for* another type. For instance struct X { operator const char*(void) { return "X!\n"; } }; int main(void) { std::cout &lt;&lt; std::string(X()); } (I don't recommend actually doing this however :) )
 I have to agree that gcc error messages _do_ suck. error: ISO C++ forbids declaration of ‘operator!’ with no type could easily be: error: ISO C++ forbids declaration of ‘operator!’ with no return type One word in the error message would have avoided your need to explain what they mean by "no type". 
The return type of a function is the type of the function, this is a common idiom. What other type could the error be referring to?
It's like how Feynman said &gt; "If you think you understand quantum mechanics, you don't understand quantum mechanics" It has nothing to do with you not being good enough and more to do with the sheer scope and complexity of what is trying to be understood. Very few people actually know *all* of C++ inside and out because that's really not necessary in most cases.
Try [Clang](http://clang.llvm.org/). The errors tend to be much more readable. It'll even do things like spell-correction, so if you call operateOnfoo, it'll suggest something like "unknown symbol operatoOnfoo; did you mean to call operateOnFoo"?
Thank you. I tried finding something like this at the time but couldn't.
If you're programming in a vacuum then sure what you say is true. When you're working on large systems with a team of people then you can no longer get away with just knowing some small subset of the language. You will have to work with third party libraries that do make use of obscure template metaprogramming wizardry, or some other library that uses exceptions or even worse exception specifications, or weird overloading. Each library will use some small subset of the language that they thought was reasonable and made sense, but when you combine all these libraries you get a mess and unless you really are knowledgeable in all aspects of the language, you may end up pulling your hair out.
I thought I knew C++, then I had a job interview last week for a C++ developer job. Turns out that I don't know it as well as I thought I did. Fucking virtual pointers and tables. Almost didn't get a second interview. I am now in the middle of reading "Effective C++" and there is sooo much that I didn't know or was just doing so inefficiently. Damn language is vast.
need to see real code!
It is vast. Interesting they are asking such questions. Well it *is* difficult to interview someone for a programming job. It's just so hard to tell if someone is going to be competent. Knowing how the language works is one level, trying to figure out if someone has an eye for design is a totally different level (which I haven't met a CS major yet who has this).
Microsoft Visual Cobal 2012
&gt;Alternatives that seem significantly better suited to the same jobs litter the landscape, and the obvious direct competitor — Objective-C — is in some ways the least of them. LOLWUT?
Common idiom in C++ != Common sense in human.
You left the .Net off the end. And the #.
No, it's a idiom in general for statically typed languages. Sure, there can be confusion for someone learning the language, but we're not talking about that. We're talking about how usable it is for someone that does know the language. And in that case it's quite obvious what the problem is. &gt;error: ISO C++ forbids declaration of ‘operator!’ with no type Let's pick this apart as someone who has some C++ experience, but has never encountered this particular error before. &gt;error: ISO C++ forbids You did something you're not allowed to. &gt;declaration of of ‘operator!’ You did something you're not allowed to where you declared the `!` operator. &gt;with no type You left out a type where one was expected where you declared the `!` operator. At this point, if you have even minimal C++ experience this should immediately narrow down the possible problem areas to a single line, namely: operator!() { So, we have a function declaration, and somewhere in there we need a type. There are only two places a type could possibly go in a function declaration, before the function name, or in the parameter list. At this point alarm bells should be going off in the head of anyone who has worked even a little bit with C++ functions because every function needs a return type and this function doesn't have one. Let's assume this doesn't happen and keep picking it apart. If we were missing a type in the parameter list the error would have been different, the compiler would have complained about not having an appropriately overloaded function and printed out the overloads it does know. It didn't do that, so it's not that. The only place left a type could go is in the spot for the return type, so the error must be talking about that. This is a very straightforward error which provides plenty of information to fix the problem. Saying 'return type' instead of 'type' does not further disambiguate things as there's nothing else it could be referring to. ------ Now the 50+ line error spews from something that you didn't write due to a template error can be quite confusing even for someone that does know the language.
More like Microsoft Visual ADD ONE TO COBOL GIVING COBOL 2012
&gt; c++ really does have the fastest compilers out there right now I assume you are talking about fastest code generating compilers because C++ compilers are actually incredibly slow.
So Qi is a text parser? I don't understand what problem he is solving.
I use C++ to stay humble.
I don't think thats a good metaphor. Quantum mechanics fundamentally does not make sense (we can not understand it directly with our senses, only indirectly through mathematics. We actually have to ignore our senses to understand it.) C++ on the other hand does make sense (even the template meta-programming), if you take the time to learn it. Its just that most of these "C++ haters" obviously never took the time to learn it. You have to know what something is in order to know what to hate about it. What I "hate" about C++ is mostly the C part. The decaying of arrays and functions into pointers is especially ugly (functions are not first class objects etc). Also I'd be happy if global or static state was forbidden or only allowed to be immutable (mutable static state does not scale to manycore). The syntax for functional style compile time programming is also quite verbose. Warts and all its still my favorite because of its many powerful abstractions. The boost libraries are quite amazing. 
As someone who uses C++ purely in the pursuit of a hobby, I find the hate to be a bit odd. Maybe it's that I just don't spend enough time with it to have gotten to know its nastier sections; maybe it's that I'm currently the only developer in the project, so I'm not having to deal with someone else's 20% of the language - though I'm pretty sure I'm using a lot more than 20%. Regardless, I've found it to be an extremely flexible language. To me, C++ just seemed like the best choice, since a) I wanted to target low-end PC's (my goal is to be able to play my games on a 1ghz AMD Duron w/256mb ram - I bought it in 2002, but it was a bit old even then) and b) it has an *amazing* wealth of libraries available.
Meta-commentary: getting rather tired of seeing that weird-lookin' guy's mug on every third link in this subreddit. It'd be different if he were bringing anything new to the table but I find his stuff generally uninteresting, link-bait pablum. If we could stop linking to this blog so often, that'd be great.
very interesting. would be nice to see some comments on this.
The improved performance is very nice, but I think the cleanliness/elegance of the solution is almost just as important. When exceptions are used correctly (for events that are "exceptional"), they realy are a very powerful tool.
Turning on exceptions does incur a very slight performance overhead, but it turns out to be something like one instruction per function call. Peanuts, basically. The slow part is *throwing exceptions*, but that should be rare enough not to matter.
Exceptions were *historically* slow on 32-bit Windows, where the calling conventions evolved before exceptions became popular and required setting up and tearing down a chain of exception handlers. 64-bit Windows doesn't have this problem and I wouldn't expect it on any other modern OS.
&gt; The logic is this: In order to handle return codes properly, you must check them all the time. When you're writing time sensitive code, checking an error code that isn't going to be set 99.9% of the time is a big fast waste of time. In fact, looking at any flag that isn't set 99.9% of the time is really stupid. I agree that exceptions are a better way of dealing with errors in C++ than return codes, but this is nonsense. Loop predicates will usually evaluate to true; is that stupid or inefficient? No, of course not. The more predictable a branch is, the _less_ costly it is on modern processors... and that's completely ignoring the fact that if you're checking a return code, you've already just incurred the cost of the function call itself (assuming it was not inlined). This makes even less sense since code that uses exceptions often has to, as some level, wrap C functions which return error codes, and upon checking those error codes throw an exception. 
I never understood the exception hate, but one dicey micro benchmark is not at all convincing for a performance comparison. More convincing is the blog's logic: use exceptions for exceptional things. That is, don't use them for the common case, and they won't then cause performance issues for you.
It is stupid to continuously check a loop predicate that you know isn't going change for a while. That's part of what loop unrolling is all about. You only check the loop predicate on an occasional basis. No need to check it a million times if you can avoid it using some other logic. To your second point. Either way, you're wrapping a C API with *some* method for handling errors. Exceptions are superior in both design and performance. Reality is that in terms of the code that the compiler creates, it looks more like how C programmers tend to write error handling than error codes, which is to have a goto's all over the place to handle some error and cleanup.
As fas as I'm concerned, knowing C++ is not about knowing the obscure things (template-metaprog, allocators, etc). It's about knowing how to use it effectively to have a simple and robust code. The key elements are the proper understanding of how to handle exceptional situations, and how to have a simple and yet robust memory management. In more technical words : it's about knowing RAII and the standard library. This is the first step to know C++. Then, eventually, there are all the OO key concepts (LSP, Demeter Law, DIP, OCP, etc), and also the C++ dichotomy regarding the value and the entity semantics. Template-metaprogramming ? new overloading ? Nah! It has nothing to do with the proper use of the language.
It's not even one instruction per function call. Exceptions were explicitly designed to be implementable with zero overhead in the non-throwing cases.
I think the C++ exception hate comes not from performance dings, but from the difficulty of writing exception-safe code. I've been parachuted into enough projects with resource leaks and crashes due to people not writing exception-safe code. Let's face it, there's a reason Sutter's books were written... P.S. RAII is your friend, but it doesn't cover every case -- there are, ahem, exceptions...
I know this isn't fair to say, but if I looked like that guy, I sure as hell wouldn't put my face on my blog. Then again, approximately 50% of people think Gizmodo's Jason Chen has Down Syndrome, so it could be worse.
It's not that he's necessarily bad looking, it's just that hideous teenager mustache... plus, what is he _wearing_? Is it like a micro-fleece scarf? or some kind of technical climbing jacket from REI? Whatever it is, why wouldn't you take it off for your little author bio photo? The mind boggles. So many bad decisions in 100 square pixels.
Effectively zero *speed* overhead, perhaps. The space overhead of all those jump tables is horrendous, though, which could make a difference if you're developing for a platform with limited memory rather than something like a desktop PC.
Care to elaborate some more?
People hate exceptions because they are glorified goto statements. Writing exception safe code is akin to writing structured code using only goto statements. Even the trivial "Hello, world!" is not exception safe. I agree they are more efficient than checking return values (for the exceptional case), but it's really a trade off for speed over correctness. 
loops and functions are just glorified goto statements too.
Then you're using them for logic, which they were never meant for.
[Accelerated C++](http://www.acceleratedcpp.com/) by Koenig &amp; Moo
I highly recommend this. I'm going through the exercises right now, and I'm learning stuff that I should have known from the beginning, without spending ages on language syntax like most newbie books. In my previous projects, I've been treating C++ as "Java with pointers" and not even using the stl.
Reading the contents, pretty much exactly what I'm looking for. Any other popular ones that I could pick up at a bookstore?
Sweet, thanks.
Yeah it's not much but, how much news does this subreddit get anyway?
Picking up Accelerated C++ tomorrow morning, yay!
Hrm "C++ Common Knowledge" or, "C++ Coding Standards" or, "Effective C++"? None of them is a beginner book though. But they're grouped in small sections, and mostly easier and more general than the "Exceptional C++ XX" series. But again they're for refinement of C++; the language itself is a bunch of raw material that, once you learn it, you have to learn how to /use/ it. "Effective C++" would be the first of this little list to get imo. I guess, knowing Java, it might be ok to 'jump ahead' like that. 
I also recommend [Effective C++, by Scott Meyers](http://www.pearsoned.co.uk/bookshop/detail.asp?affid=TRE&amp;t=199&amp;item=100000000105464). I think it's absolutely essential though I've bought it and not read it (yet). It's just a collection of tips to prevent errors and improve your program designs. It was suggested to me by someone on Stack Overflow when I kept asking questions about seemingly random memory errors and other crashes.
I'll hop on that once I finish Accelerated C++, thanks!
&gt; It was suggested to me by someone on Stack Overflow when I kept asking questions about seemingly random memory errors and other crashes. You're making a pretty good case right there for why *Effective C++* is absolutely essential.
Programming C/C++ is a bit weird compared to Java/Python...a good book you should pick up separate from your real "learning" book is "How not to program in C++" it's humorous, and helps you realize the things you might overlook. 
I used [this book](http://www.amazon.com/Primer-3rd-Stanley-B-Lippman/dp/0201824701) to learn C++ as my first programming language circa 1998, and for me it was a magical experience. I remember reading it front to back as I progressed through my semester, and at the time it defined for me what computer science and software engineering meant. It was like holding raw power in my hands. I still own it to this day. It's an odd book, written by C++ compiler writers; for people who haven't yet learned programming, but are otherwise deeply interested in system-level programming; who are starting to learn programming with C++, yet will some day *become* C++ compiler writers. :D You'll probably find it both conceptually below your level, and prosaically over your head. It's quite verbose, starting off with the absolute basics, yet quickly touches on a number of highly technical details that probably constitute Too Much Information for what you need. Yet conceptually it's quite pure, and if you *do* manage to read it front-to-back, you will end up with as solid an understanding of what it means to program in C++98 as anywhere available. Circa 2010, and not your first language, I'm not sure it will hold the same promise of mystery and discovery. Moreover, the world of programming languages has evolved a *lot* in the past decade. C++11 will be coming out next year, and is a huge improvement in the language -- the biggest in 13 years. Take a gander what is in store before either deciding to commit C++98 to memory -- or elsewise rejecting C++ as hopelessly dated and confusing.
Without any doubts [Programming -- Principles and Practice Using C++](http://www2.research.att.com/~bs/programming.html). It's a great way to get off on the right foot on modern C++. EDIT: Accellerated C++ is great but IMHO a bit old fashion.
You're right, I'd forgotten how frustrating those errors were. I've subtly edited my post.
Your mum.
The design and implementation of C++ (by Bjarne S) is good fun and can help you understand it's current state. Effective C++/STL/More C++
There is also the follow-up [More Effective C++](http://www.amazon.com/More-Effective-Improve-Programs-Designs/dp/020163371X/ref=sr_1_1?ie=UTF8&amp;qid=1290770981&amp;sr=8-1) which I highly recommend as well.
[Advanced C++ Programming Styles and Idioms](http://www.amazon.com/Advanced-C-Programming-Styles-Idioms/dp/0201548550) is pretty unconventional: the author assumes you are already familiar with programming concepts and C++ syntax, and presents a plethora of C++ idioms, at least half of which are... let's say *unconventional* applications of C++'s power of abstraction, that make you think outside the box. Amazon's synopsis and the reviewers there do a better job at describing it though, so give that a read. You should read an intermediate-level C++ book or two before diving into this one in order to truly appreciate its eccentricity (which should open your eyes considerably). It is a rather strange book: it reads more like a novel than a desktop reference.
C++0x makes it really easy to have message passing like in Objective C, so I decided to write an article on it. The base class is only 39 lines of code, compiled with gcc 4.5 with c++0x enabled. 
A few questions: 1) How does it handle inheritance? You say the maps are COW. Does that mean if a derived class overrides the method that's called when the prototype is matched, that the entire map is copied for all those derived instances just because of one overridden function? 2) Did you consider using this with the named parameters from Boost? 3) Is there some equivalent of @selector? 4) One of the things that makes Objective-C's messages flexible is that they don't have to be bound to any real method in the instance--which makes dealing with unknown types much easier. Do you provide some equivalent of respondsToSelector?
 &gt; How does it handle inheritance? No, the entire map is not copied. COW is applied to different instances only. Internally, a shared_ptr is used to the method map. If the shared_ptr is not unique, then the map is duplicated before adding the method. &gt; Did you consider using this with the named parameters from Boost? No, but it's a possible extension. &gt; Is there some equivalent of @selector? It is trivial to add it. &gt; Do you provide some equivalent of respondsToSelector? Again, it's trivial. Internally, methods are not bound to object instances, and therefore a method wrapper can be easily retrieved and passed around as a function object. 
the site requires me to log in to see the code -- at 39 lines, is there another place you could keep them? heck, even pastebin would work fine.
whats up with all the "val ()" in ATS? .. 
I'll post it here: #ifndef MP_OBJECT_HPP #define MP_OBJECT_HPP #include &lt;unordered_map&gt; #include &lt;memory&gt; #include &lt;stdexcept&gt; /** base class for classes that wish to have dynamic message passing ala Objective-C. */ class mp_object { public: /** the default constructor. */ mp_object() : m_method_map(new method_map) { } /** invoke method dynamically. @param proto prototype function. @param args arguments. @exception std::runtime_error if method is not supported. */ template &lt;class RET, class ...ARGS&gt; RET invoke(RET (*proto)(ARGS...), ARGS... args...) { //find method method_map::iterator it = m_method_map-&gt;find((void *)proto); //if found, invoke it if (it != m_method_map-&gt;end()) return static_cast&lt;typed_method_ref&lt;RET, ARGS...&gt; *&gt;(it-&gt;second.get())-&gt;invoke(this, args...); //throw exception throw std::runtime_error("unknown method"); } protected: /** adds a method to the method map. @param proto prototype function. @param method method to add. */ template &lt;class RET, class CL, class ...ARGS&gt; void add_method(RET (*proto)(ARGS...), RET (CL::*method)(ARGS...)) { //copy-on-write if (!m_method_map.unique()) m_method_map.reset(new method_map(*m_method_map.get())); //add method m_method_map-&gt;insert(std::make_pair((void *)proto, method_ref_ptr(new specific_method_ref&lt;RET, CL, ARGS...&gt;(method)))); } private: //method reference class method_ref { public: //destructor virtual ~method_ref() { } }; //typed method reference template &lt;class RET, class ...ARGS&gt; class typed_method_ref : public method_ref { public: //invoke method virtual RET invoke(mp_object *obj, ARGS... args...) const = 0; }; //specific method reference template &lt;class RET, class CL, class ...ARGS&gt; class specific_method_ref : public typed_method_ref&lt;RET, ARGS...&gt; { public: //constructor specific_method_ref(RET (CL::*method)(ARGS...)) : m_method(method) { } //invoke method virtual RET invoke(mp_object *obj, ARGS... args...) const { return (static_cast&lt;CL *&gt;(obj)-&gt;*m_method)(args...); } private: //method RET (CL::*m_method)(ARGS...); }; //shared ptr to method reference typedef std::shared_ptr&lt;method_ref&gt; method_ref_ptr; //method map type typedef std::unordered_map&lt;void *, method_ref_ptr&gt; method_map; //ptr to the method map type typedef std::shared_ptr&lt;method_map&gt; method_map_ptr; //the method map ptr method_map_ptr m_method_map; }; #endif //MP_OBJECT_HPP 
Was gonna post this. Great book!
Can you explain what problem you are solving with this code? The simple example doesn't seem very useful.
It provides message passing ala Objective C. In ObjC, a class does not have to implement a message during compile-time, as in C++. 
My boss swears by [The Annotated C++ Reference Manual](http://www.amazon.com/Annotated-C-Reference-Manual/dp/0201514591) and I'm sure it's great, coauthored by Margaret Ellis and Bjarne Stroustrup... but I'm guessing its not for teaching, as it's titled 'reference'
&gt; People hate exceptions... Then why have they grown popular with other languages such as Java, C#, Python, Ruby, Perl, OCaml, F#, PHP, Objective-C, D, Delphi, Free Pascal, Ada, Prolog, Javascript, and even Haskell? Seriously, the only really popular languages that don't use exceptions anymore are C and it's forerunners Fortran and Cobol. To be a good multi-language developer these days, you need to not be afraid of exceptions.
&gt; Loop predicates will usually evaluate to true; is that stupid or inefficient? No, of course not. The more predictable a branch is, the less costly it is on modern processors... Yes, but often fuctions are not called inside loops, thus there is not a good history for the CPU to be able to determine how predicable a certain branch may be. In these cases, the brach is not cheap.
Do you speak about static linking or dynamic linking? With static linking, you will have to recompile the code that uses the recompiled library anyway. With dynamic linking, the prototype functions will be exported, and so the dynamic linker will find them when the DLL is loaded. 
Oh I use exceptions, for exceptional cases. Without exceptions, your code has one, very explicit execution path. With them and you have two execution paths, the normal case, and the exceptional one. I've just seen the exceptions hype digress into using exceptions for return codes for everything. Every method in your code should not have to be enclosed from start to end in a giant try/catch, but I've seen just this pattern happen again and again. I know that's not how you use exceptions, but not everyone seems to get it. This is why I hate them. They do end up being used like glorified goto statements.
Can you clarify? It seems like you're saying that if a function is not called repeatedly, the CPU's branch predictor may not be in a reasonable state for any conditionals it contains... which is a moot point, since the kind of optimization we're talking about only concerns frequently executed code. It may be that you're referring to an implementation detail of branch prediction with which I'm not familiar, though. In that case, I'd love a more detailed explanation.
Take the following code for example: result = Foo(); if (result != 0) HandleFooError(); result = Bar(); if (result != 0) HandleBarError(); result = FooBar(); if (result != 0) HandleFooBarError(); result = YadaYadaYada(); if (result != 0) HandleYadaYadaYadaError(); The problem here is that each individual branch is not executed enough to stay in the CPU branch predictor cache. However, the total number of comparisons on the 'result' variable is quite considerable. Branch prediction excels at loops, but not that well with the above type of code.
&gt; but not everyone seems to get it. Well, there are a lot of bad programmers out there. From VB to C++ to Python and PHP, you'll always find people misusing features of a language. That shouldn't be a reason for us to hate a feature. (Otherwise, your job will always be miserable because you'll always find people misusing language features no matter what language you use.) If we encounter people misuing features where we work, then we educate them.
OK, And why is that better than just calling the functions?
"Programming -- Principles and Practice Using C++" by Stroustrup. Its the new way to learn C++. The old way was to learn ugly C first and then C++ and finally STL. As in you learn the least important and least useful things first and the most last. At best these introductory books have STL annexed in a small chapter at the end. That is crazy. STL should be ingrained in every single chapter from the very first "hello world" program. Because modern C++ is all about using STL. (Pointer arithmetic, casting, C arrays, switches, unions, etc. That C baggage is best not learnt at all.) We all learnt it that old backwards way, but you dont have too. Seriously, if I was going to do it all over again this book would be my first. 
The objective C way seems bad practice to me. Why would you not want the compiler to tell you if you try to invoke a method that does not exist or with the wrong arguments. Its not like its something you can choose to ignore. You have to deal with it one way or another. The code is quite nifty though with its variadic templates and cast to fro void *pointers.. some comments though (unless I'm missing something): -it seems to me class method_ref and typed_method_ref are unnecessary middlemen to specific_method_ref. If they where removed invoke method could be made none virtual. -Also couldn't the whole method_map_ptr be made static as all objects of the same class will share the same method addresses anyways, they really don't need per object method_map's (kind of like how different objects of the same class can share the same vtable). The this pointer will differentiate the objects when calling.. -It seems to me there is no sharing of method_map's between instances (even with the shared_pointers). All instances of the same class have their own newed methodmap's, filled with the same addresses. 
There's already a 0.8 release available too. You can get it from http://github.com/cpp-netlib/cpp-netlib/downloads -- the announcement is here too: http://cplusplus-soup.com/2010/11/21/the-c-network-library-v0-8-out-now/
In theory you could extend a class at runtime with message passing (i.e. bind new methods to it depending on some condition), create delegates and forwarders, etc. This is not possible with native function calls, as all member functions have to be visible to the compiler. This was the reasoning behind all the questions I asked the author in my other comment -- just as a proof-of-concept for message passing, this sort of misses the point of using messages in Objective-C in the first place. 
&gt; The objective C way seems bad practice to me. Why would you not want the compiler to tell you if you try to invoke a method that does not exist or with the wrong arguments. Its not like its something you can choose to ignore. You have to deal with it one way or another. Actually, Objective-C will warn you if a message is unrecognized for the object you're sending it to (based on what messages are visible from the local TU and any headers that are included). But then, that's one of the benefits of using a language where message-passing is built into the compiler--you get compile-time safety-checking. Here the burden of safety-checking is shifted onto the programmer at run-time.
 Sadly, I suspected it was Borland before I clicked the link. 
Downvoted because I'm sick of seeing this blog in this subreddit. gst, you're doing most of the submissions; seems like every post he makes ends up here. Please scale it back a bit.
You don't need your classes to inherit from a specific class in order to use two different classes in the same context. 
&gt; In theory you could extend a class at runtime with message passing (i.e. bind new methods to it depending on some condition), create delegates and forwarders, etc. This is not possible with native function calls, as all member functions have to be visible to the compiler. The most important advantage is to have a dynamic type system. You don't have to have complex class hierarchies. As long as classes implement the messages you want, you are good to go. &gt; this sort of misses the point of using messages in Objective-C in the first place. I addressed all your points. 
&gt; Actually, Objective-C will warn you if a message is unrecognized for the object you're sending it to (based on what messages are visible from the local TU and any headers that are included). But then, that's one of the benefits of using a language where message-passing is built into the compiler--you get compile-time safety-checking. Here the burden of safety-checking is shifted onto the programmer at run-time. It only tells you if the message you're sending is visible from the point you are sending it, which is often wrong. In C++, if you want to be sure the underlying class supports the message, then all you have to do is cast the object to the appropriate class and call the message directly. If the class does not support the message, then the code will not compile. This can be encapsulated to a macro: #define TEST_MESSAGE(OBJ, CLASS, MSG) if (0) static_cast&lt;CLASS *&gt;(OBJ)-&gt;MSG You can use it like this: TEST_MESSAGE(obj, rect, draw)(); The code that will be created is: if (0) static_cast&lt;rect *&gt;(obj)-&gt;draw(); If the class 'rect' does not implement 'draw', then the compiler will not compile the code. The code is inactive, since it will be never executed. 
Better than mine, at least. Mine is around 2000 lines of C, contains quite a few bugs, and attempts garbage collection currently. I'm working on it, though! :-)
Based on my own experience: * [Programming -- Principles and Practice Using C++](http://www2.research.att.com/~bs/programming.html) * [The C++ Programming Language](http://www.amazon.com/C-Programming-Language-Special/dp/0201700735) * [The two "Effective C++ Programming" from Mayers](http://www.pearsoned.co.uk/bookshop/detail.asp?affid=TRE&amp;t=199&amp;item=100000000105464) Next, tackle on STL specific stuff
Well, did you count the lines after the first 90?
The problem is serializing pointers, references, vtables and basically anything with an address. Do you also serialize the memory on the heap? If you just naively serialize and deserialize those your objects will no longer work when you put it together again in another process. And most object like std::string etc contain pointers. If you want to pass objects between processes CORBA is the way to go.
He's misunderstanding Greenspun's Law. You don't have to do anything special, it just happens.
Lisp really is magic. I guess all those braces distort the space-time continuum just enough to make things work.
Meh: http://www.ioccc.org/1989/jar.2.c
Cheeky: &gt; ,**O**;}**B**(**F**)**U** **S**(**C**(**A**(**T**(**E**)?**D**:_)));}**L(i,s)p** &gt; "cons"P t,"t","t"};B(S)int **Li,s;p** u;if(
&gt; Compared with Lis.py my effort is incomplete, inefficient and leaks memory. Other than *that*, Mrs. Lincoln... how was the play?
Right, but you admitted that none of my points were implemented (yet). That means your system is not a useful replacement for Objective-C, because all of the key parts that make Objective-C worth using are still missing.
No, you can't do any compile-time checking in this system in a safe and guaranteed way. Period. A class may have a function with the same name as a message defined, without actually adding a message for it. It may also assign a message to a function whose symbol isn't the same as the globally visible function. Neither of which can be checked. For example this code will compile when it isn't safe and not compile when it is: void foo() {} void bar() {} struct A : mp_object { A() { add_method(::foo, &amp;A::bar); } void bar() { /* whatever */ } }; int main() { A a; TEST_MESSAGE(a, A, foo); // fails to compile, even though the message 'foo' is defined and calling it with invoke() would succeed TEST_MESSAGE(a, A, bar); // compiles, even though the message 'bar' is not defined, and therefore invoke() will fail at runtime }
Certainly. And doing memset(1, 0, 0xFFFFFFFF) will crash the process (in either language). You don't obey the rules of the library, and then you complain it doesn't do what you want. You also say: &gt; fails to compile, even though the message 'foo' is defined and calling it with invoke() would succeed Which is not true: invoke(foo) will throw an exception. &gt; compiles, even though the message 'bar' is not defined, and therefore invoke() will fail at runtime You are supposed to bind methods and messages of the same name. 
It was a proof of concept. Your points can easily be implemented. Is there a need for such a thing in the world of C++? I don't think there is. 
&gt; it seems to me class method_ref and typed_method_ref are unnecessary middlemen to specific_method_ref. If they where removed invoke method could be made none virtual. The type of the target class needs to be known in order to cast mp_object to that class, in order to invoke the method. &gt; Also couldn't the whole method_map_ptr be made static as all objects of the same class will share the same method addresses anyways, they really don't need per object method_map's (kind of like how different objects of the same class can share the same vtable) If you make the method map static, then every object derived from mp_object will have the same methods. &gt; It seems to me there is no sharing of method_map's between instances (even with the shared_pointers). All instances of the same class have their own newed methodmap's, filled with the same addresses. If you create a copy of an object via a copy constructor, it will have the same methods as the source object. 
Beautiful...
Thinking in C++ by Bruce Eckel. Also nice that is available for free on his site. I find he really gets in to the cracks of C++ better than any other books. 
Oki. I see. I skimmed the code and thought mp_object class was templated, and not the invoke method itself. That way every class would get its own type of mp_object. I wonder if you did it that way you could get around the virtual method by downcasting using the "Curiously recurring template pattern". http://en.wikipedia.org/wiki/Curiously_recurring_template_pattern#Static_polymorphism Cheers
&gt; I wonder if you did it that way you could get around the virtual method by downcasting using the "Curiously recurring template pattern" I don't think so. Using this pattern would not allow derived classes of derived classes of mp_object to add methods to the object. However, if the code used functions instead of methods, and the first argument of the function was an mp_object-derived instance, and if mp_object was the first inherited class, mp_object could be passed to the function, saving the virtual call...
Yes you are right derived (of derived) parts of the object would not share the same mp_object as the basepart so that would not work. Also forgot that even if base class mp_object class was templated with type of derived class to allow the upcasting. The invoke method would still be templated much the same on method and arguments like before. Cheers
So how is this useful? I'm not being pedantic but I just don't see the application.
Old fashioned in what way?
It allows for more flexible code with less rigid hierarchy class. 
This book. Read a chapter, make a program using what you just read. Repeat.
Interesting concept, but I'm still trying to decide if all that extra mess gets you any really all that practical in the end.
This is weak. Using Metatemplate programming I can have factory method that construct objects based on number of parameters needed in the constructor. http://stackoverflow.com/questions/4209151/compiler-cannot-deduce-the-specialised-template-in-c
Best thumbnail ever.
Neat, my main complaint is that the types have to be exact, not just convertible to the correct type. I don't know if this is how objective-c message passing also works, but it's a big difference from how normal function calling works in C++.
pretty effectively captures what drives me nuts about boost. You just want some simple http library, and next thing you know you are compiling in Spirit and tons of templates and yadda yadda yadda, and all to use what is probably the equivalent of a few hundred lines of C code.
RAII is usually implemented as a C++ class with cleanup code in the destructor. The object sits on the stack as a local variable, and when it goes out of scope, the cleanup code runs. I'm not sure if it makes much sense in plain C, because C doesn't have destructors.
I'm neither a C++ programmer nor do I understand RAII myself very clearly, but I remembered this snippet from the [C++ FAQ](http://www.parashift.com/c++-faq-lite/big-picture.html#faq-6.18) which might help your understanding a tiny bit: &gt; If you dissect the words of the RAII acronym (Resource Acquisition Is Initialization), you will think RAII is about acquiring resources during initialization. However the power of RAII comes not from tying acquisition to initialization, but from tying reclamation to destruction. A more precise acronym might be RRID (Resource Reclamation Is Destruction), perhaps DIRR (Destruction Is Resource Reclamation). Also from [here](http://www.parashift.com/c++-faq-lite/exceptions.html#faq-17.6) in the FAQ: &gt; In Java, non-memory resources are reclaimed via explicit try/finally blocks. When this mindset is used in C++, it results in a large number of unnecessary try blocks, which, compared with RAII, clutters the code and makes the logic harder to follow. Essentially the code swaps back and forth between the "good path" and the "bad path" (the latter meaning the path taken during an exception). With RAII, the code is mostly optimistic — it's all the "good path," and the cleanup code is buried in destructors of the resource-owning objects. This also helps reduce the cost of code reviews and unit-testing, since these "resource-owning objects" can be validated in isolation (with explicit try/catch blocks, each copy must be unit-tested and inspected individually; they cannot be handled as a group).
I found that the discussion page for the Wikipedia article on RAII contained a lot of useful information.
Yep, but the real power comes in using it expressively. Here is a way to ensure that a lock gets automatically released: class AutoUnlocker { LockType&amp; l; public: AutoUnlocker(LockType&amp; l_) : l(l_) { l.Acquire(); } ~AutoUnlocker() { l.Release(); } } void DoWork() { AutoUnlocker lock(global_lock); // do work } But you can also use it to expressively show scoped, critical-region actions: void DoOtherWork() { // do some setup work { AutoUnlocker lock(global_lock); // do fast, critical-region work } // do some more non-critical region work { AutoUnlocker lock(global_lock); // do some other critical-region work } } _Edit_ oh yeah, don't forget destruction order: void DoYetMoreWork() { AutoUnlocker lock(global_lock); MethodThatCrashes(); } bool DoEvenMoreWork() { AutoUnlocker lock(global_lock); if(/* error case */) { return false; // or throw } return true; } Voila, global_lock is still released.
This. Acquiring Resources at Initialization is good in that it lets you avoid code like foomgr *foo = malloc(sizeof(foomgr)); int rc = foomgr_init(foo, a, b, c); if (rc != 0) { /* probably */ goto cleanup1; /* or whatever */ } in favor of foomgr f(a, b, c); /* exception is thrown on error */ /* now I can just use it */ but the real power is cleaning up resources automatically when they fall out of scope: not having to think about _every possible thing in your function that can go wrong_, and in turn not having to write error handling code for every one of those possible errors.
Here is an example of how to do resource allocation in C vs C++. Look how easy it is for users to create cppmain vs cmain. The C way: struct CLibType { Handle a_; }; int init(CLibType* mt); //must be called for all allocated CLibType's. Might fail. int deinit(CLibType* mt); //must be called for CLibType's that where sucessfully initialized int cmain () //Error prone. Your user can easily use CLibType wrong. { CLibType * pval1, * pval2, * pval3; int val1Init, val2Init, val3Init; pval1 = NULL; pval2 = NULL; pval3 = NULL; pval1 = (CLibType*) malloc (sizeof(CLibType)); val1Init = init(pval1); if(val1Init == NO) goto cleanup; pval2 = (CLibType*) malloc (sizeof(CLibType)); val2Init = init(pval2); if(val1Init == NO) goto cleanup; pval3 = (CLibType*) malloc (sizeof(CLibType)); val3Init = init(pval3); if(val1Init == NO) goto cleanup; //use pval1 //use pval2 //use pval3 cleanup: if(pval1 != NULL) { if(val1Init == YES) { deinit(pval1); } free(pval1); } if(pval2 != NULL) { if(val2Init == YES) { deinit(pval2); } free(pval2); } if(pval1 != NULL) { if(val1Init == YES) { deinit(pval1); } free(pval1); } return 1; } That was ugly, verbose and error prone. Here is the C++ way. We wrap the unsafe CLibType struct in a RAII struct which makes cppmain fool proof and easy to use: struct CLibType { Handle a_; }; int init(CLibType* mt); //must be called for all allocated CLibType's. Might fail. int deinit(CLibType* mt); //must be called for CLibType's that where sucessfully initialized struct MyRAIITypeWrapper { CLibType* pmt; MyRAIITypeWrapper() { pmt = new CLibType; if(init(pmt) == NO) { delete(pmt); throw "init failed"; } } ~MyRAIITypeWrapper() { deinit(pmt); delete(pmt); } }; int cppmain () // Fool prof. Your user cant use CLibTypeWrapper wrong. { CLibTypeWrapper pval1; //init for pval1 might throw, if it does pval1 is deallocated up, and we exit cppmain CLibTypeWrapper pval2; //init for pval2 might throw, if it does pval2 is deallocated, and pval1 is deinited and deallocated, and we exit cppmain CLibTypeWrapper pval3; //init for pval3 might throw, if it does pval3 is deallocated, and pval2 is deinited and deallocated, pval1 is deinited and deallocated, and we exit cppmain //use pval1 //use pval2 //use pval3 return 1; // pval3 is deinited and deallocated, and pval2 is deinited and deallocated, pval1 is deinited and deallocated, and we exit cppmain } 
RAII means: * when an object is created, it acquires a resource. * when the object is destroyed, it releases the resource. 
If you have to dispatch 50-60 different objects of the same tree.. this is a lot better than makeA() const; makeB() const; makeC() const; makeD() const; makeE() const; makeF() const; makeG() const; makeH() const; makeA() const; makeB() const; 
- It also means that if an object fails to accuire a resource in its constructor it should signal this by throwing an exception. - Also ideally it means that a constructor should only construct other RAII objects. Otherwize things get messy when a construction fails after sucessfully creating some resources but not all. In that case the constructor has to manually tear down any sucessfully created non-RAII-objects (as the destructor will not be called for objects whose constructor failed).
I think the important concept here for why this is important for a C guy is exceptions. In the old days I was also a C firmware guy. When doing this sort of stuff we would just acquire the lock at the top, do stuff, and then release the lock at the bottom. Straightforward and easy. C++ made throwing exceptions and unwinding to a catch block a central part of the language. It made certain things much easier and other things much much harder. You could no longer look at a simple section of code and logically follow it through without constantly wondering, "hey, what would happen if this function call threw an exception for some reason". That's the ultimate reason why this RAII stuff is helpful because the deallocation of resources happens automatically and irrespective of if your code block finishes normally or if an exception causes it to terminate unexpectedly...
RAII is totally dependent on constructors and destructors: knowing that these functions are going to be called during allocation and deallocation, even when the variable is a local variable on the stack. I can't imagine how you would pull off something like that in C, especially for local variables. Because you have these member functions--one guaranteed to run at creation, and one at destruction--it is easy to make the lifetimes of resources match the lifetime of the object. And it works regardless of whether your object is on the stack or the heap, no matter what sort of exception, early return, continue, or goto the code encounters. After using it a while, you realize it is much easier to manage heap memory this way than to ensure every new has a delete in every execution path. You also find that auto_ptr is there to give you a way to put a heap allocation on the stack without having to write your own class with RAII just to manage one pointer.
 OK. I had gotten the local struct sitting on the stack part since after the function returns the stack frame is destroyed which automatically deallocates the struct for you (provided the struct doesn't have pointers to other structs that need freeing inside it). But I guess the part I was missing is that the destructors are automatically called? Maybe that's where all the setjmp/longjmp voodoo comes in. I can see how this would be somewhat more verbose in C, but I am not sure why it would be more error prone. The examples below done with an explicit malloc could have just as easily been done with a local struct sitting on the stack and an init() that takes a pointer to that struct type to operate on it, yes? Also, isn't it normally the destructor's job to contain any and all clean-up code associated with destroying an object? I was under the impression that was the normal way to do things in C++, so would you guys be able to tell me when it would be beneficial NOT to do it that way? Thanks, guys!
&gt; the destructors are automatically called? Correct. &gt; I am not sure why it would be more error prone. If your struct contains stuff that needs to be explicitly free'd when the struct goes away, C has no mechanism for handling that, other than requiring the programmer to do it. Consequently, if you initialize such a struct in C, and then _for whatever reason_ exit the scope of that struct without calling the appropriate foo_destroy() (or whatever) function, you've got an error. &gt; isn't it normally the destructor's job to contain any and all clean-up code associated with destroying an object? Yes; anything the struct contains that isn't automatically cleaned up when it falls out of scope. In practice, this generally means pointers. &gt; when it would be beneficial NOT to do it that way? I can't think of a case when you wouldn't want to have your destructor completely clean up the struct. Maybe if the nature of your design stipulates that ownership of one of the struct's members is passed outside of the struct? I dunno, that seems like bad design. So to answer your question, almost certainly never.
There is a common variant to RAII that still works great: * when an object is created, it's data is initialized so that it knows it doesn't have a resource yet. * During some other member function call, the resource is acquired. Since this is outside of the constructor, you do have to make sure you do the 'right thing' if it is called twice. * In the destructor, check if it has a resource, if so release it.
This is a bit of an oversimplification, but RAII basically means, in C terminology, you are guaranteed a constructor and destructor call for every malloc() and free() that are called. Obviously it's tied to objects but with some creativity with structs you can get a similar system in. After that you can just subvert the regular malloc/free calls with certain ld flags (going off of old memory here so i might be slightly off). And in doing that you can add whatever you need to see if you can positively identify the struct being created/destroyed and add in a call to its constructor/destructor via function pointers. I did a quick google on overriding malloc et al and got this, which sounds about right: http://stackoverflow.com/questions/929893/how-can-i-override-malloc-calloc-free-etc-under-os-x 
Do you know how stack allocated variables in C work? All variables in a C block are stack based variables (ie. local variables in each function). When the block of code is no longer in scope, the stack is popped, and the allocated stack memory consumed by the stack is released. A stack is essentially a memory buffer, and a pointer to the next free slot, and as you enter/exit blocks of code, the pointer moves up/down. With RAII, the C++ runtime guarantees that when a object is created (memory allocated), a constructor function is called by the runtime. Likewise, when the object is no longer in scope (either when exiting a block of code or calling delete), the C++ runtime will automatically call a destructor (without the need for the programmer to do this themselves). So with RAII, you essentially get a free call to a constructor, and a free call to the destructor. Essentially, anything you allocate/create/acquire in the constructor, you release/destroy/free in the destructor. The C++ runtime will call the code for you. Best of all, if the object is created on the stack, the destructor will automatically be called without you having to call delete when the object is no longer in scope. An example: class FileIO { private: FILE *f; public: FileIO() : f(0) {} ~FileIO() {if (f) fclose(f);} bool Open(char *filename) {f = fopen(filename, ...); return (f?true:false;} }; main() { FileIO aFile; if (aFile.Open("somefile.ext")) { // do something } // dont have to call close, since when aFile goes out of scope, the destructor gets called automatically. } 
malloc and free are not the whole answer...that is more of GC thing...RAII != GC. A better concise definition is that you can with RAII, when you hit certain points of a function, guarantee that cleanup code blocks get called. For example, when you fopen a file, you can ensure that fclose is called regardless of how you return from the function. In C++ object the cleanup code is paired with the setup for the object. You can emulate this in C with goto but it's verbose, tedious and error prone. RAII in C is can be also arguably emulated with massive macro hacks but without compiler support is probably not a good idea, and I've never seen a truly usable implementation. RAII is one of the truly useful things C++ brought to the table...I do mostly C now and it's one of the two things I really miss (the other being the STL).
RAII is really necessary in C++ because of the exceptions. In C you might ensure that the code goes through the function, but in C++ unhandled exceptions will force the execution to leave the function so that it won't reach to the code where deallocation happens. For example in C you can do something like below: void foo() { int* p = allocate_some_memory(); if (some_function_returns_error() == 1) goto end; do_something(); end: delllocate_the_memory(p); } But in an exceptional case in C++: void foo() { int* p = allocate(); some_function_throws_exception(); do_something(); deallocate(p); } will leak in case the function throws an exception. Either you should put try {} catch {} blocks everywhere just to release memory (which may become cumbersome), or better you can use: void foo() { std::auto_ptr&lt;int&gt; p(allocate()); some_function_throws_exception(); // In case of exception p is deallocated automatically. do_something(); }
RAII is also fairly exception-safe, which I don't think is something you can achieve in C. Correct me if I'm wrong, but IIRC, C uses "vector exceptions", i.e. you tell the OS to call a specific function whenever there is an exception. This function decides whether the program continues as normal or crashes out. This is virtually useless for RAII as the exception-handling function has no idea about the contents of the stack.
Nobody has mentioned smart pointers, although I don't have the greatest c++ skills from what I have read I thought a lot of times raii is used with non stack allocated variables being held in smart pointers which do slow but predictable memory management via ref counting, so that you can keep a resource as long as something needs it(is pointing to it) and you don't have to worry about ownership that much since (assuming raii was setup properly) the resource object will have its destructor called immediately when you no longer need the resource.
Exceptions are setjmp/longjmp voodoo. When a C++ exception occurs, it magically calls the destructors for the objects that will be removed from the stack to reach the 'setjmp' location. RAII is a technique that uses that voodoo. Why is it less error prone? In C, this would look something like this: if (setjmp(&amp;s)==0) do_stuff(); else cleanup(); void do_stuff() { a = malloc(sizeof(a)); cleanup_add( free, a ); b = lock_mutex(m); cleanup_add( unlock_mutex, m ); function_that_might_call_longjmp(); unlock_mutex(m); cleanup_remove( unlock_mutex, m ); free(a); cleanup_remove( free, a ); } Basically, if you longjmp, you need a way for cleanup() to know what to do. You'd need something similar to close files and network connections or to release any other type of resource you've acquired. In C++ stack unwinding calls object destructors. This makes it relatively easy to achieve the results above without having to explicitly call the cleanup_remove(). In fact, you don't even need to explicitly call methods to free or unlock the resources unless you need the resources returned before the function returns. 
Using smart pointers is one way to use RAII without reimplementing the wheel. &gt; ... which do slow but predictable memory management There's nothing slow about smart pointers. 
I think this is "two-phase construction". I prefer this to RAII which always got me into trouble. 
Thats the ugly two stage initialization. Best avoided if at all possible. Not all objects have valid null state. (There are exceptions to the rule like containers). Some of the problems with two stage initialization is: -Users can forget to call the init function.. but still try and use the object as if it had been initialized. -When users use the object, they must check if the object is in a constructed state, or in a constructed and initialized state. -If the Init method throws, the object still exists. And can be used by users (if someone catches the exception). 
I mean that reference counting memory management has more time overhead then other forms of gc(although the fact that in c++ some stuff can be put on the stack avoiding gc may change the equation).
If you have a design background and less than a year of personal programming experience, there's hardly any chance of you landing a job in this field any time soon. What you're better off doing is leveraging your design degree to work as maybe a UI designer or web designer, and then gradually work more and more towards programming positions.
* You're honestly totally unqualified to be writing commercial software, I would be weary of companies willing to hire you to do production work. * For what type of position, because you're not going to get a job working on their high performance infrastructure. Inexperience and high performance are mutually exclusive. * I would say more than a semester or maybe two worth of introductory programming. * See above, but the companies doing what you would like to do are the usual suspects. That may have come across harsh, but I think a good dose of reality does the mind right. If you want my honest opinion, one of the better routes for people with your background to build a career is through part-time open source work. At the investment firm I work at we always keep our eyes on developer and open source mailing lists, scouting you might say and we do actively recruit that way.
Work for free. Sorry, but spending a few weeks just working out there is the only way to really get started if you aren't straight out of college. I'd try craigslist, one of their "i had this idea last night for a website right after i finished my bag of weed, all i need is a few god programmers, im sure it can't be hard, its just a computers" guys, preferably near you, and just do whatever he says for 3 weeks. If anything comes out, you can throw it on your resume, if not if you part on good terms you have a reference, otherwise you just try again. Rinse, repeat, and try to move your way up the ladder to paying jobs, or an internship, then finally a real job. The key here is to take small jobs, so you can pump up your work experience in different ways, and generally try to slipstream your way into the real job market. Otherwise just internships at large companies, or try applying for entry level contract jobs with an outsourcing firm (takes luck I guess). Also, working on open source projects helps a TON. I know all these don't pay, but you can't get a paying job in this field without doing a bunch of free work, but then, you know that, you're in design...
Thanks for your feedback. I understand I'm starting from scratch here, and unqualified for a lot of jobs out there. I'm trying to figure out how to cut the most efficient path to start programming professionally. And how to best allocate my time/resources. I'm finishing up a semester and to be honest, I think I could have been much further along if I studied on my own, started working on some open source projects and personal projects as you mention. 
Thanks. This was helpful - I guess gradually building up a portfolio of work seems to be the best way. I also don't mind unpaid jobs/contracts, at least starting out. I imagine it would be easier to get a 2nd bachelors in CS or bridge into a MS CS program, but I'm limited by financial resources. How large of a variance is there (in terms of job prospects, pay, etc) between university educated and self-taught developers? I imagine after a certain point, it becomes all about an individual's contribution and output, would that be accurate?
Self-taught here. Depends on your company. I'm the only one at my place, all the app guys are CS and all the systems guys are EE, and I do some of both. The cultures are different, and everyone has their own biases, personally, self-taught can be just as stupid as CS, or slow as EE programmers, its just a question of whether the person has talent and a decent work-ethic, but everyone has their own opinion. Honestly, know what you're talking about, read the algorithm design manual by... some guy, and understand parts of it, and do some stuff and you should be fine. Oh, and make some stuff you can show people. Half the interview is luck, more than half is personality. Don't stress too much. The variance in pay and job prospects is minimal based on degree, but honestly it varies WIDELY based on corporation and corporation size. Systems generally pays better than application, so if you can do systems (generally lots harder in some ways), good on you. OTOH, its easier to get jobs in web dev, and your background makes that far easier. Also, consider making a website for a local charity, or some other organization, if you decide to go this path. For web dev's you're looking to create a portfolio much like designers. Also, if you're in the design side, talk to your friends about sending jobs your way, that's a big advantage. There's more, but it really depends on which way you choose to go, each has it's own characteristics. You don't have to choose now, but I'd start with web dev if I just wanted to get in the game, and move on to either app/games or systems depending on where you find jobs. The slope of output vs. education is very steep, education almost doesn't matter if you have decent experience.
&gt; I'm hoping to learn about the most pragmatic way to go about getting my first programming gig. It's difficult (or maybe you get lucky). &gt; What do you suggest is the best way to get my foot in the door? Get a programming job. Any programing job where you write code. This is not to form you as a programmer but to put some experience on your CV. &gt; I'm pretty autodidactic and can spend the next few years teaching myself the foundations of CS. That's a good and a bad thing. It's good because you will put effort in learning stuff. It's bad because when you learn by yourself you tend to look on things until you understand them enough to put them in a program, not until you actually know them (front to back). &gt; What types of companies should I be looking at, that a) deal with high performance issues You probably shouldn't look at companies that deal with performance issues until you have something to offer them. At the moment, you do not. I remember seeing an ad for working at Google a few years back. It basically said "if you're fascinated with low-level OS development, AI, implementing distributed file systems" and so on, give us a call. Most programmers (as in 90+% are not familiar with any of those areas). Being familiar with them doesn't mean you undestand the code. It means you understand _why_ it was written as it was, what are the advantages and disadvantages of writing it that way and so on (on top of generic programming experience and understanding). What you should have for a company that looks for HP programmers: - solid _programming experience_ (you get this by working elsewhere). **This will take at least a year.** - solid _understanding of algorithmic efficiency_ (you get this by studying algorithm classes and the "big O notation"). You have to understand this to such a degree that if somebody asks you to prove mathematically that an algorithm is optimal you understand what that means and how the demonstration should go. I would start this by studying sorting algorithms: get the algorithm idea, make a reference implementation by yourself (without looking at other sources), then look at public reference implementation, then analyze and deduce their space and time complexity requirements. **This will take less than a year** (it was a year in my university course, but that was four hours per week or so). _I've had questions on that in three positions I've worked with._ - solid _experience with algorithm design and implementation_. You get this by implementing algorithms and structures from scratch, in a language supporting pointers (C or C++ would be best). Out of the top of my head: implement some data structures (linked lists, queues, matrices, some trees and graphs), implement some algorithms (searching, sorting, graph traversal, matrix arithmetic and naive and decomposed matrices multiplication, etc). **This will take at least a year** but you can combine it with the others. _This also comes up in interviews a lot_ - solid _experience with creating programs that solve real problems_. You get this by solving programming problems for yourself and others. You will probably get the most use out of a high-level language (I use python myself but others will do). To do this, set problems/questions for yourself and solve them programatically ("can I write my own file search program?" "Can I improve it?" "Can I write a program that reports HDD occupied space by file type?" etc). Once you have these under your belt, you're in the business of competing with others who (on top of these) have experience competing in the HP field. On top of these, you should also touch on some other fields as well that are not necessarily programming, but that often go together: some formal programming methodologies, working with SQL/regular expressions, set up and use some consistent coding conventions, using source control and, get used to debug your own (and other's) code - you will get more than you want of that when you work in a project. &gt; and b) are willing to hire someone with my background/lack of experience? (big vs. small, telecom vs internet, etc) (I'm in NY if that helps) I'm not familiar with NY, but I think you should go for an internship or no experience/trial position (with the market level in NY I'm not even sure there _are_ any "no xp required" positions). Anything that shows as "programming experience" on your CV. You could also go for interviews with teams who practice pair programming (extreme programming).
Okay, this is my personnal story, i think sharing it could cheer up OP. 4 years ago, i graduated from an engineering school (I am french, so it's pretty much a big fucking deal) in physics, especially in optics and photonics. I took one C++ class, mostly because I was interested and better than most students (i got the equivalent of A+, probably got my diploma because of it). Soon after my diploma, I landed a job in a small company, mostly because they needed someone with a physics background (to understand what the customer needs) and the ability to learn CS fast. It turns out I was both. For one year I learnt some BS (C#.NET, a little bit of java, network administration), but i also learnt good practices. You can think of me a tech support guy on steroids. I was the guy the customer would call a 8am the monday morning, just to scream because his fucking system was down and needed some one to fix asap, as he was losing 100 000€ a day. Most of this job was boring but there were two big advantages. First, I learned programming, and second, I met a ton of people. Eventually, I learnt that one of those people was looking for a PhD candidate. I sent my resume and eventually, I was taken. Today, i'm doing a PhD in robotics (computer vision, HPC on a side note), I met awesome people (as in Stroustrup awesome) who teached me pretty much everything I know, I now can explain to my GF what C++ TMP is, how a compiler works, and why P!=NP matters. I won't work at google, but I sure know that I can land a pretty good job in CS, computer vision of HPC in one year (I was already contacted by some automated trading companies, and some other stuff that can pay for code). So, I think you can do it, but you will need patience, willpower and an ass ton of luck. My advice would be to land a job that fits what you can do right now and will allow you to switch to CS at some point (I read someone suggested web design or UI design, sounds like a plan to me). Working on Open source project could be a really good entry point as well (OpenOffice could use a UI designer), my personal experience on open source projects helped me a lot to prove my credibility. Technically, drop Java right now, every body can do it. Stick to one language (preferably C or C++ if HPC is what you care about) and become a master at it. Know how other languages work as well (C, Haskell, LISP, etc.), COBOL and FORTRAN can be interesting, but pretty much as latin could be (you don't need to speak it fluently, only to decipher it) and maybe one or two scripting languages. Don't give up, but first get a lame job that can feed you, and maybe train you. The get to work on some open source projects, or personal projects, there is a lot you can learn with stupid stuff (like a software to take account of chips in a poker game). Good luck, and Godspeed.
&gt; I now can explain to my GF what C++ TMP is, how a compiler works, and why P!=NP matters Either your girlfriend is really technical or you rock at explaining things!
&gt; I have also tried to gain some insight into the concept by looking up ways it can be done in C, but they all seem to involve all manner of preprocessor/setjmp/longjmp voodoo It can't be done in C in any meaningfull way (that I can think of). The idea of RAII is to link object lifetime with resources acquisition and release (resources can be memory that must be freed, open handles that must be closed, synchronization locks that must be releasedd, remote connections that must be closed once opened and so on). It is not necessary to use it, but it is the most elegant way the C++ community found of automating resource management. The idea is to hide the complexity of acquiring and releasing a resource in an object's constructor and destructor. Creating the object automatically starts managing the resource (not necessarily creating it though) and destroying it automatically releases the resource. This takes advantage of the destructors being automagically called when the code goes out of scope (whether by a return statement, or by reaching the end of the scope). "goto" instructions do not take care of destructors, which is one reason why their use in C++ code is usually discouraged in best practices. The RAII idiom is good because it decreases the number of things you have to be mindful of, like releasing resources before every exit point in a scope (return, break, continue). consider this code: int someFunction() { SomeTypeA *localA = 0; // this should become a copy of a synchronized resource, // then we want to apply an operation on it SomeTypeB *localB = new SomeTypeB(); // this is a local resource requiring cleanup someGlobalMutex.lock(); // lock global mutex if(someGlobalSynchronizedA) localValue = someGlobalSynchronizedA.clone(); else { someGlobalMutex.unlock(); // unlock global mutex delete localB; delete localA; return; } someGlobalMutex.unlock(); // unlock mutex - duplicated code localA-&gt;someOperation(localB); // finally call the operation delete localB; // duplicated code delete localA; } You can avoid the duplicated code in this example with a goto ( :( ). This code looks simple. Now consider what happens if clone can throw an exception. To handle that case, the code becomes: int someFunction() { SomeTypeA *localA = 0; // this should become a copy of a synchronized resource, // then we want to apply an operation on it SomeTypeB *localB = new SomeTypeB(); // this is a local resource requiring cleanup someGlobalMutex.lock(); // lock global mutex if(someGlobalSynchronizedA) { try { localValue = someGlobalSynchronizedA.clone(); } catch(CloningException e) { someGlobalMutex.unlock(); // duplicated code delete localB; // duplicated code delete localA; throw; // the exception will be handled in the calling code // the purpose of this try-catch block is to simply release resources } else { someGlobalMutex.unlock(); // unlock global mutex delete localB; delete localA; return; } someGlobalMutex.unlock(); // unlock mutex - duplicated code localA-&gt;someOperation(localB); // finally call the operation delete localB; // duplicated code delete localA; } If you have another operation that can throw an exception in the synchronized block, things become ugly very quickly. Now, consider the RAII version: int someFunction() { std::auto_ptr&lt;SomeTypeA&gt; localA; // don't care about releasing this std::auto_ptr&lt;SomeTypeB&gt; localB(new SomeTypeB()); // don't care about releasing this { // artificial scope boost::mutex::scoped_lock lock(someGlobalMutex); // don't care about unlocking the mutex if(someGlobalSynchronizedA) localA= someGlobalSynchronizedA.clone(); // if clone() throws, localA is released if needed, // localB is released and the mutex is released } // unlock happening automagically when getting out of scope localA-&gt;someOperation(localB); // finally call the operation } In this, std::auto_ptr is a RAII impleementation of a smart pointer (scoped, non-copiable pointer) and boost::mutex::scoped_lock is a RAII implementation of lock() and release() for a mutex object. This code looks agnostic of any exceptions the calls might generate, but it is **completely safe**. You can add return or throw in between any two lines in it (or unsafe code) and the resources will still be released. Further than that, it has one clear execution direction, is simple, and clearly transmits the intention of the code (clone someGlobalSychronizedValue, and perform someOperation on the cloned object. Then, release everything).
Find some company that is hiring "junior developers" or interns. They don't expect much from you (if you're doing C++ most of your job will entail moving translatable strings into resources/plists, writing unit tests, documenting other people's code--basically all the stuff that the real programmers _should_ be doing, but we all let the interns do because it's excruciatingly tedious work), and you might not even be trusted with write-access to the repository for awhile, but you're better off not having such responsibilities until you actually know what you're doing. The fact that you have no prior experience or education is not _so_ bad as long as you make some kind of effort to learn about data structures and algorithms. Programming is more of a meritocracy than most other fields, and what will make you succeed above all else are intelligence and an ability to quickly learn new information. That said, being familiar with a few popular libraries and APIs (for example, whatever you do, don't go into the interview having never used or heard of the STL or Boost) is a must. And start checking out C++/Java projects from Github, SourceForge, etc. and go through the code. Not so much to participate in development, but to simply familiarize yourself with how real applications are structured. If you have no idea what the words STL, Boost, GitHub, or SourceForge mean, I'd recommend you start studying now because you've got a lot of work ahead of you.
A lot of bigger companies will say publicly that they require a CS degree just to get an interview, but if you go in self-taught with 5 years' experience and a recommendation from someone who's already an employee, they're usually willing to give you a chance. The pay scale differences depend heavily on what country we're talking about. In Europe it can make a big difference, as the minimum salary the company is allowed to give you has separate grades for your level of education. If you're self-taught with minimal experience, expect to be at rock bottom. Having the equivalent of an American BS in computer science but zero work experience will probably get you another 10% or so. Having a doctorate and 5 years of work experience will get you something like double what rock bottom makes. On the other hand, if your company doesn't totally suck and isn't run by an autocrat, you should be getting pay rises regularly anyway. After a year or two the differences in pay grades blur dramatically. Also, if you're a good enough programmer and can prove it, the salary is up for negotiation anyway.
Find a job working in a research-y group that could use some people with your math background, but would like them to code. Like at my company. ;-) Seriously, send me your resume.
You seem to be extremely interested in the topic, which I can easily tell from what you've picked up so far. If you need income fast, you won't find it through a programming job, there are countless candidates who are far more qualified than you (which I will explain in a second) because they have degrees. What you need to do is prove yourself to future employers. So, just work a temporary job to pay the bills, and create yourself a github account. Start contributing to open source software, and fill your github with projects. Depending on how enthusiastic you are, your github link will be the only thing you'll need on your resume for employers to look at. Most people get the programming jobs cause they have a degree to back themselves up, and usually through school they work on a lot of side projects. You need to focus on that and get it as strong as possible, and you'll eventually (in couple years) find a good programming job you want. Also... networking, go meet people, learn their names, learn what they're doing, collaborate with them. Make a name for yourself and you might eventually find an opening through one of them. Good luck, and welcome to world of programming :) TL;DR: Find a temp job, create github, work on Open Source projects, meet people
ptrb's response is good, but I would like to add a few things: &gt; The examples below done with an explicit malloc could have just as easily been done with a local struct...? Yes. While RAII is useful for memory resources, they are *extremely useful* when it comes to non-memory resources (database connections, file handles, locking and unlocking mutexes, etc...). Here is an example for a mutex: // Mutex.hpp class Mutex { public: void Lock(); void Release(); }; class MutexLock { Mutex&amp; m; public: MutexLock(Mutex&amp; mtx) : m(mtx) { m.Lock(); } ~MutexLock() { m.Release(); } }; // FooBar.cpp { MutexLock lock(a_mutex); Foo(); // may throw... if (Bar()) { return; // early return } // etc... } // whenever we exit the scope, the mutex is released. In this example, I don't have to think about when I should unlock the mutex. In any case (normal program flow, early return, or an exception occurs), the mutex will be unlocked. &gt; I can see how this would be somewhat more verbose in C, but I am not sure why it would be more error prone. Generally speaking, the more verbose something is (i.e. requiring more statements -- not just more characters), then the more challenging that code will be to maintain. But aside from that fact, RAII helps avoid bugs the most in maintainance. (Yes, it helps reduce the number of bugs in initial development too -- but let's ignore this for the moment.) When maintaining code, someone may add an early return or a break statement. In C, whenever you do this in block where you are controlling resources, the person performing the maintanance must be careful to ensure that resources are cleaned up. This gets more complicated when the block is controlling multiple resources. The company I work for ships a C library for motion control and I have seen this problem maintaining code many times -- a small change to fix a intermittent bug will then all of a sudden result in a new larger bug related to resource cleanup. In C++, when you use RAII, this is not a worry anymore. If the maintainer forgets about a resource, it's OK -- the compiler does not forget about it. &gt; Also, isn't it normally the destructor's job to contain any and all clean-up code associated with destroying an object? Yes. This is typical C++. RAII though is more aimed at creating and using classes whose sole job it is to control access to resources (like the locking a mutex example above). &gt; I was under the impression that was the normal way to do things in C++, so would you guys be able to tell me when it would be beneficial NOT to do it that way? It is the normal way. It is generally NOT beneficial to do otherwise. RAII does cleanup the normal way. But as I stated above, RAII is generally referred to when using custom classes to control access to resources. I hope that clears up some things. Good luck!
I'm *glad* it is hard to write exception safe code. Because it's far too easy to write return-code unsafe code. I'd rather an exception blow up in my face than silently ignore an error, which is what happens in 99% of the API's that have return codes.
Cool story bro, but I don't see how it helps OP. If you want everybody to know how awesome your life is, do a AMA.
Well, pretty much like op, i switched career from something unrelated to cs. I'm just saying it's possible, unlike everyone else. Besides, my life is not awesome.
By 'fairly exception safe' you mean RAII **is** exception safe :-). C does not have exception handling. You do have setjmp/longjmp which can be used to emulate exception like behavior with considerable work. Also many vendors hack exceptions into the C language, but that doesn't count.