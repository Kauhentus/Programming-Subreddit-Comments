Oh, that's neat!
Yeah, last problem (I hope). Whenever I define these vectors (in my header), VS2013 tells me that "No overloaded function takes X arguments," where X is the number of floats or GLuints in my vector. I wonder what that's about. EDIT: Looking on stack exchange, it seems like a bug with VS2013... bummer.
That’s wrong. A deque is usually implemented [as a vector of vectors](http://stackoverflow.com/a/6292437/1968) (a vector of pages, or blocks). A deque *can* be implemented in the way you’ve mentioned but that displays different performance characteristics than those guaranteed by `std::deque`.
Similar questions have come up a few times on reddit: * http://www.reddit.com/r/gamedev/comments/1vl33e/so_i_was_watching_the_documentary_the_story_of/ * http://www.reddit.com/r/gamedev/comments/23je56/using_runtime_c_as_a_scripting_language/ * http://www.reddit.com/r/cpp/comments/1a5h83/runtimecompiled_cedit_and_continue_for_ms_vc_gcc/ I've [collated some of the approaches to runtime compilation of C++ on the wiki](https://github.com/RuntimeCompiledCPlusPlus/RuntimeCompiledCPlusPlus/wiki/Alternatives) for [RCC++](http://runtimecompiledcplusplus.blogspot.com/).
You and I have different definitions of "legit" and "read". 
really, the project sounds nice, but i really dont need to read an "article" about how you dragged a QWebView onto a form. make progress with the actual project and then write about that.
Hi, great question. I work for Red Hat and manage the toolchain team. We're actually a significant part of both the upstream development of gcc/g++ and contribute directly to the definition of ISO C++ standards. We've done this for many years. Quite often C++ features are proposed for the next standard and get a prototype implementation in g++, so we really are doing a lot more than packaging and distributing GNU tools. Why? We feel open standards and open source are good for the greater community, good for our customers and partners and, well, good for us too. Everyone wins!
Reminds me of [otter](http://otter-browser.org/). A browser in qt5, trying to recreate the interface of the old opera browser.
Intel Amplifier (VTune) or other profiling tools.
Content excluded, these are terrible videos. Maximum 240p resolution and half the screen is covered by the same blue text that does not change. 
What technical issues can warrant a 240p resolution?
was more thinking about the big blue text... the resolution is just bad, tho some of them go up to 360p ;)
Very interesting, thank you for posting this.
BTW, it was a totally unnecessary blog. :)
Yes, it nicely compliments the work I did on [range comprehensions](http://ericniebler.com/2014/04/27/range-comprehensions/).
Indeed, thanks again :) Constructs like these, when you are recreating yield or other keywords in C++ really shows how powerful the language is. However, I think this is not the true way to go for C++. I know that C++ is known to give the programmer the possibility for OOP on a low level but it could be much more comfortable to code in in comparison to what it is now. I would love to see an upcoming version of C++ to ignore backward compability and start fresh. With modules, readable keywords, a huge standard library and other features more modern languages have, and removing all the C-stuff like the Preprocessor, unsafe casts and malloc/new. I think this could potentially move C++ to a much more usable spot, where C# and Python reside today. Sorry for this offtopic post, it's just something that came to my mind after reading the two articles. Edit: If I am getting down voted because this comment is not so much about the blog article that's fine, but if you are down voting me because you disagree, at least post a useful answer.
As a nitpick, isn't it expected that if you write for (auto el : col) then el is of type that *col.begin() would yield and not an iterator? Then you wouldn't have to dereference el when you want to write out the number. (Regarding range based for loop example)
&gt; a totally unnecessary blog. I guess people didn't understand the joke. BTW, I agree, it was a complete waste of bandwidth as well :-) 
Really cool, thanks for posting. I don't see any license though.
Key bit: &gt; Starting with Visual Studio "14," we will stop releasing new versions of the CRT with each release of Visual Studio. C++ runtime will keep changing, though (see Herb Sutter's ABI proposal for the plans to deal with that).
&gt; This 2,696 line file had 223 conditionally compiled regions of code (#ifdef, #else, etc.), over half of which were in a single 1,400 line function. This file was compiled 12 different ways to generate all of the common implementation functions. And people wonder why they've been so slow to add C99 features...
I don't understand why so much downvotes. It's somewhat offtopic but I agree with you, some type of "modern mode" a la strict mode of javascript, where you can only use modules and other newer and better constructs seems a good idea.
Quite interesting. I got a poor man's generator together using `boost::function_input_iterator`, and `boost::iterator_range`. You can find the code [here](http://coliru.stacked-crooked.com/a/05f85730b7332ca8). Unfortunately, doesn't compile with gcc-4.8, though. Of course it doesn't offer any `yield` syntax...
Because C is dead for Microsoft, they only add C features required by the C++ standard. C99 library compatibility is required for C++11/14 compliance, so they added it.
You have to consider programming languages as simply tools in a toolbox which are used when the time is appropriate or the company you are working for uses it. You may find you won't find a job using C++ until you have a bit of experience in a professional environment using other languages.
They've also added C99 language features that are not part of any version of C++.
Just enough for allowing specific open source projects to be ported to Windows. If you wish I can link Herb Sutter posts and talks where he makes the point full C support is not happening. In fact, since Windows 8 C++ is even supported at kernel level.
No, they've been shipping it for a long time.
What actually is this? The title nor linked page give anything away
I guess the down votes are probably because of the offtopic post. Anyway, thanks for your reply, it's nice to know that I'm not the only one with these thoughts.
It is part of FOSS projects Microsoft is willing to help to compile on Windows. &gt; Just enough for allowing specific open source projects to be ported to Windows.
Static linking FTW
&gt; The sprintf functions are now up to 8 times faster than they were in previous releases. would like to see iostreams performance become comparable to FILE streams as well. i have my reservations since currently calling std::cout for certain types invokes ::new.
It's his implementation of most of the c++14 features (as the reddit title says).
&gt; Whereas before we would have released msvcr140.dll in this forthcoming release, then msvcr150.dll in the next release, we will instead release one new CRT in Visual Studio "14" then update that version in-place in subsequent releases, maintaining backwards compatibility for existing programs. Well FUCK, welcome to even more DLL hell. And how do you expect to maintain backwards comparability? More SxS shit or are you going to enforce an init call/magic string check with a version # required that turns runtime features on and off? That'll be great, let's bloat the shit out of your runtime dll so that version n-10 will work the same it always did. And let's also blow all the optimizations away because we're now doing a shitload of if-then-else's to jump around version x y and z. "Hmm, which version of MSCRT.dll is this? I can't go by the filename any more, I have to *open the goddamn property page* and translate the version # to what some random KB article says I need". 
This is my experience as well.
I highly recommend everyone reads the new cmake build system documentation ( http://www.cmake.org/cmake/help/v3.0/manual/cmake-buildsystem.7.html ). It is a good introduction too 'modern' cmake. 
Could you use a `vector&lt;T*&gt;`? Or, more safely, `vector&lt;unique_ptr&lt;T&gt;&gt;`?
gotta say last time I looked at the documentation I was pretty scared just having started learning programming/c++. But this looks way better from a first glance, definately gonna look into it.
Once you know C++ very well, finding a job to fit your language would not be a problem - the shortage of experienced C++ programmers in finance here in NYC is dire, and there are plenty of other fields where reliability and performance matter, from telecom to transportation to energy. You can be choosy then. But for your first 10-15 years programming, it may not be as easy to find a pure C++ job, so don't shun other technologies (it helps to know them, anyway)
I'd like to see how I can specify C++11 support with the new CMake. &gt; [project(MyProject CXX11)](http://www.cmake.org/cmake/help/v2.8.12/cmake.html#command:project)?
It helps that this is actual documentation instead of a half hearted attempt with a 'hey buy our book if you need more info!' at the end of each page.
Not to mention that the CMake book is getting a bit long in the tooth these days.
Herb Sutter talks about that here: http://arstechnica.com/information-technology/2013/06/c99-acknowledged-at-last-as-microsoft-lays-out-its-path-to-c14/ Or it's an article that says that he talked about it at least. ;)
They're working on an automated way of enabling C++11 stuff in compilers but I don't think it made it into 3.0. 
Well, to Visual C++ rather. mingw already compiles portable open source projects for windows just fine.
Nice, BUT... &gt; Finally, let’s create a class that will be able to give a signal of the found solution. It will signal by throwing FoundGoal exception. OMG? Are you seriously throwing an exception there to jump into a catch block for processing the result? 
Mmmm, I love me some optional&lt;T&gt;
Welcome to the real world where binary interfaces are well defined.
Depending on the implementation, iostream performance is at least as performant as stdio after disabling the syncing between the two. I didn't know about the object creation prior to your comment. I'd like to know more about the instances that that may occur. http://stackoverflow.com/questions/5166263/how-to-get-iostream-to-perform-better http://codeforces.com/blog/entry/5217 Unfortunately, I don't have any test results from VS 11 or 12.
Adding just the bits people actually use in some projects of interest is rather different from your original claim that they're only implementing the C99 things that are part of C++11.
Microsoft (R) C/C++ Optimizing Compiler Version 18.00.21005.1 for x86 http://pastebin.com/yZC6Tzwc
I had [enough pain](https://vinipsmaker.wordpress.com/2014/02/15/why-is-it-a-bad-idea-to-rewrite-tufao-build-system-under-qmake/) with qmake and I won't go back to it. But yes, at least they implemented good C++11 support.
This approach is mentioned [here](http://marc.info/?l=boost-users&amp;m=118835451021207) quote: &gt;If you want to stop at the target city, the short answer is : use a visitor and throw an exception
That's just sad.
Why?
I think a better table would include the currently-released C++ compiles (GCC etc) that support C++11/14 so you can see how CTP1 is stacking up.
This might help for clang/gcc/icc/libc++/libstdc++ support info. * [Clang](http://clang.llvm.org/cxx_status.html) C++ status, as of March this year. [libc++ status](http://libcxx.llvm.org/cxx1y_status.html) as of March as well. * GCC support for [C++11](http://gcc.gnu.org/projects/cxx0x.html) and [C++14](http://gcc.gnu.org/projects/cxx1y.html) as of April and December 2013 respectively. Not sure if there are more recent documents off the top of my head. [libstdc++](https://gcc.gnu.org/onlinedocs/libstdc++/manual/status.html#status.iso.2014) support status — not sure on date, though copyright indicates 2014, so it was probably updated sometime this year. * icc support for [C++11](https://software.intel.com/en-us/articles/c0x-features-supported-by-intel-c-compiler) and [C++14](https://software.intel.com/en-us/articles/c14-features-supported-by-intel-c-compiler) as of December last year. (Far as I know, versions ≤8 are Dinkumware and modern icc uses libstdc++. I could be wrong, I don't pay a lot of attention to icc.) Might be other compilers I'm missing, but those are the big three others that come to mind.
&gt; What - if anything - have we learned form C++ I had to laugh like hell. 
So, the debug build issued 2 allocs during the integer to ascii conversion, not bad so far. The release build issued 5 allocs prior to any io. Issued another 7 for the integer conversion. I added more integer and some new floating point conversions to your example code. There were no additional allocations. Can you provide another use case that shows poor memory allocation behavior affecting actual throughput?
Still no full C++11 support.
Look at the other compilers and corresponding libraries (see the other post that provided links). They are basically done with c++11, anger mostly even with 14. Ms is not even close... Intel is also a bit behind, but at least on first glance it seems less severe.
So basically my numbers are to big so it confuses the computer into printing random values in a way? 
Not even constexpr.
gcc's library still has a long way to go
One easy way to solve this would be to store your values as double which don't overflow; if your number gets too big, the value it will have is +infinity. Of course using floating point has its own issues with numerical stability (where 1.1 + 0.1 != 1.2 - this example isn't necessarily true, but binary floating point addition != decimal floating point addition since rational decimal floating point isn't necessarily rational when expressed in binary). Additionally, instead of doing x * x * x * x, you can use pow(x, 4). It will be slower but more mathematically accurate (it's optimized for speed &amp; numerical stability, but of non-integer exponents). pow() is floating-point. If you really want to keep doing integer math, you have a few options: use int64_t (using uint64_t will expand your range by another 2x if all your numbers are positive, but it can wrap to 0 when it overflows). If you want arbitrary size integer math (i.e. without the precision issues of floating point), I would recommend using GMP/MPFR. According to wikipedia, these are also wrapped via boost: http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Arbitrary-precision arithmetic**](https://en.wikipedia.org/wiki/Arbitrary-precision%20arithmetic): [](#sfw) --- &gt; &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), __arbitrary-precision arithmetic__, also called __bignum arithmetic__, __multiple precision arithmetic__, or sometimes __infinite-precision arithmetic__, indicates that [calculations](https://en.wikipedia.org/wiki/Calculation) are performed on numbers which [digits](https://en.wikipedia.org/wiki/Numerical_digit) of [precision](https://en.wikipedia.org/wiki/Precision_(arithmetic\)) are limited only by the available [memory](https://en.wikipedia.org/wiki/Memory_(computers\)) of the host system. This contrasts with the faster fixed-precision arithmetic found in most [arithmetic logic unit](https://en.wikipedia.org/wiki/Arithmetic_logic_unit) (ALU) hardware, which typically offers between 8 and 64 [bits](https://en.wikipedia.org/wiki/Bit) of precision. &gt;Several modern [programming languages](https://en.wikipedia.org/wiki/Programming_language) have built-in support for bignums, and others have libraries available for arbitrary-precision [integer](https://en.wikipedia.org/wiki/Integer) and [floating-point](https://en.wikipedia.org/wiki/Floating-point) math. Rather than store values as a fixed number of binary [bits](https://en.wikipedia.org/wiki/Bit) related to the size of the [processor register](https://en.wikipedia.org/wiki/Processor_register), these implementations typically use variable-length [arrays](https://en.wikipedia.org/wiki/Array_data_structure) of digits. &gt;Arbitrary precision is used in applications where the speed of [arithmetic](https://en.wikipedia.org/wiki/Arithmetic) is not a limiting factor, or where precise results with very large numbers are required. It should not be confused with the [symbolic computation](https://en.wikipedia.org/wiki/Symbolic_computation) provided by many [computer algebra systems](https://en.wikipedia.org/wiki/Computer_algebra_system), which represent numbers by expressions such as *π*·sin(2), and can thus *represent* any [computable number](https://en.wikipedia.org/wiki/Computable_number) with infinite precision. &gt; --- ^Interesting: [^GNU ^Multiple ^Precision ^Arithmetic ^Library](https://en.wikipedia.org/wiki/GNU_Multiple_Precision_Arithmetic_Library) ^| [^Computer ^algebra ^system](https://en.wikipedia.org/wiki/Computer_algebra_system) ^| [^Factorial](https://en.wikipedia.org/wiki/Factorial) ^| [^Standard ^ML](https://en.wikipedia.org/wiki/Standard_ML) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ci5azo3) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ci5azo3)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Any plans for the `&lt;codecvt&gt;` header? Of course it depends on the compiler support for char16/32, but even that hasn't prompted GCC into implementing it. Only libc++ has this so far (last I checked).
[http://msdn.microsoft.com/en-us/library/ee292114%28v=vs.100%29.aspx](MSDN) says it's been available since 2010, and I've used it with 2013.
In [Two's Complement](http://en.wikipedia.org/wiki/Two's_complement), 11111111 is -1, 10000001 is -127.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Two's complement**](https://en.wikipedia.org/wiki/Two's%20complement): [](#sfw) --- &gt; &gt;__Two's complement__ is a [mathematical operation](https://en.wikipedia.org/wiki/Mathematical_operation) on [binary numbers](https://en.wikipedia.org/wiki/Binary_number), as well as a binary [signed number representation](https://en.wikipedia.org/wiki/Signed_number_representation) based on this operation. Its wide use in computing makes it the most important example of a [radix complement](https://en.wikipedia.org/wiki/Method_of_complements). &gt;The two's complement of an N-bit number is defined as the [complement](https://en.wikipedia.org/wiki/Method_of_complements) with respect to 2^*N*; in other words, it is the result of subtracting the number from 2^*N*, which in binary is one followed by N zeroes. This is also equivalent to taking the [ones' complement](https://en.wikipedia.org/wiki/Ones%27_complement) and then adding one, since the sum of a number and its ones' complement is all 1 bits. The two's complement of a number behaves like the [negative](https://en.wikipedia.org/wiki/Additive_inverse) of the original number in most arithmetic, and positive and negative numbers can coexist in a natural way. &gt;In two's-complement representation, negative numbers are represented by the two's complement of their absolute value; in general, negation (reversing the sign) is performed by taking the two's complement. This system is the most common method of representing signed integers on [computers](https://en.wikipedia.org/wiki/Computer). An N-bit two's-complement numeral system can represent every integer in the range −(2^*N* − 1) to +(2^*N* − 1 − 1) while [ones' complement](https://en.wikipedia.org/wiki/Ones%27_complement) can only represent integers in the range −(2^*N* − 1 − 1) to +(2^*N* − 1 − 1). &gt; --- ^Interesting: [^Signed ^number ^representations](https://en.wikipedia.org/wiki/Signed_number_representations) ^| [^Least ^significant ^bit](https://en.wikipedia.org/wiki/Least_significant_bit) ^| [^Most ^significant ^bit](https://en.wikipedia.org/wiki/Most_significant_bit) ^| [^Overflow ^flag](https://en.wikipedia.org/wiki/Overflow_flag) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ci5iwhl) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ci5iwhl)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; The CRT for desktop apps must run on all supported operating systems (in Visual Studio 2013 this included Windows XP) in other words: but not in vs 2014. Great. So if i have to continue to target xp i have to stick with vs2013?? Seriously? Goodbye c++11. What a mess.
Well, that is partial at least. &gt; C++11 constexpr is listed as "Partial" because VS14 CTP1 doesn't support it on member functions. Note that there's a separate row for C++14 constexpr, which has not yet been implemented. 
User-defined literals, finally!
while we are at it, why cant all the individual crts/runtimes not be distributed and maybe updated via microsoft update?
The documentation states that this controversial use of exceptions was arrived at after much debate in the email group. Unfortunately the link was dead. Can anyone summarise the reasoning?
It seems like the C++ programming jobs that I saw in finance were primarily in HFT. While this industry seems very interesting to me, one of my friends recommended that I avoid this type of job, as there isn't too much applicability of skills to other industries. Any opinion on this?
That actually sounds like a great idea, thanks!
Where do you live? A job in my company just opened up due to someone moving. It's primarily c++, though we do use python, c#, and a few other minor or dsl languages here or there. CppCon is a good idea as is any C++ users group near you for networking. Languages are just a tool, but transitioning to C++ is much harder than transitioning to C++. So your backing in C++ will prove valuable in the future. 
Looks interesting, What can you do with this? What would be a practical use case?
On VCBlog [I explained](http://blogs.msdn.com/b/vcblog/archive/2014/06/06/c-14-stl-features-fixes-and-breaking-changes-in-visual-studio-14-ctp1.aspx): The current plan of record is that VS14 RTM will support XP targeting. However, management may change the plan. My PERSONAL opinion is that it's time to cut XP targeting (it's burdensome and has been causing headaches for improving mutex/condition_variable): XP is out of support, Server 2003 is out of support on July 14, 2015, and even XP Embedded is out of support on Jan 12, 2016. Again in my PERSONAL opinion, given that VS14 RTM is planned for 2015, then it takes some time for user-programmers to upgrade, then more time for them to ship their programs to end users, I think it's clearly time (user-programmers who want to target out-of-support OSes can continue to use VS 2013 while it's supported). But I don't get to make the decision.
could i work around that with static linking?
No - XP incompatibility is caused by the CRT/STL calling stuff that doesn't exist on XP. Static versus dynamic doesn't affect that.
of course. sorry, i am tired as with all the chaos in baghdad.
There can be many use cases for this: - Statical code analysis of Python code - A python JIT compiler - A python to $PLACE_YOUR_LANGUAGE_HERE converter - A code optimizer by looking at the AST and transforming it (Removing dead code blocks) - A tool to find all imports (Which could be used for something like py2exe/pyinstaller) ... That said, it's not like you cannot do this with other implementations, mainly naming here CPython itself here. I just recently started working on this, so I haven't had yet time to implement some examples. How to work with it: You can use the visitor implementation: https://github.com/vinzenz/libpypa/blob/master/src/pypa/ast/visitor.hh To visit each and every element in the AST an example for the visitor usage is in dump.cc: https://github.com/vinzenz/libpypa/blob/master/src/pypa/ast/dump.cc It's currently used by the parser_test to dump the parsed ast, so I can verify visually that it parsed it, as I expected it.
People are still complaining about VC++ being behind, but it should be noted that this seems to represent a continued acceleration on their behalf. I think VC++ could catch up with clang/gcc during the C++14 period and hopefully will be ready to match their progress when C++17 comes around. It should also be noted that VC++ is achieving this while still trying to implement a complete C++ AST. Not having that is a pretty big disadvantage and adding it surely requires a significant amount of work.
This question really belongs to /r/cpp_questions as the big, red warning created by the “Submit a new text post” told you. That being said: Primer Plus is usually considered to be a really bad book that just tried to steal the good name of the Primer. Accelerated C++ is said to be an extraordinary good book, but the sad thing about it is, that it is clearly outdated by now. The current version of primer is what is usually recommended today (with “Programming - Principles and Practice using C++” by Stroustrup as the alternative).
A Tour of C++ is not a replacement for Accelerated C++ but I guess it is the most similar up-to-date thing we have.
Areas where C++ is popular: high-performance finance (It's not *just* HFT), video games, embedded. I saw someone ask about location. If you are in / near Charleston, SC, drop me a PM. I know that at my job we've been having difficulty hiring people with the knowledge we want (we are very much a high-performance C++ shop).
Do NOT buy C++ Primer Plus by Stephen Prata. It's a horrible book teaching C++ as if it were C, relying only on the good reputation of the actual C++ Primer by Stanley B. Lippmann to get sales from confused beginners. 1. is a solid choice. Make sure to get the most recent edition.
I wonder what skills were they referring to. I came to finance from embedded, and most skills transferred, I only miss my oscilloscope.
I had the Third Edition of C++ Primer by Lippman and hated it, the Second Edition was recommended but the 3rd edition was out by the time I got it. I love the latest C++ by Stroupstrup (4th), better than the 3rd which I liked too 
I was going to write a post about how they seem to over-specialize concepts from Haskell like *fmap* and this type itself being a sort of *Either* with *Left* and *Right*, but after reading the whole paper, they do address my criticisms. It just seems that what they should have proposed first was a *Monad/Functor* interface with *fmap*. This paper misses out on some of the more interesting aspects of *Either* as well. For example, the function, *either(f,g,e)*, which returns either *f( fromLeft(e) )* or *g( fromRight(e) )*, useful for when a left or right value can equally produce the needed result. A series of expected results would likely be put into a container, so the functions *lefts*, *rights*, and *partitionEither* would be useful. One thing that separates *Either* from *expected* is that *Either* is rather ambivalent to whether it holds a *Right* or *Left*. It can be used for error handling, calculations involving two variants, or even nested to support many. Haskell's Either: http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Either.html More Either goodness: http://hackage.haskell.org/package/either-4.3.0.1/docs/Data-Either-Combinators.html I definitely think this takes a step in the right direction. C++ does not support error handling outside of exceptions very well and I do try myself to use them as sparingly as possible. 
The 4th edition by Lippman is excellent (for pre-C++11), and the 5th edition, which covers C++11, is at least as good. Seriously. Agree about Stroustrup's 4th edition. The 3rd edition was full of information, but I thought the organization and typesetting left something to be desired. The 4th edition is really nice (I've only used the Kindle version).
[Overview here](https://www.youtube.com/watch?v=MdWE_k9zpWI)
Better question is, why should we care? Don't get me wrong, reimplementing full C++ STL is no mean feat, but why should we mess around with this implementation instead of using libc++ or libstc++?
for a real job - frameworks!
BTW STL this is kind of oftopic but it si related to algorithms.... is transform_if missing for a good reason, or just usual : -not used often enough to be in std -nobody proposed it to be in std ? 
Nobody has proposed it. I might.
Neat little trick
Works for C files too!
This is pretty sweet , and I'm happy go see as a .net dev that the general implementation is nearly the exact same. Thanks for sharing!
yeah I'm guessing it works for all the languages implicitly recognised by make - fortran, modula2 and pascal included 
What is the point of this? Why wouldn't you just call the compiler directly? g++ file.cxx I just don't see the benefit of using make on a single file.
I'm on the phone but I believe the syntax is g++ $(CXXFLAGS) blah.cxx
Nice little trick (and nice to know how make works), but i personally don't like it. Setting CXXFLAGS could be a trap, as it affects your whole environment. Hard to agree on compiler options you would use for every single compilation. I would rather rely on shell command history, e.g. I have my test.cpp in one of misc directories that i use for hello world type of programs and invoking clang/gcc isn't taking that much time for me.
This is not a standard library implementation. It is an implementation of some proposals given during the road to C++14. Some are going to be in the upcoming library fundamentals, some have fallen by the way side or not commented on. libc++ and libstdc++ do not provide everything found in this library, nor will they, as some proposals have been denied, or expanded upon, or even delayed (such as a `range&lt;Iter&gt;` type, due to a talk given by Chandler Carruth recently)
That's totally fine, you don't have to agree with me. If you find this approach useful, then it is great. I'm just saying I would not use it. I would invoke the compiler directly. I just can imagine an occasional experiment where I need to use some library, and now I have to make sure my include/link options are added. Therefore I will go for direct invocation, rather than making this trick support this use case.
Because g++ file.cxx vies you a binary labeled a.out, and the author's way does not.
cool, thanks!
Put the variables in a `Makefile`, but don't add any compilation rules unless you need to -- as you can see it's not needed in simple cases. This is just the normal way I'm using make.
I found it by googling the phrase “why no lambdas in google c++ style”. Seems like a particularly bone-headed requirement when you consider how *extremely* useful lambdas make the standard algorithm library.
Of the list, I agree with the RTTI and exceptions points. Not the rationale mind you. RTTI adds overhead, even if it's not used. Exceptions prohibit certain optimisations and can make certain code tricky to make robust. However, I'd be reluctant to ban the use of either. I think the author is right in that it's a guide trying to make C++ into Java. 
Google is an EOE. As such all the individual contributors need to feel appreciated and respected by the organisation. EOE in this context does not imply race or gender, but rather skill sets. It is not 'fair' to let a poor Java or Ruby hack feel bad about themselves for not being able to produce high quality code in C++ as easily and as perfectly as they would in say Java or Python. Hence just like a jockey that has to weigh in before the big race, the use of C++ at Google is intentionally handicapped so as to not depress the advocates/practitioners of the other languages. I'm ashamed to say this, but as ludicrously and needlessly complex as Folly is, Facebook is really taking the lead in how C++ is used within these web 2.0 companies - good for them! 
Can't really disagree with anything, except the unsigned int crap. Just use uintX_t where X is your size.
Many of the services that belong to Google have a ridiculous bad quality nowadays. Good example is the youtube-sorting: See this [channel-view](https://www.youtube.com/user/Gronkh/playlists?sort=da&amp;view=1&amp;flow=grid) and scroll down. You see that the sorting for newly loaded playlists is [completly broken](http://i.imgur.com/AagNGvL.png) (look at the timestamps of the video. It should be sorted oldest-&gt;newest as a whole, but it restarts at the next chunk of playlists). The oldest-newest sorting for videos on the other hands works perfectly fine, so we can only assume that someone at a Google implemented this sorting of items with a timestamp twice (of which one implementation is still wrong). They also obviously never tested that second implementation properly.
You missed the [setjmp](http://en.wikipedia.org/wiki/Setjmp.h) option.
He hasn't even formulated an argument that is possible to agree with, because the very first section he quotes doesn't list a pro... or "convenience in typing". If he doesn't understand why you don't call a virtual from within the constructor, *he's* a non-starter... certainly not Google's C++ style guide. I stopped reading right there. 
 &gt; "Provide a copy constructor and assignment operator only when necessary. Otherwise, disable them with DISALLOW_COPY_AND_ASSIGN." This is the only part (of google's style guide) I sorta-agree with. If you want to make a complex object and don't want to deal with getting copying and assigning right (which is often hard), then disabling copying and assignment and sticking it in a smart pointer is a good idea in most cases. I guess it depends on what you mean by 'necessary'. 
&gt; it leaves behind a "zombie" object Can you elaborate or give an example? I was under the impression that, when used properly neither already initialized members nor the object itsself should remain when the c'tor throws an exception.
You were too thin to both: - understand what the author disagreed with - find the expandable sections with the rationale
Though I agree with many points in this post, it misses a more detailed discussion of the reasoning behind google's decisions. Today's C++ is a highly complex language with many features, many of which can be misused to write code that is very difficult to maintain or hides important performance issues behind seemingly innocent notation, especially when people are just starting with the language. With 10.000+ developers, I'd suppose that (strong) coding guidelines are a must to avoid such scenarios. A more homogenous codebase eases maintainaince by others, and strong rules can help to avoid stupid mistakes, such as accidently temporarily copying complex objects or even long arrays all over the place, having resource leaks in case or exceptions etc. That said, some of the rules mentioned in the article don't really seem to help with any of this (no complex constructurs -&gt; no RAII? No use exceptions?), but there might be other reasons (interoperatbility with other libraries and languages maybe?), so I'd love to have a look at the reasons google states for those rules, not just the rules themself.
Equal Opportunity Employer I guess.
In the very first quoted bullet item, there is no "pro"... Do you think it's a good idea to call virtuals from within the constructor? 
Also notable is Sean Parent's talk (done at various places, first link I could find is [this one](https://www.youtube.com/watch?v=IzNtM038JuI&amp;list=PLHxtyCq_WDLXFAEA-lYoRNQIezL_vaSX-&amp;index=10).) about his experience with C++ at google. TL,DV: During code review he found function that was approximately three screens long, with multiple loops and nested loops and some branching for good measure. After thinking about it, he saw it could be replaced by find+rotate from STL. It took him multiple days to convince the original programmer that it is equivalent and the senior code reviewer dismissed the change, because "nobody knows what rotate does". So, if you want to write smart C++, don't go to google I guess.
[There are very practical reasons for not using exceptions.](http://www.joelonsoftware.com/items/2003/10/13.html) Exceptions are "goto". Actually, in C++ code they are *worse* than "goto" itself, because a goto in C++ can only goto a label in the same function scope. A lot of supposed "C++ gurus" will lambast the use of "goto" for flow control whilst at the same time defending ubiquitous use of exceptions... I can only assume because, well, it's "proper OOP" and someone told them goto makes code bad, and exceptions makes code good, somehow. The performance cost of exceptions is negligible, however, unless you're setting up lots and lots of exception handlers in a tight loop, which would be a code smell in the first place. 
what's the modern equivalent to getter/setter? I disagree in one point: exceptions are hell-sent evil that need to be avoided. Seriously.
ok, is that really such a big win? g++ file.cxx -o anyNameYouWant I mean, I guess it saves you some typing.
Why? 
I didn't know what rotate did. I read for about four seconds on cplusplus.com and now I do. If that's beyond the level of thinking expected at google, I for one am worried.
Using RAII properly generally results in copy/assignment either working correctly by default or being disabled by default. Rule of Zero exceptions should be quite rare.
Eh, [cppreference.com](http://en.cppreference.com/w/) is generally more recommended. cplusplus.com has been catching up with its reference, but their tutorial is still terrible.
I didn't say "many", actually... and what does the testing of code have to do with the fact that exceptions are gotos? I didn't say anything about the testing or lack thereof, or say anything about the use of "if" and "switch".
Because they are worse than goto, when it comes to creating a pile of spaghetti that is difficult to test, debug, and maintain. 
&gt; Pros: &gt; **Convenience in typing**. No need to worry about whether the class has &gt; been initialized or not. Emphasis mine. &gt; Do you think it's a good idea to call virtuals from within the constructor? No, and most probably neither does the author. Why do *you* think he does ? This is about "the doing work" part. 
You're not quoting the author. You're making up quotes of your own. Since you are not reading the link, I'll quote for you... here's from the GSG, which he quotes... it's his first bullet item, in entirety: ' "Avoid doing complex initialization in constructors (in particular, initialization that can fail or that requires virtual method calls). ... Constructors should never call virtual functions or attempt to raise non-fatal failures. If your object requires non-trivial initialization, consider using a factory function or Init() method" ' Here's what he says *right after* he quotes this section of the GSG: "100% deal-breaker." 
struct?
How is handling an error at the point it occurred not less spaghetti than potentially throwing it up through stack frames and handling it somewhere far away in the code base? At best, as far as the programmer who is reading the code is concerned, the exception is handled nearby, but many times, that is not the case (because it wouldn't be possible or appropriate to do that, anyways). The exception is handled somewhere else in the code, perhaps many frames above in the call stack. A maintainer of this code may now have to follow the strands of spaghetti of nested exception handling to determine where the code goes. 
I personally like to ditch exception, but it is the only way to properly exit(calling all destructors) outside main.
I dont know what your point is exactly - perhaps we disagree on what the article is saying? It explicitly says this is a little trick that eases the friction of creating tiny little one-off apps a little bit. it saves a little bit of typing. Its not supposed to make your life a whole bunch easier. Its something many people do not know that perhaps they might like to know
Exceptions could be handled identically to return value errors if you liked. Or you are given the option of using more centralized handling. It all depends on how you style your handlers that determines whether exceptions are good or bad. For instance Google does lots of fire and forget processes. A centralized handler could more easily log and retry than having the control flow logic in nearly every method (or keeping the stack super short).
&gt; Exceptions are "goto". Isn't "goto fail;" the one useful place to have a goto? &gt; will lambast the use of "goto" for flow control whilst at the same time defending ubiquitous use of exceptions It depends on what you are using those exceptions for. Exceptions are not a flow control method and shouldn't be used as such. They are a "I could not do the operation you assumed would not fail".
Of all these, the exclusion of lambdas, bind, and rvalues seems like throwing the baby out with the bath water. I've had the privileged of starting something new and writing pure C++11. It is an enormous improvement and really smooths some of the warts out there (albeit while adding some complexity and extra questions like the lifetime of a static variable inside of function object created with bind as that function is moved to a thread). Lambdas, bind, and closures in general are extremely powerful. You can even do things like bind a function with an argument and move it to another thread, or create state for your function object by passing in a reference (not a pointer and not a const reference) and then modifying it. 
My point is that I question the utility of this trick. To me, you can already one line compiling a file by calling out the compiler directly. Perhaps some would prefer this trick, I don't really like it myself just because it adds a layer of indirection. make thing.cxx could do many strange and crazy things depending on how I've modified my various flags in my bash_profile (which, btw, I wouldn't recommend setting those flags there anyways just because it makes it so that now every time you compile anything you may or may not apply those flags.) Knowing how to run a compiler is a pretty basic skill in the C++ world. The complexity of typing a few extra characters is worth it to me.
But your constructor should know that and handle that within itself.
Agree with all of this. To use a topical analogy, Google is kicking many own-goals here. See the [LLVM coding standards](http://llvm.org/docs/CodingStandards.html) for an alternative. 
Which is also why they have a rule to not have static/global class instances. You can't rely on construction/destruction ordering. 
And that is (one of the reasons) why you should use RAII rather than cleaning things up in the destructor of a class that does not exist specifically to own that resource.
&gt; Exceptions could be handled identically to return value errors if you liked. They would add significant code bloat over simple error codes to propagate up the stack. 
Citation neeeded. IMHO it's difficult to say outright that "language feature X is too costly" because it requires an apples-to-apples comparison. In the case of exceptions, you would have to compare them with some other form of error handling which behaves identically. To me anyway, it's not immediately obvious what that would be. 
&gt; Exceptions are not a flow control method and shouldn't be used as such. They are a "I could not do the operation you assumed would not fail". If that's the case, then one can't argue that "goto" is a flow control method any more than exceptions. Exceptions can certainly start executing code a lot "further away" than goto. 
It might've been "nobody understands what rotate does", but that really isn't much of an improvement. 
They are not entirely accurate, and a little outdated. I'm also not sure what the external site guide looks like these days (it's similar, but not identical to, the internal one). The Google C++ style guide has its problems, but banning lambdas ain't one.
It depends entirely upon the type of error. There are time that an error cannot be reasonably or fully handled at the point it occurs. In those cases, exceptions are a way to allow reliable propagation of the error up to a point where there is sufficient context to properly handle it.
Prior to C++11, there was no standard closure type, so Google had its own. This part of the guide is 1) outdated and 2) enforcing consistent style while a transition plan was developed.
&gt; There are time that an error cannot be reasonably or fully handled at the point it occurs. And that time is often. I said that in my comment, actually. If you read the Spolsky link, his point is that exceptions are invisible and create many exit points. Explicit error handling and returning of error codes is not invisible, and you know where the next line of code to execute is at. Returning error codes is reliable too... it's not often a function fails to return (unless you throw an exception, of course). 
I don't mean before main, I should have said beside main. I would not go to extreme to disallow static variable, Google allows POD static variables, I am not sure why they disallow static `vector&lt;T&gt;`.
What are the better style guides for modern C++? [LLVM's](http://llvm.org/docs/CodingStandards.html)?
Author here: as the next sentence says, I find it understandable that people may wish to avoid virtual calls in constructors. It's the two-stage init that's unacceptable. (and while I consider C++'s invariant-preserving approach to virtual calls in constructors to be the only rational design choice, contrary to Java or even D, I can't remember having to rely on it in practice, and I could live without it)
&gt; Author here: as the next sentence says, I find it understandable that people may wish to avoid virtual calls in constructors. It's the two-stage init that's unacceptable. Am I not reading the same article? Here's what I see: "I can understand Java programmers being confused about virtual functions in ctors..." That doesn't say the same thing at all. 
Sure, I'll try to rephrase that.
Looks great. Did not like the other approaches I found so far. The lambdas as regular function replacement are good enough for me. 
it doesn't matter either way - the documentation explains sufficiently and is easy to find, and that's more than enough!
If a person cannot get past his own programming zealotry and accept company programming standards, I wouldn't want them a part of the team. 
In bash you can set the environment variables for a particular program execution: CXXFLAGS="-std=c++1y" make With this command make will get the $CXXFLAGS value you set without changing your shell's environment.
I think you're right about the reasoning, but I'm sure there are good reasons behind all the decisions. The problem is, every removal of a feature always makes a large codebase simpler (by some measure). The problem with the lowest common denominator is that you miss out on a lot of useful stuff. As for the specifics, no exceptions is an easy one. You may not agree with their choice, but you have to acknowledge that there are valid arguments for maintainability and predictable performance for avoiding exceptions in the lowest common denominator camp. No exceptions =&gt; ctors cannot fail =&gt; no RAII. The prohibition against complex ctors may be a combination of that and the pre-c++11 lack of delegated ctors.
Thanks! Yeah, I think lambdas are a great substitute - most places where you use a function you can use a lambda instead! Although, if you really need functions you can always #include them in via a header, too :)
it's a different way to code, with RAII and smart pointers.
My $0.02: I've done a lot of work using this style guide religiously, largely because it was what my team came together over. It was also the most cogent guide we could find online that was more than merely prescriptive. It's exactness was the overall deciding factor. I hated using it at first. Overall, the guide is very regressive, and chops the legs off of C++ such that it's not much more than "C with namespaces and classes." You could even go as far as to say that most arguments in the guide reduce to "this isn't a problem in Java or Python because you can't make this mistake, so don't use feature X here at all." Ultimately, it keeps you from doing anything that would allow bugs to creep in by mistake or by misinterpretation, by keeping it all nice and simple. And this is where this style guide actually helps. The problem is that C++ provides almost *too much* leverage. Left unchecked, developers will likely use the language to its limits, which inevitably will confound other members of the team. While the program may be a masterpiece of template code, move semantics, and other concepts, it's now unmaintainable by anyone but the original author. Business wise, it's vastly preferable to have an uninspired piece of software if it means you can fix bugs while half your staff has the flu. Also, consider that the goals of the Google C++ style guide align incredibly well with Go. To me, this is a very salient case *against* using C++ if that guide is at all a good fit for new development. Go has a very tight language spec, and a (IMO) superior concurrency model that is easier to construct, reason about, and debug. And it's still a compiled language. Anyway, I'm proud to say that I have delivered excellent results, and relatively bug free code using this guide. If left to my own devices I probably would have used too much template magic and other mechanics that, while are all valid C++, would be harder to debug and understand by other members of my team. The resulting codebase is boring as all hell to read, but stable, reliable, and works incredibly well. The downside is that compared to conventional languages, the result still takes a long time to compile, is twice as much code as is needed (header files), and relies heavily on smart pointers to manage memory (may as well use a GC). Again, this is why I mentioned using Go earlier. One last thing: this guide does not stress the importance of "const correctness" in class construction. Add that to your work and you'll really have some solid code to rely upon. tl;dr: For new development, either forgo this guide completely, or just use Go. Otherwise, you'll just piss off experienced C++ developers by using this thing. Edit: I forgot to mention that the GSG has a *massive* blind spot for exception safety. Just because your code doesn't use exceptions, doesn't mean that the libraries you use don't throw. This includes the STL; the guide should steer you away from throwing variants of STL functions/methods, but it doesn't. So be on the lookout for coders not throwing try/catch around third party code, and refusing to use basic RAII (also not mentioned in the guide) to make things airtight. Either that, or just except that every exception-based runtime fault should halt the program, and that it's an acceptable failure mode (probably not).
Can you provide an example as to when you'd want to call a virtual function in a constructor? 90% of what I can find says "don't" as it will "ignore" the virtual and call the base class version. Are there examples where you'd want to call the base class version when constructing a child class? I understand why 2-level init is undesirable.
The only problem with lambdas is that you have to be careful about capturing and how that can easily back into multithreading issues. Everywhere in the C++ spec where "this works... except when" applies, the guide shoots it down without fail. Which is too bad since lambdas do a fantastic job of reducing line-count and increasing readability (provided you understand the new lambda syntax). I mentioned this elsewhere in the thread, but the overall tone of the guide is "Python/Java lets me to this without fail, so obviously the C++ feature here is broken."
IMO, this is *exactly* what happened. To me, Google C++ Style Guide almost reads like a Go v1.0 blueprint. The distinct lack of exceptions is a giveaway if you ask me.
Just to talk about wxWidgets... is it possible to create totally stand alone executables with it? With Qt for example you need to ship tons of (and very heavy) DLL files in Windows if you want to cope easily with LGPL license.
FWIW, I've had conversations with *very* capable engineers that need a few go-rounds of code review before the concept of "value semantics vs reference semantics" really starts to make sense. Then *I* start to become aware of how insane that all is - that any line of code does very different things based on the implementation style of the type - despite it being a very useful mode of optimization. Forcing reference-semantics, with a handful of exceptions, and then backing that up with smart pointers makes for some very rock solid code, even if it isn't exactly idiomatic.
And error codes add significant code bloat over exceptions when you want to unify your error handling. Everything is a trade off.
The GSG prohibits the use of exceptions in new code. So you use C-style error returns instead. Coupled with aggressive logging, you completely get away from Java-style "throw and trace" debug handling, and simply follow the application log instead.
What definition of flow control are we using? I was assuming the "I am currently at X but need to execute Y". While losing you could apply that to exceptions, they are usually considered "X bad thing happened, deal with it uniquely from my usual return location". Put another way goto is explicit control flow, exceptions are implicit control flow.
But getter/setter is the only way to create read only properties in C++. .NET not withstanding of course. Also, it enables a whole host of optimizations. For example, it's preferable to return a "const std::string&amp;" instead of the internal, non-const "std::string". You need a function to do that conversion; there's no other way to do it.
&gt; Exceptions could be handled identically to return value errors if you liked. Except in a bigger/more complex codebase you never know which exception you'll get and you have no idea what can actually throw. So no, you can't really handle them like return values. Also, with return values you can always slap the return-value-unused attribute to get a warning every time you forget to handle it. No such luxury with exceptions.
exactly. so i dint really understand his critic on getter/setters.
I've keep this book titled '[The C++ Standard Library](http://www.cppstdlib.com/)' at my side while coding. I rarely need to search references on the internet.
I don't agree with this. Exceptions can be used well and can produce error handling code as nice as anything. The problems occur when they're not used correctly. Here's my go-to source for the right way to use exceptions: http://exceptionsafecode.com/ To give you an idea of the right way, here's what most people that don't like exceptions think is the right way to deal with them: - Carefully check return values/error codes to detect and correct problems. - Identify functions that can throw and think about what to do when they fail - Use exception speciﬁcations so the compiler can help create safe code. - Use try/catch blocks to control code ﬂow This is actually a wrong way (slide 36). The right way is easier and more reliable both than the above and than using error flags: It's basically to use RAII for all clean-up and to use try/catch only in a couple specific places (slide 180). The code that results is a lot easier to read than having status checks everywhere as well. Slides: http://exceptionsafecode.com/slides/esc.pdf I agree with Dave Abrams on the subject: &gt; Exception-handling isn’t hard. Error-handling is hard. Exceptions make it easier!
I never claimed there was one. You referred to using `make` this way as a "Nice little trick" but then you brought up a problem and indicated that you would rather rely on the command shell history. So I just commented on how to solve that one little problem you brought up. You can also write a makefile that contains only the flags (and not any dependencies or targets) and the OP's trick with `make` will still work but you won't have to bother with either shell history or the environment. Maybe that will make this trick more useful to you.
One of my favorites from LLVM coding standards is "Do not use RTTI or Exceptions" it violates “you only pay for what you use”.
LLVM style is in part like google's. No RTTI, no exceptions. I might agree on the no-RTTI part *specific* to how LLVM hierarchies are organized and used. That being said, I've worked on a llvm fork for 4 months now, and these two rules have been a pain the ***.
Love posts like that. Learned something new today.
can i join this circlejerk? 
This seems like the best encapsulation of Google’s C++ philosophy. If you’re goal is to use C++ in a large, variegated organization (arguably a more ‘real world’ use), Google’s style guide makes sense. If you go with “we’re all adults here”, you might want to look for a different code standard.
On top of that, it is 3 lines of code to grok vs about 100 lines of somewhat complex code. It is hard to argue that using a std library function to do explicitly what this function is trying to do is hard to understand. I think some people say "Nobody will understand this" they really mean "I don't understand this". The fact that the guy was a "mentor" gave him a superiority complex. The fact that he convinced the author that the rotate function was correct and doing everything right should have been more than enough to say "Well this is a huge improvement".
There are better ways to avoid potentially failing constructions than using an `Init` method though: - private constructor - public static method building the object and return an optional instance There, you're done.
Yes, the problem is that LLVM (and Clang) are constantly fighting to produce the slimmer binaries possible, and both RTTI and the Zero-Cost Exception Model are huge consumers of binary space :x Of course, for most of us, the concern is irrelevant.
well, while banning lambdas might seem stupid, they don't really do much more than what you could do with a functor, do they?
I thought it was Adobe, not Google.
"Implicit flow control" sounds like "invisible", which is precisely the point ofte Spolsky blog I linked earlier.
Of the...
I agree. I think style guidelines for large codebases should be narrow. The onus to justify using features with debatable value, or potential detriments, should be the developer's. Maybe more importantly though, if those things are so *critical* to writing quality software, arguments against it should probably be justified in those terms.
&gt; No such luxury with exceptions. Start with a no throw declaration and expand to include what you actually throw as you go. As long as you are handling it at every level it would not be too difficult to do. It just would go against the design point of exceptions so no one does it.
"huge" ... for certan values of "huge" yes (10%-17% according to some tests i saw on the internets).
We are talking about completely different things here. I am saying exceptions are great for handling unforeseeable problems that are best handled in a unified way. You are saying exceptions provide an invisible flow control mechanism. The entire point of a unified way is hiding the implementation detail which isn't material to how the method is written. As long as the exceptions are actually exceptional and handled correctly (note this means verifying your library calls properly) they shouldn't be a problem.
That reasoning is logically unsound: "don't do X because it does exactly what it's expected to do". (it's also incorrect to say "ignore"). But as I said just above, I don't have an example from personal experience.
The reasoning is that while it does exactly what it's expected per the standard, the standard is written from a compiler efficiency point of view vs a program logical consistency view. If you've gone through the trouble of defining a virtual function and putting an implementation in the child class you'd expect RTTI to apply and the virtual function to be called when the base class calls the virtual function. The constructor/destructor are exceptions to this because during construction and destruction the object is not fully constructed so you can't do the proper V-Table lookup without some really messy compiler logic that would kill performance. So the behavior is defined, but defined in a way that 90% of people who haven't studied compiler construction will forget in the moment: the safer thing is to avoid "Technically correct but logically unexpected" bugs by putting the guideline in "don't call virtual functions in Constructors/destructors". I can't think of a case where you'd clearly want the above behavior, but I can think of ways to code it if desired where the behavior is explicit and avoids virtuals in the constructor/destructor.
He now works at Adobe, but it was about his stint at Google.
A Google engineer has added a comment to the post that he'll talk about the reasoning behind the style guide at CppCon ([here](http://cppcon.org/program-preview-1-of-n-2014/), the one by Titus Winters). Edit: [link to the video of the talk](https://www.youtube.com/watch?v=NOCElcMcFik)
I dunno, it seems like a real brain bender to me: "Removes the top element and appends it to the end." Nope, still don't get it. I'm sure the loop they wrote to move the first to the last that covers the edge cases was far more clear.
Personally I prefer POCO C++ Libraries Coding Style Guide: http://pocoproject.org/blog/?p=7 Other than that, Joint Strike Fighter Air Vehicle C++ Coding Standards ([PDF](http://www.stroustrup.com/JSF-AV-rules.pdf)) also make more sense to me than Google's guide, especially given the context (e.g., I wouldn't always agree with "no exceptions" for general purpose programming, but in the context of strike fighter software development /* presumably hard real-time */ I'd say that's fair enough ;]). It's worth noting that, in contrast with some, um... _not as well thought out guides_, JSF AV C++ is actually very much fine with using templates (and not just for containers, like generics in other languages) and even *recommends* these for certain use cases: * In 3.1 Coupling &amp; Cohesion &gt; Source code should be developed as a set of modules as loosely coupled as is reasonably feasible. Note that generic programming (which requires the use of templates) allows source code to be written with loose coupling and without runtime overhead. * 4.10.9 Inheritance &gt; Class hierarchies are appropriate when run-time selection of implementation is required. If run-time resolution is not required, template parameterization should be considered (templates are better-behaved and faster than virtual functions). Finally, simple independent concepts should be expressed as concrete types. * 4.12 Templates &gt; Templates provide a powerful technique for creating families of functions or classes parameterized by type. As a result, generic components may be created that match corresponding hand-written versions in both size and performance [2]. Although template techniques have proven to be both powerful and expressive, it may be unclear when to prefer the use of templates over the use of inheritance. The following guidelines provided by Stroustrup[2], 13.8, offer advice in this regard: 1. Prefer a template over derived classes when run-time efficiency is at a premium. 2. Prefer derived classes over a template if adding new variants without recompilation is important. 3. Prefer a template over derived classes when no common base can be defined. 4. Prefer a template over derived classes when built-in types and structures with compatibility constraints are important. // On a side note, it can be quite amusing to find developers fretting about templates "complexity" in 2014 given that they're good (and simple) enough for military strike fighter software development (a field not exactly known for experimenting with fancy software features -- nor "regular" deployment conditions) -- makes one wonder what extremely sensitive applications they must be working on ;-) Other references: http://www.codingstandard.com/section/references/
The object's dtor isn't called, but the dtors for local variables and for child objects that have been constructed are, so if you are doing RAII correctly all resources held by the class will still be released. The problem of errors in dtors is trickier but one solution is to have a separate `close ()` method for handling the error. Then the dtor will release the resource (and ignore any errors) only if `close()` hasn't already been called. In practice, it's pretty rare to need to handle errors that occur while releasing resources, so in those cases you can just let the dtor take care of it, but in the other cases you can handle it by closing it explicitly.
I don't think they do anything that you couldn't do with a functor, apart from possibly providing more information to the compiler. But it could also be argued, that C++ doesn't do anything that you couldn't do with an assembler. (yes that was ad absurdum)
I keep cppreference bookmarked. I never need to search a book for information.
&gt;Of course, for most of us, the concern is irrelevant. C++'s willingness to cater to those that need correctness within tight bounds is a big reason for its success. Exceptions violate that rule, and if you can violate that rule why are you using C++?
If I went through the trouble of writing a class member function (virtual or not), I expect, as pre-condition, that class members are initialized and class invariants established. Why would anyone expect otherwise unless they come from Java or D or another language where that's the grim reality? As for the design decision behind this, see Stroustrup's http://www.stroustrup.com/bs_faq2.html#vcall &gt; It has been suggested that this rule is an implementation artifact. It is not so. In fact, it would be noticeably easier to implement the unsafe rule of calling virtual functions from constructors exactly as from other functions. However, that would imply that no virtual function could be written to rely on invariants established by base classes. That would be a terrible mess. 
I stand corrected on the Ctor/Dtor issue. (Shame on me, I should know better without looking it up!) As for close() - it usually isn't a problem for "intended observable behavior", but I've had it create pretty awkward code paths, and creates a diagnostic hole (unless you log inside the dtor, which is another can of worms). Besides, it's easy to forget the explicit close().
More like `std::size_t`.
Sir, I see what you did there
You're right, I was misremembering.
The sarcasm is strong in this thread, but I honestly would like to know where I can find another code standard as detailled as this one. Your link was eye-opening but I still can't write "adult" C++ without the feeling that I am screwing future maintainers over.
&gt; Exceptions violate that rule, and if you can violate that rule why are you using C++? There is more than one type of efficiency. Sometimes speed matters but a 50% overhead in executable size does not. (The last time I actually measured the difference for a large-scale application compiled with and without exceptions enabled, that 50% was about right, but that was several years ago so possibly the technology has improved since then.)
I don't even have it bookmarked. Google Search is my homepage, so I just "c++11 std::whatever" and cppreference is always the top (or 2-3 items down).
cppreference is a great quick reference. The book has more examples and explains how pieces of the library fit together. I go to cppreference to see how a function works. I use the book to see how a whole section of the library works, particularly C++11 additions like threading and chrono.
Yes, IMO the main difference between having a book and a website is that you can read a book cover to cover and learn all the c++ features the book has to offer and then you can use the book as a reference. But, you cannot read a website cover to cover. &gt; "nobody knows what rotate does" And this is the one reason why programmers don't even know about so many c++ standard library functions in the first place like std::rotate
All true. And I agree. The only thing C-style error handles have going for them is that they're verifiable at the call site. In a process where code changes are gated by code reviews, this is preferable, as it doesn't stress total knowledge of the system over inspecting deltas to the program. Exceptions, in contrast, require a *deep* understanding of the program in order to verify all the ramifications of any given exception flying up the stack. It can be the difference between looking at a .diff, and weeding through the entire codebase.
Yeah, he contributed to it.
That's a bit of a slippery slope. Using functors (or any of the other equivalents) vs using lambdas has a bit of a different cost delta than writing C++ vs writing everything in assembler.
Lambda functions are there for the human, not the compiler. Language features aren't about increasing language power anymore, they are about increasing expressiveness. 
It implies object envy. You shouldn't need stacks of getter and setter methods, your behaviors should be together with the state they require. 
Other than the linking issue is there a compelling reason to prefer WxWidgets over Qt these days? They're both old school style APIs. Only gtkmm tries to be modern. Not that I have any experience with gtkmm, either. Just curious if anyone had any informed thoughts on the Wx vs Qt vs gtkmm thing. I've only used Qt.
The spec for the C++ language is very comparable to other languages, its mainly the STL that comprises a large portion of the spec
Ok, I see your point. It does solve the problem with whole environment being affected. It just i think it doesn't play well with OP's trick. In order not to type flags over and over again you need them stored somewhere. Makefile or shell var or some other file? i would go for makefile if i had to (if not i would let the shell store the command in its history, as it is not something too critical and it is ok if it disappears ). OP's trick is a no-makefile trick. If i don't have to worry about flags at all, then calling compiler directly would still be preferred, as it is more flexible. Again, just my opinion.
How do exceptions balloon the size so much? I'm having trouble wrapping my head around that.
There's a transition plan?
&gt; getter/setter is the only way to create read only properties in C++ Not true. I've gotten a lot of good mileage out of this paradigm. It provides read-only attributes with no function-call overhead. The reference gets optimized away by the compiler class MyClass { public: type read_write_property; const type&amp; read_only_property; MyClass(): read_only_property(read_only_property_backing) {} private: type read_only_property_backing; }; MyClass m; m.read_write_property = 1; //fine m.read_only_property = 1; //compile error const m.read_only_property_backing = 1; //compile error private
Requiring all references be const is dumb. Discouraging move-semantics (value types) is dumb.
Can you shed some quick light onto what happened here? 
We never confirmed exactly what was using all the space. The application in question didn't use exceptions and the experiment was to see whether disabling them in the compiler would be a useful benefit, though I don't think anyone was expecting that much of a difference. With so much space used anyway, my guess is that with exceptions enabled the compiler was pessimistically setting up some sort of jump tables and stack unwinding code for every scope where there were automatic variables, or something along those lines.
The runtime costs of exceptions depends on platform. Windows' Structured Exceptions are (a) defaulted ON, (b) add a ton of overhead [1]. More reasonable implementations just look up the instruction pointer in a table for each stack-level and run the appropriate code. But, ugh, that's not what I wanted to post about. 50% overhead in code size can be negligible (if you don't load those segments) or awful (if you do, for the icache pressure). [1] I noticed this and didn't bother investigating deeply. Someone please tell me they don't do this anymore, or that I didn't understand what I was seeing. 
Googling only "std::whatever" does it pretty much all the time for me.
That would try to run the command `CXXFLAGS`, which is very unlikely to exist. You can just use `$CXXFLAGS`, or if you need to be more clear about where the name ends you can use `${CXXFLAGS}`
I haven't checked how things work with VC++ recently, but I'm afraid you were probably right about SEH. It was very odd how they kind of tried to merge Windows and C++ exceptions. The test I mentioned was indeed done with VC++, so perhaps SEH is the explanation for the surprisingly high overhead we observed.
**edit:** this is general commentary, not about the specific article Most of these megalithic "style" guides are ham-fisted attempts to save terrible coders from themselves. I'd liken it to saying "Sure you can drive a car - but it must have padding in all directions, speed limited to 5mph, and no driving on public roads!" Two pages is about the right length for a style guide IMHO; the main purpose of it should be to ensure that different developers working on the same project can develop together without fouling up the source control system. 
&gt; exceptions are invisible and create many exit points I've never understood this argument. To me it seems to be a benefit as it segregates two modes of thought. In general I want to reason about the normal operation of my code. This is shown explicitly and is uncluttered by issues relating to exceptions. On the other hand, sometimes I need to be able to reason about handling exceptions. This is by its nature non-local and related to the semantics of the call graph. Exceptions seem to be the most appropriate mechanism for this. 
See [this Stackoverflow Q&amp;A](http://stackoverflow.com/q/23579832/819272). Boost.Range lets you write this in a fairly straightfoward way.
Sure, but they do it significantly clearer and a lot less verbose in the right use cases.
I admit that there are gaps in my knowledge of the c++ stdlib, but in my line of work (game development) we use surprisingly little of it. Engines tend to have their own datatypes and algorithms to take better advantage of the different architectures we're working on, where the standard library functions can be a bit too generic. I don't like this much, but am not in a position to change it
Well the downside of QT is that it's more of application framework than a GUI library. You need to run a custom preprocessor on your source, it has it's own build system, IDE, etc. This is both good and bad. On one hand it's a complete system and makes your application really portable but on the other hand it works it way into almost every part of your code. Sometimes you want something more contained if a GUI is only one part of a large application. gtkmm doesn't use native widgets unless you are on GTK-based Linux so I don't think it's really in the same space.
Well then, a lot less extreme example: Why use std::find (or other algorithms) instead of writing raw loops?
Okay, that's pretty clever. I didn't notice how references make that possible. Very nice.
Borland C++ &gt; Visual C++ &gt; g++ Also I make better decisions than everyone at Google and I am too good to work there
&gt; &gt; (GSG) "Provide a copy constructor and assignment operator only when necessary. Otherwise, disable them with DISALLOW_COPY_AND_ASSIGN." &gt; &gt; &gt; &gt; This is absolutely correct; those operations should either exist or be disabled. In fact, very modern style is the "Rule of Zero" - objects should have correct copy semantics without you even having to write a user-defined copy-constructor etc. The way I understand this rule, you either provide your own copy and assign ops (breaking the rule of 0) or disallow copy and assign altogether. 
You can call private member functions of the class. Can't do that with functors.
 friend
Right. Let's pollute the class definition with friendship declaration for every possible functor I may need. Yay.
I am not saying it is a good idea. Just that it is a possibility. Also IIRC you should be able to define the functor inside the class definition and giving it access this way.
Qt doesn't use native widgets either.
You can always do set(CMAKE_CXX_FLAGS "-std=c++11") on LLVM and GCC
No; it says that your object should have zero user-defined copy constructors etc. but still should have correct value semantics.
I agree about usage of custom algorithm for game development for performance reasons. But, the way I use the C++ stdlib is that while rapid prototyping I use the stdlib to get quickly started and later on use the custom data structures and algorithms to fine tune things. I know this doesn't applies everywhere, but whenever it does apply, it works great.
How is this different from Cap'n Proto?
See the Benchmarks section in the docs: &gt; Cap'n'Proto promises to reduce Protocol Buffers much like FlatBuffers does, though with a more complicated binary encoding and less flexibility (no optional fields to allow deprecating fields or serializing with missing fields for which defaults exist). It currently also isn't fully cross-platform portable (lack of VS support).
If you take in the context, you wouldn't be so sure. Throughout the whole guide, gsg all but forbids using value semantics. In the one place where they allow it, they say that if using value semantics is necessary, you should provide copy and assign operators. Otherwise, explicitly disable copying.
I remember looking at disassemblies and seeing compiler-added calls at the start and end of /every function/. Ugh. F'in windows man, F'in windows.
*Disclaimer: I am not following too closely the Windows platform though, so take this with a grain of salt.* If I remember correctly, the VC++ ABI between 32 bits and 64 bits was completely overhauled. In 32 bits VC++ did NOT use Zero-Cost Model Exceptions, but instead would setup/tear-down something at each `try/catch` scope; for 64 bits though, they jumped the gun.
Thanks! Yes, the compilation speed is probably the worst part of working with C++! cppsh does offset this a bit in that it only recompiles scripts when they actually change (it keeps a cache of the built binaries), but it's still noticeable when the script does change! Hmm... I'd never thought of that - that could be a cool idea for future!
Aren't functors created by overloading `operator()`? The style guide says you should avoid overloads.
Scientific computing. I work for NASA and I work almost exclusively in c++ with some python to support the astronomers. Mind you I am coming from a scientific background (B.S. in physics) so I feel challenged and enriched by the work: Building tools to analyze scientific data for a specific high energy mission. You may not be enamored with our legacy restrictions.
The lack of VS support is due to the use of some C++11 constructs that VS does not support; hopefully with the next release (VC++14) the situation should improve here. Regarding the binary encoding/flexibility, there is a bit of a debate between the authors over on HN. I personally find Cap'n'Proto plenty flexible, and I could care less about the binary encoding: I just want it to be fast (both for reading and writing), so I'd rather have a benchmark than some words.
Why do all of these tools (FlatBuffers, Cap'n'proto, ..) all need external files for configuration. Why dont they only rely on code you write? For myself i am writing a little serialization library (which is not yet finished) that only relies on cpp code like: class foo { public: foo(int* i, float f, std::shared_ptr&lt;int&gt; si, std::string s) : _i{i}, _f{f}, _si{si}, _s{s} {} //foo(void) {} private: int* _i; float _f; std::shared_ptr&lt;int&gt; _si; std::string _s; public: int* get_i(void) const { return _i; } float get_f(void) const { return _f; } const std::shared_ptr&lt;int&gt; get_si(void) const { return _si; } std::string get_s(void) const { return _s; } }; class foobar { public: foobar(std::shared_ptr&lt;foo&gt; f) : _foo{f} {} private: std::shared_ptr&lt;foo&gt; _foo; public: const std::shared_ptr&lt;foo&gt; get_foo(void) const { return _foo; } }; int main(int, char**) { std::stringstream testStream; outObjectStream out{testStream}; inObjectStream in{testStream}; auto d_foo = make_descriptor(make_constructor&lt;foo&gt;(pointer(&amp;foo::get_i), value(&amp;foo::get_f), shared_pointer(&amp;foo::get_si), value(&amp;foo::get_s))); auto d_foobar = make_descriptor(make_constructor&lt;foobar&gt;(shared_pointer(&amp;foobar::get_foo, d_foo))); foo bar{new int(42), 0.1f, std::shared_ptr&lt;int&gt;(new int(23)), "test 323"}; foobar fbar{std::shared_ptr&lt;foo&gt;(new foo(new int(42), .1f, std::shared_ptr&lt;int&gt;(new int(23)), "test 323"))}; // write d_foobar.write(out, fbar); // read from a stream foobar bar2 = d_foobar.read(in); } It basically depends on you creating a descriptor which describes the object. The make_constructor will create an object that stores all the data the constructors needs. It also reads all the data and than constructs directly an object of that type. I am planning on adding additional elements that can be set via setter after object construction. Also the ability to save class hiearchies is planned (and in a protype worked). Out of curiosity, would there be interest in such a tool that only relies on cpp code - or do you all prefer the way over an external tool?
I would agree `std::find` should *generally* be preferred to a hand-written algorithm. However, unlike lambdas, I think it would be hard to argue that `std::find` is syntactic sugar; especially in a code review. Honestly, I think a few people might be missing the real point of a style guide; including the original author. The overall goal is really to maximize the benefit of using a language across a group of developers. It isn't to maximize the benefit locally.
A related option is using compiler plugins to generate the serialization config. C++ ODB does this: #pragma db object class person { friend class odb::access; #pragma db id string email_; string name_; unsigned short age_; }; I know it's an added bit of non-standard complexity to use gcc plugins, but I find it less annoying than a whole custom configuration file and build step.
You can probably use dlopen / dlsym to load ELF executables as well. I'm not really sure how you would then implement the calling of functions defined by your application though. You would have to somehow tell the linker about those. Also, if you're calling dlsym, don't forget to put names through the C++ name mangler (look at c++filt in binutils! It might help).
Out of curiosity, how is the performance of using these functions compared to using more traditional C++ approaches?
I suspect we haven't updated the external guide in a while, then.
I agree that discouraging value semantics is bad. However it is very true that you should either do one of those two things (support value semantics, or disable copying). Doing neither of them would be very bad. 
Can anyone point me to some links about that hourglass pattern? I've been trying to figure out what it is but I couldn't.
cppreference site design makes my eyes bleed. cplusplus much easier and understandable
Why all the downvotes?
This is the solution that I want to avoid (and I'm currently using).
Seems quite neat actually. Though it does make the object bigger.
Looks good but if you don't add an assignment operator the class will be non-copyable. Add something like this to fix that: MyClass&amp; operator= (const MyClass&amp; mc) { this-&gt;foo = mc.foo; this-&gt;bar_backing = mc.bar_backing; return *this; } 
Using a reference to an uninitialized member is not a problem in this case, however. At least no more than this code is: int x; int&amp; y = x; x = 10; What would happen at this point if we said: cout &lt;&lt; y &lt;&lt; endl; Is it `10` guaranteed or `10` only by circumstance of the compiler, platform, and alignment of the moon? The question here is whether the variable `bar_backing` at the point of initialization of `bar` has a memory address or not. This can easily be determined by changing the type of `bar` to `int&amp;` since that may only be bound to an l-value. If it then works, it is because `bar_backing` is an l-value at the time that `bar` is initialized implying that `bar_backing` has a valid address in memory. Which really changes the question to: When one declares `MyClass obj` what is really happening here? Is a memory chunk able to fit `sizeof(MyClass)` immediately found and allocated, such that `&amp;(obj.bar_backing)` would return a completely valid memory address to an `int`? I think you'll see that indeed, the entire memory block is allocated at once and that using the address of an uninitialized member is perfectly fine - but dereferencing it is nonsense.
&gt; Using a reference to an uninitialized member is not a problem in this case, however. No this is different but perhaps I used the wrong term. A better term is that referencing an object that has not even been constructed and doesn't even exist yet is undefined behavior. So in your example, it would be as if y was declared before x, and was assigned to the potential future location of x even though x doesn't yet exist. Within an initializer list, all members get constructed in the order they're declared. Before they are constructed, for all intent and purposes they don't even exist. So it's not simply that they don't yet have a value, it's that it doesn't even exist at that point. That is the undefined behavior.
FYI, construction order follows declaration order in the class, not mem-init-list order. (Which is why mem-init-list order should always match the class declaration order; some compilers warn.)
Many lot of the bans like that are because new features need to be used carefully. They would rather ban use of new features first, find out what the pitfalls and gotchas are, then gradually allow them over time with usage guidelines instead of just letting everyone go nuts and end up with a bunch of ticking timebombs because people didn't know better how to use the features. Also in many cases Google has code internally that they wrote to solve a problem that is already established in their code base, and it wasn't until later that the standard library included its own solution to the problem. They would rather keep around what's established and what they know works instead of pull in code that might not interoperate well with their established libraries. It's more important to have solid code than to have cutting edge features. 
My understanding is that the lack of exceptions is a historical artifact more than anything else. It would just be too much work to retrofit all the code to be exception safe. 
&gt; Discouraging move-semantics (value types) is dumb. It's a new feature. They'll start allowing it eventually, once the best practices for how to use it can be established. 
I really like both Visual Studio and Qt Creator. Wondering how this stacks up.
Wondering what is meant by gtkmm tries to be modern?
Lol'ing at all the Java noobs using lazy loaded getters (mainly because of performance reasons) being the real reason we use getters/setters. I've been saying for years structs are just fine. I use them for embedded systems and the produced assembly is actually legible in comparison. + If you really need a fancy getter/setter in the future, there are ways of using templates that won't break the interface later.
Maybe pgina can help you: http://pgina.org/
I can sort of understand the exceptions thing. It only takes one person to write exception unsafe code and you've got resource leaks. Of course, this can be fixed by the alternate style guide of RAII everything.
And all of a sudden, you don't have `operator=` any more.
I remember [this](http://www.reddit.com/r/cpp/comments/24ipsq/a_c_error_handling_style_that_plays_nice_with_c/) from last month.
Totally agree. The pain comes again and for what? To create apps that you can't even resize? :)
We use getters and setters not just to make properties "read only", but to be able to modify the getter/setter if necessary. It is useful to embed additional code, for debugging or logging, every time an access to the variable is made. Your example poorly represents a real case, because it assumes that every value of 'foo' satisfies the invariants of MyClass. This is seldom the case. (And you pay for an additional pointer member inside your class).
Yeah thats not true. You can verify it in the assembly but the compiler always optimizes it out.
&gt;In all the examples where I actually use this I already have the big three defined for the class for other reasons. However, since initialization goes in declaration order it's fairly trivial to move the private backing above the public part and it should no longer be undefined behavior. &gt;And writing custom constructors for these would negate any advantage of just writing a setter. Anytime I've actually had to use this I've needed the big 3 to be defined anyway for other members, so its not any additional overhead. &gt;And if you're worried about function call overhead, then inline the definition of the setter with your class. Also the semantics.
&gt;We use getters and setters not just to make properties "read only", but to be able to modify the getter/setter if necessary. It is useful to embed additional code, for debugging or logging, every time an access to the variable is made. This is absolutely true, but I've so rarely seen this put into practice in most codebases I've seen that it makes the semantic trade-off worth it imho.
"We have converted most of the CRT sources to compile as C++, enabling us to replace many ugly C idioms with simpler and more advanced C++ constructs. " Awww yiss
&gt; but I've so rarely seen this put into practice in most codebases This was even pointed out in a Going Native talk, but unfortunately there are so many can can't remember which. I do remember however that it was a claim that went unchallenged by the whole audience. That anecdotally suggests many agreed with it.
The [STLSoft Properties Library](http://www.stlsoft.org/doc-1.9/group__group____library____properties.html) makes switching a public variable to a getter/setter property almost seamless*. *Implicit conversions will be required to be made explicit.
Wx and Qt are loaded with mandatory classes and macros and naming conventions that essentially exist to work around limitations of C++ as it existed in the 90s. Many of the design choices would make no sense in the context of modern C++. As far as I know gtkmm is the only open source gui api that makes a point of avoiding historical cruft and leveraging standard modern standard C++ as much as possible.
I don't know why people discourage themselves from learning COM+, I never really had any trouble with using the COM+ API in C\C++. Sure it takes me a half minute longer to think about things then straight x11 but most libraries are like that for me anyway. I understand why it takes a huge hit for being platform dependent without adding much in the way of convenience but it's worth learning for dedicated platform developers or system admins like myself because it fills in some holes for the OS features that the straight WinAPI doesn't allow you direct or convenient access to. If you already know vbscript or powershell you're about a third of the way there anyway.
Sounds like a good question for /r/cpp_questions .
Download gVim and start using it :) it will take a while though... additionally can use plugins like https://github.com/Valloric/YouCompleteMe to boost your experience. Additionally see: https://stackoverflow.com/questions/4237817/help-with-configuring-vim-for-c To learn vim run the vimtutor, http://www.openvim.com/tutorial.html and/or http://vim-adventures.com/ And don't forget /r/vim ;) Good luck! EDIT: If you're going to use a lot of plugins use a plugin manager see: http://www.openlogic.com/wazi/bid/262302/Three-tools-for-managing-Vim-plugins
I've now reviewed some plugins and whatnot and to be honest I still feel that the experience is inferior to that of Eclipse.
The switch from an IDE to vim is always rough. I spent hours over hours trying to get autocomplete to work like I was used to (from Qt creator etc.) After giving up multiple times, I finally just forced myself to do everything in vim for 2 weeks. Latex, c++, rust, config files etc. It worked for me and I'm a happy vim user now. One thing I noticed is that a lot of the things I missed about IDE's are quite unimportant to me now. For example, I turn off autocomplete even though the clang plugin is quite awesome. Editing in vim is just so damn fast if done right. So what I would suggest is to try a fairly vanilla vim withough any plugins. Only a few line .vimrc, nothing pulled off the internet. Learn the editor, don't worry about all that fancy stuff. Another option is to just simply stay with Eclipse and not look back. Even though it might never let you go...
Linux. Don't worry about debugging, file browsing, project managment, auto complete and all that. There are solutions for all of these things, but again, vim imho is not about it's plugins. I loved the MVS c++ debugger, meanwhile I can't remember the last time I've used a debugger.
I believe an_underscored_type_or_variable_name is easier to read than a ACamelCasedTypeOrVariableName. Being pretty is a subjective criterion though. 
Still, its what I need.
or maybe [/r/vim](http://www.reddit.com/r/vim) :)
One question to ask is why do you want to switch? Is there any functionality you miss in eclipse, or do you just want to try something different? If you want to try out vim I second theslimde's suggestion to try a vanilla setup first to learn the editor before installing any plugins. Also since you're on OSX, have you tried XCode? 
That's nonsense. There's a fair bit of industry specific knowledge that comes with HFT (the finance stuff), but once you get over that you learn *a lot* about how the language and your whole computing system hang together purely because you're trying to squeeze every last nanosecond out of the code. Another benefit you won't find in most other areas of finance is that dev is regarded as more or less a front office function, rather than a cost centre, because the trading infrastructure is just so integral to the business. This is a ~good thing~
I need step-by-step debugging, and can't get it to work with Eclipse. Xcode has no support for refactoring in C++.
Personally I find that it's just as easy to open GDB in a separate tmux pane.
On one hand, it is great that the non-standard behaviour of `%s` in wide character functions is finally, finally, fixed. OTOH it's mind boggling just how much code is going to be _silently_ broken by this change. I must admit that personally I wouldn't have had the courage (or recklessness?) to do what they did, especially without at least a single version in which the correct behaviour would be optionally available but still off by default, i.e. as if `_CRT_STDIO_LEGACY_WIDE_SPECIFIERS` were defined.
They already gave the reason for why, introducing a bunch of conditionals and piling fixes and patches over the old code base put them where they are right now. It's not a silent thing either, they clearly say what is going to happen and it will only happen when you compile against the new runtime, it's not like the rollout will suddenly break every single app that depends on the behaviour. You as a developer now have to fix your code, or stick with the old build environment, it's easy as that. I for one welcome this "drastic" change, it's about time that the CRT is getting cleaned up. They want to start with a clean slate and guarantee compatibility, they now have the chance to just go ahead and put together all those loose ends.
Oh crap, I really hope that in a few months when \ if we migrate I will remember this ... 
Those are not mailing lists, they are usenet newsgroups. You need to use a news client or google groups interface. Usenet is an ancient protocol (NNTP) from the good old days before everything was some stupid, gamified website full of power-hungry mods. It used to come standard with any ISP but now pretty much none offer access.
I miss those days.
Oh. Thank you man. So except for google groups interface, I have no way to have them delivered to my inbox, correct?
Only if you don't use C++/CX.
Specially given the keyboard navigation capabilities and thread management of Usenet clients. Still enjoy using it .
Yeah, this is going to be a fun migration. I don't think I've ever actually seen `%ls` used, which suggests that nearly every *wprintf call with a string parameter in existence needs to be updated... Sure would be nice if VC++ had warnings for incorrect format strings like gcc.
tbh i would prefer range way mentioned below since i find it more clean to have copy/transform separated from if part, but for oldSTL this might be a good thing before Eric Niebler makes his contribution to std :) 
All they have to do is implement format string checking.
Me too. I hate the google interface so have pretty much discontinued usenet. It's unfortunate because that's where a lot of the best coders collaborate. Sutter, Meyers, and yes, even Bjarne. Not like it's some incredible nightmare to maintain and set up either. An ISP could do it easy but none of them do. Really sucks. Was probably killed because of the alt.binary groups. Easy enough to block those but maybe they were too scurred of pirates to think clearly. All the sadder because NNTP is by today's warez standards a really shitty way to go. Was great as a BBS though.
It'd be an idea for Microsoft to post a script for checking whether a project has any format strings that could be problematic. Or include it in Code Analysis.
You laugh, but check out libpng. `setjmp` is how you "catch" "exceptions".
Yeah, this article is silly. You can do all of this so much easier with cx. 
I saw below that you don't like munging the CXX_FLAGS, but are you opposed to add_compile_options or target_compile_options?
is this for real?
This is a question for /r/cpp_questions, but cppcms is a framework that makes creating a website with C++ easier. I would recommend against using C++ for websites, unless you have a good reason to use it, or this is just for fun. If you wanted to use CGI, here is a basic tutorial: http://www.tutorialspoint.com/cplusplus/cpp_web_programming.htm
I appreciate it. I will repost there and delete this. 
I am unable to download the parsertl from http://www.benhanson.net/cpp/parsertl/Zip/parsertl.zip 
The link should work OK now.
maybe some initializer_list would make the thing more readable.
I mean, it works pretty well. It's weird, but it works.
This is a pretty bad article on timing. Beware. &gt;The real problem is that the function is affected by discontinuous jumps (for example: a manual time adjustment) and may be updated in ticks. Oh and it's not actually very precise. It's very precise, actually. If you aren't writing kernel code there's little point to having nanosecond resolution given you're probably also not using real-time scheduling and even then it's generally not easy to stay in the nanosecond range. Even zero-wait yields to epoll_wait on a dyntick-configured Linux box with no other processes running generally only puts you in the tens of microseconds range for average hardware. &gt;There is a user-mode accessible instruction, rdtsc (read timestamp counter), which looks like what we need. On Intel, the behavior is reliable as stated in the documentation: RDTSC is unusable for this kind of timing unless you're the kernel (there's no way to use it properly for this kind of timing from user space on any of these operating systems) even on Intel systems as it's affected by all sorts of things and should never be considered. There are many ways it can skew or appear to run backwards and it's worth grepping for rdtsc in Linux's source to see the effort required to correctly use it as a clock source. There are much better alternatives today for the kernel to use and it's only used as a fallback. &gt;Two remote servers exchanging information will anyway hardly be time synchronized at a microsecond precision Servers will be. This is pretty normal today thanks to NTP and GPS. &gt;The function you are looking for is QueryPerformanceCounter It has no drift correction so that 'time since start' you get via subtracting from a zero time is going to get increasingly inaccurate quickly. It's not meant to double as an epoch timer so isn't suitable for that "two remote servers" case. &gt;(which calls rdtsc, just so you know) They surely switched to HPET/ACPI many years ago. &gt;The clock_gettime() function is what we need. If you're in a hurry, you can use the CLOCK_REALTIME_PRECISE id which will return the local time. If you want something with stronger guarantees, you need to "translate" the Windows code. Oh God why. Why would you attempt to use the non-monotonic wall-clock timer as the base offset of a monotonic timer? And why would you use a clock source that is drift-corrected on Linux and then a clock source that is not drift-corrected on Windows to implement the same API? And why would you base a "portable" API on 100ns intervals instead of 1ns intervals? And why only on one of the timing functions and not the other? &gt;Attention to details like this is what makes the difference between working and rock-solid software. #frencharrogance \*facepalm\*
Trying to think of what I should do next. This was good practice with JSON, but now I'm thinking, what if there was a way to document the entire structure of C and C++? Eventually, I'd like to do this with other languages, namely Python, Ruby, JavaScript (especially jQuery, Node.js, and Express.js libraries). For the second level, I was thinking adding another array that was divided int several arrays: dependencies (pointers to other header files), macros, variables, functions, operators, classes (which would also indicate the parent classees, variables, functions, operators, etc.), etc. At any rate, it was something worth kicking around.
I'm only opposed to pass arguments that are not cross-plataform. The main reason why I use CMake is cross-plataform support. `CXX_FLAGS` is interpreted diferently in different compilers.
Agreed. I use it for the same reason. However, until such abstraction exists, it's trivially easy to wrap the platform-specific piece in a conditional block. That is, after all, how a large percentage of the CMake-provided modules work at the bottom layer.
&gt; It's very precise, actually Milliseconds is not enough. Try sorting a log file when 20 items in a row have the same millisecond timestamp. And realtime audio processing ideally needs a resolution as high as 10 microseconds which is basically impossible on Windows without QueryPerformanceCounter (although relatively easy on most other platforms). It's not for scheduling (you certainly don't get called for every sample) but it helps greatly if you know precisely how many samples to generate each time rather than maintaining an oversized buffer. The author doesn't seem to know about std::chrono::steady_clock which is guaranteed to be monotonically increasing (independent of the user time) and is nanosecond resolution on most Unix/Linux/Mac platforms. It's millisecond on Windows, which, yeah requires QueryPerformanceCounter.
I think it's already implemented in the VC static analyzer? Or is it implemented in cppcheck and I'm getting the two mixed up?
Why not put it on github/bitbucket?
&gt;However, this approach has the drawback of awkward syntax compared to just directly accessing properties of an object, as well as the potential to have function call-overhead. There is no function call-over head, getters/setters/small functions will more than likely be inlined, if optimisation is turned on. Also read-only "attributes" are not always defined as variables within a class. If you want to be strictly OOP-ey about it, you should declare all your member variables private and only use functions to retrieve information about the object. As your private member variables could change with the implementation of the class. e.g. you could desribe a circle by it's radius, area or diameter; but you only need to store one of these properties; but in the interface there should be functions that can retrieve either of these (e.g. circle.area(), circle.radius(), circle.diameter()). Sure, this is not as "neat" as C# properties (which I assume is where you got this idea from), but it has the same effect; only C# provides syntactic sugar.
&gt; No matter what resolution of timer you use that's fragile design This. When he said &gt; I know what you're thinking. You think you have a good idea and you could "make it work", but I'll share a secret with you: you won't. I thought: "No, I have no idea to make it work, because it is basically impossible"
It will be interesting to hear that talk for writing/maintaining legacy software. Although, I doubt that this will change the general opinion that Google is too big and unskillful to change bad habits. C++ was slow on improvements and now the speed is increased too much for the company to hold. C++ has never been their priority (no big company ever have). This talk by Titus Winters is (will be) far too late to protect Google's decisions.
I don't really understand why you've done this. I've worked with JSON quite a bit, but now that I think about it I don't think I've ever written that much of it out myself. It's for computers to read and write, it just has the side benefit of being human-rewriteable too. Writing a Json library in c++ would, in my opinion, be a far better way to learn something about both. Also I'm not quite sure JSON is going to work well to describe the C++ grammar.
Previews &amp; more information: - http://books.google.com/books?id=XpyJAwAAQBAJ - http://my.safaribooksonline.com/9781430266679 - http://www.springer.com/computer/book/978-1-4302-6667-9 - http://link.springer.com/book/10.1007%2F978-1-4302-6668-6 After a brief examination, looks interesting from a library-builder perspective, and seems to cover material not commonly found in the "regular" programming books (for instance, chapters 6-11 are usually just briefly mentioned or covered in one chapter; EDIT: Appendix A, Hands-on Analysis of Linker-Loader Coordination, is available online: [(ZIP file)](http://www.apress.com/downloadable/download/sample/sample_id/1544/)). // BTW, Chapter 1 refers to the [following post](http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/), which also looks interesting. Thoughts?
I was a little skeptical but this actually looks like an interesting read. Gonna use my safari account to read, thanks.
Incidentally, I am using C++ in a large organization (the MLoC counts I've seen thrown around by googlers are significantly smaller than ours). Still, Google's style guide makes no sense. Thanks for redditing my article, by the way, all that attention was fun.
The compile/build complexity on C++ is one of the weakness of the language, hope the modules proposal can land to improve this part.
&gt; People often take it for granted that their platform's implementation of IOStreams does a good job at sequential reads and writes. I guess it depends on what circles you travel in. Most people I know take it for granted that IOStreams is horribly inefficient.
Use SSDs. *scnr* *trollcomment*
Actually, both machines I evaluated the benchmarks on had SSDs -- one connected via SATA, and the other attached directly to the PCIe bus. "Drive" is misleading, but I'm not sure what a better word to use would be.
Everyone starts with no experience...
&gt; Being a C++ programmer implies that, at the very least, you care about the performance Welp. I use Qt and I need to use C++ for ABI reasons (link' against other libraries), I suspect my code runs quite slow :-) 
FYI, I made a device that does high throughput image acquisition and we have a persistent problem with an effect called the SSD write cliff. In our system level design we assume our drive is half the rated speed. http://regmedia.co.uk/2011/04/07/ssd_write_drop_off.jpg
I think this is a C++ board? :-)
I have been using the std::chrono method and made ~1 ms precision with no problems on any of my target systems. Querying the timer should be accurate, the problem is something like `Sleep` which is not guaranteed because Windows isn't a realtime operating system. Nevertheless with multimedia timers you can get &lt;1 ms jitter.
I saw this on the isocpp feed and I've been looking forward to it ever since.
Does this match up with (for example) Anandtech's long-running benchmarks?
The "strength of C" is that it attempts to closely represent hardware capability. Other "high level" languages attempt to easily represent ideas the way a human would imagine them, whether they are easily represented in hardware or not. C shows you the hardware, which is good for speed, but requires the programmer to do extra work.
Just use a db? With berkelydb or firebird-embedded and so forth I would assume the engines are already tuned to handle typical usage patterns as fast as feasible.
You should have a look at http://kentonv.github.io/capnproto/
My software writes 2TB of RAID0 SSD from start to finish. Most benchmarks do a few GB on an empty drive. I don't now about Anadtech, but you can see the effect using CrystalMark on Windows.
steady_clock isn't guaranteed to be monotonic, you still need know your platform or check is_steady. Until gcc 4.8 it wasn't monotonic in libstdc++.
You wouldn't happen to be making a high speed camera?
Ever looked at [TPIE](http://www.madalgo.au.dk/tpie/)?
Doesn't look like it? They don't show much they wrote. As the graph from NIST shows the speed changes depending on the storage.
No. Speed meets specs at the start but as we fill the drive, the realized write speed goes down, like the graph I linked to.
"As we fill the drive" -- the graph doesn't show any kind of fill rates, what fill rates are we talking about achieving after 10 minutes (where the cliff hits?) The graph seems to suggest that the write cliff is time-based rather than dependent on fillrate.
Interesting. What causes this?
I looked at it briefly and while it might be interesting, it doesn't seem to have to do much with neither compiling nor C or C++. This is mostly a book about linking, not compiling, and particularly about shared libraries. And while I hesitate to state anything without actually reading the book, on the Linux side, I didn't see anything in the book not described in the well-known [Ulrich Drepper's article](http://www.akkadia.org/drepper/dsohowto.pdf). And Windows treatment seems to be quite superficial.
Please please please don't go all template'y. Go ahead and do templates in the background but don't make me have things that are &lt;blah(void)&gt;operator==()()blah. Just to do something insanely basic. 
Thanks, I've seen it mentioned online a lot recently but haven't really taken a close look so far. I developed an IO framework to deserialize/serialize tuples of tensors (e.g. Eigen matrices) using range-based syntax, and use this for most of my projects. It's very limited in that currently, it can't handle anything more, and does not support network communication. At some point I will probably need to switch to Cap'n Proto.
That's not really an apples-to-apples comparison, since good databases create their own raw block devices independent of the host filesystem. These tests are supposed to measure reads, writes, and copies pertaining to files within the file system.
No, I hadn't heard of it before. The external memory priority queue that it supports looks interesting, though. I might use it at some point.
I know what the standard says, but implementations don't always follow standards so it is more important to know your platform. For example, libstdc++ std::string doesn't conform to the standard even in 4.9. The reasons for steady_clock and string breaking the standard are unfortunate but probably necessary compromises we'll be living with for many years.
Beautiful docs, mind sharing your tools and technique? Still reading through but why c++14 only? Which features require it?
A couple of different factors come to mind. I imagine the biggest one is the wear leveling algorithm in SSD's.
I'll try that and see how it compares to the other methods. I'm not sure why others downvoted you; your approach seems perfectly reasonable.
Agree mostly. PTP is usually necessary to get microsecond level accuracy. Newer intel chips have an invariant_tsc, which is very helpful to measure timings on multiple cores.
Looks fun. Which frameworks did you use?
Cute! Short, but looks fun =)
super cool.
what level course was this an assignment for?
&gt; People often take it for granted that their platform's implementation of IOStreams does a good job at sequential reads and writes. Does it really? Never.
Obvious questions: * Do you have TRIM working? * Have you considered setting aside more reserved space? * Do you ever give the drives time to recover?
Gotcha. Thanks for the explanation, I will definitely take a look at the source of your docs when I get to a computer. One minor suggestion: instead of exceptions, consider if you could use assertions instead. It seems to me that the exceptions you throw are usually development bugs as opposed to input validation. Following the standard library, iterators don't usually throw, they assert in debug or destroy your life in release.
I'd vote for a C++11 version. I find it difficult enough getting working C++11 cross-platform support, let alone C++14 :\
It says its homework in the video description.
Very cool stuff. Was java 8 inspiration for this, and if not, what else was inspiration?
I do not recommend that other people necessarily use the settings I mention in the article for their own systems. The "best" settings on a particular system will certainly depend on many parameters. I hope to run the benchmark on more systems over time, and then see what kind of general trends show up. At this point, I would put up some interesting visualizations. You are right that I did nothing to explain how much better the best settings are when compared to other settings for the same file size. I suppose people may be more convinced to use the benchmark if they see the distribution of runtimes for a particular file size/IO task. I will put up some visualizations soon -- thanks for pointing this out.
Very good job! And graphics are nice.
I am also interested in the frameworks you used.
Very much so. I was also inspired by the existing STL and things like python itertools.
I'll start exploring the possibility more in depth.
I thought it was C++ **AND** C board. Or do you think you can know C++ knowing nothing about the C subset?
framework plz
1. Yes, it passes TRIM check which required updating the newest, newest Intel RST and also the updates on the mobo. 2. I can't set aside more space. My application is to literally take 1TB of images at max rate. I would expect better performance if only fill half the drive. 3. No, my application involves taking as much data down as possible. A JBOD via Ethernet would be too expensive compared to planning for half rated performance.
I too am also curious what graphics frameworks you used.
[Reconstructing Cave Story](https://www.youtube.com/playlist?list=PL006xsVEsbKjSKBmLu1clo85yLrwjY67X) It's a video series where we go through the re-programming of the game Cave Story. It gets into some deep C++ concepts throughout.
WE WANT TO KNOW
Thank you, but I have a hard time learning from books. I don't really retain the knowledge very well.
What does "God Mode" do? I need to know...however, seeing as that hot air balloon just landed on the suns face I will assume you are already in god mode :)
If you're going to submit this here, you really should at least say what libraries you used, and to make it actually useful post the source as well.
Thanks. I liked the slides. And it did not take me one hour going through them.
I love Scott Meyers and this is great. What really got me is this line (from the slides): &gt; You chose C++ =&gt; the situation is already complicated. Too few developers seem to appreciate this.
Can you explain why the situation is not complicated using Java, Python, C# or any other language? What is it about choosing C++ that makes "the situation" complicated? I appreciate that C++ is a complex language but Scott made a very good point in his slides: most of the time, the user is hidden from the complexity. Yeah, OK compile errors. It takes about a year to get used to those and then you learn how to read them. Yeah, OK memory leaks. Once you get to the smart pointers, you rarely new/delete anything ever again. If you're creating memory cycles every week, you're doing it wrong. How often do you get memory or resource leaks in Java or C# code in *production* code? The truth is quite often because something, somewhere is holding a handle to your file or string or whatever. I think there was recently a major case of the lack of RAII bringing an Android app to its knees. People have actually built businesses on finding Java server memory leaks. The whole "C++ is complicated" thing needs to be rethought. I've had people who have never done C++ use C++ successfully when given a framework and a code review process that snuffs out bad code. On the flip side, I recently embedded v8 into a C++ app and now I can't tell what is leaking and what is not as easily as I used to be able to. Something, something, get off my lawn. Ok I'm done...
&gt;What is it about choosing C++ that makes "the situation" complicated? I think his point is that the causality is reversed: C++ is used when you have a difficult problem where you care hugely about details across a huge range of details, from allocation to object hierarchy. If your problem was simpler you'd choose a simpler language where some of the details are taken care of for you. The complexity of the language to some extent reflects the complexity of the problems it's used to solve (not all of it: a huge portion of the complexity comes from the fact that it needs to preserve backwards compatibility all the way to C).
I would suggest just finding a non-trivial project you are passionate about and more advanced features will begin to incorporate naturally. Everytime you find a bit of pain in the way you are designing something, you can seek out a more advanced, clean solution that will widen your knowledge base.
Bjarne's comments about C++11 being a new language are spot on. The new language really does make it easier to write code. I disagree that you care hugely about a wide variety of details when using C++. The only thing I usually care about is whether it is fast enough and can I compile it where I want. I get both of these for free. My code has gotten a lot simpler since I've switched exclusively to C++11.
Many people put library support above language choice. Unless I'm competing with google, I'm not going to choose c++ for a web-scraper. I'll choose python or ruby or perl. If I want a simple ui to get data from some business unit and into a database I'd use html/js/+, or vba, or excel+ or something. I'm not likely to write a c++. If I want to do some visualization of data I'd use R or D3 or some scipy stuff. I think the assumption here is that when you have a simpler problem, there are higher level languages, frameworks, and libraries that can solve them more efficiently. Sometimes you just fuck around in excel and do what would take 500 lines in c++. When you care about other things that are driving major requirements, then c++ is a good choice, and in those cars the bigger requirements are more complex than the language. If I care very much about performance or scalability or compatibility across a bunch of components then those override the language features and then sometimes c++ is the right choice. In those cases Meyers assumption usually applies. The situation is already complicated. You chose C++ because of its flexibility or low-level control that it gives you to meet the complexities of your situation.
This is a fair explanation. Thanks for taking the time to write it out. Though I still think in the long run, library choice can only affect initial velocity.
I know it sucks, but I think the key is to do really long exercises. Oh, you know how to solve this problem and don't want to waste 3 hours coding it up... Still code it up. You'll do something wrong, or have to scrap the whole thing due to a design error. It'll be frustrating, but forcing yourself into it is the only way. 
Well it's also worth remembering that the library/IDE/tooling support is there if you want it. Not only decades of C++ libraries, but also every C library. I think this is going to be the real challenge for RUST et al to catch up with all those man hours of work
Sort of. For the standard stuff, yeah. For OS-specific things, sure. But there isn't a [CPAN](https://pypi.python.org/pypi) or a [gems](https://rubygems.org/gems) or [pypi](https://pypi.python.org/pypi) or [JSAN](http://openjsan.org/) There is this http://en.cppreference.com/w/cpp/links/libs and this: http://ccodearchive.net/ When people ask, they get these answers: https://groups.google.com/forum/#!topic/comp.lang.c++/-VEOa0UgfoI Listen to those answers: &gt;* Standard C++ has only features which are considered essential and which can be implemented on all (or nearly all) platforms. &gt;* Progress is driven by need. There is no fifth wheel in a carriage not because it is not possible to have it, but because there is no need in it. Do a google search of "web scraper in c++" and you'll see: &gt;* If you're doing web scraping, C++ is probably not the best language to use. &gt;* Don't use C++ for this (I'm doing something similar as I'm typing this). &gt;* When you still can't get it to work, you can always implement HTTP yourself &gt;* Well there are several tutorials on using sockets with C++. For the actual processing of the data I would look into the boost::regex library. [This poor guy](http://stackoverflow.com/questions/834768/options-for-web-scraping-c-version-only) got tired of being told not to use C++ and someone actually answered his question. &gt;"It has to be C/C++ and nothing else so please do not direct me to Options for HTML scraping or other SO questions/answers where C++ is not even mentioned." Though to be fair it is possible and this is a great example of how it can be done in modern c++ http://rosettacode.org/wiki/Web_scraping#C.2B.2B #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;boost/asio.hpp&gt; #include &lt;boost/regex.hpp&gt; int main() { boost::asio::ip::tcp::iostream s("tycho.usno.navy.mil", "http"); if(!s) std::cout &lt;&lt; "Could not connect to tycho.usno.navy.mil\n"; s &lt;&lt; "GET /cgi-bin/timer.pl HTTP/1.0\r\n" &lt;&lt; "Host: tycho.usno.navy.mil\r\n" &lt;&lt; "Accept: */*\r\n" &lt;&lt; "Connection: close\r\n\r\n" ; for(std::string line; getline(s, line); ) { boost::smatch matches; if(regex_search(line, matches, boost::regex("&lt;BR&gt;(.+\\s+UTC)") ) ) { std::cout &lt;&lt; matches[1] &lt;&lt; '\n'; break; } } } And there are 1,040,000 mostly unhelpful results being promised by google. Compare that experience [with this one](http://www.google.com/search?q=web+scraper+in+python) Where there are tons of directly applicable answers and examples and walkthroughs, although only 545,000 results. And this is the python version for comparison. import urllib page = urllib.urlopen('http://tycho.usno.navy.mil/cgi-bin/timer.pl') for line in page: if ' UTC' in line: print line.strip()[4:] break page.close() The message is that if you don't already know how to write a webscraper in c++ then apparently you shouldn't be trying to learn how to write a webscraper in c++. However, if you _insist_ all you have to do is write socket-level code and re-implement HTTP protocol. Whereas in python the message is that there are some good libraries written just for that scenario. There is an anti-library tone in the threads asking about why there is no CPAN for C++. There is a "fuck-you do it all from scratch" mentality in the threads where people ask for help and there is a "c++ can't do that easily, it's too hard, and they'll tell you to fuck-off if you try it" blanket dismissal by others. They all together combine to make it so that doing non-complex stuff in c++ doesn't ever feel non-complex in comparison to doing that simpler stuff in higher-level languages. Thus, many simple problems get solved by people outside of c++ and c++ is reserved for when you really do want to re-implement http yourself while scraping the time from a webpage. (because you have code-injection stuff or some other non-trivial requirement). right? At least with c++ you can do that, but unfortunately the message (incorrect as it is in modern world) is that you _have_ to do it the hard way. I don't mean to rant. I just wanted to highlight one example of a user-experience to people who like and use c++ so that we might do what we can about it. If stuff is easier these days then it used to be, if it is more standard, if there are libraries like libcurl or boost:asio that should be in the first answer rather than "don't use c++" as the first answer then we can do better to tell people about them rather than let the haters and the "fuck you RTFM and write it in assembly you pussy" people do all the talking for us.
Shit floats
People seem to think garbage collection saves developers from worrying about memory management. It doesn't. It just removes direct control and forces you to coax the garbage collector into doing what you would otherwise do manually.
You don't have to be Google to care about efficiency. If the majority of your costs is renting EC2 instances then even a 20% efficiency gain can make a huge difference to your profitability.
See wikipedia: [Name mangling](http://en.wikipedia.org/wiki/Name_mangling). TL;DR: It's relatively easy to compile functions into assembly, so a C++ compiler first translates class methods and overloaded functions into intermediary functions. Those intermediary functions need to have unique names, so in case of overloading, the signature of the original function becomes part of the name of the intermediary function.
Good point, I hadn't considered doing that. That is definitely something I will be implementing.
For $11 on Amazon I highly recommend *Code: The Hidden Language of Computer Hardware and Software*. This book would mostly fit under your last bullet point. The younger guys at my school thought it was a waste of money, but a few of us older folks thought it was brilliant.
Alex Darby has written a pretty decent "Low Level Curriculum" series: http://www.altdev.co/2011/11/09/a-low-level-curriculum-for-c-and-c/ // Note that there are 12 parts total, for instance: - http://www.altdev.co/2012/05/07/cc-low-level-curriculum-part-8-looking-at-optimised-assembly/ - http://www.altdev.co/2013/05/22/cc-low-level-curriculum-part-12-multiple-inheritance/ For x64 assembly, see: https://www.youtube.com/playlist?list=PL0C5C980A28FEE68D You may also be interested in the following: - http://www.reddit.com/r/cpp/comments/28qefm/book_advanced_c_and_c_compiling/ - http://www.reddit.com/r/cpp/comments/26m2mw/presentations_x86_internals_for_fun_and_profit/ // see also the resources mentioned therein -- in particular, http://gcc.godbolt.org/ is great for experiments As for the compiler flags, compiler docs are still often the most complete and up-to-date source; for instance, for GCC see: https://gcc.gnu.org/onlinedocs/gcc/index.html#toc_Invoking-GCC Don't forget to look at the target-specific flags, e.g., https://gcc.gnu.org/onlinedocs/gcc/i386-and-x86-64-Options.html As already mentioned, computer organization &amp; design (and possibly a follow-up course, like computer architecture) are very useful in this context. The respective books would be Patterson &amp; Hennessy (2013) "Computer Organization and Design - The Hardware-Software Interface" -- and then Hennessy &amp; Patterson (2011) "Computer Architecture - A Quantitative Approach". There are on-line courses on these topics; in particular: * http://www.ece.cmu.edu/~ece447/s14/doku.php?id=schedule // you can take a look at the readings to find more resources and to help you organize a personal reading list: http://www.ece.cmu.edu/~ece447/s14/doku.php?id=readings * https://www.class-central.com/search?subject=cs&amp;q=hardware // Now that I mention courses, and since you're asking about compilers &amp; build process, Stanford's self-paced course may also be of interest: https://www.coursera.org/course/compilers Currently the following one may be quite relevant: * "The Hardware/Software Interface" -- *starts in 5 days!* https://www.coursera.org/course/hwswinterface 
It does give you memory safety though. (Well, not in C++ but ...) 
What exactly do you mean by safety here? 
&gt; And now, if you'll excuse me, I have hundreds of reviewer comments to process... That's makes me happy to hear, one of the great things about books, over say blogs, is that more eyes review the contents. It's similar to that often purported benefit of opensource, except it actually happens :-)
I would like to second the recommendation of "The Hardware/Software Interface." I recently took this course (in person) at UW, and I firmly believe it was the single most valuable CS course I've taken. This was the course that really opened my eyes as to how software and systems really work, and how we programmers can leverage that knowledge in writing software. It's also a perfect lead-in to courses on networking, operating systems, embedded systems, and more. You can see the full syllabus at the Coursera link above, but the major topics are: * x86(-64) assembly -- includes a solid treatment of the stack disciplines in 32-bit and 64-bit x86 * debugging with GDB, compilation, linking, libraries * caching and the memory hierarchy * exceptional control flow (traps, aborts, I/O interrupts, and faults) * virtual memory * dynamic memory allocation The aspect of the course that contributed most to my learning was the book, *Computer Systems: A Programmer's Perspective*, 2e, by Bryant and O'Hallaron. This book is well written, is super engaging, has fantastic examples with real C and x86(-64) code, and has great problems to test your understanding. Through the Coursera course, you'll have the opportunity to do more in-depth programming labs. (The book itself doesn't contain many involved programming exercises, but the authors of the book do supply the programming labs used in the HWSWI course.) If you're ambitious and want to learn beyond the scope of the HWSWI course, the CS:APP book also provides an introduction to processor design, I/O, networking, and more that's not covered in the course. I can't overemphasize how valuable CS:APP was to my understanding of systems and software at a low level and how well it set me up for more advanced topics in CS. 
Eh. You certainly shouldn't always be implementing move as swap, but the difference between three pointer assignments and two is usually meaningless. Just because you're doing something for the sake of performance doesn't mean that you have to rice it and get 100% of the potential performance gains when you can get 99% with less code (and it's not just less boilerplate: not using swap means a second copy of your cleanup code). I'm a bit more sympathetic to the delayed destruction argument, but I'm inclined to say that if it matters you're in very dangerous territory anyway. Moved-from objects should be promptly reset to a known state if they're going to stay alive after the move. 
I keep meaning to check into (modern) cpp dev, now I guess I finally have a good introduction - now all I need is an acceptable IDE and a good reason.
You can still use RAII and manage the memory explicitly. Things like `std::shared_ptr` and `std::unique_ptr` can help you manage your ownership policy. You can store the smart pointer into an `std::unordered_map` and then provide member functions to explicitly delete things. As an example here's some old code of mine [that illustrates the concept](https://gist.github.com/Rapptz/18996b7640de360a63c4). 
RAII is for objects with a defined lifetime in your program. Enemies and game objects don't fit that definition.
For some reason I thought RAII was for *everything*. I might have been told wrong.
I've found xcode to be decent.. It is following the new standards (c++11 &amp; c++14) closely in terms of understanding and highlighting the new syntax elements.
I think most of the time when you need complicated explicit control over resource lifetime, using destructor is just harmful. Thing like a opengl texture handle, if you wrap it inside a class with destructor calling `glDelete()`, if you want to put it in `std::vector`, you have to disable copy constructor and implement move constructor to avoid reallocation freeing the texture. It is just more complicate than explicitly calling `glDelete()`.
I agree with slacksoft, RAII only partially applies to things like enemies. It does in that once your main game goes out of scope, it should destroy all the enemies with it. But it doesn't apply to the case of the enemy disappearing because the player killed it. In that case the destruction isn't triggered by some scope exit, but by some game logic which explicitly destroys the object. But that doesn't mean you have to use raw pointers for managing enemies, put them in some sort of container which does the resource management for you.
Preallocate and reuse. If you are allocating memory in the middle of your game you are doing it wrong. 
Ekhm, Every game has a hard limit on a number of objects. When you see some object 'dies' it is only marked as dead. No memory gets freed. This 'dead' object may be reused immediately without allocation and deallocation FOR FREE without RAII or any other GC. 
&gt;put them in some sort of container which does the resource management for you. I'm honestly very new to STL containers. Mind giving me some pseudo code to help understand the process?
&gt; You might initialize a new enemy, Enemy enemyNew; but how would RAII take care of it? RAII wouldn't, the lifetime of the object is defined not by scope but by the semantics of your program. Note however you can use RAII to manage the resources attached to the enemy, just not the enemy itself. &gt; To initialize them is easy enough, with or without a pointer, but when it dies, you need to explicitly tell it to free up its resources. When it dies you destroy the object (note that this does not involve a call to delete if you are using pre-allocated objects). &gt; most C++ video game developers are still stuck in a lot of old styles. Actually it is just that the problem has been solved already, if your engine has a method of tracking resources you have a solution. And since scope isn't as important for games, RAII isn't important at that point.
It doesn't work. I've been working on a similar library for abstracting GPU computation. The problem is that if you pass the parent provider as a template argument the template depth gets too deep and it kills the compiler. clang 3.4 just dies after hanging for half an hour and gcc 4.7 outputs half a gigabyte of errors if you make a mistake for a stream built up over ~500 steps. The problem is that type checking the stream object is proportional to the template depth so the work to check the stream is proportional to the sum of all numbers less than the length of the stream.
Not always... I would agree with you for a server but there's just too much going on in the client to preallocate everything. You've got a lot of wasted memory sitting around in your prealloc block that the rest of the system could be using for other things. Then again I haven't done any serious game programming in about five years so maybe the state of the art has changed since then.
I'm not sure avoiding pointers is really necessary. Instead, you should be looking at how to *use them properly*. For the instance you describe, something like shared_ptr&lt;&gt; or unique_ptr&lt;&gt; seems like it would do the trick.
Open-source projects are only as good as the people who actively contribute to them. Just some food for thought. Also many projects have users that could review and improve the code, but are too apathetic to do so. :(
I agree with /u/YouFeedTheFish about XCode. I've always enjoyed using it on my Mac. That being said, I use Visual Studio primarily at work. I also use it at home a lot. Microsoft has created a wonderful development environment. Now if only their compiler could catch up. They're still lagging on C++11 and C++14 implementations. If they've implemented everything you need, though, it's a great environment to work in.
If someone tells you their software tool or method is good for *everything*, they are a fool, or they are selling something. Run the other way. 
The 3 vs 2 pointer assignemenst is often meaningless since the move assignment operator is inlined 99% of the time. The delayed destruction argument is avoided by implementing the assignment operator in this way: &gt; T&amp; operator=(T val); Which handles both the move and copy assignment - provided you defined the proper constructors - automatically. 
Here are some handy links for you: http://en.wikipedia.org/wiki/Rule_of_three_%28C%2B%2B_programming%29 http://flamingdangerzone.com/cxx11/2012/08/15/rule-of-zero.html The thing you need to understand is the rule-of-3 from C++03 because the C++ used to auto-define 3 functions for you: Destructor Copy constructor Copy-assignment operator In C++11, this has been expanded by 2: Move constructor Move-assignment operator to become the rule of 5. What this says, is that if you find yourself defining *any* of these functions, you *must* define (or delete) the others. Otherwise you are not maintaining the resource properly. The corollary to this, is that you cannot manage more than 1 resource in this fashion, otherwise you are not exception-safe (&amp; you're likely not managing the resource correctly). Someone very cleverly noticed that the auto-generated functions are inherited. So if you have a move-only resource A in our object, without doing anything, your object is move-only. If you have a move-only resource A &amp; a copy-only resource B, then your object will fail to compile unless you explicitly define the 5 functions (of course, then it necessitates the question: are you sure you're managing resources correctly?). This is where the Rule-of-Zero comes from: http://flamingdangerzone.com/cxx11/2012/08/15/rule-of-zero.html If your object is doing resource management, that is the *only* thing it should be doing &amp; it should only manage one of them. As an extension of this, in C++11 (&amp; especially C++14) it is considered very bad form to have any explicit calls to new/delete; this actually applies to other kinds of resources. File handles should be similarly wrapped in resource handlers where the open is done as part of the object. RAII comes into this that the constructor allocates &amp; initializes the resources. There is no distinction. In the destructor this is unwound. The reason you want exactly 1 resource owned is in case you throw during the constructor during acquisition failure. If you own multiple objects, then you have no way of knowing which one failed at what step so you can't know how to de-initialize the object. If you only own 1 resource, either you have thrown before you acquired it (i.e. no need to tear down), or you threw the exception yourself so you would release it before throwing if you acquired it in some invalid state. Personally, instead of a std::unique_ptr&lt;Texture&gt; where the ~Texture calls glFree, I would have a TextureHandle that is move-only &amp; implements ownership transfer safely; the only thing it manages is the underlying texture ownership. Texture would wrap a TextureHandle &amp; let me do things like bind/unbind. Value semantics are very powerful &amp; ownership is much clearer. Additionally, it lets you allocate these things on the stack or as part of a contiguous array. RAII really only applies to resource ownership. If your object doesn't directly own exactly 1 resource and this owning this resource isn't the object's sole reason for existing, then you're not using RAII properly. If your object doesn't own the lifetime of any resource directly (i.e. it contains objects that hold onto resources), then you don't need any destructor. How does this apply to Enemy? Enemy wouldn't own any resources directly &amp; as such wouldn't need a destructor. It might have a Texture, but you don't need to do anything to manage it as the compiler does the heavy-lifting of auto-generating the destructors &amp; doing all the exception safety for you.
My GL is a bit rusty, but what you may want to do is something like: BoundTexture TextureHandle::bind(); BoundTexture unbinds in the destructor &amp; would be a move-only resource (you can only bind a texture once at a time, right?). Then you can control the lifetime of how long the texture is bound for (e.g. if you pass a const&amp; &amp; it's a move-only object, then you know no-one else can take over the bind ownership). Thus you are very certain the texture is unbound when you're done with it. The texture lifetime is domain-specific to your application. If you feel that it's best freed only when the game ends, then sure. If you have a particularly big texture that's no longer needed, you might want to free it sooner to free up GPU resources. We're trying to give domain-agnostic advice (i.e. C++ guidance). You have to decide how that best applies to the application you want to build. I can promise you this: if this is your first experience with this, you won't get it right. If this is just something you're doing for fun, experiment. Then you understand how this stuff works by making mistakes &amp; finding out what works well &amp; what doesn't. If it's something you're doing with other people, try to stick to keeping things as simple as possible. Stick to tried &amp; true datastructures (std::vector), try to keep lifetimes simple (prefer value -&gt; unique_ptr -&gt; shared_ptr in that order), etc.
I'm trying to rap my mind around this. So why exactly would I want a texture handle to be move only? Wouldn't I want multiple objects to be able to use the same texture? Also, what purpose does "moving" a resource do me? So let's say I have an enemy, and it has a texture. It dies and then what? You said I shouldn't use std::unique_ptr&lt;Texture&gt; in favor for a Texture object that wraps/manages a TextureHandle, but why is this any better than just using a unique_ptr?
Plus, I'm still not quite getting why I should be using these smart pointers. So let's say I have an std::unique_ptr&lt;Texture&gt; where the ~Texture calls all the glFree calls, there's still no explicit way to say, "Hey, I need to free the resources in Texture now, please destruct yourself." I think the reason why you don't need to call the destructor is because it is limited to the scope of its use. The texture is completely tied to the Enemy, and so when the Enemy dies, the texture goes out of scope. I'll read the articles you gave and see if I can figure this out. A lot of this information is very new and I'd like to understand it a bit more before I get even more lost in my code.
std::unique_ptr&lt;T&gt;::reset() if you want to destroy the underlying object before it would normally be. However, ~std::unique_ptr&lt;T&gt; calls ~T() for you, so you don't need to do anything special in your destructor - let the compiler do the work. I'm trying to stay abstract from your game example because it's dependent on how you structure that which every developer will have a different way of doing.
For the record, don't do what this guy says. Use whatever abstractions best fit the domain and don't assume you end user (programmer) is to stupid too open a set of angle braces.
You should avoid *raw* pointers (eg, `Foobar*`) like the plague. Using smart pointers is a good compromise in a lot of situations. For your enemy types, you could `make_unique()` (in C++14, or copy someone's implementation for your compiler) them into a container for tracking. For things like OpenGL resources, the same can apply with the smart pointers controlling the lifetime. If you do it "right", then you shouldn't have a single `delete` in your code (except maybe in a destructor). Note, there are performance implications using lots of heap allocation if you're writing a game. You could always write your own memory allocator and preallocate a chunk of memory to use as your own heap, but that's a can of worms in itself. ps. If you have to call `glDelete()` with a resource you want to free (eg, a C API), you can stuff a lambda into smart pointers for the deleter. Eg, auto myFoo = std::unique_ptr&lt;Foobar&gt;(get_foo(), [](Foobar* f) { free_foo(f); }); Though it's best to give that special `unique_ptr` a name than to sprinkle lambda deleters everywhere in your code.
Just the link to the fixed QCH file is worth my upvote. Thanks!
So he actually finally finished the damn thing, nice!
Once you know the basic constructs and are able to use them to solve simple homework problems, the next step is to just start writing real programs. If you're not at the point where you can come up with good projects to work on here are some suggestions: [Universal Machine (ICFP programming contest 2006)](http://fileformats.archiveteam.org/wiki/Universal_Machine_(ICFP_programming_contest_2006)) Basically you write a super simple virtual machine to run programs according to the provided spec. It's great because once you have the working virtual machine the contest has provided a program to run on it that is basically a little text based operating system with puzzles and things. You can also make more creative use of basic text input/output: [ASCII art fluid dynamics](http://www.youtube.com/watch?v=QMYfkOtYYlg). Writing a fluid dynamics simulation might be a bit advanced, but you can simulate all kinds of things and display animated visual output with ASCII art. [Conway's game of life](http://en.wikipedia.org/wiki/Conway's_Game_of_Life), for example, is one common project. You can also use text based output to draw normal images, using an external program to display them. The [PPM image format](http://en.wikipedia.org/wiki/Netpbm_format#PPM_example) is simple to output and, for example, is the output image format of the [business card ray tracer](http://fabiensanglard.net/rayTracing_back_of_business_card/index.php). A program to generate a ppm image of the [Mandelbrot set](http://en.wikipedia.org/wiki/Mandelbrot_set) is a good place to start. Another use of text based IO is the [Common Gateway Interface](http://en.wikipedia.org/wiki/Common_Gateway_Interface). Web servers use this to interface with another program running on the server to create dynamic web pages. The web server starts up a program to handle a user request and the program writes its output to the standard output (e.g. std::cout) in the HTTP format, and the web server sends that to back to the browser. To do this you'll install a free web server and then configure it to run your program via CGI. You can even use the built-in input/output facilities to generate audio. A few simple math functions can be used to calculate raw PCM audio samples of simple sound waves. You can save them to a file to play using an external program (such as Audacity), or you could pipe that output directly to /dev/audio if you're working on platform that provides that. If you're not satisfied with the built-in input and output facilities then there are also lots of C++ libraries that provide additional options. One I've heard about recently as a good option is [Cinder](http://libcinder.org/). 
This is very useful, thank you. In the past, in order to access the C++ standard library doc from Qt Creator, I used the sources of *libstdcpp* (the sources are commented using the doxygen syntax), to generate a .qch file. The result was half-working, though. The version you are proposing is much better.
&gt; What kind of container is best really depends on how you're going to access the objects. In the very vast majority of cases it is best to use vector or an unordered container. 
It may be more complicated to implement, but it is much harder to misuse than say glDelete.
Nice title. XD
Freezes my QtCreator under openSUSE Qt Creator 2.8.1 Based on Qt 4.8.5 (GCC 4.8.1 20130909 [gcc-4_8-branch revision 202388], 64 bit) **EDIT** The official from [here](http://en.cppreference.com/w/Cppreference:Archives) works though.
I assume it's because of the older version of Qt Creator. I always install the latest Qt Creator on Linux from http://qt-project.org/downloads#qt-creator Any reason for using 2.8.1 instead of 3.1.2 from qt-project?
take a look at the sidebar: http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list it's the general consensus that there is no "good" online c++ tutorials, so learning should be done from the books. refer to the list above. when you're in need of specific details, refer to http://en.cppreference.com/w/ (avoid cplusplus.com)
Don't know if gnawer has the same reason I have but the reason for me is that debugging in anything QtCreator &gt;= 3 is horribly broken (for me I should say). Either qtcreator locks up, or it doesn't show locals, or breaks in some other spectacular (and really really annoying) way. I also get a feeling that buttons and text and stuff is kind of slighly blurred with Qt Creator 3 and above. It isn't completely blurred it just doesn't feel as crisp as 2.8. And of course 2.8.1 is the version in the official package manager repo. Edit: However Qt Creator is still the best in my opinion, I use it for all C and C++ development both at home and at work. Edit2: Seems like the last version of Qt creator I tried was 3.1.1 and not 3.1.2 (which must be relatively new since I stumbled upon the debugging bug only a week or 2 ago in what was then the latest qt creator release). It also appears they have fixed some sort of debugging issue so I will probably have to give 3.1.2 a go.
I was thrown off by the O'Rielly cover haha. Glad to see this coming out, its going to be my going away present to my coworkers in hopes that they don't destroy my codebase.
Maybe I shouldn't have started with the Universal Machine, since that's probably the most complicated program. The concepts involved aren't too complex, but there's a lot of individual parts you have to write to make it work. In order from simplest to hardest you'd do: ASCII-art conway's game of life - easy; you just have to figure out how to represent the game grid, figure out how to update the grid, and figure out how to print the grid using std::cout. Then you just run a loop display, update, display, update, etc. Mandelbrot set in PPM format - PPM output is trivial, so you just have to figure out how to calculate the mandelbrot set (which is much simpler than it may sound). Then you just have to download a program that can display PPM images. CGI - start with outputting a static webpage. The only difficulty is understanding the HTTP protocol and the interface between the server and your program. digital audio - It may take a bit of work to figure out the right format (16-bit ints or 32-bit floats, signed/unsigned ints, etc.) Audacity is nice because you can just pick the format you want when you open the file. Universal Machine - This has the most moving parts, but no difficult mathematical concepts or anything.
Okay, so now I'm starting to get it. I should use these smart pointers to avoid having to write new, special code for my destructor, so that way everything is really contained in my Texture class. All this really reminds me of why I use Vector instead of raw arrays now. I don't have to worry about the array existing outside of the lifetime of the object, because it'll go out of scope and clean itself up, and so there's a lot of safety involved with it. I'm still not completely sure what the point of moving the owner is. Don't I just want the texture to live and die with its owner? Why exactly would having the ownership move be a good thing, specific to this example (I know I'm asking for domain specific advise, but I'm having a bit of trouble completely understanding it all, thanks for all your help by the way). 
Bad translation for html, I guess.
Also which one of the C++ books would you recommend because there is a decent collection.
Plus, for other compilers you do need to include the headers that would otherwise be in the precompiled header, introducing preprocessor #ifndef MSVC... 
I'm not actually using qtcreator. But I was curious about the offline help, and that's just the version that comes with my distro. 
Using `enable_if` always looks ugly, I always use these set of macros to make it cleaner: #define ERROR_PARENTHESIS_MUST_BE_PLACED_AROUND_THE_RETURN_TYPE(...) __VA_ARGS__&gt;::type #define FUNCTION_REQUIRES(...) typename std::enable_if&lt;(__VA_ARGS__), ERROR_PARENTHESIS_MUST_BE_PLACED_AROUND_THE_RETURN_TYPE #define CLASS_REQUIRES(...) typename std::enable_if&lt;(__VA_ARGS__)&gt;::type #define REQUIRES(...) typename std::enable_if&lt;(__VA_ARGS__), int&gt;::type = 0 Then his example could be rewritten like this(assuming we have `constexpr` in the compiler): template &lt;typename U&gt; FUNCTION_REQUIRES(is_convertible&lt;U, T&gt;()) (T) value_or(U const&amp; v) const { if (*this) return this-&gt;value(); else return v; } template &lt;typename F&gt; FUNCTION_REQUIRES(!is_convertible&lt;F, T&gt;()) (T) value_or(F const&amp; f) const { if (*this) return this-&gt;value(); else return f(); } It looks cleaner even with macros. Its even cleaner I think using template parameters like this: template &lt;typename U, REQUIRES(is_convertible&lt;U, T&gt;())&gt; T value_or(U const&amp; v) const { if (*this) return this-&gt;value(); else return v; } template &lt;typename F, REQUIRES(!is_convertible&lt;F, T&gt;())&gt; T value_or(F const&amp; f) const { if (*this) return this-&gt;value(); else return f(); } It would be really nice if boost included these macros in its `enable_if` utility. I think `enable_if` with these requires macros is much better and cleaner than tag dispatching, plus it produces nice, clean and informative error messages on modern C++ compilers. If you need multiple overloading than conditional overloading can solve the problem quite nicely. Also, in C++14 we can get close to `static if` using generic lambdas. A complete C++14 solution to the problem could be this: template&lt;class T&gt; decltype(auto) id(T&amp;&amp; x) { return std::forward&lt;T&gt;(x); } struct then { template&lt;class Then, class Else&gt; auto operator()(Then t, Else) { return t(id); } }; struct else_ { template&lt;class Then, class Else&gt; auto operator()(Then, Else e) { return e(id); } }; template&lt;class B&gt; constexpr auto if_ = typename std::conditional&lt;B, then, else_&gt;::type(); template &lt;typename U&gt; T value_or(U const&amp; v) const { if (*this) return this-&gt;value(); else if_&lt;(std::is_convertible&lt;U, T&gt;())&gt; ( [&amp;](auto _) { return _(v); }, [&amp;](auto _) { return _(v)(); } ); } 
I worry that next we will see postmodern C++, where the compiler has to "interpret" your program according to a social context.
I'm not a fan of one pch to rule them all. http://stackoverflow.com/questions/290034/is-there-a-way-to-use-pre-compiled-headers-in-vc-without-requiring-stdafx-h 
&gt;macros Nope. 
Consider reading [this](http://flamingdangerzone.com/cxx11/2012/06/01/almost-static-if.html). It makes the boilerplate less ugly and doesn't use even uglier macros.
&gt; Let me warn you then. I only showed you that such techniques exist. I really do not recommend employing them in your code Always good to know that you can do something, even if it just serves as a way to deWTF when you run across a piece of code someone else writes that uses such a technique. I, for one, don't think I will ever use this dark magic.
In that category I'd also consider Stanley B. Lippman's "Inside the C++ Object Model": http://www.amazon.com/Inside-Object-Model-Stanley-Lippman/dp/0201834545 With the understanding that "The language covered within the text is the draft Standard C++ as of the winter 1995 meeting of the committee." Still a classic, though. At the same time, a nice reading after D&amp;E is Bjarne's "Evolving a language in and for the real world: C++ 1991-2006." http://www.stroustrup.com/hopl-almost-final.pdf Related slides: https://parasol.tamu.edu/people/bs/622-GP/CandC++evolution.pdf
That is one way to do it, but it gets ugly real quick when the predicate gets just little more complicated: template&lt;class T, EnableIf&lt;boost::mpl::or&lt;std::is_convertible&lt;T, int&gt;, boost::mpl::not&lt;std::is_default_constructible&lt;T&gt;&gt;&gt;&gt;&gt; void foo(); Now C++14 provides `std::enable_if_t` to get rid of the `typename` stuff, and it takes a boolean instead, however it can be a little error prone as well. For example, if you forget the parenthesis(which is easy to do), then this will break: template&lt;class T, std::enable_if_t&lt;std::is_convertible&lt;T, int&gt;() || !std::is_default_constructible&lt;T&gt;(), int&gt;=0&gt; // BOOM! void foo(); So the macro makes it cleaner and simpler.(Ultimately we need a language feature instead of a macro). However, if you still think it looks ugly, you could use [ZLang](https://github.com/pfultz2/ZLang) to make it cleaner: template&lt;class T, $(requires std::is_convertible&lt;T, int&gt;() || !std::is_default_constructible&lt;T&gt;())&gt; void foo(); 
You have a std::vector&lt;Enemy&gt; (or any other DS). When one dies you remove it from the vector, when one spawns you push_back it to the vector. and you'd probably want to use octrees instead of vectors, but the logic remains the same
&gt;For C++ questions, answers, help and advice see /r/cpp_questions. and http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list
I'm just super excited about this manual: http://show.povilasb.com/309bec16fb202e8d37e37. Before i used online ones, which of course is way slower :)
Somebody posted [something very similar](https://github.com/jeaye/stdman) some time ago but with one crucial difference: it used http://cppreference.com as the source, not http://cplusplus.com. This is better because cplusplus.com is generally seen as error-ridden and not advisable in the C++ community. (Another difference was that this installs the formatted documents as standard man pages rather than shipping a separate command line tool.)
I find it hard to interpret these wildly varying numbers. A look into [the documentation](https://gcc.gnu.org/onlinedocs/gcc-2.95.3/cpp_1.html#SEC8) can clarify a lot though: &gt; `#pragma once' is now obsolete and should not be used at all. [GCC] [and](http://clang.llvm.org/docs/InternalsManual.html#multipleincludeopt) &gt; If a buffer uses [include guards] and is subsequently `#include`‘d, the preprocessor can simply check to see whether the guarding condition is defined or not. If so, the preprocessor can completely ignore the include of the header [Clang] – In other words: both GCC and Clang implement special rules for include guards which make using `#pragma once` unnecessary, and consequently no performance is gained by it. As for whether performance is *lost* – I doubt that you will be able to measure the (tiny, if existing) difference, it will simply drown in noise from sampling.
That doc quote was from gcc 2.95, and `#pragma once` was [undeprecated in gcc 3.4](https://gcc.gnu.org/gcc-3.4/changes.html). Besides, the main benefit of `#pragma once` is not having to (re)write the tedious guards (especially during refactoring on codebases where the guards are formed from the file path).
I was interested until I saw this.
2014. The year in which most compilers default behavior is c++98/03
Thanks for putting it in the AUR for arch users. Thank you very much, this might come in REALLY handy.
&gt; #pragma once was undeprecated in gcc 3.4 Which I personally celebrated. Yet it appears they still hate `#pragma once` due to reasons yet undiscovered.
Apparently, the [OS might screw it up for you](https://groups.google.com/forum/#!topic/comp.std.c++/xSckAEPI35I) if you are using distributed file systems for your project headers.
Yeah, I guess that is an advantage of traditional header guards. For `#pragma once` to work, there needs to be a way for the compiler to tell when two files are actually the same one, while with header guards the responsibility for figuring that out goes on the programmer.
It's rarely a good idea, but you can also just outright have multiple copies of the same header that all get included (e.g. if you use lib A and lib B, and lib A has an embedded copy of lib B that's compatible with your standalone copy).
I don't know about this, but I think cplusplus.com is much easier to read then cppreference. I didn't encounter any error in cplusplus.com so far.
It also supports C++11.
Nice post, thanks for that! Why are range-based for loops not supported? The ndk r9d supports clang-3.3 and gcc-4.8, why would certain features work and others not? If really true, what's the logic behind it - what works, what doesn't, and why? (e.g. it might be that the library is not fully compliant, but I would think range-based for-loops are a language/compiler feature)
I don't get it, I have a small NDK demo application that uses range-based for without any issue. https://github.com/pjmlp/TownGL Edit: Rephrased the content.
http://pastebin.com/TRc5yRF8 - that's the content of my vimrc
&gt;this Wikipedia page says that all modern compilers support it. I need xlc, and it's not listed (because it isn't supported). xlc supports much of C++ 11, so I figure it does count as modern 😉. Ah, did you mean "hipster" when you said "modern"? Ok, never mind then 😉.
Author here. Please note that this is for 10000 files included 3 times each, so there is absolutely no real-world difference between the speeds of various include methods. For a single file, all numbers are well below 1 millisecond. The time to actually compile a real-world file would far far outweigh any tiny boost you get from using #pragma once over normal guards.
Good to know, thanks. I actually agree that `#pragma once` *should* become part of the standard – but then, modules will make it obsolete once again. That said, I don’t think that the un-deprecation changes the performance considerations my comment highlighted (which was the *sole reason* for quoting the reference), and I stand by my statement that OP’s test does not provide interpretable numbers.
That's what you think, but my company has roughly 50,000 includes per file :( I appreciate your research and will hold an immediate review session monday to change all of our guards.
As the author of https://github.com/jeaye/stdman, I would note there's one more difference that's, in my opinion, even more crucial: stdman uses man (as in, /usr/bin/man), whereas cppman is its own program that is in no way compatible with man. This makes stdman much more portable.
Hot tits, you're right. I'm caught skimming parens.
 &gt;A post-classical compiler, on the other hand, might perform the following analysis: &gt;* The first four times through the loop, the function **might return true**. * When i is 4, the code performs undefined behavior. Since undefined behavior lets me do anything I want, I can totally ignore that case and proceed on the assumption that i is never 4. (If the assumption is violated, then something unpredictable happens, but that's okay, because undefined behavior grants me permission to be unpredictable.) * The case where i is 5 never occurs, because in order to get there, I first have to get through the case where i is 4, which I have already assumed cannot happen. * Therefore, **all legal code paths return true**. ***WAT.*** I must be missing something, because unless the compiler can resolve both the table contents and the parameter value at compile time, I can't fathom how it decides to optimize the first 4 comparisons to always be true. 
They are supported just fine, the author must have made small mistake somewhere.
The range-based for loop is mostly just syntactic sugar.
Good catch. I have that book somewhere. Those three books are great. Thanks for the link to the paper by bjarne. Ill check it out.
If so, I think I can add another backend using cppreference. User can switch between them.
Stop. Take a deep breath. Now, re-read the GCC numbers.
I stand corrected 😢
This would be *great*. +1e6!
I see, thanks. That is a totally asinine optimization because, IMO a decent compiler should minimally emit a warning, but on that basis it makes some sense. 
As I recall, standardizing `#pragma once` was dissuced by the committee and rejected. Apparently, it is not possible to implement portably.
I've heard that argument, but it's pretty hard to care. If you're symlinking a file, it's an open question as to whether the compiler should treat that as one file or two. If you're compiling files on some wacky file system, where two equivalent paths may refer to different files, I don't see how it's the compiler's job to unfuck that situation. I guess you could argue it's the compiler's job to figure all that out if there was a compelling use case for doing all that stupid shit, but there's not.
[Here](http://blog.llvm.org/2011/05/what-every-c-programmer-should-know_21.html) is the explanation for why those sorts of warnings aren't useful or easy. I strongly suggest reading all three parts if you program regularly in C or C++.
Sure, because the C++ standard doesn't define the filesystem and all the wacky things you can do on the filesystem. But since most of us don't use wacky filesystems, we should benefit from the simplicity of `#pragma once`.
I'll agree with not easy, but they can be useful. I see a future where strongly typed compiled languages like c++ utilize compilers that emit useful warnings via integrating static analysis when heuristically deciding if a warning should be emitted. Just need to get the SNR right... I've read the clang posts on the subject (as well as a blog post by Chandler C I think), but I still believe there is room to improve. Nonetheless they are good reads.
man has advantages, certainly. But the ability to follow tags when viewing the documentation in Vim is fantastic. And if it can be viewed in Vim, then it is portable enough for me! 
Some interesting talks. I've been signed up since super-early-bird so I'll be there.
Clang's static analyzer will warn about some cases, and if its huge speed hit is actually required for that, I'm glad the compiler doesn't try to do the same. I don't think anyone will disagree that it'd be really nice if compilers were better at warning you when bad things will happen due to UB, but it's not worth it if it makes every compiler take ten times as long or if it's so spammy that most people just turn it off.
Try web.archive.org
It's more difficult to write fast and correct code in C++98. If insults are what's needed to justify the status quo of using the inferior, outdated standard, then do what you must.
I use C++11/14 by choice when I can: on new projects, for which the actual and potential user base have the option, capability and desire to use C++11/14 compilers and library support, or can otherwise run them [clarification: I write applications and libraries for evolutionary biology simulations and analyses, with programs that are run on everything from laptops to HPC clusters]. My point was that in the real world, these conditions are not always met for new projects, and quite a bit of code is out there that is C++98 that needs to be maintained regardless of personal preferences. I could have expressed this point in a nicer way, I suppose, but the snark in your post led to the snarky tone of mine. Incidentally, I do not consider "garage/hobby/school" projects to be insulting. Powered flight has come out of garages. But anyone who questions the need to support C++98 projects is, in my opinion, not only rather isolated from the real world but either naive or arrogant or both.
The undefined behaviour sanitizer can be helpful although it works at runtime and will only see undefined behaviour when such behaviour is executed. In one of my projects an array read overrun similar to the article example was found. There was a signed-integer overflow in hash-table code. It also found use of a NULL reference in a unit testing library. With the actual compilers used, these errors were benign, although the array read overrun error could have been a problem depending on memory layout. The read overrun was not found by several static testers including Clang, cppcheck, Visual Studio Code Analysis, and Coverity. The undefined behaviour sanitizer is shipped with current versions of Clang and gcc.
Would be interesting to see if we will see optimization opportunities in C++ code by using this effect like js.asm does. For example imagine a switch statement that the compiler can already implement as a jump table but has to make sure that the value is in range, so it doesn't jump into the nirvana for other values. But if you add a default case that invokes UB, the compiler may legally skip the range check.
I saw `#define IS_CONVERTIBLE(U, T) std::is_convertible&lt;U, T&gt;::value` and just closed it. Not like this, man. Not like this.
_Please don't do this_. Just include the damn thing up at the top. If the compiler doesn't understand pch; it'll just include it like normal and everything will be fine. Otherwise you essentially have a completely invisible include unless you pour through all the project settings.
I'm wondering if using Qt and the built in macros can address this problem?
Spirit.Qi: This is the parser library allowing you to build recursive descent parsers. The exposed domain-specific language can be used to describe the grammars to implement, and the rules for storing the parsed information. 
it doesn't stand for anything, it's Qi or chi: https://en.wikipedia.org/wiki/Qi, presumably to follow a similar theme to the 'karma' generator library.
Once C++ incorporates modules, hopefully we'll see compilation times start to reach more sane levels. At which point, the compiler can use some of the saved time to improve the warnings.
&gt; It is just more complicate than explicitly calling glDelete() Nonsense. Put the texture handle into a `std::unique_ptr` with an appropriate handle type and deleter.
The same author had a huge role in the development of [Boost.Phoenix](http://www.boost.org/doc/libs/1_55_0/libs/phoenix/doc/html/index.html) which is named after his daughter. So I guess he just prefers simple names rather than something more descriptive like [Meta State Machine](http://www.boost.org/doc/libs/1_55_0/libs/msm/doc/HTML/index.html).
It stands for bad practices such as non-descriptive naming.
His previous book has been quiet valuable in my path of learning C++. I can't wait for preview to start.
Just watched “The Last Thing D Needs”. It’s funny, I definitely tend toward agreeing with the C++ committee where Scott doesn’t. * list::sort makes sense as stable without being called stable_sort for me, because it’s the only one of the two that needs to exist. Giving it a more specialized name implies the existence of the general name. * Calling list::remove list::erase would be nice for consistency, but at least it makes sense from the perspective of how list::remove works, attaching the surrounding two members to each-other. But then, you get into the argument that the internal implementation should be hidden away in a more consistent interface (even though we all know we’re really working with a doubly linked list \*wink wink\*). * The capturing variables thing with lambdas makes perfect sense when you realize the concept is you’re “bleeding” the outer scope into the inner one. Like he said, you’re dealing with two scopes simultaneously. The capture list is your chance to explicitly specify how the two scopes interact (conceptually).
Cool. I had planned on working on something like this, but had other commitments in the last few months. You should check out https://github.com/mizvekov/fp It's a library that wraps fixed-point arithmetic support on top of other numeric types. Up to now I had tested only the built-in integers and floats, but by design this library should be able to wrap your safe integers so one can have a safe fixed-point integer type.
"Why's this one named the 'Titanic'?"
"Because it's big." No, boat *types* though. Or maybe it was just general nautical terms because one was "flotilla".
&gt; MSVC++ untested. I wouldn't hold your breath. Agreed.
If I'm not mistaken, GCC 4.9 supports return type deduction, among other handy C++14 features (polymorphic lambdas, `std::make_unique`, and double range versions of `std::equal` and friends come to mind). It -- GCC 4.9 -- is what I'm working with presently. `std::make_unique` stands out the most, but that's probably because I use my own `std::optional` implementation rather than `std::experimental::optional`.
&gt;Windows has MinGW. Incidentally I developed/tested this project on Windows with MinGW/GCC 4.8.1/.2.
Yeah but the issue is that GCC 4.8/4.9 doesn't support return type deduction for friend functions due to some bug. I submitted this [bugreport](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=59766) upstream a while ago but still I don't think it's being worked atm.
But.. But.. VS is the only compiler on Windows!!1
http://msdn.microsoft.com/en-us/library/dd570023.aspx
If `s.first` is `Sign::Positive`, then we know that `a&gt;0`. Therefore we know that `a` is at most `max`, and is at least `1`. `max-max==0`, and is well defined. `max-1` is well defined, and is some number greater than or equal to 0 but less than `max`. Therefore this is well defined. Since `a` and `b` are of the same type, comparing them with less than is also well defined. If it were possible for `a` to be less than 0, then there would be the possibility of undefined behaviour (signed integer overflow), but this is not possible since we insure that `a&gt;0` before proceeding.
I've never really run into that problem, mostly everyone I've needed a build from provides a mingw build
Ah ok, I'm following now. Thanks.
Adding another good one myself, for reference: http://lwn.net/Articles/250967/ What every programmer should know about memory
'Fraid the link you posted is to `localhost`.
GCC/clang untested. We wouldn't hold your breath.
Note the new cool control flow analyses, handy to catch those pesky uninitialized variable errors. If you have suggestions or want to leave a bug report, do create requests on the public [issue tracker](http://youtrack.jetbrains.com/issues?q=project%3A+%7BReSharper+C%2B%2B%7D).
Looks like Poco uses CMake so what's the problem there?
CMake and XCode are not good friends. The key is that I use other libraries where multi platform works quite well. I simply can't be wasting my time fighting with POCO everytime it upgrades, or my IDE upgrades, or my OS upgrades, etc. 
I remember answering a relatively similar question on StackOverflow: [How to zero out array in O(1)](http://stackoverflow.com/questions/10797284/how-to-zero-out-array-in-o1/10797852#10797852). Of course, the tradeoff in the solution of using a generation is that each access requires a check against it. I would be interested to know about alternative approaches.
How is this going to work with those with C# plugin? is it a separate plugin? EDIT - never mind. question is answered in the comments on the page. 
That's what you get for using C++.
Such optimizations are quite use case specific, so its hard to tell without reading the article. Obvious general approach would be based on vector&lt;unsigned&gt; &amp; memset/fill (i dont think vector&lt;bool&gt; exposes its memory anywhere). Your solution would probably not scale to bits well, though ;). Lets wait for update.
It would be nice to have prebuilt binaries for the poco libraries but linking against them isn't too bad. I've only used them in a toy app but I put them in an external libraries area and just linked against them after they were built and all worked fine. Integrating them into my build otherwise seemed like it would be a nightmare.
Blog post is [here](http://upcoder.com/9/fast-resettable-flag-vector/).
*Disclaimer: I'm a dev on the product.* You'd find most features useful to you as a developer in both products: syntax highlighting, live templates, navigation features, find references, rename and introduce variable/function refactorings, auto import, smart completion and other coding assistances, etc. What we see as our main advantages compared to VAX: * Better understanding of source code. As far as we can tell, VAX often uses heuristics and just can't handle some of the more advanced or recent C++ constructs. ReSharper's aim is to be fully correct when processing user's code, from performing macro substitutions, to processing typedefs, overloading functions and most importantly - handling all the template magic (correct template instantiation and understanding of variadic templates). Unfortunately, better understanding comes at a cost - we've had to implement many parts of a C++ compiler frontend (a daunting task, as you can probably tell), and you might notice that perfomance is somewhat subpar to VAX at the moment, but we're working on it. * Better code understanding allows us to provide better navigation (derived symbols, declaration/definition, specializations searches), more precise reference search and safer refactorings. * Our aim is to implement many of the refactorings that C# users are used to in future builds: extract method, change signature, safe delete and others. * Last but not least, ReSharper includes a host of code analyses, context actions and quick fixes that aim to help developers catch and fix potential code quality issues, redundancies and compiler warnings, or apply common code improvements. To provide some examples: ReSharper can tell if a qualifier is redundant, replace a C-style cast with a suitable C++ one, point out usages of Microsoft-specific extensions, catch missing or redundant template and typename keywords, find unreachable code and uninitialized variables. All available analyses are listed in the ReSharper's options dialog, and new ones are added with each new build. If you don't have time to try it out yourself, you can take a look at a [screencast](http://www.youtube.com/watch?v=pxxvRubo1XQ) which shows some of the noteworthy features.
You'd still be precompiling the libraries, wouldn't the build script be something to the effect of /external/&lt;remoteenvname&gt;/libraries ?
One would think, then why is it such a battle, while other libraries are so straightforward? 
Very cool, I was just going to start writing my own implementation, glad I saw this post!
It's summer time, you shouldn't waste it stuck in front of a computer, copy-paste the following: https://github.com/chriskohlhoff/executors/ slap your name on it and then lay back and enjoy your summer vacation and the Google provided quatloos. :) 
Thanks! Yeah I'm still going! I released a video this past sunday. Also feel free to check out /r/reconstructcavestory too!
I was thinking "Has massive leaking problems". Sounds like interesting convention though.
Thank you for elaborating! Wow, what is showed in the video looks great. I'm going to try it out now.
Is this from the asio dev?
Yeah CK is doing an executors proposal as well.
&gt; I was thinking "Has massive leaking problems". Well, you know...that just goes without saying. :p But you know...without that completely unexpected input the Titanic would have likely had a rather long lifetime. FUCKING USERS, THEY RUIN EVERYTHING!!!
Personally, i totally agree with you.
You should be learning a programming language out of a book and doing practice exercises as you progress through. Videos don't seem like the right medium for something like this. 
great news then - R# made a massive difference to the C# programming experience, looking forward to the same in C++
&gt; You might initialize a new enemy, Enemy enemyNew; but how would RAII take care of it? In all the game projects I see, developers always use pointers for these type of objects (enemies, players, shaders, et cetera). To initialize them is easy enough, with or without a pointer, but when it dies, you need to explicitly tell it to free up its resources. I usually use vector for such things. For example: // somewhere vector&lt;Enemy&gt; m_Enemies; void AddEnemy(Enemy&amp;&amp; enemy) { m_Enemies.push_back(enemy); } void RemoveEnemy(int index) { m_Enemies[index] = std::move(m_Enemies[m_Enemies.size() - 1]); m_Enemies.pop_back(); } The enemy destructor will get called when you remove it, and you can free up whatever you need there.
Those anyone has any info on how largely the networking will be based on asio? (Copy-paste or only the realy simple stuff?) Because It seems that asio is quite huge to be included as is. 
The guy claims that his optimized version has the same O complexity as the original. However in the O notation, one normally assums a word size of Theta(log(n)), so the optimized fill operation would have a complexity of O(n / log n) while the original is O(n). 
It would be simply awesome if cppreference could be extended to other libraries too. Like if you could upload your projects doxygen providing it meets a certain standard and is well maintained. I'd love to be able to browse the doc for boost, then the stdlib, then POCO and some small lib all with the same style and theme.
finally ! I would gladly print all that, eventually buy a book...
Looks good. How many talks will be given at the same time? Can I view other talks later?
The schedule is still in development. There will be six tracks.
&gt; Can I view other talks later? I see this questions continues to get non-committal responses. I know I'm not the only one who is going to be massively disappointed if it's not the case.
Since it isn't actually bit vector, why not go the full hog and have guaranteed O(1) clear operation? (Dense + Sparse array "algorithm")
all comments fully appreciated! (needless to say to star it if you like)
Any thoughts on Project Panama in this context? // https://www.techwell.com/2014/06/project-panama-unites-java-and-cc
Thanks for sharing. It is a shame but I haven't heard about this Java project. At first sight, it sounds as an excellent idea. I guess Panama is much more ambitious in the sense that it looks like a complete redesign of Java Native Interface. I will look at it in details later...The CJay repository is a JNI wrapper to C++ -&gt; Java.
Using clang might have been a viable solution, however we decided against going this way for several reasons: * ReSharper is a managed codebase. Using clang would meant introducing a large native component into the codebase, and would require a lot of work to integrate it with ReSharper's interfaces and data structures. * We'd need to support several Microsoft-specific language extensions and quirks which clang does not handle yet. * clang would probably be quite slow as a completion backend and as a solution indexer. Even QtCreator, which has enabled experimental usage of clang code model, retained its old parser for initial indexing. Having our own code model allows us to do other optimizations: for example, ReSharper saves symbols after a header has been processed, and can use them later for resolve without doing a full reparse if this header is used in another .cpp file.
Have you looked at the (now retired) CNI interface from gcj? https://gcc.gnu.org/onlinedocs/gcj/About-CNI.html It was retired because nothing else (e.g. the Sun/Oracle JVM) supported it, but presented a very nice API the last time I looked at it. At least, compared to JNI. But that's not hard.
My first impression is "wow nice, so I can easily use the JVM in my C++ apps ? "
Interesting; I've been thinking about what it would take for a complete safe-integer solution and this seems like a nice start. Ultimately, I'd want to support a few more things: 1. configurable {over,under}flow policy: * throw * modulo (e.g. unsigned) * undefined (e.g. "whatever the underlying native type does") 2. sized integer types, e.g. sized_integer&lt;32&gt;, optionally non-natively-sized int types as well. 3. bounded integer types, e.g. bounded_integer&lt;-10, 5&gt; The current proposal for an unbounded std::integer type gives me hope that there might be more movement in this direction for standard C++. The native C integral types are just too error prone to be allowed to live. 
&gt;The native C integral types are just too error prone to be allowed to live. I agree, if only because of their variable width. I love `&lt;cstdint&gt;` and `std::size_t` though.
I think it's a cool idea but I'm more of a fan of something like this: [https://github.com/danielearwicker/cppjvm](https://github.com/danielearwicker/cppjvm) Personally, I don't think the run-time binding style offers much over raw JNI except a nice, more C++ idiomatic interface. If you generate wrappers directly you don't have to mess around with signatures and you get compile-time parameter validation and your IDE completion can help you methods, classes, and arguments. Looking at your code example it doesn't seem much easier or shorter than using raw JNI IMHO.
We'd very much like to record all the sessions and make them available online. But doing so is very expensive and we are not yet ready to announce that kind of commitment.
Yes I had. As you said since it is a retired project I have concentrated my efforts in wrapping standard JNI.
Apologies for the necromancy, but I found your post and was puzzled by it as well. &gt; I didn't include erase, but that should show the same perf characteristics It's much different in this instance. Approximate Stroustrup's remove with: template&lt;typename T&gt; void remove(T &amp; c, int val) { int index = val % c.size(); auto remove_pos = c.begin(); std::advance(remove_pos,index); c.erase(remove_pos); } And change test() to: template &lt;typename C&gt; void test(std::string name, std::vector&lt;int&gt; v, C &amp; c) { { std::cout &lt;&lt; "testing " &lt;&lt; name &lt;&lt; "..." &lt;&lt; std::endl; timer t; for (auto val : v) insert(c, val); for (auto val : v) remove(c, val); } if (c.size() != 0) std::cerr &lt;&lt; "ERROR: output is not big enough!" &lt;&lt; std::endl; }
No you can't use the JVM but call methods and convert some STL types more directly. In any manner, I got your point, and maybe seamlessly is too strong here.
clang has an overflow detector that found bugs in SafeInt. http://embed.cs.utah.edu/ioc/ It is a runtime detector though, so you need some good tests to take advantage of it.
While the specified size is great, they all still suffer from the narrowing implicit conversions problem and (for signed types) undefined {over,under}flow behavior.
Thanks for the reply!
I have updated the tests to incorporate removes as well: https://ideone.com/MN7DYK As predicted, erase() performs the same as insert(), with std::vector performing an order of magnitude slower than std::set across the whole test
Your remove operation is different from Stroustrup's specification. He specifies removing the item at am randomly-selected index -- not a randomly selected value. Finding the iterator for the 1375th member of a vector is trivial. Doing the same for a set is much more involved. Edit: I had meant this to reply to your other post. I am on my phone now, will add more later.
Thanks again for your feedback! I added a new local branch to work the reflections and avoid the ``CJ::setSignature``, or in other words, shortcut the step that you have to define the descriptors of each method. The part that you mention the `java.lang.String`` was already implemented. You can call ``CJ::j_cast&lt;jstring&gt;("my_string")`` to convert ``std::string`` "myString" to ``java.lang.String``. To cast back from ``java.lang.String`` to ``std::string`` one can use ``CJ::j_cast&lt;std::string&gt;(jobejct)`` (pay attention to the ``c_`` and ``j_``). I went far beyond that. For example, one can ALREADY convert ``java.util.ArrayList`` and ``java.util.Map`` to ``std::vector`` and ``std::map`` using ``CJ::c_cast_vector&lt;T&gt;`` and ``CJ::c_cast_map``, respectively. Please if possible, look at the implementations in the file ``UnitTest.cpp`` and the called java methods in ``java/example/Example.java`` Let me know.
Sweet. Although I have to confess that my gripes have never been with the compiler. Kudos, though.
&gt; std::advance on a random access iterator will be an O(1) operation (just increment a pointer once). This experiment requires iterating through the elements in an O(N) fashion, to demonstrate the CPU cache behaviour I also tried this with stepwise increments to the vector iterator, it didn't change the runtimes at all. &gt; Im not sure what the line 'int index = val % c.size();' is doing. How does this calculate is the location of the element to be found? It selects a random index to remove (that is still within the container). The modulus just ensures that it doesn't try to remove an element past the end of the container. If I was less lazy, I would have generated a new random sequence for order of removals, rather than using the existing one.
Your code: bool remove(std::set&lt;int&gt; &amp; c, int val) { return 1 == c.erase(val); } Doesn't meet the spec that Stroustrup laid out. That removes element with value val, not the value at *index* val. The source I used was [this](http://www.stroustrup.com/Software-for-infrastructure.pdf), see the page labelled *50*. &gt; Once the N elements are in order, we remove them one at a time by selecting a random position in the sequence and removing the element there. For example, if we choose positions 1, 2, 0, and 0 (using 0 as the origin), the sequence should shrink like this: &gt; 1 2 4 5 &gt; 1 4 5 &gt; 1 4 &gt; 4 So that's where I get my interpretation from.
People learn different ways, but I agree practice should be done no matter how you are learning.
/analyze and /sdl Go tell Google to build Chrome with mingw, then come back and say that it is a viable solution. Incidentally, does mingw target Xbox? 
Even if it is big, it covers mostly basics so I would hope that it'll be mostly copy'n'paste
At the bottom of the page, it's advocated to use Casablanca if one uses C++11. I just want to chip in that it's possible in Qt to use lambdas like this: manager = new QNetworkAccessManager(this); connect(manager, &amp;QNetworkAccessManager::finished, [&amp;](QNetworkReply* reply) { //Parse response here }); manager-&gt;get(QNetworkRequest(QUrl("http://google.com"))); 
I'm wondering, is there any chance of enlisting the help of C9 in particular and/or GoingNative organizers in general? One of the strongest points of GN were the high-quality videos of the talks -- and you could watch them later (either if you didn't make it -- or, as it can also be of importance in case of larger, multi-track conferences like CppCon -- attended other sessions) as well as refer the others to particular talks (including your colleagues -- not every org can afford to send the entire team on a conference, especially internationally). The presence of materials (at least the slides) that you can refer to afterward is a significant added value -- notwithstanding the extra opportunities (like networking) that physical presence can bring. // I realize that you're perhaps not the person to ask about this, the only reason I bring this up is that it seemed that some kind of support [was being hinted at before](http://blogs.msdn.com/b/vcblog/archive/2014/04/03/cppcon-the-cpp-conference.aspx): &gt; The GoingNative organizers and the Visual C++ team are going to put their efforts behind organizing and promoting the new CppCon to be held in Bellevue, WA September 7-12, 2014. 
I think it's specified in that way to exercise a mixed-use case. The vector test changes far more data than the set or list test, but the list and set read more memory (especially with the erase operations specified). It would be possible to create a boost::multi_index instance that is specialized for those operations. That may perform better than the vector.
Plumbing in an appropriate multi_index container looks about like this: #include &lt;boost/multi_index_container.hpp&gt; #include &lt;boost/multi_index/ordered_index.hpp&gt; #include &lt;boost/multi_index/identity.hpp&gt; #include &lt;boost/multi_index/member.hpp&gt; #include &lt;boost/multi_index/random_access_index.hpp&gt; using boost::multi_index_container; using boost::multi_index::indexed_by; using boost::multi_index::ordered_unique; using boost::multi_index::random_access; using boost::multi_index::identity; typedef multi_index_container &lt; int, indexed_by &lt; ordered_unique&lt;identity&lt;int&gt; &gt;, random_access&lt;&gt; &gt; &gt; multi_index_int; void insert( multi_index_int &amp; c, int val ) { c.insert(val); } void remove( multi_index_int &amp; c, int val ) { multi_index_int::nth_index&lt;1&gt;::type &amp; random_index = c.get&lt;1&gt;(); int idx = val % c.get&lt;1&gt;().size(); random_index.erase(random_index.begin()+idx); } In my own testing, it's still slower than std::vector and better than the std::set code I've been using.
Herb Sutter and the GoingNative organizers are not just involved in the conference; they have been an integral part of the CppCon planning committee. Herb Sutter and other GoingNative organizers have been an active part of the organizing committee for CppCon. The CppCon staff is sort of a composite of other conference staffs and community organizers: many C++Now/BoostCon organizers are involved, Meeting C++ organizers are involved, the GoingNative organizers are involved, the Standard C++ Foundation is involved (obviously, as the Foundation is hosting it!), and various other members of the C++ Standard committee are involved. I know this is a cheesy tagline, but CppCon is a gathering of the /entire/ C++ community, hosted by the /entire/ community :). P.S. Jon Kalb is, by the way, our head honcho; he is the conference chair. So, he's generally the right person to ask! -- Bryce (Jon's minion-in-conference-organizing)
Thanks for the reply! So... any info on whether C9 is going to be involved in the video-making part as in GN? :-)
I was about to suggest you answer my [SO question](http://stackoverflow.com/questions/24542936/vector-vs-map-performance-confusion) but someone else beat you to it. They also included the use of an 'order statistical tree' that beats all of these on the removal clause. I wish I had more upvotes for you!
Thanks for the link. The OST data structure is new to me. It would naturally be useful for calculating statistical data (presumably that's what it is named for), like decile limits from the full set.
&gt; manager = new QNetworkAccessManager(this); I get uneasy nowadays if I see something like this. This is one of the reasons I like to stay away from QT, because it virtually forces this kind of coding style. (i.e. "dumb" pointers owning resources).
I'm a pretty frequent contributor to Stack Overflow ([my profile](http://stackoverflow.com/users/150634/joseph-mansfield)) and have been wanting to write a bit more about C++, so I worked on my website and have started posting articles. I recently had one of my answers on SO [hit the top of /r/programming](http://www.reddit.com/r/programming/comments/20e096/why_should_i_use_a_pointer_rather_than_the_object/) and wanted to elaborate on it more, so that's what this article is about. Enjoy!
Currently I am only using pointers for one use: weak non owning references. Consider a tree of objects where objects own and create their children... Occasionally a child needs to refer to it's parent in a non owning fashion. Pointers can be used for this and can be initialized in constructors.
I have covered cases in which they might be needed or are appropriate. I don't think I'm making a blanket statement, just making sure that it's clear that raw pointers are often not the appropriate type. They can be used, sure, but they're just not as expressive as other types.
If you don't need to be able to change an object's parent, that specific use-case could be done with a reference.
I've coded one: it's called `Ptr` (naming conventions...), and its only sugar is that it throws on accessing a null pointer (but not an invalid pointer, for obvious reasons) because process reboot is costly so I prefer a constant penalty to a sudden performance drop (and the accompanying DOS).
`std::weak_ptr` is intrinsically tied to `std::shared_ptr`. It provides a similar interface (for example, you can get the `use_count`), but doesn't contribute to the reference count.
If object a creates object b in its constructor there is no way to pass a weak_ptr to a to b's constructor. Try it.
TL;DR: small children shouldn't play with fire.
The article itself may make that assertion, but the title of this post is quite a blanket statement: "unsuitable for modern C++" I admittedly only skimmed the article itself and only noticed the title difference at second look. Posts that seem like absolute statements don't catch my attention very well.
"Don't catch my attention"... yet you skimmed and commented. 
As opposed to reading thoroughly, yes.
You can do it with `enable_shared_from_this`, but I'm not convinced that using `enable_shared_from_this` is ever a good idea.
If the parents lifetime is guaranteed to outlive the child then a raw pointer is fine and even recommended. Herb Sutter made that recommendation somewhere, but I can't find it on his website. It could have been in a talk.
That's mainly the point IMO. If you need a pointer to the parent but don't want a circular reference, that's how you do it. Pass "this" and store it in a weak_ptr in the child.
raw pointers are beautiful for non-owning, optional objects that are destroyed within the scope they are acquired. std::string::c_str() has been doing this since forever and not a lot of people complain about it which means that people get it (and people getting it is 80% of the battle.)
Other problems arose when I tried that... If memory serves it was because the owning object didn't yet have a reference count (because obviously it's ctor hadn't returned yet and the first reference hadn't yet been assigned to it).
Just to let everybody know, I am locally developing a new branch that will automatically call ``java.lang.reflect.*`` to avoid all the messing with JNI descriptors (``CJay`` ``setSignature`` method). Changes inspired by @sneakpants comments.
Replacing foo(const char *) with foo(const std::string &amp;) won't help you at all since std::string has an implicit const char * constructor. As for the rest if this article, the option to use pointers when it makes sense is the reason I use C++, if I didn't need that stuff I'd choose a simpler language.
How can a random nullptr be introduced? Trying to imagine where this could not be preventable and I can only see weak_ptr being the case.
Just some weeks ago I was helping a C++ beginner with his project and I always told her not to use raw pointers, shared and unique can co whatever you want. Then she wrote a tree class and I noticed that raw pointers which do not have ownership can actually be used and had to explain that to her. A "dumb pointer" would be a great addition to the standard library.
&gt; Consider a tree of objects where objects own and create their children... Occasionally a child needs to refer to it's parent in a non owning fashion. Pointers can be used for this and can be initialized in constructors. The very fact that this question exists, and it generates a thread within a thread of dubious answers is the living proof that C++'s promise of "automatic" memory management still has a long way to go. I had exactly the same problem laying down my tree structures during my initial "yay! smart pointers FTW" moment.. only to quickly realize that, by using them, my code got ugly and it ended up using unwritten rules anyway (ie, Node* parent is an exception) and a lot of problems wiring functions.. should I get a shared_ptr here?..oh sorry I meant a const shared_ptr&lt;Node&gt;&amp;... Or a Node*? By the time I was done you could hardly see the real code anymore, all cluttered by the red taping of the const shared_ptrblablabla and ugly .get() all over the place. After a couple of weeks of this nonsense I realized I didn't want to spend my next 5 years dealing with this shit and I went back to a pure pointers and the SIMPLE convention that Nodes own their children and WILL destroy them in their destructors. Simple code, easy on the eyes.. never created any problems. It's depressing to see how tree data structure really show the weakness of a language.. garbage collected languages really shine when implementing trees. 
Well, the very premise of a pointer is that it may be null (otherwise you would be using a reference).
Indeed! Let me fix that!
Well sure, but can't you have some sort of sanitization on the input to remove null pointers? Thought if the pointer is also used as flag of a no op when it's null, then no way to avoid it 
Speaking of QtCreator, are you guys planning to ever ship this plugin for QtCreator or is it going to be strictly for Visual Studio? I'm trying to avoid Visual Studio as much as possible for C++.
Does it ask you to include &lt;iostream&gt; with the following code? int main() { std::cout &lt;&lt; "hello" &lt;&lt; std::endl; std::cin.get(); return 0; }
One would hope that the *developer* pays attention to what it suggests you include!!
I'm not getting any suggestion at all unfortunately.
I always use references as method parameters instead of raw pointers. One can use references as members too but I'd prefer std::reference_wrapper instead (or a shorter named typedef). This is mainly because of references being inherently non-null. Well, you know if someone writes a code for passing a reference to a null-pointer, I count it as trespassing.
Yeah http://i.imgur.com/2FswBWU.png
Are you including the `.cpp` into your project? That's what I needed to do in order for it to get recognized and for resharper do its thing.
What do you mean I am including the .cpp in my project? This is how my project looks: http://i.imgur.com/ErGYYSv.png
I agree with your point that this is kind ridiculous.... But I don't think abandoning smart pointers is the right call... They are absolutely needed in a language that lacks garbage collection but has exceptions... This problem is infrequent enough that using a pointer in this case doesn't bother me... I may implement my own weak pointer...
That sounds plausible but I've never used XCode so I can't say.
The blog post only explains already existing optimizations performed by current compilers - there is nothing new that would have to be proven. 
&gt; Simple C++ Example Using Virtual Dispatch It doesn't.
Nice summary. Is there a particular reason that override couldn't have had the same placement in the grammar as virtual? I'm thinking it would be "nice" or useful if we could do something like: #ifdef HAS_CXX11 # define OVERRIDE override #else # define OVERRIDE virtual #endif class Lol : public Base { ... OVERRIDE int someVFunc(); };
Slides &amp; demos: http://blogs.microsoft.co.il/sasha/2014/07/06/materials-sdp-2014-sessions-workshops/
That's actually my point. Override and virtual are redundant (override adds more info). I use virtual in child classes to indicate to the reader that I'm overriding, in c++98. So it would be cool if I could use override in place of virtual in those cases. I'll probably continue to use virtual in addition to override so it's easy to see at a glance. And the example I used, I hope, was "override *int* somefunc()".
If you're just indicating it to the reader, the macro doesn't have to expand to virtual - it can expand to override or nothing. Regarding the placement of override, the point is that it's more difficult to disambiguate the use of the token at that point. Where it appears now it's unambiguous. It was important for disambiguation to be straightforward to get contextual keywords into C++11, especially since the current override syntax (not using attributes) came in very close to the end of C++11 standardization.
I am grateful that attribute syntax was not used to do this! Good explanation, thanks.
gcc 4.9 contains a lot of new devirtualization effort, so if you are working with code where that might matter, try upgrading. 
I really like these new keywords, reminds me of C#. Also a wonderful explanation on the keywords as well.
Personally I think that every class should be final unless you have a really good reason not to do that. Why? Multiple reason. Final can make your classes faster (it can effectively eliminate virtual function calls), it makes it harder for people to do non-obvious things with your code (you design the class to accept functors to change behavior, not rely on inheritance. Or, composition over inheritance). And all in all, it makes an API easier to reason about. You know when you stumble across a final class that you aren't supposed to inherit that class, monkey around with some magic function and then pass the mess around. Overall, this makes things much easier to reason about. Pure virtual classes + small object hierarchies + mostly final classes = an easy to reason about code base that should be pretty performant.
There aren't really any deprecated language features - most of the changes in C++11 and beyond are idiomatic. Though in some cases keywords have been repurposed or overloaded (i.e. typename, auto, static). The only actually deprecated aspect of C++ that you might run into is auto_ptr. And I suppose, while not officially so, you can probably consider assigning 0 (or NULL) to pointers in place of nullptr to be deprecated as well.
I use YouCompleteMe in vim, which uses clang for static analysis. Here are my compiler settings: # enables all warnings (both past and future!) '-Weverything', # I'm writing C++11 code, I don't want to be warned about not being c++98 compatible '-Wno-c++98-compat', '-Wno-c++98-compat-pedantic', # causes warnings if you use static values '-Wno-global-constructors', # causes warnings about main and statics '-Wno-exit-time-destructors', # causes warnings about class/struct padding '-Wno-padded', # causes warnings from conditional macros that check an undefined value (e.g. #ifdef _WIN32 in linux build) '-Wno-unused-macros', # similar to above, but complains about their lack of definition '-Wno-undef', You can combine all these in to your compile: clang++ -Weverything -Wno... file1.cpp file2.cpp -o prgname Or alternatively look in to setting them as CXXFLAGS in a cmake project. This will warn about some deprecated features, but most C++ is around to stay. The only thing you want to really worry about if you're trying to modernize your C++ is to use the STL in place of any and every type that you can. Prefer std::vectors/std::arrays over C-arrays[], use std::unique_ptr/std::shared_ptrs over raw pointers and so on. For a full list of warning message options, see http://clang.llvm.org/docs/UsersManual.html#options-to-control-error-and-warning-messages Everything on that list will likely have a -Wsomething, and a -Wno-something version. Since you're enabling everything by default messages that you do not want to see will need to be added with the -Wno-something syntax. Footnote: People will argue about performance of the some parts of the STL over their C-like counterparts, like shared_ptr for example. However, this kind of premature "optimization" leads to bugs and potentially no discernible benefit. It's better that you learn to profile your code and find out WHEN it would be beneficial to use raw constructs instead. Though the majority of the time the performance loss will not be due to STL but to your own programs design! So take any blog posts you see bashing the STL with a pinch of salt, they may be right but often don't tell you WHEN it's right to do it that way :D
Don't forget exception specifications are deprecated.
I'd also add `-Werror` to turn warnings into errors.
That doesn't make sense. How do you then write allocators? At least malloc/free/something for aligned allocation is needed.
There's one big case the author missed in his article: interop with other languages/DLLs. You can't really use smart pointers there, or any C++ objects whatsoever. Pointers and primitive types are the only thing you can use.
Yeah, if you're only checking your code during your actual compilation process that's probably worth doing, unless you have savant-like ability to read compilation messages :D YCM shows warnings while editing though so I just turn a blind eye to the ones I don't want to fix, such as -Wunused-parameter warning on a function whose prototype I don't control (though I definitely do want to know about forgetting to delete an unused param on my own functions so do not disable it completely).
Fair enough, but I was talking about user code. I don't think it's unfair to say allocators are outside of the scope of 99% of user code. Given OP says he wants to learn C++, certainly for his use case it may be of help. Interesting discussion in the comments on [this stackoverflow answer](http://stackoverflow.com/a/22251080/955273).
Warnings exist for a reason, and as such we *always* have `-Werror`. To mitigate the problem you described, I'd do one of the following: void function(int foo, double /* bar */) void function(int foo, double) I favour the 1st because the parameter name is still visible inside the comment, so if you have meaningful variable names you know what the parameter is, but it doesn't throw an unused parameter warning. Of course the world isn't black and white, so there are always exceptions to the rule. One example is explicitly checking a floating point variable against 0. It's a perfectly valid situation (eg: it's been *set* to zero, not computed). In these cases we [`#pragma` ignore the warning](https://gcc.gnu.org/onlinedocs/gcc/Diagnostic-Pragmas.html) for that instance only. #pragma GCC diagnostic ignored "-Wuninitialized" foo(b); /* no diagnostic for this one */ #pragma GCC diagnostic pop
Cool, I wasn't aware of that pragma. Any idea of how portable it is? I presume clang will accept it, any similar functionality for VS? Edit: meant pragma said syntax :D
I mean custom allocators for your own data structures.
No, they aren't portable unfortunately. VS uses warning codes. GCC uses strings. I only compile with gcc at the moment, but I think you are correct about clang accepting it. Here is the [VS pargma](http://msdn.microsoft.com/en-us/library/2c8f766e.aspx) syntax. If you want cross platform code you could hide the differences in a macro... something like: #define WARNING_DISABLE(gcc, vs) \ #ifdef _WIN32 \ #pragma warning( disable : vs) \ #else \ #pragma GCC diagnostic ignored gcc \ #endif (and a similar one for `WARNING_POP`) Then you'd use it like: WARNING_DISABLE("-Wuninitialized", C4700) foo(b) WARNING_POP Note this is untested code! 
I also suggest reading [this](http://amattn.com/p/better_apps_clang_weverything_or_wall_is_a_lie.html) (which I only just found right now!)
Final classes will break code relying on [EBO](http://en.cppreference.com/w/cpp/language/ebo), which I predict will cause nasty regressions until final-sensitive EBO is generally applied.
omitting the argument name is universal (language specified), the other thing you can do is: void function(int foo, double bar) { (void)bar; It looks weird, but it doesn't generate code. Some people #define it to a macro call UNUSED or similar.
Yes, Clang C++ Modernizer is what you're looking for (although it's generally on a tooling side, as opposed to being a compiler): - http://clang.llvm.org/extra/clang-modernize.html - http://clang.llvm.org/extra/ModernizerUsage.html - http://blog.llvm.org/2013/11/google-summer-of-code-c-modernizer.html WIP: https://cpp11-migrate.atlassian.net/secure/RapidBoard.jspa?rapidView=1&amp;view=planning
There are also ~~extern~~ export templates that were removed completely without a grace period and you're no longer allowed to assign string literals to `char*` and the likes (which was only deprecated in C++03). If you look at `[diff.cpp03]` (C2 in n3337), you'll find that there are actually a lot of differences.
It's a rule of thumb. Pretty much correct in every case for novices, and if you want something more advanced, you'll have to understand the reasons behind the rules and should know when to apply them or not.
That won't work. It should be: #ifdef _MSC_VER #define WARNING_DISABLE(GCC, VS) __pragma(warning( disable : VS)) #elif defined _GNUC_ #define WARNING_DISABLE(GCC, VS) _Pragma("GCC diagnostic ignored" GCC) #else #define WARNING_DISABLE(GCC, VS) #endif
I think you mean `export`ed templates. `extern` templates are new to C++11.
You're absolutely right, thanks.
&gt; I use helper functions in cpp files, but don't prototype them in headers Those helper functions should be static or in an anonymous namespace. -Wmissing-prototypes only complains about exported functions.
data() method returns a pointer.
cool, hadn't seen that one. Which spec of the language is it in?
Yes, fair enough, my point is that at the application-level you can use `std::vector` (and only drop to passing a view into the existing container to a C-style API when it's truly needed). I guess that's one remaining category of pointers use (non-owning / views, etc.) no one has a problem with. // Well, that, and `this` ;]
what do you mean?
C++11; I guess however that the main-reason for it's existence is this: std::tuple&lt;int, float&gt; fun() {return std::make_tuple(3, 4.5f);} int main() { int i; std::tie(i, std::ignore) = fun(); }
Nice article! And if you're looking for a nice language that enforces these good habits, try Rust. SCNR!
&gt; # I use helper functions in cpp files, but don't prototype them in headers '-Wno-missing-prototypes' Doesn't this disappear when you make the helper functions static? It should...
Facepalm ... 
Hehe, I guess I should review the core language features at some point. It's been a couple of years since I started using C++ and if I'm honest there was more than one moment during the initial learning process when I felt my brain leak out of my ear :D
I'm not sure I understand how the final keyword breaks EBO. Could you elaborate?
The feature that allows you to specify which types can be thrown from a function. It was only ever used to try to imply that *no* types can be thrown, but that usage is replaced by a dedicated keyword `noexcept`.
"Maybe". Reference members are strange beasts and break a non-trivial number of things (default copy/move assignment, for example).
You're misunderstanding - the point is not to be able to allocate memory off the heap via `std::malloc()` so that you don't use `new`, and free it with `std::free()` do that you don't use `delete`, it's that idiomatic C++14 advises against manual memory management at all. No `new` *means* no `malloc`, no `delete` *means* no `free`. `malloc` is just the C equivalent of C++'s `new`, and `free` for `delete`. Idiomatic C++14 says that if you want to allocate an area on the heap and obtain a pointer to it, you use `std::make_unique`, which returns a `std::unique_ptr`. No more `malloc/new`, and no more naked pointers for managed resources. No more `free/delete` when you're done with the memory, `std::unique_ptr` will do that for you automatically. Manual memory management *is* error prone - memory leaks and dangling pointers are one the biggest reasons why pure OO languages were developed in the first place. C++14 smart pointers seek to address this error proneness.
As the name suggests, the EBO requires being able to inherit from the class. Making a function object final forces it to be a member instead of a base, and thus occupy space.
I expect this to be a little controversial because many people are pretty excited about using `auto` a lot, but I strongly feel that it's being misused without really thinking about what it *means*. The article should explain why! Thanks for reading.
Not making any judgments, but I would like to point out that this is in opposition to Herb Sutter's advice, "Almost Always Auto". Read the section in [this article][article] titled "(Un)readability" for more information. [article]: http://herbsutter.com/2013/08/12/gotw-94-solution-aaa-style-almost-always-auto/
I actually meant to link through to that article. Thanks for reminding me. Yeah, I disagree with his suggestion because it doesn't look ahead to what `auto` will mean when we have concepts and generic lambdas. However, many of his arguments (particularly the example in (Un)readability) actually back me up. He's promoting writing against interfaces, not implementations, which is actually what I'm promoting too. His definition of `append_unique` has an argument `Container&amp; c` and he remarks about how it's neat that we only need to know that `c` is a `Container`. Well here he is just mimicking concepts by naming the template parameter appropriately as `Container`. When you have `auto`, that's an entirely different thing, because you aren't being told what the requirements of the variable are. Edit: Added my response to Almost Always Auto to my article.
The macros seem unnecessary. You could just do "app.route("/whatever") = [](){...}"
I strongly disagree with this. I am with Herb on this one: `auto`, for the first time ever, provides us with the opportunity of having an (almost) unified declaration syntax in C++. We should *embrace* this and clean up with syntax cruft introduced by C-style declarations, which led to a lot of horrors, such as the most vexing parse. Embracing and proliferating AAA may lead to slightly more verbose but in the long run much cleaner code. Your article derives some semantics for `auto` (“impose no restrictions on the type”) – but this is just a convention. By making AAA idiomatic through usage, we can re-purpose `auto`’s semantics to something more usable. In addition to that it has been my observation (expanded [on Stack Overflow](http://stackoverflow.com/a/41514/1968)) that the exact type of an object is rarely useful, much less required (OO people know this as “program to an interface, not an implementation). For AAA this means: derive an object’s interface through its usage, not through its declaration. This isn’t so much an explicit advantage, but it shows that the often-cited “disadvantage” of `auto` does not actually exist.
I find that my suggestion is more unified *and* cleans up the syntax cruft you're talking about. The latter happens when we get concepts, so we just have to wait for it. It's more unified because it treats the type deduction with `auto` like the type deduction with template arguments.
I have to admit that I don’t understand what you mean by that: how does your proposal lead to a unified syntax? To clarify, what I mean is that with AAA, every (local) variable declaration will look like this: auto name = expression; // And in the special case of value init: auto name = type{}; // or auto name = type{expression}; This visually unifies and highlights declarations and makes code more readable as a result. Without AAA, by contrast, you can easily lose count of the different syntax forms for declarations (there are at least 20) – and since they’re not introduced by a common keyword (`auto`) they get lost in the noise (I never understood why C-like languages don’t have keywords for variable declarations such as `var`, `let` or `Dim` used in other languages).
I'm unifying these: some_type name = /* must be a some_type */; SomeConcept name = /* must meet requirements of SomeConcept */; MoreGeneralConcept name = /* must meet requirements of MoreGeneralConcept */; auto name = /* can be anything */; With these: void foo(some_type); // Must take a some_type void foo(SomeConcept); // Must take something that meets SomeConcept requirements void foo(MoreGeneralConcept); // Must take something that meets MoreGeneralConcept requirements void foo(auto); // Can take anything In many cases, we'll be able to use a concept name instead of the type itself, which supports the "program to an interface" idea. I think pretending that `auto` is a "this is a variable" keyword like `var` is only worse. I know that it sucks that C++ declarations aren't easily identifiable, but that's not a problem for `auto` to address. Perhaps C++ just needs an alternative declaration grammar, but my proposed convention would still apply.
&gt; I know that it sucks that C++ declarations aren't easily identifiable, but that's not a problem for auto to address. Perhaps C++ just needs an alternative declaration grammar The thing is that an alternative grammar is *never* going to happen. Whereas to shoehorn an existing feature into a new idiom has happened several times before (in fact, yours and C++14/17’s proposal do the same for `auto`, albeit less outrageously). (I like how you update your post to integrate clarifications.)
To me, readability and being able to look at the code and understand it is top priority. I am very much of the mindset that language features such as auto and var take away from the understanding portion. Seeing exactly what the function you are calling returns helps that. I do think it has its places, foreach is a perfect place for it. I get why its there, I just feel it takes away from a programmer who DIDNT write the original code and makes it more difficult to absorb. Even yourself 3 years down the line, I can see that developer going to many of the functions just to see what it returns. EDIT - typos
Yes! Finally a voice of reason. I know Herb Sutter continues advocating the "always use auto" approach, but he is simply wrong. Auto has its uses, but can dangerously reduce the readability of code if used indiscriminately.
I strongly disagree. One thing is constraints, another is type inference. Besides, auto allows for optimizations that sometimes are very hard to see. Scott Meyers made a talk where he showed several examples where using auto made the code more efficient. Auto is really great due to a lot of reasons: much better to refactor code that is using auto, better efficiency, better convenience, and (most of the times) better readability. 
This response only serves to support my argument (I'm surprised to see that lots of people agree with your point). You have decided an arbitrary point at which `auto` is acceptable, which is the point at which you consider that somebody knows enough about C++. The example was only illustrative and could easily have been more complex and used functions from other library. To understand how a variable can be used, I shouldn't need to decipher its initializer. It has nothing to do with it. It only introduces an extra step and couples the type of a declaration to its initializer. This is just poor coding. A more complex example (yes, it's made up, but it represents not understanding the initializer): auto betaTransform = magic::computeAlphaTransform(epsilon) * std::sqrt(theta); I want to access an element of the matrix form of `betaTransform`. How do I do it? What form is it in? What operations can I perform on it? Using `auto` does this to almost every declaration, just to a varying degree.
I agree with the author that 'AAA' style can make code hard to read. Working in Visual Studio makes it easy to let the IDE do the work and find out the type of an `auto` variable when needed. However, if I view my code in a raw editor, it can get quite annoying. Now I use `auto` liberally while hacking, but make a pass through the code and replace some `auto`s with concrete types once the form of the code has stabilized. I will still leave the `auto`s for iterators and ugly nested templates though. I like the idea of declaring variables with a concept name. I really feel that concepts will wake up C++ as a language, and suddenly everything will fit together.
I can see that `auto` is extremely convenient for some quick hacking. I'm not going to suggest it should never be used in that way because sometimes it's a good idea to be pragmatic.
Herb Sutter isn't a God you know... A lot of people disagree with Herb Sutter on C++ related issues, including Scott Meyer's and Andrei Alexandrescu. Too often, especially on this subreddit, people just take the word of Mr. Sutter or other people as if it's the final authority on an issue. It's rather disheartening.
This has been something that was request a few times on this sub-reddit.
Did I come across as exalting him? I just linked to another article written by a reputable guy which argues the other way. You know, for discussion…
This seems to be the intuition of several programmers but there is simply *no evidence at all* that this is true. You may feel that way but claiming it’s “simply wrong” strikes me as too strong – not least because I’m convinced of the opposite: I have not experienced any reduction in readability; on the contrary, as I’ve explained above.
&gt; He's promoting writing against interfaces, not implementations, which is actually what I'm promoting too. I think that using C-style declarations instead of `auto` goes against this tenet. AAA supports "writing against interfaces, not implementations" for the same reasons that guepier stated: &gt; For AAA this means: derive an object’s interface through its usage, not through its declaration. This isn’t so much an explicit advantage, but it shows that the often-cited “disadvantage” of auto does not actually exist. If I don't use `auto`, then each variable declaration must be preceded by an explicit type. So each time I change an interface, I have to revise the affected variable declarations and make sure the types are what I want them to be. To some extent, you have to do this even with `auto`. But the cases are far less numerous. &gt; When you have auto, that's an entirely different thing, because you aren't being told what the requirements of the variable are. It's only sometimes beneficial to have the programmer syntactically acknowledge that the requirements of an interface are being met. C-style variable declarations are at one extreme, and AAA is at another. But as Sutter mentions, we are used to hiding type information all the time. (Sometimes we don't want to hide the variable name or type, which is why I like Swift's grammar more than C++'s.) Considering all of the projects I've worked on in the past, I'd much rather be at the AAA extreme than at the C-style extreme. I also really like the uniformity in syntax that `auto` brings to C++ -- it's one of those features that really makes C++11 "feel like a new language". Everything's just so much cleaner.
That's simply untrue that there's no evidence at all. C++ isn't the only language that has type inference, other languages including even C# have it and had it long before C++. We can learn from how other languages have made use of type inference through coding conventions and guidelines rather than have to rediscover everything for ourselves. The coding conventions for languages that do "auto/var" type declarations typically specify that auto/var is acceptable to use when the type is part of the initialization. So this would be acceptable: var x = new Foo(); auto y = Bar(1, 2, 3); It's clear that x is a Foo and y is a Bar. This would be unacceptable: var x = SomeFunction(); It's not clear what x is, you'd have to look at the declaration of SomeFunction. Now it's very reasonable to think that maybe with C++ we may need to take insight from other languages and find ways to extend them to apply to C++ in spirit. That's fine and so maybe auto's use is more liberal in C++, but to say that auto should almost always be used is absolutely wrong. If it was the wrong approach for other languages, what makes us think we're so much special with C++ that it won't be wrong for us? It also is fairly vacuous, what does it mean to "almost always" do anything? If auto were the answer to everything, then why does C++14 have a new declspec(auto)? Why are there proposals for C++17 to overload auto and customize it's behavior for certain classes? The reason is because auto actually has many invisible pitfalls when used too liberally.
I don't understand the problem with your example. Sure, if you'd declare it as auto&amp; c; instead of Container&amp; c; you could surely argue against auto for the sake of readability. But actually I think the problem comes down to good naming, which is something being preached all the time. If you want something that's a container, then declare it as auto&amp; container in my opinion. That's way better than your 1 letter name c. That way you don't have a single place (the type in the variable declaration) where you can look for information, you instead got that information at every place of usage within the variable name itself. I don't care about it's type, so I use auto and give a variable name that gives information about what that variable is. It especially shines in generic templates which I often find overly complicated due to bad naming for one.
&gt; If I don't use `auto`, then each variable declaration must be preceded by an explicit type. So each time I change an interface, I have to revise the affected variable declarations and make sure the types are what I want them to be. To some extent, you have to do this even with auto. But the cases are far less numerous. This is not a problem with concepts. My suggestion is to use the full C-style type only for now, until we have concepts. Then, if the use of your variable fits into a more general concept, the type of its declaration should be that concept. &gt; It's only sometimes beneficial to have the programmer syntactically acknowledge that the requirements of an interface are being met. C-style variable declarations are at one extreme, and AAA is at another. But as Sutter mentions, we are used to hiding type information all the time. (Sometimes we don't want to hide the variable name or type, which is why I like Swift's grammar more than C++'s.) Considering all of the projects I've worked on in the past, I'd much rather be at the AAA extreme than at the C-style extreme. With the convention I'm suggesting, you only ever syntactically acknowledge the requirements that are necessary to understand the use of a variable. You use `auto` when it could be anything, you use a concept when it meets the requirements of one, and you use a specific type when it must be that type. If you use `auto` for everything, you lose way too much information. Suddenly the initializer is more important for working out what a variable is used for than the type of the variable itself. That's an unnecessary coupling. The initializer should be free to be whatever it wants, as long as it provides an object that meets the requirements of the declaration type. &gt; I also really like the uniformity in syntax that auto brings to C++ -- it's one of those features that really makes C++11 "feel like a new language". Everything's just so much cleaner. This is very interesting because it doesn't feel clean at all to me. It just feels wrong, like we're using something inappropriately. The problem with AAA is that it's not just the opposite side of the spectrum, but it fundamentally changes the way we think about `auto`. It becomes a kind of `var` keyword. I don't think that's a good idea because we'll end up with differing uses of `auto` throughout our code. Additionally, using `auto` experiences other problems, which I pointed out in my article.
Good naming can help, but it isn't enforced in any way. With concepts, if you tried to initialise `c` with anything that wasn't a `Container`, you'd get a nice error telling you so. And just because your variable is named `container`, that doesn't mean I can trust you 100%. Also, I prefer to keep type information out of the variable name when I can.
&gt; It's not clear what x is, you'd have to look at the declaration of SomeFunction. That's because your developers are idiots and used `x` and `SomeFunction` as identifiers. Instead of creating conventions to work around morons you should fire them. Continued idiocy like that is going to cause a lot more problems than the `auto` keyword ever could.
Wow...
It's going to be only for VS. Porting ReSharper is hard, and if we could do it, we'd probably make it run on our own IDE platform - IDEA.
I can absolutely trust a compiler to check types for me. I can't absolutely trust a programmer to correctly name a variable. Naming is not enforced.
Exactly. It's incredibly easy to write an example in which `auto` is hiding important information that should accessible at a glance. It also ties a variable declaration to its initialiser, so that if the initialiser changes, its type does too.
&gt; I can absolutely trust a compiler to check types for me. So then you agree that there's usually no good reason why you want to assume you do it better and tell the compiler what type it should assert.
Most of this article assumes that C++ has a set of features that it just doesn't have. Would it be nice to write: ForwardIterator it = begin(list); Perhaps, but we can't. Not yet anyway and possibly not ever.
Concepts are almost inevitable. They've long been implemented in a branch of clang and a working draft is being formalised at the moment.
Do you now...OK. Yeah, I can see that...because there'd be no other way at all the compiler could check type safety if you don't explicitly tell it what it usually already knows.
TIL, branch of clang == current language specification upon which we should develop our coding policies.
By that logic you should not do: Foo(Bar(1, 2, 3)); But instead should write: Type x = Bar(1, 2, 3); Foo(x);
&gt; Good naming can help, but it isn't enforced in any way. With concepts, if you tried to initialise c with anything that wasn't a Container, you'd get a nice error telling you so. Concept predicates don't really help with this. Its entirely one way, and you still get a good error if the type changes when using `auto`. auto container = ...; // If its not a container, we will get an error here that `size` can't be called containter.size(); &gt; And just because your variable is named container, that doesn't mean I can trust you 100%. And just because you use the `Container` concept predicate doesn't guarantee that I can trust you as well that you will just use the variable as a `Container`, for example: Container c = ...; c.find(x); // WRONG, but will still compile for many containers 
I don't see how that follows. My policy when it comes to function call nesting is that you can only nest functions up to one point of failure. If there are two points of failure then the function call has to be split up. This has nothing to do with auto, however.
 std::vector&lt;std::unique_ptr&lt;gtkmm::widget&gt;&gt;::iterator dirty_iterator = dirty_list.begin() Is pretty confusing to me. I would think that auto dirty_iterator = dirty_list.begin() is pretty clear, precisely because it _hides_ the unneeded information.
Not sure what you're expressing surprise about, but I'd hope it's not the idea that nondescript names are more destructive to a product than any perceived lack of information created by `auto`.
Concepts are a part of the C++17 Core Working Group, it's not just a branch of clang. Keep in mind that coding policies for C++11 were developed far before 2011, the year when C++11 finally got standardized and published. People were using C++11 features and hence developing policies and discussing proper use cases going all the way back to 2008 when GCC added a bunch of C++11 functionality, which back then was still known as C++0x.
Sounds like some major work was done internally to support this. Exciting to think we may have a first-class alternative to vc++ on Windows again!
and this is exactly my point. I know nothing else about your code, I dont even see the declaration of dirty_list. But in the first line of code I now know EXACTLY what it is and what its holding. Remember, I may not be the one who wrote this code. But now I do know something about it because it tells me what its an iterator of. .. and again, I never said auto doesnt help with readability. I do think it has its place, but when it comes to the understanding of the code, I believe it takes away from it, and ultimately hinder the developer(s) of that code. EDIT - more typos :(
Explicitly declaring types does **not** allow you to see exactly what a function returns, but rather allows you to see what it's being implicitly converted to. The point of `auto` is that when reading and understanding code it doesn't matter so much exactly what the types in question are, but more what you can do with them. 
I meant Bar to be a function call, not a constructor, if that helps. My point is that when you nest function calls you are doing the same thing as when you use auto. This: f(g(x)); Is like: auto y = g(x); f(y); (There are differences in move semantics as well, but I am trying to keep it simple)
But this is clearer: InputIterator dirty_iterator = dirty_list.begin(); It provides the *appropriate* information and hides the rest.
&gt; It also ties a variable declaration to its initialiser, so that if the initialiser changes, its type does too. This is generally what you want anyway. The few and rare cases when it's not you need to specify type. MOST of the time you honestly should not care or worry what type a variable is. That information is mostly useless and unnecessary, especially when compared to the semantics of its use and *the description provided by its name*. Consider the following: fun1(fun2()); That's an equally nondescript example. We can learn nothing from it and would be silly to make policy arguments on such an uninformative example. More helpful would be something that we might see in the real world: for_each(begin(elements), end(elements), printer()); All type information is omitted in this example the same as in any initialization (`printer` is probably a type but it could as easily be a function call). The same arguments made against using `auto var = some_call()` are at work here. Yet, unless someone has gone of the deep end here we should have a fairly good understanding of what this code does because we know C++ (those who don't should learn, not dictate policy). Further, we don't care what `begin` and `end` return. We just want to know that they work within the semantics of a call to `for_each`. That's the kind of thing the compiler is quite adept at telling us and why we use a statically typed language rather than a dynamic one. We don't need to go tracking down what each type in this expression actually is and we want the compiler to respond to changes in those returns by extrapolating and resolving the new types. Such is it with auto initializers. The name of the variable and its surrounding context is sufficient to gather most of the necessary information needed to understand the immediate code. When its not, explicit types isn't going to be a lot of help either. Now, it would indeed be nice if we could assert a set of semantics through concepts so that errors are more informative than the vomit we've gotten used to. But that doesn't exist yet. People like Sutter who recommend use of auto initialization are making those recommendations in the context of the current language, not a possible future version.
&gt; Keep in mind that coding policies for C++11 were developed far before 2011, the year when C++11 finally got standardized and published. It's worth discussing and playing with possible standards for the future. The OP's article though reads like an attack on policies used and recommended for the *current* language and the meat of that same article depends on features that don't exist yet. "Learn when you should and should not use it," is not a statement of anticipation or proposal to peers. It's a statement of tutelage.
I felt this way at first, but now I am really digging it. My code is much more intelligible and *extensible*.
No they are not the same and the difference is not trivial or minute. Also this is why auto is very confusing. It's very easy to make mistakes when using auto and a lot of people do not understand the subtleties about how auto works. I make this same mistake myself and see it in other people's code and it causes huge headaches because if you make a simple mistake using auto, the compiler ends up spitting out pages and pages of incomprehensible error messages that could have been avoided if you actually specified the type you intended to use, rather than the type the compiler thinks you meant to use. In order to nest function calls in a way that works according to your example, you would need to use this ugly hack: decltype(auto) y = g(x); f(std::move(y)); It's ugly and I don't see how one can go from my argument about using auto when it's clear and explicit what type will be inferred by the compiler, to now having to write your code using the above abomination and breaking up nested function calls. That decltype(auto) is being introduced in C++14 exactly because of how often this mistake pops up. decltype(auto) infers the EXACT type of the expression, whereas auto infers the type that would be used if that expression were passed into a template with some additional rules and exceptions about handling initializer lists. These two are not the same, surprisingly. In fact, C++14 and C++17 are adding a lot of extra jazz and complexity to how auto works to avoid common mistakes when people use it, like being able to overload auto (?!) and customize auto for your own types.
In that example, you don't have a variable declaration. It's just a bunch of operations applied to different objects and I agree, it is pretty readable. But as soon as you introduce a variable, the question becomes "Well what can I do with this variable?" or "What is its interface?". The `auto` keyword hides this. As soon as you see something in an initializer that you don't understand, you're missing important information: auto betaTransform = computeAlphaTransform(epsilon) * std::sqrt(theta); Now tell me what you know about `betaMatrix`. What can you do with it? How would you find out? You would have to decipher the initializer. If I instead gave the actual type, you suddenly don't have to care about the initializer: Matrix betaTransform = computeAlphaTransform(epsilon) * std::sqrt(theta); In fact, it turns out that it doesn't matter that `betaTransform` is a `Matrix`, as we only use it in ways that require it be a `Tensor`, so we use that instead: Tensor betaTransform = computeAlphaTransform(epsilon) * std::sqrt(theta); Now we have exactly the information we need about `betaTransform` to understand how it will be used and what we can do with it.
I feel that if we adopt the AAA style now, we'll be kicking ourselves later. We should start writing our code now to prepare for the inevitable features of C++. When concepts arrive (which they almost surely will), this convention will ensure your code is more intelligible, just as extensible, less bug prone, and more uniform. It seems like the C++ community are pretty divided on this.
No, you are the one misunderstanding: I never said that you should use them in normal code, but at some point, memory needs to be managed: It is impossible to implement things like `std::make_unique` and friends without. So: You DO need them. This is non-withstanding to the trivial fact that you shouldn't manage resources without RAII.
Concepts introduce some complexities. That's why we don't have them yet. When we have them, I'll certainly use them. In the meantime, I'm ok w/ auto., particularly when it's used in short functions where the intent is clearly understood. No harm, no foul. This is a great discussion, btw..
I never said they are the same. I actually said they are not the same. This talk of decltype(auto) is really a distraction because it has nothing to do with the choice of using explicit types vs using type deduction. The use-case for decltype(auto) is doing advanced TMP where using explicit types is not an option. The use-case for auto is when you are programming against a known set of prototypes or a known duck-typed interface and you are well aware when things return values or references and when you might require a conversion. For example: T x = foo.bar(); // vs auto x = foo.bar(); T &amp; y = foo.baz(); // vs auto &amp; y = foo.baz(); // We know x is a value, so we move if we are done with it, not forward. blarg(std::move(x)); It's in this context that nesting function calls is similar to using auto.
An update: The regular sessions will be professionally recorded. http://cppcon.org/cppcon-video-recording-2014/ Sorry for our prior vagueness; we've been working on putting together video recording for a while, but we didn't want to announce it before it was 100% confirmed.
&gt;I never said they are the same. If you didn't intend to mean that replacing: foo(Bar()); With: Type x = Bar(); f(x); Are the same... then what argument are you making? It's reasonable when speaking about replacing one thing with another, that the two things are the same in some sense. &gt;The use-case for auto is when you are programming against a known set of prototypes or a known duck-typed interface and you are well aware when things return values or references and when you might require a conversion. If the use case for auto is as you state, which seems fairly sensible then the idea that one should "almost always use auto" is wrong. What you said in the above quote is not the same thing as "almost always use auto".
Hi guys, I'll try to get back to you today with clarification about the extended Q&amp;A matter. -- Bryce
&gt; This would be unacceptable: &gt; var x = SomeFunction(); &gt; It's not clear what x is, you'd have to look at the declaration of SomeFunction. The argument I am making is that if you object to not knowing the type of x above without glancing at the declaration of SomeFunction then you could also object to not knowing the type of the first argument to f in f(g(x), 2); without looking at the declaration of g. &gt; If the use case for auto is as you state, which seems fairly sensible then the idea that one should "almost always use auto" is wrong. What you said in the above quote is not the same thing as "almost always use auto". I think it is the same thing as "almost always use auto" because what I described above is almost always the kind of programming being done.
&gt; But as soon as you introduce a variable, the question becomes "Well what can I do with this variable?" Let's test that assertion: auto begin_it = begin(elements); auto end_it = end(elements); auto print = printer(); for_each(begin_it, end_it, print); I for one don't find that I need type information any more than I did before. As to your examples. 1. I assume you mean `betaTransform` because I see no `betaMatrix` anywhere. I actually know quite a bit about the thing. I know that it's some sort of algebraic type (unless someone's done something stupid so that "up" means "down"--that always screws up everything though). I know that it's the result of applying those mathematical operators to `epsilon` and `theta`. 2. The really interesting thing here is not only that I'm not getting any more information than I had before, but the little slip you made in your question about the first example. I certainly know nothing extra about `Matrix betaTransform` than I do about `betaMatrix`. 3. Then it would be named `betaTensor`. I don't need to know that the static type is `Tensor`. I don't care. I only care that it is a type that represents a tensor, hopefully in a way that enforces those semantics via compile-time type checking and I do NOT know that's what `Tensor` does with the context available either. We also never witness code in such a vacuum of context. We can and should assume that the people working on the code are familiar with the domain. I for one wouldn't know a whole lot about the examples you give because I'm unfamiliar with that domain. Once brought up to speed though it should be readily apparent to me what that code does and I'd hope the type system is being leveraged to tell me when I don't. I'd hope that `Tensor` isn't just a `vector&lt;vector&lt;int&gt;&gt;` or some nonsense with equally useless semantics for example though of course in C++ I don't know that's not the case. Furthermore, your second and third example could have major performance impact. If I were using expression templates for example for the return of `operator *` on the types supplied then by assigning to `Matrix` or `Tensor` I'm forcing evaluation of that possibly lazy type. If I then do `return beta[3]` I've done a possibly large amount of pointless computation. Until concepts actually exist the best way to work with that is to use `auto`. As the language *currently is* the recommendation to use static types rather than `auto` seems to me to be mostly flawed for this reason--it would discourage this kind of optimization and generic style programming. When and if concepts come into the picture then code like your second and third example become better, more expressive, etc... but at this point do more harm than good. Your examples could be done with a complex, type erasing monstrosity. In some cases it might be good, but it's going to be a lot of extra work and won't come free at runtime. Edit: Let's also look at the conversion of the auto-typed version above and see if we can make it more legible by introducing type: std::vector&lt;int&gt;::iterator begin_it = begin(elements); std::vector&lt;int&gt;::iterator end_it = end(elements); std::function&lt;void(int)&gt; print = printer(); for_each(begin_it, end_it, print); Personally, I find the verbosity hinders legibility and it's not actually useful to me anyway.
When I read the first bit, I was building a "I disagree with this guy because..." in my mind. I was going to say that if you use auto for an iterator, just make sure the container restricts to types it understands. I was going to suggest an interface. But then you talked about concepts and how they put sanity back into the situation and I agree with you 100. Concepts are just a compile-time interface. I didn't think about using the concept type rather than auto but I agree whole-heartedly.
&gt; auto x = it&lt;is_default_constructible&gt;(some_func()); Cute! I like.
I think your fundamental argument (be careful with overusing `auto`) has merit, but watch out when you make sweeping statements about the value of the declaration. The variable type is certainly subordinate to the initializer, since in your example I can look up the definition of computeAlphaTransform (likely with a single keypress) and likely already know the type of the `*` applied to it. In contrast, what do you know about the following? Matrix&lt;double&gt; betaTransform; I know the operations that are done on it, but no clue as to what data is in it other than the name. Since both auto and manual typing include the name we can discount that value and come with less overall.
I think the argument in this case was that the IDE can auto-complete or expand the method definition if you need to know it.
But you're just passing some iterators into a standard algorithm that most people are already familiar with. Both are fairly readable because it's a trivial example. Though even now, with the types explicitly declared, I at least know it's printing a list of integers as opposed to strings or some objects with overloaded ostream operators that do god knows what. Here's an random example from the first file I opened in a C# project I have to work on once in a while: void Panel::Update() { foreach (var reference in GetOutgoingReferences(_object.Data)) { var outgoingObjRef = reference.Object.IncomingReferences.FirstOrDefault(); if (outgoingObjRef != null) { var field = outgoingObjRef.Fields["_foo"]; _children.Add(field) } } } If I was trying to get an idea of what the Panel class did, I'd be much happier knowing what types are involved at a glance.
&gt; Concepts are just a compile-time interface. Except there is no proposal for having concepts as an interface(like what was proposed in C++0x). The concepts-lite proposal is proposing concept predicates(ie `concept bool`) which does basically what we can do now in C++11(ie type trait introspection + `enable_if`) plus some better overloading. With predicates there are no compile-time guarantees that you are following the interface.
Oh man, bummer. Maybe I got the wrong impression from this article. I get the predicate as "So long as this type meets this concept's predicate, it is usable." And I get how a predicate can't then be used as a type for a variable. If that is the case then I'm still all for using auto almost everywhere.
This is awesome. Hopefully we can get some more detailed debug info eventually. Anyone aware of a reference for implementation of things like uuidof or declspec property?
I pinged the rest of the planners, for clarification RE: what will be recorded. First humor me and let me clarify the format of the conference. The conference is six days long. The first day doesn't really count; on day 1, we open registration in the afternoon, and there's no scheduled content. The last day is a half day of content, end at around noon. On the other four days, the scheduled conference material will run from 8AM to 10PM. There's a lunch break, and a dinner break (roughly 2 hours for each); and we have shorter breaks in throughout the day. The scheduled program consists of: * The Regular Program (aka sessions). This is the meat of the scheduled program. We solicited the community for proposed talks/lectures/tutorials, and through a process of peer review we selected ~100 of the proposals. Most of these talks are 1 hour in length. These talks will be given in the morning and afternoon; most of the time, there will be six of these talks going on concurrently (in some cases only 5). * Plenaries/Keynotes. These are talks given by well-known C++ figures. Keynotes/plenaries are not scheduled concurrently; the entire conference will attendee these sessions. We will have five keynotes/plenaries; they are all scheduled in mid-morning. We've announce four so far: Bjarne (Keynote), Mark Maimone (Keynote), Herb Sutter (Plenary), Scott Meyers (Plenary). Stay tuned - we'll be announcing the fifth soon. * Evening Content. AKA things that happen after dinner break. These events are usually a little less "scheduled" (i.e. not lectures) and more discussion-oriented. We'll have a number of panels, and (tentatively) lightning talks. The unscheduled program is the personal interactions that C++ conferences (such as C++Now or BoostCon) are known for; discussions, spontaneous hack-a-thons, group meals, etc. My fondest memories at C++Now are the nights that I've stayed up till 2AM, trading stories with friends (both new and old). I still remember the year that my boss and a co-conspirator we had just met camped out in the hotel lounge, hacking together new ideas they had come up with earlier in the day. To answer your question: * We'll record everything in the regular program; and all of the plenaries/keynotes. Everything from 8AM until dinner; but not the breaks, and not breakfast/lunch (obviously :P). * We will not record the evening content; historically these have not been recorded at other conferences like C++Now or BoostCon. After dinner, no cameras. * The usual on-mic audience Q&amp;A at the end of each talk will be recorded; we won't record the traditional mobbing of the speaker which occurs after that and usually carries over into breaks. Usually, the speaker will ask if people have questions; some people will ask them on-mic but others will wait to pounce on the speaker with lengthier (or private) questions after the cameras are off. -- Bryce 
&gt;The usual on-mic audience Q&amp;A at the end of each talk will be recorded That's great, and basically what I was worried might be missing. Thanks very much for the detailed clarification! Will there be specific Q&amp;A sessions like those at GoingNative? The ones I've seen were essentially hour-long, scheduled talks with most/some of the speakers on stage and the audience being able to ask them general or specific questions about C++ or their field (whatever that may be).
I don't know, I find this use of `FirstOrDefault()` to be far more incomprehensible than the use of `var` in your example. Why does the code want to skip past most of the data associated with an incoming reference? I seriously doubt that replacing `var` with specific names will improve understanding of this snippet. Moreover, your example has `reference.Object.IncomingReferences.FirstOrDefault();` If you are ok with this sort of chaining in general (though maybe not this specific case), do you have an argument against `var` (or `auto`) that doesn't also argue against chaining? You'd have the same amount of available information if you had written: var obj = reference.Object; var incomingReferences = obj.IncomingReferences; var outgoingObjRef = incomingReferences.FirstOrDefault(); In practice, I don't find my understanding of code to be hurt when types are omitted. Done right, I find it helps remove clutter and unneeded distraction. When done poorly, `var` is usually the least of the problems.
&gt; I don't see how that follows. It follows because you wrote: &gt; var x = SomeFunction(); &gt; It's not clear what x is, you'd have to look at the declaration of SomeFunction. If instead you wrote: Foo(SomeFunction()); It is still equally unclear what type `SomeFunction()` returns. Detrinoh's point is simple. To remake it in C# to avoid your red herring about auto in C++, your argument against this code: var x = SomeFunction(); Foo(x); ...also works against this code: Foo(SomeFunction()); Since you allow for the latter, it seems you are being inconsistent and now have an opportunity to clarify your position.
That's probably going to fall into the category of the evening panels (which will not be recorded). More details about the evening panels will be announced, probably in the next week or two.
Honestly man I really can't see how a simple policy such as "Use auto when the type is clear from the declaration" results in something such as "Don't chain function calls." I mean sure we can argue technicalities and semantics, but reasonable people can absolutely adhere to the principle that auto should only be used when the declaration makes it clear what type is being inferred, and also be allowed to chain function calls. The sky won't fall over any kind of logical inconsistency from using auto in that manner and also chaining function calls. Other than just arguing for the sake of arguing and finding technicalities which are strictly of academic interest, I don't see how the whole function chaining argument is really of any practical engineering value.
When you have your function *interfaces* properly "conceptified" (Concepts-lite, or suggestive naming for template parameters without it), you don't have to repeat all that verbosity inside your *implementations*. template&lt;class OnePassRange&gt; void median(OnePassRange&amp; rng) { auto first = begin(rng); auto last = end(rng); auto middle = next(first, distance(first, last)); std::nth_element(first, middle, last); } Why would you ever want to use `InputIterator first = begin(rng);` etc., when you already know you have a concept `OnePassRange` that either formally checks the requirements of having proper `begin()` / `end()`, or just duck types this? In C people use long and verbose type names and short and meaningless function and variable names LongAndVerboseType x = foo(y); In modern C++, descriptive variable and functions names, and `auto` to reduce verbosity, achieves the same auto best_candidate = max_element(employeeList, JobDemands); // assuming range-fied max_element The function context in which this expression lives should make it really clear what you can do with `first_match`. If it doesn't, your function is probably too long and should be refactored. Explicit typing is just a crutch to mask bad naming. 
Borland?
 struct A { virtual ~A() {} virtual int foo() { return 0; } }; int main() { A a; return a.foo(); } Am I wrong to assume that it shouldn't matter if the function is virtual in this case? Since **a** is not a pointer and not a reference, **a** can only be an instance of an **A** class, shouldn't the function just be called directly even with optimizations disabled? At least that's what I'm seeing when compiling: _TEXT SEGMENT $T1 = 32 a$ = 40 main PROC ; File d:\a.cpp ; Line 6 $LN3: sub rsp, 56 ; 00000038H ; Line 7 lea rcx, QWORD PTR a$[rsp] call ??0A@@QEAA@XZ ; A::A ; Line 8 lea rcx, QWORD PTR a$[rsp] call ?foo@A@@UEAAHXZ ; A::foo mov DWORD PTR $T1[rsp], eax lea rcx, QWORD PTR a$[rsp] call ??1A@@UEAA@XZ ; A::~A mov eax, DWORD PTR $T1[rsp] ; Line 9 add rsp, 56 ; 00000038H ret 0 main ENDP Command line: cl a.cpp /Od /GR- /Fa"a.asm" Compiler version: Microsoft (R) C/C++ Optimizing Compiler Version 17.00.61030 for x64
Hmm, could someone exemplify what this could be used for in practical real-life? I'm not familiar with boost::mpl but from a quick look the documentation uses dimensional analysis (type checking of representations of physical quantities) as a practical example.
I find that you're only further demonstrating my issues with `auto`. You say that the first example is still easily readable. For you, perhaps, and maybe also for any person who might consider themselves a beginner of C++, but that's because it's a simple example. Understanding what `first` is still means figuring out what the initializer does. The initializer could easily be more complex. The point is that you're still choosing an arbitrary point at which `auto` is "okay" - the point at which you feel the code should be easily understood. There's nothing arbitrary about what I'm proposing. Your final example demonstrates this well. I could easily be unsure about whether `best_candidate` is an iterator pointing at the maximum element or the value of the maximum element. You've hidden that away from me and so now I have to go and look up `max_element` to find out.
cygwin, mingw, borland, intel, http://en.wikipedia.org/wiki/List_of_compilers#C.2B.2B_compilers
It definitely needs more examples! Here are a few plausible ones: * Taking an argument pack (which is difficult to work with) and extracting the arguments. E.g. if you're writing a multi-dimensional indexed container - MyContainer&lt;KeyType1, KeyType2, KeyType3, ..., ValueType&gt; - you can extract the last value in the pack and typedef it, then chop it off and use the remaining arguments as your key types. * Cleaning up argument types - e.g. removing const refs from every argument in a pack and throwing the result in another type container. * Checking a template argument to your function or class is in a list of acceptable types - you can use this to give a nice static_assert error message rather than failing noisily later on in the compilation. * Similarly you can make assertions that a list of template parameters have some desired property - e.g. you could require that all but the last two parameters are of integral type. All that said, this is primarily a library aimed at people who write libraries - I don't think you'd see much use in frontline code.
I've worked with quite a few IDEs and for C++ development Microsoft's Visual Studio is the best so far. Two killer features for me that distinguish it from other IDEs are: - Code browsing, which is vastly superior to any other product I've tried, especially on a big code base (&gt; 1 MLOC) - Integrated debugger (less important to me, but still slick) Visual Studio apparently lags in the areas I am not really interested in, such as automatic refactoring and Intellisense. I also find it very bloated and memory consuming. I should add that I spend most of my work day in a "Unix IDE" - vim, screen, cgdb, ctags. Compared to Visual Studio, I like the flexibility of the environment and ease of use over a slow network (very important to me). What I miss is a good code browsing support. ctags and cscope are far behind MSVS code browser.
Intellisense with C++ is 'ehh' because unlike C# or VB.NET the language isn't integrated with the framework and VS doesn't know it like the back of its hand, and I've never even attempted automatic refactoring with C++, so I can see exactly where you're coming from. The massive SQL dump that VS creates for Intellisense is a huge pain as well but that's the way things are.
Kdevelop, Kdevelop, Kdevelop. When the Clang parser will be stable enough it will beat any other IDE hands down imo. Once you get accustomed with the semantic highlighter you would never use any other editor.
Awesome. I will take a look at it. Is it multi-platform? I love CB because I can run it on Linux and Windows as well. Thanks!
Visual Studio hands down. Debugging and code browsing are what win me over. Mostly the debugger. Its just so easy and seamless to put a breakpoint at some obtuse code, and just actually watch it work. It just speeds up my understanding of the hard bits and finding the root cause of bugs 10x. Also, the intellisense tends to not break as much as other IDE's I have used. I have tried Eclipse, and honestly, its a Java IDE shoe horned into C++. I think their whole view system is great in theory, but in practice just never worked as well as I had imagined. Code Blocks was ok. I liked it because it was a bit easier to change compilers than VS. But overall, it was just a lesser version of VS. QT Creator was too shiny for me- it was almost distracting (note last time I tried it was several years ago). It tried to be too slick with its metallic interface. I never used it for an extended period of time because I stare at this thing for hours on end, I need it to be pleasant. 
Hahaha. The KDevelop thing seems rather funny, since other people in this thread seem very keen on it. I haven't had many problems with CB, other than getting used to so many features (most of which remain unused), but I suppose maybe it is a matter of getting used to it. I don't really know.
Hi! I have personally been only using CB and VS only (as IDEs), but I have indeed used other Text Editors (such as Sublime Text (which Syntax I really love, but the lack of work in projects and fully dedication of C++ make it quite unusable for object-oriented stuff) or even Notepad++). VS's underlining when it finds some issues with your code, makes it REALLY useful, specially if you are a beginner like me, but it seems SO trivial to make a choice between all of them.
My favorite so far is Visual Studio, but what I am looking forward to is the C++ IDE in work by JetBrains. They did a great job with IntelliJ which is my favorite Java IDE now. 
Hi! I like VS for the debugging and compiling a lot (it works fine for me), but I do prefer CB's feel. I made my own theme on it, and the intellisense works perfectly, as good or even better (for me) than VS's. But I cannot deny that when it comes to debugging I still go back to VS with my code/project, but I will blame myself on this last point, because I haven't really given a proper try to CB's debugger. So I am still learning my ways. :D
Awesome. Does it have a name? When is it planned to come out? Free? Open Source? Price? :D
It does not have a name yet afaik but you can find it easily on Google from "jetbrains c++". A release date is not known either. There is a feature list (http://www.jetbrains.com/objc/features/cpp.html) and a video (https://www.youtube.com/watch?v=KS6lV_f8mHs) available, showing some things the IDE will provide. It will probably be similar to Visual Studio, closed source with a simple free version and an upgraded version for some hundred dollars.
I know I'll probably get downvoted to oblivion, but I like Xcode.
I use Emacs and Visual Studio for debugging. I hate debugging with Visual Studio but Emacs GUD sucks on Windows (I love it for Linux).
I really like QtCreator on Linux. It is very customizeable, has a good understanding of syntax (+optional llvm/clang integration) and its text-based QMAKE system make setting up an environment very transparent. I like Visual Studio if I HAVE to program windows-specific things. I hate CodeBlocks and Eclipse, they are filthy java IDEs full of bloat.
CodeBlocks is not a Java IDE. 
Came here to post exactly this. IDEA is just incredible and I can't wait to give their C++ IDE a good run. I am not being unrealistic and expecting the perfect IDE in a v1 product but I have a feeling it is going to be great. I would *love* for it to have full Clang support (once Clang on Windows is a little better, it is getting there though). Clang just makes C++ so much nicer. I will happily stick down a few hundred £ for JetBrains C++ IDE no matter what though, I have that much faith in the company.
It is, but I think you have to install all (most?) the KDE framework to make it work with windows/mac.
There is a huge difference between Kdevelop 3 (and the early 4.x.) and 4.5/4.6. The only real problem I have with it is that sometimes the parser crashes with very heavy metaprogramming code (mostly CGAL, 99% of boost works fine). Again, there is a very promising [project](http://milianw.de/blog/katekdevelop-sprint-2014-let-there-be-clang) to use clang as the parser for Kdevlop, which would basically highlight any error (syntactic AND semantic) without even compiling.
Qt Creator is excellent. XCode is excellent if you do things the XCode/OS X way, otherwise it's unusable. Visual Studio is excellent if you do things the Visual Studio/Windows way, otherwise it's unusable. The others I've tried have mostly been fairly horrible. It's going to be interesting to see what JetBrains come up with (from experience with their other tools I suspect it's going to be too busy, cluttered and Windows 95y for my taste, but we'll see).
I recently moved to cross platform dev and I found netbeans quite good at it. The integrated debugger work well on every platform, it has a great autocompletion with things like auto-include, a good make/cmake generator, and it is reactive enough.
Two things: First, I think Herb Sutter advocates "almost always auto" (AAA, name suggested by Andrei Alexandrescu, and which is the title of the article), *not* "always use auto" (which would be AUA). :) Second, a huge amount of the discussion in this thread is of the form "but if we had concepts we would write "Conceptname x = ...;". That's exactly true and I'm one of the ones pushing strongest for that -- and it's a reason to use "auto x = ...;" today because it will mean the same thing, namely to deduce the type (*not* specify it exactly, no conversions, etc.), just adding the ability to check that the deduced type matches the concept. This is a direct (and intended) evolution of declaring with auto per AAA. If you like that, you're in good company, and you should use auto today. Telling people to declare with concrete types today is not at all the same thing and puts you on a different path of committing to concrete types, which is the opposite of "Conceptname x = ...;".
you're right, I was confusing it with something else.
For me Xcode is a really good IDE for ObjC, but C++ support lacks everywhere. For me it is hard to believe they code OSX or iOS Kernel stuff with C++ in this IDE
1) [Vim](http://www.vim.org/) * lovely clang powered code completion with [YouCompleteMe](https://github.com/Valloric/YouCompleteMe) [(see gif)](https://camo.githubusercontent.com/1f3f922431d5363224b20e99467ff28b04e810e2/687474703a2f2f692e696d6775722e636f6d2f304f50346f6f642e676966) * syntax checking via [syntastic](https://github.com/scrooloose/syntastic) * and some more plugins [(my vimrc)](https://bitbucket.org/Urfoex/fresh-project-initializer/src/679f2b8557e19bf475fb1fca97f01d320e8b81bc/cpp.vimrc) * customizable * colorful * low on resources * works with other languages and filetypes * extensible via scripts and plugins [list of vim scripts](http://vimawesome.com/), [Vundle plugin manager](https://github.com/gmarik/Vundle.vim), [list of vim scripts](http://vim-scripts.org/index.html), [list of vim scripts](http://www.vim.org/scripts/) * works over ssh * no need for mouse or touch or other devices and interactions than keyboard * intuitive work (after learning Vim via vimtutor and other helpful resources ;-) ) * cross platform and free and open source * and more... 2) [KDevelop](https://www.kdevelop.org/) * colorful * showing doxygen comments on hover * showing tooltips with clickable links * list of code completion with signature and types * little helpers, e.g. for class creation, changing method definition/declaration, creating methods * cmake support * gdb support * upcoming clang powered code completion * (cross platform and) free and open source 3+) [Qt Creator](http://qt-project.org/wiki/category:tools::qtcreator), [NetBeans](https://netbeans.org/), [Code::Blocks](http://www.codeblocks.org/) * good stuff * cross platform and free and open source 
Agreed on vim for CPP and gdb for debugging. The only language I regularly use an IDE for is Java. If I wrote lots of Objective-C or was cursed to only write for .Net I think I'd probably favor the respective ides until I was comfortable with the language. Not sure why you were down voted, maybe because vim isn't an IDE?
Add Visual Assist X to VS and you get awesome **refactoring**(amongst many things) as well. Combine that with the debugger, and VS is best. Sadly, its not available on linux/unix systems(which are not Mac).
JetBrains is working on updating their ReSharper tool to work with C++ in VS. They're still in the "early access program" stage (but you can download and install it to try it out). If it works anywhere near as well as it does with C#, the Intellisense and refactoring it provides will be *awesome*. We still have no idea on release date though (probably "when it's ready). I'm really looking forward to it :D
1. Intellij IDEA. This one is crazy. I code most of stuff in python and its python IDE is pretty much best around. Code completion is great, has nice dark theme (programmers love their eyes). Also it has great php/html/js support. Debuggers work great, hell it can even debug javascript which executes in browser, how cool is that? SQL support while working on web applications is also very handy. Even has some angularjs support. It also supports ton of other things i dont even use. You name it - chances are its supported. 2. Visual Studio 13 + VisualAssistX. Black theme again! Although even without it Visual Studio was one of the best c++/c# IDEs i used. Debugger is awesome. I love the way compiler settings are managed. Beats makefiles IMO. VAX provides terrific code completion. Yes, i am one of those bad spoiled programmers who need things like point&amp;click debuggers and code completion heh.. 3. QtCreator. I dont use it much but it integrates well with Qt. Can have back theme (!!) and rather nice code completion. However its far from what VS+VAX provides. There are some other minor things i dont like but overall i think its greatest cross-platform IDE and it has great potential. Afterall it has nice debugger! Ill probably try KDevelop since you people say so many nice words about it.
I primarily use Visual Studio 2013 on Windows and XCode on Mac. I haven't had many issues other than the lack of C++11/14 features in VS2013's compiler. Both have their strengths and weaknesses but I do enjoy working with them. I'm looking forward to JetBrains upgrading ReSharper to work with C++ in Visual Studio, because it's fantastic for C# and I'd love to use it. They have an early access build available but it's not perfect (but what works does so really well). Since those are the only C++ IDEs I've used, my third IDE is JetBrains' IntelliJ IDEA for Java/Python/PHP. It's \*fantastic\*. I love working the environment.
Vim, YouCompleteMe, UltiSnips, Ag, vim-git, and other miscellaneous scripts, vim plugins and [customizations](https://github.com/uxcn/dot-files/blob/master/.vim/vimrc). Being able to script the editor saves a lot of time on repetitive things. Other useful things would include tmux, ninja, gdb, gtest, ccache, and distcc. I'm not a fan of the default YouCompleteMe configuration, so I try to use a custom one that gets compile info from a compilation database (CMake) instead of the config. I also usually switch the default config to fallback to a `main()` file when there's no corresponding c/cpp. I'm not really a fan of VS. I'm sure it's productive once you're comfortable with it, but I've personally found the documentation to be overly verbose and generally difficult to navigate. There were too many magic menu/configuration incantations needed to do simple tasks. Eclipse is slightly better in that it's somewhat intuitive. Being modular is also a plus, but the modules aren't very cohesive. The memory footprint can get out of hand as well, unless you group projects into separate workspaces.
Currently I just use Qt Creator: fast, with great features (refactor, create method in h/cpp file, change method signature in h/cpp, etc etc), totally free, works in all 3 main SOs, uses CMake natively and I can use it for C++ projects without slightest Qt trace if I need it. The other "IDE" I use is notepad++
I dont really use IDEs much for C++ develpment. I find it much better to learn the tools. I usually use Sublime Text with clang complete and gdb. The code completion, navigation and editing is vastly superior to visual studio. 
I did a multi-platform C++/linux system in 2009. We maintained the CMAKE and vcproj files separately. There was a bit of double-maintenance there, but it worked for us- adding new projects and files was rare enough that it wasn't overly burdensome. I hear you on installing libraries. That was a harder-than-it-needed-to-be undertaking. We didn't use unit tests at the time, we had a bunch more automated integration tests that ran outside any dev environment. We scripted our builds/deploys via our automated build systems. We found lots of bugs by running in two environments, and mostly fairly scary memory overrun type bugs (mostly stemming from our mysql access lib, which used straight C). 
I use Vim for everything, but unfortunately, not an IDE. You can get very close though, and it looks like you have.
&gt; Other than just arguing for the sake of arguing and finding technicalities which are strictly of academic interest, I don't see how the whole function chaining argument is really of any practical engineering value. Well, that's what I find interesting about this very common argument against variable type deduction in C++ (and C#). The arguments against it also work against other programs that the arguer would allow, so it really does look like they are just arguing for sake of arguing and ultimately just trying to sound more intellectual when they are really just saying, "I don't like it."
Where is the kdevelop clang integration now? Not stable I'm assuming?
&gt; But it sounds like your argument is just "if everything else is perfect, then the loss of readability from omitting the type names is acceptable". To clarify, I find when done right, it improves readability by removing clutter and unneeded distraction. Done wrong, it is usually because the code is confusing whether or not types were spelled out, and the code should be fixed. &gt; I think that's the easiest part to understand. It is easy in the sense of the mechanics of what it is doing. Maybe with more context, the why would also be clearer, but having debugged more than my fair share of programs that introduced bugs by skipping important data via `First()` or `FirstOrDefault()`, it makes me suspicious whenever I see that call. That snippet is grabbing data out of the outgoingObjRef and adding it to children. The question I have is why add only the first reference's "_foo" field as a child, why not all "_foo" fields? And bringing it back to var, what type name could you add that would answer this question? 
I've done professional work in several IDEs for C and C++, as well as sans-IDE (primarily VIM, but uemacs way back when), and done plenty of hobby work with a couple others. I'm not in love with any of them -- each has upsides and downsides. If I had to pick a top three, though, the list would probably be: #1. NetBeans 7.4+ (preferably 8.0) *Pro* + Decent completion (though Eclipse often does better) + Very nice code navigation + Decent debugger + Also covers C and Java, the other two languages I most often use + Usually plenty fast once warmed up + Multi-platform *Con* + Occasional slowdows + "Find Uses" could be quicker + Poor Android NDK support #2. Eclipse 4.x (preferably 4.4) *Pro* Same list as for NetBeans, plus: + NDK support + Good XML editor *Con* Same list as for NetBeans, plus: - Takes more tweaking than NetBeans to get responsive behavior - Impressive amount of unnecessary inconsistency and complexity in the UI - Some of the worst UI design that still manages to be usable (yes, this is really worth more than one bullet) #3. vim with a bunch of plugins (ctags, YouCompleteMe, GNU global, etc.) *Pro* + fast + everywhere (though sometimes with missing features due to unavailability of plugins) + easy to use for remote work + efficient editing (to be fair, I've been a Vi user since the 80s) + easy to extend and support new tech, at least on Linux + decent completion (when YCM is available -- only acceptable, if not) + "one text editing tool to rule them all* (i.e., works reasonably well for virtually any text editing tasks, and very well for many) *Con* - code navigation isn't great for C++ (though it's fine for plain C) - Major PITA to get tweaked to the point where it's really stellar for coding - lacking in integrated tooling (e.g., debugging, analysis, etc.) Impact of this is somewhat less, since I'm comfortable in a *nix environment. Honestly, it's really more or less a tie between NB and Eclipse. If the Android NDK support were up to snuff in NB, it would be the clear winner. Of the well known IDEs, my reasons for not including some are: a. Visual Studio - can't run under Linux - IntelliSense gets messed up *way* too often - cumbersome install and update process - *way* too damn expensive I do have to admit, though, that if I developed solely on Windows, I'd seriously consider spending the cash - the debugging and analysis tools are stellar. b. code::blocks - primitive debugger interface - poor code completion - poor code navigation - cumbersome to keep up to date I've not used either of QT Creator or KDevelop for any complete projects. What little noodling I've done with them hasn't been promising, though.
Which syntax checkers do you use with syntastic?
I was going to say the same thing. I use Sublime Text+SublimeGDB+SublimeClang and that's it. The editing capabilities of Sublime Text is much much better than Visual Studio's. I simply cant live without the multiple cursor feature.
I never claimed that this is anything other than a subjective matter, "I don't like it.". I absolutely agree that this matter is subjective, based on personal taste, customs, and sensibilities. That's why I find this whole nitpicking over trying to find logical inconsistencies and contradictions absurd. I'm not the one who started this weird tangent over how a coding guideline that advocates using auto when the type can clearly be inferred, somehow results in a logical contradiction about how functions can't be chained together. All I said is that reasonable engineers can have different coding policies without it resulting in some kind of academic argument over logical contradictions. Basically, it's possible to believe that using auto in certain cases is helpful, and that it is also okay to chain function calls. The two don't contradict one another for any practical purpose other than having purely sophist arguments over the Internet.
Interesting. Jetbrains ides always felt pretty minimalistic to me. But I use Netbeans for my day job...
&gt; Eclipse I've found to be unbearably slow even on very good hardware. So it has all the negatives of VS, not a lot of the positives, and it's frustratingly slow to boot. I hear that so often, and about NetBeans, as well. Still, I haven't had general performance related complaints about either of them for ages. It is true that Eclipse is configured by default to feel very slow for things like code completion, with 200ms or 500ms minimum delay before showing results. That can be configured, though. I do get occasional delays in NetBeans, but they've become very infrequent (once a week, say ...) in recent versions (7.4 and 8.0).
&gt; I fear this thread is going to turn into a "my IDE is Vim/Emacs" joke tho. Why joke? Honest question. I’ve happily used VS for a decade before switching to the terminal and Vim full-time (obviously combined with a switch of the operating system), and while I miss *some* things, on the whole I find Vim with proper plugins to be the superior IDE.
1. sublime text 2. source insight 3. vim
It is usable for not too big codebases , i tried it with the LLVM-Clang code and it was a bit buggy and slow.
I can't think of a case where it improves readability. If the type isn't important than it's easy enough for me to ignore. And i'd rather ignore a hundred lines of overly verbose line-wrapping templated iterator declarations (which could just as easily be improved with a typedef) than be left wondering even once. It's probably easier to write, but we can probably agree that shouldn't be a factor if readability is the tradeoff. For your question, I don't know why it only saves one. Maybe the references are sorted somehow. I don't think that's a question that can be solely answered or not answered by having type names or not. It's not the best code. But that's kinda my point. It's not the best code and I'm not overly familiar with the players, that's when you want as much information available to you as possible. It's for the same reasons we add comments and try to pick good names for things. You can take away any of those and say that if the code is well written enough you should be able to manage just fine, but that'd be crazy. I can think of type declarations the same way I think of asserts; comments that can't ever get out of date or lie to you. And they're not really for the person who wrote it, or someone who's intimately familiar with the code. If you're absolutely certain the type can be inferred easily from context, then sure I'd say go ahead and use auto. Here's an example of auto being used in a way I agree with: void foo(const std::map&lt;int, std::string&gt;&amp; table) { for (auto entry : table) { //.... } } The type of entry is obvious because we know the type of table and we can be expected know how a ranged base for loop works. But even then, imagine someone comes along and adds a hundred lines of code to the top of method before you get to the loop. Yes, it's terrible that someone did that, but it happens all the time, and now I'm left wondering again.
I've not used this with C++, but have you tried `ctags` for c++ function jumping and such?
Do you ever add anything on the non-Windows side? Or always use the cmake VS generator?
&gt; So while it isn't completely open, I'm still not exactly sure what you mean by a "Microsoft Way" A start is that if I would make a default cpp console application, it does not even compile for any other compiler. I mean, what's the idea of _tmain and what is this _TCHAR thing? Then if you add a little code, it starts to complain that it already knows about min and max, I find it hard to use Visual studio for a project that is pure c++. 
Does Borland and Intel generate VS C++ ABI compatible code? That is what Clang/LLVM is working on now. It has targeted the mingw ABI for some time now.
It must have gotten better. Last time I tried it, it was far from cromulent.
I really need to add kdevelop to my linux box and give it a whirl. I've been very happy with qtcreator's new rev but some of those features are super nice.
I actually use [ClangComplete](https://github.com/pfultz2/ClangComplete) rather than SublimeClang, becasue on linux its easier to setup and it also will do completion for include files.
Thank you for posting your .vimrc. I didn't know about vim-airline. I like to use [tagbar](https://github.com/majutsushi/tagbar) to show me the entire file layout when I load a file. Here is my way simpler [".vimrc"](https://gist.github.com/AranHase/34cd40766d01b2504d12).
I've been using codelite lately it has a lot of cool plugins and basically creates makefiles for building but you can specify your own. One thing that annoys me is that you can't do split screen text editing but supposedly that feature was pulled out and will be put back in the next version. I used to use CB but it doesn't generate makefiles you have to download a separate 3rd party program to do it. Also, whatever version apt is installing on my Ubuntu machine (13.something?) Crashes on first startup.
For the lazy: https://github.com/quarnster/SublimeGDB https://github.com/pfultz2/ClangComplete
Honestly? Vim inside tmux. It does everything it needs you really well plus with vpn + ssh I can work from anywhere... 
Yes Intel C++ compiler do this well. It is also integrated into Visual Studio. Borland/Embarcadero for x64 uses now Clang. 
QtCreator is my favourite as well. I use the beta with clang integration and although the auto-completion on a large code base can sometimes be really slow, it is totally reliable and the on-the-fly highlighting of problematic code (for example printf format not matching arg-list) is just amazing. I've used VS at university and it is a great IDE for Windows. For C++ you probably want to add Visual Assist X. It is just so much better than IntelliSense.
1. Notepad++ 2. Notepad++ 3. Notepad++
Eclipse use to be really really bad performance wise but in my opinion it does much better these days. However that is not why I dislike Eclipse, the big problem with Eclipse is serious bugs that hang around forever. I can’t trust Eclipse so tha tmakes using it difficult. It is really sad that Eclipse is neglected so as it is an excellent IDE when it doesn’’t screw up you installation all on its own. For python developement I have found nothing better than PyDev for the way I work. In case you are wondering, things that Eclipse does that should have been fixed long ago: 1. Deletes your entire workspace for you. This is a massive fuck up on the part of the Eclipse development team. 2. Trashes installation information forcing you to find and manually enter in all of the update repository web addresses. 
1. vim 2. ed 3. cat &gt; dev/mem
1. Qt Creator 2. Vim Qt Creator as a fully-functional IDE, Vim because it's just a good editor. Everything else is too awful. Visual Studio is out of question because it's proprietary and windows-only. KDevelop doesn't have a lot of progress going on - just go and look at the amount of commits in their repo - it's almost dead. 
Just as a side note, you've made the biggest mistake that people make when using auto, and it's actually one reason why I think auto doesn't work as well in C++ as it does in say... C#. for(auto entry : table) Should be: for(const auto&amp; entry: table) The two have different semantics and will behave differently, and yes I've been bitten by it and plenty of other people have been bitten by it. One of the downsides of auto is that it hides so much information that is valuable and useful in conveying the intent behind a snippet of code. Once you eliminate that information you basically have to be sure that your code works properly, because if it doesn't work properly and you need to make modifications, or try to get a feel for what's going on... well your code won't be of much help because all the types have been erased and so you're going to be left guessing.
Do you use IDEs for any language?
Nuget now support s c++ so its much more like appget. Not all library s are avsilable. Vcxproj is just an msbuild file which you can generate off of a cmake 
I see. Very interesting. Intel has the same problem as Clang that the ABI changes every version. The MSVC devs must feel like gods with the ability to break ABI every version. =) Most other platforms/compilers have stronger compatibility guarantees ([modulo bugs :(](http://article.gmane.org/gmane.comp.compilers.clang.devel/37216)).
That's true. Unfortunately I haven't taken much time recently to program outside of work, so my exposure to VS2013/C++11 has been limited. At work we're still on VS2010 so we're even more limited in the C++11 features we're allowed to use (by the compiler and management).
GCC will warn when auto_ptr is used in c++11 mode: $ g++-4.8 -std=c++11 -o foo foo.cpp foo.cpp: In function ‘int main()’: foo.cpp:7:26: warning: ‘auto_ptr’ is deprecated (declared at /usr/include/c++/4.8/backward/auto_ptr.h:87) [-Wdeprecated-declarations] std::auto_ptr&lt;int&gt; bar(new int(10)); There is also the good old `-Weffc++`, this won't catch C++11 specifically, but it will catch plenty of other "bad style" issues in C++. 
&gt; I'd do one of the following: &gt; &gt; void function(int foo, double /* bar */) &gt; void function(int foo, double) I tried that for some years, but it just makes the code look like garbage and I can't remember ever catching a single bug with that warning. Now I just use `-Wno-unused-parameter`, saves a lot of time and keeps the code clean of ugly workarounds. It's the only warning I disable when doing `-Wall -Wextra -Weffc++`, aside for some local `diagnostic ignored` when including old code. 
YouCompleteMe has its own clang-based syntax checker, I believe.
&gt; There were too many magic menu/configuration incantations needed to do simple tasks. Oh God I'm using VS at work and this is driving me nuts.
Honestly I am looking forward to using libc++ instead. 
Well, it's partly reserved word.. when trying to compile certain library in VS2013 (it was compiled with VS2010 before), I had to change this bit: #define override virtual Because the compiler said you cannot define a reserved keyword.
You can use whatever compiler you want - just intellisense will not be aware of it. There's an option to specify exact command line for the compiler, including the executable path.
http://qt-project.org/doc/qt-5/qcommandlineparser.html It was added on 5.2, but it is better in many aspects (api, features, support).
http://en.cppreference.com/w/ - Great reference site that is updated very quickly with additions to the standard (updated as soon as individual paper is accepted? has had some C++14 features for while) http://channel9.msdn.com/Tags/c++?sort=rating - MSDN's Channel 9 is pretty great. Quite a bit of MS related content, but also a lot of general C++ talks and lectures, from C++ conventions and channel9 itself. You also probably want to check the following sites/blogs from time to time as they are written by prominent people in the C++ community ([Plus more suggestions from stackoverflow question on blogs to follow](http://stackoverflow.com/questions/151974/c-blogs-that-you-regularly-follow)): * Bjarne Stroustrup: http://www.stroustrup.com/ * Scott Meyers: http://scottmeyers.blogspot.co.uk/ * Herb Sutter: http://herbsutter.com/ [http://stackoverflow.com](http://stackoverflow.com/questions/tagged/c%2b%2b) - Is quite a good place to learn (and help others!). If you can answer the majority of questions without much cross checking then you know you've somewhat mastered the language (at least for day to day use :D)
&gt; than the lack of C++11/14 features in VS2013's compiler Which features do you need that it doesn't support?
I always modify files/directories using the IDE, and then use the command line to update. For changes in existing files, I build from within the IDE.
 1. Qt Creator - the first IDE I found that didn't use the "kitchen sink" style of UI design. Obviously it's still an *IDE* and that means a lot of options and subwindows and things, but it's done a good job of keeping things clean. 2. Text Editor And `build-essential` 3. That is all.
Visual Studio all the way. The main reasons being: 1. Debugging is just great 2. Debugging images and matrices with Image Watch is even better than Matlab 3. VA X autocompletion Under linux I'm using QtCreator, it's pretty decent, but can't compete with VS. I use VS for a completely cross-platform project (with CMake as build system), so it is quite the opposite from "made for windows C++ only"!
&gt; I use Vim for everything, but unfortunately, not an IDE. &gt; You can get very close though, and it looks like you have. Looking up what an IDE might be on [wikipedia](http://en.wikipedia.org/wiki/Integrated_development_environment) shows features that are also available (with plugins) in Vim. Things like: * code editor * build tools * debugger * code completion * version control * class browser * plugins * scripts/scriptable Sometimes I see IDEs that are just lousy MS-Notepad-like code editors with a build-&amp;-run button inside. [(e.g. processing)](http://en.wikipedia.org/wiki/File:Processing_2.2_Mac_OS_X_Screenshot.png) Standard Vim (and Emacs and Co.) can do way more in basic/clean installation. What would you say is Vim missing to be an IDE? Lack of included features on installation? Not being specialized on some languages? Missing pushable buttons/sliders/checkboxes?
Qt Creator all the way. Followed by Visual C++.
It looks like Scott Meyer's is in the auto camp as well judging by the [contents of Effective C++11](http://scottmeyers.blogspot.co.uk/2013/01/effective-c11-content-and-status.html).
I guess the standard ones (which would be 'gcc') as I haven't changed them. And, as chebertapps said, there is also one integrated in YCM (See ["Why did YCM stop using Syntastic for diagnostics display?"](https://github.com/Valloric/YouCompleteMe))
I don't see what this has to do with auto. Either you want by value or by reference and its clear what each syntax does, but neither is always right.
cool, i guess i havtn payed some $$$ to MS in waht, a year?
&gt; I'm not a fan of the default YouCompleteMe configuration, so I try to use a custom one that gets compile info from a compilation database (CMake) instead of the config. I also usually switch the default config to fallback to a main() file when there's no corresponding c/cpp. +1 Using compile_commands.json from CMake for YCM is really nice as you just need to put your additions into your CMakeLists.txt and CMake will update the compile_commands.json which then will be used by YCM. Nice. Clean. Easy.
Emacs, XCode, and Visual Studio
For more information on how `memory_order_consume` can be used to optimize the double-checking locking pattern for thread-safe singletons, see [this article][1024cores]. The basic idea is that we replace the acquire fence with a consume fence, which is always desirable to do when possible. If you would like to learn _why_ data-dependent reordering can happen on Alpha in the first place, see [this article][memory_barriers] on memory barriers. I would actually recommend reading the entire article if you would like to use atomics and don't have a background in processor design. It briefly goes through the MESI protocol and describes scenarios involving CPU communication where certain hardware optimizations are put in place (write buffers and invalidate queues). These scenarios show how unintentional memory reordering can happen at the hardware level, when correct synchronization is not used. Unfortunately, if neither GCC, Clang nor ICC efficiently supports `memory_order_consume`, then there will be no real benefit in using it (along with its friends `std::kill_dependency` and `[[carries_dependency]]`) for the time being. Does anyone know if compiler support improved for GCC 4.9 or Clang 3.4, or whether there are any tentative plans to provide explicit support for `std::memory_order_consume`? [1024cores]: http://www.1024cores.net/home/lock-free-algorithms/lazy-concurrent-initialization [memory_barriers]: http://irl.cs.ucla.edu/~yingdi/web/paperreading/whymb.2010.06.07c.pdf
&gt;I don't see what this has to do with auto. This problem is so common that Stephan Lavavej, a member of the C++ committee who works for Microsoft provided the following proposal to help avoid the problem I mention, because it does happen quite often and not just among novices. I've seen big name C++ guys give talks on C++11 and even their slides have the problem of accidentally using auto when the context calls for either using auto&amp;, or even using auto&amp;&amp;. Now tell me how many C++ developers you know are able to differentiate when it's better to use auto&amp; or it's better to use auto&amp;&amp;. Chances are not many. The proposal, which details why "for(auto entry : table)" is error prone and should be avoided, can be found here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3853.htm Basically the proposal is that one will be able to write: for(entry : table) As for the example posted above, if he wanted to mutate all contents of the map by value then he should have passed the table by value to allow for copy/move elision or even just allowing std::map to perform the copy operation, which it can do a heck of a lot more efficiently than the expensive task of copying every single element of the map one by one in order. He would then iterate over the map by reference as follows: void foo(std::map&lt;int, std::string&gt; table) { for (auto&amp; entry : table) { //.... } }
It's not so much that I need them, I'm just disappointed it's taken so long to get to the incomplete state they're in now. I can't remember off the top of my head, but I was working on a little project with a friend and I wanted to try and get it running in VS2013 (he was using mingw which I do have installed but I like the IDE) and there were a couple things that were causing issues. **edit** It might have been constexpr. But I'm not positive.
Thank you, that clears up a lot! I was wondering why MSFT would do the equivalent of tweaking the Itanium ABI every year. The library ABI compatibility changing makes much more sense. =)
Me too, on Linux as well. I wish it's debug features weren't in an unmaintained state though.
Modifying compile options for release/debug builds, modifying properties for solutions, switching from static to dynamically linked libraries would be a few examples. It seemed like as the solutions grew, the more difficult it became to do things. Anything that wasn't the VS default seemed to need a web search to do.
I love it! One problem though - my version of man can't cope with man "std::get(std::tuple)" due to a shell quoting issue further down the line. Does anyone know a good workaround?
http://msdn.microsoft.com/en-us/library/hh567368.aspx#corelanguagetable Besides this (the non-compliance of the wizard) there are plenty of shortcomings. is it so hard to make: std::vector&lt;int&gt; a {1,1}; compile (as an example)? They are at least forcing me to make a lot of exception/defines to make it work.
That looks like it uses inheritance to me. You're constructing a temporary MyConcept instance, with embedded Impl&lt;T&gt; for no especially good reason I can see. If I was writing this, I'd discard lines 4..36 and implement display_thingy as: template&lt;typename T&gt; void display_thingy(T const&amp; t) { t.display(); } If your toolset supports concept checking, you could use that to get prettier error messages. e.g. http://ideone.com/GEH5F8
The inheritance is more a trick internal to the concept class. Key is that you're not inheriting from an interface to implement polymorphism. Making display_thingy a template misses the whole point. Not everything in your system can be a template and runtime vs. compile time polymorphism are quite different creatures. Concept checking would be useful in the constructor.
It's pretty similar to Sean Parent's technique: https://www.youtube.com/watch?v=_BpMYeUFXv8 .
If you want different feedback, then clarify what problem you're solving. Otherwise, we're left to guess.
Actually, the rule you cited is not a rule for C++11, but I think we would be better off if it was (sections 7.1 &amp; 7.3 in that paper). Put simply: If the value returned by a load-consume changes, it should change values all the way down the dependency chain, too. C++11 currently doesn't require that, but the only time we actually break that rule is when coming up with artificial examples like (x - x) that are problematic for compiler writers.
Yeah, that's more or less what I based the idea on. Need to watch again though to see how much I plagiarized and whether he goes over anything I missed here.
That's a technique, not a problem. Have a nice day.
His technique will work well if the "sub-typed" objects are iterated over when stored in collections. The problem solved is increasing encapsulation by making inheritance an implementation feature. Greater encapsulation =&gt; greater maintainability. Edit: Clarity. 
http://ideone.com/ozGgyp 
While `(x-x)` is an obvious example that isn't likely to appear in real code, I don't imagine it would be too difficult to contrive one that would.
Thanks!
Only way I can think of in the current language would be macros. Something like so: BEGIN_CONCEPT_CLASS(MyConcept) CONCEPT_FUNCTIONS( (void, display) (int, fun) ) END_CONCEPT_CLASS() The implementation would not be pretty. I think I'd rather type.
Do you mean for a heterogenous-type collection without a common base class? If I needed to store Thing1 and Thing2 instances in the same collection, I'd want to use a common base class for them. The alternative is a wrapper like `boost::any` (or similar) along with a visiting function of some sort. The obvious solutions to this type of problem in C++ are: 1. Use a function template with static dispatch, as in http://ideone.com/GEH5F8 2. Use a base class and virtual function, as in http://ideone.com/6xqVK8 Any/every competent C++ programmer can tell you the tradeoffs involved in these design decisions. There are situations that rule out using one or the other. When I look at the code provided, I see 32 lines of complexity and attempted cleverness that's likely to lead to ugly compilation errors (when code changes) and unexpected performance implications (`new` call + virtual dispatch). I could *imagine* circumstances that justify that, but that's silly to do (because it's a solution in search of a problem).
If so, we'd love to see it. That's exactly the kind of feedback that N4036 is looking for!
&gt; Thanks for CEGUI [Wasn't me](http://crazycpp.wordpress.com/wtf/) Any time you introduce a new way of doing something (and as others showed I'm not the first) there's going to be a learning curve. You balance that with the difficulties associated with the original way of doing things vs. the gains in the new. In my experience forcing callers to deal with pointer semantics just to have polymorphism carries a lot of problems. People may be more used to it but I'm not convinced anyone really gets it right. Value semantics are a lot easier to handle, leaving pointer semantics to when you actually need pointer semantics.
I agree, although it does remind me of Boost :D.
[Boost.TypeErasure](http://www.boost.org/doc/libs/1_55_0/doc/html/boost_typeerasure.html) exists for this. I've never used it for anything real, but it handles this simple example well: #include &lt;boost/type_erasure/any.hpp&gt; #include &lt;boost/type_erasure/member.hpp&gt; #include &lt;iostream&gt; using namespace boost::type_erasure; BOOST_TYPE_ERASURE_MEMBER((has_display), display, 0) using Displayable = any&lt;has_display&lt;void(), const _self&gt;, _self const&amp;&gt;; void display_thingy(Displayable const&amp; thingy) { thingy.display(); } struct Thingy1 { void display() const { std::cout &lt;&lt; "Thingy1\n"; } }; struct Thingy2 { void display() const { std::cout &lt;&lt; "Thingy2\n"; } }; int main() { display_thingy(Thingy1()); display_thingy(Thingy2()); } 
LOL. Some people just can't see past the end of their own nose and go pretty off the deep end when anyone around them even hints at doing so in their presence. Calm down there, buddy :P My doing this won't make the sky fall on your head.
&gt; Calm down there, buddy :P I'm perfectly calm, thanks for your concern. If neither a base class nor a function template worked for an actual situation, I'd apply `boost::function` and `boost::bind` (for C++98/03) or `std::function` and a lambda (for C++11).
&gt; What would you say is Vim missing to be an IDE? I'd say it's more historical. Vim was never intended to be a device that *just* edits code, but a device that is purely a *text editor*. Occasionally, I use Vim to do more than write code, such as taking quick notes, or making small changes in configuration files. An IDE would be a bad choice for these tasks, so what does that make Vim? I guess the point I'm saying, is that you can make a multi-purpose text editor into an IDE, but it's not easy to make an IDE into a multi-purpose text editor. Though I'm sure there are some that are decent (i.e. Atom, Emacs, Sublime Text 2) and many that are terrible (Eclipse, Visual Studio). I just prefer Vim in those cases.
You could also do this: http://coliru.stacked-crooked.com/a/f8382e9bf856110b However, this is *without* the type erasure so not sure if helpful..
I understand your sentiment, but I don't agree that the examples you've provided solve the situation I stated equally well. 1.) boost::any might be too broad, introducing the potential for misuse of the collection. 2.) function templates are wonderful, but if you need to perform more than one operation against an item in a collection, you can't without some gymnastics. 3.) base classes and virtual functions are fine too, but OP is positing that hiding inheritance as an implementation detail is a good thing, which I am open to considering. 
[Link for the lazy](https://www.hackerrank.com/)
&gt; open to considering. Which is why I asked for his motivation and problem that he's solving. Since he responded with nothing but insults and abuse, I assume he has none apart from showing off something clever-ish.
In addition to the ones already named, the only official (as much as anything C++ may be "official") information portal is the C++ Standard Foundation's http://isocpp.org And for the more hardcore crowd, WG21's papers (at least the public ones) are available at http://www.open-std.org/jtc1/sc22/wg21/docs/papers/
Wantanabe should have his name legally changed to "Merlin".
FYI, you made the colon at the end of the link a part of the URL.
&gt; Not everything in your system can be a template What do you mean?
I don't see how: for (auto entry : table) // vs for (auto &amp; entry : table) is any more error prone than: for (std::pair&lt;int, std::string&gt; entry : table) // vs for (std::pair&lt;int, std::string&gt; &amp; entry : table) I actually find the former less error prone because there is less noise so its clearer when a copy is being made. You are also sure no conversions are taking place. 
I just pasted it as text -- everything else is browser magic. ;P
I'm not a compiler backend dev so I'm probably not the best person to ask... Something related to this certainly comes to mind: http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html
Nice work. Does anyone use [boost future](http://www.boost.org/doc/libs/1_55_0/doc/html/thread/synchronization.html#thread.synchronization.futures)?
Nothing big, just a small observation. Skip to the last paragraph if you want the tl;dr. Feedback appreciated as always.
Have you thought about what descriptive name would be appropriate for this practice? Maybe a four-letter acronym? Here are my suggestions: &gt; "Locking object unlocks too" (LOUT) &gt; &gt; "Destruction owning no goodies" (DONG) &gt; &gt; "Exceptions Will be OK" (EWOK)
I don't know if I'm being wooshed or if you don't know about RAII :P
This pattern is very well known and is calledResource Acquisition Is Initialization or simply RAII http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization &gt; How far can we go without offending the company's C++ police? *Not* using this pattern is what will (should) offend the C++ police.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Resource Acquisition Is Initialization**](https://en.wikipedia.org/wiki/Resource%20Acquisition%20Is%20Initialization): [](#sfw) --- &gt; &gt;__Resource Acquisition Is Initialization__ (__RAII__) is a [programming idiom](https://en.wikipedia.org/wiki/Programming_idiom) used in several [object-oriented languages](https://en.wikipedia.org/wiki/Object-oriented_programming_language), most prominently [C++](https://en.wikipedia.org/wiki/C%2B%2B), where it originated, but also [D](https://en.wikipedia.org/wiki/D_(programming_language\)), [Ada](https://en.wikipedia.org/wiki/Ada_(programming_language\)), and [Vala](https://en.wikipedia.org/wiki/Vala_(programming_language\)). The technique was developed for [exception-safe](https://en.wikipedia.org/wiki/Exception-safe) [resource management](https://en.wikipedia.org/wiki/Resource_management_(computing\)) in C++ during 1984–89, primarily by [Bjarne Stroustrup](https://en.wikipedia.org/wiki/Bjarne_Stroustrup) and [Andrew Koenig](https://en.wikipedia.org/wiki/Andrew_Koenig_(programmer\)), and the term itself was coined by Stroustrup. &gt;In RAII, holding a resource is tied to [object lifetime](https://en.wikipedia.org/wiki/Object_lifetime): [resource allocation](https://en.wikipedia.org/wiki/Resource_allocation_(computer\)) (acquisition) is done during object creation (specifically initialization), by the [constructor](https://en.wikipedia.org/wiki/Constructor_(object-oriented_programming\)), while resource deallocation (release) is done during object destruction, by the [destructor](https://en.wikipedia.org/wiki/Destructor_(computer_programming\)). If objects are destructed properly, [resource leaks](https://en.wikipedia.org/wiki/Resource_leak) do not occur. &gt;Other names for this idiom include *Constructor Acquires, Destructor Releases* (CADRe) and *Scope-based Resource Management* (SBRM); this latter term is over-specific and inaccurate, since RAII ties resources to object *lifetime,* which may not coincide with entry and exit of a scope (notably variables allocated on the free store have lifetimes unrelated to any given scope). However, using RAII for automatic variables is the most common use case. &gt; --- ^Interesting: [^Object ^lifetime](https://en.wikipedia.org/wiki/Object_lifetime) ^| [^Dispose ^pattern](https://en.wikipedia.org/wiki/Dispose_pattern) ^| [^Destructor ^\(computer ^programming)](https://en.wikipedia.org/wiki/Destructor_\(computer_programming\)) ^| [^Resource ^management ^\(computing)](https://en.wikipedia.org/wiki/Resource_management_\(computing\)) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+civ9clu) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+civ9clu)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Why is this news?
Read Scott Meyers.
The article isn't about RAII, it's about the fact that we can write exception-safe code without using exception handling code. I guess the article wasn't clear on that.
Don't want to be *that guy* but check [this](http://www.cplusplus.com/reference/mutex/lock_guard/), yep it's called: std::lock_guard &gt; On construction, the mutex object is locked by the calling thread, and on destruction, the mutex is unlocked. It is the simplest lock, and is specially useful as an object with automatic duration that lasts until the end of its context. In this way, it guarantees the mutex object is properly unlocked in case an exception is thrown.
I've seen too many people excuse their non-use of RAII because "we don't use exceptions here!" This article explains why that excuse is invalid, and why "we don't use exceptions here" doesn't mean you should write brittle code.
The article isn't about RAII, it's about the fact that we can write exception-safe code without using exception handling code. Just a little appreciation of that fact. I'm not pretending I invented anything. I guess the article wasn't clear on that.
I have a programmer friend dong a phd in something with genetics, he does a lot of GPU programming for vast simulations. I guess that kind of work is mainly done in universities though. How about economics, I bet they‘ve got some unsolved computational problems hanging about.
The beauty of the pattern that he is trying to apply is the fact that everything can be implemented/used as a value type. The whole mess with base classes and pointers is out the door. Polymorphism and memory management become an implementation detail of a class. xxx::function type erasure is a very nice technique but only usable for abstracting single function interfaces and if you store them the memory management becomes very important. The other big advantage over base classes is that Thingy1 and Thingy2 are NOT dependent on whatever package the base class is implemented so they are very loosely coupled and independent. Please watch the video from Sean Parent and be amazed by the beauty of the pattern he used at Adobe. https://www.youtube.com/watch?v=_BpMYeUFXv8 
The thing is, this is basically how you *should* always write scoped mutexes. Not just mutexes either, but anything where you need to guarantee something happens as soon as you go out of scope, like scoped pointers freeing memory. It is certainly nice to appreciate it and when you learn RAII for the first time, the language suddenly seems really awesome. Regarding exceptions though, I disagree that you ever need them. In fact, the minute you add ONE exception to your code, you now need to basically support it everywhere. In general, I think exceptions are generally unnecessary. If you are suddenly in an unrecoverable state and you throw an exception, you pass the responsibility to someone up the chain to do something about it who probably has no idea what to do now. So you either just crash, or you end up in some super fucked up state. So in this case, exceptions vs return values vs just crashing don't really make much difference. If you're using exceptions as basically a return value saying if something went wrong, you end up with a mess of try catches and the worst part about it is that it's not immediately apparent to the reader where the exit points are. Plus if someone accidentally has code in their destructor that throws and they forget to catch it, your process blows up.
NASA has a lot of C++ heavy projects. If programming systems software for data processing pipelines, or analytical tools for individual missions doesn't appeal to you, then may I suggest the climate simulating supercomputer? http://www.nasa.gov/topics/earth/features/climate-sim-center.html
I’d say bioinformatics is still one of the answers. As you’ve said, the last few years have seen a ton of interesting problems that needed efficient solutions (mainly due to the availability of cheap sequencing, which produced large amounts of data). But now we’re left with a heap of algorithms and an even larger heap of really, really bad implementations: bioinformatics software is predominantly terrible. And even though there are some solid tools, these are all just plumbing: way too low level for effective use in research and health care. People are slowly going over to writing higher-level tools but they (a) sacrifice performance in the process, and (b) still don’t know enough about software engineering to produce robust, easy to use programs.
I just want to point out that programmers working on systems/language tools is pretty much just as indirectly helpful to scientific advancement as direct applied scientific programming. e.g. The thousands of folks working on the GNU / Linux project have certainly contributed immensely to scientific advancement by just making the tools available.
can you please give examples of each of the points you make? Are these open source programs/frameworks you speak of?
any open source frameworks around these problem sets that need people to help out?
Yes, I agree with you. However, I'm looking for opportunities specific to fields, where programming is used to implement the science into reality. Writing software seems more fun when the problem set being solved is interesting. Most people get stuck in writing stuff which essentially boils down to retrieving data from a data source and displaying it elsewhere. 
The article *is* about RAII. &gt; write exception-safe code without using exception handling code And in the article you show how this is done using RAII.
This is was I do for a living. AI, robotics, high energy particles, data processing and machine learning, and data science in general, applied to a variety of fields are all areas where C++ skills are liked for.
Take the dominating desktop genome browser, [IGV](http://www.broadinstitute.org/igv/home). It’s a complete mess, for several reasons. 1. It copes extremely badly with memory pressure. Try loading an annotation exceeding a few hundred megabytes (I need this). The program just hangs indefinitely. Part of the blame probably lies in the fact that the program is written in Java, badly, and does not keep track of what it does with memory. 2. The user interface is a primitive Swing GUI which doesn’t integrate at all with the system and is consequently hard to use (anybody who’s ever had to use the default Swing file browser knows what I’m speaking of). 3. The code is a complete mess. Getting a `NullPointerException` is the rule, not the exception (pun intended). When you then go and look at the source code (it’s OSS), you quickly realise why: the code was written without any understanding of software engineering, and no quality assurance whatsoever. This is a nicely illustrative example for the general trend: complete lack of quality assurance (almost no project has unit tests), no software architecture, no know-how of user interface design (be it graphical or textual), no APIs for easy automated access.
&gt; Matlab, Maple, or Octave Compiled languages are much faster than all these. All three are great but they are 100x or more slower for large data sets.
Have no private firms stepped in to address this problem and release competing products? My first thought is that the market for this would be huge...
Yes: the market is huge, and private firms *are* starting to get into this, but too slowly. I’m not saying that nothing’s being done, just that there’s still a huge market for more.
GEANT4
I can second this opinion. I've been looking at some of the protein crystallography software, and [oh man...](http://www.ccp4.ac.uk/cvs/viewvc.cgi/libmmdb/mmdb/) Free news everywhere, awful parsing, incomprehensible variables, ifs upon loops upon ifs upon loops upon ifs. It's a total mess. Recently the community agreed on a new molecule file format that would allow columns larger than 80 characters (well, 79; the 80th had to be a newline).
is it illegal to post the index of the book ?
No, posting the index of a book for non-commercial purposes has generally fallen under fair use.
&gt; 100x or more slower for large data sets. [citation needed] More the most part I agree with you, but many of the builtin functions of these languages call underlying optimised libraries like BLAS, etc. Comparing them is a lot more complicated than a simple 100x.
Yes they are fast when calling underlying libraries. But for many methods there is some unique algorithm that is in the innermost loop (such as a PDE stencil, or limiters for Euler codes). Nobody has made a pre-packaged library for it, but it must be very fast. I did a large array benchmark for my company comparing matlab, python, fortran and C++; short answer is the non-compiled languages were about 300x to 600x slower. Fortran and C++ were equal speed. If you'd care to enter a friendly contest, you can propose an algorithm of your choice that we and others write in different languages and compare the speed. It's a great way to get rid of the speed mythologies floating around.
&gt; for many methods there is some unique algorithm that is in the innermost loop I had meant to qualify my earlier comment that it only applies when the builtin functions do what you want. But I ended up leaving it unsaid, sorry. When you have to write it all yourself then of course performance really plummets. &gt; If you'd care to enter a friendly contest, you can propose an algorithm of your choice that we and others write in different languages and compare the speed. I use only Matlab for research, but my for my use case there is such a thing as "fast enough". By that I, mean so long as I can process a representative data samples within a practical time, I only actually care about relative timings. But I actually completely agree with you regarding native languages. I use C++ for all my personal coding and am just waiting for the opportunity to rewrite the Matlab proof-of-concept code into a proper C++ library, maybe even OpenCL.
Computer vision and machine learning is a hugely growing field as well. I'd say the main languages used there are C++, Python and Java. OpenCV (www.opencv.org) would be the most popular example, there are a lot of others, for example shape modeling stuff (www.statismo.org) or neural network toolboxes.
Thanks
You have to actually buy it for 50 bucks. You will get updates to it until the final version is released at which point you'll get ~~the physical copy~~ a digital copy. I mean it's a free market, but this idea of releasing things early just strikes me as poor form. Donald Knuth released drafts of his TAOCP Volume 4, but as is standard for Donald, instead of asking people to pay him to review his work, he actually PAYS people who find errors in his work, he sends them a check for 256 dollars. That's a classy way of doing a pre-release review... But then again as I said, it's a free market and Mr. Meyers can do whatever he wants.
Main use case IMO is because constructors can't return error codes.
&gt; You have to actually buy it for 50 bucks. You can get the electronic version for $42.99 on [O'Reilly](http://shop.oreilly.com/product/0636920033707.do). It's PDF-only during the early-release program, but other formats (such as EPUB and MOBI) will be made available when the book is done. If you're an existing customer and use a discount code, you can get it for half off. I just bought my copy for $21.49.
This is like $43. I loved his previous books, but idk if I'd pay $43 for it.
A bit of a late reply, but this is obvious from the right hand side what the type is. auto x = vector&lt;double&gt;{4.0}; Sure it is the same as vector&lt;double&gt; x = {4.0} but it also is not. Benifits of auto in this case are: * The type is only mentioned in the right hand sign * As a side effect of this, searching for variable declarations is _always_ in the form of auto variable_name * auto *forces* you to initialize your variable 
True, thank you.
Llvm compiler has pretty good static analysis 
I always use valgrind - requires linux.
doesn't clang only work on osx? 
Wouldn't you have to exercise all code paths to detect everything?
Yes, you should have tests set up for that and they should be propagated with the code and run on a nightly (or some other regular interval) basis.
Nope, clang works on BSD, Linux, and Windows.
No. It works on BSD, linux, and windows.