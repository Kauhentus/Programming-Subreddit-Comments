The committee spends significant effort to avoid breaking back compat because national bodies have made clear they will not accept anything less. If you have major breaking changes, you are creating a new language, not improving an existing one.
asked a similar question a few weeks ago, [here](https://www.reddit.com/r/cpp/comments/6ssf3o/coding_for_charity_what_are_some_c_open_source/)
If you are interested in general desktop software take a look at KDE, if you are interested in compiler stuff take a look at clang. If you are interested use the search function on github to find something that matches what you are interested in.
Thanks for the Invite! 
https://github.com/fnc12/sqlite_orm
&gt; Do you actually know of successfull, large scale, non-trivial refactorings based on clang-tidy outside of Google? I've recently replaced our own custom optional as well as `std::experimental::optional` by C++17s `std::optional` in a code base with about 100k lines of C++ code with a custom clang-tidy check. I don't know if you count that as non-trivial and as large scale, but the switch from `QVector`/`QList` to `std::vector` should be of similar difficulty with an additionally step to first replace e.g. uses of `first`/`last` by `front`/`back`. The only thing that probably can't be done easily is to replace Java-style iterators by C++ style iterators - but I don't know why anyone would use these anyway, given that none of the algorithms work with Java-style iterators. So people would need to replace these by hand. &gt; Not to mention, that I wouldn't be surprised if a significant part of qt users code base can't be compiled with clang. When you don't write standard compliant C++ you can't complain that your stuff doesn't work anymore. &gt; Regarding fighting the frame work: As I said, I haven't used Qt in a long time - where do you have to fight the framework to use it with modern c++? The ones I remember off the top of my head: * Return objects that have signals or slots from functions by value: Since the `Q_OBJECT` macro automatically defines the copy- and move operations as `deleted`, you currently can't return instances of a class with that macro by value from functions. The only way to work around that that I found so far is to first create an (abstract) base class that has the desired signals and slots, then create a derived class (say `A`) that has the implementation and an additional derived class (say `B`) that stores a `std::unique_ptr&lt;A&gt;`. Then `B`'s constructor just has to connect `A`'s signals to it's own (inherited) version of these signals. `B`'s implementation of the slots just forwards to slot of the same name in the stored `A`, and the move operations replace the stored `A` and reconnect everything correctly. * Templated `QObjects` with signals and slots: Basically needs the same trick as above - one abstract base class that defines the signals and slots and the class template inheriting from it without itself using the `Q_OBJECT` macro. Has the restriction that signal and slot argument types cannot depend on the template parameters, but can be combined with the trick above to get templated `QObject`s with signals and slots that can be returned by value. * Raw owning pointers in Qt and lifetime management: Qt manages ownership with raw pointers. That causes several issues. First, consider a factory for `QWidget`s. Typical advice (see e.g. the note in core guidelines C.50) is to return the objects by `std::unique_ptr&lt;QWidget&gt;`. If you want to put the widget into a `QLayout`, you need to use `std::unique_ptr::release`, because `QLayout::addWidget` expects a `QWidget*`. This is just a slight inconvenience here, but the real problem is that Qt can't distinguish between owning and non-owning pointers. If instead of a widget from a factory you have a widget `w` with automatic storage duration, then the only way to put it into the layout is to use `layout.addWidget(&amp;w)`. However, now the layout needs to be constructed before the widget, because otherwise the layout will delete the widget as it assumes it owns the widget, even though it doesn't. Failing to do so causes the destructor of `w` to be called twice.
People should see Sutter's talk before bitching about moc (or C++/CX). Things like moc exist for a reason.
May be you mean [RAII](http://en.cppreference.com/w/cpp/language/raii). Or something like [scope_exit](http://www.bfilipek.com/2017/04/finalact-follow-up.html).
This is a wide topic. You may want to google and read up on: * C++ destructors (understanding destructors is crucial!) * C++ containers, like `std::vector` and `std::string` * C++ smart pointers like `std::unique_ptr` and `std::shared_ptr`
For [InputIterator](http://eel.is/c++draft/iterator.requirements#input.iterators), we just need `*a` to return `reference`, which must be convertible to `value_type`. It doesn't have to actually be a reference. For [ForwardIterator](http://eel.is/c++draft/iterator.requirements#forward.iterators-1.3), `reference` must specifically be `value_type&amp;` or `value_type const&amp;`.
Nothing to do with Google. They are a quantitative fund, that is a kind of hedge fund that uses quantitative finance to determine how to manage the investment portfolio. You do have to be pretty strong on basic mathematics to work there. Weirdly everyone wears slippers all the time.
If you're into music, specifically the piano, you may want to check out https://github.com/linthesia/linthesia . It is an open source clone of the popular Synthesia software (its codebase is even based on code from when Synthesia was open source). There are many many bugs that can be fixed. Some off the top of my head are the controls being different from what they are displayed as, and a hardcoded (compile time #define) path for the graphics. The overall appearence needs work, such as fonts and the UI responsiveness, and also a windowed mode. There's also a bug that causes high CPU usage for no reason. Some of the code can/should be modernized to utilize some newer features of C++11 and C++14.
I've heard about garbage collectors for c++ but have no Idea if they are really usable. 3 Tips to make memory management simpler though: - Avoid dynamic memory allocation unless you have a very good reason to do so: In c++ you can create objects directly on the stack (which might be counterintuitive for Java programmers, but very natural in c++). - If you have to create an object on the heap for some reason, use std::make_unique / std::make_shared - Use containers like std::vector, if you want to have an array whose size is unknown at compile time (don't use `int* a = new int[N]`). There is rarely a reason to write `new` at all and you essentially can ban delete from you vocabulary entirely.
Gtk+ can be argued to be horrific for various reasons, at least: * For C++ users; gtkmm is not bad for what it does, but it is also not brilliant either -- and what it has to do is not simple. Just viewing the talk I've got the intuition that Qt is in a whole different class in that regard (not surprising given it is C++ from the begining). Plus gtkmm gives you extra bugs over Gtk+, which is not fun. * the situation of the stability of the API, even only the C one, seems to have been a complete mess during the 3.x series. To the point https://blog.gtk.org/2016/09/01/versioning-and-long-term-stability-promise-in-gtk/ seems to rediscover soname bumps (wtf? if you are a major library maintainer and things have been so messy that you have to promise you will handle soname correctly, you have done some things *very* wrong) -- to be clear: there is no problem in changing an API, and it is even great if it is for a better one, but you have to do it correctly! * even the win32 deployment story is not simple, yet alone more modern targets. Random hypothesis: I'm not sure about that but I've heard that at one point Sun was investing in Gnome/Gtk+ (obviously this was a long time ago) and Gtk+ was actually usable during that period. 
It will stop the need for header files but the C++ world has a lot of programmers who refuse to use modern idioms because they dislike change. The most ludicrous example is people using makefiles instead of switching to cmake. So you will still run into header files for decades.
Answer: hopefully. Answer: probably not. The amount of code that would need to be changed to completely remove header inclusion is massive. They may be surplaced with module-style inclusions in the future, but headers aren't going to disappear at any point in the future.
&gt; I come from Java but I am somehow experienced in C, If you want to learn C++ forget all you know about both Java and C. They are all different languages. Even if there are syntax similarities, the semantics differ. Don't use `new` in C++ for instance.
People don't "refuse to use modern idioms" just because they "dislike change." Nor is CMake vs Make an example of this. People have valid technical reasons to stick with old idioms: Existing code that they don't have the resources to update, existing code that works well enough that it's not worth the risk of introducing bugs by modifying it, lack of support for new idioms in the toolchains they must use for their target platforms, and yes- inherent downsides of the new idioms.
https://github.com/include-what-you-use
So header files will still be necessary for modules to work?
Every time I hear someone refer to something about software being "modern" I hear Audrey Hepburn in my head saying "it's very modren" like it's a value unto itself.
Herb Sutter did some work on that area. https://github.com/hsutter/gcpp However, since you’ve mentioned you are learning c++, I wouldn’t advise to try it before actually having a full grasp of how memory management is done in C++.
I have a love-hate relationship with Qt. On one hand, Qt is amazing. It is largely comprehensive in terms of functionality and brings C++ closer to the likes of Java and C# in terms of available tools. It boasts many good design decisions and keeps things super simple most of the time. It's so nice having one code base that works on so many platforms. On the other hand, many of Qt's design decisions were clever 20 years ago but are pretty silly today. Qt is another victim of "we don't wanna break user code". That's fine for those existing clients, but it puts the framework on a death march into irrelevance. I become more and more hesitant to rely on Qt for new projects. I have a serious suggestion for Qt: create a new programming language. Keep it low level; keep it compiling to native binaries; maintain interop with C/C++. Qt has invested heavily into all this infrastructure (a meta compiler, an IDE, etc.), I feel the natural step is a language that needs no meta compilation. Since I know a new programming language will never happen, I just want Qt 6 to switch from UTF-16 to UTF-8, drop all the reference counting in favor of move semantics, decrease the insane reliance on heap allocations, etc.
Wow, reference to the man who was a TA in a course i took. This is the first time that I stumble upon someone from my university. *feels proud*
Why is this CppCon video posted from a channel other than the official CppCon channel?
I'd use modules right now if it were part of the standard, but it just isn't there yet and using existing implementations could waste a lot of time and effort if it turns out not to be the one they standardize on.
I gave a talk on C++ Modules at CppCon 2017 (and also implemented support for modules in `build2`) so I've done a bit of research/thinking on this. Firstly, it is unlikely system and C headers will ever be modularized so those we will continue to `#include`. While I expect the C++ standard library to be modularized (there is already an early proposal for this), there will probably still be a few headers for macros (current Modules TS proposal does not support exporting macros and I hope it will stay that way). Then we have third-party libraries that will probably get modularized at various rates (if at all). So my feeling is that most real-world C++ projects will continue combining inclusion and importation for some years to come. Though this doesn't mean there won't be "pure modules" codebases, especially if your only dependency is the C++ standard library. Needless to say, if you are interested in C++ Modules, I would recommend that you watch the videos of my talk as well as Nathan's (on implementing modules in GCC) once the videos are out.
Not that I'm aware. What makes you think that?
Realistically? Not for decades to come.
To be more precise: I'm not sure, if the standard even has an official notion of header files, but I very much doubt that they will remove the `#include` functionality in the foreseeable future. For one, due to backwards compatibility and second because modules will most likely not offer the same functionality.
As you have some experience with modules, what's their impact in build times? Do they help considerably reducing them?
It was hosted by the Northwest C++ users group.
I do. Why wouldn't you? If you care about performance you shouldn't be handling strings at all.
Reflection sufficient to do the things you described can all be done with macros, and with boost pp it's not even particulrly difficult to implement, let alone use. And then Qt wouldn't have to be its own obnoxious mini universe within C++ to the extent that it is. I can forgive something like protobuf because it generates bindings for multiple languages but I haven't seen anything in Qt that wouldn't have been better done in-language (as un-ideal as macros are).
The pitfall is that it's a pretty complex feature and people are going to abuse it to death. That is a very serious pitfall and one that has me quite worried. Sometimes, esp in application dev where the programmers aren't nearly as skilled at C++ and the applications are less generic, it's not so terrible to just write something out instead of using something complex to make it generic so it can be used in a grand total of two places.
It's almost perfect, tbh. Weird name for it though, usually seen it called ScopeGuard. You can also implement (as of 17) ExceptionGuard, which only executes the lamda if the scope exit is caused by an exception. This saves you having to call `guard.dismiss()` by hand often.
I'd like to see specific examples of Qt MOC problems that cannot be solved with reflection, or reflection + CRTP, before we start heralding metaclasses as the savior.
QScopedPointer or std::unique_ptr? It's so nice of you guys to ~~copy~~ reimplement all of the standard library and bring it to me alongside with the existing one in an incompatible fashion. I'm never tired of choosing one over another!
With the disclaimer that this will depend on the compiler and that these are still *very*, *very* early days in term of implementation maturity/optimization, at the beginning of my talk I run a modules vs headers build demo of a real C++ library (`libbutl`, the `build2` utility library which has slightly more than 30 modules). With Clang and using modularized `libstdc++` I get ~3 times speedup.
If it's not at least 3-5x speedup I will be disappointed. This is what I've typically seen when switching to unity builds which have a similar large reduction in parsing work.
I think that I'm not alone on the desire to keep the angry political discussions outside this board... This is really... tiresome. Most of us come over here because we want to talk about C++ and nothing more than that.
You are comparing a limited hack that completely destroys whatever structure headers provide to a sound physical design mechanism that has a chance of actually improving the code structure, not only the build speed.
Suppose you are writing a zip iterator over input sequences returning unique pointers. The closest one gets with the STL1 is a proxy reference: tuple&lt;&amp;,&amp;&gt;, and a tuple value type. The problem is, that the tuple of references to tuple conversion only works for Copyable types, but unique pointers are not copyable. 
Weird. This is not on the official CPPCon youtube channel. I've been hitting refresh on that one all day. Is this recorded by some random dude in the audience? Not that i'm complaining like. :) 
&gt; In my opinion, Qt should get rid of all it's containers¹, essentially all of the threading stuff (e.g. QThread, QMutex, QFuture) as well as the Qt smart pointers You aren't required to use them except for the case with the Models: there was a QList somewhere in the API. &gt; Lars didn't really answer Odin's question why they don't just use free functions operating on std::string or std::wstring for the unicode specific stuff instead of member functions on their custom string. Replacing QString with std::string will break all the code. So that's a meaningless question.
&gt; As far as QThread goes: As long as the standard library doesn't have support for message passing between threads I'd say yes (and afaik, there is nothing preventing QThread from using std::thread internally). Let QThread be in the library for the backward compatibility. We have std::thread now, so we can use std::thread with QEvent loop until C++ gets something comparable.
prefer build time sandwiches tbh &lt;&lt; which is how I read it the first time somehow I must be hungry
My problem with Qt is the QApplication: you can't use Qt properly as a library when multiple independent modules link to it. But, yes, it's only about QString. There is also QList that pops up in some APIs. All other classes and containers aren't required when using Qt.
&gt; You aren't required to use them except for the case with the Models: there was a QList somewhere in the API. I know, and I don't use them. But passing containers in signals or properties isn't supported for all std containers by default and you have to register them manually, at least that was the case last I checked. IIRC, `std::shared_ptr` has to be registered manually and e.g. for `std::set` there isn't even a macro to register it for all types `T`, so you have to register it for every kind of set you'll have individually. Getting rid of the containers in Qt and instead improving support for std containers should reduce the amount of code that needs to be maintained significantly and thus makes sense, in particular since it also improves interoperability with other C++ libraries. &gt; Replacing QString with std::string will break all the code. So that's a meaningless question. No it isn't. Qt only guarantees API compatibility within major versions, not across different major versions. If it is feasible to provide a clang-tidy based tool to transform user code that uses `QString`s into user code that uses `std::string`/`std::wstring` + additional free functions for the unicode specific functionality, then it might be worth to get rid of `QString` entirely. This has the benefit of not having to convert between the two when interfacing with 3rd party libraries (most of which expect a `std::string`, or probably a `std::string_view` in the future) as well as reducing maintenance burden, just like removing the other containers.
Haha ok, cool, thank you! Is that the people that try to outperform index trackers, and 90% or more of them fail to?
That seems like all the more reason to beat unity builds. I also hope it's better than unity builds so they can diaf
&gt; No it isn't. Qt only guarantees API compatibility within major versions, not across different major versions. If it is feasible to provide a clang-tidy based tool to transform user code that uses QStrings into user code that uses std::string/std::wstring + additional free functions for the Unicode specific functionality, then it might be worth to get rid of QString entirely. This has the benefit of not having to convert between the two when interfacing with 3rd party libraries (most of which expect a std::string, or probably a std::string_view in the future) as well as reducing maintenance burden, just like removing the other containers. Qt also guarantees ABI compatibility across different versions and compilers and simply exposing most of std types in the API would make that impossible. Not to mention that `std::string` can no longer be COW, which is very important for `QString`. Furthermore, and this is my IMHO, I think that the move to `std::string` would be a downgrade in most areas, except probably performance wise on some compilers. `QString` is superior to the std string in terms of api convenience. Also, you can still easily convert it to std string, when you need to interact with some other libs, and in newer Qt versions (5.10+) you would even be able to avoid conversions altogether with `QStringView`.
If such a feature is implemented as opt-in rather than default there is no reason to think that it would break older code.
If you are interested, I have a project that could use some help! [chemfiles](https://github.com/chemfiles/chemfiles) is a library for computational chemistry (use in scientific research!). It is written in C++11 with a C API and multiple languages bindings (Python, Fortran, Rust, and Julia. Javascript --- node.js and ams.js using emscripten --- is a WIP). There are quite a few changes that can be done without any scientific knowledge: [using shared pointers in C API](https://github.com/chemfiles/chemfiles/issues/87), writing a new parser for the selection language, writing some kind of optimizer for the selections engine, ... The code is modern and small-ish (12k LOC), so getting in should not be too hard. Come say hi on [gitter](https://gitter.im/chemfiles/chemfiles) if you are interested!
I forgot to ask you directly, but during your talk, you mentioned that an easy way to improve your build time would be, in theory, to moduralized libstd (e.g. have a module that do export all the include of the stl) But I'm not clear as to why this would be faster than just include the header as today, why would it bring faster compile time?
Clang tends to run faster and give better messages, and they are building more tooling around clang. Yes, they need to be careful, and surely they run tests under gcc (or both). I would also guess that it gets compiled against gcc as part of a git hook. In general, compiling with multiple compilers finds more programmer errors than compiling with any one compiler.
Obligatory mention of http://www.boost.org/ There is an open source C++ review subreddit at https://www.reddit.com/r/cpp_review/ too.
Definitely not. C++ Modules (in the Microsoft implementation at least) can come in shared library, static library and header only library forms too. By implication header files will be around to stay as they are one of three main forms of C++ Module.
Yeah, I'm hoping someone just got their wires crossed. Like maybe whomever is uploading is in charge of both NWCPP and CppCon channels, and just uploaded to the wrong one. Otherwise I'm not a big fan of this. (I've seen a channel take C++Now videos and add ads to make some money.) I'll wait and watch it on the CppCon channel.
OK, well Bryce mentioned on the other one that NWCPP hosted the panels, so OK then, I guess.
if the comittee isn't willing to break existing codebases by making 'await' and 'yield' keywords they ain't gonna be removing headers
I'm not the OP and I don't know if this post is related, but a similar issue was brought to my attention the other day by [this Twitter thread/rant by Isabella Muerte](https://twitter.com/slurpsmadrips/status/913458948862128128) (retweeted by Meeting C++, thanks Jens). I wasn't at Cppcon this year so I'm not sure which talk (if any) the tweets are referencing, but could anybody "in the know" comment on whether we are going to finally see the end of the interface/implementation split, or are we still going to have to deal with it in a modules-enabled world?
Check out https://github.com/Mudlet/Mudlet, text games client in C++11, Qt, and Lua.
Because parsing takes time. An imported module doesn't need to be parsed.
Isn't there some problem with Apple not allowing static (or dynamic?) linking on the AppStore or something like that, and that resulted in basically you not being able to, strictly, put a LGPLv3 Qt app onto their store, because of Apple's terms? Or something like that? I remember it at least being legal grey-area (or prohibited by Apple).
Can you ELI5 why interface/implementation split is bad and what is more ideal? Personally I think header/module + shared library distribution can still work fine to some extent.
What do you consider advanced? Personally I found cppcon conference talks pretty interesting, but it is probably not a very good way to learn c++, as the talks are very specific, examples are simplified and it takes a long not time to watch them compared to the amount of information you learn. One website you should probably have a look at is https://www.fluentcpp.com, which covers a lot of topics around c++.
I think it's a valid comparison, and target, if we're trying to ballpark expected improvements in build times. It's reasonable to expect modules to provide a similar speedup to unity builds given that they are both achieving their speedups mainly by eliminating massive redundant #include parsing, template instantiation and semantic analysis. I don't see the extra bookkeeping needed for the extra structure inherent to modules imposing that much of an additional overhead over unity builds. Unity builds are hacky for sure, but like other hacks they do get used in industry, particularly the game industry due to pragmatic concerns of minimizing the edit-compile-debug cycle times when you have a lot of code inlined in headers. I'll be as glad as the next guy to see unity builds die once we have a non-hacky competing alternative with c++ modules :)
Anyone have opinions on [C++ Concurrency in Action](https://www.manning.com/books/c-plus-plus-concurrency-in-action-second-edition)?
&gt; develop my coding skills &gt;offers working on bost Boost is probably the last thing I'd recommend to someone who is not an expert in C++. You need a serious understanding of templates for almost all of it.
Yep. Good book, recently spent two days learning from Anthony Williams at CppCon. Note that the 2nd edition is due to be published Real Soon Now. Buy the MEAP now, and you get a PDF of the 1st edition as well.
Ah but he'd definitely develop his coding skills as he requested. Might as well go straight into the deep if you're going to.
These are open content panels at the conference, run by NWCPP. We don't record these sessions; others are free to, and I believe they did. TL;DR We are fine with this, these are not official recordings that belong on our main YouTube account.
To clarify: These are open content panels at the conference, run by NWCPP. We don't record these sessions; others are free to, and I believe they did. TL;DR We are fine with this, these are not official recordings that belong on our main YouTube account.
Might be a little *too* deep IMO. I've been using templates and have no problem making simple classes/functions that depend on them, but boost goes to a level that is just above what any normal person would use template for. Also they have some really ugly hacks to replace some things that the language hasn't standardized yet/compilers don't support correctly.
But `enum class` are not supposed to convert implicitly to integers, and yet they work fine in switch statements (which is probably one of the few ways you want to use them in the first place).
I recently purchased this book out of a lack of a good central online resource for C++ concurrency. So far, this book has been going well. The book assumes you have a good background in writing modern C++ (C++11/14); this isn't something for beginners to the language.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/73m8c9/what_are_some_pluginscharacteristics_of_the_c/dnsduj3/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Agreed, and the title is quite misleading in this case..
"I don't know make, but from the cmake tutorial over there, I learned to do some easy things". There, fixed!
Game industry doesn't know of /*.inl files?!
What are you using today, if any?
I did use testrunner-lite still up until about a year ago because it allowed somewhat easy way to run the tests on multiple devices. After that, since moving to qt, qtest/qmake combo provided decent enough runner capabilities with just "make check". Albeit even that did not provide segfault guarding. However, getting atleast a suite wide results was just matter of generating a template report when qtest based suite failed to generate a report on its own. 
I have the feeling that many here criticizing Qt for its API stability have no clue about industrial software development... Changing API is a deal breaker for most companies. Just look at Windows, part of why it is so successful is that old APIs from Windows 98 still work. Qt may have its problems as well, but it most likely is NOT the API.
Yes, use Linux and KDE, and contribute to KDE. You will learn a lot by many very friendly people! 
It's not a 'how to' by any stretch of the imagination. It's just 22 minutes of someone typing code into visual studio from some source verbatim. It doesn't explain anything at all. You've been told by a mod at /r/C_Programming that this content shouldn't be posted there. It shouldn't be posted here either.
It was a problem before because you had to pay apple 99$ to be able to send apps to your iphone from xcode, but this restriction has been removed so nowadays you can change lgpl components would the need arise
Even more than this. Correct me, if I'm wrong: Every instantiated template (e.g. basic_string&lt;char&gt;) is compiled and generated for every object file it is used in. Only at linking all but one copy of the generated function are thrown away.
Why hopefully? Why are headers considered bad/deprecated/uneeded?
They are slow and error prone (include guards, pragma once, etc).
Because due to their transitive nature (headers include many headers which include many headers etc...) they are the number one reason for slow compile times. Each time a compiler evaluates a compilation unit it must first read all of the included headers into memory which can quickly turn into 100s kB for what appears to be a small number of headers. That whole body of CPP then needs to be parsed and compiled. All of this takes a lot of IO, CPU and memory. And each and every CPP file needs to go and do the exact same thing, over and over again. Ultimately, it's a very crude text substitution mechanism from the 70s that is very inefficient. Things like precompiled headers partially work around the situation but they have their own problems. The real solution will be modules, where each module is only processed once, and then other compilation units can import them without having to fully read and process them again, for each file. Most modern languages have something like modules. CPP uses headers due to its C legacy.
Anthony Williams is a reference in modern C++ multithreading. That book is great. 
That's encouraging! :)
I'm reading C++ Primer while on holiday in Spain. Evey chapter has some excercises but they aren't part of the 'story' so you can skip them without breaking the flow of the content. So far I'm really enjoying this book.
That's not going to change, is it? The individual translation units will still need to generate those templates, no matter if they came from a header or a module. I have no idea what fraction of time goes into this though. Anybody know? Is it actually useful (in terms of compile time) to `extern template` commonly generated templates, and then explicitly generate them yourself somewhere? 
Is this largely eliminated through proper style and practice or will this be an issue even in the best of situations? I'm mostly a C# Dev trying to get into C++, sorry if these are a bit basic or lacking intuition.
More ideal, but I do admit it is a matter of choice, is that each 'thing' in your project gets represented by one file rather than one pair of files. It looks a lot neater (again, personal opinion), it reduces the amount of repetition (all those function signatures!), and it ensures all function-level comments are located near both the declaration and the code (rather than just near the declaration, like in many projects). I suppose one could argue that .h files represent the public interface of a class, and .cpp files represent it's private implementation, but that's only partially true: .h files leak a lot of implementation detail anyway. Having a few `export`s in there to make it clear what is public and what is not is, I think, a major step forwards in that sense, even if the code is available for reading. Moreover, I believe people are looking into a system whereby a module file is really the only thing that gets distributed, replacing both .h files and .lib files (so you'd drop the module file into your project and it is immediately ready for use, pretty much removing the need for comprehensive package managers). In that case the split between interface and implementation doesn't matter at all anymore; the only thing the end user gets to see is the module file, which only makes the public parts of the module available to him no matter what it's internal structure looks like. 
Bitkonga is best bitcoin exchange site in Nigeria, I have hundreds of bitcoin to naira exchange with them
Our projects are using a custom pch file, so no stdafx.h/cpp... I hope your incoming fix will take care of that ! Good job by the way =) 
I don't trust Bitkonga. I only buy my bitcoins from selected Nigerian princes.
No problem at all. You can certainly make things a lot worse by doing the wrong things. The general principle is to avoid as much as possible including headers in headers. A header is usually included by other headers, so you have a transitive property and you pay the cost of the include many many times over. A CPP file is not included by other files so if you have an include you don't need you only pay that cost locally to that CPP. Not that CPP files shouldn't I only include the correct headers, but the focus is usually on headers themselves. When defining types in CPP one of the important thing that a compiler needs to know is "how big is this thing?" so that it can allocate memory for it. In order to know that it needs to know how big the component parts are (things like data members). The size of the data member is defined in the header for that type, so we need to include the header for a value data member. However, there are lots of cases where we don't need the header included in our header. For example, if it's a pointer member - all pointers on a given platform are the same size, so the compiler doesn't care whether it's a pointer to Foo or Bar, it's still 32 bits or 64 bits etc... In this kind of situation where a compiler needs to know of another type, but not everything about the type, we can "forward declare" the type without including the header in the header. This is hugely more efficient and is recommended everywhere where it's possible. Any CPP file that uses our new header will still need to include all the headers in question, but we've prevented the include fan-out. You can read more online about which situations allow us to fwd declare and which require a header. So, we spend a lot of time making sure our codebase adheres to these and other rules about compilation speed. The result is that it's still orders of magnitude slower than most other languages. We really need modules to break out of that. But if you aren't diligent you can easily make things 2x as bad.
Not sure if you have a lot of experience with multithreading and just need to learn about c++ interfaces? If not there are things that are not language specific that are probably more important to learn. Can you clarify?
No. Consider for example a class A, which contains a field of type B, which contains a field of type C. Those are spread over the header a.h, b.h and c.h. If you want to instantiate an object of type A, the compiler needs to know, how much memory he has to allocate at runtime. So it looks at the definition of class A, to calculate the size of the type. It sees, that there is a field of type B, so it calculates the size of class B by looking at its definition. And the same for the field of type C in class B. Due to the isolation of each compilation unit (that is every .cpp file) and a resulting lack of type information of already compiled types during compilaton, you cannot eliminate this problem with proper code style.
With a sandwich, build time definitely passes much faster!
Thank you for taking the time to write that. I appreciate it and it's something that I can see saving me both time and headaches as I'm moving forward in C++. From your post, it appears that dealing with headers properly adds a substantial bit of time to whatever project you are developing. 
Yes, they still have to be instantiated multiple times, but at least part of the work needs only to be done once (parsing, non-dependent lookup) There is no `extern template` anymore (and afaik there was only one compiler that ever supported it in the first place). You can forward declare and explicitly instantiate it though
enable() and disable() are certainly simpler, but what if the action to be taken depends on runtime information and a boolean value that is the result of the computation needs to be passed to an enabling/disabling function? A good API would have all the 3 functions, but having 'set_enabled' is a defacto requirement. 
Yeah, it certainly adds to the task at hand. Luckily in the last few years tooling has got much better in this regard. There are tools such as IncudeWhatYouUse which will partially automate this for you. However, as a CPP developer you'll need to be very familiar with this area. Don't worry though, after a while like anything else it becomes second nature. All the best in your learning!
Alright, so what if a.h, b.h, and c.h are all interdependent. Compiler tries to compile a.h, sees it needs b.h and tries to compile, then it sees it needs c.h, which in turn needs a.h. What are headers actually good for then? At first glance it would seem to eliminate code re-use which is a good thing, but it also seems like it would just create an incentive to do everything in headers. Or is that not even possible? 
Thanks! 
Why not the other way around? Qt existed before STL was a thing. QScopedPointer existed before std::unique_ptr was a thing. Not to mention that Qt runs and works where the C++ support has been historically rather poor/lagged behind.
http://preshing.com/ jeff preshing wrote some good blog posts about high performance lock free programming also gave a talk at cppcon: https://www.youtube.com/watch?v=X1T3IQ4N-3g&amp;t=305s herb sutter att cppcon Lockfreeprogramming: https://www.youtube.com/watch?v=c1gO9aB9nbs&amp;t https://www.youtube.com/watch?v=CmxkPChOcvw and understanding whtas going on at low level https://channel9.msdn.com/Shows/Going+Deep/Cpp-and-Beyond-2012-Herb-Sutter-atomic-Weapons-1-of-2 https://channel9.msdn.com/Shows/Going+Deep/Cpp-and-Beyond-2012-Herb-Sutter-atomic-Weapons-2-of-2 at least those were really interesting to me! beside those links as someone mentioned C++ Concurrency in Action seems to be a really good book, i'm reading it my self at the moment :)
its really good, i'm reading it my self at the moment
thats great, maybe I haven't found them yet :)
Oh I see. If that really was the only issue. Great for all of us that they removed the fee :-)
You can listen to podcast also, cppcast is great
I agree 100% with what you're saying. But it's probably worth also pointing out that there are patterns such as PIMPL which can eliminate this. However, they come at the cost of runtime performance (dynamic allocation and pointer indirection). Like most things, it's a matter of tradeoffs.
A small correction: the compiler doesn't compile a.h, but it compiles x.cpp (which represents a single compilation unit), which includes a.h. And include means, that the preprocessor (which runs before the compiler) simply replaces the #include statement with the file contents of the header files (and this transitively). And of course there are ways to instruct the compiler in such a way, that he doesn't get stuck in the circular dependency. So what they are good for? Actually they reduce compile times. A header file contains (usually) only the declarations of your types and functions. Not the actual definition/implementation of it. So it reduces the amount of code to compile for each compilation unit, because the definitions are compiled only once. However with including all those declarations, the amount of compiled code for each compilation unit exceeds in many cases the amount of code to be compiled in more recent languages. And yes, it is certainly possible to put everything in header files and have only one cpp file. (There are actually libraries who do this. They are called "single header libraries". They exist due to the lack of portability of C++ code). But if you'd do that and change only one file, you had to recompile the entire project every time, because there is only one compilation unit. Or TLDR: headers allow to share declarations for multiple compilation units, which makes incremental compilation possible and reduces compile times for each compilation unit.
EditorContextMenus.CodeWindow.ToggleHeaderCodeFile option for header/source toggling Window.Split* for splitting all of these can be mapped to whatever key combination you want
&gt; Reflection sufficient to do the things you described can all be done with macros, and with boost pp it's not even particulrly difficult to implement, let alone use. And then Qt wouldn't have to be its own obnoxious mini universe within C++ to the extent that it is. look at how terrible the macros have to look if you remove moc: https://woboq.com/blog/verdigris-qt-without-moc.html you basically have to write all your prototypes twice
A module interface can define non-inline functions and variables so, yes, if you wan to, you can implement any module as a single interface file. There are, however, several drawbacks to doing this. I talk about the issues in my CppCon presentation, but, in a nutshell, keeping implementation in interface may cause unnecessary recompilations as well as reduce parallelism opportunities (it also reduces the readability of your interface but that you can probably work around). Unnecessary recompilations will happen when you are changing implementation details of your module: since they are in the interface, all the consumers of your module will have to be recompiled anyway. The parallelism issue arises from having to compile the implementation as part of the interface. If it were split, then we could start compiling module consumers (as well as the implementation) as soon as the interface is done. The build system in cooperation with the compiler could probably work around the former e.g., if compiler produces identical binary module interface file for unchanged interfaces then the build system detects that and skips recompiling the consumers.
I was reading something, may be SO, that basically said the .cpp files each got compiled by a unique compilation instance (don't know the name, sorry) instance. They also says that header files are all done in a single instance, and there's part of the speed problem. In that case, you'd expect to only need header files, or possibly just an entry point. Is this correct? Dated? Or just somethig that half true? Thanks for answering my questions. I appreciate it.
&gt; old APIs from Windows 98 still work umm
I'm stuck with c++11, so I can use `is_detected` but not constexpr-if.
&gt; EditorContextMenus where do i changes this?
&gt; the .cpp files each got compiled by a unique compilation instance (don't know the name, sorry) instance _Each_ .cpp file gets one, called a translation unit. &gt; header files are all done in a single instance, and there's part of the speed problem Only if one is using precompiled headers, but otherwise no.
That was `export template`; `extern template` was introduced with C++11 and is implemented by every major compiler vendor.
Not sure what you're deliberating about... Lots of Win16 APIs still exist in user32.dll.
Think about where you would _use_ `is_detected` for SFINAE... ;-]
Sorry, yes I mixed them up, you are right of course
You mean it wouldn't make syntax much better because I would have to wrap it in an enable_if anyway?
Right; it just gives you a (constant-expression-) bool – you still have to use that value somewhere, be it `enable_if` or `static_assert`. :-] (Which isn't to say that it doesn't yield better syntax; just that it doesn't do anything on its own.)
got it. see my edit :)
To that, I wouldn't use 'is\_detected' in conjunction with 'is\__some\_type_' – the former pairs with something that SFINAEs, and the latter also pairs with something that SFINAEs, namely `enable_if` rather than something that produces yet another boolean. This feels like a bit of an XY-problem; what is your actual goal?
This comes up every few months... Given that string and vector have recently gotten great performance improvements in MS's STL, please give the same love to regex. We've got a large project that can't use boost and the regex code is dog slow on our windows builds. 
Oh, I don't deny that lots of APIs still work. But just as many broke along the way.
You should communicate with more than utterances, then. ;-]
The big advantage with Boost is diversity of choice. Some of the older libraries are template excessive true, but the newer ones don't tend to repeat that mistake and use modern C++ instead of hacks. They are certainly a good grounding in the current state of the art. Just choose any library admitted in the past five years according to your interests.
Is Scott back in the panel this year ? Any talk by him?
The whole blogpost is "I absolutely liked it!!" with minor politics and social points. I don't have a problem with that. But there nothing else - no real technical details, no post thought analysis. Nothing but water. Don't get me wrong - I'm happy that this person got to CppCon. It's always good to meet new AND qualified people, who are willing to share their ideas and thoughts. I just don't get why we have this blogpost here. What am I supposed to take from it? 
Clearly you're not thinking like a "normal person" – there used to be excessive _preprocessor usage_, but there has always been excessive usage of templates. ^/s
&gt; I just don't get why we have this blogpost here. What am I supposed to take from it? The fact that there's a female who likes C++ is newsworthy, obviously. ^/s
In the newer libraries, really no
Do you really mean to indicate that you don't know what sarcasm is, Niall?
Tools -&gt; options -&gt; environment -&gt; keyboard As far as i remember, cant check it now 
I might have been that person, though my concern was more related to the heap allocation associated with exception ptr, so maybe not. I'll check it out either way though :-). 
Good point. There arises a "better C++ than the C++ itself" dilemma, not just a local conflict of incompatible smartpointers.
Anyone who takes Qt's side on a "better C++ than the C++ itself" debate needs a bit of introspection... &gt;_&gt;
You use `decltype` and `auto`, so you already require a C++11 compliant compiler. So why do you add the Boost dependency? Aside from Boost.Assert (just use `&lt;cassert&gt;`) and Boost.Binary (minor functionality), every other Boost header you use is in C++11. Also, you should be using `&lt;..&gt;` instead of `"..."` for system header files. In addition to being convention, the angle brackets will not search your local directory. Why reimpliment `max` [here](https://github.com/HDembinski/stateful_pointer/blob/master/include/stateful_pointer/tagged_ptr.hpp#L14)? It's in the standard library already. A suggestion in general I would make is to try to duplicate the interface of `std::unique_ptr` whenever possible. So for example, there is `make_unique` for both array and non-array types, but you split it into `make_tagged_array` and `make_tagged_ptr`. `make_tagged_array` also takes more arguments than `make_unique` does for array types. Lastly, I would make it more clear in the documentation that this class must perform the allocation: there is no `tagged_ptr(T*)` constructor, because you need ensure it is aligned to the correct memory boundary. That could be an issue for some people.
Strange. Isn't `is_detected` not even part of any current standard but just of the `Library Fundamentals TS v2`?
yes but it takes ~8 lines to implement it in C++11
eh, I was just wondering if there was a shortcut for SFINAE on template class template methods: template&lt;typename T&gt; template&lt;typename U = T, typename std::enable_if&lt;std::is_same&lt;TypeManipulator&lt;U&gt;::type, int&gt;::value ,int&gt;::type = 0&gt; void Class::Method() {}
I see. No – a type can either always produce a value, _or_ it can SFINAE out; `is_same` and `is_detected` both do the former, and thus must be used with something else that does SFINAE (or with `static_assert` or `if constexpr`) in order to be useful. :-]
That's cool feedback, thanks! * Yes, the lib is C++11. * I cannot avoid Boost, because I rely on Boost.Align to allocate aligned memory on the heap. There is no such functionality in C++11, see http://www.boost.org/doc/libs/1_65_0/doc/html/align/rationale.html * IMHO the distinction `&lt;..&gt;` vs. `".."` is one of many needless C++ idiosyncrasies, but I see your point. * My `max` is `constexpr`, while `std::max` is not until C++14 * I duplicated the interface of `std::unique_ptr` whenever possible. You are right that I should merge `make_tagged_array` and `make_tagged_ptr`, but I think it is fair though if `make_tagged_array` does more than `make_unique` for the array case. The restricted interface of `make_unique` for arrays does not make sense to me. * Agreed, I should explain in the README that using this library conflicts with other heap allocation customisations.
&gt; IMHO the distinction `&lt;..&gt;` vs. `".."` is one of many needless C++ idiosyncrasies, but I see your point. It's not needless if you work on any codebase large enough to eventually end up with some header with the same name as a header from some library. I.e., it's never needless at all, merely not-yet-needed. ;-] &gt; You are right that I should merge `make_tagged_array` and `make_tagged_ptr` `std::make_shared` makes `shared_ptr&lt;T&gt;`, `shared_ptr&lt;T[]&gt;`, and `shared_ptr&lt;T[N]&gt;` (and likewise for `std::make_unique` for `unique_ptr&lt;T&gt;`, `unique_ptr&lt;T[]&gt;`~~, and `unique_ptr&lt;T[N]&gt;`~~), so the precedent is to _not_ need two differently-named factory functions for normal pointers vs. arrays. (Unless the resulting type is also completely-differently-named, at which point I would question _that_ rather than the factory functions.)
Obviously not!
AFIO uses the hooks support in Outcome to track the current handle for the current thread, and thus one can deduce which handle and path a failure relates to. I'll be doing up an Outcome tutorial section on how to implement something similar. But for sure, it's a lot more involved that simply subclassing some exception type and adding in some payload to be type erased.
I think a better design would have been to always use `".."` for includes and prefix the std library with "std", like so `#include "std/vector"`. Of course there is nothing to do about it now, and I guess C++ had no choice in the matter, because they inherited this from C. I hope we get modules in C++20, which should fix all this. According to [this](http://en.cppreference.com/w/cpp/memory/unique_ptr/make_unique), there is no `make_unique` for `T[N]`, but I will try to add one, when I make the change.
In order to set its headers apart, the standard library does not have extensions on its files, hence `string` vs. `string.h` (like C has) or `string.hpp`. This problem comes into play with third-party headers other than the stdlib, e.g. Boost, Qt, MFC, etc. &gt; According to this, there is no `make_unique` for `T[N]` There is – it's what 'array types with known bound' are. `T[N]` is 'with known bound', `T[]` is 'without'.
there's also "Concurrency with Modern C++" by R. Grimm: https://leanpub.com/concurrencywithmodernc
I like using quotes to include related files from the same directory, e.g. a cpp files corresponding header. So it's nice to have distinct syntax for "foreign" includes.
it does seem very unusual for a boost library to wrap another (large) open source library
oh yeah the i already know this toggle command lol :D but not sure about the split, its weird to use ..
&gt; so it does work with classes/environments that also customize heap allocation. Do you mean "doesn't"?
I don't know if the macros would have to look like that. I don't see any reason why you would have to write prototypes twice, should always be able to avoid that with macros. For example, instead of: CS_SLOT_1(Public, void mySlot(int x)) CS_SLOT_2(mySlot) You could write: CS_SLOT(Public, void, mySlot, int); Not a thing of beauty but better than repetition, and it's definitely not worth going through all the QT silliness just to avoid that.
&gt;There is – it's what 'array types with known bound' are. `T[N]` is 'with known bound', `T[]` is 'without'. And the constructor for arrays with known bound is explicitly deleted, with the comment "Construction of arrays of known bound is disallowed."
Yes, as others have pointed out, you avoid re-preprocessing and re-parsing the standard library declarations for every translation unit. This turns out to be quite costly (think of all the template and inline functions that typical C++ translation unit pulls from the standard library). &gt; have a module that do export all the include of the stl As I pointed out in my talk this approach of "exporting" headers (we call it "guerrilla modularization") has many issues and is probably only suited for prototyping and exploratory modularization. To be practically usable, the standard library implementations will have to be modularized properly, that is, by adjusting their code to export the correct set of entities, etc.
You're right, for `make_unique`; `make_shared` supports both, and my point stands regardless. :-]
&gt; Not a thing of beauty but better than repetition, and it's definitely not worth going through all the QT silliness just to avoid that. Ugh, having tried to do it for a few classes at some point, I entirely disagree. I'd rather start using another language than having to write stuff like this.
Glad to hear, I've had the 2nd ed for a while, but it's been in queue behind _Programming Rust_ and _Haskell Principles_ For anyone curious, here's Anthony Williams from last year: [CppCon 2016: Anthony Williams “The Continuing Future of C++ Concurrency"](https://www.youtube.com/watch?v=FaHJOkOrfNo)
Whoops, you are right.
&gt; There is – it's what 'array types with known bound' are. Are you sure? If I read (3) on [this page](http://en.cppreference.com/w/cpp/memory/unique_ptr/make_unique) correctly, `std::make_unique` is deleted for `T[N]` with `N` being a compile-time number. 
I'm sure there are things you can read, but actually trying and testing atomics (and mutexes I suppose) in small programs is going to be the best way to learn how to actually write programs. 
I fixed the issues you pointed out.
Forward declaring someone else's type creates a problem for evolution of the codebase. If they need to change the name for any reason, like it turns out to make more sense in some other library, they can't provide you a fix. If you've #included their definition, they can transitively include the new header, and provide you an alias, and the migration is relatively transparent. 
&gt; I'm not sure, if the standard even has an official notion of header files It does. The standard even specifies exactly which header files contain which declarations of the standard library.
But this isn't really portable, is it? It seems like you are relying on the assumption that an aligned pointer, after conversion to an integral value, is exactly divisible by the alignment value, but as far as I know this is not guaranteed by the standard.
[xkcd](https://imgs.xkcd.com/comics/compiling.png)
[Original Source](https://xkcd.com/303/) [Mobile](https://m.xkcd.com/303/) **Title:** Compiling **Title-text:** 'Are you stealing those LCDs?' 'Yeah, but I'm doing it while my code compiles\.' [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/303#Explanation) **Stats:** This comic has been referenced 972 times, representing 0.5735% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_dnsyfmq)
1 - Follow your dreams, do what you love. Make what works best for you. You rock for sharing a library that you built! 2 - Oh god, please nobody use tagged pointers except as a gag. Somebody like me is going to wind up having to fix what you did ten years later after it explodes and catches fire on some compiler/OS/hardware that didn't exist when you wrote it. ... Now, if you will excuse me, I'll just go back to having flashbacks to hacking around with tcmalloc environment variables to try and work around the tagged pointers in LuaJIT on 64 bit Linux...
We've seen great success in parallelizing our builds. I'm running on a 32 core machine with 32 GB RAM, and now linking is the bottleneck.
That describes pretty much every piece of code that exists.. given 10 years it *will* break in some way. I'm not arguing for or against anything - just stating that "because it might break in the future" is no way to write a program.
Thanks for sharing! :)
Thanks for sharing! :)
That's a pretty confused view of modules. Modules are orthogonal to how we satisfy the underlying symbols (e.g., by linking the object file or a static/shared library). And once modularized a header-only library would naturally become a module-(interface)-only library.
What the reason? Prison?
I don't find is_detected or void_t enhance expressiveness and simplicity, compared to just use decltype. If you could use c++14, is_valid is the way to go.
Pointer tagging is a huge perf benefit over external allocations for ints/floats in dynamic languages
I already practice it at home with a PC. I want to learn more at outside too, because I have too many free time in buses. (2-3 hours a day)
Tagged pointers are immensely useful for dynamically typed languages like Lua. They are 100% worth the pain of portability issues- the alternative is to heap-allocate numeric values, and/or to bloat the value representation, often to twice its size.
It would be interesting to provide examples of how to use modules with CMake and other build systems.
As far as I know CMake still doesn't support modules (nor any other publicly available build system except `build2` for which you can find the documentation/examples [here](https://build2.org/build2/doc/build2-build-system-manual.xhtml#cxx-modules-build)). Unfortunately, building modules is one area that is only covered in the talk. So if you want to better understand the challenges involved, you will have to wait for the video.
https://github.com/supercollider/supercollider/
No, it's accurate. I have Gaby's agreement on it. Modules say nothing about how much implementation you expose into the module interface. You can expose everything, just like header only libraries, and thus everything can be optimised accordingly as a single translation unit. Or you can expose a pure extern API of a shared library, and maybe LTO inlines stuff, maybe it doesn't. Modules are no longer modules as they were originally hoped to be, they're now just a slightly more fancy precompiled headers implementation. Nothing more than that. There are hopes, true, that a later TS will improve on the current "Modules" TS, but it's out of scope for now.
I asked a [question about module on stack overflow](https://stackoverflow.com/questions/46469921/is-everything-after-an-exporting-namesapce-not-exported) not so long ago. I wonder if you'd recommend such practice. I really like how modules change the style we do C++, but I wonder if I still need to separate stuff in multiple files. In my question, I ask if it's possible to implement exported functions in the same file they are exported. As I understand module right now, the definition of the function, even if it's in the same file, since it's not exported, it should not affect the interface, or cause recompilation of importer in the case I change the implementation. I don't quite like separating stuff in multiple files, and I hope that we could export code, and implement in the same file.
Good article. The stack diagrams help a lot with understanding how exactly coroutines work.
Unfortunately really fixing regex' problems (e.g. consuming a machine stack slot for every backtrack position) is going to require a complete ABI-incompatible engine rewrite and we have higher priority things right now (like finishing '17).
When will the video be published
https://social.msdn.microsoft.com/Forums/en-US/ef99e9f5-2a48-423b-b6c0-fa5617d7c63d/how-do-i-get-c-to-work-on-visual-studio-for-mac?forum=visualstudiogeneral Visual studio for mac is optimized for the .NET and Xamarin toolchain
I'd be interested to know what it took to add module support to build2, or what it takes *in general*. 1. How do you do proper dependency tracking between files? For the preprocessor, it was as simple as asking "What files are included by this one?" and doing timestamp checking. I'm assuming it may differ between module implementation. Does a module file say "These files made me" or something along those lines? What about _before_ the module file has been created? Since TUs declare the module they belong to, and not the other way around, how do you know what files will be needed to make a module? Is it as simple as just scanning all the files? 2. What about parallelism: How do you know what can be built in parallel? Can we build a TU before building the modules it depends on? Do we only need to create the module exports definition first, and compile its constituent TUs first? 3. Recompilation: Must we recompile a downstream TU if non-exported definitions are changed in an upstream module? (I assume yes, in case the compiler wishes to do aggressive inlining). Can we optimize recompilation by ignoring layout changes of private data members of an exported type?
&gt; &gt; Replacing QString with std::string will break all the code. So that's a meaningless question. &gt; &gt; No it isn't. Qt only guarantees API compatibility within major versions, not across different major versions. Doesn't guarantee, but nobody would ever make a change that breaks 99% of the code even across different major versions. A working tool to transform user C++ code I've seen only in Google. Maybe we'll get there. It'll be totally fine with such a tool. I've researched a bit why the Qt4-Qt5 version change has happened. Looks like it was mostly influenced by the transition from Nokia to Digia and the emphasis on QtQuick/QML. I don't see any similar prerequisites for Qt6.
You missed this: &gt; On other platforms, extra memory is allocated to guarantee the alignment of the pointer. 
Yeah, I understand. You guys are doing a great job. 
Tagged pointers are consistently the one feature that I don't consider "might break in the future" so much as "I just started a fire in your house. You can put it out or you can let your house burn down. This will destroy everything you cherish unless to take immediate action to stop it." Given that all of the replies to my comment so far are in disagreement, I guess my opinion is not universally held. :) The only thing I get grumpier about as being clearly a bad idea in the long term is naming things "Next" or "Next Gen" and using that as the permanent name. On the bright side, I did learn a lot about stuff I had always taken for granted that time LuaJIT exploded on us.
&gt; I'm assuming it may differ between module implementation The major difference is that module dependencies must be known before the TU can be compiled. It's a bit more similar to generated headers that the more typical static headers, in that the build system has to know to build the dependency before the compiler can correctly compile the dependents. Tools like build2 parse target files to find module dependencies before building targets (see https://git.build2.org/cgit/build2/tree/build2/cc/parser.cxx for a good chunk of the code responsible for this parsing). One would hope that compilers of the future will provide options for inspecting and exporting dependencies like this so that all the build tools out there don't have to reimplement this functionality. IMO, a far-future compiler will hopefully just obsolete most of this build tool machinery by putting it into the compiler itself. The compiler should be able to start building a TU, observe a module import, then suspend the current job and pick up a job to build that module on demand if it is missing or out of date and not already being built by another job worker. (FWIW, this is the core compilation/module architecture of so very many other languages of all walks, from the likes of Python to compiled managed languages such as C# all the way to "systems" languages like Rust.) &gt; Can we build a TU before building the modules it depends on? Not in general, for the same reasons that you can't build a TU without first updating any generated headers it may depend upon. If a TU requires a declaration from a module, that information must be available. Because modules don't correspond one-to-one with an interface definition on disk, there isn't an automatic way for a compiler to see `import m` and understand that the interface definition for "m" can be found in `src/modules/m/interface.mxx`. That said, sans a particularly nasty module dependency graph, a build can still be largely parallel. Basically the same situation we have today; worse in some respects (more serialization required in the face of dependencies) and better in others (far fewer unnecessary transitive dependencies in the build graph). &gt; Must we recompile a downstream TU if non-exported definitions are changed in an upstream module? Mostly, compiled modules include only their exports. Both Clang and MSVC support this notion. The build system thus only needs to trigger dependent TU compilation when the public parts of a module change. A smart build tool that uses hashes instead of timestamps will really excel here (assuming compilers properly support reproducible builds and don't put garbage like timestamps into module binaries) since a compiled module's contents should only change if its exports change. As with headers, developers should be careful to keep modules small and only export a related group of items (one doesn't want to touch or even _have_ an `included_everywhere.h` header and the same would be true of modules).
Pick one: 1. Fast compilation 2. Multiple levels of inline optimization You can already have those speedups with headers. Just use forward declaration and pimpl to avoid leaking abstractions of third party or OS dependent libraries with HUGE headers. If despite doing the former if you can't get faster compilation times it is just because you code is bad, it has low cohesion and high coupling. As a last resource try compiling everything as a single file. If that doesn't work either you are not using namespaces when you need them or the headers of your code or the headers of the libraries you use are a total cluster-fuck that will not compile unless those headers appear in certain order or worse: some of them are mutually exclusive. 
IIRC it only says what the effect of writing `#include &lt;string&gt;`is (i.e. which classes / functions get defined in the current translation unit) there doesn't have to actually be a true `string`file anywhere in the filesystem. But this is really just hearsay knowledge - I didn't check the standard myself
Section 19.2 is pretty clear on what `#include` does: &gt; A preprocessing directive of the form &gt; ` # include &lt; h-char-sequence &gt; new-line` &gt; causes the replacement of that directive by the entire contents of the source file identified by the specified sequence between the " delimiters. The named source file is searched for in an implementation-defined manner. If this search is not supported, or if the search fails, the directive is reprocessed as if it read And so on. 
though I agree `hana::is_valid` is nice, I don't see why you would like that and dislike `is_detected`, they solve similar problems similarly
Non-normative, but C++14 [headers]/1: &gt; Each element of the C++ standard library is declared or defined (as appropriate) in a _header_. has the footnote: &gt; A header is not necessarily a source file, nor are the sequences delimited by `&lt;` and `&gt;` in header names necessarily valid source file names. I.e. the intent is for this to fall under the as-if rule such that no header need actually exist, it only need behave as though one does when `#include`d.
Duplicate of earlier https://www.reddit.com/r/cpp/comments/73v522/cppcon_2017_matt_godbolt_what_has_my_compiler/
TIL. Is the original purpose of this to allow precompiled headers?
I think it was to give compiler vendors both a theoretical optimization opportunity and an out if they wanted to keep their implementation hidden. But I've never come across an implementation that used this particular loophole, so ¯\\\_(ツ)\_/¯
What does this have to do with the representation of a pointer as an integral value?
Thats the duplicate one. This one was posted 4 minutes earlier. *2017-10-02T19:36:08+00:00* vs *2017-10-02T19:40:35+00:00*
Thanks for looking that up
Tl;dr: this library is fully portable and will be AFAIK. Edit: Well, maybe not, see comment about whether there is a guarantee that the memory address is a multiple of the alignment value on all platforms. It could be a multiple plus some arbitrary offset. I hear your warning, but I don't share your pessimism. I do rely on Boost.Align for this to work and on bit masking. Bit masking is portable by the standard AFAIK. That leaves only Boost.Align as a source of non-portability. I looked into the Boost.Align code, and it looks solid. They even provide a fall-back solution in fully portable code, although it is not efficient. In the future, language support for aligned allocation is going to increase rather than decrease.
True, I cannot cite the standard on this, so maybe it is not portable. It would be weird though to have aligned memory adresses that are not multiples of the alignment value. If this lib was accepted into Boost, we could easily check it on a ton of platforms...
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/73uq9x/c_practice_exercises/dntixp9/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Got it, thank you for the input.
That was my favorite non-keynote talk! The content is well outside my comfort zone, but the presenter did a great job keeping it interesting.
For CppCon they are normally pretty fast. If I remember correctly, last year's were all out in a matter of a couple of weeks.
Visual Studio for Mac is a joke. It's not made for C++, it's just a reskin of MonoDevelop, which is a C# IDE. Xcode is the IDE which will give you best integration with the OS X/iOS environment.
Believe it or not: MFC still is actively being developed. I am not at all saying this is a good thing. In fact, I am quite horrified by this fact :-)
`build2` was designed from the grounds up to handle generated source files, including generated headers, as part of the main build stage (as opposed to an ad hoc pre-build step that most build systems use for that). Even with that architecture supporting modules was a bit of a challenge. Modules are actually "worse" than generated headers since there can be dependencies between them which means you have to compile the interface units in the correct order. Now to answer your questions: 1. In `build2` we "shallow-parse" the translation unit (after preprocessing, if necessary) to extract the module dependency information. If you study the proposed modules grammar, one thing you will notice is that the module declaration as well as the import declaration both can only appear in the global scope (i.e. they are "top-level"). Which means you can fairly easily tokenize the TU and then shallow-parse it without implementing a parser for the complete grammar (fun fact: I mentioned this and how we exploit it to Gaby and he said this design was not an accident ;-)). You might think this parsing is very complex but it is actually pretty simple, the parser implementation is ~200 lines. Anyone is welcome to study and reuse the [`build2`'s implementation (MIT)](https://git.build2.org/cgit/build2/tree/) (see the `build2/cc/parse.?xx` files). 2. In `build2` we don't do anything special here. Module dependencies (extracted per #1) are entered into the dependency graph and then the main build stage sorts everything out (i.e., builds everything as fast as the dependency constraints would allow). 3. Yes, if you change the module interface unit, then every module consumer (as well as all its implementation units) are recompiled. We are still in the very early stages of module implementation so we don't do any of the optimizations you are talking about (most of which would probably also require cooperation from the compiler).
Yes, you can have a non-inline implementation (definition) of an exported function in the interface unit. There are, however, some practical drawbacks to doing it this way and they are discussed in the guidelines mentioned above. Note that there is no such thing as "non-exported implementation" of a function. If you exported a function then it is exported. If you provided its implementation in the interface, then every time you change this implementation, the module's consumers will have to be recompiled (unless the build system in cooperation with the compiler is able to detect and ignore such cases; again see the guidelines for details).
While I (nor most compiler developers I spoke to) necessarily agree with the idea of the compiler becoming the build system, otherwise this is a fairly accurate representation of the modules semantics. So thanks!
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/73ubja/how_can_i_run_cc_code_on_visual_studio_for_mac/dntnl1b/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
The compiler as build system only works if everything is in language X, otherwise you get multistage builds, which are typically awful. Also, unfortunately, the compiler is the best at reporting actual dependency information, and every other tool can only approximate that. Make based systems work with this by making the build depended on make fragments emitted by the compiler. Which is weird. But unless we can get a better dependency dump from the compiler (don't say XML or JSON ;) this is hard. Especially for the non-module interop case. 
A key problem is that the C++ standard mostly pretends that libraries and linking don't exist. Translation units are combined in unspecified ways. And linkers are not, for the most part, produced by the compiler group. 
Amusing, but off-topic.
NP, it was a fun bit of research to go verify all the things I thought were true but wasn't sure about. :) Regarding compilers and build tools, I didn't mean to say that they should replace the _entire_ build system; just the parts that handle the C++ internals like modules and header dependencies (and in that far future, also template instantiations and all the other wasteful stuff that a compiler today has to repeat over and over for each TU).
I just with there was some implicit constructor so I could pass a std::string to anything expecting a QString if I didn't care about the conversion cost. Probably 85+% percent of the time, I wind up writing little wrapper adapter stuff sprinkled all over and the performance implication just doesn't matter because it happens rarely. The other 15%, I would be happy to get surprised in a profiler and have to be more careful.
Why can't you use std::aligned_storage and placement new instead of Boost.Align? Or wrap T in a struct and use the alignas keyword (which is effectively what std::aligned_storage does) Most of the rationale in the Boost.Align page you linked is about some standard libraries not implementing it properly, is that still valid?
I absolutely agree that using tagged pointers *will* cause problems for you in the future. I just disagree that this means you should never use them. The potential upside from them is huge, and sometimes using half as much memory really is worth the future headaches.
Nice article. * I think return value of the `generator&lt;T&gt;` should be `T&amp;` instead of `const T&amp;`: - Generator produces values to be consumed by the caller. - Generator with (unconditionally) const data renders `generator&lt;std::unique_ptr&gt;` and `generator&lt;future&lt;T&gt;&gt;` useless. - Generator with non-modifiable generated values can be spelled as `generator&lt;const T&gt;`. * I think deferring exception to iterator dereference is wrong. If an exception was thrown it should be delivered immediately, not when someone decides to take value from iterator. In current implementation exception thrown after last co_yield is lost. * I find co_await very useful for generators. It can be implemented with semantic of yielding range element by element (like `yield from` in Python). &gt; If a coroutine were to include a co_await expression, then it could conceivably suspend without a value being present... AFAIK implementing await_transform with desired semantic should prevent suspension from awaitables not covered by await_transform. If anyone interested there is modified example here: [gist](https://gist.github.com/Serikov/b28115e3b13a7c0ec45ab76468ddb0bd) Changes: * generator&lt;T&gt; produces T&amp;, generator&lt;const T&gt; produces const T&amp;; * yielding const object in generator&lt;T&gt; is compile time error; * calling begin multiple times on generator doesn't consume elements; * rethrowing exception directly from unhandled_exception, not from `operator*`, avoids missing exception after last yield; * `co_await container;` or `co_await generator;` to yield all values from container/generator. 
https://stackoverflow.com/questions/34737737/relation-between-numeric-representation-of-memory-address-and-alignment
&gt;allows you to use up to **24** bits to store extra state inside the pointer - assuming 64-bit platform like x86_64 - assuming 16-byte alignment, that's 4 bits Where is the other 20 bits coming from? 16-bits from the current 48-bit x86_64 hardware? even so, still 4 bits short of 24. Someone ELI5; I'm just not seeing it. 
Kenny Kerr is a great C++ Developer, and I do mean that with a capital D ^(or C++??). It's a shame that he isn't a regular contributor to MSDN magazine anyone. I save all the issues where he has an article.
I think this might allow for the include guard optimization, so the file doesn't have to be literally included if it's just going to be empty anyway. Most compilers do that nowadays, I think.
As of right now, under the Modules TS, you will have both interface and implementation modules. There is an ongoing argument between committee members Gabriel del Rio and Richard Smith on *what* a module actually constitutes. Word is we'll see an alternative implementation of modules in the form of a paper come January/February. The primary issue I have with both of these is that the current approach of "find dependent interfaces" is moved from its current location in translation phases 3 and 4, to a brand new translation phase with the very descriptive name: "run the preprocessor (because what if the token for the module name or imports is a preprocessor define???) and then parse the output from some script or other tool and hope for the best". This is honestly the biggest problem I have with modules and the argument that we should let an extremely important step involved in creating collections of translation units be given to some magic tool that will act differently based on which build system you're using sounds like a huge mistake that should be fixed before it gets added to the language or we'll be stuck with a subpar system for the next 30 years. 
`is_detected` replaces `is_same` not `enable_if`. `is_same&lt;A,B&gt;` is `is_detected&lt; only_A, B &gt;` where `only_A` is a tdmplate that knly works when passed `A`. But we can make the test a myriad of things. `is_detected` then makes it easy to write the `bool` trait from it. As a general rule, use SFINAE when you need the possibility of "no valid overload", and tag dispatching when invalid arguments leading to hard errors is ok. 
The only complaint I have about current coroutines design is that it would be what, second language feature in C++, that necessary allocates on heap? And we know how everyone loves the first one...
What's the alternative? Why not just use one larger header file available everywhere?
Can modules speed up compilation times at all? For example, if I include &lt;functional&gt; and &lt;algorithm&gt; in a source file then that translation unit quickly becomes a very very large file for the compiler to parse once the pre-processor is done. I’m not too familiar with how/if compilers automatically pre-compile template-heavy headers currently, so any pointers on that would be appreciated. And secondly, I assume a template only module would have all code end up in the module interface unit and the module implementation unit would be empty?
since the nature of template, I don't think modules would significantly decrease compilation time of template. It's just another precomplied header.
Did we read the same blog post? Eva discussed five talks that will be posted soon (one already is) with details to help you decide if you should watch those particular videos. She also shared her experiences and impressions of the "feel" of the conference. I think that (other than consistently mis-capitalizing CppCon) this is exactly what a trip report should do. It explains the writer's experience and expectations and give details about what she found. What are you expecting of a trip report? Sample code? Bug reports? If everyone that attended the conference wrote a trip report of this quality, first time attendees would have their expectations dialed in almost perfectly.
It's almost like this is a slide deck which is meant to be accompanied by a talk...
Perhaps a [range adaptor](https://github.com/ryanhaining/cppitertools) would be useful vector a{1, 2, 3}; vector b{4, 5, 6}; for (auto&amp;&amp; e : chain(a, b) | enumerate) { /* ... */ }
I found this history view pretty interesting: https://youtu.be/86seb-iZCnI?t=7m27s
Wow thanks. So that kills the idea that the lib is portable. :/
I just tried it out on my computer, how far you can push Boost.Align to give you overaligned memory.
With all due respect, I expect more observations and less emotions. From anyone writing technical blogpost. Because observations can give us the meaningful insight. Emotions can not. And I can give you an example of proper report - [albeit from C++ standard meeting](https://hatcat.com/?cat=3). This is actually very interesting read with author own expirience, actual process and conference observations. There [are](https://vittorioromeo.info/index/blog/cppcon2016_trip_report.html) [others] (https://blog.quarkslab.com/back-from-cppcon-2016.html) about CppCon 2016 - and they DO contain the sample code. It immediately gives reader context and a quick summary. If you want to write blogpost about your experience in any area - you absolutely can. But from technical reddit sub posts I expect not just the attendee expirience but the actual details. I can spend 5 minutes on video to judge is it worth watching for me. I also expect submissions to value my time, and do not mislead me with improper title. 
Just the fact, that the compiler doesn't have to reparse those templates over and over again might make a difference. It also has to do the non-dependent lookup only once.
Until we get modules, I don't understand why we don't get a system-wide cache of precompiled headers. A compiler could very easily save the result of the header parsing into the user's home folder and reuse that next time the header is to be parsed. 
Yes, modules can speed up compilation quite a bit, especially for template/inline heavy code, like the standard library. See [my reply](https://www.reddit.com/r/cpp/comments/73mf6t/will_header_files_will_stop_existing_in_further/dnrm2pe/) in another thread for some concrete numbers (as well as disclaimers). Regarding the template implementation code, yes, that along with the inline functions/variables has to be in the module interface unit. If the module only contains template/inline implementation, then there is no reason to have the implementation unit at all.
I agree with most of what you say in this reply but I don't see how it is the same as what you originally said ;-). Also, can you elaborate on the "as they were originally hoped to be" part?
Why slack over the IRC channels on freenode? Both ##C++ and ##C++-General are very active.
You should also use angle brackets for the boost headers, as they are system headers from the perspective of your library.
If somebody has seen this: is this useful talk or just a cool but not practical talk? 
I thoroughly enjoyed the talk. Matt goes over the basics of assembly and some of the features of compiler explorer. If you're already familiar with both of those, you could theoretically skip it but I still think it would be an enjoyable watch. 
That depends on your level of knowledge. If you are someone who still believes that shifting is a cool way to speed up multiplication, by all means go and watch. If you are already aware how smart compilers are, there's less need. 
That said, even the LLVM codebase which is written by the experts of this realm assumes such numeric mapping! So, although not really portable by standard, but still useful in practice.
[boost::irange](https://stackoverflow.com/a/29966402/700825) :)
[removed]
Great talk. Thank you very much.
This post currently has more upvotes than the other so I'll post my comment here: great talk, very interesting.
liked it. Favorite talk that is uploaded so far.
&gt; Also, can you elaborate on the "as they were originally hoped to be" part? Even as recently as Daveed's Modules proposal there had been ABI management improvements in Modules. Not what we actually need, which is to formally specify shared libraries with a formal secondary layer of ABI abstraction such that a Module may consist of zero to many shared libraries for example, and with a standardised event pump which probably ought to be the one from the Networking TS with a few tweaks. But even Daveed's mild proposals were considered too controversial, and they were dropped in order to have some chance of progressing "Modules" which are now not really Modules at all. A better name for them would be "Build acceleration TS". The MSVC implementation of Modules implements Gaby's doctoral work on a universal interface definition language for the C++ ABI, so a sort of freshened up Microsoft IDL for C++ 14. But even that is deeply contentious, and it's not an option for standardisation in the near future. I also think Gaby's IDL needs to standardise an event loop. He disagrees, though he was sympathetic to the rationale why. I think if he thought it were feasible he'd agree, but even the IDL is a tough tough sell right now. There is also the elephant in the room which is that a Modules IDL is actually a political measure to work around the fact that we cannot agree on a standardised ABI for all compilers. Gaby will say that is an incorrect comparison, but I'd counter that Reflection + standardised ABI = most of Modules IDL. We then push it onto library writers, as indeed any practical Modules IDL would require vendors to ship in any case. That turned into a bit longer than I was intending to write, and is probably an unfair assessment of things in places. It's a big big topic, and an unusually large gap between what we all know we need to do and what has actually or will be implemented (virtually nothing in twenty years). That leads to stronger more emotive language than ought to be the case. But it's a pure case of political dysfunction that this stuff wasn't fixed decades ago. I think everyone can agree that the present situation annoys them profoundly!
The spread of misconceptions on Twitter could be stopped or at least slowed down if the service allowed the users to combat it. Reddit does it through the downvote button and although it has its disadvantages, it does a good job at filtering the wrong facts or not well thought-through opinions.
They both were posted within a few minutes of each other. When I go to ‘new’ this one shows up last == oldest, so I think you’re wrong?
Great talk. Awesome tool. I loved how attendees were submitting feature suggestions and Matt was either saying “already exists” or seemed excited at the thought of implementing them. I like the care he is putting into maintaining links. Can’t believe his costs are only 200 $ a month. For all the crap I’ve slung at “the cloud” over the past few years I’m starting to wonder if maybe it was actually under-hyped after all. It sure does seem to be enabling ideas that were unimaginable at the time.
Disclosure: I work with Matt. The C++ guys here use godbolt.org often for our work. It is an awesome way to check your assumptions about how your code is being compiled, and where any hotspots may reside. For example, I was originally a little hesitant to use ranged for loops with newer C++ versions, but was able to prove to myself that the generated assembly is nearly identical to a traditional for loop, if not a little faster (e.g. https://godbolt.org/g/jrK8KL). If you are in a situation where cycles and microseconds matter, then it can be an invaluable tool.
I would say 4 lower bits and 17 upper bits. Because addresses with bit 47 set are reserved for kernel space, you could actually use that bit for user space.
You completely lost me again. Like what does an event loop has to do with modules? &gt; A better name for them would be "Build acceleration TS" What about the isolation from the preprocessor as well as the module linkage for non-exported names? IMO, Modules TS in their current form is something that one can realistically expect to be standardized and have a positive effect on C++ code structure. In other words, they are a step forward. What you are describing is a pipe dream. For example, there is just no way anyone is ever going to standardize shared libraries. 
&gt; What about the isolation from the preprocessor as well as the module linkage for non-exported names? I remain unsure if that's actually going to happen in practice. The problem is that no good alternative to the preprocessor remains in lots of circumstances. So, you're still going to be using the preprocessor a lot, and thus, the power to isolate won't be used in the real world. I'll put this another way: my answer on stackoverflow on how to implement Modules support into existing header files using the preprocessor is very, very popular. &gt; IMO, Modules TS in their current form is something that one can realistically expect to be standardized and have a positive effect on C++ code structure. In other words, they are a step forward. I entirely agree. &gt; What you are describing is a pipe dream. For example, there is just no way anyone is ever going to standardize shared libraries. It came much closer than you might think. I developed a prototype which implemented a portable binary loader for all C++. We used metaprogramming to inject metadata for symbol linkage, then a binary parser which could use that metadata to load any arbitrary binary in whatever format or architecture. So, in other words, I had a *parallel* shared library implementation to the C shared library system which comes with your platform. And it worked great. But a working prototype and a WG21 standard is a huge gap. We put a provisional estimate of US$1m to US1.5m to get to C++ finished standard. That's a big ask for any single group, even in a big rich multinational.
&gt; Disclosure: I work with Matt. congrats :P 
I found it funny how embarrassed he was about "earning" a few bucks with the site (via peatron). Considering the time he probably spend to set this up and the great service this does to the c++ community (and maybe others), I don't see any problem if this allows him a few extra dinners with his family.
&gt; build2 we "shallow-parse" the translation unit (after preprocessing, if necessary) to extract the module dependency information. Does this mean that build2 already supports some kind of automatic object file dependency generation? My project has various ‘output’ executables which all include some subset of headers and those headers sometimes have associated TUs. Right now I parse the compiler-generated dependency files for the executables, check if the named headers have associated `.cpp` files and then construct a list of object files which I link in with the final executable. The advantage is that I don’t manually have to specify the dependencies of the executables nor even specify all the executables, a wildcard match on `bin/*.cpp` is sufficient. The disadvantage is that this is a rather ugly `Makefile` and two shell scripts.
Most of the asm is the same, aside from the few extra MOV, SUB, and SAR lines in the indexed version. Not sure what the deal is there. If you use an iterator, the code is nearly identical, which makes sense: https://godbolt.org/g/PjqeJk Also, it is always fun turning the optimization up to O3 and see your code size quadruple in size! Wrt moving windows, I always just grab them by the tab and drag. Seems to work fine for me with Chrome.
You can use a custom allocator.
clang-tidy is extensible, though, that's the whole point of it.
KDE Frameworks are a set of lightweight libs and not a monolith, you shouldn't even notice it.
&gt; Does this mean that build2 already supports some kind of automatic object file dependency generation? Yes, more precisely, `build2` extracts header/module dependency information from various compiler outputs (e.g, `-M`, `/showIncludes`) and enters it into the dependency graph. While `build2` does not support what you are doing out of the box (and most likely never will since this technique relies on too many assumptions), you could probably implement it (in C++) as a custom rule. Also note that a utility library (a static library/archive containing all the object files) can be used to get most of the functionality you are looking for and this is supported by `build2`.
Exceptions don't require heap allocations (I'm assuming that is what you are referring to). The Itanium ABI requires exceptions to use heap allocations, and most vendors use the Itanium ABI. Windows does not use the Itanium ABI, and their exceptions are stored on the stack.
Cool, thank you! I agree that my current setup relies on many assumptions which just happen to work for this particular project, but knowing that it would probably be implementable as a custom rule makes me hopeful that one day I can leave make behind. Thank you for your work on this project!
Is there a particular reason it is not just WebKit across all platforms Just curious
This is basically a totally arbitrary style choice, angle brackets and quotes have zero difference other than searching in the current directory, and clearly many people use quotes in situations where the current directory search isn't needed.
Nice talk - thanks !
&gt; It's not needless if you work on any codebase large enough to eventually end up with some header with the same name as a header from some library. I.e., it's never needless at all, merely not-yet-needed. It wouldn't be enough for the header to be the same, the entire path as taken from the location of the source would have to be identical. Since standard headers are distinguished by not having .h, and every other library typically makes you specify the library name as the first part of the path (`#include "boost/...`), a collision here is basically impossible in practice, and much less of a concern than say an include guard collision. Let's just admit that &lt;&gt; vs "" is stylistic.
Hm, acording to [n4680](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4680.pdf), linked on cppreference, you can define operator new in promise type and it would be used for allocation of coroutine state. There is some mention of coroutine *frame* allocation using global allocation function, by I guess these "state" and "frame" are supposed to be the same? Anyway it's just that in every example of coroutines I've seen so far, there is always talk about compiler removing allocations as optimization, so I assumed there is no way to provide preallocated storage. Yes, should have checked paper before.
"May allocate" is not much better than "always allocates", and I heard compiler can remove unnecessary allocation from coroutines too. And it would make coroutines' situation even worse. But I didn't know about ABI's role in that - thanks.
VS Profiler dev here (Ignore my user name =]). Fastlink issues should be addressed in the most recent update to VS 15.5. Please file bugs via the VS feedback button if you see problems! Instrumentation is still supported, but is in the process of being refreshed since it has been ignored for far too long. Business priorities are finally pushing us into investing more in Trace profiling.
Thanks. We are actively looking into integrating more of these two concepts as time and business needs dictate.
&gt; a collision here is basically impossible in practice In theory. In practice I had to deal exactly with these include issues because the same people that used "" wrongly also managed to bypass the prefix directory in the include path of some libraries. 
Consider this example: https://godbolt.org/g/26viuZ . It makes the following: template &lt;typename T&gt; generator&lt;T&gt; seq() { for (T i = {};; ++i) co_yield i; } template &lt;typename T&gt; generator&lt;T&gt; take_until(generator&lt;T&gt;&amp; g, T limit) { for (auto&amp;&amp; v: g) if (v &lt; limit) co_yield v; else break; } template &lt;typename T&gt; generator&lt;T&gt; multiply(generator&lt;T&gt;&amp; g, T factor) { for (auto&amp;&amp; v: g) co_yield v * factor; } template &lt;typename T&gt; generator&lt;T&gt; add(generator&lt;T&gt;&amp; g, T adder) { for (auto&amp;&amp; v: g) co_yield v + adder; } int main() { auto s = seq&lt;int&gt;(); auto t = take_until(s, 10); auto m = multiply(t, 2); auto a = add(m, 110); return std::accumulate(a.begin(), a.end(), 0); } Compiles into: main: # @main mov eax, 1190 ret In clang, heap allocation elision for coroutines is on by default. For heap allocation elision to work, the lifetime of the coroutine should be fully enclosed in the callers frame (as frequently the case with generators and sometimes with tasks). With -O2, you will get no heap allocations for well design coroutine types when lifetime of the coroutine allows. (edited: addes spaces for code to look better)
I was promised CUDA support. Is there no CUDA support?
I will add a disclaimer about the portability issues.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/741w2j/cpp_lil_program_need_help_to_improve/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
That's really impressive. But there will be a way to customize allocation behavior, right? Because just adding two `co_yield 0;` into `take_untill` breaks optimization: https://godbolt.org/g/PgAf71
If those issues exist then you can also get a collision with &lt;&gt;. Complaining about "" in this situation is just criticism misplaced; it makes it moderately more likely but the real problem is having incorrect/nested/redundant -I/-isystem paths passed into your compiler lines.
Ha! Interesting. I expect that for coroutines with initial_suspend =&gt; suspend_always adding more code into the body should not be affecting whether allocation is elided or not. Well, it is a young feature. Probably a little bit of tweaking of coroutine optimization passes should make it more robust. Yes. By overloading operator new and delete on the promise_type you should be able to supply your own allocator. Though, if compiler can put the coroutine on the caller's frame, it will ignore the allocator you provided. 
Very interesting talk, well done! Especially the enthusiasm, being open to suggestions and even saying you can't explain things well while you did a very good job at explaining complex things in a simple way.
This sub is for discussions. You should post your question on r/cpp_questions. It's in the sidebar.
 Thank you. Will do.
Fantastic... KDevelop is excellent, but has a lot of room for improvements. Personally I would love to see it also integrate build2 and packaging. 
What is the difference between this and ubuntu 17.04 kdevelop? In other words, what do I have to look forward to when this is mainstream and tested?
Why is this a problem to you? Did someone in your project specialized std:array to have abnormally large size? If you need compile time guarantee of its size, you can use static_assert.
I think you can through custom build system support. ~~However, you won't get extended highlight unless you have a clang compilation database.~~ As /u/scummos said, you even have highlighting with custom build system, as long as you configure the project correctly.
Sure, but it’s nice not needing to rely on implementation defined behavior. I was very surprised when I found this out so I’m curious to know the reason behind it.
Adding a member variable to implementation doesn't change the behavior specified in the standard.
I probably didn’t explain the situation very well. It’s not about having an odd specialization added; that was just an example. The point is that the standard lets an implementation add member variables at the end of `std::array`, which means that the size may not always be what’s expected.
The const in question is the constness of the `std::function` object, not the callable that it is wrapping. It implies nothing about the latter. 
Did i get it right that it is now possible to tell the debugger not to step into certain functions? Like template mess and the like? Where to find more information?
&gt; std::function happily calls functions that are not const. const on a member function merely indicates no (non-mutable) internal state of the object is modified in the function. Calling some other function, const or not, which does not operate on std::function's own internal state, does not violate that. &gt; const normally suggests that an function call is threadsafe Not sure why you think that, but that makes little sense. Don't think I ever heard that before. Other non-const functions in the class may for example be modifying a variable returned by a const function from other threads. So if no thread-safety mechanism is involved (mutex/atomic operation/...) it is not thread-safe. const doesnt have anything to do with that. *edit* quick search leads to e.g. https://groups.google.com/forum/#!topic/comp.lang.c++.moderated/zztZ1FNfaAA
WebKit is available under OSX and GTK+ by default, but under Windows only IE is available by default and you need to install WebKit.
As of C++11, const does imply thread-safe, at least for standard library objects. [Herb Sutter gave a talk on the topic](https://channel9.msdn.com/posts/C-and-Beyond-2012-Herb-Sutter-You-dont-know-blank-and-blank). 
It's indeed a Boost/TR1-era mistake that the LWG has recognized, although we can't do anything about it. You're correct that the problem is multithreading. The STL's policy is that const member functions are simultaneously callable and that it won't do anything to observably damage that guarantee. (This is actually extended to a few non-const member functions that are observers, basically the const-overloaded ones like `operator[]()`.) While user code is under no such constraints (your const member functions, like function call operators of predicates given to STL algorithms, can read/write global variables without synchronization, as long as they meet the other usual requirements), the STL's multithreading policy continues to apply when it invokes user code *if* that user code follows the same policy. The only exception to this rule that I am aware of is `function::operator()()`, because it is a const member function that calls non-const member functions, and yet `std::function` provides value semantics (copying a `std::function` results in a totally independent, non-shared copy). In Boost and the LWG's defense, `function` was designed long before C++11 multithreading and its const guarantees crystallized (the const policy seems obvious now, but it wasn't before). In practice, this doesn't usually cause problems because people don't usually set up the scenario for doom, but the potential for doom is still there.
* enable(bool) acts depending on runtime information in boolean value. * disable() is optional function that is just alias for enable(false) call. There are only 2 mandatory functions: enable(bool) - writing, bool is_enabled() - reading. * I don't know any C++ Standard or Boost documents that require set_enabled() function.
Hi! Steve here. 1) We've always had the ability to modify what functions we won't step into. This is controlled by the natstepfilter and is fully documented here and check it out and let me know if you have any questions: https://msdn.microsoft.com/en-us/library/dn457346.aspx#C++ Just My Code 2) What I actually demoed was the first "step-*through*" in 15.5 which is stepping over all the library machinery to implement std::function and then stop on the other side in the function that was wrapped. This isn't yet part of our general just my code mechanism but is a one-off for std::function because of the sheer volume of requests we got for this. you can peek into &lt;yvals.h&gt; and &lt;functional&gt; in 15.5 bits (coming very soon to preview channel!) to see how we did it, but we intend to generalize it for others to use. I haven't watched the video yet (I'm vain, but not that vain ;-)) but the joke I made in the room which may or may not be in the video about project "feefee" is a reference to 0xfeefee and 0xf00f00 which are used with #line directives to control the debugger behavior. please treat using those yourselves as undocumented behavior that we might change at any time. :-) but we won't break std::function going forward. 
hey everyone. Steve Carroll here. feel free to ask any questions you have on the talk.
I think the issue: If `std::array` did not have the same size as the built-in array, but instead had some overhead, no one would use it. Instead, everyone already uses it, assuming it has the correct size. So why does the standard leave the door open here for an implementation that would probably not be acceptable to many users. I guess they might want to leave the door open to allow future changes for problems we can't anticipate right now, but it's hard to imagine what that might be.
Thanks for the presentation and quick answer again! I knew about 1 vaguely And with 2 Its now clear to me how this is a huge improvement! Going to check out some of the new features soon!
Its interesting that you linked to the documentation for VS 2015. So I've been wondering - whats up with the new documentation for 2017 and later? Is all the material transferred? I for once have trouble using the new documentation website and often read on the 2015 one...
I'd guess the proper link is https://docs.microsoft.com/en-us/visualstudio/debugger/just-my-code#BKMK_C___Just_My_Code
I don't know why they did it, but for reasons like this I start new projects almost always with the empty project template.
~~`std::array` is an aggregate type which means it can be initialised like this: `std::array&lt;int, 4&gt; x = {0, 1, 2, 3};` This is a guarantee. If it had other members, this would not be possible. Being an aggregate means that the class has no constructors. So there is no way that `std::array` can have members other than the contiguous array of elements. Therefore its size IS exactly the size of the equivalent raw array.~~ Edit: nope, looks like I'm just tired and emotional.
I think the compiler is permitted to add padding after the array in the struct. This is why the size of `std::array` can be larger. Correct me if I'm wrong.
Yeah, this one is 4 minutes older. If you hover over the "1 day ago" (or whatever) right below the post title, it gives you the timestamp.
[Because it uses precompiled headers by default](https://stackoverflow.com/questions/41710453/why-does-visual-studio-c-require-including-stdafx-h-even-on-files-that-dont)
i suck. yes, i was going fast. the docs.microsoft.com link is the right link.
Well, it depends on whether you view the state of the function object that gets wrapped by std::function internal or external state. As has been noted, `std:::function` generally behaves like a value type, NOT a reference type (`std::string` vs `string_view`). So it is imho not unreasonable to expect that const member functions of `std::function` do not mutate the wrapped function object (just like const member functions of std::string don't modify the wrapped char array).
I hadn't considered that, and I'm not sure without doing some research. So all I can say with certainty right now is that implementations won't add data members beyond the raw array. 
But aren’t extra members simply zero initialized when not included in the initializer list? E.g: struct S { int a; int b; }; Can be initialized as `S s = {1}`, which results in `a=1` and `b=0`. For that reason, extra members can be added on to the end of `std::array` while remaining standards conforming
That's not the question I'm asking. I'm asking *why* precompiled headers are enabled by default and why MSVC decided it wasn't worth asking the user if they want to use like before.
That is true, actually, so I'm wrong. There are common compiler warnings for missing field initialisers, at least, so if there were other members which weren't being initialised it's likely that the world would notice and complain.
https://codecraft.co/2013/01/02/how-sutters-wrong-about-const-in-cpp-11/
Watched your talk, tried to "cl.exe /Za ..." and got: winnt.h(12366): error C2467: illegal declaration of anonymous 'struct' You guys are even more conformant (given enough flags) than clang (at least when used without `-fno-ms-extensions`).
Why do you want to rely on the internal of containers that you use instead of the contract of their interface?
To try to lock you into their platform.
I completely agree with the critic about herb going a little bit too far in his talk, but it doesn't change the fact that accessing an object only through const member functions should generally not introduce a datarace. Afaik this is exactly the way all of the standard library behaves - with the exception of `std::function::operator()`.
I don't understand why KDevelop isn't using at least some of the functionality from Qt Creator. It's "intellisense" or code completion and a bunch of other features are A LOT faster. Also debugging support is much better. I am not trying to knock KDevelop. I am sure this is a good release. Just voicing my concerns to see if anyone has solutions. 
Or just, oh I don't know, use [std::complex](http://en.cppreference.com/w/cpp/numeric/complex).
Ah, I found it. It was moved under 'Windows Desktop Wizard'. UI is different too. 
I wouldn't call this a tutorial. It's just an implementation without commentary. I'm also not fond of the implementation. You use getter and setters, which is Java style, not C++. There's no reason not to have `real` and `img` as public members; this just adds additional syntax in a case where additional syntax lends nothing but more keystrokes. There is zero chance of confusion or error by writing `example.real = 4`. Also, where you used `const` only in places that weren't necessary, and missed it in most places it should have been.
Some questions that came up when I watched the talk: - When you say that you expect vs15.6 to be fully conformant to c++17, does that mean that the preprocessor will behave just as the one in clang and gcc with regards to variadic macros (I don't know if this is a conformance problem or if the standard is just ambiguous here)? - You mentioned that you cleaned the sdk headers for /permissive-. will they also finally stop defining the min/max macros (and similar) by default? - Do you expect vs15.6 to be able to compile the upstream ranges-v3? - Any Idea when intellisense support for modules will come to msvc?
This is the tip of the iceberg of visual studio nonsense. Part of using it is working around stuff like this. 
They can't stop defining the min/max macros because then gdiplus.h won't work. And this is **not** a [bug](https://developercommunity.visualstudio.com/content/problem/114149/gdiplush-does-not-compile-when-nominmax-is-defined.html?childToView=116917).
I feel like this leaves out a lot of considerations, like the probability distribution of individual entries. If 99% of the time, the first entry is selected, it is most likely that if an if-else sort of structure would be preferable.
Knowing this, how would this be done today?
They could stop defining them **by default** / undefining them at the end of the sdk header files or make sure that gdiplus.h does no longer depend on that macro. The amount of code that starts to break as soon as you (potentially indrectly) include windows.h without `NOMINMAX` is just stunning (it is not just min/max, but those are the most common ones). It includes even their own products like the ms Version of the guideline support library.
STL explained the circumstance perfectly, additionally there are some [standard papers by David Krauss](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0045r1.pdf) which elaborate about this in detail (and also about the missing wrapper for move only types). So hopefully is standard is improved regarding this issue in the future. Additionally I want to mention that there are improved reimplementations out already, which solve this issue through being: * partially const correct: [cxx_function](https://github.com/potswa/cxx_function) - this is the draft oriented wrapper by David Krauss * full const correct: [function2](https://github.com/Naios/function2) - Note: I'm the author of function2 so this is a shameless self promotion. 
Honestly, this question is more suited for Stack Overflow, rather than this subreddit.
The type's size is also a contract, in a way.
Fair enough. That way other people could have found the solution too.
"Why" questions are difficult to answer without having full notes from the meeting(s) where the decision was made, but in this case I think we can at least guess at a couple of reasonable possibilities. I can see a couple of obvious possibilities. The first is that `std::array` will (at least normally) be a `struct`, and the compiler may pad structs to some reasonable boundary, so if (for example) you create a struct of 7 char's, it may easily pad that out to 8 (i.e., a 64-bit boundary). In quite a few cases, compilers will do this by default with all `struct`s and `class`s, so turning it off specifically for `std::array` would often be difficult (and counterproductive). The second is that an implementation might want to add some extra data for debugging. For example, it could add a block of data initialized to some known pattern of values, then when the `array` gets destroyed, it could check that the extra space still has those known values. If they've changed, it can warn you that your code is writing beyond the end of the allocated space.
Pre-compiled headers speed up the build process. Think: boost header files and their static library counterparts.
Like I said with another comment, that's not what I'm asking. Plus I've posted a resolution.
I do agree with everything you say. In fact, there are many ridiculous defines in SDK, like, for example, GetObject, LoadImage, CopyFile or DrawState that you can totally have in your code base sometimes even without knowing they are being replaced.
Following from your small_vector example, if a module returns types named in modules that it imports but not exports, are the functions returning those types unusable without the client importing the module which exports the name of the return type? ex: Given this module interface unit: // -------- hello_say.mpp -------- #include &lt;string&gt; export module hello.say; static std::string greeting = "hi"; export namespace hs { const std::string &amp;get_greeting() { return greeting; } } Will this compile? // -------- greeting_size.cpp -------- import hello.say; size_t greeting_size() { auto &amp;g = hs::get_greeting(); return g.size(); } How 'bout this? // -------- greeting_address.cpp -------- import hello.say; void *greeting_address() { auto &amp;g = hs::get_greeting(); return &amp;g; } 
he's such a cool dude
Yes, C++ is being changed and new features are being incorporated into the language. These don't happen that often. Iirc, major changes come every 6 years (that is what is planned at least: every three years, a new version of the C++ language comes out, but these versions oscilate between a major changes version and minor changes version, and after a "major changes" new version, a "minor changes" comes in, and then another "major changes", and on and so forth; so you get major changes every 6 years if everything goes as planned). What is important is the fact that newer versions of C++ are compartible with its olders versions. This is by design. The general rule the C++ people follow is that if a planned feature breaks something about the earlier versions of C++, then it should be reworked, declined, etc. That is, what you were doing in a particular version of C++ will remain valid in the newer ones. About the book. It is up to date, for sure. There is even a newer version of the book which covers C++14. I can't really say if it's a good book on C++ or not. I personally dislike the materials by Bjarne Stroustrup. I know some people who like it though. If you follow it, do the exercises and so forth, you'll probably be fine.
Correction: There's no such thing as a major/minor release cycle for C++, despite some wishes along those lines early on. The Committee and Working Paper simply aren't set up to deliver features in that sort of pattern. Technical Specifications allow features to be developed and then integrated without too many headaches, but that's a "when the TS is ready" thing.
There might be bug [here](https://1drv.ms/i/s!AhizK85Flux1sxwCoip0Fl9Yrjig). LIB environment variable has both x86 and x64 paths. I guess this happens because nested cmd.exe is executing VsDevCmd.bat.
Paging /u/AndrewPardoe
&gt; we can't do anything about it. Well, compilers could warn about it.
I've no background on programming but I'm eager to learn, and apparently this is the most recommended one for absolute newcomers. I was planning on reading up on C++ Primer after this as well
&gt; "Why" questions are difficult to answer without having full notes from the meeting(s) where the decision was made And that's assuming the topic was brought up at all.
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4159.pdf
But it _can_ change the std::function's internal state, if the std::function is wrapping a user object with a non-const operator(). This means the implementation of operator() of std::function contains a const_cast... generally a sign of a mistake...
Good point.
&gt; In other words, what do I have to look forward to when this is mainstream and tested? Keep in mind that Windows and macOS users have no problems using the latest version. The only thing you get with an old version is more bugs and less features.
The question was, why the is not part of the contract.
You could have a Pentathlon class, which has a vector of Athlete objects, which each hold their scores.
* enable(bool) acts depending on runtime information in boolean value. Enable(false) is semantically confusing. It's like telling someone to sit-down up. * disable() is optional function that is just alias for enable(false) call. No problem with that. It can also be an alias to set_enabled(false). * I don't know any C++ Standard or Boost documents that require set_enabled() function. There is nothing in the standard or Boost that requires enable(false). Naming of functions is very important and enable(false) just does not cut it. 
Do you have any plans for supporting CMake? [Boost is allegedly moving to CMake](https://lists.boost.org/boost-interest/2017/07/0162.php) and it would make it easier to check out your library.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/746nmz/how_to_make_a_class_with_a_lot_of_variables/dnvxaqj/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7439vw/why_does_msvc_forcedefault_stdafx_use/dnvxbws/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
After thinking about it, you're right, it can be part of the contract. Usually it is expressed as a class being a thin wrapper over other type or through backward and forward ABI compatibility guarantees, i.e. by using PIMPL. The only mention of std::array size that I can see is &gt; "This container is an aggregate type with the same semantics as a struct holding a C-style array T[N] as its only non-static data member.". [1] I'm not an expert in Standardese to decide if that means it cannot have other non-static data members, but it looks like that to me. [1] http://en.cppreference.com/w/cpp/container/array
blah blah... VC++ sucks in standard conformance and sky is blue... Did you try the https://blogs.msdn.microsoft.com/vcblog/2017/09/11/two-phase-name-lookup-support-comes-to-msvc/
C++11 was the pivot point. There is a before and an after C++11. C++14 and C++17 are incremental additions onto C++11 so yes, C++11 is still very relevant. 
&gt; Right now, C++17 just got finished and is now in the balloting process Actually, C++17 passed final ballot successfully.
It seems that removing decltype from the return type makes this code work. template&lt;typename INTEGRAL&gt; constexpr inline typename std::enable_if&lt;std::is_integral&lt;INTEGRAL&gt;::value, bool&gt;::type nearly(INTEGRAL value, INTEGRAL to, INTEGRAL margin = 0) noexcept { return value == to; } template&lt;typename DECIMAL&gt; constexpr inline typename std::enable_if&lt;std::is_floating_point&lt;DECIMAL&gt;::value, bool&gt;::type nearly(DECIMAL value, DECIMAL to, DECIMAL margin = 0.00001f) noexcept { return abs(value - to) &lt; margin; } MSVC still has some issues, but the good thing is that it is getting better with each release. 
 template&lt;typename INTEGRAL&gt; constexpr inline std::enable_if_t&lt;std::is_integral_v&lt;INTEGRAL&gt;, INTEGRAL&gt; nearly(INTEGRAL value, INTEGRAL to, INTEGRAL margin = 0) noexcept { return value == to; } template&lt;typename DECIMAL&gt; constexpr inline std::enable_if_t&lt;std::is_floating_point_v&lt;DECIMAL&gt;, DECIMAL&gt; nearly(DECIMAL value, DECIMAL to, DECIMAL margin = 0.000001f) noexcept { return abs(value - to) &lt; margin; } This compiles. Seems to be some weird interaction with decltype.
I also had this problem with OpenCV because they use &lt;&gt; incorrectly and if you have multiple versions of OpenCV it wouldn’t read the local files but instead load the system ones due to the search path order.
think about it : in five years, do you want the code you write today to look like it was written five years ago or eleven years ago ? 
I know I will be beaten hard for saying this, but well - Amicus Reddit, sed magis amica veritas. As of 2017, among most respected thinkers in the industry, there is more and more understanding that traditional mutex-based thread sync suxxxxx big time (or more precisely - that message-passing asynchronous processing models rulezzz forever :-)); Examples of talks about the advantages of async processing on major conferences range from "Thinking outside the Synchronization Quadrant" by Kevlin Henney on ACCU2017, to "Naked Coroutines" by Gor Nishanov and "The Asynchronous C++ Parallel Programming Model” by Hartmut Kaiser on CPPCON2017 (and actually, my own two talks, both on ACCU2017 and CPPCON2017 - though I hardly qualify as a "respected thinker" ;-)). As a result - I don't think we should be speaking about "multithreading" anymore; instead - it is about "using multiple cores", OR about "non-blocking programming". "Multithreading" is just one way to _implement_ these two things, but even when we're using threads underneath to implement multi-coring (or, Wirth forbid, non-blocking), it is still better to avoid using thread sync at our app-level logic. First, thread sync causes cognitive overload (breaking magic 7+-2 number too easily), which in turn drops developer's efficiency. Second, mutex-based programs are inherently untestable (ouch!), which makes them to break more easily. Third - each mutex is a potential for a thread context switch - which can easily cost up to a MILLION CPU cycles (if we account for cache invalidation costs), which means that async programs kick the *** of the mutex-based ones most of the time. Sure, there are exceptions (like "in high-frequency trading, sometimes they have to use app-level sync, though atomics-based and not mutex-based") - but overall, async processing becomes recognized as The Way of writing concurrent programs.
Exactly. Also, C++2z (hopefully 20) will (also hopefully) be the pivot point, too. Modules, coroutines and concepts would be worth learning and adapting to.
Several projects &amp; libraries decided to switch to C++11, but no more recent standard. As you can see [compiler support](http://en.cppreference.com/w/cpp/compiler_support) for C++11 is becoming almost universal, but it isn't the case for more recent norms. Since C++14 adds nice things, but no deal-breaker change, projects will probably stick to C++11 for a while as it's better supported. It will take some tie before a switch to C++17 considering that no compiler/standard library pair implements the full set of features.
Depends hugely on what you are writing. Most C++17 features are additions to C++11 rather than replacements, C++11 on the other hand adds many features meant to replace others.
You forgot there was also C++2003. Also, C++11 was a major language refactoring, it introduced lambdas and move semantics, concept of multi-threading, among other things and there were a lot of features that were added to it that shifted the initial planned release date from 2008 to 2009, 2010 and finally 2011.
Official story: First they took 5 years break since compilers did not implement C++98 for a long time Then they started adding a lot of crap including concepts whose addition and then removal took a lot of time. Additionally a lot of people pushed to add stuff to C++11 since they knew if it does not get into C++11 it will take a loooong time to get it into C++. In response to that and to avoid C++11 debacle(you know the "C++0x was not meant to mean hex" joke?) Herb and company introduced train(wreck) model where ISO ships every 3-4 years everything that is ready.
There were a few very major discoveries that changed the way we saw the language. The biggest one was the Turing-completeness of template meta-programing. As a result of this we now have things like `constexpr` and variadic templates. The other major development was formal move semantics and r-value references. These completely changed the way we think about exchanging information.
There is no plans to support CMake in nearest future. But if Boost support CMake I'll try to add this support also to Boost.UI.
In any case, as far as I can tell, you can pick any of the two and you'll do fine. As a side note, I've read the first frew hundred pages of C++ Primer. Although it's fine in my opinion, I found it sort of shallow in the sense that the exercises and examples are generally too simple-minded, and don't really correspond to the sort of thing you're supposed to know and get used to in a more realistic programming setting. In that aspect, "Programming: Principles and Practice Using C++" and also "The C++ Programming Language" are much better choices. &gt; I've no background on programming C++ Primer isn't exactly a book on how to program. It's about teaching you C++. You can read it and learn from it without going through a "how to program" course, but I don't think it'll be an interesting choice. In this sense, "Programming: Principles and Practice Using C++" is a better route.
I see. I remember seeing this major/minor thing in a presentation I think by Herb Sutter a while ago. Possibly I got confused and misinterpreted what was being said.
If an exception is not eventually caught, there is no guarantee that the stack is unwound and destructors are run.
things DID happened in those 13 years. it's just that it takes time to turn it into a standard.
C++03 wasn't really a new language version though, just bug fixes and clarifications for wording of the 98 standard. (That those fixes were needed is possibly of the explanation for the delay in new language features being standardised.)
The shared pointer constructor takes a pointer to a preexisting object. It is not involved with constructing the object, so it can't take those arguments. It would be ambiguous if it did, because you wouldn't be able to differentiate between constructing a new object that has a ctor that happens to take `T *` and wrapping an existing `T *` in a smart pointer. 
&gt; Why didn't anything happen for 13 years? Boost? TR1?
&gt; this book is published in 2009 Paperback: 1312 pages Publisher: Addison-Wesley Professional; 2 edition (May 25, 2014)
Well, it added value initialization.
We knew templates were a Turing complete language before C++ was standardized. Unruh presented a template program that generated prime numbers in error messages in 1994. This was surprising, but not shocking. Turing complete is actually a low bar. 
A lot of my customers are just now migrating to C++11. I'm certainly not alone in this. I think 14 and 17 will be much more manageable steps after this.
This is a problem with header file design. More specifically, this is a problem with old header files that have accreted functionality over decades like layers of mud turning into concrete. The Windows team owns the Windows header files. We've been successful at influencing the team in the past, and will push on this, but can't make the min/max change ourselves. 
Thanks for the kind words!
* We're working on a conforming preprocessor. I don't know if the clang/gcc variadic macro issue is a conformance issue either, but we're evaluating every behavior and doing the right thing. Sometimes this means gcc/clang are doing something non-standard; we have to choose then whether to continue MSVC-like behavior or copy them. With more details on this issue I can find a more specific answer. * Getting the Windows SDK headers to compile with /permissive- was about getting them to be standards-compatible. Defining macros is standards-compatible. See below discussion or more on this. * We expect the one after 15.6 to be able to compile upstream ranges-v3. /u/CaseyCarter expects to finalize and merge any needed changes then. * We're working on better IDE integration for modules. They're still at TS and still in the experimental stage in our compiler. Our focus right now is in getting the implementation and specification correct. Edit: Fixed Casey's name
I've pinged folks who know more about the IDE than I do. /u/spongo2
It doesn't work. You get errors in combaseapi.h (add /Zc:twoPhase- - really?) or GdiplusStringFormat.h (fixed in 10 SDK, not 8.1).
Glad you liked the doctest talk :)
There is a small book called C++ Today: The Beast Is Back that has a nice overview of the history and state of C++ https://isocpp.org/blog/2015/05/cpp-today-the-beast-is-back The portion titled The 2000s: Java, the Web, and the Beast Nods Off gives some of the reasons.
Very true, it used to be worse years ago. I tried various things, including `std::enable_if&lt;std::is_integral_v&lt;INTEGRAL&gt;, bool&gt;::type` which caused it to complain about *illegal use of namespace std in expression*, confusing the heck out of me... Eventually I just gave up.
Imma ignore the passive aggressive tone in your comment, but it's certainly valuable information. I don't like constraining development to a particular compiler. And I suppose alternatively I could've somehow made Visual Studio use GCC anyway. Worth a shot. So thanks.
First of all, try completion again in 5.2, it's much faster now. QtCreator's completion is fast, but it kind of stops working if you use it with complicated non-Qt code. I think you can switch it to be clang-based, but then they use the same backend as we do. Debugging support indeed feels more solid in QtCreator, that's true. Help welcome :)
Read the announcement! :)
You will still get highlighting, but when you're not using qmake or cmake, you will have to set up your compiler include paths and flags by hand for your project for that to work properly.
I tried that too, only difference being `std::enable_if&lt;...&gt;::type`, without a `typename`, which might have been what it missed... I tend to forget that keyword.
I thought it was in 2003, I'll look into it 
Nice write-up! My company got bought and the new owners wouldn't pay to send me this year. I went the last 3 years and it was great! Can't wait for all the videos to get posted.
T[N] is raw C array, std::array&lt;T, N&gt; is a class. They are not related. 
Thanks for this, it was very interesting
One source: http://www.informit.com/articles/article.aspx?p=30667
For this library to work, I need to dynamically allocate memory for over-aligned data. It says [in the rationale of Boost.Align](http://www.boost.org/doc/libs/1_65_1/doc/html/align/rationale.html), that there is no support in C++11 for this sort of thing. I don't know what's the current status in C++14 and C++17. If you look into the Boost code, you find that they use all sorts of platform-dependent calls to allocate the over-aligned memory.
I rewrote your example using constexpr if that is anyway better way than SFINAE... https://godbolt.org/g/WCdxrH I get a clang warning, but I am not sure if it is a real problem or not. It works on my 15.3 VS with standard version compiler switch. 
As has been pointed out it would cause ambiguities. It could be implemented using a flag argument, or alternatively you can get the following right now (pre C++17 requires an extra function to construct the `shared_maker`): void test(){ std::vector&lt;std::shared_ptr&lt;std::tuple&lt;int*,int,const char*&gt;&gt;&gt; v; int a = 42; v.push_back(shared_maker(&amp;a,a,"Hello world!")); } [Click me](https://godbolt.org/g/scLisU)
`NUMERIC margin = static_cast&lt;NUMERIC&gt;(0.00001f)` has the same effect without a warning.
They are related. `std::array` is the only container required to be an aggregate, just like raw C arrays are.
It's a new feature for C++17; see [P0035](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0035r4.html).
I recently used `&lt;system_error&gt;` in a project. My god was it elegant, I could return errors produced by the OS or my own errors all from the same function, without stomping on the OS error "namespace". Best of all was automatically being able to produce error messages - I know it's just using `FormatMessage` or equivalent but taking that burden off of me, in a portable way, was great. Especially when errors could come from multiple sources. I found [this](https://akrzemi1.wordpress.com/2017/07/12/your-own-error-code/) super helpful in implementing my own `std::error_code`. My only gripe was not being able to short circuit. So I couldn't do: std::error_code a() { /* ... */ } std::error_code b() { /* ... */ } std::error_code c() { return a() || b(); } I instead had to do: std::error_code c() { auto err = a(); if(err) { return err; } return b(); } Granted that's made slightly smaller with `if` init-statements but it's more natural to write `a() || b()`.
Thank you, but what I meant is that IDK if the warning is a real problem? My guess is that compiler gets confused since he would need to know that ternary operator result depends on types.
passed this along to vsdevcmd owner. thanks.
I think you really need to use the RS3 SDK if you are going to throw the conformance switches
Many posts so far seem to answer the question from your title but ignore the content of your post which asks specifically about learning materials. I don’t know that I would want to risk learning all the useless quirks from 11 that have since been fixed or resolved in 14/17. Significant useability improvements were made to the standard library as well. Variable templates come to mind, as does variant, apply, optional, etc... I’m sure the older 11 books were great, but if possible, find material updated for 17 to go with it
I'd like to but it's hard to find good highly rated material for C++17 that's actually introductory like the book, C++ Primer i mentioned. If you have any knowledge of any introductory books that are really good, that'd be amazing
Is it hard to modify/change/learn the new standards provided by C++ 14 or 17? For example if i learn C++11 now (since the highly recommended introductory book covers c++11), will it be hard for me to advance into C++14 or 17 in the future?
The ternary operator result is always a float here, regardless of what type `NUMERIC` is, because both branches must produce the same type. The warning is correct – when `NUMERIC` is integral, initialization with `0.f` is narrowing; but the warning doesn't represent a real problem, since it will truncate and yield the value you want anyway. EDIT: Using a cast makes your acceptance of the default behavior explicit and thus silences the warning.
At this point, the killer feature I'm missing from VS Code is CMake integration. There is soooooooo much information about the project sitting in those sweet sweet CMakeLists.txt files -- one should never see little green squiggly lines about unknown include paths with a CMake project!
&gt; will it be hard for me to advance into C++14 or 17 in the future? no, but you'll learn stuff that you'll have to forget afterwards since they were only stop-gap solutions: http://www.bfilipek.com/2017/07/cpp17-details-simplifications.html
I did consider a similar solution as well, had not considered the use of constexpr like this though. I'm right there with you regarding the warning. Oh, and it's ignoring margin because... I forgot it. Margin was added later to remove the magic number, carried over to the integral version where I forgot to use it. Thanks for reminding me, would've been most certainly a bugger in the future.
yes, but if you read the warning it mentions 0.0001 (actually around that since it is not representible) and that can never be truncated since it can never be picked by ternary operator, but like I said compiler gets confused since he does not know that ternary op depends on types. Is it clear now what I am talking about? It is quite hard to be clear since a lot of stuff is happening here, templates, ternary op common type, compile time knowing of what will be result...
Yes, it's clear what you're talking about — I addressed it directly, i.e. that your code is equivalent to `NUMERIC margin = std::is_floating_point&lt;NUMERIC&gt;::value ? 0.00001f : 0.f`. All floats have the _behavior_ of truncation when cast to an integral type; the actual value is irrelevant.
&gt; We expect the one after 15.6 to be able to compile upstream ranges-v3. /u/CoderCasey expects to finalize and merge any needed changes then. So MSVC will compile ranges-v3 without any workarounds, or these changes are workarounds so MSVC can compile the upstream repo?
&gt; the actual value is irrelevant not really, replace 0.00001 with 1.0 and clang will not complain. He only complains because normally this code is "bad"(he shuts up if you want to assign 2.0 or 16.0 or 0.0 to int), but in this case compiler is not smart enough to see 0.00001 will never be assigned to int.
&gt; had not considered the use of constexpr like this though I think it is quite amazing addition although it sounds stupid/simple when you see it for the first time. I just refactored some of my TMP_if code (with 2 helper functions that take std::true/false_type) into if constexpr and it was much nicer.... Regarding margin: test your code :) Also maybe some compiler will warn about unused function [argument](https://stackoverflow.com/questions/3599160/unused-parameter-warnings-in-c) 
The cited 24 bits are all lower bits. The idea of this library was to give only access to bits which are available on all platforms, by exploiting the platform-independent feature of allocation of over-aligned memory. Unfortunately, the library is nevertheless not platform-independent as was pointed out here.
MSVC guy here. Thank you for the feedback! I see that folks on this thread have helped you with some other options for your code. Glad to see you're unstuck if you do choose to continue to use MSVC. A few points from your post: * [MSVC conformance](https://blogs.msdn.microsoft.com/vcblog/2017/03/07/c-standards-conformance-from-microsoft) has been improving rapidly and consistently since we started our project to [rejuvenate the compiler's parser](https://aka.ms/CompilerRejuvenation). We had a long way to go: it's not easy changing a 35+ year old compiler without breaking the world. But we're clearly committed to conforming with the Standard and I believe our progress shows that. * We expect to declare C++11/14/17 conformance early in 2018. We won't have a conforming preprocessor at that point, and we will have bugs for a long while. But all the features should be solid except for corner cases. We are very serious about conformance. * MSDN documentation pages have moved over to [docs.microsoft.com](https://docs.microsoft.com/en-us/cpp/windows/desktop-applications-visual-cpp) for VS 2017. It's a way better experience, I think, and allows developers to submit changes to the pages. I'm also under the impression that it's faster, but I haven't measured. Lastly, **you don't have to switch away from Visual Studio even if you move away from MSVC**. You can easily use other compilers in the same IDE that you love. For example, [I use GCC running under Windows Subsystem for Linux whenever I think I'm going to actually learn concepts](https://blogs.msdn.microsoft.com/vcblog/2017/02/22/learn-c-concepts-with-visual-studio-and-the-wsl/). We test Clang/LLVM for Windows with our libraries, and they produce binaries that work well with our debugger. Older versions of Clang/LLVM had a VS installer package, but it's easy to set up on your own. You can continue to use VS. Oh, one more thing: let me apologize on behalf of the broader organization that the VS feedback forum gave you an error page. I have sometimes been frustrated by our bug reporting systems, feedback forums, developer community site, etc. The best way to report a problem in VS 2017 is to use the [Report a Problem](https://docs.microsoft.com/en-us/visualstudio/ide/how-to-report-a-problem-with-visual-studio-2017) option in the IDE. And, of course, you can always reach out to us directly. We're very active in the community and we have direct email addresses you can use (such as visualcpp at Microsoft.com) that stick mail in our inboxes rather than in some bug reporting system. 
you deleted your comment, but if you still care about the answer I already wrote here it is: Nah, I am looking this from the perspective that values are statically known, aka clang can warn when there is a "guaranteed loss" int x = 4.0f // good, no loss (4.0f is exactly equal to 4 int) int x = 4.01f // baaaaad , loss (4.01f is not equal to 4 int) And according to my experiments it looks like it is what it tries to do, but here it gets confused since it does not know that 0.0001 can never be assigned to int, only 0. 
Thanks for the call-out in the footnote to Titus' talk! One presentation which has been consistently overlooked (imo) in all of the trip reports I've read is Tony van Eerd's Postmodern C++. It is very much in the "revisiting things we already know" category, and is without exaggeration one of the funniest things I've ever seen
To be honest I did not learn much c++ from books. I learned c++ 98 in uni, I’ve spent the last 10+ years learning modern c++ from blogs/videos/tutorials/stackoverflow etc.. not books. it helps I use it daily for work :) C++ 11 is not old, but the books that teach it and have no mention of how it has changed since are old. Go where the ball is moving, not where it is. 
&gt; Why didn't anything happen for 13 years? Why did c++11 features take so long to develop? They didn't take all that long individually. Move and rvalue semantics came in somewhere around 2005-2007 as I recall, for example. The ISO committee unfortunately had decided to follow a feature-driven release cycle for a while, which in turn got held up on the original Concepts work that had been (and still is) a desired core C++ feature. Essentially they didn't want to burden users and vendors with learning new stuff unless it seemed _really_ worth it. Eventually they realized the error of that model, cut concepts, and got C++11 out the door. Ever since, C++ has followed a time-based release schedule focused on ~3 years. C++20 is going to come out with or without any particular feature; if Concepts or coroutines or reflection or whatever else isn't ready by then, the feature will just target C++23 instead. The newer TS model also helps here by allowing features to ship out-of-band with the C++ release schedule which makes C++ standard development appear even more agile now as compared to the pre-C++11 days. Of particular note, we've now gone through 3 of those release cycles and _still_ don't have any form of Concepts in the draft. Feature-based releases just don't work for an organization that ultimately just votes yea or nay on applying patches to a document and doesn't actually produce those patches by itself. It's nearly impossible for the committee to set a reliable target date for a feature as they have no means of directly controlling the resources directed to that feature's development.
/u/CaseyCarter will make a new branch of Range-v3 to be compiled with a future MSVC compiler (hoping that one is the one after 15.6). He will apply any needed workarounds such that Range-v3 will compile with MSVC. The workarounds should be acceptable to push back into the main Range-v3 repo. Net-net, MSVC will compile the upstream repo. Edit: Fixed Casey's name
Tried it recently? I got it working last week with a cmake plugin. On mobile now so can't look up exactly which, but yeah, it pretty much self configures just like clion. 
@flyingcaribou, I'm with you on that! CMake integration is definitely on our radar. I'd love to get to the point where close to zero setup is required to enable IntelliSense for CMake projects. hopefully soon. :) Rong
Is it the same (or similar enough) as [this one](https://www.youtube.com/watch?v=GPP64opjy_Y) from BoostCon? 
I'm guessing the one after 15.6 will be called 16.0 due to ABI differences? 15.4 isn't released yet, so 15.6 sounds like a long way away :( Really good that you are sharing these plans with us though. PS Enjoyed the talk, though Dan needs to demand better content. Steve got all the good bits :)
There _are_ a couple plugins for that, fwiw. They're... spotty, but they work. :) https://github.com/vector-of-bool/vscode-cmake-tools I'd much rather see VSCode go in favor of a generic project-build-metadata-provider plugin. Writing tons of infrastructure code for CMake integration that can't be reused for support with Meson, build2, UnrealBuild, SCons, etc. would just be a shame. 
Have you put `typename` in front of that?
I'm British! Talking about money is inherently embarrassing :)
Thanks on both counts! I plan on doing a cost breakdown at some point for my Patreon folks so they can see where their money's going. The "cloud" is pretty inexpensive if you're cautious enough!
Thanks for the plug. I've had CMake Server support for _months_ now, and still no word from the C++ extension devs. If they really want the compile flags for a file, [all they have to do is ask](https://github.com/vector-of-bool/vscode-cmake-tools/blob/develop/src/api.ts) :). Also, I'm currently undergoing a large refactor which should fix some longstanding issues and add some nice features.
Seems similar. From what I heard from those who saw it at both conferences, it improved a lot even from BoostCon
Thanks, I might just wait for the CppCon talk to be released since this looks like something that's definitely worth a watch.
@vector-of-bool, I mentioned your extension in my talk :) I was going to reach out to you to chat about how we can work together to get the CMake support integrated with the C++ extension. Does it count if I ask here? :) ...Seriously, let's follow up offline to figure something out! Do you mind sending me an email at ronglu at microsoft.com?
Had the same experience rewriting some socket code. Especially combined with a class like LLVM's ErrorOr or Expected error handling in C++ just became a whole lot nicer. http://llvm.org/doxygen/classllvm_1_1ErrorOr.html https://weliveindetail.github.io/blog/post/2017/09/06/llvm-expected-basics.html
Ambiguities aside, that'd just be the totally wrong semantics. Allocating an object to be stored in a smart pointer is fundamentally a very different thing from storing a pointer into a container. There _should_ be a clear distinction between them. That said, the `make_foo` smart pointer wrappers are bad for a variety of reasons. Having to repeat the type as in your example is one. Their inability to work gracefully with non-public constructors is the really big problem. Though "using more space" is a complete non-issue; terseness is not a primary trait for good language design. If it were, your example would look something like `v++!{$,:s}` (I mean, certainly that _obviously_ says to append to `v` a new element constructed with the default allocator binding the active object and the moved contents of `s` as constructor parameters, all using clear and idiomatic Terse++17 code). Great for code gulf; garbage for large long-lived codebases. :)
I would guess that the one after 15.6 will be called 15.7, but I don't actually know. We don't worry about the VS schedules so much. We're focused on 15.4 and 15.5 right now, and they'll let us know what else is in the pipeline soon enough : ) Feel free to harangue /u/CoderCasey about this. He really doesn't have enough people yelling at him day to day. 
Oh hi! I'll contact you via email, see what we can do.
Do you mean /u/CaseyCarter?
Yes, thank you! Harangue him too! (It's been a long two weeks.)
flattery will get you... everywhere! alas, our versioning story is super complicated in VS. Andrew is right... we're going to do 15.N series VS updates for the current planning horizon. the compiler will continue in its non-ABI breaking 141 / v19 series for the foreseeable future. At some point in the future we will release a SxS toolset for the ABI break series but for now, we are going to continue making our ABI stable versions better.
Hello ---sms---, Steve passed this along to me, per his reply. My suspicion is that the LIB path including the atlmfc\x64 directory was already in the LIB environment variable at the time that vsdevcmd.bat was called. The developer command prompt's behavior will prepend directories to the variables in almost all cases. This can be confirmed in the __VSCMD_PREINIT_LIB environment variable (after vsdevcmd.bat is executed), which captures the value of LIB before any modifications are made by the developer command prompt scripts. A few additional things here: 1. vsdevcmd.bat without arguments will setup the environment in the default configuration, which targets creating x86 architecture binaries. Specifying -arch=x64 (or -arch=amd64) will setup the environment to target creation of x64 binaries, if that was the intent. 2. The full set of options that may be passed to vsdevcmd.bat can be found by running 'vsdevcmd.bat -help'. 
- The "problem" I was relating to is that the MSVC preprocessor doesn't directly expand variadic macro argruments: https://godbolt.org/g/qC3fUD - I know the windows headers are not your direct responsibility, but as they had to touch them anyway I thought there might be a chance they took care about that too. - If you excpet the current ranges-v3 not to work with "the one after15.6" does that mean, you expect that toolchain to be not yet quite fully c++17 conformant or that the current ranges-v3 version makes use of implementation defined behavior / extensions in current clang/gcc compilers. - I completely understand that your focus is on ironing out any problems with the specification and implementation of modules. Just eager to try them out :) 
No haranguing necessary. Overall the new Microsoft openness is refreshing. The blog posts you, STL and Steve, Dan and others like Herb really make a difference to the community. Sure we are all impatient and often don't care as much as you guys about you breaking Windows builds for conformance purposes, but we are engineers/pragmatists and the visibility really helps. [As does the focus on cross-platform support.] Thanks! PS an autofuzzing tool to exercise our code would be a fantastic tool addition :)
Great to hear.
If you don't mind my asking, how experienced were you when you first went, and what do you gain from going vs watching the presentations on YouTube? I'm very interested in going to one of these, but my company never sent anyone to a C++ conference, even though we are a C++ only shop. I'd like to sell the idea of sending someone (preferably me, obviously) and I'm looking for as many arguments as I can. Thanks! 
I did't invoke vsdevcmd.bat manually. I was trying to use Open-&gt;Folder functionality and tasks.vs.json to build my project with bjam. What happened is that linker got seriously confused trying to link x86 libraries with x64 binaries. I couldn't find `__VSCMD_PREINIT_LIB` anywhere. Here is how to reproduce what is on screenshot: * File-&gt;Open-&gt;Folder, choose any folder * Right Click on the folder in Solution Explorer, Configure Tasks * in tasks.vs.json write "taskName": "task-name", "appliesTo": "**", "inheritEnvironments" : ["msvc_x64"], "type": "command", "command": "notepad.exe" * Start this task * Use Sysinternals Process Explorer to watch environment variables. 
This year was my first time. The conversations in the hallways were at least as valuable, if not more valuable, than the presentations themselves. Often I found others doing similar work at different companies and we could talk about how we handled the same problems, or I found others who had amazingly different ways of thinking and experiences than me, and our conversations enriched my knowledge and curiosity massively.
What is the benefit of using Expected instead of throwing a custom exception with the same info?
I just finished C++ Primer, and I dare say it is VERY relevant today. Of course there are some things fixed and updated in C++ 14, but if you follow Primer up with Effective Modern C++ you'll be all set for most C++ 11 and 14 stuff. And nothing can beat a great book when learning all the small quirks of C++. I highly recommend it! Very much worth it. The core of C++ is still just C++ after all.
You weren’t confused.
&gt; That said, the make_foo smart pointer wrappers are bad for a variety of reasons. Having to repeat the type as in your example is one. I disagree with this guidance in the *strongest possible terms*. `make_shared` isn’t even a wrapper, as it does something that users can’t do by themselves. I know you mean well, but saying that they’re “bad”, when most programmers would disagree with you, has the potential to confuse beginners about the current understanding of the community. I recommend using such unconditional language for things that have a strong potential to create incorrect code, like owning raw pointers. Having to mention what type you want to construct isn’t an issue. That information has to be provided somehow, and `vector` isn’t going to have a dedicated member function like ‘emplace_back’ that implicitly does this for you. In most cases, `auto sp = make_shared&lt;T&gt;(args)` avoids all repetition. In this case, the chance of getting the type wrong can be avoided by asking the vector’s element_type. As you said yourself, terseness is not an overriding concern. &gt; Their inability to work gracefully with non-public constructors is the really big problem. That is an issue, but there’s a reasonable workaround (tag types).
Such a shame you titled this post the way you did. Perhaps 'Frustrated with MSVC++ SFINAE - advice?' The thread contains intelligent and useful advice, soured only by the title because haters gonna hate and see this as an affirmation, whereas Microsoft have come on leaps and bounds in the last few years. 
The common belief of the first half of 0s was that managed languages is the future and C++ is done and doomed. But suddenly (surprise!) Microsoft have begun "back to native" crusade, pushing C++ as a frontrunner. This is how C++ has risen from the ashes.
Indeed, I have code that have type depending on complicated template metaprogramming. Other IDE simply doesn't report error when misusing the metaprogramming and don't handle deleted function via sfinae, and simply stop auto completion whenever `auto` is initialized with that metaprogramming result. KDevelop is the best when it come to correctness. However, there is some caveats: - Types that are dependent on a template parameter don't have or have little auto completion. - Ctrl + Click don't link to the used specialization of a template. For example, `std::function` will always be grey, not green, because the parser don't pick specializations. - Autocompletion inside templates, and autocompletion of template function in general could benefit from a lot of improvement - Recent feature like expressions in noexcept(...) and init-lambda capture are not highlighted at all. I recently started posting patches to some parts of the parser, and I hope that I'll become familiarized with codebase enough to help to solve those problems.
As long as it is properly configured in your vcxproj file, you'll be fine. Eg. &lt;ClCompile Include="YourCustomStdafx.cpp"&gt; &lt;PrecompiledHeader Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'"&gt;Create&lt;/PrecompiledHeader&gt; ... &lt;/ClCompile&gt;
* That preprocessor issue looks like our problem, not the Standard's. Yeah, we'll fix it. * Windows has some incentive to clean up their headers (e.g., get rid of min/max, etc.) but it would probably break more code than it helps. * It's an ordering problem. 15.6 should be fully conforming. (Predictions are hard!) After it's conforming, /u/CaseyCarter will actually do some honest work and fix Ranges. * We all are :) But IDE integration is the kind of thing we have to get right. Really right. So it'll be slow in coming. Sorry.
Thank you! The openness is easier for us too. I've passed the autofuzzing recommendation on. Thanks for the suggestion! 
&gt; Of particular note, we've now gone through 3 of those release cycles and _still_ don't have any form of Concepts in the draft. Actually, the latest draft, paper N4687, has concepts in it.
Here's a brute force check: #include &lt;variant&gt; #include &lt;string&gt; #include &lt;array&gt; template&lt;typename T, std::size_t N&gt; void check_(std::index_sequence&lt;N&gt;) { if constexpr(N &gt; 0) { static_assert(sizeof(std::array&lt;T, N&gt;) == sizeof(T) * N); } } template&lt;typename T, std::size_t N, std::size_t... args&gt; void check_(std::index_sequence&lt;N, args...&gt;) { if constexpr(N &gt; 0) { static_assert(sizeof(std::array&lt;T, N&gt;) == sizeof(T) * N); } check_&lt;T&gt;(std::index_sequence&lt;args...&gt;{}); } void check() { using seq = std::make_index_sequence&lt;std::numeric_limits&lt;std::size_t&gt;::max()&gt;; check_&lt;char&gt;(seq{}); check_&lt;int&gt;(seq{}); check_&lt;float&gt;(seq{}); }
They also didn't really understand the ISO process. We've since learned a lot about how to maximize throughput while still following the rules.
Exceptions are a controversial topic. They are expensive in terms of time and space. For these reasons they're not supported by embedded C++ compilers and often avoided elsewhere. My understanding is that this is because you need to store information on how to unwind the stack when an exception occurs. Another, perhaps more academic, argument is that whether a function throws or not isn't immediately apparent and can be easily missed and/or ignored. For example the declaration `int a();` gives you no hint as to whether the function can cause an exception. The `noexcept` keyword is a step towards addressing this problem. Compared to things like `Expected&lt;int&gt; a();` or `Optional&lt;int&gt; a();` where the declaration immediately makes apparent that this function can fail and you must try very hard to not handle the failure case - and if you do ignore it it doesn't immediately crash the program. Furthermore, a `noexcept` can be silently removed and the program can still compile even though the behaviour has changed significantly. Where as changing the return type would cause immediate compile-time errors.
Does this talk ever actually go anywhere? I made it twelve minutes in and he was still talking about how &lt;system_error&gt; is not a logging framework.
&gt; I know you mean well, but saying that they’re “bad”, when most programmers would disagree with you "Bad" as a synonym to "imperfect." Which they are, and which does not mean that they should be avoided in favor of raw pointers or `new`. I could have chosen slightly better verbage there, though, sorry. :) &gt; Having to mention what type you want to construct isn’t an issue. It is when it's superfluous given the context. Your exact argument could be (and has been) used as an argument against the need for `auto`. ;) &gt; and vector isn’t going to have a dedicated member function like ‘emplace_back’ that implicitly does this for you Nor should it. That's not by any means the only possible solution. Library solutions are possible here that don't require special support from containers. One was already provided in this thread. &gt; That is an issue, but there’s a reasonable workaround (tag types). Reasonable in a world without generic code. Sometimes needing special tags just to work around class visibility doesn't lend itself well to composition. Tag types are also tricky in this case as the obvious and naive solution is trivially defeated using initialization via `{}`. It also requires bloating up class definitions with tag types and extra constructors just to enable usage with the `make_` functions. It works, but it's not ideal.
In the real world, you will still find a ton of pre-C++11 code chugging along. For starting from scratch today, it's probably great to try and start with the latest stuff and not invest a lot of time learning older ways of doing things unless you have to. That said, 11 isn't that shockingly different from 17. There are some new features, but nothing as big as was introduced in 11.
The book doesn't cover all of the latest features in C++. In that sense it's 'outdated', but it's still probably the best book for learning programming and C++ at once. It teaches programming well, and it teaches the 'modern' style of C++ well. Lately C++ has been moving too quickly for books to stay up-to-date very long. The only way to be on the cutting edge is to read blogs and articles about new developments in C++, or follow the standardization committee and compilers as they add things. If you want to do that, you must first possess the proper grounding, which _Programming: Principles and Practice using C++_ will provide pretty well. Of course you don't really need to be on the cutting edge. If you were to just learn C++14 and stick with that for the next five years you'd probably still be doing more 'modern' C++ than most.
add globbing. This is such a deal breaker from the beginning and the advised work around is laughable for cross platform targeting builds. Otherwise nice build system. Not sure who's going to win the build system wars but it looks like bazel based on recent samplings. FOSS projects will slowly take up on meson and academic or enterprise projects use bazel seems to the of jist of what I'm observing personally.
Or it contains a pointer to the underlying function object, which wouldn't require a const_cast. eg: class foo { int* b = new int void bar() const { // type of b here is int * const, or a const pointer to a non-const int *b = 2; } }; Edit: Here's another example with functors. struct bar { int x; void operator()() { x = 2; } }; struct foo { bar* b = new bar; void operator()() const { (*b)(); } };
Is there any way to get an `Expected&lt;T&gt;` to give a compilation error if there are code paths where it's not handled?
Thank you very much for the thorough explanation, sorry to stray from the main topic's question but do you happen to recommend and compiler for windows or is visual studio good enough?
This is a meme talk! The same talk was given at CppNow 2017. Honestly, if you want to know more about &lt;system_error&gt;, read this three part series https://akrzemi1.wordpress.com/2017/07/12/your-own-error-code/
Try VS 2017 Community edition. It's better in almost every way!
Ah sorry I do mean VS 2017, so is it a good compiler for a complete beginner? 
On windows, it's probably the easiest way, and way good enough. Unless you're doing template metaprogramming, you're not gonna encounter serious issues. Sure, currently VS 2017 is not supporting C++17 completely, but a large part of it is supported and they are doing really fast to support the rest of it.
thanks for the response, i'm learning from books that only cover C++11 and some C++14, so I think i'll be fine
C++17 is really worthwhile, even for basic stuff. I strongly suggest you to look into it! If init statements and structured binding are a great addition, and (relatively) easy to understand for beginners.
Visual Studio is good.
I really like a lot of aspects of the look and feel of this tool. However, if you're on a windows machine, what are the reasons you'd use this over visual studio 2017 for c++?
I agree... Yes and no. More relearning for the advanced features like move semantics and `decltype(auto)` just to name a few, but others like `auto` and lambda functions remain the same. It will be a great place to start. I still need to get my hands on a fully compliant C++17 compiler. Looking forward to the parallelization portion of the STL! (`std::transform_reduce(...)`)
Yeah, don't worry. You can learn most notable improvements in C++17 in a day, after you are comfortable enough with C++11/14. 
That is dramatically understating the amount of new machinery being added in C++17.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/74d5yh/can_anyone_have_solution_for_this/dnxdcpa/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
No. This talk is mostly useless if you don't already know how &lt;system_error&gt; works. And it is mostly a waste of time if you do.
Highest pay is in financial industry -- up to $300k/year. Legalized frontrunning -- high-frequency trading, etc. Can be higher (options/etc start playing part), especially if you move into director-lvl role in large institution. Prepare to love and promote diversity, company values and other bs. Also, forget about expressing opinion on the side, your behavior outside of work will still be subject to company policies. Plus environment is typically stressful with a lot of politics. Any profession where you "work for a man" is not good when it comes to making money. If you need money -- you need to be business owner. "Profession" == "making a living" != "making money". 
&gt; what are the reasons you'd use this over visual studio 2017 for c++? Multi-language projects are nice with VS Code. I can jump between C++, Python, and CUDA with a variety of linters, syntax highlighting, etc running for each language. The plugin ecosystem for VS Code is pretty awesome, there are custom tools available for tons of languages.
Would you still recommend it if I do the vast majority of my code in C++ in the VS2017 environment? I fairly rarely touch javascript and when I do, it's at a total beginner level. I currently just use sublime for that.
to add to that when things started moving into cloud,the managements everywhere could see ,in nice colorful charts,how almost double instances (read expense,okay here am making up numbers,but you get the gist) were required to run the same logic.
qtcreator with mingw is the way to go, if you want to learn the language in a standard conforming way.
Is there a large difference between the two you stated and microsoft visual studio?
Not for pure C++ development, no. Refactoring alone would keep me in VS2017.
Like everyone here said, [Visual Studio Community Edition](https://www.visualstudio.com/vs/community/) with [Visual C++](https://en.wikipedia.org/wiki/Microsoft_Visual_C%2B%2B) is great for Windows. If you're ever interested in any cross-platform development then take a look at [GCC](https://gcc.gnu.org/) and [Clang/LLVM](http://clang.org/), both of which Visual Studio can use as a compiler with plug-ins if I'm not mistaken.
mingw(i.e. gcc) is standard conforming, while msvc isn't yet (and there are still lots of work/bug for them to do).
Neither of them conforms perfectly. For a rank beginner, perfect conformance is much less important than ease of installation, ease of use, good debugging, and plenty of documentation (to name just a few obvious points). In all these respects, vc++ wins almost without trying.
VS has plugins for Python and CUDA, there's of course code analysis and syntax highlighting.
This presumes that "MSVC C++" is different from standard C++ sufficiently **for a beginner**, which is utter bullshit. It's bullshit even fir a professional. A beginner might stumble across C++/CX or /CLI, which are different languages though. Source: building my stuff with 3 different compilers.
&gt; Edit: Fixed Casey's name I never even knew there was anything wrong with my name. Thanks for fixing it!
nothing is perfect. But gcc does a much better job. msvc works hard on this lately, but you'll need turn on /permissive- which is also quite buggy.
I use Code for editing,, VS for debugging. Despite Code being an Electron app, it's actually faster and snappier. My VS is pretty vanilla but using the quickopen UI to open a file... it can take *seconds* after I hit enter until the file finally shows, even if I've had it in a tab recently.
Does the c++ standard say std::array should be the same size as T[N]? From what I know so far, it does not. So it does not matter if it is an aggregate, that does not make it related to raw C arrays. 
I was using notepad++ for editing and MinGw for compiling.
No, that's wrong, the stack is unwound and destructors are run in every case. 
Well, I tried it before replying without success. You are half-right, in **clang-build.ps1** there are two functions related to pch : **Project-HasPch** and **Get-ProjectStdafxDir**. **Project-HasPch** seems fine as it's looking for the real pch .cpp. But **Get-ProjectStdafxDir** is not : it's looking in project headers for a constant file name : **$kNameStdAfxH** (wich is **stdafx.h**), the issue is here ! In my case, **Process-Project** fails because, no **stdafx.h** is found but pch .cpp is found... Once I get a better grasp of the powershell syntax, I may try to fix it.
You're right, although most implementations of std::function have a "small function optimization" where it is contained... But the type-erasure also throws a spanner in. Still, as argued in other comments _logically_ the std::function contains the function object - it's not a reference type in its external interface, even if it is implemented as such internally. It's logically closer to an std::optional than std::ref.
Absolutely. Where I work we use C++11 as a standard, because some of our platforms might be expected to have older compilers. Eventually we'll move to later standards, but C++11 is what we can reasonably expect good support for right now on some devices like routers and NAS boxes.
Huh ? Last time I checked, VC++ didn't have cppreference doc accessible with F1 on std stuff, inline error reporting backed by clang with fix-it support, clang --analyze integration, etc. Also, "ease of installation"... You must be joking. Sure, vs2017 is scores better on that point than the previous version but it's still far from download -&gt; next -&gt; next -&gt; run
I recommend the Msys2 environment, it provides GCC 7.2, Clang 5 and tons of libraries like Boost and Qt. 
There are plenty of smaller financial organisations (headcount &lt; 30 say) who pay excellent salaries and have very exciting high performance problems to solve. But they tend only to employ people with excellent C++ track records. &gt; Prepare to love and promote diversity, company values and other bs. Also, forget about expressing opinion on the side, your behavior outside of work will still be subject to company policies. Plus environment is typically stressful with a lot of politics. This can happen in any large organisation, whether its purpose is to move money around in circles or not.
OK, thank you for pointing that out. A fix is coming very soon (later today). 
&gt; C++ is much harder to learn than many of these other technologies, and makes me wonder if C++ is the right choice from a money standpoint compared to the effort you need to put in. It's very hard to articulate an effort:money ratio. From my end, I disagree: I think C++ is relatively straightforward, the computer does what you tell it (more or less); whereas I find JavaScript to be an adder's nest of inconsistent surprises. The good thing for me is that most people seem to disagree. There are high paid positions for experts in every language. A lot of people have mature C++ codebases now that require a lot of attention. That by itself will keep the C++ programmer in work for a long time. &gt; I currently do not use C++ in my job, but I'm looking for C++ opportunities due to liking the nature of the work that C++ opportunities usually entail, and enjoying C++ (especially Modern C++). I think that's a great thing to say in any interview. :)
&gt; maximize throughput http://lmgtfy.com/?q=define+maximize 
The main Problem with this is that there are very few projects, where you can use constexpr if
What's a meme talk?
I believe you can hop onto WSL and try out gcc as well. I would definitely look into calling a compiler yourself at least once or twice.
&gt; The main Problem with this is that there are very few projects, where you can use constexpr if Well there is a lot or projects where rule is we can not use it until VS implements it, now VS implements it. :)
It's a very interesting book, I would advise beginners or anyone familiar with C++ to read it.
&gt; But they tend only to employ people with excellent C++ track records. Isn't it obvious? Why would anyone pay you top money if you are not top of the crop? &gt; This can happen in any large organisation Did I say it happens only in financial corporations?
In hindsight, I do regret my poor choice of words up there.
&gt; it's still far from download -&gt; next -&gt; next -&gt; run Are you sure? In your sequence you still have to add a point where you check the checkboxes on what you want, but that's all there is to it IIRC? Also in a nicely configured PS it's merely `Find-Package VisualStudio2017Community, VisualStudio2017-workload-manageddesktop | Install-Package`.
&gt; Also in a nicely configured PS it's merely The fact that this command is possible is nice but you'll pry my `pacman -S qtcreator g++` from my cold dead hands :p 
You the real MVP.
[except.handle/9](http://eel.is/c++draft/except.handle#9): &gt; If no matching handler is found, the function std​::​terminate() is called; whether or not the stack is unwound before this call to std​::​terminate() is implementation-defined ([except.terminate]). See also [except.terminate/2](http://eel.is/c++draft/except.terminate#2): &gt; In such cases, std​::​terminate() is called. In the situation where no matching handler is found, it is implementation-defined whether or not the stack is unwound before std​::​terminate() is called. In the situation where the search for a handler encounters the outermost block of a function with a non-throwing exception specification, it is implementation-defined whether the stack is unwound, unwound partially, or not unwound at all before std​::​terminate() is called. In all other situations, the stack shall not be unwound before std​::​terminate() is called. An implementation is not permitted to finish stack unwinding prematurely based on a determination that the unwind process will eventually cause a call to std​::​terminate().
&gt; others like auto and lambda functions remain the same. That's debatable: for instance in C++11 you can't do `[] (auto) { ... }` but you can in C++14. And in C++14 you can't do `[*this] { }` but you can in C++17. Even simple stuff like `auto foo{23};` has a different meaning in c++11 and 17. in c++11, foo is a `std::initializer_list&lt;int&gt;` and in c++17 it's an `int`.
Tutorials and such for C++ on MSVC will often start using Windows-specific stuff (even non-library stuff like Windows naming conventions) very early on without proper explanation. They'll also often rely on the VS build system and UI without any explanation of how the compile/link process works. Of course, a "standard C++" tutorial can be followed perfectly well on VS/MSVC (except maybe for the build/run process), but such tutorials usually assume GCC on Linux or MinGW, making that a better choice for learning the core language.
Well here are some companies I guess pay a lot of money for C++ devs: DRW(Godbolt left Google for DRW), Google, FB.... "Problem" with C++ is that IMAO it is hard and it attracts a lot of "autist" types unlike Java or Javascript so you have a lot of competition. I know I am generalizing, but Haskell and C(kernel stuff) are the only other areas of programming languages where I feel people attracted to it because it is this highly powerful monster that is hard to tame. So in a ironic twist C++ beside being harder also means more competition. Oh and there is no C++ on web in a normal sense... Yes all the big companies build their services using C++, but almost nobody actually uses it for frontend.
True. 😃 I forgot about the change in `auto`'s type deduction regarding braced initialization. Good catch! ^(have an upvote, too!)
Hey Steve, great talk, on the slides, there was a listing of Filesystem as an experimental thing, though it had an asterisk? What was that about? Is MSVC not shipping filesystem outside of std::experimental for some time? (psst, any update on this: https://developercommunity.visualstudio.com/content/problem/109571/intellisense-cant-resolve-some-base-type-members.html I'm willing to do code changes for a workaround, I just don't want my developers sad when they type things and get red squiggles.)
I wonder why there isn't `emplace` function for `unique_ptr` as `u = make_unique&lt;T&gt; (...)` is using twice as much memory (you allocate it first and delete previous memory only after). It could be quite bad for large allocations (especially on embedded\GPU).
I use VSCode daily, and there is only one thing that annoys me - the fact that every update brings a version that will not run on my system (and I have to use a special glibc compiled just for it). The latest version of the C++ add-in crashes often, but it still is acceptable. I'm looking forward to see more improvements happening on the C++ front, I hope I won't see for long red squiggly lines for valid code (like on: namespace x::y::z {....}), but it's fast and effective. I just hope the VSCode team will notify me when a plugin subprocess dies, and they can give me a clean way to reload the plugin when that happens. Because yea, with C++17 I see crashes quite often. (Later edit: I'm talking about [this issue](https://github.com/Microsoft/vscode-cpptools/issues/19) - I know CentOS7 and RHEL7 are old, but people still use these things)
I think that it is straightforward when you look at features in isolation, but my biggest fear about C++ is that you need to be aware of a lot of features, and their edge cases in order to not be blindsided. That is, you think you're doing one thing, but some edge case behavior is masking that and doing another thing. And they don't always end in a compiler warning/error. That right there is probably my biggest C++ fear. Perhaps it is because I have not used it in a professional environment, where there are recommended patterns and styles and subsets of features that are allowed/forbidden etc. In that case it may be a bit more manageable. I guess it makes sense when you're comparing it to Javascript, where the Javascript programmer also thinks one thing is going on while it's actually doing another. Thanks for your comment.
Thanks, I do not doubt that C++ has high paying jobs, I am more curious about the distribution rather than the outliers. What I was trying to understand from this thread was I have friends who are working with Java/C#, who do not code outside of work and are making about $140k-$150k in medium cost of living areas in the US. Yes, I understand, comparing is bad, do what you love, they may be putting way more work than you think they are, but I'm trying to speak from a relative standpoint. That is, they don't have to be the greatest Java/C# programmers to be able to make that kind of money, nor do they have to chase after the companies listed elsewhere in this thread, who they have actually failed to get into themselves! They are good software developers and they make a lot of money. If you cut out all the outliers in C++ skills, are you still left with plenty of good paying C++ roles?
On many platforms exceptions are zero-cost, you only paying if exception is thrown. Stack unwinding happens even if exception is not thrown. 
Stack overflow recently did another programming career vs salary survey. You could enter where you live, years experience, education, language/framework etc... I’m on mobile, I’ll see if I can dig up a link.... Edit: that was easy, I found it through ARS: https://arstechnica.com/gadgets/2017/09/devops-and-data-science-are-the-big-software-dev-money-makers/ Direct link: https://stackoverflow.com/jobs/salary
You can use `__attribute__ ((warn_unused_result))`
That's the thing though, do I have to get into Google and the like to command a high pay with C++? I suppose what I'm trying to understand is whether C++ jobs are undervalued compared to other technologies for the effort you put in, ignoring the outliers. 
It will be kinda nice to have time stamps on the points you are making on each video.
A memory leak exists because a new does not have a corresponding delete. Trying to find these memory leaks by yourself is like trying to find a needle in a haystack. Spend the time installing Valgrind.
are you even on linux?
Thanks heaps! I just opened openFrameworks and the art made there is so beautiful &lt;3 I'm installing openFrameworks rn, going to give it a shot!
&gt; A memory leak exists because a new does not have a corresponding delete. or maybe it has a delete but the delete is only called at the end. Imagine the following software: int main() { std::vector&lt;std::string&gt; vec; do { auto str = receive_network_message(); vec.push_back(str); } while(str != "quit"); } there's no missing delete, but it can grow to arbitrary high memory usage.
yeah, however I have 0 experience in linux.
IDK, to be fair I think that you should learn language you like unless it is Haskell, Clojure, C, D... (aka cool languages that have almost no job market). In other words for me 5-10k more per year is enough to convince me to code in shit language I hate... But without false modesty and false ego I must say that you should also know that certain languages fit better different people... I like C++, for it's many cool features but also because it fits my "worldview" and my brain has trouble with reading/writing dynamic languages. Not to mention that a lot of OOP that is common knowledge among medium skilled Java/C# people is magic to me. :) Obviously because in C++ there is less OOP than in other languages, but part of learning C++ is not just deciding you want to learn it, part of it is are you and C++ a good fit. :) 
Also, C++17 offers an official attribute for this: `[[nodiscard]]` (supported by gcc 7+, clang 3.9+ and VS 2017.3+)
Well, I was talking about notable (for a beginner user) changes. Of course, from the library side it'll be much bigger. But it's mostly just "removed a deprecated feature" or "improve/fix/generalize some already existing thing" (like constexpr lambdas). Apart from filesystem TS which is big and probably can't be learned in a day (but it's not really fully supported by any of the compilers and it won't really change how you write your code), and maybe a parallelism TS, there was no really that big and complex features that can't be understood quickly. `constexpr if`, `structured bindings`, `string_view`, `fold expressions` - in most cases you can just read an article on cppreference about them and just start using them. 
&gt; On many platforms exceptions are zero-cost, you only paying if exception is thrown. Yes compilers use "zero-cost" exceptions now-a-days but my understanding is that this just means the unwind table is produced at compile time and loaded when an exception is thrown. Hence why they are zero-cost until you the exception occurs. &gt; Stack unwinding happens even if exception is not thrown. The stack knows where to move to when a function returns, it doesn't know where to move to when a function throws. Hence the need for the table.
Work out how to install valgrind would be the best suggestion assuming your code doesn't apply RAII principles.
&gt; is using twice as much memory (you allocate it first and delete previous memory only after) What? Where do you see a second allocation?
I'm usually amazed by the production value of the recordings of CppCon, but here the audio guy totally dropped the ball.
`pacman` is not an option on Windows, so your point doesn't stand. Which makes VS the easiest one to install on Windows.
Or slide numbers, if the notes are from attending CppCon.
&gt; pacman is not an option on Windows, uh ? that's what comes with MSYS2 actually
Wow? Didn't know that. Is it possible to integrate it with the linux subsystem (i.e. making clang/gcc work with it)?
Don't know, sorry. I think that Junest (arch linux on chroot) might be more useful in this case: https://github.com/fsquillace/junest
If you are in 64-bit Linux, I recommend Clang + LeakSanitizer.
how can you not install valgrind? It's should be buildable from source at the very least cd ~/ mkdir -p temp cd temp wget ftp://sourceware.org/pub/valgrind/valgrind-3.13.0.tar.bz2 tar xf valgrind-3.13.0.tar.bz2 cd valgrind-3.13.0 ./configure make sudo make install if you're on debian/ubuntu just run sudo apt-get install update &amp;&amp; sudo apt-get install valgrind
Yes, if you are using a name in a TU, then it needs to be imported (or included). So the first example (`greeting_size.cpp`) is invalid since you are using std::string::size() without first importing it. The second example is fuzzier: current clang accepts it but I can also imagine an implementation that will interpret it differently. While I have some thoughts on this in the modules design guidelines, I think this will prove to be a tricky area in using module.
I think what they mean is that they wish to replace one object owned by `unique_ptr` with another, but such that the original is destroyed first. That way only one such object is allocated during the process. Emplace isn't the right name for such an operation, as emplace operations simply create a new object, this is something more complex. /u/tgolyi wants, I think, is this: `auto original = std::make_unique&lt;BigObject&gt;();` `// Replace, without having two BigObjects existing at once` `original.reset();` `original = std::make_unique&lt;BigObject&gt;();`
Not everyone's notes are so organized.. example, mine: "The best part of C++11 threading library is std::unique_ptr" -- Ansel from Copperspice The HFT guy disables HT and turns off every core except one in their CPUs.. but approves of C++ exceptions. Also it made a real difference to them when placement-new on a null pointer became UB. Titus still doesn't believe in not_null Nico Josuttis hates std::string_view so much he dropped an F-bomb, wonder if he gets bleeped I got to try "gflags /i yourfile.exe +sls" from James McNellis's talk I had no idea about http://www.quick-bench.com (was actually thinking of building something like that myself) tcmalloc and jemalloc are worse than the default glibc malloc (at least for John Lakos) 8-bit floating-point numbers are a real thing in ML (see https://github.com/google/gemmlowp ) until gcc 8, noexcepting a thread function is the workaround for broken exception backtraces Facebook compiles with frame pointers. And Linux eBPF can be abused into general-purpose deadlock detector Chandler says IACA is magic, I should try to make sense of its output C++/WinRT programs can be compiled with clang (at least the one in live demo was)
I haven't been too impressed with AVX-512 on Skylake X so far - benchmarks appear to show that it offers no benefit over AVX2. Call me cynical, but I'm guessing it's still just 256 bits wide under the hood (shades of early SSE implementations ?) ? 
Ah, I see. Thanks! Though same thing would happen even with regular pointers. There is no avoiding it.
Check out the comments on his [CppNow presentation](https://www.youtube.com/watch?v=w7ZVbw2X-tE). Here is the top comment: &gt; Is this a presentation or a 4chan thread? There is a meme on EVERY slide.
or replace `new`/`delete` with `std::unique_ptr`.
I wouldn't be surprised if it was done just for compatibility with the phi's AVX 512.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/74ffpr/tracking_down_memory_leak_in_cpp/dny2t5x/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/74cofa/best_c_compiler_on_windows_for_a_newcomer/dny2uky/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
And those are very dated memes.
I'd say it is more about the smallest common denominator and for many non-toy projects that is not the latest and greatest. Personally I'm in the particularly unpleasant situation that I sometimes have to code for the intersection of msvc 2017 and gcc4.9. Ancient Gcc prevents me from using new language features and msvc breaks for all kinds of template code that I have to write instead.
Right now here in Ireland there is a local recession in C++ roles. The contract market has been dead since Spring thanks to Brexit uncertainty, and since Trump came in US companies have noticeably kept remote working contracts within the continental US and not been keen to hire Europeans. So I've been unemployed almost this entire year, great for WG21 and Boost work, not good for income because Ireland doesn't pay unemployment welfare to the entrepreneurial class like me. Here in Ireland C++ is a very minority skillset anyway, here is mostly a web dev environment, followed by Java and iOS and then .NET. Most C++ roles tend to be in automation, tending to very legacy embedded systems. Think 1980's C++. Equally, just across the pond some C++ contract devs in HFT earn £750/day in London. C++ in the 1990s used to be a good general purpose skillset. In the 2010s, it's a very niche, specialist skillset, pay and conditions generally declining relative to other skillsets like data science. But that's always the case with any maturing tech stack.
Unfortunately most of the things that rock about AVX-512 aren't visible until compilers start using those things. Lots of instructions to make autovectorizng stuff easier.
My opinion on that is that if you are stuck on gcc 4.9 that is the problem of your project/organization, not a compiler problem. :) 
As mentioned, its great for viewing code and projects (via opening a folder) since its considerably more lightweight than VS. It also does better with mixed-language folders and projects.
I can't speak highly enough of this talk, one of my favorites I saw all week!
GCC (version 7.something) and Intel (version 17.0.4 but better with version 18) correctly generate AVX-512 instructions.
"I can generate instructions" and "I have taught my autovectorizer to make best use of them" are very different things.
http://www.sisoftware.eu/2017/09/12/avx512-performance-improvement-for-skl-x-in-sandra-sp2/ The Vector 32 bit float results: AVX512: 1800 AVX256: 1000 Nothing wrong with AVX512 on Skylake X There are still some issues with compilers, particular MSVC which only uses 16 of the 32 registers(supposed to be fixed next update I think). 
Oh, fair enough. There were definitely improvements in the autovectoriser in Intel 18 vs 17.0.4.
Well, in this case -- that whole sector (HFT trading) is an outlier, because 300k is kinda normal expert-lvl pay with (actual) outliers shooting towards 1mil. Most of them are in high-cost places, though (NY/etc). But be prepared to sell your soul, sign ridiculous contracts and be sued if you leave to competitor. &gt; If you cut out all the outliers in C++ skills, are you still left with plenty of good paying C++ roles? It depends. For example in Houston market for C++ devs is tight and over there Java/C# are better in terms of job security. With salary ranges being more or less the same. C++ is hanging on for now (thanks to recent life blood infusions of C++11/14/17), but it is no longer on top of the world -- companies realized that it makes sense to use it where it is actually needed and write everything else in Java/C#/etc (ideally outsourcing C++ part to Russia/etc). 
Exceptions can be problematic. For example: [this](https://stackoverflow.com/questions/45497684/what-happens-if-throw-fails-to-allocate-memory-for-exception-object/45552806). (don't take me wrong -- I love convenience, but damn...)
Some processors support full 512 bit operations and some others break them into 256 bit operations. I forget which ones do it completely. There should still be benefits from extra registers, denser code, and the new instructions when optimized software+compilers can use them
`function2` looks quite interesting. I really like the one-shot example. However "Converbility" and "Cobvertible" in the README look like typos. Did you mean "Convertibility" and "Convertible"?
Lol this guy has an unhealthy fetish for metaprogramming
well, maybe we aren't there yet. But we now start C++20 before C++17 is balloted, for example. Previously they waited for feedback before introducing changes - that's why 03 is a bug fix release instead of new features, etc.
I will take 500 compile errors if it means 1 less runtime error.
&gt; we now start C++20 What exactly beside moving TSes and bug fixing into draft was done wrt C++20? Anyway I guess process made progress, but obviously I am still not convinced that is the best possible... But obviously hard to know from the outside.
Well, the announcement said: "nothing new". The post said "lots of fixes". I think it is a good question considering the 'hashtagfakenews'
Sure, we all know windows and macs are better at that than linux. when it is in a stable linux version, however, that's when you know it is working. so I wanna know
Is there a tutorial or guide to help the compiler use autovectorizer?
I agree with /u/Splanky222, the hallway conversions, the ability to ask questions during the presentation and talk to the presenters afterwards are huge advantages. During my first year, I went to a talk about clang sanitizers and I learned a bunch. I went back to my office, deployed clang and used the address sanitizer on our legacy product and was able to solve a memory leak problem that has plagued the product for years. The best thing about this conference is learning about things you didn't know existed and things that are coming so you can get ready. Some of which you learn about in the presentations and some of those things you learn about in the hallway. I got my employer to send me by offering to give an overview presentation when I got back and direct my co-workers to videos and slides relevant to what we do. It turned into a modern c++ pep talk and I got a lot of people excited about C++11 and what was to come in C++14. It led to compiler updates, we use the CPP Core guidelines, we use clang-tidy now, and more. It's totally worth it to go.
I’m working mainly with AVX-512BW/DQ (for image processing), rather than float - it’s possible I’m hitting cache/memory latency or bandwidth issues, but where I see the expected 2x improvement from SSE to AVX2, I’m seeing almost no improvement from AVX2 to AVX-512. (This is with hand-coded intrinsics, not auto-vectorisation, and using gcc 7, ICC 17 and ICC 18.)
I would hope so!
Just generate markdown and put it through Pandoc.
Other than "see piece of code you think should be vectorized, dump into Godbolt, see not vectorized, file bugs against the people who own the autovectorizer" not really. Sometimes compilers have different pragmas and similar but for that you'd need to consult your compiler's documentation. For example, MSVC++ has [`#pragma loop(ivdep)`, `#pragma loop(no_vector)`](https://docs.microsoft.com/en-us/cpp/preprocessor/loop), [`#pragma fp_contract`](https://docs.microsoft.com/en-us/cpp/preprocessor/fp-contract), [`#pragma fenv_access`](https://docs.microsoft.com/en-us/cpp/preprocessor/fenv-access), and [`#pragma float_control`](https://docs.microsoft.com/en-us/cpp/preprocessor/float-control) all of which interact with the vectorizer.
DOCX are not trivial documents to generate. I'd recomend either using other PDF-generating tools or using Word's own tools such as "mail merge" or akin.
Im glad to see coroutines getting more attention. I found them very useful, I implemented Fiber on top https://github.com/kurocha/concurrent
I like using OpenMP to make it explicit which loops should be vectorized (Though the compiler is free to do others too, of course). omp simd mode has lots of ways to give hints to the compiler to improve and tune the vectorization. Plus it's portable among all the popular compilers. [Basic overview](http://bisqwit.iki.fi/story/howto/openmp/#SimdConstructOpenmp 4 0). [More detail](http://www.hpctoday.com/hpc-labs/explicit-vector-programming-with-openmp-4-0-simd-extensions/)
Well, if you are incapable of reading the announcement, no point in reciting it here.
That's a bit pessimistic. There are lots of situations where the autovectorizer's hands are tied because it's not allowed by the language to make an assumption (these pointers don't alias, your int is never negative, etc.) or to produce slightly different numerical results (e.g. as in FMA, or in a reduction). You can get a lot of value from carefully restructuring the code and/or adding compiler flags to relax these constraints under guidance from the vectorisation report, or you can whack it with OpenMP as raevnos suggests. You'll still find performance cliffs which vary between compilers, but there's a long list of things to try before filing bugs (which might be end up closed as invalid).
Agreed. Excel will open CSVs natively and then it shouldn't be too hard to use a VBA macro to do the mail merge in Word. At least, I would hope :)
Where did I say otherwise? However, what **IS** a compiler problem (even if there are ways around it) is the fact that MSVC is 1) Not yet fully conformant in its handling of templates 2) Produces more ICEs than gcc and clang even in cases where it would usually behave conforming Luckily MS is working hard on solving those.
This. Write something in VBA. It can open and read the CSV, be able to create a word document, and put stuff in that word document. Maybe write a UI with VBA in Access, if you have it. 
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/74imuo/question_about_csvs_and_docxs/dnyosht/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I think this may be the closest thing to a motto for C++ metaprogramming. Maybe for metaprogramming in general. It's perfect. Just got to get those 500 compile errors to be readable... Concepts to the rescue!
I think that's more of a quality of implementation issue (or a problem in the Itanium ABI). However large OSs tend to kill processes that are consuming too much memory anyway, so this point is moot on those platforms. And on the smaller platforms where it's definitely important to keep going when memory runs out, and the compilers don't have to comply with Itanium ABI, that's where the compilers could do better if they so desired. 
&gt; when it is in a stable linux version, however, that's when you know it is working. that's not what "stable" means in linux-land (eg Debian Stable). "stable" in debian just means "won't change fundamentally until the next debian version".
wait, so i cannot ask for guidance on how the language can be utilized for real world application of the language?
I'm happy to put up with the worst offenses of Template error messages, even. I'm an alcoholic anyway. May as well motivate it with something.
&gt; You can get a lot of value from carefully restructuring the code and/or adding compiler flags Sorry, I'm an STL maintainer, the compiler flags I get are the compiler flags my customers use. And changing compiler settings which apply globally to touch a specific loop isn't something I'd recommend doing even if you get to change those things. &gt; or you can whack it with OpenMP as raevnos suggests That falls into the compiler-specific pragmas I pointed out in my comment. &gt; there's a long list of things to try before filing bugs If "restructuring your code" causes a vectorizable algorithm to start being vectorized, that's an optimizer bug. Optimizers exist to restructure code for you. You do of course need to understand what kinds of algorithms are vectorizable on your target hardware to have realistic expectations of what the compiler can do for you.
&gt; it’s possible I’m hitting cache/memory latency or bandwidth issues Back of the napkin calculation of memory bandwidth consumption of your algorithm should be feasible, which you can compare with published specs for your chip.
&gt; large OSs tend to kill processes that are consuming too much memory anyway, so this point is moot on those platforms what if I limit my process memory via ulimit? ;-) &gt; I think that's more of a quality of implementation issue (or a problem in the Itanium ABI). No, it is a problem in C++ language and Itanium ABI did the most correct thing given circumstances. Hopefully it is going to be fixed soon.
This talk blew my mind! I can't believe how powerful constexpr lambdas and constexpr if have made the language now. To see a parser combinator implemented at compile time in actual readable C++ is awesome!
I'm glad that you like the library. Thanks for noticing me about the typos, I corrected it.
&gt; Sorry, I'm an STL maintainer, the compiler flags I get are the compiler flags my customers use. And changing compiler settings which apply globally to touch a specific loop isn't something I'd recommend doing even if you get to change those things. That's fair for you, but there are lots of people who do have that control. I also wouldn't change flags for a single loop, but there are options which will be tolerable for many applications and enable a range of optimisations which otherwise wouldn't be available. I'm on my phone so won't look the specific flags up, but for example you can relax floating point semantics in a way which enables FMA and reordering reductions without the more unpleasant parts of -ffast-math. Very few people are writing code which requires or exploits such fine control of precision (and n.b. FMA actually increases it!). &gt; That falls into the compiler-specific pragmas I pointed out in my comment. I disagree. OpenMP SIMD is a standard, reasonably supported by modern desktop compilers. There are variations between compilers, as I said, but I would say this is a better expressed with more subtlety. &gt; If "restructuring your code" causes a vectorizable algorithm to start being vectorized, that's an optimizer bug. Optimizers exist to restructure code for you. I have already given a couple specific but widely applicable examples of cases where the compiler can't make a restructuring because the language specification does not permit it to make inferences, no matter whether the programmer considers then reasonable. (I suppose in some cases, the compiler could test the assumptions at entry and branch - and indeed they do - but the trade-offs here are much more complicated.) I'll give one more: I recently had to split a deep nest of dense numerical compute (read: a good candidate for vectorisation) into several different cases because the naïve implementation did not allow the compiler to know which dimensions of which arrays contained stride-1 access amenable to efficient vectorisation. The division into cases exploited both information about the shape of the arrays only known at runtime, as well as constraints on the relationships between dimensions which are known to be present in the data but never explicitly enforced in code. The compiler could have generated a test for each possible cases, but would have hit a combinatorial explosion, while I was able to break out the half dozen cases which were encountered in practice.
i appreciate the responses guys. i didnt realize this kind of post was not allowed. sorry. thank you for taking the time. i will look into all of the above.
Wow.
&gt; OpenMP SIMD is a standard *A* standard, not *the* standard. It's not part of the language and thus support for it is compiler-specific.
function2 is at first glance missing a `function_view` type, which I use often. Other more niche types are guaranteed call-once, immutable-call, bounded-storage, and trivially-copyable function type erasure objects. Those rest are obscure enough that exposing types for them in a general purpose library is overkill, but function view rocks. 
gcc, clang, MSVC, Intel and probably others all support it. Are there any modern C++ compilers that don't?
And not being guaranteed to have the same size doesn't make them unrelated.
For the record, I'm an ex C++er who landed a Haskell job, which also had better conditions compared to the alternative C++ offers. 
MSVC++ implements a very old OpenMP version which does not have `#pragma openmp simd`. I tried on GCC and it emits the exact same code whether you say that or not. https://godbolt.org/g/63uuQL
Really? Wow. I can see that for the 2015 edition, since OpenMP 4.0 only came out in 2014, but the 2017 compiler doesn't support it?
I saw the `coroutine_func` and I wondered: right now, coroutines are doing dynamic allocation and some sort of type erasure by default. But wouldn't a non type erased coroutine be possible? Every non type erased coroutine would store all of it's info on the stack, and would guarantee that no heap allocation is performed. Every coroutine would require to be a different type, a bit like lambdas. That would maybe open the possibility of constexpr coroutines.
The 2017 compiler [supports OpenMP 2.0](https://docs.microsoft.com/en-us/cpp/parallel/openmp/openmp-in-visual-cpp) [which was released in 2002](https://en.wikipedia.org/wiki/OpenMP#History). The implementation hasn't been touched in forever.
Yikes. I need a new job. Hire me to work on updating it? 
Is there a way to get rid of the comic talk bubble. I couldn't read the article.
You'd need to talk to /u/spongo2 's counterpart on the optimizer team about that.
[`using namespace std;` is a bad practice](https://stackoverflow.com/q/1452721/2176813), never use it. Not only that, but the code formatting is horrible and the website is unable to even handle characters such as `;` and `&lt;` in code blocks. Next time proofread what you post.
Does this proposal supports this syntax? (Type v) =&gt; v.member()
People would love to have their program compile into a single return statement because the compiler was able to do all the work.
I agree it is not what it means in debian land. debian land is not all of linux. ubuntu has two stable versions: 16.04 and 17.04. Soon, they will have 3 versions for a month before replacing 17.04 by 17.10.
you go to the link and you read the conclusions. they say: "For end users, nothing will change." no point in reading more about the announcement after reading that conclusion. yet, OP claims there is something more to this? please learn reading yourself before accusing others.
[Ubuntu uses the same meaning](https://wiki.ubuntu.com/StableReleaseUpdates)
Yeah and Visual Studio 2017 doesn't support 512L either ; 128 and 256 bit encodings of the new instructions are not available. :_(
I've got a problem, in that I understand what coroutines are, but don't have a clue what I could profitably use them for. I've never been in a situation where I thought "at this point it would be really good to temporarily suspend this task, and go do something else for a while". How do you decide it is time to suspend (what criterium)? How do you know to return to the task later (wouldn't it require some global administration of pending tasks that are occasionally triggered)? The only class of problems that I can think that makes sense in this context is long running computational tasks, which I offload to separate threads. I don't have to worry about suspending or restarting such tasks, since the OS does that quite capably. So what additional benefit would coroutines provide? What (practical) things do other people use them for, and how do they deal with the questions mentioned above? 
Yes it does, since there is no other relationship between std::array and T[N] in the standard. 
I get the same effect with trivial fragment shaders. If I add some more complicated arithmetic the AVX-512 begins to show it's teeth. Disclaimer: my working sets are split into 64 byte cache line sized chunks. I did a writeup of my initial disappointment at http://codrspace.com/t0rakka/software-rasterizer/ The situation has changed dramatically after I added more functionality. Will write about it after more pieces are in place. 
It doesn't say the stack is not unwound except if an exception handler is found. It says it is 'implementation-dependent'. In practice, I know of no compiler than doesn't unwind the stack even if there is no exception handler. 
Thanks - that's a useful additional data point.
play around with python generators
Here is some instruction. https://www.youtube.com/watch?v=Pc8DfEyAxzg
Did you mean Matt Bentley?
Both GCC and Clang skip destruction. #include &lt;iostream&gt; class raii { public: raii() { std::cout &lt;&lt; "Constructed" &lt;&lt; std::endl; } ~raii() { std::cout &lt;&lt; "Destructed" &lt;&lt; std::endl; } }; int main() { raii x; throw 1; } Output: Constructed terminate called after throwing an instance of 'int' Aborted (core dumped) Destructor is not called. 
Both contain `N` elements of type `T` stored consecutively in memory inplace (not in some remote heap location). 
It's not like the section with this conclusion was titled "Internal changes" or anything, and there totally are not five other sections in the announcement ... sigh.
https://cgit.kde.org/kdevelop.git/commit/?id=64d7d49ed3a243e407b039f45c8b03e024e8fa08
[According to the article](https://imgur.com/8Ie7370)
Is that LLVM compiling iitself or some other compiler doing the work? I don't trust badly documented benchmarks, even less when they include Intel CPUs. If the icc got anywhere near that test rig the results are garbage.
That AMD Ryzens are able to compile LLVM without segfaulting is news by itself :D
What does that have to do with having the same size? If your claim is that they're unrelated becuase there's no relationship in the standard, then say that. You *just* claimed that two things not being guaranteed to have the same size makes them unrelated when you said "yes it does". Except you know you can't say that claim because there are already counterexamples in this thread.
Watching this right now and am completely lost. Metaprogramming and functional programming are weak spots for me (learned C++ on 98 when templates were only really used for containers) Need to pick up some modern C++ books as I've been stuck in C or C with Classes land for a long time.
There is an example in the talk of generating an "enum" meta class, and he says that it would have all the features of `enum class`, but I wonder how switch statement constraints would be enforced (handling all values or having a `default:` case, not using values that aren't in the enum).
? RTFA. This was a link into just one page of the review. It should be common sense that if you care about the specifics of a benchmark that you should go read the benchmark intro page, where they show: All of these tests were done with the systems running at DDR4-3200 with their maximum number of memory channels in use and using NVMe storage, using Ubuntu 17.10 daily with the Linux 4.13 kernel and GCC 7.2 and enforcing the "performance" CPU scaling governor. The Core i7 8700K was tested with the ASUS PRIME Z370-A motherboard, 2 x 8GB Corsair DDR4-3200 memory, Corsair Force MP500 120GB NVMe SSD, and integrated graphics. Let's see how Coffee Lake is performing under Linux with this hexa-core Core i7 CPU. All benchmarks were run in a fully-automated manner with the open-source Phoronix Test Suite benchmarking software Really, this is just common sense.
&gt; I don't trust badly documented benchmarks This is a 100% automated and open source benchmark suite. It compiles llvm (using gcc, afaik). I don't think Michael likes to test icc, but I could be wrong there.
&gt; That AMD Ryzens are able to compile LLVM without segfaulting is news by itself :D It *was* news; AMD released a fixed Ryzen a while ago, to account for that issue.
I think you’re describing stackfull coroutines?
The most compelling use cases I’ve seen were networking (IO). They allow writing code in the synchronous style, but automatically getting the asynchronous version at runtime. 
No. Stackful coroutines are different. Stackful coroutines need their own stack, and can be suspended and resumed anywhere in their execution context. Ironically, because of that, I'm confident they would need large amounts of memory because they need another whole stack (does that make sense?). A replacement for stackful coroutines would be [boost context2](http://www.boost.org/doc/libs/1_65_1/libs/context/doc/html/context/cc.html). Edit: You can think of stackful coroutine as Fibers and cooperative threading. They are very similar.
That would be a good use case, but wouldn't it require you to use socket functions that have built-in support for coroutines? Instead of simply blocking, the function must know to call yield at the appropriate time. And you must set up a mechanism for resuming it later. It still seems a lot of work compared to just using a thread... 
Thanks for clarifying.
Found that paragraph on page two. My error for not checking the index and my error for expecting the graph itself to link to more detailed documentation. Also the last graph explicitly showed g++ as part of the compiler flags, so I expected a difference. 
System setup was on page two, I really should have checked the index before complaining. 
Ah, Phoronix pages. I have premium there, so I'm not even aware that there are pages :'-)
It's a little sad that ThreadRipper is included in other benchmarks on this article, but not the LLVM compilation.
There are articles and talks about using coroutines together with ASIO/Networking TS. And it looks pretty good. 
Great report. We definitely need a solution to our "C++ build systems" problem. Looking forward for your a video about that. 
There is a PHP compilation test (which is C, not C++ like LLVM) on page 4 that includes TR. The result is pretty bad for TR, barely beating 8700K.
It looks to be no more "bad" than Intel's HCC compared to their 8700K. 8700K took 57.31 seconds with 6 cores and 12 threads 7980XE took 45.03 seconds with 18 cores and 36 threads 12 second improvement (45.03/57.31 = 78.6%) with 3x the cores 1800X took 67.41 seconds with 8 cores and 16 threads 1950X took 52.37 seconds with 16 cores and 32 threads 15 second improvement (52.37/67.41 = 77.7%) with 2x the cores, which is better scaling. To be honest, it looks like that PHP compilation process is not heavily multithreaded, based on those numbers. The performance should scale better on both Intel and AMD than that. The Intel 7960X with 16 cores and 32 threads should not beat their flagship 7980XE in a bench that is multithreading well. I think the PHP compilation process must involve a large, single-threaded section somewhere, to explain the poor core-scaling demonstrated, and to explain why it favors the 8700K so much, since it has such a high single-core boost frequency.
&gt; wouldn't it require you to use socket functions that have built-in support for coroutines? Yes. That's why it's easiest to let someone else do the hard work for you (e.g., Boost.Asio) and build on top of that. Just recently at work, I successfully updated a legacy single-threaded, single-connection codebase to support multiple-connections by using Boost.Asio + coroutines. It was a lot easier than the various other options we looked at... explicitly asynchronous (would require too many changes to the existing codebase to support a callback/completion architecture), or preemptively multi-threaded (too much legacy stuff to have to make threadsafe).
Yes, that could very well be the case. An LLVM compilation on TR would definitely be more illuminating.
I posted a link of Chrome compilation a while back and Ryzen sucks... https://www.reddit.com/r/cpp/comments/6kncnu/compile_speed_or_recent_cpusryzen_surprisingly_bad/ tl;dr do not trust one benchmark, different compilations stress different parts of CPU.
A great talk, I only wish there would be some more constraints-like techniques described. Were there any other talks about Concepts on CppCon? Mr. Brown's one is more about the general theory and the `requires` clause.
That uses Visual Studio 2015 Update 3 to compile which has, AFAIK, a single-threaded linker, and Chromium has fairly long link times. My guess is that the single-threaded linker costs it any gains it made during the actual compilation phase. I'd be curious to see Chromium benchmarks using LLD.
This is the most perfect video series for my question. Thank you!
At work, on Intel Xeons, I've found that when using a SSD, msvc will actually multithread its linking. But on HDD, nope. Won't even link and compile in parallel. Also need to tweak the number of threads used by msvc. 
It was early Ryzen within a certain time frame. Even when the issue came out, some people did not have any problems.
It should also be common sense for web pages not to do this page nonsense ...
Your post have many ads
Anandtech author replied in that thread, if you are passionate about lld ping him. :)
I’m not sure I follow. That page nonsense is not only their business model, but the same model that 99.9% of professional product review sites use. I get paid for my day job, don’t you? Should they not also?
Oh, that's cool. Is this merged already? Kinda strange that they didn't mention it in the release notes.
&gt; "grab some 'za" This is not OK. You and Godbolt should both be ashamed.
I spoke to him a bit on Twitter trying to figure out the discrepancy with the Ars results, and the basic guess was the change in compiler toolchain. (Though the Ars ones don't seem internally consistent; the 1800X is way, way too slow.)
The Chromium build system (ninja) spawns one MSVC process per source file, so compilation is parallel regardless. I thought it was Visual Studio 2017 that introduced the multithreaded linking, but I could be wrong!
One per source? Ouch. MSVC can spawn 8 processes per module. Wasting a lot of resources there. As to parallel linking, maybe it was just my imagination. But there is definitely more parallelism with SSD.
Moore's law has unexpectedly discovered that the matter consists of atoms.
Version 1.1 (just released) supports VS2015.
Version 1.1 (just released) fixes this issue.
"Clang Power Tools" version 1.1 (just released) adds support for VS2015 and fixes several bugs. Thanks for your feedback and contributions. 
" The reason being that the first and last time I did any freelancing, my client was kidnapped, tortured, mutilated, and left for dead in the Mojave Desert. " I'm not 100% sure if I'm missing the punchline here...
cool, I'm not 100% sure if I'm *getting* the punchline here, so I guess...overall...we balance out?
Yes `function_view` is on my list of planned features, since it could be really useful, especially with the possibility to convert non owned functions back to an owned one: `function&lt;...&gt; f = function_view&lt;...&gt;{}.acquire()`. Additionally immutable calls are supported through using the signature `function&lt;void() const&gt;`.
And with variadic templates and potential UFCS one could add the functionality directly to `std::vector` if one wanted to.
&gt; That is an issue, but there’s a reasonable workaround (tag types). Would you mind illustrating how that works?
That does seem like something that would be nice to have in the standard. I do that with `boost::optional` all the time.
&gt;"[Regarding VIM] Ben Deane in particular was a little flabbergasted and confused why anyone would choose a "bloated, slow starting editor". However, I learned that Ben uses Emacs. Glass houses, Ben. Glass houses." This is the best statement ever. Everyone knows if you need a text editor, Visual Studio is the one you reach for. :)
I missed this talk (your talk, I assume) because I was practicing my own but I do want to try to fully grok your position on modules. Does your talk cover it (if so, happy to wait for the video)? Do I need to wait for a blog post? I mentioned this at cppcon to both Scott W. and you that the people who've done heavy build-systems-y things have a unique perspective on modules and I'd like to fully comprehend what you want/hope to get from them. 
FWIW, I've attended the talk and am involved in "heavy build-system-y things" as you put it and I am still not sure what exactly is wrong with modules from the presenter's perspective. Pretty much everything said about Modules TS in that talk is factually incorrect FMPOV.
&gt; my biggest fear about C++ is that you need to be aware of a lot of features Not really, unless you can't use the standard library (or any other library). But, if you're trying to write any performance-focused software, you should be aware of how computer hardware works. That's the thing you are programming after all.
I wouldn't worry too much, this talk is really pushing the boundaries of the possible with C++17, and as they mentioned several times there are problems with constexpr in all sorts of places in the standard library to confound matters. And there are compiler bugs to contend with as well. While I'm happy with standard template usage and some limited use of SFINAE, I doubt I'll be using any of this until 2020 at least for production code; particularly when requiring C++17 is not going to be possible for several years. That's not to diminish the achievements of the speakers though; I watched this while I was writing some pybind11 code this afternoon, and was awed by what's possible.
To test private methods in a unit test, can do '#define private public' in your test file before you include the relevant header. Edit: To everyone who downvotes this: I never said it is nice solution. This 'trick' is used in a big software framework I know, so for a fact it works despite academic claims that it is UB. Declaring methods public or private does not change the layout of classes in gcc and clang. Why would it, the access restrictions are entirely something for the programmer, the bytecode should not change if you declare something public or private.
&gt; One per source? Ouch. MSVC can spawn 8 processes per module. Wasting a lot of resources there. As to parallel linking, maybe it was just my imagination. But there is definitely more parallelism with SSD. Why is that wasting resources? It guarantees parallelism even when MSVC is unable to. (I mean, this is ultimately just comparable to what `make -j[n]` does.)
brilliant!
Hi. There is no punch line. This is an actual real thing that happened to my first and last client. I don't know how to make this convincing. It *happened*.
And welcome to the world of undefined behaviour
Definitely wait for the blog post. My talk is very cursory and I didn't even talk about modules all that much because I was actually waiting for /u/berium's talk before making final decisions. There are, in my not so humble opinion, a lot of unforeseen consequences with the Modules TS. My blog post will tackle these and mention all the problems.
I don't know why you are being downvoted `#define private public` is actually undefined behaviour if an STL header is inside (cfr. [here](https://stackoverflow.com/questions/27778908/define-private-to-public-in-c))
It's not a crazy idea: in dlang the default is having unit tests in the same module as the objects tested: https://dlang.org/spec/unittest.html This system scales linearly with the complexity and the size of the project, and can be scripted. I'm curious how it turns out for you, let us know.
One advantage of `function_view` is that you can wrap a non-movable object. Type erasing `aquire` removes that advantage. So either it weakens what `function_view` can do, *or* it cannot be guaranteed to work. As an aside, did you implement efficient cast to/from std::function, where you type erase storing your type *within* the `std::function` instead of storing a `function2` within the `std::function`, and vice versa? I think with a bit of care you can make auto foo = [](); function2&lt;void()&gt; f = foo; for (int i = 0; i &lt; 1000000; ++i ) { std::function&lt;void()&gt; f2 = f; f = f2; } not result in an unbounded cascade of wrapped function type erasure overhead. Admittedly, the `f=f2` would only work if you memoized the type erasure (!) or special cased assignment-from. function2( std::function&lt;Sig&gt; src ) { if (!src) return; auto it = type_erasure_memoization.find( src.target_type() ); if (it != type_erasure_memoization.end()) { auto construct_from = it-&gt;second; construct_from( this, src ); return; } where `construct_from` created for a type T takes a `function2` and one of a set of kinds of type that have a `.target&lt;T&gt;()` method, and copies/moves the `T` into the `function2`... Nevermind; almost certainly overkill. I was hoping this would be easier. 
If you need to test private member function, you should put these member function in another class, and test that class. Read about separation of concern.
That’s UB! If your using clang or gcc you can use -fno-access-control when compiling your test file instead.
Those test you do with `#define private public` might be completely irrelevant. If your tests invoke undefined behavior, they might report to succeed even though all asserts fail.
You could use the friend keyword to give access to e.g. a test class. The class doesn't have to be defined (just forward declared), so it can then exist only in the test/fixture TUs of your choosing. I think I may have picked up this idiom from someone on Reddit (thanks!). It's not perfect, but it's the least intrusive method I've seen.
Unfortunately, I couldn't get to everything during my presentation and my talk doesn't have an announcement of any kind regarding a solution. Hopefully the prototype is successful enough to move our ecosystem forward.
Why all those down votes? What specifically do you disagree with?
As stated by others that's UB, but you can do something like #ifndef UNITTESTCONTEXT #define testableprivate private #else #define testableprivate public #endif class C { public: void someMethod(); testableprivate: void someOtherMethod(); }; 
Without knowing more, maybe they just mean that you introduce needless context switching due to the number of processes competing for CPU time.
I thought OOP was supposed to be kind of like the embodiment of separation of concern. The point of a class, in my view, is to group related methods and data together into a logical unit. This solution seems kind of on par with the silly one I'm trying out in terms of elegance. I guess you'd make a class with public members, and have a variable of this type as a private member variable of the main class. Then either the parameters needed by the functions of the private member have to be passed in, or they can be members of this "separate of concerns class", which are accessed in the main class with separationOfConcernsClass.member, which seems kind of clunky to me, but maybe I'm just not seeing it.
I like this idea!
hmm so actually thats not a good idea then.
nice! Didnt know this option existed. Glad I started this thread, there is so much good information here. Thanks everyone.
I was just thinking on my drive home: wouldn't it be nice if the standard had a keyword "testableprivate" that behaves as you've described.
cool!
OOP is a useful tool, but a bit too much praised in my opinion. But the concern here is not OOP: it's rather how to separate classes correctly. Unit test are checking if a class works according to what it's supposed to behave. Private method don't fall into the category of observable behavior. If you feel the need to test for a private methods, it's because you class need to deal with another level of abstraction. That abstraction should be made into a class, or a free function. And then you should test it.
Interesting. I've definitely used that to emplace move-only types. However I don't see how it helps create shared-pointers to objects with non-public constructors.
I'm exaggerating a bit, but I wouldn't be surprised if something like that happened.
That is in the release, yes. It was simply forgotten in the announcement.
Here is the long answer: https://www.youtube.com/watch?v=dHbjRvkMbjo Enjoy.
Oh, fair enough, is there a back story to it :p ? I feel like it would warrant a paragraph or two in a future blog post.
Don't kink shame.
Rust is taking this route so it might be interesting to look at how their generators differ from C++ coroutines. One issue I'm aware of is that it exposes objects that can contain pointers into themselves, which in Rust will just mean the object needs to be immovable but in C++ probably would mean generating a move constructor that fixes up the pointers or something. I'm not aware of the details enough to know if C++ coroutines allow for recursion, but heap allocating them makes that possible by allocating stack frames on-demand rather than reserving an entire huge stack. Probably not a good idea to actually *do* this but it's a possibility.
Personally I think 99% of private methods should not / need not be unit tested in the first place. Private methods only exist to serve as helper functions for the public ones and should be tested in that context. It is just a rule of thumb, but it has served me well: If you feel the need to unit test a private method, that functionality should probably be put into it's own free function / class. 
You provide a public constructor taking a private tag type. (If you want to prevent implicit conversions from constructing that tag improperly, templates can be used to demand that the argument actually be of the right type.)
I don’t think that this is on-topic.
Because msvc already knows how to preload files, compile them, and assign them to the proper process. An msvc process will work on several files at a time and it does a good job at it. But by trying to manually force it to one process per file, you're using up extra memory and resources that then need to be synchronized by the OS instead of the application that was built for this purpose. We've done countless tests on trying to get faster builds. One per file was probably the worst next to allowing too many processes. 
I missed the talk (feeling kind of under the weather for the whole conference) but it was nice to meet you.
No questions, but Steve and Daniel are excellent speakers. I felt engaged the whole time, thanks!
How do I make the constexpr vector work? https://wandbox.org/permlink/N09DEr3VRjVEolvd
https://wandbox.org/permlink/x8bM3WF0FEpsHkps You still need a non-const object in order to call non-const member functions. The member function being `constexpr` allows it to be used when ultimately producing constexpr values.
Ah makes a lot of sense, still haven't wrapped my mind entirely around constexpr. Thanks.
Our talks were actually scheduled at the same time :)
There is! I might make a post about it, but its been 5 years and I've have to look up some dates and what not so that the story can be verified.
did you find it?
No articles or blogs, but RTTI is disabled/forbidden in a lot of places where I worked.
&gt; You do of course need to understand what kinds of algorithms are vectorizable on your target hardware to have realistic expectations of what the compiler can do for you. This isn't nearly enough -- you also need to know what patterns the compiler supports for autovectorization, which for some compilers is much more limited than the instruction set. This simple loop fails to be vectorized by the latest version of a popular compiler targeting SSE2, despite the ISA having a direct mapping: void foo(short *__restrict dst, short *__restrict src1, short *__restrict src2, int n) { for(int i=0; i&lt;n; ++i) dst[i] = src1[i] * src2[i]; } 
I suspect this might change slightly as composition becomes favoured over inheritance (which I feel is the overall trend in programming right now) . C++ is kind of clunky with that unless you use RTTI now and then. Edit: It's also quite useful to avoid having extremely large interface definitions with lots of useless virtual functions in some cases. This is really going to depend on the scope, but in the canonical example, say you have a Person interface from which Wizard and Warrior inherits, and you want to create an "Attack(Person\*, Person\*)" rule, you might need dynamic_cast to handle special rules easily.
An obvious one is the use of raw *owning* pointers.
I build my tests using -fno-access-control. As far as I know MSVC does not support this flag, but gcc and clang does. 
srand() and rand().
There are proposals for multimethods, even by Bjarne Stroustrup himself: http://www.stroustrup.com/multimethods.pdf I agree that C++ desperately needs native multimethods. It would also eliminate your use case of `dynamic_cast`. But as far as I know, in practice your problem is usually solved via unique IDs for each class because `dynamic_cast` used to be (is?) very slow.
Inheritance and dynamic dispatch through virtual tables has been replaced by std::variant and dynamic dispatch through lambda visitation. Makefiles has been replaced by modern cmake.
That would be very nice indeed, right now you would have to write a generalist X(A,A) that dispatches to X(A,B) and X(B,B) and so on using dynamic_cast (or some other means), which is rather annoying. The basic problem is simulating that dispatch, be it through dynamic_cast or some kind of virtual function (say returning from an enum of possible subtypes) or any other ID mean. As usual, I'd say use the cleanest solution that's as fast as you need it. Edit: [this](https://tinodidriksen.com/2010/04/cpp-dynamic-cast-performance/) seems like a good rundown. 10 times slower might be acceptable, depending on the circumstances.
this was done in 2010 and he's using reinterpret_cast for this...
wut, it's completely untrue
How exactly is that UB? It is a text replacement by the preprocessor.
Fair enough, that is better.
If you need runtime open polymorphism, there's no way around inheritance. 
Care to show a plugin system implemented with std variant ?
Bracket[] arrays and raw new/delete
I think he was referring to vscode, not vim there.
We don't write unit tests; in the vast majority of cases it is simply not the right granularity for testing. Pretty much all the bugs we deal with (and I looked through the last three years in our bug tracker to verify this!) are the result of complex interactions between classes, or between processes, and they usually depend on very specific configuration parameters as well. I could not find even a single case in which someone calls function X, and it returns Y instead of Z so we know X is wrong - so why should we test for that? Instead we rely on system-level tests, which we conduct by hand. That's inconvenient, but I honestly don't know how to automate those, since they invariably rely on having a very specific configuration database driving the system. If we could automatically populate that database, we wouldn't need it to begin with - we would automate the _system itself_, rather than its test suite. Perhaps some of this is due to the nature of the software that we write; it interacts with dozens of types of devices and other software systems, none of which we can demand to be configured just so for our use. Instead we have to accept whatever configuration is available and test against that, but that implies we always need to adjust our test cases to match the available facilities. Again, if we could automate that, we would already do it in the software itself, rather than only during testing. Having said that, if we were to write unit tests, we'd go with the 'friend test_class' approach. It is by far the least invasive solution, it doesn't rely on refactoring just for testing purposes, and it doesn't require UB preprocessor tricks. 
Anyone have a TLDR; version of this talk on what the use-cases of this are, and maybe why it's a separate tool and not integrated into VS? (I haven't watched the talk)
From what I gather this is helpful whenever you cannot find the cause of the error directly in the call stack (or the error is not easly reproducible), because you can navigate backwards in time (automated with breakpoints on write to specific adress, queries etc.), but it has a very big memory overhead, and sometimes significant cpu overhead (but it's also highly multithreaded, tries to use ALL the threads for recording the trace). They don't give exact numbers but I would suspect for larger applications trace files to reach 10s-100s GBs (maybe even in TBs) for a few minutes session (For about a 2 minute notepad session it amounted to 200MB). But if you can afford it, it's worth it.
If you've replaced your inheritance with std::variant (and I don't even know how exactly that would work), then you've also replaced all your runtime performance with molasses. 
Lambdas should be preferred over `std::bind` expressions, for performance reasons among others. Even though both came in with C++11.
I think that is entirely too simplistic. Following your reasoning, the only things that could end up in 'private' would be trivial in nature, which is at odds with the generally accepted goal of maximizing information hiding. Moreover, it would mean that the poor sap who is forced to provide 100% coverage (or who wants to verify a tricky internal state, or set up an initial state that couldn't otherwise be reached without resorting to a full system-level test), wouldn't be able to use private sections at all. I don't see how that could possibly help software quality. Besides, it's all unnecessary anyway, since you can just declare your unit testing class to be a friend. That gives you the exact same kind of access, with the added benefit of not having to compromise your design just for testing purposes. 
Inheritance and dynamic dispatch through virtual tables was the phrase. Inheritance by its own has been replaced by composition. Variant visitiation is a lot more performant than virtual tables and variants are kept on the stack and can be kept in vectors.
There are a lot of problems with that approach: - You are scattering code for derived classes all throughout your source code, instead of centralizing it within the class. - You have to pre-declare all possible types you want to store in the class. You cannot have the variant in a static library and add additional types in another library or in an application, for example. - For each constructed object, it forces storage to be allocated for the largest possible variant member, potentially wasting a lot of memory. At best you could say it is an additional technique for specific situations, but it's hardly a complete replacement.
I feel bad for std::bind. It's a lovely bit of work... rendered completely useless by lambdas before it even got a chance to be useful.
If you show me the plugin system using virtual tables first to make the problem specific.
That is untrue as a generalisation. Virtual hierarchies are easier to optimise than std::variant and visitation, for example by recognising that each implementation of a virtual function is identical in every case of a switch statement, and collapsing it down to a non-virtual, inlined piece of code. Devirtualisation also allows you to use many of the design benefits of classes, in the form of facility inheritance, while remaining on the stack. The benefits of keeping the variant on the stack may or may not outweigh optimisations like these depending on the use case. I have recent practical experience where replacing a variant with a class hierarchy with all the virtual bells and whistles gave a 15x performance improvement. So no, variant is unequivocally NOT a replacement for anything except raw unions, and even then it comes with a performance penalty. That's why you're being down voted. 
I remember looking at boost::bind and boost::function in some awe as a new graduate, so I agree completely. 
1) As opposed to inheritance and virtual tables? 2) In 95% of the cases defining the variant in your application works superb. For the rest you can do typelist magic. Or the legacy way with a drop in performance. 3) Objects too large for the stack should be kept on the heap with std::vector or std::unique_ptr but this is not unique to variants. It's not a COMPLETE replacement but it is a replacement for 99,999% of the situations you encounter in programming including C++.
Of the top of my head: the entire C library, C-style arrays and casts, non-RAII resource management, digraphs, goto, new, delete. Personally I'd add octal numbers as well, but maybe that's just me. 
It's a shame that dealing with folders on qtcreator is still such a pain in the ass 
They need to break it apart by link time and compile time. Assuming linking is single-threaded.
Shush. Compile time is pertinent information.
Nothing to do with legacy
Which has been replaced by what, exactly? I think you'll find, also, that that's not a C++ feature. 
By the way, as you've asserted that a call to std::visit is faster than the equivalent call to a virtual method, I'd be interested in seeing the benchmark proving that. I guarantee that you won't be able to reproduce that result. 
C++03 student here : what's the replacement for new and delete in modern revisions? 
Stack/heap storage isn't usually chosen because of size, it's because of lifetime requirements vs cost or ability to copy. Variants can be very wasteful depending on your needs, and if you suddenly have to add a large type later on, everyone pays for it. This is one of many reasons that variant is much more suited to cases where the set of types is well known and unchanging. Which is obviously not 99.999% of the time. 
&gt; Variant visitiation is a lot more performant than virtual tables and variants are kept on the stack and can be kept in vectors. Do you have a benchmark at hand to confirm that assertion? I'd be very interested if it turns out to be true.
As an aside, I'd recommend https://github.com/effolkronium/random as a convenience wrapper around &lt;random&gt;. 
Use unique/shared ptr instead of raw pointers.
There are none. He probably meant make_unique / make_shared but they are not actual replacement. std::array is a valid static array replacement thou. 
Typically, `make_shared`, `make_unique`. An example that sticks out in my mind: `f(new A (), new B ())` is basically always a bug. The order of eval here is indeterminate. If either of `new A ()` or `new B ()` throws after the other succeeded, then you leak. And obv both can throw, at least `std::bad_alloc` if not more. The fix is `f (make_unique &lt;A&gt;(), make_unique &lt;B&gt;())`, or similar. You want to get the pointer into an RAII object as quickly as possible so that theres no opportunity for it to get leaked -- even within a single "full expression". That's why raw new, delete is often code smell, if not an outright bug.
My reasoning for posting it here is this: LLVM is a large C++ codebase that is painfully slow to compile (from experience). So it is a good benchmark for latest CPUs. And I am always on the lookout for what my next build box should contain, as I am sure are others.
Bro do you even trigraph?
Are you coming from a functional programming language? Just because you can emulate functional programming in c++, doesn't necessarily mean it always makes sense to do it.
uuh.. yeah ? // plugin_api.h struct foo { virtual ~foo(); virtual std::string plugin_name() const = 0; float get_average_bananas() const = 0; virtual void do_stuff(std::vector&lt;double&gt;&amp;, float bananas) = 0; }; // plugin_api.cpp #include &lt;plugin_api.h&gt; foo::~foo() = default; // my_plugin.cpp #include &lt;plugin_api.h&gt; struct my_plugin : public foo { std::string plugin_name() const override { return "a great plug-in"; } float get_average_bananas() const { return rand() % 100 + 100; } void do_stuff(std::vector&lt;double&gt;&amp;, float bananas) override { ... } }; extern "C" foo* create_plugin() { return new my_plugin; } // main.cpp int main() { auto plug = dlopen("my_plugin.so"); auto fun = reinterpret_cast&lt;foo*()&gt;(dlsym(my_plugin, "create_plugin")); if(fun) { if(auto plug = fun()) { std::cout &lt;&lt; plug-&gt;plugin_name() &lt;&lt; " running"; if(plug-&gt;get_average_bananas() &gt; 50) plug-&gt;do_stuff({0.5, 100.}, whatever); } } } more or less... 
you should claim the "hash master" flair :p 
Really late answer :D The template visualization was my bachelor thesis together with a colleague. It was far from done back then and also isn’t now. The things that work, just work but there’s a lot missing. Many instantiations that are not recognized or cannot be resolved yet. However, it will be improved in a master thesis now (not by me, I’m implementing fold-expressions and constexpr if currently). Hopefully, it will be usable in next year :) 
On rare occasions, `goto` is absolutely necessary.
This approach reminds me of what you can do with conan nowadays
He might be trolling, but I don't entirely disagree either. The C++/Java style notion of class/object unfortunately conflates a lot of distinct notions, and compared to more modern languages it definitely feels outdated and clunky. E.g. in Rust or Haskell instead of using abstract base classes the idiom is to use trait-qualified generics. Using traits instead of inheritance to achieve polymorphism also completely solves the problems with multiple inheritance. Modern C++ features like type_traits and generic lambdas go a long way towards making C++ feel fresh. I don't think that classes will ever go away, but I think the days of large complicated class hierarchies are numbered.
Your example doesn't work unless the main program and the plugin uses exactly the same compiler. Variant helped me dodge a major bullet there. 
Yeah, std::bind is pretty amazing in its own right as an engineering feat. There's a bit too much magic in there for my liking though. The fact that the number of params at bind time and runtime don't have to match is pretty incredible given that c++ is a static language. I much prefer lambdas in that they are far more explicit and easier to reason about. The fact that they can be declared inline really helps with code readability IMO. 
Really?I only see it in machine generated state machine code.
Consensus is hard, but I seriously consider the memory stability guarantees of `std::map` a legacy that still haunts us. See [`insert`](http://en.cppreference.com/w/cpp/container/map/insert): &gt; No iterators or references are invalidated. This seriously hampers the ability to implement `std::map` as anything but a node-base container, in a time where compactness and contiguity of memory are necessary for efficiency (since the creation of C++, the disparity between CPU speed and RAM access speed has widened a lot!). This single requirement prevents using a B-Tree, for example. --- Keeping with `map`, the fact that its `reference_type` is `value_type&amp;` aka `std::pair&lt;Key const, Value&gt;&amp;` is also painful. A great trick for compactness is to separate keys from values so that the search operation does not muddy the cache lines with parts of the values. An example implementation being: union Raw&lt;T&gt; { T value; char _; }; struct Map&lt;Key, Value, Less, Allocator&gt;: Less, Allocator { std::size_t size; std::size_t capacity; unsigned char* is_present; // bit set Raw&lt;Key&gt;* keys; Raw&lt;Value&gt;* values; }; With the keys/values arranged in a tree-fashion (breadth first) or similar for good cache locality. This allows pretty fast look-up of the key, with densely packed cache lines, and then a single jump into the value array. *Note: it also allows speculative prefetching (such as with `_builtin_prefetch`, see [GCC builtins](https://gcc.gnu.org/onlinedocs/gcc-5.3.0/gcc/Other-Builtins.html)) on the children as their indices are known in advance: the children of `i` are at `2*i` and `2*i + 1`, its grandchildren at `4*i`, `4*i + 1`, `4*i + 2`, `4*i + 3`... oh, and prefetching does not trigger segmentation faults so no bounds checking is necessary.* --- Oh, and on the subject of the Empty Base Optimization... it would be great if Zero-Sized structs took 0-bytes. Leveraging EBO for efficiency every time is *really* annoying.
round _to what_?
I can't resist to mention that type traits are classes. So are lambdas ;). But of course they aren't what you'd call a traditional class. Large class hierarchies where imho never so typical in c++ (outside GUI frameworks) as in Java, thanks to templates (but I started to learned c++ around 2009, so maybe I just missed that phase).
Oh, sorry if I wasn't clear. English isn't my first language. But if an int calculation ends up being 2.8, the console will display 2. So what I am looking for is for an arithmetic solution that will round it mathematically correct. If the calculation of int is 2.8, it should display 3 as a result or if it is 2.2 a 2 as a result.
Glad to hear ! Still using VS2015, so this is another good reason to upgrade.
TL;DR * A time travel debugger just like mozilla/rr. Record trace and then replay. * Can attach to a process that is still running and then start recording it (unlike rr that cannot attach) * Records each core (unlike rr which is single core and linearizes all threads) * Still has rough edges but being used extensively within the Microsoft organization (it seems) * Recording speed can be a little slow right now * Seems to be Windows only. Not clear whether it requires performance counter support like rr does * Some advanced features like being able to query the trace via LINQ (the trace is indexed) * Not integrated in Visual Studio but that might be in the future depending on demand from user * For longish traces trace storage is approximately 1 bit per CPU instruction Generally seems like a really interesting project.
I think there is a fundamental misunderstanding at the root of your question: in C++ and most strongly typed programming languages, arithmetic calculations on integers always result in other integers. For example, if `int a = 5, b = 3;` the expected, correct result of `a/b` is `1`. If an arithmetic calculation on integer values ends up yielding `2.8`, there is a (possibly implicit) conversion to a floating point type (`float` or `double`) somewhere. Now, if you want to round a floating point value to an integer value, a simple way to do it is `int(a + 0.5)`, which takes advantage of the fact that the conversion to integer always truncates decimals, does not round up or down (you could also say it always rounds down).
Yes, I do understand everything you said and the properties of an integer. But this is part of an assigment: "We assume that R1, R2, R3, and R4 have an integer valued resistance. After input of the four values, the program should output the result arithmetically rounded to the next integer. You may for this exercise assume that the builtin integer division rounds towards zero for all operands. Use of floating point arithmetic is not allowed." We have to first calculate the resistance in a wiring and afterwards round the result correctly
If the last paragraph of my previous answer does not cut it, I'm afraid this is a topic for /r/cpp_questions or stackoverflow then (see the sidebar). Good luck!
And the iterator non-invalidation guarantees of unordered_map that don't allow you to have compact buckets.
You could do simple fixed-point arithmetics
I think I remember a compiler developer saiying that - in principle - optimizing variant is not harder than virtual function calls (should even be simpler as the set of alternatives is bounded) but as there was much less demand for it, compilers concentrate much more on virtual function calls. And of course std::variant has the disadvantage of not being a native language construct, which makes specific optimizations somewhat harder. This is also my main critique of the original statement. Even if variant and Co were superior to inheritance (I don't agree), they are far from being supported and established enough in c++ that you could call inheritance deprecated. Actually, templates are imho much more of a competitor for runtime polymorphism (which is of course only one aspect of inheritance) than variants.
&gt; Your example doesn't work unless the main program and the plugin uses exactly the same compiler. here's the example that works, with the three files compiled with * g++ (-std=c++14 by default on my machine) * g++ -ggdb -std=c++17 * clang++ -std=c++11 http://s000.tinyupload.com/?file_id=41638489482713461761 And std::variant doesn't allow to add new types at runtime so it is entirely irrelevant for this &gt; Variant helped me dodge a major bullet there. I'd really like to know how. If you're on windows where the compiler version problems are, then only VS2017 supports it so you couldn't possibly have "compiler version problems" since there's a single version to use. (I'm not even talking about mingw which isn't compatible at all with VS afaik)
That's no longer part of the standard. 
http://coliru.stacked-crooked.com/a/e245b5bb4290c3e3 But bear in mind that by doing this you lose a lot of precision. If you want to do more than one operation then I would advice you to implement fixed-point arithmetic (which is pretty much equivalent to the above but uses more digits for the fractional part and does not discard it after operation) and do the rounding at the very end.
I'm not saying it's the best method, but it's our assignment... so I have to do it like that....
I've never seen a compelling example, although I have seen a mountain of gotos that were used in lieu of RAII. At any rate, I most certainly would NOT consider it to be "modern C++" if you start throwing around gotos. 
If I need to escape multiple nested loops, I always use `goto` if `return` is not an option. A boolean flag is so much uglier and possibly also slower.
iostreams
I understand, that's why I provided the solution.
What is the calculation being done? Are you computing the resistance of the resistors in parallel, so result = 1/(1/r1 + 1/r2 + 1/r3 + 1/r4)?
It is quite useful in C++11 code. You don't want very verbose lambdas all the time with all those argument types. C++14 got it right by allowing auto there. I tried writing a converter from std::/boost::bind to lambda in my code using clang-tidy. It worked most of the time, but the code was too verbose since we were stuck with C++11. The teams rejected the changes and preferred bind over lambdas.
**Iostream!!!**
There are so many hash maps to choose from! Here are all the benchmarks from OP's post: https://tessil.github.io//other/hash_table_benchmark.html
Variant can't help, but neither can this. C++, the language, says strictly nothing about how one calls a virtual function. The above works by accident, and that is the worst kind of "works".
&gt; The above works by accident, No, it works because sane platforms define an ABI. The language doesn't say anything about shared libraries anyways (and even less dynamically loaded) either so we're already in platform-specific territory once that's part of the requirements.
FWIW, I could have misunderstood, given I've never actually tried to build Chromium on Windows.
Is there a replacement?
Not in C++ it isn't.
&gt; Using traits instead of inheritance to achieve polymorphism also completely solves the problems with multiple inheritance. there's not much practical difference between traits and "classical" inheritance. You still need [vtables and such](https://doc.rust-lang.org/book/first-edition/trait-objects.html#representation)
There's a chance we might get [this](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0645r0.html) in STL, but that only solves a part of the problem.
&gt; Variant visitiation is a lot more performant than virtual tables and variants are kept on the stack and can be kept in vectors. I had to roll back a lot of code from std::variant to a hand-rolled pre-generated union code because variant would lead to hundred of megabytes of code bloat in debug mode #include &lt;variant&gt; #include &lt;string&gt; #include &lt;vector&gt; #include &lt;array&gt; struct t { std::variant&lt;int,float,char,bool,std::array&lt;float,2&gt;,std::array&lt;float,3&gt;,std::array&lt;float,4&gt;, std::vector&lt;t&gt;&gt; var; }; int main() { std::visit([] (auto&amp;&amp;...) { }, t{}.var,t{}.var,t{}.var,t{}.var,t{}.var); } this takes 8 gigabytes of ram to compile, and more than two minutes on an intel 6900k. $ time g++ -g variant.cpp -std=c++17 g++ -g variant.cpp -std=c++17 139,04s user 3,87s system 99% cpu 2:22,94 total $ du -csh a.out 129M a.out 
Name platforms who define the ABI (for virtual calls with C++) and who who actually abide to them? I know of one and a half, and one is COM, which is not for C++ only, it's much more than that ... And of course, you're right, language knows of no shared libs eithet. Also... Not to be a purist, but ABI hinders optimization.
The point is std::variant makes it easy to design your interface properly by minimizing corner cases. Inheritance tempts you to define interfaces relying on undefined behavior like you are doing here. Try compiling your program with gcc nightly and the plugin with clang 3.6.
&gt; Name platforms who define the ABI (for virtual calls with C++) and who who actually abide to them? On x86_64, macOS, linux, BSDs (and actually Windows with WSL :p), all use the itanium abi and conventions. ARM also defines an abi (which is if I'm not mistaken mostly based on itanium abi too): http://infocenter.arm.com/help/topic/com.arm.doc.ihi0041e/IHI0041E_cppabi.pdf
&gt;The point is std::variant makes it easy to design your interface properly by minimizing corner cases. you still didn't answer how you would add new types to your std variant at runtime. &gt; Try compiling your program with gcc nightly and the plugin with clang 3.6. I don't see what this would change. GCC hasn't changed the [C++ ABI it uses](http://refspecs.linuxbase.org/cxxabi-1.83.html) since gcc 3.4 if I'm not mistaken and clang uses the same ABI anyways. 
Can you elaborate? Personally, I like iostreams. * With iostreams, terminal user interaction, files on a disk, string streams, network sockets etc. all have a common abstract representation * The streams themselves are split into front-end (i-/ostream) and back-end (streambuf) * Operator overloading allows for clean serialization independent of the target * The whole library is very abstract and allows for maximal customizability (`basic_...`, `char_traits`, `imbue`, `xalloc` etc.)
What's wrong with iostreams? See [my other post](https://www.reddit.com/r/cpp/comments/74t184/question_which_c_featureslibrariespatterns_should/do1b793/) for the reasons why I actually like iostreams.
If the integer is the quotient of two unsigned integers, one approach would be adding half the denominator to the numerator before dividing: unsigned c = (a + b/2) / b; This works because it is semantically equivalent to adding 0.5 before truncating. Note that integral division rounds / truncates towards 0 in C and C++. PS: Hello fellow ETH student. :)
Defining a keyword as a macro is ill-formed (see [lex.key]).
This function would be objectively worse if it did not use goto statements: https://github.com/boostorg/beast/blob/develop/include/boost/beast/websocket/impl/read.ipp#L129 Most of the complexity comes from the requirements for composed operations. This code is structured to always "return" from initiating functions (function calls that start with `async_`) and to have a single point of exit which performs the upcall. Also note that these macros `BOOST_ASIO_CORO_REENTER`, `BOOST_ASIO_CORO_YIELD`, could not be implemented without the use of `goto`. Without those macros, this code would be significantly more complex and error-prone to review. You can see for yourself by going back about 4 months before I rewrote it to use the macros. 
Which is replaced by what?
In light of Bjarne's talk, I wouldn't consider that an aside. IMO, something like that is a must-have if srand and rand become "not valid".
I'm not an English major. I write code for a living. Glass houses /u/IAMBJ, Glass houses.
Usually I pack multiple nested loops into a separate function / lambda and use return. Not because I want to avoid goto, but because multiple nested loops are usually complex enough that they warrant their own function (and are in fact often an algorithm that can be reused)
Is this what the C++ Core Guidelines are for? https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md 
In some performance critical sections of code they are useful to emulate function calls - they are much cheaper
Regarding your first statement, I wouldn't be at all surprised if the variant picture does improve significantly, but there are certain cases that I suspect cannot be optimised in the same way. I won't go into detail here - it would be far easier to demonstrate in code and I don't have an example to hand. I'll get round to putting together a comparison one day. I started a benchmark but never finished. 
Another one: typedef. Using statements have much nicer syntax. I haven't used typedef in C++ since I found out about them.
A few type aliases here and there with using really helps with that sort of complaint.
You're right. TIL. I thought these don't go as far as virtual functions.
How does this compare to sparsepp? Edit: Nvm -- was on mobile, now i see the benchmarks!
haha thanks xD
integers are already round :) 
Aside from what doom_Oo7 said. The exact opposite is true: You can't define interfaces with variants at all. You can define a set of allowed types, but that is an extremely inflexible approach that also prevents separation of interface from implementation. You can get some of that flexibility back by turning functions into templates (taking std::variant&lt;T...&gt;) and together with concepts or other TMP tricks you can define interface requirements and with modules you might be able to hold the compiletimes in check ... you see where I'm going with this. Now, I'm not saying there aren't many cases, where variants aren't clearly superior to inheritance (e.g. representing a value in a json data structure), but they are a far cry from being able to adequately replace inheritance based polymorphism completely (and imho also not for 99.9% of the cases). &gt; Try compiling your program with gcc nightly and the plugin with clang 3.6. Have you actually any proof/documentation that this isn't guaranteed to work or are you just grasping for straws?
Plus, they can be templatised. This is easier to add later if you started with a using than a typedef. 
Interesting If I may say so, that function is still a monster ;) but I trust you that is about the best you can do without native coroutine support (have you compared this to a statemachine based solution?). 
Isn't localization harder with iostreams?
Value-Oriented-Programming. They both use classes to encapsulate data and maintain invariants, but one treats instances as values (copied/moved) the other as objects - which implies pointers/references. Of course VOP hasn't replaced OOP, nor should it. Just hope it tips the scales as time goes on. And no OOP isn't a C++ feature in the sense the OP was suggesting (there's no `oop` keyword!) but it is part of the language design. But so is VOP, possibly moreso - value types are the default (default copy ctor, etc)
There's only undefined behaviour in the given example if an exception propagates across the plugin/main binary border. This is unlikely to work correctly if they were built with different compilers or even sometimes, different flags. Even so, this is nothing to do with the C++ standard, it's outside of its scope. And this is not a problem that variant can solve, or any other type. 
Value support has certainly improved over time, which is welcome. It's no coincidence that Herb Sutter's metaclasses talk included an example which was "regular type", to my eyes. I myself use both class hierarchies and VOP together in my work, and yes, many of those classes are *also* templates, and to watch all the overhead of the language features melt away into something extremely efficient is highly gratifying! I remember a post a while back which complained that `std::unique_ptr` was long winded and a pain to type. *Good*. 
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/74ulul/integer_arithmetic_rounding/do1fwg9/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Transaction to user blelbach could not be issued due to insufficient balance. ^[Deposit](https://np.reddit.com/message/compose/?to=tanglepayTipBot&amp;subject=Deposit&amp;message=Send%20me%20an%20address%20to%20deposit%20some%20iota) ^| ^[Withdrawal](https://np.reddit.com/message/compose/?to=tanglepayTipBot&amp;subject=Withdrawal%20iota&amp;message=iota%20to%20withdrawal%3A%20xxx%0Aaddress%20to%20withdrawal%3A%20insert%20address%20here) ^| ^[Balance](https://np.reddit.com/message/compose/?to=tanglepayTipBot&amp;subject=Balance&amp;message=Please%20tell%20me%20my%20balance!) ^| ^[Help](https://np.reddit.com/message/compose/?to=tanglepayTipBot&amp;subject=Help&amp;message=Please%20tell%20me%20more%20about%20you!)
Hmm… well I use `std::map` quite a bit; is there a better solution?
That is literally just a more structured way to specify a state machine. I can't see removal of that structure improving anything.
One can easily make a `stringstream` use a `filebuf`. But why does anyone want that?
`unordered_map` is really strange actually: &gt; If rehashing occurs due to the insertion, all iterators are invalidated. Otherwise iterators are not affected. References are not invalidated. If it were only iterator invalidation, you could actually use an open-addressing implementation. A dual array implementation (hash+index and key+value) could even possibly work. The references being mandated "stable" even when iterators are invalidated is however really tough. It's not the end of the world, but forces you to: - use tombstones instead of `swap_back`, which complicates the implementation somewhat, - use a jagged array (an array of arrays of size N, N, 2*N, 4*N, ...) to grow without invalidating existing elements. :x 
If it works for you, it's fine. If you want better performance, however, finding a drop-in replacement is complicated because it will probably satisfy all those requirements in order to achieve its performance, which means you have to carefully check that nobody accidentally relies on them.
I rely on it to extend and optimize some functionality of a third party library without modifying the library itself and I don't see a reason to reinvent the wheel badly.
Cool thank you, that's really helpful! :-)
Thank you too!
LLVM implements[1] a lot of different map structures. [1] http://llvm.org/docs/ProgrammersManual.html#ds-map
auto_ptr
That function is terrible and therefore no valid argument for goto.
I was using `std::tr1::bind()` for years before I could use lambdas, so even with them arriving in the same final version the effort of standardizing it wasn't a complete waste of time.
&gt; I only see it in machine generated state machine code That is however still a valid reason to keep it. The one example that I know, where the alternatives are even more ugly would be state-machines (You *can* use a loop with an enum, but it doesn't get better that way and can end up slower). 
For a small project with no big requirements, iostreams suffice. Projects with strict requirements (speed, latency, short binary, localization, etc), they are a pain in the ass: + Iostreams are inherently slow (it's a trade-off for that interface you listed). + Unicode support is inexistent (thus localization is painful). + Hard to optimize the generated code (compilers can't help but accept the global states of iostreams and their complexity, which can prevent other optimizations as well). + The `operator&lt;&lt;` overloads is a design I've only seen in C++, no other language does something like that (even Perl is sane here). The pattern is to have dedicated interfaces for formatting and writing to/reading from streams that don't overlap.
removed in c++17
Yeah, I must concede on this one. When Torvalds went after iostream as a case where C is better, he went after a low-hanging fruit.
I've used maps quite a bit to store things like id + data... always kinda liked the guarantee that modifications don't invalidate associative containers. Nothing beats an iterator for a reference to something, especially if you have to erase it. Wouldn't what you're proposing make modifying the map somewhat slower? I can understand the memory bottleneck in searching, but my gut feeling still is that this is best handled by supplementing the library with new associative containers like flatmap etc. with "splitting" the key and value to separate storage. 
Hi folks, I'm the dev lead for windbg and formerly a dev on time travel debugging. I'll try to answer any questions you have about this.
the greatest news I never heard
Hm, hang on, C stuff is horrible in its own way :-).
Sounds like easy way to end up with a messy code though.
Iosteams have been around as long as C++ has and really feels like the original designers were looking for an excuse to use operator overloading. You probably could shoehorn proper formatting or access to any type of IO mechanism (Raw sockets et al) but the standard never actually did. Whenever I try to do anything remotely esoteric (like non-blocking IO or sockets,) I seem to end up having to dick around with iostream way too much. All that's not bad, by the way, starting out with something that doesn't quite work as smoothly as you'd like is a great way to build understanding of the problem domain, but we never went back and built something better in its place. boost::asio tries, but that adds a mountain of complexity with not enough documentation.
Boost::bind was covering us for years
On the other hand, I rely heavily on this property of `std::multimap` to implement a functional priority queue, because the one included in std doesn't let you do things like update priorities
This is the right answer but sometimes not a great idea if the loop is highly performance critical
If you give the functions internal linkage (`static` or an anonymous namespace) there should be no performance penalty
If you give the functions internal linkage there should be no cost to using a function instead.
Agreed... my point was not about C being better, that was Torvald's claim... Having said that, try to sell fstream as opposed to write() (or FileWrite in windows). It is not that C is better, it is just that iostream is overly complicate, and not really good on its intended purpose.
I think bind can be more concise, but I don't associate brevity with mess. If anything, lambdas encourage much less messy code because the body of the bound code is kept in the local scope, whereas with bind you tend to write a function outside of that scope and point to it. There are proposals for abbreviated lambda syntax, also. 
Isn't that why [`std::unordered_map`](http://en.cppreference.com/w/cpp/container/unordered_map) exists? It's unfortunate that the "map" implementation is ordered, but that doesn't negate the existence of an unordered map in the C++ standard lib.
And when using non-owning raw pointers, it's best to make them const when possible. `void fn(const A* const a) { ... } // const pointer to const data`
Take Qt for example (even though it does not use `std::bind`). Writing all event handlers in object constructor where they are subscribed to would be degrading to code quality. Especially when these event handlers do a lot. I also frequently use `std::bind` in another framework but use case being identical. Using lambdas instead would be a disaster.
Can you explain what you mean in your last statement? What is overlapping with what? What does a dedicated interface look like?
iirc, zero-sized structs are larger than 0-bytes so that pointers to different structs are always unique, for the purpose of pointer comparisons and pointer arithmetic.
Sorry, I'm not sure to understand what was the display problem when you opened the article?
No, you see, there's nothing about lambdas which forces you to write your code in such a fashion. If your callback is already defined elsewhere, which makes sense, then a lambda can be written whose body is simply to call the callback. This is one of the cases where the equivalent std::bind formulation will be more verbose. But the lambda will be smaller and faster code, and easier to change in future. 
If your goal is to learn from the experience, then sure. If your goal is to make and release a completed game, then stop and pick a game engine. 
What are *your* reasons for considering C++?
Ah I see. Always thought doing this is a dirty thing that should be avoided. Thanks for pointing it out ;)
Do you plan to open source this?
I'm not the dev lead for time travel debugging, so I can't speak to that part. For the UI part (WinDbg Preview), we'd like to open source it at some point. The main problem for WinDbg is code borrowed from other teams that need to agree to open sourcing their code.
Do you want to make a game or do you want to make a game engine? Because you're going to need a game engine before you can make a game, whether you make your own or not. I'd *highly* suggest using a game engine. There are a ton of good ones out there that use C++ as the primary language, but C# is popular as well. You don't even need to use one of the big name engines either, there's engines out there that focus solely on 2D games. Godot might be a good choice for you. I've played with it but I haven't really gotten too in depth. It's completely free (no royalties), it's open source, you can develop in C++, it has a lot of nice features for making 2D games (true 2D engine instead of putting a 2D scene into a 3D engine, you can use pixel based units *including pixel based physics* for true pixel art games, and a lot more), and the licence allows you to modify the engine however you like. Even if you still want to build your own engine you should take a look at the Godot, since the open sourciness might help you figure out what you'd need to do to make your own. Is there any particular reason you don't want to use a pre-made engine? Do you want to build the game "from scratch" or is there some other reason? 
Personally I find projects that have a planned lifespan of more than few months are rarely a good idea unless and income source is attached to them. If you want to learn C++ consider using a game engine like cocos2d or a mutli-purpose UX library like Cinder.
Check out the Commodore 64 kernel sometime. There are jumps into the middle of subroutines all over the place. Only so much you can do with 8k. 
You should only need test the public API of the class. The public API should ensure that the invariants of your class are always maintained. If you expose private parts of the class then you loose that power. I agree with /u/graciot that you should extract the private functionality into helper classes and test those separately. 
Unity is a good engine, but it runs C# instead of C++.
How does it compare to http://www.pcg-random.org/?
I mean that you can't draw a line between formatting and writing/reading streams. There's no separation: iostreams are responsible for formatting your data structures, and handling the stream itself (stringstream is one example that went too far with this). Rather than making it all one thing, one could design an API for formatting, and another for streaming (e.g. buffer writers/readers).
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/74wtle/game_development_w_c/do1puq9/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Fair enough, I guess should have been clear that I was talking about modern x86 architectures.
How "from scratch" are we talkin here? If you want to really go at it, you can make your own game engine using OpenGL to render your graphics. But this can be a bit painful, and not really worth it in my opinion if you're just making a 2D sidescroller, so if you don't want to deal with pushing and popping your rendering surfaces, basic matrix math and other stuff, I'd recommend using SDL or even just using a framework like Godot or Oxygine. Best of luck to ya!
The two examples you've given are completely different and both have use cases. If you know the type of "It" at every call site where you want "It" to "Quack" then the first example makes the most sense - but you could also just call "Quack()" directly on the object in that example. The virtual example allows you to call "Quack()" when you don't know the derived type + allows you to store references/pointers as the base type (a collection of different types). So, as with most anything in C++: your use case determines what makes the most sense and there is no "always" :)
That's why I been emphasizing "when possible" and presented an example that when this would fail underneath 
Harder compared to what? `printf`? Why do you think it would be harder? (I'm curious; I've never done localization in C++)
Can you provide a link to where you learned about them?
Can you not inline to avoid the performance penalty?
That is a great comparison! I'm very happy to see my own emilib::HashMap in there too, and honestly quite surprised to see it being so competitive in all but the first test. It seems the hash function in these tests is the identity function, which is never a good choice unless you know the keys are random. This is the reason why emilib::HashMap (and google::*hash_map) fares so badly on the first test (the only test with non-random keys).
&gt; you also need to know what patterns the compiler supports for autovectorization, which for some compilers is much more limited than the instruction set If this happens you should file bugs. I filed this one and the optimizer folks said the fix is trivial and they just needed to add the `short` opcode to their table.
Thanks, I really appreciated this. 
Actually, in both of your examples the "MakeItQuack" portion is doing more harm than good: * In example 1 it does nothing but make the compiler do extra work to remove it. * In example 2 it makes it so the compiler can't tell the real type of the object - if you simply called the "Quack()" function on the given instance the compiler can see the derived type is final and remove the virtual dispatch completely.
Cmon, u know examples were just used to show the syntax, of course no one writes brainless code like that in real programs 
I don't know. I've not used PCG before. [effolkronium/random](https://github.com/effolkronium/random) simply uses the C++11 standard &lt;random&gt;. The point I intended was that, if you're going to use &lt;random&gt;, the library I linked can make it much easier as a drop-in replacement for rand(). You'd need to ask someone with more technical expertise with RNG to comment on the performance of PCG vs the RNG methods used in the C++ standard.
[you're welcome](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4190.htm)
&gt; Iostreams are inherently slow (it's a trade-off for that interface you listed). Not an issue for linux users then, everything is already a file and I think even sprintf shares its implementation with fprintf. 
Doesn't work if `a+b/2` overflows.
Instead of one giant function with gotos, this likely could have been a dozen or so functions with clear purposes.
cppreference?