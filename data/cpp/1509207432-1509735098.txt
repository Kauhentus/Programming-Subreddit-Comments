From every package manager that want to convince me I require to show me forks of some big repositories where they get rid of 3rdparty directories in favor of that package manager. For example show me fork of [UnrealEngine](https://github.com/EpicGames/UnrealEngine) where you get rid of most libraries from [ThirdPart](https://github.com/EpicGames/UnrealEngine/tree/release/Engine/Source/ThirdParty) and fork of [Qt5](https://github.com/qt/qtbase) without [3rdparty](https://github.com/qt/qtbase/tree/5.10/src/3rdparty).
What about the "class something" example. Is that UB according to the standard?
Part of the problem is it also requires near universal community involvement. Imagine if npm existed but there were only a dozen libraries published and all of them outdated. The C++ community has been without a healthy (portable) package manager for so long that they've gotten used to it, so even when a new solution becomes available, no one rushes to publish their library, and the package manager stagnates.
&lt;soapbox&gt; My opinion is no, we don't need a universal, where "universal" means cross-platform, package manager. I believe that package management is the responsibility of the operating system, not the language. This belief is in part formed because I'm an old man who once read books like [Linkers and Loaders](http://www.iecc.com/linker/). This book points out that linkers and loaders perform many closely related, but conceptually different, tasks. The line between them is fuzzy but in any given context (binary executable format + OS) the line is clear. Linkers and loaders have influenced a lot of my thinking about who should do what job. I also worked for many years on .NET (loosely, "the C# runtime", for those of you who would never touch Windows with a 10 meter yardstick.) .NET, like many "modern" languages, blurs the lines between language and runtime, language and build system, runtime and OS, library and language, etc. All the languages the kids are using these days have package managers and build systems and runtime environments and debuggers built-in. That's fine. In my opinion, C++ is made for a different purpose and has different needs. The [IPR paper](http://www.stroustrup.com/gdr-bs-macis09.pdf) that /u/zvrba links to in these comments does a good job of defining how close C++ can get to the boundary between language and OS. And C++ Modules are, as far as I can tell, trying to walk along that same line. Both of these efforts try to provide expressiveness to the language by providing a more usable representation of the code. But neither effort wants to break outside of the language into operating systems, build systems, etc. I know the situation for C++ programmers on Windows has historically been lacking. On Unixish systems you can just grab a library with your OS's package manager and everything magically configures and it's ready to go. (Remind me to tell you one day of the horrible bug I found with WSL and GCC and MinGW libraries installed on my Windows system somewhere!) Unixish systems have long blurred the line between the OS and the programming language. I hold no umbrage towards Dennis Ritchie for this, but it's kind of his fault : ) It's worked well in Unix, but I think it's the wrong model. I like a separation between programming languages and operating systems. And yes, I lived through the many efforts that the VC++ libraries team had to make our OS libraries work better. DLL hell. DLL fusion. Universal CRT. It's a hard problem. Maybe Microsoft has made it harder on ourselves than we should have. I don't know. And so...should we have a universal, cross-platform package manager? No, I don't think so. Package managers are about binaries. Does Windows need a "universal", works for all Windows needs, package manager? Absolutely yes. I want a package manager from the Windows team. I want to see NuGet and OneGet and Chocolatey and Windows Store and all these different technologies die, or at least recede into their respective corners, and be replaced by an actual aptitude or dpkg or zypper or pacman. I don't even care if it's the Windows Store being a package manager, as long as it can be run in a non-annoying fashion. Does C++ need a "universal", cross-compiler package manager? I don't think so. How about a universal build system? Maybe. But there are so many build systems now because people have so many different needs. I think it's a hard goal to achieve. I'm happy [to review and discuss any proposals](https://isocpp.org/std/submit-a-proposal) but I'm going to start off discussions biased towards "weakly opposed". OK, what about a universal, cross-compiler source manager, such as [VCPkg](https://github.com/Microsoft/vcpkg) ported to other compilers? I think this one is most likely, and most useful. But what happens when VCPkg starts moving its boundaries? What happens when it starts distributing binaries? Or VSIX extensions for Visual Studio? Do I want it on Unixish systems then? No way in hell. I don't know how VCPkg can be cross-compiler right now and so I'm kinda happy it's MSVC-only. I've clearly got some opinions on this topic. The TL;DR is that I'm afraid of any effort that tries to solve too many problems for too many people at once. (I worked on the [Phoenix optimizer](https://en.wikipedia.org/wiki/Phoenix_\(compiler_framework\)) after all!) Universal package managers, and universal build systems, walk too close to that line for me personally. Right now I'm happy to help solve the Windows/VS/MSVC problem, or the VS/whatever compiler problem or the MSVC/whatever platform problem. But I don't think I'm smart enough to be able to intelligently support any effort that solves bigger problems. &lt;/soapbox&gt; 
Long, long, answer posted to the comments. 
Because install() is a bad feature of CMake. It's not necessary in this case. If the library is already in the CMake workspace, you don't need to go through an intermediate representation to use it. Just use the target! Going through an intermediate representation is the best way to lose all the ABI changing flags propagated by the top level project (such as the standard version or sanitizer settings). 
&gt; Once the dtor of shared pointer starts to run, there is no longer an object there from point of view of the standard. So when you attempt to copy the object in ~test, that is UB. Are you sure about that? 6.8/1 says that the lifetime ends when "destructor call starts", but you can still refer to the object in limited ways: &gt; after the lifetime of an object has ended and before the storage which the object occupied is reused or released, any glvalue that refers to the original object may be used but only in limited ways. For an object under construction or destruction, see 15.7. 15.7 then says (emphasis mine): &gt; For an object with a non-trivial destructor, referring to any non-static member or base class of the object **after the destructor finishes execution** results in undefined behavior. My understanding is that accessing a `a_test_object` in `~test()`, which is *before* `~shared_ptr()` has finished, is not undefined behaviour.
It has to null the raw pointer it holds and most likely does it before it decrements the reference count / deletes the object. 
Yes, this is my point in [this comment](https://www.reddit.com/r/cpp/comments/79ak5d/a_bug_in_stdshared_ptr/dp0fu1z/). It isn't like you are not allowed to pass *this to another function or generally use it inside a destructor. That's not UB. So it's not at all clear to me that touching a shared_ptr from within it's own destructor should be UB. 
TIL the parentheses in a lambda are optional
No, in general it's not UB to access an object during its destructor, otherwise destructors wouldn't be allowed to do anything useful! People here seem to be saying it's specifically UB to access an object of a type defined by the standard library after its lifetime has ended, i.e. after its destructor has started. But to be super pedantic, I can't find language in the standard that explicitly forbids what you're doing. The closest I've found is this: &gt; §17.6.4.10 &gt; ... &gt; [ Note: In particular, the program is required to ensure that completion of the constructor of any object of a class type defined in the standard library happens before any other member function invocation on that object and, unless otherwise specified, to ensure that completion of any member function invocation other than destruction on such an object happens before destruction of that object. This applies even to objects such as mutexes intended for thread synchronization. — end note ] But you're not explicitly calling any member functions on `a_test_object`, and the copy constructor is not explicitly specified in terms of member functions called on the object being copied. Also, notes are non-normative, though they're generally not wrong either. Finally, I suspect if you raised this issue with the standard committee, they would tell you your use case is not intended to be supported. At best they would add some language explicitly forbidding what you're doing.
non-sense, you can't define lambda inside the function
`shared_ptr` sees that the last reference to the object is gone and starts executing the destructor. Now dtor tries to revive the object being destructed by copying the `shared_ptr`... Purely practically, two questions to think about: 1. How do you think `shared_ptr` is supposed to resurrect the partially destroyed object or cancel destruction? 2. If resurrection isn't your goal, still: how is `shared_ptr` supposed to know that the reference count is being incremented by the destructor and should be ignored?
This is my impression as well. Although, the real use case is more complex than in the post. The pos is just to illustrate the problem. The real usecase where I encountered this was something like this: std::shared_ptr&lt;event_loop_resources&gt; get_windows_event_loop_stuff(); Where the global resource is used in a number of places. One of which is inside a windows event handling loop. The structure of the related windows code was such that, to reliably tell the event loop to terminate, you needed to use part of the windows API (because of limitations in the Win32 api) to tell the loop to terminate. So to tell the event loop to terminate you needed to do something with get_windows_event_loop_stuff(). Ok, that's fine. But what happens when the underlying event_loop_resources object gets destroyed at program termination or DLL unload? Well, it's destructor runs **but windows might still be generating events** which call into the event handler, which will want to touch the event loop resources, and boom, there is the problem. Part of the issue is the Win32 API doesn't let you make this part of the event handler stateful. So you have to have some global state somewhere to handle whatever state needs to exist. So you use a std::shared_ptr to deal with that state. But oops, now you have this problem. I'm sure there are other similar cases where a std::shared_ptr is used to manage some global resource and there is a similar issue at program termination to what I've encountered here. I will certainly grant you that the Win32 API is terrible and that's a big part of the reason this code pattern happens in the first place. But there is a lot of bad old code like that around and we have to deal with it. 
That's the same I wrote though, except that you don't default-construct the returned `std::function&lt;void&gt;`. My point was that if you rely on the implicit conversion from lambda to function, using the `template&lt;class F&gt; function (F)` ctor, it fails. If you use the implicit conversion from `nullptr` or a default constructed `function`, it compiles: #include &lt;functional&gt; namespace N { struct S { static std::function&lt;std::function&lt;void()&gt;()&gt; x; static std::function&lt;std::function&lt;void()&gt;()&gt; y; static std::function&lt;std::function&lt;void()&gt;()&gt; z; }; decltype(S::x) S::x = [] { return nullptr; }; decltype(S::y) S::y = [] { return std::function&lt;void()&gt;{}; }; decltype(S::z) S::z = [] { return []{}; }; // only this fails to compile with MSVC } 
Interesting. Given the likeliness of having a match in the first group (Matt mentioned 99% for load factors below 93% in another comment), I guess the quadratic nature of the probing does not slow down the look-up much, while at the same time preventing much clustering. I am liking this SSE group idea more and more.
So... I've been reading the standard, since I was curious about whether this behaviour was actually explicitly marked as UB, or if it just sort of fell through the cracks. I've found something curious, which I'd like to hear some feedback on. It's related to the behaviour of the use_count variable in shared_ptr. The following behaviours are specified in the standard (N4296): - After default construction, use_count = 0 (20.8.2.2.1/2) - After construction from a raw pointer, use_count = 1 (20.8.2.2.1/5, 10) - After copy-construction, it is identical to the value in the copied shared_ptr (20.8.2.2.1/14, 19) - After construction from a weak_ptr, it is identical to the value in the copied weak_ptr (20.8.2.2.1/25) - During destruction, there is a test to see if use_count&gt;1 (20.8.2.2.2/1.1). - After destruction, use_count is one less than before (20.8.2.2.2/2). At no point does the standard mention that use_count needs to be incremented when copy-constructing, so the only legal values appear to 0 and 1. The 'shared' nature of this variable is also not made clear at all. Is this an oversight? Or are we supposed to "just get it", and would I find similar issues throughout? If the last, and given that it is apparently so hard to correctly specify behaviour using English text, why isn't the standard (at least for library objects) not expressed directly in code? (this also comes back to what Herb Sutter is saying during his meta-classes talks, which is that one simple page of code replaces dozens of pages of standardese) And while we're at it - the remark about use_count() being inefficient seems to suggest a possible implementation using atomic&lt;int&gt;. So now we are always paying for atomic access, even though only a handful of functions actually allow atomic use of the shared_ptr. I guess it is a bit late to ask why those aren't in a separate class, with shared_ptr not being thread-safe to begin with... 
I'd say this is spam - just a sales pitch for the book with a table of contents sample, not a review or discussion or anything - but there's no link to *buy* the book, so I'm not sure what this is.
I don't know the Win32 API that well, so bear with me, but how can there possibly be events delivered to the event loop during *program* termination? As far as I know the event loop is usually a manually driven `while (GetMessage(...))` loop running on the main thread, so if the *program* is terminating, that loop must be done right?
That 16 packages is a joke. Why there is no boost packages?
As an extension to the pure virtual interface pattern, you can put your entire implementation class in the .cpp file and expose it only through a static factory method. This reduces the need for extra method declarations, though it does add extra boilerplate for construction, and prevents using the implementation for inheritance (as it is only exposed through your static factory methods). Example below, note that you don't need to write forwarding implementations of any methods, and you don't need a separate FooImpl header to boilerplate-copy the definition. // foo.h class Foo { static unique_ptr&lt;Foo&gt; create(); virtual ~Foo() {} virtual void foo() = 0; virtual void bar() = 0; }; // foo.cpp class FooImpl : public Foo { void foo() override { printf("Implementation!\n"); } void bar() override { printf("Another implementation!\n"); } }; static unique_ptr&lt;Foo&gt; create() { return make_unique&lt;FooImpl&gt;(); }
I'm curious, how does your custom shared_ptr handle this scenario without crashing?
[Storing non static member function pointers as void*](https://www.youtube.com/watch?v=EbnRt-omrFY&amp;feature=youtu.be&amp;t=880) (or in this case unique_ptr&lt;void,...&gt;) is [not supported](https://isocpp.org/wiki/faq/pointers-to-members#cant-cvt-memfnptr-to-voidptr), right?
I'm not suggesting he use it across platforms. It's easy to write some CMake that effectively says "If I'm on Win32, use this VCPKG installation of OpenSSL, otherwise use the system installation or download it as a CMake external project". I work on a cross platform project and we do that [here](https://github.com/highfidelity/hifi/blob/master/cmake/modules/FindOpenSSL.cmake). It's a much better solution than relying on existing Win32 OpenSSL binaries which are typically made with VS 2013, meaning another set of VC Runtime binaries we had to ship.
Yes, there is a thread you have to run that does a `while(GetMessage(...)) dispatch(...);` It doesn't have to be the main thread. In fact, it's really nice if it isn't. For example, in dlib (where this issue arose) the user can just create window objects whenever they want and do things with them and the user doesn't need to lock down their main thread with a `while(...) dispatch()` loop. In fact, the user doesn't need to create any threads or really even know about the event loop in most cases. So what happens is a background thread is started if any GUI calls are made to dlib. This background thread contains the event dispatch loop. The problem arises since the dispatch loop just calls this magic windows routine: LRESULT CALLBACK WndProc ( HWND hwnd, UINT message, WPARAM wParam, LPARAM lParam ) { // process events here } That routine doesn't let you pass much information into it and since it's not part of a class the only way to pass in additional information is via some kind of global variable. There are a whole bunch of weird problems with the Win32 API that lead you to needing to pass additional information beyond those 4 arguments into `WinProc()`. So the question is two fold, how do you deal with this background thread and these additional state variables? A nice thing to do is to have an object that wraps it all up. This object owns the thread and all the auxiliary state variables you need. Hence, std::shared_ptr&lt;event_loop_resources&gt; get_windows_event_loop_stuff(); So what you do is call `get_windows_event_loop_stuff()` at the top of WinProc and you are good to go. Except there is this problem of shutdown. To really simplify, you end up with code like this: LRESULT CALLBACK WndProc ( HWND hwnd, UINT message, WPARAM wParam, LPARAM lParam ) { auto global = get_windows_event_loop_stuff(); // process events here ... if (message == global-&gt;magic_shutdown_message) return shutdown_loop; } So what will happen in some cases is you will have `main()` end, but some GUI thing is still running on another thread. Or maybe it's just a DLL unload. Doesn't matter. Then global static variables get destructed, specifically the global variable in `get_windows_event_loop_stuff()`. And now there is this race condition where it's possible that no one holds any references to the global event loop stuff, so when the `shared_ptr` in `get_windows_event_loop_stuff` is destructed it's the last one. So it calls the destructor on `event_loop_resources`. And now `event_loop_resources::~event_loop_resources()` needs to get the event loop to unblock so it can terminate gracefully. So it calls a windows function that then calls `WinProc()` with the magic shutdown message and BOOM. You just called `WinProc()` from within the shared_ptr destructor.
Java didn't standardize on a build / dependency system, but when Maven came along and provided one everyone eventually switched to it because it just made things _soooooo_ much easier. C++ build options are obviously more complicated adding in things like static versus shared binaries, threaded versus non-threaded, and all sort of other things that can make one binary library incompatible with another. That said, CMake goes a long way to making this viable. CMake's failings in this regard, IMO are * Not having recursive dependency resolution * Not being network aware On the second point, everything has to be local, or YOU have to specify the specific URL from which you will fetch a source package. While you can specify a GitHub URL and commit hash, this is still not ideal. It would be better to have something similar to a Maven repository where packages can be advertised along with their dependencies and fetch locations so that you could just add something like `external_depend("net.g-truc.glm", 0.9.8)` and bam, you've got your GLM dependency. 
All it's got to do is free the object before it decrements the counter. How you do that in a thread safe/atomic way is more complex. But you get the idea. There are a lot of ways to accomplish it. For instance, std::shared_ptr::reset() does it in a way that accomplishes it apparently.
You should consider adding your library to currently fighting c++ package managers like buckaroo, cget, conan, conda, cpm, cppan, hunter.
You should consider adding your library to currently fighting c++ package managers like buckaroo, cget, conan, conda, cpm, cppan, hunter.
I'm not a standardese expert, but the way I put it together * Shared pointer constructor (20.8.2.2.1/18): Effects: If r isempty, constructs an empty shared_ptr object; otherwise, constructs a shared_ptr object that **shares ownership** with r. * use_count() (2.8.2.2.5/7): Returns: the number of shared_ptr objects, \*this included, that **share ownership** with *this, or 0 when *this is empty The definition of use_count is what tells implementers that they have to increment it every time they add another shared_ptr that shares ownership. "After copy-construction, it is identical to the value in the copied shared_ptr" (20.8.2.2.1/19) is true in that if you're copying a shared_ptr with count 1 into another, after the operation finishes they have identical values of use_count (that is, 2).
That's why `[](){};` and `[]{}();` both compile.
Do you speak about timertt library? It has nothing in common with package managers for C++.
`a_test_object.reset();` is equivalent to `std::shared_ptr&lt;test&gt;().swap(a_test_object);` -&gt; `a_test_object == nullptr` in `test::~test()`
I mean that your library should be available via package managers, not only via tarball.
&gt; Because install() is a bad feature of CMake I think manually copying files is worse. And install is used in [Effective CMake](https://www.youtube.com/watch?v=bsXLMQ6WgIk) so I don't think it is a bad feature. &gt; If the library is already in the CMake workspace, you don't need to go through an intermediate representation to use it. Just use the target! You should be able to use the target after installation as well. &gt; Going through an intermediate representation is the best way to lose all the ABI changing flags propagated by the top level project (such as the standard version or sanitizer settings). The ABI changing flags should go in the toolchain not the top project. So that way all projects are built with the same settings.
This books looks interesting, sadly it's a bit expensive.
[Amazon link: C++ Templates: The Complete Guide (2nd Edition)](https://www.amazon.com/C-Templates-Complete-Guide-2nd/dp/0321714121) (non-referral) I see your point - this is one of the least effective “hey buy this thing” pages I’ve ever seen. However, I have approved this post as non-spam, as this is Josuttis’s own site (I checked whois, after noticing the typoed copyright at the bottom) and he is a Committee member. More importantly, his 1st edition coauthored with Vandevoorde was one of the books that was invaluable to my learning. Being partially responsible for my career earns him the benefit of the doubt. Nico just needs more effective web design (I sympathize since I don’t know how to web either, it’s just that my website intentionally doesn’t try to make money in any way). Also HTTPS.
it's considered "the template book". I can see why someone would just post its release
Good C++ books are an amazing investment. Paying a few hundred bucks for my shelf of C++ books in college when I had little money (Effective C++, Exceptional C++, the 1st edition of this, and others) was the smartest investment I ever made.
Qt will use the preinstalled libraries if available and then falls back on the third-party directory. You can see it [here](https://github.com/mxe/mxe/blob/master/src/qtbase.mk#L12) with MXE. I dont know about UnrealEngine, I dont know if they set the buildsystem up to find dependencies correctly or not. Of course, that is unrelated to a package manager
This subreddit is not an MSVC bug reporting site (same for any other compiler, unless you can spark interesting discussion). Please submit bugs through Report A Problem in the IDE. I am refraining from removing this with mod powers, but another mod can.
The problem is you can't create lambda inside a lambda. Is there any solution if I need to DO SOME WORK inside the inner lambda ? Move the inside lambda outside isn't a considerable solution.
You sure about that? What about this example? Does this program have UB? #include &lt;iostream&gt; struct something; void print(const something&amp; item); struct something { something() { a = 4; } void print() const { std::cout &lt;&lt; a &lt;&lt; std::endl; } ~something() { ::print(*this); } int a; }; // Right here we touch item after it's destructor has started. So it's lifetime has ended and this is UB? Really? void print(const something&amp; item) { item.print(); } int main() { something a; } This is basically what's happening with the shared_ptr. shared_ptr destructor runs, calls a function and that function then touches the shared_ptr, all within the shared_ptr's destructor.
I'm not storing the member function pointer when that's passed - I'm storing a pointer to a heap allocated copy of that member function pointer.
&gt; Honestly, C++ is my favorite language and I tell people all the time that it's really easier to use than they think. But this kind of "well, the standard lets me make it shitty so I did" thing here is totally counter productive to selling other people on the idea that C++ is something they should be considering. What do you propose then? You talk as if there is an obvious way for compiler to do something and it doesn't do... UB exits because there is either no good way to handle this case, or there is no portable way to handle it.
&gt; The problem is you can't create lambda inside a lambda. You certainly can; VC++ 2010 couldn't IIRC, but that was a now-fixed compiler limitation that was never in the standard.
This is fine, as long as you don't use any virtual function calls of the very same object you are currently destroying. 
I will wait while there will be a clear winner :)
So Qt should use the preinstalled libraries if available and then falls back on package manager. The reason of using package manager is to get rid of such 3rdparty directory.
Well, so far it seems to me that this isn't UB and moreover it does have a reasonable solution. I haven't seen any arguments to the contrary. So far the only citations of the standard that I have found or that have been posted here seem to indicate that this isn't necessarily UB. If it's not fundamentally UB then there is an additional question about the relative speed of a shared_ptr that didn't behave this way. If the runtime overhead needed to avoid this behavior was significant then that would be an argument against it. So far I haven't seen that, but I admit that it's not totally obvious to me how to do it in as fast a way as std::shared_ptr is currently implemented. But I also haven't thought about that aspect of it much. And really, most of the comments so far have been knee jerk claims of UB when really it's not so obvious that it is UB.
Have you tried std::shared_ptr&lt;test&gt;(new test) instead of make_shared? make_shared allocates one memory block for both the shared pointer and the object together. So it may not be able to separate the destruction like you expect. 
Na, still happens in that case.
Oh, right - missed that :)
The first edition of this was excellent. 
Version 1 of this book is absolutely amazing and I'll definitely buy the 2nd version. If you do any intermediate to advanced template programming, this is the be all end all go to book imo. 
There is no hope for package manager in C++ :P https://www.reddit.com/r/cpp/comments/792y88/timertt120_a_lightweight_headeronly_c11_library/dp0ob18/
The 1st edition was really one of the more important template books. 
So has anyone here read this book? I was going to pick up the first book to learn about templates but I was afraid it would be too outdated by now with all the C++11 and further stuff.
It's not a book, it just tells you how to instantiate one.
&gt; To fix this crash, the copy ctor would have to check, is the ref count zero? That would still access an object being destroyed. 
Agreed, I am there myself right now and putting money into the great books, and spending the time to study them has definitely gotten me way ahead of the lectures!
I dunno, I think it's probably fun to share bugs.
no worry about a referral link, wouldn't harm much :)
&gt; touching a shared_ptr from within it's own destructor should be UB It isn't, but relying on the object being in a logically consistent state during destruction is not wise, unless you know exactly how its destructor is implemented.
Yeah, I agree too. I have Bjarne's bible (Both for c++03 and c++11) and a numeric copy of Effective Modern C++. I learned C++ that way :D
Can you give a example ? Also must inside a namepsce.
Who needs web marketing skills when you have the appraisal of /u/STL. This is now on my Xmas wishlist. 
Order of destruction of globals is not guaranteed. It seems unlikely, but it's possible that std::cout was destroyed before a_test_object, thus losing the output.
Because it's a shit ton of work to port boost to the build2 system ?!
In general yes, but I have high expectations of things in std::. Especially important objects like std::shared_ptr. It's also easier to get into this situation than you would think. Like the [situation I encountered](https://www.reddit.com/r/cpp/comments/79ak5d/a_bug_in_stdshared_ptr/dp0m0qv/).
Na, it's not that. You can remove the cout statement and it still happens. 
You can already install it with cget with `cget install -X header https://downloads.sourceforge.net/project/sobjectizer/timertt/timertt-1.2/timertt-1.2.0-headeronly.tar.bz2`.
A tarball should be sufficient to work with any package manager.
Windows' x64 ABI is very restrictive. All code must be binary compatible with the C ABI...
Yes, but the idea of a separate array of meta data is the same. The innovation here is that metadata is stored such that it can be used efficiently using vector instructions. I have a hashtable (https://github.com/rigtorp/HashMap) designed for a work load with lots of deletes and where 95% of lookups fail. Designs using tombestones like google densemap and llvm densemap really struggle with this workloads since probe lengths become very high. I will try and modify the solution presented here with backshift deletion.
The way I would solve this would be to split the getter so that the window procedure code is calling a different function to retrieve global context. Having the winproc obtain an owning reference is unnecessary because the event loop has to be running have a window and enter the winproc in the first place, and arguably dangerous due to the cyclical dependency. Having separate getters and objects allows destruction of the internal context to be deferred until the windows have been destroyed and the event loop exited. There are ways to associate data directly with windows, by the way: dynamically generated winproc thunks and window properties. They generally only reduce and don't eliminate the need for globals, though. 
Yeah, that's only important at the boundaries of your own language abi.
Yeah that would be a way to do it. You then have to have a function that returns a non-owning pointer so the window procedure can call it. Someone later on might then call that function and get into the sort of trouble people get into with non-owning pointers, which isn't great. But there aren't a lot of great ways to deal with the Win32 API anyway.
`decltype(S::z) S::z = []() -&gt; std::function&lt;void()&gt; { return []{}; };` should be fine.
I copyed the wrong statement but what i was trying to do was say if Senior is Y or y then do something, otherwise do nothing.
If the ref count is checked, then object freed, then ref count decremented, we'd have to hold a lock on the ref count for the duration of the three operations. Freeing the object could take unbounded time, so in the worst case you are introducing a very long block on any users of the shared_ptr. That is much worse than a "small amount of additional overhead".
Another fail http://rextester.com/QJL58025
Very interesting talk. But there's one usecase that isn't mentioned at all (iirc) : iteration over the elements in the set/map. I assume that it is not the primary concern at google, but I guess that this implementation performs worse than a flat hashmap ?
I'll point out that rextester is using a fairly old version of MSVC (VS2015.1). But.. it's still broken with current bits (v19.12.25805). :-[ When you file the bug, post a link to it in this thread, please.
Instead of storing one flag per byte, using bitflags you store 8+ flags per byte. Seems pretty straightforward... &gt;_&gt;
The refcount is not being destroyed at any time; `shared_ptr` has a dynamically allocated control block that is pointed to by each instance of the `shared_ptr`. And it is perfectly fine to access member variables of the `T` from `~T()` .
&gt; I don't know how VCPkg can be cross-compiler right now and so I'm kinda happy it's MSVC-only. Its cmake-based so it seems it wouldn't be difficult. Package manager's like cget work similiar to vcpkg(and provides a cmake toolchain file like vcpkg), but its cross-platform. Of course, all that is achieved by using cmake's high-level build scripts. There are of course other cross-platform managers as well. &gt; The TL;DR is that I'm afraid of any effort that tries to solve too many problems for too many people at once. (I worked on the Phoenix optimizer after all!) Universal package managers, and universal build systems, walk too close to that line for me personally. I agree, taking on the task of standardizing some universal package manager or build system would be to massive to try to solve everyone's use case. What I am proposing is much more limited in scope. The goal of this is to standardized the package's metadata. So a author can provide one file and it could ideally support any package manager. The metadata for packages is very similar among package managers, so this should not be difficult. The other goal is to standardized the description of the toolchain. This is useful to help interoperability among different build tools. Most toolchains provide similar descriptions(such as compiler, flags, etc). Its just a matter of defining a format that could be used consistently across build tools. The C++ committee may be the wrong place for this, as it usually deals with language rather than defining packages and toolchains. Perhaps another group could be developed to help standardized such items.
Per [LWG 2224](https://cplusplus.github.io/LWG/issue2224): If an object of a standard library type is accessed, and the beginning of the object's lifetime does not happen before the access, or the access does not happen before the end of the object's lifetime, the behavior is undefined unless otherwise specified.
This is a really good talk about embedded systems development. The talk covered a decent chunk of content without completely glossing over everything or getting too bogged-down in detail. I am keen to get my hands on one of the washing machine simulator boards he's designed for having a play with.
You created a circular reference, you should be glad it crashes quickly.
That would be an issue for `atomic_shared_ptr&lt;&gt;`, but for regular `shared_ptr&lt;&gt;` no one can be waiting on the lock if you're destroying the object.
The standard library has blanket wording which prohibits touching standard library objects after their lifetime has ended. The shared_ptr's destructor has started, therefore its lifetime has ended, therefore you may not call member functions on it. See http://eel.is/c++draft/requirements#res.on.objects-2 *Core*'s wording allows user objects to call their own member functions in destructors or similar but there is no requirement that standard library types have sane behavior in these cases. Even if one could argue that does not apply here, you're recursively reentering shared_ptr's destructor, which also has blanket prohibition at http://eel.is/c++draft/reentrancy 
I feel that the `.reset()` case is also problematic even though it happens to be working correctly for you. I can't find it right now, but I'd expect some clause to the effect that it's UB if side-effects from destructors of objects in standard containers affect the container. You could make a similar example with `std::vector`, where the contained type's destructor reads the global vector currently being resized or whatever. I doubt that there should be any sort of guarantee of stable behaviour -- the container operation has to be free to implement its functionality in any order such that the pre-conditions and post-conditions are met.
&gt; it's not UB to access an object during its destructor Yes, but it is UB to access a standard library object during its destructor.
Accessing a partially destroyed object is not UB; if that were the case you could not access any members inside the object's own destructor which wouldn't make any sense. Clearly you want something like vector to be able to access the pointer to the elements it stores in order to destroy them in its dtor, or to pass pointers to its own insides to helper functions that do that on its behalf.
Yes, they can. 1. `weak_ptr` exists. The decrement to decide whether the object needs to be destroyed needs to be an atomic operation, otherwise a weak_ptr could "resurrect" the object after another thread has done the check and started destruction. 2. shared_ptr has atomic_Xxx operations, so every shared_ptr can be touched in an atomic way 3. atomic_shared_ptr (and the shared_ptr atomics) protect the identity of the shared_ptr object itself; they don't do anything about the reference count.
Yes, the reset case is also UB via http://eel.is/c++draft/requirements#res.on.objects-2
Let's keep it in Christchurch! Awesome place! Date: Not in winter. 
The text in your quote is: &gt;If an object of a standard library type is accessed, and the beginning of the object's lifetime does not happen before the access, or the access does not happen before the end of the object's lifetime, the behavior is undefined unless otherwise specified. [ Note: This applies even to objects such as mutexes intended for thread synchronization. — end note ] However I don't think that covers the `.reset()` case. The `reset` function does not begin or end the lifetime of `a_test_object`. Sure, `test` has its lifetime ended, but that is not an object of a standard library type.
For some reason I was thinking the reset call was in the dtor :)
You don't have to do the final decrement, since the shared_ptr just got destructed anyway. So you don't need the lock the whole time.
That would certainly clarify and answer the question if incorporated into the standard. Thanks. 
It's in the C++17 draft, 20.5.4.10/2.
&gt; you're recursively reentering shared_ptr's destructor Can you clarify this? I don't see a destructor being invoked more than once for one `shared_ptr`. I do see two distinct `shared_ptr`s trying to destroy the same object though.
The execution path is: 1. `a_test_object` is set to a value (from `make_shared`) 2. main exits. 3. Global variables are destroyed, one of which is `a_test_object` 4. `a_test_object`'s destructor starts. It is now UB for the program to touch this variable 5. `a_test_object`'s destructor decrements the reference count, finds 0, and calls `test::~test`. 6. `test::~test` tries to touch `a_test_object`, triggering UB. 
Why is the [kindle version](https://www.amazon.com/Templates-Complete-Guide-David-Vandevoorde-ebook/dp/B075MJNCCH/ref=mt_kindle?_encoding=UTF8) of this book still ~$51? I could see the hardback copy being this expensive, but this price seems steep. 
https://www.reddit.com/r/cpp/comments/79ak5d/a_bug_in_stdshared_ptr/dp13oz3/ If the ref count isn't decremented, then a weak_ptr will happily access the deleted memory, or access the memory while it is part way through destruction.
Is there a mutex used to access the underlying `event_loop_resources` object? Sounds like it's being accessed from multiple threads, right?
Figure there's no harm to mention: I'm the speaker, happy to answer any questions.
I think you really need to narrow your objectives here. Developing a 3d game unless it is trivial will be too large a task. What do you want as a priority? Game design or OpenGL programming or C++ language, or 3d engine use (like Unity), etc, etc. A book on C++ templates is useless - what makes you think learning templates is a priority? Sorry to be blunt, but be realistic. There are a lot of videos on intro to Unity 3d, perhaps start there.
Do you have a list of recommended books that are still applicable to modern C++?
https://handmadehero.org By now there's a lot of episodes to catch up on but its a great to watch. He writes in C but many of the ideas are transferable.
A fully functional 3D game from scratch is a monumental task- I'd recommend a good game engine. Even then, expect to put in a lot of work. 
I'm in the process of making a *2D* game engine, and it already took *years*. It's not an easy task, bit it sure is doable. Take a look at [what this guy did](https://kircode.com/post/how-i-wrote-my-own-3d-game-engine-and-shipped-a-game-with-it-in-20-months). A book about c++ templates is not enough. A lot of tutorial, many more book, and practice, practice, practice... With enough patience you'll do it, but be sure to be up to the task.
Great talk - I learnt a lot!
If OP is interested in C++ specifically then [Unreal Engine](https://docs.unrealengine.com/latest/INT/GettingStarted/index.html) may be a better choice, as Unity only uses JavaScript or C# iirc.
Probably best to [start with SDL](http://gameprogrammingpatterns.com/update-method.html), make some 2D games and learn about object oriented programming, and [game loops](http://gameprogrammingpatterns.com/game-loop.html) first, and when you've got to grips with linking libraries in C++ and you can make something half-way playable that isn't spaghetti code, then you should move on to OpenGL and stuff like graphics pipelines, vectors, matrices, shaders, etc.
[Probably best to start with SDL](http://lazyfoo.net/tutorials/SDL/), make some 2D games and [learn about object oriented programming, game loops, etc](http://gameprogrammingpatterns.com/contents.html) first, and when you've got to grips with linking libraries and you can make something playable that isn't spaghetti code, then you should move on to [OpenGL and stuff like graphics pipelines, vectors, matrices, shaders, etc](http://lazyfoo.net/tutorials/OpenGL/).
He actually writes in C++ but keeps it very similar to C.
Be careful with it. I saw quite a few posts about how bad the code and engine is organized. I didn't check the code? but you might want to look around before you learn bad habits.
I honestly don't like Handmade Hero very much. Developer actually writes in C++, not in C. He's very opinionated and considers most of the C++ features bloat or malpractice. He also refuses to use build systems and builds his project with a batch script. No doubt he's a talented programmer, he teaches a lot of interesting theory, but I don't think his practices would fly in real world. As far as I know, the only game he worked on in ten years was The Witness, which has been produced by Jonathan Blow, who's also a rather eccentric person.
If the purpose is to learn C++, Unreal Engine is not the best. Many conventions they follow aren't your textbook modern C++. You will probably be writing small scripts for game logic rather than actually designing software with C++.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/79edqt/my_goal_is_to_create_a_3d_game_from_scratch_using/dp1fcja/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Thanks, good points. Any other suggestions?
As the original commenter said, it depends on what your goal is. If you're trying to make a game, definitely use a game engine. If you're trying to learn graphics programming, look up OpenGL. If you're trying to learn C++, design some small game using a minimal graphics/game library (with the focus being design). Et cetera.
I don't even know what C++ Templates actually are. I thought that would be obvious as I'm teaching myself and I also said I wasn't sure if it was for beginners. And my focus would probably be C++ &gt; OpenGL &gt; Game Design. And it would be a trivial engine. Like making my own barebones Minecraft game like I saw someone else do.
Effective Modern C++ by Meyers is the only one I’m familiar with - working at the frontier of the language means there are no books for me to read :-/
Exactly!
Still trying to understand the standard... Can you explain why the existence of weak_ptr implies that destruction must be atomic? The only words that seem relevant that I can find are in 20.8.2.6: "Concurrent access to a shared_ptr object from multiple threads does not introduce a data race if the access is done exclusively via the functions in this section and the instance is passed as their first argument." Suffice it to say, the destructor is not in that section, so it may introduce a data race. As for your second point, it seems to me that those functions were kept separately from the rest as to allow implementations to have a fast thread-unsafe implementation. If they force thread-safety throughout the class that would be an accident of implementation, rather than a language feature one can rely on. As for your third point, isn't the reference count is contained within the shared_ptr object?
&gt; Can you explain why the existence of weak_ptr implies that destruction must be atomic? Billy wrote "decrement", not destruction. As for why, simple: THREAD1 a: if (refcount == 1) { b: --refcount; c: destroy object } Now thread 2 comes in and executed `weak_ptr.lock()` after thread 1 has executed label `a` but before it has completed label `b`. 
Very informative, well done! I went in to this thinking I already knew most of the caveats to the linker and I was proven very wrong :)
This talk was great. I laughed, I cried, I rewrote my GENie scripts.
cout, I believe, is special in that it is guaranteed to still be available during this phase. 
&gt; Does Windows need a "universal", works for all Windows needs, package manager? Absolutely yes. So what's the show-stopper? The infrastructure in Windows Setup based on versioned and uniquely-identified components and features as combinations of components is already in place. Even versioning is built-in so it's possible to have parallel installations of different versions of a same package. Hmm.. on a second thought the installer's database seems to be sufficient to implement Nix-like package management where "use" of a package would pull all components into own bin, include, lib hierarchy and generate VS property sheets for easy inclusion into the project.
I pre-ordered this book on Amazon last year, release date was 8th September... They still don't have it in stock so I'm still waiting on it.
Great talk, I think we need to shine more light on the linker. I come from a VC++ background and haven't come across the double creation, double deletion issue before. I believe the visual studio linker doesn't operate with a defined order. Do you know if this issue exists in the VC++ domain?
Sydney for sure :D
Cargo is terrible. Need a custom compiler flag to build your code? Not supported, just pass it as command line flag every time you call `cargo build`. Same with test flags, like running tests single threaded. Don't get me started on build scripts, they are unconfigurable hacks. Whatever a package maintainer does in there, it's a black box. The fact that you have to write build scripts to bash linker flags into your build logic shows how terrible the whole thing is. And then the package manager is so tightly coupled with the language's build logic, that it's really your only option to build any Rust project. Rust gets away with it because the language is mostly platform-independent and most of their programs don't require any of this. C/C++ would be an absolute nightmare with something like that. 
Fully agree, not only C++ related but any computer subject in general. Good authors take the effort of producing great content and should be rewarded for that. I also did the same, got lots of good books throughout my CS degree, many of them were obtained as gifts, much more appreciated as any toy.
What are you working on at the moment?
I think the issue is caused by the shared_ptr still visibly holding the pointer while decreasing the reference count. By first setting the members to nullptr you would avoid the resurection and give a consistent view -&gt; either the shared_ptr holds a reference or it does not, as it is right now it leaks an inbetween state if accessed in the destructor. So "fixing" this would require additional writes on destruction for behavior the standard currently does not require. 
So far I am enjoying these bugs as "interesting edge cases" but the caution is definitely a good reminder at least.
That was news to me too!
Just finished implementing class template argument deduction guides and tests for the STL. Starting work on constexpr char_traits next week.
Yeah, this is a reasonable inference, but the fact that order of destruction of globals within a single translation unit *is* defined, combined with the properties of [`std::ios_base::Init`](http://en.cppreference.com/w/cpp/io/ios_base/Init) means that `std::cout` must still be alive at any point in your code, thankfully.
Just finished watching this. This is a GREAT talk. Very educational.
I bought mine on Amazon the week before cppcon and got it the same day.
I read this all with great interest. I have what I think is a good solution for the motivating problem. It seems to me that we all agree that the issue is bringing the shared pointer back to life within its own destructor. But the reason OP is attempting to do this seems to be as follows - that in the destructor, the resource being managed cannot be freed until later, after the event loop has terminated, so this destruction needs to be deferred until then. Very reasonable problem. The issue is trying to raise the shared_ptr back from the dead in order to solve that problem. My solution is pretty straight-forward - add an extra level with a `std::unique_ptr`, and then move the resource from that `std::unique_ptr` if you need to: struct test2 { std::unique_ptr&lt;test&gt; resource_; ~test() { if (needsToDefer()) deferTillAfterEventQueue(std::move(resource_)); } }; std::shared_ptr&lt;test2&gt; a_test_object; So you never interfere with the normal shutdown of the shared_ptr, just in some cases snip out the actual resource to be destroyed elsewhere. 
Effective CMake has some nice ideas, but I don't believe in some of them, install() being one of them. Sure, you can be a responsible user and do things properly, even with install(). But the large majority of people don't. It's quite error prone and it is stifling innovation in general (can't use sanitizers with most setups as the flag doesn't cross the binary boundary properly for example). That's why, just like every other dangerous feature or misused feature in a language or API, they should be discouraged in this scenario. Note that install() was added to please the autotools users migrating and have "feature" parity. If install() was just a way to tag what needs to be packed by CPack and nothing else for applications, it would be much better. Libraries? I'm not so sure.
&gt; relying on the object being in a logically consistent state during destruction is not wise But that doesn't mean anything. Are you forbidding touching _any_ methods or members during destruction? Surely that would make destructors essentially useless? Yes, there are specific known limitations on calling virtual methods in the destructor. Documented limitations are fine. General fear and doubt is not fine. If there are other things we cannot do during a destructor, we need to know what they are, and not just "Avoid doing things in a destructor". To me, the obvious problem is trying to increase the reference count of `this` during the destructor. You wouldn't really expect a `std::shared_ptr` to handle that sort of abuse, and there needs to be clear language in the standard prohibiting that bad idea. 
It _should_ be UB and documented as such. Trying to increase the reference count of a reference counted pointer during its own destructor seems like an obviously bad idea and should simply be disallowed.
Section 20.8.2.6 says "Concurrent access to a shared_ptr object from multiple threads does not introduce a data race if the access is done exclusively via the functions in this section and the instance is passed as their first argument." The destructor is not listed in that section, so it seems to me there is no guarantee on thread-safety for the destructor to begin with. 
&gt; It isn't like you are not allowed to pass *this to another function or generally use it inside a destructor. But that isn't what you are doing. If you were only doing that you would never have this issue. There are _two_ different C++ objects here - `a_test_object` which is a shared_ptr, and the `test` it contains, which is also your `this`. `a_test_object`'s destructor is called. It starts to destroy its resources, and at some point in the destructor it calls the destructor of `test` - your `this`. Your destructor is free to do whatever it wants with its `this`, but it is in fact calling a method on `a_test_object`, which has already been partly destroyed - or completely destroyed for all you know, because the only guarantee you get is that `test`'s destructor is called sometime after `a_test_object`'s - for all you know the memory has already been returned to the heap. Imagine an element in the middle of a `std::vector` which, when it's being destroyed due to the `std::vector` being destroyed, calls `add` on the `std::vector` that contains it! You'd expect that to go really really badly. Why would you expect what you're doing to go any better?
Should have specified Amazon UK
I really enjoyed watching the talk, and it was quite good a first experience. Just a small correction from my side, regarding the introduction. Value types were quite common in programming languages when C++ came about. All OOP languages with exception of Smalltalk had them. It was Java's and scripting languages adoption that moved the pendle too much into references as first approach camp. While not so relevant to the whole content, it might turn off some audiences.
Yes, there is a mutex inside the object along with a bunch of other stuff. It's just a way to pass information into the Win32 callback. One of those things is a mutex.
You need to start with a general book on C++, like "C++ Primer". If you don't even know what templates are, it's too early to read a book on templates. /u/Stormatree already suggested a good course of action, grab libSDL2 and make a simple 2D game using SDL functions to render. This should get you familiar with some basic patterns used in game engines and keep you busy for a few weeks at least. Then you can rewrite your rendering code to use OpenGL.
&gt;I am refraining from removing this with mod powers, but another mod can. As you should... obviously. You have a conflict of interest. Even posting this stickied comment seems like some degree of mod abuse to me.
That would work, but where do you get this other event queue that can defer the operation? The thing we are trying to destruct is basically the last event queue type of thing you have available. Unless it's event queues all the way down :)
How could the `shared_ptr` have been completely destroyed before the `test` object's destructor finishes? It's a nested set of function calls. The `shared_ptr` destructor is literally calling `~test()`, so we going to be inside a still running destructor for `shared_ptr` when all this happens. It's possible to implement a smart pointer that still works in this situation, without undefined behavior. Although as others have pointed out, it's hard to see how to do it efficiently in the case of `std::shared_ptr` and `std::weak_ptr`.
&gt; The destructor is not listed in that section, so it seems to me there is no guarantee on thread-safety for the destructor to begin with. As I understand it, and looking over the functions, that section talks about the `shared_ptr` object itself, not the pointee (the shared state where also refcount resides). Re dtor, There is a paragraph in "20.8.2.2 Class template `shared_ptr`": "Changes in use_count() do not reflect modifications that can introduce data races."
Nothing ever "should be UB" on it's own. Things are UB because either writing the contract that defines their behavior is much easier to reason about when there are preconditions that rule out certain things or because we want to allow for variation in how things are executed (e.g. because that lets compilers better optimize the results, or for hardware to be more efficient). I don't see how either of those situations arises here for a generic case of a simple smart pointer of the kind that was common before C++11. However, given the other things the current `std::shared_ptr` needs to do like work with `std::weak_ptr`, as others have pointed out, it's not so obvious how to do a full implementation of `std::shared_ptr` that is as efficient as the current implementations given the interactions that need to happen with `std::weak_ptr`. So maybe it's reasonable to say here that `std::shared_ptr` doesn't support this case if it really would have a negative performance impact. But not because "it seems like a bad idea".
You forgot vcpkg
&gt; By first setting the members to nullptr It has to happen *after* decreasing the reference count because of weak_ptrs. Though the "proper", (almost) no-overhead place to fix it is copy ctor: it will increase refcount by 1 (the atomic increment will return the old value [or new, depending on implementation]) and if the new value is 1, it will set its own contents to `nullptr`. I'm not sure that I like the consequence where copying a non-null `shared_ptr` can end up in copy being `nullptr`. It should probably throw a `bad_weak_ptr` exception instead (like `shared_from_this` when called from ctor). In any case, the OP should IMO redesign his program to use `weak_ptr` at appropriate places.
My work is using the Intel 11.1 compiler for no reason other than no one could be bothered to upgrade over the years. I wish I could use even C++11.
Why would a weak_ptr be aware of the internal state of any individual shared_ptr? I thought everything interesting would live on the heap, either as individual block or as part of the owned object (with make_shared)? 
Then it can easily be added to them. 
`weak_ptr` aside, without *atomically* decreasing the refcount first you cannot know whether the pointer can be set to null and pointee destructed. `shared_ptr` does not hold a reference iff `use_count() == 0`. The shared state will be deleted *after* the refcount has reached zero. In any case, the OP is still trying to copy an object that is undergoing destruction. I agree with others that this is UB because *other* object than the one being destructed is inspecting the state being destructed.
&gt; without atomically decreasing the refcount first you cannot know whether the pointer can be set to null and pointee destructed. My understanding of the process: * copy members of shared_ptr into locals vars * zero out members &lt;- optional to avoid recursion * if ptr_to_refcount != NULL do decrement &lt;- destruct happens here * shared ptr no longer exists At what point do weak pointers need access to the members of this last shared_ptr object? &gt; I agree with others that this is UB because other object than the one being destructed is inspecting the state being destructed. I can agree that the standard does not give any consistency guarantees, what I want to understand is why std::weak_ptr or anything else really would prevent zeroing std::shared_ptrs members.
If that’s the only reason why haven’t you gone through the effort yourself? At my work I put the effort to move forward as soon as I find a use case or scenario to justify it. Some of our code has pretty good coverage in tests, so in those cases the justification can be as simple as: it compiles clean and passes tests :) For others it is nearly impossible to justify. Embedded systems that mostly use C; Have been in production for years with known issues but nothing ‘show stopping’
&gt; https://youtu.be/7THzO-D0ta4?t=1199 &gt; smart build systems could techincally generate PCHs for subprojects or dependencies &gt; they don't exist so we'll never know Actually, that's what CMake does with cotire: https://github.com/sakra/cotire
&gt; can't use sanitizers with most setups as the flag doesn't cross the binary boundary properly for example Why don't they cross binary boundaries? If the flags are in the toolchain it will apply to every binary build. &gt; Note that install() was added to please the autotools users migrating Install has been part of cmake since 2001, but used the `install_*` variant of commands, which are now deprecated in favor of the unified `install()` command. Furthermore, I don't see how moving to cmake pre-2.4 will help improve innovation. Also, autotools and cmake aren't the only build system that has install. There is meson and b2 that use install as well. Plus, package managers, which build from source, uses install, like conda, cget, conan, nix, etc. &gt; That's why, just like every other dangerous feature or misused feature in a language or API Misuse is requiring users to copy the files manually to wherever they want. install() ensures the files are copied correctly, and in the correct directory. &gt; If install() was just a way to tag what needs to be packed by CPack and nothing else for applications, it would be much better. That makes no sense. Why is it acceptable to only install through CPack and not a manual install? Its the same thing, doing`make install` vs `make package &amp;&amp; dpkg -i *.deb` will accomplish the same thing. &gt; Libraries? I'm not so sure. But how do you expect users to use you library? I don't want have to deal with manually copying it to certain directories. I just want to install it and use it(with `find_package` or pkgconfig ideally).
I got it, but haven't read it yet. It covers C++11/14/17 and C++20 concepts.
No, I mean outside the object. It sounds like this object is being destroyed by one thread, then during destruction it is being accessed from another thread. That's a very simple case of UB, and if I'm understanding correctly that's the cause of your problem, and it would still be a problem regardless of the design of shared_ptr.
There's another category of UB in which behavior for an uncommon or impractical use case is left undefined because defining it would incur unnecessary overhead. This seems to be the case for what you are talking about.
Yes, I meant that to be included in the second case. The way you wrote it is clearer though :)
The compiler is allowed to reorder instructions as long as the result is the same as if they were executed in order. In your case it doesn't make a difference if the `shared_ptr` destructor finishes before `~test()` is called, so you have no guarantee that `~test()` is going to happen before the `shared_ptr` destructor finishes. https://herbsutter.com/2013/02/11/atomic-weapons-the-c-memory-model-and-modern-hardware/
This all happens before any of the contents of `event_loop_resources` is destructed. The `event_loop_resources` destructor is what tells the event loop to shutdown and it then waits for the shutdown to complete before anything is destructed. That part works fine. The issue is that telling the event loop to shutdown causes the event loop to run which then tries to grab the resource via the `shared_ptr`.
SwissTable does not suffer as much from tombstones. Matt covered this a bit in the talk: when an entry is deleted we mark it as tombstone only if there are no empty marks in the whole group. Thus tombstone count does not increase on every delete unlike dense hash map or Robin hood without back shifting.
So then this is UB since the call to `item.print()` might be moved after the destructor? #include &lt;iostream&gt; struct something; void print(const something&amp; item); struct something { something() { a = 4; } void print() const { std::cout &lt;&lt; a &lt;&lt; std::endl; } ~something() { ::print(*this); } int a; }; // Right here we touch item after its destructor has started. So its lifetime has ended and this is UB? Really? void print(const something&amp; item) { item.print(); } int main() { something a; }
It should be pointed out that they are only optional when the lambda takes no arguments and does not use `mutable` or an explicit return type (e.g. `[]() -&gt; char { return 0; }`)
Thanks, very kind of you to say! Yes, it's a never ending rabbit hole :-).
Thanks, really glad to hear that!
This came up in one of the questions; I didn't do a good job providing the caveat beforehand unfortunately. The talk is pretty focused on Linux linkers, so it's useful for people writing code on *nix, or that has to be cross platform. I have heard that Windows linkers handle this sort of thing better, but I don't have any first hand experience. 
&gt; I want to understand is why std::weak_ptr or anything else really would prevent zeroing std::shared_ptrs members It's probably doable, but it'd be a case of overspecification. The object is being deleted, so accessing it from the "outside" is UB anyway.
I just copied the list from http://pfultz2.com/blog/2017/10/27/universal-package-manager/ :P
Is this the same idea as `tagged_tuple` in the [Ranges TS](http://open-std.org/JTC1/SC22/WG21/docs/papers/2017/n4684.pdf)?
Thou hast made a spelling error in thy first line, good sir ;-)
Too bad, spellchecker can't spellcheck your minds. :) Thank you.
The main problem is invoking the destructor with a reference count of 0. Any further reference counting will attempt to destruct it again. Some reference counting implementations set the count to a sentinel value (like -INT_MAX/2) which avoids this problem.
IFC don't contain comments or documentation at this point. We are keeping a keen eye on what is semantically irrelevant from IFC perspective. I believe we can do more damage by focusing too early on "optimizations".
The goal is the same. The means are slightly different. tagged_tuple requires TaggedTypes as template parameters, which need to be defined somewhere before you can create the tagged_tuple. The main purpose here was to have something, that allows me to create the "names" and types in place.
The find() function from the talk showed linear probing of the groups: group = (group + 1) % num_groups_; Was this part of the 15% untruthfulness? 
I second this! Hunter can fetch dependencies directly from CMakeLists.txt, and install them automatically into a folder of your choosing. This is great if you want to share a project or switch systems, because you won't have to spend time installing a bunch of dependencies other than cmake. 
which languages? I must admit that I'm not well versed in that era of language design, mostly being interested in earlier and later languages.
So here goes a brief list, not all of them are OOP based though: Object Pascal, Mesa/Cedar, Eiffel, Modula-2, Modula-2+, Modula-3, Oberon, Active Oberon, Component Pascal, Delphi, Ada, Oberon-07, CLU, Common Lisp Reference parameters in C++ are known as *var* or *inout* parameters in some of those languages. You might find strange me listing Common Lisp, but that Lisp variant actually does have value types, it isn't only about lists.
I like it. What kind of speed does it bring?
They don't cross boundaries because they are not all transferred in all scenarios. If it's possible to be misused, don't use it at all, like any bad API. You can't justify the usage of install() because every tool you mention has been implementing it to be compatible with traditional Linux style builds. You know which tool don't implement install()? Buck and Bazel, other modern build systems able to handle HUGE trees. A build script is just a build script, it should build. There's no reason to give a build script mostly unaudited administrator rights to write files in the default directories. Especially considering the files usually will conflict with other package managers. And how do you uninstall them? Not so easy! If you want to install an application, there are many well behaved formats to accomplish that, and that's what CPack is providing. If I want my users to use my library, I expect them to embed it in their build tree and use the pure native and direct CMake feature that has always existed: add_subdirectory(). And the build will be correct by default!
Could someone elaborate on the code sample at [20:52](https://youtu.be/xVT1y0xWgww?t=20m52s) with a "Marginally faster" title on it? I can't get my head around it. Given `std::string`s should read as `static std::string`s, what all the `template`s and `struct` supposed to do? In the case if I need to export global or two from my TU should I wrap them in such a struct?
It sounds like you're describing a linked list
I implemented SwissTable and it is indeed faster than DenseMap. On my workload DenseMap is slower than std::unordered_map due to the many tombestones causing long probe lengths for failed lookups. SwissTable still doesn't beat my implementation using backward shift deletion on my production workload. Here is my SwissTable implementation (still work in progress) https://github.com/rigtorp/HashMap/blob/hashmap2/HashMap2.h
How? What? Where? Why linked list?
So the one such thing was discussed 17 days ago: https://www.reddit.com/r/cpp/comments/75wc6j/nuple_a_named_tuple/?st=j9d5wb34&amp;sh=d68a1c0f And also I found the one from a year ago. How many named tuple implementations do we need?
Speed is like tuple. The function call should be optimized away by the compiler. I don't like the macro either :-( But without reflection synthesis of types, it is either that or handcrafting the type that has the access function with the desired name, afaict.
trackable_ptr holds a pointer to it's next element, right? An ordered collection of elements, connected by pointers, is a linked list.
Yeah I was thinking the same thing. Wish there was a way to avoid them but might not be for a few more years.
Thanks for the link, I missed that one. It seems similar to this one: https://github.com/duckie/named_types. However, those examples use compile-time strings as keys. I doubt that IDE will figure that out for you. My version and tagged_tuple from the range library have appropriately named access functions which any decent IDE should be able to figure out :-) As for the number: Several alternative versions are good, I think. It shows that there is real interest, which might make it easier to finally get named tuples into the standard (using some nifty reflection mechanism that allows you to really invent them on the spot without macros). 
&gt; They don't cross boundaries because they are not all transferred in all scenarios. What scenarios? Because if the toolchain setting dont apply then that sounds like a bug in cmake. Nor do I see how an integrated build would help in that area either. &gt; You can't justify the usage of install() because every tool you mention has been implementing it to be compatible with traditional Linux style builds. Linux is not the only OS that uses this workflow. This is used by any unix-based OSs(which includes mac), and even by windows with vcpkg. And this workflow is used because it has been proven to work effectively across many dependencies at the 10000+ scale. &gt; Buck and Bazel, other modern build systems able to handle HUGE trees. I don't see them as being proven to work at a large scale. The buckaroo package manager has only about 300 packages. Nor am I seeing any distros moving to any of those build systems either. Also, a brief look at the documentation shows that they both have have an install command from their CLI. &gt; A build script is just a build script, it should build. Its just one line in most build scripts to support installation. Otherwise, who will take care of installation? Ideally, you want the build script to do it because it knows the usage requirements(and you want libraries to install their usage requirements). &gt; There's no reason to give a build script mostly unaudited administrator rights to write files in the default directories. Then dont install into directories that need admin rights. &gt; Especially considering the files usually will conflict with other package managers. Cmake default installs into /usr/local on unix-based systems, which is the directory for users to install their own libraries. System-level package managers do not install in there. &gt; And how do you uninstall them? Just remove the directory, or use a source-build package manager. &gt; If you want to install an application, there are many well behaved formats to accomplish that, and that's what CPack is providing. But why can't it be done with a direct install? Its almost the same except going through a debian requires admin rights. Where as a cmake install doesn't need admin rights and can be installed in a local home directory. &gt; If I want my users to use my library, I expect them to embed it in their build tree Which doesn't scale. I dont want to rebuild the dependencies everytime I install an application or library. It also makes it difficult for a package maintainer(system-level or source-build) to incorporate the library so users could easily install it with their favorite package manager. &gt; use the pure native and direct CMake feature that has always existed: add_subdirectory(). Or the modern native cmake features such as `find_package` to use cmake's native targets for prebuilt binaries.
Ah, yes trackable_ptr's form linked list indeed. This is only needed to have multiple trackable_ptr's to the same trackable. They ONLY used when trackable moves, or destroys. You do not iterate over them. Elements does not stored there. trackable_ptr holds the pointer to trackable. Here is previous discussion about trackable_ptr itself https://www.reddit.com/r/cpp/comments/6x6fui/trackable_ptr_stable_pointer_for_stack_allocated/?ref=share&amp;ref_source=link . 
Yeah IDE awareness is nice, but your version would probably be disliked by macro hating people, so that's a trade-off :) Well on one hand it is good to have alternatives but then in the end if you have to use one, it's hard to figure out which one you should choose without looking through every single one and playing with it. I mean if all of the alternatives would be gathered in one comparison table explaining downsides and upsides of each version that would surely be great. As an example - every man and his dog has written their signal/slot implementation for C++ but I spend a lot of time finding the one which would let me do one of the basic things in my opinion - connect to a lambda. And signals/slots has a lot of things which could be different in their behavior and interface, but it's often really hard to tell from library help pages which one are you getting.
Other solutions like the one you linked use less obvious macros. The `$` in the nuple code is a macro. A comparison table would surely be helpful.
Good. Combine it with named variants and you have an algebraic data type system for C++, sort of. Just need to implement pattern matching, but that can't be too hard (wink wink nudge nudge).
An excellent website/app for learning many programming languages is sololearn. That's what got me started. 
I laughed at this. A lot.
Very interesting (and concerning). Clang seems to do fine: https://godbolt.org/g/NAen18
Thanks, I have added it to the list above. I will aim to maintain this post with user recommendations!
I once tried to do that and concluded that it is impossible because the standard does not guarantee which operations will be performed while editing e.g. a vector. It actually did not work either with gcc or msvc (do not remember which one, nor the details of the operations in question) Either I made a mistake, or trackable is buggy.
As a reference into C++ features or standard library documentation, I find http://en.cppreference.com/ better than cplusplus.com
The problem with cppreference, in my opinion, it can be a little cryptic to someone new to the language. I still find it quiet difficult to comprehend at times. However, I will include it to the post above! Thank you! 
It blows my mind that someone decided that writing a 4000 line tool in CMake script was preferable to trying to get a solution built into CMake submitted. That said, I guess it's easy to use and works?
I've seen several of these; to me a named tuple is just a struct. The only thing I can do with a tuple and not a struct is reflect over the fields, so if we are using macros, why not a reflectable struct instead? I wrote a macro that declares a strict and an associated function that return a tuple of pairs of string, reference to member. It's also kind of nasty to have to repeat that macro over and over, the user shouldn't need to call more than one macro. 
We do not track vector actions (I tried to go that way before too :) ), we only track element pointers in vector. Knowing element pointer we can calculate its index. I am 99.(9)% sure that this is correct by standard http://en.cppreference.com/w/cpp/container/vector (vector.data() guarantee to be in continuous space). Knowing index - we easily get iterator. Is this one not work for you ? https://github.com/tower120/trackable_ptr/blob/master/vector_interaction.cpp I actually did some fixes when I write that post, so maybe it is now fixed? Or you just tried? Actually, quite possible it's just a bug indeed. Please fill in issue with your problem at github.
&gt; was preferable to trying to get a solution built into CMake submitted. well, a lot of stuff that comes with CMake is written in CMake-lang anyways so does it really matter if it's in-tree or out-of-tree ? 
I did not mean that I tried to use your code. I tried to write some somewhat equivalent stuff a few years ago (and I also was not trying to handle vector specifically), but ran into a wall (basically vector did some operations that surprised me, and prevented me to track in all cases, or at least so I thought). I'll try to find back the details, but I could very well have been wrong, and actually I kind of hope so, because this still would be useful to me.
&gt; well, a lot of stuff that comes with CMake is written in CMake-lang anyways Honestly, I'd never actually looked at that side of the CMake source (mostly just poked in the generators to see if I could sort out bugs), and that's kind of terrifying. CMake script is...a different beast from anything else I've worked in. The only things worse (for me) has been bash and batch scripts. 4000 lines of it in one file is a testament to humanities arrogance, but hey, faster builds. I haven't tried it yet, but I'll probably check it out soon. Thanks for the links, they're interesting reading.
`magic_get` solves part of the `struct`-as-tuple issue without macros. Macros can probably be meaningful if they offer more :)
But you said that trackable is buggy... You doubt in possibility of converting element pointer into index/iterator? https://stackoverflow.com/questions/46953268/converting-stdvector-element-pointer-to-index Or you doubt that trackable will contain correct pointer? If so - trackable(trackable&amp;&amp;) basically send to trackable_ptr its new address. Thats all. It is even not aware about containers.
Well I should have not said trackable is buggy before even remembering the details of my issues when I was not even using trackable -- I was merely talking about the concept. What I meant is that I fear is that what trackable attempts to do is actually not possible in the general case when using a vector -- and at the same time I hope it actually is, and that my only problem is that I was stupid when I try to write my own trackable-like thing. Actually I can't see why it would not work, except maybe if vector implementation are trying to be smart about optimizing the choice between copying/moving/swapping and so over depending on the traits of those operations for your type - but even then I'm not sure why we would not be able to track. Or maybe they might not be expected to guarantee exclusive usage of move operations when a copy is also also available. But again I don't remember precisely so let's pretend all of that does not exist, until I come back with more precise and substantiated infos :)
plf::colony gives us a vector like interface without invalidating iterators. [http://plflib.org/colony.htm](http://plflib.org/colony.htm)
[TheCherno's C++ Series](https://www.youtube.com/playlist?list=PLlrATfBNZ98dudnM48yfGUldqGD0S4FFb) is pretty good. Mostly uses Visual Studio, but his explanations are straightforward and I like his style of presentation.
&gt;Lovely, right? Uses macros, so no.
What is magic get? I've seen posts here but I've never understand whether it works for all types, whether the types need to be registered, etc. Also, does magic get let you access the name of the field? I mentioned above, that it's a tuple of pairs of string, reference to member. 
What is your workload? How expensive is the hash function you are using? I took a quick look at the implementation: - you need to fix the resize policy. When calculating the load factor you need to count the number of tombstones as well since they increase the probe lengths. So for SwissTable (and dense_hash_map) `load_factor = (size + num_deleted) / capacity`. - when deciding to `rehash()` you can choose to rehash to a larger table or rehash the table inplace. The latter is useful when you have a lot of tombstones, so after rehashing you will have a lot of empty slots. - there is a very efficient algorithm to do inplace rehasing which ends up moving only a very small fraction of the slots of the table. - when doing benchmarks you can't set a size `X` and go with it. For each table implementation you want to see how it performs at its highest and lowest load factors otherwise you are not comparing apples to apples. So for size `X` you need to discover size `X + k` which is the point where the bucket_count() of the table will increase (right after it grows). Then you benchmark at sizes `X + k` and `X + k - 1`.
Thank you, I have added this tutorial to the post!
This is nice (except for the macro use of course). Now replicate the same thing for std::variant and you have a basic algebraic data type system. Just add pattern matching (can't be too hard, right? Right?)
It transformed as a language with C++11 imo. Too much to list.
C++ Primer 5th edition(not 6th! Different author) is a great reference and book to read and refresh your memory on modern c++. After that you can pick up a book like C++17 STL cookbook. 
Yes.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/79jx2x/c_help_with_pow/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list
I think this will work because there's no std::shared_ptr involved around it. You control the behavior of "something" so you can choose to make it's behavior sensible during the constructor. Since you don't control std::shared_ptr, you can't rely on it doing something sane after the destructor has started. If shared_ptr allowed the destructor to result in increasing the ref count, it could end up in a state where it is still alive (or should be) after the destructor has completed. That doesn't seem right. In your example, you may technically be nuzzling up against the edge of UB, but you aren't doing anything insane so it'll probably work. UB is sometimes Useful Behavior.
Could you not just use a flag to ensure the object is only constructed and destroyed once? E.g. https://wandbox.org/permlink/tgDKkS3FfYj7G4RK
Yeah, forget about my microbenchmarks, they are awful. Luckily I can replay my production transaction logs and get a apples for apples comparison for my exact workload. The work load is something like this: * Average ~1M active entries of size 32 bytes * Average ~20M insert-erase pairs per day with 100% hit ratio * ~250M lookups with 1% hit ratio * Table is pre-sized such that on 99.99% of days it never needs to rehash when using backward shift deletion (dense map will need to rehash due to excessive tombestones) * Using the MurmurHash 64bit integer finalizer/mixing function * It is preferable to trade some average operation time for lower 99.9999% operation times. Because of the low hit ratio of lookups it is important to avoid tombestones or manage them very well. I increased the block size from 16 to 32 using AVX2 instruction and now my SwissMap is only 20ns slower on average than back-shift deletion, with a lower standard deviation and 99.99%. I will add the better rehash policy at least to bound the average probe length for missed lookups. What's great is that this map is as fast as my specialized map and still a direct drop in replacement for unordered_map (sans iterator rules).
Yes, as others have pointed out, it is definitely fine. I'm just making a point that the reason it doesn't work with `std::shared_ptr` isn't because it's UB to do this kind of thing inside a destructor or because of compiler instruction reordering or anything like that. It's because, again as others have very helpfully pointed out in this thread, the standard explicitly disallows this type of usage for objects in the std:: namespace. But it's not disallowed in general and you could make your own smart pointer object that worked in this situation.
I think that will work, it's similar in spirit to the approach that is/was used for initializing iostream globals in some standard libraries, the nifty counter idiom. Personally I prefer not to mess around with placement new and flags; so I'd rather just use static locals for this purpose, and that's why I recommend those. Your solution, for example, has a very subtle bug (or at least, I'd argue it's a bug). If you have multiple globals created by this technique, they won't destruct in reverse order of construction. That's why you have to use a counter, and the destructor has to decrement it and only destruct when it hits 0.
Modules as specified in the TS generate object files and interfaces. Of course, no TS is adopted as written. Maybe you're thinking of Clang modules which are [precompiled headers on steroids](http://clang.llvm.org/docs/PCHInternals.html)?
a) plf::colony is not a vector. It is unordered. b) plf::colony performance degrades significantly, after erasing &gt;50% of elements. https://github.com/tower120/plf_colony_test . In other words - plf::colony slower then std::vector at start, and becomes MUCH slower if you have peaks in container size (you emplace 10'000 elements, than you erase 9'000 [after this - it is literally equal to std::list in terms of iteration speed] ).
My point is that you have context A which triggers the destructor of `event_loop_resources`, and context B which tries to access `event_loop_resources` apparently with no check whether it is a valid object to use. After context A calls the destructor, no other contexts should try to access the object. That's a problem regardless whether you use a `shared_ptr`, or a raw pointer, or references, or any other handle for the object.
Depending on the instruction latencies and throughputs it might be worthwhile to increase the group size to some multiple of the vector register size. 
So, I think the talk explains: I use a templated struct, only because statics of templated structs are already guaranteed to have unique initialization, and indeed this is the mechanism that C++17 inline variables are built on. It's just prior to 17 that's the only way to access it. So, instead of declaring a global directly, I use a static of a template struct, and then I alias a reference to it for convenience. You should declare this stuff in the header, not the cpp, so I'm not completely clear about "export from my TU". Also, I first thought you were asking about the performance aspects of it. So I whipped up this godbolt snippet demonstrating, leaving it here for anyone who's interested: https://godbolt.org/g/ZWFPUQ.
Hmm... true - theoretically `std::vector` implementation may use copy, instead of move. I didn't thought about that, and my objects where always moved. But! We can ensure that object will be moved, by deleting copy constructor... Using another wrapper :) `std::vector&lt;move_only&lt;trackable&lt;T&gt;&gt; &gt;`, or something like that... Actually, thanks for pointing that.
Iteration *is* mentioned in the talk and said to be very fast.
https://github.com/apolukhin/magic_get#requirements-and-limitations
I didn't really understand that. He writes: &gt; T must be constexpr aggregate initializable and must not contain references nor bitfields But he has an example with a `std::string`, which is not constexpr constructible.
[The equivalent page in the Boost docs](http://apolukhin.github.io/magic_get/boost_precise_and_flat_reflectio/requirements_and_limitations.html) seems to go into more detail, distinguishing that the constexpr requirement is only for the "precise" API. He distinguishes the difference in the [Motivation](http://apolukhin.github.io/magic_get/index.html#boost_precise_and_flat_reflectio.motivation) page.
Okay. I understand. The thing is, that more and more caveats are piling up. It's just so much effort, to avoid a macro that just works exactly correctly with any caveats. I don't see any technical justification for it.
Part 2..... http://www.danielelliott.co.uk/2017/10/29/pacific-2017-trip-report-part-2/
99% of the time when I use tuple it's as a composite key in a set or map, where all I want is the free operator&lt;() and operator==() that I would have to implement for struct. I rarely if ever end up calling get&lt;&gt;() on it anyway.
This talk was really great. It crystalized some subtleties in the language rules for me. The extended Q&amp;A at the end was great too. Something I didn't really pick up on was: Why exactly should we default to writing operator= with a ref-qualifier?
Yeah, that was one of the simplifications I made so that the code is easier to follow
I see what you mean, yes, a bit over 3x.
Iteration is usually faster for a flat_hash_map than a std::unordered_map because the data is more dense in memory (whereas std::unordered_map chases pointers). However, if you have a large table that you erase most of the elements from it, iteration becomes more expensive.
Looking forward to P0194 and beyond :)
It's open-source now: https://github.com/intel/parallelstl
https://github.com/intel/parallelstl is the open-source release of this, which happened recently.
Also, see other programming languages submissions, C and C++ stand out with their mammoth unrecognizable mess. Compare to php - $tok = ftok(__FILE__, chr(time() &amp; 255)); $queue = msg_get_queue($tok); $variants = array( 'agggtaaa|tttaccct', '[cgt]gggtaaa|tttaccc[acg]', 'a[act]ggtaaa|tttacc[agt]t', 'ag[act]gtaaa|tttac[agt]ct', 'agg[act]taaa|ttta[agt]cct', 'aggg[acg]aaa|ttt[cgt]ccct', 'agggt[cgt]aa|tt[acg]accct', 'agggta[cgt]a|t[acg]taccct', 'agggtaa[cgt]|[acg]ttaccct', ); // IUB replacement parallel arrays $IUB = array(); $IUBnew = array(); $IUB[]='/tHa[Nt]/S'; $IUBnew[]='&lt;4&gt;'; $IUB[]='/aND|caN|Ha[DS]|WaS/S'; $IUBnew[]='&lt;3&gt;'; $IUB[]='/a[NSt]|BY/S'; $IUBnew[]='&lt;2&gt;'; $IUB[]='/&lt;[^&gt;]*&gt;/S'; $IUBnew[]='|'; $IUB[]='/\\|[^|][^|]*\\|/S'; $IUBnew[]='-'; // read in file $contents = file_get_contents('php://stdin'); $initialLength = strlen($contents); // remove things $contents = preg_replace('/^&gt;.*$|\n/mS', '', $contents); $codeLength = strlen($contents); // do regexp counts $messages = array_flip($variants); $workers = $results = array(); foreach ($variants as $key =&gt; $regex){ if($key == 0 || $key == 2 || $key == 4 || $key == 6) { $pid = pcntl_fork(); if($pid) $workers[] = $pid; } if($pid &amp;&amp; $key &gt; 7) { $messages[$regex] = preg_match_all('/' . $regex . '/iS', $contents, $discard); } else if(!$pid) { $results[] = $regex . ',' . preg_match_all('/' . $regex . '/iS', $contents, $discard); if($key == 1 || $key == 3 || $key == 5 || $key == 7) { msg_send($queue, 2, implode(';', $results), false, false, $errno); exit; } } } // receive and output the counts pcntl_wait($status); foreach($workers as $worker) { msg_receive($queue, 2, $msgtype, 4096, $message, false); $message = explode(';', $message, 3); foreach($message as $line) { $tmp = explode(',', $line, 2); $messages[$tmp[0]] = $tmp[1]; } } foreach($messages as $regex =&gt; $count) { echo $regex, ' ', $count, "\n"; } // do replacements $contents = preg_replace($IUB, $IUBnew, $contents); echo "\n", $initialLength, "\n", $codeLength, "\n", strlen($contents), "\n"; or js - var fs = require('fs'), i = fs.readFileSync('/dev/stdin', 'ascii'), ilen = i.length, clen, j, q = [/agggtaaa|tttaccct/ig, /[cgt]gggtaaa|tttaccc[acg]/ig, /a[act]ggtaaa|tttacc[agt]t/ig, /ag[act]gtaaa|tttac[agt]ct/ig, /agg[act]taaa|ttta[agt]cct/ig, /aggg[acg]aaa|ttt[cgt]ccct/ig, /agggt[cgt]aa|tt[acg]accct/ig, /agggta[cgt]a|t[acg]taccct/ig, /agggtaa[cgt]|[acg]ttaccct/ig], b = [ "-", /\|[^|][^|]*\|/g, "|", /&lt;[^&gt;]*&gt;/g, "&lt;2&gt;", /a[NSt]|BY/g, "&lt;3&gt;", /aND|caN|Ha[DS]|WaS/g, "&lt;4&gt;", /tHa[Nt]/g ]; i = i.replace(/^&gt;.*\n|\n/mg, ''); clen = i.length; for(j = 0; j&lt;q.length; ++j) { var qj = q[j], m = i.match(qj); console.log(qj.source, m ? m.length : 0); } while(b.length) i = i.replace(b.pop(), b.pop()); console.log(["", ilen, clen, i.length].join("\n")); or even rust - extern crate regex; use std::borrow::Cow; use std::fs::File; use std::io::{self, Read}; use std::sync::Arc; use std::thread; macro_rules! regex { ($re:expr) =&gt; { ::regex::bytes::Regex::new($re).unwrap() } } /// Read the input into memory. fn read() -&gt; io::Result&lt;Vec&lt;u8&gt;&gt; { // Pre-allocate a buffer based on the input file size. let mut stdin = File::open("/dev/stdin")?; let size = stdin.metadata()?.len() as usize; let mut buf = Vec::with_capacity(size + 1); stdin.read_to_end(&amp;mut buf)?; Ok(buf) } fn main() { let mut seq = read().unwrap(); let ilen = seq.len(); // Remove headers and newlines. seq = regex!("&gt;[^\n]*\n|\n").replace_all(&amp;seq, &amp;b""[..]).into_owned(); let clen = seq.len(); // Search for occurrences of the following patterns: let variants = vec![ regex!("agggtaaa|tttaccct"), regex!("[cgt]gggtaaa|tttaccc[acg]"), regex!("a[act]ggtaaa|tttacc[agt]t"), regex!("ag[act]gtaaa|tttac[agt]ct"), regex!("agg[act]taaa|ttta[agt]cct"), regex!("aggg[acg]aaa|ttt[cgt]ccct"), regex!("agggt[cgt]aa|tt[acg]accct"), regex!("agggta[cgt]a|t[acg]taccct"), regex!("agggtaa[cgt]|[acg]ttaccct"), ]; // Count each pattern in parallel. Use an Arc (atomic reference-counted // pointer) to share the sequence between threads without copying it. let seq_arc = Arc::new(seq); let mut counts = vec![]; for variant in variants { let seq = seq_arc.clone(); let restr = variant.to_string(); let future = thread::spawn(move || variant.find_iter(&amp;seq).count()); counts.push((restr, future)); } // Replace the following patterns, one at a time: let substs = vec![ (regex!("tHa[Nt]"), &amp;b"&lt;4&gt;"[..]), (regex!("aND|caN|Ha[DS]|WaS"), &amp;b"&lt;3&gt;"[..]), (regex!("a[NSt]|BY"), &amp;b"&lt;2&gt;"[..]), (regex!("&lt;[^&gt;]*&gt;"), &amp;b"|"[..]), (regex!("\\|[^|][^|]*\\|"), &amp;b"-"[..]), ]; // Use Cow here to avoid one extra copy of the sequence, by borrowing from // the Arc during the first iteration. let mut seq = Cow::Borrowed(&amp;seq_arc[..]); // Perform the replacements in sequence: for (re, replacement) in substs { seq = Cow::Owned(re.replace_all(&amp;seq, replacement).into_owned()); } // Print the results: for (variant, count) in counts { println!("{} {}", variant, count.join().unwrap()); } println!("\n{}\n{}\n{}", ilen, clen, seq.len()); }
yeah, I didn't explain that well enough. Basically, because you don't really want to support, given a type `X`, `X() = thing;` (because it doesn't really make sense). You want to only really be able to assign into lvalues, not prvalues or xvalues - it's a good way to prevent accidental bugs, basically.
I find these much better - https://www.youtube.com/watch?v=r-ziiln0Hj8&amp;list=PL5jc9xFGsL8GrLgRN3NXDoIglex3ruLKV https://www.youtube.com/watch?v=Vc1RyqWFbiA&amp;list=PL5jc9xFGsL8G3y3ywuFSvOuNm3GjBwdkb https://www.youtube.com/watch?v=c_SucEr4iPw&amp;list=PL5jc9xFGsL8E_BJAbOw_DH6nWDxKtzBPA https://www.youtube.com/watch?v=U6mgsPqV32A&amp;list=PL5jc9xFGsL8FWtnZBeTqZBbniyw0uHyaH https://www.youtube.com/watch?v=LL8wkskDlbs&amp;list=PL5jc9xFGsL8E12so1wlMS0r0hTQoJL74M https://www.youtube.com/watch?v=7arYbAhu0aw&amp;list=PLE28375D4AC946CC3
It's funny that you produce an infinite recursion, but call for a bug in some other code.
Ah okay, your explanation makes a lot of sense. Thanks!
Unhandled exception in event handler or main entry function causes application crash. Now it shows error message dialog and user can work again. There is too many event handlers in each application to force developer handle exceptions in each of them. Disabling exception handling in Boost.UI isn't supported yet.
Another way to think of this would be to suppose we implement our own shared_ptr, and then we inline what OP is trying to do in his destructor. ~shared_ptr() { if (--control_block.refcount == 0) { auto tmp = *this; // assume copy-ctor increments control block's refcount } // then tmp's dtor triggers infinite recursion (UB) }
I can't promise you that this isn't UB, and I'm not sure if this will do what you want, but it does prevent the segfault on both gcc 5.4 and clang 3.8: if (auto ptr = std::weak_ptr&lt;test&gt;{a_test_object}.lock()) { // do whatever you were going to do with ptr } else { // a_test_object cannot be copied } 
It's a bit different: magic_get isn't intrusive: it also works with many types that haven't been specifically designed to work with it, making it a useful tool for things like serialization libraries. It has many use cases; I will try to find the relevant link again once I am on a proper computer.
IIRC no, you can't get fields by name. There is an open issue explaining that it isn't possible as is with the current state of C++.
You are correct of course, but that's exactly why I specified VS. I want to see if there are any open-source projects who were developed using VS and MSVC (with sln and pdb files) 
You are right, I should've been more specific. My question was more: if anyone has started an open source project using MSVC (which usually comes with VS) with sln files and pdb files. GCC and CLANG don't work with these files, so I assume any contributor will be using VS&amp;MSVC.
Exactly. That's why I'm asking if someone out there decided to start an open-source project without being cross-platform or just wanting to use VS due it's abilities along with the "built-in" MSVC&amp;MSBuild. Have you seen sln or pdb files in an open-source project?
Indeed :-)
I have no real use-case for std::variant yet, but that should be possible, too, of course. As for pattern matching, I am sure somebody else will take on the baton, right?
No arguing here. I say so myself. Sadly, this seems to be the only way to produce in-place names. Unless you know a different way?
With a single macro, the macro code would be way more complex. If I have to use macros, I prefer simple ones.
it's honestly not that terrible when you get over the ${ }. I used it to write some code generation boilerplate for my project, and you can get quite far without too much fuss. (although metaclasses can't come soon enough :p)
As far as I checked the [C++ source](http://benchmarksgame.alioth.debian.org/u64q/program.php?test=regexredux&amp;lang=gpp&amp;id=3) the regular expressions are built inside a loop and this is far from optimal (AFAIK). I could be wrong but the other example code provided here, from Rust at least, seems to build the regular expressions outside the loops.
Maybe I'm biased or old-school, but why use a C++17 feature when the alternative is known for ages and mostly everyone can read it by now? Plus if you want to separate the two things (initialization and checking), then I would simply put the initialization in its own line as you have shown on the slide before (24?). This new syntax for if/switch is one of my least favorite things in C++17, I just think it doesn't create clearer code if. But then again, it's such a small thing it is not too important and it was great talk showing a lot of the other really good things about C++17. :)
&gt; Now it shows error message dialog and user can work again. Not a good design. Exceptions should not be caught and accepted, they should be logged, and the system should gracefully shutdown, sending the log to the developer if possible. 
In the given example, it doesn't matter one way or the other, because it is a very small function with really just the `if constexpr` in it. If the function is slightly longer, having the ability to initialize a variable for the if-statement alone prevents polluting the outer scope. if (const auto m = getData(); useDefault) { // do the default thing } else { // something else, maybe as part of A/B testing } `m` is not used in the condition, but its scope is limited to the `if`/`else`. Not the greatest thing since sliced bread but nice, sometimes.
Great write-up, was a good conference. Hopefully there will be another one.
Eh. The front page of the project says that it's a toy precisely because of the use of macros, so you aren't saying anything the writer didn't know about. (Don't get me wrong: macros are the spawn of the devil, and have very few uses in modern C++. With C++17's `constexpr if` the last real usage for macros is the obscure [X Macro](https://en.wikipedia.org/wiki/X_Macro) technique, which is rarely necessary, and I hope that we'll soon figure out a way to do that in C++17 without macros at all.)
because "reasons" , IIRC Scott said mutable is a due to grammar limitations or because they forgot to update the grammar.
In my experience a well written code base often has those really small functions where it doesn't matter just like in your example. So it is either: * Small enough that the outer scope already ends after the `if` anyways. No need for new syntax. * Instead of `if(const auto a = ...; b) ...` you often have `if( const auto a = ...; a ) ...` which can already be reduced to `if( const auto a = ...) ...`. Again, no need for new syntax. * If you really have `if( const auto a = ...; b ) ...` you could always transform it into an explicit scope ` { const auto a = ...; if( b ) ... }` - which if distributed to multiple lines properly I find more readable and easier to reason about. Plus... no need for new syntax. Even if it is the last case, you don't *have* to use an additional scope in most cases. I often find this an acceptable style, and if there is an actual reason that an instance's scope must end sooner, an explicit actual scope communicates that much clearer.
Hmm...wouldn't that prevent certain useful idioms? For example, assignment to a proxy returned by an iterator. Code: ``` proxy iterator::operator*(); template&lt;class T&gt; proxy&amp; proxy::operator=(T&amp;&amp;); ``` 
I won't argue against that. It is such a small effect that I did not even mention it on my summary slide :-) There are at least a dozen things I would have preferred to be in the standard before `if` with initialization...
&gt; As much as I'd love a modern header only version of OpenSSL... That sounds like a security nightmare...
What about MSVC?
Seems to work https://godbolt.org/g/cguQwk
It's already in boost http://www.boost.org/doc/libs/1_58_0/libs/fusion/doc/html/fusion/container/map.html
Proxy types would have non-qualified `operator=`. The point isn't to *disallow* unqualified assignment operators, it's just that in the typical case `operator=` should be `&amp;`-qualified so it makes for a better default than unqualified.
If they were my types I would just change them, but fair enough for third party types. 
How is that similar? That's accessing by type, isn't it?
You probably want to qualify `operator=` with `&amp;&amp;` in those cases. Otherwise you end up with weird behavior like: std::vector&lt;bool&gt; v{true, false, true}; auto val = v[1]; val = true; // this assignment changes v assert(v[1] == false); // FAILS
Can somebody who watched this honestly say if is this one of those mental mazo... I mean exercises or is this useful?
It's not a big difference. Your tupple isn't better than structs (or std::tuple). It's not clear why this tuple need. How to construct i.e. vector of this tupples? Boost tuple looks better and much clearly. And macros in 2017...
Well, it's simple for you, but complex for the user. If you write it from scratch, it's very complex, but if you use a macro library, it's pretty simple, e.g. with BOOST_PP. 
You mean like this? namespace fields { struct name; struct age; } typedef map&lt; fusion::pair&lt;fields::name, std::string&gt; , fusion::pair&lt;fields::age, int&gt; &gt; person; Sure, this is nice, if you like to prepare those name-structs. Really. But the whole point of people returning tuples (if not for generic programming) is that they do not care to prepare the type. A named tuple can be created in-place. And yes, it uses macros, and yes, I would like to get rid of them, too :-)
 int main() { std::cout &lt;&lt; "Error found, source code too large to understand" &lt;&lt; std::endl; }
I disagree. If the user gets the tiniest bit wrong in calling a variadic macro, the error messages are even worse than template spew. At least for my brain ;-)
LLVM/Clang has saved me allot of time and effort on my wasm,android,ios,windows,osx and linux cross compiles. I literally use 99% of the same code and can easily compile, deploy(using in house methods) and debug very easily and quick! I'm so impressed by it, that I'll be releasing a video on how I use it soon. Oh and I use c++2a and modules-ts from the trunk.
Can you please upload your presentation slides in Cppcon GitHub repo? Thanks
Here's what I think is UB and why I think it is UB. - binding a const foo reference to a memory location where there is no foo, and then accessing foo through that reference, is undefined behavior no diagnostic required. - it follows that if u invoke copy ctor of object when the object is already destroyed, you get UB. - it is true that in the destructor "you may access the object but only in limited ways". For instance ~foo call ends lifetime of foo but not yet its members. Their destructors will run later. I dont think this "limited ways" extends to calling copy ctor, because it doesnt say so, and other parts of standard say otherwise.
I'll be honest I don't fully grok what you said (although it sounds correct). Guess I should re-watch that talk!
What about this case: ``` string_view container::operator[](string_view); ``` How do you make the return value unassignable? To prevent this: ``` c["key"] = "value"; // doesn't do what you think ``` 
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/79izxj/a_comprehensive_set_of_resources_for_modern_c/dp3mprs/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I think this is exactly the motivating case. If you qualified `string_view::operator=` with `&amp;`, this wouldn't compile.
Can`t wait on those videos!
`string_view const container::operator[](string_view);`, as in ye olde Meyers material?
Great minds think alike I guess: https://github.com/boostorg/beast/blob/caf144388778ecb59ff011d10c2afc38b62e5b1a/include/boost/beast/http/fields.hpp#L317
Unfortunately, I don't have the luxury of changing the `string_view` implementation :(
Const return value, aside from member function. ;-] `string_view const container::operator[](string_view) const;` I guess
You'll find plenty of open source C++ project with VS solution files checked in. Set up your own project now. Don't wait anymore! A few points: 1. You'd check in the .sln and the .vc(x?)proj files but not the .pdb files. .pdb files are regenerated when building the project. 2. Yes, you have more chances of having other people contribute to your project if its easier for them to do so but it's not a requirement. You may even ask nicely for CMakeLists.xt contribution so non-VS users can more easily get started. 3. I cannot emphasize this enough because there's a lot of developers that are obviously confused about software being open sourced. **What makes a project open source is one thing and one thing only: the [LICENSE](https://opensource.org/licenses) file.** Choose one, check it in and, boom, your project is now officially open sourced. Using Vim, Linux, gcc has nothing to do with source code being open sourced.
I think it's illegal to bind a new `const shared_ptr&lt;test&gt; &amp;`, as in when invoking the copy ctor, and then refer to the object through that reference, after the dtor is called. I don't think 6.8/1 gives you license to do that. 
Thanks, i will check it out rigth now
I ran this on our codebase. The unnecessary includes and forward declarations feature is pretty nice. I found way too many unused lines... I found a strange errormessage though: X is forward declared but needs to be #include Can someone give me an example how this would even compile. And Link.
Looks to me like a more-verbose, less intuitive version of Louis Dionne's Dyno[dyno ](https://github.com/ldionne/dyno) . Also, no clear usage example is shown, and I can't image of how code *should* use Polly.
&gt; oncurrent access to a shared_ptr object from multiple threads does not introduce a data race if the access is done exclusively via the functions in this section and the instance is passed as their first argument. That is about the identity of the `shared_ptr` instance itself. The reference count is part of the reference count control block, not the shared_ptr. In particular, copying a shared_ptr only takes by `const&amp;` but edits the reference count; the usual "multiple readers are safe" guarantee means the reference count needs to be atomically modified. &gt; the destructor is not in that section, so it may introduce a data race Yes, if you try to destroy a shared_ptr which is being touched by multiple threads, that is a data race. An independent `weak_ptr` instance isn't part of that `shared_ptr` you are destroying. It would be very strange if touching an unrelated standard library object could introduce data races. &gt; As for your third point, isn't the reference count is contained within the shared_ptr object? No.
Yes, this.
The cause of the problem is that in unix OSes, the main program can override symbols from a shared library. That is, the shared library will have its own memory allocated for the global variable, but the loader redirects the initialisation to initialize the copy in the main program instead. In Windows, the main program cannot redirect a DLL's symbols this way. You will therefore end up with two copies of the variable, with the main program using its copy, and the DLL using its own. This may work better if the global is just a string, maybe, but will also break if the global is intended to be a singleton. (We had both problems, the crash and the duplicated singleton, in our code already.)
I really hope that XRay talk has slides and video. Last time I checked its documentation was rather lacking, but the idea underlying the tool is awesome and needs to be widely accessible.
Wow. I ordered it the day it launched. Got it in australia 5 days later. 
This is supposed to be a tool to speed up compile times right?
[Sad :(](https://i.imgur.com/LTpYQZq.png) [And still waiting](https://i.imgur.com/CW7CVMb.png)
Yeah all the talks were recorded and should go up in the next few days. Was a good talk and Dean seemed like a nice guy.
Wow. Have you contacted amazon? 
Yup, generic message. Decided to contact them again just now though.
I cant say no to watch a Carruth video of C++ wich deals with that BIG issue (at least in my opinion) C++ devs has day after day, dont call me wrong, CMake does the job but i think we can and deserve somethin better and more standard
So, for anyone who's actually managed to roll their own toolschains for daily work: * How do you distribute it / keep it consistent across multiple developers on the same project? * Do you also use a sysroot image too on linux builds, to additionally dodge an variance among developer's build machines? I've looked at doing something like the above, particularly since we target NDK too for some of our code. Would like to hear about how other devs have their build systems standardised for non-host toolchains
while i understood that not_null can be used to to implement _true_ value semantics for pointer types, I didn't quite understand where reference_wrapper was lacking in this regard, especially as members of product types. Could someone explain?
One betterment: a function defined in a source file but not declared in a header file gets flagged as an issue. I don't think it should. There's nothing wrong with that.
There's a few open issues dating back 2 years about forward declarations and includes. Unfortunately I'm not sure either why code can compile. 
It's a rare thing to do, usually you just forgot static or an anonymous namespace.
&gt; There's nothing wrong with that. there sure is. If it's in a single source file and not referenced in any headers it should be marked as static as to not be visible from outside.
I don’t know the specifics of this tool, but here is my guess for that rule: in Chrome a function will either be declared on both header and source file, or it will belong to an anonymous namespace to clearly indicate that it belongs to a specific translation unit. This is a stye choice, as far as I know. The clear benefit of such style is that you can ignore these functions wholesale when generating code documentation, since most tools will offer you an option to hide anonymous namespace’s details.
You could commit the compiler(s) into version control this way you ensure that everyone is using the same compilers. It would also mean that you can check out any old revision and know the compiler that was used then should work with the code provided. However that could also be an issue with different operating systems/distros, and any dependencies the compiler might have.
So this is the old "how good is the regex library" test. Be interesting to try implementing it using https://github.com/hanickadot/compile-time-regular-expressions.
Why are you even trying to do this in the first place. Literally every word of your title contributes to it being a bigger red flag.
Man, I want to know the specs of that computer. Just after seeing that talk I went ahead to recompile a clang refactoring I have at work for some new code we have and it took just above 1 hour with an i7.
Have you looked at the [intermodule singleton](http://www.boost.org/doc/libs/1_65_1/boost/interprocess/detail/intermodule_singleton.hpp) from Boost.Interprocess?
&gt; `**void** main()` heresy
&gt;This is a style choice, as far as I know. The clear benefit of such style is that you can ignore these functions wholesale when generating code documentation, since most tools will offer you an option to hide anonymous namespace’s details. It's not really a style choice and that's not exactly the reason why. If you define the same symbol in two different translation units, even if the symbols aren't exposed outside the translation unit, they will still conflict. If you have "int foo() { ... }" defined in two source files you'll get a compile error. Putting |foo| in an anonymous namespace or adding the keyword static, gives the symbols translation unit local storage so they won't conflict anymore.
&gt; you'll get a compile error. So... We already have a tool to detect that situation, then?
It sort of bothers me that it isn't Folly.Polly.
When I said a style choice is because some people won’t care about the risk of a future build break while others will... meanwhile the compile doesn’t care either way, until you actually duplicate the symbol. In that sense it is a style choice...
This reminds me of [Boost.TypeErasure](http://www.boost.org/doc/libs/release/doc/html/boost_typeerasure.html).
So this is a question that has bothered me for a while. In the C++ standard (3.6.1) it says something along the lines of main() must return type int. If we assume that the purpose of this return is to be consumed by whichever system first invoked the program, then what is the purpose of this return value in the context of a freestanding environment (bare-metal)? I mean technically there is no "system" to return the value to. This would be a use-case for which void main() would be a legal statement in the C standard. - What is the rationale for C++ (3.6.1) enforcing an int return type for all programs _even_ those running on a freestanding environment?
Eventually, might be after a 2nd library is built and linked to a application.
Just to give a shoutout: I've never used Sourcetrail itself, but I use and contribute to their Visual Studio plugin ([github](https://github.com/CoatiSoftware/vs-sourcetrail), [marketplace](https://marketplace.visualstudio.com/items?itemName=vs-publisher-1208751.SourcetrailExtension)) that generates clang compilation databases. AFAIK that extension is the only one that does what it does, and it works pretty well; I can clang-tidy our giant codebase and pretty soon^^TM I'll probably be able to whip include-what-you-use into shape. I think it could be a lot faster for what I use it with but profiling puts all the slowness in the VS API (EnvDTE stuff). I experimented some with using lower-level interfaces but got busy at work and haven't gotten back to it yet. I'd also prefer different things about the options, but I can get it to do what I want as-is.
thank you for this explanation , made it quite clear
What kind of CPU beast does he have in that laptop? I want one.
His slide and discussion of Red Hat Enterprise Linux is a bit misleading. While RHEL 7 does come with gcc 4.8 (no surprise given it was released June 2014), Red Hat does make newer compiler versions available to developers with their Red Hat Developer Toolset. The latest release of RHDT (7) gives developers gcc 7.1.1 as well as access to a preview version of clang/llvm 4.0.1 
Ran this on my code base. I handle dependencies as git submodules, so they're all in my working tree and cppclean ran on all of them. It spit out a bunch of warnings, then a stack trace: ```
That was the topic of much discussion at the conference :). He was building on a 64-core machine at his home I think. 4.5 minutes for a clean build (ccache disabled, he assured us!).
spam
!remove
OP, A human moderator (u/blelbach) has marked your post for deletion because it is not appropriate for r/cpp. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/79t71y/convert_string_into_integer_in_c_without_using/dp4n3m5/,%20was%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Needs to try. What about parsing speed in particular and about user interface responsibility in general on large code bases? Say, Linux kernel source? Windows source? Mentioned Understand was very slow and wasn't able to parse such amount of code. SourceInsight can (not an ad, it is just the only tool (well, except such tools like grep or vim) which was able to work with large code). What about tweaking of parsing? Like defining custom macros to help parser understand some tricky code? What languages does Sourcetrail support? (Haven't been at its website yet)
&gt;Why is forward declaring preferable to including? It's cheaper. The compiler doesn't need to parse the definition of the class, plus the definitions of all the member types, plus everything else dragged in by the header. It just needs to know how to mangle the names.
Alright. I'm trying it out now
[removed]
it supports Java and C++ for now, about macros: yes, good point, in Visual Studio there's `__INTELLISENSE__` def. So if intellisense cannot cope with some parts of your code (to present errors when editing), you can use this macro to provide 'easier' code.... so maybe we could have `__SOURCETRAIL__` :)
thanks for the contribution! without this plugin it would be painful to setup the project (at least for me). Now you can do it with one or two mouse clicks.
Thanks for the pointers.
Number 10 made me legitimately laugh out loud. Others were frightening, yes.. but that was FUCKING HILARIOUS.
I like this picking on MFC (#6). The real horror starts when I have to maintain such code from time to time! Unfortunately, Qt is only slightly better when it comes to 'glorious and idiomatic' C++ though (using it nonetheless) ...
Interesting and informative. I've encountered many mysterious linking problem in my time; and I've rarely understood precisely what the issue is, and why the fix works (the fix usually being to change the order of linking). So the talk helped with that. The take-home message seems to be to declare globals (if any) using `inline` in the header. That seems like an easy way to avoid some problems with linking.
AMD EPYC x2
I have seen them both being checked into version control, and on a local package server. In one case the build system checked the correct compiler was being used. 
Did you read the post? I know it’s horrible but I’m interested in what approach you’d take - ie. Not using a singleton because they suck but what would you do instead?
Thanks I’ll check it out.
how did LLVM build so fast on his machine? It takes significantly longer for me :(
Also [Dynamix](https://github.com/iboB/dynamix)
Compile error is the best case. Worst case is, you load a dynamic library at runtime which exposes "hidden" function `foo()`, then another DLL which happens to expose and use an unrelated function of the same name, and suddenly you get weird crashes because it's calling the first one instead of the second one. 
&gt; you can use this macro to provide 'easier' code.... Well, I thought about the opposite: you tell your tool "Just expand this weird macro to that" (or to nothing). It also allows to skip unknown keywords. Anyway, thanks. Understanding Java is useful too sometimes.
Great talk if you want to know how to use [pmf::polymorphic_allocator](http://en.cppreference.com/w/cpp/memory/polymorphic_allocator), and how to make your custom container allocator aware
&gt; Needs to try. Well, just tried the Windows version. Very raw at the moment. 1) Why it can't be run without installing? It crashes at start. 2) Why it is built as console executable if it is not? 3) Very inconvenient project setup dialogs. May be it has some purpose or mental model, no idea. 4) You can move status panel vertically, but you can't move its columns horizontally (to view the full path of files failed to parse). Or at least, show the end of file path, not the beginning. 5) Very slow parsing. It spawns parsers by CPU cores number, but it takes all CPU power and makes other apps slow. Spawn less instances or limit CPU usage, because… 6) Parsing is very slow. 16 minutes to parse 4500 C/C++ files by 8 cores (it is only 9% of WinNT source). SourceInsight spends 2.5 minutes to parse 52000 files by one cpu core. 7) Interrupted at 10% of parsing, program crashed during saving indexed results. So, summarizing above - nice idea, good aim and it is worth to try it again in Beta status. 
Excellent blog post. Number 7 has to be fake... I mean it has to be! 
Although I see some people in the audence agreeing with the speaker(so I guess he knows what he is talking about) I found this talk totally useless. I did not understand the problem or the solution, and to make it even more fun I did not even care since it seems academic... So I wish this talk used a bit more time to introduce the problems that should be fixed. note: I am not a C++ expert so talk maybe is just not for me. 
Thanks for you feedback! 1) Because it's currently not a portable build, but we can make it one if demanded: https://github.com/CoatiSoftware/SourcetrailBugTracker/issues/505 2) Because it has a commandline API for indexing and this is our current solution to do both with one executable: https://www.sourcetrail.com/documentation/#COMMANDLINE 3) Real-world projects can be very diverse and over time there had to be more and more options added. Do you have any specific suggestions for improvement? 4) It's possible to change column width, but only in the title bar for some reason: https://github.com/CoatiSoftware/SourcetrailBugTracker/issues/506 5 + 6) Our parser is based on clang libTooling which allows for very detailed static analysis by building the full abstract syntax tree (AST), just like a compiler. This gives us information like which specializations of a template exist. About 66% of indexing time is used by clang libTooling to build the AST, the rest is our traversal and data storing. We hope that we can improve indexing time further, but there is a limit to that. If you want less CPU usage, please change the number of "Indexer Threads" in the preferences. 7) Can you please provide us a log file or crash report? support@sourcetrail.com
OK, thanks for clarifying. Where can I find this log file?
Really? I thought it was the least interesting one. 
I have one of my own. This code running on production server to sign sensitive data in the company I work for: DLL_EXPORT(LPCTSTR) GetSHA1HashString(LPCTSTR str1) { TCHAR szBuf[80]; // Do some stuff if (!str1) str1 = _T(""); try { CString hash = _T(""); // generates the sha1 in a very inefficient way. hash = szBuf; return hash; } catch(...) { return FALSE; } } There's so much wrong with this function, and I'm not even posting the code that actually does the SHA1 generation, which is ridden with bugs and inefficiencies. This code was written by a JS developer who wanted to hack his way through a DLL used in the server and who couldn't be bothered to ask a C++ developer to do it because, you know, *C++ ain't that hard as you guys make it to be, and you are not the only ones that can write it. Stop think that you are better than us*. I've tried a million times to explain what is wrong with this code, but no one cares because they don't understand the idea of use after free and going out of scope, so they think I'm just trying to be a jerk about their code. Since then, this dll has been ejected from the C++ codebase that I maintain, and I make sure none of the web developers have anything to do with the C++ codebase (our code is cross-platform, runs on x86-64 and Arm, has tests and builds for asan, lsan, msan and ubsan constantly. It is also written in C++14, so obviously this snippet above cannot stay with the main codebase).
Got the same backtrace, Maybe it is related to the error message before: Exception while processing 'foo.h': unexpected token: Token(u'false', 756, 761)
I don’t know, the article didn’t live up to the title... They are not stories. They are examples of bad code. The kind of code I imagine a first year prof gets submitted to them as assignments all year. I don’t have a good example off hand, but I was expecting more...
This is more like it, a story! And a good one at that. JS devs working inside my code base... nightmares for weeks!
Awesome video overload. I haven’t caught up on cppcon videos and now there are some great looking videos from pacific c++ already! I haven’t been disappointed by one of Jason’s talks yet, looking forward to this one
Some of these were a massive stretch, but it does give a nice and short overview of some less-known language corners
In the spirit of Halloween, massive ridiculous stretching is permitted!
No. It hints to problems that might slow down overall development - as the title says. A few of the possible fixes might also improve compiletime.
Then what is it supposed to do? It removes unused includes.
The script itself does not do that.
It might be that the source file which actually uses that header producing the warning, happens to include the required header, which declares the predeclared class, before including the problematic one. Everything just works because including order happens to be just right.
Let's forget the template for a moment. If the singleton is returning a static in the main program, how is that function being called by the DLL without creating another static? You could pass the function pointer to the DLL, but of course it would be better to just pass the data to the DLL in the first place. This brings us to what to actually do. Pass the data to the DLL. Over complicating things like this is begging to be bitten by something you don't expect.
Does this compile? Is FALSE (really int, right?) implicitly convertible to LPCTSTR (really const char*) ? Seems misleading but I guess it's like returning null in a very un-expressive way. 
&gt; When I said a style choice is because some people won’t care about the possible future ODR violation while others will... FTFY The build only breaks if you are lucky.
I don't think, that's the case. The classes were only used as pointers or templatespecializations (eg. std::vector). On a few occasions I even found it the other way around, telling that I don't need to include something because it was already included in our precompiled header. 
What are things it does that don't involve compile time?
From the readme: - Classes with virtual methods, no virtual destructor, and no bases - Global/static data that are potential problems when using threads More can bei found in the readme.
That's not really the worst part, although it is also stupid. The main problem is that this Windows type named CString has an implicit conversion to char* as well. This is actually a user type with an overload as in: operator const char*() const However, as you would imagine, the object is being destroyed as you are leaving the scope, so a dangling pointer for every single SHA1 generation they do in a daily basis... basically thousands of them.
Please enable "console and file logging" in the preferences. Log file can be found via the menu at "Help -&gt; Show Log Folder".
Ok, but where does it say the control block is thread-safe? As far as I can tell everyone just assumes you can have a shared_ptr to the same object in two threads, and releasing it in one, the other, or both at any time is perfectly fine. That implies the control block is therefore thread-safe, but I have been unable to locate any words to that effect in the copy of the standard I looked at. Now, I'm willing to accept the sentence you listed for this, but I find that sentence extremely difficult to understand. "If a modification can introduce a data race, it is not observable in the use count" - something like that? But that seems opposed to the idea of thread safety... 
Why another "reflection"-like library for enums? There's [Better Enums](https://aantron.github.io/better-enums) already. I suggest that you explain the differences/advantages to other libraries in the `README.md` at least...
Must be missing something: Example 1: What does it have to do with filesystem ? What is Fs::Value::Register ? Example 6: What is so bad with this code (apart being pretty standard ugly MFC code) ? I've seen much worse. Much much worse.
It is undefined because the standard never says that they are dereferenceable. From [[iterator.requirements.general]/7](http://eel.is/c++draft/iterator.requirements.general#7) &gt; Just as a regular pointer to an array guarantees that there is a pointer value pointing past the last element of the array, so for any iterator type there is an iterator value that points past the last element of a corresponding sequence. These values are called past-the-end values. **Values of an iterator i for which the expression \*i is defined are called dereferenceable. The library never assumes that past-the-end values are dereferenceable** [...]. You already quoted [[container.requirements.general]/6](http://eel.is/c++draft/container.requirements#general-6) stating that `.end()` returns such a past-the-end iterator.
dereferencing anything that points to nothing is UB. It's better to say: sort(vec.data(), vec.data() + vec.size());
This is the complete paragraph: "For purposes of determining the presence of a data race, member functions shall access and modify only the shared_ptr and weak_ptr objects themselves and not objects they refer to. Changes in use_count() do not reflect modifications that can introduce data races." It distinguishes between modifications of the `shared_ptr` object itself (modifications CAN introduce data races) and modifications of the control block (use count -- modifications do NOT introduce races).
&gt; \#include &lt; stddef.h &gt; That's a deprecated C-compatibility header, always use `cstddef` and the other C++-variants instead.
[quicktype](https://quicktype.io?l=cpp) infers types from JSON sample data, then outputs C++ structs and converters (Go, Java, C#, TypeScript and other output languages are also supported). quicktype gracefully handles many cases that similar tools fail on, such as non-homogeneous arrays and weird property names. It also tries to detect when JSON objects are better represented as maps, rather than fixed structs. We also have a CLI, which supports fetching JSON from HTTP, as well as generating code from JSON Schema: $ npm install -g quicktype $ quicktype -o LatestBlock.cpp https://blockchain.info/latestblock We would love your feedback. Is this useful to you? What can we improve? Please let us know if there's anything we can do to generate more idiomatic code, or what additional options you'd like to see! quicktype is [Open Source](https://github.com/quicktype/quicktype), and we'd love contributions. We even made [a series of videos](https://www.youtube.com/watch?v=M7DorNOxRnU) showing how we implemented C++ support. Thank you! 
Thanks for reading the article in depth enough to spot that! It's not deprecated, it's just the C header. There's a range of opinions on the subject of which to include, with arguments for both. I'm not too bothered either way, but [many of the libs I use already include &lt;stddef.h&gt;](https://github.com/ocornut/imgui/blob/master/imgui.h), and it wouldn't change any of the discussion in the article about optimizing compile times with /d2cgsummary. 
&gt; It's not deprecated, it's just the C header. It **is** deprecated in C++ and is only provided for backwards compatibility with C. One should always use the C++-variants of the C headers in C++. See annex D.5 [depr.c.headers] from [the draft N4700](https://github.com/cplusplus/draft/blob/master/papers/n4700.pdf).
I believe the standard says that you can have a pointer pointing past the last element of an array (for the sake of comparisons), but dereferencing that pointer is UB. So it has to be the same with iterators.
There are also Zombie names in the language - things deprecated, but the names are kept for implementors that want to keep backwards compatibility. I think the section title in the standard actually uses 'zombie' (and the index includes an interesting link to it)
I stand corrected, thanks! However I'll continue to use stddef.h and if I get a chance to discuss this with a member of the committee I would encourage continued compatibility with C as much as possible.
&gt; However I'll continue to use stddef.h But why? It's not like the file shown in the blog post, which includes the deprecated header, would ever be compatible with C, as it uses many C++ features. I see no good reason, why you wouldn't want to use the non-deprecated header instead (which is `cstddef`).
Although technically inapplicable here, the C (not C++) standard gives special dispensation (so to speak) for the `&amp;*` sequence, so taking an address one past the end of an array, dereferencing it, and immediately taking the address does give defined results, even though dereferencing it under any other circumstances would lead to UB. I'm not sure though: given that the C++ standard references the C standard as a base, it's just barely possible that a C++ compiler is required to respect this as well.
Because cstddef declares more than stddef.h does, and stddef.h is already included by many libraries I use so this reduces the number of header files.
You should instead be looking for an excerpt that says such behavior is defined. If there is nothing that explicitly or implicitly defines the behavior, well, you can obviously conclude that it's *undefined*.
http://wg21.link/cwg232 seems relevant to quote: &gt; dereferencing a pointer to the end of an array should be allowed as long as the value is not used
The construction cannot work in C++ if at least one of the operators is overloaded.
That wouldn't be acceptable ;-) Is it about the regex library or is it about the nitty gritty details of how the regex library is used?
so, it's a json parser generator?
It's really just a showcase of the power and expressiveness of the JSON C++ library, https://github.com/nlohmann/json.
I guess I ask this after all... Since no-one asks anyhthing :) How this catamorphism is useful in C++? I read https://en.wikipedia.org/wiki/Catamorphism , but being unfamiliar with Haskell, I understood a little...
It certainly showcases [nlohmann/json](https://github.com/nlohmann/json), but it goes far beyond what that library does–we're focused on eliminating the drudgery of manually authoring all of your typed models, then writing the `to_json` and `from_json` functions, which is pretty tedious work. For example, reading our sample GitHub data from the GitHub API requires writing 700 lines of really monotonous C++ if you want typed structs–quicktype's goal is to automate this. You can learn more about what quicktype does [on our blog](http://blog.quicktype.io/debut/).
Not quite. The code quicktype generates uses [nlohmann/json](https://github.com/nlohmann/json) to parse JSON, but then it converts that dynamically typed JSON to static data structures that quicktype inferred from the JSON you gave it.
Note that this is the one "deprecation" that's been around forever but isn't actually "deprecated" by any of the real implementations; all known implementations still put the C things in the global namespace. There are a few exceptions, e.g. `&lt;cmath&gt;` declares more overloads of things that can subtly change program behavior when it is included vs. `&lt;math.h&gt;`. 
Note that MSVC's iterators in debug mode will detect this UB. (Our container iterators are never pointers.)
The section title was my doing - however, the brilliant index entries (there are at least two) were not. Edit: Also, the zombie names are reserved names of removed things, not just deprecated.
Note that you can do this in a way that doesn't trigger UB: ``` sort(container.data(), container.data() + container.size()); ``` Dereferencing an end iterator is UB, but forming an end pointer is not. (Dereferencing that end pointer and attempting to use the value would still be UB) 
Thanks for the clarification - indeed I've not seen any 'deprecated' warnings on the compilers I test against.
This talk isn't really about ref-qualifiers and /u/ubsan only touches this tangentially at the end. They aren't that hard, so if you want to grok them start by reading cppreference and then read some library code that uses them (range-v3, hana, std library, stl2, ...) and ask questions.
1. Soppose you have a non forever fixed json and have your field naming convention (`m_*`, or `*_`). It's ok to manually adopt the code once, but when data structure changes you have to regenerate the code and all you modifications are gone? 2. What about optional fields? 3. When I have a data structure that is used as a field value in a two different json sample (I know they are the same) so I I end up with two code pieces with the same type name for both of them (a duplicate) or may be with two distinct names (if the different names for fields were used). I guess 2 and 3 can be resolved by JSON Schema?
Quite literally undefined behaviour.
`cstddef` defines a tiny handful of extra things - which are important to C++, like `nullptr_t` - and otherwise just includes `stddef.h`. There is no "extra" header included here; you're almost 100% guaranteed to be pulling in `cstddef` from some other header anyway in C++. Prefer to use `cstddef`. It's more correct, no slower, and it could not possibly matter less if other C++ libraries you use erroneously include `stddef.h` instead.
&gt; I've seen this code in production and on Stepanov's website (code co-authored by Stroustrup): Just for the sake of stating the obvious: there is a _lot_ of code in production that is dead wrong and even the C++ luminaries are (sometimes) fallible. :)
For pointers, yes. For `vector::iterator`, no.
1. We have a nice options framework that would make it very easy to add custom field suffixes and prefixes. 2. Optional fields are detected and represented with your choice of `std::unique_ptr` or `boost::optional`. 3. On the CLI, you can use the `src-urls` option to unify multiple samples into a coherent set of models. We don't expose this in the web UI yet, but it will do what you desire!
Interesting. Thanks for sharing. I will have to look into the windows behaviour that you described, as this may very well be a relevant issue. 
I think you need to be more explicit about your use cases, who the competitors are in this space, and why I would prefer quicktype. Let me get the ball rolling by saying that I would never use this, if I didn't have other languages to worry about than C++. Having an npm dependency and having to generate code externally to my build chain just is not worth it, for something I can very easily do with macros: DECLARE_REFLECTIVE_STRUCT(Foo, (string, greeting), (vector&lt;string&gt;, instructions)); from_json(const json&amp; j, Foo &amp; f) { reflect(f, [&amp;] (const char * name, auto &amp; x) { x = j.at(name)(); } } Something basically along these lines is pretty easy to do. The implementation of my reflection library is like 100 lines of code. Another major downside of your approach is that I don't see any way to nest custom types. For example, I can now do: DECLARE_REFLECTIVE_STRUCT(Bar, (Foo, f), (int, x)); And this will also deserialize from a json, where the Foo is nested. I don't see how I would be able to achieve that with what you have. And that's a huge part of the power of having a map from a recursive data format to native types. So all that said, I will tell you that your main competitor is protobuf, and protobuf style alternatives like capn proto. But these typically offer much more advanced things, like being able to version structs in backwards compatible ways, and so on. So I'm not sure what the target here is; maybe people using multiple languages but with very simple requirements, and then the advantage is that this is easier to use than protobuf? Seems a bit niche, but whatever the target is you should make it clear.
Isn't it still `UB` in 100% of cases with vector, since you can only do pointer arithmetic on arrays (allocated by `new[]`), but, if I'm not mistaken' vector uses `new`.
&gt; Is FALSE (really int, right?) implicitly convertible to LPCTSTR (really const char*) ? FALSE is `0` so the intent was indeed just `return NULL;`.
You can do pointer arithmetic on any pointer, as long as you don't over-/underflow the buffer. `int i; int* p = (&amp;i)+1;` is perfectly valid. In the case of `std::vector` it uses `std::allocator::allocate`, which in turn calls `::operator new(bytes)` (not `new T`), to allocate, however many bytes were asked for, and you can do whatever you want with the returned pointer, including pointer arithmetic.
If this were the case `vector` would be unimplementable. There are some bugs in the object model and this may be one of them.
Look at the line numbers -- nearly 750 LOC is a lot of wasted effort...
Maybe I misunderstand your point about nested types, but quicktype will automatically generate types and code for nested JSON objects. As for the target audience: at least anybody who needs to consume or produce JSON for which there is no existing C++ (C#/Java/Go/...) binding code. Regarding competitors: there are many tools like this for other programming languages, and they're all severely limited compared to quicktype. See, for example [JSON-to-Go](https://mholt.github.io/json-to-go/). I haven't found such a tools for C++. And yes, you can declare all those structs yourself. With your macros it'll be less work. With quicktype it'll be even less ;-)
Thanks for the feedback! We go into greater detail about the use case [on our blog](http://blog.quicktype.io/first-look/), but honestly we're still trying to figure out how people could use a tool like this, or even if they would. 1. Yes, the dependency on `node` is a drag if that's not part of your stack. We're looking into static compilation that would remove this dependency. 2. How does your macro look for our `github-events.json` sample? What if your JSON properties are illegal identifiers in C++? What about heterogenous data, which is common in JSON? All of this is tedious to implement manually. 3. I'm not sure what you mean by not supporting nested types – most of our samples demonstrate this (e.g. in `pokedex.json`, the `pokedex` has `pokemon` which have `evolution`–we even support recursive data structures). Perhaps I don't catch what you mean. 4. I will check out protobuf more carefully, thanks for the tip!
To add to the point /u/davidsiegel made on your point 1: We really want to make quicktype so that you don't have to manually edit the generated code. If you feel like there's something missing in that regard, please [post an issue](https://github.com/quicktype/quicktype/issues) - we want to know.
I'm actually more of a C programmer who uses as little as I can of C++ to pragmatically make programs, similar to most of the game developer programmers I've worked with. Although some of the code I work with now uses C++11, for comparability reasons I don't update libraries to use new features such as nullptr_t just because they exist. I don't think using stddef.h is erroneous, and I don't think cstddef is more correct, because I don't need any of the things in cstddef for the code in question.
Excellent! I only found one index entry. Can you link to the ones you know? Added: http://videocortex.io/2017/Bestiary/#️-zombies--brains
It seems like it will handle nesting, for certain specific cases. But there are cases that are just fundamentally ambiguous. Given: { "foo": 0, "bar": { "hello": false, "there": true } } Does this become: struct Something { int foo; Bar bar; }; Where struct Bar { bool hello; bool there; } Or, does it become: struct Something { int foo; map&lt;string, bool&gt; bar; }; ? How can you tell the difference? Either could be correct, in general. In general, this whole approach of backing out the schema is IMHO highly heuristic, and it's not something I would trust. Can you assume that something should be an `int` and not `optional&lt;int&gt;` because it happens to be present in every case in your limited dataset? Or the reverse? What about the case above? Honestly, the only thing I can think of is using this for cases where the data you can feed into quicktype is *all* the data. That might happen a lot in ad hoc processing but I can't imagine where this would come up in real software engineering. You said that quicktype is less work than my macros. That's sort of true; if you never want to specify or maintain the schema explicitly, it's less work. The problem is that the output of quicktype is not fit for maintenance, because it itself repeats the types in many places. So once you start using quicktype, you have to keep using it. That means that if the output schema isn't what you want, you need to add something to the json to make it what you want. And you need to track that, to be read to use it again, the next time you have this issue. This is all very roundabout. Personally I'd rather just specify my schema by hand, as long as I don't have to repeat anything, which is exactly what my macros do. But even if I wanted to use something like quicktype, I would have it output something like the macro that I showed you. That way, the output of quicktype doesn't contain repetition, and is itself easily maintainable. In that case quicktype can be used as a one-off to generate an initial schema, which can be tuned as necessary.
&gt; (Our container iterators are never pointers.) That actually makes a lot of sense. Would I be correct in the assumption that in release mode the iterator machinery is erased by the compiler and it acts as a normal pointer. (If that makes any sense)
I heard that this is exactly the case: there are no `100%` conforming `std::vector` implementations. I.e. remember [this thread](https://www.reddit.com/r/cpp/comments/67xpeg/is_there_an_obstacle_to_implementing_vector_in/)? I think this is the related [CWG issue](http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_active.html#2182).
Yes, the less we repeat ourselves, even in generated code, the better—perhaps we can borrow some ideas from your macro to factor our output even further. We share your opinion that quicktype uses heuristics! We use them for struct unification, map identification, and even our integer inference uses the 'heursitic' that if we don't see a double (or any non-integer type), that the numeric type is integer. All of these assumptions are fallible so quicktype requires some supervision and a little confidence that your JSON values will not spontaneously change type relative to the samples you provide. As you said, we can't get 100% confidence unless the sample and the runtime JSON value are identical. Currently, out map heuristic is very crude–if a type has over 20 properties, all of the same type, it becomes a map. This can be overridden in schema or as a CLI flag, and we [plan to build some better heuristics](https://github.com/quicktype/quicktype/issues/3).
Yes. Given the presence of a spec that says the Earth should not be spinning, I encourage users observing the Earth in fact spinning to treat that as a bug in the spec :)
For 2, you could extend the macro to also specify the property name, I agree that has downsides of its own. If the data is heterogeneous then you have to solve it the way you always solve it in C++: with either a base clase, or std::variant. I didn't see you use either in your github-event.json solution so I'm a bit confused. You can use `optional` with my macro, that part is not a problem (not really a good solution for heterogeneous data though). Re 3. See my post above; you support nested types in certain situations but not others. You basically support nested types by never considering the possibility that something should actually be deserialized as an unordered_map&lt;string, Foo&gt;. I'm no longer sure that protobuf is similar to what you're doing. I admit that I simply don't have use cases like this. That said, I think you are still better off using quicktype to generate code that leverages something like my macro. That will make the generated code nearly free of repetition, and allow it to be maintained directly.
For proper integration of quicktype into your development process I would always suggest using it to generate a JSON Schema from your sample JSON first, and then generating code from that Schema. The Schema is what you maintain. It doesn't repeat itself, and there is additional tooling for it.
There were bugs in previous compiler releases but AFAIK Eric Brumer and Co. have fixed them all.
Right, but then you may as well just generate the macro code instead. Then I can maintain the macro call, and I need zero additional tools.
&gt; int i; int* p = (&amp;i)+1; is perfectly valid. Because &gt;For the purposes of these operators, a pointer to an object that is not an element of an array behaves the same as a pointer to the first element of an array of length one with the type of the object as its element type. But for `std::vector v(42)` it looks like `v.data() + 2` is `UB`, since `data()` cannot be an array according to `C++` std and vector requirements (since it doesn't acquire it's memory from `new T[N]`). &gt;you can do whatever you want with the returned pointer, including pointer arithmetic. Not according to the standard, if [I read it correctly](https://timsong-cpp.github.io/cppwp/expr.add#4): &gt;When an expression that has integral type is added to or subtracted from a pointer, the result has the type of the pointer operand. If the expression P points to element x[i] of an array object x with n elements, the expressions P + J and J + P (where J has the value j) point to the (possibly-hypothetical) element ... otherwise, the behavior is undefined. Likewise, the expression P - J points to the (possibly-hypothetical) element ... otherwise, the behavior is undefined. 
Yes, I think the macro has a lot of merit and would make a great option in our output (or even as default behavior). Our `kitchen-sink.json` sample exercises many of the corner cases, and shows some of the limitations we still face. For example, we have no notion of tuples–they become arrays of unions–yuck! You're right that we get the map case wrong a lot. The `spotify-album.json` sample contains a few map values with only a few keys and we generate structs rather than maps. We don't yet have a quick way to correct this, but we're looking for ideas (too bad JSON doesn't support comments, that might be a nice quick-and-dirty way to annotate the sample)!
Ah, didn't notice those. I assumed only a bit of code got bypassed. 
I guess it's just a matter of approach. For me, if someone dumped a bunch of this data on me, and told me "do X", I would start with the requirements for X. If I have to make a histogram of pokemon weaknesses, I'll start by specifying (with my macro) a struct that can be deserialized that captures that specific data. If I'm then told "do Y", I'll add things to the struct, and continue to share that struct between the two use cases. And so it goes. It is true that quicktype allows you to parse out all the data and guess it's type, but to actually use any field in a real program I have to think about what it's type is, and once I figure it out I may as well write it out. But, obviously it's quite likely you've encountered use cases that I've never encountered, and vice versa, which is why we think about the problem differently. The edge cases with tuples and maps are really rough, no doubt. This post attracted my attention because I've been using my macro + nlohman/json approach very heavily for configuration, and it's been working very well. That's a much easier problem as I'm specifying the schema I need, of course. Best of luck with improving your heuristics on this stuff!
Wonderful, thanks!
[LEMON](http://lemon.cs.elte.hu/trac/lemon) is a good alternative for the BGL.
Indeed there is this case from "The C++ Programming Language" 4th edition: void f2() { std::string s = "but I have heard it works even if you don't believe in it" ; s.replace(0, 4, "" ).replace( s.find( "even" ), 4, "only" ) .replace( s.find( " don't" ), 6, "" ); assert( s == "I have heard it works only if you believe in it" ) ; } which was used as one of the example for the [Refining Expression Evaluation Order for Idiomatic C++ proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0145r3.pdf). Understanding [all the details behind why it breaks was a fascinating exercise](https://stackoverflow.com/q/27158812/1708801).
Sorry for the late reply, but nope. This is actually what I tried at first. Then I tried to download and install symbol database from Microsoft. Still no luck. Our project written in Qt is really large with tons of symbols, templated code, etc. Either CDB just can't handle it, or it is its Qt Creator interface, I can't tell. But it is slow, very very slow. And unreliable. Debugging in templates works only sometimes, lot of times the debugger just breaks in random code instead of requested one. When starting the debugging with breakpoint preset in templated code, the process will just hang in random LoadLibrary() calls during the startup. The same applies for current version (4.4.1). Debugging in Visual Studio just works very nice and fast for the same project. But like I said, all the other stuff in VS is bad and switching between 2 IDEs is also annoying.
It wouldn't have occurred to me to try that, but it makes sense that it works. Good to know if you (situationally) prefer that syntax I suppose.
But wouldn't change the fact that it's is a bug in implementation. And no certainty that it will be fixed or even if this implementation is still (or was ever) maintained =)
I absolutely love your presentation! Including the emoji's. In addition I also learnt a lot! Thanks!
&gt; similar to most of the game developer programmers I've worked with Bah, _those_ guys. :p They are a small fraction of the modern game development industry, fwiw. :)
Much obliged. 🎃🕷🦇
&gt; Note that this is the one "deprecation" that's been around forever but isn't actually "deprecated" by any of the real implementations; all known implementations still put the C things in the global namespace. If by that you mean that including &lt;cmath&gt; will also provide ::fabs, and so on, then there are several very real implementations where this doesn't happen. We actually had several issues where people couldn't compile us because we included the c++ version of a header and used the function in global namespace...
"living dead" is the other one.
Perfect! Updating!
In practice, I can't see how this would not be UB, because taking the address of an object is bound to be UB if said object does not exist at all (which will inevitably be required by some, if not all, containers), and I doubt we could overload the address-of operator everywhere without either breaking access to the original type, or breaking the taking of its address in other contexts. So maybe the standard does not explicitly says it to be UB in a simple sentence, but if there is no way for it to be defined, it is simply not.
A little side-note: under the Win/MSVC calling conventions, I believe a class { type* ptr }; is not equivalent to a raw type* ptr; This render the theory of 0-cost for e.g. unique_ptr not 100% true, although on a modern processor I doubt somebody can show me a concrete example of it causing a non-trivial perf problem. EDIT: now that I'm thinking again about it, even if it is actually the case this, if you use LTO you will probably only have an effect on module boundaries.
Fair enough, but if it's only a small deal when passing into functions, I can get over that as being really inconsequential. Besides, they can't change Windows calling conventions. Best effort is all I expect from compiler devs, not for them to change the universe :)
So, after reading around for a bit, it seems the situation is a bit more complicated than I expected. My understanding of the problem is as follows: auto p = ::operator new(sizeof(T)*2);//1. auto pi = ::new(p) T();//2. ::new(pi+1) int(); auto pi2 = pi+2;//3. 1. Memory is allocated 2. Elements are constructed 3. `pi` only points to the first element (an array of 1), pointer arithmetic is undefined since there isn't actually an array, only two elements that happen to be located next to each other in memory I would argue however, for `std::vector&lt;T&gt; v(2);` that `v.data()+2`, still has well defined behavior, because [the standard says so](https://timsong-cpp.github.io/cppwp/vector.data#1), and the implementation has to make it true. Whether it does so by adding a hint to the compiler to treat it as an array, having the compiler treat consecutive elements of the same types as an array, make a special exception for types called `std::vector` or whatever else. I don't know to which degree the standard has specified, for `std::vector`, exactly how memory is allocated, and objects constructed, but let's say, for the sake of argument, that it requires it to essentially be the same as the above code example. I would say it has either, implicitly made that well defined too, or there is an actual contradiction, in which case, by the principle of explosion, everything is undefined behavior.
I feel a lot the same way. I wish that the C++/WinRT project created a WPF projection as well so we could do WPF without C++/CLI or C#.
What it seems the argument boils down to is, is &amp;*vec.end() well defined? I don't know, if in doubt, use something you know is well defined. vec.data()+vec.size() is well defined.
Slides (coming soon) &amp; more information: http://llvm.org/devmtg/2017-10/ 2017 LLVM Lightning Talks: https://www.youtube.com/playlist?list=PL_R5A0lGi1ABrnDbIkbiXl5RPDNM33KQg
I not sure who '*those* people' are. As for pragmatic programmers who use new features only when they offer known benefits and taking care to ensure compatibility with the systems and tool chains they are working with, they're not a small fraction of the modern game development industry that I know of.
Is there any use case for this? 
Why does it have to be a template singleton? Can't you at least use a global instead? class ObjectFactory { }; // just write this like a normal class ObjectFactory&amp; globalObjectFactory() { static ObjectFactory s; return s; } I don't know windows well but I think this should work, even across DLL boundaries, and even if you call this function before main.
Somebody, somewhere, will design a macro that depends on this syntax.
It lets you write things like this - avoiding declval - which is one of the nice features of trailing return types. std::function&lt;auto (T a, S b) -&gt; decltype(a+b)&gt; func([](T a, S b){ return a + b; });
Did anyone notice hilarious irony of having to **interpret** a (dwarf) code produced by **compiling** code to be able to unwind a stack? ... and curious fact that unwind may allocate memory? ... and my personal (recent) favorite that throwing exception can actually fail due to lack of memory?
Fair enough, I'm familiar with this aspect of trailing return types but it never occurred to me to use it inside a std::function type.
I still use BGL, and have also used [igraph](https://github.com/igraph/igraph) (in C) for a few algorithms.
With the proviso that vector::iterator can *be* a pointer (though of course it's not required to be).
I'm reasonably certain it *can* work, if you're willing to work at it a bit. To do it, you'd overload `*` to return a proxy type that in turn overloads `operator&amp;` (and when valid, can also support conversion to T and assignment from T).
Presumably, the same use cases it has for normal functions
What is the terminal he is using? Does he have any kind of customization?
Just ran it again, and I didn't get the message about an unexpected token
Sorry to be a n00b but can someone explain exactly what is going on in each of those examples? And how can this be useful?
We use docker. Very easy to create the toolchain and then very easy to distribute. I'm going to tidy it up a bit and release it with a blog post at some point, hopefully soon.
He had 128 cores.
It was a 128 core machine in his home especially built for making clang builds fast.
&gt; The library never assumes that past-the-end values are dereferenceable [...]. How do you interpret that? Does library (STL?) assume that whatever container you pass, it wouldn't try to dereference past-the-end element or that any and all standard containers' past-the-end elements are not dereferenceable?
&gt; You can do pointer arithmetic on any pointer, as long as you don't over-/underflow the buffer. I thought that you can do pointer arithmetic even on one past-the-end element, no? I mean "can" as in "guaranteed by standard".
Given that this is draft, does it mean that dereferencing a pointer to the end is still undefined?
Sorry to say, but that sort of stack-overflow-level of nitpicking toxicity is exactly why experienced and productive programmers do not post things on forums like those. You might think your comment is well intended but the reality is that it is just toxic. They are building something cool and providing insight about an actually very rarely discussed compiler feature (which afaik was sort of discovered a few weeks ago by one of Unity leads), and you are derailing on which shade of grey they should use according to your boy scout manual. This guy worked on CryEngine ffs. We've all made those mistakes, but please learn to sit down, breath and try to post less.
&gt; Without destructive move (which is not currently supported in C++) Can it be supported in principle? Move is a programming convention, not a language feature. std::move by itself does not move anything. In the following code: { T object; SomeFunc(std::move(object)); } in general there is no way to tell at compile time what SomeFunc will do to `object`, its definition can be in another translation unit. It can leave the object in moved from state, leave it unchanged, or do something entirely different with it.
Was there. Thoroughly enjoyed the talks! thanks for organising.
I don't think it can be supported so long as we have no fancier "lifetime" stuff à-la rust. There are two issues: - need to tell the compiler that you can't use this variable anymore - may (sometimes)n eed to ensure that the destructor isn't run, just that the memory is released.
Documentation for std::initializer_list is [here.](http://en.cppreference.com/w/cpp/utility/initializer_list) Relevant parts are: Pre C++ 14, &gt;&gt; The storage for std::initializer_list is unspecified (i.e. it could be automatic, temporary, or static read-only memory, depending on the situation). Post C++14, &gt;&gt; The underlying array may be allocated in read-only memory. Moving an object requires changing it and an object in read only memory can not be changed an i think this is the primary reason why std::initializer_list can not be moved from. 
I don't think even this would help, you can do something like `SomeFunc(&amp;object)` and now it is no longer a local variable, just an address in memory. `SomeFunc` can do whatever it wants with the object, including putting it in a moved-from state. 
well yeah but that's not using std::move :p But yes, we're reinventing rust here.
If by one past-the-end you mean `end()`, then going backwards is fine. If you mean `end()+1` then you are already in undefined behavior land. That said I don't know of any compilers that care. Maybe it would matter if you were programming for 16-bit x86 with far pointers, but I am personally glad that I don't know.
SomeFunc could use std::move. Also you don't have to use std::move to move an object, it is just a helper to obtain a rvalue reference.
For the love of god zoom out, the constant panning is giving me motions sickness.
This is not the answer. The compiler may choose whatever memory is appropriate. If read-only memory is unsuitable, it wouldn't use it. 
As the author and implementer of the movable initializer list proposal, I'd say it's because the committee denies that the problem is significant. Voice your concern on the std-discussion mailing list or contact your compiler vendor. 
It also works for stuff like `extern "C"` declarations. I haven't found a place where it doesn't work where old-style signatures work.
The signature type `int(float, char)` can also be written as `auto (float, char) -&gt; int`. This is nice for when the return type can only be expressed in terms of the argument types, e.g. `auto (T t, U u) -&gt; decltype(t + u)`. I also prefer this syntax since it makes your function names lined up: auto foo() -&gt; int; auto bar() -&gt; std::string; auto long_fn_name() -&gt; LongTypeName;
The problem is that you don't always have vec itself, but only an iterator that might be a copy of vec.end()
Great article. While it's common knowledge to many, I'd say that ADL is at least as weird and annoying as shadowing when you don't understand it.
The design made it easy to do auto il = { 1, 2, 3 }; // il has type std::initializer_list&lt;int&gt; f(il); g(il); without incurring use-after-move. However this doesn't turn out to be a common use case.
*Walter Brown's* Lightning Talk (12min) referenced: https://www.youtube.com/watch?v=NLebZ3XT92E
This is my favorite Lightning Talk at this year's CppCon.
I understand what the standard says, the issue is more why did they think it was a good idea? Putting something in read only memory means you're going to have a copy to do later on. The current solution is basically to use a reference type because in that case you're just copying a pointer.
I doubt they'd listen to a random guy, but glad to see there are people who share my opinion. In your proposal, do you also make it `constexpr`?
Your example would not break with what I'm thinking, since it would only move if the initializer list is a temporary or is explicitly moved from (and allow the compiler to move from if it sees nobody uses it later). For the arguments you send there, I'm assuming perfect forwarding so it would copy if it's a lvalue and move if it isn't. With what I'm thinking, you could write your example auto il = { 1, 2, 3 }; // il has type std::initializer_list&lt;int&gt; f(il); g(std::move(il)); And by doing that, you'd save a copy when initializing g.
It looks like ...Haskell. 
I really like quicktype for generating swift4 (codable) data types. Only thing I really wish it had is the ability to 'merge' results from multiple requests. Thats the only way to let it correctly recognise types for optional fields
To second what /u/ocornut posted, these comments have made me question whether I should post on this subreddit at all, as I was hoping for a discussion of the subject matter at hand. I will likely continue to post links here, as the analytics show that the silent majority are interested in the material.
`SomeFunc` can do terrible things if you give it the object without `const`. Much worse than putting it in a moved-from state. You could replace the virtual method table to call `evilDestructor` instead of the real destructor if you felt like it. Actually, it can also do this with a `const_cast` so you're never 100% safe. Just don't send your objects to a function you don't trust.
It's not about whether you are a "random guy". It's about putting it out there that people actually care about this
[This](https://www.reddit.com/r/cpp/comments/4r8j75/why_stdinitializer_list_does_not_have_atdata_and/d4zbmex/?st=j9gw1x5j&amp;sh=3290c010) reason is a bit more convincing. That answer comes from a thread where i asked about std::initializer_list "strangeness" too. 
Well, if someone who has good writing skills (or at least better than me) posts something on Visual Studio user voice or the like, I hope they link it here so I can give them my votes.
It still doesn't say why it needs to invoke the copy constructor of what you give it. It's a template, so you could specialize it on movable values. Unless I'm misunderstanding something. Edit: this discussion also had [this](https://www.reddit.com/r/cpp/comments/4r8j75/why_stdinitializer_list_does_not_have_atdata_and/d50bu27/) gem, so looks like I'm not the first.
Sean Parent has non-proposal for language support for destructive move (syntactically, a dtor with an argument). Follow the links in the article. 
&gt; It still doesn't say why it needs to invoke the copy constructor of what you give it I think in terms of the concept, std::initializer_list&lt;int&gt; e = { 2,5,7 } ; is the same as: const char * s = "foo" ; In a sense that,with the latter,you do not put "foo" in s,"foo" is created someplace else and then its location is stored in s. Initializer_list is the same because "{ 2,5,7 }" array is constructed some place else using copy constructor and then a read only "pointer" to where the object lives is stored in e. "e" does not own the array object and hence its not allowed to mess with it.
It's always nice to have another tool it seems that most of the things it picks up can already be caught by other tools. For an improvement it would be nice if it supported the compile database (compile_commands.json) that can be output by CMake when using Makefiles / ninja generators. Using this you could pick up the correct includes and defines etc for each file. 
What I'm thinking is you could have std::initializer_list&lt;int&amp;&amp;&gt; e ={2,5,7}; Where `2,5,7` are just regular rvalues, which can be moved from later. So obviously you can't allocate them in the same way as your `"foo"`, but constructing things in place is something that people like to do. And if you're looking at the big picture, there's no logic when you can `emplace_back()` and `push_back(T&amp;&amp;)` something but you can't use move semantics for the constructor. If you look at what I linked above, the main reason seems to be that initializer list was made before move semantics.
Tbh, I never really had much trouble with shadowing. Compilers are pretty good at warning about this. As for ADL, well, they’ll need a creepier name to make that particular list. 
Agreed, shadowed naming was always fine for me but ADL was very surprising when I figured out what was going on. Drawing a blank on a creepy name though. edit: speeling
Postmodernism is an sickness that needs to be purged! 
I tried watching this multiple times and I still have no idea what it is about.
I did not like the talk (which, for the most part, consisted of reading the presentation). For instance: why are smart pointers overused? What should they be replaced with, when, and how? The author says he'd like to see less of those - where? In his company or generally? Why? Four minutes is just... insufficient. 
The talk was meant as a teaser. The format for a **Lightning Talk** at CppCon is 5 minutes (max). 
As for `shared_ptrs` (note that I specifically said `shared_ptr`, not *smart-pointers*, in general), I think *Herb Sutter* explains it best (it in detail), in his CppCon 2016 talk: https://youtu.be/JfmTagWcqoE?t=57m18s https://youtu.be/JfmTagWcqoE?t=34m21s 
Right, right, but this may not be an appropriate topic for such a format.
Thanks, I'll watch those soon. :)
That's a good thing.
You might be right about the format: it took Herb almost *an hour* to convincingly explain this point, in his excellent CppCon 2016 talk :) My talk was meant to be more of a [tapas](https://en.wikipedia.org/wiki/Tapas) for entry-level C++ :D 
What is this talk a teaser for? I didn't see anything obvious from watching it.
Too bad :(
I think you misunderstood my comment. I didn't see anything obvious (that it is a teaser for) from watching it. Is it a teaser for another talk? 
Who's the old man between Turing and RMS?
I believe it's [Grace Hopper](https://en.wikipedia.org/wiki/Grace_Hopper)
https://spark.apache.org/graphx/ graphx https://github.com/STEllAR-GROUP/hpx. HPX 
Check out CombBLAS [http://gauss.cs.ucsb.edu/~aydin/CombBLAS/html/](http://gauss.cs.ucsb.edu/~aydin/CombBLAS/html/). It has a different approach but pretty good performance.
I **referenced** 2 talks in the slides: - one is another lightning talk from CppCon 2017 by **Walter Brown** (link was already posted in this thread) https://www.youtube.com/watch?v=NLebZ3XT92E - the other one is by **Alexander Stepanov**: https://youtu.be/COuHLky7E2Q 
(Didn't watch it). If it's about `shared_ptr`, then, if there's no threads, chances are, design is poor.
I don't quite understand why we are still discussing macros and their (in)applicability to modules. Seriously, who cares about macros? It's a near-dead, legacy mechanism that nobody outside of Boost.PP authors would seriously consider using. It makes zero sense to have them inside modules.
I have to second the question. I am a PhD student in pure math, and my research heavily uses category theory and infinity category theory. I find it incredible that it's actually useful for something in the real world. Most mathematicians don't even have much use for category theory.
Even though he says shared_ptr in his slides, I almost never use unique_ptr either. It isn't that I don't think ownership is crucial, but I think exposing unique_ptr is still not high level enough. I tend to work with data structures that can be moved since they contain more information than a simple pointer. As for pointers to data structures, I essentially don't use those either since their stack size should be small anyway. Maybe I'm able to get away with this because I essentially never use inheritance and thus never worry about virtual functions. Every time I've thought about a program from the standpoint of the data I'm manipulating, class hierarchies and virtual functions never cross my mind.
EDIT: my rant was pointless because I had a misconception that a module automatically exports all its imports, which is not the case. &gt; So if you want to keep everything in a single file, you can. You can also keep (almost) everything in headers. The problem is that it's not a good idea - the dependency graph gets bloated, and you recompile all dependencies when changing something just in function body. I think many people hoped that modules will make headers obsolete, but it's certainly not the case. 
Tiedye Thanks :-)
No, we're hoping specifically that we can get rid of the artificial split between declaration and definition, and that the modules proposal is smart enough to only cause dependencies for changes to the actual interface, rather than the implementation. Since we are starting with a grand new thing here, and since I don't see any reason why it wouldn't be technically feasible, I believe it should be discussed. The compiler doesn't have to become a build system, but if we can improve our compile times, for example by allowing the compiler to pass hints to the build system, I don't think anyone would have cause to complain. 
Is the execution time on release builds much much faster ? In my experience, some Qt Container Classes (QList for in stance) are really badly optimized when used in a debug build and the collection the container is holding very very large ! If a Qt container holds hundred of thousands of objects or whatever, try using a Standard Library container class !
&gt; You can also keep (almost) everything in headers. You cannot keep non-inline functions in headers which is *almost everything* that anything that does something useful is made of. &gt; I think many people hoped that modules will make headers obsolete, but it's certainly not the case. It certainly is the case: headers will be replaced with module interface units. And if you want to, you can put all your implementation details in the interface (whether it is a good idea from the sense of design point of view is another questions, but you certainly can). And build systems/compilers can even make sure you don't pay for doing this.
The Module TS does not require an artficial split between declaration and definition. 
That is also a misconception, independently of any realistic module specification for C++. The practice of separation of headers/implementation files is an architectural convention.
Dependency recompilation is just one problem, and I agree that it could potentially be solved by clever implementations. I think that a bigger problem is that you will have to add extra dependencies to your module that are used in the function bodies, and they will transitively be imported by other modules. This will cause a dependency bloat or even circular dependencies. When declarations and definitions are split into two files you can put a lot of your dependencies only into the definition file. If you want to do the same with modules without splitting them into two parts, there would have to be a way to import things in a non-transitive way, for use just inside the function bodies.
&gt; the modules proposal is smart enough to only cause dependencies for changes to the actual interface This is a quality-of-implementation issue and from recent discussions it appears to be fairly straightforward to do.
Can the build system/compiler make sure that `dep2` is not imported by everything that imports `hello`? export module hello; import dep1; import dep2; export void say_hello (a_type_from_dep1) { a_type_from_dep2 x; //... }
Neither `dep1` nor `dep2` is imported by anything that imports `hello` since they are not re-exported. What you are probably trying to ask is whether it will be possible to avoid recompiling everything that imports `hello` because of modifications to `dep2`?
I don't see why people keep saying this. There are plenty reasons to use shared_ptr in a single-threaded context as well. Any place where you would otherwise be reference counting a resource by hand, for example. Or where an object may be deleted as the result of user action, but still be necessary for a while for ongoing processing - even if all of that happens in one thread (in my case, the processing happens on another device, and we need to wait for that). 
Less verbose, actually. Here is the `drawable` example from dyno's docs, rewritten using Poly: #include &lt;folly/Poly.h&gt; #include &lt;iostream&gt; struct IDrawable { template &lt;class Base&gt; struct Interface : Base { void draw(std::ostream&amp; out) const { folly::poly_call&lt;0&gt;(*this, out);} }; template &lt;class T&gt; using Members = folly::PolyMembers&lt;&amp;T::draw&gt;; }; using drawable = folly::Poly&lt;IDrawable&gt;; struct Square { void draw(std::ostream&amp; out) const { out &lt;&lt; "Square"; } }; struct Circle { void draw(std::ostream&amp; out) const { out &lt;&lt; "Circle"; } }; void f(drawable const&amp; d) { d.draw(std::cout); } int main() { f(Square{}); // prints Square f(Circle{}); // prints Circle } With Poly, defining the `drawable` type takes only 7 lines of code. With dyno, I count 15. EDIT: I've updated Poly's docs with this example, and credited Louis.
No, I had a misconception that all imports are automatically exported. Sorry for making a useless argument.
Sure, but that doesn't say much - today I can also stuff all my code in headers, and suffer from horrendous compile times as a result. The question is specifically about sticking definitions and declarations in one module file, and still enjoying efficient compilation.
Well, to be fair, it is not entirely useless. For example, the point about potentially ending up with a circular dependency between interfaces (which is illegal) is a valid one.
It is all the STL algorithms, taking ranges _and_ a pair of iterators (overloads). In addition, you can use member pointers as callables to the higher-order algorithms (e.g `for_each`), and you can pass a _projection_ function to be applied to each element before the algorithm operates on it. That is in addition to the various views, actions, and utilities. You get no argument from me about the sad state of range-v3's docs. I made a conscious decision to spend time writing standardization proposals instead of docs. I've had lots of people volunteer to help with docs, but nobody has actually followed through.
Macros help prevent users from screwing up a bunch of boilerplate. They do something the language can't express without them. People absolutely still use macros, from innocuous ones, to egregious ones, to everything in between. There should be a replacement before we decide to force their obsolescence. 
I have a problem with the BMI being 100% implementation defined. Beside the issue of no forward or backward compability from one version of the compiler to the next, what will happen to tooling ? Will clang-based tools be able to understand gcc BMIs ? What about ICC and MSVC ? Will the provide either an api or a spec ? What about IDEs ? Not all of them use clang, and they often use a clang version a few versions behind the current release. And, as you said, the fact that the fact that not changing the interface still leads to everything being potentially rebuild is an issue.
No obsolescence is forced. Simply declare you macro in a header file, include it and you're done. There's nothing wrong with including headers when you need a header, even in a modularized world. What I don't understand is why sone people what to shove macro down into modules, which make no absolute sense, as allowing this basically breaks the whole language. If you want to use the processor, use the preprocessor and include your files.
Is there a question hiding in there?
That is possible. If you have a concrete scenario, I would like to know about it so I can study it and see what can be done.
Well, technically the compiler could compile the exported stuff of a module into a separate file, use that to compile other modules, and then recompile everything together with the implementation. If there is no cycle between the actual exported items then that could potentially work. My knowledge on the topic is small though, so I'm just hypothesizing. And if dependency cycles are illegal by the standard, then that would have to be changed.
At some point, we hit physics and logic :-) import declarations aren’t transitive.
Yes, that was my misconception that I contributed to this topic :)
The CLI supports that! Just specify as many JSON files as you like, and their types will be unified: quicktype --lang swift foo.json bar.json
Would you be willing to wait until 2050 for modules with a standardized BMI? In fact, nothing in the current proposal mandates a BMI or that it has to be a file. There could theoretically be a BMI-less implementation. Say, a compilation system could store all this in-memory. Also note that BMIs are not meant to be a distribution mechanism since they can be sensitive to compile options. For example, they most likely will not be installed (except, perhaps, for a standard library). &gt; the fact that not changing the interface still leads to everything being potentially rebuild is an issue As I mentioned in the post, this is a quality of implementation issue and which doesn't appear to be particularly hard to solve.
I just explained why they want it. Just because you can use a header doesn't mean they should have to. If we're modularizing the world, then either let them use the preprocessor in their modules, or give them a solution that isn't just "use the thing no one likes". You act like people actually like using macros, we get disparaging comments about how people are dumb for wanting macros in their modules or whatever. They just do things that are impossible to do in any other way _in_ the language. If we were to fast track some of the meta classes dependencies then we'd be most of the way to not even needing them. There's even a whole paper about modules and how macros interact in the latest mailing: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0837r0.html Until I get code injection, meta classes, reflection, and other such features I get to choose between murdering compile times by writing a code generation step similar to moc, or using macros. I don't want to have to add a header file just to express that my modules have macros the users might want. 
IMHO, initializer_list is an immature design. Not only its limitless, but also its [inefficiency](https://akrzemi1.wordpress.com/2016/07/07/the-cost-of-stdinitializer_list/). You better simply not use it in performance-critical code.
One of the reasons that the compiler has to spit out the entire recursive set of headers for a particular C++ (or C) file is that preprocessor macros can affect which headers are transitively included. That means that although file1.cpp and file2.cpp both include and depend on file3.h, only file1.cpp causes file3.h to also pull in file4.h. Make, at least, isn't happy with conditional dependencies. Modules don't generally have that problem, so you can just give the build system each modules direct dependencies and let it work the whole thing out. And it's a per spec requirement that module dependencies form a DAG. 
One of the reasons that the compiler has to spit out the entire recursive set of headers for a particular C++ (or C) file is that preprocessor macros can affect which headers are transitively included. That means that although file1.cpp and file2.cpp both include and depend on file3.h, only file1.cpp causes file3.h to also pull in file4.h. Make, at least, isn't happy with conditional dependencies. Modules don't generally have that problem, so you can just give the build system each modules direct dependencies and let it work the whole thing out. And it's a per spec requirement that module dependencies form a DAG. 
What do you mean?
I suggest [this](http://poster.keepcalmandposters.com/21718.png) approach.
Yes, well put!
You make a good point about the binary interface being toolable. See the paper [C++ Modules Are a Tooling Opportunity](http://www.axiomatics.org/~gdr/modules/tooling-opportunity.pdf). It missed the pre-Albuquerque mailing deadline, but I sent a copy to the committee reflector and it will be part of the post-mailing; it is only fair to share a copy here. As ever, I look forward to comments and feedback. One point you should take away from that paper is that the Visual C++ team is committed to making the IFC format specification publicly available to the entire C++ community and is eager to partner with any C++ tool vendor in the development and refinement of that format. Note: That copy quoted the wrong sentence from P0804R0 and that will be fixed in the next revision. 
&gt; n fact, nothing in the current proposal mandates a BMI or that it has to be a file. There could theoretically be a BMI-less implementation. Say, a compilation system could store all this in-memory. Excellent point that is often missed. See also the section "Processing Module Uses" from the paper [C++ Modules Are a Tooling Opportunity](http://www.axiomatics.org/~gdr/modules/tooling-opportunity.pdf)
I agree with you about `shared_ptr`, and I think you really intended to specifically address only `shared_ptr`, but the quote is "Another thing junior developers don't get is that smart pointers are not the answer for everything. I would like to see much less shared_ptr.", which IMO is talking about both.
I'm glad you have explained the "I cannot have everything in a single file" issue both in the website and here at reddit. :) I hope now that modules get standardized soon (and build systems start to support them) so we are able to begin to use them. This is one of the more exciting new features for C++ imo.
The problem with exporting macro from module is deeper than you think. Clang make it work quite easily, because their implementation of module TS is basically precompiled header with the module TS syntax. This is very wrong in my opinion. Module interface are very different from precompiled headers. But again, clang want macros. For example, GCC took a completely different approach. They started with their LTO implementation to output binary information about an interface. Macros is simply not a thing in this context, and cannot exist. If you want macro in your module system, I'll tell it again: *you need to break the whole language and compilation process*. Why? **Because you will need to turn the compiler into a preprocessor.** Let's first analyse how the compilation process works for module interfaces and why it cannot work. Module BMIs are generated as a product of their compilation. To compile it, you first need to run the preprocessor. The preprocessed file is then compiled, and a BMI is generated. Note that during the compilation process, the file is already preprocessed. No macro exists at that point. Since no macros exists, the compile has no means to export macros. To export macro, you will need the preprocessor to *not expand macros*, because you will need the compiler to know about them. And then, for the compiler to read them and understand them, you will need to **turn macro into language entities**. Which is illogical. Those language entity will be able to apply operations on the source file, and process other language entity as text. You ask to break the whole language. Then, let's look how the importation process will look like. First, the preprocessor needs to run. It will expand every macros. Note that the importation process has not started yet, because the preprocessor cannot know about modules, as they are only know by the compiler, and the preprocessor cannot read BMIs. Then, after the preprocessor will effectively leaves macros there because they didn't exist yet, since we have to import module to know about them. So the compiler needs to read the BMI, read macros, and expand the not processed macro. Again, the compiler will need to understand macros and even will have to make a second pass. Clang may allow macro to be exported because they used precompiled headers, but clang's module implementation is simply broken, and (hopefully) be fixed in the future. Macros has simply no place in the module world. Your library that export macro should provide a separated header. It makes it explicit that this library needs macro. Explicitly importing macro using include is nice and explicit. Want the preprocessor? Use the preprocessor.
&gt; Does library (STL?) "The library" in this context means "the C++ standard library". &gt; whatever container you pass You don't pass containers, you pass pairs of iterators. Not all iterators come from containers. Even iterators that come from containers might not actually be the past-the-end value for the container: std::vector&lt;int&gt; v{1, 2, 3, 4}; auto iter = std::find(v.begin(), v.begin() + 2, 0); The `v.begin() + 2` iterator is not `v.end()`, and is actually dereferenceable. But `std::find` still assumes that its second argument is not dereferenceable. In general `std::find` has no way of knowing if the "end" iterator passed to it is dereferenceable or not, so it always assumes it isn't. The rest of the C++ standard library uses the same assumption whenever it uses a pair of iterators denoting the beginning and end of a range. &gt; it wouldn't try to dereference past-the-end element Right.
I've been using `dyno::poly` directly which is more concise like above except that you don't appear to have concept maps which means everything has to have that exact draw function.
I saw how they do iterators and I was like: oh boy... BGL has many problems but there's no way I'd be switching to LEMON.
You can think of the nested `Members` type alias as the concept map. You should read Poly's docs, in particular the section on non-member functions. You can use a lambda instead of an exact `draw` member function. https://github.com/facebook/folly/blob/master/folly/docs/Poly.md#non-member-functions-c17
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7a4ot4/coding_problem_implementing_the_price_improvement/dp75lyn/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
As someone who picked up C++ quite recently and knows a bunch of young C++ developers I completely disagree with his points and call this talk bollocks. If anything I find myself having to stress the importance of relying on the compiler to do resource management for you (e.g. don't pass a ****ing pointer to be filled in because "it's a big object" but rely on return value optimization, don't just use raw and shared pointers... etc) mostly to "senior" developers. Same goes with writing cache aware code or code that is properly designed in terms of the splitting of responsibilities . Of course both young and old developers can be shit, amazing or (like most of us) mediocre. Quite frankly, if you learn C++ in 2017, you're probably doing it because you like the language, you're probably learning modern C++ and you are probably learning it because you understand the importance (and need) for hardware-aware code and care about having good abstractions. If you learnt C++ in the 90s you probably learnt i because "it was the popular thing to do" and you don't give a toss about any of the above mentioned things, you just learnt it to earn a living. Again, I'm not trying to generalize, but I really think associating these types of mistakes with junior C++ devs is wrong. If anything most of the stuff I've seen newer C++ devs struggle with is the legacy that C++ and C codebases and ecosystems carry with them.
Reason is laziness in the ownership design. In a single-threaded scenario, the owner is easily known at any point in time **in a vast majority of cases**. What people do is, they justify laziness by inventing requirements which require a shared ownership.
how are you so sure?
Thank you, that paper is interesting. I will have to think about it more to have an opinion :) While you are here, do you have an opinion on using modules to get away with source files, and having a way not to rebuild a dependency chain if a module is modified but the interface it exports is not ? A lot of people bypass the "C++ doesn't have good build/package tools" by providing headers only libraries, and while easier to use, that approach has a lot of issues, starting with compilation times. 
&gt; Seriously, who cares about macros? well, everyone who wants to have at least a tiny bit of reflection. The day you can do template&lt;std::string blah&gt; class foo { std::string name() { return blah; } int get_ + blah() { ... } }; is the day macros aren't needed anymore
&gt; you need to break the whole language and compilation process why is this a bad idea again ? :p I think that most people would be happier with a simpler compilation model, like Java or C#.
Very nice to hear. And nice to hear it may not even slow down compilation that much. I had some concern about this before, and started thinking where should I split interface and implementation, but it turns out it won't be much needed, or not as much as I thought before. The only place where separation is required is when you want classes to mutually use themselves, *and* you want to place them in different modules. I think that's an acceptable limitation, and things can change in the future. But one can always put those classes in the same module, and still implement everything in the interface.
Yes, well put!
ITT: nitpickers.
damn bro! you put so much effort into build2 - Maybe I should give it a chance one day! keep up the hard work!
Using juniors and interns in this talk accomplished nothing more than a cheap attempt to assert some form of authority by the speaker, as if to say that if you don't agree with the speaker then you must be a junior C++ programmer and you just don't get it. It's a lousy way to establish credibility on any topic. I would advise the speaker in the future to avoid using seniority as a reference point. The topics he discusses have ABSOLUTELY nothing to do with seniority. Some of these are common mistakes made by juniors and seniors alike, some of these have nothing to do with 'getting it' but rather are things people have genuine disagreements about. The speaker is welcome to present their preference and even argue why they believe their approach is superior, but note that absolutely nothing in this talk required the speaker to reference anything regarding seniority.
&gt; The only place where separation is required is when you want classes to mutually use themselves, and you want to place them in different modules. I think that's an acceptable limitation, and things can change in the future. But one can always put those classes in the same module, and still implement everything in the interface All correct. I believe we (inclusive) all need to have more hands-on experience with modules before we attempt more semantics modifications.
As someone who has worked for ~15 years commercially in C++ I find this talk to be fairly on point. auto, smart_ptrs, exceptions all provide amazing benefits to the language, but at a cost. Newer developers learn these without understanding the cost or underlying concepts they enhance (e.g. newer devs will happily use 'auto', but no longer consider what the declaration of that auto would be if it wasn't auto. Is it a var, const var, const ref, ref, pointer? Does it matter? Yes. The difference between auto and auto&amp; can be significant when optimizing high performance code). I find myself stressing the importance on having to manually manage resources appropriately and not rely on the compiler. Return optimisation is compiler dependent, where as passing a pre-allocated pointer to have values populated is a known quantity (and also you consider heap vs stack based allocation). I'd rather not spend hours optimizing code because of a specific compiler on a specific operating system. &gt;&gt; If you learnt C++ in the 90s you probably learnt i because "it was the popular thing to do" and you don't give a toss about any of the above mentioned things, you just learnt it to earn a living. No. You learnt C++ because you wanted a high performance language that gave your direct control over the hardware. The same reason you learn C++ today. This is why C++ is still the defacto language in the development of scientific software, games, operating systems (Windows), financial trading systems, embedded systems etc. &gt;&gt; If anything most of the stuff I've seen newer C++ devs struggle with is the legacy that C++ and C codebases and ecosystems carry with them. This, IMO is because they don't understand the original concepts of the language that the newer features are enhancing or abstracting. There is this whole "pointers are bad, use shared_ptr" movement going on, in reality there is nothing wrong with pointers. There is only problems in how your design manages them. (FWIW, we use raw pointers exclusively because we don't want the overheard of smart_ptrs).
Thanks for the kind words.
Thanks for posting this doug. I really hope some people at Epic are looking.
One point you mention but I'm still not clear on is the disconnect between modules and file names. While it makes sense in that c++ historically has completely separated symbol names from file names, what is the consequence in terms of finding and knowing which modules to build? * Does it mean that that the build system and compiler have to compile *all* modules in all search paths before it knows what to do/if there are collisions/the dependency graph? * Does it mean we have to duplicate our dependency graph outside the modules already listed in source? While it's hard to imagine avoiding providing a set of search paths, I think many were hoping that import statements in source would provide Single-Point-Of-Truth for specific module dependencies. It seems like the disconnect between module name and file name might force dependency information to be duplicated (and have an opportunity to get out of sync)
It would be good if people interested in modules would read the whole proposal. [P0273][1] is also worth reading. [1]: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0273r1.pdf &gt; I cannot export macros from modules &gt; And you know what, we already have a perfectly fine mechanism for importing macros: the #include directive. So if your module needs to "export" a macro, then simply provide a header that defines it. This way the consumers of your module will actually have a say in whether to import your macro (which always has a chance of wrecking their translation unit). P0273 does discuss this, and I think there are good reasons for allowing modules to export macros, and reason why retaining #include for macros is insufficient. It also is not the problem some make it out to be. Modules are still isolated and deliberately exported macros are rarely the problem. &gt; I cannot modularize existing code without touching it &gt; This one is also technically correct: there is no auto-magic modularization support in Modules TS. I believe it strikes a good balance between backwards-compatibility, actually being useful, and staying real without crossing into the hand-wavy magic solutions land. P0273 discusses this as well and I don't think it can be called 'hand-wavy magic' as Clang's pre-modules-ts system demonstrates viability. I was quite impressed with how well it actually worked on well behaved code. It does depend on the code not doing 'module-unfriendly' things, which most large codebases do do. So in that sense it may not really allow your particular codebase to be untouched. But it does have the value of minimizing what has to be changed _and allowing the codebase to support both modularized and non-modularized builds at the same time_, the value of which I think some underestimate. I think the transition to modules is really important. The legacy support discussed in P0273 and implemented in clang shows that it works. And I think it will be really important for actually getting the most out of modules in real projects as quickly as possible. --- I do think you address the concerns over build system issues well. Clang's pre-ts system did work by implementing a build system, and I think it will be good, and I think you show that it's viable, to keep that all separate.
&gt; do you have an opinion on using modules to get away with source files, and having a way not to rebuild a dependency chain if a module is modified but the interface it exports is not ? Yes, the IFC format that VC++ uses is being designed to be sensitive only to semantically-significant changes. It is not perfect but it is getting there. The IFC format themselves are not distribution format, and they don't replace source files -- just imagine debugging scenarios, you want to step through functions with the source file in front of you :-) The IFC files capture the semantically relevant part of the interface. &gt;A lot of people bypass the "C++ doesn't have good build/package tools" by providing headers only libraries, and while easier to use, that approach has a lot of issues, starting with compilation times. Yeah, that is an unfortunate state of affairs. With modules, the notion of "header only library" is trivialized and just disappears. However, we need to solve the "good build/package tools" problem. I am hoping that C++ tool vendors will come together in collaborative forum to make progress there. My view is a good build and/or packaging system for C++ will take modules as foundational construct. 
That was a very clever insight about text formatting as a hidden performance tax. Log rotation and C++17 dependency are the sticking points stopping me from trying it out on my latest embedded project, but keep up the awesome work! I will keep an eye on this one.
I don't know a compilation process for C++ that requires JIT would be simpler :-)
You're a customer even if it's a free product! My proposal doesn't cover constexpr but it's a trivial change. Actually I had to take care to account for constant expressions in the implementation even without constexpr accessors.
But compilation is inefficient in the header case because the header is recompiled for every translation unit that includes it. In the modules case, the module is compiled once whether or not you stuff the definitions in with the declarations. I guess you still suffer having to recompile everything that depends on the module if you change the module implementation. Is that what you're getting at?
Quite the contrary, in fact. The decision to dissociate module names from source files that might contain a module interface source file is informed by (a) existing header file practice and experience; (b) looking at other modern languages that have module support. In fact dissociating file names from module names allows a packaging system to have a more principled approach; same for build systems. A module **isn't just one source file**. Coupling the module name with the source file that contains its interface definition will just perpetuate the header file disaster -- even if it is something we are familiar with. You want to map to the entire collection of module units. That starts getting outside the language. That is where you want robust tooling.
Where does the build system and/or compiler look to know which module files to associate with the module names listed in the import statements of the target translation unit? With PP `#includes` the header lookup resolution was defined via the complex set of rules on `-I` paths and `&lt;&gt;` vs `""` includes. Will a similar set of rules exist for modules? or will the full set of pre-parsed modules files to pull from be handed to the compiler? if so, where would that come from?
"A shared pointer is as good as a global variable" - Sean Parent That's less than a minute, but if you think about it, that's all you need to know about how shared_ptr is so often wrong.
I can tell you from the `build2` experience it can all be done without any ad hoc, compiler-specific search paths. Given a translation unit the build system can ask the compiler for the names of directly-imported modules. Given this list it can map each name to the BMI file name *in a way that makes sense in this build system's worldview and/or the preferences of its users*. Just think about it: I like the `hello-core` naming convention, you like `HelloCore`, and someone else wants it to be `hello/core`. After that, all the build system has to do is provide the compiler with the module name to BMI mapping. If you are interested in details, you can read how this is [handled in `build2`](https://build2.org/build2/doc/build2-build-system-manual.xhtml#cxx-modules-build). I think relying on any kind of ad hoc, header-like search paths will be a real step back that will cement C++ in the pre-distributed-compilation stone age.
&gt; Where does the build system and/or compiler look to know which module files to associate with the module names listed in the import statements of the target translation unit? That is among the implementation-defined aspects of resolving modules -- just like it is for resolving header file mapping. I would like to see improvement in this space. But, it isn't something to be defined at the language level -- that would be the wrong place, given the widely diverse environments where C++ is used. &gt; With PP #includes the header lookup resolution was defined via the complex set of rules on -I paths and &lt;&gt; vs "" includes. No, it isn't defined; that is another misconception. It is left to each implementation to define. And they have all come up with fairly elaborate rules that regularly trip developers. 
And I am googling half of your recommendations, it was useful. It just left me with a lot of questions, which is thanks to the format.
&gt;As someone who has worked for ~15 years commercially in C++ I find this talk to be fairly on point. Your number of years isn't particularly relevant. Some people with 15 years of C++ experience are stuck programming using 15 year old conventions and standards which by today's standards are obsolete. Others are very experienced developers who have gained the wisdom that comes with seeing the language progress for over a decade. Most are a mix of both, the good and the bad, and so simply declaring the number of years you've programmed gives us no insight as to where you are on the spectrum. Overall it has no bearing on the conversation. &gt;(e.g. newer devs will happily use 'auto', but no longer consider what the declaration of that auto would be if it wasn't auto. Is it a var, const var, const ref, ref, pointer? This point doesn't mean much of anything. By itself, `auto` **never** resolves to a reference. You need to force it to resolve to a reference by using `auto&amp;` in which case it **always** resolves to a reference. As a consequence, the difference between `auto` and `auto&amp;` is no different than the difference between `std::string` and `std::string&amp;`. Hence your claim that `auto` incurs some kind of cost that is not otherwise present is unjustified. You have an issue with someone making a copy vs. taking a reference and while that issue is justified, it has nothing to do with using `auto`. &gt;I find myself stressing the importance on having to manually manage resources appropriately and not rely on the compiler. This is likely due to you being familiar with how C++ used to be 15 years ago rather than how it is today. Modern C++ development is about automating resource management by using a structured approach that ties resources to clear and well defined scopes. Arguing against this is similar to how people argued against the use of other structured programming facilities back in the 60s arguing that using functions, if statements and while loops was slower and more complicated than just using a straight forward JMP instruction. The recommended practice today is that a resource should be managed either by having its lifetime be related to the lexical structure of your source code (this is what people call stack allocation or automatic lifetime), or having it tied to an object whose lifetime is managed in that manner. It's a kind of recursive approach to lifetime management where lifetimes piggy back off of one another until you arrive at the base case where an object's life is tied to its lexical scope. &gt;There is this whole "pointers are bad, use shared_ptr" movement going on, in reality there is nothing wrong with pointers. No, it's raw pointers are bad, use smart pointers. The principle here is that you should always use the most specific and restricted type that performs the operations that you need, and a `T*` is simply too vague to have much of any meaning. A `T*` can represent so many different things it's ridiculous. It could be used for a nullable type, a polymorphic type, a dynamically sized array or a single value, when in actuality you usually only want one of those things. Hence prefer to use a `T&amp;` when you only need a single non-null value. Use an `std::optional&lt;T&gt;` when you want a nullable type. Use an std::vector&lt;T&gt; when you want a dynamically sized array, etc etc... Basically use the most specific type needed to satisfy your requirements, but using a `T*` is simply too vague.
If you change the module implementation but the interface is unchanged, you don't need to recompile -- at least that is the experience the Visual C++ compiler is trying to provide.
&gt; No, it isn't defined; that is another misconception. It is left to each implementation to define. And they have all come up with fairly elaborate rules that regularly trip developers. Does this concern you? I would be afraid that this could cause a logistical nightmare especially since BMI format is also implementation defined. I don't, off the top of my head, see how any implementation could get around (at least partial) pre-parsing all modules in the search space. 
I'm fairly certain the relevant Epic folks will have seen Aras' original post. I know that the team puts a lot of effort into compile times, when I worked at Intel they spent a fair amount of time with our compiler team to both improve the Intel compiler speed and their own code. Large middleware engines are difficult to get extremely fast to compile due to the large feature set needed. I'm also not sure how much of the compile time is due to their pre-compilation code generation.
&gt; I not sure who 'those people' are. ... the people you just mentioned who are C programmers who only barely use C++. A tiny subset of game developers here in late 2017. :) &gt; As for pragmatic programmers who use new features only when they offer known benefits The benefits of newer C++ features are quite widely documented. The features can't even easily get into the standard without proving to the committee that they provide benefit. :) &gt; and taking care to ensure compatibility with the systems and tool chains they are working with That's the best point for using older anything I've seen in this thread. :) It's though not a defense for avoiding basic C++11 features or sticking to C89 headers since every current-gen gaming platform (all desktops OSes, both relevant phone OSes, the three major gaming console brands, and even Web/emscripten) uses either recent MSVC or recent Clang toolchains in their official SDKs, to my knowledge (I know relatively little about Nintendo's SDKs, but I'm led to believe they finally entered the modern world with the Switch SDK). &gt; they're not a small fraction of the modern game development industry that I know of. A large majority of the major studios I'm aware of - from Activision's to Zenimax's - are relying on relatively modern C++ toolchains. They give talks about the advantages of those toolchains at CppCon, even. :) Being pragmatic is great and obviously something we do (this is why I butt heads with the committee's decisions sometimes and a large reason why SG14 was started), but we've _long_ since passed the days where using C is pragmatic.
More talks to watch! Maaaan, I'm never going to catch up.
Macros still make sense in a lot of places, but I really hate the idea of exporting/importing them through the modules system. For one, as gracicot explained, they would require mixing of language and text processing (to the degree that you have to run the preprocessor and module include logic multiple times until we reach a fix point - this is crazy. Second, it means complicating yet another modern language feature just to accommodate quirks of old codebases (and if you don't consider the necessity to export macros a quirk now, consider the situation in 10+ years). Finally, we can always add the ability to export macros later on (if this turns out to be really, absolutely necessary), but we will never be able to remove support for it, when (almost) no one needs it any more, so it would just add to the technical debt of c++. @ /u/GabrielDosReis: Hold the line!
Interesting! I'd like to push you further toward *structured logging*. The low-latency logging framework I use at work is printf-based, which is generally fine except that it only really understand fundamental types (integers, floats, C-string) which makes it a pain when one wishes to log *objects* which are necessarily *nested*. I want to be able to drop in a full object, and let it format itself. And bonus points if the *structure* of the object is logged. Now, I am talking not about actual JSON serialization (performance would go down the drain), and indeed, I am not even talking about serializing the *name* of the data-members (mostly redundant, slowing us down), but something that exposes the *structure* of the object (0: first data-member, 1: second data-member, ...) where each data-member can itself be an object. --- I think this could be efficiently streamed, and with easy "recovery", with the following approach: - logs are a stream, a per-thread stream, which may (or not) be multiplexed, - logs are emitted in fixed-sized chunks, if a single log statement overflow it will simply use multiple chunks^1 ^2, - a chunk is tagged by: - the thread id^3 , - a thread-local log id, - a log local chunk counter (to detect missing in-between chunks), - a completion indicator (to detect missing trailing chunks). That's basically our stream. It can be written to file, or streamed on a UDP port (or several) to distant servers. It doesn't even matter if the chunks of a single log/application end up on multiple servers or in multiple files, they can be reassembled later. ^1 *This chunking behavior is neat because ring buffer for fixed-sized elements are very easy to implement!* ^2 *This chunking behavior is also neat to jump to random chunks in the file; with all chunks having the same size, it's easy to jump +100,000 chunks ahead or backward!* ^3 *In the applications I worked with, the chunk would also be tagged with a machine id (IP) and a process id, so as to be able to dump multiple processes log in a single aggregated log stream.* --- For the content itself, I would propose that for each log (1) the log format be written and (2) the arguments be written in a structured fashion (as suggested above, logging an object as an ordered list of fields). To help display the arguments, it could also be helpful to log *metadata* about the objects; it could be as simple as: - the first time an object of type X is encountered, log a unique type-ID + X name, - each time X is to be serialized, first serialize its unique type-ID and then the value. There are arguments for logging the mapping in a separate stream (think of it as an index stream, much more compact, easily searchable), which would be persisted more durably. Then visualizers can be built by having a side-database of "name of struct to name of its data-members". Note that the absence of the data-member name is not too bad, as you still see the raw data, and can use its index instead. Unlike a ProtoBuf stream, the discovery of structures here is ad-hoc, and a single stream from multiple processes may feature multiple incompatible versions of the mapping for the same type name. This is normal. And yes it makes the life of visualizers slightly harder, but they're off the fast-path!
You can't use includes as a get out for not being able to export macros. The modules paper wants to remove all use of the preprocessor, in its current form it doesn't even come close. If you try to import to modules that export the same name, you cannot. Module imports should be equivalent to namespaces similar to python. The preprocessor allows you to include a file inside a namespace, while not elegant, it fixes the issue of poorly mashed library classes/namespaces/etc. You cannot hide template definitions in another file with modules - they have to be in the same export file, so your export file becomes unreadable. Today you normally include the cpp file at the end of the header, modules don't help here. Modules as they stand are an incomplete solution, they do not represent what people imagine when they think of modules (see all other languages with modules). It is a fantastic idea, and I'd fully support it, if it wasn't so poorly proposed.
I think you've extrapolated my sentence (which is understandable) further than I intended. I also think you're making assumptions about my knowledge of the state of the games industry, and you shouldn't. It really feels like we're getting off on the wrong foot over an extremely minor point which wasn't even the subject of the article I linked to.
&gt; I think relying on any kind of ad hoc, header-like search paths will be a real step back that will cement C++ in the pre-distributed-compilation stone age. I may agree that tying includes or imports to specific file names creates unnecessary conceptual constraints that my prevent desirable outcomes, but regardless of whether the file name solution is a good one, the mapping has to be solved one way or another. Fileame + search path was the solution for PP includes. But my question is still: * What is the (or is there a) solution (or at least *a* reference solution) for solving module to file mapping with Modules TS or similar?
&gt; give them a solution that isn't just "use the thing no one likes". When you are using macros, you are already using the preprocessor (I assume that is what you meant with "the thing no one likes"), so what would be wrong with using the preprocessor for fetching those macros via an include.
It would certainly be happy with a simpler compilation process, indeed. That's why we need less preprocessor, not more ;) Having a clear separation of how we use the language (importation) and how we use the preprocessor (includes) is a great start. Then, incorporating stuff like reflection, metaclasses and compile time programming will reduce the need for macro even more, and maybe completely remove the need for it. By that time, let's not break how the whole language work please? 
This looks nice! So IIUC, your `folly::Poly&lt;...&gt;`is a facade/adaptor for any abstract interface, that will look into the interface's `Interface` and `Members` templates for what to actually call? Hopefully /u/louis_dionne/ will accept your code challenge and provide something similar for Dyno :) 
Jitting C++ would be awesome, imagine live editing a running program!
&gt;Does this concern you? Of course it does. It does not mean that the solution must be at the language level -- you can do more harm there. &gt; I would be afraid that this could cause a logistical nightmare especially since BMI format is also implementation defined. but there is no need to prematurely panic or to cry wolf. We need to conduct a calm analysis of the implications and opportunities. &gt; I don't, off the top of my head, see how any implementation could get around (at least partial) pre-parsing all modules in the search space. Why would it do that? 
I think the context here (at least what johannes1971 is trying to point out) is that this only works if you put the module implementation and the module interface in an interface module and implementation module respectively. But what johannes1971 wants to do (if I'm interpreting correctly) is to put both the interface and the implementation in a single implementation module and not suffer from increased build times. Do you mean that VC++ working to resolve that?
At the implementation level, VC++ provides `/module:reference` for resolving modules and corresponding IFC. That scheme can be extended to source files or MANIFEST of sort. My biggest concern is the possibility that the C++ community will miss an opportunity to bring the tooling ecosystem into modern settings.
Well, being able to JIT C++ is one thing; requiring JIT is another :-)
Thanks for the encouragement :-)
&gt; You can't use includes as a get out for not being able to export macros Why not? &gt; The preprocessor allows you to include a file inside a namespace, while not elegant, it fixes the issue of poorly mashed library classes/namespaces/etc. Only works for header only libraries and even there only in the rare cases where one header doesn't include a header from a different library (e.g. the standard library) &gt; Today you normally include the cpp file at the end of the header, modules don't help here. Irrespective of that is really the "normal" way to do it, you can do the exact same thing with modules.
That's certainly true, the iterators are weird and if you want to use them properly with the standard library / range based for loops etc. you'll probably want to write a wrapper. Personally unless I just quickly need one of the more complicated graph algorithms (in which case I find LEMON slightly less cumbersome) I'll just roll my own graph class. Based on the lack of modern C++ graph libraries that appears to be what everyone else is doing too.
Unreal is funny because of how organic it has grown. The amount of gameplay code still in the engine is pretty surprising. They also have multiple libraries for things like Math because of how Blueprint works :( UE4 compile times have also gotten noticeably worse within the last 2 years.
&gt; non-c++11 style api, to be fair, C++17 makes it more pleasant to use. for(auto [o1, o2] = out_edges(*v1, g); o1 != o2; ++o1) std::cout &lt;&lt; g[*o1].name &lt;&lt; ' '; or even if(auto[o1, o2] = out_edges(v, g); o1 == o2) leaves.insert(v); 
Simpler than that, actually. `folly::Poly` will inherit from the interface's `Interface` template, so it gets the right member functions. The interface, in turn, looks up the right function to call in `Members` by way of `poly_call`.
&gt;&gt; is that this only works if you put the module implementation and the module interface in an interface module and implementation module respectively. There is an underlying assumption in this statement that build system relies solely on modified time of the file to decide on whether something has to be rebuild. If we are not constrained but that assumption, I can see no fundamental problem in figuring out if users have to rebuilt even if your entire modules is in a single file. Turbo Pascal has been doing it in the 80s. 
&gt; &gt; You can't use includes as a get out for not being able to export macros &gt; Why not? Because the modules paper says that the preprocessor should be decommissioned. How do you do separate the template definition and declaration using modules?
&gt; but there is no need to prematurely panic or to cry wolf. We need to conduct a calm analysis of the implications and opportunities. I hope I'm not giving the impression panic, wolf crying, or lack of calm. I'm trying to understand the boundaries and and implications of Modules TS by asking the experts (you guys). &gt;&gt; I don't, off the top of my head, see how any implementation could get around (at least partial) pre-parsing all modules in the search space. &gt; Why would it do that? Unless I'm missing something it seems the module to ~~module-file(s)~~ module-interface-file mapping has to be done at some point. AFAIK parsing a module file is necessary to determine what it exports which would be necessary for determining that mapping. At some point that parsing has to happen. It would seem that it would either have to be done on the fly or pre-computed and then have that precomputation of the mapping passed around. For includes the analogous mapping is resolved by passed around via a bunch of Include paths and using standardized presidence rules(which is itself not ideal/disgusting). I'm wondering if there is a reference or straw-man approach for passing around or performing this mapping once/if Modules TS is accepted. I think you might be starting to answer that question in your response here: https://www.reddit.com/r/cpp/comments/7a3t2w/common_c_modules_ts_misconceptions/dp7dm7p/
&gt;&gt; modules paper says that the preprocessor should be decommissioned I am pretty sure that this is an exaggeration. Original module paper http://open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4047.pdf was talking about *isolating from macros* as one of the explicit goals, as in no macro can leak in or out of the module, but, that simply follows from: "a module unit acts semantically as if it is the result of fully processing a translation unit from translation phases 1 through 7". Modules TS does not preclude the use of preprocessor in any way. 
Can you specialize `Members`? I can see how it "maps", but it looks like it only maps to one thing unless I am mistaken.
That's cool to know that it's possible to do such things. Thanks for the concrete example
Glad you liked it :) Thanks for the inputs. Out of curiosity: Would C++14 support help? 
Because now the user, needs to both import the module, and include a header. I don't want to use _more_ of the preprocessor. If I could, I'd avoid headers entirely, I'd not use macros. I'd prefer if we could, maybe we can't according to /u/gracicot (sorry I haven't gotten around to reply yet), in lieu of having all these other proposals fast-tracked, having macros exported.
I work in UE4 occasionally, and am very happy to move back to my own game code for it's fast compile times (even without Runtime Compiled C++) and near instantaneous load times. But then I remember I have about 0.000001% of the features of UE4...
You can't specialize `Members`. 
C++11
Is this asynchronous spdlog?
Thanks for the link, the revision I saw specifically mentioned the goal to not use the preprocessor.
In the docs, it is repeatedly mentioned that in C++14 you have to use a macro, whereas C++17 is macro-free. What C++17 feature is in play here? 
Glad you liked it :) I liked your chunked stream suggestion. I think it'd be pretty easy to implement it as a separate logger in llcpp. I'll find the time and give it a try. Regarding the full object logging, I'm afraid it'd be much harder. At it's core, what you're suggesting is very similar to serializing objects. Yeah, we don't have to create the JSON boilerplate or get the type name, but without proper compile-time introspection, it's almost impossible to do what you want. I say "almost", because (AFAIK) you could do it only if you start messing with the class definitions of the objects you want to dump. See [cereal](http://uscilab.github.io/cereal/quickstart.html) or [sqlpp11 table definition](https://github.com/rbock/sqlpp11/blob/develop/tests/Sample.h) for how people usually do it. Personally, I love these approaches for their hacking story-value and *get shit done* attitude; as a developer though, they're rather annoying. However, if this is something that will be valuable to people, and these people are willing to accept the need to add more code to make it work, I think it could be done.
Java and C# don't necessarily require JIT, in fact there are plenty of C++ like compilers to choose from, not paying attention to the work done by UWP teams? :)
Nope.
I understand it's inefficient all right, I'm ranting about it needing a copy after all. You can already get away from the copies by wrapping your stuff in a reference type, the issue is the syntax is longer so it's a pain. 
Am I correct in assuming that this build system would not work to make makefiles for Linux?
Fair enough. Apologies for my role in wrong footedness. :)
In the [homepage](http://tensor-compiler.org/) it is said that the performance of this library is "competitive with other high-performance sparse linear and tensor algebra libraries like Eigen, Intel MKL, and SPLATT". No detail is given though.
I watched the rest of the talk and saw that Linux seems to be supported. Seems pretty nice, I'll give it a look. Nice work =)
Nice work! I am implementing similar constexpr format string parsing for [the fmt library](https://github.com/fmtlib/fmt) and wondering if you've done or plan doing any benchmarks of compile times? Also how do you handle user-defined types in logging?
Two, actually. Auto template parameters and constexpr lambdas.
Look, that's not fair. If you need metaprogramming, code generation, reflection etc. -- this is a concern for additional proposals not related to modules. I understand that the preprocessor can be abused to do amazing code generation (e.g., Boost.PP) but I have to be honest, I wouldn't allow any of that stuff in my company -- it turns code into an unmaintainable mess.
Here's how I see it: * The first goal after modules are supported is to completely modularize the Standard Library * The Standard Library uses macros * Ergo, two options here: either stop using macros or somehow massage macros into modules We are talking about Modern C++, so which is more modern - removing macros or accomodating them? If we remove them, we get an instant benefit: all greenfield development that use the new macro-free Standard Library now no longer needs the preprocessor. 
I LITERALLY just wrote a library that does most of this a day ago... I guess I'm not as clever as I think I am :(
Honestly, depending on how granular we want modules to be, it might make more sense just to have the build system specify the available modules much like it already specifies which libraries to link. That way there would be no need to preprocess anything to find the list of dependencies. The cost would be *some* level of redundancy between the build-side list of dependencies and the source-side bringing things into scope, but if a module is basically a library then that's no different than today, and fairly standard across the whole spectrum of languages. I'm not familiar with the discussion around the current design of the module system- /u/berium do you know if this was considered at all?
This really makes no sense... shared_ptrs are the logical choice anywhere you have resources shared among multiple objects. Sure it could be marginally more efficient if you force the programmer to be responsible constructing / destrucing said resources.... but that kind of defeats the whole point of abstracting code i.e. making it easier to write.
Don't forget the upcoming `std::observer_ptr&lt;T&gt;` for when you need an optional, possibly null reference! (The alternative would be `std::optional&lt;std::reference_wrapper&lt;T&gt;&gt;`.)
&gt; Support for all consoles. However, some platform holders prefer the code to not be public, so you have to go through them to get the code. I'm curious - what code do they not want to be public? Code on how to build on their platform...? That seems very odd?
I did find it a bit condescending towards junior devs as well (as if all these mistakes were their "faults"). I still kind of enjoyed the talk though.
Runtime Compiled C++ kind of does it already.
Really like to see libraries using C++17! :-) Nice! The question I'd ask though if I was looking for a logging library: Does it compile on VS2017.4 (or VS2017.5 Preview)? I think if using C++17, it's wise to stick to the common subset that runs on clang-5, gcc-7, VS2017.4 - at least as of today. 
Thanks :) I've done a compile time benchmark based on [format-benchmark](https://github.com/fmtlib/format-benchmark) from fmt. The results are that `llcpp` takes twice as long to compile and about three times in size as `fmt` on the bloat benchmark. I admit, it's not that great to say the least ;) Keep in mind that I haven't spent any time optimizing the compile-time code. I understand that it could be crucial for larger code-bases, but it wasn't my goal. While others may prioritize differently, In my opinion: run-time &gt;&gt; compile_time &gt; size. I've wanted to try [templight](https://github.com/mikael-s-persson/templight) to see if I can improve on the compile time issue, I'll update when progress is made on that front. Regarding user defined types: at the moment they're not supported. As I've replied to matthieum's comment, that would definitely require extra code from the user. I believe that's the way `fmt` tackles this feature, right? I have a couple of alternatives, they're still rough sketches and I've yet to decide which is better: 1. Split the log lines when a special terminator is reached (i.e. `%v`). Then output the first half, then output a user provided log line (kinda similar to how I've implemented prefixes), and then output the last half. Obviously, do this as needed for more special terminators. 2. Replace the special terminators with user provided format (char tuple concats). Then evaluate this as if the user has entered everything by hand. This would require better matching between the call arguments and the format arguments. 3. Make the special terminator a kind of *nested* log line. So in the section of the binary format that would usually hold values, you'll find a nested binary format: the format string and then values. While this will probably be the easiest one to implement, I'm afraid it would make the format even less robust than it currently is...
That makes two of us ;) I'd love to see your work though!
Unfortunately, no VS support :( The reason is that I use a GNU extension for templated user defined string literals (see [this](https://github.com/blapid/llcpp/blob/master/include/llcpp/detail/udl.hpp)), which is only supported by GCC and Clang. This is supported for these compilers with C++14, thats why making a C++14 version won't be too difficult. An alternative would be to use something like [typestring](https://github.com/irrequietus/typestring), but it involves macros and would break for log lines which are long enough. A third alternative would be something like [hana strings](https://github.com/boostorg/hana/blob/master/include/boost/hana/string.hpp), but that also requires macros and would make C++17 a hard requirement. Another light at the end of the tunnel is [n4121](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4121.pdf) and you might be interested in reading [this](https://blapid.github.io/cpp/2017/10/31/llcpp-a-quest-for-faster-logging-part-2.html#fn:1) footnote from the blog posts. I'm open for suggestions though, if you can figure out a way to make this work in VS which doesn't involve macros, please let me know!
/u/sakarri Already made a few good points I agree with so I will not go over those again The one problem I can see with auto is having function with auto signatures than declared as the rval for a variable assigned with auto, which basically means you need to look at the functions body to see the type of the lval. But that's a unicorn I spotted maybe once and to be honest I would say the fault here stands with the standards committee for allowing auto return from non lambda functions to being with. But, again, this is a unicorn case and any reasonable coding guideline would probably make it go away. &gt; I find myself stressing the importance on having to manually manage resources appropriately and not rely on the compiler That is, in my opinion, a wrong approach. You should rely on the compiler, when it comes to low-level memory management, loop unrolling, using the proper asm instructions, optimizing operation ordering... etc, the compiler is likely better than you and optimizing things manually will result in unreadable and slow code. This is exactly the kind of argument I (a young developer) would have with a more elderly developer and I think any reasonable C++ developer would back me up here in saying that: You should rely on the compiler to optimize for you. (From the "functional guy" picking up C++ to the Bjarne Stroustrup himself) If you have a piece of code that's killing you performance wise and you just can't get it to work and you run cal/valgrind and you see "Ah, blast, I could unroll this loop manually or use move semantics implicitly or memcopy this thing over here and get the performance I need", good for you... I don't think those situations come up very often and the more advanced the compilers get the less of an issue this will be. Sure, there are situations when low level memory management is still useful, I wouldn't want my high performance database to use streams and smart pointers to load resources into memory... but, if a junior developer is writing the critical parts of your high performance database you have bigger problems to worry about at an organizational level :p &gt; This, IMO is because they don't understand the original concepts of the language that the newer features are enhancing or abstracting. Partially yes, partially no. Some of the original concepts of the language can be tracked down to C being poorly designed. C is wonderful language and I wouldn't dare blame the guys that designed it for the decisions they took more than half a century ago. However, with a modern perspective I think anyone half sane can see: * That null terminated string, are simply a bad choice. * That array decaying to pointer is retarded (bad for code quality, unexpected, hard to understand, useless... etc). * That not having a comprehensive standard library is poor choice. (Hello people still manually writing lists !) * That having a type system which is so poorly designed you can't have a memcopy with the (T* src, T* dest) signature or a free(T* res) is, again, a poor choice. * That the amount of UB the language allows for where it could instead generate compiler errors is a dick move * ...etc, I'm not familiar with C, if I were (and if I had the time), this list could go to infinity. So I wouldn't blame any modern C++ developer for being completely shocked by those concepts, because they are illogical for modern programmers and rightfully so. I understand why C++ had to go with them, because there's this (arguably useful) cult of not breaking backward compatibility. However, if I had to teach a developer to code in a system programming language 10 years from now and C++ still had those concepts embedded in the language, I'd say "fuck it" and teach him a more logical language like Rust or D rather than having him fight with legacy left to us by people which are long dead for reasons which are long gone. I would say that if new C++ developers don't want to understand the nonsense that is left to C++ due to an inheritance from C they have a right not to and the committee and compilers maintainers should respect that right unless they want people to start switching to language without that burden. But, going back to the original point... I can't see many cases where these legacy constructs within the language would be relevant to a "junior" developer and if they do that means whatever codebase that junior dev is working on is in need of a serious refactoring.
Shared pointers are useful also in single-threaded code. I for one use it a lot for the ability to have a weak pointer. Imagine a system where you at some point in the future would like to schedule some action to take place on an object. How do you store that information? A nice and generic solution is to store the object in a shared pointer and store a weak pointer in the object that might later execute the action. When it is time, lock the pointer. If it is not present, do not execute the action.
ANY language can be faster than any other language in a small enough test. By "many" they mean 3 of 10. C is very fast because it is a simple language which has a lot of time put into compilers and optimizers. In that rust post on reddit there is actually a long discussion of issues of benchmarking. The language benchmark explicitly focuses on idiomatic code and not fastest case code. Make sure to read http://benchmarksgame.alioth.debian.org/dont-jump-to-conclusions.html Feel free if you would like to dig into some of the ways you could make the C++ versions much faster.
I don't really trust language benchmarks, for reasons including what you mention. If you were expecting your code to run faster, you might have a problem with the language. If you "just want it to run faster" C/C++ and Rust are in a similar class, JIT compiled and runtime languages are in a slower class, and scripting languages tend to be at the bottom with the most overhead. That said, C lacks features that C++ provides, and those features come at a cost. Sometimes, the compiler can optimize the final code so that the performance delta is negligible. In such cases, the language matters less than the compiler toolchain. Languages in the same class can usually be optimized to about the same level of performance, so rewriting code from C++ to Rust may have negligible performance improvement for the effort it took to refactor. 
Good timing, I was thinking of swapping my logging over to spdlog, but have delayed because it doesn't seem to have a feature to not evaluate the arguments when the log line is below the log level. The framework I'm currently using does have that, and I could add it into spdlog easily enough though.
Mine is unfortunately part of (and sorta tightly coupled with) my personal utility library I'm rewriting right now. I made a few design differences though: - performance trumps everything, so the code is not as nice as I'd like it to be - log lines must be &lt; 64k (less data written to file) - uses pre-allocated memory mapped files (synchronization with filesystem is always explicit) - each thread allocates it's own log file, so all log files in a session have to be merged offline (to avoid any sort of program-side synchronization, except tls I guess) - concatenative formatting instead of printf-style (so you could theoretically hook up your own formatters I guess?) The performance is pretty good but I haven't done comparative benchmarks yet cause some features are still missing. If you REALLY wanna check it out, it's [here](https://bitbucket.org/kronikarz/ghlib2/src/82e465bf2e933272d83be6210f7e7b6dad44c286/Source/ghlib2/Log/MassLog.h?at=unstable&amp;fileviewer=file-view-default) (ugly).
I remember hearing something about how you shouldn't mix `import std;` and `#include &lt;vector&gt;`, for instance. Am I mistaken? If not, how am I supposed to use any library when I want to use modules if they don't all upgrade immediately?
Their are some API's for specific platforms controlling certain console specific features (For ps4 I believe things like the touch pad thingy on the controller). Just like the dev kits they are under a NDA (or similar agreement). You are not allowed to show dev kits to people outside the company and they should be stored in a room without windows. I know this doesn't really answer the question but this is not my expertise. (So take my answer with a grain of salt). I also have no idea why some dev kits are under such a strict NDA.
It's almost 2am and I didn't read in detail, but I have a question - why do you parse the format string beforehand? Why not just save the raw string and arguments (e.g. `fwrite` them, possibly prefixed with length), and then parse+display them offline in a separate "log reader" program.
People overuse anything. I have seen code use std::unique_ptr for objects that could be placed on stack, including std::strings. 
At one point, I had improved the performance of the C++ n-body code to be faster than any other language on 64 bit (even the C version). However, once you start talking about differences of 5%, all sorts of weird effects start to become important. The most obvious thing is that I do not have the exact same hardware as what the benchmark uses, so optimizations on my machine are pessimizations on the benchmark machine. So as long as you keep that margin of error in mind, the benchmarks are still useful. So while Rust is, indeed, probably as fast as C++, Java and C# still lag.
Low-lantecy logging is what I used to work on in a previous job, so I love this kind of stuff. Your implementation seems cool but ultimately doesn't solve any real-world problems as far as I can see. Also I can't tail a log file any more :( &gt; In the case of async loggers, they show relatively low call site latency, but if you measure the total time to completion (that is, including actually writing the log) you don’t necessarily get better performance from them. This is a moot statement. Low call site latency is exactly what you want from a fast logger. If you claim that actually formatting and writing the log results in slower performance, then I can simply claim that parsing your binary file with a python script is even slower, because it is. You could also do a lot to reduce the amount of logging that really happens if you're serialising it. The most obvious is, you don't need to print "LOG" every time you log a line because that's implied. You also talk about using ```time``` and ```std::chrono::system_clock::now``` to measure time and you talk about nanoseconds. Neither of those options are nanosecond-accurate. Casting the time to ```std::chrono::nanoseconds``` does not make it nanosecond-accurate, it only gives nanosecond-precision. If you are serious about providing nanosecond-*accurate* timestamps I'd suggest looking into [```rdtsc```](https://en.wikipedia.org/wiki/Time_Stamp_Counter), an x86 instruction that gets the number of clock cycles since reset. You then use ```std::chrono::high_resolution_clock``` as a baseline and then interpolate between the timing points with your ```rdtsc``` value. This is the most accurate you're going to get in software, without using some kind of hardware-based time source as the baseline. Now, this change (using ```rdtsc```) won't really affect the relative scoring of your system against other loggers, but it will improve the accuracy of your timestamps. &gt; Each framework have showed between 10,000 ns and 1,000,000 ns in different runs. Seems to me to indicate some flaws in the libraries, and/or some kernel magic happening. I'd be interested in how much memory those libraries are using; those latencies are similar to page fault latencies. I'm not sure why ```sched_rt_runtime_us ``` would affect other libraries and not yours. I'm also not sure what those benchmark numbers are. Are they nanoseconds? Micros? What is it timing? Having a latency and throughput measure would be much more useful, than these meaningless benchmark numbers.
On x64, an 8 byte object should just get passed in a register (if it's one of the first four arguments):https://msdn.microsoft.com/en-us/library/zthk2dkh.aspx
The main reason is it lets us verify that the arguments you've passed to the log call matches the arguments in the format string. Just like (and potentially better than) what the compiler does when you make mistakes with `printf`. It also opens up new possibilities, such as fixed length string formatting (see [date/time month formatting](https://github.com/blapid/llcpp/blob/master/include/llcpp/detail/prefix.hpp#L37)), which are a bit faster and can be more compact. Another idea is to replace some of the formatting in compile-time when their values are known in compile-time. I haven't really explored that path yet, but the compile-time parsing lays the framework for that. That being said, it could be a good idea to allow the users to disable the compile-time parsing if they don't need those features. I'll add it to my backlog.
We use it to do that in the past. I don't know the exact status of Linux support today, but I suspect it needs some love. We have run it on Linux itself as well, and we were discussing to put that back using .Net Core, but we had to introduce a plugin system first for NDA platforms in priority before CppCon. What I can assure you is that if any platform support is added, we can enrich the tests and CI on Github to make sure it continues to work without regressions. Adding support for platforms in Sharpmake tends to be fairly easy.
In the Fortran world module files are implementation specific. It made a giant headache trying to distribute packages. I think this is a huge mistake. 
So far, only Microsoft and NVidia have accepted their under-NDA platforms Sharpmake support to be open-source. Other platform holders might change their mind later, but it's clear that Sharpmake would violate our NDA with them and they could refuse for that simple reason or they could be afraid to open a Pandora box if they make an exception for a part of what is under NDA. By generating FastBuild makefiles, Sharpmake is exposing more details about the platform SDK, like how compiler and linker are called, etc. I personally think it would be better for their platforms to accept and embrace open-source in that case, but I completely respect their right to decide. After all, it can be considered a legal issue and there is no turning back once they say “ok”.
He told me
Nice!!
First of all, thanks for taking the time to read and comment :) &gt; Also I can't tail a log file any more :( Well technically you'd just have to pipe it through a smart-enough parser (admittedly, smarter than the one I've implemented so far). &gt; This is a moot statement. Low call site latency is exactly what you want from a fast logger. If you claim that actually formatting and writing the log results in slower performance, then I can simply claim that parsing your binary file with a python script is even slower, because it is. Parsing it with a python script is indeed slower, but as I wrote in the posts, that claim is missing the point. Parsing can be done either (much) later, on demand, or on a different machine. Why burden your over-loaded, performance critical machine, with that work? Regardless, in my opinion, what you want from a fast logger is *both* low call site latency *and* better overall performance (throughput). You really have to consider both; obviously, with respect to your specific application needs, don't you agree? &gt;You could also do a lot to reduce the amount of logging that really happens if you're serialising it. The most obvious is, you don't need to print "LOG" every time you log a line because that's implied. Not quite sure what you mean by that. Could you elaborate? &gt;You also talk about using time and std::chrono::system_clock::now to measure time and you talk about nanoseconds. Neither of those options are nanosecond-accurate. Casting the time to std::chrono::nanoseconds does not make it nanosecond-accurate, it only gives nanosecond-precision. If you are serious about providing nanosecond-accurate timestamps I'd suggest looking into rdtsc, an x86 instruction that gets the number of clock cycles since reset. You then use std::chrono::high_resolution_clock as a baseline and then interpolate between the timing points with your rdtsc value. This is the most accurate you're going to get in software, without using some kind of hardware-based time source as the baseline. Now, this change (using rdtsc) won't really affect the relative scoring of your system against other loggers, but it will improve the accuracy of your timestamps. Thanks for the tips. The base for my benchmark was NanoLog's benchmark, which used microseconds, but the calls were too quick to be able to distinguish with that precision. I'll try `rdtsc` and update. &gt;Seems to me to indicate some flaws in the libraries, and/or some kernel magic happening. I'd be interested in how much memory those libraries are using; those latencies are similar to page fault latencies. I'm not sure why sched_rt_runtime_us would affect other libraries and not yours. It's quite easy to see, [this](https://github.com/Iyengar111/NanoLog/blob/master/nano_vs_spdlog_vs_g3log_vs_reckless.cpp) is the original benchmark (I'll updated mine soon, but it's pretty much the same). Compile it on your machine and give it a go. Anyway, I can't say I'm an expert in this area, but using `sudo time ...` shows 4 major faults across the board, 1600 minor faults for `llcpp` 2600 for `spdlog` and around 50,000 for `NanoLog`. I'm also very puzzled by the `sched_rt_runtime_us` thing. That's why I've left those results out. &gt; I'm also not sure what those benchmark numbers are. Are they nanoseconds? Micros? What is it timing? Having a latency and throughput measure would be much more useful, than these meaningless benchmark numbers. Which numbers are you talking about? I just realized that I forgot to add it to the github README but in the post I wrote that everything is nanoseconds unless stated otherwise before the tables. 
I use it to implement delegation and to avoid having a strong retain cycle between the object and its delegate which holds a weak reference to it
&gt; The language benchmark explicitly focuses on idiomatic code and not fastest case code. In practice it doesn't, the C++ for mandelbrot uses OMP, the one for n-body uses SSE. 
game. changer.
I guess the result just means that the re2 library is slower than pcre.
I thought that worked with `std::stringview` now. Or do you want to get it to work with `std::string`?
What macros in the standard library couldn't be removed in favour of something better? `INT_MAX` should be `constexpr int INT_MAX`. `NULL` can die, that's not proper C++ since `nullptr`. We should remove as many macros as we can because we can do better now.
I've worked with a logger like this - very fast. It went one step further - it did *not* write out the format string. It just wrote a number that the viewer would look up to find the format string. All format strings were gathered into a table via some custom build step. This was obviously more headache to maintain (and track versions, etc), but faster.
I’m no expert but I could have sworn on a CPPCast episode where they interviewed the original author of the chrono library he said high res and system clocks were the same. If I recall correctly he even went so far as to say he regrets adding in high res clock. 
Is there any legit use for macros assuming the meta programming Herb Sutter mentioned make it through the standard? It took years before `constexpr` got somewhat usable, and that could be the same for modules. But thinking of the future, not encouraging the use of the preprocessor is a great thing.
I think the benchmark game does not provide you with absolute "language speed ranking", the focus is more on languages/compilers that have the similar level of performance. [This](https://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest-firstlast.svgz) clearly divide languages into 3 categories: fast-faster-fastest
Don't sweat it. It's such a new project that by the time all the kinks and corner cases get worked out C++17 will be more widely supported. I'm going to try it in a test env with C++17 at home first to get a feel for it.
Macros are only there because we haven't improved meta programming enough yet. But hopefully, by C++23 we will have a perfect replacement for macros. Deprecating them should start early. Everyone agrees that macros are evil and you only use them because you can't do without.
Another idea for logging (which could be a layer on top of your logger) Many loggers have "severity levels" - ie only write this log if we are running at debug serverity, etc. ie (but usually more levels) trace level - logging of normal control flow LOG_TRACE("calling foo()"); ... LOG_TRACE("inside bar()"); warning level - something not good happened, log it, but continue (return error, throw, etc) LOG_WARN("file not found %s", file); fatal error level - log this line then die LOG_FATAL("WTF!?!?"); Now you don't want your logs full of TRACE stuff, because it just spams up the logs. So you run at WARN level instead of TRACE level. Great, but when a bug/crash happens you look at the logs and say "wish we had TRACE info now" So here's the idea: Save "recent" TRACE info "for a while" - in memory (circular buffer). Only write out TRACE info to file if you hit a WARN or ERROR etc line. So this doesn't save much logging time, but it does save logging space. ALTERNATIVE: Run a hybrid: (spam filter) X (compressor) on your logger as you output stuff. "Oh this message, I've seen that a million times, let me just collapse that to "logged FOO a million times in the last hour" 
Yes, it was considered and the current spec supports that scenario too.
&gt; I remember hearing something about how you shouldn't mix `import std;` and `#include &lt;vector&gt;`, for instance. Am I mistaken? That is incorrect. You may have run into a compiler bug, but the spec never calls for no mixture of that nature.
That's good. I'm glad I was mistaken. I haven't tried out modules yet, but I've watched all the videos; I definitely misheard something.
https://www.reddit.com/r/rust/comments/71rbui/difference_stdtime_and_time/dndbjvr/?context=4
&gt; Because now the user, needs to both import the module, and include a header. Which is actually fine, and one would argue better: now the user can tell when to expect isolation and code hygiene, and when she might get the old time-honored CPP macros and its effects. She has a visual clue of how much composition is there.
Once we get source location and stringification, together with modiles, they will cover almost all uses of macros. However, there will always be things like `BOOST_OUTCOME_TRY`; basically something that the language cannot yet do. IMO, to remove the preprocessor completely, we have to add a different, improved macro system in for the rare cases where the language is deficient. Naturally, these macros should be replaced with first class language features as they are discovered.
There are probably many edges cases where nobody has thought a proposal for obviously, but hopefully the required language features will be implemented soon enough.
&gt; The modules paper wants to remove all use of the preprocessor, in its current form it doesn't even come close. The [module design](http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2016/p0142r0.pdf) never pretended to remove the preprocessor. In fact, it specifically states, section 4.1 on page 5: &gt;&gt; While many of the problems with the existing copy-and-paste methodology can be directly tied to the nature of the preprocessor, this proposal suggests neither its eradication nor improvements of it. Rather, the module system is designed to co-exist with and to minimize reliance on the preprocessor. We believe that the preprocessor has been around for far too long and supports far too many creative usage for its eradication to be realistic in the short term. 
I would be interested to look at that revision.
Ha, `constexpr` was usable from the beginning :-)
I just saw a talk by one of the authors, with good looking performance numbers in it. They should also be in whatever actual published papers there are about it.
No worries. This [2015-era paper on transition paths](http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2016/p0141r0.pdf) actually laid out in section 4.2.2, on page 6, how you could have `&lt;vector&gt;` imports `std.vector` (assuming that is a thing) so that both the import declaration and `#include` co-exist.
I mean, I think that's only a perceived benefit if we assume modules would auto import macros. If the user has to opt-into macro imports as suggested by [p0837](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0837r0.html) then we get that same benefit. I mean, a library that I work on for example has a macro that the user doesn't _have_ to use, but it's really unfortunate otherwise: https://gist.github.com/playmer/5d4ed556eb0fd059a6ffb13687486b62
He probably said something like "system and high res will often be the same clock", because often the system's wall clock IS the same as it's high res clock. Indeed, the standard says "high_resolution_clock may be a synonym for system_clock or steady_clock." That said, in the case your system *does* have a higher-resolution clock than the system clock, you want to be sure you are using it.
Whenever I see yet another logging library I'm always dismayed when find the same shortcomings. Skipping runtime formatting is at least a step in the right direction. But unstructured string-based logging quickly breaks down under how volume. If you require a post-processor anyway, why not go all the way and strip those strings entirely. You log only the minimum amount of data necessary, and identifying log events using an ID with additional metadata also goes a long way to provide proper filtering and searching.
It was usable, but had many failings. Having to turn `if(x) return 0; else return 1;` into `return x?0:1` might be a minor annoyance, but this forced you to rewrite a lot of code if you wanted to move to `constexpr` for as much stuff as possible. And maybe in C++30 we'll have `virtual constexpr`, which will only compile if the virtual type can be statically determined at compilation time or something. And the whole program will be able to compile to a single return statement.
Support for -std=gnu++14 would would suffice for me!
This is neat, but is an paying a memory and performance price. A destructor of your object can remove the object from the scheduler with less overhead.
Or you could simply say that the owner of the shared resource is whatever outlives sharing objects and give them a simple reference to the shared object. If that abstraction is easy, it is more simple and more idiomatic C++ (use references when you *do* have an object). IOW, your choice is not so logical to me.
Try fmtlib. Spdlog is a logging framework based on it.
Will private members of exported classes be visible in other TUs too?
I don't know why this is, but I've never seen my biggest concern about the current modules proposal addressed. And that is the fact that modules could provide the opportunity to finally retire namespaces, but this opportunity is just thrown away. Why are namespaces bad? Because they don't really do anything for me. Name conflicts are rare and namespaces force a bunch of syntactic clutter on me for no good reason. Modules could provide an import system similar to Haskell's that allows you to choose which symbols to import, only import symbols qualified or hide certain symbols that would produce a name collision. But instead of taking this route, the authors insist that namespaces are a good idea and worthwhile keeping separate from modules, thereby locking us out of having a sane import system forever.
I will try to find the one I read, if it is, as it seems it may be, outdated, then I'm very pleased that you are now embracing the preprocessor as a currently useful part of the build. Either way, modules feels to me a copy of the msvc declspec import and export. I really really want modules to work, but given the chance to do something big that works properly, I feel you've basically done half a job and we'll be left with something that never quite works as we want. I'd want something similar to the python system where an import generates a scope. I'd want something that makes it easier for me to write code, not something (like the msvc declspec) that gets in the way. How do you import two modules that export the same name? Is it possible?
It's 2017 and we are still mucking about doing logging frameworks, next you will tell me that there is yet another text formatting library. It's time to move on! 
&gt;This all came out as a surprise to me because I always heard that with C++ you can get at least C performance as it is almost a superset of it. That‘s right. 
Thanks for the explanation!
Would the cost of the advanced features C++ has compared to C affect the benchmarks if they are not used?
I've tried out build2 on a small project using trunk clang and modules, and it seems very promising. I wanted to try out a completely preprocessor-free and "everything in a single file" style, and it worked like a charm. What I'm not sure about is whether or not I can implement class member functions in-class without the compiler inlining everything. As far as I understand in the current standard every in-class implemented function will be marked as inline implicitly, but it's up to the compiler to decide what will be actually inlined. So what will it actually do in the case of modules? I think it would make sense to be able to write "class foo { int bar() { return 42; }};" instead of "class foo { int bar(); }; int foo::bar() { return 42; }", à la C# / Java.
In the video, Mac support is mentionned,... does that mean Sharpmake can generate XCode projects?
I think there is mixed use in the same project and mixed use in the same translation unit. In my experience, none of the current implementations handle mixed use within the same translation unit.
Glad to hear you've enjoyed modules support in `build2`. Regarding inlining, I don't believe the modules specification changes anything in this regard.
We had this discussion before and I still feel we are on a different wave length, but let me try ;-) First of, I'm only going to discuss the very common situation in which .h and .cpp files come in pairs, like for example button.h and button.cpp. It doesn't matter much for the discussion, it just makes it easier to talk about things. In my .h file I have the following: // comment block with meaningless corporate mumbo-jumbo. #include statements /*! class description. */ class class_declaration { public: /// function description. /// @parameter name description. void function_declaration (type name); }; /// Global variable description. extern type global; Pretty simple, right. And in my .cpp file I have: // identical comment block with meaningless corporate mumbo-jumbo. #include statements, at least one of which is for the .h file above. void class_declaration::function_declaration (type name) { First of all, we have lived with .h/.cpp pairs (it is usually pairs) for decades, and of course we can continue to do so in a module world, with one file responsible for the exported declaration, and another responsible for the definition.
Given that C benchmarks are also C++ benchmarks, benchmarking of C against C++ only benchmarks specific implementations and not the languages themselves. Take the C regex example: it can be ported to C++, and algorithms based on void* types can be replaced with templates, and be faster than C. 
It could be argued that `return x ? 0 : 1;` is more readable :-) &gt; And maybe in C++30 we'll have virtual constexpr, which will only compile if the virtual type can be statically determined at compilation time or something. And the whole program will be able to compile to a single return statement. I understand the sarcasm in the comment; but remember that back when I introduced constexpr functions (even the more restricted form that you are disparaging), there was no shortage of people opposing it on the ground that it was unsound, unimplementable, "you should be scared because it requires a VM for C++", etc. I now watch with amusement because the same people are now saying they aren't "powerful enough". In any case, please have a look at section 5 titled "Object Orientation" of the [generalized constant expression paper](http://www.axiomatics.org/~gdr/constants/constexpr-sac10.pdf) for where my real thinking was.
.obj files and executables and linking models are all implementation defined today too and that doesn't cause problems.
I don't think so. C++ has a philosophy of "don't pay for what you don't use". As far as I'm aware, the only feature in C++ and not C that imposes a significant runtime cost is runtime polymorphism. Pretty much everything else can be abstracted away at compile time. Lambdas and templates can actually improve runtime speed, because a lambda passed in to a templated parameter can be inlined by the compiler, while a function passed as a function pointer typically can't. 
The name made me smile. I think I would have a good time using it.
Maybe add a convenience layer called `napkin`?
&gt; I will try to find the one I read, if it is, as it seems it may be, outdated, Let me be clearer than that: that paper may have existed only in your imagination. I never wrote the words you are trying to credit me with. [Here is the first edition](http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2014/n4047.pdf) of the design paper. I very much want to believe you have misunderstood something and I want to give you the space for that. But, please don't use that for willful advancement of disinformation - the sort of misconception that /u/berium's post is denouncing. &gt; modules feels to me a copy of the msvc declspec import and export Please be more specific; this statement sounds like FUD. &gt; but given the chance to do something big that works properly, I feel you've basically done half a job and we'll be left with something that never quite works as we want As ever, I am eager to learn from those who actually solved the problem. Any concrete reference that completely solves the problems that "we" want will help. &gt; I'd want something similar to the python system where an import generates a scope. That is what you want; I don't know that is what "we" want. Furthermore, I am trying to solve a problem in the C++ context, that works for C++ at scale. It isn't copying something done in language X. &gt; not something (like the msvc declspec) that gets in the way. Explain why the Module TS gets in your way. That would help me understand what the problems you are seeing are. &gt; How do you import two modules that export the same name? Is it possible? No two modules can export the same entity -- this is basic ODR; pure C++. Two modules can export the same name, as long as they designate distinct entities. Just write an exported declaration for them. A module can reexport a name exported by another module. Just reexport the module, or write an exported using-declaration for it.
Right, if I recall correctly, you may have reported a bug on that to me. It is an implementation bug (literally how VC++ produced the standard library IFC), not a spec bug or restriction. I cannot imagine a world where the spec would prohibit that or would not support that.
&gt; Can we do it tomorrow, in our brave new modules world? I'm hoping yes. Like I said earlier, the answer is yes. Exactly what you wrote. &gt; so I don't want a change to a function body to cause recompiles of all the stuff that really only cares about my exported symbols. Exactly what I said earlier. The IFC format that VC++ is using is targeting exactly that -- only semantically relevant interface changes affect recompile of the consumers. As I said earlier, all of we (inclusive) will benefit from hands-on experience -- you trying it on concrete programs, me learning from your reports about scenarios missed by the implementation. I feel we are right now discussing cases that we both agree should be possible, and I am saying they are supported. The next step is concrete experiments. The one aspect that /u/berium and I discussed here is a scenario where source location changes affect recompilation because some other data are invalidated. That is an implementation issue, not a spec issue. 
They will be. I proposed not exporting them at the 2015 Lenexa meeting, but EWG did not agree. I still believe it will offer a much better experience not to expose them to consumers of the module.
If I recall correctly, Windows Events are done this way.
Self-promotion: if you wish to log structured data in C++, you might be interested in joedb, the journal-only embedded database: https://www.remi-coulom.fr/joedb/intro.html
Indeed, ETW (Event Tracing for Windows) works this way, and if you can, you should use it. It's no option for cross-platform/non-windows though.
Thank you!!
Are the extensions for the game consoles available from the platform holders websites? It might be worth adding them into a forum post or something like that? Microprofile (https://github.com/jonasmr/microprofile) added the necessary source code to a forum post on both sites which allowed easy access for people who have the appropriate NDAs already :).
OK. What about the actual flaws of the Modules TS? 1a) Distributed builds. More complex graph - less opportunities to parallelize compilation. 1b) Distributed builds. Either we have to copy BMIs over network (if they can be copied), or build same BMIs on every node. 2a) Name clashes. Consider two third party libraries: third_party/x/a.mxx | module a; third_party/x/a.h | ... third_party/y/a.mxx | module a; third_party/y/a.h | ... src/main.cxx | #include "x/a.h" | #include "y/a.h" // OK, different path | import a; // ??? which one 2b) Name clashes. Consider two applications in a single project: src/m/m.mxx | module m; src/x/a.mxx | module a; src/x/main.cxx | import a; import m; int main() {} src/y/a.mxx | module a; src/y/main.cxx | import a; import m; int main() {} src/makefile | x : x/main.cxx | y : y/main.cxx `make x y` -- will it build `m` twice or will it break because it puts both `a.bmi` is a same directory? 
What was the reasoning for having them be exported? This is a major gripe of mine that usually involves reverting to silly stuff like PImpl classes or arbitrarily moving helper methods into an anonymous namespace inside the source file. The migration over to modules feels like the perfect time to get rid of this headache.
http://www.boost.org/doc/libs/1_65_1/libs/graph_parallel/doc/html/index.html
Very sorry to hear that. Are there any notes of the arguments why they decided that way? I know there are probably more important battles for you to fight, but (assuming more people are interested in this) maybe this could be brought back to EWG again with more backing from the community.
Could someone elaborate on how exactly it could be "abused"? I really like it as it clearly highlights that there could be an error and forces users to handle that error.
Yes it can
We are putting them on forum posts, yes. I was confirming a detail with a platform holder before adding that to doc.
&gt; It could be argued that return x ? 0 : 1; is more readable :-) I don't disagree that it could be more readable, but everyone knows it's going to be optimized away (hopefully), so nobody would usually bother to change it (it tends to be more a style convention than anything). When you have 15 different possibilities and you had a switch, going to a ternary form looks pretty ugly and error-prone. The only concern I see as somewhat valid for constexpr is that debugging can be complex but that's an implementation issue. The same problem appears with metaclasses as well. When I saw constexpr at first, I saw the limitations on the functions and was like "too annoying, I'll stick to constexpr variables initialized with literals. C++14 made them much better and now the main limitation is the limited support in the standard library. For your paper, it looks interesting but my brain is too tired today to read through it unfortunately. I'll give it a fair chance tomorrow.
Correct. I've updated my comment. Thanks!
&gt; I don't know why this is, but I've never seen my biggest concern about the current modules proposal addressed. And that is the fact that modules could provide the opportunity to finally retire namespaces, but this opportunity is just thrown away. That has been addressed, both in the modules papers and many of the discussions and presentations about modules. You not agreeing isn't the same thing as them not talking about the issue.
Word of the day!
That's awesome, thank you very much.
Use [`std::array`](http://en.cppreference.com/w/cpp/container/array) and you won't have this issue. auto x = std::array{2, 3}; // deduced as `std::array&lt;int, 2&gt;` 
no, cause auto simply can't deduce array type. It always deduces any braced-init-list as std::initializer_list.
I see the IFC format more like a shippable BMI that all compiler can understand. Compilers could still implement their own format, but translate between IFC and their appropriate format (whether it's in memory, or in some files) back and forth. That will allow compiler their current flexibility of implementation, while providing a format that everyone can understand. If that format is somewhat stable, it could become a shippable BMI­. We could even embed the IFC in static libraries, so consuming fully modularized library would indeed require one file. Do you think it's still something possible or something from the far future?
"Cannot" or "will not"? The template function example shows that the compiler *can* do the deduction. If the answer is "Use std::array" as in a previous reply, I guess that is a good enough reason to choose not to support it.
&gt; "Cannot" or "will not"? [dcl.array] Specifically says using `auto` with an array type is ill-formed. It's not that it doesn't work by omission; it doesn't work because it was singled out to not work.
yeah he sort of just left that hanging. I suppose he meant "overused"? I'm not sure. Other languages have optional semantics that are used constantly though, so I'm not sure it's necessarily a bad thing.
Name clashes are easily solvable. Since they don't have that transitive nature of headers. // xliba.mxx export module libx.a; export import a; // a from x lib // yliba.mxx export module liby.a; export import a; // a from y lib Then in your main: import liby.a; import libx.a; // use both! Simply tell to your build system that `yliba.mxx` and `xliba.mxx` are using different libs. The module name from one lib won't affect the importation of the other lib.
Auto type deduction is a different set of rules to template/decltype type deduction.
Them addressing it has been "modules and namespaces are orthogonal", end of story. Why are they orthogonal, because they want it to be that way? That's a design decision and I have yet to see an actual reason why it should be that way.
Not even necessarily that. RE2 is pretty darn fast. In this case though, it looks like he's parsing &amp; constructing the DFA in each parallel for so it's probably doing that 4x as often as it should.
Worth noting that [this only works in C++17](http://en.cppreference.com/w/cpp/language/class_template_argument_deduction). About time, too.
Does this work? `std::array x = {2, 3}`
I don't know about that. Windows logging works the way you mention and that might be great for the computer but its horrible for humans. Hard to use for the programmer, hard to examine by ops, fragile (you went from just needing the log, to needing the log, the metadata, and worst of all the log viewer). I think fast, plain text logging is a better approach.
&gt; &gt; Simply tell to your build system that yliba.mxx and xliba.mxx are using different libs. alternatively, bash repeatedly with a stick people who don't put their module in some kind of "project name" namespace 
&gt; No, we're hoping specifically that we can get rid of the artificial split between declaration and definition wouldn't this make compile times longer by virtue of having everything inline ? 
[dcl.type.auto.deduct]/4 says: Example: const auto &amp;i = expr; The type of i is the deduced type of the parameter u in the call f(expr) of the following invented function template: template &lt;class U&gt; void f(const U&amp; u); 
Yes indeed! We managed clashes with namespaces gracefully since they exist. If you worry about module name clashes, you should first worry about classes name clashes in namespaces, as it's the same problem.
&gt; If you change the module implementation but the interface is unchanged, you don't need to recompile does this means that VC++ would not inline anything ? 
&gt; Naturally, these macros should probably be replaced with first class language features as they are discovered. I disagree, to the contrary I think that language-level features should be replaced by in-language features (as long as it can be made to not kill compile times)
If I understand correcly, its pretty similar to [this](https://github.com/blapid/llcpp/issues/2) suggestion. How does that look to you? 
&gt; Them addressing it has been "modules and namespaces are orthogonal", end of story. Far more has been said about it than that including arguments supporting that they are orthogonal, and other reasons such as the backwards compatibility goals, desires for the modules semantics to work in a broader set of languages than C++ (e.g., C and Obj-C), and many others. This has been talked about over and over in presentations and papers for many years, and it definitely has not consisted of nothing but an unsupported assertion that "modules and namespaces are orthogonal." &gt; Also I don't get why I'm being downvoted here. My guess is because you're apparently unfamiliar with statements that have been made for the opposing viewpoints. &gt; I can't be the only one who really dislikes all the std:: and boost::asio::ip::. I don't like deeply nested namespaces, but that's hardly a criticism of the current modules proposal. Deep namespace hierarchies have never been necessary. Without modules very shallow namespacing (as in, a single visible level, with maybe some hidden namespaces for certain implementation details) is sufficient. With the current modules proposal we keep that, including sharing a single namespace over many modules. It would be absolutely terrible to combine namespaces and modules in a way that meant I couldn't organize my program into multiple modules without having a bunch of extraneous naming. The current modules design also improves things by reducing ODR issues, such that namespacing is no longer required to avoid many ODR problems. Non-exported symbols don't need any namespacing and problems with exported symbols are much more likely to be diagnosed directly, making it safer, though not entirely safe\*, to export un-namespaced symbols. \* The reason we can't make it entirely safe to have multiple modules export the same symbol is another one of those things that actually is discussed in the relevant materials.
read it thoroughly, there is an exception clause right after that for copy-list-initialization.
Pretty sure he ment to say that the set of rules is different when deducing braced-init-list. If you want to know more a good place to start is Item 2 of Effective Modern C++
[Yes](https://godbolt.org/g/LP6rUp)
My reasoning behind that belief is that I see macros in general as very dangerous, no matter the macro system. They can easily make code incomprehensible. By substituting them out with core language features, you send a message about not using them, and they will appear less in code overall. Of course, I could easily be completely wrong. I do agree with you that, at least most of the time, having library solutions is nicer than in-language features. It's just that macros introduce arbitrary code changes at a single point (although the language feature you substitute them for would too...)
&gt; It's just that macros introduce arbitrary code changes at a single point and the good thing is, you can just press F2 on it and go see the exact code that runs, instead of trying to grok the meaning of the keyword from the standard, cppreference, and two dozen blog posts
As-we're-guessing I guess that the way the program uses the re2 library is slower than the way the other program uses the pcre library. *"It depends how you write it!"*
I agree; I am very disappointed that this was where the discussion stopped. I am probably not clever enough to fully understand the modules TS or the two viewpoints here, but I am not sure the build2 authors are sources I can trust to dismiss SACHandler's claims that there could be schisms in the community or problems with build tool interoperability caused by modules. The reason is build2's authors always present it as a panacea that will solve all these other problems, such as package management and dependency fetching, assuming the entire world starts using build2. There seems to be a lack of concern with what other tools are doing, right down to the fact they named the thing "build2" when many people already called Boost Build that. Build2 actually seems cool so I'm not trying to hate on it. But it also isn't part of the C++ standard and in general if the fix for the modules TS is to create complex tooling that people will argue endlessly about maybe there's more work to do. I am also really curious what forcing modules to adhere to some of the rules SACHandler mentions would sacrifice.
No, it does not mean that. Inlining is a decision that the backend makes, mostly based on criteria orthogonal to modular code organization (which is mostly a front-end thing).
&gt; For instance, they use streams and std::endl (that forces buffer flushing) inside loops (see link [2] below). Have you made those changes to the program and measured before/after ? [Improved programs are welcome!](http://benchmarksgame.alioth.debian.org/play.html) 
If I remember correctly, people were concerned about 'external' friends. I honestly don't know it is a real problem in practice -- nobody had hands-on experience with modules at the time on large enough codebase. My suspicion is "no".
Damn, no GPU support. We recently benchmarked a number of sparse-matrix vector multiply implementations, and the GPU based implementations absolutely smoked the CPU counterparts.
I didn't take formal notes, as I was presenting. If I understand correctly, people were concerned about 'external' friends. I don't know that is a sound programming practice in modular worlds. Yes, I would love to see EWG revisit this issue based on actual experience.
Context matters. You almost didn't get any version of `constexpr` to complain about.
&gt; The language benchmark explicitly focuses on idiomatic code and not fastest case code. Program authors are encouraged to write idiomatic code (because others complain so much when they see code that isn't) and they are encouraged to write fast code (because others complain so much when they see code that isn't). There's a place for both.
Would you be able to inline member / friend functions if the private members are not exported?
I don't understand how it can work. I have a module which exports a function `inline int foo() { return 0; }`. I compile an object file `main.o` which calls this function. Now I change `foo()` to return 1: at this point `main.o` has to be recompiled, since `foo()` might have been inlined in it, right ? 
Not your idea of idiomatic or not your idea of slow? :-)
Even if you didn't export private member variables, you still need their size to put the class on stack. Also, you cannot inline member functions without knowing the private members.
&gt; That will allow compiler their current flexibility of implementation, while providing a format that everyone can understand. Exactly right. I see people complain that their favorite compiler has "optimized format" for their own "optimized ASTs". The idea isn't that every compiler has to adopt the IFC data structures as their internal representation, or that it should be the only format they should support. Rather, the idea is to have a widely shared, common format, with APIs on top. &gt; We could even embed the IFC in static libraries, so consuming fully modularized library would indeed require one file. I think that was on my 2015 CppCon presentation :-) &gt; Do you think it's still something possible or something from the far future? A lot depends on the C++ community, and I hope we get something along those lines in the community.
Are you making assumptions on what is in your '.o'? 
So, name visibility is a "name lookup" issue. Code generation uses far more "facts" than usually available to just name lookup or type checking. Note that just because the names are not visible means that the compiler has no way to represent (in some other abstract form, such as offsets, etc.) class layout and member access. Also, remember that the compiler can also use LTCG/LTO technology -- not necessarily the full gamut.
So what do you say? Any opinion is welcome.
Mind sharing your benchmarking results?
Not my idea of idiomatic. Also it doesn't fit &gt; and not fastest case code. The fact that vector instructions are idomatic can be argued, however they are not portable, not covered in most (all?) C++ courses, and often only used when you need maximum performance. My point is that, from the description, benchmarkgame is supposed to give you an idea of how languages typically performs. SSE (or OpenMP) doesn't fit that description: If your project requires explicit SSE code, you probably wouldn't have used Java or C# anyway so the comparison doesn't even make sense. 
what would there be in there apart from compiled machine code ?
Thanks again for the quick response. Your contributions are highly appreciated. Keep it up! I can't wait to use modules in my codebase ^^,
There are two meanings of inline. 1) Hint to an optimizer (which compiler is free to ignore per the standard) 2) A workaround for ODR violation you would have had in pre-module world if you put the definition of a function in a header that is included in multiple TUs. In a module world, #2 use of inline is irrelevant. #1 has been mostly ignored by compilers already. I can imagine that an implementation may chose to not to include the body of the member function into a hash (digest, whatever) for the purpose of determining whether users of the module have to be recompiled at lower optimization settings or not. In fact, in the compiled artifacts, your "technically inlined" function may end up in the .o/.obj and BMI will retain only the declaration if compiled at low optimizations level. Unlike constexpr functions that will always have to be put into the BMI.
I find the serialization aspect to be somewhat similar to the *printing* aspect. If you want your objects to be printable in human-readable format, you implemented `std::ostream&amp; operator&lt;&lt;(std::ostream&amp;, MyType const&amp;)`. Well, here you implement another function instead, like: LogWriter&amp; operator&lt;&lt;(LogWriter&amp; writer, Person const&amp; p) { return LLCPP_OBJECT(writer, p, name, age); } where the macro expands to: writer.object(p, {"name", "age"}, std::tie(p.name, p.age)) Actually, you might even propose a macro which deals with the whole function declaration/definition; to reduce the boilerplate further. As in: `LLCPP_DEFINE_OBJECT(Person, name, age);` and done. It's much simpler than cereal or sqlpp11 because: - you do not care about *deserialization*, it's only done by generic code in the visualizers, - you rely on the user to spell out, for each type, which fields should be shown.
`Other` types are generated when there are name collisions, the only exception being the top-level type, for which I apologize. It's complicated. Our new rendering framework doesn't do that anymore, but the Swift renderer is not ported over yet. If you'd like to help we'd love to work with you ;-)
A lecturer (now coworker) of mine once put it this way, paraphrased: "The language benchmarks usually rely on the most-C++-y but least efficient C++ library feature for a test, like using `std::map`* where other languages' tests use a hash-based dictionary built-in. In C++ you can implement a better dictionary but in many other languages you can't even replicate the built-in dictionary. Those benchmarks are like footraces between languages but where only C++ has its shoelaces tied together... and still either wins or nearly wins." *the same applies for more up-to-date tests using things like `std::unordered_map`, which is typically a lot less efficient than the hash-based dictionaries present in other languages.
&gt; Also it doesn't fit ? &gt; … from the description, benchmarkgame is supposed to give you an idea of how languages typically performs. From this -- *"It depends how you write it!"* ? &gt; … so the comparison doesn't even make sense. What way do you have to see the difference it could make, except by making the comparison? 
Rust uses neither re2 nor pcre. It has its own regex library which has been the subject of many optimizations by its author, drawing both from existing implementations and academic papers, with extensive fine-tuning. Now, one may say that Rust enabled the use of quick development of such intricate low-level code because of its safety features (or whatever), which *may* be true, however the crux of the matter is that different regex algorithms &amp; implementations are being compared. I imagine the Rust implementation could simply be wrapped up in a C interface for use in C and C++, and then C/C++/Rust would all perform similarly in the benchmark. Of course, it's still an advantage to Rust that it's immediately available. --- *Note: also, I believe that Rust's regex implementation does not support some features, such as backreferences, in order to guarantee O(N) matching.*
I was not talking about `inline` as a keyword / C++ concept, but inlining as an optimization performed by the compiler, whether the `inline` keyword is here or not. In my experience, almost everything is inlined. I remember I once had some massive algorithm with expressions templates, boost::asio, 3 different containers etc etc... when compiled under -O3 *everything* disappeared and ended up going into a single massive `main()`. &gt; your "technically inlined" function may end up in the .o/.obj and BMI will retain only the declaration if compiled at low optimizations level. yuck, so back to "a function ends up being compiled in every translation unit" ? Is that the sole alternative ? 
I would note that using other libraries is perfectly allowed by the rules; C uses an external hash map implementation (since none is provided in the standard library). Of course, in C++, adding a dependency on an external library can be a daunting prospect... but if someone was willing to take up the challenge, I'm pretty sure an implementation using `google::dense_hashmap` would be welcome by /u/igouy since it's nearly an industry standard at that point.
Note: those leading C programs use `pcre`.
While I'm glad to see all the thought going into the metaclasses proposal, the `$` still really grates on me. I can't identify precisely why, but it just seems wrong and I don't like it. Which is *totally* a rational flow of reasoning. maybe I dislike it because it reminds me of Perl (that's almost certainly it, lol)
No, it wouldn't. The module will be used to produce an interface file, and since the interface isn't changing, there is no need for the interface file to change either, so no dependencies will get triggered - assuming of course the build environment is smart enough to support this. That's actually what I was asking... 
&gt; an implementation using `google::dense_hashmap` [Improved programs are welcome!](http://benchmarksgame.alioth.debian.org/play.html) :-)
And it reminds me of PHP as well. The $ notation is really a poor choice in my advice. I would prefer the @ symbol at this point, like annotations in Java or Python
&gt; In C++ you can implement a better dictionary … So why don't the program authors choose that better dictionary?
I edited my post for clarity. &gt; From this -- "It depends how you write it!" ? Yes, but peoples that look at these numbers probably won't write vectorized code so benchmarking vectorized C++ against idiomatic Java isn't representative of anything. &gt; What way do we have to see the difference it could make, except by making the comparison? None, but these numbers don't give us any useful information. Of course vectorized C++ is faster than Java, But it isn't representative of the 'average'/'typical' C++ code, so it answers "how fast can C++ get" and this statement: &gt; The language benchmark explicitly focuses on idiomatic code and not fastest case code. Is wrong.
An abstract representation that is expanded at link time? 
Thanks. It is good to know what we are doing is useful to the community. At CppCon 2017's "Grill The Committee", someone asked "what keeps you going, on long-term projects like this", this is part of it; the sense that we are doing something useful, that matters to the community.
A lot of those benchmarks tend to be "pure" C++ implementations, meaning they use whatever the standard library has to offer. That's the trick with C++. It _can_ be really great but its out-of-the-box experience leaves a lot to be desired. :)
/r/cpp_questions 
I think there are folks who want to use `@` for user-defined annotations like in Java or Python, which the current C++ attributes system using `[[`/`]]` apparently can't handle (I'm still not clear on _why_, though it has something to do with the weakly-specified grammar of attributes; it was explained to me once on the std-proposals list but I failed to grok the explanation fully).
Those paraphrased comments tend to suggest that "whatever the standard library has to offer" is worse than whatever the Python library has to offer :-)
&gt; … these numbers don't give us the 'advertised' information: they aren't representative of the 'average'/'typical' C++ code. Please quote the text of the "advert". 
&gt; The language benchmark explicitly focuses on idiomatic code and not fastest case code. 
When buying a book, I don't think you are paying much for the actual paper.
STL's appraisal or the book? :) 
Modules (whether you have traditional separation or single file) will not prevent inlining. On the contrary, with greater emphasis on the component as a whole, the code generator has now better opportunities for optimization and inlining -- pretty much like LTO or LTCG. The Module specification on purpose is not using any form of "vtables" or "witness tables" or whatever they are called to describe the effect of modules.
That is correct, but those are code generation issues, and the code generator plays by different set of super-rules.
Slides: http://chandlerc.github.io/talks/cppcon2017/going_nowhere_faster.html 
Wouldn't this be problematic if yoi wanted to link, for instance, fortran object files with c and c++ object files ?
You are however then making the programmer responsible for cleaning up this outliving owner. Say for example we have a display where we want to show the same image image zero or more times. However we also want to display multiple different images, yet we have limited system memory. What image data do we keep loaded? You could have multiple object instances (lets call them imageInstances) referring to a unique Image resource object holding the pixel data. Whenever you call the destructor on the last imageInstance, you would then also free memory. No one iamgeInstance owns the image. If you don't use a shared_ptr, then you are forced to manage which images are loaded into memory. It's all fun and games until you have hundreds of images that you have to construct / free by hand.
I'm a big fan of using @ for _something_ (worthwhile) in C++. It is not in the "basic character set", but guess what!? - anyone who complains about not being able to type @ won't be able to email us about that complaint, so I think we are safe. :-)
&gt;&gt; yuck, so back to "a function ends up being compiled in every translation unit" ? Not sure I understand. If something is in .obj, it is already compiled and ready for linking. I was pointing out that with modules, you do not have to treat member functions which are inline by virtue of being defined inside of a class definition as inline functions. They can behave as if they were defined outside of class definition, at least if module was compiled with low optimization settings. 
Could you expand on what you see as problematic, with concrete examples?
I read that as "Towards a more powerful and simpler Herb Sutter." I don't think the world is ready for that.
Yes, I would say similar, but not the same. Maybe both are useful. (Coincidentally, I'm currently using the 'context' idea in a similar way in a current project.) The only difference is that context is typically stack-based, whereas a "recent logging" buffer might also include things that weren't exactly in the context, but happened recently. Sometimes those provide a clue. "Oh, you had just done a save (which looks like it did some conversion thing, hmmm), and _then_ you hit print..." AHA! Print and Save are probably separate contexts, and should be unrelated but sometimes they are (unfortunately) related - particularly when bugs are involved.
He has said in several talks that the $ syntax is just a placeholder to make a working proof of concept. The final proposed syntax will be open to the committee.
Slightly off-topic but his photo makes me think of Medic in Team Fortress 2.
I would prefer reflexpr(ClassName) instead of $ClassName, seems to fit better with decltype(...), constexpr(...), noexcept(...) and similar constructs.
Misused, not abused: https://godbolt.org/g/TyaEou constexpr auto isTrue = [](bool b) { return std::optional&lt;bool&gt;(b); }(false); return isTrue ? 1 /* wrong */: 0;
But it is not C++'ish again. There is little differences between @ or $ or ` or # or any character you pick in that matter. The best solution is decorator function style. https://herbsutter.com/2017/07/26/metaclasses-thoughts-on-generative-c/
&gt; Far more has been said about it than that including arguments supporting that they are orthogonal, and other reasons such as the backwards compatibility goals, desires for the modules semantics to work in a broader set of languages than C++ (e.g., C and Obj-C), and many others. This has been talked about over and over in presentations and papers for many years, and it definitely has not consisted of nothing but an unsupported assertion that "modules and namespaces are orthogonal." I tried, but I couldn't find anything but the unsubstantiated claims in the "A module system for C++" paper, P0142R0: &gt; One of the primary goals of a module system for C++ is to support structuring software components at large scale. Consequently, we do not view a module as a minimal abstraction unit such as a class or a namespace. In fact, it is highly desirable that a C++ module system, given existing C++ codes and problems, does not come equipped with new sets of name lookup rules. Indeed, C++ already has at least seven scoping abstraction mechanisms along with more than half-dozen sets of complex regulations about name lookup. We should aim at a module system that does not add to that expansive name interpretation text corpus. Complexity hasn't stopped the committee from adding other things like perfect forwarding and all the hacks required to make it work like reference collapsing. The rules should be intuitive and a module system as proposed by me is intuitive. &gt; We suspect that a module system not needing new name lookup rules is likely to facilitate mass-conversion of existing codes to modular form. And I suspect that with modules introducing a scope, it would allow mass-conversion just as well. Where is the proof? &gt; Surely, if we were to design C++ from scratch, with no backward compatibility concerns or existing massive codes to cater to, the design choices would be remarkably different. But we do not have that luxury. This is assuming that modules introducing a scope are not backwards compatible, which is just bogus. Again, if that's all they got then that's pretty weak. Moving on ... &gt; I don't like deeply nested namespaces, but that's hardly a criticism of the current modules proposal. Deep namespace hierarchies have never been necessary. Without modules very shallow namespacing (as in, a single visible level, with maybe some hidden namespaces for certain implementation details) is sufficient. &gt; With the current modules proposal we keep that, including sharing a single namespace over many modules. It would be absolutely terrible to combine namespaces and modules in a way that meant I couldn't organize my program into multiple modules without having a bunch of extraneous naming. My criticism wasn't actually the deep nesting, but the required qualification of names everywhere. No other language does this, but somehow C++ is this special snowflake where everything would supposedly fall apart if we didn't qualify our names everywhere. I call bullshit. With an import system like Haskell's, unqualified names are the default; conflicts can be handled in multiple ways, whichever you as the programmer prefer. &gt; The current modules design also improves things by reducing ODR issues, such that namespacing is no longer required to avoid many ODR problems. Non-exported symbols don't need any namespacing and problems with exported symbols are much more likely to be diagnosed directly, making it safer, though not entirely safe*, to export un-namespaced symbols. So would modules that do introduce a scope. I'm not saying the current modules TS is worse than the status quo, but severely lacking in some areas, like scoping and imports. 
[/u/merimus](https://www.reddit.com/user/merimus) is simply wrong to say that - *"The language benchmark explicitly focuses on idiomatic code and not fastest case code."* You are simply wrong to suggest that *"the 'advertised' information"* is supposedly *"representative of the 'average'/'typical' C++ code"*. The home page tells you what to expect: [Will your toy benchmark program be faster if you write it in a different programming language? It depends how you write it!](http://benchmarksgame.alioth.debian.org/)
Thanks for writing this up. Now that I know a bit more about what the x-ray talk is about, I think I might watch that video as well..
As another comment notes, `optional` bool can be really confusing. `optional` pointers are unnecessary: this topic has been beaten to death but like it or not, C++ pointers (all of them) already encode the optional semantic. Finally, I generally disapprove of using optional as return values, it's very easy to overuse them in that context. In most cases, you either have a value or an error, and an optional is basically saying you have no information about that error. The classic case is a function that parses an int from a string. You could have it be written as `optional&lt;int&gt; parse(const std::string&amp;)`. However, you would never be able to tell whether you failed to return an int because the string was nonsense, or whether the integer it contained was just too large.
I think it is great: It is short, but stands out at the same time.
&gt; Parsing it with a python script is indeed slower, but as I wrote in the posts, that claim is missing the point. Parsing can be done either (much) later, on demand, or on a different machine. Why burden your over-loaded, performance critical machine, with that work? Regardless, in my opinion, what you want from a fast logger is both low call site latency and better overall performance (throughput). I'm not sure in what aspect of low latency you work on, but this isn't consistent with any of my experience, working in HFT. If you are doing low latency then you aren't trying to maximize throughput, at all. Usually you have some cores available that none of the critical path code is running on, and you would just have the logger thread running on that core, and usually that core has more than enough power to keep up with as many loggers as you're likely to have on that box. When you are looking at log files, it's often to deal with a real, on startup, production issue. You are literally losing money by the minute, every minute you fail to figure out the problem and re-launch you pay the opportunity cost of not trading. I don't want to have to run another script, I don't want to move the data to another box. I want to cat, grep, and less the log ASAP to my heart's content, figure out what is going on as fast as possible, make an adjustment, restart, and tail the new log. This whole workflow would be slowed down considerably by having a binary file format and accessing it through a python script.
The $ already has a meaning in my head from Perl and PHP and Shell scripting. Using it for something very different will mean I have to unlearn a little of something every time I switch between files. Using a slightly different arbitrary symbol may actually help reduce the cognitive load if it's a little less common even if there is no technical difference.
Does anyone have an answer to the question asked at the very end about how the processor avoids essentially invalid code?
You could imagine an implementation that keeps track of which function definitions were imported and their hashes (it's not just inlining that's a problem) in `main.o` and then compares this with the module file to determine if a rebuild is needed. You could also imagine a mode that only imports always inline functions. Current implementations do not do this, so as it stands you will get full rebuilds, but this can actually be solved properly in a modules world as opposed to headers.
The "traditional" alternative would be to pass in an int by reference and have the return value being an error code. Can still be easily ignored however. How would you feel about it returning `Either&lt;int, std::error_code&gt;`?
Sure that would be fine, that's basically what outcome is. 
&gt; fragile (you went from just needing the log, to needing the log, the metadata, and worst of all the log viewer). I think fast, plain text logging is a better approach. There are standard tools available everywhere for that. And the metadata can be embedded in the executable. &gt; I think fast, plain text logging is a better approach. Plain text logging is neither fast, nor does it scale. If your log volume is very low and or the number of subsystems you log is small, then it's fine.
`$` isn't there to stay. It's a valid character in identifier in gnu-flavored c++
well, I just want to do ld foo.o bar.o where foo and bar are object files coming from whatever language
&gt; at least if module was compiled with low optimization settings. in that case, from what I can see with GCC for instance, "low optimization settings" means -O0 which is too low to be useful if you want to debug and keep some speed.
Isn't that called std::expected ? https://github.com/viboes/std-make/tree/master/doc/proposal/expected
In the common case there is perfectly valid memory for the CPU to continue reading from past the end of the array, just it'll compute &amp; speculatively store nonsensical results. Once the branch comes back that says that speculation was wrong it just avoids doing the actual writes and throws out everything it had done otherwise. If the read would trigger a page fault that's when I'd guess it just stalls and waits for the branch to see if it should proceed with the page fault or not.
I don't have any experience in HFT. It's also quite possible that in HFT, the existing solutions are good enough because you have available cores to do the work. The applications I've worked on could be under heavy load for extended periods of time. You could throw more CPUs/EC Instances on the problem on some of them but then you'd pay more money for infrastructure. Others were legacy code that didn't work in a distributed manner and had to be optimized in different ways. Therefore I had to consider both latency and overall performance. Regardless, if you're only looking for the smallest call site latency, according to my benchmark this approach is still faster in the majority of the cases (though, as I said in the posts, better and more objective benchmarks are needed). &gt; I want to cat, grep, and less the log ASAP to my heart's content, figure out what is going on as fast as possible, make an adjustment, restart, and tail the new log. Well, this use-case is still possible. Just use the parsing script instead of cat (or cat and pipe to a script). In my experience, the time it takes me to find the problem is orders of magnitude higher than the time it takes to parse, is this false in your experience? On my machine, the script can parse around a million lines (~80MB) in under 10 seconds. Keep in mind that the script is not optimized at all, and could be way faster (i.e implemented in C++). "on startup" issues would be parsed in under a tenth of a second, is that really "slowed down considerably"? &gt; logging to human readable directly should have no additional latency cost, and is extremely valuable. Again, I still believe the value isn't in whether the file is textual or not, but whether your application meets its demands and you have a decent way of viewing the logs - all of which are satisfied with my approach. Regarding "should have no additional latency cost": is this hypothetical or can you back this claim? My benchmark shows that it *does* have an additional cost (and my approach isn't even async). If you have a logging framework that I didn't compare to, and has no additional latency cost, I'd love to try it.
this isn't even his final form
You're only allowed to use libraries written in the language being tested or came with the standard library. Thus why C++ is using the much slower RE2 library (because it's in C++) rather than the faster, but written in C, PCRE library. If you were doing this in the real world you would of course use PCRE in your C++ codebase and there'd be no difference. But that rule also means you can't just have a C library the solves the problem, call it from python, and bam! Python is now as fast as C!
Yes, it's a nice way to detect functions which should be marked as static, which in turn is a good way to find dead code as clang will warn on unused static functions. However cppclean also raises this warning on local template functions, which seems a little excessive. They can't be marked static and I don't feel like anonymous namespace really adds much there.
Generally this is not going to show up at the ISA level (instruction set architecture, the specification that the software sees/relies on) and is microachitecture-dependent. That being said, it may have performance impact (prefetching, etc.), which is, again, very much dependent on the underlying microachitecture (e.g., see http://blog.stuffedcow.net/2015/08/pagewalk-coherence/). At the ISA level Itanium offered speculative loads, which also allowed to branch to a custom recovery code, which made aggressive speculation somewhat easier for the compiler side (although there are always trade-offs): https://blogs.msdn.microsoft.com/oldnewthing/20150804-00/?p=91181 / https://www.cs.nmsu.edu/~rvinyard/itanium/speculation.htm / http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.7166&amp;rep=rep1&amp;type=pdf
We need to distinguish between the Modules TS and its implementations. The TS is trying to deal with semantics of the language features and to impose as little constraints on the implementation as possible. Scenario of a quick edit-compile-debug cycle is an important one and I can see implementations exploring various strategies of avoiding recompilation of the users when not necessary. With regard to low optimization level, not sure about GCC, but, in clang inliner only kicks in at -O2. At -O1 it only attempt to inline "always-inline" function. Thus, at -O1, you would not have to recompile the users if you change the body of the inline function. 
I don't like `reflexpr`. From the examples I've seen, it's used a lot more per function than `decltype`, so a more abbreviated syntax is much appreciated. And I don't see the parallels with `constexpr` and `noexcept` to justify this form.
This might cause issues for Objective-C++ though, which makes liberal use of @.
Doesn't `@` has a meaning in Perl also? As well as %, &amp;, *. I don't think that ruling out some syntax solely because it has different meaning in some other unrelated language is a way to go.
and the end of visual debugger
Sure @ also has a meaning in Perl, but it is a lot less common. (Most random symbols and letters seem to have a special meaning in Perl, but a normal scalar variable is one of the most common things to use.) Aside from Perl, @ is less common in terms of number of other common languages that use it for something special. Bash also uses @ when dealing arrays (though differently from Perl) but that's fairly obscure compared to normal variables with $. Not that I am a huge @ fan, specifically. I just don't particularly like $ for the metaclass stuff as a stylistic preference in C++.
&gt; I believe that's the way fmt tackles this feature, right? Right, there is an extension API for user-defined types. Overloaded ostream operator&lt;&lt; is supported as well.
It checks out. TF2 *is* written in C++, after all.
Dayum. And I was wondering, who it reminds me of.
Any Newcastle, Australia c++ devs here?
Same in Amazon's Spain. Still waiting so far.
In 5 years we would have the existing compilers protected by UNESCO. Writing a new one from scratch would be nearly impossible. Anyway, *very* interesting proposal.
[Challenge accepted](https://github.com/ldionne/dyno/commit/80580186ebd9bbd8ad1c64d7a67b8a96392e3da8): DYNO_INTERFACE(Drawable, (draw, void (std::ostream&amp;) const) ); struct Square { void draw(std::ostream&amp; out) const { out &lt;&lt; "Square"; } }; struct Circle { void draw(std::ostream&amp; out) const { out &lt;&lt; "Circle"; } }; void f(Drawable const&amp; d) { d.draw(std::cout); } int main() { f(Square{}); // prints Square f(Circle{}); // prints Circle } Now, that's only 3 lines and there's no repetition. Yes, I'm cheating because I use a macro, but to be fair Folly.Poly also needs macros in C++14 (and Dyno is C++14). I guess the next step is to see whether I can do better without macros in C++17 with Dyno. I don't think I can.
In a sense, Dyno is more "low level" than Folly.Poly; it implements a concept map engine under the hood and gives you a programmatic API to manipulate it, and it then uses that API to implement type erasure. It also uses policies for storage and dispatching, which is the main initial motivation for the library. This is very flexible and programmable (e.g. you could use it to implement actual static concept maps if you threw enough TMP at it, and you can easily add new dispatch or storage policies), but it comes at the cost of the domain specific language that you need to use when defining interfaces, etc. I'm (really) glad you find it more intuitive than Folly.Poly, and I personally share that view, but I think not everybody does. Some people may prefer an approach whose usage looks slightly closer to traditional inheritance, which Folly.Poly provides. I think this is a worthwhile goal and I wouldn't be surprised if people with less exposure to advanced C++ than you likely have preferred Folly.Poly over Dyno. 
&gt; I'm working on a tool that attempts to automatically detect inefficient heap usage patterns in C++ programs, for example, lots of small (possibly unncessary) allocations, unused heap memory, poor copy patterns resulting in needless allocations, allocations that live much longer than required, etc. Maybe you could try to make it work in Heaptrack ? https://github.com/KDE/heaptrack
Probably better would be to leave C++ alone and stop trying to improve it, it is already bad enough.
Heaptrack (and other profilers that use interception) can find some of things I'm looking for, but not all, since it doesn't know how the program actually used the memory, just how much was allocated and when. That being said, instances where Heaptrack (or any other heap profiler) was useful to you would be helpful to me, like specific fixes that were informed by the profiler. 
&gt; Heaptrack (and other profilers that use interception) can find some of things I'm looking for, but not all, since it doesn't know how the program actually used the memory, just how much was allocated and when. well, yes, exactly: I think that it would be interesting to have this additional knowledge inside heaptrack instead of having to juggle with two different software for memory benchmarking. I've used it quite a bit to reduce spurious allocations in the "critical loop" parts of my app, mostly by running it on unit tests.
Qt also has great support for ECMAScript (JavaScript) http://doc.qt.io/qt-5/qtscript-index.html
I am 200% with you on that. It feels wrong, because C++ does not use symbols in this way. When you see a special symbol, you think "ah, an operator". But here it is used like a keyword. I think all examples work better when you replace $ by the keyword "meta".
Just invent some obscure n-graph to replace it.
`[^]` to indicate you're going in the *meta* level
Duh! The answer was there all along. #EMOJI
What's going on with his right eye? The background is showing through.
I thought Vader had a tie bomber not a fighter
Wow, I didn't know about the bug #6. It's crazy that this code compiles: #include &lt;string&gt; void f() { std::string(foo); } 
That's the glasses redirecting the light coming from the side, like a prism.
It took much longer than expected, but it is finally out. Summary: Catch 1.x.x is no longer under active development, but serious enough bugs will get fixed and PR's potentially accepted. Catch 2.x.x is rebased on C++11 and removes various legacy features. 
Not really crazy, that's expected, you can put parentheses around declarators. Very similar to how you use parentheses in pointer to function declarations: `void (*foo)()` in order to avoid the `*` being part of the return type.
for those of us stuck in 2016, which state of the art logging framework should we all be using?
The changelog says "Non-const comparison operators" but I can't seem to find anything on the topic from googling. Does OP mean doing something like: // Non const int volatile int foo = 3; // Const int const int boo = 4; // Comparison between const and non const variables bool res = foo &lt; boo; I don't really see what the issue can be with this (allowing one of the variables to be non const)?
I think a single character for reflection is really powerful. Imagine you're going to make an operation on all your class members. If reflection is simple enough, you won't even think about writing that operation multiple times. I see reflection not in deep dark places in libraries surrounded with meta magic, but as a general tool used in many places to make the code more expressive. Even in non generic code.
&gt; Traditionally, to handle tensor algebra, mathematics software has decomposed tensor operations into their constituent parts. So, for instance, if a computation required two tensors to be multiplied and then added to a third, the software would run its standard tensor multiplication routine on the first two tensors, store the result, and then run its standard tensor addition routine. &gt; In the age of big data, however, this approach is too time-consuming. For efficient operation on massive data sets, Kjolstad explains, every sequence of tensor operations requires its own “kernel,” or computational template &gt; “If you do it in one kernel, you can do it all at once, and you can make it go faster, instead of having to put the output in memory and then read it back in so that you can add it to something else,” Kjolstad says. “You can just do it in the same loop.” Isn't this what C++ linear algebra libraries have been doing since blitz++?
 struct foo { int i; }; bool operator==(foo a, foo b); // ok bool operator==(foo const&amp; a, foo const&amp; b); // ok bool operator==(foo&amp; a, foo&amp; b); // not supported The last `operator==` is not supported, because it might potentially modify its arguments.
I think they mean user-defined `bool operator&lt;(T&amp; a, T&amp; b);`
Rarely used: http://en.cppreference.com/w/cpp/numeric/valarray
Thanks
Can we just give Chandler like twice as much time? Or like a day? I would go to that...
You're right, probably crazy is not the right adjective. I should have used "scary".
I really like std::accumulate and std::iota. They sometimes get overlooked since they reside in &lt;numeric&gt;. There are some other functions in &lt;numeric&gt; and since they can take their operations as parameters, they can be used against their intended behaviour. Though, it is useful be careful that it might be unreadable for others. &amp;nbsp; For example: std::inner_product(a.begin(), a.end(), b.begin(), 0, std::plus&lt;&gt;(), std::equal_to&lt;&gt;()); Calculates the number of matching positions between two containers. 
I like the clean break from C++03
I don't understand the discussion about fixing `map::operator[]`. Doesn't `map::find()` already solve the problem of looking up without adding entries? Rather than change the semantics of `map::operator[]` as was mentioned in the Q&amp;A, it seems we could just educate our developers on the current semantics of the container.
Seriously, i have started to like reddit. If this was StackOverflow, the question would have been closed as off-topic, reddit community is great
"scrazy"
Just updated to the v2 on my own codebase and everything works fine, thanks for the continuous work on this very nice library !
"rarely" is relative though. Debian Code Search finds [43903 uses](https://codesearch.debian.net/search?q=valarray&amp;perpkg=1)
Cheater! jk ;-D I feel it's really important to also have a macro-free interface that is as clean as possible. I'm not concerned with the fact that Poly needs macros for older compilers. That problem will go away over time.
Speaker here. I don't think anyone was seriously suggestion we fix/change `std::map::operator[]`. Even if we all agreed it was weird, it would be, practically speaking, impossible. One could imagine extending it, though. And I suspect most codebases have their own versions of some of these extension ideas. &gt; Doesn't map::find() already solve the problem of looking up without adding entries? It does. Yes. But this is very much an experienced C++ programmer's perspective on the situation. There are easy and obvious fixes to all the bugs in the talk, once you know what you are doing (e.g. stepped on it enough times) &gt; it seems we could just educate our developers on the current semantics of the container. On one level, that's what this talk is. "Here's the thing we find ourselves consistently having to educate people about. Here are some counter-intuitive corners of the language that consistently produce problems. Hopefully you won't make some of these mistakes now." And yes, for the vagaries of map's operator[], because changing it is basically impossible, this is our _only_ choice. This is the worst option. Ideally things are intuitive and/or the tools can catch badness. And for people who work in large codebases with large numbers of programmers, you should be understand the places where education is our only defense, because you should expect them to be perpetual problems as you onboard new people. 
I really like `std::nth_element`. I recently used it in the implementation of a genetic algorithm to help trim down my solution pool. I expand the number of solutions using mutation and crossover, then cull it back down to size by partially sorting it, and then dropping off the worst performers.
Law of the least astonishment. Operator[] in the right-side of an expression is not expected to have side-effects.
std::future?
There are people that think, it is currently neglected because of good reasons.
protobuf and anything based on it (like grpc): &gt; Memory allocation and deallocation constitutes a significant fraction of CPU time spent in protocol buffers code. By default, protocol buffers performs heap allocations for each message object, each of its subobjects, and several field types, such as strings. These allocations occur in bulk when parsing a message and when building new messages in memory, and associated deallocations happen when messages and their subobject trees are freed.
Hey, great talk! I am happy to have guessed that map[] was your number one source of bugs, having walked into it myself some time ago too. I think it is an especially weird situation, because normally one does not expect it to be set to 0 in C++, whereas it is the expected behavior in other languages.
I'm waiting for Catch 22
Another question: I saw you used map.find(k) != map.end() in order to check for existence of keys. I normally use map.count(k) &gt; 0. Is map.find() preferred over map.count() speed-wise (for normal maps)? 
Sorry, won't happen. You might get Catch2 v22.0.0 though
&gt; You're only allowed to use libraries written in the language being tested or came with the standard library. **Not true** (the new C++ program uses pcre) although it would look better to show stellar performance with a C++ library. 
Close enough. 
What are those reasons?
Someone did -2 votes, sensing SO lover
Good choice. `std::nth_element` is the kind of thing we can use now &amp; then, but we don't think of it. BTW, I took your comment as an occasion to refresh my knowledge of this function. Looking at its [cppreference page](), it's apparently specified as having linear-time average-case performance and log-linear-time worst-case. I'm wondering why. Median of Medians will do selection in linear-time. Writing an Introselect that starts with Quickselect and switches to Median-of-Medians thus gives us both a fast average case and a linear-time worst case. And we've known about this since 1997.
And perhaps rightly so. Scott Meyers (I think) said that `std::valarray` is poorly specified. I couldn't say what the problems are, though.
Thanks for taking the time to reply. How would you feel about deprecating it? Developers of new code can be warned to switch to the named functions which communicate intent more clearly, while old code can still work. Do you believe this is something the standard library should do?
&gt; I tried, but I couldn't find anything but the unsubstantiated claims in the "A module system for C++" paper, P0142R0: You actually quote one of the reasons given: &gt; One of the primary goals of a module system for C++ is to support structuring software components at large scale. Consequently, we do not view a module as a minimal abstraction unit such as a class or a namespace. Which is to say that modules as they're advocating for are to support physical structuring, rather than logical structuring, of the program. That alone disproves your claim that there was never a single supporting point in favor of modules being distinct from namespaces. &gt; And I suspect that with modules introducing a scope, it would allow mass-conversion just as well. Where is the proof? The problem here is that P0142's talk about "not needing new name lookup rules" is actually insufficient to enable the kind of mass conversion discussed. See Richard Smith's and Manuel Klimek's work on what modules actually have to be to deploy them to 100s of millions of lines of code. I'll be glad to compare it to your comparable work getting your module system to work with existing C++ codebases. &gt; This is assuming that modules introducing a scope are not backwards compatible, which is just bogus. If you can come up with a proposal that preserves the kind of compatibility necessary then go ahead. I'll read it. In particular I'll be interested to see how you avoid the ABI changes that the current proposal was designed to avoid, and how you deal with the 'legacy' issues addressed in P0273. &gt; My criticism wasn't actually the deep nesting, but the required qualification of names everywhere. No other language does this, but somehow C++ is this special snowflake where everything would supposedly fall apart if we didn't qualify our names everywhere. I call bullshit. That C++ requires qualification by default has nothing to do with modules. Yes, the lookup rules can be changed to allow unqualified access when there aren't any ambiguities. Clang already has code to deal with improperly qualified names, and uses it to present "did you mean?" errors. But changing the lookup rules would most definitely break compatibility. &gt; With an import system like Haskell's, unqualified names are the default; conflicts can be handled in multiple ways, whichever you as the programmer prefer. "Do it like Haskell" is hardly a sufficient proposal, but feel free to try implementing it. Let me know how it goes. &gt; So would modules that do introduce a scope. While also achieving the other goals of the current work? Where's your substantiation?
Personally, I never felt like I could presume what `map::operator[]` does because by definition it doesn't actually index the container. I realize now that most people's first impression of the operator is quite different to mine.
Count has to go over all element, while find can stop at the first match. 
Please no. C++ is already verbose enough as it is.
I don't know off hand, but I wonder if its to maintain stability. Kind of like how we have `sort` and `stable_sort`. Now you've got me curious though :-)
You are presumably thinking of the free function `std::count()`. The `count()` method of std::(unordered_)(multi)map could be implemented as just a wrapper for the `find()` method (only in the non-"multi" case) or for the `equal_range()` method. So it may be marginally slower, but not notably. It would certainly be terrible QoI, to say nothing of violating the standard's complexity guarantees, for the containers' `count()` methods to be O(N).
I wouldn't get too hung up about it. Reddit sometimes auto up/down votes submissions as a kind of normalizer, or something like that.
Good thought. But I'm afraid Heapselect isn't stable.
Yeah, I would also assume that count for non-multi map would do map.find() and stop after the first hit. 
In this case we actually need the value, not just to verify its existence, so `find` is what we want here. https://github.com/facebook/folly/blob/master/folly/MapUtil.h#L52
Thanks for downvoting, but - that still does not prove I am wrong. C++ from the start had bad design, eclectic collection of features and it was way too complex. And now it is even more complex and will be much more complex. Why is that needed?
Eh, I have no idea then.
As a maintainer of our large C++ codebase, if we had a good alternative and we could actually convert our codebase sensibly (read: automated), I would be happy to deprecate it in our code. I'm skeptical that there's a practical pathway here, though. &gt; Do you believe this is something the standard library should do? Speaking as a committee member, the chances of this happening are pretty close to zero. I'm not even sure everyone agrees with me that its weird and that we'd change it in a perfect world. But even if we did, actually deprecating this and replacing it would be a very longshot. 
@ has a special meaning in Java, Objective-C, and Python to name a few. I feel like $ is less common than @ in other languages, honestly. But either way, I don’t think other unrelated languages should be the metric by which you decide which symbol to use. I think it’s more important to figure out what is going to cause the least trouble for things that already integrate with C++, such at sidechain compilers and Objective-C++.
Happy to see that they try to improve the bottle-neck : the interface between cpp and programmer!
**Company:** [Microsoft](https://www.microsoft.com/) / [Visual C++](https://blogs.msdn.microsoft.com/vcblog/) / [C++ Compiler Front-End](https://careers.microsoft.com/jobdetails.aspx?ss=&amp;pg=0&amp;so=&amp;rw=1&amp;jid=325634&amp;jlang=EN&amp;pp=SS) **Type:** Full time, Software Engineer II, but other levels available **Description:** Are you passionate about compilers and parsers? Do you want your work to matter? Do you love C++? Do you want to add compelling new features to make programming in C++ easier and more natural? This job is for you! We are the Visual C++ team and we produce C++ compiler for Windows, aka MSVC. We’re looking for a brilliant developer to join our compiler front-end team as we take on new charter for C/C++ compiler development. As a software engineer on the C++ compiler front-end you will have the opportunity to work directly with customers, MVPs and other Microsoft teams as we add new features to the standard C++ language and libraries. We work closely with the C++ Standard committee and continuously evolve our compiler to adhere to the language evolution. We are embarking on some very ambitious and impactful features in several fronts, including language innovations from C++ 11, C++ 14, C++ 17 standards, making the parser robust and fast, and most importantly investing in compiler conformance that truly target cross-platform development across Windows, Android, iOS. Your passion and skills in language design, parsing, binding, static analysis and C++ programming will help you succeed as a key member of the team. Required Basic Qualifications: * 2 years of C/C++ knowledge and experience * 2 years in compilers and parsers development * Bachelor of Science degree in Computer * Experience designing/shipping imperative language compilers/languages/APIs/tools Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: *This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter*. **Location:** [Where's your office - or if you're hiring at multiple offices, list them. If your workplace language isn't English, please specify it] **Remote:** Not initially, but Microsoft considers such requests on case-by-case bases after several years in the company. **Visa Sponsorship:** Yes **Technologies:** C++17, we build a lot of open-source libraries for QA **Contact:** Apply for job [1078351](https://careers.microsoft.com/jobdetails.aspx?ss=&amp;pg=0&amp;so=&amp;rw=1&amp;jid=325634&amp;jlang=EN&amp;pp=SS) on Microsoft Careers website, you can email any questions you might have to yuriysol at microsoft
Link to Who's hiring C++ Devs thread post: https://www.reddit.com/r/cpp/comments/75590f/whos_hiring_c_devs_q4_2017/dpasfun/
For broad and more opinion related questions, Reddit is probably the best place. For a specific things in code, I cannot count how much the stack overflow community has done for me. The amount of high quality questions and high quality answers is enormous.
Does anyone use std::enable_shared_from_this ? I came across it recently, not having seen it before. It seems to impose some serious restrictions so I don't think I'll be using it.
For broad and more opinion related questions, Reddit is probably the best place. For a specific things in code, I cannot measure how much the stack overflow community has done for me. The amount of high quality questions and high quality answers is enormous. Also, visit /r/cpp_questions
Well, stackoverflow self regulate itself by only allowing high quality questions about specific problems other people can encounter. This is why you can expect quality answers and quality questions from the stackoverflow community. Reddit self regulate itself by putting down less useful or less wanted comments. Your "thanks" for example isn't a response or an addition to your post. It should have been an edit. Each community have their way to sustain itself and keep and encourage wanted content. Learn them and try to use them to promote and share your content.
For the silent majority may I ask what is all the hubbub? Project web page has a terse description: &gt;A modern, C++-native, header-only, test framework for unit-tests, TDD and BDD 
I use it with all my classes that are managed through `shared_ptr`. It allows passing `this` to functions that expect a `weak_ptr`. And I pass around `weak_ptr` instead of raw pointers because of the limitations of the Cereal library, and also for the added memory safety.
I haven't actually used it so correct me if I'm wrong. But it would seem that if you want to enforce correct use of `shared_from_this()` then you'd have to have a private constructor and use a factory to make `shared_ptr` managed instances. I could see that being a tradeoff I wouldn't want to make in certain cases.