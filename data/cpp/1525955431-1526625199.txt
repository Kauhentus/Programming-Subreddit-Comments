https://www.youtube.com/watch?v=mYrbivnruYw
Get the pro version then. Never got any ads.
Crash dumps are likely to contain information about what you were doing, they can't promise there is no personal data in it (even if they try to avoid it), promising otherwise would make them legally liable.
&gt;VS2012 &gt;C++ conformance I mean, it's better on the C++ conformance than the C conformance but it was still exceedingly bad. 2013 improved a bit, but they started getting actually close to the standard in 2015.
7++ No matching operator++ found for rvalue expression of type int
&gt; But there’s a bit of a downside there in terms of documentation (the defaulted A parameter has to be documented anyway) and compiler error messages (they’ll probably include the value of the A parameter, cluttering up my error output, which is probably the worst part of C++ for newbies). My thoughts on this are that it would be amazing if the compiler could recognize default arguments in an instantiation and leave them out or use the more specific name. GCC and Clang already do this to some extent, but one counterexample where they don't is when listing candidates. In addition, collapsing inline namespaces by default is something I think would work well by default. For example, print `std::vector&lt;int&gt;` instead of `std::__1::vector&lt;int, std::__1::allocator&lt;int&gt;&gt;`. For example, print `std::string` instead of `std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt;&gt;`.
In either case, it's a great forest to hide a tree in.
Some compilers have been getting pretty good at hiding default template arguments. If f expects a vector of floats, f(std::vector&lt;int&gt;{}); results in error: could not convert 'std::vector&lt;int&gt;()' from 'vector&lt;int&gt;' to 'vector&lt;float&gt;' for gcc and error: no matching function for call to 'f' f(std::vector&lt;int&gt;{}); ^ note: candidate function not viable: no known conversion from 'vector&lt;int&gt;' to 'vector&lt;float&gt;' for 1st argument void f(std::vector&lt;float&gt; ); in clang
How does it compare to gdb?
This kind of thing _again_ goes back to my dream text format change where the compiler could insert fold markers into the text around the long, expanded part and the user could interactively unfold it in the terminal or a text editor without redoing the whole output. Similar to how UNIX terminal colours are embedded in the text.
It has
As for the compile error, with the `if constexpr` method you can just use `else if constexpr(true){ static_assert(false, "Any message you want"); }` and it usually keeps the errors pretty simple to understand.
Can you show any proof? Last time I checked I couldn't find any indication of this.
If it helps, you can simply paste the code, highlight it, and press the code (&lt;&gt;) button in the toolbar to format it. This is identical to what you'd do on Stack Overflow.
I hope so. But it will only be so long until MS requires a MS account to log on to Windows. They already make it f*cking hard to circumvent it and create a local user account and log in with that. You have to click on the tiniest text during setup and click about 5 times "No I am sure, I do want a local account", and then on several occasions, if you're not careful and click the wrong button, you've suddenly switched to a MS account.
&gt; you cannot downgrade or stabilize the development environment. You can. The older toolchains are all installable. 14.00, 14.11, 14.12, 14.13, 14.14. I think even some XP toolchains might still be somewhere. The IDE is now separate from the toolchain. &gt; My feeling with VS2017 was like being a beta tester, constant updates, fixes There's some truth to that. But in previous versions, you'd have to wait 3 years until any of it got fixed or improved. Tough luck. Now, it's a few months usually, sometimes even faster. But I do agree that sometimes new releases break things, and then that's very annoying. But that happens on GCC and Clang too. That's one reason why you can go back to the older toolchain without a problem.
Ran the code through cppcheck.\[src/modules/client\-util/Client.cpp:54\]: \(error\) Memory leak: remote \[src/modules/client\-util/Client.cpp:59\]: \(error\) Memory leak: remote \[src/modules/client\-util/SSLClient.cpp:144\]: \(error\) Memory leak: remote \[src/modules/client\-util/SSLClient.cpp:149\]: \(error\) Memory leak: remote \[src/modules/common/CastUtil.h:358\]: \(error\) Memory leak: d \[src/modules/common/CastUtil.h:654\]: \(error\) Memory is allocated but not initialized: d \[src/modules/http/HttpRequest.cpp:1587\]: \(error\) Same iterator is used with different containers 'headers' and 'headers'. \[src/modules/sdorm/gtm/GTM.h:46\]: \(error\) Invalid number of character '{' when these macros are defined: 'INC\_GTM'. \[src/modules/reflection/Reflection.cpp:435\]: \(error\) Uninitialized struct member: cstruc.prosetser \[src/modules/server\-util/SelEpolKqEvPrt.cpp:415\]: \(error\) Common realloc mistake: 'polled\_fds' nulled but not freed upon failure \[src/modules/server\-util/SelEpolKqEvPrt.cpp:469\]: \(error\) Common realloc mistake: 'polled\_fds' nulled but not freed upon failure
A 4th alternative that is not listed is to use: template &lt;typename T, typename A&gt; class expert_vector { }; template &lt;typename T&gt; class vector: public expert_vector&lt;T, std::allocator&lt;T&gt;&gt; { }; This has the advantages of both (2) and (3): - from (2), you can pass `vector&lt;T&gt;` to any interface expecting an `expert_vector&lt;T, A&gt;`, because of subtyping, - from (3), no matter how good/bad the compiler is, there's no default template parameter to elide. Of course, it does mean using public inheritance without run-time polymorphism, which has a few downsides of its own...
I think you have some browser extension like RES that does that. I don't think that's a reddit thing.
My bad, I didn't realize Reddit has no way of making that easier out of the box. I don't mind the 4 spaces thing, but wow. In that case, the best bet is any decent text editor, where you should be able to do the equivalent of selecting everything and pressing tab to indent it all, then paste the result.
Side note: changing `HTTP_1_X` in your code from `char const*` to `constexpr std::string_view` lets g++ change the generated code from an explicit strcmp to just comparing exactly eight bytes: https://godbolt.org/g/wM948y
It not self-contained and has 3 mandatory dependency: [fmt](https://github.com/fmtlib/fmt) and [http-parser](https://github.com/nodejs/http-parser) and asio (or boost::asio, but it is not the default choice). While the first one is in conan-center the second is not and asio is of version 1.10* (at least 1.12 needed) and the boost asio version is 1.65.1 (at least 1.66 needed) but seems like the whole boost is available for 1.67, so for me (the one who has no prior experience of porting libraries to Conan) there are certain difficulties. If anyone would be making a recipe for conan I would provide any help needed on the part of RESTinio sources. PS A bit strange: I can find http-parser here https://bintray.com/bincrafters/public-conan (added May 03) but not here https://bintray.com/conan/conan-center 
No idea.
I would agree that you can claim to e.g. have a feature done before it works in all cases. I would imagine that Hana and range-v3 are pushing the C++ features to their limits and thus you are right to claim support for features even if they don't work in all cases. But if a feature so buggy that it is essentially unusable, then it's wrong to claim that the feature exists. E.g. this bug: https://developercommunity.visualstudio.com/content/problem/228657/guaranteed-copy-elision-fails-for-types-with-user.html My testing shows that this behaves exactly the same in 15.7. This is almost the simplest class possible, yet it doesn't work with guranteed copy elision, which you claims to support since 15.6. I would argue that with kind of bug the feature is essentially unusable. And that you should stop claiming that guranteed copy elision is implemented.
I'm not sure what your question is; by virtue of having `begin` and `end` member functions that returns iterators, `sum` _is a_ range... Range-based algorithms allow you to use e.g. `accumulate` without saying `begin`/`end` and naming the range twice. As to `accumulate` vs. range-for or something else, yes, summation in general should be done with `std::accumulate` or `std::reduce`. However, given that the code you've shown is _modifying external_ state, one could argue that `std::for_each` is actually ideal here as it encourages encapsulating all mutable state into a function object. (Of course, not modifying external state is generally better when you can afford it, so `accumulate` is certainly the correct inclination.)
This is because it uses `char_traits&lt;&gt;::compare` (which is `constexpr`) internally, which is what this code _should_ be using instead of `strncmp`, as /u/kwan_e hinted at.
Yeah, the `strncmp` is somewhat wrong anyway since `buf.size() ≤ HTTP_1_X.size()` is not guaranteed so I decided not to comment. I suggest https://godbolt.org/g/fb5isA, which - optimizes down to the raw compare - enusures size check stuff for lhs.size &lt; rhs.size - moves the constexprif ugliness to a separate function, `cxx20emulation` probably, allowing to not repeat the pattern
Step 1) find a working compiler for your target environments Try this for any embedded target or Clion with Cygwin or Mingw
**Company:** [Carbon Black](http://https://lifeatcb.carbonblack.com/) **Type:** Full Time **Description:** [Our Carbon Black Response Sensor team is looking for Software Engineers with experience in macOS or *nix development, computer security, and development of enterprise-grade endpoint software systems. Ideal candidates have experience developing code against posix on unix platforms writing code in C/C++ for endpoints as well as a passion to create a world class security product. If you love computer security and digging into complex engineering challenges, then we want you on our team! ] **Location:** Waltham &amp; Boston, MA, USA or Portland, OR, USA or Boulder, CO, USA **Remote:** Yes **Visa Sponsorship:** No **Technologies:** [Required: do you mainly use C++98/03, C++11, C++14, C++17, or the C++20 working draft? Optional: do you use Linux/Mac/Windows, are there languages you use in addition to C++, are there technologies like OpenGL or libraries like Boost that you need/want/like experience with, etc.] **Contact:** Interested PM me or send email to baleman@carbonblack.com
But it does cross-compile
The funny thing is that, he is talking about c\+\+ ON Windows only :\) and if you ever noticed guys from Microsoft never talk about c\+\+ portability.
&gt; This calls for #if __cplusplus &gt;= 202001L I seem to recall the committee asking recently for examples of code that *had* to be done with macros; this would seem like a prime example.
http://reddit.com/r/cpp_questions
without visual assist x, vs is only better in visual debugger. For uniform cross platform experience, qtcreator would be a better choice.
Sanity check, does gcc support all of these? 
This talk was from the [Build conference](https://en.wikipedia.org/wiki/Build_(developer_conference)), which is run by Microsoft for developers using MS technology. Criticising someone for talking about X at a conference themed around X seems a bit unnecessary.
Is that true? My experience with boost has been that most boost headers pull in a bunch more boost headers and bloat the build. If you could pull a _single_ .h file from boost, then sure, but I've never used it that way.
Windbg?
That's more suited to kernel debugging?
You could also use specialization, avoiding funny names like `expert_vector`: template&lt; typename... &gt; struct vector; template &lt; typename T, typename A &gt; struct vector&lt; T, A &gt; { using value_type = T; using allocator_type = A; // ... vector( int ) {} }; template&lt; typename T &gt; struct vector&lt; T &gt; : vector&lt; T, double &gt; // just a dummy "allocator" type { using vector&lt; T, double &gt;::vector; }; int main() { vector&lt; int &gt; v = "hello"; } Online: http://coliru.stacked-crooked.com/a/7554cb368994c54f
VS2017 support please!
Not that I don't entirely disagree with you.. But comparing Crates to VCPKG\+Conan is disingenuous. Crates is a wild\-west style repository of packages, while VCPKG and Conan are curated.
&gt; You can. The older toolchains are all installable. 14.00, 14.11, 14.12, 14.13, 14.14. I think even some XP toolchains might still be somewhere. The IDE is now separate from the toolchain. Good for them. So, at this point: * does the XP toolchain support code analysis? * does the XP toolchain compile cleanly with DirectX #includes? * since the resource editor is part of IDE, does it work as it suppose to work with different toolchains? * is there a technical explanation why vcredist jumped from 6,401KiB (VC11) to 14,267KiB (VC15) in such a short timeframe? That's more than 50% increase! * why do I have to update, the updater in order to do the VS update? Anyway, it's 2018 we got more serious things to [fix](https://www.reddit.com/r/cpp/comments/8hpx0q/visual_studio_157_is_out_with_c17_conformance/dyllevi/).
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8ig8fv/need_help_with_converting_a_simulink_model_to_c/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Conan is decentralized, it's normal that repositories have different packages in them. 
VS 2017 is much more modularized than previous versions, you select the features you need and download install those. A full offline package would be quite large and take forever to download if you have slow internet. Regardless, a quick google search of `vs 2017 offline package` gives these helpful links as the top result. https://docs.microsoft.com/en-us/visualstudio/install/create-an-offline-installation-of-visual-studio https://docs.microsoft.com/en-us/visualstudio/install/install-vs-inconsistent-quality-network TLDR: You have to create your own offline install with the packages you need, on the command line, and then install from the command line using the same options. They also recommend you use the web installer.
You dont have to download, build, and install the C++ standard library.
Yeah, but the backtick is meant for things `inline` in your text. When you try to use it for whole blocks of code, you get: `#include &lt;iostream&gt;` `int main() {` `std::cout &lt;&lt; "Hello, world!\n";` `}` vs what I really wanted: #include &lt;iostream&gt; int main() { std::cout &lt;&lt; "Hello, world!\n"; } 
Oh, I thought you meant the new UI uses ``` for code blocks (which is what GitHub, Slack, and some others use).
Many key boost libraries are header only so installation is trivial
It doesn't though. You could use tag dispatching: bool starts_with(std::string_view, std::true_type); bool starts_with(std::string_view, std::false_type); return starts_with(buf, std::bool_constant&lt;(__cplusplus &gt;= 202001)&gt;{}); Or something thereabouts.
Small addendum: If you actually want to use the shown code, you'd have to inherit `expert_vector`'s constructors manually by writing `using expert_vector&lt;T, std::allocator&lt;T&gt;&gt;::expert_vector;`.
Use an X macro, this problem has been solved since the 60s.
Note that I didn't write *any* method, not even in `expert_vector` :) In this case, I'm not sure I'd forward *all* constructors. Specifically, I'm not sure whether it makes sense to expose the constructors taking an allocator or not: - any function accepting `expert_vector` by reference cannot construct a `vector` anyway; so would not benefit from it. - `std::allocator` is stateless, so there's little point passing instances, it just clutters the interface. On the other hand, it might be a good idea to provide a constructor to implicitly convert a `expert_vector&lt;T, std::allocator&lt;T&gt;&gt;` into a `vector&lt;T&gt;`. What do you think?
In the only decent text editor you press &gt; to indent ;) just kidding ... I know how to circumvent it, but I really prefer formatting systems with delimiters for code instead than having it indented ... at least if it's online and not in an editor.
At [17:30](https://youtu.be/UsrHQAzSXkA?t=17m30s), Herb says that `*optional` throws if the optional is empty, but it's actually UB. If you want it to be checked, you have to write `optional.value()`
Non-Markdown version of the links In this comment: **Link Text:** 17:30 **Link URL:** https://youtu.be/UsrHQAzSXkA?t=17m30s ^(Preventing misleading links on reddit by providing the links behind the markdown. **Why?** u/reallinkbot/comments/8igale/why_do_i_exist/)
 Use LTSB.
&gt; LTSB Well, good suggestion in principle. But LTSB doesn't even have WSL yet - that's a real bummer for developers. I really use WSL a lot, particularly as it works so well with VS now.
Then maybe they shouldn't send them by default. I'm not sure why people try to rationalize these things away. No one would want this stuff on by default, Microsoft just tries to slip it by everyone.
Two issues with Clion: 1) Proprietary C++ parser, so I have had a lot of trouble getting it to recognize a lot of the constructs I use. Also, a significant lag for recognizing new C++ features. 2) Written in Java, so sometimes freezes up during what I guess are GC pauses.
ו think the trend now (which I support) is to use the preprocessor less, not more. Making it more powerful well just encourage more abuse which we try so hard to avoid today.
Nice... But the very next "senior" candidate I'm going to interview with 90% probability will be unaware of pretty damn simple stuff like SFINAE, have vague understanding of move semantics, and no variadic templates experience at all. I'm not entirely catching up myself with c++17, tbch, not saying about any further changes, and from what I see around, adoption for the new stuff is really, significantly delayed.
Can you legally buy LTSB as a regular user or do you need to have a company credit card ?
I think gcc-7 should.
Probably my favorite Sutterism
Thank you. Interesting library. A few days ago I published a link to my reflection-based library (https://github.com/tdv/mif), which added metainformin for reflection without code generation (using macros). After that, you have access to any field of the struct, for example (https://github.com/tdv/mif/blob/master/examples/reflection/src/main.cpp). There are many libraries that also generate code: thrift, gSOAP, protobuf, etc. Why do you prefer to generate code to add meta information?
Maybe we should require opt-in for concepts that have significant non-syntactic semantic requirements? Or for all concepts? Sure, there are some concepts that have purely syntactic requirements, but I think most have some extra "laws" that need to be satisfied and it might be a good idea to make people think for a moment about whether they've really satisfied them.
It might shock you to know that many people can and do use gdb without googling.
DJGPP 4life
There's people out there that use vim too.
Thanks. 
If you don't send crash dumps bugs go unreported. It's a "dammed if you do, dammed if you don't" situation.
No it's not. The solution is to not collect it by default. Don't do it behind the user's back. It's their computer, their data, their Network, not Microsoft's. Have it be opt in, and a clear choice for the user, not some behind the scenes nonsense that only sophisticated users know how to turn off. 
Nope. He was generating stock assembly and then had a little translator into 6502-ese.
Got the Pro version. Still got ads. I looked it up and apparently you have to now get the Enterprise version to avoid the spam. And Enterprise isn't available unless you sign on to some business tier program.
Play stupid games, win stupid prizes? Use the wrong algorithm, get the obvious bugs? We'll never be able to capture everything pertinent about an algorithm into the type system. You should never expect that everything that compiles must be bug-free just because it passed the type system. You'll always need to read documentation and *think* about what you are doing. In this case, one of the major pertinent details that separate sorting algorithms is how they handle equivalent[1] values. The most commonly discussed issue is stable vs non-stable, but it isn't the only detail. That variant of counting sort[2] isn't designed to handle multiple equivalent values as distinct. This isn't wrong, it's just a trade-off that should be documented prominently. Note that other variants of counting sort such as the one on [wikipedia](https://en.wikipedia.org/wiki/Counting_sort#The_algorithm), *are* able to preserve equivalent values in a way that would work fine. Note that sorting algorithms typically also support passing a comparator that isn't &lt; or ==, and this algorithm would do poorly with any comparator that isn't comparing the whole object. Also, since when are we considering `.data()` as a salient detail? This is a clear case of moving the goal posts since it doesn't apply to anything, even "value" types like `string` and `vector`! auto v1 = vector{1, 2, 3}; auto v2 = v1; assert(v1 == v2); // Passes assert(v1.data() == v2.data()); // 💣 BOOM! Is anyone actually surprised that that fails? Then why are you expecting it for `span` and `string_view`? There *are* cases where using non-owning types can cause problems, I wouldn't claim otherwise. But this article really doesn't seem like a valid argument against them. [1] Note that sorting usually deals with equivalence, not equality. While I guess the example code uses `StrictTotallyOrdered` as an attempt to deal with that, it seems disingenuous not to mention that. Especially since the only difference between total and partial orders is a semantic one, not a difference in signature (ignoring for the moment the return types of the new `&lt;=&gt;` operator, which haven't historically been part of that concept definition). [2] I'm pretty sure that isn't actually a counting sort. Counting sort only handles small integer keys, so it can't be used in that case and it doesn't make sense to be generic like that. This is just putting things in a `std::map`, and letting it do the sorting in `O(n * log|unique n|)` time. Counting sort isn't a comparison sort, so it shouldn't depend depend on &lt;. It seems really odd that in an article about the importance of EqualityComparable, the example chosen to illustrate the issues with irregularity isn't even using == since map only uses &lt;.
CLion?
&gt; What's an use case for that? Pretty much every use case for polymorphism where you can know all the types in advance. Here's a few examples: * Your configuration file supports different types of values for a property, e.g. plain string or complex object. Think of parsing JSON into a more high-level structure than just raw JSON tree data. You could have a `std::variant&lt;PlainStringOption, ComplexOptionA, ComplexOptionB&gt;` based on what the user typed in the config rather than having to deal with the raw JSON itself. The types in the variant could be simple aggregate structures. This insulates you from the the underlying storage format. * You have a normal enumeration but discover that for some enumeration values you need to store data. This data is only relevant for those particular options and should not be accessed or used otherwise. You can convert the enum to a variant of a bunch of empty structs that simply act as distinct types and a couple value-holding structures. Think of Rust enums. * `std::optional&lt;T&gt;` can also be written as `std::variant&lt;std::monostate, T&gt;` but with any arbitrary number of other options. Consider `std::variant&lt;std::monostate, RresultType, ErrorType&gt;`. * You have several concepts/classes that are mostly similar but have subtle differences - using virtual functions would require using dynamic_cast in some places or else adding virtual functions that aren't used by all classes. Instead, you can combine them all into a variant, and bam, you have static polymorphism - use `std::visit(v, [&amp;](auto const &amp;v){ v.do_struff(); });` to call functions they all share, and [`overloaded`](http://en.cppreference.com/w/cpp/utility/variant/visit#Example) for the cases where you need to treat them differently. &gt; is this the only reason you would prefer a Variant over inheritance? You could still use inheritance with variants, you just wouldn't need to use polymorphism - no vtables will need to be generated or used by the compiler. And yes, you wouldn't need to use RTTI if you didn't want to, but that's the least interesting bit.
That pin crap made me cringe, but I guess if you are in codebase that abuses shared pointers I guess that it is an ok safety move.
Wow thanks! This is a bunch of useful information. Really. I'm saving this.
If you let users decide, they never accept, they get bugs you never know about and they complain but you have no idea why. The updates weren't forced until it was obvious that users would keep their system insecure and it became a security risk. There are ways to still disable them, but they are harder so that almost every computer can be kept up to date.
So despite being pitched as being like `static if` in D, `if constexpr` still requires contortion for cases where you want to apply design by introspection. -_-
Last time I used vim, I couldn't figure out how to type stuff.
I would rather use Notepad.
&gt; they never accept, I wonder why &gt; they get bugs you never know about Before broadband software was tested before being released. &gt; they complain but you have no idea why. Or you could give users the option to report a crash when it happens. Firefox literally does this exact thing. &gt; The updates weren't forced until it was obvious that users would keep their system insecure and it became a security risk First, security updates and other updates with forced nonsense are not separated by microsoft. You have to take them all or nothing. Second, it's their. Fucking. Computer. What do you not understand about that? You sound like some sort of Microsoft apologist shill. There isn't any rationalizing these user hostile decisions. Microsoft is taking advantage of their customers not having the knowledge, time or energy to counter their continual bait and switch tactics.
I'm curious, what specifically do you consider to be hostile to users? I personally prefer windows 7 and think they fucked a lot of things up, but I'm not sure I'd go so far as to consider it hostile from a UI standpoint. Unless you're referring to the telemetry?
same, I'm not sure why the above user is describing it as difficult. Just click no and you're done.
Maybe one day it will have C99 support. ^probably^not^though.
after seeing your comment I fired a new CI job to build the library examples with VS 2017, but it got stuck somehow (It seems gitlab CI has no public Windows runners). I will try with my own runner, and see what happens.
I always think of it this way: * and -&gt; are unsafe pointer operations, so are they in optional also. The checked access is, as you say, .value() and the query .has_value().
Try to set up Appveyor.
Uh... Yes ? Many embedded targets are supported by GCC, and mingw distros are currently at GCC 7.3 which supports 99% of c++17. You can use whatever compiler you want with clion so I don't see how it is relevant.
here you go https://ci.appveyor.com/project/Manu343726/tinyrefl/build/1.0.1
Yeah, Ubuntu shared objects are hard to get working on an NT kernel.
It's not that logical to say that objects should not be default-constructible because there are often cases where a default constructor shouldn't exist. The problem is c++ has 'discovered' interesting concepts (for example, separation of allocation and initialization) which were never formalized into the language in a way that they are explicit. All objects should be default-constructible, including primitives, when declared like this: Foo bar; But if we actually want a "partially initialized" object, i.e. an object which is allocated but not initialized, we should have had something like this: declare Foo bar; (you can replace 'declare' above with a more suitable keyword, I used it only for showing the example). The above would be useful in many cases: * would make the code more readable for humans, as intent would be a lot more visible than what it is now. * would make the code more readable for the machine, allowing code checking tools to reason more easily than now about the state of variables. With such a mechanism like the above, we wouldn't have to think about how default-constructibility is overrated.
&gt; The STL doesn’t ever need default-constructibility for anything. Forward iterators (in the definition of the standard) have to be default-constructible: http://eel.is/c++draft/iterator.requirements#forward.iterators http://en.cppreference.com/w/cpp/concept/ForwardIterator
&gt; The STL doesn’t ever need default-constructibility for anything. Uhm... std::vector::resize()???
A generic algorithm is not correct or wrong and does not have specific requirements until instantiated. It can have generic requirements though. Therefore, there is no need to change anything, span and string_view are fine as they are. 
Nope, you can pass an extra argument in case of the type isn't default constructible: void resize( size_type count, const value_type&amp; value );
You don't have to use the `(size_type)` overload, you can use the `(size_type, T const&amp;)` version.
Amazing... Afaik, Scott Meyers is a self-employed C++ author and educator. While his books and talks are amazing and I have learned a lot from them, I don't think he has ever put much code into production and he himself is very open about how much of what he teaches he himself has learned by asking a lot of questions to many different people that can probably be considered the REAL experts. In other words, Scott's main contribution is that he is good at aggregating and distilling knowledge, but that knowledge is ultimately received from others. On the other hand, Stepanov is responsible for the genesis of a huge part of the C++ standard library. Basically, without Stepanovs insights and work, its possible that C++ would look very different today, or maybe even not exist as more than a curiosity language. To call either one of these a clown or not an expert is simply insulting, to both of them personally and to the C++ community as a whole.
But then you have to implement a sane copy constructor. If copy-constructing is easy, then default constructing won't be too difficult either, will it?
What if you \*know\* you are downsizing, is the value then still required, even though never used?
Even worse, https://developercommunity.visualstudio.com/content/problem/19630/generators-using-co-yield-are-18x-slower-than-a-fo.html is still valid. In my test I get a slowdown of 13x on a x64 build when generating integers compared to a hand-writter iterator (and of course a heap alloc).
Re coroutines - not yet, looks like a heap alloc is still there even in simplest cases.
You need to use the [/Zc:__cplusplus](https://blogs.msdn.microsoft.com/vcblog/2018/04/09/msvc-now-correctly-reports-__cplusplus/) compiler command-line argument.
Another old-school rationale: How did `std::vector` work before placement-new became a thing? It had to default-construct the (elements in the) backing array unless it was using a vendor-specific placement-new behind the scenes, right?
Yes, I think so.
I know. But how often do you want to add a bunch of elements that all have a specific value? Usually only when that value is, in some sense, the default value...
I know. But how often do you want to add a bunch of elements that all have a specific value? Usually only when that value is, in some sense, the default value...
Yes, but resize is intended to allow for either. If you _know_ you're increasing or decreasing in size, you should be using a bulk add or remove function instead.
a) I never said that STL is not amazing, I just said that Stepanov had nothing to contribute to C++ in the past 20 years. b) I am sorry, but "aggregating and distilling knowledge" means that Scott is an C++ expert. You can not do what he did by just simply talking to people and writing down what they said without understanding it. Sure there are thousands of C++ experts better than him in the world... But that is still 100+x less than the number of C++ developers who are better developers than Stepanov. On the other hand Stepanov is free to shi*t on TMP/Boost/Meyers quite easily, but that does not make him an expert. 
You'd have had to use placement new for that default construction anyway in most implementations...
&gt; before placement-new became a thing What do you mean by "before placement-new became a thing" ? I was using placement new in, IIRC, 1996. I don't remember it to be vendor-specific. Am I missing something ?
 Thanks for the errors you pointed out Jeff, currently I use valgrind to check for any memory leaks, also am contributing to the techempower benchmarks project to get hold of any open issues present in the framework, But I'll agree I never ran cppcheck on the source code earlier. I'll definitely make it a point to run checks and re\-organize/correct any discrepancies found in the process. Thanks for taking the time out and evaluating the code. 
Maybe a `resize_emplace(size, ...)` would do the job.
You mean the "empty state" of the right value after a move? It's more like an undefined state basically it can be empty or whatever.
The empty state of an object that has been moved from needs to be destructible and assignable. Same conditions as an object that has been default constructed.
I once saw `Foo bar = void;` proposed as an explicitly uninitialised declaration syntax, which I liked quite a bit. It conjures up the image of calling some dangerous, unknowable value from the abysmal void. That probably wouldn’t play well with the recent efforts to make `void` a useable type. 
This type of performance tuning is such a lost art nowadays. Thanks for the write up.
&gt; There’s no such thing as a “default” playing card Joker?
I'd prefer passing a generator expression (lambda returning `value_type`): - works with non-copyable types, - allows customizing the value returned (need not all be equal), - allows throwing/aborting if not expected to be called.
Yes, which is rather unfortunate :(
Then why are security updates not separated? Also auto crash reporting and security updates are two separate things. Also all of this ignores Cortana sending entire browser history back.
&gt; “Modern” user-defined types should not provide zero-argument construction That admonition doesn't seem justified. Do you consider `optional` modern? Would you argue that `string` and `vector` should have their default constructor deprecated as part of modernization? I assume not since you are using them later in the article. Perhaps a better rule would be "types without a reasonable default value shouldn't be default or value constructable." But I think a large number of types (but clearly not all) can come up with a reasonable default. For containers, that value is clearly the empty state. For integers, bools, and pointer, that value is clearly zero/false/nullptr[1]. As mentioned in the article, unique_lock is an interesting case, since it is both useful in a few cases, but also quite dangerously easy to typo. I'm leaning toward it being a good idea, but only if we get [tooling support](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85736) to detect obvious bugs. Additionally, I think it is unfortunate that the default init syntax sometimes means call a default constructor and sometimes means give me uninitialized memory, depending on the type. Luckily the language provides a better alternative: value-initialization. If you get in the habit of using `auto name = Type();` to declare variables, you will get the sane defaults. It also covers simple structs without constructors, even those imported from C. I'd actually be in favor of making value-init be the result of using default init syntax, even in structs, and require explicitly marking the places you really want uninitialized memory (I've seen `int i = void;` proposed as the strawman syntax for this). [1] I know this is sometimes disputed, but I think the empirical evidence is clear. Try running `rg --no-filename '(auto|bool)\s+\w+\s*=\s(true|false);' -o -r '$2' | sort | uniq -c` on a few codebases. When run against all opensource code I have checked out on my desktop, I get 22968 false, 7154 true. When grepping for integral types initialized by a literal, ~75% are initialized to some form of 0. For pointers, it is the only literal value.
Well written. Thanks for starting this discussion!
In most of industry, yeah. I work in defense as a FPGA design engineer, and I am always working with our software teams to minimize wasted clock cycles to maximize their performance interacting with my hardware. My favorite optimizations usually start with my devices sending software more interrupts than they can handle (due to their poorly written interrupt handling architectures). "What do you mean you can't handle a measly 11 Gb/s?!" &lt;- something I actually said jokingly to my SW team when they complained about me sending them too much data.
I'll consider MSVC standards conforming only (and only) if it works with range-v3.
&gt; You can not do what he did by just simply talking to people and writing down what they said without understanding it. Maybe not, but he still aggregated and distilled what others already knew and understood. Stepanov, on the other hand, created something truly new and unique, which is still a big part of the C++ standard. There's a huge difference there, and it's not to Scott's advantage.
It's also not usually worth it. A day of work for 20% gain? If json parsing is not a bottleneck, probably not worth the investment. If it is, then it's totally worth it. In your industry, your needs might be such that the effort is worth it.
20% performance for a day of work is not worth it in your opinion? Even if, maybe it's not worth for a single app. But for a library used by thousands of developers across thousands of apps? I'd say definitely worth it.
FYI I got a custom version of LLVM built and managed to get a statically linked binary that's only 11MB after compressing with UPX, if you're interested.
This is from D. I think this approach would run into some serious problems in C++. Consider: std::string s = void; // or declare std::string s; s = "hello"; // BAM! UB Th first thing the assignment operator needs to do is (possibly) reclaim existing memory. Unfortunately, we haven't initialized the object, so references to the internal state result in undefined behavior. So the next step is to say that maybe the compiler can transform the assignment into an initialization. OK. std::string s = void; init_string(s); // init_string is in a different TU s = "hello"; // Assignment or initialization? The compiler can't tell what operation is meant by `=`. In general, this is an unanswerable question. Even if the definition of `init_string` were available, I think that this is probably equivalent to the halting problem. And you can't guess, because you get UB in two different ways if you're wrong. The only thing you could really do at this point is always use in-place new to invoke the constructor. std::string s = void; new (&amp;s) std::string("hello"); I don't see this as a usage win. Oh... and don't forget that you have to explicitly invoke the destructor also. Here's why: std::string s = void; maybe_init_string(s); // hmm... s.~std::string(); // Do I need this or not? The compiler doesn't know and neither do you. Also, there's no way to test `s` to check to see if it was initialized, because if it wasn't then you're referring to uninitialized memory. UB. FWIW, the *only* case for which this is guaranteed to be valid is if the variable is trivial. In those cases, there's no internal state that needs to be evaluated for assignment to be valid.
If it's not on a critical path, not really? Yeah this is totally worth it for the json library. But the commenter lamented people not doing low level profiling like this more often and it's usually because the time invested has to be less than the time gained, which is not always the case.
Who says that one should never worry about optimization. It is mostly said that one should not optimize prematurely. As Chandler Carruth put it „measure first“.
Nice article. I used VTune a few years ago now, and recall that it completely lock my system when profile 64-bit programs, I hope that's been fixed.
void isn't nullptr, I don't understand why the UB would necessarily be so. I would assume any use of a void variable other than assignment would be a compile time error. Classes are tricky. How would you signify what members can initialize the void object. Only allow a late call to the constructors?
I know like 4 vi commands to rarely use it like a crippled-notepad because it comes with every unix/linux system. :]
`map::operator[]` then?
Which is a shame because this sort of thing is what makes programming fun.
Just looked on ebay and there are keys for ~ 20 €. (OEM key sale is legal in Europa/Germany!)
Isn't LTSB like Debian stable? Like sure its a stable release but it probably also is older then you... when what you actually want is more like a Ubuntu LTS. Stable but not rusty.
Now Microsoft only needs a repository for end user software and they reached the point where Debian/Ubuntu was like 10-15 years ago. :P
&gt; If it's not default constructible you can't put it in a std::map that's just not true. map::insert() only requires a move or copy constructor, and map::emplace() can call an arbitrary constructor. you only need a default constructor for the `map[key] = value;` syntax
&gt;“Modern” user-defined types should not provide zero-argument construction That is _way_ too simple. How about a container type? Should we always require a container to be filled with at least something? 
You could have an attribute (something like [[noinit]]) to indicate that you want default-initialisation, with value-initialisation being the default for all cases where you don't put the attribute. That wouldn't break anything, and might fix a few... 
For the record, I'm not advocating this as a feature for C++. I just want to point out that it's not especially compatible with the language, and especially with classes. `void` isn't used as a value. It's syntax used to select a particular form of initialization: none. The UB comes from using indeterminate values in the "partially formed" object, not dereferencing a null pointer. The point about non-trivial classes is that if you don't perform default initialization, then you can't generally know the state of the object unless you are very, very careful about how it is used. 
It depends. For some types moved-from is only used as an optim, and basically the object should be somehow dropped but C++ prefers undefined things (in this case not in the UB sense, but still...) so that you can shoot yourself in the foot. But for others (unique_ptr comes to mind) the moved-from state is well defined (plus not any kind of conceptual trap representation, even if the user's code need to check it in areas where it can potentially have this value), everybody depends on it, and if a default constructor exists it should yield the same state.
No it does not matter to me if something takes 0.8 vs 1 seconds to load. Not even in the case of 8 vs. 10 seconds. I'd say it gets interesting when it's more than 30 seconds that can be saved. But that would mean 2.5 vs. 2 minute load times. Or, of course, when you're regularly parsing json, so that it's a statistically significant part of your program.
Ah I didn't realize that, statement retracted. I still think default constructibility is a good thing, especially if you have a fallible constructor, because you have to do something like: object o; try { o = object(1,2,3); // can throw } catch (std::runtime_error ex) {} If you want the object in the outer scope. 
&gt; the STL typically value-initializes (empty-list-initializes, to be precise) Are you sure about that? I thought the stdlib completely avoided ever using brace-init with user types. Unfortunately there are differences between even `T()` and `T{}` due to this terrible loophole: https://godbolt.org/g/aSr1XP
You do realize that making stuff const-correct would have fixed it without any other changes, right? 
I thought containers used empty-list-initialization in C++11 and +, but maybe not and maybe they just use = T(). Its a shame though (but maybe it is due to deficiencies like the one you cite). But I still don't find that a general rule that default constructors shall not be used because of the indirect hazard they could imply because of deficiencies of the language is a good idea. The original deficiencies should be fixed instead. 
&gt; There's a huge difference there, and it's not to Scott's advantage. Like I said: 30 years ago. And even if he did it today that does not make him an expert on everything related to C++. BTW Stepanov also called Google developers hopeless. Are you now gonna try to claim that Google hires idiots? 
I agree that *default constructors* shouldn't be avoided. Value init is a good thing (see my top-level screed on this post)! It is just that ~~uniform~~ unicorn initialization syntax didn't actually pan out as well as intended, and may cause more problems than it solves.
Doesn't looks like intellisense recognizes that option in gcc/clang mode. The macro stays at `201406` (and it keeps crashing on moderately complex gnu++17 projects).
Well it causes some problems for sure, and maybe for now that even render them improper for some usage in the STL. But more than it solves? At least in my own templates I would prefer avoiding risking UB because of uninit values than prevent a construction that should not have been allowed. Are there other problems with empty list initializers? 
Sure, if you can use 17, it'll be years and years before I can use that in my environment, we're just barely moving to 11 now...
do you have access to boost::optional in the meantime?
&gt; Are there other problems with empty list initializers? They are a weird special case that are defined to behave differently from all other forms of list-init for one thing. That means that `T{args...}` can potentially invoke a different constructor if `sizeof...(args) == 0`. I generally recommend everyone basically forgets that list-init is a thing and only uses it when absolutely necessary. With any luck, once wg21.link/p0960 gets accepted, those cases will be much fewer. In short, use round parens like `T()` instead. They are less sharp and pointy, both visually and semantically. I am *absolutely* not suggesting that anyone intentionally use uninitialized values, unless they have thought really hard about it, considered the tradeoffs, and decided that yes, it is actually worth it in this case. And added a comment explaining why! 
heh, we seem to be caught in an edit/reply race condition loop!
Placement new was a thing from the very beginning. 
C# solves this by having a special reference paramter that requires the function to initialize what is passed to it `out`. It also explicitly disallows passing a non-initialized variable that other way `ref`. You could do something similar here although I am not coming up with a decent syntax off hand. Your compiler could even warn you if you passed an uninitilized thing to something expecting an initilized one.
I never remember: is `auto v = T();` (or `T v = T()`) a sort of special case or is there any risk of copy/move construction?
You know I'm not actually sure, I'll have to check on that.
&gt; an undefined state 'unspecified' and 'undefined' are very different things
If it's a library how do you know whether it is on a critical path or not? 
Self-reply: according to http://en.cppreference.com/w/cpp/language/copy_initialization it seems to be guaranteed copy-elided starting in C++17, so I guess there is no absolutely perfect solution for C++14 and less.
&gt; I don’t think so. I think the more likely conclusion is that generic code does not, and should not, create arrays like this. By avoiding such operations, we enable our generic algorithms to work even with types (like playing_card and regex_error) which are value-semantic but deliberately not default-constructible. I think avoiding arrays like `T data[N]` in generic code is reeeeally optimistic.
You can always use `auto&amp;&amp; var = T() ` since c++11 if you want to be super-duper sure. In practice however, as long as T is move constructible every compiler will avoid calling the constructor even with opt=off. I'll post a godbolt link as proof once I get back to my desk. 
And Scott has, to my knowledge, never created something even close to what Stepanov has created. And nobody has claimed Stepanov is an expert on everything C++ related. Nor had anyone in this thread defended anything Stepanov had said. All people is saying is that calling Stepanov a clown that hasn't accomplished anything in 30 years make you sound like... a clown. :)
Some links woukd help...
/u/STL 's stuff on channel9 is awesome. (Meow.)
&gt; I thought containers used empty-list-initialization in C++11 and +, but maybe not and maybe they just use = T(). Containers don't do either and in fact don't do _anything_ directly; they tell the allocator to, via `std::allocator_traits&lt;AllocT&gt;::construct`. The standard-mandated behavior of both the default allocator (`std::allocator&lt;&gt;`) and `std::allocator_traits&lt;T&gt;::construct` is ::new (static_cast&lt;void*&gt;(p)) T(std::forward&lt;Args&gt;(args)...)
I can't think of a single `T` for which `auto var = T{};` or `T var{};` would cause problems in C++17.
He defends ideas that other people think is terrible that would push those people's view of what the industry should look like by decades.
My main argument is for consistency. It is easier to just say always use `auto var = T() ` and only use braces as a last resort, than to start carving out exceptions. Can to think of any reason to use T{} rather than T()? Also there is a major downside. See the link. T{} compiles even if T has a deleted default constructor! Do you really want that!?!
It is just logical - if there is nothing that could distinguish different values of an argument passed by value, passing anything in specific is just wasting registers. I've looked at the Itanium C++ ABI (generally used on Linux), and it turns out that it does pass something: see http://itanium-cxx-abi.github.io/cxx-abi/abi.html#empty-parameter "Empty classes will be passed no differently from ordinary classes. If passed in registers the NaT bit must not be set on all registers that make up the class. The contents of the single byte parameter slot are unspecified, and the callee may not depend on any particular value. On Itanium, the associated NaT bit must not be set if the parameter slot is associated with a register." I assume there is some justification for this but it's not called out.
&gt; You can always use `auto&amp;&amp; var = T()` since c++11 if you want to be super-duper sure. Wouldn't that bind a ref on a temporary that will die at the end of the statement, given the bind is not const so the lifetime won't be extended? But also before C++17 you still need to have accessible move/copy constructors if I understand correctly, even if they might eventually not be used. At this point, it is either: use {} but potentially get the = delete; problem, or `auto var = T()` but that might not even work if you have no copy/move constructor and are using C++ &lt; 17.
You can always use T var = T(); if you like your types really explicit. But this really feels like a special case of AAA where you are still explicitly stating the type.
Agreed, which is why I wouldn't actually do that. :-]
Finding poorly written parts of internal software and optimizing libraries that can lead to interactivity of any users of software that uses the library are two very different things. I'm not sure why you would ever think someone else's time to tighten up their own library is somehow silly. 
Yeah, `clang` would likely perform the first optimization. Either MSVC is not as aggressive or he isn't giving the compiler enough info (LTO and PGO). 
I would rather see `T t;` perform value initialisation. Specifically, it zero-initialises non-class types, and zero-inits then calls default constructors for class types -- i.e. `T t;` does what `static T t;` does today. Then we can use the syntax `T t = void` to mean what plain `T t` means today: default constructors are called for class types, and non-class types are indeterminate. In other words, we provide a safer default (variables are always initialised unless we specifically request otherwise), with an opt-in "no init" syntax for the rare cases when this is appropriate.
And I'm not sure what you're trying to say that I haven't! The op wanted to know why low level optimization techniques aren't used widely. Because they are not worth it unless your improvement is in the critical path. For this library, this speedup was worth it. For your code maybe not.
One of my pain points exactly. Anything moderately complex becomes unwieldy with variant. I really (really really really) hope we get language level variants and pattern matching someday. A man can dream. 
The paper would make sizeof(void) != 0, and addressable. So even if nothing specific is actually being copied, the appropriate amount of stack-space does need to be allocated for the variable before the function is called, or am I wrong?
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8gypp4/without_form_and_void/dytlj4p/?context=3.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
A relevant favorite (paraphrased) quote from Alexandrescu. "A 1% performance gain saves Google 10x my salary every year just in datacenter electricity costs." 
As noted, those are Defect Reports (which are bugfixes to the Standard), not top-level features. I included them in the table for completeness, as they are still papers that were voted into C++17 retroactively, but their relevance is fairly narrow. (For example, the resolution to CWG 1581 is of great interest to anyone trying to make `constexpr invoke()` but not many others.) Clang has listed some of these in their C++20 table instead.
Yeah, I'm not sure what scenario /await:heapelide is supposed to handle but it doesn't seem to handle even the trivial case I posted above. I'm not in favor of automatic heap elision if it's going to be this fickle and especially if it relies on the optimizer. 
Here I was replying to this: &gt; For example, I optimized a process cutting hours per invocation from a large pipeline. That's worth it, but when I spent a day to cut a few seconds off something, it might not be as worth it. Are you just using two different names? 
Suggest people upgrade to Win10 - But spyware! No, we'll stay on a decade old Win7 (age of WinXP at the time of Win7 release btw). Suggest LTSB branch with no such bloatware - but it's too stable and doesn't update as frequently! I mean it's true that MS doesn't really offer something in-between Pro and LTSB for regular users, but that's still a strange argument to make against upgrading from Win7. Also, while the current LTSB release is based on 1607 version of Win10, the next one is slated for 2019 so soonish WSL in LTSB would become an option (the only big option missing from the currently release, IMHO).
It was Facebook obviously and I think it was in relation to an stoi-like function and optimizing it for most likely input range. 
Google, Facebook, some tech company with "oo" in their name. :p
http://reddit.com/r/cpp_questions
I never said it was silly or a waste of time. I think you might want to reread what everyone is saying. You seem to have missed all context. 
Why? If it's going to be added it's better to be compatible with the C one if there aren't strong reasons to not do that. And having it as an attribute doesn't really fix the problems preventing its addition to the standard in its current form.
I believe you need to take a step back and read the posts better. I am a different poster than the person you replied to, and you're misreading what both of us are saying.
At the end of the day, in all jobs I ended up with requirement that it has to be built with Visual Studio and XCode and they only really understand their own projects. That makes writing a new build system pointless. Instead a project generator is called for (that's what makes everybody end up with CMake despite its warts) and a dependency manager preferably on top of that.
I'm by no means well versed in the topic, so I will only give you a pointer: * https://isocpp.org/std/submit-a-proposal
Most applications don't require thinking about performance apart from avoiding stupid designs, so they're most likely right about that.
I agree, but I'm maintaining a math heavy API, and most of the computations are matrix arithmetic, numeric differentiation, etc.
The awesome thing about vtune is that you don't necessarily need to be a follower of the "lost art" to see the impacts.
Oh, I had only been thinking about for primitive types, but that obviously wouldn't cover a lot of use cases. If I were King of C++, I'd be tempted take this opportunity to make the "Classes have invariants. Structs do not." rule official and say that the `= void` syntax is only applicable to primitive types and structs not containing classes (at an level of nesting or inheritance). But I'm sure there are also reasons why that wouldn't work that I just can't think of.
I'd like to know. It's about time (only partly joking - std::chrono introduced time units at least). Support for units does seem like an ideal candidate for standardisation. Units could be a Core Guidelines remit, perhaps, with GSL adopting or adapting boost units or an alternative. Or just standardise already! It's well understood and the language facilities are sufficient - constexpr, variadics, UDLs. I don't know of a reliable way to search for past proposals. That would be a useful facility on isocpp.org. Sometimes I get lucky searching the iso forums. Here, though, 'dimensions' and 'units' are pretty poor as search terms.
This recent discussion on the std-proposals forum shows that there is interest and some activity: https://groups.google.com/a/isocpp.org/d/topic/std-proposals/jvdcehfvuD4/discussion 
I've messed up recently with string processing and was surprised how compilers are very smart sometimes, but some other are not that much. In my case, I was using a base16/32/64 encoder/decoder. Replacing the switch block with ternary expressions allowed the code in the loop to vectorize, giving me a neat 2x speed up compared to the previous version. Good article. Intel VTune is a very useful tool. I liked how clear the reports were. I highly recommend using the Intel C compiler altogether (for analysis/optimization cycle at least), as it can instrument the program to get some additional information from the execution. The Vector Advisor was a useful tool too. Remember that nowadays we got powerful free open source tools to get tons of information about our programs without a lot of cost. In Linux in particular we got `perf`. It is incredible how it advanced in the last two years.
I didn't lament anything, I said this was a lost art - meaning the skills necessary to do this type of tuning aren't being taught these days. I've seen all too many developers write code with horrible performance and not even know where to start trying to identify the issues. If their IDE doesn't present them with the answer they're lost.
Clang is very good at generating lookup and jump tables from switch blocks, although it does it too many times. There are cases in which a simple if/else block (a branch which is often predicted correctly) is faster than accessing memory.
I think in this case it is likely produced because the compiler is not completely sure whether the pointer changes or not. I wonder what would happen if the function marks the pointer as const (not the pointee), like `struct json_parse_state_s * const state`; something like C++ references.
References have the same aliasing issues as pointers.
C++ doesn't have a "null" object representation for a generic type. What value (moreso, what _type_) would `v` have if the condition is false?
The type of `value`. o_O
because it's not stone age anymore
P0977R0 - Remember the Vasa. Personally I share the same concern.
That'd be `?:` `x ?: y` is `x ? x : y` (except evaluate `x` only once) `x ?? y` should probably be `x ? *x : y` (with same caveat)
I suppose it's because you still need to deallocate the memory block containing the ref counters.
I plan to talk about this to whoever will listen at rapperswill. Unfortunately, Bjarne doesn't like it, because it's redundant. Well, i wish we didn't have the in place syntax at all, adjective syntax is enough. 
Pointers often have this effect. If you want the optimizer to function while still using pointers, you should use the keyword 'restrict' or __restrict__ in c++. This has to do with the concept of aliasing and the default rules relating to it. The author likely could have got the same benefits by adding the __restrict__ keyword to the pointer in the function definition.
It's fine, nobody uses Linux anyway.
Could be because calls to it are inlined everywhere, to avoid inserting null checks everywhere and have smaller code, that null check will be performed in the deleter instead.
Yes, definitely! I think the downloadable builds should be statically linked. Can you file an issue on github detailing how you did the static build (even better would a be PR making CI generate static builds :))
So we have papers on: * Merge Coroutines * Alt Coroutines * Executors * Alt Executors * Pro Graphics * Anti Graphics * Merge Ranges * Standard Library Concepts * Reasonable Compromise for Terse Concepts Syntax (p1079) * Text Formatting * Package Management * Lightweight Exceptions Rapperswil is going to be a memorable meeting.
Unfortunately, last time I checked, OWASP had no such things for C++. One way forward is to port their implementation in any language to C++. Orthogonally, I strongly encourage you to use a [content security policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP) that is as agressive as possible to mitigate the risks.
It's been proposed, http://wg21.link/n4120 is the first paper I could find. Generally rejected for being insufficiently motivated.
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8irfxr/why_is_deleter_in_stdshared_ptr_called_always/dyu60y0/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Yeah, I also loved the adjective syntax. There's the argument that it cannot constraint types based on the types of other arguments, like "the first argument must be convertible to the type of the second argument" but in these cases we can always fall back to `requires`. I almost think we should first pick a syntax to solve simple concepts like `Range`, `Regular` and even `ConvertibleTo&lt;T&gt;` and `SubClassOf&lt;T&gt;`. The adjective syntax fix that. After we can take all our time finding the syntax that can constrain a parameter type by the type of another argument.
Whoever pitched it that way was wrong. Emulating `static if` from D was considered and rejected.
Even though it's against specification, AFAIR.
Erm, how exactly? You can only reasonably move from the object once.
I suspect the proposal for Graphic Coroutine Span Range Concept Syntax Future Executors will be controversial.
By my count there are at least 4 papers on lightweight exceptions!
It started with the idea of a simple user friendly syntax that's easy to read, and mutated in something as short as possible that can handles everything, which for me is not a desirable goal. As you said, we already have a syntax for the complicated case.
I was thinking about passing args without using perfect forwarding, so no move: template&lt;typename ...Args&gt; void resize_emplace(size_t iSize, Args&amp;&amp;... iArgs) { // somewhere, for all new elements emplace_back(iArgs...); // no perfect forwarding } std;;vector&lt;std::unique_ptr&lt;anything&gt;&gt; v; v.resize_emplace(10, nullptr); It's not a perfect solution since it won't work for value\_type that take r\-value in their constructor.
That just leaves downloading and install, then. all of boost, because the boost headers are not header only the way people commonly think of it. The key benefit of header only libraries is that they work by themselves. You copy just the header, and it works. THAT is what makes installation trivial. Unless i'm mistaken, boost is.. not that. You can't just copy one boost header into your project and expect it to work, it is not a standalone single header. To use boost, header only or not, you need to install all the boost headers, or manually pick out what you need, because they tend to include other boost headers.
Good way to search for proposals: The [Cpplang Slack](https://cpplang.slack.com) has **npaperbot** who you can message to search for papers.
And don't forget iostreams v2, and low level file i/o!
Zero overhead exceptions are a very intriguing proposal. It combines the strengths of runtime exceptions and expected/outcome. For those of us that still error codes, I think it would be a big bump in readability and a reduction in boilerplate.
From http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0977r0.pdf &gt;The foundation begun in C++11 is not yet complete , and C++17 did little to make our foundation more solid, regular, and complete. Instead , it added signif icant surface complexity and increased the number of features people need to learn. C++ could crumble under the weight of these – mostly not quite fully -baked – proposals. We should not spend most our time creating increasingly complicated facilities for e xperts, such as ourselves Absolutely.
Thankfully, gcc, clang, and MSVC all support `__restrict[__]` on pointers and references, *and* on member functions (making `this` restrict).
You may be interested in the Missed Optimizations Diagnostics of Clang. Here is the kind of things you can get: http://lab.llvm.org:8080/artifacts/opt-view_test-suite/build/SingleSource/Benchmarks/BenchmarkGame/CMakeFiles/n-body.dir/html/_org_test-suite_SingleSource_Benchmarks_BenchmarkGame_n-body.c.html And some slides about it: https://llvm.org/devmtg/2016-11/Slides/Nemet-Compiler-assistedPerformanceAnalysis.pdf
As much as I agree that package management is a huge issue, wouldn't it be wiser to let the community settle this one? We have a ton of build systems and the community was able to come to a relative consensus on CMake and while I don't know that everyone loves it, if you use it correctly, it does its job very well. I imagine if we give it a bit of time, the same thing will happen for package managers. We've already got a couple of options that I'd like too see have a chance to gain traction.
That won't work either. The body of the `starts_with(std::string_view, std::true_type)` will still be parsed when `__cplusplus &lt; 202001L`. If you replaced with with something like: template &lt;typename T&gt; auto starts_with(T, std::true_type) -&gt; std::enable_if_t&lt;std::is_same_v&lt;T, std::string_view&gt;, bool&gt;; you could avoid a compilation error, but that's uglier than a `#if` (in my opinion at least)
&gt; wouldn't it be wiser to let the community settle this one? If you read the paper ([P1067R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1067r0.html); am I the author), you will see that it doesn't propose to standardize anything. Rather its goal is to establish a frame of reference (do we need a portable "consumption manager" functionality like what many system package managers already provide or something more like project manager similar to Rust's Cargo) as well as get the discussion going. &amp;nbsp; &gt; the community was able to come to a relative consensus on CMake I hear claims like this but I don't see any hard evidence. What I do see evidence of is that CMake is loathed by a substantial part of the C++ community (here is a [snapshot](https://codesynthesis.com/~boris/tmp/cmake.png) of a recent discussion as one example). Settling on, let alone standardizing something like this wouldn't seem wise.
Yes, please. I super don't want to have to keep getting intrinsics from 3 different frontends every time the committee decides to constexprize something we optimize.
Not that's exactly what I do, download, extract and include the headers in my project. Trivial
It's not about some mythical "stone age" (aka "hurr durr it's old", even though Linux kernel is 26 year old, GNU began in 1983. Heck - the still common x86 started in 1978). You don't use vim because you don't want to learn. And that's a good thing - embrace it instead of talking bullshit like "it's not stone age". You like GUIs, then you better use them whenever you can, as that's what computers are for. If you don't try to use a computer in a way that you find the most enjoyable/comfortable, then i will hack you with Emacs via sendmail.
I sincerely doubt that. A quick look at the boost docs for, lets say boost.Algorithm, thats probably something useful. Lets see what that header looks like, shall we? https://www.boost.org/doc/libs/1_67_0/boost/algorithm/algorithm.hpp #include &lt;boost/utility/enable_if.hpp&gt; // for boost::disable_if #include &lt;boost/type_traits/is_integral.hpp&gt; Well, right there you already need boost.utility and boost.type_traits Which themselves include yet MORE boost headers I'm not sure what asinine world you live in where thats trivial, but it isnt the real one. The key benefit of header only libraries is they are ***one*** header. One. That you can just pick up and drop in. Boost may *have* header only libraries, but they are not *trivial* to install. They may be relatively easy to install, compared to binaries, but they are not trivial. But hey, maybe it's an outlier? Lets pick something simple, [boost.assert](https://www.boost.org/doc/libs/1_67_0/boost/assert.hpp) ..oh, oh no, even something as trivial as an assert macro still includes two more headers, themselves which also include more headers. TLDR: Boost libraries are header only without the key benefit of being header only. 
Sure, if you want to put all of boost into your project, go ahead. In that case, whats the benefit of being header only again? If you're going to be including all of boost anyway..
I did not know that! Thanks!
Fair enough!
I think you should be fair to CMake. Yes the CMake language is often loathed, but in the same sentence most people often say "it's the best thing we have / worst of all evils" etc. And with modern CMake, it isn't as bad as it used to be (but I agree - still has its problems!). I am pretty sure if you do some analysis of GitHub C++ projects, you'll find that most of them will have a CMakeLists.txt, while no one other build system will be that widespread. I'm also quite sure you would find that a very large percentage of successful projects (let's say all with more than 50 or 100 stars) will have a CMakeLists. Most often when someone posts his library here in /r/cpp, and it doesn't have a CMakeLists yet, one of the first comments will be "Add a CMakeLists, if you want your project to be used".
Sutter's proposal on value-based exceptions (P0709R0) is the first alternative (to exceptions) error-handling proposal that I actually like.
If a software I used took 8 seconds to load I would certainly not use it again
Oh I thought we were talking about data?
I really don't want this to descend into a CMake debate, so just a couple of meta-points from me and that's it: &gt; "it's the best thing we have / worst of all evils" Is it wise to standardize on it, though? Why can't we make something better? Look at Rust: they have built a build system/package manager that is widely considered (even by the Go folks) the gold standard all while designing and implementing a new programming language. &gt; I am pretty sure if you do some analysis of GitHub C++ projects [...] That's the thing: C++ code on GitHub is probably not even 1% of all C++ code out there. 
http://www.stroustrup.com/P0977-remember-the-vasa.pdf
I updated /r/cpppapers/ with the new papers
The problem is that language support is probably the only way to get a really nice units facility. F# Units Beats Boost.units in power, usability, composability, ... by a pretty large factor, and there is nothing in C++20 that shortens this gap.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8iuzzv/helpnoob/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I think the underlying problem is the lack of *good sense of design*, especially at this complexity level. As an example, look at the Atom proposal -- basically every second sentence starts with "As a special exception..."
Yes! Stroustrup really hits the nail on the head. C++11 uniform initialization syntax clashing with initializer lists is still a daily annoyance, 7 years later, which really leads credence to his *Law of Unintended Consequences* between parallel proposals.
minor correction. I don't use vim because I see 0 value for time spent, when MSVC does everything better.
Why did you get this removed? By no stretch of the imagination is this "help". It's an interesting question, I was hoping to get an answer, and instead you removed it for no good reason.
Yes, everyone who shares my opinion is in fact the same person. 
Optimization techniques aren't just for the processor level, but basic application code as well. I recall a situation years ago where a point of sale processor wasn't able to keep up with all of the transactions coming from the various cash registers and was having memory problems. The team didn't know how to solve it, called IBM for help (this was running on an AS400). Turned out the problem was solvable by using a basic fly-weight pattern for Message objects. Seemed the system was thrashing on allocating and garbage collecting what could have been reusable objects. No one on the team that wrote the code even knew how to identify that issue. 
I agree. I'm sure there are reasons why you might do it, but having a mutable global shared_ptr seems like the underlying problem here.
That's not minor correction. That's something which is barely related to the original comment.
?: should work
Cool, cool... I wonder if the massive compiler bugs, interface lag and compiler-linker setting GUI issues have had any work.
Ok so, `char8_t` is just a 8 bits storage that is unsigned and doesn't alias to `char` such that `void f(char); void f(char8_t)` are different overloads. `std::u8string` is just a `basic_string&lt;char8_t&gt;`; size() returns the number of bytes - code units. And iteration is done over bytes/code units. So, it's not very Unicode aware. it's storage. It's a type that can store Unicode, not handle it. But it's a foundational work that will let us add more clean Unicode facilities, including code-point and graphemes iterators in c++23/26. However, `std::u8string s = u8"foo";` will guarantee that `s` contains utf8-encoded text. 
Pretty sure this is a gcc extension only
Thanks for correcting me. That is less that what I was expecting (I was thinking that it would be unicode aware giving the number of characters for size / iterations instead of bytes). Anyway if it serves as basis for a unicode aware string it is a very welcome addition.
How could it be `true` in the second case - the compiler doesn't know if it *can* be `constexpr` in that context without the logic in the function, and the function now branches based upon its `constexpr`-nature.
CMake is prevalent in open\-source projects. In business I haven't seen it much. We'd rather fight with dead build systems like rake or even more obscure ones ;\) \[anecdotally\]
amen!
We'll get to most of your papers I think.
All David Stone's papers would improve issues I deal with constantly. Thank you! Lets hope some of it is well recieved. \- constexpr Function Parameters \- Automatically Generate More Operators \- Implicitly move from rvalue references in return statements \[thumbs up\]
Sure, most of mine went through a study group or are about a new study group, so they get priority. I was more meaning for everybody who spent so much time writing papers without going through a study group. I think we need to more clearly emphasise that direct submissions without going through a study group are not wise. I wouldn't ban direct, sometimes you need to submit a "please really don't do this" paper wearing your hat as a domain expert. But for feature proposal papers, they ought to go via a study group, else the load becomes infeasible
it's actually perfectly normal. The "if" is not a "constexpr if" which mean that both branches will be compiled into the runtime version. simply replacing it with "constexpr if" wouldn't work though because it would be evaluated while parsing the function and would be force evaluated to false all the time.
Herb is amazing. Every year he presents something incredible
&gt; giving the number of characters for size Turns out this is either really complicated or not all that useful (or both). It depends on how you define "character". I think the unicode standard goes out of its way to avoid using that word, prefering terms like "codepoint", "grapheme", and "extended grapheme cluster". Which would you like to use? Codepoint is simple, fixed size, and fast (linear scan, but trivial to accelerate with simd). However, they aren't really much more meaningful than the raw bytes, except as feedstock to other unicode algorithms, many of which could probably be made to work over utf8 bytes anyway. Most implantations are using utf16 due to unfortunate accidents of history, so they already need to do their work on code units rather that code points. At the other extreme, "extended grapheme cluster" probably most matches most people's intuitive definition of "character", but is complex, can have unbounded size (you can always add more modifiers), and changes over time as the unicode spec is updated. This is how swift defines their Character type. Give [this](https://developer.apple.com/library/content/documentation/Swift/Conceptual/Swift_Programming_Language/StringsAndCharacters.html) a read if you would like to see how this works out. It is an interesting parallel universe that I can't tell if I like or not. I'm split between the part of me that says "but it the only thing that is *correct*!" and the part that points out is both horribly inefficient and a poor match for the type of programming I do. BTW I'm not trying to badmouth anyone's plans. More like recognizing the huculean (and frequently under appreciated) task they have set themselves on. 
Indeed.
I really love his proposals. Insanely detailed, full of technical depth, interesting anecdotes and inspiring ideas. He cares and listens to the community and designs language features with compelling, motivating examples in mind: the meta-class proposal, for instance, outlined how QT MOC and WinRT could make use of the proposed feature, in order to solve existing challenges and design problems. It’s immensely important to have someone like him in the standard. Thanks, Herb!
you know you are reading a good proposal when you find yourself thinking "why doesn't it already work like this?"
All I see for the paper is the abstract and background, is it missing more content?
Hm. If I correctly understood the gist of the proposal - he suggests returning `union + discriminant`? Why not just completely automate it into proper sum types, that way it can be done seamlessly without extra overhead, just like in Rust and others? Though that's just the mechanism behind it. The proposal itself is much more in-depth. It would be just strange if C++ gets this and no proper language-level sum types. It'll have mechanism that basically use them underneath, it'll have inferior (in comparison) library-level attempts to replicate them (variant/optional/expected, etc.), but not actual language feature.
Even modern cmake suffers from wishful thinking about how the C\+\+ language works for large systems. For example, letting targets change the ABI is really a non\-starter, but it gets often recommended. It implements its own header scanning to determine build dependency, and will disagree with the compiler, leading to unstable rebuilds. Part of the problem is that everyone thinks autotools is way to complicated, as though the people making autotools wanted it to be complicated. I do have some hope that modules will provide a better way. If there's tooling to make sure that there's a deterministic mapping from `import module` to a specific module binary interface, for example. 
Wow, better than getting std::expected. A `throws` keyword in function signatures sounds far better than noexcept. This is awesome. It's nice to see one of the weak points of the language addressed and appears to be done well. C++20 is looking good. Thank you Herb.
Pretty neat as always coming from him. I like how he want to make the language simpler and even more powerful.
The way I see it, there's very little chance of this being in C++20. It hasn't been presented in any general standards meetings yet, so it's simply too early to just adopt it. It's also a rather fundamental change, so it will take a fair deal of time to process and adjust. If it progresses really smoothly, I could see it being adopted into C++23. I look forward to hearing what concerns are raised around it and what the reception is from projects that have exceptions banned.
For some reason, the discussion about special case treatment of heap exhaustion makes me very nervous. I regularly have a few tens of Kb of heap available, dealing with variable size data structures coming in, and terminating on heap exhaustion is not an option.
You might be interested in supporting [Zach's library](https://github.com/tzlaine/text), which looks to be aiming for eventual standardization.
How does the rust implementation differ in that regard? Aren't rust enums implemented as a union + value to tell which one is correct? (similar how std::variant is implemented?)
I’m pretty sure that’s how Rust does it - a union plus an integer tag.
The C++ standards committee is sooooo slooooooooow ☹️ Yes, I know there are good reasons for this but it is frustrating.
&gt; Hm. If I correctly understood the gist of the proposal - he suggests returning union + discriminant? Not exactly. That is one possible ABI to implement the proposal, though. Because the proposal clearly indicates - at a language level - the error type, that allows system ABIs to designate alternative paths for passing exceptions. Just like error ABIs today do, albeit with the overheads of dynamic dispatch. His paper shows some calculations of instruction overhead; basically, the proposed model is hypothetically smaller and faster than either return codes (and sum types) or than today's C++ exceptions. &gt; Why not just completely automate it into proper sum types, that way it can be done seamlessly without extra overhead, just like in Rust and others? The paper is a generalization over what sum types can accomplish. Note that Rust and other contemporaries are working to make errors-via-sum-types less awkward to work with and closer to what Herb is proposing (in no small part because his paper sources the same research!). See Rust's recent-ish `?` operator and the `try` proposals in flight, or Swift's various offerings on the matter. As Herb notes, in C++, the community in general wants error propagation. Rust in turn requires explicit error handling, albeit with some syntax to automate propagation. There's a very important line between "propagate errors implicitly" and "propagate errors automatically" and Herb also calls that out with suggestions in the paper. I suspect that topic is going to be a major contention, perhaps just behind the OOM topic. And it's not an either-or kinda thing. Nothing here precludes also using sum types when and where appropriate. :) &gt; It would be just strange if C++ gets this and no proper language-level sum types. In terms of _proper_ sum types in C++, I don't think they're possible (to do well) without this proposal and its OOM suggestion! Remember `std::variant`'s years-long design process and eventual compromises, literally all due to the unfortunate fact that C++ types can throw when they move, which in turn is only allowed because OOM throws exceptions. Alternatively, C++ would need to solve the even-harder problem of adding relocation (aka destructive move) to do sum types correctly. Rust and its ilk have trivial relocation as a core assumption of the language. The feature isn't easy though; see how Rust handles relocation of `Drop` types for example (which affects ABI around mutable reference parameters). The other half of sum types - the actually hard but actually interest part - is the language support for pattern matching and conditional destructuring. Which can and should be explored independently of the exception model, IMO, since there's no dependency between the two. 
Nah, we won't forget ur carefully pampered baby. I just hope it doesn't get axed before it starts walking
On the other hand, it feels like the next generation of build tools (Meson, QBS, and Build2) are seeing an increase in adoption recently, so it may be a bit early to say that CMake has won.
&gt; see how Rust handles relocation of `Drop` types for example (which affects ABI around mutable reference parameters). What are you referring to here? I don't believe `Drop` affects ABI at all in Rust.
&gt; Do you need to avoid heap allocations and dynamic casting in the error case? As mentioned in Sutter's paper, avoiding heap allocations is indeed necessary in many constrained targets or real-time programs. &gt; Do you need to disable RTTI? Are you just looking to avoid the runtime and binary size cost of the unwinder? This might be necessary on the most memory constrained targets indeed. &gt; Are you trying to make the error paths more explicit? This is actually interesting, as there are several (distinct) benefits from this, however few are realized as long as OOM is treated as a regular exceptions since as mentioned by Sutter OOM errors pervades most code. Even without making error paths explicit to humans though, Sutter's proposal has the benefit of making them more amenable to static analysis and thus optimizations. &gt; Mine is primarily focused on the cost of the unwinder when it isn't necessary, and assumes the heap allocation and dynamic cast is fine. That is colored by our heavy use of exceptions for error handling, but sometimes needing to use errors as values and not wanting the costs to be silly in those cases. I think this underlines the current situation of exceptions quite well; when people fear using them because of performance issues (whether actual or supposed). I am not sure whether Sutter's or your proposal will solve the issue, but I am glad to see research in this space. --- On an unrelated note, I wonder if a second class of "special" exceptions should be born, dubbed "panics", which the final user could choose to implement either as a special exception or specify that they lead to terminating the program immediately. For users not interested in catching OOM situations (notably those using overcommit), it should provide immediate optimizations opportunities.
I recommend all Andrei Alexandrescu talks, just because he is hilarious :\)
Please write a tutorial on how to provide your own iterators. There aren't many good resources out there. Ty!
I usually find them useful in avoiding to rely on polymorphism to store mixed types in a container. 
+1 what’s the problem here? Rust doesn’t even have drop flags anymore :/
How so? Would you mind explaining the details on this and where this would make a differernce?
Unfortunately, once something goes in, it's hard to change in a breaking manner. For my personal projects, I'd gladly take breaking changes and fix my code accordingly if the features I want were implemented sooner.
In a lot of cases, the answer to "why is there no ___ in std" is because ____ is a great idea, but nobody has thought of it, or they've thought of it but everyone's been working on higher-priority changes. I honestly don't know the process for proposing a feature for std, but one way to get started might be making a proof-of-concept. Show everyone that it can work and that it's faster than a heap-allocated map.
It is definitely interesting to be able to share the IDE/Debugger, I tried with both VSCode and VS2017. The only failing I ran into was that I could not figure out how to view the output from the person's console application and a minor quirk that find in files did not search the shared workspace.
The proposal has this to say: &gt; Program that relies on `bad_alloc` throws and does handle them. Many systems typically already perform all allocation through a limited set of allocation functions, so they can just change the implementations of those existing functions (or wrap them) and implement them in terms of the `new(nothrow)` and/or `try_` functions and throw `bad_alloc` or some other exception on failure. Programs that have such a limited set of allocation functions can make this low-impact localized change to retain their current behavior. Is this feasible for you? If not, now's an excellent time to start discussion on how the proposal could better serve your needs here.
there is : https://github.com/serge-sans-paille/frozen
It's actually not impossible to do yourself. Ben Deane and Jason Turner cover this (and more) really well in their [constexpr all the things!](https://www.youtube.com/watch?v=HMB9oXFobJc) talk. It turns out it's far from the only thing that's not `constexpr`which you might expect to be. One of the interesting takeaways from the talk is that in many cases they're not `constexpr` not because it's hard to do, but because nobody thought to do it. It's a really interesting and fun talk. I recommend it.
The layout of a Rust enum is optimized based on the invalid values of the types it contains. For example, because references can never be null, and `Vec&lt;T&gt;` contains a reference, `Option&lt;Vec&lt;T&gt;&gt;` can use "`Vec&lt;T&gt;` except with a null reference" to mean `None`, and "`Vec&lt;T&gt;` with a non-null reference" to mean `Some(the_vec)`. The discriminant overlaps with the reference, in a sense. In general, there are lots of types like this. References can't be null, bools can only be true or false, C-like enums can only contain the set of values in their definition, `char` can only contain valid Unicode, etc. So the Rust compiler looks for "holes" in the contents of an enum where it can store the discriminant, before using a separate tag value.
I have a feeling you are taking issues that are specific to you and assuming it's a problem everyone else is experiencing.
(To people unfamiliar with Rust: Rust used to add an extra “drop flag” to structs with manually-implemented destructors, now Rust stores the flags on the stack instead)
It sounds similar to this [`compact_optional`](https://akrzemi1.wordpress.com/2015/07/15/efficient-optional-values/) article I read once.
We haven’t implemented that - but CTAD serves a similar purpose. Try `array arr{ 11, 22, 33 }` in C++17 mode.
Oh, right, because it's not a template. You don't technically need the enable if bit, though it is probably warranted. Still disqualified as a "can't do it" example. Ugly AF isn't can't.
I agree with pretty much everything /u/matthieum said. The other reason I dislike exceptions is that they aren't very beginner\-friendly, in that it can be easy to introduce bugs if you aren't careful around throwing code. I don't think the new proposal does much to help this situation, but without the runtime and binary size overheads, it does make the tradeoff more interesting. Given how much unique\_ptr has helped with exception\-caused memory leaks and the fact that the function signatures would explicitly call out that they throw exceptions, I might lean more on the side of using exceptions.
&gt; what you'd like to see improved about exceptions Personally, I don't like the non-local flow control. I'm okay with some simple language construct to say "this thing I'm calling may fail in some way, and if so I don't want to handle the error, just pass it to my caller". But I want that to be explicit. I want to be forced by the compiler to either explicitly handle the errors coming from a function call, or to explicitly say I'm not handling them. Exceptions as they work today, and as best I understand lightweight exceptions, don't give me this. It forces me to do what I call non-local reasoning about my code.
Unless you want to use them for non-numerical types... Boost.Units lacks the ergonomics, but not the power or composability.
you tried "-Wall -Werror -Wextra -pedantic" ?
You mean like `[[nodiscard]]`?
Calling conventions are all about agreement of how to how the calling and called code exchange data, based on the signature of the function. The calling convention can be defined in any way that enables the transfer of parameters and return value and otherwise allows the program to run correctly. So in case of passing empty classes by value the calling convention can state that nothing is passed \(no registers, no stack\). The calling code would know it should pass nothing, the called code would know to expect nothing. Even if the called C\+\+ function tries to do something funny with the value like take its address or read the single byte of its representation, the compiler can do anything that still conforms to the language specification \- it doesn't need any information from the caller of the function.
I am just curious does these "zero overhead exceptions" are in any way different in practice than the current exceptions? I mean they will be a specific type of new exceptions or they can replace the current exceptions without breaking changes in the code? If they are new type of exceptions do we still have access to the `.what()` methods, to return a error message, or they only return an error code?
Well, we will get one of those right then.
Mainly because there is no `constexpr new`.
``` class A{ public: __attribute__((warn_unused_result)) int add(int a, int b){ return a + b; } }; int main() { A a; a.add(1, 2); return 0; } ```
 class A{ public: __attribute__((warn_unused_result)) int add(int a, int b){ return a + b; } }; int main() { A a; a.add(1, 2); return 0; } 
Right. I can definitely see default being useful for some projects.
Agreed. I‘m lucky that I work on a codebase where making big changes is not too hard, but I am not the kind of developer the C++ committee cares much about. The principle of marking functions as `throws` rather than `noexcept` seems so much more sensible that I‘d gladly take a breaking change for it. Plus it‘s actually a word while `noexcept` isn‘t.
&gt; A `throws` keyword in function signatures sounds far better than noexcept. This is awesome. ..exception specifications? Didn't we already have those?
&gt; Modeling OOM at language level by Undefined Behavior &gt;implementation-specified behavior Just to point out that these are not the same things by any means. I.e. what you fear regarding compilers optimizing stuff at the expense of UB (as they should) wouldn't happen with implementation-specified behavior.
Some say there is no love witout pain, but papers like this one make me think it may not be the case in a few years.
This is pretty much a consequence of destructive move: sometimes you can’t statically know whether a destructor did run, and need to track this at run time somehow 
But from the other point of view - C++20 is nearly complete already (IRC Rapperswil is the last meeting where major features such as this can still be voted in), so this paper just arrived to late to have a chance to even be considered for the standard.
What I like about this is that noexcept wouldn't really be needed. noexcept is complicated and annoying. 
&gt; &gt; &gt;When one of today’s dynamic exceptions is unhandled in &gt; &gt;f &gt; &gt;’s &gt; &gt;body, regardless of whether it originated &gt; &gt;from a nested function call or a &gt; &gt;throw &gt; &gt;statement throwing a dynamic exception, the exception is auto\- &gt; &gt;matically caught and &gt; &gt;propagated: If the caught exception is of type &gt; &gt;error &gt; &gt;, we just return it. Otherwise, it &gt; &gt;is tra &gt; &gt;nslated &gt; &gt;to an &gt; &gt;error &gt; &gt;with a meaningful value for all &gt; &gt;std:: &gt; &gt;exception types &gt; &gt;; f &gt; &gt;or example, &gt; &gt;bad\_alloc &gt; &gt;would be translated to &gt; &gt;std::errc::ENOMEM &gt; &gt;. &gt; &gt;Otherwise, w &gt; &gt;e can additionally store as payload a &gt; &gt;raw &gt; &gt;pointer to an &gt; &gt;exception\_ptr &gt; &gt;to the &gt; &gt;dynamic &gt; &gt;exception \(see &gt; &gt;§ &gt; &gt;4.6.4 &gt; &gt;\) &gt; &gt;, without sacrificing trivial movabil\- &gt; &gt;ity &gt; &gt;.
I think you can add [[nodiscard]] to all your functions using libclang python bindings.
I was thinking about implementation on stack. :)
Ouu, looks nice. I'll check. ;)
while I see what you mean, `constexpr` allocator/`new` + conditional constexprness would allow to have the same implementation both on stack and heap.
Yup, but next standard is a loooong time after that ☹️
&gt; It’s immensely important to have someone like him in the standard. std::herb
Hm, if only there were some kind of repository of information that was easily accessible, through which we could learn concepts which aren't immediately obvious... Maybe even a place where people ask questions, and other people answer them, all free and publicly available...
Can't this comment tree rest in peace?
That wouldn't be `st::map` then though. I.e. not only in terms of name, but the specification/interface (though something very close is possible, as mentioned in other comments).
I guess I'm starting to see why Rust was voted to be the most loved programming language for 3 times in a row, that is indeed fascinating and technically elegant. I guess you could make the contract that the exception reference cannot be null and thus have a similar efficient memory layout but yeah I'm not sure whether that can be done and whether Herb had that in mind when he created this proposal. Thanks for your very detailed explanation though!
It's either this meeting or San Diego where the last new evolution papers can be targeted at c++20. We're still a ways off before the last evolution paper can be voted into the working paper for 20.
We could have an allocator which gives memory previously allocated on the stack no ?
Doesn't this just copy the Rust model?
&gt; Why not just completely automate it into proper sum types, that way it can be done seamlessly without extra overhead A sum type is a union + discriminant. Rust can sometimes optimize away the discriminant, but it is not possible in general. For example, if I have a function returning an int64, how do you remove the discriminant? Note that the proposal also talks about implementing the discriminant as a CPU flag. This can often be more efficient than folding the discriminant into the union. For example, on x86, if you use CF as the discriminant, you can set the discriminant in one instruction, `CLC / STC`. You also detect whether the exception was raised in one instruction, with `JC / JNC`, instead of looking for the magic number in the register (requires a comparison at least).
C++ had dynamic exception specifications. The proposed one is static. As the paper says: &gt; You’re in a statically typed programming language, and the dynamic nature of exceptions is precisely the reason they suck.
&gt;And unless I'm missing something, there is no way to use std::thread on MacOS and change that size, correct? You can probably do this via [`std::thread::native_handle`](http://en.cppreference.com/w/cpp/thread/thread/native_handle) and the according system thread functions for manipulating thread stack size. 
&gt; If I understand correctly this paper suggests a new kind of exceptions that will be complementary to the already existing "dynamic exceptions". This not a new kind of exception, this is basically syntactic sugar for `expected` / `outcome`. The error type is always 'std::error'. From page 14: &gt; If you love expected/outcome: This is embracing expected/outcome and baking them into the language, the function always returns exactly one of R or E — plus the special sauce that you get automatic propagation so you don’t have to manually return-up the results, and with a distinct language-supported error path so that callees can write throws (instead of return unexpected) and callers get to cleanly put all their error handling code in distinct catch blocks (instead of if(!e) blocks) while still writing in the same basic expected style (see §4.5).
The handle applies to a thread that's been created. By then it's too late to set the stack size. At least that's my understanding on MacOS. 
Great, thanks for going into detail for me. I appreciate it.
As usual, excellent paper from Herb. Not surprisingly, Herb seems to have taken heavy inspiration from Joe Duffy's work on Midori. Duffy's blog posts on error handing in Midori is a must read. I really, really want Section 4.3: `Proposed extension: Treat heap exhaustion specially`. However, given how controversial this will be, I am not very hopeful :(. As the paper says, this is already being used by a signification chunk of software today. Very few software can actually recover well in the face of OOM. They may not even be able to detect OOM. And yet, we are all paying the cost of bad_allocs. As the paper says: &gt; Treating heap exhaustion the same as all other errors violates the zero-overhead principle by imposing overheads on all C++ programs. As noted above, platforms and situations where heap allocation cannot be recovered from are common, but programs and programmers in that camp are paying for the current specification in performance across their programs. This is spot on. Fail-fast on OOM makes a huge chunk of my code noexcept. This improves optimization and reduces binary size.
&gt; And asking us to rewrite it to use new "try to allocate" functions is insanity: the current paradigm is for these functions to throw. If people want different behavior, give them new syntax for that purpose. I am sure there will be a compiler switch for the old behavior. Herb's point about 'Treating heap exhaustion the same as all other errors violates the zero-overhead principle by imposing overheads on all C++ programs' is very true. I much prefer to fail fast on OOM, and marking significant chunk of the standard library as noexcept will improve codegen and binary size for me. &gt; Finally, I'm not sure why this behavior must be specified on a per-allocation basis, and not, say, as a compiler switch. Where are you getting the per-allocation basis? IF this change is made, all `new` will become nothrow by default, just like a compiler switch. &gt; Bonus: why is this needed at all? Is the proposed new exception mechanism unable to deal with OOM? This is orthogonal to the main proposal, which is why it is marked as an extension. The paper goes into significant detail why this is desirable. See page 30.
&gt;On another note, I don't quite understand how you can have a thread library that does not allow you to set the stack size. C++'s abstract machine has no stack... :/
The stack bits set to indicate whether a dropped type has in fact been dropped. The fundamental problem is that in branchy code - or across certain ABI boundaries - there's no deterministic way to know if a value has been dropped or not. To avoid a double-drop in circumstances that aren't statically provable to not need it, Rust keeps some state bits around to indicate drop'ness. At least last time I checked. I'd be happy to be corrected if this is not true.
`boost::thread` seems to have the api you look for: [https://www.boost.org/doc/libs/1\_61\_0/doc/html/thread/thread\_management.html#thread.thread\_management.tutorial.attributes](https://www.boost.org/doc/libs/1_61_0/doc/html/thread/thread_management.html#thread.thread_management.tutorial.attributes)
Those bits do indeed exist, but they are not part of the ABI. They are purely local to the functions that need them. Part of why this is possible is that you can't move out of a mutable reference (or any reference, for that matter) without leaving a valid object behind in its place, effectively emulating C++'s non-destructive move semantics.
I did the math; we had more papers multiple times in the past. The highest was 172 at the last pre meeting mailing. I am working on a position document with some stats about this.
There have been proposals to add this ability to the constructor, but they hit the problem that the C++ abstract machine doesn't know what the stack is. Then there have been subsequent proposals to allow passing platform-specific parameters to the thread constructor, with the explicit intent to tweak it in a way to have a portable enough way to specify the thread stack size, but I don't know what happened to those; I haven't seen any newsletter revision for some time already.
Just like the incredibly useful “C++ io has no file descriptor”
&gt; Show everyone that it can work and that it's faster than a heap-allocated map. It should be obvious that it can work and that it is faster than a heap-allocated map typically. I think a better reason for why it might not be standardized is that the committee hopes that `constexpr` can be extended so that more things in `std::map` can be `constexpr`, and `constexpr` allocators can be created that use the stack and use `constexpr placement-new` etc. etc., and then there wouldn't need to be a second, parallel `constexpr-friendly` implementation of `std::map`.
&gt; The only failing I ran into was that I could not figure out how to view the output from the person's console application Works fine using the integrated terminal in VSCode.
If by allocator you mean `std::allocator`/`polymorphic_allocator` then even if you design such allocator it must be `constexpr` too which is currently not possible by its definition.
I agree that not being able to specify the stack size of an std::thread can really suck, and I'm by no means trying to downplay the fact that this functionality being missing can be a real, blocking issue for users of the std::thread primitive, but I think the last bit of the original post is a little silly. If there was a reasonable assumption that fulfilling a requirement for your software would necessitate threads being able to have their stack size set, then some due diligence would have shown that std::thread was not an acceptable replacement primitive to use instead of whatever was there already (instead, something like boost::thread with its boost::thread::attributes allowing stack size to be specified might have been a better fit). As a corollary, if there was no reason to assume that stack size issues with threads was something that would come up over the course of dealing with your requirements, and some new requirement was handed to you that /did/ necessitate having increased thread stack sizes, then the possibility of having to change things up behind the scenes should've be just as expected as it would be with any other requirement change. I guess what I'm trying to drive home is that changing requirements can suck, but making sure you know what your tools can and cannot do before you start using them can make it suck less. There shouldn't be any surprises when things like this happen so long as due diligence is done.
The overloads taking a bool_constant need to go in a detail namespace anyway, and once there the SFINAE is unnecessary. So this isn't terrible: namespace detail { template&lt;typename T&gt; bool starts_with(T, std::true_type); template&lt;typename T&gt; bool starts_with(T, std::false_type); } // ... return detail::starts_with(buf, std::bool_constant&lt;...&gt;{});
&gt; allocation failure doesn't mean available memory is zero, it only means that available memory is less than the requested amount. To be super specific, the requested amount can actually be readily available, just not in a single contiguous chunk. 
Yeah, this was Thomas' paper, [P0807](https://wg21.link/p0807) (not so much optional braces as totally different solution). Was discussed in Jax but room was split fairly evenly for-against (26-12-30).
I didn't get that idea. From "What we teach": &gt; **Function authors**: Prefer to write exactly one of unconditional `noexcept` or `throws` on every function. Dynamic exceptions and conditional noexcept still work as well as ever but are discouraged in new/upgraded code. 
I don't rely on bad_alloc now nor plan to. Running with no-exceptions, and thus most of standard STL isn't currently usable. Also, heap is only used in specific contexts and tasks in the whole program ( parts that deal with incoming network data and things like SSL certificates etc ) If treating OOM special and defaulting to fail-fast/terminate, that would still keep STL off limits for those scenarios.
Embedded C++ here 1) bitfields don't handle endianess changes well. Which is a real shame and why bit masks still exist. 2) It's more flexible to use inheritance/interfaces. 3) comments are super super valuable. If you initialise a timer, what's the clock speed? What's the overrun value, what's the original clock speed before your prescaler. 4) Focus on abstracting your business logic first. Your state machines etc. Use the same rtos, compiler, network stack between projects. 5) It's okay to go light on abstractions in some places especially interrupt handlers. 
Oh okay !
It says "correctly". You don't need to be correct for it to work if you get lucky, you need correct for it to always work.
Why do you think it's a good point?
In Rust: (foo()? + bar()?)? In C++: try foo() + bar() The difference is that `?` applies only to a single exception, while `try` applies to all exceptions in an expression or statement.
&gt; C++ would need to become infinitely more powerful to allow ‘variant’ to perform the optimization’s that Rust Enums can already do It's already powerful enough, as the linked library demonstrates. It would just take hours to compile. :-]
For reference, (P0484)[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0484r1.pdf] is the paper that proposed the additions. It was discussed by SG1 (in Jacksonville iirc) and is still in need of iteration. That is, it's not not shot down or anything, it just needs more work.
Well, it is entirely possible to construct a program where this sequence would be always correct, where there is always a contiguous kilobyte of heap available but a megabyte is not. In fact, test-allocating a large chunk can be used as a strategy to gracefully deal with imminent memory exhaustion.
That's very exciting to hear!
why not read the paper instead of asking someone to do the work for you?
I work in game development. I was doing a cooperative multitasking system similar to boost's stateful coroutines. To handle situations where a task with live objects was interrupted, I would throw an exception. On Xbox One, throwing an exception while the debugger was attached would freeze the system for a full 3 seconds. I reported the problem to Microsoft, with simple sample code, but they never bothered to solve it. We wound up having to tear down live objects manually (which is awful) to work around the problem. Technically we do still have exception handling enabled, but it's unacceptable to throw them in user code.
Not entirely, at least for simpler cases. C++ would need a `std::is_never_zero_v&lt;T&gt;` or the like and a lot of the simpler cases of this kind of compressed discriminant are possible. And that type trait is useful for all kinds of other reasons (e.g., container initialization optimization). Said type trait can be written today, albeit having to manually overload the trait for some types. Getting the full variant "niche-filling" optimization certainly requires more work out of the language. Though I do rather wonder whether that actually matters. (How much of Rust's enum layout optimization is just golf instead of useful work?)
If I remember correctly, there are some proposals to allow a similar `native_handle` type for file related things as there is currently for threads.
That would be great!
It looks like something compiler vendors could implement in months rather than years. I think it will be available in some compilers by next year. For those [working on cloud services](https://github.com/Ebenezer-group/onwards) it could be adopted pretty easily.
It seemed I was the user was attaching a debugger via VSCode to the app which is why the terminal output was not connected, oops.
There's also `__func__` in C++11 or later.
&gt; I don't rely on bad_alloc now nor plan to. Running with no-exceptions, and thus most of standard STL isn't currently usable So what part of the proposal makes you nervous? It implies no changes at all for you. :) &gt; If treating OOM special and defaulting to fail-fast/terminate, that would still keep STL off limits for those scenarios. "Defaulting" is the important part. Via allocation-failure hooks and via optional OOM-aware interfaces, you'd potentially be just fine with the proposed hypothetical standard library. I'm honestly interested to know what you'd rather have instead. The combination of fail-fast and separate OOM-aware interfaces covers every single use case I've ever had or can think of having, so I'd love to learn what else is needed! &gt; saying testing is difficult regarding heap exhaustion By that he means that getting 100% code coverage in OOM tests is effectively impossible for many target C++ users. Anecdotally, we have a codebase that allocates a few million times before it even shows the first window to the user. No "tweakable dlmalloc" is going to effectively test all OOM paths in that allocation pattern unless we only want to test one code commit per week. :p
That doesn't provide you with as much information. Which overload or template instantiation for example. 
Can't we make an exception?
&gt; This is especially important since I'm not at all convinced that the people who hate exceptions today will stop hating them if this proposal becomes reality. They will still have old code bases they cannot easily update, they will still have ancient coding standards lying around, they will still dislike the distance between throw and catch, and they will still have a gut feeling that exceptions are automatically, always bad. So you might be introducing a lot of deeply breaking changes here in order to appease a crowd that is unlikely to appreciate it, while at the same time chasing away your core audience. I'm one of those people that hate exceptions. There are two primary arguments that I know of against exceptions: 1) Speed 2) The function signature is not explicit about the return path of code. imho most of the implicit parts of C++ are the worst parts of the language. If there is a compiler flag that enforces the throws keyword in a function signature, I will use exceptions. You can say some people who have been avoiding exceptions for years will not get with it and continue to do so, and that is a possibility but the second they are working with a team of people and there is a style guide or a lead or something enforcing correct practices, they will get with it too. The only people left behind is legacy code bases with teams that are constantly behind, but that happens with all new C++, not just exceptions. Is this proposal perfect? No, but it's a lot better than what we currently have. Exception handling is one of those points in C++ that could use an improvement. Whenever a community is divided you know it isn't as good as it could be.
It depends on what you’d like to learn / focus on in C++. Are you interested in object-oriented programming? If you’re just starting out, learning about algorithms, in say the CLRS textbook will be beneficial. It is of no use to spend time learning the language if you omit learning about the different algorithms out there and how they can used to solve different problems. After doing this, you can learn and implement some interesting design patterns, but understand these can usually be implemented in a number of languages. If you have done this, the next step maybe to leverage the powers of the lower level language, and learning about how you can interact more directly with the hardware compared to languages such as java. You could also look into meta programming with templates. Understand that there are full textbooks on all of the focus points I’ve noticed. Consequently, I think the better question maybe how you can become an experienced programmer before you get into all of the nuances of a particular language. The best path to learning a programming language is similar to learning a natural one... spend time writing code; I normally recommend that my students work on some pet project to encourage this. 
Thank you for taking the time to respond to my question. I appreciate that you took time to help me. It means a lot to me. To answer your question, I'm not drawn to any particular aspect of C++. I just realized recently that I get a euphoric sensation from typing things into a machine and seeing the expected response. In regards to CLRS - is that necessary? I'm extremely bad at math in almost all of its shapes and forms, and at the same time very interested in programming. Can I ignore the former in favor of the latter? Or will have to learn the algorithmic fundamentals? In regards to the pet projects that you mentioned, can you give me any examples of a pet project that would be fitting of someone who is interested in getting started learning C++? Thanks again for your comment.
&gt; So what part of the proposal makes you nervous? It implies no changes at all for you. :) It leaves most everything in status quo, because STL would still be largely unusable, and would encourage more libraries to be designed the same way. &gt; "Defaulting" is the important part. Via allocation-failure hooks and via optional OOM-aware interfaces I'd like to see what that OOM-aware interface proposal would look like for things like std::function for instance. 
As much as people dislike macros they have been essential to C++ programming ever since C++ came around. They should be treated as what they are: first class citizens. Instead of banning them from modules we should thrive to make them easier to use and less error prone. This is basically what Rust does and it looks like it’s working out for them. Are there any real world applications that do not at least somewhere use macros? And may it be the libraries they depend on. How do you do effective logging, unit testing, reflection, serialization, etc. without the use of macros? Now I have no idea what macro support does to modules but I think macros will be around for a long time unless C++ changes drastically. 
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8j1eg0/experienced_c_programmers_what_was_your_path/dyw8qmd/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Sorry, I didn't know that my queston was off-topic.
&gt;They should be treated as what they are: first class citizens. Instead of banning them from modules we should thrive to make them easier to use and less error prone. There are countless efforts and papers to make macros unnecessary or less used. Proposals to add macros into modules not only encourage the use of macros, it *also* complicates the build toolchain and impacts performance of build times for most users because some "big users" do not want to refactor a single line of code in their codebases. I find it really, really unfair. &gt; Are there any real world applications that do not at least somewhere use macros? I have been writing code myself making use of no macros except for #ifndef and and #include, and yes, it is production code. So here you have an example. Effective logging, reflection, serialization are things that are being worked in many, many other proposals with things such as lazy parameters, reflection for C++, metaclasses, code injection... I *honestly* think that macro propagation from modules should be *banned*. If you want to use macros, #include files. This does not preclude your use cases, but it does not make other code bases compilation serial and complicated. I still stand by my words: making macros into module imports is a really bad idea, at least it looks like that for now. And it is done just because some users want them for their own projects: just provide an extension if you need it, do not make it standard for making it painful for the rest of us.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/8j1lky/help_with_calculator/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
As has been said here already, the issue is that the concept of a stack is itself not entirely cross-platform. 
LOL. Why do you think that? This proposal hasn't been reviewed by the committee at all. Has any compiler vendor said they would implement this? Just because they could (we don't know yet) doesn't mean they will.
Macros aren't first class citizens. They're pre-citizens. They cannot be referred to by actual code, only by other macros. They don't obey namespacing rules. There are better solutions to what macros are used for. &gt; How do you do effective logging, unit testing, reflection, serialization, etc. without the use of macros? If all you have is macros, then yes, macros will be used for those things. But if you have, say, reflection as a first class citizen, then you can do most of those things WITHOUT macros. You are thinking about this the wrong way.
This is just an abstract and problem background for a demonstration I will be making. Sorry, should have made this clearer I guess.
I think you should really read the paper to appreciate what dragging modules into the preprocessor will mean (import becoming a preprocessor directive, and so on). Also note that having modules a purely language-level mechanism doesn't mean we won't be able to use or even "export" (via `#include`) macros. Nobody argues macros are useful in many contexts and will be used for many years to come.
About effin time. Too bad it's going to take years before it can be relied upon. Add 2-3 years after the relevant standard is published.
I've never understood you masochists... 
That's not the best use of C++. You can do it but it was never intended for GUI development. Use .Net or C# Like the comment said, using C++ for GUI development is masochistic.
Look at QT, you can use it from VS 
Agreed. Much as I love C++, this would be far easier in C# with an (old) winforms GUI
This is like not being able to use goto in constexpr functions: feature racism.
&gt;Where are you getting the per-allocation basis? The paper suggests a new set of functions that would throw instead of terminating: try_new, try_reserve, etc. So he's basically adding new functions and keywords that have the current behaviour, and making the existing ones do something else, allowing programs to choose one behaviour or the other on a per-case basis. That's something I really don't get: why break compatibility, why not simply add new functions and keywords for the new behaviour? Just to be clear: I understand the proposal to be adding a new type of exception handling, next to the existing one, and not to be removing the existing one altogether. Correct me if I'm wrong. &gt;The paper goes into significant detail why this is desirable, refer page 30. I've seen that page, and I only see reasons why we should keep things as they are. It is _already_ possible to install a new-handler that will simply terminate, so that behaviour is already available to people who want it (whereas the alternative, installing a new-handler that throws, is explicitly forbidden by the proposal). And there is already a compiler switch that eliminates exception handling altogether. So in other words, the behaviour those people want is already there, and it doesn't require breaking the programs of people that do not want it. Going through his arguments: &gt;Testing is much more difficult. What about the people that do want to make the effort? Should they just move to a language that doesn't preemptively kill their programs? &gt;Recovery requires special care Not really. We accidentally did it, just by being exception-safe in the first place. Note that I'm not claiming it will succesfully recover from _all_ OOM, but the most likely case for us is a large allocation when loading data. And yes, there is the new try_reserve, try_pushback, try_new, try_whatever (try_map::operator[]? probably not...) - so should we now go through our software, and for each allocation decide if it is likely to be handled correctly if it fails, and if it is, replace the allocation by a try_ version? That's an utterly pointless activity: we will mark _all of them_ as "go ahead and try", because even if we are wrong we will be no worse off than by calling a terminating function, since the end result will still be termination. &gt;In many programs, heap allocation failure can never happen as required by the standard And in many other programs, it can. Here's another question: if the problem with existing exceptions is that RTTI is so expensive, why not fix that, at least for the existing exception mechanism? One could imagine annotating exception classes with some mechanism for allowing fast identification. This would place some restrictions on what can be thrown (only classes that have the special annotation), but if this were built into std::exception a large amount of software would never know the difference. This would also be a breaking change, but at least it would break far less than what the proposal is asking to break... 
Fine, let's have a proposal for proper macros then. Something that works on a slightly more advanced level than text substitution, something that respects namespace boundaries. Macros were certainly useful back in the day, but not having a mechanism to limit their scope somehow was a major mistake. Now we have another chance, let's not make the same mistake again please? As for needing them: sure, we need them. And they will be available, thanks to the magic of the #include statement, which will not in fact be taken away. 
Well, I do have problems with bugs in the msvc compiler and if you look at connect, lots of other people have too. That being said, I'm pretty sure, that feature was not developed by the compiler devs, so this isn't time taken away from improving the compiler.
Hello /u/Lemonpyjama 1) I really miss a range-based for construction for POD-structs. Last month I had a chat with David Vandevoorde about it at the conference, he's working on a reflection metaprogramming proposal about it, but the best scenario for it is C++23. 2) I personally prefer composition over inheritance, but it's a matter of taste. 
Nonsense. For one thing, the standard mentions 'stack unwinding' numerous times, so clearly there is something there called a 'stack'. 
Can you give me an example of a machine which is capable of running multiple threads and lacks a stack? I am fascinated by unconventional computing architectures and that would be a delight for me to learn.
What's being asked for is to be able to pass one variable of type `size_t` in order to trigger one specific, common function of native threading implementations. Why could it possibly need more work? 
Neither is a filesystem, yet somehow there is a filesystem API now. Presumably a stack could work the same way: there is a function that can be called to set the stack size, and platforms that store their automatic variables in the cloud instead of in local memory just ignore it completely. 
There is no black/white answer here. The only solution that makes everyone happy would be to make it a user choice if their module should export macros. And as much as I support kicking out macro support, just migrating a code base to modules *with* macro support is already a pain. There is just so many broken C++ code out there that needs fixing, that forcing people to also rethink their macro code will really hinder the adoption of C++ modules. Just look at boost. Boost as a whole is not valid C++ code. It has funny things like defining `timer` both as namespace and class, main methods in headers and there are essentially two parts or boost that can never exist in the same translation unit because they define the same template differently. And that doesn't even mention all the missing includes and typos! If we now completely disable macro support, we pretty much have to rewrite all the weird parts of boost. The same goes with most other big C++ projects. And I'll personally buy a life-time support of &lt;insert preferred beverage&gt; for whoever will fix all those projects. But so far I'm already struggling with convincing people that they need to fix obvious typos and missing includes in their code. Disabling macro exports doesn't mean that suddenly everyone finds the motivation to clean up their code. It means that many people just don't adopt modules at all. Removing them *will* hinder adoption, which will also cause that many of these small bugs won't get fixed in the near future. And this also means all those libraries will still ramp up our own project compilation times. Not having C++ modules for as much code as possible is just not a good situation. But if we say that exporting macros is a user-choice, than whoever has the time and knowledge to get rid of them can refactor, disable the feature and be happy. And whoever doesn't have the resources for that at least doesn't block the progress for the rest of us.
Macros in rust are absolutely not like macros in C++, it's the same idea than templates
&gt;The function signature is not explicit about the return path of code The return path is still not explicit; the catch handler may be at any point in the call stack, there is no clear indication on the function itself where it will go next. &gt;The only people left behind is legacy code bases with teams that are constantly behind, but that happens with all new C++, not just this. Exceptions are different. You can start using any C++17 feature on a piece-by-piece basis: a variant here, an optional there... Exceptions require far, far more care: you must have try/catch blocks in all the right places before you can even think about throwing anything anywhere. This is especially problematic because exceptions tend to be generated at the lowest levels of the software, i.e. in code that is likely being called from numerous places, thus requiring try/catch blocks in all those places before exceptions can be introduced. If you have any amount of code, that's a huge barrier to entry. 
Naming a thread is also missing - very useful for debugging... It exists in pthreads and Windows.
&gt; just refactor them or use #include!!! That's an insufficient answer for the real world if modules are ever going to really be deployed beyond green field projects.
Going out of stack size is undefined behaviour, so from the point of view of c++ there is always enough stack size
&gt;The return path is still not explicit; the catch handler may be at any point in the call stack, there is no clear indication on the function itself where it will go next. This proposal puts a `throws` keyword in the function signature so you can see which functions throw and which ones do not. `noexcept` sucks at this, but `throws` sounds like it could be ideal as long as the compiler supports enforcing this, like `-Werror=suggest-override` does.
Simply put, macros do not need to be a language feature. If you want to preprocess your source files you don't need a standard. Just fopen, fmap, and start poking. Then give that to the compiler and let it parse according to the standard. 
&gt;The return path is still not explicit; the catch handler may be at any point in the call stack, there is no clear indication on the function itself where it will go next. This proposal puts a `throws` keyword in the function signature so you can see which functions throw and which ones do not. `noexcept` sucks at this, but `throws` sounds like it could be ideal as long as the compiler supports enforcing this, like `-Werror=suggest-override` does. This is implicit, so I hope such a flag would default to this behavior, but if a function calls another function that can throw, and there is no setup to catch that throw, then now your function needs the `throws` keyword in it too. This is explicit and eliminates this problem. As far as updating the code base being no small feat, the alternative is an expected return type or an error code. Both already exist in a code base (if throws are not being used) and both will continue to be valid, so no, there isn't a large change necessary to the code base.
This "beyond green field" projects use #includes now, so some refactoring will be needed anyway.
&gt;And whoever doesn't have the resources for that at least doesn't block the progress for the rest of us. I think part of the OP's point is that, in a way, these legacy projects *are* blocking progress for the rest of the community by requiring that modules include the extra complexity of macro exporting, thus delaying the delivery of the modules feature.
Speed is a valid argument, regardless if the premise is incorrect.
&gt;So what part of the proposal makes you nervous? It implies no changes at all for you. :) That's not how I read it. The behaviour he is relying on is that new returns nullptr when OOM, but in the proposal, new will either return a pointer to memory or terminate. Actually he is precisely the kind of person the proposal is trying to reach: people who don't want to use exceptions for whatever reason. And he, at least, doesn't seem happy with it... &gt;I'm honestly interested to know what you'd rather have instead. The combination of fail-fast and separate OOM-aware interfaces covers every single use case I've ever had or can think of having, so I'd love to learn what else is needed! How about the interface we have today, the one we spent the last 20 years writing and testing software against? Where new can throw, rather than just terminating? It already has fail-fast for those who want it, in the form of new-handlers. And it already has OOM-aware interfaces. 
The downvotes are because there are plenty of great options for doing MMI work in C++. Qt, for example.
I remember an earlier presentation on modules where Doug Gregor started out with a joke where the punchline was "and then all you have to do is change all of your source code to use `import` instead of `#include`. It was a joke because from his perspective that was obviously not an acceptable solution and would fail to meet the goals for modules. It's true that moving to any modules system would probably require some refactoring. But as we've seen there are ways that it can limited to the degree that it's actually feasible. For example the legacy mode described in P0273 allows well behaved code to be designated as a module so the any #includes of that code's headers could be treated as modules without actually having to rewrite the code to replace #include with import. 
Thanks, I see. Not sure about other compilers, but I think in clang the bigger implementation challenge is coming from libraries that are not in modules, because they are copied into every module which means that clang has to handle all the duplicated declarations (i.e. some decl duplicates are invalid and need to be rejected, some are valid and need to be merged into one). Obviously supporting as much of C++ with modules helps here as we can put more headers into their own modules. But I also feel that the standard should not develop what's the easiest to implement in the compiler but what's easier for the user. And the 'simple C++ compiler' ship has anyway already sailed.
&gt; I am sure there will be a compiler switch for the old behavior. As Herb wrote, the major point of this paper is to get rid of the necessity of -fno-exceptions compiler switch to defragment C++ community on error handling.
So provide an isolated solution, but not one that just makes everyone else pay in compile times the bill of those users.
Hmm, I think you have to give me an example for an "isolated solution", otherwise I don't see what you mean.
Functions that aren't marked can still throw either type of exception and the compiler can't know in general that it won't. I don't see what's complicated about adding a plain `noexcept` to a function if it shouldn't throw anything, though.
what
#THIS IS LIKE NOT BEING ABLE TO USE GOTO IN CONSTEXPR FUNCTIONS: FEATURE RACISM.
There is no reason, why you would have to stop using headers. If you have an old code base, just continue to use them. I don't see a reason, why an ancient code base would suddenly switch to modules. Old code bases can also use a gradual transition. Refactor code, that can be moved to modules, to use them. If it can't, stay with headers. If you have functionality, that requires macros, you can even define a header defining the macros and importing a module for the remaining functionality, to get most of the benefits of modules. I want a way to confine macros. They can't be put in namespaces, so they are global names and I want to limit that. Modules currently provide that. Macros are basically text replacement. Headers are text replacement/insertion. They match pretty well. Modules are intended to be something different. I don't think macros and modules match well.
Because the paper is 58 pages and I didn't have the time? Also I have read some pages but English is also not my first language so I'm sure I will misunderstand some of it anyway. Since probably there are people here that already have read it / understand it, I thought it would be not rude to ask as these deterministic exceptions would work in general \(I am also not expecting a full abstract, as I asked some specific questions about it\).
Noone is saying that you cannot have some way outside of the standard or find alternative ways with #include. Do not put all the burden on us normal users.
I don't see the need to export macros between modules. Just put the macros inside a header and include it in each modules. Boom. Done. If modules keep it promise to speed up compilation time, it will massively adopted whatever refactor it required. 
Some C++ features like macros and goto are left out of modern C++ because key committee members dislike them. For example, all types of C++ control flow (including do-while) can be used in constexpr functions, but goto cannot because some believe that “goto should never be used for anything”. Another examples is modules, where everything can be exported, except macros, because some believe that “nobody should use macros in modern C++”.
Opt-in macros, as a minimum. If they throw them away directly and #includes (which are very visible) must be used, I will not cry. Modules should not be affected by macros. Do not make build time problems and other traditional hijacking problems from macros for the rest of us, please. Both are really bad.
&gt;the standard should not develop what's the easiest to implement If the tool vendors have no capacity to handle the resulting complexity, it doesn't matter how simple the feature to use -- you just end up with another template `export`, something that nobody can implement and therefore use.
&gt; Another examples is modules, where everything can be exported, except macros, because some believe that “nobody should use macros in modern C++”. Bullshit. The reason that the community is considering not being able to export macros is that there are serious difficulties with this problem that no one has yet figured out a good way around - as the paper linked by OP (you did read that, I hope?) that shows that modules that export macros seem to make parallel compilation impossible. Considering that almost all C++ developers believe that macros are dangerous as a source of bugs _due to the community's almost fifty years of experience with them_ and you can understand why there's a lot of sentiment for charging ahead with modules without macros. Remember - _you could still use your stupid macros_ from your 1980s codebase! The `#include` statement would still work - existing code would still compile - macros would just not be exported from modules _just like today_ where macros are not exported from a compilation unit, but are copied anew into each new compilation unit.
Because then you might as well define a new language. Perhaps you could call it D, or Rust.
&gt;And as much as I support kicking out macro support, just migrating a code base to modules with macro support is already a pain. If you're already having to refactor to support modules, would it be that much more trouble to factor out macros into old school includes?
The keyword doesn't work as you describe. Leaving out the OOM _extension_, programs continue to work as they do now. You opt in to the new exceptions with `throws`. The OOM extension is the only breaking part. &gt; It is already possible to install a new-handler that will simply terminate, so that behaviour is already available to people who want it I think the paper addresses it pretty well: &gt; That hook is at the cost of some overhead on all functions for which heap exhaustion is the only reportable error: not just the cost for the hook check itself in that function (which is expected to be cheap), but rather that it prevents the compiler from removing the error propagation logic entirely which we could do if the function were noexcept. Also, if we add the hook described there to the language primarily to use it in this way to treat heap exhaustion specially, then having that hook will incur small but nonzero overheads on all code that can throw. For a proposal mainly focused on bringing back not paying for what we don't use, this is consistent. There is a lot of software out there paying for `bad_alloc` when it would bubble up and crash the process anyway. I think there should be an option for projects that handle it correctly and use regular `new` as opposed to an allocator or some other type of localized strategy, but at the cost of everyone else is not ideal. &gt; What about the people that do want to make the effort? Should they just move to a language that doesn't preemptively kill their programs? Just a note that this becomes a non-issue when the program is changed to work as it does now. I take it you're more concerned about the program terminating than about the non-death tests being unable to detect it. &gt; Here's another question: if the problem with existing exceptions is that RTTI is so expensive, why not fix that, at least for the existing exception mechanism? … This would also be a breaking change, but at least it would break far less than what the proposal is asking to break... First, it makes the core proposal this breaking instead of a proposed extension. Second, I'm skeptical that this is breaking far less. Third, RTTI is just one of several discussed overheads. Also see page 42, which addresses improving the current system.
&gt; I don't see the need to export macros between modules. Have you read the committee papers discussing the issue? That's where I would start in order to try to understand the motivation.
&gt; If there is a compiler flag that enforces the throws keyword in a function signature, I will throw exceptions instead of returning an expected type. I think a libclang tool for this would be a viable option. It should be a fairly straightforward AST matcher.
&gt; I don't see a reason, why an ancient code base would suddenly switch to modules. Because if you don't put them into modules, they get copied into every module that textually includes them. Compile 5 C++ modules that heavily use boost and you have one gigabyte of module files. And we have to ODR check all those boost copies for integrity when we load the modules. Repeat this for every non-module library you use.
This is to cater to people who would like it clear which parts of the function can throw. If the codebase is consistent with `try` being there for places that can throw and nothing being there for places that can't, then you know when reading that you have to think more carefully about the `try` places, but don't have to take exceptions into account for the other places.
I mention about that in my post. I still think that there are things that do not belong in the standard. Their work is great, they do it, I just complain. That is a fair point. But still, the committee should serve the community and that does not preclude them from finding alternative solutions for their uses. Not everything should belong in the standard. I think the standard should accomodate common uses as a priority, not specific use cases from a handful of companies and on the way just make for a far worse experience in this case. I do not mean they do not have their point, but please, if this happens, make it opt-in in some way because this can make it for a half-assed improvement in compile-times.
Look into Qt (GUI framework) with QtCreator (IDE).
The standard defines stack unwinding: &gt; As control passes from the point where an exception is thrown to a handler, destructors are invoked by a process, specified in this subclause, called *stack unwinding*. It doesn't have to define stack to do this.
Setting the size of the stack is a very useful feature. If an application wants to create a large number of threads, it might benefit from allocating a smaller amount of memory per stack if it knows that the path of execution does not require the larger amount.
It's not that it's unable to deal with OOM, considering that it deals with it better than dynamic exceptions (which often do heap allocation). But the difference lies in having a huge amount of functions marked as 'throws', or having only a small amount be marked as such. Most people won't be recovering from OOM anyway and won't expect it to happen in their programs at all. Having 'try' and 'throws' everywhere just because of dynamic allocations that you don't even care about feels pretty cluttered. Of course, those who do want to handle it should be able to, but I'm still not sure what would be the optimal solution. I can't say that I'm strongly in favour or against it, but it does feel justificated to at least consider treating OOM differently from other exceptions.
What is the “right” way to do what the example in the paper is doing in your eyes? i.e. If building on windows import module A, Unix import module B? Is the example flawed somehow?
http://wg21.link/P1031 *Low level file i/o*
If it was a straight-forward feature with minimal impact then sure. But as it is, it changes a very fundamental part of C++ in a new way, and not only that it also attempts to make it part of the C FFI. I don't see how something like this can be accepted into standard in 1-2 meetings into almost complete standard. It simply needs more discussion. Note, that if the feature moves to the next C++ standard it doesn't mean it'll be implemented 3+ years later. A lot of features are implemented as they get accepted into the standard (though that depends on their complexity).
The post raises a very valid point. But that last statement in your post is really not necessary. I doubt anyone involved in the standardization committee has malicious intent.
Clang modules have existed for almost a decade now, and they have supported macros from the get go. Been using them for over 5 years already without issues. &gt; since most modern organizations strongly deprecate C++ macros. If you ban macros you can’t use many parts of the std library, hell you can’t even use ‘assert’ which all organizations I know of encourage using, and you probably won’t be able to support different platforms with different compilers either. So I call bullshit. Name one single organization that bans macros.
This is off-topic here :-) This subreddit is about the C++ language itself. Anyway, Qt is a very popular C++ framework for application development. And it's cross-platform too, which means porting the application to another operating system later on if needed is much easier.
I was making a bad joke. ;)
It's not so much it lacks a stack, but that the stack could be unconventional. For instance, ~~coroutines~~ resumable functions are "stackless", and that works because they still work with the abstract machine. Similarly, stackful coroutines like we see in Boost benefit from compiler support for split stacks, which would not be possible if the abstract machine overspecified what a stack must look like (ie, stack register, downward growing, contiguous etc etc).
OK, will do that when 15.7 comes online in AppVeyor (since CTAD also doesn't work yet in 15.6). For now, I wrote my own `make_array` (from [cpp.reference.com](http://en.cppreference.com/w/cpp/experimental/make_array))
Sorry, didn't *catch* it ;P
You are going to take much more time and effort for nothing, C# will make a GUI (only will run on windows) efortless with Visual Studio But there are other options, Sciter is a HTML+CSS gui designer for C++, QT, GTK, wxwidgets and at Github there are much more projects to build Guis Embarcadero has a GUI maker as Visual Studio with C# but the ide is not free, look at it because it will make your life much more better
Depending on the definition of 'large' this really indicates a design problem. 
Having read some of the papers being submitted to the committee recently, I seem to recall 'what outstanding things do macros do that we can't solve in the language?' and 'the compiler and human see two different things'. That's two reasons why macros should be excluded from modules. Modules aren't C compatible, so why take C baggage? It's an opportunity. Leaving them out now, is keeping the genie in the bottle, and it's easier to add them into modules later if there really is a pressing need and even if there is, we can still use them right now with current non-module builds. My experience of other developers' use of macros boils down to 'how can we abuse the preprocessor since it's there'. Code-gen is a classic case (especially since the preprocessor can be doing exactly that) - macros used to make 'smart enums', reflection, and message classes for messaging libraries. Compare that to, say Java, that can easily use an xml format for the message, and then a code-gen step in the build to produce the jar. In C++ there's never been anything to stop us from doing the same - make does pretty much anything, so leaning on make (or your preferred build tool) to do the code-gen up-front rather than the preprocessor removes one (ab-)use case of macros. And parsing pure C++ is easier for tooling, and the authors of Visual Studio, Resharper, and Visual Assist will all thank you. Another 'abuse' of the preprocessor is often laziness or lack of insight - what could be done in one well isolated header or library leaks everywhere due to macros. Developers rarely apply DRY and SRP to macros and macro usage. I'd like to see modules exclude macros altogether. 
You could use std::embed with code injection and metaclasses and have something similar at constexpr time I guess. Though I would not recommend that as a general way of writing code. Can get very magic quickly.
It's better to have a module system that disallows macros than a module system that allows macros but doesn't exist. If people aren't prepared to put the effort in to clean up their code to make it module\-compatible, then their heart obviously isn't into it sufficiently. I'm not judging, I'm just saying. I think a relevant question is: could you rewrite the standard libraries in such a way that they could be written as modules, and don't need macros? If the answer to that question is "yes", then I would say that there is sufficient grounds to introduce modules. If the answer is "no", then, well, hmmm. As much as I like the idea of modules, I think there may be a fatal flaw macro\-wise. It's this: suppose your code has optional features: ./configure \-\-with\-foo That usually means, ultimately, that the compiler #defines something to make the code compile in the right way. But then, once you've used #define, you've used the preprocessor, which is what we were trying to avoid.
Or you could use something like config.hpp.in (this is what I currently do) and generate constexpr vars. Those can be included from modules. Now just use that and done hehe. You just do not need macros for config. The only thing I find limited is that constexpr if just supports templates. I think it should be extended or a la version(VersionName) should be added as in D. But there must be a way for conditional code compilation without templates.
I'm on a memory constrained platform. Lots of our threads are set to a 4k page size (the minimum supported on the platform). Other threads call functions in middleware and require 64k stacks to run - more in debug configurations. The size difference is a big deal from a memory budget standpoint.
But you don't have reflections now, do you?
Cool. I'm really talking about programs like Apache that use a thread-per-connection model. A busy webserver will typically use hundreds of threads. It just doesn't scale, hence it's a design problem.
You don't deserve the downvotes... 
I don't see what part of the article deals with functional programming or embedded systems.
This case sounds like the same idea that the GSL was designed to work upon... that the rules aren't baked into language or enforced by compilers, but that our tooling systems should support us in our application of the rules. If the common case is noexcept, then it should be unwritten unless you want to be explicit for some reason. 
YEah, but it was done right only in Windows Creator Update (SetThreadDescription function), way after C++ threading is standardized
pass
Building on a API that was using std::thread, I was forced to just increase the stack size for the entire process (setrlimit()) to be able to get the stack size I needed. The alternative would have been to reimplement the API to use pthreads. 
I imagine the only way it could do it would be... to recurse back through the evaluation of the function after evaluating it. That is, it would need to evaluate the function the first time to see if the context *could* be constexpr, and then *re*-evaluate it a second time, if it could be, to see if the constant path could be used. The spec certainly doesn't say to do that.
That's not the comparison being made here, though. The paper talks about a large portion of the standard libraries that could go from throwing *only* `bad_alloc` to being `noexcept`. It's fairly indisputable that exceptions are overall slower than "no error handling mechanism at all." :)
And a paranoid heap manager can test write/read into the allocated region before returning it to the caller. If you have heap guards or security checks turned on, it likely does anyway.
Could have said nothing
This will be deleted shortly. try here: http://reddit.com/r/cpp_questions
&gt; How much of Rust's enum layout optimization is just golf instead of useful work? I suspect it's lot of "4 bytes here, 8 bytes there" that adds up to some sizeable memory savings and cache improvements, combined with some cases (e.g. in Servo's DOM) where it enables people to remove some indirections. Like the sibling comment mentioned, it works well with automatic field reordering, which is also "3 bytes here, 7 bytes there" of removing padding. IIRC this field reordering also applies to the discriminants themselves, so they can often be stashed in any remaining trailing padding. Then there's also the fact that Rust has zero-sized and uninhabited types, which allow for more styles to be zero-overhead. For example, you can pass around zero-sized non-copyable and privately-constructible tag types for things like session types, and you can instantiate generic code to use types like `Result&lt;T, !&gt;` and all its branches will be optimized out because the layout optimization removed the discriminant entirely.
If we're introducting new type of exception handling, maybe it's possible to do something like `throws (auto)` that automagically detects if any function used inside is `throws (false)` and marks function as `throws(false)`? It solves a lot of problems described in paper - e.g `std::transform` and even helps with no-throwing `new` - just mark your `operator new` as `throws (false)` and enjoy smaller binaries or mark it as `throws (true)` and catch OOM errors.
All of this is focusing on micro-optimizing byte-by-byte processing. But the golden rule for processing strings quicly is *never go byte-by-byte*! Your CPU has registers larger than a byte, so use them! Vector (SIMD) registers are particularly well suited to string processing, and yet for some reason, few people seem to use them for that. That is the only way to get 10x faster, rather than just 1.22x. As an example, this would be a much better way to write the string parsing parts: #include &lt;emmintrin.h&gt; const auto singleQuote = _mm_set1_epi8('\''); const auto doubleQuote = _mm_set1_epi8('"'); const auto slash = _mm_set1_epi8('\\'); size_t handleString(char quote, const char* start, size_t bufferLen, char* outBuf) { const auto search = _mm_or_si128(slash, quote == '\'' ? singleQuote : doubleQuote); const auto end = bufferLen &gt;= 16 ? start + bufferLen : start; auto pos = start; auto outPos = outBuf; while (end - pos &gt;= 16) { // load 16 bytes into an SSE register. auto vec = _mm_loadu_si128(reinterpret_cast&lt;const __m128i*&gt;(pos)); // preemptively copy the vector to the output area. _mm_storeu_si128(reinterpret_cast&lt;__m128i*&gt;(outPos), vec); // set the bytes to 0xff or 0x00 based on if they equal a search char. auto matches = _mm_cmpeq_epi8(vec, search); // extract those bytes to a 16-bit bitmask in a GPR. auto bits = _mm_movemask_epi8(matches); if (bits == 0) { // can use _mm_test_all_zeros(matches, matches) with SSE4.1 // No special chars found. We already copied to output, so move on. pos += 16; outPos += 16; continue; } // Get the number of normal bytes. Use _BitScanForward on windows. auto plainBytes = __builtin_ctz(bits); // Advance past the plain bytes in both input and output. pos += plainBytes; outPos += plainBytes; if (*pos == quote) break; // handle slash. } // Handle tail of &lt;16 bytes. Or if you want to get fancy, over allocate // your buffers by 15 bytes, pass the string length and buffer length // separately, and you won't need *any* byte-by-byte processing. return outPos - outBuf; } https://godbolt.org/g/6UYrxU While this is platform-specific code, it is supported by all x86_64 cpus because SSE2 is part of the base specification now. SSE4.2 adds the borderline magical pcmpXstrX family of instructions specifically targeted at string processing, but in my testing they are better reserved for more complex tasks than this. Also there are direct equivalents of all of those operations on [power8](https://github.com/mongodb/mongo/blob/master/src/mongo/db/fts/unicode/byte_vector_altivec.h) and [arm64](https://github.com/mongodb/mongo/blob/master/src/mongo/db/fts/unicode/byte_vector_neon.h) (that one was contributed by a [third-party](https://github.com/mongodb/mongo/pull/1222), thanks!) If you put the platform-specific bits behind a type like `ByteVector` you can write your [algorithms](https://github.com/mongodb/mongo/blob/2edb91e2a51f0bdcf89f51b1a3281443c13e52c5/src/mongo/db/fts/unicode/string.cpp#L170-L210) in a platform-independent way.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8j1ryk/c_inventory_system_with_gui/dywxkta/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
No problem! r/cpp_questions is the right place.
Map and unsorted_map has much better search complexity. 
Obviously you'd use binary search on the array, and the cache coherency is much better. So probably not.
constexpr if
Sire, for some definition of constrained and lots of threads. In our case, it was ~50 threads when closer to 12 would've been ideal. We had to compromise as we needed to learn what could be made concurrent in a multimillion line program that evolved over 20 years. Step one was learning what was possible to run concurrently, what the teams could manage in complexity and then refactoring to enable concurrent execution. Better thread/core management comes next.
I agree with comments here like: "Is this proposal perfect? No, but it's a lot better than what we currently have." The proposal is sound. The committee just has to iron out some details. Compiler vendors can implement the core ideas and make changes to their implentations as those details get ironed out. 
Just like co-routines or modules, the spec can be proven in a shipping compiler today to help work out the kinks in the spec. So we might see it sooner than 2020 anyway.
Wow. Another interesting data point would be what the topic distribution is on. Like, even language vs library.
I gotta admit It's pretty well made lol.
Fine, but that's off-topic right? The point of C++ is to have low level control -- is X on the stack, on the heap, in a `thread_local` global variable, etc. If you can't control the stack size, then you end up paying for something that you don't use. So, &gt; Setting the size of the stack is a very useful feature. Right?
I was slightly expecting a scheme interpreter implemented as a template metaprogram ;)
It would be useful for some people, sure. Not as useful as being able to easily set a timeout on socket ops, but the current networking proposal doesn't provide for that either. Hey ho, it is what it is.
FWIW the data above is not correct, I mentioned to Bryce that he did not count the way we did pre-meeting mailings in the past, and that bucketing it by year injects skew. The actual paper count has actually been flat since late 2014, when we did go up to a new plateau. For example, of the past 9 meetings, 6 had mailings bigger than this one (and the 3 that were smaller were during the C++17 CD/DIS period which is always a lull). So this meeting's mailing is average-sized or smaller compared to the past ~4 year plateau.
I've thought about it a lot and i'm never satisfied with that definition of handling a memory allocation failure. It seems to me that in this case and many others, the design of the program should be changed so that it can complete a large operation without running out of memory or producing partial results. Sure the program kept running, but if it can't actually do what you asked it to do, did you really handle the error?
It's not just “useful”, it's implicit in the definition of thread or of thread library. Just like some strings are longer than others, some threads need more stack than others. A thread doing complex highly mutually recursive graph algorithms (like my app) might want a few threads with big stacks. A web server worker thread might need a small stack. The MacOS 512KB thread stack size was never intended to apply to all threads. It only makes sense if it accompanies a way to change that size. For C++ to use a convenient OS default as a rigid unchangeable number doesn’t make sense. 
It's not exactly helpful, but something interesting - `lint` itself _does_ warn about untested return values from all functions, and it even has multiple versions: foo returns value which is sometimes ignored and foo returns value which is always ignored
Understandable goal, but maybe just use a variant? 
I sometimes feel like application developers are getting shut out of the Standards process. It sometimes feels like the concerns and interests of the C++ committee are increasingly estranged from what application developers want, at least since 11. Doubly so for cross platform developers. This thread thing, designing the only thread library in history (to my knowledge) disallowing thread stack size change, is maybe a symptom of that.
I wouldn't have thought so, but I can try to check when I am in front of a proper computer. Traveling this weekend. 
I don't even see why calling it a stack size for this particular (and optionally supported) method would be a problem. You can add a note like: "if you don't know what a stack is, don't try to use this method". 
I just thought of yet another example about why trying to explain ovecommit in the C++ standard would be a terrible idea: the standard does not care about stack overflows and (rightfully!) does not pretend that all the code in nondeterministic ambiently UB just because of unknown stack size limitations.
That doesn’t work cross platform.
If setting thread stacksize is unimportant, can you name any thread API other than std:thread, in any language, that does NOT allow for flexible thread stack size? 
These numerals aren't as attractive as in the post but I really wanted there to be exactly 42 '1's. int i = 0; i += 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1; i += 1 + 1 + 1 + 1 + 1 + 1;
No need for digits, but this whole thing start to look like the IOCCC... #include &lt;iostream&gt; int main() { int fourtytwo{}; std::cout &lt;&lt; ++ ++ ++ ++++++ ++ ++ ++++++ ++ ++ ++ ++ ++ ++ ++++++ ++++++ ++++++ ++++++ ++ ++ ++ ++ ++ ++++++ ++ ++++++ ++ fourtytwo &lt;&lt; std::endl; } Coliru: [http://coliru.stacked\-crooked.com/a/93ec2ab1ac914601](http://coliru.stacked-crooked.com/a/93ec2ab1ac914601)
IIRC java only allows setting the stack size on a per-VM level, not per-thread. I also never said it was unimportant.
What about applying strategy pattern? 
I don’t very much understand how is every langue du jour getting a dedicated package manager solving any problem. It’s almost like we’re coming to a point where we need a package manager for all those package managers. Like I don’t know, an apt-get or a brew?
The thing that concerns me and of which I can argue is limiting static throws to single type. Unfortunately I couldn't find arguments against allowing any exception type, given it's statically known and conforms to some interface like the named std::error, or inherits from it. Such approach would limit static exceptions to simpler cases on the OS API boundary, where context could be easily deduced. Would be nice to see some adjustments towards allowing additional context stored in error object.
So? You want to screw up the design of modules? If you want modules to support macros, you'll probably be the first to complain about the mess it makes.
The API of this special class would be a bit better than the generic `std::variant`, but under the hood, they'd be similar.
Yes, indeed, reality is never that simple, especially then a legacy codebase is involved. 
Awesome, thank you very much! I am really too afraid to just hit that "Install" button, I have a feeling that if I am unlucky it will install these whole workloads without any further confirmation... ;-)
&gt; Though I would not recommend that as a general way of writing code. Can get very magic quickly. well currently we're *already* doing this, except with damn ugly macros or external preprocessors such as Qt's moc. A pure in-language alternative is strictly better.
That case is not too bad (doesn't really matter), but when you only have two names, you get `findEmployees("Simon", "Rick")` which is not clear. `findEmployees("Simon"first, "Rick"last)` is better. Or `findEmployees(nullopt, "Rick"last)`.
X only works in 99.99% of cases, therefor it's useless and wrong.
Thanks, I had know that but didn't manage to fix the style sheet for mobile yet.
 std::strong_ordering operator&lt;=&gt;(const char*) const noexcept; Sure the declaration reflects its use /s
I don't get the big deal. Just put all your macros in a separate header file and #include it. Then everything is in its place: c++ modules for c++ code, preprocessor includes for preprocessor macros. 
The native package managers are great and if you are in a situation where you're only on one OS, throw up your own repo and package your code that way. Problem solved! The big issue we have is that people are scattered across OS's. Let's say you're on Ubuntu and I'm on Fedora, we both use the same compiler and we want to share code. Now each of us has to learn to make both RPMs and DEBs. Now our other friend on Mac comes in and we all have to learn how to make brew pacakges. This gets out of hand very quickly, where library maintainers now have to maintain packaging scripts for a ridiculous number of systems. We've already to extent solved the building issue cross\-platform with CMake, so a common package manager that is simple to use can be very useful and abstract away the OS\-specific details.
Since you're a beginner the best way to get better is to get your hands dirty. Therefore, just program harder and harder things until you get better. Follow tutorials until you're comfortable on your own, then work from there. If you wanna become a developer I would suggest pick the path you want to go down and stick to it once you have gained experience programming.
But how many package managers are out there? I lost count, personally. Developers want a very small number of package managers — ideally one — to package for and be done with. It feels to me that at this point people are looking at each other, waiting to know what’s it gonna be. And then someone mis-interprets this silence, and introduces yet another package manager. You speak of a headache in deploying on varying operating systems. I think that replacing this headache with that of packaging for varying package managers is scarcely an improvement. There’s an XKCD about standards that fits here.
thanks. i am some experience programming in java, and we use some c++ (little) and java in my current job. 
If you have a hobby, write since code to do something with that hobby. The usefulness of the application is irrelevant. Just try to do it. If you succeed, try to make it faster. If you don't, learn why what you are trying to do isn't working. You will learn volumes working on personal projects compared to doing challenges or things you have little or no desire to work on. In the process of solving your own tasks, you will get to read other people's code and learn how others solved similar or the same problem. 
holy moly, that's a tome for ya
Can you give an example application
😕 that defeats the purpose of the advice... However, to clarify... Lets say I'm a huge family guy fan. I can make an application that queries a database to find all episodes that have a specific word or theme. It doesn't really matter what it does or how useful it is. It matters that you want to do it. This alone will build a portfolio and expose you to pretty much all the problem solving skills you will use on the job.
Depends where you're at now. Super quick and easy: Write a program that takes arguments of the form "4 + 4" and prints "8".
Excellent 
There is a special place in hell for websites that intercept mouse events.
Lots better
Wow.
Since I am the author of the post you mention, I just want to make a quick comment here: &gt; Do I believe macros support in modules should never be added? My personal option is No. I’m not enthusiastic about supporting macros in modules, but I still believe a reasonable compromise can be made. No, you cannot make a reasonable compromise. If you dirt your modules with macro interaction you have effectively converted modules into #includes: you will be affected, as far as my understanding goes, by mutliple problems, including import reordering affected by the order of the macros. I vote for a big *NO* to this. If people want macros there are alternatives: - code generators and transform your #defines into constexpr one thing that is missing here is how to do some kind of if constexpr for configuration without templates. I must admit that. - use macro + #include as you do in your post. But never, ever, convert modules in the equivalent of preprocessor directives. Actually, if this goes in and affects all users, I am going to be very disappointed because this will mean that instead of serving language evolution and the community (I do know some corporations work really hard on the C++ standard, so in this sense, I apologize) they just serve their particular code bases interests at the expense of the whole community. This should not be in the standard. Modules should be *modules*. Note that I am not against finding alternative, opt-in solutions, but at the time you mix macro propagation with modules, you have already compromised many, many things for the rest of us. Too many, IMHO. If what the committee representatives want is to not touch a single line of source code from their code bases at the expense of the whole community, I know what to expect from now on, so I think I should reconsider if C++ serves my purposes well, because the compile-times and lack of isolation are, literally, at the top of my pain points in the list of things to solve right now. 
Great episode. There were some solid video references Phil made that c++ devs should check out if you're interested in TDD and functional programming.
Not easy to find a job in C++ nowadays especially without any prior experience.
&gt; The behaviour he is relying on is that new returns nullptr when OOM Which isn't actually C++. Unless one passes `nothrow` to the allocator, it will throw. There is no way currently to pass `nothrow` to allocators called internally by standard allocators. Yes, `-fno-exceptions` changes that, but that's not part of C++, and is exactly the "bifurcation" that Sutter's paper mentions.
Thanks for clarifying. I understand that I need to build a portfolio by working on applications that I like.
&gt; For variant to encode the discriminat in a Type it needs to know 1) that this is possible and 2) where in the type it must be encoded. Reread what I wrote a bit more carefully. &gt; People have you examples where this matters above (e.g. Option&lt;&amp;T&gt; having the same layout as a reference and being able to use that In C ABI). Never-zero covers that all-important case perfectly well.
This shit is extensive, woah! Instant bookmark. 
you mean the hand-symbol mouse cursor?
Yeah. True. But based on developments to the language at a rapid pace..I am guessing that c++ will be adopted more, which I am not sure. I want to work something not related to web development, and I also like c++, so I chose to switch to c++. In your experience as a c++ developer, what applications did you work on. 
This doesn't seem like a very good argument. It's mostly just kind of pointless vascilating, until they get to the real argument, "multiple declarations", and a superficial similarity with storage specifiers - however, in my opinion, the worst part of const-west is that people confuse storage-specifiers and cv-qualifiers. Yes, consistency with pointers and member functions is great, but the most important thing is that different things look different! constexpr int const&amp; x = 0; static int&amp;&amp; y = 0; // at class scope int foo() const&amp;; constexpr int const&amp; bar() const; The confusion with storage specifiers gets really weird with constexpr pointers - why is `const char* x` a pointer to a const char, but `constexpr char* x` a const pointer to a non-const char? `constexpr const char*` looks kind of silly, while `constexpr char const*` makes it obvious that `const` and `constexpr` are of different kinds.
&gt; No, you cannot make a reasonable compromise. That's unfortunate. One of the compromise I though about and shared in this post was to ship modular macros as an extension, enabled during the transition for huge codebases. I have a hard time pinpointing what's unreasonable in this. &gt; If you dirt your modules with macro interaction you have effectively converted modules into #includes At this point I wonder if you actually read what I was proposing after the text you quoted. What I proposed was not transforming imports into preprocessor directives. Me too I'm against mixing C++ the language with preprocessor stuff. This is one of my main concern. Yet the example I shown there kept preprocessor separated from C++ imports. Read carefully. &gt; as far as my understanding goes, by mutliple problems, including import reordering affected by the order of the macros. The Clang folks are saying the the order of importation won't matter since the imported module won't have access to the same preprocessor state. Personally, I don't think that it's the way forward, `import lab.a;` should not play with the preprocessor. I mean, it's called the preprocessor not for nothing: it happens before the compilation. Clang mixes the two step as far as I know, but this only applies to clang. And this is also why a plugin for clang or something like that could be developed to ship an extension that would allow the smooth transition big tech companies are pushing for. &gt; If people want macros there are alternatives For the most part yes, but there are a lot of macros today. This is why I think should start developing tools that will help separating macros from code or as I mentioned in my post, making a tool that would generate a header that would only contain the macros you need to change the preprocessor state as required by the interface. &gt; But never, ever, convert modules in the equivalent of preprocessor directives. I agree. &gt; they just serve their particular code bases interests at the expense of the whole community. This is why I think we should have a reasonable compromise. I think it may exist. Only in my small blog post I proposed two. I'm sure if we analyse objectively what makes us against modular macros, and what are the needs owner of very large codebases are trying to fulfill, we might found something that makes everyone happy. We should stop seeing it as black/white, good/evil. It serves nothing. &gt; Note that I am not against finding alternative, opt-in solutions, but at the time you mix macro propagation with modules, you have already compromised many, many things for the rest of us. Yes. This is in part why I wrote the blog post. I think discussions are way too polarized to understand each other. I think the two solutions I mentioned are great examples that we have to ability to address concerns we have about mixing modules and preprocessor. Transforming `import` into a preprocessor directive is a bad solution in my opinion, because it only address the needs of large codebases, without addressing our concerns about the future of the language. Creating a tool that would generate glue code may please both sides. Shipping an alternative version of clang that supports macros may be acceptable too, to allow smooth transition over time. A new kind of preprocessor directive could be a thing too. As long as they stay the preprocessor of course. As far as I understand, the priority right now is to ship module, and think about macros later. This would be the most sensible thing to do right now, at least, in my opinion.
&gt; It leaves most everything in status quo, because STL would still be largely unusable See the part about the STL offering OOM-aware interfaces. I mean, this isn't complicated. Many of those folks avoiding the STL are still using a very-STL-like alternative. The key bit is to merge the important bits of those alternatives in some form into the standard itself. Since those alternatives _today_ typically rely on C-style error codes, this proposal is a first-step in making those kinds of STL evolutions have a higher quality. &gt; I'd like to see what that OOM-aware interface proposal would look like for things like std::function for instance. `std::function` is broken in several notable ways that have several proposals for fixing, none of which can work in a back-compat way. The fact that `std::function` can't use custom allocators is just one of the many reasons that folks like me don't use (some parts of) the STL. There may well be no way to fix it, and that's fine; it won't be the first standard library type to get deprecated if an incompatible but superior alternative is provided. Sutter's paper doesn't mislead here: he is clear that it may be necessary to roll some fixes into new alternative types that address those kinds of issues (absolute worst case: a whole "STL2"). Even if bits of the STL are never "fixed" there's still value in this feature. It still helps avoid bifurcating the language by allowing `-fno-exceptions` to become purely legacy; there's nothing non-standard about writing custom containers after all, but compiler flags that change the very semantics of the language are _very_ non-standard. :)
&gt; [...] and the impact on build system should be reduced. If `#import` will require loading a BMI, then it will have exactly the same impact as described in P1052R0. I still like this better than the stealth `import` directive in the Atom proposal since here a tool vendor can simply no support preprocessor import.
Hello again. Sorry will not quote since I am quickly writing from phone. Yes, basically my worry is, indeed, mixing the preprocessor propagation with modules. Also agree: modules without macros support should be landed first. Also I do not get actually what is wrong about #define + #include... I think that should be enough combined with maybe with some kind of config refactoring... but of course I did not do an in-depth analysis. 
Pass one `size_t` _where_? What does that API look like? Is that `size_t` the minimum, maximum, or exact requested stack size? What is the interface required for platforms that require this `size_t` to be a multiply of a particular number? What allocator if any is used for this stack space? Should the function fail or proceed with a default if the `size_t` turns out to be invalid for the platform's capabilities? Should the `size_t` be paired with more information for heterogeneous architectures or platforms with transparent split-stack support? What about all the _other_ missing thread construction properties that people complain about that are "so obvious," like permissions or debug names ? How should the thread creation interface remain extensible for future platform needs without needing _another_ backwards-incompatible interface extension? Shoving a `size_t` into an interface every time something extra is needed is a pretty terrible solution. See the explosion of `operator new` overloads as a very prime C++ standard example thereof.
Yeah, I wasn't too sure about that. It would still make the build system more complicated, and also compilers more complicated. It would require multiple kind of BMI. A first pass would detect which preprocessor BMI need to exist and their dependencies, then preprocess the files to list which C++ BMI need to be compiled, and then finally continue as usual. It would be complicated and would need a lot of compiler support, but would at least not be impossible to implement like the atom proposal. The best I can think right now would be to ship (preferably as a plugin, opt-in) an extension for Clang to support modular macro for the time they finish the transition to modules.
No, I mean [this](https://www.youtube.com/watch?v=cnvaHW6zNLY&amp;t=10s): &gt; // Add mouse-down/up event handlers document.addEventListener("mousedown", mouse_down, false); document.addEventListener("mouseup", mouse_up, false); 
That example is of no more importance than the others, I assure the article isn't revolving around that sole example, doesn't sound sensible to me. May you're nitpicking on it for some reason? I don't see any. I didn't cover cv-qualifiers vs storage class in detail to not lose focus and because the article is already too long. I have answered on that years back [here](https://stackoverflow.com/a/13076065/1000282) and [it links to a case](https://stackoverflow.com/questions/13073952/where-in-a-declaration-may-a-storage-class-specifier-be-placed/13076065#comment17772772_13077637) where oversimplification causes confusion on where storage qualifiers should be put, which can be inferred with ease carefully reading Ritchie's "Meaning of Declarators".
I know the meaning of declarators, I've been writing C for a very long time. I couldn't see any other real arguments for const on the left - most of the article was about declarations-as-expressions. Which, okay. But that has nothing to do with const on the left.
email igor at igor@knockknock.org.uk
So your argument is basically aesthetic, just because they're of different classes, they should sit opposite in the table. May be fine for you, but makes no sense to me.
Glad you enjoyed :-)
Are you seriously not able to answer any of those questions yourself? The problem was solved by the pthreads interface, I believe around 1995. They pass an object containing initialisation parameters to the created thread. This is an extensible and proven solution, so I would suggest stealing that. 
&gt;Which isn't actually C++. There are people out there programming in that dialect. Those people are the reason for this paper, so you can't just wave and say "we don't care, it's not the standard". &gt;that's not part of C++, and is exactly the "bifurcation" that Sutter's paper mentions. So instead of "bifurcation by way of compiler switch", we'd now get "bifurcation as a matter of programmer choice" (because now the programmer will have to choose whether to use the terminating allocation functions, or the non-terminating allocation functions). Let's be clear: that is a major step backwards. 
My point was that OOM is really not the death sentence that some people make it out to be. Unless, that is, the language is changed to make it so... Even for people who haven't given much thought to OOM recovery, it turns out to work pretty well as long as you write your code in an exception-safe manner. &gt;the design of the program should be changed Buy a license and we'll talk ;-)
True this. Also where I live it seems quite regional. There are loads of C++ jobs 40-50 miles away but a lot less closer to home. If you want to make a career as a programmer and are just starting out I'd pick what's popular (probably java or something webby) then build your C++ in your own time whilst working as a dev. But then its only worth doing it if you enjoy it enough and/or there are actually jobs for it where you are.
50.0 miles = 80.5 kilometres. _____ ^(I'm a bot. Downvote to 0 to delete this comment.) ^[Info](https://www.reddit.com/user/bot_metric)
Yeah. Actually I am currently working as support and want to become a developer.
Actually in this case template &lt;typename... Filters&gt; std::vector&lt;Employees&gt; findEmployees(Filters&amp;&amp;... filters) {...} seems more appropriate to me. This is a none-or-many scenario, not a none-or-one-or-another-one-or-both-scenario if you catch my drift...
Hello /u/gracicot, again. &gt; Familiar? But what happens if somewhere you forget to define it at one place of many? Yes, you get an ODR violation. It can be sometimes hard to debug such mistakes. Sure, the best solution would be to add the compile definition for all your translation unit, but some prefer to do it this way. I think we should not worry too much about how people do horrible things like #defining something by hand at each module. As a minimum, they should using a config.hpp.in if needed and use that or use a -D compiler flag, which is global. I think modules should not be concerned about fixing others broken code and not recommended practices but in doing a better build system for all of us. I strongly disagree with people wanting to be accomodated to their weird practices instead of them accomodating to the rest for the benefit of a good module system. It is as if I wanted to be given a degree in CS by lowering the level instead of by studying more myself. Not fair.
Sometimes I think if giving it a tey. I did in a small project. But it is not as mature. Anyway it can already handle real world cases and its C/C++ integration tempts me more and more. But async programming with fibers amd GC, not sure about how to handle performance in those scenarios. I am very familiar with C++.
Are you using /permissive- flag? 
I'm afraid msvc on godbolt is about a year old, according to its version.
Well, we just got another cppreference but in terms of language specification, not API docs. Good content, but please get rid of the mouse behaviour
Weird bug, like `T` is assumed to exist out of nowhere.
I started using vcpkg for a month, it works well and it's simple to use. Although there isn't as much as features than Maven or Gradle, it just manage your packages. Quick start: https://github.com/Microsoft/vcpkg/blob/master/docs/index.md Package list : https://blogs.msdn.microsoft.com/vcblog/2016/09/19/vcpkg-a-tool-to-acquire-and-build-c-open-source-libraries-on-windows/
I know it would take me long time before i become good developer in C++. Difficulty is a matter of our practice. Although java is easy... It fits into other categories such as number of programmers and code bases... only modern programming languages such as golang, scala won't have this problem I guess. Actually I don't want to work in web development. And I know c++ as I use it some times in my work. So, I don't know how to proceed if I want to work with c++ developer. I know java, but where ever I see, all the opportunities are in web development and web services.
&gt; - modules are a vast improvement to compilation speed if an only if someone does not have the great idea of being able to inject macros into modules from outside of the modules themselves. That will compromise from tooling, to compile times and hygiene.
The world is basically web development and web services, whether we like it or not. And it is only going to get worse. Choosing C++ just to be sure that you don't do webdev seems a bit weird (and there are quite a few C++ web development). What do you dislike about web dev, and what would you want to work in ? Games ? System ? Embedded ? Mobile ? ML ? 
As much as I like many of fluent.cpp posts and the great talk about algorithms that was just given by its author, I think that sometimes abstraction is taken too far for little return. When I have to work I really like all this stuff but sometimes it is just not worth the trouble when you put time investment vs return. Just my 2 cents and by no means want to criticize much of the great work delivered, which is not little.
Indeed, this makes a good argument to make macros module-scope and allow just external includes
&gt; What is the exact relation between module and package manager? The two are pretty much orthogonal. We could try to automatically derive the package name from the module name if there is some convention followed, but that's about it. &gt; Do modules allow any sort of versioning? This is a vague question (versioning of what), but the answer is probably no, versioning will be done at the package level. &gt; Can modules provide IDE-independent, universal C++ project file? No. &gt; How does compiled / preprocessed module look like? The format/content of the binary module interface (BMI) is compiler-specific. Generally, the closer it is to the object file the greater the performance benefit one can expect, that's probably where most implementations are headed. Which means that BMIs can even be incompatible at the compile option level and are definitely not a distribution mechanism. &gt; Do we need full source code of a modular library? You will need source code for the module interfaces.
Which is basically what I think that should be done: no macros from inside to outside of modules or viceversa.
&gt; We could try to automatically derive the package name from the module name if there is some convention followed, but that's about it. This looks like a place for improvement. We could have a module meta file or something similar that would allow different package managing tools to use the same file. Basically one configuration that works for every package manager.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
The debugger is definitely a great plus. Also, a big reason I use VS as my main is because of [Visual Assist](https://www.wholetomato.com/). The refactoring tools and productivity boost I gain from that is just super invaluable. I know Creator has `Alt+Enter`, but Visual Assist is such an amazing plugin with all of it's context sensitive refactoring tools. I've disabled the default Visual Studio Intellisense because Visual Assist just does such a good job of it, and it's a lot faster that way. If that plugin did not exist I'd say they were about equal for my needs. So to rephrase, Visual Studio with Visual Assist is what I'm actually vouching for here.
I also used VS with Visual Assist, there's not really something I miss now that I work with QtCreator.
I agree with the others, you have to get your hands dirty, c++ jobs require experience. You should try to (re)create a little game, like Pong, in my opinion creating games is a good way to learn because it's fun (if you like games) and you can quickly see results on the screen. To do that you can use a library like https://www.sfml-dev.org/tutorials/2.5/ which is simple to use and well designed. And follow this tutorial for example http://gamecodeschool.com/sfml/coding-a-simple-pong-game-with-sfml/ Once you achieved that, you may want to transform your Pong into a BlockBreaker, and read some books / articles to understand some data structures you had used and to improve your c++, like : - C++ Primer - Effective C++ - More Effective C++ - Modern Effective C++ After months of development, you will be experienced enough to discover that you aren't. At this moment you can start to read, understand and sometimes criticize CppCon talks, articles (short list): - https://arne-mertz.de/ - https://www.fluentcpp.com/posts/ - http://preshing.com/archives/ With the https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines all along. (That's was roughly my path)
Needing to load any sort of BMI during preprocessing will lead to the kind of insanity described in P1052R0. Which perhaps suggests the solution: make `#import` preprocess the module interface unit itself keeping the preprocessor state changes (i.e., exported macros) but discarding the result of the preprocessing (i.e., module interface declarations). Then "expand" `#import` the preprocessor directive into `import` the language declaration so that the BMI is loaded at the language level. I think this might actually be worth putting out there... 
TIL. I thought grimoire had something to do with "grim." I searched and found that it had something to do with grammar rather. It's more proper definition is that it's a book of spell. 
Report it!
Aren't all implementations at the moment just "proof of concept"? I think we need to postpone a verdict on compilation speed until compilers based on modules hit a final release.
&gt; I do not see preferring broken practices as a strong argument to mess up the full system for the rest of us. Hmmm... I should have been clearer in my blog port then. My argument was that modules will make this malpractice impossible to do, enforcing better alternatives. My point was that you would only need to pass the flag when compiling the interface of the module that need the config, leaving the rest of your files isolated from that defined macro. This work under the current TS that don't support macro exportation.
Why do browsers even allow that? I can't see any valid use case outside of games. This could be a "gaming" permission bundled with OpenGL and audio for example.
Oh, I see now. I misunderstood you. I agree how it currently works. No need for more :P
I have a question about this, can someone say what's the difference between device\_registers\_ref and device\_registers\_placement, I understand about the pointer/ref difference but not the last one. `auto device_registers_ptr = reinterpret_cast&lt;DeviceSetup*&gt;(DeviceControlAddress);` `auto &amp;device_registers_ref = reinterpret_cast&lt;DeviceSetup&gt;(DeviceControlAddress);` `auto device_registers_placement = new (reinterpret_cast&lt;DeviceSetup*&gt;(DeviceControlAddress)) DeviceSetup;`
&gt; By the way, I had the scary realization that standard feature-test macros are... well, macros Well, there's no great way around it in the current language, unfortunately. Only top-level global variables in a compilation unit are guaranteed to be constructed at the load time of a program, and these global variables are what is used to register your test to be run. Global variables that are not in a namespace are not guaranteed to be constructed until code in that compilation unit is called, so top-level global variables seems to be the only solution to "statically registering a test". But since these variables are global, they need to be uniquely named. To guarantee this, you need to glue together the full file path and the name of the test into a variable name, and the only way to do that right now is with a macro.
&gt; would be a net improvement over what is currently proposed, and I like that it would allow the processor state not to be mixed in an horribly hackish fashion with the C++ language. That was the point of my example. If we were to integrate something the would act as macro exportation, I would prefer to see something like this. The ideal for the language would be to leave macros out of this. &gt; However, from a build system perspective, it do not make things any easier and i don't like the idea that a build system will have to read a BMI. As /u/berium replied to me, the preprocessor could parse the module interface itself, without the need for reading the BMI. Although even that have it's own implication, like each TU will need not to compile, but preprocess every modules interfaces, which is not nice, and kind of break the goal of modules. Still better than `import` being a preprocessor directive though. &gt; It's also a burden on compilers vendors versus not having exported macro at all. Yeah. I feel that even if modular macro are accepted, it would end up as a mostly unimplementable feature, and would end up like exported templates.
Depending on how things go, we may be adding a new `catch(expr)` operator which will convert any lightweight exception throws by `expr` back into their underlying `_Either(A, std::error)` form. So we agree on that problem, and we're on it!
I literally came onto here to say that I just finished scanning the mailing, and noticed that perhaps half the papers are about existing work (whether agreeing, disagreeing, or simply posting changes based on feedback). Given that C++ 20 goes to stone very shortly, a rise in those sorts of papers is highly likely, and also very temporary. They are not indicative of anything except a pending new C++ standard definition. What would be much more useful is a trend of "new" papers by topic over the past two decades. I appreciate that "new" is tricky one. The fact there are like four papers all on exception handling this mailing, and yet all four are new and evolved independently and concurrently, I think is an important thing to take note of. I also think that going backwards, similar "topic spikes" may have occurred, illuminating consistently recurring pain points in the language and ecosystem. All that said, I am not volunteering to do that survey. Still trying to get P1027 *SG14 Design guidelines for future low level libraries* out the door ...
This is mostly true. Modular macros is only implementable in Clang right now only because they based their module implementation on their PCH mechanism. Google is mostly using Clang and I think it's one of the reason why they proposed atom. Bringing modular macro to the standard will ma'am everybody switch to clang. That won't happen so if atom is accepted as it is currently proposed, it would end up like exported templates, dead, with one compiler implementing it. Big companies need macros to make a smooth transition? Compilers are open source, they can patch them, I mean, clang. That was the other compromise I proposed: ship modular macro as a in-house extension, make the gradual transition, then disable it. I'm not even fan of my other compromise, the preprocessor only solution, it has negative impacts on build systems, but it's still better than what atom proposed (`import` being secretly a preprocessor directive). &gt; You're missing the point here I think. I think you didn't understand what was the message I was trying to convey: I'm not advocating modular macros. I'm all against it. But if we can find alternative solutions, like a module interface generator, or a separated header generator for macro to be included, if there is solutions for the common usage of macros without training our module system, it would be the best. If we can ship stuff as opt-in extension (for those willing to implement it) then don't change the standard for your needs. If we can't possibly get to a common ground *with all other solution proposed*, then maybe something like my `#import` could be less hurtful than the original atom proposal. I don't want to engage in a holy war. I'm trying to see both side and try to have a balanced view of the story. If we could communicate with atom advocates and listen to their needs, and if they can listen back to our concerns, then maybe we can find a solution. That was my point. The examples I posted are example, and surely not the best we can do.
I thought feature test macros were defined by the compiler? I mean there is no need to export feature test macro as far as I know. There is nothing preventing their usage, even with the module TS.
 std::herb::stutters();
&gt; A major benefit of (also) doing it at the C level is that C is the lingua franca portable platform language that all other languages bind to. Therefore this facility would help integrate C not only with C++ error handling, but also with Rust, Swift, and many other languages that can speak C and also return A-or-B from functions as their main error handling implementation. The carrot for WG14 would be that C facility would benefit all C speaking languages, and help C be more useful in its key role as a glue language. A notable gain is that C could now receive exception throws from C++, and propagate them, perhaps even convert them for other languages. Overall the proposal looks very good, but I would not *expect* something like this to ever exist
Well, perhaps then I'm not sure what you mean by "feature test macros"? I was commenting on the macros used to register tests in popular C++ test libraries like Catch and Google Test... 
[https://github.com/onqtam/awesome\-cmake](https://github.com/onqtam/awesome-cmake) also exists and has much more links inside it ...
I've already reported some [void_t related weirdness]( https://developercommunity.visualstudio.com/content/problem/228514/stdvoid-t-fails-given-valid-type-in-presence-of-pr.html) The original weirdness I found was a use of void_t that would succeed or fail depending whether a previously defined but completely unrelated template had been specialized using void_t. With a few alterations I can make it also conditional on the name of the template parameter, so it works with T but not U: https://godbolt.org/g/cTXPsS
No, feature test macros are, as far as I know, a define that exists only if a particular feature of the language exists, like `__cpp_lib_optional` is defined only if the compiler supports optional. As for testing framework, this can be done just like my first example in my blog post. Separating macros from C++ symbols. But then, I can imagine a modular future that has a test framework in headers... It's not as if module would magically make tests faster to compile. 
I agree completely. Supporting macros in modules is well within reach. In a list of imports, each imported module’s macros would have no effect on the subsequently imported modules but would have effect on other code following the imports. And two conflicting macro names would be an error at the use site. This postulated C++ future where macros don’t exist is a very, very long way from commercial practice. Modules will have macros, and to force a set of imports to have a parallel set of legacy includes is overbearing pedantry. Whereas modules with macros would be relatively straightforward to adopt even in a large codebase like Unreal Engine, and would greatly clean up the current mess of includes and special build processes.
Ah, thanks. Now I know what feature test macros are, it's not a big atrocity that they are actually macros appearing in every compilation unit, though, yes, it's a little unaesthetic. The name makes them clear they're defined by the compiler anyway...
Love it!
Is there a technical reason we can’t we just have a constexpr new? If referenced by a static variable, its contents could be compiled into the application.
&gt; Surely that is #include? Yes! I have difficulty understanding why this is not enough. If it was for magically using C libraries, then a code generator might do the trick. For the rest, I mostly agree. Asking to being able to use a feature without refactoring is too much. Yet those big companies have influence and we must deal with it.
100% agree - I'd have totally done that if I wasn't writing a lib that was being used cross-OS, cross-architecture. My json.h parser is being used on x86, Arm, MIPS, PowerPC, and some funky little PIC chip (and that's only the usecases I know of!), and I don't really want to sacrifice the single-source-path-for-all-variants as that reduces the maintenance burden for me.
&gt;Problem #1: the parameter std::nullopt express that we pass “no” parameter. But at call site, it hides what role this parameter should have had in the function. It’s no parameter, but no what? No first name? No last name? No something else? I'd argue that this has nothing to do with the optional. What do "John" and "Doe" indicate? You might guess from the context, but to be sure you have to look at the signature of the function, you can't really work around that in C++.
&gt;break parallel compilation I keep seeing this but I don't get it. Isn't it *modules* that break parallel compilation? Right now builds should be roughly three levels deep (generate generated headers, compile all cpps, link) allowing full parallism at each level. Using distributed compilation like icecream, we see benefit up to - j400 when we saturate GigE. With modules you now have inter-cpp dependencies so you can't compile *all* files at once. My main concern with modules is that our compile times would go *up* due to reduced parallelism and it being harder for distributed compilation. (This is just my intution, not from direct experience and I really hope I'm wrong about this.) On the other hand it it trivial avoid losing parallelism when adding macros to modules (well, trivial relative to the rest of the work at least, not to minimize the hard work going into that.) You just need a similar muliphase build: extract just the exported macros from each module, then compile each module using the previously extracted modules from all imports.) Then you have no (additional) inter-cpp dependency edges caused by macro exports. What am I missing here? Taking that idea further I've wondered if it would be possible to take that idea further and and a third step in the middle that extracts just the exported definitions of each module as a token stream. This should then allow all phases to have full intra-phase, parallelism by removing the inter-cpp dependency edges. I can also see an obvious way to do distributed builds in that world. What's the catch? I know some very smart people are working on this so I assume there must be a reason this wouldn't work. I'm not a compiler guy so apologies if none of this makes sense. I do have a weird hobby of writing and optimising build systems, so I see these problems through that lens. 
The problem I have with catch (and this is somewhat mitigated by being explicit on which exceptions can be thrown) is that I have at least a perception of try/catch code being brittle. "Oh, an unexpected exception was thrown at this level; add another catch block". I want to be *forced* to handle errors. Any back-channel communication is something I want to forbid in my code; if possible via the language; if not via style guide. I'm happy to have all (current C++11/14/17) exceptions be fatal to my app. I'll see if I can word up a coherent argument to Herb's letter to SG14 since he was explicitly talking about separating error and non-error paths, and that's what I *don't* want.
eww. Should be able to whip up a quick userscript to deal with that.
[I did](https://developercommunity.visualstudio.com/content/problem/252157/sfinae-error-depends-on-name-of-template-parameter.html)
Unfortunately, it seems to me that the right approach is full PP support in modules now, and deprecation later, in order to give a migration path to existing code. Not supporting macros now and adding them in later on wouldn't actually serve any purpose. So there is some merit in the suggestion to only have macro support as a compiler extension. However, the standard doesn't (and AFAIK can't) deal with compiler extensions. It would instead need to be made an optional feature that is still standardized, but not required to claim standard conformance. It would be in the best interest of compiler vendors to provide the feature, in order to give their users a migration path. A future C++ standard could then deprecate this optional part of the standard. Obviously this doesn't lower the workload of the committee at all, and thus wouldn't fast-track modules support. But it would give the standard the freedom to at least claim right from the beginning that you shouldn't depend on this feature long-term.
yeah this looks like the same error showing up in a different way. Glad I'm not the first.
&gt; Or is the intent that everything in the C++ standard library returns an _Either and, only if it isn't checked does it become an exception? Herb's current proposal preserves existing C++ throw semantics. It is hoped that under the bonnet, the mechanism to implement lightweight exception throws will be via a C `_Either(A, B)` where all C++ functions marked `throws` would be in C terms `_Either(A, std::error) ...`. C++ injects code to check the Either return from all C++ functions marked `throws`, and if `B`, it propagates them up the call stack. So, `_Either(int, std::error) x = catch(foo());` is simply a way to tell the compiler to not do the default propagation, and revert to the C semantics for this one call of `foo()` here right now. One could write instead: ``` extern "C++" int foo() throws; extern "C" _Either(int, std::error) foo_extract() { return foo(); } ... _Either(int, std::error) x = foo_extract(); ``` So, the hoped for `catch(expr)` operator would be shorthand for the above. I have a paper in the works explaining all this stuff, still hoping to have it done by the meeting.
That's fair. It was a bit of a tough sell to add the first SIMD optimizations to our codebase, but the benchmarks were hard to ignore. We also had to prove that the same algorithm could work on at least one other (non-x86) platform just by swapping out the ByteVector type. On the bright side, I think [std::simd](https://wg21.link/p0214) will have everything you need to do this, and it seems likely to land in C++20 and have polyfills available for older versions. Sorry if my post seemed a bit harsh. I've just seen so much "high performance" string processing code that works byte-by-byte that it makes me irrationally frustrated. Anyway, if you are interested, feel free to use that code. And if not, at least I had fun writing it! :)
&gt; My main concern with modules is that our compile times would go up due to reduced parallelism It's true that module interfaces have to be compiled at the outset, before any translation units (including other modules interfaces that import them) can start compiling. However, the expectation is that module interfaces will be very fast to compile and then compiling the rest will be also a lot faster so the overall build will be faster as well. In a sense, with modules you need distributed compilation less. &gt; and it being harder for distributed compilation How so? With modules (unlike headers) the build system finally has the exact view of what the inputs are for each translation unit. So our expectation is that modules will pave the way to reliable, generally-available distributed C++ compilation where you don't need to pray that the headers on the remote how are in the exact same location and are compatible enough that the result won't be skew.
Really impressive. My wishlist item is to show when the compiler considered two items as possibly aliased or when it can assume an alias\-free optimised function. I suspect this is deduced in backend analysis rather than the AST so may be not likely. Nice uncluttered interactive web view. It would be excellent to incorporate this within Matt Godbolt's compiler explorer.... 
Your post was not harsh - the lesson is an important one for people to learn! Thanks for taking the time to educate :)
Not sure. It's certainly doable in general (i.e. Rust guys managed to "constexpr" everything excluding actual syscalls and such), but there is probably a lot to consider to implement it for C++ and nobody has done it yet (not that surprising considering even getting non single-expression constexprs took additional standard).
&gt; Faster is always better, but it is almost at the point of diminishing returns since it dropped bellow the point that you are likely to go do something else like check email in the edit-compile-test loop. You must be really good at managing your compile-times but I can tell you that, for me, personally, and I do not mean your experience may differ, is a problem compared to other languages. Once you get distracted from a compile, there is a context switch in your brain and this drops my productivity quite heavily. You might be different but I think there is research about this problem for normal humans like me. &gt; In many ways I'm currently more concerned about the large rebuild times Well, that is something that concerns you. For me, as I told you, it is when I am in the edit-compile-test cycle. The faster it is the more productive I get, especially if there are no long interruptions between compilations. In my experience anything that goes beyond 15 seconds or so starts to bother me. But that, again, might be myself. &gt; However macros are currently the only tool in our toolbox that lets us add missing syntax to the language Yes. So why don't you #include a file that has an import plus another include with macros? This works today, AFAK. Do you really need to propagate macros *from modules*? I do not see this like a promising solution, though I must admit I do not know all implications to the greatest detail. &gt; What additional recompilations would be needed if macros could also be explicitly exported? As far as I understand the problem is that with macros, for the people that do not need it, you still need running the full preprocessing phase before starting to do the actual compilation. &gt; I actually think we should work to make macros better integrated into the core language. Things like supporting namespaced macros. This was refused already years ago. &gt; Also our unittest library captures a lot of extra information that would be impossible if we used normal functions. As a more complex example, we recently introduced a macro Well, again, what is the problem if you use macros to use plain #includes and use imports for what should be modules? I fail to get it. 
I've been working my way through this one. It's among the best C++ books I've seen, and even better since it has c++17 features and some mentions of c++20 possible features
&gt; You must be really good at managing your compile-times but I can tell you that, for me, personally, and I do not mean your experience may differ, is a problem compared to other languages. Once you get distracted from a compile, there is a context switch in your brain and this drops my productivity quite heavily. You might be different but I think there is research about this problem for normal humans like me. ... Well, that is something that concerns you. For me, as I told you, it is when I am in the edit-compile-test cycle. The faster it is the more productive I get, especially if there are no long interruptions between compilations. In my experience anything that goes beyond 15 seconds or so starts to bother me. But that, again, might be myself. Oh, I'm also easily distracted. That is why I worked hard to get most single-file compile-edit-test loops &lt;10 secs. For unittests it is often &lt;5, although one I've been working on lately spends &gt;30s just on *codegen* which I don't expect to improve with modules...
&gt; It's true that module interfaces have to be compiled at the outset, before any translation units (including other modules interfaces that import them) can start compiling. However, the expectation is that module interfaces will be very fast to compile and then compiling the rest will be also a lot faster so the overall build will be faster as well. In a sense, with modules you need distributed compilation less. In my view, if we still need tomanually separate interface and implementations into separate files like .h and .cpp, then modules were a failure. The number one thing I'm hoping to get out of modules is a DRY mode where the computer is responsible for separating things into files rather than humans. This is how (almost) every other language works, and one of the things I hate in C++. &gt; pray that the headers on the remote how are in the exact same location and are compatible enough that the result won't be skew. That is an *insane* way to do distributed builds. That is in no way reflective of how it is supposed to work, even with headers today. We use [icecream](https://github.com/icecc/icecream) to ensure that even the compiler binary is the the same across all nodes regardless of their native OS or distro. We use `-E` with either `-fdirectives-only` (gcc) or `-frewrite-includes` (clang) to package a TU including all of its headers into a single stream that gets sent to the remote nodes to finish preprocessing and compile. This is done in a chroot so it doesn't even have access to the files on the remote host. This is fairly easy to set up (we did it for two different build systems) and can guarantee bit-exact results to compiling locally (excluding debug sections for various reasons unfortunately).
&gt; That's an insufficient answer for the real world if modules are ever going to really be deployed beyond green field projects. You could say the same about RAII and in fact destructors in general. There are still plenty of projects out there which started before C++98 was a thing and are written as C with classes. I also disagree. No one needs to be able to do everything all at once. If you are using C++2a on a huge, old project then when you first flip the compiler flags you'll be using #include and lose nothing. If you want to make use of a brand new shiny third party C++2a price of code, you can #import it. If you write some new functionality, and don't make heavy use of macros then you can stick it in a module and #import it. If you use macros absolutely everywhere and have no inclination to do otherwise, then you can still stick with #includes. The thing is ever since the "modern style" became popular (some time in the mid 2000s), people have been reducing the use of macros to the point where the use in many current projects is very limited. Those can be moved over piecemeal and relatively quickly. I understand not breaking compatibility, but modules don't do that, but like any new feature no one is forced to use it and it some old codebases won't move. 
Haha. It is terrible the wait times. They break the flow.
&gt; That is an insane way to do distributed builds. This is unfortunately how most people do distributed compilation these days, at least on Windows with VC.
It's so cool and thanks for your work. By the way, would you like to accept pull request? I would like to help.
The weirdest bug I ever found myself was with gcc. Any program compiled with sse instructions enabled instantly crashed No idea how that one made it through testing
&gt;This is unfortunately how most people do distributed compilation these days, at least on Windows with VC. Fair enough. I've spent much more time optimizing the *nix dev experience since that is what 90% of our c++ devs use. My point is that those issues aren't inherent in the header model (which to be clear I don't love even if we've made it work well for us.) I don't think MSVC not implementing useful features show problems with the header model as much as with MSVC. &gt;Yeah, this and things like __FILE__ are another can of worms. Actually, that works fine. As does actually using the debugger. The problems are things that don't really matter but prevent bit-exact output, like gcc embeds the working dir in debug, or relative vs absolute paths. Have a look at the output with those flags. It is probably better (and bigger 😕) than you are expecting. 
hey thanks
That looks like the same "C1XX didn't know how to compare the contents of decltype" which was supposed to be fixed in 15.7. The workaround is to add a unique tag type in each void_t to make the compiler consider them different.
Do you mean [PC Lint](http://www.gimpel.com/html/pcl.htm)?
Try /r/cpp_questions. As a hint, why do you divide the total by 5 and then divide the result by 0.2, which has the effect of multiplying by 5? This is pointless and likely introducing inprecision due to floating point calculations.
I meant classic lint, but sure that probably also works :P It seems odd that clang-tidy wouldn't have this rule.
Hello, I really don't think that this subreddit is the place to ask your question \([r/cpp\_questions/](https://www.reddit.com/r/cpp_questions/)\) is probably much better\). I'm pretty sure the problem come from `mark=((first+second+third+fourth+fifth)/5);`. The `/5` will do an integer division. Since `mark` is a float, you probably wanted to say `/5.0f`. Some remarks: - use a for loop, instead of writting 5 times the same question. - declare your variable where you use them. - don't use `#include &lt;math.h&gt;`, you don't need it here. And if you need some c++ stuff, include the c++ header, not the c one (`#include &lt;cmath&gt;` instead of `#include &lt;math.h&gt;`).
You can't even select text on that page... :\
&gt; There are people out there programming in that dialect. Those people are the reason for this paper, so you can't just wave and say "we don't care, it's not the standard". ... I think we're in violent agreement on this topic. :) My point is simply this: this paper removes the need for `-fno-exceptions` on the _language_ level, and lays the foundation for evolving the standard library to likewise remove the need for "NotSTL" alternative libraries. This paper has to come first. :) &gt; So instead of "bifurcation by way of compiler switch", we'd now get "bifurcation as a matter of programmer choice" Which, again, is _already the case_ as we're agreeing about. Let's not let perfect be the enemy of the good. We can harmonize part of the language and be in a better place than we are today even if we haven't harmonized _everything_. &gt; (because now the programmer will have to choose whether to use the terminating allocation functions, or the non-terminating allocation functions). Let's be clear: that is a major step backwards. Maybe. Folks who care about OOM are possibly already using alternative functions (by not using the STL). The open question is how many folks are doing that. If it were true that a majority of C++ developers actively rely on `bad_alloc` and actually use it right, then yes, removing OOM exceptions would be a non-starter. That's also why OOM is left as an optional part of the paper.
&gt; Are you seriously not able to answer any of those questions yourself? Stop being a jerk. I'm not the guy blocking the paper. Those are (some of) the concerns with the original paper, which I'm sure you've read twice and you've provided useful feedback to authors. You also via that communication know exactly why the "just use pthreads" suggestion is dead in the water and explicitly rejected by the committee/SG1, which I'm not going to bother providing here since I'll apparently just be implied to be an idiot for sharing their reasoning. I'm sure, since it's all just so easy, you've already got standardese wording and a sample implementation to send to committee, because every solution is as easy as firing off aggressive posts on Reddit.
&gt; ./configure --with-foo That usually means, ultimately, that the compiler #defines something to make the code compile in the right way. But then, once you've used #define, you've used the preprocessor, which is what we were trying to avoid. There's no need to modules to export macros to handle that. module foo; #if defined(HAS_FOO) export constexpr bool has_foo = true; #else export constexpr bool has_foo = false; #endif Modules are still compiled as TUs with the preprocessor and all that entails. The question at hand is whether module interfaces can _export_ macro definitions.
Solution: `template &lt;typename T&gt; using c = const T;` Or middle-const: `const int` -&gt; `cionnstt` Now nobody is happy. Victory achieved.
This is proper /r/assholedesign
Why not just restrict macros to header files? Every once in awhile you hit a scenario where macros are the only way to accomplish something and it's nice to be able to work around limitations in the language. Modules for complete... ah... modules. Headers for when you need direct file inclusion and macros.
 This is so good. This should be an option on every C++ compiler.
Yes, it's true that old projects don't have to adopt new features. But with modules one of the main motivations, at least for part of the community, is specifically to provide features to support large scale projects for use on their existing large scale projects. So if we introduce a modules feature into the language that doesn't do that, that basically rules out them ever getting modules they can use because we're not going to add two separate modules features into the language. Furthermore I'd say it's important that standardized modules be proved to actually solve the issues of large scale projects, and the way to do that is to use them on large scale projects and see if they work. Unless there are plans for writing a brand new 100 million line green field project to test out the modules proposal then the only way to test it is to try out modules on an existing 100 million line project. That's basically what went into P0273 and some of the other papers that have been presented to the committee, and I think it would be a mistake to ignore the experience people have had deploying modules so far.
I agree, it should be a single list of criteria, not one argument for every possible criteria. I can only image this function after a few years of maintanance, having 10 different arguments, and every time an argument gets added every usage of it in the code base needs to be updated to the new signature.
I really enjoy Walter E Brown's cppCon presentations \- particularly his lightning talk, "Whitespace ≤ Comments \&lt;\&lt; Code"
Needlessly rude comment? Have a downvote.
&gt; Or looking through the classes of a compiler. Typo or pun, that’s the question...
Largely, it directly challenges the assertion that OOP is a great \(or even good\) paradigm for writing good programs. Even if performance is not your primary focus, isolating components into clean, compact arrays enables testing, concurrency, and maintainability. The talk is polarizing. Listeners either resent the message or latch onto it as a new way to program \("new" to that person\). I, for one, found the talk amazing. It changed the way I look at developing software. In general, I find reasoning about the data to be far more useful and powerful than reasoning about the code.
Hi, I'm Andreas the creator of C++ Insights. I'm glad you like it. I would love to see pull requests coming. Also feel free to open issues for things that look wrong.
Right now the rules for macros in modules are too lax. It would be better if a macro could traverse modules only if it is defined and immutable. No need to look for its definition somewhere else, no need to worry about \[order dependence\]\([http://www.open\-std.org/jtc1/sc22/wg21/docs/papers/2018/p0955r0.pdf](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0955r0.pdf)\).
I guess that isn't reliable enough from language designer's point of view. 
Good point about Java. But Java does let you set the stacksize per VM, which is perfectly fine, and it lets you right multiplatform multithreaded code with an appropriate stack size. 
broken link?
He did name his article bikeshedding and thats what it is.
That's meaningless.
and doesn't work without javascript
So in summary, the people defending the C++ design have two main points, if I understand them correctly: First, is that there might exist a machine with threads and without a stack; or if not a real machine like that then an abstract machine like that; and therefore a thread should have the default OS thread stack size on whatever machine it is on. Second, is that the ability to set the stack size to something other than the OS default is not important. Users who want to do this either have poorly designed code or misunderstand their own requirements. 
Link is broken? 
Thanks. Eventually I managed to install VS 2017 using the online installer. Sometimes my internet connection is not that horrible. An offline package has the advantage that the users can use some download software to resume the download in case network break or computer restart.
Sorry about that, http://app.jobvite.com/m?3As1LjwC is the job description and https://lifeatcb.carbonblack.com/blog/ is about working at Cb. Thanks for letting me know the link was busted!
What's wrong on mobile? Looks as good as any other code-based post I've seen (which are all fine at best)
&gt; can you name any thread API other than std:thread, in any language does NOT allow for flexible thread stack size? Ocaml. Haskell. 
Hi, I will have a look into it. It looks strange.
Would that work if you had an exception class that is over a kilobyte in size?
To all the morons complaining about the mouse, can I suggest you bother to read the introduction? The mouse capture is there to help you. As for writing some extra JavaScript to switch the mouse intercept off, an alternative is to press the little button at the top right of the page :) Also, the page is designed to be downloaded and used locally. Of course if you bothered to read it you would know this. If you don't fancy any of the above then I suggest you carry on being idiots. Just do everyone a favour and do it quietly please
Exactly. I'm glad this idea is taking hold. Modules need to be designed with tools in mind and involved if they are to be adopted.
If you add the noexcept keyword to the copy constructor, it will compile. No idea why it is required in this specific case...
&gt; Things in the standard should serve the community as a whole "so it should do what **I** want, not what other people (who are actually doing the work to make it a reality) want" --this guy
&gt; "so it should do what I want, not what other people (who are actually doing the work to make it a reality) want" so it should target common use cases for common people, not cases from a handful of people that want to avoid touching code from a 40 years-old codebase.
Plus the bug report yesterday in [this post](https://www.reddit.com/r/cpp/comments/8ja13j/the_weirdest_error_ive_found_in_a_compiler_msvc/), I'm glad I didn't uninstall my VC 2015 after I installed 2017 some days ago...
You don't understand that there is no "serve the community as a whole" because the language users have drastically different use cases. If you feel your side is under-represented, then there is a clear path for you to at least partially rectify that and it doesn't involve making whiney-sounding reddit posts.
Madness! Jason Turner's videos are one step above everything else one advanced C++. Cool stuff.
This is not a question. This is a bug report and it's good to let every one know.
I do understand that if there are many million users and 3 or 4 o 8 are big corporations that want a good refactoring for them this is not going to be most of them, but even if they were 50% of the community, they would still be trying to inject something for their use case that is very harmful in build times for the rest. I disagree fully with your assesment that there is not “community”. Most companies are not going to have those huge old codebases if there are several millions of C++ users. By that, the proportion would be of some percentage but never a majority. It is just that they have representatives in the ISO, committee, remember, it is an ISO standard, not a private business standard.
Needs more upvotes.
Reposting my SO answer: This is your bug, not MSVC's. 1. `Foo`'s copy constructor is not `noexcept` and has no move constructor. 2. `Bar`'s implicitly declared move constructor is not `noexcept` either, because it has a `Foo` data member. 3. `emplace_back` may reallocate, and since `Bar` appears copyable, that reallocation will copy the existing elements to preserve the strong exception safety guarantee. 4. This copying may be done, depending on the implementation, from a `const` or non-`const` lvalue `Bar`. 5. Your unconstrained constructor template hijacks copying from a non-const lvalue `Bar`. 6. Explosions and fireworks follow. The fix is to constrain the constructor template so that it doesn't hijack. For example: template &lt;class... Args, class = std::enable_if_t&lt;std::is_constructible_v&lt;Foo, Args...&gt;&gt;&gt; Bar(Args&amp;&amp;... args) : foo(std::forward&lt;Args&gt;(args)...) {} 
&gt; I was wondering what could possibly go wrong here? 
Happy to talk to anyone about the Software Infrastructure role in London, as I am in that group.
In theory it can. The web interface enforces std=c++1z. I did skip checking for the language in the transformations, so it may show C++ stuff even without the std-option. Are there any special things you have in mind for C?
Uh I didn't came up with this :-( It's brilliant:-)
Oh, and apparently it wasn't a compiler bug, it was a user error.. so it extra doesn't belong here.
I also edited my previous reply :)
will do next time, but it seems I can't std::move() this to /r/cpp_questions
Rather looks like some ABI mishap, not a compiler bug.
Importing macros INTO a module isn't worse than any other dependency you import from other modules. INJECTING macros into the imported modules is what would be problematic, but I don't think anyone seriously proposed that.
What do you mean by "hardware verification"? Specification compliance?
Looks like Reddit has eaten your bandwidth, I was just playing around and suddenly it's down, both Insights and your site. Great work, keep going!
Hardware design verification. Verifying logic designed using hardware descriptive languages like verilog. 
I'm not proposing to allow the exportation of macros across module boundaries because they have lots of other problems (if you look at my past comments on similar post, you will see that I'm a strict opponent of that idea). What I'm saying is that exporting macros from the imported to the importing module would have no significant effect on isolation, compilation speed or general suitability for parallel and or distributed builds. A module A that imports another module B isn't self contained anyway - you need the (exported) information from B to correctly parse and compile A anyway and it doesn't make a big difference (as far as the build is concerned) if that information includes macros or not. In fact, Google is already using their own version of modules that do export macros with great success.
Go with UVM SystemC. It combines UVM and C++: http://www.accellera.org/activities/working-groups/systemc-verification/uvm-systemc-faq https://www.accellera.org/images/news/newsletter/Using-UVM-in-SystemC-DVConUS-2014.pdf
By self contained I mean you just need the symbolic references. Not to open a file and parse something.
Last year at CppCon2017, I was awarded the jury’s first prize for my poster about my smartref library, because of the high quality and innovation of the presented approach B-). A question that I often got, during the conference, as well as on the Internet, was about when such a library would be useful.
You wouldn't need to do that if macros got exported either.
i have a question regarding point 2. of your list: does the class `Bar` have a move-constructor anyway? following [this](http://en.cppreference.com/w/cpp/language/move_constructor) list: - `Foo`has no move-constructor declared (because user declared copy-constructor) now i see 2 possiblitys: a) the templated constructor of `Bar`counts as user declared constructor, so we have no move-constructor either. or b) `Bar`'s implicitly declared move-constructor is deleted because it has a member(`Foo`) with no accessible move constructor? i am okay with the rest, just wanna tighten my understanding.
I never claimed constexpr would imply const for memberfunctions, but that doesn't change the fact, that you can't use them to change a constexpr variable or can you provide a counterexample?
Haskell starts low and dynamically expands the thread stack size as needed. I think Go does this too. I don’t know much about OCaml threading I have to admit. Anyway if the thread stack size expands dynamically you don’t need an explicit way to set it. Think of a gearshift on a car. A general purpose car needs a way to change gears because sometimes it is climbing hills and sometimes driving on flat ground. Cars with manual transmissions (pthreads, any OS threads) let the user set the gear. Cars with automatic transmissions change the gear as needed (Haskell, Go). But you can’t produce a car that has one gear setting and then complain that anyone who doesn’t like that setting is a bad driver, or that changing gears is wrong because it's unfair to hypothetical cars without gears.
The bahavior seems to vary from compiler to compiler: https://godbolt.org/g/g3zNDE
C++ can be a lot faster than Verilog for running tests, and can more easily integrate with other software to do software development with simulated hardware. There are valid reasons to use C++ for hardware verification. I do it. I wouldnt worry.
Not disputing that. I just wanted to point out, that you apparently can't rely on this in practice
A constructor template is never considered a copy/move constructor. The rule is actually in terms of overload resolution results rather than "move constructors" (and I need to rewrite that page). `Bar`'s move constructor isn't deleted since you can construct a `Foo` from a (non-const) `Foo` rvalue using the copy constructor.
Thank you! And thanks for reporting. Rebooted the server. Looks like I need a bigger one. Amazon probably. Didn't except so much traffic in the beginning.
Please do not glob sources! https://github.com/acdemiralp/cmake_templates/blob/23b74c9d081b4d5c8a0fe315e9d2642b7ac3e71f/normal/library/CMakeLists.txt#L18
Also, if you are on Firefox and use an extension like NoScript, you can turn off the only script on that page and you'll be able to easily select anything.
For a long time that was the defacto way to do things in the industry. C++ still does give some benefits over UVM such as potentially faster test bench speed, the ability to interact with other software, and the potential to use of a golden model designed by architects to check against. I think as time goes on more companies are realizing UVM benefits outweigh C++ but I wouldn't fully discredit it. The other thing to keep in mind is that some hardware companies just move slowly. With chip design cycles potentially taking multiple years and with hundreds of thousands of lines of C++verification code the move to UVM could be a slow process.
Do companies like AMD, Intel still work on c++ for verification?
I use c++ for constraint random HW verification for a big company, there's a (in house) framework available that facilitates many of the features also provided by UVM. C++ allows high level of abstraction while having good performance. It's suitable for the job. Why exactly are you worried?
This sounds similar to my line of work. I wasn't sure if other companies are doing similar work. If, at all, I have to switch jobs I don't want to end up with no option.
As a relatively new to cmake user I keep reading this, and avoid it myself. However u junior dev in my office challenged me on it and I realized that unlike target_* I don’t understand the problem. What is wrong with glob sources?? TLDR why not? 
We still use C++ for verification. I’m a step removed from the work, but I don’t think UVM could replace our internal verification framework. 
Uhm...can anyone provide a summary of all the different ways to define a concept? In this article I see `template &lt;typename T&gt; concept Concept = constexpr_bool_value_v&lt;T&gt;;` `template &lt;typename T&gt; concept Concept = OtherConcept&lt;T&gt;;` `template &lt;typename T&gt; concept Concept = requires(T const&amp; a) { {expression} -&gt; ConceptOrType&amp;&amp;; };` Can I mix and match? How do I express that a concept should satisfy a `requires` clause as well as another concept?
There may be a later blog on the different ways we can be used in the Concepts TS, and my thoughts on what's (currently) in C++20. To answer your other question, it's possible to mix and match, because concepts are predicates. ```cpp template &lt;typename T&gt; concept Concept = constexpr_bool_value_v&lt;T&gt; &amp;&amp; OtherConcept&lt;T&gt; &amp;&amp; requires(T const&amp; a) { {expression} -&gt; ConceptOrType; }; ```
If you glob sources, then there is no file CMake can watch to detect that the configuration of the project has changed. You have to run CMake yourself to get changes to appear. Some IDEs have learned to compensate for this behavior (which I think is too bad, honestly). You also have the problem of files accidentally participating in a build (leftovers/copied files/undeleted/etc.) and if you're globbing just on extensions, you can have only a single target per directory. Explicit file management means that only files you include will be built and when you make those changes, CMake knows to regenerate the build system (because you changed a listfile).
Thank you 😊 
it's, like, in the *future*, man
&gt; It aligns with existing practice on systems like Linux where virtual memory overcommit means that our current C++ standard heap exhaustion design is fundamentally unimplementable and already ignored in practice, where bad_alloc can never happen and new is already de facto noexcept. "Overcommit" == "no bad_alloc" is an urban legend. It stll can happen if your address space is fragmented enough and simply does not have a contiguous chunk of the requested size. And it's not rare at all.
All I see is a blank page. Fix your site.
Yes, but Stroustrup's Vasa paper still rankles with me, because his own efforts at putting C++ on a more solid foundation have not met with success. Uniform initialization was the first example - it was meant to rationalize the way initialization works, but backfired terribly. Other Stroustrup initiatives: default comparisons (didn't work until Sutter came along and fixed it), operator dot (not as good as the inheritance-based proposal), and pushing for "natural" syntax for Concepts (if it wasn't for Stroustrup we might have agreed on something better by now). The reality is that we have lots of high-quality papers.
Globbing is "explicit file management", too. The directory listing is the list of files in the target. Having a single source of truth is a good thing. You can keep your scrap files somewhere else outside of the source tree. Obviously you can't use globbing with multiple targets in the same directory. This isn't a point against globbing in the other case though.
Yes
Conceptualize?
Formally, your argument breaks down at #3: "appears copyable" isn't a thing. `emplace_back` requires "T shall be `EmplaceConstructible` into `X` from `args`. For `vector`, `T` shall also be `MoveInsertable` into `X`." ([sequence.reqmts] Table 82). It notably doesn't require that `T` be `CopyInsertable` into `X`. Since `CopyInsertable` has uncheckable semantic requirements, it's impossible for an implementation to determine if a type `T` is `CopyInsertable` into a container `X`. (Yes, that means that the requirement in [vector.modifiers]/1 "If an exception is thrown while inserting a single element at the end and `T` is `CopyInsertable` or `is_nothrow_move_constructible_v&lt;T&gt;` is `true`, there are no effects." is unimplementable when `T` has a throwing moves.) I think we have a standard defect.
I've found some but they don't provide all the answers, because they were only about free functions. According to that Stephen, beauty comes from `using` to alias long type definitions, and using default parameters. So the solution in the article could be rewritten like this: template &lt;typename T&gt; using Isnt_a_reference = std::enable_if_t&lt;!std::is_reference_v&lt;T&gt;&gt;; // ... template &lt;typename U = T, typename = Isnt_a_reference&lt;U&gt;&gt; void f(T&amp;&amp;) {} 
&gt; Globbing is "explicit file management", too. No, not in the context of CMake build systems (unless things have changed substantially, but my habits mean I never saw the change). 1) Use globs. 2) Add a new file to the directory (which matches the glob). 3) Run the build. One of two things happens: 1) Nothing. Your file isn't included in the build because the generated build system is still the same. 2) You are using a build system that has learned the anti-pattern of needlessly re-running CMake all the time. CMake gets run and your build system gets regenerated. The file is included in the build. #2 doesn't happen for make, nmake, ninja, or any non-IDE CMake generator to my knowledge. The correct way to do things is to explicitly add sources to the listfile. The build system generated by CMake doesn't care at all about the contents of the directories. The generated build system doesn't have any idea you used a glob to generate it. The only time that build system gets updated is if CMake gets run, and CMake only runs automatically when the inputs to the build system generation change. It's an anti-pattern that IDEs have learned to work around it or that people have gotten into the habit of pessimistically running CMake all the time. 
Obviously not, but thats not really the point. ( why would you design humongous exceptions in the first place ? ) I'm saying there are real scenarios where you sometimes grab the memory if its available and perform some optimization, reindexing or caching task or whatnot, if it's not available you don't - also beyond small embedded systems. Not being able to allocate a huge chunk doesn't indicate OOM. Not being able to allocate as per request is mostly _not_ a license to kill the program, and shouldn't be dealt with in a _hook_ either.
Did you try std::forward?
The other answers are valid, but they don't talk about the most important feature: correctness and repeatability. If you run a build with "make", it should always give the same binary result considering the same input. If you can end up in a situation where this isn't true, then you don't have a trustworthy build system. And requiring a manual step in order to build correctly isn't working either, humans make mistakes, we want to eliminate that possibility with good tooling. An example of why this is important: Files can be optional in a build and cause side effects when they are included. A common example for that would be in a test program where you just declare your tests and they automatically register. No file is required, but not including them in your build means you aren't testing properly. It's not necessarily easy to notice for a human either.
Those with huge existing C++ code bases that are still growing are suffering compile times the most, and at the same time are the ones that must migrate them to modules incrementally. So IMO the key question is whether new language features should be incrementally usable on old code based or require a new rewrite. I understand that those that want this now for small projects don’t care, and that those who are writing the proposals prefer to choose the easy problem to solve, but I also can understand those suffering the issues that the new feature is supposed to solve complaining if the feature is unusable.
Thank you! That makes a lot of sense. I think that correctness and reproducibility of builds are key. We normally put hooks to package / version our releases and check that there are no local modifications to checked in files. Of course, using glob would bypass this because unversioned files could be included... I better check all of our cmake files for glob asap.
Yes, this particular defect is known since time immemorial - see LWG 2158 and 2461 - and is tricky to fix when `construct` can mutilate constructor arguments as it sees fit. I intentionally wrote in colloquial terms rather than standardese.
I should clarify that /u/tcanens is spot on in terms of what implementations do in this case, despite that the standard is defective and unimplementable. We pretend that `is_copy_constructible_v&lt;T&gt;` means that `T` is `CopyInsertable` into `vector&lt;T, Alloc&gt;`, and we ask the allocator to copy `T`'s instead of asking the allocator to move `T`s when `is_nothrow_move_constructible_v&lt;T&gt;` is `false`.
I don't care enough. I see it on my phone but didn't work on firefox+ublock on Ubuntu. 
If true, that doesn't stop it being very common. And I'd argue it's false: it has as long as history as gcc, and some neat language extensions as well.
This could all be a lot easier if CMake's scripting system was more declarative and IDEs/editors could reasonably automatically add sources in all CMake projects. Without globbing, what happens in practice is that someone generates an MSBuild solution, then in the IDE adds a new source file, which modifies the solution, and builds and works just fine, and then checks in their locally-working modifications without the necessary CMakeLists.txt change. If we're all lucky then they notice the build break in CI soonish (and before anyone else checks out the new code) and fixes it ASAP, but more realistically, either someone else notices after getting latest code that won't build (and they then have to track down the mistake) or the original coder being involved in focused work doesn't notice the CI break until hours later (if at all). Globbing tends to work slightly better since users are more likely to delete a file off disk (and make sure it's deleted in source control) when they remove it from a solution in the IDE. Stray files certainly do happen, but more rarely; and when they do happen, they're not really any worse than the alternative (CI build will break and someone eventually has to go fix it). Aside from IDE integration, globbing could work well with a lockfile approach to source lists. That is, check-in an explicit list of sources, but allow for nice easy tools (CLI or IDE) that can re-scan and update the lockfile with new/remove sources. Basically just like git or p4 reconcilation would do.
&gt; Those with huge existing C++ code bases that are still growing are suffering compile times the most, and at the same time are the ones that must migrate them to modules incrementally. Agree. &gt; I understand that those that want this now for small projects don’t care, and that those who are writing the proposals prefer to choose the easy problem to solve, but I also can understand those suffering the issues that the new feature is supposed to solve complaining if the feature is unusable. Personally I care about incremental builds and iterations when I work. But that is me, I know there are other cases. &gt; So assert pretty much sums it up: it’s a module that exports a single macro and everybody uses it. If that can’t be made to work the feature is broken. Why? I do not get this point. The official discussion has been for years, from people like, to name someone with not a big name, but THE creator of the language, Bjarne Stroupstrup, that macros are bad, follow bad rules, should be avoided. All attemps to make exposed macros into the language and other extensions were refused for one reason: we must get rid of macros. So if we are encouraging, again, using macros, but not only that, also messing up new features with perfect macro compatibility *we are doing it wrong*. #includes + isolation inside the modules should be the way to go. Adding extensions for the sake of "no, look, if we do this I will not have to refactor anything" does not do any favor to language evolution. But it is not only that, it is that it is not by any means impossible to do a refactoring with the existing tools (#include + refactoring) but some people just want it more convenient... no, this does not make sense sorry. It does not because I have been hearing at least for 15 years that we should get rid of this stuff but nothing is done in its favor. I am also a bit so so with the feature test macros... why not something else. Let macros die already, they have created lots of trouble already. &gt; Also range-v3 and libc++ have been modularized using Clang modules (which support macros). Compile times improve by 20%. With these tiny improvements it is even worth asking whether the feature is even worth it. I do not know how this has been done exactly, but if one of the problems is the reparsing nowadays, in theory with modules and without making a cancerous-macro-favorable macro system compile times with several processors should be quite better just because it can be parallelized more efficiently and the importing is symbolic. Besides that, there are other improvements as well, as the isolation they provide which we also need, as I always say, to not make::my_code::look::like_this just bc I am afraid of breaking it if I do not qualify it. 
I know Concepts will provide better constraints, diagnostic, tooling (Intellisense,...). Can we expect better compilation time for templates somehow? 
**Company:** Disbelief **Type:** Full time **Description:** Disbelief is a game development studio focusing on contracting and consulting services. We’ve worked with both AAA and independent studios to help their projects ship. Notable projects we’ve worked on include Gears of War 4, Borderlands: The Handsome Collection, and Perception. At Disbelief, we’re problem\-solvers, first and foremost. We like to dive in to figure out the reasons something isn’t working before fixing it. We take pride in solving problems others can't. We value a sensible work\-life balance and work environment. We work with leading edge technologies to make them perform at the top of their capabilities. Currently, we’re looking for a junior programmer. This opportunity is for a full\-time position in Cambridge, MA or Chicago, IL. Junior programmers at Disbelief are called on to develop and debug in a variety of areas from game play to core engine programming. You are expected to learn new systems and projects as you grow as a developer, with support and training from more senior members of the team. Most importantly, you will work to solve problems with the help of the team. *Key Responsibilities* * Clearly communicate your work to others * Debug code * Estimate task work * Consider performance when writing code * Document your code *Skills and Requirements* * BA/BS in Computer Science, or equivalent experience * Excellent communication skills, both verbal and written * Some type of systems programming in any language. * Good understanding of C\+\+ * Knowledge of version control with P4, git, or equivalent **Location:** Cambridge, MA or Chicago, IL **Remote:** No, but we do enjoy working from home up to two days a week, when project constraints allow. **Visa Sponsorship:** No **Technologies:** Most of our work is C\+\+ of varying standards with a sprinkling of other languages as needed for tooling. We do a lot of graphics programming work, using shader languages and platform graphics APIs. Since we often are debugging the lower levels of systems, being able to read x64 or ARM assembly is useful. Primarily we work with Unreal Engine 4, but we also work with Unity and custom game engines. Our work uses rendering, physics, audio, VR, AR, and other APIs frequently. Our primary platforms are PC, Xbox One, PS4, Switch, and VR/AR devices. **Contact:** jobs@disbelief.com
**Company:** Disbelief **Type:** Full time **Description:** Disbelief is a game development studio focusing on contracting and consulting services. We’ve worked with both AAA and independent studios to help their projects ship. Notable projects we’ve worked on include Gears of War 4, Borderlands: The Handsome Collection, and Perception. At Disbelief, we’re problem\-solvers, first and foremost. We like to dive in to figure out the reasons something isn’t working before fixing it. We take pride in solving problems others can't. We value a sensible work\-life balance and work environment. We work with leading edge technologies to make them perform at the top of their capabilities. Currently, we’re looking for a senior programmer. This opportunity is for a full\-time position in Cambridge, MA or Chicago, IL. Senior programmers at Disbelief are leaders and developers in their project. You should be comfortable working independently and with a team to develop, test and integrate software into a larger codebase. A key responsibility is mentoring and guiding fellow programmers to improve. *Key Responsibilities* * Clearly communicate your work to others * Mentor fellow programmers in and out of your team * Communicate with clients on team progress and problems are they arise * Debug code with precision * Estimate your and others work * Asses impact of issues on schedule * Diagnose and solve performance issues * Document your code * Study version histories and code documentation to solve present problems * Implement features in innovative ways *Skills and Requirements* * BA/BS or MS Degree in Computer Science, or equivalent experience * Excellent communication skills, both verbal and written * 3\-5 years of experience in writing software in C\+\+ * 3 years in a game development, or similar role * Experience working on a large code base * Experience with version control with P4, git, or equivalent * Experience with multi\-threaded systems **Location:** Cambridge, MA or Chicago, IL **Remote:** No, but we do enjoy working from home up to two days a week, when project constraints allow. **Visa Sponsorship:** No **Technologies:** Most of our work is C\+\+ of varying standards with a sprinkling of other languages as needed for tooling. We do a lot of graphics programming work, using shader languages and platform graphics APIs. Since we often are debugging the lower levels of systems, being able to read x64 or ARM assembly is useful. Primarily we work with Unreal Engine 4, but we also work with Unity and custom game engines. Our work uses rendering, physics, audio, VR, AR, and other APIs frequently. Our primary platforms are PC, Xbox One, PS4, Switch, and VR/AR devices. **Contact:** jobs@disbelief.com
It doesn't really give a reason why commutativity is necessary. Fortunately /u/BillyONeal told me reason: https://www.reddit.com/r/cpp/comments/7ajsfn/_/dpcixhe?context=1000
&gt;I want everyone onboard to be able to benefit from the value of Stephen’s talk *Googles "what is SFINAE"* &gt;This feature is used in template metaprogramming. I need to stop drinking
Great, have been using exactly that technique for a while.
It would still be useful to have something between a simple fold and a reduction that requires commutativity, e.g. for matrix multiplication.
Article is not worthwhile reading if you already know SFINAE.
Indeed. I would very much prefer versatility over performance here...
Although I may have misunderstood the article, Wouldn't an overload of accumulate with execution policy and forward iterator sufficed ? What is the need for an entire new function implementation ?
&gt; https://github.com/serge-sans-paille/frozen Looking at the source code, I think this is a reimplementation of several STL containers and algorithms. Is that right? The implementation seems very primitive. For example, it uses an array to implement `set`, and uses recursion in the most basic quicksort if I am right.
The real TLDR of why not in cmake is that this is a deficiency in the design of meta build systems like cmake. The only implementation solution for meta build systems is to put a sentinel file for the glob \(as an internal target\) and regenerate the target direct build files on that sentinel changing. For direct build system globing is not a problem and often makes for clearer declarations that match what users expect. As people naturally group related source files into directories.
Yes, but it's all `constexpr` hence done at compile time. There's certainly a build time hit but runtime is O(1)
`std::accumulate` is a guaranteed left fold, which can't be parallelized. So this: const auto v = {a, b, c, d}; accumulate(begin(v), end(v), 0); is guaranteed to be evaluated as `(((a + b) + c) + d)` (see the first image on that page). `std::reduce` has no such restriction.
https://pastebin.com/wyc9NRne
As most open source projects, GCC has a bugtracker - https://gcc.gnu.org/bugzilla/
And reduce could also calculate \(a\+d\) \+ \(b \+ c\) or similar. Commutativity is required.
&gt; And this is where the problem starts :-) That behavior should be stopped at the earliest - by a manager, co-worker, anyone that ever happens to see it. Sure. :eyeroll: &gt; Also VS 2017's latest "Open folder" support for CMake projects is really nice - no need to run cmake anymore and use the VS Generator. Just open the CMakeLists.txt in VS, and it'll do the rest. That presupposes that we use VS2017 or even have the option of using it. Or that Open Folder would actually work for us. (It might if we used purely CMake for our entire build system, but we don't.)
And you can't forward\-declare std::set, so...
Seems like the cmake server should/could have fixed this, but I don't know enough details.
Well, for one thing, "file descriptor" is a POSIX\-ism which would be interesting to provide on non\-POSIX platforms ;\).
SFINAE itself is not that advanced. See the section in the article titled SFINA-what? for a basic simple example of SFINAE. Abusing SFINAE for template meta programming is a bit more advanced. You should first understand template meta programming topics before trying to understand how SFINAE can be used for that 
Note that filesystem has the "Mac Truck of Standard Behavior Exceptions" [http://eel.is/c\+\+draft/fs.conformance#fs.conform.9945\-2](http://eel.is/c++draft/fs.conformance#fs.conform.9945-2) which probably would be insufficient for what you want on stack size :\)
And I still can't get fold expressions to compile :\(`#include &lt;iostream&gt;` `template&lt;typename... T&gt;` `auto fold_sum_1(T... s) {` `return (... + s);` `}` `template&lt;typename... T&gt;` `auto fold_sum_2(T... s) {` `return (1 + ... + s);` `}` `template&lt;bool... B&gt;` `struct fold_and : std::integral_constant&lt;bool, (B &amp;&amp; ...)&gt; {};` `int main()` `{` `[[maybe_unused]]auto w = fold_and&lt;true, false, true&gt;::value;` `[[maybe_unused]]auto a = fold_sum_1(1, 2, 3, 4, 5, 6);` `[[maybe_unused]]auto b = fold_sum_2(1, 2, 3, 4, 5, 6);` `return 0;` `}` `C:\Users\alex\Desktop\projects\woo\woo&gt;cl /EHsc /permissive- /std:c++latest main.cpp` `Microsoft (R) C/C++ Optimizing Compiler Version 19.14.26428.1 for x86` `Copyright (C) Microsoft Corporation. All rights reserved.` `main.cpp` `main.cpp(14): error C2059: syntax error: '...'` `main.cpp(14): note: see reference to class template instantiation 'fold_and&lt;B...&gt;' being compiled` `main.cpp(14): error C2976: 'std::integral_constant': too few template arguments` `C:\Devel\VS2017\VC\Tools\MSVC\14.14.26428\include\xtr1common(20): note: see declaration of 'std::integral_constant'` `main.cpp(14): error C3770: 'unknown-type': is not a valid base class`
Helpful article. It seems important to note that while addition and multiplication are commutative and associative in mathematics, they are not necessarily associative in floating-point arithmetic. I was thinking about this recently and wondering what the result is. I really hope it's not UB. Is it just a potentially less accurate result?
It's not UB. The algorithms do not \*require\* that the operation is associative and commutative, they specify that the order of application of the operation to elements is non\-deterministic. If the operation isn't associative and commutative, the result will be similarly non\-deterministic. 
Looks like `reduce` can take a different type for `init` than what comes out of its iterators, too. That's a solid improvement. 
I've always thought of SFINAE as a sort of "inverted pattern matching"
he has a question about gcc vs standard and as far as I know cpp is a good place to ask that.
But why wrap it in a separate alias? You could just use the `enable_if_t` directly: template &lt;typename T, typename = std::enable_if_t&lt;!std::is_reference&lt;T&gt;{}&gt;&gt; void f(T&amp;&amp;) {} For clang 5, it still gives good diagnostic: enable_if.cpp:12:5: error: no matching function for call to 'f' f(i); ^ enable_if.cpp:8:6: note: candidate template ignored: requirement '!std::is_reference&lt;int &amp;&gt;{}' was not satisfied [with T = int &amp;] void f(T&amp;&amp;) {} ^ For clang 4, its pretty awful: enable_if.cpp:12:5: error: no matching function for call to 'f' f(i); ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/5.4.0/../../../../include/c++/5.4.0/type_traits:2388:44: note: candidate template ignored: disabled by 'enable_if' [with T = int &amp;] using enable_if_t = typename enable_if&lt;_Cond, _Tp&gt;::type; ^ Which needs to use the `typename std::enable_if&lt;!std::is_reference&lt;T&gt;{}&gt;::type` instead, which is pretty ugly. I find a `REQUIRES` macro works well: #define REQUIRES(...) \ bool PrivateBool_ ## __LINE__=true, \ typename std::enable_if&lt;(PrivateBool_##__LINE__ &amp;&amp; (__VA_ARGS__)), int&gt;::type = 0 template &lt;typename T, REQUIRES(!std::is_reference&lt;T&gt;{})&gt; void f(T&amp;&amp;) {} And in clang 4, it points directly to the requires clause: enable_if.cpp:17:5: error: no matching function for call to 'f' f(i); ^ enable_if.cpp:12:23: note: candidate template ignored: disabled by 'enable_if' [with T = int &amp;, PrivateBool___LINE__ = true] template &lt;typename T, REQUIRES(!std::is_reference&lt;T&gt;{})&gt; ^ enable_if.cpp:5:25: note: expanded from macro 'REQUIRES' typename std::enable_if&lt;(PrivateBool_##__LINE__ &amp;&amp; (__VA_ARGS__)), int&gt;:... ^ While clang 5 still has a clean message: enable_if.cpp:17:5: error: no matching function for call to 'f' f(i); ^ enable_if.cpp:13:6: note: candidate template ignored: requirement '!std::is_reference&lt;int &amp;&gt;{}' was not satisfied [with T = int &amp;, PrivateBool___LINE__ = true] void f(T&amp;&amp;) {} ^ In addition, the macro takes care of making sure the boolean used is "dependent". So it is simpler than having to write hacks like `typename U = T`. 
\&gt; Specifically we're using std::set\&lt;T\*, Comp\&gt; where T is forward declared and Comp is not inlined. So, like this: #include &lt;set&gt; struct T; struct Comp { bool operator()(T*, T*) const; }; std::set&lt;T*, Comp&gt; s; Which [compiles fine](https://godbolt.org/g/T8gNCB)?
For now :)
To be fair, T\* is complete even if T isn't...
The pastebin is missing `#include &lt;set&gt;` somewhere. Maybe your original project gets it from a precompiled header. But your pastebin is probably not showing the real issue right now.
I wish there was a way of adding attributes to operators to signal "this is commutative" or "this is associative" that the compiler could use for optimizations and/or throw an error if you do something that is very likely stupid (like reduce with subtraction).
&gt; cpp is a good place to ask No one suggested otherwise.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
when `#ifdef DEBUG` does not work anymore, modules will not succeed...
 \*\* Company: Focusrite:\*\* \*\* Type: Full Time\*\* \*\*Description: We're looking for an outstanding software developer to join our software team. If you've used Launchpad or Circuit before, you'll know we love to innovate. We'd like you to work with us to specify, design and build exciting new Novation products. You'll need to quickly evaluate new technologies and contribute to team best practices. Most importantly, you'll write clear, high performance, well documented and maintainable code. You'll also contribute to the maintenance of current products, comprising desktop software and high\-level firmware for our Novation products. An ideal candidate is likely to be: A seasoned software developer, with experience of a variety of programming languages, technologies and operating systems, such as: * C\+\+ \(essential\), Juce, Qt, network programming, multi\-threading, macOS, iOS, Windows, Linux * Familiar with GUI development and what makes a clear design * Able to think abstractly from a high\-level \(architectures, design patterns, concurrency, API design\) all the way down to low\-level "C style" code \(communication protocols, bits and bytes\) * A computer science graduate or related education * Passionate about music and audio technology * Able to solve complex problems without compromising on quality * Able to clearly communicate technical designs verbally and in documentation\*\* \*\*Location: High Wycombe\*\* \*\*Remote: ***Team based in High Wycombe, flexible arrangements available including working days in the London office***\*\* \*\* \*\*Visa Sponsorship: No\*\* \*\*Technologies: C\+\+\*\* \*\*Contact: [https://focusrite.workable.com/j/16CA3B809C](https://focusrite.workable.com/j/16CA3B809C)\*\* 
That’s true but I don’t quite follow why this case is different to the other algorithms that have had execution policies added. Don’t all the others guarantee the order of execution in the existing version but remove that guarantee for the “execution policy” version? Is there something special about this operation that I haven’t grasped?
&gt; modules are a vast improvement to compilation speed 20-30% compile time reduction is a welcomed improvement, but it is not world-changing. If your code used to take 50 seconds to compile, it takes 40 seconds with modules. And the amount of work required to retrofit modules in an existing application is non-trivial.
If I remember correctly, IEEE floating point commutability is not about accuracy but numerical stability: give the same result vs give different results within the same error tolerance.
Can you provide and link a better minimal example that actually fails to compile on Godbolt. I can't replicate your issue.
Not embedded C++ dev here, why wouldn't you abstract interrupt handlers?
Was held back due to usage experience. There was a call in early april, here are the minutes. Meeting on June 4th http://open-std.org/JTC1/SC22/WG21/docs/papers/2018/p1070r0.pdf 
&gt; There is a reasonable fear that attributes will be used to create language dialects. but that's what we want to do ! that's what made java and c# and python successful !
There are two different issues that are getting confused here: 1) Incomplete template arguments for containers. This is and always was not allowed by the standard. However, to some extent it does work - even in GCC 8 you can make a set of a forward declared type as long as you make sure you don't instantiate certain methods (possibly implicitly by the compiler). This can be a portability mess. This is not the problem for you since you use a pointer to an incomplete type anyway. Your problem is: 2) GCC 8 checks if the custom key comparison function on instantiation. This actually works in the minimal example you posted. But it does not work if your comparison operator actually operates on a pointer to a base-class of the forward-declared thing in the `set`. So what fails now with GCC 8 is: #include &lt;set&gt; class Base; class Player; // in some other header // class Base {}; // class Player : public Base {}; struct BaseComparator { bool operator()(const Base* a, const Base* b) const; }; class Foo { Foo(); std::set&lt;Player*, BaseComparator&gt; foo; }; I don't know whether that is mandated by the standard or not.
While I really like the open folder functionality, experience shows it is less stable and doesn't provide the full feature set of a project file, so I can completely understand teams that prefer the usage of project files (although the autogenerated ones are horrible)
Daveed Vandevoorde talked about a fun thing called "storage promotion" that would allow that inside constexpr stuff. http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0784r1.html
It seems that GCC supports this, the flag is `-fgnu-tm`.
It's definitely not gone away. Just there last week I was part of a private email discussion regarding memory transactions and persistent memory, because the natural way of atomically updating more than one location in persistent memory is obviously enough a memory transaction. So, to put it in layman's terms, if you wanted to `fdatasync()` two files concurrently as an all-or-nothing operation, that's what memory transactions on persistent memory gets you.
&gt; It was one of the first Technical Specifications, and it was the first and only one that has "died". [Arrays TS](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3820.html) was the first one to die, in 2013. 
In many cases it can be *more accurate*. Sequential fold, which accumulate is, can lead to big errors. Tree summation is often used to reduce errors, and reduce is allowed to use such a strategy, but not accumulate. 
Internal error: https://godbolt.org/g/iCMFTd
I am 100% serious. Some people hate decorators, but without them you wouldn't have any of the libraries that make these language famous: Spring, Django, etc etc. 
The difference compared to the other algorithms is that providing `std::par` to the others results in the same algorithm being run, but executing in a nondeterministic order. So with `std::for_each`, every element is still processed once, but the order potentially changes. With `std::transform`, the order of the transformations doesn't change but they still get the same position in the output (as long as you're not using something like `std::back_inserter` which doesn't support parallel iteration). The same is not true for `std::accumulate` - a call to `std::accumulate` may not even compile as a call to`std::reduce(std::par, ...)`. The reason is that accumulate is strictly a left fold, so only requires to be able to invoke the accumulation function as `accumulator = f(accumulator, *it);`. Parallel reduce requires to be able to accumulate together two accumulators. Between that issue, and the fact that it results in a behaviour change beyond just order, it was made a new algorithm.
Side note, on the use of \`\` `static_assert`: \&gt; This seems completely pointless to me: if the static\_assert would fail it wouldn't compile without it anyway. A `static_assert` will likely leave a much better error message than a regular compilation error \(Concepts are supposed to improve that\). \_\_\_\_\_\_\_ Love your game!
In some cases reduce may even be more accurate - `std::accumulate` can have issues when using a float accumulator that gets to a large value resulting in small additional values getting dropped altogether due to precision. `std::reduce` due to computing separate accumulations of sub-sections of the input can be less affected by this issue.
Python decorators are beautiful in their simplicity. A decorator is just a callable that takes a function (or a class in py3), does something with it (wraps it, modifies it, registers it, etc) and returns the result. The decorated function is run through the decorator and whatever it returns is assigned to the name. There are similar proposals for c++ reflection and metaclasses. I hope the just use the python syntax directly, although I think it is unlikely. 
&gt; And guess what’s the last thing you should do to a utfX encoded string? Randomly slicing it into views at code unit/bytes boundary. This is just anti-utf8 FUD. It is no more wrong to slice at a code unit boundary than at a code point boundary. Code points are not characters! At least when you slice at an arbitrary byte it is trivial to use utf8's self synchronizing design to expand or contract to the nearest code point boundary. Trying to find the character (aka "extended grapheme cluster") boundary nearest a code point slice is *much* harder, and it changes over time with each update of the unicode standard. TL;DR unless you really know what you are doing (or are able to reduce your input domain to simple cases) avoid slicing strings at fixed lengths if at all possible. However, splitting before or after a sane sentinal (space, newline, keywords, etc) is fine both in utf8 bytes and with full codepoints, but it is easier and more efficient to just do it with bytes.
First, [a compile-time C compiler](https://redd.it/5dnan4). Now, a compile-time ARM emulator, unintentionally. I am now expecting an unintentional compile-time C++ compiler. The recursive part will be awesome...
Can someone explain this fear to me? From my point of view c++ already has language dialects, is the fear more are created?
It always irritated me that this code would not compile under C++14. using LockGuard = std::lock_guard&lt;std::mutex&gt;; LockGuard lock() { return LockGuard{mtx}; } 
I know what Python decorators are and how they work, I use them myself. But as any powerful feature, they tend to become overused, and when the are, the code becomes incomprehensible. You are programming in Python any longer, you cast spells from your ~~spellbook~~framework.
I know this is an area where reasonable people can ~~be wrong~~ disagree, but I fall into the camp of favoring programmable programming languages. I find that anything that makes the code closer match the problem domain rather than the base language syntax is usually a win. Even if it takes a bit longer for a newcomer to get up to speed on a codebase. (This assumes you are using reasonable tooling that supports things like "go to definition" or "find all virtual overrides". If not, doing anything is hard...)
Great post! Anything similar to rvalue in the standard/standardization pipeline? It looks like a technique which would be useful in general.
My experience is that they are responsive.
Did the second half of gaddis in 12 weeks after teaching myself the first half in about a month. Wasn't too bad, learned a lot, passed with no prior programming experience. Pointers suck, binary files suck but the rest is doable and really cool.
Slightly off topic, but has there ever been a proposal add a language feature whereby one could move from a local variable AND remove it from scope? Parts of our team are being drug kicking and screaming into using `std::unique_ptr` and we're seeing a rise in use-after-move crashes. These make it much harder to sell `unique_ptr`'s benefits. In the kind of code we write, any reference to a variable after calling `std::move` on it is a bug nine times out of ten. We typically don't reassign moved from variables and we don't really care when the destructor gets called. If C++ provided a way of flagging a moved-from variable as never-to-be-used-again, I'd be all over it. Of course, such a feature might be unimplementable if done inside a branch or loop, but again the majority of these issues are in pretty straight-forward code. auto item = Factory::createItem(foo); ... prepareItem(*item, bar); ... consumer.SubmitItem(std::move(item)); ... if(item-&gt;CheckJustOneLastThing()) // CRASH { &lt;/grump&gt;
Is that not even a compiler warning? If it is a warning, couldn"t you promote the warning into an error?
This is actually quite nice, thanks for sharing this. Saved.
There's a clang tidy check for that.
Nope. https://godbolt.org/g/vkt69N
In Swedish uni we had a course 10 weeks for learning lots of data structures and c++ with no prior course in c++. Most students passed first or second time. Last project was implementing a*. Those 10 weeks were "halftime" meaning you are expected to spend 4h a day during weekdays. Doubt many did tho.
In the code snippet where you declare the 3 namespaces, the "using" are all the same :) 
 consumer.SubmitItem([&amp;](){ auto item = Factory::createItem(foo); prepareItem(*item, bar); return item; }()); /*Will not compile, item not in scope*/ if(item-&gt;CheckJustOneLastThing()) {...}
Yes we have automated clang-tidy checks for this now that fail code reviews for things like this
Notably, the c++ standard has a very subtle difference between `using ns::name;` and `using namespace ns` when it comes to resolving `name` after the directive. In the former case, the symbol is visible as if defined in the same scope while in the latter case it is visible as if defined in the nearest scope enclosing both the imported name as well as the point of the `using`-directive, which in the case of import a standard namespace should always be the global scope. This leads to peculiar behavior when trying to import standard defined literal operators directly. Both `gcc-7.3` as well as msvc will issue warnings for this, complaining about the missing preceding underscore: #include &lt;chrono&gt; namespace foobar { using std::chrono_literals::operator""s; } The warnings are generated by default, without `Wall` or `Wextra`. The standard specifies that user-defined-literal-operators declarations must begin with an underscore. Whether `using` directives constitute a declaration I am unsure, imported names can however surely not be used in declarator so I would lean towards `no`?
Agree. Also most common character based slice operations (separate on space, colon, semicolon, slash etc) work perfectly fine with utf8. That isn't to say that c++ desperately needs better utf8 support.
nit: probably not a good idea to overload swap() as a free function with two arguments that doesn't actually swap them
&gt; TL;DR I'm not sure if you know what it means... you TL;DR section is almost the same length of the comment. Just found this interesting.
You'd like rust.
I think that I would. I love a strict compiler. 
Use after move is a compilation error in rust.
I would like Rust much more if it adapted C/C++ syntax. It's syntax is alienating.
_My_ concept of competency is simply being able to do what I'm paid to do. With C++, for ~~better or for~~ worse, that means knowing a build system or two. I'm a professional; my coworkers are professionals; everyone involved is getting _paid to do this – just _crazy_ expecting people to be able to do what their literal job, right?!
&gt; My concept of competency is simply being able to do what I'm paid to do. With C++, for better or for worse, that means knowing a build system or two. Cool. "Knowing a build system" or not has really nothing to do with the original comment this discussion spawned from. :) &gt; FWIW, I don't know CMake much at all; I loathe it, Yay, we agree that CMake is a steaming pile! &gt; But you can bet your ass that if we used it at work I would learn it, and fully expect everyone else to, too. Thankfully everyone knows what they need to know. "Rewriting a monstrous legacy build system" is not in the realm of what anyone but a tiny handful of us here need to know. And for those of us who _do_ need to know, there's that whole "find the time and money to actually do all that work despite the essentially zero benefit of doing it" problem. :)
Great. Will give it a try! Thanks. Keep on!
There's one use case for this that I've found handy from time to time: template&lt;typename Tag&gt; // Tag used to stamp out distinct types struct widget { ... }; using foo_widget = widget&lt;struct foo_widget_tag&gt;;
Probably the closes is ["Destructive move" proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4158.pdf) It proposes to leave the moved-from object in destructed state, but not removing it from the scope completely. 
What about Rust's syntax is not C-like to you? If anything, C/C++ is the closest comparison I could make to Rust.
I'm wondering how inter-thread communication is supposed to work here. I suppose you should call enqueue in one thread and process in another. Only the process function does not block. I'd expect the receiving thread to be able to call process in a loop but that would just waste a whole core if no messages arrive. What am I missing?
The program is valid but the part of the standard which governs destructor calls is 12.4 [class.dtor]/14. It mentions that the syntaxes which allow the destructor to be found are class member access and qualified-id. Apparently GCC only recognizes qualified-id destructor calls within class member access expressions. [Class.dtor] also notes that `~C();` doesn't work as a complete statement, but only because the tilde gets greedily parsed as a unary-expression, not because `~C` would be a prohibited identifier if it were somehow found there.
A) This is not smart. B) You can but it doesn't delete the class and the memory is not freed. C) Static-like access doesn't work for the current class but only super classes in so far as I am aware D) [link](http://rextester.com/UJAUA97960)
I mean, std::visit is pretty much super ugly without the overload thing. But this actually makes it kinda nice. Last time I tried it, MSVC didn't support some feature that was required for it so I couldn't use it, but I guess they've fixed it now.
You can do it with a struct. I do this a lot for structure versioning.
Part of the question, though: just because it _is_ legal to do some of those things... should it be done? Is it good hygiene to move from a (local) variable and then reuse it? Are the potential uses thereof frequent or important enough to not just add a warning or common linter check against it? Local variables, function parameters, member variables... they're all a little different in uses. I feel confident saying that one should *never* be allowed to reuse a moved-from parameter (at all, in any way) and greatly appreciate any compiler/linter that warns on such misuse. For member variables, move-and-reuse happens often enough in legit code that any blanket warning on use-after-move would probably be a large wall of false-positives and contracts would be required to catch the legit buggy misuse-after-move cases. For local variables... my experience tells me that use-after-move of any kind is wrong, but whether it should be allowed is a question that the wider community probably needs to answer.
Even with member variables, I’m much more inclined to take a swap-with-local-then-move-the-local approach than a move-from-the-member-then-reset-it approach. The swap might not fit absolutely every scenario, of course, but it feels a lot cleaner than reassigning a moved from member. 
Interesting approach, but obviously not every call to `std::move` can be shoehorned into such a construct.
I stand corrected, thanks! I've corrected the post with that.
Very nice. I have just a few comments/requests. 1) Can you add something about when to use EventDispatcher vs CallbackList. In my reading of the documentation, it seemed that EventDispatcher could do everything CallbackList could do, and it was unclear to me what the use case for CallbackList was. 2) Underscore followed by a capital is reserved for the implementation and is undefined behavior if you use it. using _Callback = typename std::conditional&lt; std::is_same&lt;CallbackType, void&gt;::value, std::function&lt;ReturnType (Args...)&gt;, CallbackType &gt;::type; There are some rules about when you can use a leading underscore, but I think the simplest rule to keep out of trouble is to just not use a leading underscore at all in your code.
Be careful with forwarding references in constructor, it could be called instead of the move/copy constructor. I generally try to takes values in constructors and then move them into members/base classes.
Is there going to be an `overload_linearly` added to the `std`?
&gt; I feel confident saying that one should never be allowed to reuse a moved-from parameter (at all, in any way) and greatly appreciate any compiler/linter that warns on such misuse. Try implementing `swap`.
... dammit. :)
2, Underscore. Done. _xxx -&gt; xxx_ 1, Added the difference between EventDispatcher and CallbackList in the readme. Just quote here for your convenience, In brief, EventDispatcher equals to std::map&lt;EventType, CallbackList&gt;. CallbackList holds a list of callbacks. On invocation, CallbackList simply invokes each callbacks one by one. Think CallbackList as the signal/slot system in Qt, or the callback function pointer in some Windows APIs (such as lpCompletionRoutine in `ReadFileEx`). EventDispatcher holds a map of `EventType, CallbackList` pairs. On dispatching, EventDispatcher finds the CallbackList at the event type, then invoke the callback list. Think EventDispatcher as the event system (QEvent) in Qt, or the message processing in Windows. CallbackList is ideal when there are very few kinds of events. Each event can have its own CallbackList, and each CallbackList can have different prototype. For example, ```c++ eventpp::CallbackList&lt;void()&gt; onStart; eventpp::CallbackList&lt;void(MyStopReason)&gt; onStop; ``` However, if there are lots of kinds of events, hundreds to unlimited (this is quite common in a GUI or game system), using CallbackList for each events will be crazy. This is how EventDispatcher comes useful. EventDispatcher is ideal when there are lots of kinds of events, or the number of events can't be determined. Each event is distinguished by an event type. For example, ```c++ enum class MyEventType { redraw, mouseDown, mouseUp, //... maybe 200 other events here }; struct MyEvent { MyEventType type; // data that all events may need }; struct MyEventTypeGetter : public eventpp::EventGetterBase { using Event = MyEventType; static Event getEvent(const std::shared_ptr&lt;MyEvent&gt; &amp; e) { return e-&gt;type; } }; eventpp::EventDispatcher&lt;MyEventTypeGetter, void(std::shared_ptr&lt;MyEvent&gt;)&gt; dispatcher; ``` (Note: if you are confused with MyEventTypeGetter in above sample, please read the "Event getter" section in Event dispatcher document, and just consider the dispatcher as `eventpp::EventDispatcher&lt;MyEventType, void(std::shared_ptr&lt;MyEvent&gt;)&gt; dispatcher` for now.) The disadvantage of EventDispatcher is that all events must have the same callback prototype (`void(std::shared_ptr&lt;MyEvent&gt;)` in the sample code). The common solution is that the callback takes a base class of Event and all events derive their own event data from Event. In the sample code, MyEvent is the base event class, the callback takes one argument of shared pointer to MyEvent. The advantage of EventDispatcher is it has more features than CallbackList, such as event queue. 
Breaks a bit if you use some [crazy lambdas](https://cppinsights.io/lnk?code=I2luY2x1ZGUgPGlvc3RyZWFtPgoKaW50IG1haW4oKQp7CiAgW10oYXV0byYmYSkKICB7CiAgICBbJixlPVsmXShhdXRvJiZmKSAtPiB2b2lkIHsgYSgpOyBmKGYpOyB9XSgpIHsgZShlKTsgfSgpOwogIH0oW10oKQogICAgewogICAgICBzdGQ6OmNvdXQgPDwgIndoaWxlIHRydWUhIiA8PCBzdGQ6OmVuZGw7CiAgICB9KTsKfQ==&amp;rev=1.0).
Do not use unicorn initialization in generic code if you mean to just copy (or move) - you can accidentally call an initializer-list constructor instead. The forwarding reference form needs constraints. The corresponding deduction guide should decay the type, which means it can be simply written as template&lt;class... Ts&gt; overload(Ts...) -&gt; overload&lt;Ts...&gt;; An allegedly simpler form is to remove the constructor altogether and make `overload` a C++17 aggregate instead: template&lt;class... Fs&gt; struct overload : Fs... { using Fs::operator()...; }; template&lt;class... Fs&gt; overload(Fs...) -&gt; overload&lt;Fs...&gt;; This is the form we showcase on cppreference.
When a thread call `process()` to dispatch the queued events, all event listeners are invoked from within the same thread. So most CPU power is used on the event listeners, not the event loop. And this depends on how the architecture is designed, 1, If this is a pure event driven system, then no messages in the queue means the thread is really idle, nothing waste. 2, If there are other tasks to do and events are handled beside those tasks, call `process()` between the tasks. For example, call `process()` in a timer or in a GUI idle message. Then the CPU power is split between the tasks and the event listeners, nothing waste again. In brief, if there are other tasks to do, don't put the event loop in its dedicated thread, make it shares the thread with other tasks. 
I think we shouldn't change the meaning of `std::move`, but maybe use something like `std::destructive_move` for what you want. I think it would have been better to make less strong guarantees on the result but that ship has sailed.
[Yes.] (https://youtu.be/dHTVrIt6gcI)
How about... not using nested namespaces? One namespace per library is all you need. 
Try implementing a generic function that takes 3 parameters of the same type by non-const reference, and efficiently rotates this list one element to the left. ;-) My point is no, this is not something unique to `swap`. Any generic permutation algorithm on a sequence that preserves values but not order and only requires MoveConstructible and MoveAssignable is going to have to assign-to a moved-from value, either via `swap` or using move assignment. And we don't want to require `swap` for all such algorithms as that would introduce inefficiencies. `vector::insert/erase` also will move-assign into moved-from values, though these generic algorithms are not value preserving. And using `swap` in place of move-assignment for these algorithms would be heart-breakingly inefficient when the `value_type` is `int`. Could we make `vector` magic? Yeah, we could. But there is tons of code in the wild that creates custom vector-like objects that would be left out in the cold if we did. The motivation for doing so would have to be incredibly strong. So at the very least, we appear to need moved-from values to be move-assignable-to (in addition to destructible of course). Is that it? No, those are just the most common cases. I've seen other cases in the wild where it is efficient to re-use the capacity of a moved-from string or vector. In these cases although the container is moved into the function, it is typically used as a local scratch pad for further computations as opposed to having its buffer transferred somewhere else. In any event, [this model](https://stackoverflow.com/a/7028318/576911) has proven to be the most useful and practical over the past 17 years since the development of *non-destructive* move semantics. *Destructive* move semantics would obviously be another matter, and could co-exist with and complement our current *non-destructive* move semantics.
&gt; modules standarize public API import/export - no macros like `LIBFOOBAR_API` around class/function definitions That's still outside the purview of the language standard. Symbol visibility for runtime or start-time loading and things like weak linking are not standard and do not have to be related to C++ modules.
Thanks for the explanation. Makes sense.
In semantics and good practices but not in syntax.
This blog is so hard to read. So much "conversation" that it takes too long to actually find the content. 
Thanks - I used to think it is beyond the scope, but apparently it isn't :-(. I added some discussion on the importance of defining ALL the overloads (ouch!). NB: including unsigned ones, I counted 14 distinct types (including not only signed char, unsigned char, and char, but also such rarely-used beasts as wchar_t, char32_t, and char16_t).
You can follow the group's progress here: https://groups.google.com/a/isocpp.org/d/forum/tm/topics. Though it's been almost exclusively group call invitations for the last 3 years...
The kind of use-cases I had in mind was for example a background-threaded renderer. If nothing happens the thread should remain idle, if something happens that requires a new frame to be rendered the main thread sends a message with some info about what happened to the renderer. Renderer renders and sends a message with a frame back. The renderer might have nothing to do for a while and be fully loaded a moment later. Using sleeps or a timer in the renderer thread would hurt responsivenes.
I got your idea now. I think we may add some synchronization primitives (std::condition_variable) to block the event queue processing thread, thus the thread can go real idle when there is no events in the queue, instead of a busy loop which wastes the CPU power. Good point, thanks. 
Ok, let me poke the beehive a bit: &gt; Meson depends on: &gt; * Python3 &gt; *Ninja build system Why not start with that? &gt; the project generation is still better in CMake I wonder what's the point of a project generator/meta-build system that can't actually generate any IDE projects since that's the only valid reason to prefer it over a native build system.
My team is evaluating whether switching to CMake or Meson a large C project. Any thoughts on why Meson would be a better choice than CMake \(or the contrary\)?
Replace with "IOW".
Its support for VS has been improving over time. I did not give it a try lately just cause I do not need it. But I can say that in my experience Meson is really solid. If it does not appeal to you noone forces you to use it. For me it works great.
If you are suing Windows and need Visual Studio support I do not know bc I do not know how good the project generator backend is right now. If it is not a critical feature I would pick up Meson myself. Let me know more about your requirements.
Meson repeats CMake's mistakes, and adds some new ones. Premake is a better option.
That is a totally superficial assesment. With premake can you setup easily...? - sanitizers - tests - installation - switch library kinds - add custom targets - generate source from idls - detect tools and dependencies gor your whole build - switch easily between build paths, prefixes etc - set rpaths at build and install time - detect build flags valid for certain compilers - check if something build, compile and/or link in a machine All these things are required for several use cases such as packaging, portability... no, Premake is fairly limited. It is just that it is easy. But Meson is as easy as it can be for serving complex needs.
IMO if programmer make mistake because of misreading/misinterpreting code it is not only language syntax failure but also editor/IDE failure. Maybe instead of compiler warning on perfectly valid code one should request proper highlighting of such code fragments in one's editor/IDE?
?? So use MSBuild then and later try to compile somewhere else...
Because interrupt handlers can be called at 100kHz+, if your abstraction adds a microsecond then you can see how that adds up.
&gt;I wonder what's the point of a project generator/meta\-build system that can't actually generate any IDE projects since that's the only valid reason to prefer it over a native build system. Well, then everyone would use XCode or MSBuild or qmake, right? No, not a valid point. Wait, I can feel how you could propose, maybe, build2 :\) Just kidding. I am just sharing my experience: it is working great for me. I have also used CMake and I warn people about the superior capacity of CMake project generation. But if you do not need that, as I did not, there are many appealing points for me to use it. If you think that the only valid point is that, just do not use it. I disagree and it is useful for my use case.
&gt; in-house DSL Non-turing complete DSL. Which is a godsend when you have that one person on the team who wants to astroengineer everything and you end up with build definition frameworks within frameworks.
So basically you are agreeing that it cannot handle valuable things from that list such as precompiled headers, sanitizers, packaging and installation. If I have to do whole programming for getting all these day to day things such as detections and adding targets then there is obviously something wrong with the tool for my use case. I prefer to use something that is already serving me well, that is the point of using such a tool isn't it?
So what does Meson give you that CMake doesn't?
Documentation in good conditions. A scripting that does not waste hours from my work, more out of the box targets... did you read my post? It is all there...
and what if you have good programmers on the team?
If all your team is high quality people then you could probably use GNUMake and not feel limited. Unfortunately it has been my experience that this is rarely the case on software teams.
Though that limiting. What if I want to develop in Visual Studio? The idea of build generators like CMake and Premake is to give you the option to choose which build system to use.
It *can* be. To my understanding there is no mention in the standard that the keyword `char` is signed or unsigned version of the type. Whereas, there are specifications for the others. Similarly, there is no mention in the standard that there should be 8 bits in a byte. For pretty much every modern and common situation, there are 8 bits in a byte. It's also highly likely that the `unsigned char` is the default. 
Right, I can see how that becomes a problem in debug builds. But when you work in embedded environments, aren't unoptimized builds way too slow to run on the hardware? If that's the case then an abstraction that's properly optimized away, shouldn't be a problem, right?
That is one of the features. Yes. But not the only one. Out of the box targets and detections for speeding up builds should not be underestimated. And Meson AFAIK has been for some tome adding VS solution generation. What I dnt know is the wuality of it right now. Just give it a try...
What about std::byte, size_t, ssize_t ... ?
size_t` and `ptrdiff_t` (`ssize_t` is a POSIX thing, not a C++ thing) need similar treatment, yes; `byte` is an enum class so it doesn't.
It wasn't committed, so it had to go back and try again.
By "it *can* be", I was referring to the implemented range. Implicit conversion would have occurred between the types, one such way would be a narrowing conversion, but implicit nonetheless. It says a little later on in the same paragraph: &gt; In any particular implementation, a plain `char` object can take on either the same values as a `signed char` or an `unsigned char`; which one is implementation-defined. In reference to your note on `CHAR_BITS`, you're absolutely right. I just didn't feel I needed to specify, the point was that it isn't standardised to exactly 8 bits, though many treat it as such. Thanks for strengthening that point. ... yeah, my bad there, that was a typo. I forgot the negation. I wanted to imply that the implemented that is commonly used is range of `char` = range of `unsigned char`. Thanks for pointing out the typo indirectly. 
&gt; Implicit conversion would have occurred between the types ... Right, but the article is about overloading, and regardless of `char`'s behavior it is always a distinct type.
That's a good and fair point.
&gt; std::pair&lt;T*, T*&gt;::pair(&amp;(*b), &amp;(*e)) Dereferencing `end` iterator? STL (if he is reading) would not allow that.
This is precisely why C++20 is adding [`std::to_address`](http://en.cppreference.com/w/cpp/memory/to_address).
There was a proposal to add a "forwarding operator", which would be equivalent to std::move if it was applied to a local object. Although I don't believe it was actually proposed, it could have been made illegal to use a variable after the forwarding operator was applied. Unfortunately the proposal didn't find favour in any case.
Dat face tho
How can I remove that? It appeara automatically...
Does the std library have nested namespaces? 
Yup, std::filesystem for example
Yes Windows and Visual Studio is a requirement, including linux and mac os X. Our project has modules in C++ and some core components/drivers in C. We also have some 'glue' code in python but that is mostly for test and automation.
So you could try a quick test with the windows backend and see how mature at this point. I know there has been work there. If that does not work just use CMake in that case.
Do you need `typename` here? I thought type aliases were more forgiving about that?
&gt; It can be. It is still a distinct type, which can cause all kinds of trouble (such as preferred overload chosen for signed char argument being f(int) rather than f(char) ). 
To the best of my understanding, with the code provided in OP they will work automagically. 
How do they compare, support-wise? I know Kitware is quite active in terms of supporting CMake and/or fixing bugs, but I don't know for Meson.
The community is active but it is a purely open source project driven by volunteers as far as I know. They are quite responsive in general.
&gt; I know this question may not be 100% C++ related, but I assume this subreddit has the highest concentration of database devs and people that need high performance tooling, so it seemed like the best place to ask. Definitely NOT the case. You'll likely do better at /r/Database/. There is a ton load of disruption currently happening in data persistence, and lots more to come. A lot of innovation is currently happening, and lots of startups trying to become the next Oracle. So, to be honest, I wouldn't expect things to get much better for you, choosing any one technology will lock you in, and you'll get ransomed when they IPO. The only way to avoid this is either never use any non-substitutable feature, or stick with more traditional storage. I've asked WG21 to decide whether to study this domain space more (see http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1026r0.pdf). We'll see how it goes, but if approved, any progress is likely years away, C++ 25 at the very earliest, and even then, as an experimental TS.
At some point the thought crossed my mind, but there are two issues with such an approach: (a) if overloads have to be different depending on size, then writing one for int_other_t becomes ugly (or even VERY ugly - with #ifdefs etc., ouch); and (b) it doesn't handle all those char/signed char/unsigned char/wchar_t/... types which can accidentally interact with supposedly-integer overloads (while approach in OP does handle them). Overall, both approaches are more-or-less-ugly workarounds (each with its pros and cons) for a rather ugly original problem ;-( .
&gt; Maybe instead of compiler warning on perfectly valid code I appreciate my compiler warning on valid code. `if (x = 2)` is valid, but never really what I want. Assigning to a variable and then never using it after often hints at another bug hiding in there.
I’d say: go with cmake. The main gripe with cmake is that a lot of its modules are old and still use crusty techniques that even the end users should not be using anymore. Given cmake’s popularity — it still has problems with maintenance. It’s a mountain of code. But good luck to smaller projects like meson: once they start being serious and support lots of software, they’ll be even worse off unless someone commercially invests a lot of into them.
I don't quite follow -- I know the size of `int_other_t` because it's a compile-time constant and I can use that. Whether it's 32 bit or 64 bit it's examinable and usable; maybe I need an `if constexpr` but most of the time for my purposes I haven't even needed that. I also don't follow how one approach is different with respect to `wchar_t` -- that is some type in the system, maybe the same as an intNN_t and maybe not. Either mechanism will provide an overload, either directly or by converting to the normalized_integral_type. So where's the difference? If wchar_t is an alias for e.g. int32_t on some but not all systems, then I can get an unexpected overload on at least some systems.
Interesting. Googling around a bit, I can't find any reference to this proposal. You wouldn't happen to have a link for the curious, would you?
QMake is a cross platform meta build system like CMake rather than an IDE project file like XCode or MSBuild.
The real solution would be for the build tools (make, ninja, nmake, jom, msbuild) to implement folder dependencies that are more like files and look at modification time. The way, a glob would impose a dependency between the folders it operates on and the makefiles. Then if you change directory contents, the makefiles will get rebuilt by rerunning cmake. 
Yes – the nested `type` being accessed here is `choose`'s, not `disjunction`'s.
Well, I know that, but it is very very Qt oriented AFAIK.
I think they're referring to /u/sphere991's [P0644](https://wg21.link/p0644).
&gt; Technically the name `int_other_t` is reserved since it ends in `_t` but practically this isn't a problem. It's only reserved for POSIX, not for C++.
I could give you that CMake is going to be still the "big" one. So support for CMake, especially since it was adopted by Microsoft and Android Studio and it is the native project format for CLion, it has an advantage here. However, I would not describe using Meson as a nightmare at all. It is already perfectly capable of handling dependency detection and the workflow well enough for me. True, sometimes I spend fixing some rough edges or reporting problems, but with CMake sometimes I found myself sometimes trying to do something that was impossible for me directly because I had to do contortions with its syntax and I could not even understand what was going on with lists vs strings, variable interpolation, \*\_found vs \*\_FOUND vs \*\_NOTFOUND and many other weird animals. In Meson I do not spend time on these things that I can spend on other things, maybe in improving myself things writing a nice wrap file, for example. True, it will be difficult to compete against CMake "enterprise\-wise" but Meson, at its current stage and having attracted critical mass already into the project, it is perfectly capable already. That is my experience, and I did not stop searching just after the first build system I found, you can see I tried a few ones, all with too big warts. CMake is fine, of course. Maybe the most well supported one. But my point here is: do not be afraid of using Meson unless you really need XCode project generation. Visual Studio project generation? I will give it a try when I have some time but there has been work on that before already, so I expect it to have matured. If I am not wrong the guys at GStreamer were working on this.
&gt; First they ignore you, then they laugh at you, then they fight you, then you win. I know what you lads are all thinking here. You come here to laugh and to pretend that rust is an overhyped language that has not lived up to its claims. I get it. It looks funny and all, but it is really childish. Surely, it would be almost impossible to have a list of Python's Friends, because the list would be so long no one would even bother to look at it, however you're missing the mark. I was in the Natural History museum this weekend. You know, the true and first Natural History museum that you not even have to specify the city, but yes, in London. I saw Charles Darwin statue there, and I couldn't help but to think how rust is unique. It is the missing linking between the Combustion Engine and Intergalactic Exploration. We are the Charles Darwin of our age, and we are seeing your finches losing their beaks and dying, and you can't even realise that it is not about you or technology. This is all noise on the evolutionary chain, and rust is the apex predator. But why bother engaging people who have their minds shut. You can't even understand the difference between Universal Types and Existential Types. Every time I come to this sub I feel like a god walking among monkeys. Now I know how our evolutionary ancestors who invented fire felt when they saw their enemies freezing to death. We are not fearful, we are not laughing. We are making history, while fools are grappling around their own foolishness.
Wrong sub
Yeah, it makes tons of assumptions about using Qt, and is generally a complete mess.
Premake is a disaster (I've used it professionally for years). Even leaving aside the fact that it's extremely obscure, it's still not very good. Manually adding every single file is not a mistake. In fact, it's the recommended way to do CMake: https://stackoverflow.com/questions/1027247/is-it-better-to-specify-source-files-with-glob-or-each-file-individually-in-cmak.
Check out [scylladb](https://www.scylladb.com/). It's a C\+\+ reimplementation of Apache Cassandra, but a lot more than just a plain rewrite. It uses a share\-nothing thread\-per\-core design and is fully asynchronous. Both [scylla](https://github.com/scylladb/scylla) and the [seastar](https://github.com/scylladb/seastar) framework it's built on are fully OSS.
IMO Meson is far more humane to use. Yes, CMake is far better than it was before, but it's still a stinking macro language. Meson also generates for Ninja by default and uses tricks to avoid rebuilding programs when the shared libraries changed but the ABI didn't. FWIW GNOME is moving over all their projects to use Meson. 
The way QBS works with tags and generation rules is really nice too. There model of how software is built is one of the best I have seen from a build system.
They are fully OSS, but very complex and not fit for this purpose. Scylla is a wide column store, and whilst it's incredibly fast and scan scale to immense values, it doesn't have the same performance something like aerospike (or really anything cobbled together from rocksdb and raft) has when it comes to syncing, putting and querying K-V pairs (because it's not made solely for that). I know of syclla, it's an amazing DB for many things, but I wouldn't use it for caching. Also, it's a prime example of "open source but not really, pay for some support", compiling seastar is madness. 
The main thing that cmake misses is a warning against old constructs and a switch that makes these warnings an error. They go to the length of not documenting the old cruft in docs for the new versions IIRC, but the software still supports it. I’ve had a change of mind about cmake. Until recently I was very much against it on the grounds that it was under-specified and thus everyone did stuff their own way. That’s still the case, but it doesn’t preclude me from using it correctly. I’m using it for everything now. The docs are decent about exposing weirdness, e.g. `if` docs state the rationale for its weird behavior. Some of cmake idioms are IIRC not documented at all, like e.g. `${${}ARGx}` to access the enclosing function’s scope and bypass the macro string substitution meant by `${ARGx}`. 
Just so you know: I love you all working on this. I seriously do. Better late than never. I’m all for `_Either(A,B)` in C. Heck, I’ve implemented something like it with a libclang preprocessor that turns it into proper C-ese, but getting it into the language would be even awesomer. C++ interop is a good thing; dynamic exceptions have made it tedious. 
Have you looked at [MongoDB](https://www.mongodb.com/what-is-mongodb)? It's pretty easy to [build](https://github.com/mongodb/mongo/blob/master/docs/building.md) if you have a recent C++ compiler + python.
CMake is so strange that even if using for a long time and trying to find the documentation how to do things, I still made plenty of mistakes in if statements, interpolation, blank space handling and strings vs lists. I still could not get around it from the top off my head. In that sense it is a terrible experience. Of course, when it works great and its IDE support is its killer feature in my opinion. But using it is a bit frustrating. As long as you use all these fancy exported targets and all of that, it works great. But it is not obvious at all. Maybe because I am familiar with Python, but it took me a \*remarkably shorter time\* to do in Meson correctly: \- condition handling \- cross compilation \- options management, Options management, for which CMake cache is a terrible as hell to the point I had to workaround it and accept it. But not only me, people in my team could not understand it either. I think I can spend much less time in Meson with the builds and contributing small things, maybe wrap files in the future. I will accept that it is not the most popular build system. But it can do all I want and I hope that some day it can have proper IDE support at least for Visual Studio \(I do not know if this is true yet, maybe \*it is\*\), note I did not try for a while. Maybe /u/jpakkane knows more about the status. XCode would also be great. For the rest, it is already more than decently covered. Of course, even after the CMake rant, I still think it is a very useful build system. I just prefer Meson for the reasons described and will keep using it and hope to improve it on the way.
&gt; maybe I need an if constexpr And with normalized_intergral_type the whole question and the need to use ifdef/if constexpr doesn't arise to start with (="I don't need to care about other_t and its size - concentrating on the logic I have to do"). Sure, "my" method has some boilerplate overhead, but "your" method also has some other boilerplate overhead. &gt; maybe the same as an intNN_t and maybe not. From cppreference: "It [wchar_t] has the same size, signedness, and alignment as one of the integer types, but is a distinct type.". As it is a distinct type - it means that to cover all the possibilities and avoid counter-intuitive conversions/promotions you will have to write f(wchar_t) overload explicity (as well as half a dozen of the other overloads such as f(signed char), f(unsigned char), and f(char32_t) and f(char16_t)), but I (using normalized_... stuff in the OP) don't have to worry about all these cases (normalized_* stuff described in OP will find matching integer type automagically - which will work exactly because of "same size and signedness" from the quote above, so the problem with counter-intuitive promotions won't be able to occur). Overall, it is all about convenience, and if your method works for you - great, but what I am saying is that the whole thing is not _that_ obvious (neither it is _that_ simple). 
I was only at a talk by MongoDB Dublin last Monday on your C++ code as part of the Dublin C++ users group. I came away feeling that you're better than average on the proactive upgrading of old code, but there are a number of unfortunate design decisions in the codebase. You do things like pass STL containers by moved value, instead of passing Ranges/iterator pairs or view objects. And you spend too much time upgrading things by hand instead of automating upgrades via LLVM tooling. I was also surprised to learn that the substitution of Boost with STL was not more problematic. I'd also take some issue with your locking implementation and strategy, but every database person is always going to disagree with every other database person on that, with never a consensus found. Still, interesting talk, send thanks to your Dublin team please.
Why would you return a lock? Smells.
It's also shit.
&gt; I'm staying it's outside it's scope (the same way a physics engine doesn't do the rendering for you). well that's the whole point right here isn't it ? how many people nowadays are using separate physics engine versus the one integrated in their "broad scope" game engine like unity 3D, unreal engine, etc... 99.999% use the integrated approach. It's just so much better from an user experience point of view.
Boost isn"t a single library though. It's a collection of a hundred different libs.
Not all abstractions are optimised away unfortunately. You have to check the asm, even empty functions are sometimes called. 99% of embedded software is not in an Isr and will meet your timing requirements with whatever abstraction you throw at it
It's the one on your medium profile that is being grabbed. It's the same when you post github links ;) .
You can use cpp as extern functions which you can then call into, but I would recommend to use java for the GUI at least. 
It is possible, however then you are on your own. You will have to bootstrap your native code from Java app and use jni often because only absolute minimal amount of Android API is exposed in ndk. However if you used framework like Qt it would be less of a problem.
I like to also specify --schedule-random when running CTest. That way you can catch tests which incorrectly require other tests to have executed. CMake 3.11 moved CTest process management over to libuv. When running sub second tests in parallel people should see better throughput with 3.11.
you could also randomize the order of test cases within the binary if using doctest - check the [```--order-by=```](https://github.com/onqtam/doctest/blob/master/doc/markdown/commandline.md) option (or also for [Catch2](https://github.com/catchorg/Catch2/blob/master/docs/command-line.md#rand))
Much I love IIFE's, simpler would be: { auto item = Factory::createItem(foo); prepareItem(*item, bar); consumer.SubmitItem(std::move(item)); } if(item-&gt;CheckJustOneLastThing()) // doesn't compile 
It depends on what you mean by app. If you mean 'app' in the terms of a gui application, it's possible, but just don't, way too much effort for very little payoff. If you mean 'app' as in a daemon or a service, or a command line argument you run via adb shell, then yes, it's perfectly easy to write a native app like this. 
Aside from the NDK, you can also create Android apps with the Qt GUI framework. You can even get a more or less nice looking UI when you use QtQuick. Futher informoation regarding this topic can be found on their page: https://doc.qt.io/qt-5/android-support.html
Sweet, thanks a ton! I ended up going with a Saturday class that meets 1/week with a 4 hour block for the full 16 weeks.
Wish there was something like this for mruby.
unicorn == uniform
&gt; Try implementing a generic function that takes 3 parameters of the same type by non-const reference, and efficiently rotates this list one element to the left. ;-) Fair. :) What I amended my no-longer-confident statement to exclude reference parameters? :) &gt; vector::insert/erase also will move-assign into moved-from values Into moved-from local variables or parameters? Member variables and array elements are definitely a very different story. &gt; In any event, this model has proven to be the most useful and practical over the past 17 years since the development of non-destructive move semantics. Destructive move semantics would obviously be another matter, and could co-exist with and complement our current non-destructive move semantics. I'd say destructive move is a different case. I don't think what I'm arguing here is really destructive-move, since that also implies a lot of semantics about destructors and lifetimes (that impact RAII) that aren't really important to "warn on some categories of use-after-move." What I'm arguing is maybe acceptable (though you're certainly giving me doubt!) is to have community-accepted rules (e.g. via the Core Guidelines) frequently enforced via "default" static analysis or default compiler warnings for common misuses of moved-from values that are more commonly misused than used correctly. The nice bit about SA or Core Guidelines is that there's a clean way to disable any particular check in code where it'd be a false positive. e.g., for use-after-move of local variables or (non-reference) parameters, if there really is a need to do so, `[[gsl::suppress(xx.use_after_move)]]` and carry on. If the check has to be disabled _all the time_ then it's a bad check; you've convinced me that non-const reference parameters should not be warned about. I'm not (yet) convinced that the rule doesn't make sense as a default check for local variables or (non-reference) parameters.
Why not just take `const &amp;`?
I started a toy project https://github.com/tower120/cpp2android. Project not finished (and maybe will never be), but you can see how it is possible to work with java objects directly from cpp. It relies on using android/java object proxies(wrapper objects that communicate with java part through jni calls). If you want something like this but simpler, look at Signal part of the project. They should work as is. You may copy/past it to your project, or make your own version of this. Example project should compile and work directly from Android Studio &gt;=3.0 without additional build machinery.
&gt; Well, then everyone would use XCode or MSBuild or qmake, right? Well, no, because that isnt cross platform and also IDEs suck for specifying options and other complex behavior, they also aren't, or don't tend to be, reproducible. Sharing IDE project files is a pain too, at least for Visual Studio. I don't want to have to much about in the IDE settings and then commit their constantly changing files to git(Which ones are required and which ones are user local, if any? Now your .gitignore is full of cruft), it's much easier and simpler to use CMake, plus it's more powerful, you can use it to download dependencies(either through native CMake or executing some other script). And adding a new dependency is a snap, just create an imported target and add it to target link libraries, when you try to build CMake will just update everything to work for you, reliably. Plus CMake doesn't need anything else installed, and can already generate Ninja files. And if you don't need any of those features, don't use them? All you really need to build something is add_executable/library, target_link_sources, and target_include_directories. Note: I only have experience with Visual Studio, maybe others have better gui settings, but i still stand by CMake because it's much nicer to use than the VS settings.
Sure. I was replying to the assertion of another person. I totally agree with you. Meson is a similar idea but beefed up and better implemented. On the downsides, you need Python and the IDEs project generation is not as good AFAIK. 
&gt; ${${}ARGx} wait what
One thing that might help that I hae done is you can pretty much keep the event type a size_t internally and use a constexpr hash(e.g. fnv1a) to give the char */char[N] event types better performance when the name is known at compile time(this is common). Along with this, unorder_map can be very fast for integer types. This gives the more readable string names a fast path
This is useful to me, especially having some example code implemented against various bindings to compare. For example, the boatload of preprocessor macros in oolua.cpp is a big negative in my book. I'm curious about what looks like a method call in sol2.cpp with 100+ arguments that doesn't seem to have an equivalent in mo the others I looked at. I guess the fields need to be "wired up" individually, where it's somehow done automatically elsewhere? Being a template instantiation, does this have a significant negative effect on compile times? lua.new_usertype&lt;lbs::basic_large&gt;("cl", "var", &amp;lbs::basic_large::var, "var0", &amp;lbs::basic_large::var0, "var1", &amp;lbs::basic_large::var1, ...
It negatively affects compile-time mostly because internally all of that gets dumped into a std::tuple. The new interface is likely to be initializer_list (or similar) driven, with an entirely new backend to boot. This will allow braced key-value pairs and avoid off-by-one errors or missing comma derps that come up from time to time.
Well, passing `const&amp;` in constructor will force copying every arguments. Passing by value in constructors and then move them into members is always a move, and will support move only lambda.
Visual Studio project generator is fairly good but perhaps not excellent. We do run our full test suite through it so we know it works. We have also gotten reports of people doing development with it on non-open source code bases. XCode is a bit bigger problem. The main reason for this is that its project file format is so terrible that it makes you wish it was XML instead. If anyone wants to work on improving either of these, we are glad to accept patches. This is not a perfect solution, but it's the best we can do with the resources that we have (which consist almost entirely of the free time of contributors).
IME this is going about accomplishing the desired goal of running tests faster (by running them in parallel) in a completely backwards way. There is no advantage in having multiple binaries, what you want is a single binary (so you avoid pointlessly relinking N binaries on change) and a test framework that lets you partition the tests in some way. Because CTest does not require 1-1 mapping between its tests and binaries, you can just register the same binary to be run multiple times, with different arguments. You can even register the tests automatically, e.g. [Catch2 provides some utility scripts in contrib](https://github.com/catchorg/Catch2/tree/master/contrib) and there is always [the quick-n-dirty solution](https://github.com/master-keying/mks/blob/899a2e817702d573a976cc7c1d3e4e97dc434260/CMakeLists.txt#L277-L285). 
What you're asking the user to do is to manually populate a cache. You could easily cache the results of a glob expression and automate the process, so what's the point? How is Premake bad? In my experience it's better than CMake in every single way: much faster, much easier to use, documentation that makes sense, extensible, and so on.
&gt; Manually adding every single file is not a mistake. Yep. At least for large projects with many developers. &gt; [I've seen horrible, horrible bugs shipped as a result of people putting globs in a build system, then not discovering components have been shipping broken for months in some unrelated part of the system because inputs were moved around, and nobody noticed because the build "succeeded". Globs make figuring out what depends on what in large build trees impossible.][1] [1]: https://www.reddit.com/r/cpp/comments/524844/recommend_a_build_system/d7itluo/
Ah, forgot about that scenario. Though wouldn't that incur double copy in case the value doesn't have efficient move constructor?
You know what really grinds my gears? That `--output-on-failure` is not the default (although you *can* set it in an environment variable, thankfully).
Then in that case make a constructor that takes forwarding references, but you must constraint it the right way. Although I usually assume that move is more efficient than copy, since most classes have a generated move constructor.
Never tried it but I think SFML runs on Android, might be a lightweight alternative compared to Qt \(depending on the use case\).
No, that's incomparable. If you use an all-in-one game engine, all the sub components, physics, graphics, sound, etc... follow the same conventions, same code style, have the same documentation accessible from a single place, etc etc - all of this reducing the mental load when working on another part of your project. Using a bunch of independent tools smacked together is the opposite of this.
&gt; since most classes have a generated move constructor. Which is often equivalent to the simple copy (though depends on the codebase of course).
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8k5uql/android_development_possible_with_cpp/dz5rzti/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
!remove
OP, A human moderator (u/blelbach) has marked your post for deletion because it is not appropriate for r/cpp. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/8k3odm/any_interesting_open_source_work_in_the_highspeed/dz5s0qr/,%20was%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
My original thoughts not using unorder_map is that event type should support non-hashable types. But you inspired me. We may treat hashable and non-hashable types differently with template specialization and SFINAE. Also mixed type events sounds interesting. Thanks
Within a function, `${ARG0}` is the variable holding the 1st argument. Now functions create their own variable scope, but macros — don’t. So macros have access to the surrounding scope. But macros also have arguments, that are substituted as strings — they are not variables. Inside of a macro, `${ARG0}` is the string substitution of macro’s argument. But what if you wanted to access the `ARG0` variable in the surrounding scope? You need to write it in a way that doesn’t look like the macro arg substitution, yet has the same meaning. Thus you can use the empty substitution `${}` to get the substitution looking differently. The macro substitutor will ignore it, and it will be substituted as a variable. Thus, in a macro `${${}ARG0}` is the variable `ARG0` in the surrounding function, but `${ARG0}` is the first argument to the macro itself. There is an infinite number of such substitutions of course. You can have: set(arg ARG) message(“${${arg}0}”) This doesn’t match any macro substitutions, so variable substitutions kick in. First it becomes `${ARG0}` and then whatever the first argument of the surrounding function happened to be.
Yeah, I was playing with constexpr hash tables to try and get array like performance with nice identifiers. The mixed type was a side effect of code like template&lt;typename K&gt; constexpr reference operator[]( K &amp;&amp; key ) { auto h = hash( std::forward&lt;K&gt;( key ) ); return arry[scale( h )]; } The hash now becomes a customization point and can really use any operation that returns a size_t depending but from what I understand is that unordered map is really good for integrals but the difference for other types it isn't that far from plane map. there are other flat maps and stuff out there too, google has a dense one I believe that works well. I suspect that because the event count is small one could do a vector of struct { size_t hash; Listeners listeners; }; and get good performance as long as Listeners is a vector like type of maybe a pointer and a size_t. measurements though :)
TouchBar support for MacBooks is my favourite feature ! Also good to see that they finally bring some new build systems to CLion. Many will say that it's not Makefiles support yet \(and they are right\) but I think it's a good info that things are effectively moving, CLion start to be decoupled from CMake, and Makefiles support might come soon! And Gradle is a good choice as Gradle plugin is already complete and well integrated in IntelliJ IDEA, they don't need to make it from scratch.
You see? People try to blame me for the switch but I did not have a choice! :(
I'm also using firefox\+ublock, but on fedora, and the website loads fine. I don't think it's a problem with the page.
there's no difference here - a framework can provide its own interface to access its components.
I never even heard of using Gradle for C++
Same with Google Test: `--gtest_shuffle`. It also has an arg to pass the randomization seed (In case you're that paranoid)
Hi /u/vector-of-bool. I have finally had the time to try out your CMakeRC script! Unfortunately I am getting compiler error “c2026: string too big” when I try and compile your repository with -G “Visual Studio 15 2017 Win64”. Is this a platform you have tried yourself? It works on both gcc 6.3.0 and clang 6.0.1 which is great but I really need MSVC to work for this to be a useable solution for me. If you have any suggestions i would be gald to hear it!
What is the specific use case for these kinds of bindings? Exposing scripting functionality in your app? 
It is true that all/most testing frameworks provide something like filters, catch, doctest,.. and that you can run the same binary with different filters and that you have parallel execution. But this can create additional overhead. Finding names, adding groups together, having extra startup scripts.... You need to care yourself that the expensive tests are in parallel \(extra groups\) This does not simplify and accelerate development, it adds additional burdens, imho. For many projects, in daily work, multiple smaller binaries scale better than one large one. During development, especially adding new tests, but also after after changes. But of course, there are always different ways to do things and what works better for some must not work better for others.