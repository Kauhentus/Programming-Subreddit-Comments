How would this differ from an optional&amp;?
std::optional&lt;T&gt;&amp; would be a reference to an optional that still owns its contents. OP wants an optional that just holds a reference to its contents. In this case std::optional&lt;std::reference_wrapper&lt;T&gt;&gt; would be the best option. 
No, it's not useful without Coroutines being turned on. We do offer `OUTCOME_TRYX(expr)` as a clang &amp; GCC only extension. It provides proper expression semantics for the `TRY` operation.
I think you might get away with including iosfwd instead of iostream in the header. 
For me this looks like overengineering. Simple `std::ostream*` is, well, simple and do the same work even more - there is no code bloat and 2 layers of abstractions to understand the code
I believe the library you are talking about already exists - boost.
I think you mean: get a compile error since std::cout is not a file stream. Which shows that a complete refusal to learn even the basics of the c++ standard library is not an option :-). Edit: Added smiley to make it look a bit less derisive.
I haven't tested Clang with `_CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES`. (The CRT gets indirectly tested through the STL.) I'll take a look today, maybe it's something simple.
I don't think that people would like writing `f(OUTCOME_TRYX(g()));`
This class doesn't meet requirements of InputIterator. And what's with that `bool done`? Just use a pointer, look at the implementation of `std::experimental::generator`.
You are correct in that there's not one (and that one is needed!). The coroutine facilities given by the compiler are only really useful once the library designers craft the classes that make proper use of them. I suspect that this is going to be a very exciting time as coroutines can (should?) change quite a few libraries, like boost::asio and ranges (a tip of the hat to Niebler who has already incorporated coroutines). Coroutines are a big deal.
It's just an example. Most of the actual production code I write using these semantics don't boil down to something this clean (including, namely, the "default" value)
I already thought about that, but the .get() call is ugly. :-/ Using such a construct is an option for my own code, unfortunately I want to use it in a callback of a library interface. A generic lambda would match this construct, but I find it to unexpected not to get the type itself after "dereferencing". The library user shall think about the parameter type as an optional. With this indirection its not longer just an optional. The (user side) code I have is like: auto perform_work = [](std::ostream&amp; log, auto optional_data){ // ... }; I also thought about using a std::unique_ptr with empty delete function. But there is not reason to forbid coping my optional_data. All three solutions look like a hack to me. A std::optional with real references would be a good solution, but this was removed during the standardization process. Nevertheless, thanks for your idea!
**Company:** [Stellar Science](http://stellarscience.com) **Type:** Full Time **Description:** Hiring for multiple positions and will consider across all experience levels. Stellar Science is a computer software company providing leading-edge scientific software for its customers. We develop innovative new technologies by combining quality modern software development skills with scientific expertise in fields such as: geometry processing, image processing, scientific visualization, simulation, and numerical optimization. **Location:** 1. Albuquerque, NM 2. Tysons Corner, VA **Remote:** No remote work available **Visa Sponsorship:** No, only US Citizens **Technologies:** Required: Cross platform C++11/14/17, Boost, STL, Qt; Additional: CMake, git, OpenGL, OSG, Java, Python **Contact:** [Careers page](http://stellarscience.com/careers_jazz.html ) or [E-mail](mailto://employment2@stellarscience.com) 
I agree that functional style programming is difficult to debug, though I don't think it has to be that way. Most debug tools show single values while good functional style programming works across entire data structures. 
How so? 
I've sent a self-contained repro and analysis to the Unified CRT team. It looks like there are only two occurrences, so they should hopefully be simple to fix (I didn't investigate a fix, I just found the root cause). I suppose this could also be reported to Clang as a missing ms-compatibility issue, although I haven't done so (yet).
I think such a 'bill of rights' at least shows that things that are important to a many C++ developers are also considered important by the committee. At the same time, I do think they should 'aim high' w.r.t. what they want to enable. Without ambition, would we have ever gotten r-value-references, move semantics etc.? At least now we seem to be getting close to getting modules, ranges and concepts, which everyone wants yesterday. One way the committee might strive to make life easier for mere humans, is to get rid of some of the more horrid such-and-such-is-UB, no-diagnostic-required bits of the standard. Almost each of those needs to be kept in mind all the time, and if there's more than 7 of them (and yes there are!) then life gets much harder than need be IMO... 
I really loved Herb's working-metaclass-demo and talk. And no, getting that in C++20 is indeed a no-starter! Can you imagine how quick they'd have to be? It'd need to published something like Q1 2020 at the absolute latest, it's still being developed, and the reflection stuff that it depends on is also still being worked on and not ready for a TS... 
Use boost::optional or other optional implementations that can hold references, like this one: https://github.com/tcbrindle/cpp17_headers/blob/master/include/stx/optional.hpp
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [tcbrindle/cpp17_headers/.../**optional.hpp** (master → e416b34)](https://github.com/tcbrindle/cpp17_headers/blob/e416b34c132e05487ef8d7fd197f4513d6535c1d/include/stx/optional.hpp) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkz6wtb.)^.
I feel like several people up and down this thread have contradicted my post without fully considering what I wrote. Even if you can pretty print entire data structures in you debugger (and in fact e.g. gdb will pretty print most/all standard library data structures), that is not the issue. When you use something like ranges, there is no intermediate vector to print, because its lazy. All of the actual transformation happens deep inside the ranges library, and they still happen by applying the entire sequence data point by data point, not by applying each transformation to the whole data structure. Having intermediate vectors is simply slow, there is no way around it. If you can't afford to be slow, then you only have two options. Either explicitly perform your transformations point by point (i.e. with for loops), which is less declarative but also much more transparent to debuggers. Or, use a library which allows you to declare the high level operations you would like to apply to the whole data structure step by step, but then internally executes that code point by point. The latter approach is simply much less transparent to debuggers.
I think you're being overly harsh. While Boost has some super-awesome stuff... it's not perfect. Sometimes you don't want to add any dependency to any other libraries. Sometimes you want more attention to warning-less code than Boost (apparently) has the luxury for. Or perhaps you'd like to use one part of it, without having to add other parts of it. Boost maybe a bit too integrated.
&gt; Can you imagine how quick they'd have to be Yes, 2 years... :) I know this is fast according to ISO standards, that was my point. ISO wastes time on this pamflets instead of considering is there any way they can be more productive... But ISO is politics and ISO members most of their time work on something else... So yes 2 years is plenty of time to implement a language feature, unfortunately it will not happen for C++,
Interesting, thanks. Coroutines are definitely huge. Has there been any discussion of adding standard library components that make use of coroutines? It seems like an obvious addition.
It's not perfect, but many many libraries are very high quality, and many many libraries are header only. Honestly there is almost no excuse to avoid boost pp if you are doing something like a reflective enum (or any homespun reflection). You could literally copy the files to some directory in your build tree (afaik boost pp and vmd are independent of the rest of boost). It makes a huge difference as well because doing non-trivial stuff with the preprocessor from scratch is so incredibly painful. boost PP really makes it surprisingly bearable.
&gt; c) nobody can get fired from ISO, my guess is that people apply for study group... get frustrated due to ISO process or dont have time due to work that actually brings food to their table... So you have this undead work in progress features. I mean Chandler was chair of reflection group for years. It produced approximately nothing. Is Chandler stupid? No he is actually quite smart and knows a lot, but I bet he has a lot of other work to do... 
Boost Circular Buffer: http://www.boost.org/doc/libs/1_61_0/doc/html/circular_buffer.html
&gt; coroutines can (should?) change quite a few libraries, like boost::asio Why would this change Asio which already supports both [stackless](http://www.boost.org/doc/libs/1_64_0/doc/html/boost_asio/overview/core/coroutine.html) and [stackful](http://www.boost.org/doc/libs/1_64_0/doc/html/boost_asio/overview/core/spawn.html) coroutines and which has a [customization point](http://www.boost.org/doc/libs/1_64_0/doc/html/boost_asio/reference/async_result.html) for seamlessly integrating arbitrary completion mechanisms into initiating functions?
Actually you are correct, thanks for pointing that out. I just meant that there are some instances of code that are very completion-heavy, and that those will benefit (in terms of clarity) from coroutines.
Is [`std::experimental::observer_ptr`](http://en.cppreference.com/w/cpp/experimental/observer_ptr) close to what you're looking for? Rather than having `optional` semantics, it has single-object pointer semantics.
I would add that he's not "just" "quite smart", but he's smart and knowledgeable as hell. But I think you bring up a very valid question: &gt; I mean Chandler was chair of reflection group for years. It produced approximately nothing. I would be very curious as to why that may be.
Pointers as "object which may or may not exist" is a semantic I've seen a lot of C++ design gurus discourage in the last several years, especially with `optional` being standardized, and I see little reason not to continue that trend. 
Thanks for the explanations &gt; "Inject this code at the current scope". Will I be able to return this from a function though? Say those 3 lines are repeated all over the place, how would I abstract over it?
I think you do have a valid opinion (well, every opinion is valid I guess, but you do also have some points). Try to be a bit less sarcastic, maybe you'll get less downvotes. +1 for me, not because I 100% agree, but because I think it's good to hear all opinions.
The ring buffer seems like a weird example, because it seems to run completely opposite to the first two points - almost everybody needs to implement one sooner or later, so the committee *should* codify existing behaviour and produce a high-quality ring-buffer for everyone to use. The fact that it seems trivial to me suggests that its a great candidate for standardization, because the major design decisions already have a good consensus solution.
2 years may seem like a long time, but... the ISO/C++ work is all volunteer work, 3 weeks per year. And they also have to combine the Concepts stuff, the Modules stuff, etc. etc. .... and the devil is in the details, after all.
They wouldn't be repeated all over the place, they are generating case statements for the values of an enum.
That's fine with me (using gcc7), thanks!
*Ctrl+F "C++"* &gt; 0 match Why is this here?
And they've filed this in their database as MSFT#13017256, with a fix planned.
No one stops you from using this clang fork today and spread metaclasses all over your code, but I guess you want stability and portability and getting all vendors to agree on a feature at that scale takes time. There's no way past that.
Agreed and removed.
Yeah, why can't we use raw pointers as optional references? They model the concept perfectly, and don't have any other use in modern code.
I don't know dude, I used boost pp before, I didn't like it. It's too complicated, and it leads to really noisy error messages if something goes wrong. For instance look at this thing: http://www.boost.org/doc/libs/1_62_0/libs/preprocessor/doc/ref/seq_for_each.html It lets me iterate over a list, but only if the list is formatted in an awkward way: `#define SEQ (w)(x)(y)(z)` There's also all this extra stuff, about "auxiliary data". It's also not easy to iterate over a sequence twice, at least in my experience. In the example they show, you go from `(w)(x)(y)(z)` to `w_ x_ y_ z_`, but there's no way to go to like `(w_)(x_)(y_)(z_)`. You can go to `(w_) (x_) (y_) (z_)` but the macro will put spaces in between the items and then it's not a legal sequence from then on. That's really frustrating. For a while, I would use a thing called "map macro" from here: https://github.com/swansontec/map-macro This thing is very small and self contained and IMO superior to the `BOOST_PP` things. And it doesn't require that crazy `()()()()` syntax. But there's still problems with the map-macro. The biggest problem is that it leads to really noisy error messages. The thing has a built-in limit of 365 items in a list, but the thing is that because of how it is engineered, it always expands at least 365 times even if the list only has three elements. In newer versions of gcc and clang this doesn't lead to as much clutter, but in older versions you would get error messages that are like 10 times as long. The other problem with map-macro is that it doesn't work on MSVC. MSVC has a different and nonconforming implementation of `__VA_ARGS__`. In principle it can be fixed, there are various work arounds to the MSVC preprocessor bugs that are explained on stackoverflow. But the map macro is complicated enough that I wasn't able to do it in the amount of time I was willing to work on it, just working from the SO examples. In `visit_struct` I ended up merging a patch from a guy who used a really straightforward, but verbose, implementation of map macro. https://github.com/cbeck88/visit_struct/blob/master/include/visit_struct/visit_struct.hpp#L162 In this version the (repetitive) dispatch code is generated by a python script. https://github.com/cbeck88/visit_struct/blob/master/generate_pp_map.py Seriously, that was like the best patch I ever got from anyone on github. &lt;3 I now use this macro in every project where I had been using `BOOST_PP_SEQ_FOREACH` and all that other nonsense. It may not be pretty, and it may not have a shiny boost label, but it does exactly what I want and works on every platform, and doesn't generate clutter in the error messages. And I don't have to include many thousands of lines of `BOOST_PP` stuff. I'm sure there's large projects where you really need the full power of `BOOST_PP` but I never ran into any such situation myself yet. Point is, I don't think you need to jump for `BOOST_PP` just for home-brew reflective enums or whatever. I do agree with your bit about so many boost libs being reinvented over and over: filesystem, datetime, optional, variant, any, hash, PP, fusion, ptree, interprocess... I have seen a lot of projects that reinvent some subset of those things, it is sad, but usually people have their reasons and did it reluctantly, not out of irrational hatred of boost. But also, most boost libs are not perfect -- they are battle tested, but they also have flaws. Sometimes the flaws make them unsuitable. And sometimes boost libs are really crufty, especially the ones that want to support like ancient buggy compilers that are mostly irrelevant now. Once there's too much ancient support code, it means that no one other than the original guy or guys is going to pick it up and carry it forward, they'll just roll their own thing. I'm so much happier to have a map macro where I know exactly how it works and what it's going to do, than to use some really large thing I only partly understand and can only partly read. And not to get a bunch of spam about it's cryptic internals every time something goes wrong.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [cbeck88/visit_struct/.../**generate_pp_map.py** (master → a409c18)](https://github.com/cbeck88/visit_struct/blob/a409c18cf800595d14f2d2bc7eacd772048863ba/generate_pp_map.py) * [cbeck88/visit_struct/.../**visit_struct.hpp#L162** (master → a409c18)](https://github.com/cbeck88/visit_struct/blob/a409c18cf800595d14f2d2bc7eacd772048863ba/include/visit_struct/visit_struct.hpp#L162) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkzf7qu.)^.
It probably wouldn't take very long to write up a class that behaves identically to a pointer but doesn't have an operator&lt;&lt;.
Or my ts:: optional_ref from type_safe: github.com/foonathan/type_safe
If they wanted to: #define try(...) OUTCOME_TRYX(__VA_ARGS__) f(try(g())) 
If you compile with `/EHsc`, the compiler assumes that `extern "C"` functions can't emit exceptions, and optimizes accordingly. In practice for sane code, this is true. The STL uses using-declarations to drag `::meow` into `std::meow` - there are no wrapper functions (excluding various overloads for math stuff etc.) so there's no opportunity to mess with `noexcept` specifications.
Boost Circular Buffer - allocates and manages the memory. This is not the same as ring view which accesses a collection in a circular fashion. Of course this raises the question why Boost doesn't have a ring view
boost pp supports multiple data structures. The reason why seq is the most ubiquitous is because on a compiler without variadic macros (and I think they were not part of the C++ standard until 11), it's the only way to have an arbitrary length sequence without explicitly specifying the length. However, you can treat this purely as an implementation detail. I've never written out `(x)(y)` by hand, ever, and my users don't need to do it. I convert from variadics into sequences at the entry point of my macros, it's trivial: #define MY_MACRO(...) BOOST_PP_VARIADIC_TO_SEQ(__VA_ARGS__) MY_MACRO(w,x,y,x) // expands to (w)(x)(y)(z) The transform you are describing is an included macro in BOOST_PP. #define ADD_UNDER(s, data, elem) BOOST_PP_CAT(elem, _) #define MY_MACRO2(...) \ BOOST_PP_SEQ_TRANSFORM(ADD_UNDER, x, BOOST_PP_VARIADIC_TO_SEQ(__VA_ARGS__)) MY_MACRO2(x,y,z) // becomes (x_)(y_)(z_) Because of boost pp, it was nearly effortless to support creating reflective enum classes with nice user facing syntax, where the intiializer for each element was possible but optional (like in a real enum). In 5 minutes I had found how PP handles branching, and that they had a way to quickly test if something was a tuple (something in the format `(x,y)`) or a normal token. With boost it would have taken hours to figure out how to do this and I probably would have just dropped support it for it (like the author of the current thing did). Anyway, it doesn't seem like you actually dug too carefully into BOOST_PP or even read the documentation carefully, since your concerns are addressed there and I had no trouble with it. Maybe you should give it a second shot? I'm planning a blog post where I explain my smart enum and how exactly I leverage BOOST_PP, maybe you would find it interesting. (Note I can't speak to the MSVC issues but it looks like BOOST_PP is aware of this and took it into account while writing so I don't think it would be very difficult. That's another great thing about boost; they look at all the major platforms. If I was writing a lib, I would never bother testing it on MSVC).
If I remember correctly (either from cppcast or somewhere else), Gor Nishanov said recently that there will be another proposal (maybe TS) with library generators and other tools built over coroutines. There is apparently a need to not go too fast and have enough xp with using the language feature to be sure the committee don't end up with something that is not good for most usage. 
Are there any good references you'd recommend (no pun intended)? Would you disagree with [this advise](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#notes-1)? And what about when memory use is constrained?
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [isocpp/CppCoreGuidelines/.../**CppCoreGuidelines.md#notes-1** (master → 82755da)](https://github.com/isocpp/CppCoreGuidelines/blob/82755da67988a356d08a483d6984b348c3e99a27/CppCoreGuidelines.md#notes-1) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkzh8ci.)^.
That's interesting, maybe I didn't dig enough into BOOST_PP. Would enjoy reading your blog post!
&gt; I know those coming from Rust-land feel amazement how long it has taken C++ to replicate Rust's Result&lt;T, E&gt;, but I'm very sure ours is enormously superior to theirs already simply by doing so much less because it doesn't need to. Factual backing to this would be welcoming. Otherwise it just sounds as opinion. &gt; Taking time to get design right is one of the big things which continues to separate C++ from the upstart systems programming languages. Long may it continue! Not my experience. And, by the way, I see Rust's evolution process much more open and pragmatic. There are long discussions in their RFCs, and I have never seen their level of discussion in programming language theory on C++ lands. Also, C++ is just trying to catch up in many fronts, meanwhile preserving C compatibility.
Note they cite `optional` alongside the other options. Many online sources don't point the way towards `optional` instead of pointers because many of those sources were written before `std::optional` was confirmed to be part of C++17. 
Why do so many people try to write "simler" enums using macro and stuff? 1. To iterate over a simple enum just add a `count` named last enum value and write `enum_range&lt;T&gt;` function using it. 2. Conversion to/from strings may be done writing `to_string`function by hand (and using compiler warning about switch over not all enum values to check). It seems to be somewhat redundant, but often you'll need different names than the ones in enum (e.g for displaying you'd probably like to add translation, for serialization you'll need different names for compatibility). 3. For flags it's probably better to use `enum_bitset` - basically `std::bitset` with `operator[]` using enum values instead of size_t. 4. You can't forward declare this kind of "enum" and IDE often will go nuts about basically anything declared using macro (especially through BOOST_PP) Of course, being able to do these things automatically is pretty great, but without reflection it's not worth the trouble, I think.
 I know this is not your job in a sense that you should not know about every existing library, but did you compare it to Google StatusOr?
FYI, `std::ostream &amp; out = log.value_or(std::cout);` should work (mainly because the reference side of things is handled by `reference_wrapper`).
it's similar to the properties and $classx example in the paper. totally possible.
&gt; And no, getting that in C++20 is indeed a no-starter! Can you imagine how quick they'd have to be? Pretty sure some dedicated people could bring up a whole language in this time frame. But they also don't have some weird requirement like "stuff has to be implemented in at least two compilers".
When C++ gets pattern matching (due to playing catch up, not because it's in the process of finding the best design), maybe then, such data structures become more usable in it.
Boost *could be* the library they are talking about, there is nothing actually official or elevated about the boost library's usage over joe schmoe's library. Until Boost is in some way officially endorsed it's just the *most common* choice in the lack of official direction.
&gt; Adding everything and the kitchen sink to the standard makes the language seem impossibly complex and imposing So does lacking something as simple as IP networking in the core language. I'm with you in methodology though, why not officially adopt some things like boost.asio as the official networking library and have it ship with compilers that want to target certain "feature levels". Just because you don't need something programming a microwave doesn't mean the language can't have an official solution or recommendation for it. Or they could create a subgroup for these and leave full committee to steer the core language.
That's UB.
&gt; compared to Rust's `Result&lt;T, E&gt;` which has an enormous number of not well thought through member functions What are you *talking* about? There are only [16 member functions](https://doc.rust-lang.org/src/core/result.rs.html) in the core type (plus separately some trait implementations, and a few variants of unwrap based on type constraints): * State accessors (2): `is_ok()` and `is_err()` * Conversions to `Option` (2): `ok()` and `err()` * Reference casts, which would have no C++ equivalent (2): `as_ref()` and `as_mut()` * Transformations (6): `map()`, `map_err()`, `and()`, `and_then()`, `or()`, and `or_else()` * Iteration (2): `iter()` and `iter_mut()` * Unwrapping (2): `unwrap_or()` and `unwrap_or_else()` That's it. What's not well thought through? What's excessive? This strikes me as a pretty strong model to follow. The thing about `Result&lt;T,E&gt;` in Rust is that it takes almost no time to explain to people - it's either a `T`, which is a truthy state, or an `E`, which isn't. Done. All the member functions do what they sound like they do, have simple implementations and meanings, and yet are pretty powerful - we get a monad in two types. &gt; Fewer member functions, less type sugar, less complexity. Even simpler. [...] Neither this Result nor Expected have repeated that mistake, and I expect that both will slim even further if they are standardised. Less is more! It doesn't seem like you understand the Rust Result type. You can't really get simpler or less complex, unless you just choose to not provide functionality. And just from browsing your documentation and github implementation, it seems like you're going for both *far* more complexity (having two different class templates, why? And all sorts of policies that I'm not going to try to understand) and less usability (not providing anything but the accessors and unwrappers). I'm also not even sure that you actually have fewer member functions than Rust does - it's quite difficult to actually find them, but your `result` has at least 7 member functions that just check state. So yeah, I have no idea what you're talking about, sorry.
Pretend it's expensive to initialize.
I think you greatly underestimate the amount of work required to polish features (especially language ones) to a point where they are standard-quality. It does not only need to "work", it needs to be so good that we know we'll want it forever and without any backwards-breaking tweak. That's _really_ hard to achieve. All of this is predicated on the fact that once something is in the language, we can't take it out. In fact, one of the very reasons why we want a bill of rights is because that may also come with some responsibilities on the user side, which would allow us to break unreasonable constructs with more confidence. You took the address of a function in std::? Too bad!
I think what you wanted is `observer_ptr`. It is a type safer wrapper over a simple raw pointer, aka the world's dumbest smart pointer. http://en.cppreference.com/w/cpp/experimental/observer_ptr. You can find the whole implementation in the proposal (which you can find by googling). It does not overload &lt;&lt; as its a pure proxy. I use this in my codebase because we are transitioning from older C++ so there are some owning raw pointers and some non-owning ones. As I find non-owning raw pointers I convert them to observer_ptr (though I call it view_ptr to make it shorter). As a bonus observer_ptr does not allow pointer arithmetic nor comparison to literal 0.
optional in C++ 17 is always owning though. raw pointers in design guru C++ is always non-owning. So there is no intersection. In fact the argument that the optional, non-owning semantic is already taken by raw pointers, is exactly one of the reasons why `std::optional` doesn't support references. (Note: the reason it doesn't is because assignment is murky, but my point is that this tipped the scale towards simply not supporting references, as opposed to forcing a conclusion about assignment).
First thing that stands out to me: use perfect forwarding everywhere where you're just passing along `Args...` right now, e.g.: ``` template &lt;typename delegate_t, typename... Args&gt; void StoreJobInfo(Args&amp;&amp;... args) { detail::delegate_size_checker&lt;sizeof(delegate_t), BufferSize&gt;::check(); new(m_buffer) delegate_t(std::forward&lt;Args&gt;(args)...); } ``` The second part that stands out to me is that I wouldn't bother with all that complication. A lambda already captures its arguments; there's not much need to have your own `args` as well. Just bind lambdas. Let the lambdas deal with all the argument binding needs. That'll collapse down a bunch of your code. Thirdly, I'd just drop all those overloads for various function types. Let the user deal with that. This is especially useful as you're missing a few bazillion overloads to deal with different member function types (`const` `volatile` `rvalue` functions anyone?) and the current 4 variants of Microsoft ABI (`__stddecl`, `__fastcall`, `__cdecl`, and `__vectorcall`) and so on. Fourthly, the explicit `char buffer[]` is incorrect, as it's missing alignment. Use `std::aligned_storage` instead. On Intel CPUs it'll likely Just Work(tm) with your code but with efficiency loss; on other architectures, that code is very likely to raise `SIGBUS` signals for unaligned value access. The `_detail::Delegate` type may be a little expensive. I feel like you'd be better off just using `std::function` everywhere and - if that turns into ap roblem for you - replace just that with a complete and peer-review fixed-buffer replacement. Even if you plan right now to replace it, I'd save that kind of optimization until the very last thing you do.
Boost implements `optional&lt;T&amp;&gt;` using a single `T*`, so it doesn't have any overhead, but can seemlessly replace a unique\_ptr or shared\_ptr. 
Good bot. 
Any sort of unified syntax for heterogeneous computing will be great for HPC. Will it be possible for applications to run on any sort of combination of gpus/accelerators/memory paradigms to work in a "plug and play"-like manner? 
I think the bigger detractor is that many people consider Boost "bloated" or "too complex". I mean, sure, it probably blows hard having to compile boost yourself, but for those of us who can simply `auto apt install libboost-dev` it's basically an extension of the standard library. 
As @Xirema said, it is a toy example. But in real application, if you are developing a library and you want to reserve changing the default `std::cout` to something else without breaking binary compatibility, you can't use 'default value'. 
&gt; it needs to be so good that we know we'll want it forever and without any backwards-breaking No. :) What this logic is missing is that delaying feature is bad. And the fact it is hard to fix it still does not mean that you should not ship feature that 95% will work find now and not in 3 years. And again ISO has throughput problems, if this can be fixed or not IDK, but i doubt spending time on this vague mission statements helps. Wrt bill of rights: sounds like generic PR useless pamphlet. If you want to explain how wrong I am : give me one specific difference it will make in development. And I do not talk about feeling good, etc. How will code nihilists ;) write today be affected by this. 
Eh, it's not too hard to compile boost. Cross-compiling it so that I could use it in cross-compiled windows binaries was a pain.
The problem isn't how hard it is to add boost or boost modules to your current project it's that there is the language gives no direction to use boost, poco, libuv, glib, qt, or any other framework that supports networking and you end up with 20 different types in 20 different libraries to represent a TCP connection. It's one thing to have a non perfect library the standard option but it's another to have 20 non perfect options that are all different.
Not currently. It's not entirely obvious what you would be able to do with a template. If you have good ideas, I'd love to hear them.
The instructions to build Chromium on Windows can be found here: https://chromium.googlesource.com/chromium/src/+/master/docs/windows_build_instructions.md They haven't been updated since the default compiler changed to clang but I suspect that no updates are needed. Some parts of VS are still needed (the linker, for instance) but the free version of VS works. I installed VS 2017 Community edition on my home machine and then set GYP_MSVS_VERSION=2017 to tell the Chromium build system to use it. Then go through the steps listed at the link above. The right version of clang-cl will be downloaded along with many other required packages. You don't need to go to llvm.org/builds, and doing so will not be helpful, actually. Source: I work on Chrome for Windows
This twitter thread covers some of the reasons: https://twitter.com/marcaruel/status/892017367454273538 
Thanks for the shout-out. I would agree that I've had a great experience working with the VC++ time. I do my best to provide high-quality bug reports and they have been very responsive and helpful. Regarding strict aliasing, Chromium doesn't build with strict aliasing enabled on Linux so it seems unlikely that that is the reason for switching to clang on Windows.
Yeah, it's an allocating container. I just noticed you had "or ring buffer" in parenthesis, so thought it would do.
If you havnt seen it yet, the ACCU video has been posted: https://youtu.be/6nsyX37nsRs
So, that's it then, we have a proof that no such statement will ever appear more than once, so there's no need to abstract over it.
You mean replace the reference with a pointer and null it out to signal the end? That's certainly a possibility. Is it as clear? Probably so in fact. As for the rest of the iterator, it's all simple code and provides no benefit in a tutorial on how to write the generator itself.
&gt; And again ISO has throughput problems You say this as if it were a well known truth, but it's not. In fact, some people think we're going too fast, and that can be argued very reasonably. Look at your coworkers; do they know C++11 and C++14 very well? I'm sure they are not complaining that we're moving too slow. Experts may complain, but they're just that: experts. &gt; If you want to explain how wrong I am : give me one specific difference it will make in development The paper, [P0684r0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0684r0.pdf), gives an example of this: &gt; Let's consider the simple case of `std::accumulate` and Peter Sommerlad's p0616. As specified, `std::accumulate` is O(n^2) when invoked on anything with an O(n) `operator+`, like `std::string`. This is because the accumulated object is copied at each step, rather than moved - by specification. The only code anywhere that can be broken by making changes to that specification is code with user-defined types where move is not an optimization of copy - and yet we are fearful to proceed. A "safe" deployment plan for this issue would likely require two language versions: one to introduce a `std::accumulate` variant that explicitly chooses the existing copy behavior, and a second to change the default - and with such a plan we are still left with both versions indefinitely. Or, we can collectively act boldly and say "Anyone broken by this change deserves it", which is likely true. Having a bill of rights for users does not help for the above issue. Having a bill of responsibilities does, by somehow codifying that if you rely on that sort of detail, we will screw you and you'll have deserved it. If you spend even just one hour in LEWG or LWG, you'll see that that sort of question comes up all the time, and often we just kinda do what seems right, but we don't have clear cut guidelines to act.
Another example I just thought of is [`lock_guard`](http://en.cppreference.com/w/cpp/thread/lock_guard) and [`scoped_lock`](http://en.cppreference.com/w/cpp/thread/scoped_lock). `lock_guard` came first and was not variadic. And then we wanted to make `lock_guard` variadic, but we could not because it would have been an ABI break. Oh my, you should have seen the discussion that this sparkled in LEWG. We lost a bunch of LEWG time trying to discuss solutions and alternatives, and we ended up adding a new class, `scoped_lock`, which is exactly the same as `lock_guard`, except it's variadic. If we had clear guidelines on doing ABI breaks, perhaps it would have been reasonable to wait until the next scheduled ABI break and then do the right thing, which was to just make `lock_guard` variadic. This is just from memory, and I've come across other situations where having clear guidelines of what we can and can't break would have been a time saver, or would even have produced a significantly better design. I truly feel like this is addressing one of the most important issue the committee is facing these days, and I am grateful to Titus for pushing this initiative. 
&gt; If we had clear guidelines on doing ABI breaks, perhaps it would have been reasonable to wait until the next scheduled ABI break and then do the right thing, which was to just make `lock_guard` variadic. Oh but wait! Changing this will also break someone doing some crazy SFINAE to detect whether `std::lock_guard` is a variadic template or not! Oh noes! See, there again having clear guidelines would allow us to say "we don't care" without any doubt. Should I keep going?
Are you sure? I was under the impression it used a bool "initialized" and an adjacent data block. One of the requirements was that the data was not stored on the heap.
Dancing.
It's a pointer to user-supplied data, not owned data. Like a reference... ;-]
If I could use reflection to instantiate a templated type or function with another type that could be nifty. I could use it for testing - pass in "type" lists for the templates to test and the types to parameterize them on. I could see this being useful for benchmarking different container implementations.
Default arguments are bound statically to virtual functions. If you want defaults for virtual functions, this is one of the techniques available. Otherwise (if you use default arguments) you risk having a virtual function call with the default from the base class.
I think one reason is because the `count` value pollutes the API, since it can be passed as a valid enum value all your code needs to cater for that possibility.
Edit: WARNING -- contains failed thought experiment. Enter at own risk. I don't know; that's an interesting question. How would that work? The above is clearly not a string: o.default_value()$ and o.name()$ are obviously evaluated. But that raises the question: if you're going to pass that injected statement around, whence "o"? How and what would one bind to the o in the new context. Would it be meaningful to do so? What would the diagnostics be if you got that wrong? I think it's likely that you cannot currently pass around an injection expression (or whatever they call it). That said, if such issues were worked out, I don't see anything barring it from being a future improvement. That said, consider: constexpr void make_case_statement(auto const &amp;E, auto const &amp;o) { constexpr { -&gt; { case o.default_value()$: return E::(o.name()$); } } } // ... for (auto o : $E.objects) if (!o.default_value.empty()) make_case_statement($E, o); No idea if that works, but I don't see a reason why it shouldn't. EDIT: No wait, that would inject the case statements into make_case_statement, and not into the enclosing expression. It wouldn't work.
Of course, boost::asio doesn't need to change in response of C++ coroutines. However, an asio-like **standard** library would definitely need to support C++ coroutines and not provide a different, competing, mechanism. This means that boost::asio cannot be standardized as-is, but would require some rework.
&gt; whilst I like Scot Mayer as much as the next guy Effective C++ is old, half of that book is now irrelevant and half could be better thought Got a second edition of Effective C++ here. Thought I'd check how much of it is irrelevant. Of the 50 items listed, I'd hazard that the only ones that are now obsolete are the ones regarding new and delete (in as much as you just plain shouldn't be using them). Perhaps six or seven of the fifty. So between ten and fifteen percent, rather than the fifty percent you suggest. A further few could be expanded on; the section on "Constructors, Destructors and Assignment Operators" needs expanding to include the newer constructors, but they're by no means wrong. A lot of it is still very relevant, and as for "half could be better thought"; I've no idea what your complaint is. Perhaps you mean they need to be updated to account for new language features? That's true of a small number of them, but a lot less than half, and what he wrote is still true and still helpful and still will make people better programmers. That book does already exist anyway; Effective Modern C++ should be read after reading Effective C++. I disagree with your assessment; having looked through Effective C++ over the past few minutes, I find it is still a valuable read, skipping over the memory management pieces. For someone with very little C++ experience, the potential mistakes it highlights are excellent learning material; I see people still making those same mistakes today. If they'd read this, they wouldn't.
Many thanks Stephan
Thats it! Thank you very much!
This claim is false: &gt; convert our generator to provide iterators so it acts like a normal C++ collection The iterator won't work with standard algorithms.
I would augment #3 and #4 to suggest the committee and STL maintainers take account of non-optimised code when considering APIs, behaviour and implementation details. To illustrate by way of example, consider a std::vector which you reserve(), push_back() a number of ints, then read them back and sum them; then compare it to a hand-rolled simple version. With optimisations enabled, the results will be very similar, but disable them and/or link against the debug runtime and it falls apart very quickly: type /Ox /MT /Od /MT /Od /MTd my_vector_index 1.00 2.24 2.48 std_vector_index 1.07 5.08 12.75 std_vector_iterator 1.04 6.27 16.71 This makes development much harder, especially when working on interactive products where performance affects the usability and outcome of the code (eg. in games). It's also excessive, the hand-rolled version slowed down, but only by a factor of 2-3x, not 16x. This is also just the tip of the iceberg when it comes to "Debug Matters". I've worked on projects where the code will no longer compile without optimisations because we hit hard limits in the compiler, linker or symbol generation. You end up with executables that are too large to operate on the target platform. You end up with enormous compile or link times that are comparable to optimised code, just because of the sheer amount of code generated. There will always be a cost for running unoptimised code, but using standard library features shouldn't exacerbate this to excessive degrees, as with the 16x slowdown listed above.
&gt; (unless you want to save declaring a string literal with the same name as the class, I guess) yes, that's almost always what you want to do in my experience. &gt; You can also just write them out if every time I "just" did something that the language was able abstract for me I'd be writting tons of very small amd64 assembly snippets.
I looked at the code quickly. The synchronisation of the queue type seems to be that you lock the whole queue on each read and write. Unfortunately that solution doesn't scale at all, in terms of performance. Try some throughput benchmarks, increasing the number of reader and writer threads, and I would suggest that you will see throughout falling off a cliff very quickly. Libraries in this domain (Intel Threading Building Blocks, FastFlow http://calvados.di.unipi.it, Boost.LockFree etc) implement queues which do not lock the entire data structure on read and write, only those elements which are unsafe to access at that time. Clearly this becomes more complex. http://www.1024cores.net has some nice articles on some of the techniques used, if anyone is interested. In summary it is likely that someone building a multithreaded application passing data through such pipelines is going to be keenly interested in throughput, and this requires a specialist implementation. 
They're not "confused" about the goals. Library design and development is absolutely a goal. One of the biggest wider community complaints about C++ is lack of standardised networking .
MS VC++ implementation has &lt;experimental/generator&gt;
I program for shits. Every last one of my customers is an absolute shit.
Engaging aggressively compilation time and build system problems
just thought of another use : `operator dot`-lite. For instance it could be nice to implement a matching class which does what's it's told to do when its implementation is here, and does nothing if not. e.g. $can_be_optional foo { int bar(); }; would also create a specialization of std::optional&lt;foo&gt; which adds optional&lt;foo&gt; { int bar() { if(impl) return impl-&gt;bar(); else return {}; } }; 
They don't write std vector. Your std vector has debug checks; bounds checking, iterator invalidation. Your handrolled probably just corrupts memory in debug if used wrong. Your std vector probably has compiler switches you can use to strip those checks. They are not mandated by the std. 
As I said, I don't see the value in providing that boiler plate code in this tutorial -- it's just distracting. I'd much rather link to somewhere that describes iterators in more depth. Do you have a suggestion?
&gt; It's one thing to have a non perfect library the standard option but it's another to have 20 non perfect options that are all different. Is it really a problem ? Look in other ecosystems, do you know how many ways there are to open a websocket connection in JS or ruby ?
std::optional::value() throws when there is no value, dereferencing a null pointer results in undefined behaviour.
&gt; I've worked on projects where the code will no longer compile without optimisations because we hit hard limits in the compiler, linker or symbol generation that's the problem of MS limiting libraries to 65535 exported symbols. It kills any meaningful use of `variant` in a library since just having `std::visit([] (auto&amp;&amp;...) { }, variant1, variant2, variant3);` makes it blow up. Just checked and this: #include &lt;variant&gt; #include &lt;vector&gt; #include &lt;string&gt; class value; using variant = std::variant&lt;int, float, char, bool, std::string, std::vector&lt;value&gt;&gt;; class value : public variant { }; struct visitor { template&lt;typename... Args&gt; void operator()(Args&amp;&amp;...) { } }; int main() { visitor vis; variant v; std::visit(vis, v, v, v); } creates a ~1.3 megabyte .obj on MS windows with more than five thousand symbols. It's sheer madness. I wanted to try with 4 values passed to std::visit but the compiler bails out with `fatal error C1202`. I really think `variant`, `optional`, &amp; other metaprogramming toolbox tools should just be compiler intrinsincs.
I don't think it is actually. Defining a `try` macro would be UB as it overrides the `try` keyword. But function macros only expand if there is an opening bracket immediately after the function name, so: #define try(...) ... try // does not expand try (foo) // does not expand try(foo) // expands So technically speaking, not UB. But sailing very close to the wind!
Google StatusOr is fine. It's very similar to what some Boost libraries use internally, and during the peer review some people felt the v1 library needed to look a lot simpler and more like StatusOr, and I believe v2 has thrown them a lot of meat on that. Google StatusOr is not designed around STL principles however, and there is a huge gap between a STL vocabulary type and a type one quickly throws together. To put the difference in perspective, you'd be finished a StatusOr in a workday, whereas Outcome v2 took me several weeks to implement. Big difference.
Am I the only one hearing **Lisp** (or maybe homoiconicity) screaming in my head? Not that it's a bad thing, but we've come full circle now... 
Firstly, the Outcome v2 design is the result of a Boost peer review. It is a consensus design reflecting the input of many of the world's leading experts in C++. Sure, I may have chosen a consensus design which will be rejected in a second peer review, but I would doubt it. The proposed Boost implementation provides many extensions over the basic design which is heading to WG21, all of which are very useful for advanced users, but which can be safely ignored by most. I'm not going to get into the long list of what's wrong with the Rust `Result&lt;T, E&gt;` implementation, if you can't see it already then I won't persuade you. But I can say that I would expect C++'s eventual implementation of monadic operations to be able to work with *any* type which meets the `EitherMonad` concept, and hopefully it will work seamlessly with ranges too. So you'll be able to describe in functional, monadic form, a generic operator upon arbitrary EitherMonad input, then apply it to arbitrary sequences of input either sequentially on the current thread or across coroutines or across executors. Lots of parts have to come together to make that happen yet, but that would be the correct design. And one not possible when a result type locally hardcodes what it thinks map, iterator, unwrapping etc ought to be. That's a lot of why the Rust implementation was not thought through, and its users are now encumbered by tech debt. As I mentioned, *less is more*.
&gt; try // does not expand &gt; try (foo) // does not expand &gt; try(foo) // expands That claim is wrong. `try (foo)` would expand just fine. 
Pattern matching is tricky. The Rust approach looks great initially, but you find it's only really good at the 80% most common use cases. For the remainder, it either can't do it, or it's awkward to use. And maybe that can't be improved upon. But from speaking with those on WG21 interested in implementing pattern matching for C++, they suspect it can be improved upon.
As one idea, you could start by removing every instance of new, malloc, delete and free from the code you write, and then get it compiling again using only modern C++ memory management.
They aren't mandated by the standard, but that's a cop out. The purpose of having a standard library is to provide a baseline of code for projects to use so they don't have to roll their own. If the result of that standard library is something like std::vector which cannot be used by anyone who cares about debug performance then the end result is the same - people rolling their own. All those debug checks should be optional, pay for what you use, as per the rest of C++. The standard should mandate something along those lines; that STL code compiled without optimisation should perform not significantly worse than a hand-rolled version would without optimisation, and any extra functionality like bounds checking are opt-in, or at least there is a way to opt-out.
&gt; I'm not going to get into the long list of what's wrong with the Rust `Result&lt;T, E&gt;` implementation, if you can't see it already then I won't persuade you. So basically, yet again, you're not even going to bother making an argument, choosing instead to substitute a wall of text. Post after post of how terrible Rust's `Result` type is without one single rationale provided. Nothing. There is no substance in *any* of this. *Of course* you won't persuade me if you don't even try to make a justified statement. Surely if there's "a long list," there's at least one item on it that you can bring up. At the very least, one item that isn't... &gt; And one not possible when a result type locally hardcodes what it thinks map, iterator, unwrapping etc ought to be. That's a lot of why the Rust implementation was not thought through Wait, what. That's your reason for Rust's implementation being poor? There can only be one *possible* meaning to these functions! What possible benefit would you gain from being able to change what they mean?! What other functionality do you actually need - and how does being able to overengineer and policy-all-the-things remotely play into your "less is more" mantra? 
Yes, they've got a specialization for this common case. No need to store an extra bool, when nullptr already expresses the absence of a value. 
&gt;So technically speaking, not UB Yup. Its just illegal: &gt;17.6.4.3.1 Macro names &gt; 1. ... &gt; 2. A translation unit shall not #define or #undef names lexically identical to keywords, to the identifiers listed in Table 3, or to the attribute-tokens described in 7.6. . &gt;try (foo) // does not expand You are probably thinking of: &gt;(try) (foo) // does not expand
you have a race condition on m_run and m_processing also I feel like you could have a much neater interface and implementation if you made use of futures and packaged tasks, and std::function or some function_view implementation. As a /u/also_stl mentioned, a lock-free queue for each pipeline might help (though things get complicated because you then have to consider poll intervals, backoffs on the queue, etc.)
I've also posted this at SO - https://stackoverflow.com/questions/45437817/wrong-values-outputted-from-enum edit - sorry for title gore :p
I wrote this mainly for convenience and increased type-safety. 1. As RogerLeigh pointed out: a user can pass count as enum and compiler will accept it. 2. Main use of automatic to/from string conversion is serialization and debugging. I use it a lot in FreeFT and it's a great help: https://github.com/nadult/FreeFT/search?p=1&amp;q=DEFINE_ENUM Obviously it's limited if you need custom string values. 3. fwk::EnumFlags provide additional type-safety: user won't be able to pass flags of one type to a functions which expects a different one. 4. You can forward declare: fwk::enums are enum classes plus some additional info. You just have to remember to declare with proper underlying type; When it comes to IDE, I don't know about Vissual Studio, but clang-based code completion engines (like YouCompleteMe) handle these quite well. Also, latest version doesn't use BOOST_PP anymore. Btw. I wasn't aware that std::bitset existed, it can certainly be used inside fwk::EnumFlags class. Thanks :)
The reflection study group is active and making good progress. They have been studying multiple designs for compare and contrast purposes. Consensus appears to be forming now. The Metaclasses proposal builds on the reflection study group's work. The committee is a group of volunteers. If you aren't happy with the progress being made, well, get involved and lend a hand. ISO processes and procedures are not heavy. They tend to slow down the actual publication of work produced, but are not in my experience a drag on development. 
Sorry about that. My mistake. I had a different version of this blog that had C++ examples of how to correct Cyclic Dependencies, but I decided to make that a separate blog post. 
That is really cool! Could someone give a summary( or explanation) the key features of C++14 used in this project? 
Just from a glance: Using indexed loops over every element in a container - use range based loop Passing function parameters by value - should be either const&amp; or &amp;&amp; Dragging in boost to use any - if it's for fun, might as well use std::any (available in GCC7, VS2017 etc) 
Those debug checks probably are optional. Check with your standard library vendor. Compilation modes are outside the scope of the standard (though they have been discussed within the context of contract proposals).
Why did you call this library an **HTML** templating library? I skimmed through the code and didn't find anything related to HTML.
Haha hmm, that's a good point. There isn't anything validating for HTML semantics that I can recall, so I suppose it's really only an interpolation library (currently).
The answer is here: https://stackoverflow.com/questions/35536146/incorrect-assignment-of-values-in-char-enum
Ah thank you! I'll read up on these and implement them!
I read it, one of the comment says its still not corrected in c++20 drafts :(
Hm, you mean, I could could be going for something like this: pipe.addFilter([](IntType t) { return t() + 1; });
I agree with your first point, but not with your second. Apologies but I don't automatically take a bow when someone invokes programming language theory (PLT) in some argument. There's plenty of ideas in PLT but they're just that, ideas. There's nothing empirical backing their effectiveness. I prefer my libraries to be designed by people who have risen to some of the most influential positions in the C++ world over a dozen or more years of experience, then someone with much less real world experience citing PLT as the reason for something.
see the comment below that (now deleted) comment
&gt; I've clearly mentioned that enum be of type 'char' To be clear, the type system is respected: the enum is of type `rank`, not `char`. If you make it streamable by defining `std::ostream&amp; operator&lt;&lt;(std::ostream&amp;, rank)`, it will be printed whatever way you decide it should be. And since popular compilers don't appear to have implemented the 2013 DR 1601 yet, that might be your best workaround.
Is there some justification for the design of outcome somewhere? I skimmed one of those papers but possibly I missed it. The design seems... let's say "bizarre" at first glance and definitely requires explanation. I don't understand why it needs a third type. Is this something that is returned in both cases? Why either payload or exception? If you want something that is returned in either case why not have the function return a std::pair? In addition, it doesn't seem like there is any ergonomic way to do exceptions with a result (again, unless I missed it). It seems like the expected way to use exceptions is through an exception_ptr in outcome. Creating an exception_ptr looks to be fairly expensive so this is probably something I would avoid unconditionally. There should be some way to throw an exception from a `result` ergonomically that actually has a meaningful type (not a generic `no_value_type`, similar to what `optional::value` does).
I tried some compilers on godbolt and apparently only MSVC 19 gets this right.
In JS well... one? There are many wrappers on top of it you can use if you want but they all have the same building blocks and issue the same standard websocket functions built into the core language? Ruby, being a more general language, doesn't have "websocket" but it has common TCP building blocks that in language implementations use. Same for Java, Python, C#, Rust, D, Go and others - none force everyone to solve how to store an IP in every one of their projects or find how to access &lt;current platform's&gt; basic socket calls.
The main feature is "constexpr" and "raw user literals" The idea behind constexpr is that when you assign the result of a constexpr qualified function to a constexpr quialified variable, the compiler tries to execute the function at *compile time*. Obviously the things you can do at compiletime are limited - for example you cannot do IO, you cannot dynamically allocate storage, you can call only other constexpr functions, (thus you cannot call most standard library functions) A constexpr function always returns a value. That value is available immediately to the compiler after the point where that function is called - so it can even be used to instantiate templates with value parameters. If a constexpr function is assigned to a normal variable, the compiler tries to run it, if it fails, it compiles the function as a regular function to be called at runtime. So this is like "*if you can, evaluate this, else let it run later*". in C++11 constexpr was almost like a functional language - a function consisted of a single return statement only so people resorted to recursion to implement stuff like in functional languages. in C++14, constexpr constraints were relaxed, so pretty much the whole language is available at compile time. in C++17, some of the standard library classes like string_view can be used within constexpr functions. Raw literals let you define a function that is called when a string literal is followed by your custom prefix for example R"(-Some text that can contain anything and only ends when dash close is found-)"_foo; You can define an operator _foo function that takes this literal and creates whatever you want from it - this operator can be constexpr, so we can generate a value that is derived from the string - this could be a word count, a hash, a tree data structure, a regular expression automaton, a datetime - whatever you want. 
How can you test MSVC 19 on godbolt? I only got gcc, clang and icc there.
&gt; In JS well... one? There are many wrappers on top of it you can use if you want but they all have the same building blocks and issue the same standard websocket calls. In the browser, yes. In the broader ecosystem (for instance node.js), much more: https://en.wikipedia.org/wiki/Comparison_of_WebSocket_implementations (and this list is missing some other prominent ones, like uWebSockets). &gt; Ruby, being a more general language, doesn't have "websocket" https://github.com/search?utf8=✓&amp;q=websocket+ruby Ruby has, it seems, a good dozens of distinct websocket libraries. So how is it different from when you said "and you end up with 20 different types in 20 different libraries to represent a TCP connection" ? 
It's called CL 19 in the list of compilers (which is the name of the compiler driver of MSVC)
It's not just debug checks, its when library maintainers rely on constructs which don't behave well at runtime unless optimisations as enabled; relying on hundreds of tiny functions being inlined, burning out trivial lambdas, etc etc. Of course we can't expect everyone to mentally optimise their code so it performs the same in debug and release, but at the same time we should be putting limits on how much worse it can get. If hand-rolled is 2x and STL is 3x, that's fine; if STL is 16x that is not OK. As for compilation modes being outside the scope of the standard, my entire point is that they should be in scope. It is a practical concern of every developer which is swept under the carpet and ignored by the supposed C++ intelligentsia, who then wonder why everyone is still writing their own containers, algorithms and so on. If you ignore user concerns as "out of scope" then don't be surprised when they reject you, just like any business providing a service to its customers would be.
Passing by value is fine, as long as you move the value somewhere else.
&gt; `enum rank : char` I thought the `:underlying_type' syntax was only applicable to the new (since C++11) `enum class`. Apparently not. TIL. Makes sense now that I think about it, the 2 things are orthogonal. 
&gt; So how is it different from when you said "and you end up with 20 different types in 20 different libraries to represent a TCP connection" ? Because there is a difference between "My language doesn't provide a way to make a network connection!" and "My language doesn't provide a service that rides on top of HTTP as a core library!". In the former literally anything you can make work is just as valid as everything else. In the latter yes, your exact high level function isn't in the standard library but you can make it without leaving the standard library, if you use an idiomatic library it will have the same look and compatible types at the base (even if they aren't directly based on it), and you know it works where the language does. If C++ didn't ship with text IO this conversation would be "and C++ needs text output in the standard library so we can avoid the 20 different ways of getting "hello world" on the screen" and this type of response would be "there are 20 different terminal style and formatting libraries, what language has these in their standard library? How is it different than having text output?". The standard library doesn't need to solve style or websockets to be able to provide a standard base for IO, only the base needs to be in language so libraries can be built consistently for those end goals. A good example to your questions about how Node.js handles WS: You can find a common WS package like `ws` which can either be run as pure JS leveraging the standard Node socket interface or you can switch to a more performant native back end. I'm not familiar enough with Ruby to say if one of the WS packages available does the same but I assume it does. The same is true in Rust and C#, you may need a library for the high level function but each library doesn't need to figure out how do open basic IO on every target platform.
I've never heard of this toolkit before. What's up with prefixing all type names with a capital 'W'? Following Qt's style? Namespaces (e.g. wt::) would be a more elegant approach, I think.
That is very helpful. Thanks so much for your detail explanation!!!
&gt; Because there is a difference between "My language doesn't provide a way to make a network connection!" and "My language doesn't provide a service that rides on top of HTTP as a core library!". If you look into the source code of the libs I linked, you'll notice many absolutely don't rely on the language's native http stuff but instead reimplement everything by using C sockets or sometimes libuv / epoll / asio. Hell, I think some blog post once proposed reimplementing a TCP network stack in user-space to achieve more throughput with this. &gt; If C++ didn't ship with text IO this conversation would be "and C++ needs text output in the standard library so we can avoid the 20 different ways of getting "hello world" on the screen" and this type of response would be "there are 20 different terminal style and formatting libraries, what language has these in their standard library? How is it different than having text output?". I would personnally entirely be for having any kind of IO be outside of the language. I hardly ever use std::{i,o}stream. There are way too many different ways that are each better or less suited to handle different problems (logging, printf-debugging, network, serialization, GUI, ...) to have a "one-thing-does-it-all" in the standard lib. 
could be interesting for instance to detect Foo&lt;T&gt; and be able to create member functions for Foo&lt;const T&gt;, Foo&lt;T&amp;&gt;, etc...
Sorry, what's PLT?
It's under arch CL 19 2017
Yep, you're right
Hmmm - isn't it the users responsibility to manage this? Isn't the existence of multiple options a good thing? If a user creates an application which requires 20 library imports to represent a TCP connection, I don't think the C++ standards committee can do much for him.
It's not that a single application has 20 different ways to do it it's that as you move between applications there may be 20 different libraries with 20 different types and quirks for something as basic as representing a TCP connection. I'm not saying you should force everyone to use the standard option rather the fact there isn't one is a bit of a pain. Just because std::iostream and friends exists doesn't mean you can't use other methods for string IO and processing, it just means that for the vast majority of use cases you don't need to leave the standard library and everyone can look at your code and have a full understanding of how string IO is handled and what they can do.
programming language theory. I'll edit.
&gt; However, an asio-like standard library would definitely need to support C++ coroutines and not provide a different, competing, mechanism. What do you mean "*a different, competing mechanism*?" The [universal model for asynchronous operations](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3747.pdf) expressed through Boost.Asio simultaneously provides potential support for all possible completion mechanisms. The only change necessary is the possible addition of an included completion token indicating that coroutines should be used.
The closest currently available document approaching a design rationale is the v2 changelog at https://github.com/ned14/outcome/blob/master/Readme.md. A comparative analysis comparing Outcome's design to Expected's will arrive in P0762R0. And yes, the trait based switching payload|exception will no doubt inspire interesting debate during any second peer review, if they really don't like it they may insist that `outcome&lt;&gt;` be split into two distinct types, which is fine so long as they can supply a good name for the payload bearing edition. I'm still minded that as in C++ every template specialisation is a separate type anyway, it's not an issue, but if a large majority thinks I am wrong, I will yield. Regarding why have a third type, the reason why is that we really strongly want to preserve commensurability between `result` instances such that `TRY` always "just works" without tedious boilerplate. Yet we also sometimes really want to send additional payload with a failure to indicate the cause, so the third type makes a lot of sense on delivering both goals with least hassle and inconvenience. I'm not sure what you mean by "ergonomic way to do exceptions with a result", we take the design approach that if you want something like C++ exception throw semantics, just throw a C++ exception. You can combine this with `result`, replace this with `result`, or use `exception_ptr`, or any combination thereof as suits your need. Creating an `exception_ptr` costs about 5k CPU cycles, it is why there is a payload feature where sometimes you might want to return a `filesystem::path` indicating the failing path, or even a `string` with some custom text to print. If you'd like `result` to throw custom exception types, or to throw under custom rules, that's exactly what the policy class is there for. You can customise what all of the state observers do. You also can hook when a `result` and/or `outcome` gets constructed, copied, moved, converted and insert custom code at any of those points e.g. when a `result&lt;X, Y&gt;` is implicitly converted into an `outcome&lt;A, B, C&gt;`, override the outcome's state to some custom calculated values e.g. synthesise an `exception_ptr`, fetch a payload from TLS. You have lots of options there.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [ned14/outcome/.../**Readme.md** (master → 5da3647)](https://github.com/ned14/outcome/blob/5da364741db5935bf081d2c6c362bdbf27786b5b/Readme.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dl0k8g9.)^.
&gt; You say this as if it were a well known truth, but it's not. In fact, some people think we're going too fast, and that can be argued very reasonably. Remember when C++0x did not mean hex release year? :P Remember when Herb said concepts lite may make it into C++14? Remember when Bjarne listed his wish list for C++17 and he got nothing from that list? Remember when C# and Java had serialization for decades before C++(we are technically still waiting for C# to get into decades since it is only 17 years old, but I think it will be 2 decades). &gt;Look at your coworkers; do they know C++11 and C++14 very well? I'm sure they are not complaining that we're moving too slow. Experts may complain, but they're just that: experts. Complicated but answer is mostly no. Yes I had coworkers in some shitty companies I worked for where I had to beg them to let me help them :) but generally people want more stuff from C++. And I do not talk like decltype(auto) or relaxed atomics but more mundane stuff like serialization, hashing that works for std::pair :P So C++ has glaring defects that should be fixed and I am not talking about just expert stuff, but stuff that juniors that are coming from Java or C# or Go would expect to be there. That is why I thought that Herb's spin on the interface not being a keyword was such a shitty PR speak. I doubt he is really so blind that he does not see that it would be better as keyword(for juniors, I can easily read the library interface code), but he probably just does not want to say that ISO can not add it due to "reasons". In general people dislike change, esp older people, but that is not the criteria to stop the change. Criteria is if you "force" them to change would they want to go back after 3 months of using new C++. I never had anybody complain about C++ 11/14 in that situation. &gt; &gt; &gt; If you want to explain how wrong I am : give me one specific difference it will make in development &gt; Having a bill of rights for users does not help for the above issue. Having a bill of responsibilities does, by somehow codifying that if you rely on that sort of detail, we will screw you and you'll have deserved it. If you spend even just one hour in LEWG or LWG, you'll see that that sort of question comes up all the time, and often we just kinda do what seems right, but we don't have clear cut guidelines to act. For auto they went over some yuge codebases and tried to see the impact, maybe you could do it in general, although I am afraid for some cases you would need some compiler tool, and simple grep will not work :) and in this case IDK if you can even detect it(semantic difference between move and copy). In general I am pro breakage, this case may be tricky since it is a silent one... I actually thought about something that may help in this specific case and sent it to one C++ expert (with message : if this is stupid or busy do not reply), he did not reply so it is either stupid or he was busy. If you are interested pm me, just promise not to dox me, I may want to work one day in companies that fill ISO. :) EDIT: C++ expert replied and he hated the idea. :) But I don't worry too much, I am right, he is wrong ;) 
Never used any tools like this but what's the difference of this vs Awesomemium?
People who constantly ask "prove it to me" generally cannot be persuaded because they are ignorant of the issues involved and would need to be spoon fed. That's why nobody bothers proving anything to anyone who says "prove it to me". Waste of time. Try doing your groundwork basic research by reading all of the Boost peer review on Outcome, then ask some sensible questions demonstrating that you've done that research and have very sensible questions on specific points like the other comments in this thread have done. You'll get a far higher quality response than "a wall of text", as the other commenters have received.
In which case, I wholly agree. Rust has lots of really great ideas. Some of them work out really, really well. But a majority suffer from poor execution, and a minority turned out to be a bad choice. Swift did the same of course, and they rebooted the language which was a very wise, if brave, decision. C++ is full of poorly executed ideas too of course, but most of them were added a very long time ago. I'm not aware of anything I truly despise as an abomination which has been added since the 2003 standard, and that's pretty great in my book, and a testament to the calibre of the C++ leadership.
One doesn't preclude the other. Despite the PLT discussion level, which I find good, they take references from everywhere, several PLs, logics, etc, and yet, Rust was born with a practical usecase, replace C++ on a web browser. It's hard to find a PL being designed while at the same time getting feedback from a hard real world application evolving with it, by the core developers. Again, people on Rust's RFCs not only discuss PLT for sake of it. If it wasn't so Rust probably wouldn't have changed so much up to 1.0, it was so different before that.
The Coroutine TS uses `co_await`, `co_yield` and `co_return` **keywords**, while boost::asio uses `reenter`, `yield` and `fork` **macros**. These looks like two quite different mechanisms to me.
Thanks, I'll take a look. &gt; Regarding why have a third type, the reason why is that we really strongly want to preserve commensurability between result instances I think I understand this. Basically that you can use the same error type in many more places, which makes TRY much more applicable (because the error types have to match for it to work), but still enabling extra information in some cases. &gt; we take the design approach that if you want something like C++ exception throw semantics, just throw a C++ exception I have heard this kind of thing before, and the trick is what's buried in the word "you". Writing the function, and using the function, are two separate actions at separate sites quite possibly by different people. If you throw in the function, you are locked into exception handling. So you return a result. But the user of the function, might prefer the exception handling paradigm. You talk a bit about the example of parsing a string, and the boost way of doing it, in the docs, but I don't find the example is taken to its logical conclusion. Consider this interface: int parse(const std::string&amp; s); // throws bad_parse on failure int parse(const std::string&amp; s, error_code&amp; rc); // sets rc on failure If we are replacing this then we should have the following requirements: 1. nearly as ergonomic for exception users 2. at least as ergonomic for explicit error checking users 3. at least comparable performance in all cases We're now going to write this as one function, roughly ? parse(const std::string&amp; s); Where ? is some type, that I"m not sure about. The problem is that point 3. completely completely rules out any usage of `exception_ptr` (from inside `parse`, the user might use it in a conversion as you mention). With the current documentation it's very clear how to accomplish 2 and 3 together. But it's not clear at all how 1 is accomplished. Note that our old way of doing it throws an exception specific to the function (or a similar class of functions). It seems like naive attempts to throw an exception from result, will result in some kind of generic exception type. How exactly would I use the policy class to write a version of `parse` that actually meets all of 1, 2, and 3? And is this sufficiently easy? This seems like one of the typical use cases and it's rather surprising that you start having to delve into policy classes to solve it.
I don't think you're understanding the issue being discussed. The current [Networking TS](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4656.pdf) does not contain the word "*coroutine*" and defines an asynchronous model which allows the completion notification mechanism to be highly customized (see §13.2.7.2 [async.reqmts.async.token]). There's no need to add coroutine support to the Networking TS (which is based on Boost.Asio) all you need to do is add a tag type and an appropriate specialization of `async_result`.
It was you, who said that boost::asio supported "both stackless and stackful coroutines" and the links you provided show that this is achieved using macros. I was just arguing that point and I never mentioned the Networking TS. I admit I'm not versed enough in the Networking TS to say whether it's possible to make `async_result` waitable in the sense of Coroutine TS.
Agreed. I feel build times and build system issues are quickly becoming the largest obstacle when it comes to adopting C++.
The requirements in your website has "Undergraduate or graduate level degree in Computer Science or Mathematics.". Will C++ software Engineers with a degree outside CS or Mathematics be wasting time applying?
I'm not familiar with Awesomium, but it seems to me that it's more aimed at adding HTML-based user interfaces in desktop applications. Wt is a widget-based web framework, so it's aimed at making web development a bit more like desktop GUI development.
We put everything in the `Wt::` namespace, so yes, the 'W' prefix is unnecessary. It did indeed mirror Qt's 'Q' prefix style. It's just the way Wt is now, and we don't intend to change that, since that kind of breaking change would just be annoying for our existing users.
That could be also because your CPU is hammered though...
There is a certain Trumpian character to your posts here. Great design this, awesome implementation that, strongly negative characterizations of other implementations, frequent referrals to alleged wg21 member conversations supporting your claims, and the rejection of any minimally critical question as out of scope. Good luck convincing Boost peer reviewers, let alone wg21, next time around.
&gt; It was you, who said that boost::asio supported "both stackless and stackful coroutines" And I also said: &gt; [...] and which has a customization point for seamlessly integrating arbitrary completion mechanisms into initiating functions My point is that adding coroutines to the language doesn't change Asio or the Networking TS. They have their own, distinct model for asynchronous operations which coroutines can be built on top, but which coroutines do not change.
You seem to have some passion regarding this, so I'll suggest you pursue getting directly involved. You do not have to become a committee member to do so. The following link describes how to submit a proposal yourself. * https://isocpp.org/std/submit-a-proposal Some things to think about: * What compilation modes should the standard define? Note that many projects support many more modes than just debug and release and each mode can be tweaked with more granular options. * How should compilation modes be defined? Should the definitions constrain use of specific optimizations (inlining), debug support (PDB/DWARF), run-time checking (out-of-bounds, invalid arguments), contracts (pre/post condition checking), etc...? * How would you constrain performance guarantees? You state that a 16x difference is not ok, but what if, for example, such a difference is purely the result of compiler optimizations and not conditional code? Do you mandate some optimizations must then be enabled for "debug" builds? * Should vendors be allowed to define compilation modes other than what the standard would define? What if they have a DEBUG_WITH_EXPENSIVE_CHECKS mode that doesn't meet the performance goals you have in mind. Is that ok since it is opt-in? If so, where do you draw the line between what is and is not ok? Personally, I think this is a quality-of-implementation issue which is why I suggested following up with your standard library vendor.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6qvyxv/why_cpp_sometimes_doesnt_respect_type_system/dl0s8tr/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
- https://scottmeyers.blogspot.com.br/2015/02/why-auto-deduces-stdinitializerlist-for.html
Ooh wow that's awesome, thank you! I know it's "cl.exe" and its version is 19.x but I couldn't connect that knowledge to seeing "CL 19" in that list. Any idea what "RTW" means? Release to .....? I think it would be better if the Godbolt author specified the actual version - like 19.0.x, 19.10.x, or 19.11.x - this way it's clear which exact version it is. Otherwise it's all a bit ambiguous, as MS's naming scheme has been a bit ambiguous in the past in several ways.
4 cores hyper-threaded, and it's barely using 150% (so 1.5 core), yet the GUI hangs.
Why do you care so much about the performance of debug mode? Yes, it runs a bit slower - it's doing extra stuff that helps with debugging, like it says in the name. So what? You say it is a practical concern of "every developer" but you're the first one I've met who actually seems to care about it. If you want good performance, compile in release mode. If you want debugging checks, compile in debug mode. Having debugging checks _and_ high optimisation makes very little sense to me. 
I'm disappointed /u/stl isn't doing a session this year. I've always enjoyed his presentations, ever since I watched GoingNative 2012 online in college which really sparked my love for C++. 
&gt;I really think `variant`, `optional`, &amp; other metaprogramming toolbox tools should just be compiler intrinsincs. Best I can tell there's nothing stopping implementers from doing this.
&gt; This makes development much harder, especially when working on interactive products where performance affects the usability and outcome of the code (eg. in games). This statement just leads me to believe that you're "*testing*" your software by actually running and using it which is the wrong way to go about things. &gt;the code will no longer compile without optimisations because we hit hard limits in the compiler, linker or symbol generation. What does this have to do with the C++ standard?
&gt; Writing the function, and using the function, are two separate actions at separate sites quite possibly by different people. If you throw in the function, you are locked into exception handling. So you return a result. But the user of the function, might prefer the exception handling paradigm. Yes, I very much agree. So one of the things I have tried to achieve is that library A can subclass `outcome::result` with custom semantics local to it, and library B can subclass `outcome::result` with different custom semantics local to it, and if I've got everything right then the two libraries can hand `result` between one another seamlessly by either the library devs, or the end user, injecting handling code at the boundary. I *think* I've pulled it off, but it'll be interesting to see what the Boost peer review says. &gt; You talk a bit about the example of parsing a string, and the boost way of doing it, in the docs, but I don't find the example is taken to its logical conclusion ... I actually didn't know that was there! I'm not the one writing the tutorial in fact, and I was stale as to what's there. But to answer your question, the Boost peer review raised the wish to be able to replace the `error_code` + throwing overloads with a single function which might be: outcome&lt;int&gt; parse(const std::string &amp;s) noexcept; If the function failed, it may have returned an outcome with **both** error code AND exception ptr set. When you now do `parse().value()`, the exception ptr will be preferentially rethrown first, that replicates exactly the same (expensive) effect as the throwing overload on failure. If you'd like a lighter weight check which just cost the allocation and free of an `exception_ptr` rather than a full throw and catch cycle, you can inspect `parse.error()` for an error code. That behaviour, by the way, is the default policy in `outcome&lt;T&gt;`, so that works straight out of the box already.
Oh for sure. I'm always raising my wish for a "C++ 2.0" which you can opt into by tagging a namespace as containing "new C++". I'd start with permanently eliminating the `template` keyword, making everything not a trivial nor integral type move-by-default, use of batch malloc instead of the currently amazingly inefficient malloc-per-new-operator and so on. There's tons you could do and still retain perfect C compatibility, and indeed compile compatibility with "old C++". Maybe one day ...
&gt;We put everything in the `Wt::` namespace, so yes, the 'W' prefix is unnecessary [...] and we don't intend to change that, since that kind of breaking change would just be annoying for our existing users. Why not alias it? Is namespace Wt { using thing = wThing; } breaking backwards compatibility? 
I wouldn't count that as an abomination! Rather a "quirk" with *a* logic to it. Abominations are things which cost countless hours of pointless lost productivity and cause the generation of silently incorrect code. Anything which generates a compile time error, or requires the programmer to type lots more code, is therefore rarely an abomination.
"The wrong way to go about things" - what an arrogant statement. You have no idea about the situation that could lead to needing to develop code by interacting with it, and the fact you cannot envisage such a scenario says more about your programming experiences than mine.
*Going Nowhere Faster* sounds like a presentation I could enjoy.
I'm talking about the degree of the slowdown. I've said many times I completely agree that debug will be slower, but it shouldn't be excessively slow, as in my example. A 2x slowdown for a naive/simple implementation should translate into a 3x slowdown for a fully bells-and-whistles library implementation, but it doesn't, it leads to a 16x slowdown. Why is this? I believe it's because there is no thought put into at all at committee level, and library implementers take a "don't care" approach to anything except optimised runtime performance. Of course, this also has the secondary knock-on that you're making the compiler do a huge amount more work burning out all your zero-cost abstractions, which bloats compile times, especially so in STL code given it's mostly all in headers so has to be dealt with over and over and over again.
&gt; If the function failed, it may have returned an outcome with both error code AND exception ptr set. &gt; If you'd like a lighter weight check which just cost the allocation and free of an exception_ptr rather than a full throw and catch cycle, you can inspect parse.error() for an error code. That's the thing: your new `parse` function is a massive downgrade in performance compared to the old `parse` overload that took an error code by reference. So it's not really a replacement for it. Unless you never set exception_ptr, but then it cannot replace the throwing version which can have rich information. We are right back where we started, where the writer of `parse` will have to decide whether their `outcome` returning function provides rich information on error or good performance on error. The user should be making that decision. Something seems very wrong here. Part of the core mission statement of this library, supposedly, is (this is a direct quote from a proposal): &gt; It takes the best of the exception and error code. In our specific case, that means meeting the 3 requirements that I laid out previously. The design cannot meet these 3 requirements for even the simplest example without messing around with policies. IMHO the default design should be based around the error class knowing how to throw something, on demand. This retains type erasure without any pre-emptive heap allocations.
&gt; what an arrogant statement. There's no arrogance involved. Testing code primarily by manually interacting with it is not a tenable solution.
Shouting "prove it" all the time without any evidence that the shouting person has invested any effort in understanding the detail annoys me. Most people ignore such people, as do I most of the time actually. If it's "Trumpian" to bother at all with shouty ignorant people, then I guess I'm that. The rest of your post is unfair. If people make an intelligent, nuanced criticism based on having actually studied the problem instead of shouting based on some ideological position they think they are entitled to, then I'll almost always respond in detail, as I have elsewhere in this post. About 300 of the 800+ posts in the Boost peer review were written by me in response to intelligent criticism, and I know I more than acquitted my depth of domain knowledge in this area in that review. Now as to frequent referrals to "alleged conversations", I could say exactly who by name, but I don't see why it matters. I have excellent relations with many WG21 folk and most of the Boost leadership, and due my long service in the community, have done for many years now. So of course we speak frequently, why wouldn't we? Why *you* claim this stuff is "alleged" is much more interesting. Why are you so insecure as to whether I make everything up or not? Who cares? As I've said to you more than once before, it's the internet. People make up shit all the time. So relax and don't believe a word you see on the internet, none of it matters. Treat everything I say as lies if you wish. Doesn't matter. 
&gt; Personally, I think this is a quality-of-implementation issue which is why I suggested following up with your standard library vendor. It probably is, I've not done the testing on the other implementations to know how they fare, MSVC is what I've mostly been exposed to. As for proposals, no thanks, I have better things to do with my time than argue standard legalese with some wonk who cares more about C++ than their mother. I care about this stuff, but thankfully I have my own route out by just implementing everything myself, as most of us in the games industry do. For me, the STL and the standards committee are useless and nothing they do helps me in my day-to-day work; seems most people (on here included) don't care about that and would rather carry on with the academic masturbation that is meta programming and most of what is coming in C++17/20/etc. I think the *only* thing that is practically useful coming out of the committee is an attempt at a file system abstraction layer, but us game programmers will probably all end up using OS-specific calls anyway to get the performance we need. All I'm doing here is voicing the opinion that the C++ committee &amp; library maintainers should care about non-optimised performance (among other things), given the topic of the post. At the moment, from everything I've seen (including first hand experience of conversations between some well known library maintainers) nobody cares about anything except optimised runtime performance; I don't think that's good enough, but I have no desire to fight the system, I'll just go on ignoring it.
How many of these are going to be recorded?
I don't know. Then you'd have an entire library of widgets known by two names. The 'W' prefix doesn't bother me personally.
Of course it is, when business practicalities become involved. Where I work we write tests for everything we write, a novelty in games development. But we're also building on top of Unreal Engine 4, which has 2 million lines of code, **none** of it tested. If I wanted to "develop by tests" whilst working on the engine, I'd literally have to rewrite the whole thing to put tests around it. There is no mocking, no separation of concerns, enormous functions with hundreds of side effects, it's an absolute nightmare from a testing perspective. Even if every single programmer at the company did so we'd never manage it in a decade. And yet we manage to release games, as do many others with UE4, so clearly it is "tenable". That's before you even get into interactive testing, where feeling and other subjective matters come into the fray. Your statement was arrogant and presumptuous, and you do your test-first brethren no favours by acting in that manner, it just puts people off, even folks like me who *do* write tests where I can.
&gt; In our specific case, that means meeting the 3 requirements that I laid out previously. The design cannot meet these 3 requirements for even the simplest example without messing around with policies. I'm pretty much in agreement here. The core assumption in the Filesystem use case is that the cost of making a just-in-case-exception-ptr will be irrelevant compared to the filesystem syscall, but that is certainly not the case always. My current argument is that if you care about the cost of a unnecessary exception ptr, then only return a straight `result` and don't be throwing exceptions at all anyway. But I'm soft on that argument, I only hold it because I think it's the least worst given the options available. &gt; IMHO the default design should be based around the error class knowing how to throw something, on demand. This retains type erasure without any pre-emptive heap allocations. Ah, that's why I deliberately left `E` to be customised by the end user who can supply a do-nothing policy. In other words, I kicked that can down the road. Why not choose something more definite? I think it would be controversial. And if it's controversial, it won't pass a Boost peer review. v2 Outcome deliberately doesn't implement at all stuff which in v1 was found controversial, it has pared off all that stuff, but hopefully left the door open to end users to extend Outcome to fit their own needs. I know that's a cop out, but such is peer review, it's conservative.
So sad that I won't be able to go. Hopefully they'll record a bunch of the sessions again and I'll catch up on YouTube.
All of the talks are recorded professionally. You can watch last year's videos [here](https://www.youtube.com/user/CppCon).
All of the talks are recorded professionally. You can watch last year's videos [here](https://www.youtube.com/user/CppCon).
Ha, great retort labelling *me* as insecure when I challenge *your* authority arguments. 
any chance they will be hosted elsewhere as well? Or at least an official torrent? Youtube doesn't work for 20% of the world
Yes, which is why it'd have been nice if `std::optional` had been specialized for references the way `boost::optional` is, but as it is there's no optional reference in the standard library other than raw pointers and abominations such as `std::optional&lt;std::reference_wrapper&lt;...&gt;&gt;`.
 I don't personally care that much about debug mode--but the reason people care is that if the debug mode is too slow to run the game at an interactive framerate, its completely useless. For instance I have tons of SIMD intrinsics code-- in debug mode with MSVC it runs crazy slow(many orders of magnitude), and makes debug mode utterly worthless to me, so I never use it. 
Isn't Awesomemium commercial-only, and not cheap IIRC?
Why not use it to declare something similar to javascript dictionaries, just for the lulz? It already looks like an array of variants on steroids. 
They are also hosted on Channel 9.
It took a team of programmers just to come up with the title.
RemindMe! September 24 2017
I will be messaging you on [**2017-09-24 20:00:14 UTC**](http://www.wolframalpha.com/input/?i=2017-09-24 20:00:14 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/cpp/comments/6qw0aq/wt_c_web_toolkit_400_rc2_is_out_and_i_will_be/dl10di2) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/cpp/comments/6qw0aq/wt_c_web_toolkit_400_rc2_is_out_and_i_will_be/dl10di2]%0A%0ARemindMe! September 24 2017) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dl10efj) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Aw, thanks! Sorry for the disappointment. I considered submitting a talk, like about how I overhauled `vector` and the many subtleties I encountered along the way in the STL's most important and apparently-simple-but-actually-complicated data structure. I ended up not submitting anything, mostly because I felt too busy (being under pressure to finish C++17 this year makes me reluctant to commit 2-3 weeks to preparing a talk and attending). It'll be the first year in a while that I haven't given a talk (the last time was 2011 when I skipped BoostCon/CppNow).
&gt; Any idea what "RTW" means? Apparently it's "Release To Web". https://www.quora.com/What-does-RC-mean-in-Visual-Studio-2017-RC-Can-I-download-the-RC-version
I don't really understand what your are seeking (code? Feedbacks?) but mobile development in C++ is doable. I do this at work, however the GUI is platform specific. We did not try (yet) but with Qt I believe it is completely possible. I cannot show any code since its closed-source. For some context, we have a desktop application in C++ with Qt GUI that runs on Windows, Linux and Mac OS. To avoid rewriting code as well as for legal reasons (portions of the code base has certifications regarding security), the whole business layer (everything but GUI) is cross-compiled for iOS and Android. The code does discuss with a backend server, manipulates files, and performs complex computations. It should not be a surprise, but the hardest part is to cross compile third-party libraries (eg. Boost, Curl). From Android Studio and Xcode POV, our own C++ code is packaged as a shared library that the mobile app links to. These editors share the same drawback MSVC has : the moment you want to use external tools all hell break loose. Regarding GUI, I don't know if using a C++ library is a good idea... unless implementing a game or the like, using the native GUI is better with respect to the user (less disruption). In this regard, look at [this article](https://oleb.net/blog/2014/05/how-dropbox-uses-cplusplus-cross-platform-development/) that explains how Dropbox designed its application such that business layer is in C++ and GUI in platform-specific code. Hope this helps somewhat... Don't hesitate if you have questions.
A lot of really great sounding talks. Can't wait! But I haven't even finished all cppnow talks from this year :/
I wrote a 2D game engine for Windows, Linux, OS X, iOS and Android. It used SDL with OpenGL for rendering (although you can use the SDL renderer without much trouble if you don't need maximum speed). Almost everything platform-specific is handled by SDL, the only things I had to write per-platform was some better file-system support stuff, like memory mapped files, file enumeration and getting special system paths. Oh, and message boxes.
&gt; I don't really understand what your are seeking Anything that is real user experience(from perspective of a library/SDK user) and not a tutorial of X by a person selling/promoting X. So thank you for your experience, only problem I have is that IDK for a new app is it worth the trouble to write shared code in C++ if anyway app will need to use Dropbox approach(you still need to write in different languages)... 
It was you who claimed Rust's version is full of shortcomings. Was that part of Boost peer review too? Anyway, it doesn't matter. You present your material to newcomers here, full of claims, and I'm sure, just a few percentage of those reading it here must follow boost peer reviews. This is a sub-reddit, not a boost dev mailing topic. So how can't you expect people asking you to present and back with arguments what you state? This is not the same audience. **If** you were posting to the Boost mailing list you could expect not bothering explaining them your authoritative assertions because they were already in context with whatever those arguments are. This tone is ridiculous.
How many commercial clients do you have?
Ooh! So it's probably their web compiler. I think this is usually the newest "beta" version, so it would be 19.11.xyz and xyz whatever the newest pre-release build is.
Using Qt to deploy to win / linux / mac / android / ios from a single codebase here (except of course the UIs are not the same on desktop &amp; mobile). Android is the most painful due to the need to recompile Qt since the official version is stuck on gcc 4.9 and its olde libstdc++.
Then don't "prove it" — just paste at least _part_ of your supposed list of Rust's `Result`'s shortcomings. The more you backpedal, the more you lose credibility. FWIW, I _have_ followed Outcome's reviews, and I'm pretty comfortable in Rust, and I haven't a fucking clue what you're on about.
So ignore my authoritative assertions then! It could be that after two and some years developing STL quality implementations of this stuff, and indeed having programmed in Rust before under contract, I might know exactly what I'm talking about on this topic and then some. But then again I might not, and I might be making it all up. Or I might be trolling you. Or lying to you deliberately. Or anything else. It's the internet. View every claim made by anybody here or anywhere appropriately. Evidence can be faked, statistics formed from lies. Even if I supplied infinite evidence for any claim I made, it's all potential lies anyway. So no point doing so. Either believe me, or ignore me, or anywhere in between.
I'll ignore as I see no rational discourse coming out of those premises.
That's because there's no gain for me to go into detail, so I'm not going to. Either accept the claim based on authority which is as it was made, or ignore it. I'm happy to invest some time into discussion of potential shortcomings with the library in the topic, and especially to discuss alternative approaches like Emil's library which is very usefully completely different. All that is a potential gain to me in making a better library here. But to convince zealots that their religious beliefs are wrong? No time for that. Maybe when I'm retired and have more free time on my hands. Until then I have much more useful things to do like writing and debugging code.
&gt; &gt; &gt; It probably is, I've not done the testing on the other implementations to know how they fare, MSVC is what I've mostly been exposed to. This is because MSVC adds bound checks to every vector access when you compile on debug mode. For instance GCC and Clang do not. For GCC you can enable -D_GLIBCXX_DEBUG to get the same kind of debug checks. I personnally find them very useful; they have helped me sort out logic bugs quite often.
Oh, I'm absolutely going to ignore it, along with every other appeal to authority made because _that isn't an argument_. Don't you want some of your threads to _not_ devolve into shitshows? Telling people to just accept your logical fallacies is the opposite of constructive discourse.
&gt; It's the internet. View every claim made by anybody here or anywhere appropriately. **Evidence can be faked**, statistics formed from lies. Even if I supplied infinite evidence for any claim I made, it's all potential lies anyway. So no point doing so. Either believe me, or ignore me, or anywhere in between. Oh no, we got Alternative Facts in C++ :(
What do you need reflection for there? I can already write those kinds of tests right now. 
I think your choices are pretty much: * Use Qt - you can have cross-platform C++ code, including GUI. If you can live with non-native GUI, as well as the Qt licensing, then this is an option. * Write all the core code in C++, and use the native language for the GUI. This way you get the best-looking and best-feeling apps from a UX perspective, and they will be fast too because all core code is C++. I think this may be the "Dropbox-approach". I don't see any other viable approach (or any approach with C++ for that matter). For other languages, I even went as far as trying Xamarin once because I thought it'll be ideal for rapid-prototyping an app but they didn't even have a real GUI designer and designing all UI in xaml by hand seemed horrible for somebody that has never done it before. So I was pretty disappointed and gave up on this approach quite quickly.
Re: 2 Unfortunately, that doesn't quite match my experience. I've submitted bugs on connect.microsoft.com in the past, and I usually don't hear back for at least a month or more. Even if I do hear back from someone, he/she doesn't always follow up. It's extremely frustrating when I've spent a week going through the effort to supply a minimal repro case from a fairly large codebase. In those cases, we almost always end up doing // TODO: Broken in VS2012 with optimized builds #if _WIN32 #pragma optimize(off) While I'm on the topic of MS Connect, that site *really* needs a usability overall. It's painfully slow, and difficult to navigate. One time (around the VS2010-&gt;VS2012 transition), a couple of issues that I reported just completely disappeared from my dashboard. That was really annoying since I wanted to know what the outcome was! (Granted they were C++/CLR related, but still). Don't get me wrong, I'm very appreciative of the work that has gone into VS over the past several years. I just wish bug submission and feedback was a bit more pleasant. MS Connect really feels like a black hole to me.
There are many, many attractive titles. Some seems similar to CppNow
&gt; Not currently. Any imminent plans on supporting the reification operator (with whatever syntax) in your clang fork? &gt; It's not entirely obvious what you would be able to do with a template. &gt; If you have good ideas, I'd love to hear them. In my opinion it could be pretty usefull to be able to get the full template argument list of a template type. E.g. one should be able to get the information that in the type `type1&lt;type2&lt;type3, type4&gt;&gt;`: * `type1` is a template type with the template argument `type2&lt;type3, type4&gt;` * `type2` is a template type with the template arguments `type3` and `type4` I am not totally sure if it would be a good idea to add support for reflecting template types (maybe there are some negative consequences I haven't thought about), but it would totally be a nice feature. Are there any arguments against this that comes to mind? I also found a intrusive solution to my problem. But having reflection for template types and the reification operator would enable me to also write a non-intrusive solution :) #include &lt;cppx/meta&gt; #include &lt;iostream&gt; template &lt;typename T&gt; class type1 { public: constexpr const char *templ_arg() const { return $T.name(); } }; using type2 = int; class foo { public: type1&lt;type2&gt; bar; }; int main() { for... (auto v : $foo.variables()) { std::cout &lt;&lt; "type1 = " &lt;&lt; v.type().name() &lt;&lt; std::endl; std::cout &lt;&lt; "type2 = " &lt;&lt; (foo{}.*(v.pointer())).templ_arg() &lt;&lt; std::endl; } } 
&gt; Why not choose something more definite? I think it would be controversial. And if it's controversial, it won't pass a Boost peer review. v2 Outcome deliberately doesn't implement at all stuff which in v1 was found controversial, it has pared off all that stuff, but hopefully left the door open to end users to extend Outcome to fit their own needs. I know that's a cop out, but such is peer review, it's conservative. Is your goal just to be in Boost at any cost? Why not design for users, instead of designing to just be in Boost? For the record, I don't like this new library at all. I **liked** Outcome v1.0, and I was using its 'expected' implementation. Now it's gone.
It seems Connect is being phased out; any new submissions get the following copy/paste response: &gt; Thanks for your feedback. Now we have built a better report a problem site(https://developercommunity.visualstudio.com/index.html) for VS2017 bugs,please launch VS2017 and sign in, then report the problem using the VS IDE in-product Report-a-Problem tool to us by right-clicking on the feedback icon next to QuickLaunch, or by choosing Help | Send Feedback | Report a Problem from the main menu if you encounter a problem with Visual Studio in the future. For more information, see details from https://docs.microsoft.com/en-us/visualstudio/ide/how-to-report-a-problem-with-visual-studio-2017. Apparently this means that you now have to install the IDE in order to submit a bug report for the compiler. Amazing...
Nice, lots of great stuff to learn
This depends. You have code execution and data. If you are running on a system with unified memory, you've solved the data problem and only require a framework capable of generating the required architecture code. If you don't have a system with unified memory, then you need data storage structures that are aware of how they are captured and where they are executing. This can be accomplished by specializing a copy constructor when performing a copy with a lambda capture. Ultimately you still need compiler support. With [RAJA](https://www.github.com/LLNL/RAJA) (an abstraction layer similar to Kokkos but focusing more on the execution abstraction/generalization) they support execution on multi core CPUs with OpenMP and GPUs with CUDA and OpenMP4.5 backends. Obviously it's a reduced support space but it works generally well. I've worked on an OpenACC backend for RAJA and am anxiously awaiting the next release of PGI Compiler to test it out. Disclosure: I contribute to RAJA
And for MSVC it's just `/D_ITERATOR_DEBUG_LEVEL=0` to remove them for debug mode (or `/D_ITERATOR_DEBUG_LEVEL=1` for a compromise: keep checked iterators but disable iterator debugging). This whole thing really seems to be making a mountain out of a molehill...
Yeah, I understand what you're saying. The Connect site isn't a black hole, per se. It's more of a wide open sky where you're looking for one little asteroid. We've got a team that works at the VS level triaging these bugs and sending them to the right teams. It can take a while to get to the VC++ team. In our defense, we get a *lot* of really questionable issues raised in Connect. It's hard to sort the wheat from the chaff. If there's a VC++ Connect bug that's not getting attention, shoot me a mail or a DM with the link. We can only be responsive to bugs we know about. (Yeah, it's still our--Microsoft's--problem, but there's only so much the VC++ team can influence that.)
I use C++ Builder from Embarcadero. Uses the clang compiler on iOS, Android and Windows. Still uses a non C++11 compiler on macOS. Works very, very well. 
&gt; assume that C++ actually allows vector&lt;const int&gt; although it does not, that is another beautiful feature of the language to complain some other day `container&lt;const T&gt;` is a bit silly because you can (generally) make a copy of the element you want to "mutate" and then change the element in the container to be your new element. e.g. with vector, something like: auto mutable_copy = *it; mutate(mutable_copy); it = vec.insert(it, mutable_copy); vec.erase(++it); (For similar reasons, to be able to delete a git branch in Visual Studio Team Services, you must have the *force-push* permission. 🙂)
oh, fantastic. I didn't realize it. Thanks for letting me know!
Consider [Game and Graphics Programming for iOS and Android with OpenGL ES 2.0](http://p2p.wrox.com/book-game-graphics-programming-ios-android-opengl-es-2-0-676/) - Cross platform C++ for iOS and Android. Drawing using OpenGL. Minimal platform specific wrappers. I even got the code to compile and run on macOS, also.
[removed]
Ah but you have your own Channel 9, so once C++17 is finished, your fans want a video then!
I use doxygen. I like the use of markdown and it's fairly versatile and customisable. I dislike how every template specialisation is treated as its own class and that it misparses unnamed arguments.
To all involved: Heated technical debate is fine, this is edging beyond that. This sub-thread of discussion is done.
A general reminder: r/cpp is a free speech zone, but let's try and keep it civil folks!
Let's stay on topic here. Actual politics have no place on r/cpp.
Even with `/D_ITERATOR_DEBUG_LEVEL=0`, std::vector is 6x slower than optimised code, compared to 2x slower for hand-rolled. Not good enough.
I really wish the committee would take a step back and refocus on what is not expressible in the language. For example I think coroutines and compile time reflection are a great use of time but stuff like the filesystem abstraction is nice but not worth the time.
Other people tend to care about having good diagnostics in debug mode. If that costs performance - so be it. Most programmers I know would much rather quickly find an issue than have quick performance. 
Same here. It's not perfect, but it is plenty good enough to convince my customers that an appropriate mountain of paperwork is being generated. It has neat little diagrams for them to stare at, and lots of text that nobody reads or understands, but nonetheless convinces them I must be doing this documentation thing properly. And since that saves me a lot of time with the QA people I'm generally very happy with it. As for me and my team, we just read the comments in the source code directly, and don't refer to the Doxygen docs at all. 
&gt; the hardest part is to cross compile third-party libraries (eg. Boost, Curl) Oh god, yes!!! 
Looks like a cool toolkit. Though in the treeview example shift selecting doesn't work as expected. When starting from the bottom element and selecting a further up one it then uses the further up one as the basis for the next selection while it should use the first selected row.
You sir just got software engineering from a marketing/qa/customers perspective right.
My favorite is the one I'm working on, [standardese](https://github.com/foonathan/standardese). I dislike that it currently has a couple of rough edges I need to polish, but I like how it can handle a lot of C++ idioms. You can find sample documentation here: &lt;http://type_safe.foonathan.net&gt;
Obviously it all depends on what your app does, but there is a good chance you will need to write some platform-specific code at some point (eg. asking permissions on Android). However, doing the client/server approach like Dropbox is not required per se; I linked to it because I found it an elegant solution. In our case, bindings for Java on Android is done using SWIG; for iOS, you can directly link with native code. This approach suffices. IMHO If you plan to write a simple application with a "usual" GUI (input fields, buttons, etc.) and simple business use cases, then you're better going platform-specific. Yes you will have to write your code twice, but it will be faster and easier to integrate and debug, and you will save time on the long run. If you have complex and/or expensive (in terms of computational load) business rules, but relatively simple GUI, then probably it is better to have a common shared library written in C++, and platform-specific GUI. You treat your C++ library as a third-party dependency from your platform POV. Again, this is my opinion. 
Don't know for MSVC but GCC has a -Og optimization mode which tries to optimize just enough to keep debugging fluid.
Wait, so thus `int (foo);` is also valid syntax? TIL
And i was wondering why so many programmers hate c++ :D
The link https://type_safe.foonathan.net/ is not opening.
Sorry, no https.
&gt; There are really only two reasons: compilation performance, and because you might have class A calling functions from class B and class B calling functions from class A I agree those are reasons why people split between headers and implementations files. But, they aren't the only ones. In fact, in many cases where I've seen the split, the entities directly involved aren't classes: they are functions. The primary reasons aren't compilation performance. &gt; maybe that's outside the scope of the proposal itself? "build systems" aren't a language construct, so the language standards text is silent on that matter. However, many of us involved in the module system development have also keen interests in module-aware build systems and packaging. 
Yes, this beast is waiting for a modern-world-friendly license to be unleashed. Same goes for Delphi.
some programmers don't realize that most vexing parse is actually a good thing
Consider me one od those. Why is it good? C# and D managed to avoid it
I continue to be of the opinion that throwing move constructors and throwing default constructors are an abomination. Whether they come from standard library implementations, or ill-advised home grown libraries should make no difference. In the long run, they make good choices difficult to implement properly and they introduce confusion that could be avoided.
&gt; Remember when Bjarne listed his wish list for C++17 and he got nothing from that list? You will get down voted for pointing out pain points with the current process. I gather you don't mind -- just pointing out the obvious :-) &gt; For auto they went over some yuge codebases and tried to see the impact This one was truly hilarious. Good luck! 
I can't think of a way to express A a(B()); where A, B are types, differently. While it may not be a very useful construct it's nice to have it possible. I'm sure there are other examples. I see MVP as a way to disambiguate things that can be done in many ways from things that cannot. Choosing the second one still allows the programmer to do first one, but requires different syntax. If it was the other way around this would not be possible.
Actually, I do think that it was way overdue that the standard library had an abstraction for filesystem. We waited for far too long. We should be really thankful to Beman's leadership here. Your point about what is not expressible in the language is well taken; but we do need to cover basic facilities - filesystem, networking, etc. This is 2017. Every other contemporaneous high level language makes those basic facilities ready available.
I've heard this "why do you care so much about the performance of debug mode?" many times, and each time it puzzled me. If I can't use the debug mode to complete debugging trying to find a root cause of customer-reported bug, because it takes days to complete **when it completes** then that debug mode is useless to me for practical purposes. Unfortunately, more than once I had to abandon the debug mode uses of certain standard library implementations because they were just pointlessly expensive to use.
I think the problem here is lack of education on different ways of declaring a function pointer, most people know: &gt;&gt; void foo( A(*B)() ) syntax while the problem here seems to be that the above can also be expressed as: &gt;&gt; void foo( A(B() ) ) The problem continues,the pointer to the function can be called with: &gt;&gt; (*B)() but also with simply: &gt;&gt; B() The solution to this problem is to teach both ways of expressing pointer to functions.
I always wondered why we need extern function declaration within a function. void foo() { /*extern*/ void bar(); } Isn't this inherited from C? Could we live without it in C++?
This is a really interesting little example, thank you. Out of curiosity, I tried it with GCC 7.1 and Clang 4.0 (libc++) on my MBP to see how they handled it: Compiler | File size (bytes) | Symbols --- | --- | --- GCC -O0 | 550,604 | 1051 GCC -O3 | 105,572 | 244 Clang -O0 | 409,328 | 725 Clang -O3 | 4,248 | 3 (!) Clearly Clang is "cheating" at -O3 by eliminating everything, which is actually pretty impressive. Both compilers seem to do better than MSVC, but 700-1000 symbols in debug mode is still pretty heavy. I completely agree that a language-level variant type is sorely needed. The current library-level variant has its place as the sum type equivalant of `std::tuple`, but what we need is the sum type equivalent of `struct`. David Sankel has published a few papers and made a few presentations about a language-level variant and pattern matching, but I haven't heard much lately. And sadly these things don't seem to be on anybody's C++20 wish-list, so I guess we'll be waiting until 2023 at the earliest...
&gt; I completely agree that a language-level variant type is sorely needed. `union class` maybe, just like `enum class` brings enum in the typesafe C++ world ? 
Yes it will be better if these quirks are correctly taught. But it will be even better if these quirks do not exist at all.
I'd call it `enum union` personally, but I don't care what colour it is as long as we get somewhere to store our bikes :).
&gt; Is your goal just to be in Boost at any cost? Why not design for users, instead of designing to just be in Boost? I am very, very sympathetic to this viewpoint. If you know anything about my history with Boost, I have been a champion of the end user's experience. But as with everything in life, there are gatekeepers, and to gain entry you've got to pay the price. &gt; For the record, I don't like this new library at all. I liked Outcome v1.0, and I was using its 'expected' implementation. Now it's gone. I'm afraid Expected as you've seen it till now is also gone now too after the Toronto meeting. It's moving from LEWG to LWG for Albuquerque and it now looks quite different to before. Indeed it looks more like Outcome v2's result now, stripped and pared back. I haven't seen the detail of the changes yet, so I can't say more. And it may be that new Expected makes Result no longer necessary, or it could be that new Expected makes Result absolutely necessary. Those decisions are literally happening as I type this. We'll know more in a few weeks time. 
So what are the primary reasons, in the case of functions? And are they still valid in a module world? Having a declaration and documentation in one file, and a definition in another file, is a form of organisation we have only chosen _because we had to_, not because it was pretty or desirable. With the introduction of modules, now seems a good time to think about remedying that state of affairs. 
I thought about doing this a couple minutes after I replied. That seems useful. 
Template arguments aren't a property of templates, they're a property of template specializations. That's probably useful to have, though.
&gt; This is also just the tip of the iceberg when it comes to "Debug Matters". I've worked on projects where the code will no longer compile without optimisations because we hit hard limits in the compiler, linker or symbol generation. You end up with executables that are too large to operate on the target platform. You end up with enormous compile or link times that are comparable to optimised code, just because of the sheer amount of code generated. I think it's important to differentiate between what the standard says must happen, and what implementations choose to do. A raw pointer is a perfectly valid iterator type for a `std::vector` according to the standard, and will likely be just as fast in debug mode as in release mode. The fact that implementations choose to use more complex iterator types with bounds-checking etc in debug mode is up to the vendor, not the fault of the standard. Should the standard *mandate* that raw pointers are used, in the name of debug performance? Of course not. Microsoft have chosen to use debug iterators because a lot of their customers find them useful. Otherwise, why would they bother? Other vendors make different choices. The fact that the standard is flexible enough to allow both is a strength of C++, not a weakness. It's also worth pointing out that debug-ability vs runtime performance is a sliding scale, not a binary choice. GCC for example offers the `-Og` switch which is intended to provide a good balance between both. Other vendors could offer similar switches if there was sufficient customer demand.
That's because of this syntax: direct-declarator: identifier '(' declarator ')' [...] declarator: pointer? direct-declarator So that pointer to function is possible: void (*f)(int); And so is pointer to array: int (*a)[4]; And as a side effect, the following is a simple declaration: int (i) = 42; But be aware that in abstract declaration, the following is not a simple declarator, but rather it's a pointer to function: void foo(int()); Which is some C legacy. It's the same as: void foo(int(*f)()); 
Well, it follows naturally from the problem being solved, doesn't it? In debug mode we want to be able to follow control flow, so massive inlining or reshuffling of code is undesirable. We also want to have debug checks, and those take time. Obviously it would be great if we could have all that without any performance loss but it seems unrealistic. Is 16x performance loss really a problem? Or should we be happy with a 16x performance gain in release mode? I can presumably write some benchmark that shows far worse figures in debug mode, but would it be meaningful? "This mode, which does not focus on performance, is not that great for performance" --&gt; yeah, well, about that... Anyway, you seem well-placed to do something about it if it bothers you. I'm guessing not too many people will complain about a faster debug mode either ;-) 
Do you mean the ambiguous grammar is good, or the way that the ambiguity is resolved is good?
obviously the second one
&gt; for 20% of the world China?
Awesome stuff! I wish I could go :-) PS: The "C++17 and beyond filter" is not working, or the category empty. PS 2: I suggest choosing a color palette a bit less hurtful to the eyes for next year. Most are okay but some just hurt ;-) Also does similar colors mean similar topics/sessions? E.g. all tones of green are ... "X"? It's not too obvious to me. But anyway great job and this is such an awesome program!!! Wow!
Oh man, looking at this schedule and problems already: constexpr all the things, or coroutines, what can't they do? It's going to be tough deciding...
Yes, they are still valid. &gt;Having a declaration and documentation in one file, and a definition in another file, is a form of organisation we have only chosen because we had to, not because it was pretty or desirable. With the introduction of modules, now seems a good time to think about remedying that state of affairs. Now is good time to think about architecture, but we won't get there with an oversimplification or denial of reality.
Do you use FM or VCL for gui stuff? How does it compare to Qt?
&gt; Well, it follows naturally from the problem being solved, doesn't it? Only after exaggeration. I've used other compilers and libraries in debug mode without such an extreme slowdown. &gt;Is 16x performance loss really a problem? It is the difference between finishing within a couple of hours, or waiting for a couple of days of more. 
Sorry for the dumb question, but I'm fairly new to C++. What do you mean by &gt;as long as you move the value somewhere else. I understand the difference between passing by value and passing by reference, but I'm not sure what you meaning by moving the value.
Not sure if it is going to be the same talk but "C++Now 2017: Ben Deane &amp; Jason Turner "constexpr ALL the things!" is online here: https://www.youtube.com/watch?v=HMB9oXFobJc
I agree, and I think STL agrees, but problem with real life is that nobody has infinite resources, not even MSFT. :) And rewriting &lt;map&gt; looks like no fun. BTW if u/STL is interested in dirty hacks: if that sentinel node is of fixed size and not large you could stick it on the stack. It would grow the size of map and friends, but maybe it is a reasonable temporary hack if you are allowed to change size of containers in minor releases. 
Oh nice, good catch. Thanks!
Indeed you can pass by reference or by value. But when passing by value, you can either copy the value from the caller, or move the value. Moving is significantly faster than copying. Here's an example: void foo(std::string){} std::string str = "hello"; foo(str); // copy the string from here to `foo`. // std::string's constructor must allocate a new buffer foo(std::move(str)); // move the string from here to `foo` // No buffer allocated, the string inside `foo` contains // the string "hello", but `str` is now empty. When you copy, some types has to allocate new resources. But moving is a bit like shallow copy plus settings the old value to empty to prevent it from deleting the newly acquired resource.
**Company:** [Hawk-Eye Innovations](http://www.hawkeyeinnovations.co.uk/careers) **Type:** Full Time, Permanent. **Description:** Our computer vision team develop highly optimized, real-time, computer vision algorithms to build systems which process billions of pixels per second. The graphics team work on 3D rendering and augmented reality in the challenging environment of broadcast TV. **Location:** UK - London, Basingstoke, Bristol **Remote:** No **Visa Sponsorship:** No **Technologies:** We use the latest C++ features supported by Visual Studio. QT and Boost are used throughout the organisation. Computer vision teams use CUDA, OpenCV and occasionally SSE/AVX. The graphics team are looking for skills in DirectX and OpenGL. **Contact:** Check out our [careers page](http://www.hawkeyeinnovations.co.uk/careers) 
Perhaps more important is that if everyone has a bunch of mix-and-match implementations of something then they are not easily interoperable on API boundaries, ie the whole "vocabulary type" thing. At a minimum they have different names/namespaces/member functions, and worse they may have somewhat different semantics. Interoperability between your own code, other code in the same company, github libraries, etc. is the real benefit of things being in `std`.
Maybe better for /r/cprog. I have a dream where there is no more GNU. Just freedom. I've read glibc can be replaced by musl in Gentoo.
That's precisely what I can't do in an update, and what I don't want to do at all.
Some time ago I've added a pragma to force clang to treat local variables and value parameters as const by default, and use the new `var` keyword to un-const them, and `movable` to treat them as non const but fail if they are modified in any way other than by moving. Works quite well, perhaps I have to publish that somewhere.
No need for it I think. The app can consist of a static library linked into an executable, perhaps with a trivial `main.cpp`.
&gt; You will get down voted for pointing out pain points with the current process. I gather you don't mind -- just pointing out the obvious :-) I don't care, but the funny thing is that this cult mentality of attacking antibody who disagrees until they leave and then being all shocked when some outsiders have less than favorable opinion of C++ (how is that possible, all people with no downvotes on r/cpp think cpp is beautiful :P ) And I know that C++ has tragedy of commons problem, but I still feel that more could be done if committee had different priorities and actually admitted when they fail, instead of this corporate positive cult mentality and spinning failures as features. Anyway I hope finally all Big Three will be merged into C++20 so we finally get a major release after C++11.
&gt; We obviously do not talk about something that is statically traceable to null dereference. There's a lot of work being done in statically determining this as much as possible. So an assumption that the compiler can't or won't detect such things is iffy at best.
I know you're from games so I'll give you a hopefully relevant example - imagine you want to profile 5-10 different vector implementations for a variety of situations, and you want to test each of them on every component type in your game to decide which underlying storage is best for you. If you could use reflection to instantiate the test for each of the containers using each type defined in a Components namespace, that would probably be easier.
What's up with the GNU hate? I can see how some people might dislike the GPL but the GNU project has put out some amazing software like GMP, GCC, GPG, and GNUplot.
The standard is the source on that.
Funny enough, it isn't GNUplot. It's properly 'gnuplot' and isn't GNU at all. http://gnuplot.cvs.sourceforge.net/gnuplot/gnuplot/Copyright
This shouldn't be hard to implement in clang, enabled by a pragma. Maybe I should have a go at it.
Function i pasted above will withstand any amount of static analysis, when it lies in separate shared object. There is nothing iffy about that. Also, there are _waaaay_ simpler 'obfuscations' that defeat today's (and tomorrow's probably) compilers.
&gt; * The obsolete header &lt;sys/ultrasound.h&gt; has been removed. so it seems [the email was actually finished](https://cygwin.com/ml/libc-hacker/2000-07/msg00462.html)
I've always wondered what kind of impact const-by-default would have on C++. Did you find it useful? Do you end up with more `var`/`movable` than `const`? I would _guess_ that I have more const objects than non-const and that it would overall _decrease_ the amount of code, but I'm not sure. I think a lot of people would be interested in having a look at this.
In math, an [in]equality doesn't evaluate to anything, it's a statement of a fact. Thus `a &lt; b` is a predicate - it asserts that `a` is less than `b` in some context. **It's not a way to test whether `a` is less than `b`!** If we were to adhere to the spirit of things in math, you'd have something like `a ?&lt; b` for a test, as a shorthand for writing `isLess(a, b)` where `isLess` is a function.
Yeah I saw the slides which gave a nice insight on how you guys use the API but not so much on the implementation and where you guys started. I'm mostly interested in the module loading and event processing. And using the modules with the event queue. I've been prototyping both concepts separately but using something like node.js would be ideal since they seem to have both concepts taken care of + a lot of extra features like a javascript script engine. Thanks anyway for your comment.
[removed]
I'm using it for production embedded code. I've made `var` and `movable` also synonyms for `auto var` and `auto movable` although I'm not sure I didn't break the syntax by doing that. I might have made it work only in some contexts, I don't recall now. Works for me is the famous last words I guess. It's only enabled where needed, otherwise it'd be an impossible task as I'd have to rewrite all the library code.
If the separate shared object were written in C++, the runtime could theoretically perform analysis on the assembly and JIT-recompile the code. So I wouldn't bank on this staying so forever. Already you can add the IR to shared objects and do runtime specialization that way via LLVM, so it's not unthinkable that at some point in the future shared objects will be no barrier to optimization.
My code documents itself! How dare you suggest it needs separate documentation!
&gt; the runtime could theoretically perform analysis on the assembly and JIT-recompile the code. :) &gt;so it's not unthinkable that at some point in the future shared objects will be no barrier to optimization. Yup. Though with infrastructure at that level, it could probably just make sure that UB does not happen or abort the program safely on its own. Id love to have tools at this level available (and of useful performance), alas that world is probably too far in the future for me to witness it.
The "every component type" could use reflection (though I think that has its own problems*) but doesn't need template reflection. :) Here's some example code testing `vector`, `list`, and `deque` with a few types. Notably it tests `bool` and hence test funny specializations like `vector&lt;bool&gt;`. The reflection use in your example would just replace the `using types =` line way down in `main()` and nothing else really. https://godbolt.org/g/9ay8K2 p.s. writing that code was fun; I don't get to use all the new-fangled c++17 stuff much since I have to support VC++ 14.0 (VS2015) on any serious project. (*) Grabbing every component in the game assumes that a TU has all the components defined. Which means you've included/imported all of them, and that they all are defined in a way that makes them identifiable (e.g., all living in a namespace together and having a common base type, which is very much not true of many component architectures). It's far, far more realistic to expect a test of this nature to explicitly denote not only the container type templates, but also a typelist of the contained value types to test, which might very well include other special types like various smart pointers or whatever anyway. Which as illustrated above, is possible with C++17 already (and possible in C++11 too, just with a _lot_ more code to work around the lack of fold expressions and `if constexpr`).
Freedom for who? GPL is all about user freedom not Developer/vendor freedom
Even the website of the email domain is still active http://inka.de/ :-))) A piece for a museum! I'm curious, did you google for this, or were you actually around in 2000 and read/remembered that message?
I agree that there's little info on the actual implementation, the presentation was trying to show that is possible to create a C++ API that is very close to the native JS one and still have a nice looking code. Many network centric api in C++ are either too template heavy or too much OOP style with raw pointers everywhere. We've been trying to find a nice spot in the middle, featuring automatic memory management and limiting use of templates while still keeping decent performances. That itself took was filling the entire allocated hour for the presentation. I could expand on the implementation in other occasions or on my blog. It may take some time because unfortunately this is not open source software and I need some company authorizations to publish articles regarding it. The module loading has not been implemented at all, it was not considered necessary in a traditional C++ application. Replicating the node module system in C++ would be pretty interesting and challenging. Of course one should create a set of rules that every "module" should follow, to avoid global scope / namespace pollution. And then you may also think of wrapping everything into unique namespaces, similar to what https://github.com/iauns/cpm is doing. I would statically link all modules in a single executable to avoid issues with the memory for each object being allocated in different heaps. And if you prefer dynamic linking things become more complex but I think is doable. In that case you're probably forced to distribute them in source form all the time. I would explore is the use Runtime Compiled C++, general Hot reloading techniques or at the very extreme Cling (https://root.cern.ch/cling) for dynamic modules...using a real C++ interpreter will allow you to do things that are very similar to the Javascript modules system. if you happen to experiment in any direction please let me know, I am really curious on how approaches to solve this problem. Regarding the "event processing" I am not entirely sure of what you're meaning or what aspect is unclear to you. If you're referring to the concept of "event emitters" for objects, they have been implemented as a signal/slot. The "eventEmitter&lt;T&gt;" is basically a class containing - A std::vector&lt;std::function&lt;T&gt;&gt; of the "once" events from the node.js terminology. - A std::vector&lt;std::function&lt;T&gt;&gt; for all regular events. - Overloads for += and -= to subscribe/unsubscribe from events In the reality of implementation we are not using std::function because it allocates under the hood, we use an implementation that never allocates in any case and fails at compile time when one exceeds the predefined size. Emitting or processing such an event means just calling each registered std::function for that event emitter instance. If you're referring to the IO event processing, we're using libuv primitives that are all old-style C callbacks. When you read a file or do a network operation or ask for a timer, the libuv api allows you to register a C callback parameter that will get called when the operation has been executed. Internally each of these C style callbacks in turn calls at some point one or more of the event emitters described before. The initial setup of an app means registering all callbacks for I/O operations and then waiting in the event loop (uv_run in libuv terminology) until there are events to process. When there are no more events, the process exits. Using hypothetical modules with event queue could be no different from using the event queue from anywhere else, you just register all event handlers on relevant objects on the current event loop. The current event loop is implemented as a thread local variable, to avoid passing a pointer to the event loop everywhere, just like node.js is implicitly doing by adopting a single thread model. Writing applications that have a central event loop ends up being very pleasant, even when writing simple command line utilities for system automation or small scale software. You can for example spawn secondary processes and register a timeout that can interrupt the operation in few lines of code without resorting to using threads and similar stuff. Have fun with your research!
Very detailed trip report! &gt; It’s important to note that the removal of AFTs was not a rejection of having a terse syntax for defining constrained function templates in general. There is general agreement that such a terse syntax is desirable; people just want such a syntax to come with some kind of syntactic marker that makes it clear that a function template (as opposed to a non-template function) is being declared. I really don't understand this desire for a syntactic marker. IDEs are perfectly capable of semantic highlighting - just use different colors for normal function and template functions. This is very useful in a lot of places, for example I use the same name for member variable and constructor parameters. You can then write `S(int x) : x(x)` and see which variable is what at a glance due to coloring.
It may be slightly off-topic, but it seems to me [musl](http://www.etalabs.net/compare_libcs.html) is much better, in every aspect. Any particular reason why distributions are all still sticking to GNU libc? It's supposed to be a drop-in replacement?
What libraries are you using? I prefer libcppMakeAirMechClone for its simplicity: just call cppSelfDocumentingVariableNameFunctionMakeAirmechClone(); from main(), and when you compile, link, and run the executable, it procedurally generates a clone of AirMech. If you want to customize how closely the cloned version resembles the original, pass a struct cppSDVNMACvariance by reference (pointer), containing a number of clamped floats indicating 1 for exactly like the original software behavior and 0 being completely random. Example: /// Compile and link against libcppMakeAirMechClone. #include &lt;mac.h&gt; int main(){ cppSDVNMACvariance *settings = new cppSDVNMACvariance; // Define settings here. cppSelfDocumentingVariableNameFunctionMakeAirmechClone(settings); /// Output is binary executable. } I'm sorry. This is a joke that went too far. 
haha no I googled because I wanted to know why in hell would there be something related to Gravis Ultrasound in GlibC
Haha! :-) An awesome find.
VCL is not cross platform, only FMX is. Never used Qt, so can't help you there. 
Wauw, thank you for your extensive explanation. I look forward to the blog posts already! I can invite you to my GitHub PoC if you want. it is not public yet since it's far from done and I haven't had the time lately, but I've listed a lot of articles on the subject in the readme which might be of interest to you. It's based on the plugin architecture and uses dynamic libraries. Because then you can swap them in and out and other people can even write modules like with node.js! The functionality gets exposed to the main program through C functions for Application binary interface(ABI) compatibility. If there is any more information you would like I would gladly answer them if it is within my limited knowledge of course. Thanks again for you're explanation.
But then again I often read code outside the IDEs: in diffs, webpages, emails and books. 
This point has definitely been raised. Some of the responses have been that not everyone uses editors with semantic highlighting capabilities, and that people often look at code in non-editor contexts like code review tools where such capabilities are a lot rarer.
That sounds awesome, cool! :-) Can I suggest you next time stream from a site that does let you watch with AdBlocker on? liveedu.tv gives an error popup and won't let me watch unless I turn off AdBlocker.
QDoc, open source, is excellent (just browse the Qt documentation to see) http://doc.qt.io/qt-5/qdoc-index.html It is basically a "doxygen deluxe"
Even if musl is a dropin replacement and standard compliant, I wouldnt expect most distributions to work with it. For one thing I expect that there are glibc extentions or unspecified behaviour that are used by todays distros. There two more reasons which influence lib usage: supported platforms, license type.
And MIT/BSD/etc. are about freedom for *both*.
Regardless of where you fall in the debate, that's an incredibly disingenuous claim to make. MIT/BSD/etc. *do not* by any stretch of the imagination protect the particular rights given to users by the GPL. It's fine if you don't care about those rights, just don't muddy the waters with nonsense.
&gt; &gt; Freedom for who? &gt; &gt; &gt; &gt;GPL is all about user freedom not Developer/vendor freedom &gt; And MIT/BSD/etc. are about freedom for both. Both vendor and developer (until vendor locks it up).
&gt; (*B)() and also by `(**********B)()`, if I'm not mistaken.
I don't think it's really so important to recognize templates that they need to be called out that way at all. Do we need function overloads to be called out in source? Do we need function outlining to be called out when the compiler does that? I think the answer is no.
glibc being 5 times as old (and heavily used thought its lifetime) mean its much more reliable. Pretty much everything depends on libc, so switching out implementation without good reason is at least unwise, imo.
Why would they???
Underscores are not allowed in domain names.
&gt; It's supposed to be a drop-in replacement? It's explicitly *not* a drop-in replacement. It requires using specific `#define`s to get access to various functions that are not part of ISO C by default, even if you use the right `#include`s. While musl libc is, strictly speaking, standards compliant in doing this, it is still much more annoying to use. And, it's not like this is the type of annoyance that at least increases the portability of your program... if you start using these standards `#define`s, FreeBSD will start *removing* symbols they'd otherwise define, so you have to play a delicate dance to get musl libc to offer the symbols you need while not having other OSs/libc *remove* other symbols you also need.
I've written large non-trivial apps for iOS entirely in C++ including native UI elements, no objective C, no interface builder, cross compiling from Linux (no VMs), even signing the code as well. It's not for the faint of heart, but it _IS_ possible! I've started to try and figure out how to do the same on Android because I really don't want to have to deal with Java. Then, ugh, I'll have to do windows.... I intend on making a write up on how to start with a clean Debian install and end with installing apps to an iPad**, only roadblock right now is I can't seem to get lldb to recognize remote-ios as a target. (Well, that plus I hate writing documentation. ;-) I'll second many people here, I've used Qt before on a large project and have mostly only positive things to say about it. But I've not used it since the QML age or for anything mobile. **happy to share details in a PM, or you could wait for me to eventually write the howto.
Doxygen + sphinx + breathe. Some examples: https://esa.github.io/pagmo2/ https://bluescarni.github.io/mppp/
What would be gained by removing it? Actually if we remove it, we lose compatibility, and so we lose the option of compiling C libraries that use this as C++.
Well this is timely. I invite all those who are ready to stop the insanity, tedium, and agony and start producing good, useful documentation for C++ code to attend my presentation at CPPCon 2017coming up in September. https://cppcon2017.sched.com/event/BgtD/how-to-write-effective-documentation-for-c-libraries-with-minimal-effort Robert Ramey
+1 Doxygen
They're not allowed in host names (RFC 1123); they're totally fine in domain names (RFC 2181). I.e., the DNS protocol itself doesn't care.
Just stuff all the standard library concepts inside their own namespace, say, `std::Concept`. Then all the pedants can write `std::Concept::Sortable` and everybody else can do `using namespace std::Concept;` at the top of the implementation file, which should be enough syntactic marking.
&gt; But then again I often read code outside the IDEs: in diffs, webpages, emails and books. the question is more : is it really important to differentiate visually template &amp; non-template functions ?
My code was hard to write. It should be at least as hard to understand!
Good point. I guess the main problem with function templates nowadays, where obscure metaprogramming techniques are not as necessary, is that we need to define them at header files. I assume modules will fix this and the need for differentiation will diminish over time.
The archive with links to videos and repository (of slides) is here: https://cppcon.org/history/
&gt; happy to share details in a PM I would feel guily wasting your time if I do not decide to follow up and do it. :) So ping this thread if you ever write howto... or even better post a link in r/cpp
Some linux distros need to do that with glibc as well (`man feature_test_macros`)
I assure I wasn't able to spit out that comment without having read that man page. In fact the Linux man pages are a good reference on how to make sure that *musl* will actually expose a symbol you want for basically every X/Open, POSIX, BSD, or SUS function that glibc or Linux support. glibc is usually not as picky as the man page would have you believe though, unless you compile with `_STRICT_ANSI` (which I believe is implied with gcc using the `-ansi` option). They show the right way to do it in the man page, but often just do what you obviously meant if you're not in glibc's strict mode.
Binaries for windows are available on [sourceforge](https://sourceforge.net/projects/boost/files/boost-binaries/)
As others have said, the GUI stuff isn't really cross-platform. Android is obviously Java and iOS is Objective-C and/or Swift. Objective-C++ is a thing though, so that helps. But what you can do is write a lot of the core logic for your app in C++ and then treat it like an SDK. Making an NDK sdk is pretty straightforward but you'll have to go through the JNI, and it's even easier for iOS. Keep in mind that any hardware-related things you want to do will likely be platform specific, with networking being a notable exception (although Apple encourages you to use CFSocket, you can use BSD sockets just fine on iOS). What I've done in my cross-platform codebase is basically just made all of the (non-gui) platform-specific stuff compatible with an interface, and obviously the correct version is chosen as part of the build properties. 
One reason is that &amp;&amp; means different things in a template and non-template context. One is a forwarding reference and the other is an rvalue reference. And they are used differently.
Came here to make the same comment. std::vector is a value type, but an int * refers to something outside itself. I think the difference boils down to this.
I would say yes. The rules are different for template functions, for one: std::size_t foo(Container x) { Container::iterator first = x.begin(), last = x.end(); return std::distance(first, last); } If this is in a header file, either it's wrong because there's no `inline` or it's wrong because there's no `typename` before `Container::iterator`. Practically, it would be a pretty frustrating experience to look at a snippet of code and not know if it was a template function or a regular function. Yes, plenty of people use IDEs, but that's not a requirement for writing C++ code. Sometimes I prefer to just work in a text editor. A language feature shouldn't require that I have a sufficiently advanced environment to be able to know what the code is doing. Plenty of online tools (e.g. a diff in a pull/merge request) will only show snippets of the code: to determine if something is a template function or a regular function shouldn't require looking back through the entire file and other includes. Finally, from a more abstract perspective, a template function in C++ is really a compile-time function that returns another function. In a fundamental way, I think it's different from a normal function. I think a visual difference in the syntax of that is useful.
https://www.reddit.com/r/cpp/comments/6q94ai/chromium_windows_builds_now_use_clangcl_by_default/dkwdd8l/?context=1
Diff/merge is easily done in IDE. Webpages and books can color highlight or add a comment / point out in text that a function is a template. You can copy/paste color highlighted code from an IDE to email, preserving the color. As others have said, in the vast majority of cases it is not important to distinguish between normal and template functions. And when it is, you can easily adopt a workaround.
I would say then those tools need updating. The language should not accumulate cruft to accommodate 10 year old tools.
&gt; Practically, it would be a pretty frustrating experience to look at a snippet of code and not know if it was a template function or a regular function. I actually can't remember if I have ever wanted to know whether a function is a template or not. The compiler will complain if there is an error anyways. &gt; Yes, plenty of people use IDEs, but that's not a requirement for writing C++ code. Sometimes I prefer to just work in a text editor. A language feature shouldn't require that I have a sufficiently advanced environment to be able to know what the code is doing. But an IDE is not anything advanced in 2017. IDEs were common 15 years back. You should not program in 2017 using 20 year old tools, nor should a language feature implemented in 2017 be designed with 20 year old tools in mind.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6ra1lk/what_is_the_best_way_to_learn_c_basiscs_at_home/dl3i3w3/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; If this is in a header file, either it's wrong because there's no `inline` Some linkers will catch that. Compilers won't currently, but I think it would be reasonable to add a warning for any non-inline functions in headers. Though it's not always an error since of course you don't necessarily include a header more than once. Still, catching it would be good. &gt; or it's wrong because there's no typename before `Container::iterator`. Compilers will catch that. Furthermore modules eliminate the problem entirely, and I definitely don't want to be stuck with more extraneous garbage in the syntax in 10 years just because of headers that no one uses anymore. &gt; Practically, it would be a pretty frustrating experience to look at a snippet of code and not know if it was a template function or a regular function. &gt; Finally, from a more abstract perspective, a template function in C++ is really a compile-time function that returns another function. In a fundamental way, I think it's different from a normal function. I think a visual difference in the syntax of that is useful. I don't agree with either of these. The language machinery is different, but from the programmer's perspective they shouldn't really care in most cases. For example when I'm calling a function I pretty much never care if I'm calling overloads or template instantiations. Almost the only time it matters is when things are going wrong and you're having to figure out why. I think the compiler output showing the appropriate cues will be sufficient. One other example of where it might matter is when the program is working just fine but you're trying to reduce code size/template bloat. Again, the tools you use to measure the problem should provide sufficient cues. There are some other reasons to consider the terse syntax carefully, but the header problem and vague feelings of dislike for the unfamiliar I don't think are good ones.
So... how's localisation there?
You're both correct. Now what? :-)
That seems obvious: no need for documentation.
This page was automatically generated using Doxygen: http://www.boost.org/doc/libs/develop/libs/beast/doc/html/beast/ref/boost__beast__http__async_read_some.html Here is the corresponding source code https://github.com/boostorg/beast/blob/852d2b487e56ca90a88f5f4e2b846529dd129b38/include/boost/beast/http/read.hpp#L134
thanks babe
&gt;t's wrong because there's no `typename` before `Container::iterator`. The proposal to make "typename" optional has passed with flying colors. It's right in this blog post. 
I'm sorry if I'm terribly wrong but can't you just name the file something.cpp?
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6rah91/notepad_saving_source_files_as_h_instead_of_cpp/dl3kp2h/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
&gt; Do we need function overloads to be called out in source? +1. Actually, more than 30 years ago, when overloading was introduced in C++ there was a requirement of an overload declaration with the `overload` keyword. It didn't work out. So the `overload` declaration was removed along with the keyword.
deleted ^^^^^^^^^^^^^^^^0.6149 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/48467)
So, by definition freedom is at the vendor's mercy. Doesn't sound like freedom for the user to me.
Maybe, something like `$message`, so I could just `#include` a .proto file right inside a C++ source.
For one, memcpy/memmove from musl are massively slower (at least on x86) then the ones from glibc.
Never mind the amount of security attention, fuzzing, etc, it has received.
MSVC has its problems, but most of them are due to it being still binary compatible with C++ compiled twenty five years ago or more. No other compiler vendor attempts such amazing backwards compatibility. My god, they're still using the same mangling scheme from the early 1990s with Visual C++ v1.0. It's an amazing engineering achievement. As much as people dump hate on MSVC, those people have never had to deal with C++ compilers outside the big three of clang, GCC and MSVC. Those compilers are generally awful, awful, *awful*. And in comparison to the profound brokeness throughout those compilers, *any* version of MSVC, or *any* bug or quirk in MSVC past or present is but a minor wrinkle. Definitely no abomination. 
&gt; wrong because there's no `typename` before `Container::iterator` Well, yes. But more importantly, it's wrong because `Container` *is not a type*! There is no such thing `Container::iterator`. You'd almost certainly use `auto` here anyway, but the correct way to write your example would be: std::size_t foo(Container x) { typename decltype(x)::iterator first = x.begin(), last = x.end(); return std::distance(first, last); } The fact that none of the existing responses pointed out this fact seems is an interesting data point in this whole argument. 
My understanding was that typenam was only optional in contexts where a non-type wouldn't be accepted, such as a return type or using declaration. Is typename optional everywhere? And to create a parsing ambiguity, it could be something like this: foo::bar * x Is it a static data member multiplied by a global, or is it a pointer declaration?
Not in the context of block scope variable declarations. There, you'll still need `typename`.
You've got alot of buzz words, I see no examples, quick intro, etc. I don't want to get to know your program by running it. Is there a quick intro to the "purpose" and showing it "in action"? As a community we are blasted with so many different products claiming to do so much, I don't have enough time to check them all out, give me the sale pitch.
Why does it seem to you that musl is much better ? What about the glibc features that musl does not have ? 
Thanks, but what 'buzz' words? Indeed, the tutorial for new users is lacking, and the current development relies on users already familiar with the analysis. http://www.openreliability.org has good tutorials for newcomers.
I don't think it's even needed for standard concepts as they use camel case while everything else uses snake case. So `std::Foo` is a concept and `std::foo` is a type. That being said, I think they are talking about user defined concepts. I still think it's not worth adding a syntactic marker for that though.
I guess you didn't read the thread in full that it's known to be an Standard issue, that's also [exposed in other implementations](https://www.reddit.com/r/cpp/comments/6q94ai/chromium_windows_builds_now_use_clangcl_by_default/dkykjah/?context=2), and that the word "abomination" [happens to show up there as well](https://www.reddit.com/r/cpp/comments/6q94ai/chromium_windows_builds_now_use_clangcl_by_default/dl1yoxk/), coming from well known C++ references. You can't simply compose basic data structures safely these days.
I'm working in a company that creates audio software for embedded dsp (in av-receivers, cars, cinema) that is also used on android/ios, as well as playstation/xbox. We create libraries that are compiled for every platform, we try to use as new c++ as we can, we even implemented some c++14/c++11 (library) features so we can use them for platforms that only support c++11 or c++03. We use cmake to create buildconfigurations for the different platforms, which helps a lot. For example: the android NDK includes a cmake toolchain file that makes it a lot easier. The documentation is not always up to date, but examples exist that are working and help a lot. Cmake toolchain files can be found for most platforms. We use the native api's on both android and ios to create GUIs, but we do not have much interface generally. E.g. on android we are in the kernel in a lot of cases with 1 settings activity/page that has to be native to have the exact same look and feel as the other settings pages. Other times we deliver the libraries to other integrators. We have created some demo apps with simple GUIs for the sales ppl. The general consensus amoung our devs is to use the native api's for GUI as it gives the best user experience + when trying to get the multiplatform GUI toolkits to work exactly like you want it to can be a hassle which would always happen when the project starts to actually gets used and sometimes they just don't do what you want, e.g. they don't have the latest native features covered yet. 
Like clockwork, any r/cpp post that is in any way about concepts devolves into a discussion solely about AFTs and the question of differentiating syntax. Cool. I, for one, am looking forward to the many other things that Concepts will let me do.
Well you did just post a link with no context, and the topic is pretty much "why we dumped MSVC". Regarding that problem with vector push_back, that sort of outcome is always going to happen when standard v1 gives guarantee X but standard v4 changes unrelated stuff which makes that v1 guarantee impossible to implement. Same happens in any really old code base, and the correct remedy is to refactor with breaking change. So what needs to happen, and will eventually happen, is that the current suite of STL containers get put out to pasture, and a new set of STL containers get made which fix all the problems in the old containers. SG14 is doing some work next gen containers. Boost has done some work there too. And LLVM/Chandler has too. They kinda need Ranges and Concepts done first though, so everything is gated on those two, including iostreams v2 which I'm far more involved in that the containers reboot. Regarding that abomination which Gaby mentioned, I couldn't agree more. But there are good reasons it can't be mandatory, I personally would prefer default, move and swap to *default* to noexcept same as destructors, and any v2 STL containers should simply refuse to compile with types that implement throwing default construction, move, swap or destruction.
&gt; Well you did just post a link with no context, and the topic is pretty much "why we dumped MSVC". At least I tried. You can see I didn't edit the link post, and it terminates with "?context=1", on purpose...
I can, but I'm wondering if it's possible to make .cpp the default file type when working in Notepad++, rather than .h
Sorry, I didn't realise this wasn't the correct subreddit for this kind of question.
Try Sublime, Atom, Vim or Emacs when you want an editor. Like really, do it. Vim and Emacs require a little more getting used to though.
Definitely you need rebranding. It's not the first time I've seen build2-relate links here with a bunch of downvotes.
I did something similar but the pipeline must be defined at compile time and forwards the args to the first function who's result must be consumable by the next function... Nothing production worthy but a fun experiment https://github.com/beached/function_stream/blob/master/tests/function_stream_test.cpp
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [beached/function_stream/.../**function_stream_test.cpp** (master → d04c7e3)](https://github.com/beached/function_stream/blob/d04c7e39f56706a07ea1b2c1b34095f2f3b90006/tests/function_stream_test.cpp) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dl44in1.)^.
This illustrates one of the major problems with doxygen. It's a templated function with 5 templated parameters. But there is no description of the type requirements on those parameters. A user cannot not look at this page and know what types he may or may not use with this function.
Any build system other than CMake gets downvoted here.
Sure I am interested, please add me! My github username is Pagghiu.
FWIW, here's another similar concept, except this one's an exokernel: https://github.com/ReturnInfinity/BareMetal-kernel
That's not true. I've been tempted on occasion to downvote you because you seem to think that everyone on this subreddit is out to get build2 and loves cmake. 
If this were my thinking then what would be the point of posting anything related to `build2` here? I think there are quite a few people here that are interested in alternative build systems (and not just `build2`). They are just not as vocal as the CMake community. This is who I post `build2`-related news for. And thank you for resisting the urge to downvote based on superficial things like this. In fact, I wish that instead of this whole thread we rather had a technical discussion. Like anyone wants to argue that our approach to C++ modules support is completely wrong?
&gt; A mailing list thread of 20 messages where we painstakingly discover that the user accidentally built our library with libstdc++ and his application with libc++ which all resulted in mysterious crashes (exception handling was not working properly) – this is a waste of life. Should be "discovered". &gt; (Linux, Mac OS, Windows, FreeBSD, etc) Should be "macOS", following Apple's official capitalization. &gt; Note also that build2 itself is multi-threads Should be "multi-threaded". &gt; one compiler invocations rather than two Should be "invocation". &gt; (different compilers, 32/64-bit, debug/release; and don't forget cross-compilation) A semicolon is not appropriate here. &gt; to this effect, we are running build.org You own build2.org, while build.org appears to be a completely different domain. &gt; (like system headers, compiler versions, etc) Should be "etc."
This article is slightly misleading. You keep writing "Ninja does this, ninja does that", but in reality you are describing how CMake generates ninja.build. Ninja itself is fairly stupid. It does not know anything about headers or whatever, it only does what is said in `build.ninja`. The only relevant special feature is the ability to load dependencies list produced as a byproduct of the rule execution. It is a choice of build generator (like Cmake) to use this feature or instead to generate a two-step build.ninja the similar way as you do in build2. So, contrary to what article implies, Ninja is not bound to operate the way the author describes. It is just CMake that generates build.ninja in this specific way.
Fixed all except macOS (I just can't sorry). Thanks for taking the time to write this down, much appreciated!
I disagree (but thank you for a technical comment). Header dependency extraction and change/rebuild detection has to be handled at the underlying build system, there is just no other way, not for any real project. You are saying that CMake could have handled all this itself, that is, extracted all the dependency information from all the source files and encoded it in the generated `.ninja` file. But that would mean CMake would have to re-generate it *every time* you change any of your `#include`'s. I don't think this will be acceptable for most projects. EDIT: &gt; The only relevant special feature is the ability to load dependencies list produced as a byproduct of the rule execution. It is actually more specialized, Ninja knowns about different dependency styles, etc, as described [here](https://ninja-build.org/manual.html#ref_headers). EDIT 2: Just for the record, I am not the one downvoting you. 
This is not necessary. CMake could split the build between two ninja files. The first one would extract dependencies, insert them into the second one, and run another instance of ninja with the second one, which does the actual compilation. This is a little convoluted, but I think it's possible. Another question if it's worth the bother. 
A good questions to ask is whether any other meta-build system uses Ninja this way? I am pretty sure the answer is "no".
Re edit: 'gcc' format is just a subset of makefile syntax. It is not very specific to headers or GCC. I successfully used it with some completely different tools, like latex. The msvc format is indeed very specific to headers and msvc. Edit: make syntax -&gt; makefile syntax.
Like most other people on this sub, I currently use CMake because everybody else does, though I don't have any particular love for it. If a new, easier to use build system could come along and sweep up all before it, that would be great. There seem to be a few contenders -- I've played with Meson a little and found it pretty good, for example. As far as build2 itself goes, integrating the build system and package manager sounds very interesting. It seems to work very well for Rust's Cargo, which by all accounts is a pleasure to use. I can't think of any reason why the same approach couldn't work equally well for C/C++ projects. Having said that, I'm afraid I really do have to take exception to using the single-character name `b` for the build command, though. Yes, having less to type is very nice, but reserving such a name for yourself seems a bit... obnoxious. Why not use a longer name by default and let the user assign their own single-character alias if they find it useful? 
We are trying to. We have a set of simple but fully modularized "Hello World"-class libraries working, including `import std.core`, etc. We've also tried to modularize some of the `build2` but quickly ran into duplicate standard library symbols (that we have reported to Microsoft/Gaby). So I would say expect some rough edges. &gt; Is it possible to conditionally uses modules on Windows only and old header system for other platforms? Yes, that's the way we set things up with `build2` (the hello examples, on the other hand, are pure modules). Come to my talk at CppCon for more information ;-). 
The reason for choosing `b` by default rather than via an alias is because of all the various places where one may need to run the build system: all the build bots, deployment machines, etc. So maybe a bit obnoxious but if this is everyones biggest issue with `build2`, I can live with that. Plus, it is really easy to change the name: $ bpkg install build2 config.bin.exe.suffix=uild2 $ build2 --version 
Haven't tried it myself, but from what I've read the performance isn't that great (yet). I'd imagine you may at best get marginal benefits. ^(*hopes to be proven wrong*)
Does anyone now how the per thread cache for malloc compares against jemalloc or tcmalloc?
I bet it turned out to be a one liner....
btw: r/UniKernel/
 int main() { int *p = (int*)malloc(sizeof(int)); int *q = (int*)realloc(p, sizeof(int)); *p = 1; *q = 2; if (p == q) printf("%d %d\n", *p, *q); } `$ clang -O realloc.c ; ./a.out` `1 2` From: https://blog.regehr.org/archives/767 (also people on r/cpp will tell you not to write code like your original example).
Did you notice that two of those types are hyperlinks?
The function itself is fine, but whatever you are doing with it seems like a mess.
A well educated coder who cares about his craft, knows the tools, and how to write properly. Everything else are just formatting helpers. 
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6rejr0/whats_the_worst_that_can_happen_when_you_use/dl4jo91/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
https://www.youtube.com/watch?v=Evz_J9nAL6Y#t=17m16s I do not think it was Finnish, probably Danish. ;) 
The worst thing that could happen, I suppose, is Player contains pointers that aren't updated when Player is moved inside realloc, so a crash. The next challenge is ensuring every memory allocation has a corresponding deallocation. This is much easier to do with std::unique_ptr or std::shared_ptr. But why not just use some standard container instead? 
Neat! I had no idea that was a thing. What header(s) do I need to include to use std::move()?
I'm a huge fan of sphinx. But it's python only, no?
I'm using CMake and haven't tried build2 yet, but I think the discussion about build systems (and package managers) is an important one for the C++ community. Thank you for posting and keep on posting. You are extremely knowledgeable about this subject and your technical comments are always interesting to read.
http://en.cppreference.com/w/cpp/utility/move
That's one of my peeves with the language. Sure, looking at this piece of code you obviously can't tell... But the compiler should, because it can't do anything without checking ```foo```'s definition first.
&gt; As far as build2 itself goes, integrating the build system and package manager sounds very interesting. It seems to work very well for Rust's Cargo, which by all accounts is a pleasure to use. I can't think of any reason why the same approach couldn't work equally well for C/C++ projects. Meson (which you mentioned) also has this and it works roughly the same way as Rust's Cargo. See [here](http://nibblestew.blogspot.com/2017/07/managing-build-definitions-of-big.html) for an example.
I love meson, hopefully it gains more traction.
Is this a legal construction? If it is, then you're arguing for another layer of 'here be dragons' patterns besides ones with unspecified or undefined behavior; namely those deemed bad in a bill of responsibilities. Not sure how to take that. If however this construction actually doesn't make sense: amend the spec to explicitly say it's undefined behavior. Perhaps a bill of rights could then be: "If it's not explicitly demonstrated in the spec to be a legal pattern, use it at your own risk.". Again way too strong for what you're going for, I think :scratch head: **Edit**: Something random but relevant that just occurred to me. With the way the std process was talked about around C++14 (tick, tock, tick, tock; one big release and one smaller fixup/library release), the case could be made that for a number of big features they would be accepted into std but expect potentially major fixups (say, at the level of a _minor_ version bump in semver). Features wouldn't be ever removed or drastically (not even close) changed; but some specific aspects that are not source code facing would be fair game (and to a smaller extent source code facing ones). `std::accumulate`, if it were added in C++11 for example, could then get a fix in the fixup std release immediately after it was added. An example from another comment necessitates breaking ABI; could be the same? That seems stupid and too much hassle and effectively leads to everyone avoiding the new features until the next 'small' release :Y
Or, you know, just use MSYS2.
I have found that having one cpp in my libraries that includes the other cpp of the library, then just adding that cpp in my main project has decreased the build times allot and has improved run time performance. This method is cross compiler and has most if not all the benefits from modules.
I made https://teapot.nz for building complex C++ projects. We currently use it on several projects and it's made some complex deployments much simpler.
As a person that has tried seriously: autotools, cmake, scons, waf, tup, cmake, meson, tup and plain make, here it goes my recommendation: If you are not happy about cmake, you should try meson. Tup also made me quite impressed as a plain make replacement for the task of building, but it just cannot be used as a full tool for multi-platform compilation without putting a lot of work on it (I did myself once, but it is not practical). CMake is ok and can do lots also, of course, but I decided to stay with Meson and I do not regret. Scons was slow as hell, and waf was ok actually, the problem being that when I had to add testing and other things, there was a code explosion there, also not clear about working directories and so on. Autotools was the best at cross compilation and it is still strong at this. Meson supported me well in this area as well. But cmake was notorously bad at this last I tried (around 1 year and a bit ago). I encourage you to try meson if you are not happy with cmake. As for build2, I want to try it myself once I am confident I can compile projects for my setup: windows, linux, mac, android and ios.
I share your wishes :)
&gt; Visual Studio 2017 will not work. Why not? Is this because Clang can't find the relevant paths? I'm happily using Clang/LLVM 4.0.1 with our development builds of VS 2017 15.3+ (note that my environment is set up by internal batch files). VS 2015 is super terrible nowadays, you should avoid using it. With VS 2017, you can use `-fno-ms-compatibility -fno-delayed-template-parsing` with the STL.
It works fine from the commandline; it's VS-integration that doesn't (yet) support VS2017. Fortunately, fixing the installer locally as a workaround is trivial.
This is known as a [unity build](https://en.wikipedia.org/wiki/Single_Compilation_Unit), and won't have any better runtime performance than a regular build with [LTO](https://en.wikipedia.org/wiki/Link-time_optimization) enabled.
I use clang-cl bootstrap clang success, use Visual Studio 2017 libs, use *-fms-compatibility-version=19.00* Thanbks /u/STL libc++ is work. My project: https://github.com/fstudio/clangbuilder NinjaBootstrap and NinjaIterate Engine will auto build libc++, require Visual Studio 2017 
Thanks, I appreciate your kind words.
&gt; it works roughly the same way as Rust's Cargo IMO, Cargo is a pleasure to use largely because it is a native toolchain: it is written in Rust and it does building itself. As a result, you don't need to install a scripting language and an underlying build system everywhere you want to use it and you get a consistent build experience across all the platforms.
True, but he added more context: &gt; and allowing the pointer to outlive its meaningful scope If somehow outlives or needs to outlive its owner something went bad or shared_ptr needs to be used. 
The compiler might not know `foo`'s definition yet - if `foo` is dependent on a template param. So the compiler would need to stop and say "I can't actually parse this template at all, I'll just gather the token soup, and save it for later, once I know T". And that typically prevents a bunch of other things from being possible, like showing errors to the template author instead of the template user. We would like to avoid token soup.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6rie1i/cpp_http_server_based_nginx/dl5a2xt/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I tested 2017, but had no luck compiling with `-fms-compatibility`. Thanks for the heads up, by removing all Microsoft compatibility flags I got it to work. I'll update the article.
Sphinx allows you to write documentation for languages other than Python, but on its own it will not automatically extract the docs from the source code. But, there is a sphinx extension called "breathe" that is able to parse the XML output of doxygen and to make it available in sphinx documents: http://breathe.readthedocs.io/en/latest/
Potential typo: in your PersonName example you use $value, but if I understand correctly you should not use the $ in this context.
The obvious downside is that it works only for Rust (and simple C AFAICT). If you need to ship a multi-language project with dependencies, the experience gets a lot rockier.
&gt;It may be slightly off-topic, but it seems to me musl is much better, in every aspect. It's pretty much slower across the board at performance measures, sometimes quite dramatically. GlibC is particularly much better in the allocator which is often quite a bottleneck. If you switch from glibc you'll probably see a substantial slowdown. OTHO, it sounds like glibc has substantially worse error checking with respect to resource exhaustion, and is larger. Sounds like musl is more suitable for a resource constrained machine, where as GCC is going to be better for something larger. I can't remember the last time I ran out of swap. Edit: s/to glibc/from glibc
I've been reading everything that has come through about metaclasses - hopefully I'm not overreacting but I think it's the most significant new feature since `auto`. I'm prepared to be wrong. Concepts were really hyped to me, but to be honest, while they are obviously important, as an application programmer they seem mainly a way to get better error messages. Which is useful, but won't change my life. But metaclasses will allow me to dump huge quantities of cruft and boilerplate. And this is boilerplate that you can do wrong - at least twice in my life, subtle errors in a production system turned out to be due to a mistake in copying and pasting comparator operators. One time it was my fault, one time someone else's, but both times multiple talented engineers had read the code repeatedly and not seen the issue. So I hope this gets fast-tracked for C++20, though time is short...
You're absolutely right, it should be value. I've corrected the typo, thanks.
[I've tried to use modules in VS2015 but couldn't.](https://stackoverflow.com/questions/35230327/how-to-use-vc-modules-in-cmake) Were they improved in the recent versions of TS?
also http://www.includeos.org/ [edit] also https://www.youtube.com/watch?v=t4etEwG2_LY
&gt; Ninja extracts header dependencies as part of the compilation step itself. [...] Which means no auto-generated headers, at least not as part of the overall build (maybe as a pre-build step). Huh? Chromium uses lots of auto-generated headers and it uses ninja.
How does one ship a a multi-language Meson project with VC as the underlying build system? At least Rust/Cargo have a fighting chance.
So when are we getting metametaclasses?
I'm not really sure this is a good thing. I mean it obviously makes the language more powerful in a sense, but it feels tacked on and I think it will just be even more difficult to read code (iow going further into "write only" code territory).
Personally I am not particularly looking forward to debugging metaclass code/problems.
While you're at it, @joboccara: claerer =&gt; clearer. Thanks, btw. I think, I read every one or your articles when I see them. Great stuff as always.
CMake user here, I've been following build2 development for some time (mostly just here on reddit and occasianally visiting the webpage) and I have high hopes for this project. I've been planning to try it out on one of my hobby projects but last I checked the documentation was pretty sparse. Anyway keep up the good work!
Just after meta-template-template arguments.
Thanks, I appreciate the encouragement! Yes, detailed build system documentation is something I am planning to work on in near future.
Are these also composable? For example, an "ordered" (along with other boilerplate autogenerators) would be useful as some sort of mixin while the primary identity of the class is something else.
I tried to use it, but there were some issues with std.threading STL module, so I went back to header files while waiting for the fix
Can anyone show some uses Metaclasses would allow, apart from the ones listed in this article? Thanks.
Thank you, that's encouraging to hear! Btw, if there is a topic you'd like to read more about, for work or for curiosity, do let me know. And typo fixed.
Yes, the proposal includes the possibility to compose metaclasses, with the following syntax: $class value : basic_value, ordered { ... } You can have a look at section 2.5 in [the proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0707r0.pdf)
While I do believe that metaclasses are a huge step forward I think that compile time reflection and more support for metaprogramming is much more important. Metaprogramming is programming after all and while C++ offers a language for metaprogramming it lacks tools to ease metaprogramming. It is difficult to debug template instantiations. A tool like ``static_print`` which /u/saarraz1 presented a while ago on /r/cpp would be a first step and might be generally nice. Imagine doing some configuration/computation at compile time and then being able to dump the result during compilation so that one can check what was done. Ideally there would be a godbolt.org for templates and metaprogramming :) That being said metaclasses are surely very nice and the fact that they are just a "metalibrary" on top of static reflection is the icing on the cake.
So if my class should primarily be a `foo` but also autogenerate the comparison operators, the common pattern would be: $class metabar : foo, ordered { }; metabar bar { // ... }; ? Edit: It seems like you could also use `.as` foo protobar { // ... }; using bar = $protobar.as(ordered); But it seems like you can't go without an auxiliary identifier (`protobar` or `metabar`).
Right.
do you prefer debugging external code generator problems ? 
unless you can do template-metaclasses
Take a look at proposal. It has more examples. One that I enjoyed was the eliminating the need of MOC for Qt and removing the boilerplate code for properties, signal, and slots.
Ooh! Thanks!
in the second schema shouldn't it be MyMetaclass1 / MyMetaclass2 instead of class ? 
&gt; make_reopenable how would this work across TUs ? (not in this case which forbids member variables but in a case with actual member variables in the reopenable class).
&gt; So I hope this gets fast-tracked for C++20, though time is short... honestly we should just start using /u/andrewsutton's clang fork. The more usage (and bug reports! :p) there are, the more compelling it will be for the commitee to standardise this. Remember people, there was a life before march 1997.
It says that lld can't 'fully emit .pdb files'. What does that mean? Does debugging programs linked with lld not work?
I snickered. I've never seen doxygen docs that were more useful than just opening the code in a decent IDE. 
&gt; Remember people, there was a life before march 1997. False. 1997 &lt; 2011 and everyone knows life before C++11 was the dark ages
&gt; False. 1997 &lt; 2011 and everyone knows life before C++11 was the dark ages I'll say the exact same thing. If you started using C++11 features in 2011 I'm really sad for you. std::tr1::shared_ptr has been there since 2007 for instance, and boost's shared_ptr since even earlier... 2002 / 2003 maybe ? Looking through feature tables, gcc 4.3 which seems to be circa 2008, already had support for variadic templates, rvalue references, etc. GCC 4.4 released the year after had `auto`.
I agree. If we ignore the fact the concepts proposal is much more mature, I would take metaclasses over concepts in a heartbeat. Metaclasses and concepts overlap a fair amount and where they do overlap, metaclasses are the _weaker_ form which is exactly where you want to start. First figure out where metaclasses don't work well and then use concepts to fill in those gaps. It's too bad, it seems like these ideas are coming in the wrong order...
Wouldn't a better title for the blog post be "Using clang/LLVM with VS2017"? Because there's many ways to use clang on Windows, without Visual Studio.
&gt; If you switch to glibc you'll probably see a substantial slowdown. ??
Concepts is about constraining existing types. The metaclasses are about defining new ones. 
It's 2017 and I still can't use C++11. :'(
To be fair, that might be easier because you see at least the output of the code generator before you feed it to the compiler. The lack of debugging tools for C++ metaprogramming is a huge drawback.
Until very, very recently, clang/LLVM would generate incorrect binaries for my C++ code on Windows. Indeed v4.0.1 release still does, half the binaries when run will fail with illegal instruction or failure to init the CRT due to I suspect memory corruption. However clang/LLVM 5.0.0 trunk is finally working well! As VS2017 still ICEs with my code, I've been using clang 5.0.0 trunk as my daily driver last few weeks. VS2017 can even debug generated binaries properly now, the debug info seems a bit over the place, but it's usable. Well done on the clang/LLVM team! 
&gt; (analogous to how #defines do not work globally; Uhm, I have _bad_ news for you. 
I think you'll find (e.g.) the `interface` metaclass constrains a type quite a bit. Edit: I was being a bit glib. Concepts and metaclasses could sometimes be used to solve the same problems. My point is, if possible, it is likely preferable to solve a problem with metaclasses rather than concepts, simply because metaclasses (in the area where they overlap with concepts) are less powerful. As a general rule, always solve problems with the least powerful tool that achieves the goal. 
From what I can read I cannot constrain the type you pass into my library, for example, using metaclasses. 
fast forward to 2022: c++ is now javascript. javascript is java. java is cobol. cobol is assembler. assembler is c. c is c++. ... 
this one is tracked by https://issues.isocpp.org/show_bug.cgi?id=322 -- it appears it was reviewed in Toronto and needs an updated proposal incorporating feedback.
isn't this considered abuse ? 
You mean like this (section 2.7): &gt; So we propose that a metaclass also be allowed to replace class here with .is meaning: &gt;template &lt;interface I&gt; // constrained – requires $I.is(interface) This syntax is offered more/less as an extension to the concept proposal. That's understandable in the interest of language cohesiveness, but it could just as easily be defined as part of a standalone metaclass proposal. Had these proposals come in different order, that's probably how it would've been done.
I don't know if this is the tool you need, but take a look at metashell. https://metashell.readthedocs.io/en/latest/
What do you program?!
like and comments and share 
Currently you need to use the Ninja backend for that. The MSBuild backend can only compile C and C++.
$model for ORM.
Are you doing this project to help you learn C++? It's definitely possible to do but I would think it would be much easier with a scripting language like Python or Javascript.
Well both. I started with Python and then this year started to learn C++ just because I feel like it helps me "learn" different things. I could definitely go back to python. I mainly want to take info from sites and use it for my own type of like "automation". Thanks for the quick response.
!remove
OP, A human moderator (u/blelbach) has marked your post for deletion because it is not appropriate for r/cpp. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6rlfb9/star_pattern_16_in_dev_c_different_star_pattern/dl5xklk/,%20was%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
It should be, but there'd be a LOT of people in trouble if it was.
Yeah, tooling is definitely an issue.
I would definitely lean towards a scripting language for this task, personally. You are going to be processing a lot of HTML/DOM data and scripting languages were made for that. As far as what to do to accomplish your task, I would go about it the same way for any language. Start with seeing if the websites you want to get data from have a public API to access this information. Always best to use this instead of skimming the page. If not, you will need to pull the web pages you are interested in and parse the HTML looking for changes in them. You could use a string diff tool and possibly regexes to find the key words you are looking for and then output the information to you in the way that you would like. Just be careful if you are manually combing the pages that you don't make too many requests too often. Personally, I would set up a cron job that activates your program daily or semi-daily only to look for new content. Most sites don't like it when you make a bunch of requests in quick succession.
Why is everyone trying to get their beloved library into the Standard? Standard must be robust, stable, and feature-complete. By creating a proposal, you are about to create an interface, and the implementation must be created by minimum three independent vendors (libstdc++, libc++, and Dinkumware one) with their own bugs and incompatibilities. And if you want to update a certain feature, you will have to write another proposal, get it into a TS, get it inside the "Big Three" of Compilers and update all the clients to latest compiler version. Why not just making your library stable and widespread instead?
Because I don't want my peers to be stuck with `snprintf` when they do string formatting? If fmtlib is not as easy to reach as `snprintf` and `cout`, they will choose the latter two and be open to a bunch of problems.
Seems like this is a bit more advanced for what I know so far. My next step will be looking at tutorials that work with API's then. From there I should be able to mess with it I hope haha.
I have never used this and while I would really like to use C++ for web dev it seems a bit odd to me that you have to have a template (in xml) for each language you want (example code for home page: https://www.webtoolkit.eu/wt/src/wt-homepage). You have wt-home.xml, wt-home_cn.xml and wt-home_ru.xml. The c++ code in the examples don't seem to be in modern c++. I assume the documentation hasn't been updated.
Compilers can generate things like preprocessor output, AST's, llvm-ir, asm listings, etc. No reason I can see that generating metaclass output would be hard, and seems easier than debugging TMP for instance as it's a more clearly delineated layer. Also I'm sure tooling/IDE support will appear in time also. Also there are clear error messages already in the spec, e.g. `compiler.require(f.is_public(), "interface functions must be public");`
To be clear: I am not against metaclasses. I think that this is probably the "easiest" and cleanest feature of metaprogramming to be added to C++. Especially, since it is a "library" on top of reflection. I am merely saying that given current lack of tooling for metaprogramming a code generator has the advantage that you have an output that you can debug before feeding it to the compiler. It is easier to debug two functions if you can test their outputs separately instead of only their composition.
Thanks, that looks nice.
Can't stop laughing 😀
use the rel_ops luke... :) And for Concepts: they should enable sort(vec) instead of sort(vec.begin(),vec.end()) although IDK if the std:: lib was fixed to enable that...
If you care about Google vs Microsoft battles ;) : https://www.youtube.com/watch?v=Evz_J9nAL6Y#t=27m47s
As I've said before: the vast majority of current problems involving cruft and boilerplate can be solved with reflection (and sometimes combined with CRTP). Reflection can solve the comparator problem completely, for example. It can also solve serialization completely. It actually solves these problems better, too. In general, any problem solvable by both reflection and generation is better solved by reflection because it is: a) less complex, and b) it is non-intrusive and can be applied after the fact. Merging metaclasses into C++ at the same time as reflection would be a completely awful, awful idea. The absolute bare minimum would be to merge reflection first, and see how much need there really is for metaclasses once reflection has been explored. Personally I'm skeptical that the problems that cannot be solved with reflection will be serious enough and numerous enough to warrant adding something so complicated to C++.
&gt; Unfortunately, getting clang to compile MSVC based projects isn't as easy as just dropping in clang and changing a few flags. Let's get started. Why not ? I don't know why you need extra scripts for compilation. You can just install llvm (from here: http://llvm.org/builds/) and in your Visual Studio c++ project properties switch to LLVM toolset. I use it with VS2015 and everything works well. I would like to see working clang tools on windows (especially address sanitizer). It would be amazing. Also there is still lack of libraries support for clang compiler on windows. 
Just curious, how does `build2` handle dependencies? Does it use the integrated build approach(like in cmake with `add_subdirectory`)? Or does it use the prebuilt binary approach(like in cmake with `find_package`)? Or does it work with both?
Just curious, how does `meson` handle dependencies? Does it use the integrated build approach(like in cmake with `add_subdirectory`)? Or does it use the prebuilt binary approach(like in cmake with `find_package`)? Or does it work with both?
So, how we can use(try) this today?
If you consider "readability" to be the ability to glance at a piece of code and know roughly what it will compile down into, how it will utilize memory or system resources, and what its performance characteristics will likely be, then this is certainly a small step backward. However, if you consider "readability" to be the ability to glance at a piece of code and know what high-level concept it represents, what its semantics are, and how you can interface with it, then this is potentially a huge leap forward. It's all about perspective.
&gt; a code generator has the advantage that you have an output that you can debug before feeding it to the compiler. actually the example implementation has a `cppx::meta::compiler::debug($foo)` meta-whatever-function that does exactly this: https://cppx.godbolt.org/
Wt 3 is still old pre-C++11 code. You'll have to look at the Wt 4 examples in the wt4 source code to see the updated version. The documentation of Wt 4 still could use some work indeed, though. We don't need a different template for each language. The way you can do things is to have a template.xml for the template stuff, and different strings.xml files for the translations. 
Just stay away from high powered tools kiddo. 
Excuse me, I've just been a little bit sick in my mouth. 
Oh ok, it just seemed like you needed from that source code example. Thanks!
you can probably recode those in C++ lile the typesafe version of printf and variadic templates.
We support both cases and the build definition is the same for both. This means that on Linux + other platform that provide (and require you to use) system packages it does that but on other platforms it builds the dependencies from source code. [Here is a sample project](https://github.com/mesonbuild/meson/tree/master/manual%20tests/2%20multiwrap) that demonstrates how it works. [Here is a different kind of sample project](https://github.com/jpakkane/liftoff) that demonstrates how to use it in a large project with many independent subcomponents that is almost but not entirely unlike how Boost is set up :). [I wrote a blog post about this.](http://nibblestew.blogspot.com/2017/07/managing-build-definitions-of-big.html) The main difference to CMake is that all subprojects are built in an isolated sandbox so they can't screw up the setup of their parent project. To be usable as an embedded subproject the project (let's call it libfoo) needs to define a dependency like this: libfoo_dep = declare_dependency(link_with : ..., include_directories : ...) The parent project can then use it like this: libfoo_dep = dependency('libfoo', fallback : ['libfoo', 'libfoo_dep'] This is Meson nomenclature for "try to find libfroo from the system and if it is not found use a subproject with the given name instead".
Source: https://github.com/asutton/clang Online compiler: https://cppx.godbolt.org/
Things that find things that fly. We are stuck with a custom version of an old kernel that nobody allowed us to upgrade for a modern distro, and thus we can't have a modern development environment.
fmtlib is exactly that though more or less (+ more sane python-like formatting, since you don't need all those %d/%f specifiers if your function is type-aware).
Is there a way to actually enable -freflection in the built version? I've built master, and using the following command line: /mnt/c/ClangTesting/build/bin/clang++ -stdlib=libc++ -nostdinc++ -I/mnt/c/ClangTesting/build/include/c++/v1 -I/mnt/c/ClangTesting/build/include -L/mnt/c/ClangTesting/build/lib -Wl,-rpath,/mnt/c/ClangTesting/build/lib -freflection -std=c++1z ReflectionTest.cpp I got this: clang-5.0: warning: argument unused during compilation: '-freflection' [-Wunused-command-line-argument] ReflectionTest.cpp:1:1: error: unknown type name '$class'; did you mean 'class'? $class reflectable { ^~~~~~ class ReflectionTest.cpp:3:10: error: expected injection -&gt; { int sType(){} } ^ ReflectionTest.cpp:3:23: error: function definition is not allowed here -&gt; { int sType(){} } ^ 3 errors generated. With the following code: $class reflectable { constexpr { -&gt; { int sType(){} } } }; What flags are used for the cppx.godbolt version?
You need `-Xclang -freflection` I think.
Awesome! That worked! Thanks so much! Now I'm just surprised that I can't insert static members or definitions of functions. I guess the fork isn't there yet? Or am I just missing something from p0633r0?
I presented the proposal for Library Evolution Working Group during the Toronto meeting and it was quite well received. Working on addressing the feedback both from the meeting (https://issues.isocpp.org/show_bug.cgi?id=322) and from the std-proposal mailing list.
&gt; This means that on Linux + other platform that provide (and require you to use) system packages Well, it may not be a system package manager, but a separate dependency management tool like nix or anaconda that is building the dependencies. &gt; This is Meson nomenclature for "try to find libfroo from the system and if it is not found use a subproject with the given name instead". That definitely seems like the correct way to do it. Does it have a flag to disable the subproject fallback? As it would be nice to know when the dependencies fail, which seems to be useful inside a dependency management tool.
&gt; Does it have a flag to disable the subproject fallback? Not at the moment but it does have a flag to prevent downloading of dependencies. For third party dependencies you would usually use "wrap files" (see the documentation for details) to specify where your dependencies are and thus disabling downloads causes these to error out.
&gt; transactional memory &gt; fresh Ha ha All mockery aside, Transactional Memory has been studied (mathematically and simulator wise) to a very large degree over the past few decades and is one of those things that basically every student who learns about compilers learns as "a cool idea that needs a good implementation" So if they are pushing it in GCC, it is at least worth considering. I am still incredibly skeptical, but we finally have a mainstream way to test performance and viability with codes more complex than a grad student can hand port.
I am working on finishing a project after which I'll start an open source game dev experiment using modules and other available features. I hope I can do this in time for providing feedback to implementers.
/r/iamsmart
&gt; Ha ha "TSX was documented by Intel in February 2012, and debuted in June 2013 on selected Intel microprocessors based on the Haswell microarchitecture.[5][6][7] Haswell processors below 45xx as well as R-series and K-series (with unlocked multiplier) SKUs do not support TSX.[8] In August 2014, Intel announced a bug in the TSX implementation on current steppings of Haswell, Haswell-E, Haswell-EP and early Broadwell CPUs, which resulted in disabling the TSX feature on affected CPUs via a microcode update.[9][10]" So yeah mainstream availability is fresh. 
Thanks god. At least we might stop seeing posts about yet another library that does nothing practical (hopefully).
If anything, we are likely to see more posts about the library. Users of beast seem to have found the library useful - perhaps you should reach out to them and explain that they are wrong :) you can find them on the Boost developers mailing list.
No offence, but I'm just tired of seeing your posts about your library. Each time with long discussion why it doesn't seem to be useful and/or anticipated by /r/cpp readers.
By what measure does Beast do nothing practical?
No offense taken! Clearly you are in the minority, as the reviewers have spoken; the word is that they find the library immensely useful. Perhaps you should take a moment and reflect on why your world view is incongruous with the rest of your peers.
Nice! One less dependecy...Even though boost got (yet again) bigger :/
This is wonderful, don't listen to the naysayers. Congratulations :)
Judging something as impractical depends on what it is that one wishes to practice. By your assessment of Beast I'm guessing you're mostly practicing wasting time on Reddit. Meanwhile, the rest of us get to leverage Beast's advanced functionality to write better code.
According to the [talk given by Herb Sutter](https://www.youtube.com/watch?v=6nsyX37nsRs), it's not even close to being feature-complete, but should give a pretty good example of the simpler reflection + metaclass features. Apparently it can compile all of the actual code samples on his slides.
&gt; TSX was documented by Intel https://en.wikipedia.org/wiki/Software_transactional_memory The TS - and transactional memory in general - is completely independent of "mainstream" availability of hardware-accelerated transactional memory features. 
I was just looking for the state of C++ HTTP facilities the other day; perhaps I'll get over boost at some point. Considering some of it eventually goes standard in a way or the other and I've heard of putting Pango in standard do you think I can hold hope this goes forward fast?
Hmm... Pango is a text layout engine, while Beast is an HTTP protocol library, are you sure you're thinking of Pango and not something else?
The two are indeed uncorrelated. I've read those articles maybe they are just rumors, it seems the STD comitee is thinking about putting graphics (and GPU stuff) in standard. The idea gives me the shivers. So while they're at it, they might put in standard something one would expect in 2017: HTTP/WS seems a nice candidate.
I don't see any releases in GitHub? You may want to tag your releases.
It's like a ticking time bomb ready to blow up in your face. You can't be careful around it.
Would anyone be interested in yet another C++ web service application server "framework" (More like a scaffold) based on a multi threaded, multi worker beast server? 
It supports both ways (and more) but via the same mechanism (called project import). In your `buildfile` you say: import libs += libhello%lib{hello} exe{hello}: cxx{hello} $libs The `libhello` project can then come from various places: you can just copy its directory into your project's directory (we call it creating an amalgamation) and it will be automatically found and built. Or you can have it built somewhere else and then specify its location with `config.import.libhello`, for example: $ b config.import.libhello=../libhello-1.1.0 Or if you have it system-installed, you can omit `config.import.libhello` and `build2` will use the installed version (and extract all the necessary flags from `pkg-config`, etc). The [toolchain introduction](https://build2.org/build2-toolchain/doc/build2-toolchain-intro.xhtml) shows and explains all this (and how it interacts with `bpkg`, the package manager) in more detail.
Thanks a lot for beast! I'm not complaining, but he does have a little bit of a point that you're not shy about marketing your library :) I strongly disagree with the rest of his sentiment though.
&gt; Beast is a C++ header-only library serving as a foundation for writing interoperable networking libraries by providing low-level HTTP/1, WebSocket, and networking protocol vocabulary types and algorithms using the consistent asynchronous model of Boost.Asio. Okay, so how wide is the gap between Beast and using HTTP/1? And why not 1.1?
HTTP/1 refers to both HTTP/1.0 and HTTP/1.1. It does not include HTTP/2.
Such tags would only be temporary, as Beast will inherit the Boost version number in which it appears.
That sounds very interesting!
Great for the people who use boost. I'm not allowed to use it because it is more or less impossible to build and install. 
What is the point of including this in boost? We should be moving things out of boost into standalone libraries that do one thing well.
I've been working on it for the past couple of days as the back end for an angular site. I'll try to clean up the project and push to github in the next week. It's based on the [http-server-fast example](https://github.com/boostorg/beast/tree/develop/example/http-server-fast). I've added a toml config file, multi threading and striped out the ability to serve files. I've been struggling with trying to figure out an elegant way to handle both tcp sockets as well as a stream socket. If anyone has some advice on this I'd love to hear it. My plan is to run this behind nginx for http2 and output caching. 
That's great to hear, I've been using Beast for a couple of months now and have been very happy with it (and with your help through GitHub issues too).
TIL two shell commands == "more or less impossible". ^/s
Sounds like a problem with a build environment in your company.
Boost is not a single library, it's a collection of them.
Will Beast keep its name or will it be renamed to something like Boost.Http? I only ask because Beast is non-descriptive, and Boost names appear to be mostly descriptive.
Just link it statically.
I dont think spirit or hana are realy descriptive
Why are people so worried about a dependency on boost adding bloat then? Is it because there is a lot of interdependence?
That's good. I look forward to your documentation because at the moment I have only a vague concept of what Beast does.
I don't know, maybe they're adding the whole Boost into their projects instead of parts of it.
Yes, the name is here to stay. See quesiton 7 here: http://www.boost.org/doc/libs/develop/libs/beast/doc/html/beast/design_choices/faq.html
Beast is like Asio for HTTP and WebSockets.
C++ is not a python or golang and I think C++ Professional Programmers should know how to build your third-party open-source libraries. Building Boost is nontribial work, but once you study about it, you won't forget it. 
I had a very quick look: * Your queue class has a single lock on read and write. Under contention, this will performance not just badly, but disastrously. A performant producer-consumer queue should be used. * Overuse of shared_ptr/weak_ptr. I haven't looked in detail but I wouldn't mind betting that you don't need a single one of these in your codebase, never mind the vectors of weak_ptrs as class members. I could be completely wrong, but without any documentation and without spending more time looking, it gives me the heebie jeebies. * Never pass shared_ptr function arguments by value - this is very inefficient. Pass it by const reference. * constants.h is C style. Avoid #defines for constants. Among other reasons, they are untyped. Use e.g. `constexpr auto eth_addr_len = std::size_t{6};` * `return PCAP::IpAddress(++tmp);` makes me shudder whenever I see it, because someone, somewhere in the future is going to try and be clever and flip it to a postfix ++ because it looks nicer or something, then everything breaks. You're not reusing tmp, so just write `return PCAP::IpAddress(tmp + 1);` * I've seen a few sleeps in the code, some of which are for a second. This seems like a recipe for something very, very unresponsive. Again, I don't understand the design right now. * Some functions could be noexcept, e.g. Much of IpAddress's interface as it mirrors std::array. * Inappropriate auto function return type in Session class. As a reader, I have no idea what this returns until I go and look at the cpp file. This isn't what this feature is for. It's for when you cannot state the type (e.g. returning a lambda) or if it aids readability. In this case it makes readability worse. * Session's move constructor is incorrect. You are just doing a slow version of copy. The compiler generated copy/move constructors for this class are going to be correct, so use them. Just remove anything in Session declaring or defining copy or move operations. * SessionController.cpp - you can still write = default here for the destructor, instead of writing an empty body. * Your struct definitions need to be packed. Look up how to do this for your compiler. Otherwise, they may have padding which makes them incompatible with overlaying them onto raw network traffic. My recommendations. Read this: https://github.com/isocpp/CppCoreGuidelines Concentrate on getting the basics down before getting into multithreading. Document in-code as you go, it doesn't have to be (really shouldn't be!) reams and reams, but writing down why you did something the way you did sometimes forces you to ask yourself that question. And finally, write unit tests! 
And exponentially hard to debug. 
You can start by filling README.md about what your library does, how to install and use it.
How much do you squat ATG?
You are the one making the original comment, why do you not want it in boost? I think it only depends on Boost.Asio so if anything it is just adding convenience when choosing it as a dependency to be a part of Boost already.
&gt; Even though boost got (yet again) bigger :/ that's like complaining that github and sourceforge are big because they host a lot of projects
that's not even needed, it's header only and asio can also be used header-only. you literally just have to add boost folder to your include path.
people are also still worried about emacs being more bloated than vim or xterm more than urxvt in this day and age so...
Sorry for the lag! (Things were happening irl and I forgot to check...) So, in the linked part of the standard, we have this rule: &gt; In a declaration `T D` where `D` has the form &gt; D1 [ *constant-expression* ] (or `D1 [ ]`) &gt; and the type of the identifier in the declaration `T D1` is “*derived-declarator-type-list* T”, then the type of the identifier of `D` is an array type; **if the type of the identifier of `D` contains the auto *type-specifier*, the program is ill-formed**. Basically, this is standardese for: "You are not allowed to declare an array-of-auto. If you try to do that, then you should get an error from the compiler." 
Now try to make a stack. Then build a little test application where you connect two stacks to both ends of a queue and try to make ICMP (ping) work. Don't forget ARP. Once you can do that, try to make it fast. See if you can do a million requests per second :)
I quite like Doxygen's search function and its README-like features (‘pages’, I think?) which can link directly to classes/functions/things, but I suppose this is the same as a decent IDE :)
There are lots of interdependencies and for many of the libraries it would be a lot of hassle to add them to your project without just adding boost as a whole. But that doesn't mean you have to build every single non-header-only library. 
....What?? Sure...Who doesn't built gitlab binaries every now and then, just to compile a project...Now you're thinking!
Did you mean how to handle TCP/IP and SSL? If so, the server-framework example shows how this is done: https://github.com/boostorg/beast/blob/develop/example/server-framework/multi_port.hpp#L26
I meant that just like github and SourceForge, boost is an host of projects, except they have to share the same namespace when in boost.
What harm do you perceive it doing? 
I guess like any documentation you have to do a lot of work to produce something useful, e.g. actually write those README type things with a lot of links between different chunks of code. But most doxygen outputs never put that work in so you end up with class hierarchy diagrams that are unreadably complex 95% of the time, and just a dump of function/member names that are just signatures taking up more space than they do in the code without actually telling you anything. That's not entirely doxygen's fault, but when I'm trying to figure out what is going on in a code base, "find references to this thing in other parts of code" like you get in an IDE is indispensable, and class hierarchies are generally available in the same place. In the case of the doxygen output being decent, that only happens when the code itself was appropriately commented, which generally means documentation right there next to the relevant lines of code, so again no reason to go to the HTML output. I've been using CLion and highlighting a call peeks into the doxygen comments surrounding it. I guess the best thing to say about doxygen is that it's just a standardized way to comment source code, and some of these sphinx translations look pretty nice.
Those look really great. How do they interact with LaTeX math, and can you include images in the generated documentation?
Congrats Vinnie! I remember hearing your interview on CppCast, which motivated me to get more involved in open source and giving back to the community. What's next, http2 or C++23?
Maybe everyone could pronounce it like Beashttp.
Well, I still have a lot of polishing to do on Beast. I want the first release in Boost to be the best it can be. I'm writing a TON of examples right now. This is a rough guide to all of the different sample programs that will appear in the library: http://codepad.org/gzDctWtV During the formal review, a common theme was that users wished Beast would do just a little bit more (i.e. not have so many "out of scope" items). Originally, the `http::file_body` was out of scope but I was convinced that it should be part of the library. Especially because I was able to optimize it to use the native Windows and Linux facilities for transmitting files with a minimum of kernel transitions (calling `::TransmitFile` and `::sendfile` respectively). So I will be adding a few goodies to make things just a little bit easier for users. A solid uri parser is on the list (I've already done some work on that). Once that settles down then yes HTTP/2 is a planned feature and I will also write new libraries that build on Beast. For example we could use a solid HTTP client, a C++ version of curl, or a C++ version of Requests for Python. Unless someone beats me to it (hopefully)
Have you tried a C++ package manager? [Hunter](https://github.com/ruslo/hunter), [conan.io](https://github.com/conan-io/conan) and [cppget](https://cppget.org/) come to mind. I've been using Hunter lately so it's trivial to [build and include boost](http://v1ntage.io/2017/07/25/episode-003-intro-to-asio/) in a project.
You made some real good points. I will try to fix them. Also thanks for the link regarding CppCodeGuidelines, I will look forward to read it.
Can you elaborate a little bit, because I am not sure that I completely understand it. :)
Try building with [Hunter](https://github.com/ruslo/hunter). It's made my projects with Boost [much](http://v1ntage.io/2017/07/15/episode-002-an-opinionated-start-to-a-cpp-project/) [easier](https://v1ntage.io/2017/07/15/episode-002-an-opinionated-start-to-a-cpp-project/) to manage.
This isn't a "Google vs. Microsoft battle". This is a design being worked on in the C++ Standards Committee. Some of the experts involved work for Google and Microsoft and they certainly want to represent their companies' interests but I assure you that their main concern is arriving at a good design. If you're looking for a Google vs. Microsoft battle, I'd recommend bashing Bing unfairly. [The good folks at Bing don't mind it much](https://www.microsoft.com/en-us/Investor/earnings/FY-2017-Q4/press-release-webcast).
&gt; This isn't a "Google vs. Microsoft battle". Did you watch the link? Basically Wong says that GOOG and MSFT said different things, so ISO was confused. Also notice the ;) in my original comment
&gt; The TS - and transactional memory in general - is completely independent of "mainstream" availability of hardware-accelerated transactional memory features. Yes, but to get speedups you need HW, right? 
ISO has limited resources. And although some may say that TM people will just leave if TM was not there it is still a minor problem. Also like I said I think C++ should focus on mainstream problems and solutions, not sure if TM fits that.
If somebody wants a nicer experience with mutex/variable pair check out libguarded. I know this is not the point of this article, just example, but it is common problem.
SG5 started out from work initiated by an industry group founded by (I think) Google in something like 2008. So whether or not it feels mainstream to you is irrelevant. It has sufficient industry backing and has had manpower over several years. There isn't a pool of ISO employees working on standardisation, some of whom are working on TM, taking time away from getting your favourite feature into C++20. 
That's interesting. Have you tried cross compiling to iOS and/or Android?
Congratulations !
I don't agree. I mean, you can always do `git pull https://github.com/johndoe/do_the_yoga` and still have issue ... But with boost, you just can do `dnf install boost`^fedora or `pacman -Sy boost`^arch and tada, you've got all dependecies resolved. That's what turns me on in linux btw.
No, I mean a Unix socket file. 
I've been using your library for a month or so now and I have enjoyed using it, it is very much in line with the rest of asio. Something I found myself wanting was some parsing for parameters that are currently passed along within the .target() in http_request, something like a multimap&lt;string,string&gt; or similar. Do you have any plans to do this? 
I'm developing a real hate for analogies.
Ah, I see. The same solution which lets a server use both plain and SSL connections will also work for you. Use the Curiously Recurring Template Pattern to declare your connection class logic, like this: template&lt;class Derived&gt; struct session { Derived&amp; impl() { return static_cast&lt;Derived&amp;&gt;(*this); } }; Now put your code to perform I/O into the class but instead of referring to the socket or stream as a data member, call a member of the derived class (`stream()` in this example) to access it: template&lt;class Derived&gt; void session&lt;Derived&gt;::do_read() { boost::asio::async_read(impl().stream(), buffers, ...); } Finally to make all this work, declare two derived classes. One for your normal socket and the other for your unix domain stream: struct plain_session : session&lt;plain_session&gt; { boost::asio::ip::tcp::socket sock_; boost::asio::ip::tcp::socket&amp; stream() { return sock_; } }; And for the unix domain stream: struct stream_session : session&lt;stream_session&gt; { boost::asio::local::stream stream_; boost::asio::local::stream&amp; stream() { return stream_; } }; You'll have to do a bit of juggling responsibilities. The derived class has to handle anything that is specific to the type of stream, such as connecting or shutting down, while the base class handles the generic reading and writing portion of your logic. This is the style used in the Beast examples which support both plain and SSL sessions on the same port. Hope this helps! 
I haven't watched Michael's talk yet. And Michael is truly always right ;) But still, there's no battle. There is a lot of confusion. That's par for the course with WG21. *Edit: And sorry to be a bit touchy about such things, but I'm a bit touchy about such things : )*
Why do you have your own variant, file, etc? If boost doesn't have some sort of [usable] file class, wouldn't it be better to split that out? 
The variant is a special, stripped down variant with limited features. It is not a public class. The reason I made my own was to significantly reduce compile times, because some of the instantiations are enormous. In some cases, boost::variant would just crash the compiler. Beast doesn't have access to std::variant since that is not in C++11 and I try to keep things limited to just C++11 so that the library can be used more broadly. As for the file, the interface is designed for the needs of serving file data through the HTTP messages. I don't think its general enough to be its own library. Even so, I prefer the monolithic library design of Beast. Its easier to work that way when all of the components run in the same set of tests, same CI, same GitHub issues, and so forth. Since Beast has platform-specific optimizations to deliver file content over sockets, it makes sense to keep it all together. That doesn't mean that ALL new features will get plopped into Beast, I have in mind to write a few more libraries on top of Beast. The functionality in those libraries will be kept separate, in its own repository, with its own testing scripts and infrastructure, GitHub issues, and its own separate formal review. This feels like the right way to organize the subject matter.
I'm working on a uri parser (I think that's what you're asking for). You'll have iterator based access to the query parameters, so if you wanted to store them in a map you could do it. Or you could just access them with the iterator.
Can you add support for string printing to terminal with colors?
&gt; it seems the STD comitee is thinking about putting graphics (and GPU stuff) in standard. The idea gives me the shivers. Considering it took until C++11 to get std::thread, C++17 to get std::filesystem, and networking/modules are still in draft trying to make it by C++20 I think you and I will both be dead by the time the graphics TS makes it in.
IIRC when I read the proposal, it's actually possible to static_assert over template argument types using a metaclass to basically do the same as a concept would. There was an example of applying a metaclass to a type and checking whether it'd changed anything in a type. That's it: design a metaclass to only check properties of a passed type and either return the type as is or with some mark that it did not pass the check. static_assert or enable_if over that and you got almost all of concepts.
By stack, he means IP stack, to use for easy testing of your code.
Oh, so you don't plan on making Beast available outside of Boost as well? Okay, then. Otherwise, it would make sense to maintain your own parallel version.
I want to promote Boost, making Beast available outside Boost would only fuel irrational fears of having Boost as a dependency. I expect that, like Asio, Beast will become an "anchor" library. In other words a library with such fundamental utility that new users come to Boost just for the library. "Come for Beast, stay for Boost."
Text formatting != console output
Thanks :) You have essentially 2 possibilities on how to proceed with this doxy + breathe + sphinx toolchain: either you do all your formatting, image inclusion, math, etc. in doxygen and let breathe do the "translation" behind the scenes, or you inject sphinx syntax into the doxygen blocks. In the first case, here's an example of math in doxygen syntax: https://github.com/esa/pagmo2/blob/master/include/pagmo/problem.hpp#L1020 which then shows up in the output generated by sphinx as this: https://esa.github.io/pagmo2/docs/cpp/problem.html#_CPPv2N5pagmo7problemE As you see, it's properly using jsmath and the output looks good. In the same fashion, you can include images in the doxygen syntax: https://github.com/esa/pagmo2/blob/master/include/pagmo/problem.hpp#L1017 which again shows up decently: https://esa.github.io/pagmo2/docs/cpp/problem.html#_CPPv2N5pagmo7problemE However, as far as I remember the formatting capabilities were rather limited when including images from doxy (it's been a while since I last tried). For the other approach, see an example here: https://github.com/bluescarni/mppp/blob/master/include/mp%2B%2B/integer.hpp#L784 As you see, in this doxygen block I am using sphinx syntax, e.g., in the definition of math as ":math:`2^{64 \times 2} = 2^{128}`", cross-referencing other documented entities as ":cpp:class:`~mppp::integer`", etc. This works thanks to that little preamble at the beginning of the doxy block: "\rststar", which is closed by a corresponding "\endrststar" at the end of the block. These commands are in turn defined in the doxygen config file here: https://github.com/bluescarni/mppp/blob/master/doc/doxygen/Doxyfile.in#L241 and they expand to things such as "\verbatim embed:rst" etc. This is a special tag which is recognized by breathe, and instructs to interpret the enclosed block as verbatim sphinx syntax. The advantage of this second approach is then that you can write regular sphinx syntax in your doxygen blocks and everything will "just work" as intended (with some limitations, e.g., you cannot use this technique inside @param, @throws and similar doxygen directives).
Thanks a ton for this! Gonna save this comment to refer back to later. My codes are in computational physics so good math support and including geometric diagrams is really essential, and my formal write-ups are all in latex + SVG/PNG.
No problem :) I am doing scientific computing as well. So far I am pretty happy with how this documentation toolchain is working out, but as a word of caution I should point out that the breathe maintainer recently stepped down and it is not entirely clear if the project will go in maintenance mode only: https://github.com/michaeljones/breathe/issues/336
If you have plans to get your multi-precision library working for floats I'll look into incorporating it. Some of my algorithms are fairly sensitive to initial runtime values that are constant for the rest of the program where max precision is desirable, but then casting to float/double is totally fine for the compute-intensive stuff. (Some mildly ill-conditioned linear algebra gets used in constructors.)
Just what are you allowed to use exactly? 
Looks like a funny coincidence, I am working towards wrapping __float128 just these days: https://github.com/bluescarni/mppp/pull/31 After that I'll add support for MPFR. So feel free to add feature requests/suggestion/criticism if you like :) You have any link to your projects? 
This sounds pretty close to something I've been looking for to replace [Crow](https://github.com/ipkn/crow) (which leaks memory and isn't well maintained) Does your implementation have a URL handler router?
Like asio you could generate boost library from base library.
After you're done accepting or connecting the socket, std::move it into a generic::stream_protocol::socket.
Isn't using packed structs borderline hackish? They aren't portable and unless there are benchmarks to show otherwise, I'd much rather use some accessors to a raw buffer, with common functionality factored out to minimize potential for mistakes.
You misunderstand. Its not that it isn't technically possible (well, Beast does depend on several other Boost libraries other than Asio, such as Intrusive). I have a philosophical objection to making a stand-alone version. I'd like Beast to be a Boost-exclusive library. That means if you want to use Beast, you have to accept Boost. If someone else wants to port a stand-alone version of Beast they are more than welcome to it but I will not be doing it myself.
It has no URL router but I was looking at copying crows as well as the query string class that they are using. As of now every request is passed to my request handler class which parses the query string for the name of the mysql procedure to run. The result set is then converted into json and returned. 
A few other things in addition to what's been mentioned already: * There's a quite a few places where you use std::map where its unnecessary. You should usually prefer std::unordered_map unless you actually need sorted iteration order or have standing iterators that you don't want invalidated. * There's quite a few places where you could use `auto`and remove unnecessary iterators to help with readability. For instance: static std::shared_ptr&lt;Controller&gt; getController(const std::string&amp; interface) { static std::map&lt;std::string, std::shared_ptr&lt;Controller&lt;I,P&gt;&gt;&gt; controllers; typename std::map&lt;std::string, std::shared_ptr&lt;Controller&lt;I,P&gt;&gt;&gt;::iterator it = controllers.find(interface); if (it != controllers.end()) return it-&gt;second; else { std::shared_ptr&lt;Controller&lt;I,P&gt;&gt; controller = std::shared_ptr&lt;Controller&lt;I,P&gt;&gt;(new Controller&lt;I,P&gt;(interface)); controllers[interface] = controller; return controller; } } could become: static std::shared_ptr&lt;Controller&gt; getController(const std::string&amp; interface) { static std::unordered_map&lt;std::string, std::shared_ptr&lt;Controller&lt;I,P&gt;&gt;&gt; controllers; auto &amp;controller_ptr = controllers[interface]; if (!controller_ptr) controller_ptr = std::make_shared&lt;Controller&lt;I, P&gt;&gt;(interface); return controller_ptr; } * for PCAP::Logging::get_time, you should make it explicit what the returned duration count represents, since the duration used by the time_point from system_clock is not specified by the standard to be of any particular resolution and the meaning of the ticks returned by count() may vary depending on the system. For instance: std::chrono::milliseconds::rep get_time() { auto duration = std::chrono::system_clock::now().time_since_epoch(); return std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(duration).count(); } 
You should try combine it with boost::asio.
I wouldn't describe them as hackish, no. While there's no portable language support, all implementations that I'm aware of support declaring packed structs, so in practice portability isn't an issue (edit: unaligned access is an issue on some platforms, I forgot that.) My comment was in reference to the way the implementation works at present, which appeared to be memcpy-into-non-packed struct (hence the potential for issues with padding). Piecemeal copying of raw bytes, initialising each field one by one, is going to be slower, because memcpy is of course potentially very fast. Whether any copying should be happening at all is another issue. 
&gt; it seems the STD comitee is thinking about putting graphics (and GPU stuff) in standard. sure, there was a talk to standartize 2D via Cairo &gt; The idea gives me the shivers. why?
What are the main use cases for C++ these days? Historically, I think, games and financial industries have relied on C++. I'm not sure you'd make the same choices today though. IMHO, C++ inhabits this strange middle ground between performance and convenience, and that middle ground is shrinking. On the flip side, you can't get away from the fact that many legacy systems and libraries are written in C++.
Care the explain how to install boost Python v 1.53 with a clean win 10 installation and vs 2017 with two shell commands? 
Non C++ programmers need to be able to compile our code base too since they are using our tools.
Cmake, of course.
We created a monster... An extremely sophisticated, beautiful monster :)
Transactional memory support is landing in many systems programming languages, not just C++. C++'s support intentionally is not hugely distant from the C support. BTW if you haven't done low level lockfree algorithm programming with transactional memory available yet, you really should! It's worth the average 2x slowdown just for ease of achieving correctness and excellent worst case performance. I'd definitely recommend giving it a go, I came away impressed.
I turned on LTO on our codebase at work, and not only did link times get excessively long, but the executable also got enormous. Any idea how to mitigate that?
I won't be able to attend but I hope to see your speech later on youtube. I can tell you what I don't like about a great many programming documents I've read though: - no description of what something actually is. An entire library, and not a clue what it does. - no simple example of the thing in action. These are the worst offenders, and usually cause me to lose interest rapidly. But there are others: - no explanation of concepts and specialized jargon. - no description of how the whole thing hangs together: what should come first, what should come last, and some idea of how things should happen inbetween. - no description of parameters or what they do, other than in self-referential terms. - no indication of what kind of error conditions could arise. - no indication of runtime cost. 
&gt; There is a lot of confusion. That's par for the course with WG21. Gee, I'm so....filled with confidence..... 
If it was easy, we wouldn't need experts, would we?
I don't plan to include this into the current proposal but it would be possible to develop such functionality on top of the core API. Follow up -PRs- papers are welcome =).
I've watched that section and two things: 1. Thanks for a deep link to the right spot in the talk! Upvotes all the way! 2. The committee decided to hear out the experience that folks from Google reported. Microsoft folks hadn't encountered that particular issue in their usage of modules. It's good that we understand the discrepancy, technical or otherwise. 
These fears are not irrational. I had to deal with situation when we were forced to update boost version (due to bugs in one place) and it broke other stuff (because changes to API and new bugs)
"AFTs have been controversial since their introduction, due to their ability to make template code look like non-template code." Good to know ISO members program in notepad, so they can not use syntax coloring. 
http://www.dinkumware.com/
why isn't `let` postfix? &gt; let fn {7 +} vs &gt; {7 +} fn let or &gt; {7 +} "fn" let
In stone age I used STLPort.
I'll give STLport a go
Well you could port libc++ and libc. Just in case you wasn't aware, c lib implementations like libc allocate large and break them into smaller more useful allocations, which means you spend less time in the kernel.
Be warned -- it was REALLY long ago. Check what they say about C++11/etc.
For what I am doing, there is no separation currently, or I'm writing this as if it was 100% kernel.
They seem to have been keeping up for the most part, I'm not in need of all of the fancy new C++11 features. Just the containers mostly, and allocators.
Oh, you said you have syscalls so I thought you had separation. Anyway, you could the replace the libc abi calls for what ever you project has to allocate pages.
This device is a bit strange, there isn't any "exposure" api or anything so you have to call a instruction to actually invoke that code path, or manually map the address (but there is ASLR) to a function protoype
EASTL is another choice. https://github.com/electronicarts/EASTL
On a similar note, Here's how to check at compile-time if a class template was instantiated. http://coliru.stacked-crooked.com/a/3d3bcd07724add12
The easiest way would probably be to port newlib and libc++ to your kernel. You'll end up with a full modern c++ stl to use. Just know you'll need to implement a few things in your kernel to make them work.
Boost Beast sounds kind of strange
Eh, you need parts of standard C library first, parts that are sufficient for your exact subset of STL. It's perhaps just malloc/free, but are you sure? As the other guy said, dinkumware is a very good bet of getting the complete thing. Given where you ask (reddit), that's probably too expensive (maybe less expensive if they already have stuff for your architecture). BTW... It's strange that you're asking for STL when you have no C, do you realize that?
Some type_traits must be implemented by compiler... And something like initializer_list...
New types are supported by simply providing an implementation of an overloaded free function, draw(), for the specific new type. It allows the new type to be supported non-intrusively, meaning the data structure does not have to be modified at all. And this is the tradeoff for the hidden inheritance/heap allocations. The point is that the type itself doesn't have to be modified to support polymorphism, which could be a requirement when trying to adapt data structures from 3rd party libraries that can't be so easily modified to support some polymorphic behavior. Instead, a custom free function can be written that contains just the polymorphic behavior, and the related type can then be used in a polymorphic setting.
deleted ^^^^^^^^^^^^^^^^0.8476 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/03549)
Yes, Cairo, I have confused names. GPU programming doesn't work even in native APIs. I've tried a bunch over the years and none quite delivered. My last attempts were with AMD-OpenCL and I now consider OpenCL garbage. I think we could discuss a bit about coherent-branch vs dynamic branch strategies. I don't recall the compiler guessing it ever (in any of the drivers I tried). MS has experimented a bit. I'd say their attempts (I think they are called C++ AMP) are more useful than the current compute-oriented APIs; for the rest I will accept nothing less than OpenGPU stack.
Uhm, that sounds pretty accurate.
Similar: http://canonware.com/onyx/
&gt; BTW... It's strange that you're asking for STL when you have no C, do you realize that? is it ? what parts of C are needed for the STL classes ?
&gt; it can compile C/C++ code just fine Well yes and no. It can compile simple C/C++ code but the sad truth is that fully supporting all existing projects requirements on all platforms requires a _ton_ of work because they do things in their build definitions that are enough to bring grown men to tears.
&gt; and tada, you've got all dependecies resolved. That's what turns me on in linux btw. well, yes, and apparently that leads people to think that boost is "bloated".
We clearly reached the point where we should have stopped a while ago. But let's keep going and see what happens. 
deleted ^^^^^^^^^^^^^^^^0.1075 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/77321)
How would you know *not* to evaluate `{7 +}` if `let` was posfix? It seems to me that evaluation would be triggered at the `}`.
I used to use STLPort during the XBox &amp; PS2 era of game development. It was significantly faster than the compiler vendor implementations. Nowadays I use EASTL.
You were so preoccupied with whether you could ...
And typeinfo
Yes I know it is a very strange request, I've been struggling with it for the last few days
Thanks I will give it a shot
I will give it a try thanks
The basics like memcpy, memmove etc I think they are referring to, I wrote out functions that could be used for replacements (and some I pulled from various places)
Preference mostly, I wanted the construct to stand out a bit since it affects the whole scope. Lets are also limited to appearing at the start of expressions, somwehat like in traditional C. Most languages mix prefix/postfix/infix to some degree; some use infix for maths, which Snabel doesn't. I also don't plan on forcing list literals to be typed in reverse.
It's quoted due to the braces. Anything in braces will result in a pointer to the compiled block being pushed on the stack. The only thing 'let' does differently is additionally binding the name.
Bloomberg implements most of the containers in [its STL](https://github.com/bloomberg/bde) (and anything that might allocate memory) extending the traditional containers with polymorphic resources, but for many of the other parts of the STL, it forwards to the native one.
Onyx looks more general-purpose and less evolutionary than my vision for Snabel but it's definitely in the same direction.
I just noticed that newlib doesn't seem to support x64 :/ do you have any other alt's to newlib?
Newlib _should_ support x64. Looking at the source there's newlib/libc/machine/x86_64 and I'm pretty sure there was an x64 version of NACL which used Newlib as its c library. As for alternatives, Newlib is the only non-hobby c library I know of that's actually friendly to porting. There's always the option to roll your own if you're really dedicated and have a lot of time.
&gt; Yes, but to get speedups you need HW, right? Technically, yes, _to get speedups_ you need HW, but you don't _need_ the speedups* whereas you _need_ the compiler+library support to effectively use transactional memory in C++ at all**. (*) just like how games can and did use linear algebra vector types before widespread mainstream availability of Intel's MMX or SSE. (**) as with heterogeneous computing, which requires using completely separate GPU languages or non-standard extensions like CUDA as today's standard C++ lacks the necessary compiler+library support.
Cool! Seems like a nice alternative to json for config files, as it allows for comments, which json does not. Is this format standardised/documented anywhere?
I'll see if I can get that to work, thanks for your time and information!
Oh! I though the block was evaluated eagerly, the brace only bringing in a new (fresh) stack.
Regular parens will do that for you, braces add quoting and labelling on top.
Your problem appears to have been with CMake integration, not with modules themselves. The TS doesn't discuss build systems. The MSFT implementation is way better now, and the TS has changed as well. But I don't believe anyone has done significant build system work. Build2 is working on modules support though.
 Utf8String text(); //Empty UTF8 object This doesn't seem right.
Good job! Some criticism: * Why exactly did you manually define the default constructor, the copy constructor, the copy-assignment operator and the destructor? I suggest you use the compiler-generated ones. It's both less work and less error-prone (you don't have to change these should you choose to extend your class). * Why don't you offer a functionality to access the individual characters? Your approach's single benefit is that lookup is O(1) and by not offering a function to actually look up characters, you don't even leverage that advantage. * The whole vector of vectors approach seems very inefficient to me. The normal approach is to store the UTF8 string contiguously, even though that'd make lookup O(n). * What exactly does your library offer except, well, measuring the string's length? Does even that work properly? Is your library able to handle composed characters? I'd expect an UTF8 library to perform more complex tasks such as unicode normalization. However, that's probably not something a single person could implement in a reasonable amount of time. ;) * What you're doing there with `std::bitset` is unusual but not necessarily bad. The more common approach would be to use manual bitmasks. Let me know if you want me to elaborate on something
First of all thank you a lot for the criticism! It's the first time I put a project on an open forum and, thanks to you, it won't be the last :) ! For your criticism: * I don't know (or better didn't) that was even an option! I'll look into it! * This library is more of rough draft, but it'll absolutely support access to individual character soon (the main reason why I wrote this was exactly for the need of a smart iterator) * Yep, it's true but I don't know any other way without either using a union or std::variant. But I'll also look into it after thinking about the options. * Unfortunately, as I said, it's only a rough draft. I was hoping to offer a sort of base that isn't as big as UTF8-CPP or tiny-utf (the latter clocks at about "only" 2000 lines of code) * I used std::bitset because it's just a "cleaner" interface around bitmasks. But that might change with the whole access thing. Once again thank you so much for your criticism. Have a fantastic day. 
I'm so dumb... Thank you so much, it meant to be: Utf8String text; //Empty UTF8 object 
Ah, I see. So the original Java property files format, is that standardised, like JSON or YAML?
Thanks for the explanation!
Those are typically called domain sockets, IIRC...
Looking at [this](https://github.com/BassLC/idUTF8lib/blob/91159d96d6ac128774c247db09439c8b6de078ac/lib/idutf8lib.cpp#L115) line,why did you declare chr as "auto&amp;&amp;" and and not "const auto&amp;"? . You see to do this in all places that use the new way of doing loops but you dont assign the variable. 
I'm sorry if this is not what you asked as I couldn't really understand the question but I'll try: by declaring it auto &amp;&amp;chr I'm making the variable chr an r-value. I don't think pratically there's any difference but I'm showing intent of only using chr in the right-hand side of an expression. [More here](https://stackoverflow.com/questions/3601602/what-are-rvalues-lvalues-xvalues-glvalues-and-prvalues) Btw, thank you for your question. Have a nice day! 
but why
Read http://utf8everywhere.org, you'll discover what you did is a gigantic waste of time (and most likely a security liability as well)...
FWIW, it's an lvalue - it has a name. In addition, auto&amp;&amp; is a forwarding reference, not an rvalue reference. That is, it's an rvalue reference only if bound to an rvalue. This is because auto deduces the type. 
&gt; compiler+library support I dont disagree, but my point is why is there a need for ISO to get involved. In other words if GCC did what they did(implement TM) would that be a bad thing? If anything GCC may be able to move faster, although you can claim that there is wisdom in ISO that enables GCC to move in the right way, although a bit slower. 
That's really simple! A bit useless without the extensions that you added actually... :-) (or not useless, but hardly useful as a general config file format except for very specific simple cases).
The point i was making is that you are taking a reference to a variable but all you do with the reference is reading from it and hence you should take a const reference. 
Thank you for your answer! It looks like I don't even understand what I'm doing... 
Actually, that makes sense! You are probably right, I'll change it with the next couple of features. Thank you for your suggestion!
&gt; Postfix So it's not actually intended to be used in the real world, is it?
How does a shorter name make things easier for these scenarios?
[removed]
No; the former is a function declaration, the latter is a default-initialized variable.
* Your destructor calls clear on the vector. This serves no purpose. You have no need to define a destructor. * Your constructor doesn't (explicitly) initialise the vector, and calls clear on it? I can't work out why. Again, no need to define a constructor. 
I just did that just to be sure that the vector was empty but you're completely right, it serves no purpose! It will get remove in the next commit. Thank you for your response
That intro is cancer. 
* 7 places you're comparing `.size()` with 0. Prefer `.empty()`, it more closely expresses your intent and will probably be faster. * Don't use else after a return (e.g line 711), it's just a needless indentation. Static analysis tools such as clang-tidy will warn about this. * Many functions pass std::strings by value which will incur a huge amount of heap allocation, modulo moves. Pass them by const reference unless you have a specific reason not to. * Some member functions (e.g. hasKey) can be const. Make member functions const by default, make them not const when they need to be. Same principle with local variables. * get_bool or putComment - which is your function naming convention? * Why are Props heap allocated? They're already stored in a vector, and therefore on the heap, so it isn't because of stack size concerns. * Under-use of auto. Anywhere you've spelled out a nested `::size_type` would be better written as auto, in my opinion, it's less typing, less to think about and stays correct when you change your container type. * Don't throw the standard library exception types, write your own. If you throw one of the standard ones, someone using your library is likely to write a catch clause which will catch it by accident, as opposed to specifically catching YourLibraryException. 
That would be the "modules" proposal and I'm pretty irked it didn't make it into c++17.
Is this a joke?
Where does the slowdown occur exactly? How would something be done with and without transactional memory and which part is responsible for the speed hit?
Do you think that might be more due to inexperience than anything else? 
A. Because C++ relies on C's preprocessor. When in C++ you make an include what you do, literally, is to copy paste the content of the file in place where the include directive was written. Once all the processor's directives were handled (#include, #pragma, #define, #ifdef, #if, #endif, etc...) and macros replaced the code start to being evaluated by the C++ compiler, before that it's just text. B. Modules (C++20) C++20 might have a module system. this means C++ might have in the future 'import', 'export', 'module', etc...
for fun
you haven't said what the problem is 
Considering you can just plop into Rust, Java, C#, Haskell, and others and it just works like you think it does no - C++ headers files are unwieldy and unintuitive. Being able to learn to use a tool well is different than the tool being good or not. C++'s headers and preprocessor are literally a step above nothing in that regard. Hopefully modules make it into C++20
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
Here's how we build: We run `c++ -c a.cxx`, `c++ -c b.cxx`, ..., in-parallel. We get `.obj` files and lists of header dependencies for each `.cxx` file. Then we link the `.obj` files together. If compiler doesn't support `/showIncludes`, we use a C preprocessor. Build script has only a list of `.cxx` files. -------- Here's what happens with modules. First, we need to parse `import` and `module`. That requires full C++ parser, only a compiler can do that. We run `c++ -c a.cxx`. Alas, it depends on module `m.ixx`, so we run `c++ -c m.ixx`. Alas, it depends on module `n.ixx`, so we run `c++ -c n.ixx`. Success, now we have `n.ifc` and `n.obj`. Then we run `c++ -c m.ixx` again and get `m.ifc` and `m.obj`. Then we run `c++ -c a.cxx` again and get `a.obj`. Finally we link the `.obj` files together. We've parsed the files twice. Everything was done sequentially, no parallel compilation anymore. Alternatively, we have to manually write module dependencies in the build script. Instead of `sources(a.cxx, b.cxx)` there would be `sources(a.cxx : (m.ixx), m.ixx : (n.ixx))`. Imagine that you have to change build script every time when you add or remove `#include`. Nonsense, right?
beesh-tuh-tup?
why call it and idiots library? is it because an idiot wrote it?
FWIW mixing english titles with italian language videos will get you downvotes on YT. 
dumb question -- where does the "back end" and "front end" of gcc meet? I can easily understand this in llvm -- the front end spits out arch-independent IL, and the back end takes the IL and makes it arch-specific assembly. But since gcc doesn't use "IL" (as is my understanding), what's the abstraction, the interface?
So just to confirm - this doesn't modify the value of the passed value, but only modifies the value that's been stored with the lambda? I feel like generally this should be avoided - from a code clarity point of view there's implicit state that's hard to reason about if the lambda gets copied and stuff. Side note - there's a "//compilation error" on the mutable example when there shouldn't be.
for this Scott Meyers' Appearing and Disappearing consts in C++ would also be a nice start: http://aristeia.com/Papers/appearing%20and%20disappearing%20consts.pdf
GCC has its own arch-independent IL, called GIMPLE, so the principle is the same for GCC as for LLVM. But there are some important differences between GCC and LLVM in how things are done. I should probably add a blog post about that...
It is faster to type: I login to 10 different machines daily. Some of them are in-memory build bots so there is even no way to save an alias even if I wanted to.
Half of it is in some strange language that sounds like Mexican!
If you look for some good configuration language you should have a look to [toml](https://github.com/toml-lang). 😎
What does ``decode`` mean in this context? 
Most lock or wait free algorithms are slower in the average case than slapping a mutex around something. Same goes for transactional memory, you exchange average case performance for almost everything else being easier or better. Different algorithm = different speeds.
Thanks for the article! Simple question about the GCC backend: is byte defined as a 8-bit variable? Strictly speaking, it's the minimal addressable unit and may be i.e. 32 bits. So how difficult it is to port GCC to such an architecture?
Great! I'm starting a new project and I would need something like this. I'll try it! :) Maybe you should raise some issues in GitHub, like enhancements, so people can contribute back ;)
You can set BITS_PER_UNIT which is described as &gt;The number of bits in an addressable storage unit (byte). If you do not define this, the default is 8. So it *should* be easy to make this work on such architectures. But it seems like all current back ends in the GCC distribution are using the default value, so the support may have regressed over the years...
To make it more clear; I am refering to this statement from the README: &gt; Decodes and parses UTF-8 strings correctly ``Decode`` into what? ``Parses`` into what? I do not understand, what purpose this lib serves‽
deleted ^^^^^^^^^^^^^^^^0.1780 [^^^What ^^^is ^^^this?](https://pastebin.com/FcrFs94k/82053)
That looks really great actually! Thank you very much for linking it. It seems to be quite popular as well! (with Cargo, GitLab, and many other quite well-known projects using it). Very attractive for config files, and in contrast to YAML, there exists a handful of header-only C++ (and even C++11) implementations! Awesome.
This is actually useful and I have use cases for it, but I'm afraid to use it.
The API's for those C++ toml implementations are not great though. The good parsers appears to be for Java and C#. Then again, the yaml API's are not great either.
Right, thank you for the comment!
I think size_in_chars() is perfectly fine. In UTF8 there needs to be a differentiation between bit length and size length and you're going to need to be explicit one way or other (or both ways). 
Yea, and there isn't a single C++11 or onwards header-only YAML implementation. It seems like these TOML implementations are as good as it can get (, unfortunately).
Why the downvotes? Isn't installing and using clang through msys2 an option?
Probably because of how I worded it. I may or may not come off like an ass there. But I'm just wondering why somebody would go through all the effort of building clang, which takes a very long time, when you can get functioning prebuilt clang binaries with MSYS2. And those don't require Visual Studio either, which is a pro or con depending how you look at it.
It is not an STL implementation. If you look at the first include file algorithm.h: &gt; this file implements **some** of the primary algorithms from the C++ STL algorithm library emphasis mine
There is the [Bloomberg Development Environment](https://github.com/bloomberg/bde) which includes a standard library implementation [BSL](https://github.com/bloomberg/bde/wiki/BSL-and-STL). I'm don't believe everything in the C++11 standard is fully implemented though.
Thanks for correction. Yes, you are right. Regarding this should be avoided: I personally agree with you. But at times this keyword allows you to do some wonderful stuff. I will post some sample codes in some time. Those codes would make you feel importance of this keyword. 
Thanks for sharing this. I will read it. 
Wow! This is amazing. We would like to use your algorithm in production. Also we would like to offer you a senior developer contract. 
I wanted to put some contex tin a comment so it's not part of the initial discussion. The reason there's an external build system is that this is for a UE4 project, and its build files are in C#. I'm trying to allow the UE4 programmer side to specify targets only by name in a build file, and then find out everything that's needed to link them + transitive dependencies, since UE4 wants you to give it the lib names, where to find libs, and the include paths to add. My libs are already in a CMake tree, so I wanted to leverage the existing information instead of having to manually specify every possible lib. ---------- So far: * I've tried running a CMake server/daemon and requesting the info but LibA's target in the JSON response doesn't include the transitive library dependencies. * Dumping _INTERFACE_LINK_LIBRARIES_ for each target to a file manually and parsing them might work, except they follow a different format sometimes - the transitive libs listed aren't always separated by a character and there would be no way to know how to split them. Also _INTERFACE_INCLUDE_DIRECTORIES_ doesn't seem to carry the transitive includes. * Some more ideas are listed [here](https://stackoverflow.com/questions/32756195/recursive-list-of-link-libraries-in-cmake) but none are depicted as being particularly successful. Next up: * Generating a new solution/project starting from the _Root/Libs_ level into the UE4 project's directory (i.e. out-of-tree *add_subdirectory*), creating a dummy target with blank main.cpp, and then generating a .vcxproj for that dummy target. That project file will contain a flattened form of the transitive libs, all include paths, etc, and I can parse it as XML. Kinda gross though! ------------------
$150k/year with fully vested equity sound about right?
Yeah we thought about $200k, but that's fine too
So it's an algorithm that only works on non-negative integers, and whose runtime is proportional to the value of the largest element?
nah, I'll take $1m
uhh...my foolishness ? perhaps
you may be looking for CMake server mode: https://cmake.org/cmake/help/latest/manual/cmake-server.7.html
&gt; ?? s/to/from :( 
!remove
OP, A human moderator (u/blelbach) has marked your post for deletion because it is not appropriate for r/cpp. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6s5if4/my_own_sorting_algorithm/dlabffy/,%20was%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Unfortunately, the ["codemodel"](https://cmake.org/cmake/help/latest/manual/cmake-server.7.html#type-codemodel) request does not list transitive libs/deps for a given target - you can't parse the JSON to find the LibA target listed and then access all of the libraries it needs to bring in from all of its dependencies.
If you add --graphviz=graphviz.dot to your command line you get a "dot" notation file graphviz.dot that contains all the dependency information. Note this does not work with OBJECT libraries at present. We have used this before to solve similar external ( to CMake ) tool issues. Hope this helps
Oh that's interesting. Maybe could parse it to get the dependent targets and use those to try to get the .lib names, paths, and include paths!
Yes that is exactly what we did. We actually wrote our own code to parse the dot format. I think there is some open source stuff but it only took us a few hundred lines. Not sure the paths are in there and this is only library dependencies not headers. 
Some talks are in Italian yes, but most related slides are in English for everybody's pleasure. Reason is that the conference was divided in two "tracks" one with the international speakers and one with the local Italian ones. The Italian presentations are very interesting, I suggest everybody to skip through the video and read the slides. Maybe someone will be tempted to start learning Italian!
Yeah, I wouldn't rely on it for anything. It's being labeled as invalid by the C++ std committee. It's a fun toy to nerd out on for some, but it should never be used for anything serious...unless it's to change the committee's mind or something. I doubt they will, but you could try.
You could download the *actual* STL: https://www.sgi.com/tech/stl/
You can use CMake scripts to generate that information for you in any format you want. From each top level target, you request its dependencies and recurse into them, adding to a top level variable the list of targets. Then for each target, you can find the file location and create a file with all that information. If you don't have any specific top level target, you can also list all the targets in each directory and recursive in all subdirectories (there are properties for that) and just get the information you need.
Not gigantic, but yes -- mostly waste :)
What sick puppy would use msys when wsl is now an officially supported product?
I think he missed the most important part: How does it handle cross platform paths? He uses windows paths in the examples, but does it work with posix paths on windows?
&gt; Most lock or wait free algorithms are slower in the average case than slapping a mutex around something. I've done a fair amount of lock free programming and I'm not sure what scenarios, if any this holds true. To be clear, you are saying that transactional memory is slower on average but makes things easier and 'better'. What does better mean in this context? 
&gt; would be very useful to me `char *rawByteString()`, basically, a continuous block of memory with all bytes The convention is to call such a method `data`. See: `std::vector`, `std::string`, `boost::container::vector`...
&gt; How does it handle cross platform paths This doesn't make much sense. Paths are inherently platform- and filesystem-dependent and `path` is meant to represent a filesystem object on the system where the program is running In any case, `path` can accept a _generic pathname format_, which is a well-defined format described in 30.10.8.1, but whether `path` converts between native and generic is implementation-defined. It probably shouldn't be used directly. const fs::path p1 = "c:\\dir\\file"; const fs::path p2 = "/dir/file"; std::cout &lt;&lt; "generic: " &lt;&lt; p1.generic_string() &lt;&lt; "\n" &lt;&lt; "native: " &lt;&lt; p1.string() &lt;&lt; "\n" &lt;&lt; "\n" &lt;&lt; "generic: " &lt;&lt; p2.generic_string() &lt;&lt; "\n" &lt;&lt; "native: " &lt;&lt; p2.string() &lt;&lt; "\n"; With Visual C++ 2017: generic: c:/dir/file native: c:\dir\file generic: /dir/file native: \dir\file With libc++ on Linux: generic: c:\dir\file native: c:\dir\file generic: /dir/file native: /dir/file So there's nothing "cross platform" about `path` because paths are not cross-platform to begin with. Here's a few [valid paths on various filesystem](https://en.wikipedia.org/wiki/Path_(computing\)): OpenVMS: SYS$SYSDEVICE:[.DRAFTS]LETTER.TXT;4 RISC OS: LanMan::WindowsC.$.Pictures.Japan/gif Stratus VOS: %sysname#module1&gt;SubDir&gt;AnotherDir [edit: note that this is about generic and native *formats*, not encodings (UTF-8, UTF-16, etc.), which I discuss a bit in [my other post below](https://www.reddit.com/r/cpp/comments/6s5j0h/c17_in_details_filesystem/dlbexka/)]
Because WSL is Linux? Clang with MSYS2 generates native Windows binaries, where with WSL you would have to cross compile your code. Cross compiling code is generally something you want to avoid, not embrace. Would you write your code on the Windows side and then cross compile it back to Windows and do remote debugging from a WSL terminal? If you want to do graphical debugging you have to configure a graphical environment on it, which not only eats resources, but makes your workflow more complicated. You would use WSL to compile Linux binaries, and perhaps develop them on WSL too, but not native Windows binaries. Using WSL to compile your code in a Linux configuration avoids the whole cross compiling shenanigans, and you get to run/test your binaries too. In this situation you get rid of complexity for clear gains, the other way around not so much.
So the codemodel will provide the full link line and include line(s), which will contain the resolved transitive information. What is the use case for wanting to know the target names? FYI To use the codemodel correctly you will actually want to create a dummy target and link it to the lib you want information about as that will than generate the proper usage information.
Calling it `size_in_chars` is not really correct naming, as the function doesn't return that. Not every unicode code point is a character. There are also combining characters and graphemes. This can also be locale dependant. Also the name could be missleading - does it mean size in chars as in `char` type, or real character. Someone suggested accessing individual characters. If you do, do it read-only as not every "your character" is real character and mutating such a character is not a good idea.
If you care about only Posix and Windows, and you stick to relative paths and don't do weird stuff, then you can get fairly cross-platform behavior. (Note: I don't consider myself a filesystem expert at the moment)
Hmmm. This for (const auto&amp; entry : fs::directory_iterator(pathToShow)) makes me a bit uncomfortable... we seem to be iterating over an iterator? Wouldn't it be more consistent with the Ranges TS etc to have `fs::directory_range`, returning a `directory_iterator` from its `begin()` and `end()` member functions?
That's latin dumbass, the latinos created Faberge eggs during the inquisition. Educate yourself.
&gt; Wouldn't it be more consistent with the Ranges TS etc to have fs::directory_range, returning a directory_iterator from its begin() and end() member functions? Why would this be consistent with the Ranges TS and not vice versa? This is part of the standard now, whereas Ranges is just a TS.
But then you rely on `current_path()` and you're back to square one. You probably want to access relative resources using the executable's path, which requires system-specific calls. If you assume that `current_path()` is correct and that it won't change while the program is running, then yes, `"dir/file"` will work on POSIX and Windows. Might as well just use a string though.
&gt; Why is everyone trying to get their beloved library into the Standard? Because the standards committee has literally been asking for people to do exactly this!
Oh boy... where to start. 1. WSL isn't Linux, it is ELF, but ELF *is* Windows 2. The Windows kernel loads ELF executables 3. Clang can be configured as a cross compiler to produce PE binaries if desired 4. None of any of this has anything to do with simply running the Windows clang 5.0 from a bash shell
I'm not sure what you mean. I mentioned the Ranges TS as that formalises the notion of ranges that has existed since the original STL -- a sequence denoted by a pair of iterators (although I suppose the range-based for loop did that much earlier). Now we have this `directory_iterator` thing that is simultaneously both an iterator *and* a range. It just feels wrong.
I would guess that they didn't changed that part of the language, meaning you need to declare functions before their first usage.
100% agree. My implementation relies on a bunch of undefined behaviors from the standard and are defined this way only by Gcc. Though, I believe, this will work with GCC for a long time as for it not to work it would require a bunch of major changes. So use it as a compiler dependent library (with extreme caution). For instance in icc (intel c++ compiler) this would hang. As for Filip's implementation it was standard compliant but now only works with GCC. clang's now detecting a function redefinition in this case template &lt;size_t&gt; struct A { friend constexpr auto func(A); } template &lt;size_t N, class Type&gt; struct B { friend constexpr auto func(A&lt;N&gt;) { return Type{}; } }; And doesn't follow the standard correctly in some places (https://bugs.llvm.org/show_bug.cgi?id=15481) That being said I had a chat with Filip a while back and told me that there were other techniques to replace adl lookup friend injections (if I were made unusable, it's still not really the case) and that making all them ill-formed would be quite difficult for the standard.
It comes from Boost.Filesystem. Non-member `begin()` and `end()` for `directory_iterator` [were added in 1.51](http://www.boost.org/doc/libs/1_64_0/libs/filesystem/doc/release_history.html) in 2012 using [this feature request](https://svn.boost.org/trac10/ticket/5896). [This duplicate ticket](https://svn.boost.org/trac10/ticket/6521) was proposing `directory_range()` instead. I don't know why they went for the former approach. I agree that treating an iterator as a range is confusing.
happy i didn't give you a real email address, that pdf is weak on content.
If you're interested, have another look at the library. I pretty much implemented what you suggested. Worked out quite well IMO. Interested to see what you think! 
My favourite feature related to std::filesystem is the basic_fstream::open() overloads that take a path object. The internals of filesystem::path will handle unicode on windows transparently (no more char/wchar_t stuff) 
Then perhaps the name should be changed to size_in_codepoints, but the point remains. There's enough ambiguousness in what "size/length" can be on a Unicode string that explicit is better than implicit.
There was a paper in Toronoto that offered this, but, it was not accepted. This paper http://open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0584r0.pdf described how an interface file can be split between multiple files which would necessitate a first pass to collect top level declarations that will become available to the normal compilation afterwards. If p584r0 were accepted, your example would be valid.
Agreed - I really like the simplicity of something like for (auto i : someVector) and not for (auto i : std::vector::vector_iterator(someVector) ) That's the kind of thing that makes people make fun of C++.
Too bad, this seems like a really useful thing to have. Is there a chance it may be accepted after rework, or is the entire concept considered undesirable? 
~~Not present in the standard and~~ I would fight to prevent it becoming standard. If you want a `directory_range` type, fine, but don't create semantic abominations like this.
This misses the point. `vector` is a range. `directory_iterator` is not. /u/tcbrindle is not saying there should be no directory range. He is saying that directory range should not be `directory_iterator`.
Yes and no. If you construct a path with `char`s, nothing `path` can do can save you; you already discarded the information. The overloads that were added to the standard (`fstream` and friends getting `const path::value_type*` overloads) have been present in our implementation for a long time, the standard change merely documents what we have been doing.
&gt; why? It apparently works well for a very specific use case that a lot of graphics work doesn't overlap with, so it would generate more trouble than benefit to interop with the "standard" graphics API. It seems to work just fine as a library, making adding complexity to devtools of unclear benefit. Most people working on language standardisation don't work in graphics -- they are more likley to be compiler developers. Dedicated graphics developers tend to massively fuck up things like proper colorspace handling, so expecting compiler devs to properly spec and implement much in the way of graphics APIs seems like asking for trouble. Imagine if we all had to use a compiler written by Photoshop developers. Cairo is a C API, so slapping on a C++ wrapper API around it is unlikely to result in the most elegant/idiomatic C++ API possible, compared to one natively specced in C++. It freezes a feature set, and effectively forks Cairo. So as Cairo continues to evolve, users will be forced to pick between real-Cairo and the standard graphics API. Forgoing features in hope of something ubiquitous. It apparently has some level of OpenGL interop. Does OpenGL become part of C++? What if I am writing a Vulkan app, or Direct3D? Is the Cairo API optimal for modern GPU interaction? A thousand related questions related to hardware that probably have bad answers that I am too lazy to dig into.
I think you misunderstood me, but due to the way you've chosen to express yourself I refuse to engage you further. 
If by "semantic abomination" you mean the range/iterator duality of `directory_iterator`, then it's too late -- this is standard in C++17. See 30.10.13.2 [fs.dir.itr.nonmembers] in the latest draft.
Yes I guess the way I am using it is like many unity builds rather than just one. Thanks for the info about performance for vs2017.
Sure, but once you have those maybe you can accumulate the libs, paths, and headers by using those target names as a look-up at some point. Interesting idea, but might be easier to just recurse in CMake script itself to map out the lib dependencies, and you get the added benefit of accumulating the lib info as you go!
Good point - maybe I don't need the target names after all, as long as I can get the info I need for linking. Is there a way to be more specific with codemodel and only get info for a particular target(s), e.g. the dummy target? Also I wonder the practicality in terms of time of starting up a server as a pre-build step in my system. I played around with it via the console and it seemed slow, but maybe I'm thinking of the configuration step, which is slow either way.
not a fan of most sites that try and harvest emails from "hype" if your going to give out a free "cheat sheet" should atleast have email as optional for it so you dont end up with duplicates but its always good to see other people using C++17 even if the examples don't seem to be agreed with by others for anyone interested https://1drv.ms/b/s!Ar7CAEy9gW2WiQLYed6jbAPhqJaA is the cheatsheet
That sucks. A lot. I wish I had not missed that.
One more from Microsoft: https://github.com/Microsoft/vcpkg
https://www.phoronix.com/scan.php?page=news_item&amp;px=Glibc-2.26-Redis-Test
&gt; Faberge eggs Fcuk, dude, I've been trying to kick that habit ever since I became a 10x https://www.youtube.com/watch?v=JA_Mfvt1IIk
It's a bit difficult to piece back the history of this feature, it'd be nice to get Beman Dawes in here. There's [this discussion](http://boost.2283326.n4.nabble.com/filesystem-6521-Directory-listing-using-C-11-range-based-for-loops-td4570988.html), where Beman proposed `directory::begin()` and `end()`. He seemed happy about it. Then there's [this discussion](https://groups.google.com/forum/#!topic/boost-developers-archive/iN6toGpcV1w) three months later where the implementation has switched to the current one. There was some opposition to it: &gt; I'm a little surprised to see a function that returns a range named "directory_iterator". &gt; [...] &gt; I'd like to see a distinct range class [..] which can be used with the range-based loops naturally, instead of abusing begin() and end(). Beman answered this: &gt; I don't see any abuse of begin() and end(). They were put in the core language's list of ways to enable range-base for so that there would be no need to add the complication of a separate class plus a make_x_range function to handle cases like this. &gt; Unless I'm missing something, adding wrappers the user must use increases complexity and reduces teachability without adding additional functionality. The beauty of begin/end free functions is that the user doesn't even have to know they exist - range-based for "just works". &gt; Let's wait a bit to see what the committee's Filesystem study group thinks. If they want the non-member being/end approach for the standard, that would still leave directory_range as something that could be added to Boost.Range. And that's it. I don't know who's on the "Filesystem study group", [isocpp](https://isocpp.org/std/the-committee) only mentions Beman. I could track the addition to [n3399](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3399.html), two months after the last discussion.
I was in the filesystem study group in Issaquah and this was not discussed. Must have happened in Kona.
It seems to have been added between Kona and Portland in 2012.
I had a use this case for mutable lambdas -- wrote a task engine where tasks can have dependencies. So, task gets executed, probably generates subtasks, once subtasks are done -- task gets executed again (in case if it needs to generate more subtasks). Occasionally, I needed some mutable state to be stored between these executions for case when task was represented by lambda. (framework allowed user to pass lambda for simple tasks).
Is there any way to just use UTF-8, such that behind the scenes it converts path strings to the Unicode format of the underlying platform (eg. UTF-16 on Windows)? I can see it not being that simple (since C++ needs to support way more than Windows/Linux/Apple), but unfortunately that kind of makes `std::filesystem` not very useful when a lot of C++ and C libraries nowadays just let you use UTF-8 for paths on all supported platforms.
That paper deals with module partitions. I only want to use functions and types without declaring them beforehand, like C++ already does inside a class. Requiring module partitions for this would be a serious overkill.
IIRC, module partitions is delayed until modules v2.
The implementation of `path` in Visual C++ is not exactly intuitive for UTF-8 and UTF-16. The standard says that conversions between encodings are unspecified and "operating system dependent". If you construct a `path` with an `std::string`, it is converted to an internal `std::wstring` by calling [`MultiByteToWideChar`](https://msdn.microsoft.com/en-us/library/windows/desktop/dd319072(v=vs.85\).aspx) and passing `CP_ACP`, which represents the "system default Windows ANSI code page". **It does not convert from UTF-8 to UTF-16**, and so the resulting string is most likely useless and broken if the original was UTF-8. If you construct a `path` with an `std::wstring`, it is copied directly to its internal object. If the original string was valid UTF-16, the `path` object will also be. Calling `string()` (or anything that returns a generic path) will do the opposite conversion: it calls [`WideCharToMultiByte()`](https://msdn.microsoft.com/en-us/library/windows/desktop/dd374130(v=vs.85\).aspx), again passing `CP_ACP`. The resulting string is not UTF-8. Calling `native()` (or anything that returns a native path) will simply return the internal `std::wstring`. What you always need to remember is that on any Windows API, `char` means "codepage", not "UTF-8". Unless a piece of documentation specifically talks about UTF-8, you should assume a string made of `char`s is a "multibyte string", encoded in some codepage. Don't let the `path` constructor taking an `std::string` fool you. **You cannot use a "UTF-8 `path`" on Visual C++**. If you have a UTF-8 string that represents a path and you want to store it in a `path` object, you need to either use `u8path()` or convert it to UTF-16 yourself and pass the `std::wstring` instead.
"and how poor std::unordered_map&lt;&gt; generally is, unless you have around 100 items or more." #fakenews unordered_map is not bad, it is just that author uses log scale to make (log n) look not so bad, when in fact it is a disaster. Also IDK if English is authors first language, but this is extremely biased(saying "generally" and "unless you have over 100 items" in same sentence) Insertion performance is also misleading. Ordered flat maps have disastrous insertion speed, unless you construct them at once. mild offtopic: is documentation for 23 buggy here wrt complexity? http://www.boost.org/doc/libs/1_61_0/doc/html/boost/container/flat_map.html "but rather because determining the string ordering relation is always an expensive operation " false, for random strings you only compare 1 or 2 chars most of the time you do &lt; comparison. http://en.cppreference.com/w/cpp/algorithm/lexicographical_compare What article misses is that flat maps (for missed lookups especially) trash the cache and that slows down other code, and microbenchmarks can not easily account for this. hash map on the other hand needs to read very little memory to confirm 42 is not in the map. I once considered doing a realistic benchmark of this but I gave up since it is quite complicated. tl;dr : save yourself the time and skip this article and always use hash map, unless you really need the space efficiency/quick iteration and you can guarantee the size of your maps will be less than 100-200. Then this article is useful. Just remember that unordered_map implementation(as demanded by the ISO) is trash and you can find better ones and comparisons in this article are made against unordered_map. This is not my rant, Chandler says so. :) P.S. I expect downvotes since I seem hostile, but I actually think article is OK but it misses the big picture. In other words this article is dangerous if a junior stumbles onto it. All junior dev needs to know is that default map to use in C++ is not the one with 3 char len name, but the one with 13 char len name. :) 
I think linear search should be the fastest, if there is only dozen of items.
&gt; good addition to Boost or STL STL no for sure... To complicated and too easy to be used by noobs by mistake. Boost maybe but I do not like the API. .find and .count are for containers that can do .find and .count faster than std::find/count can. This one can not. 
So which unordered map should one use?
I agree that STL is unlikely given that flat_map didn't seem to make it (I am not sure why?). As for the API, in this case, it is for convenience (i.e., wrapper) with find(key) instead of repetitive verbosity of std::find/count and std::pair aggravation. This is definitely not a drop-in replacement for std::map/unordered_map.
Measure and see what works best for your situation.
&gt; unordered_map is not bad and &gt; Just remember that unordered_map implementation(as demanded by the ISO) is trash hmm.
&gt; So which unordered map should one use? I would just use unordered_map as default, then either profile or pick something using this link: http://incise.org/hash-table-benchmarks.html
"Do it yourself" is a weak blanket response. Surely there is some prior work on this issue, and I do not intend to replicate it. The previous answer hinted at a recommendation by Chandler Carruth, I would like to inquire for further information.
What about `u8path()`/`path::u8string()`? How do they work on Visual C++?
`u8path()` works as expected. It will take the UTF-8 string, convert it to UTF-16 and store it correctly. I've edited my post above to mention it as an alternative to doing the conversion manually. `path::u8string()` will convert whatever internal `wstring` to UTF-8, as mandated. As long as you got in valid UTF-16, that is. As an example: // UTF-8 LATIN SMALL LETTER E WITH ACUTE (é) fs::path p("\xc3\xa9"); The internal `wstring` now contains two `wchar_t`:`0x00c3` and `0x00a9`. The UTF-16 representation should be `0x00e9` instead. Simplifying a bit, if your codepage is [1252](https://en.wikipedia.org/wiki/Windows-1252), the result of the `CP_ACP` conversion is mostly that every `char` gets widened to a `wchar_t`. p.string() // 0xc3 0xa9 p.u8string() // 0xc3 0x83 0xc2 0xa9 p.u16string() // 0x00c3 0x00a9 Basically, garbage in, garbage out. Note that `string()` _seems_ to be giving you back UTF-8, but it just reverses the initial conversion, so results may vary. Note also the double encoding for `u8string()`. There's nothing _wrong_ with this. It all makes sense when you know that `path` doesn't do UTF-8 =&gt; UTF-16, it does CP_ACP =&gt; UTF-16. If you give it something that's not in your current codepage, it will give you garbage.
It wasn't meant as a blanket response. There's no "this is the best X". It all depends on the use-case you have. If you frequently insert into the map but almost never iterate vs insert all at once and iterate a lot vs some mix of the two.
no problem, I expected as much
Of the three libraries you cite, LLVM's is a hash map; EASTL and Loki's are sorted. None of the three have linear time lookup. Given the apparently absence of actual existing practice, do you have benchmark numbers? Under what conditions does this outperform, say, `flat_map`?
Read the context. unordered_map beside name is reasonable default. If your code is spending a lot of time inside .find/.count you need to profile, but then you should know that unordered_map is suboptimal hash map implementation. 
&gt; fs::path p1("C:\\temp"); p1 /= "user"; p1 /= "data"; cout &lt;&lt; p1 &lt;&lt; "\n"; &gt; fs::path p2("C:\\temp\\"); p2 += "user"; p2 += "data"; cout &lt;&lt; p2 &lt;&lt; "\n"; Oh god damn it.
You are right. I missed that those libs are providing ordered flat maps. Performance-wise, there's little difference from flat_map for few elements (the reference article provides the analysis). Some performance/complexity differences from flat_map: - insert does O(N) compare vs O(logN), but does not move elements around as flat_map does (i.e., O(1) vs O(N)), so it is the same as vector::push_back. - erase can be optimized in the same way to avoid moving elements around (O(1) vs O(N)).
Shame on everyone that read the code and didn't notice or mention the off-by-one error. Hint: Try compiling with `g++ -D_GLIBCXX_DEBUG` and then running it: /usr/include/c++/5/debug/bitset:264:error: attempt to subscript container with out-of-bounds index 4, but container only holds 4 elements. I added `-ggdb` and ran it in the debugger and got: (gdb) bt #0 0x00007ffff74aa428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54 #1 0x00007ffff74ac02a in __GI_abort () at abort.c:89 #2 0x00007ffff7b0af95 in __gnu_debug::_Error_formatter::_M_error (this=0x7fffffffc960) at ../../../../../src/libstdc++-v3/src/c++11/debug.cc:782 #3 0x00000000004091df in std::__debug::bitset&lt;4ul&gt;::operator[] (this=0x7fffffffcbd0, __pos=4) at /usr/include/c++/5/debug/bitset:264 #4 0x0000000000407c37 in Utf8String::is_valid_utf8_string (this=0x7fffffffce50, string="Hello World") at lib/idutf8lib.cpp:17 #5 0x0000000000407e6c in Utf8String::Utf8String (this=0x7fffffffce50, string="Hello World") at lib/idutf8lib.cpp:47 #6 0x000000000041bd60 in ::____C_A_T_C_H____T_E_S_T__ () at tests/tests.cpp:9 #7 0x000000000042e954 in Catch::FreeFunctionTestCase::invoke (this=0x706fa0) at tests/catch/catch.hpp:7290 #8 0x000000000041500d in Catch::TestCase::invoke (this=0x7088e0) at tests/catch/catch.hpp:8283 #9 0x000000000042d082 in Catch::RunContext::invokeActiveTestCase (this=0x7fffffffdac0) at tests/catch/catch.hpp:6844 #10 0x000000000042cd6d in Catch::RunContext::runCurrentTest (this=0x7fffffffdac0, redirectedCout="", redirectedCerr="") at tests/catch/catch.hpp:6811 #11 0x000000000042b91e in Catch::RunContext::runTest (this=0x7fffffffdac0, testCase=...) at tests/catch/catch.hpp:6624 #12 0x0000000000411f2c in Catch::runTests (config=...) at tests/catch/catch.hpp:6984 #13 0x000000000042debe in Catch::Session::run (this=0x7fffffffddf0) at tests/catch/catch.hpp:7118 #14 0x000000000042dd98 in Catch::Session::run (this=0x7fffffffddf0, argc=1, argv=0x7fffffffe108) at tests/catch/catch.hpp:7071 #15 0x000000000041bb5a in main (argc=1, argv=0x7fffffffe108) at tests/catch/catch.hpp:11323 (gdb) frame 4 #4 0x0000000000407c37 in Utf8String::is_valid_utf8_string (this=0x7fffffffce50, string="Hello World") at lib/idutf8lib.cpp:17 17 if ( bits[4] == 0 ) { p.s. I actually caught this by inspection, but I'm showing you a tool to help catch this type of mistake by yourself in the future.
don't forget hopscotch_map and sparsepp: https://tessil.github.io/2016/08/29/benchmark-hopscotch-map.html
What's the advantage over using an std::vector of whatever your entry type is with standard algorithms such as std::find_if, std::lower_bound etc?
&gt; .find and .count are for containers that can do .find and .count faster than std::find/count can where does this rule come from ?
Where did you learn that flat_map didn't "seem to make it"? My understanding is that the different proposals for such containers are progressing and encouraged by the committee.
AFIO provides utf erased path views exactly for this reason: https://ned14.github.io/afio/classafio__v2__xxx_1_1path__view.html
I really don't know. Would be happy if they do make it. Are there current proposals on flat containers to reference? 
Transactional memory lets you update multiple memory locations as an all or nothing operation. That makes implementing most lock free or wait free algorithms much much easier. Under the bonnet, the tsx runtime is written to guarantee even progression. That hugely reduces worst case execution times over say a mutex, making for really lovely latency distribution graphs where it's almost a flat line. If you really care about worst case execution times, a transactional memory compiler is a revelation, reduces cost to reach debugged and finished enormously. If you care instead about average execution times, then you'll be disappointed, though the comparative ease of development to debugged and finished is still a big win.
A more self-explaining interface?
The language already supports this: class Foo { public: void f () { g (); } void g () {} }; Implementing the same feature on module level would add considerable convenience. 
Here is the last published proposal: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0429r1.pdf However it might have been modified in the last meeting (or other feedback). Also take a look at colony and slot_map containers being proposed. Progress is happening in SG14 discussion group, go there for news.
The only part I miss (and it seems possible to get it in the future) is to be able to use the path type and the filesystem algorithms on other kinds of filesystems: archive/zip files, a mix of both system and archive fs, in-memory filesystem, etc. There was a discussion and unpublished proposal about that in the boost mailing list some time ago. If there was a way to use filesystem algorithms over PhysFS for example, it would be very useful.
If you solely wish to make a product, then learn the tools that allow you to achieve that quickly and easily (game engines). If you want to take an academic approach and understand what makes things tick, learn the abstraction between the hardware and software (a programming language). If you want to have a well rounded understanding of the technologies in the field of Computer Science and Games Development then I think the more academic approach of learning a programming language, such as C++, is the best choice.
Quite some game engines do use C++, for example take unreal engine it uses C++. Its just nothing like C++ you would be used to if you practiced it before. Unreal engine uses a lot of reflection to "automagically" work. Its still good to grasp some of the concepts such as pointers/references and passing by value or reference. I think if you work on a game you will experience the language right away, its a good practice. Every time you don't understand something just look it up or ask around.
I wasn't aware since I never used it, but I can understand how that works. And I don't think it would be easy to transpose that to free functions (or maybe, but I don't still consider that useful ATM).
thanks for the reply
Thanks for finding the paper. I'll do my homework in future :)
Cool! I think it's awesome that build2 is cutting-edge and the development cycle for new or experimental C++ features is much faster than with CMake. Also great that you support VC was well :-) build2 seems to have quite a strong support for pkg-config, may I ask how pkg-config or its files are still useful and used nowadays (and on what platforms? I guess Linux?)? I have zero experience with them, and also never had the need to use them with CMake.
It's the same except: - clear interface (aka lower the cognitive cost of the reader); - maintain invariant; - less chances for the user to manipulate the vector in the wrong way by subtle mistake;
Thanks! Essentially, `pkg-config` is the way to communicate information about an installed library that is *build-system independent*. Things like where are its headers (`-I` options), where is the library (`-L`) and its name (`-l`) as well as what other libraries have to be linked when using shared or static variant (which could be quite different). We now also use it for communicating the list of modules a library exports. `pkg-config` is widely used on Linux and FreeBSD, somewhat on Mac OS, and there is no reason why it can't work on Windows for, say, VC projects -- we use it in `build2` and it works great. One issue with cross-platform usage is the need to install the `pkg-config` utility. It is not hard (e.g., you can easily build it from source on Mac OS or get one from MSYS2 on Windows), but it is something that might not be there by default. There is a solution, however: the [`pkgconf`](https://github.com/pkgconf/pkgconf) project (which is a more modern re-implementation of the `pkg-config` utility) provides `libpkgconf` so the support for reading `.pc` files can be linked directly into the build system. We certainly plan to do that in `build2`, probably in the next release.
Thanks! So that's only geared for libraries that are installed into the system using the system package manager? Or is it essentially equal to CMake's `libname-config.cmake` files? And it's kind of a system-wide tool that keeps track of what is installed and where? So somewhat like a mini library-packagemanager (except that it doesn't install anything, it just keeps track)?
Nice work! The API looks a lot more flexible now.
The build system that builds and installs the library generates the `.pc` file. System package manager simply unpacks it, just like it does for headers, the library itself, etc. And `pkg-config` is simply a utility for querying the `.pc` files for compile options, link options, etc.
 int i = 0; for (const auto&amp; part : pathToShow) cout &lt;&lt; "path part: " &lt;&lt; i++ &lt;&lt; " = " &lt;&lt; part &lt;&lt; "\n"; The output: path part: 0 = C: path part: 1 = \ path part: 2 = Windows path part: 3 = system.ini Why does it yields the separator between `C:` and `Windows`, but not the one between `Windows` and `system.ini`? To indicate root?
What is a transnational memory compiler? I thought the whole point was that new instructions were going to support transnational memory. What is the compiler doing that I couldn't do myself? 
&gt; Is there a way to be more specific with codemodel and only get info for a particular target(s), e.g. the dummy target? I am not aware of any existing ability to query for a subset of the codemodel. &gt; Also I wonder the practicality in terms of time of starting up a server as a pre-build step in my system. I played around with it via the console and it seemed slow, but maybe I'm thinking of the configuration step, which is slow either way. If not previously created the cmake server will need run configure and generate once to build all the information for the codemodel. 
&gt; In this manifesto, we will also explain what a programmer should be doing if they do not want to dive into all complexities of Unicode and do not really care about what’s inside the string. It seems to me that OP wants to care about what's inside a string, ie. getting its length. This manifesto is OK if all you want to do is manipulate opaque utf8 data stored in std::string and just doing the conversion at the end when calling filesystem API (typically filepath).
&gt; What article misses is that flat maps (for missed lookups especially) trash the cache and that slows down other code Isn't this a problem with basically ALL data-structure/algorithms benchmark though ? And isn't it somewhat accounted for by judging speed + O complexity (e.g. if an algorithm is O(n), even if it's a linear itteration or linear search, it will trash the cache by pure virtue of the cpu loading up a bunch of cache lines with the container... even if on paper, running alone, it looks really good). Or is this not the problem you are referring to ?
If you wish to develop indie games look into an engine you like and learn the language most used with said engine. Game development involves much more than programming and ,generally speaking, especially with indie games, it's the art, the story and the mechanics which will pull you through. That's not to say having a game that is fast and has logic that is easy to modify isn't nice, but unless you know C++ really well it won't be of particular help with the former or the later and if you spend time learning it really well you'll realize that you still have to learn about 80% of the other skills needed to develop a game and never reach a final product. If you have a team to work with and you are the 'programmer guy' by all means learn C++, there are a lot of engine with API's written for C++. The language itself is very good for doing anything once you master it, since it's very flexible and has the concept of "zero cost abstraction", which is very nice when you want to work on something with a large codebase that has to run fast. Not to mention switching to developing your own engine will be much easier.
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
The mechanism that makes module partitions possible is the same mechanism that makes this possible. I can't imagine that we would want to do one without the other. I am still hopeful that WG21 will seize a golden opportunity here.
Correct, that part of the language has not changed yet. However, I suspect it is only a matter of time and experience writing module code.
Actually, it isn't actually that hard -- compared to other languistic requirements that compilers have to fulfill.
What I've often been wondering: Where do these performance regressions come from? Have the developers maybe reordered the optimization pipeline so that certain optimizations don't apply anymore? I keep seeing these sudden drops in performance from one compiler version to a newer one in various benchmarks; most often from GCC 4.9.4 to a newer one. This also begs the question: Are there no thorough unit tests for optimizations? Moreover, do they make an effort to fix these kinds of performance regressions? In other words, if an older version of GCC/clang produces better assembly for a certain code, should I open a bug report with the code in question?
For more background as to approach taken, I talk about it [here](http://saadahmad.ca/detecting-evaluation-context-inside-constexpr-functions)!
Just curious, what do you think about this one? https://github.com/ToruNiina/toml11
Holy mother of all UBs!
Will MSVC/Windows ever be "utf-8 friendly"? (i.e. utf-8 by default). Are there technical reasons to continue to prefer utf-16 for native encoding or is it just inertia/backwards-compatibility at this point? 
I can hack it with the following: module M; export class N // Instead of namespace N { static int f() { return g(); } static int g() { return 4; } }; Before modules, there was no advantage of doing this, since I needed the header in any case. Now, with modules, I can just use a class instead of a namespace and export the class. Especially since modules cannot be partitioned as of now - same as classes. Please don't make us write Java in C++ :).
Aren't clang modules distinct from the official modules TS ?
I think boost.graph can read dot.
Clang and Microsoft are both working to move their implementations closer to the TS. Clang is keeping some of its additional features as extensions but is otherwise aiming to be compliant with the committee's direction.
what am I looking at? (should I check out anything interesting in particular?)
Good to know, thanks!
&gt; Please don't make us write Java in C++ :) Not going to happen :-)
[P0726R0: Does the Concepts TS Improve on C++17?](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0726r0.html) Hm, so does it or not? Maybe counterarguments would be that current implementation of diagnostics is far from production quality yet, and complexity of "Sortable" etc. are actually "problem" for standard library implementers, not users..?
clang have an old version of modules that is not similar to the current standard proposal/TS, but also have an implementation of the TS. I think most people mean "clang's modules" as the clang-specific version, while "modules in clang" is intended to the TS implementation. I might be wrong.
This is great to know as someone who is working on a project using SDL. Do you happen to know if SFML ports as well?
OpenGL is a low-level graphics API, thus I would not suggest using it directly when developing a game. The same applies to X11 or WinAPI. Instead, make your life easier by using a library that does all that for you in a backend-/platform-agnostic manner. For 2D games, I personally use and recommend [SFML](https://www.sfml-dev.org/).
Yes, and that's why there is TS in the title. To be clear, `build2` does not support "Clang Modules", only TS Modules. Our modularized sample projects work with Clang and VC. Our build system tests pass on Clang, GCC, and VC (GCC's implementation cannot yet compile something like `std.core`).
Ok thanks And one more thing can i use sfml to make gui based app or is it game only ?
SFML gives you a context and events, so you can put whatever you want in there. SFML itself is not a GUI library so it doesn't do that for you. However, you can render GUIs ontop of SFML. I'd personally recommend [ImGui](https://github.com/ocornut/imgui) for that task. There exist SFML bindings, just Ctrl+F SFML. There have also been attempts to develop a GUI framework specifically for SFML, namely [SFGUI](https://github.com/TankOs/SFGUI), but I advise against it. I have used it but I had major problems and eventually switched. Its development is also stale.
wow I bet the guy who thought of overloading `/` and `/=` felt really clever.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide education, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6se36u/game_development_in_c/dlc3xr6/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
!remove
OP, A human moderator (u/blelbach) has marked your post for deletion because it is not appropriate for r/cpp. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6selol/is_c_rest_library_casablanca_still_an_active/dlc3ymn/,%20was%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Con, especially for debugging
If you want to use utf-8, you can use [u8path](http://en.cppreference.com/w/cpp/filesystem/path/u8path) for portable behavior. Using UTF-8 as the default setting would be incorrect as it would cause mojibake for any data returned by anything else on the system, e.g. the inputs to main(), which are encoded in whatever the currently selected legacy encoding is.
&gt;Will MSVC/Windows ever be "utf-8 friendly"? (i.e. utf-8 by default). I think it's unlikely that the native internal encoding will ever be UTF-8. I could see Windows + co making it easier to set the "legacy" encoding to UTF-8 (that is, make CreateFileA accept utf-8 input) though. &gt;Are there technical reasons to continue to prefer utf-16 for native encoding or is it just inertia/backwards-compatibility at this point? Backcompat / ABI stability are technical reasons. Everyone who adopted Unicode early and did the UCS-2 thing (such as NT, Java, Javascript, etc.) are now paying for it :/
Some performance regressions are introduced as a result of insufficient testing, but there are several reasons why the performance may regress for a benchmark even if the compiler is sufficiently tested. For example: * The compiler cannot know if a branch or a conditional move is the fastest – it depends on if the condition is predictable or not (i.e. depends on the values at runtime). The compiler's heuristics may have been updated to be better in average, but some programs that were lucky before may be unlucky with the new heuristics. * Different instructions have different performance on different cores, and the compiler may have been tuned so the code is faster on other (more important) cores. * Some optimizations may conflict, so you sometimes need to reduce optimization in one pass so that it does not hinder later passes that in general give a bigger improvement. My experience is that real programs usually get faster for each new GCC release, while small benchmark programs often have regressions (as one unlucky code generation change is very visible in a micro benchmark, while it is in the noise in the large programs). Compiler developers do make an effort to fix performance regressions (one of GCC's release criteria is that all regressions are addressed), so you should open bug reports when you see performance regressions! 
Yes. That serves to indicate that the path is absolute. `C:windows\system.ini` and `C:\Windows\system.ini` are both valid paths and potentially refer to different things.
Ok! So how does `pkg-config` find all the `.pc` files?
Also note that this has been the behavior of every Windows API ever, and every CRT function ever, for longer than UTF-8 has existed. This "treat every char thing as UTF-8" thing is comparatively recent.
&gt;I could see Windows + co making it easier to set the "legacy" encoding to UTF-8 (that is, make CreateFileA accept utf-8 input) though. What are the impediments to doing this currently? &gt;Backcompat / ABI stability are technical reasons. Point taken.
Short answer: The committee decided that yes, the concepts TS does improve on C++17, and adopted concepts into draft C++20. This paper was input into the Toronto meeting (note the paper's date is one business day before the start of that meeting), and so while it's probably confusing that this paper appears in the post-meeting mailing, that's just because it was a late paper (not submitted in time for the pre-meeting mailing), so its presence in this mailing is just a timing artifact. Note the post-meeting mailing regularly contains papers that were not adopted, as well as ones that were.
So you are saying i should use imgui Will gtk do the same ?
I suggest you research the terms 'immediate mode GUI' and 'retained GUI'. imgui falls into the first category, Gtk into the second one.
Some explanation would be appreciated. Also, a hint in which subreddit such question should go. Thanks!
Was the paper discussed at all? I see it linked on the EWG page in the wiki but I don't see mentioned in the schedule at all.
Are there already attempts to measure the modules usage impact on performance in full build and incremental builds? Or is it an exercise left for users willing to walk on the bleeding edge? :D
Never worked with SFML, sorry. 
Our use case had to do with user-set paths from the GUI front end of the application (Qt based), and a lot of functions internally using std::string for path representation when dealing with file I/O. Using QString::toStdString() does encode to UTF-8 but on windows we would have to widen to UTF-16. Eventually we moved all the internal functions to filesystem::path, and rely on the VC++ specific overloads to "transparently" support windows' use of UTF-16. That places the burden to wherever the path object is constructed, but coming from QString this is more straightforward to handle 
Change the binary to separate runtime and compile-time... Impressive!
I will call you and the author of unconstexpr as c++ black magician
For whatever reason, the committee thought that `std::string` could mean utf-8 so `std::u8string` wasn't needed. Thinking of bytes instead of semantics. There are many things in the standard right now that are taking encodings into account, but `std::string` can still be used for anything. The fact that `path(std::string)` expects multibyte and `u8path(std::string)` expects utf-8 is rather mind-boggling to me. So when I see `path::u8string()` and `path::u16string()`, I assume that I can pass either to the ctor and that since there's no `std::u8string`, `std::string` must be it. This, coupled with the fact that posix _does_ expect utf-8 in the ctor, makes this a hell of a mess.
The biggest concern I've heard about Concepts is that "it makes using templates easier but doesn't do anything to help write them" and this article seems to reiterate that point. Is that something to could be addressed down the road? Or are there fundamental limitations of the current proposal that prevent that?
Personally, I feel like a lot of the pain in writing complex template libraries today will be better solved by something much more expressive like reflection and metaclasses, rather than trying to make further changes to templates. 
&gt; where does this rule come from ? `std::list::sort` vs. `std::sort` `std::unordered_map::find` vs. `std::find` `std::undordered_map::count` vs. `std::count` Et cetera.