Incomplete treatment of the subject. Key points he should have covered include: * Compiler specific workarounds (#pragmas) * Init function on startup to perform little details like `value = *pint;` * Gotchas regarding local statics in inlined functions * Gotchas when "eliminating" *all* globals when you're really just replaced them with pseudo-globals
I think the assumption was that assigning `n*m` to `size_t`, which is unsigned, would somehow force `n*m` expression to produce unsigned (and therefore overflow-proof) result. This could be done as follows: `size_t bytes = n;` `bytes *= m;` That said, this could still yield erroneous results, because the original tests don't cover all cases of overflow¹ and that's why checking if `SIZE_MAX/m &gt;= n` is correct and doing so before calculating `bytes` value is even better. ¹ for example, assuming 32-bit integers, `10 * 536870912` ( 10*2^29 ) equals `5368709120`, and, after applying modulo 2^32 (unsigned overflow) it becomes `1073741824`, which passes both `bytes&gt;n` and `bytes&gt;m` tests.
I think most of us would prefer that. Just of the two options I assume I own it. 
 "Compiler-specific" makes me nervous, I would much rather something portable. An init function certainly makes sense, and that's kind of the common-sense extension to getting rid of global variables. The 'gotchas' are something I'll consider putting in a revision.
I guess I should have expanded then. This pattern is used throughout the Gameplay engine. A class generally has the same static methods: static Widget* create(); static void initialize(); static void finalize(); The `create` method allocates the memory, while the `finalize` method releases it. The `initialize` method is used to initialize global state. This is similar to a singleton, with the exception that there is no instance object for managing global state. All state is kept as static variables in the source file. Furthermore, instances are often reference-counted, cleaning themselves up when they are no longer needed.
&gt; there is no instance object for managing global state. Seems to be contradicting: &gt; All state is kept as static variables in the source file. (I might be missing something though, so please explain more or link source code so I can understand). &gt; instances are often reference-counted How often? How can a client know that the instance is something that needs cleanup or not? If it needs cleanup, how does the client know when and where to clean it up? What if another thread needs the instance still? Who calls initialize and finalize? And when? For reference counting why not use a standard shared pointer? 
Since the buffer is automatically managed memory rather than manually managed, there’s really no danger of a leak. That said, I also don’t really see the point of this code.
* What does a static function offer over a free function? * Why return a raw pointer to manually managed memory? This is doubly bad: first off, [raw pointers should **never** own memory](http://klmr.me/slides/modern-cpp). Secondly, why return a (smart) pointer at all? Return a value, unless you need polymorphic object creation (which your code doesn’t accomplish). * What does the `create` function even accomplish? Why not use the constructor directly? Your code accomplishes the same as the following code, but is substantially worse: class Widget { public: Widget() : manager_{manager__} {} private: static WidgetManager manager__; WidgetManager&amp; manager_; }; 
This is what [The Meson Build System](https://sourceforge.net/projects/meson/) tries to do. (Full disclosure: I'm the main developer.)
&gt; How often? Some class are reference-counted, some aren't. Depends on the context. &gt; Who calls initialize and finalize? The `Game` class calls `initialize` and `finalize` on all singletons. &gt; How can a client know that the instance is something that needs cleanup or not? If it needs cleanup, how does the client know when and where to clean it up? The `finalize` method cleans up all references. &gt; What if another thread needs the instance still? For it to be thread-safe, you would need to add locking everywhere. &gt; For reference counting why not use a standard shared pointer? Because raw pointers are more versatile, shared pointers are not always supported and I don't need explicit garbage collection. &gt; (I might be missing something though, so please explain more or link source code so I can understand). What I mean with "no instance object" is that there isn't a static reference to a singleton class that holds all data. Singleton: // Header class Widget { public: Widget(WidgetManager&amp; manager, const std::string&amp; name) : _manager(manager) , _name(name) { manager.addWidget(this); } const std::string&amp; getName() const { return _name; } private: WidgetManager&amp; _manager; std::string _name; } class WidgetManager { public: static WidgetManager&amp; getSingleton(); void addWidget(Widget* widget); private: static WidgetManager* __instance; std::map&lt;std::string, Widget*&gt; _widgets; }; // Source WidgetManager* WidgetManager::__instance = nullptr; WidgetManager&amp; WidgetManager::getSingleton() { if (__instance == nullptr) { __instance = new WidgetManager(); } return *__instance; } void WidgetManager::addWidget(Widget* widget) { _widgets.insert(std::make_pair(widget-&gt;getName(), widget)); } Static members: // Header class Widget { public: static Widget* create(const std::string&amp; name); static void finalize(); private: Widget(const std::string&amp; name) : _name(name) { } private: std::string _name; }; // Source static std::map&lt;std::string, Widget*&gt; __widgets; Widget* Widget::create(const std::string&amp; name) { Widget* widget = new Widget(name); __widgets.insert(std::make_pair(widget-&gt;getName(), widget)); return widget; } void Widget::finalize() { for (std::map&lt;std::string, Widget*&gt;::iterator widget_it = __widgets.begin(); widget_it != __widgets.end(); ++widget_it) { delete widget_it-&gt;second; } __widgets.clear(); } Note the complete removal of the `WidgetManager` class, as a separate class is no longer needed to manage global state.
If the outcome is identical, why would my version be substantially worse? Your version has a `WidgetManager`, which has downsides of its own. * What is the scope of `WidgetManager`? It manages widgets, does not that mean it can create them? How about finding them by name, or grouping them by type? * Who cleans up the `WidgetManager`? * What about dependencies? If the `WidgetManager` depends on the `ImageManager`, the initialization order must be specified explicitly.
&gt; (not an error, but the code is not future-proof this way.) no. This really is undefined behaviour.
https://www.securecoding.cert.org/confluence/display/seccode/INT32-C.+Ensure+that+operations+on+signed+integers+do+not+result+in+overflow is a great reference for avoiding integer overflow.
Poor analysis, poor article. Let me summarize. The article claims the following: size_t bytes = n * m; //since this may overflow //... which is undefined behavior //any code that follows may not happen. Right. The compiler has no way, in general, to know that a calculation will overflow. Even if it could know, aborting the program is a very extreme interpretation of "undefined behavior". It requires more machine instructions, and serves no purpose. It ain't gonna happen. The compiler will simply emit machine instructions and assume the overflow won't happen. Which leaves us with the underlying hardware. I've never heard of a machine that throws an exception on integer overflow. Which leaves this article firmly in the land of the hypothetical. Programmers must be pragmatic. You should know why to avoid the first sample which may exhibit bad behavior. You should *not* make any effort to avoid things that will never hurt you.
I've been a contractor in UK for the last 9 years and, up until this year, local C or C++ jobs (at least in the North West) were pretty easy to pick up. My average time between jobs was about 10 days, with contracts typically lasting 18 months. My last contract finished a few months ago, and, for 3 months, there were literally no C++ contracts that were within commuting distance. Everything was web or mobile app development. Although it pains me to say so, I agree with @yCloser that C++ is now a legacy language. Yes, there will be companies that stick with C++ for whatever reason, but JVM based languages are now 'good enough' (and in many cases better and cheaper to develop) for 95% of projects. I believe C++ development will become a niche for the low memory/embedded market and will slowly go the way of COBOL and FORTRAN. 
I did so here: http://www.reddit.com/r/cpp/comments/1syw26/the_grand_c_error_explosion_competition/ce5n910 Note that these rules apply if and only if the stdlib is used anywhere in your program.
With ref-qualifier for the assignment operator, the error message is &gt; error: no viable overloaded '=' With your proposal, the error message says something like the rvalue-qualified assignment operator is explicitly deleted. Honestly, I'm not sure which is better. Code-wise, the first one is better, because that is the motivating example for this new feature in the language. The error message your solution shows is not very directly descriptive either, but might be slightly better. That said, I think clang needs to emit a better diagnostic in the original case.
Return-by-const-value idiom is obviously good for preventing this things, but it also broke the move semantics. One should not return by const value.
https://imgflip.com/i/76rln
Since raw pointers don't own memory, I usually assume someone else owns the memory, so I should not try to manage it. 
Can you ELI5 what the problem and proposed solution is? I believe you're trying to prevent the rvalue created by the addition of the two lvalues to be assigned to by the lvalue on the right side of the assignment operator?
Concerning the thread::~thead()-stuff I believe that the best solution would most often be something like this: class joining_thread: public std::thread { public: using thread::thread; ~joining_thread() { if(joinable()) { join(); } } }; Just don't try to delete one virtually;
&gt; Are there exceptions? I'll agree with you that `&amp;` qualified assignments should be the default, but there are certainly exceptions. Proxy reference objects, e.g., [`std::vector&lt;bool&gt;::reference`](http://en.cppreference.com/w/cpp/container/vector_bool/reference), are the first example that comes to mind of an rvalue that needs to be assignable. In general, any type that has a "special" assignment behavior other than simply altering the state of the object will likely be better served by non-reference-qualified assignment operators.
What does the &amp; at the end of the declaration do? I haven't seen anything about this in the list of C++11 changes.
Am I missing something? That behavior has never been allowed. int main() { int x, y, z; x + y = z; } The statement `x + y = z` can't compile because the expression `x + y` is an xvalue, not an lvalue. No need for funny new features.
This is in the context of wrapping a library. The library expects raw pointers, so that is what we need to pass it. I'm curious why he didn't just accept std::string parameters and allow implicit conversion from char *. It sounds like it could be a little less efficient, but I think this solution is premature optimization unless he can show some significant slowdowns.
&gt; Which leaves this article firmly in the land of the hypothetical. Section 2.3 of [Undeﬁned Behavior: What Happened to My Code?](http://pdos.csail.mit.edu/papers/ub:apsys12.pdf) has a nice discussion of an actual vulnerability in the Linux kernel caused by a failure to handle integer overflow properly exactly as discussed in this article.
Replace 'int' with any custom type and it will compile (assuming there is a suitable operator+). The c++03 solution was to make operator+ return a const value, but this is discouraged in c++11 because it inhibits move assignment.
I didn't know about it either. See [here](http://stackoverflow.com/questions/21052377/whats-a-use-case-for-overloading-member-functions-on-reference-qualifiers).
[Apple's secure coding guide](https://www.imperialviolet.org/2014/02/22/applebug.html)
And let's not forget that the *initialization order fiasco* is only half the story; the *destruction order fiasco* is much rarer certainly, but also much harder to solve -- unless you use a `setup`/`teardown` approach, which singularly lacks composability.
But then you're using the preprocessor. 8(
Why not call it by its name, a Singleton? 
40mb code base is a very strange limitation. But yay imma gonna go install this now.
&gt; Such a great programming language! I'm tempted to think that you're joking. I can't rightly understand how requiring developers to learn and be actively thinking about such subtle details all the time makes C++ "great". I would personally say that it is a powerful language with a high degree of accidental complexity. All the various features interact and overlap in non-obvious ways making well-behaved composition a challenge.
It's popular because it solves a shitty problem that everyone has: How do I make it so that people can use XCode, Visual Studio, or Makefiles while not wasting my own time maintaining redundant project files.
I like that it's "very similar to javascript" but I would be much happier if it simply *was* javascript. Anyone have any details regarding how much it shares with a real javascript interpreter? Are we dealing with another half baked DSL? I thought that idea died when ant came out and the world didn't explode when you had a real programming language available to your build scripts.
Function-local static... What about destruction order? (e.g. a static variable's destructor calls the function whose local static is already gone) Also, there's so-called Schwartz counter (not thread safe last time I looked though).
I think it is best progamming language. It is not a joke :). These small particles fit into a nice composition.
I will answer. Pls wait :)
What do you use it for, curiously? &gt; These small particles fit into a nice composition. How can you say this? This entire post is based on the challenge of maintaining the expected semantics for operator overloads as they are composed into larger expressions. The composition may ultimately be "nice" but it is certainly not easy or straightforward.
Ah yes, I have encountered that years ago. Thanks for the reminder.
What makes a programming language great for some people isn't necessarily ease of use.
That paper is even less informative than the article, and in no way implies that the articles example 2 is flawed in a practical sense.
The flaw is that the programmer thinks that this: size_t bytes = n * m; if (n &gt; 0 &amp;&amp; m &gt; 0 &amp;&amp; SIZE_MAX/n &gt;= m) { ... /* allocate "bytes" space */ } is protected against overflows. In reality, an optimizing compiler will see that `!(SIZE_MAX/n &gt;= m)` implies `SIZE_MAX &lt; n * m` which implies that the earlier multiplication of `n * m` must have overflowed. It then eliminates the "dead" predicate so we have: size_t bytes = n * m; if (n &gt; 0 &amp;&amp; m &gt; 0) { ... /* allocate "bytes" space */ } The programmer thinks he's allocated a buffer large enough for `m * n` bytes, but the actual allocation may be smaller. If an attacker can influence the choice of `m` and `n`, there's an easily exploited buffer overflow in the code the programmer believes is safe.
Templated types and operator overloading and such like are mainly for library implementors, not mainstream end use code. You only have to worry extensively about all these interactions and corner cases, etc. if you are building infrastructure (for the most part). As the standard library grows and improves, the average programmer deals with these types of issues less and less often.
I agree with that comment, but that doesn't justify the articles assertion that a (possible) integer overflow allows the program to simply disregard the rest of a function.
Average programmer still have to deal the issue described above.. Actually your answer is true, but it is not an answer for original question of unknowmat. Unknowmat, please think about this: in English you have to say 'her' if your subject is woman. In other words you have to notice her gender (add some abilities and disabilities), the same way we want to express that rvalues should not be assigned by any values. We only use a different languages. And I think (totally my personal opinion) we have several tools (lets say 'grammar rules') to express those kind of abilities in C++. That is why I think it is a great programming language :)
Downvotes! Please, explain! This community is very open to any ideas.
I'll grant you there are numerous subtle problems a new programmer can run into when getting into C++. However if you do take the effort to understand why things occur I think you'll get higher quality programmers. I use it because all of my data is in a format that needs to be read in with a certain library. We only have two choices: Use the native C++ version or interface it through Python. When you do the latter it slows down by approximately a factor of 50. Which means there's essentially no choice in the matter as the C++ version of code can still take ten hours to run, depending on the input data length. At various points, though, I do need to use Python. It's a struggle to determine exactly what type certain things are, since you only see it at the point of declaration which need not even be in the same file.
&gt; there are numerous subtle problems a new programmer can run into when getting into C++. This is an understatement. I find that it takes years of experience before one can achieve sufficient competence to be allowed near production code. A novice can unintentionally tread right into undefined behavior and introduce some hard-to-reproduce non-deterministic segfault, for example. At a minimum one must be well-versed in the various idioms and techniques covered in Meyers' and Sutter's books, and probably the Alexandrescu book. In my experience, few developers are this devoted. &gt; However if you do take the effort to understand why things occur I think you'll get higher quality programmers. This is true of any system or runtime. &gt; I use it because all of my data is in a format that needs to be read in with a certain library. There are lots of reasons to use it. I work on automotive infotainment systems. It's C or C++ on my platform.
Ease of use isn't particularly high on my list either. My favorite language is Haskell, for example. I do care about tractable correctness and composability. C++ can do both of these things, but only if you're really careful and know what you're doing.
&gt; Templated types and operator overloading and such like are mainly for library implementors Agreed - but this doesn't change the fact that they are still damnably tough to write correctly. &gt; As the standard library grows and improves, the average programmer deals with these types of issues less and less I can only hope this is true. C++ tends to have some fairly leaky abstractions.
The upvote/downvote counts are fuzzed to screw with spam bots. Don't sweat it.
I love Haskell too and HM type systems in general. I think Haskell makes an interesting tradeoff between making it extremely easy to reason about correctness, but rather hard to reason about performance and the execution model. C would probably be the opposite end of this spectrum.
You're right, of course. I don't want to give the impression that I'm using Haskell in production systems - just that it is the language that most directly lets me express the ideas in my head. I find that I've become fairly curmudgeonly. I'm a huge fan of declarative programming. I hate having to micromanage everything the stupid system is doing - let the computer manage itself. Here's a precise description of the result I want, just make it happen!
&gt; I find that it takes years of experience before one can achieve sufficient competence to be allowed near production code. Yes, it is probably true. But it is not a problem, because that is why our work is more expensive for a company :) &gt; A novice can unintentionally tread right into undefined behavior and introduce some hard-to-reproduce non-deterministic segfault, for example. This is also true LITERALLY and it not depends on the used programming language. See my answer above. &gt; I work on automotive infotainment systems. Me too. The training of new colleagues is expensive. Who cares? Is it your company? It is better to work good programmers in any language and it can be painful to work wit poor quality colleagues in any languages. However, these posts are very offtopic...
if std::forward_if_no_except moves if no copy constructor exists, doesn't that make std::vector's exception guarantee less strong?
Looks like debugging C++11 will be interesting indeed.
Indeed. I find it useful to this of it as applying to `*this`. In the same was as `const` on the end of the method means `T const * this`.
What were the arguments against N3721? Most of it's (IMO) really obvious stuff that should have been in C++11.
This thread has been linked to from elsewhere on reddit. - [/r/cpp17] [I See a Monad in your Future. \[X-Post: /r/cpp\]](http://np.reddit.com/r/cpp17/comments/1z1l6h/i_see_a_monad_in_your_future_xpost_rcpp/) *^I ^am ^a ^bot. ^Comments? ^Complaints? [^Send ^them ^to ^my ^inbox!](http://www.reddit.com/message/compose/?to=totes_meta_bot)* 
Thanks for the explanation. I had to re-read it a few times but it makes sense.
\&gt; Declare a 1000 char buffer \&gt; Doesn't free I'm^so^done^bye^*flys*
chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken chicken
What's wrong is that my virus checker rejects it, but I don't know why. Perhaps some malware infected a legitimate site.
The main argument was that future was not a good abstraction because it mixes encapsulated value with concurrency. The supposedly better abstraction would be an object with one method, "get," representing an encapsulated value. Not even a functor. Then there would be another abstraction that introduces concurrency and inherits from encapsulated value. This one would have the "then" method. Which, in my opinion, is totally upside down. An encapsulated value should at a minimum be a functor: it should have the "then" method (or "transform," or whatever you want to call it). It's the "get" that introduces concurrency because it can block. 
&gt; #include&lt;iostream.h&gt; &gt; include C++ string, but use C-arrays
I see a variable allocated on the stack, which doesn't need a manual free. In fact, if you try to free a stack-allocated variable, wait for bad things to happen. Care to elaborate?
I concur with twoodfin's earlier reply. This is only an issue if you have cycles in your statics somehow. If that's the case, then you have other issues you're going to run into that don't have to do with construction/destruction. http://www.reddit.com/r/cpp/comments/1yz85c/ditch_your_globals_whats_this_do/cfpk1a1
My will to learn Haskell just increased.
The article never asserts that the compiler may disregard the rest of the function in all cases. What's incorrect is your statement: &gt; The compiler will simply emit machine instructions and assume the overflow won't happen. &gt; Which leaves us with the underlying hardware. I've never heard of a machine that throws an exception on integer overflow. There is no guarantee that the actual executable will behave as though the compiler simply translated the program to naïve assembly and thus you get the hardware's behavior for overflow. http://blog.regehr.org/archives/213 In fact there are real optimizations performed by compilers that produce apparently bizarre behavior when programs exhibit undefined behavior, such as producing an infinite loop when the source 'clearly' cannot loop forever (and when the naïve translation to hardware instructions would not produce an infinite loop). &gt; That paper is even less informative than the article, and in no way implies that the articles example 2 is flawed in a practical sense. If the generated program's version of 'undefined behavior' is that it works as desired, this is what's been called a 'time bomb': At any point in the future a new version of the compiler can introduce a new optimization that 'breaks' the program. http://pdos.csail.mit.edu/~xi/papers/stack-sosp13.pdf You never know what some future compiler is going to be able to prove based on undefined behavior, and you can get unexpected results.
I really hope resumable function aka C# async/await gets into the language. also, monad + continuation = head explode.
You should. It will at the very least make you think about how to design things.
No. Nobody will do this 'stupid things'. And in C++11 rhs should be passed by value.
Thanks!
Do check out [Learn You a Haskell](http://learnyouahaskell.com/). It's freely available in full directly on that site. If you enjoyed the submitted Bartosz Milewski post, you could even skip directly to chapters 11-13. Together, they actually do parts of what is done in the submitted post by Bartosz Milewski, expect in the reverse order. It slowly, in great detail (perhaps even long-winded) motivates monads by first introducing functors, then applicative functors, and finally monoids before properly introducing monads. They would be syntax heavy for those unused to it, but it's not so totally insane that you shouldn't be able to see what is going on.
&gt; Nobody will do this 'stupid things'. They do stupid thing everyday. https://www.imperialviolet.org/2014/02/22/applebug.html &gt; And in C++11 rhs should be passed by value. This is true in most of cases. Let me update the original post with your comment. 
Every time I click a link to this guys website I immediately think of that guy that said "that word, i don't think it means what you think it means".
Why wait? The future is already [here](https://github.com/beark/ftl). Well, sort of. The above library isn't really ready for serious use yet, but it does have re-usable abstractions for monads, functors, applicatives, monoids, and a bunch of other stuff. It even includes relevant implementations of these for futures (and other standard library types).
Completely agree. The main issue I'm having with async/await proposal is that it relies on threads, instead of being an abstraction of any type of asynchronous computation. (p.s. Maybe they updated that bit after the last time I checked)
@DrBartosz I guess you'd be able to bring life to the following discussion: "Toward a Monads proposal - providing generic make_ and .then() functions" https://groups.google.com/a/isocpp.org/forum/?hl=en-US#!topic/std-proposals/KCqwEq49GMA 
That's old style, it just refuses to go away.
Because if all you need a 5 line makefile, then your project is trivial and it doesn't matter what you use. When you get to the level of something like LLVM, Trilinos, the ITK, or any of the real systems that CMake is designed to build, it's immediately apparent that you need a build system that has much more power to control and describe the dependencies of its various pieces. It's also got to work on Mac, Windows, and Linux. CMake solves a general problem, and solves it very well. As such, when applied to small problems it can feel like overkill. Especially if you're forced to use older style listfiles or a badly implemented system. And, to be fair, in only 5 lines, I can make a library and executable that links against it: cmake_minimum_required(VERSION 2.8.12) project(some-project) add_library(foo foo.h foo.c) add_executable(bar main.c) target_link_libraries(bar PRIVATE foo)
I prefer Real World Haskell. LYAH's constantly excited style got on my nerves pretty quickly.
Preach it brotha. Technically you don't even need "project()" there, as it'll default to "Project" if it's not set.
Holy crap this would be amazing. I love CMake but it's syntax is fugly, for example a while loop that requires arithmetic makes me cry. But Lua + CMake portability would be heaven. I hope this gets implemented at some point. Edit: that wiki was last edited in 2008. Hope shattered :(
Interesting! /r/gamedev might like this.
We're able to build our software on Windows and Linux, and create deployment packages for Android and some embedded systems with CMake. Yes, the syntax sucks, but once you have that under control, the power that CMake gives you is pretty nice.
http://isocpp.org/std/submit-a-proposal
That's perfect, thanks!
I would tend to agree; but if you have a Singleton Logger...
There's no effort to hide anything. The simple reality is that (a) ranges offer a lot of mileage without needing to avail themselves of a position notion, and (b) fewer concepts are better than more concepts. The "what if I want to call find() then use the left subrange" is invariably trotted out when comparing C++ iterators with D ranges, and is equally invariably a winner - except for the fact a putative D user just uses until() and calls it a day. I've said it and I repeat: there are things you can do with iterators/positions that you can't with ranges. The only question is whether those things are important and enable idioms that would be otherwise inaccessible. A large body of evidence suggests that ranges do plenty fine without position. So we didn't add position. In fact there are other primitives that would be more interesting to add, such as constructing the difference range (the left part of find() given the full range and the right part), or telling whether two ranges start in the same position. Those would be, I think, more interesting axiomatically to define than full-bore positions. 
Even if efficient, it does mean runtime errors instead of compile-time errors; aka dynamic programming in C++.
&gt; Now, all iterable things model FiniteIterable by default, and a type has to opt in to being infinite. I'll take exception at that. Since infinite is dangerous and finite is safe, I'd rather opt in to being finite: **safety by default**. It might seem a bit harsh, although SFINAE detection of an inner `is_finite` type (evaluating to `std::integral_constant&lt;bool, /*...*/&gt;` which defaults to `false` if absent) would make it more palatable that specialization. I would also argue that there are much more users of iterators than creators, so we should optimize for users. Nice serie, really :)
&gt; I'll take exception at that. Since infinite is dangerous and finite is safe, I'd rather opt in to being finite I can respect that. Mine is an engineering choice based on practicalities. The vast majority of ranges in the wild today are finite (e.g. all the std containers). Having to opt in to *all* of them so you you can, say, sort a `std::vector` would be a sure way to piss my users off. In a redesigned STL, those containers might declare themselves finite, which wouldn't eliminate the problem but make it less awful. &gt; Nice serie, really :) Thanks!
Indeed. But that's precisely the idea behind the library. You say it as if it's a downside.
Be sure to [hop on over to the std-proposals mailing list archive](https://groups.google.com/a/isocpp.org/forum/?fromgroups=&amp;pli=1#!forum/std-proposals) and search for "tuple". I vaguely recall a couple of past discussions about tuple unpacking/pattern matching.
While I generally agree with the sentiment in the post, there are at least a couple scenarios where I think a garbage collector can be useful. 1. Embedded languages where resources other than memory are not used. For simple scripting languages where productivity is the most important thing, having a GC is important 2. Functional languages don't have a natural way of describing RAII in the way C++ can (no objects to speak of)
Refcounting and garbage collection are not the same. GC is typically understood to be *nondeterministic*. shared_ptr refcounting is *deterministic*.
Refcounting is a kind of garbage collection.
Garbage collection is not a synonym for automatic memory management. It specifically refers to having an out-of-band thing which finds garbage in some way and frees it. You could theoretically have a garbage collector which uses reference counting to decide when things are garbage, but no one does that because it'd be terrible.
&gt; In modern C++, using new or delete in your code is wrong . . . We use the smart pointer wrapper classes, such as std::unique_ptr, std::shared_ptr, std::weak_ptr, etc. Evidently, the author assumes that, whatever your pointer needs, someone has written a smart pointer class to take care of it. That's simply not true.
I disagree with you on #1 because most scripting languages will bloat out to include resources. For example Javascript now has a File API. Resources are essential to programming.
Garbage collection is a rare case when the program itself knows how scarce the resource is. Often it doesn't. For example, with connections to remote systems, you might have a rough idea of the resources associated with your connection (temp tables associated with a database connection, for example) but the external system won't send you a message asking you to please GC your connections because it is running out of resources. If you lock a file on the local filesystem, you don't know when other processes are attempting to access the file. In such cases, the best you can do is give up resources in a prompt and predictable way. C++ programmers are lucky to have a single language-supported and community-accepted method for doing so. 
Thanks. I just posted there, too. 
well, it's easier to fix problem at compile-time rather than runtime-time if it occurs randomly.
Refcounting is one of the basic ways to do GC. It does not mean that every refcounting is GC. edit: a close analogy - those books talk about GC being something that frees up "inaccessible" memory. That does not mean that freeing up space taken by a variable going out of scope (becoming "inaccessible") is GC.
I would argue that using a GC is awesome, as long as you leverage its properties correctly. Of course the GC can't do every last job RAII can accomplish: it was never meant to. The one thing it doesn't do is "deterministic finalization", which is the mistake cited in the article, regarding mis-handling file handles. The GC is not a omnipotent event handling mechanism for disused memory. Rather, it's a lazy memory deallocation strategy that borrows against the future's RAM budget, which the program pays off over time. And with that definition, you'd be nuts to use it for anything else. Also, consider that RAII is really just a way to bind a callback to the "end of scope" event. Again, this is completely orthogonal to what the GC is for. They intersect only where you choose use RAII to free memory. If you're not freeing memory, you're probably freeing other resources of some kind, and RAII is probably the right approach. Ultimately, the right answer it to use both, where applicable. The GC allows for lazy freeing, which can accelerate certain tasks immensely. RAII can get you out of situations where resources can escape due to exception handling, or logical mistakes. Together, they let you build bulletproof software: leak free on all sides. 
&gt; Functional languages don't have a natural way of describing RAII in the way C++ can (no objects to speak of) This has been done in research languages (e.g., [Tofte and Talpin](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=479538CE7C72BF363EC4D977B060ADF0?doi=10.1.1.51.6564&amp;rep=rep1&amp;type=pdf) 20+ years ago). The real issue functional programming has here is that it is common for a function to return another function. This means a closure is created in that called function's scope, but the closure is only used after it has gone out of scope.
I mentioned Vincente's future proposal in the blog (the "next" method, etc.). But I'm not sure if C++ is ready for a bigger injection of functional programming. I'ts already bursting at the seams. 
This is true. I was about to rebut you but then I realized that you were arguing about their deterministic/non-deterministic properties. This is absolutely the case.
In short: The proposed extension to std::future makes it a monad.
You can't pick a single instance and then use that to draw a sweeping conclusion. Lots of game programmers use Lua or Javascript to script simple game behavior. There's a reason they don't use C for that. Also, these scripting languages need to be usable by modders, game designers, and the like.
Why not use variadics?
I'm usually more pessimistic, but I kinda got hopeful that the future can be bright when I had a few opportunities to actually use functional concepts in real C++ projects. Lets hope the 'seams' will hold up a bit more. :)
LYAH worked well for me providing a comprehensive intro and have recommended it twice already this year.
Here's the link for the lazy* ;) http://book.realworldhaskell.org/read/ 
Thank you! The article is on my reading list, just don't have the time right now. :)
&gt; That's right, there's a cost. Multiple costs. For one, now code has to be analyzed at run time to determine what memory is still being used and what memory can be released just a question. Couldn't a compiler theoretically insert statements to allocate and deallocate memory as needed?
Allocating memory on demand is easy, deallocating it is hard. You can know when a pointer goes out of scope, but only at runtime you would be able to figure out all the other things pointing to that memory. The only options we have to work with are: * The programmer knows that because of the way she implemented other things, nothing else is pointing to it. * The programmer is using some managed pointer class that spends some processing power keeping track of this (reference counting). Most of the simpler solutions to this can't deal with circular references (what if you had two objects that pointed to each other and not by anything else? Each one only knows it is being pointed to by something else, and not about the cycle). * The runtime environment spends processing power keeping track of this. It has better program knowledge than a managed pointer, but this takes even more processing power. You also have to either "stop the world" (stop program execution) to follow all pointers/references to see where they end, or brave the hell of complex asynchronous programming, which is something the best programmers struggle with. edit: formatting
There's also this (assuming 64-bit): T* myPtr = new T(); std::uint64_t p1 = ((void*)myPtr)&amp;0xFFFFFFFF00000000; std::uint64_t p2 = ((void*)myPtr)&amp;0x00000000FFFFFFFF; myPtr = 0; If we had a GC run on pointer assignment to null, or even if we secretly implemented all raw pointers allocated via new as a shared_ptr and did reference counting on them then it may well declare that the T object formerly pointed to by myPtr is inaccessible. But if I then say myPtr = (T*)(p1|p2); I've recovered it. Why on Earth you'd want to do this is beyond me, it's just another example to show the inaccessibility of an object is non-trivial to determine.
I think the name garbage collection itself gives a biased connotation. Garbage collection, a basic sanitary service, a foundation for any modern society, of course modern programmers use garbage collection. I think it should be called dynamic memory management, and RAII should be called static memory management, similar to dynamic typing and static typing. One handles memory management dynamically at run time, simplifying the language, and the other handles memory management at compile time, necessitating a more complicated language. I think these are in some ways similar language trade-offs for dynamic and static typing. Which of these has a higher mental burden to the programmer is, as with static vs dynamic typing, very debatable as well. While the programmer does less work and cannot have a true memory leak in a dynamic memory management (garbage collected) environment, IMO it is still easy to write code which uses too much memory or uses memory for longer than it should. It is also harder to reason about the memory usage of an unfamiliar complex code base. Haskell struggles with this more than, say Java, but the problem can occur in both. In contrast, static memory management (RAII) requires the programmer to be more explicit about how memory is allocated and used (unique, shared, pools, etc), but this thinking tends to result in tighter memory management, and is easier to understand down the road. Again, similar arguments are made for and against static and dynamic typing.
Garbage collection requires that a "garbage collector" runs which does the cleanup. That implies that an agent is tasked with picking up the garbage, compared with the typical c++ "last one to use it throws it away". This thread would be more fun in /r/programming but GC is a religion with a lot of people. I'd rather be coding than worrying about something else I have little control over continually stalling my threads.
How? 
The article touched an interesting topic: Integers in C++ suck and we don't have library solutions to that in the standard. We could really use big_int and safe_int. I would also love to throw away the current mess with the arithmetic conversions, but I guess the backwards-compatability-folks would fight to the death for the insane system that we currently have. 
Hey this is WAY overdue at this point but I think I'll mention that what you've been thinking of is the default rounding behaviour when casting a floating point to an integer and not the behaviour of 'round()'
[There was one that I could find](https://groups.google.com/a/isocpp.org/d/msg/std-proposals/VROimyg83fc/bPNWxu7Dl9MJ), but it seemed to peter out with the proposer choosing to stick with tuples and `std::tie`.
No scoped resolution rules dictate that Foo::Type would search for a static member variable contained in Foo.
Haskell doesn't really have that problem so much as it has code (and literature) that is terse but extremely abstract. (I'm not saying it's not possible to write simple Haskell--like C++ it's up to the person using the language.)
Your example works perfectly fine without `typename`, and the addition of parentheses after `main`. 
There is no such thing as "Monads in Haskell." The concept of a monad is independent of any programming language and indeed, is so fundamental to be applicable to every programming language.
The parentheses after the main was a typo not present in the actual code. What compiler are you using?
gcc {4.8, 4.9}, clang {3.1, 3.4}. Why would it possibly consider `Foo::Type` to be a static variable? It has the complete definition of the class, and it can see that `Type` names a type not a member. The only time it's not able to do that is when the class is a template, because then `Type` might vary between being a member or a type based on the template parameters, which is why you have to resolve the ambiguity with `typename`. That's not the case for a non-template class, because there is no ambiguity. 
i think when cast that way, it only floors them (or ceils them if negative), isnt that how does it?
No, but its easy enough to wrap that object with an RAII wrapper like around FILE *. With GC, you dont have this choice. With a decent language you can at least write a closure that handles in scope freeing of resources, but for things like this game object has these resources, its back to the dark ages of easy to forget freeing code all over the place. 
Rounds to the nearest number. Rounds halves (.5) to nearest even.
Been telling people this for a while. GC gets you an ok way to handle one type of resource, which then locks you out of RAII. Smart pointers give you the ease of GC, but you can also build in nice handling of all other resources. And you never get GC pauses as a bonus. Luckily closures fix the single scope resource problem, but now C++ has those too so yay. In some GC designs what it stores are handles to pointers, which means every apparent memory access is actually 2 memory accesses rather than 1. 
yeah, that's exactly the opposite of what i wanted. and i just rechecked and it doesnt. it floors positives, and ceils negatives. (int)-3.6f ==&gt; 3 (int)-3.5f ==&gt; 3 (int)-3.4f ==&gt; 3 (int)3.4f ==&gt; 3 (int)3.5f ==&gt; 3 (int)3.6f ==&gt; 3 (int)-2.6f ==&gt; 2 (int)-2.5f ==&gt; 2 (int)-2.4f ==&gt; 2 (int)2.4f ==&gt; 2 (int)2.5f ==&gt; 2 (int)2.6f ==&gt; 2 
I've been watching this develop on the mailing list; it is definitely exciting. I am very glad you provide visualizers as well!
FYI some places in the docs (common problems) use the misspelling erros
I dabbled with a proof of concept arithmetic fixed size integer class, and I think with the C++11 default template argument changes and explicit conversion operators it should be possible to write an integral type that does away with unsafe implicit conversions.
Nicely done. You might like this then: http://www.drdobbs.com/cpp/increasing-compiler-speed-by-over-75/240158941 It's a good article, that talks about a few different conventional strategies. Then Walter figured out he could just do this: void operator delete(void *p) { } If you're okay with letting the OS clean up after you, this is the *ultimate* lazy-free strategy. For the D compiler, this bought him a 75% increase in speed.
No, whatever your dynamic allocation needs, there is a smart pointer class for it. Pointers in general are still useful (until we get std::optional).
The claim is only true if you stick to RAII and move semantics. I can't think of any way to accidentally return a pointer/reference to a local variable with RAII and move semantics.
Exactly. When I see people say "GC overhead", I'm confused when they omit "manual memory management overhead". Unless you've written both before, you really have no standing in such matters.
I like having some additional functionality with my keyboard events, so I wrote a button auxiliary class. But the general method would be: Create a boolean (hasJump or whatever) Whenever you want to jump, make sure it is true prior to takeoff, then set it to false When the jump key is released (and maybe after the player touches the ground again) set the variable to false
A similar technique is sometimes used to keep one pointer instead of two in doubly linked list nodes. You'd keep `auto ptr = next ^ prev;` and then, upon traversal, you'd extract the next node pointer by xoring it with address of the node you came from. I don't think this is used much, but hey, a kind of legitimate use.
I am fully confident in saying that there are mistakes in my article, and I appreciate you pointing them out to me. In particular, spending 5+ years with C++ has left me very disconnected from the latest and greatest in garbage collection.
They are, in fact, duals of each other. There's a fascinating 2004 article by Bacon, Cheng and Rajan from IBM Research about it entitled "A Unified Theory of Garbage Collection".
Thank you for your post, I've no idea what I'm doing wrong, but he keeps jumping, how do you mean prior to take off? In the take off action or like before, outside the loop?
There remains no standard class for "borrowed" or "observer" pointers (although there is at least one proposal). IMO that remains the last reasonable use for raw pointers.
Every ICE (Internal Compiler Error) is a compiler bug by definition. The compiler should never ICE - it should accept valid code and reject invalid code with proper error messages. Please file a bug through [Microsoft Connect](http://connect.microsoft.com/VisualStudio).
&gt; No, whatever your dynamic allocation needs, there is a smart pointer class for it. In the last month I've written code for the guts of a special kind of binary tree. I needed some functionalities that I couldn't find in anybody else's tree code, so I cranked it out from scratch. To make it work, I had to load the tree with pointers, all of them forming loops, large and small. I suspect in theory I could use shared_ptr, but with all the pointer loops I would have a nightmare figuring out where and when to get a weak_ptr to break a loop when something got deleted. Also, sometimes a node of the tree needs to be temporarily disconnected while moving from one place to another, but I don't want it to be automatically deleted. And some memory has to be allocated without constructor, so the pointers there are sometimes meaningful and sometimes garbage. On the other hand, direct manipulation of naked pointers is simple and clear in this case. And everything will be sealed up in private classes so the ultimate user will never see the pointers. So what smart pointer class do you think I should use?
Perhaps this pseudocode may help. bool bSpace = isKeyPressed(Keyboard::Space); if (!bSpace) { bJumpKeyDown = false; } if (bSpace &amp;&amp; !bJumpKeyDown) { bJumpKeyDown = true; bJumping = true; }
&gt; For one thing, it would finally allow iterating over two containers simultaneously without resorting to indices You can do this now also using pairs or lambdas, can't you? Is `auto (a,b) : zip(as, bs)` really so different from `auto p : zip( as, bs )` and later using `p.first` and `p.second`? Or do I misunderstand what you want?
Usually when I get ICEs, I set /errorReport:send, and recompile 3 or 4 times. I hope that does what it says it should... I've encountered similar errors like this one just a few weeks ago.
That submits a "Watson" crash dump, which are difficult to investigate. The top crashes do get investigated, but not rare ones. In contrast, if you file a bug through Connect - all you need is a self-contained source file and the command line used to compile it - Connect's front-line responders will verify that it indeed reproduces, and they'll send it to actual compiler devs who can figure out the underlying problem quite easily. The bug still might not be fixed (ICEs on weird and especially invalid code are lower priority than silent bad codegen, for example), but the chances are much much better.
One other argument I have against garbage collected languages is that it adds just more complexity for someone wanting to just write another compiler. Will something like TCC ever exist for 'D'? The amount of engineering effort required to create a reasonable garbage collector is itself fairly daunting. And yes I realize c++'s context sensitive grammar makes it near impossible for someone to write their own compiler on a whim. Whatever happened to SPECS (c++ resyntaxed to be context free)?
Even Standards Have Bugs(TM). This is what the Library Working Group spends maybe half of its time on. Stuff like [LWG 2360](http://cplusplus.github.io/LWG/lwg-defects.html#2360) "`reverse_iterator::operator*()` is unimplementable" where the Standard contained complete nonsense is important to fix, and micro-features are occasionally added through issue resolutions, like [LWG 2112](http://cplusplus.github.io/LWG/lwg-defects.html#2112)'s `is_final` and [LWG 2247](http://cplusplus.github.io/LWG/lwg-defects.html#2247)'s `is_null_pointer`.
Thanks for clarifying that! I was hoping that it would do something a bit more like a sophisticated then just a Watson dump... But after thinking about it, there is only so much info you can actually help yourselves to, given privacy reasons and all.
I've never understood it, but addressing the readers as dim-witted children is popular in a certain kind of programming language tutorial. See e.g. this, "The Little Schemer," the Lisp-with-aliens thing this copies, etc. 
&gt; Garbage collection, a basic sanitary service, a foundation for any modern society, of course modern programmers use garbage collection. I really did laugh out loud at that one. You're quite right, the semantics really do bias you!
This is surprisingly maturely written, I tend not to believe that the author really is 18 😉 Having said that... GC not being adequate for other resources is painfully true, anyone with half a brain can see that. I cringe when I think of C# and IDisposable implementation (in fact, I tend to say, oh screw it, just wrote that part in C++/CLI) RAII is great, and its generalized forms (ScopeGuard etc) make it positively *sweet*. But... but, **a lot** of legacy and other code isn't exception-safe, uses naked pointers and has unclear/complex/wrong ownership rules, **a lot** of people in our craft are struggling with C++ features that working with all that effectively, and... finally, the most manipulated resource (in line-count terms) is memory. So in the real world, GC does help a lot of people a great deal. It's a "don't make me think" thing really. Yes, as every abstraction, it ultimately leaks (pun intended), and when it does, it's worse than with C or C++ (IMO). On the other hand, complaints about GC pauses and nondeterministic behaviour WRT timing are pretty much false. Once you use heap a lot in C++, and you delete a large object graph, releasing resources and whatnot, you're pretty screwed wrt deterministic behaviour. As a consequence, you need, in both cases (GC or not), to organize yourself. And finally, I would suspect that the total time spent on heap operations is smaller, for the same total number of heap operations, with a GC, simply because done in bulk and because of the intimate knowledge of memory that a GC might possess.
You do realize that your tone and actual words in your original post are much worse than e.g. being called a leisure programmer? You seem to be butt-hurt for being shown that it's not that simple from the off. That's unwarranted.
You and the author need a mud-fight: **Memory os a resource just like a file handle** - slap! **No it isn't** -slap! 😉
&gt; I love your insinuation, knowing nothing about me, that I'm a "leisure programmer" because I recognize GC as the good thing that it is. I didn't insinuate anything of the sort. Note how you didn't answer the technical question, and instead chose to reply with this nonsense, to double down on the nonsense "insinuating" the author of the linked blog had "never done extensive work in a GC'd language" and was writing a "propaganda [article]". I like GC when I just fiddle around with algorithms on small data sets or test ideas, because I can be lazy there. I work on simulations transforming and generating gigabytes of data, and there I can't afford this. Hence my comment about my job.
&gt; interestingly enough, I seem never to use shared_ptr without a central weak_ptr keeping track of it Surely it's vice-versa? (That's what I do 😉) 
Dude... you basically couldn't formalize your ownership policy on tree elements and thought you couldn't use something standard. I say, without details, you just failed. Your question is disingenuous, because you ask it, but give nowhere near enough context to be answered. 
&gt; I think it should be called dynamic memory management, and RAII should be called static memory management Together with virtual destructors and class hierarchies RAII can be very dynamic. So how about deterministic vs. non-deterministic memory management? With GC you basically give up control over when memory is freed and give up the chance to use that to deterministically free other resources. On the other hand you gain the ability to have cycles in your object graph without having to think about this special case in terms of destruction. It's a pretty simple trade-off.
So basically not handle resource lifetime in that code at all. If that's the case, let's not handle memory at all, too! Just pass heap to be used in sorting part and destroy it when done! You need a better argument there, methinks.
Refcounting is deterministic... until I copy my pointer around enough to know where it's freed 😉
That XOR is magic if you ask me, the more I code the more I find it useful for the strangest od things... But even this basic steganographic property blows my mind despite how simple it is.
As a PL person who does not use C++ regularly I have to admit I found n3858 rather confusing. It seems to make a simple thing complicated. Why not just have `do` notation? You get use "await" or something like it as the monadic bind operator (where the continuation is just the continuation). Wouldn't this do exactly the right thing? You could have Foo foo(bar x) resumable&lt;T&gt; { code with out await; x = g(await f(y)); more code } would desugar to Foo foo(bar x) { code with out await; return T::bind(f(y),[capture strategy](someType z){ x = g(z); [[more code]] } the "capture strategy" could be to use move semantics when ever possible. The type system would then disallow you from doing terrible things like combining the list monad with unique_ptr. The code generated would not be bad at all. Since everything is resolved statically, and you only use the monadic structure when you actually say `await`, all the code is going to be pretty strait ahead and efficient. You do not need a "get" method, although a call to `return` would have to invoke `T::unit` (`return` has to call `return`!). Overloading + templates would mean you could get better type safety compared to the "computational workflows" in F# or .net LINQ. Since you say explicity what monad instance to use you would get indexed monads and restricted monad for free (better than haskell). Since the syntax would be built in compilers could give nice error messages "can not make resumable function using type `Blah` in `Foo foo(bar x) resumable&lt;Blah&gt;` since `Blah` does not define a function `bind`" Instead of explict dictionaries, you could desugar C[await f(x)] (where C is the context) into return f(x).then([convention](someType y){C[y]} and avoid all the complexity in n3858 about reified stack frames.
Interesting question! No, that's the way I end up doing it. In the example I mentioned above, I have a central table of weak_ptr to my classes (they represent audio IO handles, to be specific). When I'm asked for a new handle, I go and look if it's in the table. If it is, I return a new shared_ptr to it; if it isn't, or if it is in the table but the weak_ptr is now null, I create a new shared_ptr, put a weak_ptr into the table, and then return the shared_ptr. No one except "me" ever sees those weak_ptrs. The only things I ever hand out are shared_ptrs. I can't remember doing it any other way... So tell me - how are you doing it? I'm actually not too sure how other people use this class, this has just worked well for me.
shared_ptr is nearly always overkill - but you are talking about "observer" pointers, which to me mean the same thing as weak references - perhaps we have different terms? If you're simply wanting to delete something else other than the pointer you're representing when this variable goes out of scope, you can do this easily in unique_ptr with a custom deleter, or even more easily with shared_ptr by using aliases...
He's assuming you're a "leisure programmer" because you made multiple factual mistakes in a short post _and_ were rude and used emotionally-laden terms. `System.gc()` (note correct capitalization) doesn't by any means explicitly release memory. It is a suggestion to the garbage collector that it might consider now releasing _some_ of the memory that it has detected is collectable. It's impossible to say what its effects are going to be - whether all the unreferenced objects will be released, or none of them, or some subset. It certainly doesn't do the job of explicitly freeing a single object! Indeed, calling `System.gc()` is ["a pretty good indicator of fundamentally broken code"](http://stackoverflow.com/questions/2414105/why-is-it-a-bad-practice-to-call-system-gc). Don't use it - and particularly, don't use it as an argument as to why garbage collection is better than non-garbage collected systems.
Yes, they are equivalent to weak references, though not weak_ptr. Regardless, there exists no "smart pointer" class to represent them in the standard library, although there has been a proposal to add one.
GC.Collect() does explicitly FORCE a GC. Source: http://msdn.microsoft.com/en-us/library/xe0c2357(v=vs.110).aspx system.gc(): "When control returns from the method call, the Java Virtual Machine has made a best effort to reclaim space from all discarded objects." Source: http://www.developandconquer.com/2013/06/forcing-java-virtual-machine-to-run.html You guys are all missing the larger point though, which is if you're using either of these, you're already doing it wrong. In the vast majority of programming problems, on modern systems, it makes no difference when exactly a resource is released. For the minority of problems where resources need to be released in a timely manner, you can use C#'s using() or Dispose() or even unsafe. Similar constructs exist in Java and worst-case there's always JNI. The implication of the article is that you're only a "real programmer" if you use RAII ALL THE TIME NO MATTER WHAT. I'd argue that true PROFESSIONAL programmers understand the value of their time and that for most problems, explicitly concentrating on resource allocation isn't the most effective use of that time. The title of the article - "Garbage Collection Is Wrong" is short-sighted. That, in particular, is not "that simple from the off," as you say. An article that insinuates RAII is *always* right or that, more generally, "xxx is always xxx," is "neckbeard" in the sense that it illustrates a combination of ego and limited exposure to the topic at hand - something written by either an amateur programmer or one who's stopped trying to learn new things under the belief that this one thing is the best tool for all jobs.
&gt; The presence of this restriction makes working with type traits tricky … &gt; When the code gets more complicated, … I took the above to mean that OP's example was much more involved, and generic, where different instantiations could result in the type being dependent or not. In such a case, being always allowed to write typename is A Good Thing™. FWIW, I'm glad the restriction's gone, too. Deep inside a (generic) function, I don't want to have to care whether the typename is dependent or not.
&gt; And you never get GC pauses as a bonus. You can get those with RAII too. Consider the case where you have a single reference to a large, complex object graph. When that last reference goes out of scope, every object in the entire graph will have its destructor called and its memory deallocated.
&gt;System.gc() (note correct capitalization) So long as we're being completely pedantic, it's actually Java.lang.System.gc(). &gt; don't use it as an argument as to why garbage collection is better than non-garbage collected systems I did no such thing, I simply pointed out a bias in the article. And FWIW, GC.Collect() in C# does explicitly force a GC. 
Thanks. :)
I use shared pointers for things like inter-thread communication in thread pools and for dependency injection. Other than that, I struggle to find situations where sharing is needed.
&gt; I like RAII in C++, even though I don't use exceptions there. RAII is mostly incompatible with exceptions because you can't throw from a destructor. (In C++03 it was a "no-no", in C++11 it'll crash the program becauses destructors are implicitly nothrow.) So say you have a file/socket/database handle, and the most natural place to "close" it is, of course, the destructor. But if you want to be notified of failure [e.g., close failed =&gt; data not flushed to disk, remote host, etc. so you can retry], you can't return a value from the destructor. So you have to manually call the close method, and thus you lose the benefit of RAII. Alternatively, your "handle" object could hold a reference to a longer-lived "status" object (promise/future?) which the destructor could use to return a "value". But none of the stdlib classes are designed that way (iostreams, locks, etc.) &gt; One of the biggest reasons is that code reuse is massively easier in GC languages. Despite trying very very hard, the C and C++ communities never were very successful at widescale code reuse. I've never thought about it that way, but you have a very good point there. This is a big problem when shipping binary-only libraries. When the library returns a raw pointer, you have no idea about _how_ to free it (delete? free? munmap/VirtualFree? something else?). STL is rarely binary compatible across compiler releases, so you can't have it in a public API. The only safe thing to base a binary release of C++ code on are pointers to abstract classes, so everything is virtually dispatched. (Which is also the idea behind COM, yay! :))
&gt; So what's wrong here? Well, we've come back to square one: you still have to be educated to use these forms instead of the more direct forms. This is a no argument. You have to be educated about the best practices of the language you're using, period. In Java it's try with resources, in Python it's with, in C++ it's RAII. &gt; The only added benefit is that the resource is released no matter what path the code takes (exception, early return, break from a loop, etc). A very important detail is omitted. "Closing" a resource may fail (e.g., disk full) and you may be interested in the failure so that you can retry the operation. In Java, if the implicit close() in try-with-resources throws, you have the chance (indeed, must!) catch it. In C++11, destructors can't throw. (They're implicitly nothrow, so throwing will terminate the program immediately.) So how are you supposed to handle failures? Well, two choices: 1. Manually call the "close" method, but then you lose all benefits of RAII. (It's easy to skip over with early return of any kind.) 2. Hold a reference to a longer-lived "status" object which can be set from the destructor. But no RAII stdlib classes (streams, locks, etc.) are designed that way. In C++03, destructors may throw, but it's considered a bad practice, because, if a destructor throws during stack unwinding [another exception already being processed], the program is terminated. Java handles this much more sanely: if an exception is thrown during stack unwinding, it is suppressed, but with a twist: when some exception handler eventually runs, it can retrieve a list of suppressed exceptions. C++ has a number of interesting / cool / useful features, but all of them are, unfortunately, only half-baked. (The above text explains why, for example, I consider RAII a half-baked feature.)
His case isn't extreme, he is just inexperienced in using smart pointers. What he describes could be perfectly accomplished with shared and weak pointers.
Indeed, but that is deterministic, and so you can move it around to be executed when the pause is not important.
Isn't the above only for library proposals?
Yes, I know that, but I learned about Monads when I learned Haskell. The Monad concept itself is a bit abstract to me, that's why I have a hard time to see how and why it could be used in C++. As far as I understand it, Monads encapsulate imperative code and code with side effects. This is useful in Haskell, to overcome restrictions caused by the the pure functional paradigm, but where does it fit in in C++, which is imperative and allows side effects?
In C++ you can catch exceptions in destructors like this: ~foo() try { } catch (const file_close_exception &amp;ex) { } Perhaps you can then put a retry loop in there. The double exception is indeed an issue, perhaps it would be better if the compiler promoted a single exception to a tuple of exceptions. 
Here is another trick that could be used to alleviate the problem: pass a retry handler to the constructor. Since resource acquisition is initialization, it's perfectly acceptable to do this. For example: File file("myfile.txt", [](File &amp;f){ cout &lt;&lt; "retrying close"; f.close(); }); Now when the destructor tries to close the file and fails, the retry handler is invoked. 
What is the purpose of infinite ranges? are there useful anywhere?
This is something that I would really like to have in the language. Tuple assignment syntax would be especially useful in numerical codes, where it can make steps of algorithms that compute, update, or reassign multiple values during each iteration much more legible. The simplest algorithm that would benefit enormously from tuple assignment syntax is the Extended Binary GCD algorithm (which computes integers a, b such that ax + by = gcd(a, b)). In fact, if you are not convinced that this feature will be useful, try implementing the Extended Binary GCD algorithm in C++ (see answer to exercise 39 of section 4.5.2 in Knuth's _Seminumerical Algorithms_).
Well, this would then just be the same as using `std::tie`, which I discuss in the [linked post](http://www.reddit.com/r/cpp/comments/1w6019/is_there_a_way_of_unpacking_tuples_without/). The problem is that this doesn't declare the variables, only assigns to them, so you still need to know the types ahead of time.
For those who are interested, I've [floated the idea on the std-proposals mailing list](https://groups.google.com/a/isocpp.org/d/msg/std-proposals/4yWRnzX7J7Y/sS0qBxcK3hUJ).
Monads are hard to understand if you're looking at the generic type of what a monad is. Monads are comparably easy to understand if you're looking at instances of monads. Then they're just about composition and quite elegantly so.
It's a downside in terms of properties of the program that can be proven without actually running it, and I *really* appreciate exhaustive checks. Of course, I guess that sometimes some kind of dynamism is required; however I've rarely seen such dynamism needed. In the typical of a `Car`, for example, you will have a `std::unique_ptr&lt;Engine&gt;` and several concrete implementations of `Engine`. A `Car` has an `Engine`, its exact kind is unknown (statically) but you can be confident in the fact that it's there. If you model the `Car` with your `object`, you are not even sure there is one; this is really extreme, and I wonder where such extremity is required, since I just cannot fathom a situation where it is.
How about the other pattern, i.e. providing a failure handler?
Well, even in a non-redesigned STL, you could always declare all STL types as finite ranges yourself so that you are users can use them without worrying about the nitty-gritty details :)
This is why they invented [IDisposable](http://stackoverflow.com/questions/538060/proper-use-of-the-idisposable-interface) and the [using keyword](http://msdn.microsoft.com/en-us/library/yh598w02.aspx). If you want RAII you still have it, just not for memory. It's probably an error to call memory a resource. It is, but that's not what your program sees. Your program sees virtual memory. Even a language such as C++ is not deterministic in how memory is actually freed. That's left to the virtual memory system of the OS. 
A very well written article. However, even though modern C++ (`std::unique_ptr`, etc..) prevents memory leaks, it still does not manage to prevent *dangling pointers*. And those hurt...
I didn't read much of your article, but as someone who is *not* a fan of GC, I think you're overstating the case. RAII is great where it works. The real problem is more complicated ownership models. As programmers we should try to avoid complicated or ambiguous ownership models, but if you're stuck with one, GC is the better answer. The alternative is ref counting, which is very inefficient. The more refcounting your program does, the more likely it would be quicker in with GC instead.
It shouldn't be seen as "RAII versus GC". There's no reason a language can't have both. The problem with Java, for example, is not that it has GC. It is that it doesn't have RAII. I'm more familiar with C++ than anything else and it would be straightforward to add some form of GC to it. We already have `shared_ptr` with ref-counting, we could have a similar type without ref counting and without guarantees about calling a destructor. (So that it can only be used on types where you don't care about destructors being calling.) ( I haven't ever really used Java for a serious project, therefore I don't have the experience to comment on it. But I'll try my best! ) I would love the equivalent of `unique_ptr&lt;T&gt;` in Java. The compiler would enforce that such a ~~pointer~~ "Java reference" could not be copied, but it could be moved. There is guaranteed to be only one reference to that object, so the compiler can easily see when that last reference dies and call `close()` (or some suitable destructor, *not* `finalize()`). Even if such a type were added to Java, it would a big job to update all the libraries (standard and otherwise) to make use of such a type. That won't happen and so Java will be stuck without RAII forever. Even if such support was built into the language, I don't think it would ever catch on.
If all you need to model in your software is a car and its various engine types then this is not a good case to use this library. It's designed for objects that may or may not have certain components. You can also think of them as tags in a blog post. Each combination of mixins is potentially valid. The library does provide you with a way to have dependent mixins (as illustrated here: http://ibob.github.io/boost.mixin/boost_mixin/tutorials.html#boost_mixin.tutorials.mutation.mrules ), but that's not its main goal. For your example think of how you can also model a horse carriage or a wheelbarrow. Both have no engine, at least in the conventional sense. A more complex system could have an even bigger variety of valid objects. So big, even, that in order to list all concrete valid type combinations, it could take tens or hundreds of megabytes of source code (think about vehicle with engine, with wings, with an external movement, by human, by horse, by locomotive, by balloon, steered by a human being, steered by a computer, on tracks, on roads, in the air, in space, etc etc etc). Not to mention that a change in any of those components will inevitably require the recompilation of a huge amount of dependencies that use it. This is the classic m+n vs m*n problem of defining types. Also let's not forget that you may want to have a container that has all of your objects inside it. This is a dynamic problem. There simply is no way to guarantee the type safety in compile time. The only way to contain all those objects in a single container is to have a parent that's common to all of them. It will either be an IObject style parent with no methods of its own, or an EverythingIncludingTheKitchenSink style one that simply has every possible method, or a getter for every possible component. Both ruin your type safety and suffer from much more problems than the Boost.Mixin approach. The library helps you design systems with very complex polymorphic objects that can have a huge variety. With polymorphism you inevitably sacrifice some type safety (even in the basic virtual method approach). The more powerful you want your polymorphism to be, the more type safety you need to sacrifice, until you reach a solution, similar to this library, that has an unassuming object type that can be virtually anything.
&gt; the Lisp-with-aliens thing this copies Land of Lisp? Not sure what copies that apart from Realm of Racket. LYAH's been "copied" as [LYSE](http://learnyousomeerlang.com/). I find I tend towards either chuckling or yawning. I'm sure authors all constantly think of jokes they could insert into their text (I know I do), I just can't figure out why they leave them out. My theory so far is mean editors who only like lame jokes, like Garfield strips.
I actually felt like the prose was kind of fun. It was a refreshing departure from the typically dry style of most contemporary programming books. It was also a little more fast-paced in that it shows you a concept with a bit of description and moves on. Many books, especially Java books, can spend many pages on a single facet. The tendency to dwell on topics gets on my nerves much more than excitement. My biggest problem with LYAH is that the examples are so utterly contrived that it can sometimes be difficult to see any sort of practical utility in the exemplified feature. There's just too much of a gap between the example and real-world usage. I'm also of the opinion that no programming book is complete without exercises, so LYAH feels incomplete to me. I think these are the reasons why Kernighan and Ritchie's *The C Programming Language* is so celebrated: it's concise, its examples immediately connect to real-world usage, and it has exercises. The reference material at the end is a big bonus. For me, K&amp;R is still the holy grail of programming language books. I haven't read RWH yet to see how it compares, but I think I may start it this weekend.
Fantastic reply! I'm afraid of GC pauses on games. People using D for game dev fall sometimes on that problem, but D does not have - yet - a concurrent / incremental GC (but is one of the main goals of 2014: to improve allocation and GC).
You may want to edit your original post, then; it's very confusing to read two contradictory sentences immediately after one another. 
Garbage collection isn't wrong, it just models Resources differently. In our every day existence we experience Resources as being finite in quantity. You can only have so many file descriptors, your disk is only so big, etc. However, code can cross the boundary into an ideal mathematical world where resources are **infinite**. Or least they can *appear* to be infinite. Abstraction-wise, RAII does a very good job with finite Resources and its probably the best pattern around for that job. But life and code can become a whole lot simpler when you can treat resources as being infinite. You no longer have to worry about freeing memory, or closing streams. Now in practice the only thing we can really treat as infinite is memory, and thats only after constraining a language with that abstraction in mind. But that doesn't mean that we shouldn't keep trying to model resources as being "ideal" (infinite, costless), only that our current set of abstractions and languages do poorly when a programmer wants both finite and infinite resources to mix freely. 
&gt; In theory I guess. Ive never actually seen it. Me either, though I don't have much experience with large refcounted systems. &gt; Yet, reducing GC pauses is a huge business. True, but my feeling is that that's a sign of the times more than a fundamental flaw of GC. Until just a few years ago, almost all code written in GC languages was for scripts, batch processes, or servers. There, total throughput matters more than latency. In the past few years, with .NET on Windows and Java on Android, we're starting to see a lot of interactive software, in particular games, written in a GC language. When that happens, latency matters more than throughput. It takes a while for GC heuristics to catch up to the use cases. For example, Lua very recently switched to incremental GC. &gt; Additionally you can schedule the the RAII pauses and spread them out over frames. Sure, and in high performance apps, I like having that level of control. At the same time, once you start writing code to do that, you're creeping back towards the complexity and runtime analysis of actual GC, so you're getting less of the performance benefit of manually managing your memory.
There were proposals to change this behavior, and they weren't accepted. The next major version of VC will ship with future dtors that have conformant blocking behavior.
Ideally the library that implements the reusable bits uses an allocator and deallocator supplied by the calling application, so that the short-lived command line tool can supply a simple allocator and no-op deallocator, while an IDE can supply real allocators and deallocators.
&gt; Couldn't a compiler theoretically insert statements to allocate and deallocate memory as needed? Yes, that is how ParaSail and Rust do memory management. The problem is that C++ doesn't allow for the same safety guarantees those languages have.
Then why don't you let the GC unlock your locks? Or close your files? GC is good for memory, because it's basically free this days, but as the article and every c++ programmer knows memory is but one type of resource.
That's not practical. My range lib can't #include every std container header, and forwarding-declaring them on behalf of the standard is non-portable. Users would have to do it themselves.
This doesn't address OP's problem, which was that you need to create the result variables manually before being able to use std::tie or unpackInto.
Assuming that by 'list' you mean a container, in neither case lists are infinite in the above text. Perhaps you're talking about generators/lazy lists of numbers computed via a mathematical formula? 
&gt; They can throw if you declare them with noexcept(false). I didn't mean it in an absolute sense. I meant it as "C++03 code recompiled with C++11 compiler."
&gt; Your claim would be more convincing with an example. Much of what's done with smart pointers concerns questions of ownership. The conceptual owner of a node in my tree is the tree itself. However, the tree proper does not contain a pointer to any node except the root; direct pointers to other nodes just aren't needed. This confounds the presumption that if you own a dynamically-created object, you have a pointer to it. To force the issue, we could declare that all the nodes that point to node N are owners of N, and then use a shared_ptr. But in the usual implementations of shared_ptr (according to Stroustrup), that would introduce a reference counting system, and entail additional dynamic memory allocations to hold the reference counts. We just don't need all that. Plus, if I have a node that is temporarily outside the tree (because it's being moved to another location), I would have to make sure that the shared_ptr system knows about the "extra" pointer that I created so that the node doesn't get lost. If I were of the "always use a smart pointer" philosophy, I could make this work. But it really is simpler to manipulate the pointers directly, and simplicity is one the best defenses against bugs. Throughout this structure, whenever node A points to node B, then node B points to node A. These reflexive pointers are needed for various other reasons anyway. So if I want to detach and possibly delete node A, I can easily find every node that points to it, and do whatever needs to be done to those other nodes. The root node is of course a special case handled separately. &gt; the extreme nature of your problem shows even more that only very rare edge cases cannot be handled by the standard pointer classes. Yes, I'm doing something very unusual. It seems like all the usual things have already been done.
A lazy list, yeah. That quote was taken from the article where I was talking about Haskell lists, some of which are lazy and infinite.
So shared\_ptr would have worked, but you wanted less overhead. (Did you measure performance implications?) The ownership in the root also would have worked (with naked pointers). In both cases, you would need to traverse the tree on removal and "free" your links. &gt;No, whatever your dynamic allocation needs, there is a smart pointer class for it. Wasn't me 😉
My case was multithreading: so there's the main thread that holds the model (object tree). I use shared pointers there. There is threaded processing that needs the model elements. That gets weak pointers. Object tree is modified (for whatever reasons). Threaded processing takes care to turn weak into shared only when it actually needs to "talk" to the model, and goes hands off if it falls on a null.
Naive reference counting is terrible due to the stupid amount of updates that have to be pushed out along with the glories of 'Oh I've created a cycle, time to get another mechanism to handle that'. Oh and it doesn't work with multiple threads. Recent work on reference counting, such as IBM's Recycler collector and Blackburn et al's Ulterior Reference counting are both garbage collectors (admittedly I'm being flexible with the term there) which utilize heavily modified forms of reference counting to clear garbage.
That's an open problem. Region based memory management for the ML language by Tauplin and Toft succeeded in such a goal but had issues due to how the inference algorithm worked with some cases of idiomatic code (certain types of loops which should have O(1) memory usage had O(N) memory usage among other issues).
&gt; Wasn't me Apologies for not attributing that properly. &gt; So shared_ptr would have worked, but you wanted less overhead. (Did you measure performance implications?) Should I write a more complicated version of working code just to see if the additional overhead, which will certainly exist in some quantity, will be tolerable? &gt; The ownership in the root also would have worked (with naked pointers). So the root (or better, the tree housing) should have a container of pointers to all the nodes? The nodes currently do a fine job of pointing at each other. That container would just be something else to keep synchronized. Keep in mind the criterion for deleting a node in this code isn't "nothing else is pointing to it". Rather, a node is deleted when the user specifically requests it. Even the smartest pointer can't figure out when that's going to happen. 
Actually, I was already thinking to the point where the range lib would be in the Standard itself, in which case forward declaration is a non-issue :)
It's possible to combine positions and ranges into one type. I've done so in my D2 container library, dcollections. One of the fundamental types is the cursor, which is a zero or one-element range. All my container types support cursors, and you can generate ranges from them given the original container. All of these operations are safe, as opposed to C++ iterators.
This is a common myth which I have [already addressed](http://dl.dropboxusercontent.com/u/7810909/docs/what-does-monad-mean/what-does-monad-mean/chunk-html/ar01s02s02.html). [What Does Monad Mean?](http://blog.tmorris.net/posts/what-does-monad-mean/)
To me, RAII is conceptionally a superset of GC, because it does GC and other things, and RAII in C++11 works seemlessly. If you are writing a program where lazy memory freeing is even an option, then you can just as well use smart pointers everywhere and get "garbage collection" for free, but also all the other features RAII provides, and if you are writing something where memory management is a core issue, you can do that with RAII as well, but not with GC. In Java, for example, you are either explicitly or implicitly also "semantically freeing" memory at scope boundaries; a variable that goes out of scope is no longer accessible, and a reference set to 0 is equally lost. I don't see the advantage of, instead of right at that point handling memory freeing (or not, C++ allows for that), just losing the allocated memory temporarily until a mark and sweep "recovers" the information that said memory can be deallocated later. We have this information, why throw it away and later ressurect it with a somewhat expensive algorithm?
 &gt; RAII is great where it works. The real problem is more complicated ownership models. As programmers we should try to avoid complicated or ambiguous ownership models, but if you're stuck with one, GC is the better answer. The alternative is ref counting, which is very inefficient. Are there benchmarks on that subject? Refcounting is increasing and decreasing a cache local counter, that's three instructions (movl, inc, movl) that will usually not produce cache misses (in cases where you are doing a lot of refcounting, it is, I claim, reasonable to assume that this high amount of refcounting is at least evenly distributed in your program, and doesn't instead produce a cache miss for every ref count update). As for the comment on complicated ownership models: Yes, there's an argument in favour of GC here, but I think it is part of good program design to either don't have complicated ownership, or to deal with it in an expedient fashion. *Relying* on garbage collection to alleviate bad design choices doesn't seem like a productive mode of working to me; it's unreasonable to accept some unremarkable program eating half your system memory because the programmer of said program decided to let the garbage collector handle hundreds of lost references. 
&gt;To me, RAII is conceptionally a superset of GC... if you are writing something where memory management is a core issue, you can do that with RAII as well, but not with GC. I see what you mean, but I disagree in that RAII is deterministic, whereas any GC strategy is not. I think that's reason enough to consider them orthogonal. Also, I still maintain that RAII is really "end of scope" event registration, and is limited to that view of the world. To be a superset of a GC's features, I think RAII would have to be far more flexible than that. Same goes the other way around for any GC (ref-counted, mark-sweep... you name it) &gt; We have this information, why throw it away and later ressurect it with a somewhat expensive algorithm? As it happens, people have done the research on hybrid solutions that mop the floor with both ref-counting and GC approaches, as you would suggest: http://www.cs.utexas.edu/users/mckinley/papers/urc-oopsla-2003.pdf You're right: throwing that information away can be wasteful time-wise, but in aggregate, the memory cost of keeping those ref counts around for a large number of objects can be a drawback. It's an optimization domain, and it ultimately depends on your app. In the classic time vs space debate: ref-counting is fast but costs memory overhead, and mark-sweep is the opposite. And, FWIW, there are languages that give you both features. D, for instance, gives us the best of both worlds: scope(exit) hooks *and* a GC for everything else. Rust is another one, thanks to the GC library and the pointer ownership mechanics.
&gt; I see what you mean, but I disagree in that RAII is deterministic, whereas any GC strategy is not. I think that's reason enough to consider them orthogonal. Ah that's an implementation detail. I don't mean this entirely seriously, but a small part of the code I'm working on does a kind of bulk memory management, where a large block of memory is allocated and then filled sequentially, and each object uses essentially an empty deallocator that just decreases an *overall* ref count. The last object that goes out of scope moves the pointer to said block into a list of freeable resources that are freed when time permits (that's usually after the "reduce" phase of the algorithm; it's a hand-made implementation of a mapreduce in C that has been maintained for legacy reasons for over 15 years now and which has been extended with C++'s RAII facilities). In your view, that would be using an end-of-scope event model to implement a non-deterministic GC, if you see what I mean. It still a kind of ref-counting, but in principle this could also implement a mark-and-sweep triggered by a scope-end event at a fortuitous time. I see what you mean, I'm just not sure that the distinction you make is that meaningful. &gt; And, FWIW, there are languages that give you both features. D, for instance, gives us the best of both worlds: scope(exit) hooks and a GC for everything else. Rust is another one, thanks to the GC library and the pointer ownership mechanics. I wanted to look at D for quite some time now, maybe I'll get to it soon. But thanks for the paper! That'll go into my read-on-route-folder for monday. :)
Slides, awesome! The CEO in me rejoices. :) No seriously, good stuff, I appreciate that. If I got it right, a Monad is anything which has a type constructor, bind and return (or unit). Would it be correct to say that I've mistaken concrete implementations of the monad interface (e.g. IO) for the abstract interface?
There is also video. Yes, you are correct in your articulating your mistake. I may be able to reverse a list of bananas, but to say that lists have anything to do with fruit would be an error.
Container != range. A container owns its values; a range might not. You can define a sequence as a position and a terminating predicate. In the STL today, the terminating predicate is when the current position reaches some other known position. I'm suggesting allowing other predicates. For example, when a null terminator is encountered. And once you allow predicates, you allow someone to specify a predicate that always returns false. That's an infinite range. It's perfectly logical.
I don't think this is a fatally flawed concept, but I have some issues. Why is Widget a class rather than struct when the first thing you do is declare public members? Excess verbiage make reading and maintenance more difficult. Why have finalize() when it is unnecessary? (see comment below) It is just a bug waiting to happen when the caller forgets to call it or calls it too soon (before a use of a Widget). The name __widget is owned by the implementation because it begins with double underscore. Why are we using a map, when unordered_map is likely to have superior performance in almost every use case? (Ordered traversal doesn't seem to be possible so it must not be a requirement.) Better yet, why not use unordered_set with a comparator function that compares by calling getName()? This would use less memory with no loss in performance. Note that if a Widget's name can be changed, disaster will strike, but that is the case with currently implementation as well. Why does the __widget map not map to a Widget object instead of a Widget object pointer? This is what makes finalize unnecessary. This one change reduces overhead and allows you to remove finalize() and replace it with... nothing. And still get correct behavior. There are no bugs in code you don't have to write. What happens if the user creates two Widgets with the same name? (Answer: The second one is leaked.) You've created a data structure that allows accessing Widgets by name. An arguably useful feature (depending on the domain), but you don't have a way of exploiting that. Why not add a static getWidget(std::string const&amp; name) function that returns a pointer or a reference to a Widget (or null_ptr or throws if no Widget exists for the requested name)? If you don't add this, a vector would probably be a better container. Note the complete removal of finalize(), as a separate function is no longer needed to clean up. 
Thread-safe reference counting is much more expensive, and it's relatively uncommon to need reference counting yet be able to guarantee that it doesn't need to be thread-safe.
&gt; Thread-safe reference counting is much more expensive, and it's relatively uncommon to need reference counting yet be able to guarantee that it doesn't need to be thread-safe. I think about concurrency far too little, but if you are working with a concurrent system you are synchronising anyway. I wonder if mutexes around the reference counters would really be more than a droplet on the hot stone at that point. But I can't claim that, sure. 
I think you're focusing far too much on a simple example, instead of critiquing the pattern as a whole. &gt; Why is Widget a class rather than struct when the first thing you do is declare public members? Because structs should only contain data, not methods. &gt; Why are we using a map, when unordered_map is likely to have superior performance in almost every use case? Do not worry about performance before you've got the interface nailed down. We use an `std::map` because we want to find widgets by name. &gt; Better yet, why not use unordered_set with a comparator function that compares by calling getName()? Because an `std::map` already has the desired interface. If it's a problem, it can always be refactored. &gt; Why does the __widget map not map to a Widget object instead of a Widget object pointer? Personal preference. Can you guarantee that the pointer to the reference in the `std::map` is the same instance every time? You'd hope so, but it depends on the implementation. I've been burned by stupid stuff like that before, not necessarily with STL containers, but other libraries that have weird quirks like that. &gt; Why not add a static getWidget(std::string const&amp; name) function that returns a pointer or a reference to a Widget (or null_ptr or throws if no Widget exists for the requested name)? If you don't add this, a vector would probably be a better container. It was intended as a simple example, not something that provides useful functionality when put through a compiler. So, good news: I've decided that the `create` pattern is bullshit. It's better to just use the constructor.
Generally it's atomic operations, rather than a mutex. Of course it's still quite heavy.
I try to avoid complex ownership models, as I implied above. I greatly prefer to *understand* my software. In practice I work with others, some of whom really love shared pointers. I don't get to dictate their designs, but I have to work with them.
special keywords etc still must be used for dealing with disposable objects, this make them different from ordinary objects, in true RAII you don't need to worry about that.
True, but the trade-off is you get garbage collection for memory. If you consider how often you acquire a memory resource vs some other resource such as a database connection, it's worth it to special case those resources and have memory not require disposing. 
ok, but that's another point (the point of GC vs NO-GC), my point was just about pointing out one calling a thing RAII when it isn't.
You have to use recursion. There is no way that I know of to get the 'n'th value of a parameter pack, there are only methods for expanding the parameter pack. But in combination with your `int a` first parameter, that's enough to write a recursive definition of the function. It's just like `car` and `cdr` or "head" and "tail" in Lisp-y functional languages. You write a base case that matches on just `&lt;int a&gt;`, which corresponds to a list of length 1, and there you do your comparisons, and then you write a recursive case for lengths &gt; 1 that pops one off the list and recursively calls itself with the "tail", i.e. ignoring `int a` and expanding `parameters...`. 
So you are not actually talking about ranges, you are talking about sequences. A range is something finite, by its definition. 
&gt; RAII is mostly incompatible with exceptions because you can't throw from a destructor. I would (in my uneducated opinion) claim the opposite: exceptions are incompatible with an absence of RAII. If your function can return at almost any point, explicitly mentioning where you must release resources is more trouble than one can reasonably put up with.
Read the comments in the site. Also, the fact that he points out that he tried returning a pointer instead of returning by value is just idiocy because in C++11 there's move semantics. By the way, some person in the comments mentioned http://benchmarksgame.alioth.debian.org/
I think pretty much everyone uses this strategy. Unless it's like a high profile customer flipping out about something...trying to debug the occasional error when all you have is dumps and maybe some sparse logs is incredibly difficult. I for one prefer a solid repro over anything else when I'm trying to debug something. Can't imagine a compiler writer feels differently on that :P
Ranges in math certainly aren't required to be finite, I've never seen a formal definition of a range in a CS context that isn't just the math definition, and I've seen people colloquially talk about infinite ranges in programming languages plenty of times before, so I'm rather curious why you say that.
&gt; GC.Collect() does explicitly FORCE a GC. does NOT mean the same as: &gt; "When control returns from the method call, the Java Virtual Machine has made a best effort to reclaim space from all discarded objects." "best effort" guarantees - what? It guarantees _nothing at all._
Have you read at all the rest of what I wrote? Do you have a good solution for 1) returning errors from destructors, and 2) how to cope with stdlib which has absolutely no provisions for handling errors occurring in its *own* destructors?
You could unpack it into an array with `constexpr int params[]{parameters...}` and then use `params[n]` to access the N-th parameter.
Here is a definition: http://i.word.com/idictionary/range If you read the various types of definition, it seems that a range always refers to a finite set of values. In math, there is another term, the set, which can be infinite. If a range can be infinite, then it becomes a set. Despite all the above, I think that it might be safe to say that most people do not think of ranges being equal to sets. In everyday talk, we use the word range to speak of finite sets of values. Since in CS the term set has another more strictly defined meaning, I propose to use the term 'sequence' as it seems to be more appropriate than 'range', due to the existence of possibly infinite sequences.
Yes, I have, and those are serious issues. Never the less, as bad as "can't throw exceptions in destructors", I'd say it's better than "can't throw exceptions anywhere".
how does this compare to VA?
&gt;I will cover two compilers: Microsoft Visual Studio 2010 (MSVC) and the GNU Compiler Collection 4.5.3 (GCC) It's 2014 today...
This is, unfortunately, a nasty problem to try and solve. While there are tools like Valgrind (the DHAT and massif modules in particular) that are great for profiling memory usage, they are heavy-weight and you wouldn't be able to use it in production. Even if you could, they don't quite answer your question, and typically provide analysis at program exit, which isn't what you want. Unless someone points out otherwise, I think your best (perhaps only) bet is to use a custom allocation scheme. A quick search reveals [MemTrack](http://www.almostinfinite.com/memtrack.html), which seems to be an attempt to move in that direction. You could perhaps extend its methodology to your specific use case and have it track usage by job.
In transition for now to github, here: http://schveiguy.github.io/dcollections/ But for a long long time, it's lived on dsource: http://www.dsource.org/projects/dcollections/wiki I haven't updated it in a while (It's really due), but the cursor concept will not be changing. More on the concepts of dcollections here: https://github.com/schveiguy/dcollections/blob/d2/concepts.txt
Sets are unordered. But it doesn't matter. We're talking about C++, and the term "range" as used in the C++ standard is something on which we can call "begin" and "end" to get back iterators. Nowhere is it required that the sequence denoted by these two iterators is finite. In fact, there are iterators *in the standard* for which that's not necessarily true. `istream_iterator` is one. It might end, it might not.
Sorry that the last part scrolls over so far......no clue why that happened
You formatted it as code by putting at least 4 spaces before the text.
Ah, Gotcha! thanks for the tip
Thanks for the review. Your high esteem of it concurs with its #1 recommendation on the C++ FAQ. Here is the US Amazon link for the book: http://www.amazon.com/Primer-5th-Edition-Stanley-Lippman/dp/0321714113
Linux cgroups
I don't it was, it wasn't torrented or anything. But I fully plan on going out and buying it here soon, definitely want a hard copy
That makes sense to me. Because a future is something you want to hang around for a reason. Like you want to get the result. And if you just want to spin a thread and not have a future just do Thread thread(foo1); Thread thread2(foo2); thread.detatch(); thread2.detach(); I just don't like that it's called "async" as that name for what it's actually doing seems misleading to me. 
&gt; Because a future is something you want to hang around for a reason. That's fair enough, as long as the `future`'s behaviour is consistent for all scenarios. I could conceivably get a bunch of `future` objects from different places, and I have no way of knowing whether it will block my thread, or not. This will impact design decisions. Perhaps `async` should return something like a `blocking_future` type.
I've been reading it lately as well, and it makes a lot more sense to me than Head First C (or C++ not sure which) and a few others that were more specific to the grammar and syntax (which is what I really struggle with)
The fact that there are iterators where comparison with end() may always return false does not mean that ranges are infinite. As you wrote yourself: "Nowhere is it required that the **sequence** denoted by these two iterators is finite". You wrote 'sequence', not 'range'. Since the c++ standard shall officially adopt sequences, I think that a change in terminology will only do good. 
The fundamental problem with the article is that it misunderstands garbage collection. It’s a very understandable mistake because it’s so common, but it’s a mistake nonetheless. [Raymond Chen has probably written the most approachable explanation](http://blogs.msdn.com/b/oldnewthing/archive/2010/08/09/10047586.aspx). In a nutshell, it boils down to this statement: &gt; **Garbage collection is simulating a computer with an infinite amount of memory.** Let that sink in, because it’s a great idea. It completely changes resource management for memory because *it removes memory as a resource*. You just don’t have to think about memory any more – at all. Of course there are rather huge problems with this idea in practice: it only works if there is sufficient memory to simulate this infinity. With modern garbage collection algorithms, this means that there must be a pool at least twice as large as the largest memory requirement *at any given moment*, to avoid stalling (IIRC) – this is the reason [why GC fails spectacularly on mobile devices](http://sealedabstract.com/rants/why-mobile-web-apps-are-slow/), for one thing. It is also completely silent about other resources – you’ll still need to think about those. But since memory is *by far* the most commonly used resource, that’s a valid decision. This isn’t to detract from the many good points the article raises. But the conclusion and title, “Garbage Collection is Wrong” is just … wrong. And I say that as a big proponent of RAII.
Agreed. It's also one of the few C++ books that has a really good Kindle version as well, which was another point in its favour for me.
Why, if his data structs are appropriately designed not call sizeof?
Well, that's undoubtedly going to be a piece of the solution, but his question wasn't "how do I know how much memory an individual allocation will use". Sizeof on its own won't solve the problem of tracking per-job usage.
Well summing up all the structs involved in the code where possible would surely be the easiest way. If he splits reach into anew process then using system tools to view the res usage would work
He outright stated that using multiple processes isn't an option. Just knowing how much an individual allocation will take is useless in the context of a dynamic job. If the job is, for instance, splitting up and hashing a file, and his current design chunks that work up, with each chunk requiring allocation, then he has to have a system that does book-keeping based on runtime knowledge.
That's what i mean i think.. use the sizes you can get and send it off to be tallied elsewhere and report it regularly.
Public Service Announcement: The side-scrolling text reads: &gt; Side note, It also teaches you to actually use libraries from the start, which is AMAZING. I don't understand why so many courses or books teach you the hard way to do stuff without libraries, then they throw the libraries at you so you have to learn how to do all the stuff you were doing WITH the libraries... 
Right, but getting the sizes is the easy part. Adding that functionality to an existing application which wasn't initially set up for it isn't trivial. Since you might have a lot of places where allocations are being done, depending on the size and scope of the app, a good solution is one that doesn't require adding 100s of sizeof calls by hand. That's why I suggested he use the memory allocation system itself, which can be done with C++.
Technically it has been buildable as C++11 for a while. The headline should read "LLVM No Longer Building as C++03."
MemTrack looks interesting, but appears to provide a lot more than OP is looking for. This seems so obvious, so I'm wondering what about my thinking is wrong: Can't you just have a mapping from some identifier (hash) of each job to some structure containing memory statistics. What you do next depends on what exactly the jobs are doing. If they're simple, you could have incredibly lightweight statistics gathering. Otherwise, you can use a custom allocator that is generally just a basic wrapper around normal allocation/deallocation that first manipulate your jobs statistics. 
https://code.google.com/p/googletest/ is useful
Boost Unit Test is an option. I used it at my last job and once I started to figure out what I was supposed to be doing it didn't seem so bad. 
[Boost.test](http://www.boost.org/doc/libs/1_55_0/libs/test/doc/html/index.html) I've used a template-best framework that is super easy to include (it's header only) called TUT: http://mrzechonek.github.io/tut-framework/ It's fairly easy to use and is cleaner in the sense that it doesn't use macros.
Nope, nothing about your proposal seems wrong; the opposite in fact. I only pointed out MemTrack (which I agree is more than he needs) because that one link happened to show just about every technique he might need in order to do this correctly, in one spot, as well as being a potential candidate for modification.
I cannot say it's at all standard but libtap++ works well for me, being a perl aficionado. You can run test output through the standard perl test harnesses and smoke systems.
Used cxxtest for a class that involved a sizable project and it seemed to get the job done.
The probably cleanest solution is to rewrite your code in terms of custom allocators - as already suggested. This is not as bad as it looks, but there is no easy way to detect if you forget to install your custom allocator somewhere in the code. A quick and dirt trick is to [overload the global operator new and delete](http://stackoverflow.com/questions/8186018/how-to-properly-replace-global-new-delete-operators), or, if you use malloc/free directly in your code and you are on a *nix environment, you can set hooks on malloc or write a wrapper around malloc/free as described [here](http://stackoverflow.com/questions/262439/create-a-wrapper-function-for-malloc-and-free-in-c)
We use [UnitTest++] (http://unittest-cpp.sourceforge.net/)
I have historically used googletest, but I now think Catch is a better option due to its single-header drop-in nature. The recommended way of using googletest is to integrate it into your source tree, and it is a large number of files with a non-trivial build procedure. Compiler/library support in googletest also seems to be a bit less robust than Catch. For example, I use c++11 features in EntityX and hade to use -DGTEST_USE_OWN_TR1_TUPLE for googletest to even compile. And there are still issues with it under Visual C++.
Just remember: [Monads are just free-monad monad monad-algebras. They are all monad-algebras of the free-monad monad.](https://twitter.com/kmett/status/384766778062016512)
Boost
Google Mock looks useful as well.
just voicing my support for Catch. Super light, and does a ton of stuff! All in one header!
TBH, I didn't have a concrete example in mind, and I'm not enough of a template-programming expert to conjure up one. My point was merely that I felt that may have been what OP had in mind.
I assumw you mean TUT. no, it can't do that as far as I'm aware. When testing for a condition you pass in a text, so you know which test failed that way, but that is all.
That's pretty much Boost Test.
Prevent dangling pointers by never storing pointers. 
Darn it :-( 
similar to this proposal? http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3761.html
Thanks. What is TUT? ... under test?
I wonder about that too. From what I can see as a complete outsider, they are seriously understaffed, which is another way of saying that it isn't that important for Microsoft to have the most advanced compiler. 
[Mini CppUnit](http://sourceforge.net/projects/minicppunit/) I've found to be pretty decent. This one is my choice as the source code is pretty small so you can figure out the internals of it pretty quickly.
People don't really use ICPC (Intel) for it's langauage support, they use it for the better code gen, specifically it's auto-vectorisation.
That said, with a makefile project you *can* use G++
The framework I linked to in my original post, it's a template base testing framework (Template Unit Test I think) http://mrzechonek.github.io/tut-framework/
im biased and i like https://github.com/r-lyeh/petitsuite :D
I'm thrilled with how well all of these compilers support 11 and 14. I remember how many years it took before 98 had *any* real support in compilers. Microsoft is behind the pack, but if they'd done this well with 98 they would have beaten everyone else.
Sure, people who don't want to target AMD CPUs. Intel's compiler plays dirty tricks to *not* auto-vectorize on AMD.
No, it’s not. Boost.Test will give you the following output for `BOOST_REQUIRE(Factorial(0) == 1)`: &gt; main.cpp:9: fatal error in "Factorial": critical check Factorial(0) == 1 failed – but it will *not* give you the actual value computed, as Catch does (i.e. the “with expansion: 0 == 1” part).
&gt; I'm guessing it works for complex object types with their own overloaded operators? It does indeed. Catch does something ingenious with overloaded operator `-&gt;*` to make this work. The details are convoluted enough that I’ve forgotten them, unfortunately.
&gt; I wonder if mutexes around the reference counters would really be more than a droplet on the hot stone at that point. They would. As an example, look no further than the efforts to remove the global interpreter lock from Python. Python uses refcounting everywhere, and protects all python code execution (but not necessarily calling into c libs) by a single global lock. As this obviously prevents concurrency, it's removal has long been on the agenda. However, a naive attempt to simply guard all reference accesses with fine-grained locking showed that, especally when actually multithreaded, the locking overhead was considerably greater than the entire runtime of the program on a GIL-enabled interpreter.
A smaller team I believe.
I'm not really sure what you mean. You would be able to test any classes or functions that would be shared by the SO or DLL. If you want minimal stuff - you might want to be a lot more generous with your assert statements - then when you're done compile without them. I am no expert. 
I use [googletest](https://code.google.com/p/googletest/). My product is a shared object, and we build it like there are no unit tests. We then build another set of files that are the unit tests and the main() function that invokes googletest. This links against our shared object. Every so often we need to do something screwy to the non-test files to be able to test some internal object sufficiently, but not often.
Thanks for the list! I'm btw. one of the mentors for the Wiselib (templated C++ multi-platform algorithms library for embedded devices). So in case anyone is interested: Feel free to contact me in any way you like!
Your attitude precludes you getting much value out of testing. I used to hate dependency injection. Then I saw it used well and now I insist on it in any project I run. It is just how I think about writing code now. Gtest and gmock are great tools, but you sort of have to commit to marrying testing, not just having a fling.
&gt;&gt; Then you should have used until instead of find, no? &gt; That depends on what concept is modeled by Until and what I want to do with the resulting range. For the record, when you really want a position, you can get one using [countUntil](http://dlang.org/phobos/std_algorithm.html#countUntil). &gt; I'm guessing Until is input or forward, even if it's adapting a bidirectional or random-access range. There's probably a way to force eager evaluation and turn Until into a random-access range if the underlying range supports that, right? Phobos wrapped ranges often offer full capabilities of the source range where appropriate.
It would be nice to have a way to set a policy for other resources too, wouldn't it?
&gt; GC didn't become mainstream until something like thirty years later with Java. I guess it depends on how you define mainstream, but really by the time Java came around, I don't think many people were inventing languages that didn't have an automatically managed heap. Those that did were definitely very explicit about it. In the early 90's Smalltalk had a (brief) resurgence, and that really popularized automatically managed heaps, but even before then most languages were already doing things that way. Explicit memory management was basically "old school" even before Java hit the big time.
&gt; Your attitude precludes you getting much value out of testing. This. Once unit testing saves his ass a couple times, he'll come around, like I did. &gt; I used to hate dependency injection. Then I saw it used well and now I insist on it in any project I run. Can you elaborate? Is this just a matter of good practices, or is there a specific framework you use?
I use googletest in conjunction with a componentized system where every component has its own associated test binary. Hence, when changing one component of the system, you can incrementally reflect changes through the (dependent) components and fix bugs/compiler errors by component/test_binary. Also, since most of our code uses virtual inheritance anyways, there is little need to actually use a mocking framework, as we can easily inject mocks that are defined in our test units.
They have just recently decided that 1) C++ is important 2) Implementing what the C++ standard says is important, even if it breaks back-compatibility with the previous msvcc. Kudos for this to Sutter and STL, I guess.
Other compilers that are behind, but promising to catch up: Oracle Solaris Studio 13 (beta): http://docs.oracle.com/cd/E37069_01/html/E37071/gncix.html#scrolltoc IBM XL C++: https://www.ibm.com/developerworks/community/blogs/5894415f-be62-4bc0-81c5-3956e82276f3/entry/xlc_compiler_s_c_11_support50?lang=en
No framework, just internalizing the intent, applying the techniques consistently, and reaping the rewards.
Can you suggest any good resources?
&gt; you sort of have to commit to marrying testing, not just having a fling. Exactly. I think the key is to commit to a unit-test suite, and aim for a high coverage percentage as early as possible in the project. Once you have a good unit-test suite in play, it impacts your design decisions such that you move towards testability rather than arrive at it aimlessly from time to time. 
What I do is configure a distinct 'unittest' Make target that incorporates the *source* from the main executable or library directly, along with the unit-test code and dedicated unit-test main(). Then you just 'make unittest', which will build and run the test binary. Note that this means that you have *zero* "#ifdef unittest" declarations in main project tree. The unit-tests become a parallel code tree that instrument the main one. In an IDE, the equivalent would be a secondary debug target, or another project altogether. The key is to use the release project's source tree, rather than just a .dll/.so file and some headers. This technique allows you to dig in, and get closer to the tested code, without having to disrupt your API. It also keeps your testing framework code and dependencies distinct from your release target. Ensuring the quality of the API can also be handled in this fashion, but I recommend labeling these "black box" tests appropriately. Edit: you may also want to look into coverage analysis tools, to make sure that your tests are effective. This will help make it clear where design changes may be needed to increase testability, and where your existing tests are falling flat of your expectations.
There are many types of testing. Unit tests are a high priced sort of test that only achieves some testing goals. I'd use it more if the cost wasn't so high. Don't get all religious on me; that won't get us anywhere. Edit: 5 downvotes? Guys, don't make this into vi versus emacs. Just because I criticize your favorite tool doesn't mean I'm your enemy. I don't have a "bad attitude", I merely disagree.
you might want to try Juce http://www.juce.com/features
 #include &lt;cassert&gt; ... assert(condition) Let's you focus on the actual tests - and the infrastructure to have them run automatically. At worst, you learn *why* a unit test framework does more than just that, and what you particulary want from it. --- &gt; #ifdef UNIT_TEST And yeah, your tests should be separate projects. Start writing tests for the stuff that is easy to test. If you have core functionality that is hard to test: think why, think what you would have to change in your code base to make it testable. Would that change be "a good idea anyway"? --- Some studies show that bugs correlate with the sheer amount of code more than advanced measures (like function points). From this position, unit tests try to *battle the problems of too much code by writing more code*. 
lol
Unit tests alone are not sufficient. But they are, in my experience, critical.
MS's compiler design is decades old, too. It didn't produce ASTs at all until relatively recently, and it's been very punishing on newer C++ features.
I use [Catch](https://github.com/philsquared/Catch/) for unit tests and [Turtle](http://turtle.sourceforge.net/) for mocks. Turtle is made for Boost.Test, but I prefer Catch and I haven't run into any problems putting the two together. It's a bit wasteful since Turtle brings in a lot of dependencies from Boost.Test, but my test projects seem to build and run pretty quickly (when being smart about precompiled headers).
Hah, been using c++ for quite a while now, but i never knew that &lt;% and %&gt; were alternate operators to { and } . damn ... this language is something that one would never fully learn. 
Ironically, what would you rather have: a pointer that pointed to some still-valid-yet-long-ago-leaked object, or a pointer that traipsed you straight into a seg. fault to tell you that something you freed was long gone, enabling you to find and fix your leak much more easily? The former types of insidious bugs I see in GCed languages because someone stores off an object reference in a data structure but forgets about it tend to be much more difficult to track down than the latter. 
$0.02: MS has a history of cutting their own path when it comes to standards maintained by everyone else. As long as their software compiles against the latest Visual Studio release, they probably don't care as much about deviating from the spec as, say, LLVM or GCC.
Those are called "digraphs." They were originally designed to allow for keyboards without those characters to still use C. There are many more digraphs, and even trigraphs, in C99/C++: http://en.wikipedia.org/wiki/Digraphs_and_trigraphs#C
Raymond Chen is a really smart guy, and that's true of GC. But, has nothing to do with why people choose to use RAII over GC in many applications. It's that GC itself is the problem, not whether or not we model memory as being limitless. If I'm working on simulation code (or anything that is attempting to be "realtime"), I simply cannot allow even a partial GC sweep to run. Why? Because either I let it do it's job, and then I become unable to control for the latency it introduces, or I tell it to only do part of it's job, and I can't control for how much unreclaimed memory my dead objects are starting to use up (which doesn't allow for unlimited uptime). For a long-lived service that is designed to be high-throughput, low-latency, and functioning as part of a distributed system, one might decide that GC is not the way to go (think of the zone server for instances in an MMOG -- where one constraint is that latency beyond about 30ms is unacceptable). 
yes, i know you walked to school uphill both ways, but even the 8086 i was forced to work with in school had a "normal" keyboard. the only one that i don't remember what it had was a weird cpu with shared memory where we entered code in hex directly (after compiling on state of the art 286). then again, with hex one doesn't need braces.
Ironically, however, despite a history of sabotaging codegen on AMD, ICPC still produces significantly better code on AMD. At least, that was the case when AMD and Intel chips were roughly on-par and some of the reviewers (like Tech Report and Anand) did optimized compile tests. In fact, I recall a few of those tests showed Microsoft's compiler outperforming Intel's compiler on Intel's own CPUs.
Thanks, downloaded it. Here's some (probably) stupid questions : Is it compatible with Eclipse? (Yes, I use eclipse) and how do I install it properly for Eclipse?
They're not *completely* equivalent, though. `&lt;%` and `{` are different operators that get parsed as different entities, but have the same meaning. However, `??&lt;` is parsed *in a preprocessing stage* and transformed into a literal `{` before the compiler gets a hold of it.
I was just about to protest that those kinds of keyboards were obsolete even before I started programming... And then I remembered that the Commodore 64 I grew up with didn't have braces.
Also, 'and', 'or' exist as keywords equivalent to '&amp;&amp;' and '||'. I tend to use them because they are extremely clear.
Nice, but I'm not sure how I'm suppose to see what's-new/what-made-the-cut in C++14 from that.
For those interested, Wikipedia has a section listing the new additions to the language (though I don't know how complete it is). http://en.wikipedia.org/wiki/C++14
Yes! This process has definitely become faster which is a nice thing.
It could be. I think they're also duplicating effort between IntelliSense and the actual compiler. Sometimes IntelliSense picks up wrong code correctly, but the compiler doesn't, so there are definitely differences between them.
It could also be that they just listen to large customers (like every other large company), and those customers have old codebases and simply don't care so much about the latest C++ features. Instead, they want something else, like more TFS features or whatever.
You're not, but clang maintains a [good table](http://clang.llvm.org/cxx_status.html) of their own.
Yes, but despite that ICPC still produces better code for AMD CPUs then every other compiler. (at least in terms of raw numerical performance).
It's certainly true for compilers and server OS's and HPC.
Technically IntelliSense is a third different compiler, made by a third-party and integrated by MS into Visual Studio. It's not Office's C++ compiler.
I wouldn't mind if they marked some really unused and outdated stuff as deprecated. Maybe they could be removed 6 years (two crafts) later.
Haven't MS been a C++ shop for much longer than that? Decades even?
Interesting. I suppose every compiler has extensions above and beyond what's required by the standard. &gt; .. TFS standards What's that? A google search brought up Team Foundation Server, but I don't know what that is either :-)
Even lowly vim doesn't make this mistake :-) I'm using vim+YouCompleteMe, which relies on clang to provide autocompletions.
Yeah, Team Foundation Server. It provides source control, automated builds and things like that. But I'm just hypothesizing, I'm not saying that's actually what they're focused on. 
They have, but there was that period where they said "C# is good for everything now!" and C++ appeared to be rather neglected. 3-4 years ago they started talking about a C++ renaissance.
Well, that can't be helpful when you're trying to add new features :)
There are a great many "soft RT" applications written in GCed languages. A GCed language might not be the best choice, but they certainly exist, and in some areas (like MMOGs, for instance) are even predominant. A simulation that starts to lag behind it's inputs becomes somewhat useless pretty quickly. It's like a weather prediction simulation that takes a week to give you a good estimate for tomorrow's weather. That's the "realtime" I am referring to, not "hard RT" like the control module that interfaces with physical components for something. And it's why I put realtime in quotes in the first place. It's not a straw-man at all in reference to soft RT systems.
Among other reasons, Jim Radigan makes an argument for why in the first part of his [GoingNative 2013 talk: Compiler++](http://channel9.msdn.com/Events/GoingNative/2013/Compilerpp-). &gt; In order to successfully deliver new and innovative C++ compiler technology on a regular basis, compiler vendors must respond to yearly microprocessor advancements. Additionally, the C++ language continues to evolve (and at a faster pace than ever before) with new standards such as C++11, C++14 , and C++17. Combined, the constant evolution of hardware, language, and tooling adds a great deal of complexity to an already complicated task: shipping innovative compiler technology at a predictable (and suitably fast) rate, while coming extremely close to providing "mission critical correctness." The key phrase there is the bit about "coming extremely close to providing 'mission critical correctness.'" Regardless of what Microsoft "promises", and they have recently been not-quite-promising fast standards conformance, I expect to continue to be disappointed with their compilers standards conformance. Edit: To be clear, I don't mean to be unjustly negative towards Microsoft about this. I think there was confusion about what they promised vs. actually delivered with C++11. There is some interesting reading to do about it here: [comments from hpsutter](http://www.reddit.com/user/hpsutter). I understand why he might not, but I hope we didn't scare off Herb from participating further on reddit. The easiest way to clear up confusion in the community is to communicate more often about what is happening. A real interesting conversation was under [this comment by slimshader](http://www.reddit.com/r/cpp/comments/1h9uq7/c1114_stl_features_fixes_and_breaking_changes_in/casj9sh). There is more interesting stuff in that thread, but that specific discussion is most relevant here.
Is there any website which mentions this as the final draft? It might be just an intermediate draft.
GStreamer?
WHAT!!! Had to test it out. For those that are still skeptical: #include &lt;iostream&gt; int main(void){ std::cout &lt;&lt; std::boolalpha &lt;&lt; (0 and 1) &lt;&lt; std::endl; std::cout &lt;&lt; std::boolalpha &lt;&lt; (1 and 1) &lt;&lt; std::endl; }
Reading that made my brain explode in the best of ways... 
The accompanying n3937 is labeled ISO DIS http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3937.pdf - that's what is going out for the ballot. Not sure if there's a site that's mentioning this directly yet.
https://github.com/cplusplus/draft/commits/master
That's the commit history. But thanks for trying. 
Prior to the preprocessing stage actually. The Tokenizing stage is when the trigraph magic happens. The very first pass.
I recommend everyone give CppCMS more than just an eye-brow raising. I've not used it for anything in production yet, but I was fairly impressed with the attention to things like CSRF, session handling and caching. Lots of people will go pale at the thought of writing a website/service in C++ (which is slightly irrational when you consider your browser is written in C++ and faces exactly the same exposure) but I trust [the code](http://sourceforge.net/p/cppcms/code/HEAD/tree/) written here by Artyom Beilis, the chap behind CppCMS. He is maintainer of Boost.Locale and works at the Tel-Aviv Stock Exchange. The code is very crisp, well commented, and high quality in a std::y style. Once you realise you can just go and pick up almost any library and use it in your web app, writing a webapp in C++ can begin to look quite advantageous.
Character set conversion is *a* preprocessing stage, is it not? It is typically part of the preprocessor program in a modular compiler framework as well, is it not?
Paraphrased from C++98 2.1, the seven phases are: 1. Trigraph and Universal character name conversion. 2. Backslash line splicing. 3. Conversion to preprocessing tokens. The Standard notes this is context dependent. 4. Preprocessing directives executed, macros expanded, #includes read and run through phases 1..4. 5. Conversion of source characters inside char and string literals to the execution character set. 6. String literal concatenation. 7. Conversion of preprocessing tokens to C++ tokens. Trigraph expansion occurs on the very first pass of the code before any other translation happens. Prior to the preprocessing stage, which doesn't happen until step 4.
FYI, `boolalpha` is "sticky", setting it once at the beginning of `main()` is sufficient. Additionally, `(void)` for zero parameters in C++ is known as "the abomination".
I originally worked in Office, now I work in VC, and I can tell you that this is completely incorrect. Office uses VC, just like Windows. They don't exactly use the same versions that we release publicly, though. We have internal processes to deliver new builds to Office, Windows, and others. There are mutants of VC lurking around - for example, VC targets only x86, x64, previously IA-64 and now ARM, while X360 obviously needed a PPC back-end. However, I am not aware of any front-ends beyond C1XX (and its C1XXAST mutant, and the deceased FEACP) and EDG, and I am in a position to hear about all C++ front-ends in the company. C++ FEs are works of majesty and terror, and are not created lightly.
Good to know. It appears I was misinformed about the Office schism.
I am really loving premake lately. VS support isn't so great, though. 
Meow. Seriously though, commenting on unreleased features and scheduling policy is above my pay grade. I can talk about the work I'm doing to some extent - I'll give you a freebie and tell you that if you look at [LWG 2344](http://cplusplus.github.io/LWG/lwg-defects.html#2344) you can guess what we've implemented - but I'm not especially eager to test the patience of my bosses and boss-like entities. I will say a few things: * The Nov 2013 CTP/alpha is a months-old snapshot of the compiler. * You haven't seen any library changes from us since 2013 RTM. * Microsoft should get credit for contributing to the Standardization process. In addition to sponsoring the Issaquah meeting last month, MS spends the money and much more critically the *time* to fly an increasing number of people (including me) to the meetings around the world and let them work on proposals and issues instead of whatever their day jobs normally are. I think that's pretty cool and according to my sense of justice, people and organizations should get credit for doing cool things.
Why is it considered the abomination?
That's Stroustrup's term, but the rationale is easy enough to explain: if you call `foo(11, 22)` and declare `foo(int, int)` for two parameters, and you call `foo()` for zero parameters, then you should declare `foo()`. The `(void)` abomination is a relic of C's prototype-less functions.
why?
I have used it. I'm a bit concerned about why the author ships a renamed version of boost along instead of using the user's one
I don't know much about C or C++ but I did find it entertaining, thanks!
&gt; Lots of people will go pale at the thought of writing a website/service in C++ (which is slightly irrational when you consider your browser is written in C++ and faces exactly the same exposure) I'll give you a break as you said _slightly_ irrational. That said, there are very good reasons to be more concerned when writing a service/server in C++. Exploit one browser, own one machine. Exploit a server/service, get access to a database + salted passwords and become a hub for hackers pushing exploits to every visitor to the site. It's nice to "pick up almost any library" - but unless you're using C++11 libraries (there aren't many yet) that library probably includes stack exploits and buffer overruns. For examples look at all the errors found over the years in things like PNG/JPG readers. It's nice to see more C++ stuff appear, for sure. I'm personally still of the opinion that protected/managed languages are more appropriate for the server side of the equation.
Only one overload got deprecated, the one that doesn't take a RNG by param and used std::rand. There's 2 other overloads that are fine to use.
&gt; think what you would have to change in your code base to make it testable. Would that change be "a good idea anyway"? That's the problem, they're changes I would never make except for testing. Which is why I don't like making them. &gt; unit tests try to battle the problems of too much code by writing more code. Yes, and lower quality untested code at that. I still think they offer some value, if I can find cheap ways to use them.
How likely is it, that anyone will vote no?
I just don't entirely buy in to the idea that webapps are principally more exposed than any other server software, often written in C++, just because they have to parse some HTTP headers and cookies... and thats the job of a library. I revere the task, but not any more so because its HTTP. I think, to a certain extent, we're cognitively biased to pump up the threat posed against server-side web stacks because [its where we see the most attacks](https://xkcd.com/1138/). That, and the state of the web dev industry, both in terms of development and the established web standards, is just so rotten. &gt; I'm personally still of the opinion that protected/managed languages are more appropriate for the server side of the equation. I'd perhaps agree if the dominant language for web dev wasn't PHP, which gives you the worst of both worlds. Most of the PHP bundled libraries are just very thin wrappers around existing C libraries, not clean implementations in PHP. You don't get much except safe strings from the wonderful managed veneer here. It doesn't really protect you from the gooey center, against bugs in any of [the C libraries PHP has sucked in to core](https://github.com/php/php-src/tree/master/ext). Not to mention the interpreter itself is written in some pretty intense C, full of macros like "Z_DELREF_P" (yup, that's reference counting.) So its managed, but is it safer than well written C++ on top of a modern minimal framework or application server? You might not trust yourself to write web safe C++, but aren't we just burying the rot anyway? What about Apache? Nginx? PostgreSQL? Maria? All C... is it the task we fear here or the history of implementation? Do I really need PHP or NodeJS for something that only talks to the server via WebSockets or JSON RPC when things seem to be gravitating client-side?
Pretty good summary. Thanks
I suppose we'll have to wait for the C++17 paper to get a significant std lib update (I can't wait for the standardize filesystem). By the way, is there anyone has some news on the Module project proposition ?
I've toyed around with it a bit. The one thing that really turned me off is the compiled template system that it uses. I may just be spoiled by the template system in Wt, but it feels incredibly inflexible in comparison. The only other gripe I have is a small one, to implement input sanitizing you need to write the checks twice (javascript for the front end and c++ for the backend).
&gt; The useful life of this standard will depend on the number of C++ features, currently in use or under development, included in the standard and on the compatibility with ANSI C. If the chosen features satisfy the requirements of modern software development, and if a sufficient level of compatibility with ANSI C is provided by the standard, its useful life is expected to be 7 to 10 years. Give or take a few decades.
filesystem will probably come as a Technical Specification this year. Still, if you "can't wait" you should be using Boost.Filesystem (which is available today).
Aren't modules the "big thing" in C++17?
There are many good reasons to do this - Boost itself created the bcp tool to support just such a use case. The basic problem this solves is binary compatability. If any boost appears in the interface or if any dynamic linking is used, then your system has to agree with CppCMS on which version of Boost to use, lest you have, eg. 1.32 member functions operating on 1.55 objects. I've hit this problem before - I wanted to use a newfangled library in the latest release, but my distro only shipped a Boost 3 versions old. I could install boost from source, but then what about all the packages that are only binary compatible with the boost the distro ships? I'd have to build all them from source too. So one strategy to solve this problem is to ship your own boost and put it into its own namespace. Then, the boost you are shipping with your project will be source- and binary-compatibile with the vanilla system boost by virtue of (from the point-of-view of the compiler) actually being a different library.
It's not just about saving your ass. There's another side of unit testing, and when it clicks I think that's what brings back most developers. Imagine you're building a web server. And to build a web-server you need to write an HTTP request parser. Do you want to have to build your entire product, launch it, and send it network traffic just to test your header parsing? I could not possibly imagine working like that in 2014.
OmnipotentEntity said it's not part of _the_ preprocessing stage, referring to the C++ spec's use of that term. I think you're the one arguing semantics about _a_ preprocessing stage. Anyway, I don't know about other implementations, but clang doesn't do character set conversion at all. The physical file is expected to be UTF-8 (though there are some considerations later on to allow for non-UTF-8 literals to be passed through unmodified, producing a warning rather than an error about invalid encodings.) Strictly speaking, it is entirely up to the implementation to map the physical file contents to a sequence of characters in the source charset. This is entirely outside the scope of the C++ standard. In fact C++ doesn't even require the use of plain text files, or even files. Your source code could be embedded in some kind of IDE project file, or stored in a database or something. All serious C++ implementations have used plain text files on the normal file system but that's not actually part of the standard.
I really suggest the Michael Feathers' book "Working Effectively with Legacy Code". It will cover all of the concepts used to make code (C++ specifically in the book) more testable. Things like dependency injection, mock includes, etc. There's almost always a better solution than an #ifdef UNIT_TEST, and the book covers many of them.
(Complete newbie here) &gt;to implement input sanitizing you need to write the checks twice What is the alternative way, where only one check is needed?
To be fair, there was a new C++98 standard just nine years after this proposal.
The only alternative I've used is Wt in c++ web space. It actually uses code generation to convert your c++ verifications into jquery based JavaScript checks. If you are using something like node.js you write everything in JavaScript so its just pointing everything at the same functions. For python, ruby, etc. I'm not sure but I suspect its implementation specific.
First of all, it's unfortunately not as clear cut; a dangling pointer/reference may lead to memory corruption which as hard as hell to track. Now, between a core-dump and a memory leak, I would pick: - the core-dump when it comes to debugging - the memory leak when it comes to operating a service with high-availability (with redundancy, a not too wild leak is just a matter of recycling the process once every X) The thing is: predictable trumps unpredictable from an operability stand-point. Of course, since I use C++, it's really like I have a choice anyway ;)
Boost.Filesystem is really great but some of its implementation are more oriented toward compactness and compatibility across platform than performance (they have some inner abstraction that allocate dynamic memory, something we shouldn't see in a native standard implementation and still have cross-platform code). 
I do hope so, it is the great feature that could resolve many problems, from linking dependancy and dynamic linking, static initialization, build speed,...
Nothing seemed controversial in Issaquah, but I don't pretend to understand how National Bodies think.
Hopefully, when Rust finally settles down on 1.0 version, we might have our cake and eat it too: fast &amp; secure.
&gt; First of all, it's unfortunately not as clear cut; a dangling pointer/reference may lead to memory corruption which as hard as hell to track. No it isn't. There are tools/libraries you can compile against which will trivially find those for you, if you already have sufficient code coverage in your tests. 
I look forward to the day when I'll be able to actually put angle brackets together `&gt;&gt;` without QTCreator wanting to indent the rest of my file by 40-60 spaces.
Thanks for the link, I hadn't heard anything about duetto before. It looks like it's chasing the same dragon as the 'Wt' framework but maybe with less of a widget focus. It may be worth looking at for comparison if you are going heavily down this route.
I should probably rename the article, I think it was too extreme of me to use that phrase - good point.
You should still rather use Boost.Filesystem and submit a bug report if that is the case since Boost.Filesystem is what is getting standarized as std::filesystem.
[FFmpeg](http://ffmpeg.org/)'s libraries might be useful.
Personally I like arrays. Yes, vectors are easier and faster to use in most situations. I guess its a preference thing.
Now if there was an overload of shuffle that omitted the URNG it would be perfect. We could really use this function in the standard and specify that it was called by things like the above: // Yeah Stephan, I know that you are not a fan of default_random_engine, I'm neither std::default_random_engine&amp; global_urng(){ // this is thread-safe: thread_local std::default_random_engine urng{std::random_device{}()}; return urng; }
The only real concern I have with an array is knowing how much stack space I'm going to have. If I can't fit it on the stack I'll use `std::vector` since you should avoid `new` unless you are writing library level code. ~~Not to say you couldn't use a `unique_ptr&lt;T[]&gt;`. Probably just habit at this point since `unique_ptr` is fairly new.~~ (see STL's reply)
I'm a big fan of `std::array`.
&gt; When you have an array that does not need resizing or anything like that it is just much simpler and easier to use arrays than vectors You're right, but it's even simpler and easier to use a `std::array`. That way you get convenient stuff like range-based iteration and optional bounds checking.
Moved to clang because of Visual Studio ridiculous C++11 support and QtCreator works great! Works well with CMake and also your project doesn't need to use Qt. Itś best cross-platform C++ IDE. Haven't tested the debugging features yet but the autocomplete and static analysis is great!
I like the C-style array when my program uses a big table of precomputed data. I make the array a private static const member of a class, and write a public static function (which checks array bounds) for access.
&gt; range-based iteration This does work on C arrays too, before they degrade to a pointer. The range-based `for` loop uses the free functions `begin` and `end` rather than methods.
I've heard its just an old form of std::vector edit: haha whoops I had heard wrong. 
It's a static version of std::vector, not old.
Nitpicks: arrays *decay* to pointers, and range-for automatically recognizes arrays without dragging in the Standard Library (that is, it doesn't use the `std::begin()` overload for arrays, although it did earlier in C++0x's evolution).
&gt; I did use Microsoft Visual Studios but was told by my teacher its almost like cheating when compiling. This being it fixes minor errors in your code I'd be very interested to find out what she means by that, but it sounds like [FUD](http://en.wikipedia.org/wiki/Fear,_uncertainty_and_doubt). Visual studio can make it much easier for newbies to get files compiled and runninig simply because they don't have to deal with learning C++ and whatever arcane switches they may need on the command line at the same time. I used to use VS for most of my stuff, but lately I've relied on GCC/MinGW and clang for just about everything as they've got much better C++11 support. If this is for school it's important to know what the result will be run through and that would be what I'd focus on, but it's also a good idea (especially while learning) to throw your code through multiple compilers with lots of warnings enabled to make sure issues are caught.
1.) Most compilers DO optimization. 2.) ssh is NOT a compiler. 3.) You can have a look at gcc or clang/llvm as alternatives.
You heard wrong. `std::array` was added in C++11 (and previously TR1), but `std::vector` was around in C++98 and before in the pre-standardization STL.
g++, clang, vim, sublime text, git. ssh in a nutshell is just a way to connect to other computers and use their commandline. Use the warning flags (-Wall) to "cheat" (just kidding, use them all the time). 
I'm constantly telling my boss I need arrays.
Depends on the OS, eclipse will just look in system directories when you let it generate makefiles (assumption, i do not use an IDE)
The biggest problems are security and program correctness - it's easy to allow for buffer overflows, or lose track of the length somehow. A std::vector is usually the correct option anyway, and it doesn't really have a lot of overhead. It also has the benefit of being a standard C++ container. If you absolutely must have a fixed size array, use a [std::array](http://en.cppreference.com/w/cpp/container/array). It has a small amount of memory overhead because it stores its length, but you probably are going to do that anyway. It also has a small amount of time overhead because it verifies that you access only within the array's bounds, but again you probably are going to do/should do that anyway. And it has the benefit of being a standard C++ container, which means it can be used with many of the stl functions for operation on collections + for each loops. EDIT: I ONLY PARTIALLY KNOW WHAT I'M TALKING ABOUT - see children.
Does C++14 have number separator? int num = 123'456'789; Edit: I thik yes: page 27, section 2.14.4, [lex.fcon]. [ Example: The literals 1.602’176’565e-19 and 1.602176565e-19 have the same value. — end example ]
Saying it's a static version of `std::vector` doesn't really explain what it is. It's just a templated wrapper around C style arrays that can be easily used with the standard library.
`std::array` doesn't store its length - that is a template parameter for it. You can't dynamically size an `std::array`, so storing the length would be superfluous.
I'd been wondering if anybody took up this mantle in D, thanks for the link.
The reason I choose vectors over raw arrays is because vectors are STL containers and can have their size determined at runtime. The STL container part because it means regardless of platform it's going to be thoroughly tested and guaranteed to be compatible with other STL features such as iterators and algorithms. The runtime sizeable part is important because I often do not know the size of data I want to store at compile time. At the moment the only intersection of those features is in std::vector. If I want to to use a raw array in an STL like fashion, I adopt the responsibility of maintaining such functionality. If I use std::array, I'm constrained to static buffer sizes introducing questions like "do I really want to iterate to the end of the container?". It is far easier to refrain from using push_back than write more code (and introduce more bugs)! If std::dynarray (runtime sizeable static length contiguous container) makes it in to the standard then perhaps I will use that when applicable instead.
I have not used it, but in [reference docs](http://www.webtoolkit.eu/wt/doc/reference/html/annotated.html) it sure does look like C++ (and in title). why are you saying it's not?
What's wrong with VS? AFAIK VS 2013 has great C++11 support. That said, it isn't cross platform.
But Qt does not yet support VS 2013 (maybe in nightly builds).
The future is now: https://i.imgur.com/hOpae1b.png
Do they still allow Microsoft-specific language extensions by default?
You could improve the speed further by putting the indirection (the function pointer call) outside of the inner loop. That is: template &lt;void (*impl)(const unsigned char* src,unsigned char* dest,int count)&gt; static void blit(const pixmap_t&amp; src,int x,int y,const pixmap_t&amp; dest) { for(int i=0; i&lt;src.h(); i++) { const unsigned char* s = src.pixel(0,i); unsigned char* d = dest.pixel(x,y+i); impl(s,d,src.w()); } } static const auto all_blit[4*4] = { blit&lt; scanline_copy &gt;, NULL, blit&lt; scanline_conv&lt;1,3&gt; &gt;, blit&lt; scanline_conv&lt;1,4&gt; &gt;, NULL, NULL, NULL, NULL, blit&lt; scanline_conv&lt;3,1&gt; &gt;, NULL, blit&lt; scanline_copy &gt;, blit&lt; scanline_conv&lt;3,4&gt; &gt;, blit&lt; scanline_conv&lt;4,1&gt; &gt;, NULL, blit&lt; scanline_conv&lt;4,3&gt; &gt;, blit&lt; scanline_copy &gt;, }; void blit(const pixmap_t&amp; src,int x,int y,const pixmap_t&amp; dest) { assert_always(src.fmt() &gt; 0 &amp;&amp; src.fmt() &lt;= 4); assert_always(dest.fmt() &gt; 0 &amp;&amp; dest.fmt() &lt;= 4); assert_always((x &gt;= 0) &amp;&amp; (src.w() &gt;= 0) &amp;&amp; (x+src.w() &lt;= dest.w())); assert_always((y &gt;= 0) &amp;&amp; (src.h() &gt;= 0) &amp;&amp; (y+src.h() &lt;= dest.h())); auto impl = all_blit[(src.fmt()-1)*4+dest.fmt()-1]; assert_always(impl); impl(src, x, y, dest); }
Many C++11 features are not yet or only partially supported in VS 2013, see [this post on cpprocks.com](http://cpprocks.com/c1114-compiler-and-library-shootout/) for a complete table of supported features. EDIT: Aside from that, I also found the out of the box C++ code completion of VS to be lacking compared to QtCreator, but I haven't tried 2013 yet, so maybe that has improved. 
have fun declaring &gt;1d array it is &lt; &gt; overload
KDevelop has also released experimental support for this [last month](http://milianw.de/blog/katekdevelop-sprint-2014-let-there-be-clang).
your post should be deleted as clearly not constructive == SPAM!
5.2 builds fine for VS 2013. Plus then you don't get the extra bloat of ICU if you don't need it.
Great, I'm going to test that!
You should be aware that Juce is released under a dual commercial/GPL license. Unless you're willing to pay for it, you may not use it in a closed-source project. That aside, I think it's a very good library collection
could you legally use clang for front-end ? disregarding business decisions, from legal pow? and i know clang doesnt come with redsquiggles functionality "command line option"(it not same parsing code that is being written and contains error 234 lines above as compiling an .cpp) but afaik it is designed to make making tooling for cpp easy
Off by 1 errors with arrays are so common, even for experienced programmers.
I've seen the following code before in production: char buffer[1024]; You should be cringing right now. This just put a 1K buffer **on the stack.** Spent a good 2 days finding all of the instances where they did this and changing it to a dynamically allocated array, but really it should have been: std::vector&lt;char&gt; buffer(1024);
out of the box maybe, but VS + VAX has yet to find its equal.
&gt; It also has a small amount of time overhead because it verifies that you access only within the array's bounds Errrr, no it doesn't `operator[]` does no bounds checking just like `vector` and `deque`; it would cripple perf' if it did. It has `.at()` which does. You shouldn't really need bounds checking for `std::array` because iterating over it should be done with iterators (perhaps indirectly with range based for) or using .size(), it's not like the arrays size is ever going to change. * http://en.cppreference.com/w/cpp/container/array/operator_at * http://en.cppreference.com/w/cpp/container/array/at
My unsarcastic thanks for nitpicking. It's really helpful for me to learn the correct terminology. (I know you probably know this) A helpful, if somewhat esoteric thing you can do to prevent array decay in parameters is to use an array "reference" as a parameter instead of a pointe. I'm a bit rusty on the syntax, but it's reminiscent of the way a function pointer is declared. It means that only an array of a single specified size may be passed to a function. Using array "references" is really useful for catching logic errors at compile time, such as when arrays of the wrong size are being passed to a function expecting something else, in the fashion of contract-driven programming. 
A way to preserve this functionality and still use classic arrays is to use array references in parameters. A minor correction: Static array size is tracked, but only at compile time. The biggest issues with arrays in c++ are when "new" or "malloc" are used. Buffer overflows are also much harder for an OS to protect against when it's done in the heap. If you're using dynamic memory and don't have extreme performance constraints, just use a library. I like the STL but I know there are some who don't like it.
namely iterator semantics 
I keep looking for improvements to Qt Quick Designer but nothing much is happening there. Despite all the bugs, Microsoft's Expression Blend is pretty darn complete for WPF and QT really needs a similar tool. 
Compiler generated move constructor is missing for example.
Oh, but here you are talking about a reproducer. Most memory corruption I have seen happened, once in a long while, due to an obscure data-race. And of course, whenever executed with instrumented code (Valgrind, ASan), they would never show up because the instrumentation messed up the timing (or simply because the issue occurred with such a low probability to start with).
Ah, well, yes, those tools can mess up scheduling. You are right about that. I don't use threads any more. Ever. Everything is async., non-blocking, single-threaded, message passing processes. I've found this solves two problems for me at once: engineers who simply cannot think their way around threading issues while writing code, and indeterminacy (is that a word?) It's much easier to test async. code than it is to test code that involves threads. 
I hope you verified that this pattern was actually causing problems before spending two days making that change which added no business logic or functionality. In 2014, adding 1K to the stack isn't necessarily an issue - for example, on Visual Studio the default stack size is 1M, and you can increase that - moreover, allocating memory on the stack is significantly faster than allocation on the heap. Heck, if you're writing an interrupt service routine, it may be that heap allocation is forbidden, depending on the platform. I do agree that stack memory is a limited resource and you shouldn't mindlessly waste it, but programmer time is also a limited resource.
No it is not: http://i.imgur.com/JQrtwtx.png Edit: More documented example: http://i.imgur.com/53wpsta.png 
Throw this in a header somewhere: template &lt;typename T, std::size_t...Extents&gt; struct multi_array_ { using type = T; }; template &lt;typename T, std::size_t...Extents&gt; using multi_array = typename multi_array_&lt;T, Extents...&gt;::type; template &lt;typename T, std::size_t First, std::size_t...Rest&gt; struct multi_array_&lt;T, First, Rest...&gt; { using type = std::array&lt;multi_array&lt;T, Rest...&gt;, First&gt;; }; and then you can simply declare e.g. `multi_array&lt;int, 2, 3, 4&gt; bob;` ([Live demo at Coliru](http://coliru.stacked-crooked.com/a/78fe9281b050923d))
Ah sorry I was thinking it would have the same behavior as an array on the stack for some reason where you could simply divide the sizeofarray by sizeofelement.
Here's one reason. Let's say you write a function that should only work on arrays with 5 members: void foo(int x[5]) { cout &lt;&lt; x[4] &lt;&lt; endl; // This is safe, right? } And you have the following code in `main`: int main() { int a[3]; // This only has three elements foo(a); // So this won't compile, right? } You might think that latter won't compile. You might think that type of `a` is different from the type of the parameter of `foo`, and therefore it will fail to compile. But you're wrong. The compiler will laugh at your parameter type and just let any old array bind to it. (*Update: See the responses to this comment for more on how you should not have arrays in your parameter types - unless taken by reference or some other "indirect" route.) If, instead, you take arrays *by reference* instead of by *pointer decay*, then things become a little safer. The following will only accept int arrays with exactly 5 members: foo( int (&amp;x)[5] ); You can even use template deduction to learn the length of the array at compile-time: template&lt;size_t N&gt; foo( int (&amp;x)[N] ); Also, it's not directly possible to pass an array by value. There are some easy workarounds (wrapping it inside a `struct`), but that just leads to slightly messy code. All these problems are solved by `std:: array`. Or, of course, `vector` if you want to be able to resize at runtime.
Damn... MSVC is really lagging.
You can always buy the next version ... ;)
... raw arrays can also have their size determined at runtime
(sorry for the late reply) I am wary of "test-centric design", I do think that unit tests are just one tool in the belt, and testability is just one requirement of many. Still, for many things testability and good design are well aligned. For some things, writing tests is insanely easy - and I found that even insanely simple tests find a good number of bugs that can hide for a long time in "real" code. E.g. one thing I noted when starting to write test seriously was *getters*: if a class had configurable state, I'd add getters for the state only when needed, and often I would write setters that would take multiple (related) config parameters, or that were interdependent. Writing tests has forced me to write classes where all state that affects its functionality is actually discoverable from the outside, and that could be configured either once (in the constructor or a builder), or as often and in any order as you like, making them simpler to consume as a whole. 
I would guess it's because they wanted to be able to use range-for on arrays without #including an additional header file.
I cannot comment on legal matters. I'm a language lawyer, not that kind of lawyer.
&gt; the supposed "code bloat" is not something I've actually ever had as an issue in many years of C++ Yeah, that's a myth (especially with /OPT:REF,ICF and similar functionality). My position allows me to see lots of people running into lots of trouble with the language and libraries, and not once have I ever seen "template code bloat" be a thing. (People do complain about RTTI bloat, which is different.)
Yep. The Library *provides* `std::begin()` for arrays, and it's useful for users writing generic code, but the Core Language doesn't need this to recognize arrays.
Vectors are way better than arrays, but you should be aware that STL algorithms work perfectly with built-in arrays as long as you pass correct bounds. (C++11 `std::begin()/end()` for arrays can help.) That's because raw pointers satisfy all the requirements of random-access iterators.
&gt;Unit tests are a high priced sort of test that only achieves some testing goals. Unit tests are my cheapest tests. They are so easy to write and cheap to run, I can't possibly think of a cheaper form of testing. They also catch the most bugs of any form of testing I use, and I write some pretty large and critical software for high performance financial trading applications.
Isn't Quick Designer for Widgets primarily? And widgets are taking a back seat in &gt;= 5.0 releases to make room for QML? 
Maybe they'll finally do C99 support.
Yes and no. This is impossible (though it GCC has an extension that allows it and it will be allowed in C++14): size_t n =3; int arr[n]; The thing that is allowed is the following: size_t n = 4; int* arr = new int[n]; delete[] n; So: If you want runtime-bounds you have to manage these ressources explicitly. Let's see vector: size_t n=4; std::vector&lt;int&gt; vec(n); This is definitely cleaner code. It has also a much clearer syntax, since the type is completely on the left side and not on both sides.
Considering that they dropped the C++ front end on the floor for almost a decade when they decided that managed languages were the future, it's really a miracle that they are only 2-3 years behind now.
But is not STL which means I have a chance to introduce bugs writing my own code.
Sorry for not clarifying that, personal names.
[ld supports](http://sourceware.org/binutils/docs-2.24/ld/Options.html#Options) `--gc-sections` (except on Windows), and [gold supports](http://stackoverflow.com/questions/15168924/gcc-clang-merging-functions-with-identical-instructions-comdat-folding) `--icf=safe` (except that gold doesn't work on Windows at all).
Use &lt;regex&gt;. Regexes are easy, but you should be aware that [names are hard](http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/).
The opposite actually, Quick Designer is their QML design tool, QT Designer is their old widget based tool. 
I really wish I could upvote this more than once
Currently I have a .dat database of about 50k english names. Just trying to figure out how to implement it. I am only in AP CS so this is all a learning experience. For what I am building I need something that works 99% of the time.
You could just use `std::unordered_set&lt;string&gt;` and dump a large list name into that then check each word, but this might not fly on a mobile device. Assuming average 8 byte names then 8 * 50 000 = 400 000 B 400000 / 1024 / 1024 = 0.3 MB, so maybe not that bad on memory. CPU usage may not be as good.
I agree with your general position, but what makes you think getters as you've described are a good thing?
Good question. it's something that feels "intuitively right" - but it took me a while to think about it. State that affects behavior (like "the file this logger writes to", or "number of iterations of the password hasher") can be used for * diagnostics * creating a modified copy, * persistence / remebering defaults * Handling exceptional cases locally (such as "skip details when logging to a network drive") Especially the latter ties in with a more fundamental principle: *prefer nonfriend non-member over member functions* - i.e. everything that can be implemented as a separate entity should not become part of the core class - this helps stick to the Single Responsibility principle or at least prevents interface bloat. Furthermore getters are *harmless* - there's "nothing to abuse". Conceptual complexity of the class remains the same - all documentation is already done with the description of the class' behavior. Does that make some sense?
Yes, I see your argument but it's opposed by the concepts of encapsulation and minimalism. I'm a moderate follower of the latter, and a mild follower of the former. I've certainly written classes with internal details I didn't want to share, and most of my classes have a minimal interface to reduce confusion and code. Overall I'd rather use friend test-classes like another poster suggested, just out of my own preferences.
It's not about sharing implementation details (e.g. you'd expose the log file path of a logger, but not the file system handle). It's about things you can set anytime (exposed anyway), or things that you've set when constructing the object. I'd argue (strongly) that such getters, getters for *state that affects behavior* (in a documented way, strange cases excluded, etc. pp.) do not break encapsulation - the things they expose are things you need to know to use it correctly anyway. As for minimalism: The *signature* of an entity is "everyhting the compiler understand" (acess modifiers, member names, parameters types etc.) The *interface* of that entity is the *signature plus usage instructions*, everything you need to understand to use it correctly. They may be derived from good naming, from project standards or from particular documentation. My argument is that while the signature increases (there are additional methods), but the *interface* as a whole, conceptually, does not: there is no added complexity, nothing "new to understand". --- In my understanding, friend test classes are rarely necessary. You want to test *documented behavior* - i.e. the interface. I explicitely do not want to test implementation details (with one exception, see below). I wantto test what the *user* of my class - be that me or someone else - sees. --- As for implementation details - it's a separate topic - but there's a particular place where I've started to expose them: statistics for performance optimizations. e.g. if a class internally employs an LRU cache, I would expose statistics like size and cache hits/misses, purely for diagnostic purposes, to make it verifyable the cache "works as intended". 
I don't know if they changed their stance in the meantime, but this is what Herb Sutter said around May 2012: "**We do not plan to support ISO C features that are not part of either C90 or ISO C++.** I understand C programmers may be disappointed or angry with this answer and I’m sorry to have to say no here. It’s true, and very quotable, that "focus means saying no," but that doesn’t make it easy to say — it is hard to say no to you, and I’m sorry to say it. But we have to choose a focus, and our focus is to implement (the standard) and innovate (with extensions like everyone but which we also contribute for potential standardization) in C++." [emphasis mine] [source] (http://herbsutter.com/2012/05/03/reader-qa-what-about-vc-and-c99/) If I understand this correctly, this means that they won't do more than they have to in order to be c++11 compliant.
Ahhh right! That's what I get for reading this at 2AM, entirely skipped the 'quick' in the name. Thanks.
I don't think this is easily solved in the general case because people have names such as Taylor, Grace, or even John, which are both words and names. You would be better of trying to find a capitalized word in the middle of a sentence and assume that's a name. Even then though.
Herb Sutter said in a talk where they announced C++11 for VS2013 that they would be supporting C99, or at least an important subset of it. I remember him talking about _Bool (I think that's what the standard named it anyway, I'm don't do C). [I think it's here somewhere](http://channel9.msdn.com/Events/Build/2013/2-306)
This is easily the best way suggested so far. Just bang them all in an unordered_set, and check whether each word exists in the set (use `us.count(word)`). Memory should be too bad, and look up will be O(1).
Right, this is the product that told users that certain library calls from the standard were "deprecated." The committee was not happy about that and the fallout from those function lists still haunts dogmatic corporate security policies.
This is a perfect example of why we need punctuation for sarcasm.
&gt; or at least an important subset of it And of course that library you'd like to use to save days of work will require a C99 feature that's not deemed important.
Good old Poe's Law.
You can always use std::vector + std::binary_search, can't you? Anyway you can perhaps detect a good amount of names in English... good luck in other languages.
Throw a SQL connection at it is always a very efficient solution. 
Before premature optimization, have you tried just going through your list of names and using an efficient string search algorithm such as Boyer-Moore? 
Well iOS is a memory constrained environment, and it has SQLite bindings as part of the platform. Sticking it in a DB may be a more scalable solution that keeping a dictionary in memory.
Isn't that an issue of mixing responsibility though? If you build your application on boost 1.55 and a distrib only supports 1.52, then it is the distrib maintainers' job to bring boost up to date. For all you know, they might have very good reasons not to want certain packages on their lands, and smuggling them is probably not the solution. Another solution I don't recommend would be to downgrade your version of boost to fit the distro's
Look for an [entity recognition](http://en.wikipedia.org/wiki/Named_Entity_Recognition) library. Unfortunately, most of the open source ones are Java. As for commercial ones, there was Inxight Thing Finder SDK with a C++ API, but I don't think it is available for sale any more.
This is an instance of Named Entity Recognition. One of the best ready models to use this is [Stanford Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml), which is based on Conditional Random Fields. If you need a more complex solution than the others mentioned here, you could look at their code, and read their paper (linked [here](http://nlp.stanford.edu/software/CRF-NER.shtml#Models)), but you would need annotated training data, or implement the Stanford models precisely, so you can load their premade models. There's of course other NER programs, for C++ there's [Afner](http://sourceforge.net/projects/afner/), though I have no idea how good it is. At least you can freely download the source and it comes with a set of (english) models. It will try to tag entities other than persons as well, but presumably you could ignore this. [edit]: just noticed that you want to do this on mobile, which means the above methods will be too cpu and memory intensive probably.
I posted a reply as well, but after reading this I'm not sure if it's relevant. When you say &gt; For what I am building I need something that works 99% of the time. How do you define 'works 99% of the time'? Do you mean that it finds 99% of the names in the text? Do you mean that 99% of the items it finds are names? Either one is outside of the reach of even advanced techniques. If you mean, I need to recognise 99% of the instances of my name list, it might be easier or more difficult depending on what you count as an instance.. If you have a set of full names (e.g. John Smith), do you want to recognise partial names as well (Mr. Smith, president Smith, etc)? It might be useful to narrow down exactly what you need..
Even in C, I cannot see much reason for allowing it any more. Given that foo(int x[5]); is indistinguishable from foo(int * const x) in every way, and the former is a little misleading, all compilers (C and C++) could issue warnings for the former. (Maybe already an option?) And perhaps the standards bodies should formally deprecate the former? Perhaps we could allow an array of unknown extent, foo(int x[]); Finally, C may not have references, but we can still take the address of the array safely: foo(int (*x)[5]); int a[3]; foo(&amp;a); // will fail, as desired, to due a type mismatch
FWIW I haven't encountered any C99 libraries that can't be compiled with VC++ 2013 due to missing compiler features (a few have require minor tweaks to work around compiler bugs). Not having POSIX headers is still a problem and a lot of build systems need a lot of "persuading" to use cl.exe, but those are distinct issues from not supporting C99. The not-interesting, not-C++ parts of C99 that VC++ doesn't support are mostly things like tgmath.h, which I've never actually seen used.
It would be easier to accept if support for those C99 and C11 features that _are_ part of C++ came a little faster. Even after years there are still a bunch of C++11 functions that Visual Studio doesn't implement or implements only under different names: nextafter, asinh, etc.
One thing I'd like to add is that the names are coming from a business card, which means they are usually within the first four words that I get from OCR. 
One thing I'd like to add is that the names are coming from a business card, which means they are usually within the first four words that I get from OCR. 
Why would you trade O(1) lookup for O(logn)? That makes no sense.
subscription only?
So, should we be calling constructors with uniform initialization? 
WHAAAT. I've learned crazy c++ stuff like TMP but I've never once heard this. Is it supported by the common modern compilers? I would so use this every day if it's cross platform.
That is what I've tried before however I am still looking into it. Thanks for the code.
It's not really "arrays" but mostly "manual memory management" that is shunned. RAII is typically superior in that it's harder to mess up. That and raw arrays being stupid (no index bounds checking). A decent standard library implementation comes with a debug version of std::array and std::vector that does this kind of checking one way or the other. Good for catching errors early.
Clang's macho linker has -Wl,-dead_strip, which is similar to /OPT:REF. I don't think there's a /OPT:ICF equivalent.
I've had templates solve "code bloat", if that counts. Switching from runtime polymorphism to static polymorphism nontrivially reduced the executable size due to that the code had a million tiny functions which got inlined out of existence once they were no longer virtual.
I really liked your post, and up-voted it, but for the record: &gt;A friendly tip: if millions of people are using something successfully it can't by any stretch of the imagination be considered wrong. is such terrible logic. 
If you're on a UNIX like system you can preload your own library which wraps malloc/free. For example [libumem.so](http://en.wikipedia.org/wiki/Libumem) does this on solaris to track memory usage. You can do something similar to maintain a list of allocated blocks per user.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Libumem**](http://en.wikipedia.org/wiki/Libumem): [](#sfw) --- &gt; &gt;**Libumem** is a library used to detect memory management bugs in applications. It is based on the [Slab allocator](http://en.wikipedia.org/wiki/Slab_allocator) concept. Libumem is available as a standard part of the [Solaris Operating System](http://en.wikipedia.org/wiki/Solaris_Operating_System) from Solaris 9 Update 3 onwards. &gt; --- ^Interesting: [^Memory ^debugger](http://en.wikipedia.org/wiki/Memory_debugger) ^| [^Hexspeak](http://en.wikipedia.org/wiki/Hexspeak) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cfx0za4) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cfx0za4)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
That's because that book is mostly likely refering to `std::map` and `std::set` which are based on red-black trees. Therefore all their operations are O(logn). The solution you propsed is basically the same thing as std::map just in a contiguous container, hence the better performance. I'm speaking about about `unordered` versions which only have to compute a hash to index into an array. 
ok, i guess i just assume ppl that arent me know the diffs in the licenses, since i totally have no idea about all the little lic things ppl seem to have opinions and stuff about :P 
I know, but it seems idiotic that you must do that... I wish std::array would be usable with multiple dimensions as template parameter(lets say standard covers up to 10 dims (or whatever those string theory ppl need :P ) other solutions are here: (i think one is same as yours, but my TMP diff skills are horribad ) http://stackoverflow.com/questions/7689288/less-verbose-way-to-declare-multidimensional-stdarray
this makes little sense to me. If there's an exploit in libjpeg that's written in c, how does a wrapper for it in a managed language make it safe? 
Those C99 Standard Library functions (incorporated into C++11 by reference) shipped in 2013 RTM. We've discovered a couple of missing ones and a couple of bugs since then, which we've fixed for the next major version, but the bulk of them are there - plus C++ wrapper header support (that's &lt;cmeow&gt;).
It's a major piece of cake to "port" std::array to c++03. Comment out 2 functions as i recall.
I'll be a contrarian and say a vector of pointers to functions returning status or data is more than sufficient comoared to some overengineered test framework. We return all error strings from functions so empty return values indicate success. Each class gets an executable (that seems to be visual studio hostile but posix friendly and cross platform). We have sample stubs we copy and fill in for each new unit test set. Works fine for simulations as well.
Is this tool still around? I have hear this compiler set is no longer updated lol 
[Sneaky sneaky](http://imgur.com/v2JDNtZ)
Updated link: http://www.altdevblogaday.com/2014/03/05/implementing-a-code-generator-with-libclang/
Ok, I see your point now. I would say that managed or unmanaged is a little bit irrelevant. Certainly a whole class of bugs is avoided (memory related) by using a managed language, but ill-conceived logic can be done in any language.
He mentions that at the end: &gt; We hoisted the loop itself into template functions But it's nice to see an actual implementation.
I was not even aware of this bizarre rule; now this is really bizarre: int a[] = { 1, 2, 3 }; // a is of type int[3] auto a = { 1, 2, 3 }; // a is of type std::initializer_list&lt;int&gt; Thanks for the mess...
It's quite unfortunate though that using a reference to array require a template function: void processArray(int (&amp;array)[size]) { for (size_t i = 0; i &lt; size; ++i) { std::cout &lt;&lt; array[i] &lt;&lt; ' '; } } seems like such a useful syntax... if admittedly awkward.
OT: wow that is a dumb career move if I ever saw one... ontopic on VS bugs: "your" to_string() fails miserably (gives back 5k size string) on Inf FP in VS 2012. I guess you know about that, maybe it is even fixed in 2013... :) but tbh that is one horrible QA, if I tested to_string one of the first 10 tests would be Inf Nan on FPs...
&gt; This is universally considered to be bad style among people who know how this works. Sometimes I wonder if the C and C++ committee are mocking us with all those awkward rules. If it does not work, let's admit it and have the compiler reject the construct instead of silently *doing the wrong thing*. I am quite sick of the mess that is made of those two languages, I do agree that backward compatibility is great and all, but is it worth the mess we are in now ? If by cleaning the languages we unearth those oddities in old programs, isn't it a chance to discover potential issues ? And if you don't have the money to fix those programs... then you can just use an older compiler!
I actually finished overhauling to_string() two days ago. In the next major version it'll be completely conformant (in conjunction with CRT fixes).
(also known as replacing textual inclusion with symbolic inclusion)
&gt; *Vittorio Romeo said* : &gt; I wish `{}` and `()` didn't have different meanings when using `auto`, and I wish there was something like `auto x = make_initializer_list(...); I agree with both N3681, Scott, and Vittorio. With Scott raising this issue I guess we will get some committee Eyeballs on the matter. (Not a big issue, but one less pitfall is always good)
Both forms of the initializers suck: Contrary to popular believe `foo(bar)` is a c-style-cast for some types (like integers) and `foo{bar}` get's ugly if you have multiple ctors of which one takes a `std::initializer_list`, bonus-points if this happens in a template. Personally I prefer the new syntax.
Thanks!
Im sure valve is large enought that it uses vim, emacs, visual studio and eclipse as well
hmm color me confused, i thought to_string is a simple wrapper around C sprintf, aka nothing special for you(std::) to do... except make sure buffer is big enough to store result... but I guess that is why you mention C runtime. :) but what I find fascinating that you returned 5008 char string, and ofc before you give .c_str() to C functions you must resize it... So it is really unbelievable that bug was made... again ofc I might be misunderstanding things or implementation, but afaik idk how this was possible... I can understand CRT messing up conversion and running over buffer bounds but how both CRT gave wrong result and std managed to resize str to that huge size? do you have some pretend sprintf function that returns len (without really writing anything to a buffer) to you so you can call resize before calling it for real or what? 
am I wrong to think that in *some* cases of threading (even in case when behavior is defined aka program is DRF) shared_ptr refcounting is not deterministic, for example when one thread has weak ptr and tries to lock() while other thread decrements ref_cnt. for example on program shutdown one thread is killing managed object, the other one is trying to call .lock to send some data to it...
went to amazon for a preview... page 34: using unsigned ints - entire panel on GN 2013 disagreed with this using double instead of floats - alexandrescu disagrees with this: his tip is use 64b code and 32b data. 
This is nice. What's the timeline for modules being standardized for C++? I would trade everything I've seen in C++11 (lambdas, `auto`, `constexpr`, move semantics, ...) for faster compilation times. I never look forward to compiling anything from source. Going from O(MN) (headers * translations units) to O(M+N) (modules + translation units) is a huge improvement for large projects.
Getting the buffer size right was surprisingly obnoxious. We initially used fixed-size buffers, which were too small for big floats. Then we switched to iostreams for floats, which gave me a bad feeling at the time. When actual badness was reported, that's when I ripped everything out and used the CRT again (now with additional functions to request the correct buffer size). The "pretend" function in the Standard is `snprintf()`, which is exceedingly useful, but there is no wide counterpart (and there is no justification for this, it's simply a mistake). Fortunately VC has a non-Standard `_scprintf()` with a wide counterpart that's available for use.
I'll do it for you. c/c++ is a dirty beast but its a dirty beast for a reason. The standard library has its uses but many who use it seem to have the attitude of "its my way or the highway." Silly silly silly.
http://www.boost.org/doc/libs/1_55_0/libs/smart_ptr/intrusive_ptr.html ?
I would not trade everything in C++11 for modules, because my incremental build system works well, however I do wish for faster compilation times too. It seems there is a lot to be done though, between making it work for C++ *and* writing a proposal for Standard inclusion *and* getting the proposal accepted...
I thought that the Clang approach was very well thought-out, so I am surprised to see they actually managed to improve it by incorporating the revisions of a feature into it. I certainly hope that compilers vendors will jump on the bandwagon regarding those feature tests because it will ease significantly the life of people willing to propose an enhanced experience to users lucky enough to have a up-to-date compiler while not excluding the poor chaps who do not.
so this is usable in C already?
&gt; I know I could use shared pointers to store the value, but my way keeps me from having to dereference a pointer every time the value is needed. Seems like a premature optimization to me: the cost of deferencing a pointer is tiny. Is it really worth the cost of writing your own class and quite likely introducing bugs that you wouldn't have with a `shared_ptr`? I hope that at least you benchmarked your code against `shared_ptr`. My theory is that, given that calls to the Windows desktop API are generally somewhat expensive, you wouldn't be able to tell the difference.
That link starts: &gt; The intrusive_ptr class template stores a pointer to an object with an embedded reference count. This is not what he's asking for: &gt; What I'd really like to know is if anything like this already exists that **doesn't use pointers to store values**
In practice, this looks like it just means more work to do. If precompiled headers were made automatic and #pragma once made the default (so you’d have to explicitly mark when you want to include the same file multiple times instead), that would reduce work and address what is for me the largest practical parts of the issues.
I am guessing 2020 until all major compilers across desktop, server and embedded targets fully support it.
Yes, and in Objective-C. There is a caveat, however: Due to the way modules work, libraries have to be modularized from the bottom up. That means a library's dependencies need to be modularized before the dependent library can be modularized. With Xcode 5 Apple added module maps for many of the system libraries. At the moment that's the only environment where this is practical for real projects. 
Ugh. You're right. Now that I've read that statement out loud, I'm a little angry at myself for even writing it. I honestly don't know why I said that. Dereferencing a pointer is a non issue. The issue I'm trying to avoid is having two or more of the same handles exist at any one time during execution. I'm not sure shared pointers would be any benefit here because I need to compare *values*; not addresses. The obvious "solution" is a process scoped lookup table containing all the handles, but that *is* overkill for what I'm trying to accomplish.
Precompiled headers _can't_ really be made automatic: Unless you use the same `#include` order every single time then the header must be recompiled. So automatic header precompilation would still be recompiling the header many times and you wouldn't get significant savings. In fact it might even slow things down, because the automatic mechanism would have to inspect the translation unit to see if some existing precompiled header could be used, it would usually see that none apply and then proceed to compile a new precompiled header. `#pragma once` does not solve this issue.
That's only if you use it to initialize tons of global state and to introduce hidden dependencies. Script binding or reflection boilerplate should not cause such problems for instance.
&gt; The intended use case is to allow multiline initialization of static members of a class. ComplexType static_member = []() { ComplexType ret(42); ret.foobar = 43; return ret; }();
That was referring to Java (updated the post to reflect that). That said, have you tried to compile that?
O_o If precompiled headers are worthwhile when I set them up manually, they would be useful when automated. There are multiple issues listed, #pragma once addresses at least part of one of them, and—as I said—in my experience that would be more useful to me than this proposal.
&gt; That was referring to Java (updated the post to reflect that). Okay. &gt; That said, have you tried to compile that? Compile lambdas? https://github.com/vinipsmaker/tufao/blob/0558a6c8d1d305dd270743623e90461da8f08493/src/classhandlermanager.cpp#L53
No, I thought you meant that initialization in a header.
&gt; Getting the buffer size right was surprisingly obnoxious. We initially used fixed-size buffers, which were too small for big floats. well if you care about perf you could put a runtime check (for FP, maybe for integers also for 32b target) if you can prove result is under SSO limit or not, if not then allocate max possible amount, i dont think small memory overhead will slow you down more than determining length first... or maybe you now use stack buffer with max size always and then pass buff to string ctor... :D anyway cool story, sometimes I think STL lib dev life is boring with all those SFINAE overload if magic :P , but you get to visit a lot of cool corners, from threading, atomics :/ , rng, to_string :), also containers, algs are mostly boring except perf optimized versions and ofc min_max :D P.S Im reporting you to Andrei for using iostreams :P 
&gt; Initialize &amp;nbsp; &gt; Header Choose one. ;-)
&gt; my incremental build system works well Yeah, it's fine for small projects. And it's fine *after* the initial compile. But working on larger projects isn't nice without a good computer. And even with a good computer it can be a pain. Some anecdotal compile times for large projects: * [Here](http://www.quora.com/Mozilla-Firefox/How-long-does-it-take-to-compile-Mozilla-Firefox-from-scratch?share=1) a guy reports a 24 minute initial compile for Firefox, and 30 seconds for incremental builds. * On Windows, [people report](https://groups.google.com/a/chromium.org/forum/#!topic/chromium-dev/c16gosC3wm8) 2-4 minute compile times after editing a single file. (They solve this by using a multi-dll build to avoid a bunch of compilation by using dynamic libs instead.) * There are some reports [here (circa 2007)](http://ubuntuforums.org/showthread.php?t=650461) of compile times of the Linux kernel. The actual code compiled and machines vary, but on better machines compile times vary from 10 to 50 minutes (and the Linux kernel is all C, isn't it?) * Windows takes something like [12 hours to compile and link](http://stackoverflow.com/a/226566). Compilation times are a huge problem. Most languages that aren't C++ do not have such long compilation times. People at Google decided to write Go partly because C++ took so long to compile. There are build tools like `ninja` and `jom` and `ccache` whose only purpose is to build your code faster. Yeah, incremental builds sort of solve some of this, but even 10 second incremental builds seem to take forever in everyday development. Will modules solve all of these problems? [Probably not](http://www.drdobbs.com/cpp/c-compilation-speed/228701711). But modules should help, and I would like to see some more serious thought given to speeding up compilation in future standards.
Will you publish your work in the future? Sounds nice :)
&gt; If precompiled headers are worthwhile when I set them up manually, they would be useful when automated. Use something like premake to automate them and you'll get most of the benefit there. PCH's are a useful tool, but they only alleviate part of the host of symptoms that come out of C++'s subsuming of the C pre processor. They are a patch of sorts to reduce the pressure of needing a better compilation model, as the C pre processor forces massive amounts of redundant text processing. This overhead is really why we don't see compile times as fast as C#, for example. Tools for automating the creation / management of PCH's and the build pipe, and they're really quite good these days. Ultimately, module support in C++ would remove the constraints that prevent a reduction of compile times. Really, long compilation time is one of C++'s greatest drawbacks, and one would think that it'd have been solved by now if it could've been done with PCH's alone, especially considering how much effort and time that has been put into the compilers and tools. *edit: wording*
if you need #ifdef UNIT_TEST, you've already mucked your interface. A good interface should satisfy instantiating objects in a stand-alone fashion. Example: If you have users that are saved to a database, don't add the database connection to the user. create a database_user object that does that. I see this a lot. In fact, if you can't unit test it, chances are your design needs some revisiting.
Absolutely. I want to get it to a *mostly stable* point before I upload it to my git repo. I know I said mentioned WTL in OP, but heh... mine actually wraps ATL (minimally) at the moment so it has the same environment restrictions as WTL. That's the second big ticket at the moment: remove the reliance on ATL. My biggest hurdle to that right now is replacing ATL thunks with something else that *isn't* a HWND-to-WNDPROC map.
[This is an example of what the code looks like.](http://pastebin.com/VsV0hxf0) That code actually functions *right now*.
Great, I hope we will see it on github or somewhere soon!
I also had the need for static class initializers. However I used a non static instance variable that calls the static initialiye the first time a class is instanciated. btw I like your post
IMO use unique_ptr or shared_ptr for ownership and raw pointers for anything else. In other words, unique_ptr&lt;__HWND&gt; since HWND is a __HWND*
Handles from Windows *are* pointers. They are `void *`s that point to something in an internal table in `user32.dll` or the kernel itself. For me, that is the definition of a pointer, unless you consider the MMU special for some reason. I have written code that stores HWNDs, HDCs, HBITMAPs etc. inside something like a `unique_ptr` with an appropriate deleter to invoke the correct deleting function (`::DestroyWindow`, `::DeleteDC` etc.). EDIT: to clarify I have specifically used `unqiue_ptr&lt;void&gt;`. I could've wrapped it in my own class to carry the particular type information of exactly which kind of `HANDLE` it was, but the code was small enough that I didn't deem it necessary.
I view dependency injection as mucking the interface as well.
Ok thanks!
I don't like the term "embarrassingly parallel" for compiling since you can really only compile multiple TUs in parallel, you can not parallelize the compilation of a single TU. This means that having more than around 8 or so jobs running at once is costly and annoying. Personally the worst compile times I have faced were when I was using the Cinder library (http://libcinder.org/) to write a game for a gamejam. It seems that the cinder headers include a whole lot of boost, and changing one of our files resulted in a compile time of 16s-1m, for just that file. This is hardly what I would call "creative coding". Full builds were taking over 10m with four compile jobs running. Now this is partly Visual Studio 2012's fault, the compiler included in VS2012 is pretty much the slowest C++ compiler around, in fact at one point I compiled the project with clang and the full build took around 50s. The shear volume of the cinder headers did however totally defeat XCode's code completion. These problems ultimately stem from the way the C Preprocessor interacts with headers. I think the standard permits a compiler to cache the results of parsing a header as long as it does not change the behavior of the program, but because the header's behavior can change depending on macros this is pretty much impossible to actually do. Additionally with modules tools will be able to tell what makes up a library, this will help with build systems and dependency management quite a lot. 
While Amazon et al. trying to force feed us electronic versions that are nearly useless because of DRM, at prices similar or even higher than physical books is OK?
do you even lift?
Yes, essentially. The only annoying part is having to do `reinterpret_cast&lt;HWND&gt;(window.get())` everywhere you need to actually interact with a win32 API.
That can be abstracted away. I'll give this a shot using `std::shared_ptr&lt;T&gt;`. I really feel like this is abuse of the STL, though, considering handles are opaque types and smart pointers make some assumptions about the types they store. The other issue, however, is when the handles must be copied and passed through a window message procedure (WNDPROC). The handle itself gets pushed in to the `wParam` parameter during a `SendMessage` operation. Once on the other side, inside the WNDPROC, a new `std::shared_ptr&lt;T&gt;` is initiated with this value. **So now there are TWO shared_ptrs pointing to the same value.** So long as either shared_ptr exists, the object it points to will still exists. That is *not* the case in the scenario I just gave.
That's definitely a problem - I'm not sure if that's fixable without a central repository of all currently active handles. Given your described nature of the window procedure, all of this machinery might be of limited use anyway. In my case, it was more for operations on a DC and BITMAPs; my code didn't interact with the window procedure at all (for example, I wasn't creating these DCs from a WM_PAINT message or anything like that). And while my original example mentions HWNDs, my tiny piece of the library for capturing screenshots into buffers did not use HWNDs, instead it grabbed a rectangle of a window given an HWND in the first place.
Thanks, just did that
If you love firebird you can get a free copy http://permalink.gmane.org/gmane.comp.db.firebird.general/280
Why Microsoft would need that? 
&gt; That's not generally true, but for whatever reason it's true today. Yes, I agree but Microsoft now is paying attention to C++. Also, I've hear they are building a native C# compiler (and most probably to other CIL languages, like VB.NET?). So, native language is back to Micosoft's head. The fact is, it may change in soon. Also, I'm just waiting to see how some companies will receive clang on Windows, truly MSC-compatible.
The files aren't read any longer. With include guards, the preprocessor still has to parse the file up to the include guards, because existing macros can change their meaning. So disk IO and parsing time is required. With #pragma once, the compiler will use the already read file.
I don't know if [the first one counts as spam](http://www.reddit.com/r/cpp/comments/1zm5o2/embedded_c_using_the_wiselib_for_google_summer_of/), but this one definitely does.
Nothing. But if it was on by default, it would reduce address the issue _and_ reduce the burden on the programmer.
I’m not disagreeing that modules could improve things. I’m just saying that _in practice_ these issues haven’t been important enough to my teams to warrant the extra work that comes with modules.
.....*everywhere* .....
"And yeah, getting two and a half front-ends to accept the STL's bleeding-edge code is a lot of work. " 
First link I could find: http://blogs.msdn.com/b/vcblog/archive/2006/08/16/thoughts-on-the-visual-c-abstract-syntax-tree-ast.aspx &gt; The sad fact of the matter is, however, that the current Visual C++ compiler doesn’t really generate a complete AST. It’s what’s known as a bottom-up compiler, meaning (among other things) that it devours its AST as it produces it, leaving no durable form behind. This is an artifact of the compiler’s age. In the days of the 256K limit, a large, in-memory structure such as a whole-program AST was not feasible. I know that they also talked about it on Channel 9 last year but it would take forever to find.
I suspect what you mean when you complain of extra work is that, when writing new code, module maps are more work on top of all the existing work involved in writing headers in the first place. The module map system is only an interim solution for legacy code. Apple doesn't even recommend to developers that they turn their own libraries into modules. (Although for well behaved libraries the effort to write a module map is low.) Ultimately C++ modules are expected to eliminate headers and instead you'll just define everything in a cpp file, and declare which entities are exported from the module. This is what the developer of the module map system refers to as the 'futuristic version' of writing a module in his [presentation](http://llvm.org/devmtg/2012-11/Gregor-Modules.pdf) on module maps. But obviously this futuristic version is far more work for legacy code than module maps, and that's why module maps exist. So there are two models, a futuristic version that is less work than the status quo for writing new code, and a module map version which is easier than converting legacy code into futuristic modules. Both models _reduce_ work, rather than create extra work, as long as you compare it to the right thing.
The first one was eaten by the spam filter, and he was told to repost. So, no spam. I've blogged about GSoC 2014 and C++ a few days ago, if anyone needs an overview: http://meetingcpp.com/index.php/br/items/cpp-and-the-google-summer-of-code.html
Use the tools at your disposals, and if someone says "you're doing it wrong!" when using your way over another they probably originate from the Python loving camp. So just tell them to bugger off and go back to that abomination of a community, because they live for the "one way or the highway" rule that has _no_ place in C++. Basically there's no way that's right or wrong as long as your code is stable, works and passes any QA that you need. If you want to build your own linked list using in-place instantiation of objects on a manually managed memory pool allocated from system memory (ie, not the heap) and that has async callbacks implemented with std::function that are protected by kernel api locking facilities (instead of stl locks)... go right ahead! Mix it up! Call stdlib functions from your objects! It's OK! Call malloc in the constructor and free in the destructor. That's OK too! Feel free to register system level stuff in singletons! Wee.. if it works! great... don't listen to the haters. 
If you need to call C functions just call the C functions? There's no need to wrap them in C++ in order to make the code mode C++ like. This stuff reminds of Java with its layers on layers.
thanks, insightful. For the bounty route, what are the best methods for finding low-hanging fruit?
Thanks very much!
Oh wow. Let me try to catch up to one month ago. ;) With respect to bounty boards, I only know that job boards and bounty boards exist. I've given things like Top Coder a look, but never dove in since I'm at a point career wise where pounding code for small-ish amounts of money isn't exactly worth my time; this doesn't mean that it's a bad idea, but rather that it's not for everyone. I vaguely recall other Redditors posting about them here, so some googling might turn up some good or bad experience stories. So I'm left struggling to answer your question: I simply don't know other than to jump in, look for things you can complete, and start bidding. Try to suss out the bullsh*t, as I have seen some things in passing that would suggest that some folks want everything for nothing. I suppose you could also look at the work histories of the more successful job board users, and try to emulate their success. Overall, I would recommend not over-extending or over-committing, play your strengths, and do read up on basic contracting law before you start. Edit: added some relevant info.
It might be the case. Personally I tend to avoid compiler extensions, but then again I am not using C++ on the daily job since 2006, only on small hobby projects where compile times are still ok. However, every time I get to compile LLVM alongside a new Rust release, I have backflashes of having a free afternoon when I needed to do a *make all* on our C++ enterprise projects.
That's a very good place to start. 
I picked up Accelerated C++ and it's, by far, one of the best books I've read. It's terse and provides an amazing amount of detail for its size. [Amazon](http://www.amazon.com/Accelerated-C-Practical-Programming-Example/dp/020170353X)
This is actually excellent advice. There are few things more painful for beginners than continueing programming in say Java (or FORTRAN77) style when moving into C++.
Can we perhaps make a sticky or an entry in the sidebar for this particular question? I see this same question come up once every few days. While C++ is a language that is undergoing change at a rapid pace, I don't think that these redundant threads encourage much meaningful discussion.
I know that the fifth edition, at minimum, of *C++ Primer Plus* is not favorably looked upon. Have the issues been fixed?
It depends what your programming experience is.
This is actually exceptionally bad advice considering how much the language has changed since the beginning.
http://www.cplusplus.com/doc/tutorial/ I've literally read at least two dozen times and at least reference it over one hundred. It's such a good guide. If you can comprehend this you're on your way. 
Should you wish to learn more of the criticism regarding 5/e, I wrote a comment under a year ago on the matter with some links [here](http://www.reddit.com/r/gamedev/comments/1fezrk/jay_baxter_gives_a_list_of_books_for_people_who/ca9trf8). I know not if these issues carry over to 6/e.
Im traveling at the moment so I don't have it handy to give a title but most of the same team came together to write an updated version that covers c++11 which is also excellent. 
[The C++ Programming Language, 4th Edition](http://www.amazon.com/The-Programming-Language-4th-Edition/dp/0321563840) is the first C++ reference I've read that seems very trustworthy and I highly recommend it.
I've updated my OP. [Microsoft solved this exact issue in the .net framework by writing a custom RAII class that basically serves as a lookup table.](http://referencesource.microsoft.com/System.Windows.Forms/NDP/fx/src/misc/HandleCollector.cs.html)
I've updated my OP. [Microsoft solved this exact issue in the .net framework by writing a custom RAII class that basically serves as a lookup table.](http://referencesource.microsoft.com/System.Windows.Forms/NDP/fx/src/misc/HandleCollector.cs.html)
I’ve been wanting the “futuristic version” for at least a decade. I spent some time trying to build something like that but didn’t have the time to delve deep enough into compiler code to do it well back then.
http://projecteuler.net/about
&gt; Pop quiz: if we assume 32-bit ints then what, according to the C++ standard, happens if we calculate fib(46)? Trick question - the C++ standard doesn't say what happens when you calculate `fib(46)`.
So after reading those comments I can say that the 6th edition is no better than the 5th. He still presents C with a heavy emphasis before C++. 
&gt; a custom RAII class that basically serves as a lookup table. AKA a "central repository of existing handles", as /u/Gotebe said...
Correct. The return value is undefined.
The *behavior* is undefined. It might not get as far as actually returning---e.g., `int` overflow could result in a trap.
Yes. That's what I was confirming.
As an interesting aside, an operation with undefined behavior cannot contribute to the value of a constant expression (per C++11 5.19 [expr.const]/2) - the compiler has to diagnose such occurrences. So if `fib` was a `constexpr` function ([Live at Coliru](http://coliru.stacked-crooked.com/a/4854e4cca68a7436)): template &lt;typename T&gt; constexpr T fib(T n) { return n &lt; 2 ? 1 : fib(n - 1) + fib(n - 2); } constexpr auto fib_45 = fib(int32_t{45}); // compiles fine constexpr auto fib_46 = fib(int32_t{46}); // ill-formed the compiler diagnoses overflow for us when `fib` is evaluated in a constant expression. Also, to drive home the point that the `fib(n)` is exponential in `n`, doubling the number of bits in the representation doubles the representable range of `fib(n)`. The largest values `n` that don't overflow a signed 32, 64, and 128 bit integer are 45, 91, and 183 respectively.
I like it, and I think it's definitely on /r/cpp spirit. Incentive for good C++ usage is always good, specially when it's advocating C++ on a field dominated by another language (C).
&gt;Is it wrong that I get so much satisfaction from defeating compilers’ optimizations? Not at all - learning something new about the tool you use the most? That you can then leverage as to what *not* to do in order to improve stuff? Doesn't get much more satisfaction than that.
Is the special edition like the third edition?
Dude, while I can't help you out with your optimization problem, I commend you for finding an alternative for a native C++ implementation for the windows API. wxWidgets has so far been the only other solution for me, which is a bit of a hefty thing to include in a project.
Your missing one: http://www.amazon.com/The-Programming-Language-4th-Edition/dp/0321563840 C++11 = new edition!
Yes, from: http://www.stroustrup.com/3rd.html &gt; The "special edition" is the hardcover version of the 3rd edition. It differs from the early printings of the 3rd edition by about 1,000 corrections and clarifications, by two new appendices (just over 100 pages; also available online, see below), and by an improved index. The only difference between the current printings of the special edition and the 3rd edition is the cover (and the price difference implied by that stronger cover).
I took the picture on my local workplace library. At that time all the copies of the 4th edition were on loan :(
Thanks, man!
This is a very interesting effort, thumbs up for the roadmap in two milestones, first the clang plugin as a proudly custom extension of c++, then a proposal for the reflection working group of the standard. I hope this gets incorporated into Qt and that c++ gets a lightweight reflection just a bit stronger than RTTI. It would be great if reflection were only based in attributes as does ClReflect, without the variadic template voodoo. Still ClReflect needs to be rounded to merge the generated reflection database with the link map, and to make it into a blob glued to the executable image. Looks that you could get at least properties this way, I dunno about signal-slots (perhaps only the moc knows what those really are).
Back in the late 90's gtkmm was already using libsigc++ for similar purposes as Qt's moc. Qt guys used moc because they wanted to target compilers that weren't that up to date with C++ templates as well, which gtkmm didn't cared about.
&gt; Most calls to fib(n) result in two calls to fib(n) so one might guess that the expected run-time cost is O(2^n ). okay, trivial. &gt; In fact fib(n) is O(fib(n)) – the runtime cost is proportional to the result. More usefully, this works out to about O(1.618^n ). Huh what?
2^n is an over estimate because the trees are lop-sided and do not have n depth on both sides. Interestingly fib(n) ~= Phi^n
Qt5 added `connect(sender, &amp;Sender::valueChanged, receiver, &amp;Receiver::updateValue);`, so you don't need to use the awful string-based connecting any more. 
Interesting. Looks like Qt usage is quite advanced; and I might be uncomfortable getting so much reflection (it adds a whole other dimensions to mess up, as if we needed more :x) &gt; Allow attributes within the access specifier (`public [[qt::slot]]:`) Why ? It's what Qt does today, but users could just as easily attach the attribute to the data member or am I missing something ? &gt; Traits to get the access of a function (public, private, protected) (for QMetaMethod::access): I could not find another occurrence of the thing, so wonder if this is truly necessary. Or actually, I would challenge the design and say that `QMetaMethod::access` should only see the list of attributes that it's allowed to access. Up to people to make it friend as necessary. &gt; `Q_PLUGIN_METADATA` which allows to load a JSON file, and put the information in the binary: I'm afraid we will still need a tool for that. (Because I hardly see the c++ compiler opening a file and parsing JSON.) This does not really belong in moc anyway and is only there because moc was already existing. Hum... you might want a step to transform JSON into a single C-String, and then it's as simple as `#include "file.json.cppized"`
And that one is completely checked at compiletime? Including wrong parameter-types?
You can see for yourself that the run-time cost is not O(2^n). Just look at the diagram in the article for all of the function calls used for fib(7). The link in the article (from O(1.618^n)) gives more details on why that is the actual complexity. References, references, references.
* User also could attach an attribute to each slot or signal, but that would be a regression compared to simply annotate them all at once. * There is indeed not so much use cases for QMetaMethod::access, so that would not be a big miss. * moc will also convert the JSON into some Qt binary JSON that is then stored in a specific section of the binary so it can be read quickly without loading the plugin.
Yes. And it does automatically convert the parametters if there is an implicit conversion possible. And it also work with lambda functions.
I went to the library and found the fourth edition, here it is! &gt; http://www.flickr.com/photos/alejolp/13088519305/ 
This is a fairly standard complexity analysis problem and is almost exactly the same as the proof for the convergence of Fibonacci. Maths is neat.
&gt;&gt; In fact fib(n) is O(fib(n)) &gt;Huh what? The proof is fairly simple: 1. Convince yourself that fib(0) and fib(1) are in O(1). 2. If for some fix n, fib(n-1) is in O(fib(n-1)) and fib(n-2) is in O(fib(n-2)) and adding to numbers in O(1), then calculating fib(n) via fib(n-1)+fib(n-2) is in O(fib(n-2)) + O(fib(n-1)) + O(1) = O(fib(n-1)+fib(n-2)+1)=O(fib(n-1)+fib(n-2))=O(fib(n)) 3. Completion is trivial
They got the surname letters wrong on the 4th ed, no wonder you couldn't find it :)
The decision to switch to proportionally-spaced fonts for the 3rd ed made me not want to deal with the book. Truly horrible. I have no idea what they were thinking.
The virtual functions could have been modified to translate many pixels with each call instead of just one. For instance, allocate a temporary buffer to hold 1024 pixels in the universal 32bit RBGA format, and loop over the image reading/writing 1024 pixels at a time to/from the temp buffer. This solution would have reduced the number of virtual function calls to translate an image by three orders of magnitude. I think it would still have been slightly slower than the implemented solution, although may have been better if there were more image types as the binary size would grow in O(N) instead of O(N^2) where N is the number of image types.
Yeah, it's clear that there is a O(n) algorithm - but the two sentences, as they run along, make no sense. The article first uses *fib(n)* to reference the recursive implementation presented above with a reough guess, right after that a not-yet-mentioned, completely different implementaiton, and right behind the correct value (as I understand) for the recursive implementation again. 
Interesting how the fourth edition isn't really bigger then the special edition, even though it adds C++11.
Good talk, I share most of the authors opinions on this topic. If you know C++ well you are however unlikely to learn anything new in it.
This is quite interesting. Qt is one of the most popular libraries requiring use of introspection and can be a great test case to C++ introspection proposals.
I'm not sure how I feel about auto-parallelization. On one hand it's very handy and convenient, on the other hand it could really get in the way of a heavily optimized parallel program. Hopefully developers of such software would notice the extra threads.
The way it's written also doesn't help.
I'd like to think that anyone who explicitly turns it on and rewrites their loops to actually work with it would verify that it's beneficial afterwards, and not do so if they're already making effective use of all of the cpu cores.
I agree that the sentences make a giant leap that is not fully explained -- the link on O(1.618^n) was supposed to give the explanation. I didn't want to spend too long on the details because they aren't really relevant, so I just put the link for the curious. The sentence also doesn't make sense if you don't know what O(fib(n)) means -- I should have had a link to big-O notation. I think it's fascinating that the runtime cost of a recursive implementation of fib(n) is O(fib(n)). That's just cool. And, it turns out that the value of fib(n) is about 1.618^n. Therefore fib(n) is O(1.618^n) -- it's runtime cost at the limit is proportional to 1.618^n. If it still doesn't make sense or if you think it could be more clearly stated, let me know. I just don't want to devote much more space to it.
How many Accelerated C++ books do you have sir or madam? 
Oh, what a relief. I thought it was a default setting.
I didn't like 'auto' when I first knew about it and I still don't like it. Too much rules to memorize only to avoid typing. Some will say that it improves readability, but for me it's just the opposite. As of now I don't see it worth the trouble. 
It might be easier to compile a list of what you need to know, and then scratch out the few things your college does teach you. Basically writing real, finished, software, even if small, will teach you to go above and beyond what you learn in an academic setting. They seem to teach syntax, data structures, and .... For c++ learn the STL and learn lots of other libraries. Learn GLFW, openGL, possibly an audio library, SDL, parts of boost that you like, how to interface with a scripting language, and whatever else strikes your fancy, because there is usually a library waiting for you if you are willing to learn how to compile link and use it. Also font libraries, system specific APIs, math libraries like Eigen, and codec libraries like ffmpeg are neat.
I read the flags a few weeks ago, here are other stuff I added to my project: - /GL: to enable linker flag /LTCG. according to [MSDN](http://msdn.microsoft.com/en-us/library/e7k32f4k.aspx), you actually need to compile once with /LTCG:PGI, profile, then compile with /LTCG:PGO to optimize. - /W4: warning level recommended to compile with boost - /O2: like gcc, if you're building for release - /GA: optimized for Windows - /OPT:REF,ICF=5: linker option. REF and ICF are enabled when you build with /Gy, just added the number of iteration
Very Nice idea, a good job for boost asio !
I recently tried cppcheck and found it surprisingly fast - just a minute or two to analyse our code base. I also tried PVS, and that took upwards of an hour to do the same. I can't comment on the quality of PVS studio's results though because I only got through the first few (identical) results before the trial ran out. From that perspective cppcheck was amazing :P
Short variant: [Comparison of static code analyzers: CppCat, Cppcheck, PVS-Studio and Visual Studio](http://www.viva64.com/en/b/0241/)
What about /MP ?
&gt;Note that there is a difference in linker behavior when ICF is in effect by default with /OPT:REF explicitly specified and when you explicitly specify /OPT:REF,ICF. The default ICF with /OPT:REF does not fold read-only data. This includes any .rdata, .pdata, and .xdata. However, the default ICF with /OPT:REF results in fewer functions folded when producing images for Itanium and x64 because functions in these modules have more read only data dependency, such as .pdata and .xdata. To get full ICF, explicitly specify /OPT:ICF. So basically, you'd be better off with: /OPT:REF /OPT:ICF=5 And have you noticed any improvement of using ICF=5 instead of the default iteration value of 2?
I didn't do heavy benchmark and my code is just too small for now.
I applaud the effort but I think http server has been solved so many times already as your review of existing solutions shows. By the way you forgot [Poco](http://pocoproject.org/docs/Poco.Net.HTTPServer.html.) Why not implement a boost FastCGI? As a c++ developer there are so many battle tested web servers to choose from. What is lacking are good stand alone cgi / fast cgi libraries 
Ugh. It's not just proportional. It's italic. http://imgur.com/VJAJYzN
&gt; poco Poco's focus seems to be any networking application and is pretty large. There's even database, xml and zip abstractions. &gt; Using std::istream for Poco::Net::HTTPRequest::read and Poco::Net::HTTPResponse::write It doesn't look very async too me. It doesn't look too ASIOishy also. And ASIO has a more "library"/"active" style where user participation is higher. POCO has a more "framework"/"passive" style where user fill some holes and they're called when POCO feels like it. Nevertheless, I still have to check how their WebSocket support works and how/if other possible backends (FastCGI, ...) are supported. EDIT: [WebSocket](https://github.com/pocoproject/poco/blob/492317224154a21407ba346f6e81471561e45250/Net/src/WebSocket.cpp#L142-162) seems okay. EDIT2: [Fixed](https://github.com/vinipsmaker/gsoc2014-boost#poco).
&gt; It's italic. They've changed that in the 4th edition, ie, see the [Exercises chapter [PDF]](http://www.stroustrup.com/4thExercises.pdf). 
I guess your biggest competition, for a lack of a better term, are the servers written in C e.g. lighttpd, Nginx, apache, thttpd etc. For most projects these servers have a proven track record and they work great. They have features that may take you a long time to implement; likely much longer time than is feasible in one GSoC. 
As a rule of thumb, I refrain from overloading any function that might have it's address taken. Doing so, I evade some (very) ugly casts, and it's clearer when debugging / looking at stack traces.
Couldn't they put even less text at one page? &lt;/s&gt; I just hate when they do that - I get it, the ads revenue and shit but they are really going overboard with it.
That is because you should use BOOST_REQUIRE_EQUAL and not BOOST_REQUIRE.
Read the next sentence: &gt; Of course since the compiler obviously doesn't have support for that proposal yet, I have been manually expanding the typedef... and typename... in the prototype.
That is **exactly** what I have written: &gt; No other framework provides this. You either have to write convoluted testing code, along the lines of &gt; ASSERT(that(Factorial(1)).is(1)); &gt; or at the very least &gt; ASSERT_EQUALS(Factorial(1), 1); The advantage of Catch is that you only need *one* macro, not distinct ones for all possible comparisons.
I understood that. I just wanted to point out that you can make Boost.Test print the values in the event of an error if you use the *correct* macro.
D'oh. That's what I get for only skimming texts. Thanks a lot!
For some of them (embeddable servers)... yeah, there are some really pretty nice ones and I'll use their parsers internally to accelerate the development. The only "new" thing would be a Boost familiar look. For the other ones (ready servers like Nginx)... I'm not trying to compete with them, but this proposal could be used to create a competitor.
The special section could probably be covered by the ability to add an attribute that specifies it; but I am afraid the conversion from JSON to binary-format will have to be left to an external script for now.
Thanks for the motivation. Also... &gt; [...] would likely be hard to implement fully in one summer of code. However if you think it can be done don't let me stop you [...] I've been working with HTTP for a long time and after so much time I think I'm ready for this big step.
Former GSOC participant here. Submit your incomplete proposal using the web interface. The mentors will usually give you feedback, and you're free to keep changing your proposal until the deadline. Good luck!
Is the proposal that's referenced in the post for C++14 or C++17?
/Oy- (Disable FPO) I understand the performance benefit but mucking with the call stack makes debugging such a pain.
What is google? Seriously dude, if you're going to program take some initiative and do a bit of searching. Does the word boundary not say it all?
aw be nice
No problem! In a basic sense I think you have the right idea here. It may need refinement but I'm not that much of an expert to comment. Lots of luck. 
Is it [this](http://en.wikipedia.org/wiki/Boundary_case)?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Boundary case**](http://en.wikipedia.org/wiki/Boundary%20case): [](#sfw) --- &gt; &gt;__Boundary case__ is a frequently used term in [software engineering](http://en.wikipedia.org/wiki/Software_engineering) to refer to the behavior of a system when one of its inputs is at or just beyond its maximum or minimum limits. It is frequently used when discussing [software testing](http://en.wikipedia.org/wiki/Software_testing). &gt;For example, if an input field is meant to accept only integer values 0–100, entering the values -1, 0, 100, and 101 would represent the boundary cases. A common technique for testing boundary cases is with three tests: one on the boundary and one on either side of it. So for the previous example that would be -1, 0, 1, 99, 100, and 101. &gt; --- ^Interesting: [^Canada–France ^Maritime ^Boundary ^Case](http://en.wikipedia.org/wiki/Canada%E2%80%93France_Maritime_Boundary_Case) ^| [^Cordillera ^of ^the ^Andes ^Boundary ^Case ^1902 ^\(Argentina, ^Chile)](http://en.wikipedia.org/wiki/Cordillera_of_the_Andes_Boundary_Case_1902_\(Argentina,_Chile\)) ^| [^United ^States ^v. ^Maine](http://en.wikipedia.org/wiki/United_States_v._Maine) ^| [^Saint ^Pierre ^and ^Miquelon](http://en.wikipedia.org/wiki/Saint_Pierre_and_Miquelon) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cg1xur9) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cg1xur9)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
First things first, do you have an ssd and a fast processor? That can help a bit and cut down on times enough to possibly be a panacea until the ancient / horribly broken compile times of c++ are solved.
A language that was really interesting was clay. It was what C could have been if we knew then what we know now. Generic, fast, non GC. Basically something written in clay and something written in C would compile to the exact same thing. Its spirit sort of lives on in nimrod, but I havent seen a simpler language. RIP clay.
It doesn't seem that much faster than Java: http://www.techempower.com/benchmarks/
The rule of zero essentially states that you do not need to do resource management in most classes (apart from those that only do that, and on which you can spend extra time to make sure they are sane). This article "assumes" that you "have to add a destructor". Of course, then there is a problem.
I'd be interested to see how the [Clang Static Analyser](http://clang-analyzer.llvm.org/) compares.
As Martinho Fernandes stated in the comment section, this is really a compiler issue. C++11 deprecated this behaviour. If I define a destructor, it should disable the generation of copy and move operations.
When you say constructor, do you mean copy constructor? I'm not seeing a problem with having a "custom" constructor.
At CERN...
&gt; C++14 brings core language support for binary literals, which means you now can integrate binary literals as such in your code: &gt; &gt; char c = 0b01011010 &gt; &gt; The binary literal has to start with 0b/0B. YEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAAAAAAAAAAAAAAHHHHHHHH!
Yes, thanks, I meant copy/move.
Seriously how did this take this long? 
Hasn't taken long: &gt; In computing, C (/ˈsiː/, as in the letter C) is a general-purpose programming language initially developed by Dennis Ritchie between 1969 and 1973 at AT&amp;T Bell Labs. ;-)
...so it only took 45 years?
In a recent project we were working in, we had segfault in the static dtors (It turned out that you are not allowed to leak a QApplication¹). In order to resolve this, I created a TRACEPOINT-macro that would print the currently executed function and told everyone to add this to all their methods and especially to all their dtors, because we had found no other way of tracing the error down (in the end it didn't help with that bug either, but we profited from it in other places). So there are situations in which adding a dtor for debugging can be helpful. ¹ We couldn't just create one on the stack because this was for a library that should work both if the user created a QApplication themself and if they didn't. In the end we implemented a rule that every user would have to call a certain function that would, among other things, destroy it. 
&gt; If I define a destructor, it should disable the generation of copy and move operations. Because... why? No other language has this behavior - why should C++? This isn't very intuitive behavior - it violates the principle of least surprise. That said, I'm really not sure what the right thing to do is...
&gt; Of course, then there is a problem. Why "of course"? This isn't a feature in any other language, ever! If you look at the article, the programmer adds a destructor just for debugging purposes. It's really not obvious to even a moderately advanced programmer that this is going to change the behavior of code that never uses the destructor - heck, I've been doing C++ for almost two decades, and I know this fact, but I have to remind myself of it whenever I tinker with destructors/copy constructors, because it's not at all intuitively obvious. EDIT: added missing single word.
When he says "concepts" didn't make it into C++14, does he mean "concepts lite"? If so, that would be seriously disappointing.
yes, concepts lite was never meant to be in C++14, that is clear since the Bristol meeting of the C++ committee.
Deprecated doesn't mean a feature is gone, it means it might go away in the future. In both C++11 and C++14, compilers are required to generate copy operations for classes with user-declared destructors. They may generate warnings if they want to, but code using deprecated features is valid code.
Well, relative to how old the universe is, 45 years is negligible.
~~ boost::optional Just embed it into your application directly if you are worried about dependencies.
If you define a destructor, it means that you hold some kind of resources and the default destructor doesn't clear those resources correctly. For example, if you have a pointer to an object on the heap, you need a custom destructor. What happen when you copy this object? Do you copy the pointer? The default copy operation would do this, but this is probably not what you want. So when you define a destructor, you probably needs to define the copy and move operation yourself. 
I wish templates had the syntax that generic Lambdas do. So clean.
You should consider compare how old it is relative the creation of very first language standard
Personally, having started computing in the late 80's with languages that supported modules (Turbo Pascal, Modula-2), what I really would like would be for module support to be made available. Even though all my daily job languages all support modules, it would be nice not to have to wait a few hours on my C++ hobby projects when doing full builds. So I only need to wait about 6 years, if Doug's work ever comes to the standard. :(
No other language has a rule of three/five, either. That said, I agree that it isn’t very intuitive. But I’m not convinced that it’s a real problem, either.
What other languages even have automatically-generated-but-overridable copy and move operations for non-POD types? There's no precedent about how this should work from other languages, because that would require that other languages actually have the feature in question.
I presume that the complications mentioned in Stroustrup's earlier article on std::dynarray kept it from being added to C++14, but I wonder how stack-based allocation will pan out in C++17. I would really like to have variable-length stack allocation with class-lifetime scope in the language.
Maybe you could just use generic lambdas instead of templates. I think I'll be doing that a lot. Even where the function will only ever have one set of parameter-types, I might just use a generic lambda anyway if I'm too lazy to fully specify the parameter types!
Here's a standalone implementation of `std::optional`. Single header file, zero dependencies: https://github.com/akrzemi1/Optional I've been using this for the last year and it's great.
Thanks for the clarification. I guess my information was rather out-of-date. I've seen Stroustrup write/talk about concepts lite so many times that I assumed it was a done deal for C++14. Pity.
Yeah, that's a good point! Do non-variadic templates offer functionality that is a strict subset of generic lambda functionality?
I think you might be misusing `&amp;&amp;` and `forward` a little. I guess you're looking to forward arguments efficiently, but if you want perfect forwarding, do it fully. I would suggest this: template&lt;typename U&gt; void operator = (U &amp;&amp; value) { _setter(std::forward&lt;U&gt;(value)); } For perfect forwarding to work fully, the type of value must be `U&amp;&amp;` (not `const U&amp;&amp;` or anything else) *and* `U` must be a template type parameter, *and* `U` must be a *deduced* type parameter, *and* `U` must be used in the `std:: forward`. (And, to clarify, you can't use `T` here, as `T` is not a deduced template for this particular function template.) The reason I point this out is a number of problems in your current code. This function won't able to take an lvalue as a parameter. It would be safer to use perfect forwarding (as above), or to simply forget about `forward` and take the argument by `T` or by `const T&amp;`: void operator = (T &amp;&amp; value) { _setter(std::forward&lt;T&gt;(value)); } And this code is a little strange. You use `&amp;&amp;`, but then you allow `get` and `set` to remain as lvalues. You should put them inside `move` as follows: Property(std::function&lt;T()&gt; &amp;&amp; get, std::function&lt;void(T&amp;&amp;)&gt; &amp;&amp; set) : _getter( move(get) ), _setter( move(set) ) {} If we assume that `T` is not a reference type, then `forward` will simply pass this by value here. And anyway, I'm pretty sure you won't be able to 
I guess one obvious problem with generic lambdas is that they cannot be specialized. (I assume I'm right here?)
&gt; In some ways, MSVC is ahead (such as make_unique). libc++ already has almost complete C++14 library support. [libc++ C++1y status](http://libcxx.llvm.org/cxx1y_status.html)
&gt; If you define a destructor, it means that you hold some kind of resources Stop right there. That's not guaranteed to be true. If you define a destructor, it means you need code to run when the object is destroyed, no more and no less. Much of the time this will mean freeing a resource - but sometimes you might just want a notification that the destructor has gone off. I do this all the time if I have a memory leak. Oh, I have all sorts of fancy tools to do this, but eight times in ten I can quickly throw some print statements into the constructor and destructor and find the leak immediately. For a production code example, if you have a leak detector, you only have to have code in the constructor and destructor of objects. In one of my projects, when I'm running in debug almost all of my non-trivial classes have leak detectors in the constructor and destructor, it's built into the boilerplate. Now, my boilerplate typically disallows copies and moves by default, but I do fairly often use "Plain Old Structs" and I'd like to still keep the leak detector without having to change the copies and moves.
Don't see the really big issue, just extract the part of boost that you need with [bcp](http://www.boost.org/doc/libs/1_55_0/tools/bcp/doc/html/index.html). Sure dependencies suck, but when you can fully integrate a subset into a project its more part of the project then it is a dependency. Or what agwaman recommends :)
Was a jest :) 
Instead of using a template type to accept both LValues and RValues. It would more advantageous to simply write out the two overloads. This way when the setter is sent a value of an incompatible type you get an error at the line it's being set at and not an error inside the operator=(). 
I have no idea when bcp would actually be useful. Most of the higher-level libraries are built on the lower-level tools, so bcp usually ends up grabbing about half of boost. When is that meaningfully better than just requiring all of boost?
This depends on the definition you use for C++14: Some people say, that it really is just the document that will now become a full international standard. Others¹ also include the technical specifications that are expected this year. AFAIK the concepts-stuff was meant to be such a technical specification from very early on. Note that these TS's are also official ISO-documents, though they are not intended to be as final as a real international standard. ~~¹ among them Herb Sutter, the chair of the ISO-committee~~
It will be part of concepts-light. The following is an example from the current working-paper: auto f(auto x, const Regular&amp; y); The meaning will be equivalent to this: template&lt;typename T1, typename T2&gt; requires Regular&lt;T2&gt;() auto f(T1 x, const T2&amp; y); Source: [N3929](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3929.pdf), page 11 
Hah, just tried it, it really did pull in almost half of boost. Crazy :)
The automatic generation of copy functions was included purely for backwards compatibility with C. Since C doesn't have destructors no class with a destructor needs to be backwards compatible, and thus no automatically generated copy function is needed. Many have argued since C++'s inception against the the implicit declaration of special member functions on this basis, and instead arguing for an explicit syntax to opt-in to the default behavior. C++11 gives them at least part of what they want with the `= default` syntax.
only bad thing was that his 5 sec explanation of auto was wrong... other than that I totally agree with you, though Im kind of irritated with this functional this, functional that... cuz tbh i dont gain any insight into cpp by being spammed with fp this fp that. for all i care bjarne invented lambdas and const while drinking vodka with alex 
I guess I've never really understood what a TS is. Will there be compiler support? If gcc gets concepts lite support, will it be enabled with `-std=c++14`?
As much as I appreciate being able to type less, I have some reservations about this feature, since it will no longer be possible to tell by looking at a function whether it's a template or not. Does anyone know if this point came up during discussion, and if so, what the counter-argument was?
I've found that BCP pulls in a lot more files if you just specify certain modules, aka "bcp regex", but if you let it scan your files after individual includes then it's usually a bit better. The number of headers are still in the hundreds though. Still, what's the actual issue here? The combined file size for my BCP extracted Boost.Filesystem is ~6 MB for the headers and ~8MB for the compiled binaries. Most of which gets compiled down to nothing in the final executable. They heavily rely on forward declarations, so compile-time is rarely an issue in my experience. If you're so inclined you can probably use precompiled headers to alleviate this a bit more.
I agree that no other languages have these features. It still doesn't mean that it's obvious that adding a destructor would change the effect of code where the destructor is never mentioned.
That's debatable. You are using the destructor to do something unrelated to the destruction of the object. Shouldn't this be somewhere else, at an higher level (ex.: custom allocator)?
&gt; since it will no longer be possible to tell by looking at a function whether it's a template or not. It will be impossible for all those who didn't follow the naming-conventions of the standard. Those who did will be able to distinguish it by looking whether the type is written using snake_case or CamelCase. Since I am a firm believer into „use the standard-libraries naming-conventions“ (applies to all languages), I cannot deny that I am somewhat amused about this. ;-)
&gt; Will there be compiler support? Yes. &gt; If gcc gets concepts lite support, will it be enabled with -std=c++14? This is an implementation-detail of GCC. I am somewhat sceptical, but definitely wouldn't rule it out. On a sidenote: The proposal is currently implemented in a special branch of GCC, so GCC will likely be the first one to release it.
&gt; I am not aware of any place where Herb has referred to TSes as being part of C++14. I was quite sure that he said something like that in one of his talks. Since you visited some of these in person which I didn't chances are that I either misunderstood him or mixed him up with someone else. A small search on channel9 through some of his talks didn't turn up with it either and since it is 03:23 in my timezone I am not inclined to do much further research now ;-). 
If you're defining a set of related bitset constants, then this is very useful, especially if you have multiple bits set in each constant. You just comment at the top what each bit means and then essentially write out a table below. When you're only setting single bits, then yes, using a shift or a hex constant is probably better, but I don't know what bits are set for the number 0x26A9 off the top of my head. 
Doug's proposal doesn't help with build times if you're already using precompiled headers.
Carmack is my superstar of coding. So much respect for that guy.
As a former Ubisoft engineer, I agree with most points there. Reimplementing the standard library is pretty common in the industry, just because some consoles did not ship one, or because you don't want to find out one week before Christmas that the shipped implementation of std::map causes memory corruption on the Wii when NDEBUG is set. Better have your own that you know in details. I believe designing it not templated and only accepting string as keys is a decision made for time / resources reason. The only thing that really grinds my gears in Carmack's coding guidelines is the use of tabs to indent code... 
&gt; The only thing that really grinds my gears in Carmack's coding guidelines is the use of tabs to indent code... Tabs work great as long as you only use them for indentation while sticking to spaces for alignment and all other white-space use. 
While you may call STL code ugly, you should not underestimate using the standard versions on a big projects. In my experience, the custom versions are always harder to use and have more bugs than a standard conforming implementation. Furthermore, when a project uses the standard versions it is much easier for new people to understand what the code is doing because they already know algorithms, etc. The same goes for de-facto standards such as boost. 
&gt;Oh, this is such poorly thought-out hubris! See, even if it did have a bug (but guess what: that foreign implementation has been looked at and tried more than yours), you have all the code, if your team are such wizards (you can write a better one yourself, apparently), surely you can fix it in a blink. You are missing the point. I'm not saying our implementation is better, I'm saying that we have control upon it. Say your team find that bug in the standard library and can fix it. Then you should file a bug report to the console manufacturer with a patch and hope it gets reviewed and integrated in time. In the meantime you will have to roll out your own implementation anyway (maybe out of theirs?). I'm not saying it's a better option, it's just safer to have your own stl, shared across all platforms and all projects, than relying on 10 different ones and hope they are bug-free. 
In depth review of the code: http://fabiensanglard.net/doom3_bfg/index.php
Very cool, thanks 
I have to disagree with the notes on commenting. The comments for the function are by far not enough. One has to either read the function and understand it, or to consult some documentary. Also, the first line is also important because a One line description what the function should do is in many times needed if you do not read the whole function. Why do i think, the commenting on the Function is not enough. Here are a few important questions i would have: - Do i have to allocate the memory for the arrays, or does the function do that? - If the functions allocates the memory, what happens if the given pointer points to allocated memory? Is it freed? To get an answer to them, i either have to read the function (takes a lot of time if you have to do that for many functions), or you have to consult other documentation .. Now, having two documents which refer to the same function to always be in touch is not going to happen, therefor i think all those information needs to be put in front of the function (best in a way that it can be extrated automatically) Also, format strings are inferior to (string) streams in my opinion. One problem with them is, there is no way to make 100% sure at compile time that the format string is correct. Also slight changes might completly mess up the format string. 
Agreed on the comments. At the very least, a comment should document any restrictions on arguments (and outputs) that aren't made explicit by the parameter types. This can often be avoided by using the appropriate types, but it's a big problem with pointers. Pointers say very little about their restrictions. What are they pointing at? Should they be pointing at allocated memory? Pointing at a single object or the first object in an array? If an array, is the array null-terminated? These things need to be explicit.
&gt; that the shipped implementation of std::map causes memory corruption on the Wii I can relate to that, partially. std library can be shitty/patchy on target platform, but (imo) rolling your own is rarely the answer. Given abundance of existing implementations of good quality that contain such primitives (like boost) its just waste of resources.
Even Carmack agrees that STL was an issue back then, but not so much at all today (even for gaming).
I agree with `stringstream` hate in the article. I consider `printf` approach one of the nicest achievements of C in the API department - it's not coincidence it was later copied by many programming languages. Even Sun relented and introduced `printf` clone in Java 1.5. It's a shame that C++ still doesn't have something like `boost::format` or [tinyformat](https://github.com/c42f/tinyformat) in the standard lib.
***Exactly***. Tabs are an unambiguous unit of indentation. If used as you describe a developer's chosen tab width has no impact on alignment but the code is malleable to please people who like subtle indentation levels and those who like strong indents.
 [Yes, const nazi indeed](https://www.youtube.com/watch?v=ZWRRXP-XFvY).
No. If you want consistency across platforms, then use STLPort, uSTL, or the containers from boost, etc. Rolling your own is not right. Games companies do this all the time and it makes no sense. Having said that, its still better to use the version shipped with the complier because they are pretty tightly coupled these days. 
Thanks for your feedback. I have updated the gist now. However you will probably notice that setter_proxy will still break perfect forwarding and I have no idea how to fix this right now. I implemented inlined member function calls instead of std::function to reduce the memory overhead.
Exactly, he makes the C++11 version look uglier than it should be because he doesn't know the language. No idea why you're voted down.
If it's a "must have ship it" situation, then you fix it and ship it and worry about it later. Having said that, be honest: did it happen to you? For something as fundamental and as widely used as STL, one company, however good, does not beat tens, hundreds, thousands of others, plus all individual eyes, tinkers etc. As the other guy said, it might have been an option years ago. These days, nah. It' hubris, inertia and other poor reasons.
_Slow as fuck_ depends if the bottleneck is on the virtual call, but my plan with virtual functions is to not force an intrusive design on the user (such as cpp-netlib) that would pretty much kill the possibility of plugin-based handlers. So... it'll be used for separation of the `&lt;request,response&gt;` objects and its backend. But you concern is very valid. EDIT: The runtime-based polymorphic abstractions are completely separated from the static-based ones. You may be happier now. It's like the polymorphic allocators proposal, you might want to receive requests from FastCGI and the built in server wihtout the need to recompile your app. You might even want to share handlers through plugins.
OK, that would make sense but remember that those libraries are not necessarily designed to work on architectures like the PS3, the Wii and so on. It would require patching them (e.g. [boost for android](https://svn.boost.org/trac/boost/ticket/7716)), and if they release a new version, patching them again because they won’t officially support your architecture (unless you offer all boost developers a PS3 devkit haha). In addition, most console games, for technical reasons, are compiled with no exceptions and no rtti when those libraries heavily rely on them. Which lead to problems like: is the state of a std::map still valid if insert fails? Would you like to delve in the source code of boost to find out? Also, from the comments I have read, I think people believe game devs write a new version of the STL for every project: no, they roll it out once, and then it is shared among all projects, improved and ported onto new platforms.
War story time ! I remember one "bug", not in the standard library but in a compiler. We wanted to do a simple flag check and had written: bool is_nth_flag_set = (a &gt;&gt; N) &amp; 1; But in some cases, N was greater than the number of bits in a, and we thought that it would set (a&gt;&gt;N) to 0, but instead, the compiler entered in an undefined behavior and generated wrong assembler. Or maybe it did generate good assembly but the processor did not expect such a large operand and analyzed the opcode as a different instruction, I can’t remember. Anyway, it took us a couple weeks to work out what originally came to us as "A tester has experienced one crash after playing the game for 4 hours". Another one happened because we were using very large files to store the game's data in (just like ID Software's WAD files), and on a given platform the variable storing the value of the seek position in a file had been internally truncated to a 32-bit unsigned integer. It resulted in an overflow when reading parts of the file that were located at the end of it. Bugs like this are obviously not frequent, but they exist: you should see the dev forums of Sony and Nintendo. I think the very hackish nature of game development makes our programs more subject to them. EDIT: also, as I answered to the /u/tisti, game devs reuse their own implementation of the STL and they share it across projects. They don’t start from scratch every time!
Exactly! game devs reuse the implementations they wrote 10 years ago, and they improved and fixed meanwhile.
I think you missed the point. The std that ships with the major compilers is very thoroughly tested, in the 10 years many many more eyes have inspected the code and ran into edge-case bugs than you will ever with a roll-your-own std. The more people use a specific std the better it gets, the popular std's are popular for a reason, they withstood the test of time.
What compiler is that syntax colouring from the screenshots from? It looks like an early Borland one. I fell in love with it years ago and ported it to my visual studio setup.
&gt; But in some cases, `N` was greater than the number of bits in `a` That's a user bug, not a compiler bug. C++03 5.8 [expr.shift]/1: "The behavior is undefined if the right operand is negative, or greater than or equal to the length in bits of the promoted left operand."
If N was greater than the number of bits, then that was a problem with your code, not a compiler bug. Shifting further than the number of bits on the left hand side is undefined behavior according to the C++ standard.
Now that we have variadic templates, it's only a matter of time before we get a type-safe printf/scanf pair (I hope.)
I like the comments about vertical space. I do most of my professional work in C#, where the standard style requires braces on its own line. I'm okay with conforming to this even though I don't really like it, because code should follow a consistent style. There are some people, however, who take it to extremes. I've seen the following style and it makes me die a little inside: /// /// &lt;summary&gt; /// Frobnicates the foo with the bar /// &lt;/summary&gt; /// public static void Frobnicate( Foo foo, Bar bar ) { for ( int i = 0 ; i &lt; foo.Widgets.Count ; i++ ) { // // frobnicate the individual widget // bar.Baz(foo.Widgets[i]); } } It's my belief that the people who picked this style think it makes their code more readable, and lends it an air of respectability. Instead, it turns a short class into a hundred lines and hides bugs because you can't see the entirety of any moderately complex method. I'm still young, but my philosophy so far is that code should be shortened when such a shortening produces no significant performance impact and doesn't make the intent less obvious. More lines of code almost always means more challenges to maintain.
Most people (including the author of the poorly-thought-out linked article) don't understand why the STL's implementation is ugly. The fact that both VC's and GCC's implementations are ugly should have made the author realize that something deeper is at work. * First, the STL is required to use `_Ugly` or `__ugly` names for everything (including local variables) that isn't publicly documented like `push_back`. (Obviously only headers, not separately compiled sources, follow this rule.) This achieves extreme resistance to name collisions and macroization, at the cost of turning these names into eye-stabbing ice picks. * The STL is required to be extremely generic and reasonably customizable. We have to be prepared to work with arbitrary user-defined types as specified by the Standard, which has extensive fallout - we need to invoke their copy ctors and dtors, we have to respect their alignment, we have to be prepared for operations to throw exceptions, we have to perform careful compile-time dances in order to respect movable-only types, etc. Customizability points require more machinery - e.g. a significant amount of container complexity comes from supporting custom allocators. * The STL cares deeply about runtime performance - what would be an unhealthy obsession with micro-optimization in ordinary application code is entirely appropriate for the STL. For example, we'll use metaprogramming to detect that code (e.g. `std::copy()` or `vector` reallocation) is trying to copy integers, and invoke `memmove()` instead of a copy constructor loop. * Everything else the STL does introduces ugliness. For example, debug checks require a fair amount of support machinery (and the payoff is that they can detect invalidation, which is otherwise very difficult to hunt down). Being a fundamental library means that the STL has to respect various properties of the platform, even those outside the Standard - e.g. VC's STL has to respect custom calling conventions and /clr, regardless of whether an individual user has any code depending on that stuff. One concern applies strongly to Boost, somewhat to GCC's libstdc++ and clang's libc++, and occasionally to VC's STL: * Supporting multiple platforms/compilers is a huge headache (especially supporting older compilers). VC's STL has the luxury of targeting a single OS (Windows) and a single compiler version (the latest) but even we have to deal with multiple architectures (x86/x64/ARM) and OS variation (e.g. supporting XP through Win8.1). And finally there are the non-fundamental reasons: * I can't speak for GCC, but at least in VC we're very busy implementing features and fixing bugs. Making our sources readable to untrained eyes (most of which aren't prepared for the heavy-duty C++ machinery we must use anyways) is at the bottom of our priority list. I try to ensure that our minimal comments aren't outright lies, but otherwise we have far more important things to do. (Note that the STL has one advantage that most libraries don't have - it has an independent specification from which documentation can be derived, so code comments don't have to perform the task of documentation.) * Personal style preferences (e.g. bracing, tabs, etc.).
Have you seen [ea stl](https://github.com/paulhodge/EASTL)?
Comments are bad, documentation is bad bla bla bla... this guy never saw [a good documentation example](http://qt-project.org/doc/qt-4.8/qpainter.html#details) in his entire life. And function should be completely isolated, do only one thing and you should understand it from its name is a nice thing to have, but not a rule of the universe. [This is one of the best comments in code I've ever seen](http://stackoverflow.com/a/378987/883113). But I agree that [obvious comments and the other class of bad comments](http://prog-xp.blogspot.com.br/2011/01/creating-simple-web-server-with-pion.html) he cites are extremely annoying. About the rest of the article, I feel like all is very basic and like I've learnt nothing new. He even mixes up declaration and definition ("_We_ _learned_ _all_ _that_ _information_ _from_ _the_ _the_ _function_ _definition_"). About "_these_ _are_ _written_ _by_ _some_ _of_ _the_ _best_ _C++_ _programmers_ _in_ _the_ _world_ _and_ _great_ _care_ _was_ _taken_ _to_ _make_ _them_ _as_ _beautiful_ _as_ _possible_", they had more objectives like handle the C preprocessor fragility and _make_ _your_ _life_ _better_.
Your comment deserves more attention than the original article. ;)
Monads and sloths *scnr*
What's your opinion on the recoding of a detemplatized STL that the Doom3 team did? Is the STL generic enough that there should be no scope (even for high-performance apps like 3D games) for reinventing the wheel nowadays? E.g., can you comment to what extent MS "dogfoods" its own STL when building Windows/Office or any other application? 
Looks like vim with syntax highlighting.
The WebGL based 3D charts look they will be fairly easy to use. When I tried to find a javascript 3D charting API about six months ago, I didn't find many options. It will also be nice to fall back on the servers OpenGL when the client doesn't have WebGL.
You can already do a lot of this in c++ with their iostream implementation. For example, check how easy it is to write to a gzip compressed stream using a simple filter with [boost](http://www.boost.org/doc/libs/1_55_0/libs/iostreams/doc/index.html).
I think you are confusing C APIs with C++. I think a C++ programmer *would* implement it this way. I mean, isn't that exactly the idea of a stream?
C++ has input/output streams that work the same way. And they actually inspired java ones... Like java, you just need to write the "read"(&gt;&gt;) and "write" (&lt;&lt;) functions. And its very easy to chain them. You can do things like: network &lt;&lt; name &lt;&lt; data &lt;&lt; fubar; And the C way(like *file) is not idiomatic C++. On C++ a handle is encapsulated on a RAII class (&lt;&lt; and &gt;&gt; are also the shift operators but that's totally unrelated)
&gt;I know the epsilon won't be changed within the function, (although it could easily be copied to another value and scaled for instance, but that would be counter productive) I don't understand why, as an outsider looking in, this is noteworthy. So the function, internally, doesn't modify a value it took by value. **Who cares?** If it does change that value, it's not going to be externally visible anyway. IMO, taking things by const value leaks implementation details into the external interface.
you are gonna love this: my team leader tells me this class was broken in VS2010 when we ported from 2005. what was broken that beside some POD class had some usually short string member and class was being memcopied :D I guess SSO limits changed or you added debug checks or something like that. :)
article is crap and author is an braindead att/click who?e who knows he will get more attention if he offends STL and auto - a bunch of butthurt cpp fanatics will rage on him... so as a fellow cpp fanatic I can only say dont feed the trolls, and dont click on clickwho?e articles on the other hand Carmack is awesome : I am a full const nazi nowadays, and I chide any programmer that doesn’t const every variable and parameter that can be.
Functions should never be *declared* as taking by const value (which has no effect - the compiler ignores it), but they can (and should, when possible) be *defined* as taking by const value. This is one of the very few cases in which declarations should differ from definitions.
Sad to hear it, I'm very new to CPP so any criticism is interesting I mostly linked it because I'm a real Carmack fanatic
What are the performance implications of using coroutine over storing the current edge number and step number and fetching them each iteration?
asio::coroutine is a single int and the reenter macro is just a switch a statement. It is essentially a pretty version of storing the current edge number.
As I recall, at the time Doom3 was being written the STL was in much worse shape for the specific requirements of a game engine. In the comments of the article Carmack says he'd use the STL now.
That is one subjective post.
You cannot believe how annoying the sound is for German-speakers, since we understand every word of what they actually say. 
I feel your pain. I know a little german and I have to not pay attention to the sound, which gets annoying. There should be a foreign dub so even German-speaking viewers could get a kick out of it.
Look for the Print link. Single page article.
&gt; The C++ way is to subclass streambuf. In theory, but everyone I've heard this from also thinks it's a fundamentally broken approach. Consequently, I don't think I've ever seen this done in real life (unlike most idiomatic C++). It's far easier to use a decorator on a i/ostream, as described in the OP. 
Is there any C++ toolkit without lots of `new`s in the helloworld program? 
Yeah, binary literals are really nice. I used them to implement a 8x16 bitmap font in a readable way, just storing a glyph as 16 bytes. Together with GCC's support for designated initializers for arrays, I was able to e.g. store a '6' like that: ['6'] = {{ 0b00000000, 0b00000000, 0b00111000, 0b01100000, 0b11000000, 0b11000000, 0b11111100, 0b11000110, 0b11000110, 0b11000110, 0b11000110, 0b01111100, 0b00000000, 0b00000000, 0b00000000, 0b00000000 }}, 
&gt; “If you were interviewing me, what question would you ask me” This sort of question inevitably pisses me off. I am not a manager at any company, I have no intention of hiring you, and you haven't given me your résumé. If you're out of useful questions to ask, let's fuck off and stop wasting our time, shall we?
&gt; I am not a manager at any company It was asked of me by a senior C++ programmer, in a very technical C++ interview. Did you actually read the article?
The article was speaking of asking questions in first-person, indicating that the writer of the article was the one asking the questions. If you're involved somehow, then congratulations, I guess?
&gt; He interviewed me, and after a couple of questions said, “Well, you obviously know a lot more about C++ than I do, so tell me, if you were interviewing me, what questions would you ask?” Who exactly do you think the first "me" is in this context?
Yes. And your point is?
&gt; My point is in the first comment, to which you replied, as well as the fact that the text of the article indicates that the writer is the one asking and recommending these question Which is the case. &gt; It's a bullshit question that helps nothing Obviously, I disagree. In the actual interview at LB, I (obviously) said that the question I would ask is "Tell me about the copy constructor". And then we had a long discussion about that. And I got the job. &gt; Another knowledgeable knerd, with whom I will hopefully have a good, constructive rapport Given your posts here and elsewhere, I would be worried if I had such a rapport.
Love you back, pumpkin.
What are some questions you would ask about inheritance &amp; polymorphism?
I probably wouldn't. Both are hugely overused.
I don't think it's a bullshit question for two reasons: 1. Whatever the candidate says may make a good addition to your list of interview questions in the future, but more importantly, 2. It's open-ended and a good way to get somebody to talk about something they are passionate and knowledgeable about. I don't know why it would piss you off. It's offering you a chance to control the direction of the interview on a _silver platter._
A simple downvote is all that's needed. And keep taking the anti-psychotics.
How could I predict what someone I am going to hire is going to do in the future? Certainly, if I set them the programming exercise I specified in my blog, and they came up with an inheritance-based solution, I might be a bit bemused. But otherwise.... 
&gt; myprog.bible.txt &gt; The file bible.txt, which is provided, contains the text of the King James Bible. You shouldn't mention the Bible as an interviewer - that's way too dangerous even if it's innocently intended. Use something else (e.g. for actual tests I use Flatland).
Of course. And did you read the article to the end? Having said that, I must say I have never been interviewed by someone who knew as much about C++ as I do. This is not false modesty - I've always been able to say "I think you will find, if you look in the C++ Standard at section xxx.yy, you will see that..." It's one of my many repulsive personal characteristics. 
I've never met anyone who "loves" inheritance, any more than I have one who loves addition.
If someone objects to doing text processing on the bible (or the Koran, or the Bagadadavita, or whatever) , then that is someone then that is someone I don't want to work with. &gt; Flatland Have you actually read it? It's all about theocracy.
IMO: \#1 isn't the candidate's job, and it's not why they're there---they're being asked questions by the interviewer, who has hopefully done enough homework to have enough questions. \#2 is just as well done by asking the person directly what they're passionate and knowledgeable about, and/or engaging them in an actual conversation. (The ability to engage comfortably in conversation without prompting is, frankly, well worth both the interviewer and interviewee's time to gauge.) Controlling the direction of the interview: Why? To what end? Neither party is interviewing solely for pleasure, presumably, and it's as well for everybody to give/get the information they need to give/get and be on their way to do something more productive, since time is (nearly literally) money. Moreover, why would I, as a worker, want to work someplace where one of the first things I encounter is an interviewer awkwardly wasting time?
I know you have code snippets on your blog, but do you have a github repo or anything where I can check out your code?
https://bitbucket.org/neilb
I initially misread the title as being: **"Gonads and Sluts"**
Nope, I've never hired an academic, only engineers - and pretty successfully.
 1. No, it's not their job, but in my experiences a lot of people actually giving technical interviews _aren't_ very well prepared. I agree that they should be. 2. Yeah there are any number of ways to get this to occur, but it's good that we agree getting it to occur is desirable. On controlling the direction of the interview: From the candidate's perspective, I think this is strategically beneficial. It gives you the opportunity to present your strengths on your terms and hopefully gives you a better opportunity to find out whether you really want to work there. I think it's only awkwardly wasting time if the only reason the interviewer asks is because they're woefully underprepared and don't have anything better to ask, or if the candidate fails to take it as a prompt to talk about whatever it is that makes them awesome. I think there are a lot of people giving technical interviews and missing out on great candidates because they have a very rigid set of specific interview questions. A candidate could miss 3/10 of these while the interviewer misses the 20 other awesome qualities the candidate has.
Didn't cross my mind, but it looks indeed like some sort of "now that I've got your attention" title :-)
I agree with /r/crusader561... I run engineering teams and interview programmers all the time. I care about whether or not someone can write code, and whether they understand data structures, algorithms, networking, distributed systems, and at least a passing knowledge of databases, message queues, async. vs. sync, threading, latency, etc. It's not important to me that they are able to answer questions they can google. Even if someone didn't know the particular language, but I thought they were an excellent engineer, I'd let them learn the language on the job. Languages are far less important than actually knowing WTF you're doing. That said, they have to code in the interview. I don't even care which language they choose (most choose Python or Java, some use C/C++). 
Andrei Alexandrescu gave a talk about how to implement that, I think it was a going native talk from the most recent going native conference, or possibly the one before it. It seemed fairly straight-forward.
Do you follow a specific coding style/standard and if so, which one?
Mine.
I read the entire article and I'm obviously agreeing with your final point :P. Also, it sounds like you need to get out of your small pond.
In the United States, an employer can be sued into oblivion for discriminating during hiring, so there's all sorts of things you can't ask candidates. Even stuff you might consider mundane, like whether they have children. The correct response is to stay the hell away from all of the radioactive topics, and that includes religion. This is part of interviewer training at all large companies.
&gt; It risks legal trouble for pretty much zero gain. Not in the Uk. &gt; Also, you're asking STL if he's actually read Flatland? Well, he may have read it. Maybe he hasn't understood it. Do detect some sort of fanboi thing going on here?
I think what he proposes is a great idea and violates no laws. Anyone who gets annoyed by the bible has no grounds to sue and is somebody you don't want in your organization. Hell, I might hang a huge crucifix on the wall just for interviews to scare aware the vampires.
Yeah it is pretty easy. I think it was even an example way back when variadic templates were first announced. Maybe one day it will be available alongside the iostreams in the standard library.
Please post some link to a US law that forbids you from asking people to write programs to process the text of the King James Bible. Please note, I am not a Christian - substitute KJB for anything you like. And I don't often ask this, but I am a bit bemused by the downvotes.
The purpose of a lot of that isn't just to space things out, but to also ensure that changes are on a line basis and not column basis. Many tools, notably the version control systems are line oriented, so this greatly helps in managing changes. On large teams where actively developed code can change rather quickly, this can drastically reduce the amount of time spent on managing merge conflicts.
I can see the value in that. I still think people-readability is a much better goal than tool-readability. The extra blank spaces in the comments have no excuse either way.
I liked the article. Your comments here make you seem like an insufferable cunt, and I hope it'd come through in an interview, so I could decide not to take the job.
No, it's someone who actually knows C++.
&gt; Your comments here make you seem like an insufferable cunt Specifically?
&gt; ou stated that if it is against there religious beliefs to do so you wouldn't want them working for you. I certainly did not.
http://www.reddit.com/r/cpp/comments/20kaem/my_c_interview_questions/cg45hax
Look in that for "religious belief". Find it? No.
Sounds like the kind of interview I'd be enjoying on both sides of the table
&gt; If someone objects to doing text processing on the bible (or the Koran, or the Bagadadavita, or whatever), then that is someone then that is someone I don't want to work with. Let's break that down: &gt; If someone objects to doing text processing on the bible (or the Koran, or the Bagadadavita, or whatever) That is the "it" /u/benjobeast is referring to. If that is against their religious believes, then: &gt; ...that is someone then that is someone I don't want to work with. Don't even mention religion during an interview. I'm not going to suggest people scan *Mein Kampf* during an interview, either. Just pick a different book. Why is that a big deal?
You and I seem to be in a minority of 2 here - I honestly did not think that this harmless blog post would cause such outrage. I see now why this reddit doesn't get much traffic.
The point is that talking about religion in any way in an interview is usually forbidden by company policy. The reason for that is that it's too easy to veer off from a religious conversation that isn't illegal to one that is. For example, if you ask someone to write a program that uses the bible as text corpus, and the person happens to say offhandedly while working that they've never actually read the bible, then they would have grounds to sue for discrimination if they weren't hired because it could be argued that interview question was a stealth way of determining if they were a devout christian by judging their reaction to the prompt. Asking the interviewee what religion they are (or if they are religious at all) is strictly illegal under federal law in the US, and consequently the shitwads that care about that kind of thing have to resort to these kind of off-hand, roundabout ways of sussing it out. You don't want to accidentally associate your company with the likes of those people, intentionally or not. There are plenty of freely available text corpora, so there's just no reason to use anything as charged as a religious text.
On objection to processing religious text would very likely be a religious objection. 
It's ranked 4th in the TIOBE Index for March 2014. http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html
I'm not aware of any religious groups that prohibit processing the text of the bible (in fact, most seem to go for it, full swing), but yes, I do not want to work with such people, if they have such idiotic beliefs. After all, what next - my religion prohibits me from working with XML? Actually, I could quite get behind that. But anyway, none of this applies in the jurisdiction I live in. If you and other loonies have problems with this, then tough shit. 
&gt; The point is that talking about religion in any way in an interview is usually forbidden by company policy. Nowhere I've ever worked. And please notice I'm not asking anyone to read the bible, or to talk about religion. And that I am not a Christian. &gt; Asking the interviewee what religion they are Oh, please!!!!!!! What????
Sorry for sounding ignorant, because I am. I just learned java and javascript are two different things. What a world, what a world. So is it safe to say C++ would be good to learn since it's so widely used? I've only even looked at JavaScript and I wonder if any of the language in javascript is similar, or shares some type of a trait with other codes. Like Algebra and Geometry in the sens that they're not exactly the same, but they have the same mathematical infrastructure. 
There is no such thing as a "universal best" programming language -- it all depends on the context. If I'm developing a web app, C++ is a terrible choice, while JavaScript would be a very good one. If I'm developing a high-performance scientific computing system, I'm probably writing FORTRAN. If I'm on iOS, I'm probably working with Objective C. If I'm processing numbers and making graphs, I'm using R. Since you're new to programming, I will say that C++ is a *very difficult* language to pick up. After JavaScript, I would recommend Python if you like the quick and easy feel of JavaScript *or* you could go with C# or Java if you want to jump into statically-typed languages (C++ is also in the family of static-typed languages: http://en.wikipedia.org/wiki/Type_system#Type_checking).
Could you explain this a little more? If declaring them as such is ignored, why not go ahead and make the declaration appear the same as the definition? Either way, I think this is a case I was unaware of.
I think I'm going to go into Java, and then maybe HTTP. What would be a good program for writing an app or program for android? I'm just curious because I own an android phone. 
I can't ask what their plans are for christmas vacation next week? I really don't believe the risk you've laid out is credible. If it were, you'd have to worry about *anything* that could conceivably have bearing on age, military background, race, national origin, etc. And what the hell *couldn't* be construed as relevant in some way to all that equal opportunity stuff taken together? You couldn't open your mouth as an interviewer. Really, what we've learned here is that for some strange reason any artifact of Christianity throws various people in this thread into conniptions.
To be blunt, 'which code is the "best"' is a meaningless question (assuming "code" = "programming language"). As you learn more about different parts of computer science and pick up a few programming languages over the years, you'll see the merits of one vs the other.
I'd rather ask them what's their favorite compiler than book. Some people it helps I'm sure but I personally learn better by doing and specific research. In fact, I doubt you could learn anything cutting edge about C++11 from a book
"Tell me about the copy constructor" The copy constructor started from humble beginnings. Being the son of an out of work factory floor worker he had none of the advantages even other members of his class had. But through hard work, dedication, and leveraging the work of fellow instantiations, he was able to grow from a relatively unknown into a universal workhorse who's impact is felt everywhere. Even unseen you can find him in the background proving himself time and again to be an important member of society. We could not live without him. Yeah, I rather hate such interview questions and would very likely give you some smart ass response like that. Was once asked to, "describe a class," in one of my earlier interviews. I actually tried to answer that but each time the guy said it wasn't what he meant. Now I know better and expect interviewers to provide at least some modicum of a clue what they're looking for from the interviewee when they ask their questions.
Hired! And ninja edit - so why no downvotes?
&gt; Nowhere I've ever worked. You've related your experience. Other people have related their experience working for a company where HR would not allow anything even obliquely mentioning religion in any form. Different people have different experiences. &gt; Oh, please!!!!!!! What???? Read what I wrote carefully. I'm not saying that merely asking someone to write a program is the same as asking what religion they are, but that the consequences of asking them to do that can very easily lead to situations that could be interpreted in such a way, and that there is a long history in the US of these sort of indirect lines of questioning being used for exactly the purpose of determining someone's religion in order to discriminate against them, which is why the law is so strict and HR departments are so firm in avoiding anything even remotely related to the topic. 
You *tried* (actually not really, you just posted and basically called me stupid without any help) to help me on a post a put in r/learnprogramming concering a C++ program I was writing and you came off as THE biggest douche I've seen on the boards. I'm really, really glad that other people are seeing this. You are probably the most egotistical programmer I've seen on the programming subreddits and honestly discouraged me quite a bit when you responded to my questions (which is the opposite of the purpose of programming subreddits). One comment you had was just "Because when it comes to C++, I know WTF I'm talking about." before you ninja-edited it. I TRULY feel sorry for anyone that has to work with your in a professional setting as your interview process seems pretty shitty as well as your personality.
[heave sigh] OK, take this text file and produce a list of the 10 most common words in it. [Happy now?]
You should first find a language that walks you through the different concept of programming. C++ is an extremely tough one. It is useful, it is used, but it asks for a lot of precision in your writing. As such, it is not a good language to begin, IMO. Usually, Python and Java are used. They are pretty good for the basics, and they should be useful anyway. C is also a must-know, still IMO. Apart from the basics it should teach you, you would learn a lot about how the computer and its memory works. The only alternative to this would be to learn assembly. A final note : HTML is a markup language. It is just a way to associate meta-informations to a text. It is not a programming language.
Sure, you'll recognize stuff from javascript in C++, they both use a C-like syntax for instance. JavaScript is a dynamic language though, and C++ is a strongly typed compiled language, so there'll be lots of differences too. Java, C# and C++ have more in common with each other than any of them do with JavaScript.
Links? But you are right, I do think you are an idiot.
Oh, alright. I'm thinking of going over Java next, so I guess I'll go over C++ after that. 
&gt; I can't ask what their plans are for christmas vacation next week? In a job interview? Why on earth would that be pertinent or appropriate?
Thanks for the info on HTML, the only reason I associated that with programming languages is because it's an option to learn on code academy. I'm currently going over javascript, and I think I'll go over Java next. 
HTTP is not a programming language, it is a protocole of communication. You want to learn Java to program on Android.
Because a job interview involves two human beings chatting. This is the sort of thing normal human beings incidentally chat about. It's polite. Failing to treat the subject like a person is impolite.
Not a reasonable one though and one that could indicate possible future problems. A person who expects to be able to work while objecting to doing their job on religious grounds is not a good hire. It should be a no brainer that this is a legitimate reason to not hire a person, yet you're right...it could result in lawsuits and other nonsense. On the other hand, being asked to process the KJB would raise huge red flags for me. It would make me wonder if I'm interviewing at a place that sees no problem bombarding me with religious propaganda on a regular basis. Not something I want to have to put up with on my daily grind.
Usually "code" refers to the text a programmers writes as the source of his software. Actually, it's not just usually, it is what it means. It's pretty weird to read that in place of programming language.
Computer science is a great major! (Note that theoretical CS, like the analysis and design of algorithms, is different from applied CS, a.k.a programming or software engineering. My degree in CS focused on the theory, and it's been useful in my work as a programmer even though I don't design new algorithms.) C++ is a general-purpose high-performance programming language, permitting both high-level abstractions and low-level hardware access. It isn't the easiest language in the world to learn, but if you're given proper guidance, it isn't as hard as some people say. By "general-purpose", I mean that C++ isn't focused on a single domain, like talking to databases or rendering graphics or scientific computing. C++ is used for lots and lots of stuff, although there are definitely areas in which it is stronger or weaker. By "high-performance", I mean that C++ avoids getting between the programmer and the full power of the hardware. Other languages provide helpful machinery at the cost of making programs slower, sometimes significantly so. A couple of tips: First, C++ rewards expertise and punishes ignorance more so than other languages. If you use C++, you should be prepared on focus on it, not just dabble in it (although you don't have to focus on it to the exclusion of everything else, like I do). Second, C++ has superseded C. Some people will tell you to learn C before learning C++. *They are wrong* and it cost me a year and a half of my life to discover this. Don't make the same mistake.
Ah! Another ninja-edit. I see you do that pretty often. Honestly man, I don't care what you think. It bothered me when I was trying to get help and you told me the source I was learning from was shitty because you obvioulsy know everything there is to know about C++. But now I just see that you're all around pretty mean and I'm glad I can brush this experience off as "Oh, he's one of *those* programmers.", the kind everyone dislikes because you act like you know everything and instead of helping you boast, and not just as "Oh, I must be stupid if this guy is telling me I'm doing it so wrong." Thank you.
&gt; Ah! Another ninja-edit What?
What comments? Reading through the thread I see only some unpopular opinions and a slight mishap with a veteran of /r/cpp. Hardly justifies the backlash. I think the hive mind went completely awry here.
Yeah, zabzonk was one that sort of drove me off as well. Doesn't help that he's often giving wrong answers and they eat it up.
Sorry, again, the only prior knowledge of programming I have derives from the Matrix. 
Calling a virtual function is generally no more expensive than calling any other function, especially on modern hardware. The costs are in other things. For example, the fact that a virtual function cannot be inlined. Another example is the fact that it decreases the amount of information in the class that can be stored on the cache. I hope you're keeping that in mind when you ask that question.
If you are outside of the grace period and your comment shows that you have edited it, what would be so ninja about that? Damn. I thought someone with such grandiose knowledge of everything would have known that. My point is that you do that (in relation to how many times I've replied to your comments) pretty often. Doing this can skew the perspective of the comment someone has chosen to your reply to you. For example, when you said "Because when it comes to C++, I know WTF I'm talking about" and then ninja edited it, it changed the perception to others of my replied comment. The etiquette is that you always put "Edit: ... " even when you ninja edit. Edit: Let me know if you can't understand any of this, I'll dumb it down for you.
Sure. Declarations are meant for two audiences: compilers/linkers and humans. Obviously, declarations tell compilers/linkers how code connects together. For humans, declarations serve as the most basic and fundamental form of documentation: they say how to call a function. Due to C++'s rules, compilers/linkers ignore constness on value parameters in function signatures. (This applies everywhere - it doesn't matter in function pointer types, etc.) Since that audience doesn't care, we should consider the other audience. When reading a declaration to find out how to call a function, constness on value parameters is irrelevant - it doesn't affect the caller physically or even conceptually. (The whole point of functions is to hide implementation secrets behind an interface, and whether they modify their value parameters is an implementation detail.) Therefore, while having stuff match is generally desirable, eliminating visual noise is also desirable, and constness on value parameters in declarations is visual noise. By eliminating this, the consts that matter (behind pointers and references) stand out. It is a small detail, but important for good style.
Yeah, I made that mistake by just assuming it was a language. The only time I ever tried to do anything with Android was a few years back when I was trying to make a live wallpaper and apparently you had to make a code for it through some android friendly client. I gave up at that point, maybe one day I'll go back and conquer my past failure. Thanks for including that android subreddit, I'll definitely check it out eventually. 
&gt; And thats just the one's you didn't edit or delete I almost never delete posts (unlike many other people posting her), so what you see is what you get. &gt; You may be knowledgeable at C++ but I would never hire anyone with your attitude. Somehow, I will live with that. I post here to entertain myself, not particularly to help others. At least I'm honest in my motivation. And hey, you are so brave, setting up a special account just to make that post!
&gt; Masterbate with C++ in your head? I am totally going to try this now.
If you look through his comment history you can see he is pretty unhelpful and outright discouraging to some people asking questions. I feel sorry for /r/cpp if you guys hold him in high regard. He seems to be very knowledgeable, but also utterly disrespectful. 
Thanks for the advice. I'm still unsure what I'm going to do with my major. The most likely thing is work on upkeep for businesses, maybe, but I'd like to know just for myself how to make apps for android or similar applications. e
&gt; being able to BS about copy-ctor's for 15 minutes. I doubt you can BS about copy-ctor's for 15 minutes with a guru or a very knowledgeable engineer. But if you can keep up a good discussion, I think it shows insight into how well you know the language and how well you know details that are less common knowledge. While not everyone is concerned with that, the OP clearly is.
Absolutely yes. There *are* situations where the compiler will inline a virtual function (if it can figure out the exact type of `x` when calling `x.foo()`). There are certain kinds of people who respond with how ridiculously expensive C++ virtual calls are, etc, etc who I probably do not want working in my shop. I had one candidate tell me how he essentially implemented vtables in C, but thought that his implementation was faster because it didn't involve the `virtual` keyword.
&gt; For the most part, they don't understand the 80-20 rule and will spend inordinate amounts of time on minutia - like implementing copy-ctor's that are never used. I run into many a case when the original author never created a copy constructor for the class. Then an instantiation is copied somewhere and all hell breaks loose. Since 80% or more of all classes in C++ have a copy constructor at least thinking about its effects should be an your mind at all times. If you intend the class not to be copied then at least state so by making sure the default one can never be called. The 80/20 rule should never be used to excuse poor code quality and not doing things that demonstrably improve the maintainability of the product. Applying this concept in the way you are is a mistake and is not using it to its original intent. The 80/20 rule is about the implementation of features and the fixing of bugs. It helps decide if something should be developed or fixed. It shouldn't be used to discount doing something that you should be doing 100% of the time.
http://img.photobucket.com/albums/v111/DeeBohDamn/FlameOnHumanTorch.gif#human%20torch%20flame%20on%20200x188
Books are still pretty important but you could qualify the question by suggesting the inclusion of blogs. What /I/ want to know is that the person does self-directed study.
I'll do what I want, thanks.
Well, I don't know much about C++, and after reading the article was curious about how someone can talk for 4h about copy constructors. I guess I gotta come back another day to learn something, as you're too busy bashing the author because he has opinions.
C++ reign supreme, got it. I might check out C++, or at least get the basics of it, but I don't know if I'll be a C++ crusader. 
The candidate could have been correct. You can in fact gain speed by reimplementing virtual dispatch with a static dispatch table you then refer to by name rather than pointer. Some compilers will perform this optimization for you in some cases, which is pretty incredible. Alexandrescu discusses this in his talk at Going Native 2013.
Your version of fun clearly harassing people on the Internet. That's the definition of troll, wear it with honor dear troll. 
OK, whatever.
I disagree. I haven't found many good online resources on C++ that aren't just presentations or references (most of those aren't even half-good, some even contain harmful code). The best resources on C++ are books. Although cppreference.com is a very good as a reference site.
&gt; I will say that C++ is a very difficult language to pick up. C++ is massive, being a three-decade accretion of features. [Stroustrup's indispensable book](http://www.amazon.com/The-Programming-Language-4th-Edition/dp/0321563840/ref=sr_1_1?ie=UTF8&amp;qid=1395008091&amp;sr=8-1&amp;keywords=stroustrup+c%2B%2B), over 1300 dense pages, is only a succession of introductions to various aspects of the language. You often go to the web to get details.
This might be the stupidest interview I have ever heard unless they are actually developing a compiler or writing a book called "C++ for pedantics" I would be 100% happy hiring someone for a C++ job if they hadn't used C++ in 10 years and had forgotten most of what they knew. I am looking for things like an understanding of patterns, approaches to problem solving, interesting projects they had worked on, and their general philosophy toward software development. An ace programmer would be someone who would say, "I am up to question 150 on Project Euler and I switch languages ever 30 problems with Haskell being the latest. " In fact the developers who knew too much about a single system were the ones that I have seen causing the worst problems. Often they have a "my way or the highway" approach and tend to over-complicate things. Basically I have worked with too many programmers who would use multiple inheritance, exception handling, some #ifdefs, and templates combined with a bit of inline ASM to do hello world. 
I think zabzonk's point was that he wouldn't want to work with someone who finds that offensive, disregarding their religion.
&gt; Because a job interview involves two human beings chatting. It might seem that way, but it's not really.
I don't want to work with anyone that finds abortion offensive, disregarding their religion. I know that's a different issue but it still can be a religious belief or a secular belief. 
&gt; I'm thinking of going over Java next, so I guess I'll go over C++ after that. If your final target is C++, then **don't**. The similarities are highly superficial. C++ and Java are based on totally different concepts. One of the biggest problems that C++ has is in fact that people are trying to program Java in it. The result of that is always **huge** pain and a an unfair hatred towards C++, because it works differently.
To provide some context: STL is the maintainer of microsofts C++-standard-library and is a member of the ISO-committee that develops C++. So he might be somewhat biased. OTOH: As usual I totally agree with what he said.
The 4th edition of TCPL isn't exactly outdated as far as C++11 goes, though otherwise I'd agree that non-'book smarts' shouldn't be discounted.
I'd imagine it depends on the kind of position you're hiring for. If you're hiring for a short-term contract you'll want someone who can instantly jump in and do the job, so questions to test someone's skill at a specific language are legit. If it's a permanent position, though, you're right- someone who lacks experience with the tech he'd be using but is clearly a smart programmer is the better hire than someone who's used that tech all his life but isn't as smart as the first guy.
You appear to be butthurt. If you cannot deal with people talking back to you, you should stay away from "social" websites where you interact with other people.
Flatland? Too risky. I prefer "The Giving Tree". 
I obviously can deal with talking back to people - I'm talking back to a retard like you right now. Sorry, I am not going to fold to bullies like you - downvote away. It's not like the votes have any meaning. 
I don't really have a final goal I'm just dabbling in computer science to get a feel for it and have a basic understanding of some sort before I start school. 
You will find just as much wrong and harmful code if not even more in a book. At least when it is online it is usually able to be corrected after being pointed out. When it is in a book then you are screwed if you don't buy the newest edition of the book.
The age old cry of the douche.
Awesome. I tagged OP as "Insufferable C++ cunt". He reminds me of every arrogant programmer with no social graces that you come across now and again in this field.
Ah you may be an old douche someday. Have pity.
I like you, I've tagged you as "Stood up to zabzonk"
&gt; Beginning programmers, however, can be overwhelmed by C++'s vastness. Speaking of vastness, I think that [this Penny Arcade comic](http://www.penny-arcade.com/comic/2007/11/19/nitpicking-mass-effect-part-three) is equally applicable to Mass Effect and C++.
Of course, but it's easier to find books that are good resources on C++ than it is to find online resources. &gt; When it is in a book then you are screwed if you don't buy the newest edition of the book. Not really I don't think. I should also add that the only reason I think books are useful are as primers anyway. I do use online resources, but I learned from books to start with. In my experiences this is the way to go. Others have different experiences obvious.
&gt; Some people will tell you to learn C before learning C++. They are wrong and it cost me a year and a half of my life to discover this. Don't make the same mistake. How can you be sure learning C was a mistake, maybe it helped and it it just your emotion once you discovered better tool. A lot of programs need to talk to C libraries, that alone make learning C useful.
Well, I agree with the author about copy constructors... so I'll answer this for him. C++ lets you control how objects are copied. This is important because the language supports pointers and references where both deep and shallow copying might be appropriate depending on the specific type in question. To complicate matters, C++ will provide an automatically implemented copy constructor and assignment operator even if you don't specify one in the class declaration. The default implementation of these provide only shallow copying. To understand how dangerous this could be, imagine a string class that relied upon the default copy constructor. If string B was copy constructed form string A, and string A is destroyed... Well, B now has a pointer to free'd memory. Further, the C++ standard library makes extensive use of the copy constructors of the types you put in it... so even if you don't plan to use them directly, its very likely that you will indirectly through the STL... Because of all this, I always follow the advice given in the book Effective C++... If you don't want a copyable type, then declare a copy constructor and an assignment operator as private and unimplemented... This will cause any use them to result in a compiler error.
Sutter and Meyers both have blogs. Sutter I think writes more often. You'll find pretty solid info in them.
Calling C libraries from C++, where you should immediately wrap them in higher-level abstractions, is very different than calling them from C.
My opinion is that that's information you can't really do much with until after you learned some of the core. Not that my opinion should count I would add.
This thread has been linked to from elsewhere on reddit. - [/r/SubredditDrama] [Blogger/programmer posts his interview questions to /r/cpp, the comment stack overflows.](http://np.reddit.com/r/SubredditDrama/comments/20ldu3/bloggerprogrammer_posts_his_interview_questions/) *^I ^am ^a ^bot. ^Comments? ^Complaints? [^Send ^them ^to ^my ^inbox!](http://www.reddit.com/message/compose/?to=totes_meta_bot)* 
I guess what I have seen is too many companies that "staff up" which means hiring anyone who can fog a mirror. Often the primary criteria is various certifications. Often contracts are built upon the concept that each warm body will be billed hourly. Thus having warm bodies that are pasted with certifications will allow the company to deny that they were doing anything wrong when all those useless turds can't build something viable. We have all heard the story over and over where a company of highly certified but completely useless programmers have worked on some mega project for years only to have some junior guy upset the applecart by doing the whole thing on his own in little time in Python or some such. 
How about you rather ask Nvidius to properly support OpenCL?
Thanks, what was lost to me is that the language provides one by default! This seems to inherit from struct copying in C, but even in C we mostly encapsulate complex structs with ADTs and trying to do `*a_string = *another_string` will fail if the type string is behind a header. What is the rationale behind this choice? I assume the assignment operator by default calls the copy constructor? I program mostly in C, Java and Clojure, and in Java copy constructors aren't a thing. Everything is a reference, so assignment is simply a pointer copy; also, mutation runs rampart, so why copy this huge object state now if you can mutate it to be what you want? Cue several design patterns to deal with complexities, such as Command, Bloch's Builder, Memento... Clojure is a boon, everything's immutable by default and there's no risk to shoot yourself with incomplete copies or the like.
That's still copyrighted, you monster! :-&gt;
How does this compare to [Thrust](https://github.com/thrust/thrust) and/or what's been proposed by [C++ WG Proposal N35549](http://isocpp.org/blog/2013/03/n3554-a-parallel-algorithms-library-nvidia-microsoft-intel)? While this is super useful stuff, it looks like there are dozens of people inventing the same wheel :(
&gt; Not really I don't think. How do you figure? I mean its not like these companies are going to come out and say there are errors here is a new version of the book
It was ugly though. And wasn't particularly safe either, because from memory, his solution used some sort of a recursive templating system and had raw string pointer iteration happening, which checked for the terminating `'\0'`.
It's trivial to create a tracer member class that adds debug statements for constructors and destructors while still following the Rule of Zero. For example: http://ideone.com/UKtjZ3
So is it just a layer on top of OpenCL? If this is the case, why not use OpenCL directly? 
A smart engineer can pick up C++ quickly. It's not like learning Mandarin or something. Good engineers are far better than bad ones, and I'd rather have a good engineer who didn't know C++ than a bad one who did, but ended up not knowing a damn thing about programming or debugging. Besides, there's a lot of C programmers who don't know C++ who can dance circles around most of the rest of engineering teams. 
I said you cannot deal with people talking back to you, not the other way around. Nobody is bullying you, or at least nobody is doing it for no reason. As you already have been told, you behave very unfriendly towards everyone. Even your very first comment here asks "Did you actually read the article?". The way you treat others is how others will treat you. Think about that and you might find out, that acting like an offended child is not a useful way for you to behave. btw, you obviously do care about downvotes. Otherwise you wouldn't have mentioned them to me. Though I don't see the point of downvoting you anymore than other people already did since all of your comments are quite down in the negative.
Not quite, handles are an index value to an object in the kernel that is referenced via the handle processes own handle table that is also kept in the kernel. I would suggest getting a hold of the ntoskrnl source code from sysinternals if you want more details.
If it reduces the ridiculous amount of boilerplate code you have to write with the OpenCL API it's well worth it.
Why don't call it "Boost.OpenCLWrapper" then? The main issue I have with it is that you can't even pass a function object to it, which is something that _you can do_ with TBB, Thrust, CUDA, OpenACC, OpenMP, C++AMP (PPL), std/boost::thread, and N3554. Note that this is an OpenCL problem, not a Boost.Compute one, and can't be fixed without compiler support (i.e. there is no amount of TMP that could possibly fix this). AMD's Bolt and VexCL have the same problem. Once you try to do something not covered by the DSL offered by these libraries, you end up writing non-generic OpenCL code (and avoiding this was the only reason to use the library in the first place).
But is it worth it? It adds bugs, and if OpenCL adds extensions, then I have to wait for that layer to update and integrate them. Also I am not sure who it would benefit. If you are an experienced OpenCL programmer then you don't need it, and if you are new to OpenCL, then you would benefit more from learning OpenCL directly.
Well don't be so hasty there, even though dereferencing a point is fairly trivial it is an issue for the compiler as it prevents certain optimizations from being performed, and since handle and function argument are crucial to the definition of an API I would say solve this architectural problem first before it gets too ingrained in your code. Having said that the HANDLE type in the win32api is an index value for the process's handle table in the kernel object structure and so is an integer which is used as an id.
He discussed that in the last paragraph: &gt; By eliminating this, the consts that matter (behind pointers and references) stand out.
&gt; if OpenCL adds extensions, then I have to wait for that layer to update and integrate them. It seems you can still call out to opencl api, from the docs: &gt; The Boost Compute library is designed to easily interoperate with the OpenCL C API. All of the wrapped classes have conversion operators to their underlying OpenCL types which allows them to be passed directly to the OpenCL C functions. 
Why not use a 64-bit counter instead? 
http://crazycpp.wordpress.com/2011/01/16/hello-world/
I do wish OpenCL had better support for natively using C++. The standardized SPIR binary format should help in that regard. As for all the libraries you listed, none of them can execute code on GPUs without using a proprietary compiler (CUDA/Thrust) or using compiler extensions/pragmas (OpenACC/OpenMP/C++AMP). N3554 is interesting and I have been keeping my eye on it for a while though I feel Boost.Compute works at a slightly lower level.
Thanks. I will commit sepeku on my post.
I see Denis Demidov is a contributor to this. How does it compare with his own VexCL? Does Boost.Compute aim to supersede VexCL?
That is perfect; absolutely perfect. That there is one of the reasons I have been dumping C++ and switching to Python. For a short while C++ got back on track with the early mission statements of boost (header only library, commonly used functions becoming part of the standard lib) and the wonderful new for loop for iterating through various things like vectors. But then the pedantic types went mad with templates. Templates make some of the shit-hardest to read code in the universe. Basic templating like vectors, maps, and sets are fine but beyond that it starts to get just stupid. Look at the typical template used to go back and fourth between strings and ints. I wrap them in a macro so I can do it like normal languages but macros are a sure sign that something is wrong. I use C++ where I have to but otherwise I am now a card carrying member of the Python cult. 
See http://stackoverflow.com/questions/20154179/differences-between-vexcl-thrust-and-boost-compute. Answed by Denis Demidov himself.
Thanks, that clears things up a bit. And thanks for putting the effort into such an awesome library!
&gt;As for all the libraries you listed, none of them can execute code on GPUs without using a proprietary compiler (CUDA/Thrust) or using compiler extensions/pragmas (OpenACC/OpenMP/C++AMP). Since CUDA is also a compiler extension, I'd go further and consider that none of them generates GPU code without compiler extensions. This is a good thing. Even tho you can link against the OpenCL run-time without any compiler support, your kernels still need to be compiled by a special compiler, _at runtime_. Since all those other libraries offer you statically-typed kernels (which OpenCL does not by default), and since they all play well with generic programming (which OpenCL does not), I argue that they match better with C++ than OpenCL. &gt; I do wish OpenCL had better support for natively using C++. The standardized SPIR binary format should help in that regard. I wish that too, and SPIR will definitely help with this issue but it will need compiler support (it is just LLVM IR), and thus a language extension like e.g. C++AMP, that won't have much to do with OpenCL. So basically I hope that OpenCL will die, that SPIR will succeed, and that C++AMP or similar will go forward targeting SPIR at least in non Microsoft-based systems.