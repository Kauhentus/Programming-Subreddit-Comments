Header-only means that to ue the library you most of the time just have to `#include &lt;library.hpp&gt;` For non-header only libraries, you have to do this step as well, but also call your compiler with the correct options (most of the time you don't do this by hand but your build system or IDE does it for you). But what I wanted to say was that when compiling for instance : #include "foo.h" int main() { return 0; } will not (or almost not, unless one declared a static global array in the header...) add size to the built file since the compiler can prune unused code and merge identical functions across object files.
To standardize `#pragma once`, you'd need to standardize a mechanism for identifying a duplicate file, and I don't think you'll find a universal consensus on that. Do you use inode numbers? Then what do you do when a file is copied? Do you use names? Then what do you do if you see a copy as "a/foo.h" and one as "b/foo.h"? Do you use a checksum? Then what do you do if someone converts linefeed formats? And does that mean we have to read the file twice? Do we have to strip SCM revision information ($Id:....$) from comments? Can you get all compiler makers to agree on one solution? The good thing about header guards is that this forces you to decide on an identifier that defines what you mean with "this file". Sure you have to make it unique, but you also have to make your function names unique. I agree that having to spell out that identifier twice, and using an #endif at the end, isn't a very efficient solution and could need some improvement. But I don't think `#pragma once` as it exists is the solution. Simply because nobody knows what `#pragma once` does *exactly*. That aside, I haven't written include guards for years because my editor is doing that for me, so I wonder why people still discuss this topic at all.
&gt;We’ve implemented every C++ Standard Library feature that's been voted into C++11, C++14, and the C++17-so-far Working Paper N4567 (pre-Jacksonville). Sweet! ~~Hope that means ranges and Hana works now!~~ looks like not. Sorry, folks :/
There are legitimate cases for including multiple files with the same content though.
&gt; That aside, I haven't written include guards for years because my editor is doing that for me, so I wonder why people still discuss this topic at all. Ease of refactoring: if you rename your header, does your editor also rename the include guard? Or do you have random hex-strings as guards? 
Module syntax is kind of hideous.
&gt; the author categorized these issues as fixable, because the programmer can change the names of defines but they can't change the name of the file, or the path they reside in, or the convoluted path that confused the Linux/Windows API's equivalent of get_canonical_path ? hmm
No, then there would be one standard that doesn't piss in the face of features so useful that every single compiler implements anyway.
Oh my god. I actually had this problem and had no idea what was causing it. Holy shit. 
&gt; The mechanism doesn't have to be standard, only the expected behavior. It would be OK to explicitly call out undefined or implementation specific behavior in certain enumerable cases. Can you enumerate these cases without making the feature useless? &gt; Your editor might do it for you, but I can't make 25 other developers use the same tools with the same settings, no matter how much I try; it's like herding cats. The team has to agree upon things like tab width/tab usage, line length, line feed type, indentation, language, character set, etc., and everyone assumes these are standard settings. I wonder why something trivial like a context-dependant new-file template still is considered an obscure feature in 2016's programmer's editors?
&gt; `std::tuple&lt;decltype(std::bind(&amp;IteratorType::operator*, nullptr)())...&gt;` I'm not 100%, but I think that should be equivalent to `std::tuple&lt;decltype(*std::declval&lt;IteratorType&gt;())...&gt;`
Would it be possible to add callbacks? E.g. register a callback(std::function) which would be invoked when a property is set or when a method is called. Otherwise it is a great library. I love the philosophy behind it, especially: -ability to invoke properties and methods of classes from any arbitrary class level and no header pollution It is non-invasive and ideal for automatic binding wrappers. Btw, isn't this the same thing? https://github.com/billyquith/ponder
I probably know a lot more than you about C++. That's why I prefer to use C most of the time.
This guy made reference to obscure GCC bugs that have been fixed in 2003 : https://gcc.gnu.org/bugzilla/show_bug.cgi?id=11569
The compiler's Expression SFINAE has been improved to the point where the STL can take a dependency on it for &lt;functional&gt; (carefully, with workarounds). This also means that any equivalent libraries (e.g. boost::function) can do the same. And like Steve mentioned, we have multiple compiler devs checking in more Expression SFINAE bugfixes, fixing range-v3 blockers one by one.
I figure I'll let somebody post about their own project as their very first post. I'll look at repeated posts with no comment contributions in a more negative light, though.
With all that MS is doing with linux lately, I'd love to see a port of the compiler. gcc, clang and cl all natively on the same system would be great.
ok :)
make_tuple() in the operator*() will not preserve references to objects the internal iterators point to but make a tuple of copies instead. See how range-v3 implements the zip view here: https://github.com/ericniebler/range-v3/blob/master/include/range/v3/view/zip.hpp and https://github.com/ericniebler/range-v3/blob/master/include/range/v3/view/zip_with.hpp
That's possible. The message is very unclear and ambiguous then.
woa fantastic!
This is reeeaaally awesome. Wow! I got one "however" though: How does it work with CMake? If it doesn't yet, please make it work :-) Otherwise it's not really useful for anything but for toying around.
Hopefully this fixes the constant crashes I've been getting on some files, weird stuff, like mouse over and crash, it doesn't stay sane not even 30s on some pieces of code. Tried everything to no avail.
(see my response to sumo952 for the reasoning)
Hmm. It is actually quite confusing imho. Especially that the front page https://www.visualstudio.com/downloads/visual-studio-next-downloads-vs doesn't mention anything about that at all. Just add some well-visible info there. Or what about calling it Visual Studio 201x or something like that. 201x doesn't look that good though. No idea. Maybe just providing the info would be enough. &gt; zero friction between vs2015 and VS15Preview Sounds great.
What is that ? I mean, the fact that Visual support building code for linux is *really* cool, but, I don't get the site. On one hand it looks professionally made, on the other hand, I have a hard time trusting it was authored by Microsoft. It's is not a sub-domain of a Microsoft site, it is not signed in anyway, the whois is not useful either, and the sole reference google has to this site is this very post. Assuming the site is indeed legit, what is the marketing point of getting a new domain name for a new *feature* of an existing product ? https://www.visualstudio.com/features/linux is not cool enough or something ? /rant Did I say that the fact that visual studio supports linux build is really awesome ?
Cancelling, after &gt;20 mins it's still at ~20%, installing the UCRT, on an SSD. Maybe my VM is indeed borked.
Tried it out, and it looks awesome. Just a couple of questions 1) On the Platform Toolset it says Remote_GCC_1_0. Is there any way to change what compiler it uses or will it always use /usr/bin/g++. What if I wanted to build with clang? 2) For installing a new library, it looks like I would do the install in Linux, and copy the header files to Windows and add that directory to the Include directory for Intellisense. Is that correct? 3) Any plans for doing Debug Visualizers for the standard library types for stdlibc++? 4) Is there a way to see the output from the program when you run it in the debugger? When I looked at the Output tab, it just showed output from gdb 5) With the whole Ubuntu on Windows that was announced, are you planning on taking advantage of that to provide even more seamless integration? Thanks for doing this, this will really help me in my day to day development.
Do you plan on going full nega-microsoft and release a native linux port of VS ? I have a hat waiting to be eaten. More seriously, will you support clang or other compilers / debuggers ?
Other compilers are definitely interesting. Cross compile or still remote? When you say other debuggers, you mean like lldb? We're currently using the MIEngine in VS that integrates the VS debugger with machine interface so we can talk to GDB remotely. And I say we as I'm on the team working on this.
I am actually very interested in going the other way. Meaning a native MSVC compiler and toolchain on Linux. Imagine calling something like /opt/Microsoft/VC2015/bin/x86_64/cl in a Makefile and generating 64 bit PE files directly on Linux. I'm not asking for Visual Studio here just the basic command line tool chain.
Are you guys hiring?
we (Visual Studio) are hiring for people who are passionate about building tools for cross platform applications in Visual Studio. Everything from Cordoba apps, to VS Code, and stuff we can't talk about yet. Fill this in and it will send me a mail: https://www.visualstudio.com/en-us/news/visual-studio-jobs-vs.aspx 
thanks for the feedback. Just to drill in a bit, what's your primary reason for wanting that? - Steve the VC Dev Mgr
did you file a bug? if it doesn't fix that, please let me know. 
This would mostly be for developers that prefer the Linux desktop environment and would provide a single development environment. It would be an alternative to loading a VM to cross compile the software on Windows. Today we can just type "make windows" and use mingw32 to cross compile, so this would just give us the ability to use the "official" compiler to generate binaries.
I see... so basically the reverse of the scenario they demonstrated this morning for bash on windows? (i.e. using a windows box to target Linux and windows) I thought you meant you wanted to use the compiler for TARGETING Linux. But you really want to HOST on Linux and target Windows.
&gt; what's your primary reason for wanting that? r/unixporn (and a &lt; 1gb base system with all the necessary build tools &amp; libs to build cross platform desktop software + basic desktop functionality)
https://isocpp.org/std/the-life-of-an-iso-proposal
The only reason I ever touch Visual Studio is when something has to run on Windows. Now that I can just bundle Windows under the Ubuntu target for any software I build, I can say goodbye to Visual Studio forever. Hooray.
Duplicate names aren't bugs. It can be useful to have duplicate names if you want to have multiple header files that all do the same thing in different ways and you don't want them to conflict. You can have typos in any code. `#pragma once` just doesn't make sense. What is being only included once? The contents of that file, presumably hashed? The path the file is at? Some name that you include? If you're giving a name too, **just use header guards**.
Except nobody supports that. The whole idea behind adding `#pragma once` to the Standard would be that everyone already supports it. Either add it as it is or don't add it at all. That is, don't add it at all, because it is utterly broken as it is.
I've definitely included the same file multiple times and intended to do so.
Fuck fuck fuck fuck fuck off. No god no. That's exactly the opposite of what I want. With modules there will be no reason to ever use macros. Please don't export macros. Please don't allow people to export macros. Use constexpr. Use `std::source_location`. Use modules. Don't let people use macros.
It doesn't help anyone. It's braindead and terrible.
If you want to use `#pragma once`, you should just use it. It's legacy, it's broken, it's unfixable, it's semantically unclear and it's obsolete. But if you want to use it, use it.
But we don't want it to be standardised, because it's legacy, broken, unfixable, semantically unclear and obsolete junk.
If those developers don't want to have their editor do it for them, they can do it manually. If they try to use `#pragma once` they will just fail at code review and have to fix it.
-1 for #pragma once
when interacting with the community, I always try to ask open-ended questions ("what is your primary reason?") instead of closed questions ("is it because X?"). I am well aware that many people prefer Linux as a development environment. I'm quite glad I did because at first I had interpreted his question as wanting to target Linux from the compilers. 
Will this allow me to cross compile for an ARM Cortex-M7 microprocessor?
With whitespace specifically (and even line length), our online code review tool doesn't visualize whitespace characters. You can do it (pop-open a diff in beyond compare, which can visualize whitespace), but it's an extra step almost nobody but me takes. Then if somebody did something like copied an include guard from another file that they didn't change, the two files wouldn't both be in the changeset so you couldn't tell from the code review. If they were, I'd be surprised if somebody noticed, that kind of boilerplate is super easy to gloss over. At least Git is configured to enforce certain line endings, but like I said we don't have any kind of more complicated hooks to refuse a push if it violates more subtle rules.
Only if you are running Linux on it and can compile there. We don't have cross compile... yet.
You keep saying that, but I don't think it's true. Header guards are easier for somebody to screw up, and while there are cases where you could absolutely cause #pragma once to screw up, I've never seen one. I think several compilers actually implement them the same way anyway -- special preprocessing to detect the header guards and behave identically to however they implement #pragma once. Don't get me wrong, when I can start using modules at work I'll be the first person onboard that train, but I'm not going to suddenly wake up one day to find 2M LOC has magically become 100% C++17/20/whenever compliant either. In the meantime, I think #pragma once is the hands-down better alternative 99.9999% of the time some kind of guard is required. It works great, is _more clear_ than header guards, and isn't broken at all.
Is `inline` still a useful keyword? I was under the impression that modern compilers ignore it and instead compile to whatever it deems more efficient.
I like VS as an IDE but MSVC sucks (poor standard conformance and very bad error messages). Are there any plans to support MinGW in the future? Also a native Linux version with GCC/Clang support would be awesome.
Most open source cpp stuff that I've seen for Windows is made with VS. Cross compiling code written in VS with mingw on Linux for Windows is usually problematic for a couple of reasons. Usually VS seems to be more backwards compatible than mingw, so instead of just compiling I end up trying to fix stuff that has been deprecated or that VS has decided should be in cpp, but mingw has decided not. If you want a good example of this, try compiling fgdump from source.
this tool lets you use GCC to compile for Linux. no MSVC compiler in this scenario. 
I'm still confused on launching executables. Would they appear in the visual studio output window? A terminal pops up? If I'm using a graphical application would it launch an X11 screen if I'm running an XServer? I'd like to avoid needing to have a full remote desktop running.
I just wrote a simple C++ app and was able to build and run it on a Raspberry Pi running Arch Linux (from Visual Studio, obviously). This is incredible. 
So far, this looks like the best C++ web framework to really pop up. I dig it.
One argument I've seen goes like this: struct Type { using A = int; } void func(Type&amp; t) { Type::A x; // Fine //... } void func(Concept&amp; c) { Concept::A x; // Error: need to say typename Concept::A //... } There are also tricky things like differences in the way overload resolution works for template functions and non-template functions which mean that some people want function templates to look different from ordinary functions to make it clear you're dealing with a different set of rules (disclaimer: I am not one of those people). Throw in `auto` to signify unconstrained templates, overload resolution by concept refinement, and template specialisations and the already complicated job for a human of deciding which function gets called for a given set of declarations becomes even harder. I don't know about you, but I find this a bit tricky: template &lt;typename T&gt; concept bool Pointer = std::is_pointer_v&lt;T&gt;; void f(auto p); // Unconstrained template function : (1) void f(void* p); // Ordinary non-template function : (2) void f(auto* p); // Another unconstrained template function, different deduction rules : (3) void f(Pointer p); // Constrained template function : (4) template &lt;&gt; f&lt;int*&gt;(int* p); // Hmm, which template am I specialising here? : (5) int i = 3; f(&amp;i); Now, which function gets called at the last line? (It turns out it's (3), but I had to try it with gcc-6 to find out.) 
While you are here taking suggestions, I want to make sure your team knows why I (and I suspect many others) find development on Linux appealing. When I want to add a library to my project on Windows, I have to search for the right download, go through the installer, find where the installer put the headers and library, and then go through Visual Studio's project configuration options (for both Debug and Release if I remember correctly) and separately add the include and library paths. On Ubuntu, I do "sudo apt-get install [libraryName]-dev" and add "-l[libraryName]" to my compiler call. This difference is especially important when new C++ developers want to try using a library (lets says to play around with graphics). I had assumed that you guys were aware of the problem, and were thinking of ways to fix it, but then /u/STL [expressed ambivalence](https://www.reddit.com/r/cpp/comments/3h8o2r/biicode_c_dependency_manager_has_gone_out_of/cu59l9w) towards third party solutions like biicode. I know this can seem like a trivial problem compared to everything else a developer has to do, but small conveniences really matter in creating a pleasant experience. The first potential fix would be to have a standard directory that library packagers can target for installation. This would ease a lot of the friction involved in hunting down directories. Nowadays, Windows also has a "package manager": the Windows Store. It would be great if you guys promoted the ability to install libraries through there. Of course, now it seems like we will be able to do apt-get in Windows, so maybe someone will create a third party solution around that.
My humble opinion is that while this is neat, this a proposal with extremely broad implications; on the same order of magnitude as concepts, and does not benefit the language anywhere near as much. I'd seen many of Sean Parent's on this topic (he calls it concept based polymorphism I believe). His talks are great, and I'm happy that I know the technique, and there are situations where I would use it. But the reality is that C++ already has polymorphism built into the language, and that built in feature is inheritance. It has some disadvantages compared to this approach. It has some advantages. In most real life use cases, the differences aren't that important. Most times when I define an interface in code I'm writing, it's far too specific for there to be any chance that some random other object will match that interface. So explicitly inheriting, or adapting another object, is fine. The situation you outline where object A and object B have the exact same interface coincidentally and we cannot modify them and want to use them polymorphically is pretty rare in practice. And when it does occur, it's not the end of the world, you just do what Sean Parent suggests, and there's a bit of repetition, and it's perfectly fine. If we were starting the language from scratch, maybe this would be a better way to go than inheritance, maybe not. But that's not the situation. Anyhow, sorry to sound like a wet blanket, but this is probably my reaction to the vast majority of suggestions to change the core language (as opposed to the standard library). Your ideas are cool and you've put a lot of time into them, but it's hard to justify the increase in complexity in the language for small benefit.
Totally acknowledged that package management on windows is a weak spot. We have some investments in this area right now but too early to share. We'll seek feedback here of course when we are ready! thanks, Steve
Yes, but in this case, the functions defined in a class body are already `inline`, so the keyword is spurious.
I have approved your comment, which was caught in the spam filter. In the future, please don't use URL shorteners on reddit. The spam filter hates them, because it can't see through them. See the "formatting help" for how to create pretty links, like to the [Get Started](https://isocpp.org/get-started) page at isocpp.
I have approved your comment, which was caught in the spam filter. In the future, please don't use URL shorteners on reddit. The spam filter hates them, because it can't see through them. See the "formatting help" for how to create pretty links, like to the [Get Started](https://isocpp.org/get-started) page at isocpp.
Not OP, but I've been transitioning to CMake for all my projects, in great part because I've also been trying out Windows as a primary dev environment after more than 15 years using Linux as my primary. Because I'm sure as hell not giving up Linux as a target and occasional dev environment, and because I'm not sure I'll stay with Windows as my primary, I'm depending upon CMake to make it easier form me to stay independent. In short, I pretty much won't be using anything I can't get working via CMake generated projects, unless I simply can't get the job done otherwise. So, as cool as this looks, I (and i suspect many others) won't be doing more than toy around with this until and unless I can use CMake to gen solutions/projects for this tool. I'll either use CLion (just started evaluating that option), or simply do what I've been doing forever -- gen a makefile and use the Linux tool set I already know. Don't take this as at all negative, BTW - I'm *loving* the enthusiasm, effort, and just plain good work you guys are bringing to the table these days!
Develope on Windows and cross compile on x86 Linux to deploy on arm Linux. Muahahahaha
As an extension to 3, any chance Image Watch would work with this?
Haha, noted.
That's great to hear!
just curious, what do you mean for it to "work with cmake"? cmake already generates VS project files, doesn't it?
You can already do this: $ ~/cl-14 /EHs sizeof_funds.cxx Microsoft (R) C/C++ Optimizing Compiler Version 19.00.23026 for x86 Copyright (C) Microsoft Corporation. All rights reserved. sizeof_funds.cxx Microsoft (R) Incremental Linker Version 14.00.23026.0 Copyright (C) Microsoft Corporation. All rights reserved. /out:sizeof_funds.exe sizeof_funds.obj $ wine sizeof_funds.exe bool sizeof = 1 wchar_t sizeof = 2 short int sizeof = 2 int sizeof = 4 long int sizeof = 4 long long int sizeof = 8 float sizeof = 4 double sizeof = 8 long double sizeof = 8 void* sizeof = 4 std::string sizeof = 24 std::vector sizeof = 12 std::unique_ptr sizeof = 4 std::shared_ptr sizeof = 8 Also have `cl-11` and `cl-12` in addition to `cl-14` all working on the same box. Will write details how to set this up once we support MSVC in `build2`.
On the contrary, header guards are far from fragile or error prone. `#pragma once` is fragile because it does not have even slightly set semantics. It means nothing.
&gt;You keep saying that, but I don't think it's true. Header guards are easier for somebody to screw up, and while there are cases where you could absolutely cause #pragma once to screw up, I've never seen one. I think several compilers actually implement them the same way anyway -- special preprocessing to detect the header guards and behave identically to however they implement #pragma once. The whole point of `#pragma once` is that you don't need header guards. So no, they don't "detect the header guards". &gt;In the meantime, I think #pragma once is the hands-down better alternative 99.9999% of the time some kind of guard is required. It works great, is more clear than header guards, and isn't broken at all. `#pragma once` doesn't work 'great'. It works terribly, inconsistently and is just fundamentally broken. They're semantically meaningless.
&gt;With whitespace specifically (and even line length), our online code review tool doesn't visualize whitespace characters. You can do it (pop-open a diff in beyond compare, which can visualize whitespace), but it's an extra step almost nobody but me takes. Then make your code review tool better. If you can't, then stop using shitty closed source software. &gt;Then if somebody did something like copied an include guard from another file that they didn't change, the two files wouldn't both be in the changeset so you couldn't tell from the code review. If they were, I'd be surprised if somebody noticed, that kind of boilerplate is super easy to gloss over. Header guards cause maybe 0.1% of copy and paste errors. They're insignificant. &gt;At least Git is configured to enforce certain line endings, but like I said we don't have any kind of more complicated hooks to refuse a push if it violates more subtle rules. Then write them.
That and Nicolai's book, covers most of the C++11 additions like std::function&lt;&gt;, lambda's,variadic templates quite well. Other stuff like C++14 variable template's is covered well in tutorials online.
Sadly the standards committee had a different opinion.
Not negative at all! This is very good feedback, thank you. Please do try it out even if you can't adopt it. We'd be very interested to hear additional feedback on how this does or doesn't fit with your current workflow.
Wow. We should talk. :) I think we only copy changes but need to check. With that size source base I imagine you will hit some things we have not thought about. Please give it a try and get in touch. Would love to talk directly.
We don't have console output in VS... yet. In plan. I would be interested in talking to you about the x server in Windows, you are the second person I've seen ask about this. So far we have only been launching guy stuff on the remote Linux box.
What's not standardizable about it?
Strange, it works just fine in every project I've ever seen that uses it.
You define as if the programmer had written: #ifndef _*see below* #define _*see below* // header content #endif _*see below* /1: The macro identifier shall be constructed as if from `std::filesystem::canonical`
Except that doing it that way is fucking retarded because I want the header guard to be based on the file's content, not its file name.
So do lots of things that shouldn't be standardised, like NULL pointer behaviour. 
&gt;So no, they don't "detect the header guards". 1. [It is important to note that some compilers such as GCC, Clang, and EDG-based compilers include specific optimizations to recognize and optimize the handling of include guards, and thus little or no speedup benefit is obtained from the use of #pragma once](https://en.wikipedia.org/wiki/Pragma_once) 2. [There is no advantage to use of both the #include guard idiom and #pragma once in the same file. The compiler recognizes the #include guard idiom and implements the multiple include optimization the same way as the #pragma once directive if no non-comment code or preprocessor directive comes before or after the standard form of the idiom](https://msdn.microsoft.com/en-us/library/4141z1cx.aspx) &gt;\#pragma once doesn't work 'great'. It works terribly, inconsistently and is just fundamentally broken. They're semantically meaningless. You can repeat that as many times as you want, but ~~I don't think you can~~ you can't back it up. At this point it's just a backwards superstition.
I had issues with cross compiling using mingw too - stuff would break with the cross mingw built version, but would work with all native versions - including native built mingw! At the end i replaced the compiler with openwatcom 2.0 (the github fork that is under active development, not the dead 1.9 "official" version) which provides linux hosted binaries. However i'd like to have an alternative that works. In my case the reason i want a linux hosted compiler isn't because i'm actually using Linux for development, but because i have a VM dedicated to building my codebase for all supported platforms and Linux is the platform that seems to be able to target all of them (Windows, Linux and OS X).
&gt; Everytime I update my CMake configuration files it takes minutes for CLion to load everything again up to the point I am able to actually type into the editor again. And if you run bare CMake on your changes, is there a time difference? Can we also ask you for a CPU snapshot (https://intellij-support.jetbrains.com/hc/en-us/articles/207241235-Reporting-performance-problems)? Submit it to our support for the investigation.
&gt; The only reason I didn't rip #pragma once and #import out of GCC when I had the authority to do so, ~12 years ago, was Apple's system headers relying on them. In retrospect, that shouldn't have stopped me.) Interesting
&gt; Still, until modules actually land in C++ 17 and commercial implementations are available to developers, I can use a little preprocessor trickery to make the animals library build both as a header-only library and as a module. Please (please please please) stop showing feature example code that uses macros. People _will_ copy it, they _will_ think it's ok because Microsoft does it, (and because it's a practical trick), and this stuff ends up being the cruft of a new generation of code. The C++ community no longer starts C++ tutorials with pointers. Can we please stop writing them with macros? Otherwise, interesting implementation of modules! :) Note: "we stop writing them" doesn't refer to me - I haven't writen tutorials in years :]
I have to say, I'm surprised by the level of resistance to `#pragma once` in this thread. C++11 blessed several things that all compilers were doing anyway (like `&gt;&gt;` closing nested template declarations), and `#pragma once` seems to be on that list. Out of curiosity, how does `#import &lt;foo.h&gt;` work in Objective-C? It seems to have roughly the same requirements as `#pragma once`, in that the compiler needs to give each header some unique identifier to prevent duplicate inclusions. Yet it's used universally in OS X and iOS development (or at least it was before Swift came along), so presumably Apple managed to solve problems with multiple paths to the same header? 
I'd like that too ! Being able to target win32 with msvc from windows would be even better than the standalone build tools from an IT perspective. On the basis than administrating a linux build farm / containers farm etc is easier. (I have a jenkins instance able to spawn docker containers for a build and then dispose of it, its awesome and so much lighter than a VM based approach). One other use case I have is that I have a tool that run on linux and build installers for a variety of platforms, including windows. So I have a small exe to build. I currently use mingw-w64 for that but, I'd love to be able to use cl.exe. Other cross platforms tools that need to generate windows executable would probably benefit of this feature ( nsis, unity, unreal, etc ) 
Please please please. Stop saying macros, varargs and raw pointers are evil in C++. I use C++ because of the power it gives to me. If I wanted the language to hold my hand, I would have used java or something like javascript. And at least, if your argument is that macros, varargs, and raw pointers are there only because of backwards compatibility, then just start over and scrap all that backwards compatibility. Or make the backwards compatible mode non-default, and hide it behind a compiler switch or whaterver. Don't you think it's a bit paradoxal that the [C++ faq](http://www.cplusplus.com/info/description/) mentions that C++ "is a language that expects the programmer to know what he or she is doing", "offers many paradigm choices", "is upwards compatible with C", "has incredible library support", but then you urge people to not use these features? * C++ is a language that expects the programmer to know what he or she is doing: Yes, I do know what I'm doing, so let me please manage my own memory the way I like it. Beginners need to know what they do too. If you teach them a "safer" C++, they'll just be copy-pasting code-monkeys writing only glue code not knowing what happens under the hood. And we don't want that, right? Or do you want that??? Oh my god! You want C++ programmers to be stackoverflow-pasting code-monkeys! You want C++ to be the new node.js! You want a C++ beginner to import a left-pad module! Shame on you. * offers many paradigm choices: Yes, and that means I can use freeform functions with variable-arguments and side-effects if I need to, and if it makes my code mode straightforward. And no, I won't hide ouput formatting behind several layers of templated code. No thank you. * is upwards compatible with C: Yes baby, you want your language to be compatible with C so just stinking assume it. Adults assume their actions. You are an adult, right? You wanted C++ to be compatible with C, right? Then why don't you assume macros and varargs are what you just said you wanted, and use them freely and right away. Embrace them. They are good. * has incredible library support: Yeah, and one of these libraries happens to be installed on just about every operating system you could dream of: the C library. Why wouldn't you use it? Because it uses pointers and macros freely? Come on. Because the STL is "better"? Come on. Give me a stable C++ abi that works across compiler versions/vendors and we'll talk. 
There are no reasons to use macros when we have modules. 
No, the special optimisation is that if you have header guards, then the compiler will recognise them when reparsing a file and not bother reparsing the whole thing if the content is identical. If it's not identical, it will check the header guards and still exclude it. With pragma once, if it's not identical it will reinclude it again.
I'm actually very excited to give this a try. It could mean I won't have to switch workflows as much between my Windows &amp; Linux environments, as someone who is used to Visual Studio &amp; Windows. If I can build simultaneously for both platforms from VS, then I consider that a win. And if I can even debug from within the VS IDE, then that's just glorious icing on the cake for me. I'd like to add that this is a step forward for Microsoft, and with their other recent Linux based announcements, am optimistic that they'll be supporting Linux even further in the future. There's much to be gained from this attitude for the open source community.
Isn't the point of CMake to generate whatever build system files you want from a common description? I would expect that you could use your CMake files to generate MSBuild files to drive the compilation, so in a way the CMake support already is there. ----- It would be nice to have Make though
Qt Creator just merged a commit which changed all header files to [pragma once](https://github.com/qtproject/qt-creator/commit/39a38d5679084b515276285c044d8a27e671adb1) They used a tool for this: [guardonce](https://github.com/cgmb/guardonce)
&gt; Stop saying macros, varargs and raw pointers are evil in C++. If I start saying it, I will then be able to stop doing so. I rarely say that macros are evil. [Here's an answer of mine](http://programmers.stackexchange.com/questions/288226/idiomaticy-of-macros-in-c/288278#288278) on stackexchange, on my position on macros. &gt; And at least, if your argument is that macros, varargs, and raw pointers are there only because of backwards compatibility, then just start over and scrap all that backwards compatibility. Macros are a necessary language feature (unfortunately) that should be used only when there is no better alternative. Sample code (or feature example code) like this is, should not mix new syntax (module code) with macros, unless the interaction between macros and modules is what the article is about. For example, the author could have dismissed the possibility for using an export macro, by saying "if you need to maintain code that is compiled with different compilers (including versions that do not support modules), consider separating module-defining code in separate files. A solution using macros can be also implemented, but that leads to cruft down the line." The way the feature was eplained instead was "I can use a little preprocessor trickery to make [...]". &gt; Don't you think it's a bit paradoxal that the C++ faq mentions that C++ "is a language that expects the programmer to know what he or she is doing", "offers many paradigm choices", "is upwards compatible with C", "has incredible library support", but then you urge people to not use these features? What does the C++ faq (and what is says about C++) have to do with what I said? Your bullet points on C++ are also interesting, but have nothing to do with me, or what I said: &gt; And we don't want that, right? No. We want to be patronizing on reddit instead. &gt; Or do you want that??? Oh my god! You want C++ programmers to be stackoverflow-pasting code-monkeys! You want C++ to be the new node.js! You want a C++ beginner to import a left-pad module! WTF?! &gt; Adults assume their actions. You are an adult, right? You wanted C++ to be compatible with C, right? No, I wanted to receive a patronizing response. &gt; Why wouldn't you use it? Because it uses pointers and macros freely? Come on. Because the STL is "better"? Come on. Give me a stable C++ abi that works across compiler versions/vendors and we'll talk. &gt; Because the STL is "better"? Come on. STL _is_ better (C is nice, but give me exceptions, templates, classes and inheritance every day). 
its 2016, who still uses header guards
&gt;Completely different language with different semantics. Objective C as a whole is a different language, sure, but the preprocessor is almost identical to that in C++ (both are based on the C99 spec). As an extension to C, the Objective C preprocessor supports an `#import &lt;foo.h&gt;` directive which does the same as `#include &lt;foo.h&gt;` except that the compiler guarantees that each file is only included once. To do this, it seems to me that the compiler must have some method of determining header uniqueness, which is exactly what is required for `#pragma once`. That's why I asked if anyone with background knowledge could explain how this is implemented, and how it avoids the problems that people are complaining about with `#pragma once`. (Note that I'm talking about the `#import &lt;foo.h&gt;` preprocessor directive which has been part of ObjC for many years, not the more recent `@import` command which uses Clang modules.)
They already support clang. https://blogs.msdn.microsoft.com/vcblog/2016/01/20/clang-with-microsoft-codegen-january-2016-released/ --- For me it has worked great so far.
Sane people. 
Soothe your mammaries. He uses #pragma once which is basically same thing. Basically...
1. Conditional compilation, which is basically what allows your code to work on different compilers and standard libraries with different bugs, intrinsics, levels of C++ compliance as well as on different targets if you need to do something target specific like using different system headers... 2. plain dumb code repetition like when: - implementing testing macros in unit testing libraries (like Catch, Google Test, Boost.Test, range-v3-simple_test) - doing compile time reflection (like Boost.Fusion and Boost.Hana `_ADAPT_STRUCT_XXX`)... - return type deduction `RETURNS` macro, `RANGES_DECLTYPE_AUTO_RETURN`, ... - for loop macros like `BOOST_FOREACH`, `RANGES_FOR`, ... - concept emulation layers like `range-v3-concept_check`, `Boost.Concept_Check`, ... 3. Interfacing with anything via C, where anything uses macros for whatever, for example, interfacing with your operating system via the POSIX layer, which exports macros. 4. Hints for static analysis and code generators, since macros can expand to nothing, which is what, e.g., Qt's MOC uses with Q_OBJECT, signals, ... 5. Assigning a particular file a unique identifier (like with include guards). 6. ... there are just too many problems for which the best solution is actually to use a macro, and there are lots of problems for which macros are actually the _only_ solution. Modules don't solve any of these. 
Are you also considering better support for cross platform build systems designed for [IDE integration](https://github.com/mesonbuild/meson/wiki/IDE%20integration)? Currently everyone needs to generate their own VS project XML files which is not optimal. With this we could also finally make a [C/C++ dependency manager that is properly cross platform](https://github.com/mesonbuild/meson/wiki/Using%20the%20WrapDB).
Please tell me what the cosequences of pragma once are. We only support clang, gcc and msvc for compiling. All of them support it, so that wouldn't be a problem. EDIT: compiler names are difficult.
If they can port the full version of Visual Studio over then that's it, I'm done with Windows.
Missing this conference is not an option! I've filed for a ticket instantaneously...
There's a [heated discussion](https://www.reddit.com/r/cpp/comments/4cjjwe/come_on_guys_put_pragma_once_in_the_standard/) going on right now...
Thanks :)
You can see it as self promotion, sure. But at the same time you cannot just write code and have it completely ignored and run over by other, overly hyped projects that simply doesn't perform on their hype.
So, I can list these: - Obscure include paths that will confuse the compiler - Build system that copies stuff around I have neither of those. If I would have them, I'd rather fix my build system then using "real" include guards.
Mingw can be built to run under linux (since it's just GCC) to output windows binaries from under linux (the same way that it can output OS X binaries or basically any kind of cross-compiling). You can then build an entire windows toolchain on Linux (with Qt, etc...) and just put the built .exes on a win machine if testing with Wine does not work. See for yourself : https://aur.archlinux.org/packages/?K=mingw-w64 
I have all three of these cases in my projects, and in all cases, the header guards combined with the include order determine which one actually gets used. I don't see how using #pragma once would cause any ambiguity that doesn't already have a solution. The linked post said a "sufficiently complex" build would have problems, but didn't get to a specific example. I don't see evidence that there is a problem.
In the days of NeXT, Objective C used gcc and a UFS-based filesystem. Then for OpenStep, it was available for Solaris, HP-UX, etc and Windows (gcc and NTFS). Today with OS X, Objective C is clang and the HFS+ file system. gcc 5 still supports `#import`. So, it's a little more widespread than 1 compiler and 1 file system. 
Thanks michy232, your situation is exactly one of the ones we're trying to make easier with this extension. It's not clear from your post whether you're working on embedded/iot type apps or more broadly but either way there are are a lot of developers who are in the same situation of having to switch backward and forward between environments and we hope we can make that a little less frustrating. Let us know how you get on and what you'd like to see in future releases, feel free to PM me or hit me up at afulton@microsoft.com. 
First of all, everyone who uses a program language wants some degree of hand-holding or else they would be writing in machine code. Secondly, this whole thing that C++ makes it harder to manage your memory the way you like is a myth. It confuses intent with mechanism. The most important thing about memory management is the concept of ownership. You either own the memory exclusively, you don't own the memory, or else you share ownership of the memory. You need to understand this for each pointer you have, whether you are programming in C or C++ or else you end up with leaks or undefined behavior. In C, you do not have a vocabulary for expressing these differences in the language. A C pointer can mean any of those things. The only way someone reading your code can figure out which it is to trace through all the code and code paths that deal with that pointer. In C++, we have a vocabulary for expressing this. A raw pointer means I do not own the memory, a unique_ptr means I exclusively own the memory, and a shared_ptr means I share ownership of the memory. Thus when we say do not use raw pointers in C++, we mean that you should use C++ to express the intent. Not teaching beginners this actually makes them **more** likely to be copy-paste monkeys in that they won't even try to think about ownership. Now as to teaching what happens with memory management under the hood, whether you use raw pointers is completely orthogonal to it. You do not learn what happens under the hood by calling malloc and free. Even if you use C++ idioms like unique_ptr, shared_ptr, and even containers, you can still control the low-level memory management by using allocators for containers or deleters for unique_ptr and shared_ptr. With these you can use best-fit, worst-fit,arena, pool, and any other type of memory allocation strategy you can write or find a library that implements. Thus a C++ programmer using idiomatic C++ is more likely to have a better understanding of memory management than a "C with Classes" programmer who blindly copies malloc/free and adds or removes calls to free until the program stops crashing or leaking memory. 
Soo... many... regressions. Internal compiler errors to the max. Ah well, should have known better than to upgrade so shortly after a release.
"It is a unified interface to package management systems" So, not a package manager, then. Sigh.
Completely wipe your build artifacts and rebuild. Something in on-disk formats of certain intermediate/cache files seems to have changed in a way that msbuild/vs doesnt' detect and auto-rebuild.
Yeah it's not just internal compiler errors though, it's other quirks too like I'm getting a "Compiler has run out of heap space" issue I've never seen before, and it's also claiming that certain methods I'm implementing have not been declared despite being declared. Back to Update 1 I go.
Please submit bugs with preprocessed repros. Otherwise, it is less likely that we will find and fix the exact issues you've encountered.
&gt; We haven't even implemented hard enforcement of any of these things. We have agreements (coding standards) around all of those things, but people still violate those agreements (some more frequently than others), most often without realizing it. Same here. I made a script, originally to find other, more important problems (duplicate file names across multiple projects), later extended to check things like well-formed-ness of the file header (must have \file, \author and \brief, must have valid guard, must not include certain banned headers, etc.). Some people use it from time to time. I hope nobody ever turns that into a commit hook :-)
Templates go in headers *only*. (Exception: When you want to define and use a template in a single .cpp file only, then it can go there.) Same rule for functions marked inline.
i am ineresting too. i hope list will be updated any sugestion what book is good for starting up??
Well, okay. I'll buy those arguing people a beer if I ever encounter such a condition. And I'm from Belgium, so it'll be good beer :-)
Thanks for saving that text for me, /u/dodheim. I've edited the blog post to add it back in now that we've shipped the full Update 2 :)
One convention that can be used that isn't intrinsically bad is to put templates in 'ipp' files. Personally I don't see a lot of use in it, but I've seen it used especially in some boost libraries.
I think there are compilers (I think Visual Studio) that do a optimization step and just paste the compiled code instead of linking against it when they see fit. In that case inline is only useful to avoid linking redefinition errors. 
In progress... we're fighting an infrastructure issue
Do we have to wait another ~4 months for next Update for these issues to be fixed? Or there some other ways to deliver updates? Btw, compiler ICEs on variable templates/ acces to variable templates if they are not constexpr but webcompiler is fine.
They'll rather port full version of linux environment to windows ;P
Looks like they are working on it with canonical =)
None of these methods. If you're going to try to split the template into interface and implementation files you absolutely should not use any of the extensions that you use with C++ implementation files for non-templates. The article is showing terrible practice by using a file with the extension `.cpp` for the template implementations. As for what extension you should use, there's not really any standard because splitting templates this way is not standard practice. Standard practice is to not split templates into interface and implementation files, and that's the method I'd recommend.
That's strange since example from here: https://connect.microsoft.com/VisualStudio/feedback/details/2535591/ice-with-variable-templates compiles and works fine in webcompiler but causes ICE in update 2. Something in between RC (or which version is used by webc?) and RTM have caused it?
Well, I guess Linus won.
Worse than not having modules is having half baked modules that nobody is going to use. 
Thanks! Looks like a great high quality library.
No, I'm not saying that.
Precompiled headers help for the build time, wrapping the lib into app-specific \*.so helps with size (if you have more modules yourself).
Word.
Excellent points, well articulated, and right on target. Just to give a bit of data point here, from the VC++ module implementation and usage in real products experience. You know VC++ has a switch to allow consumption of existing header files via module interface. Well, to make that play well with duplicate header file inclusion, the analysis we did led us back to exactly "the good thing about header guard", over pragma once: the programmer has indicated a unique way to identify a piece of program text in a source file independenly of the build platform's file system quirks and other versioning idiosyncracies. It was interesting to see that the portable standard conforming way (according to current standards) led to the easiest and simplest compiler code to write :-). Dealing with pragma once required more finess -- but pragma once is also pervasive in header files on Windows platforms.
clang-format should be in the list of tools. 
I work at Microsoft, on the Microsoft C++ compiler code base. I have a fairly nuanced view of 'pragma once' and I hope that isn't considered FUD :-) The pragma is less verbose compared to the include guard pattern, that is a plus. BTW, the include guard pattern is recognized by all major compilers and appropriately optimized. The pragma once injects the underlying filesystem vagaries directly into the semantics of the input source, and that is huge minus. As /u/streu points out, you need a unique name, easily identifiable and predictable by the user, to attach to the contents. That is what the include guard does (in an admittedly verbose fashion); that is what a module declaration does.
Good summary! Also see Bjarne's and Herb's talks at CppCon2015.
about libraries google-test is definitely worth mentioning. regarding talks there is great introduction to variadic templates from cppcon 2012 (I know its quite old but it really makes clear how variadic templates works): https://www.youtube.com/watch?v=_zgq6_zFNGY
gtest is nice, but is it really a good example of a modern C++ library? Catch looks like a better testing framework if you are starting a new project today. The variadic template talk is nice, I've added it to the list.
&gt; That proposal does require you to change the existing libraries though How so? I thought it allowed creating a new file and writing: module Module.old_library_module; module { #include "old_library.h"; } export ....; int CONSTANT = OLD_LIBRARY_CONSTANT;
I notice you have both cppformat and tinyformat listed. Is there a reason why you're listing both of them? do you know the pro/con of each, or did you list them both because they both seem to be popular? On the surface it seems that tinyformat is better for small to medium sized projects where compilation/execution speed aren't much of an issue, but cppformat does better for larger projects in that it's more performant and doesn't have as much of an effect on compilation time, but the trade off is needing to do more than simply drop a header file in the right spot. It might be good to include some information comparing their pro/con's. Also, they're not listed near each other and I think they should be since they do roughly the same thing.
I recommend that you read the section about the "One Definition Rule" in your C++ book of choice. If it does not cover the One Definition Rule, get a better book.
Sean Parent's talk on "Inheritance is the base class of evil" https://www.youtube.com/watch?v=bIhUE5uUFOA is a classic. In terms of libraries * Boost Hana (http://boostorg.github.io/hana/) - Redefined how to do metaprogramming in modern C++. With Hana, metaprogramming is no longer a guru only technique. * Crow Micro Web Framework (https://github.com/ipkn/crow) - One of my main examples when I argue that it is possible to make libraries in C++ that are just as easy to use as in python * cpplinq (http://cpplinq.codeplex.com/) - LINQ syntax in C++. No macros, and extensible. * my simple_match (https://github.com/jbandela/simple_match) using modern C++ to create a pattern matching library without using macros. 
.inl is common also.
Similar lists: https://stackoverflow.com/questions/777764/what-modern-c-libraries-should-be-in-my-toolbox http://en.cppreference.com/w/cpp/links/libs https://github.com/fffaraz/awesome-cpp
Don't forget http://ericniebler.com/ for the blogs section
I'm making this list about modern C++. If you are building a 1M SLOC application you will find your way to gtest without my list :). Makes sense?
Even more simply than that, the proposal includes features that support optional 'transparent migration' &gt; ### Transparent migration &gt; With the above features, an implementation can choose to provide a transparent migration path to modules for code that already intends to provide a modular, self­-contained interface from its header files. This requires an implementation to be informed of the set of headers that it should treat as modular. It can then treat &gt; #include HDR &gt; (where HDR names such a modular header) as an import of an implicitly-generated module &gt; module unique-­name; &gt; import legacy HDR; &gt; That transparent migration path is not proposed by this proposal, but we explicitly intend for it to be a natural (and conforming, as mapping from #includes to source files is implementation­-defined) extension. Essentially, this 'legitimizes' clang's module maps approach, and makes possible an incremental transition to modules for both library vendors and library clients. A vendor can add module maps for their library as they find areas that don't exhibit the ['anti-patterns'][1] that break modules. In headers that do exhibit those anti-patterns, they can fix them over multiple releases and provide additional module maps. A client can flip a switch and immediately start benefiting from modules for whatever headers have maps without having to go around editing their code; They can even switch back and forth, and be portable across platforms where some support modules and others don't. As the library's modularization progresses, the client automatically picks up the benefits. [1]: http://clang.llvm.org/docs/Modules.html#modularizing-a-platform
Definitely let me know how that goes! =)
That's an IDE update, so change your IDE settings if you don't like the color... o_O
When the RC works and RTM doesn't, quipping support of "the process" isn't really a response. ;-]
Put everything to do with the template in the header. If you have a template class, the entire class should be defined inside the header along with all of its member functions. That's the way it's usually done.
this is a great collection of resources. good job!
Visual Studio still does not support C++98, nearly 20 years after it was released. They could have rewritten their compiler or just used Clang. There's no excuse for not having C++98 support. There's no excuse really for not having C++14 support to be honest. &gt;Also, we are committed to not add proprietary extensions to the language. If you're at a C++ Standards Committee meeting and you feel we're trying to get "how Visual Studio does it" standardized then please find me (in the EWG room, usually) to chat about it. I definitely feel like Microsoft's coroutine's proposal is bordering on 'do it like VS'. Certainly it's not as bad as the standardisation of docx.
Ah, so MinGW is GCC-targeting-Windows, not GCC-on-Windows? Makes sense actually. 
Thank you!
&gt;Conditional compilation, which is basically what allows your code to work on different compilers and standard libraries with different bugs, intrinsics, levels of C++ compliance as well as on different targets if you need to do something target specific like using different system headers... Conditional compilation is pretty much obsolete. It certainly shouldn't be in header files, which is what really matters. &gt;plain dumb code repetition like when: That's most experimental stuff like `BOOST_FOREACH`. The testing macros are utterly insane and should clearly not be used in production code. &gt;Interfacing with anything via C, where anything uses macros for whatever, for example, interfacing with your operating system via the POSIX layer, which exports macros. Those could take advantage of modern C++ to not export macros and use other functionality instead. &gt;Hints for static analysis and code generators, since macros can expand to nothing, which is what, e.g., Qt's MOC uses with Q_OBJECT, signals, ... Qt is aids. &gt;Assigning a particular file a unique identifier (like with include guards). As I said, with modules not necessary.
`#pragma once` is nonstandard and completely broken.
gdb, valgrind [conan](http://docs.conan.io/en/latest/getting_started.html) package manager for C++ Even it's not so popular, but there must be at least one package manager in Tools.
&gt; What does it mean for a static function to be virtual? It would mean storing a pointer to the static function in the polymorphic object or its vtable. In C++, this is impossible without implementing a wrapper method. &gt; Similarly, I don't know what a virtual data member is. An offset stored in the vtable, pretty much the same as virtual methods. &gt; The fact that you implemented a library for it does not mean you can "safely" say it should be in. A miscommunication. I didn't mean that this should become a feature. I meant that if people are going to be hacking up polymorphic wrapper libraries such as Boost.TypeErasure, and if the standard library itself is going to use this technique with things like `std::function`, then it might as well be a core language feature and save everyone the trouble. &gt; C++ has never shown a willingness to do dynamic stack allocations (like C VLAs) That's not exactly the same thing. VLAs are managed by the programmer. Consider an optimization that is already quite common: manually placing derived class objects on the stack, and passing them to functions taking a base class reference. With virtual concepts, the compiler could do this automatically and entirely avoid any heap allocation. &gt; Broad core language changes are supposed to be things that are very, very widely useful Considering that Boost and the standard library itself could make use of this, I'd day it's pretty useful. This is something that is nearly impossible to implement as a library, so it's no wonder that people don't use it or know about it. &gt; They solve the same problem. Why is it not viable? They don't solve the same problem. You can't force a class from a third-party library to inherit from some arbitrary base class (even if you could access the library source, such a change would require additional testing, and updating the library would be hell). It's also not reasonable to force library users to inherit from some base class to use your library, adding additional complexity to their code (and if you decide to modify the library classes, user code would likely break). Inheritance does not solve these problems, but virtual concepts do. Also, virtual concepts don't solve the problem of having default implementations for virtual methods, and wouldn't allow for patterns such as CRTP. Inheritance and virtual concepts are very similar, but certain things are just not possible with inheritance. Some people may argue that concepts could replace inheritance. I don't know if that's true, but the only time I've used inheritance in the past few years is when a third-party library forces me. 
Well, Qt is a very controversial topic. While it's definitely one of the better GUI frameworks for C++, I wouldn't consider it good modern C++. It's not even real C++ as you need a separate compilation step to generate proper C++. Also its habit to reimplement everything (QString, QVector, ...) is a pain if you want to integrate it with other libraries. As far as I know it also doesn't use any C++11/14 features which (as I already said) isn't a bad thing but why would you then consider it "super modern"? Sadly with C++ GUI frameworks you have to pick the lesser evil, so Qt isn't such a bad choice after all.
...but why did it get its own colour? Why `new` specifically, and not `delete` for that matter?
Do you mean include the implementations in the header file?
Here is a couple of more links for libraries, you can decide if you think its worth putting in the libraries section: * [range-v3](https://github.com/ericniebler/range-v3) - A modern range library for C++ * [Fit](https://github.com/pfultz2/Fit) - C++ function utility library * [Tick](https://github.com/pfultz2/Tick) - Create concepts in C++11. * [Mustache](https://github.com/kainjow/Mustache) - Mustache templates for C+11 * [yaml-cpp](https://github.com/jbeder/yaml-cpp) - A yaml parser in C++ For the tools: * [cget](https://github.com/pfultz2/cget) - Cmake package retrieval * [conan](https://www.conan.io/) - C/C++ Package manager * [hunter](https://github.com/ruslo/hunter) - Cmake-based package manager
Yes valgrind is great
Various features of this are too restrictive for this tool to be generally useful. 1) No configuration - flat out broken for far too many usage. What if I want to compile a library that supports both SSE, SSE2, and AVX instructions but need to target a deployment environment that only supports SSE2 at a minimum? I need it to be fast (say, number crunching uses, HPC, finance, etc.) so I definitely want to enable full SSE2 support but I can't use AVX even though the local build environment supports it. Configuration is mandatory for real-world usage. 2) Lack of binary support - impossible to use this in large-scale or proprietary scenarios. There are essential pieces of our toolchain that we don't even have source code to. Even for pieces that we do have source code for, we currently deal with precompiled binaries for chunks that change rarely, in order to ensure that build times are fast for the majority of our developers. We need to the ability to ship binaries in packages (including dealing with different platforms and compiler versions!) that we store in our local repos (which btw, we need local repos, because paid commercial software won't exist in the public repos). 3) All or nothing dependencies - so we can't have a build that relies on DX-only libraries on Windows and does not depend on them on Linux server environments, making this tool useless to us. 4) Dependencies dont' seem to have an option source specifiction for the repo, meaning that we need complicated setup steps to use local private repos that override or extend the public repos. 5) Seems to depend upon CMake. CMake is not the universal build system that some people think it is; it is literally incapable of expressing many very common project structures or needs and generates much slower builds than some competing tools. It doesn't even properly support PCHes for instance, and has years-old bugs that cripple PCHes (e.g. it is literally impossible in CMake to get PCHes working on Ninja properly, and you need Ninja to workaround the painful slowness of the Make or msbuild projects that CMake ineptly generates). Any C++ package tool should generate metadata about how to build dependencies (even if for many libraries this is just a set of cmake commands!) and then, if you want, just provide a CMake library to consume that metadata. Make this tool useful to folks using systems Meson, Tundra, Premake, Jam, or even home-grown build systems. Otherwise this is a niche tool only usable by a subset of C++ users and not worth supporting by library developers.
Regressions between RC and RTM are completely our fault, which is why we try to avoid changing anything that isn't critical. However, when a regression is present in CTPs/Previews/RC, and users don't report it until RTM, that's not good.
More great news from Microsoft. "join our Cross-Platform C++ Insiders group" link appears to be broken, anyone got the proper link?
But that does not modularize "old_library". It does not even expose the contents of "old_library.h" Module.old_library_module. Furthermore, if as a user I am supposed to now consume OLD_LIBRARY_CONSTANT (existing code) as CONSTANT (new modular code), then I would need to change my existing code on the consumer side (which can be another library).
So, is there an easy way to convert our Windows project to a Linux project? As you know we can not add the file system tree to the solution in VS. I tried to copy some parts from .vcxproj/.vcxproj.filter to the new Linux project but I was not successful. I could only build the filter structure in the solution but ended up adding all source files one bye one. Then, I noticed we have a new folder under &lt;VSInstallationFolder&gt;/VC/Linux/include which basically has the standard Linux header files. We are including `unistd.h` in our code but it was no in this folder. Is this expected? Can we just copy the missing Linux header files into this folder? Then I noticed some header files in this include folder complaining about `_PTRDIFF_T_` and another preprocessor which I can not remember at the moment. Do we have to add them to the preprocessor list? Unfortunately I spent most of my time just to create the new project and I had to continue with my other projects. I will update this tomorrow if I get the chance to run it on Linux. Would love to talk to you in person :)
How would this compare to Visual Studio Professional (Visual C++)?
Heh. Paging /u/gornishanov
Libcinder is a modern cross platform c++ creative coding library https://libcinder.org https://github.com/cinder/Cinder Think openframeworks but with a more recent design. It's also one of the first graphics libs to support Vulkan.
What I said is that just because something works fine for you, that doesn't mean it should be standardised.
&gt; It's quite small so best way is to just try it out probably :) Good pickup line.
Sure, but CMake doesn't have the right bits built in to build the correct MSBuild variables and logic (I assume -- I haven't tried it yet), so either Microsoft needs to provide a CMake extension or they need to document the necessary CMake listfiles code to generate a solution and/or project that will work with this tooling. My hope would be that I could set up CMakeLists files that can either generate native makefiles to build on one of my Linux boxen, or generate a VS solution that'll allow me to use this new tooling. Just like the other CMake projects I set up.
I've had OK luck with [NetBeans remote development](https://netbeans.org/kb/docs/cnd/remotedev-tutorial.html), as well. Certainly more luck than with Eclipse, anyway.
json CPP integration * https://github.com/open-source-parsers/jsoncpp Test * https://github.com/google/googletest * https://github.com/philsquared/Catch * https://github.com/cpputest/cpputest * https://github.com/unittest-cpp/unittest-cpp Analysis * https://github.com/danmar/cppcheck * https://github.com/thejohnfreeman/autocheck
I'm a fan of cppformat, thank you for writing that library.
Apologies if there are duplicates with existing links... this took a while to compile. Blogs: [b.atch.se](http://b.atch.se/) [Bartosz Milewski's Programming Cafe](http://bartoszmilewski.com/category/c/) [C++ Secrets](http://codexpert.ro/blog/) [C++ Truths](http://cpptruths.blogspot.com/) [Codexpert blog](http://codexpert.ro/blog/) [Coldflake Blog](http://blog.coldflake.com/tag/C++/) [Crazy Eddie's Crazy C++](https://crazycpp.wordpress.com/) [/dev/krzaq](http://dev.krzaq.cc/) [Eli Bendersky's Website](http://eli.thegreenplace.net/tag/c-c) [Enki::Technical Blog](http://enki-tech.blogspot.com/) [Functional C++](https://functionalcpp.wordpress.com/) [Manu Sánchez's Blog](http://manu343726.github.io/) [Multi-paradigm](http://yapb-soc.blogspot.com/) [The New C++](https://thenewcpp.wordpress.com/) [Obscure C++ Features](http://madebyevan.com/obscure-cpp-features/) [Probably Dance](http://probablydance.com/) [Simple C++ Metaprogramming](http://pdimov.com/cpp2/simple_cxx11_metaprogramming.html) [Source Code Tales](http://szelei.me/) Libraries: [Cap'n Proto](https://capnproto.org/news/) 
Well yeah, commercial compilers suck. Including Visual Studio.
Indeed. I guess for that one needs a way to export all the symbols "transparently", maybe something like: int OLD_LIBRARY_CONSTANT = old_library_module.OLD_LIBRARY_CONSTANT; See also Bames comment below, I did not recall that part of the proposal.
&gt;&gt;rather than achieving this indirectly though function objects as described in Stroustrup: The C++ Programming Language (3rd edition). Addison-Wesley 1997
&gt; Conditional compilation is pretty much obsolete. How do you target multiple platform then? `&lt;iostream&gt;`, `&lt;filesystem&gt;`, ... achieve this with macros. &gt; The testing macros are utterly insane and should clearly not be used in production code. All widely used testing libraries use macros, _a lot_. Name one that does not. &gt; Those could take advantage of modern C++ to not export macros and use other functionality instead. Are you suggesting to change POSIX to switch from C to C++? ... In modern C++ you cannot even write a portable "Hello World" program without macros, since `&lt;iostream&gt;` (and any other output library) uses macros in its implementation.
I get jokes.
Happy April Fools for you too.
One thing I forgot to mention is the Meeting C++ Blogroll, which gives you an overview on the weekly blog posts about C++: http://meetingcpp.com/index.php/blogroll.html
Pretty cool. Probably useful for people getting into Linux development who don't want to go through the whole vim/emacs route yet.
I never said nobody had any use for macros internally. I said multiple times that using them in libraries' .cpp files is fine. 
&gt; There is no reason to use macros in C++ other than for includes. And anyhow, currently, lots of C++ (basically anything that uses templates and is not explicitly instantiated) belongs in header files, so even though I agree with you that one should push macros into cpp when possible, things like `&lt;iostream&gt;` and `&lt;filesystem&gt;`, which use templates heavily, cannot do that, and have to use macros in header files.
I think that it's fairly clear that in order to wrap existing APIs you have to use macros. There's no reason to design new systems and libraries using macros. The space of problems that are best solved with "I know, I'll write a macro" is MUCH smaller than it was before C++11, and it's continually to get smaller. 
More useless features. Stop writing useless features and put some work into optimising it. Nobody will use it if it's slow as molasses.
Have you used Chaiscript in performance critical applications? I did some googling and found that it is quite slow, but I believe this is because it has a very limited runtime. If you use Chaiscript as it's meant to be used (interfacing with C++) rathern than how you use python/lua does it work well?
I'd guess vector&lt;vector&lt;int&gt;&gt; vv = {{99}}; int&amp; iref = vv[0][0]; // lset(iref) = {vv''} // lset(vv[0]) = {v'} vv[0].clear(); // KILL(v'') // lset(iref) = {invalid} cout &lt;&lt; iref &lt;&lt; endl; // error, invalidated by clear
Though I like the general idea of this proposal and I am glad that the C++ community is not giving in to the 'unicode' fade, I'd like to point out that the most useful 'Homer Simpson' Emoticon can not, to this day, be represented on an EBCDIC keyboard. In fact, It's really hard to represent any kind of hairstyle. C++ is becoming a hair-agnostic language where the conformance of the many should outweigh the magnificent of the few. After significant consultations with my stylist, it is my position that for the harmony of the greater C++, I will not oppose C++17 (It's a minor version anyway). I recognize that C++ is an hair-agnostic language now. I will continue to voice my concerns over the difficulty of representing Homer's head, because, having a Russian keyboard, he can't speak for himself. I am not taking a moral high round here, I'm just being fashionable. I realize the tide is against me, and it is best to plan a head. After some consideration, I also do not recommend adaption of any half-way measure such as the following: * Putting a hat * Shaving I feel such half-way measures actually complicate the emoticon situation and complicates C++. I makes it problematic to differentiate hats and shave-heads as well as representing different sort of hats that are not easily encoded in this pesky standard that was forced upon us. 
&gt; LLVM will therefore supplement the Standard’s happens-before relationship with an LLVM-specific happens-to-work relationship. Stealing _happens-to-work relationship_ for use in documentation for legacy code.
I tried _something_, I forget what. It didn't work for some reason so I tried a pretty dumb perl script that did work. It matched #if/#endif statements, and if the first one and the line after looked like an include guard it removed it.
&gt; Be warned: their runtimes aren’t designed to be secure and you shouldn’t ship them in production code! Is this actually true? How so?
I've run some clang tools (not too many) and they seemed OK with it, or at least it wasn't one of the things I noticed. I haven't run anything (successfully, IWYU I'm looking at you) that doesn't come with the windows clang install though.
Well, I guess there are more than one reasons why C and by extension, C++, are entirely terrible languages, syntactically, and myriad ways to improve them. However, I don't think that syntax is the biggest problem and I highly doubt that the syntax scripty website programmers prefer is the benchmark through which to measure it. C and C++ are not popular because they are concise or pretty, or intuitively understandable (debatably), they're popular because they are (actually) portable and don't enforce abstractions that are needlessly expensive. I'm afraid that your syntax argument will fall upon quite deaf ears here, especially in a C++ group where we have a language that, although may appear to be token soup to many, especially since C++11 with lambdas, is also capable of both low-level control and the extraction of much performance of the heterogeneous machine, in addition has myriad ways to abstract that problem with minimal runtime penalty, whilst maintaining a large degree of compile time compatibility with code that's decades old. It's likely you'll be charged with the responsibility to raise your argument a little higher than "uneducated people don't like your scoping character", especially when they may not even realise the true true power of the closing of the scope. I rest my case.
C++ is supposed to enable re-use so might as re-use this old April Fools joke one more time, eh? 
Nice to see some new material, I'm pretty tired of Bjarne's old overloading joke by now.
/u/milesrout, thank you for the feedback. 
Yes. http://seclists.org/oss-sec/2016/q1/363
I see this has a new installer, does this mean that the uninstaller is finally fixed? Or do I still need to run it on a disposable windows partition/VM? Edit: paging /u/spongo2 
Are you looking for an answer that's different from the "books" link on the sidebar?
I have removed this post as beginner questions are off-topic for this subreddit. Please read the sidebar.
again....consider when this was published.
Why doesn't ASLR do anything good? I thought it could stop some things, like simple ROPing.
I did notice. I actually commented only on joke submissions here today. 
ha, awesome....reverse joked! 
I was referring to [this article](https://visualstudio.uservoice.com/forums/121579-visual-studio-2015/suggestions/3487794-create-a-remove-all-remnants-of-visual-studio-fro), just in case you might've gotten a bit confused. As to if this answers my question, I'm not sure, to be honest. The reason I asked is that last time I tried to uninstall VS it broke all other VS versions, a handful of other programs, and left behind a ton of garbage. I had to format my system to get rid of all the garbage, and that's something I find unacceptable. I'm currently using VS in a VM when I have to and my code doesn't explicitly support VS because of this. However, I do realize many other people do use it, so I'd like to be able to use it without having to worry about having to format, so I can start supporting it.
At the language level, the philosophy is not to incur any unnecessary overhead and leave the semantics to the library. It is possible to write await adapters to achieve the behavior you desire. You can look at pplawait.h to see how await adapters are written to teach the coroutine machinery to work with ppl tasks. When **co_return** is encountered in a coroutine, return_void/return_value member functions of the coroutine promise are invoked. All of the variables alive at the point co_return are alive and will be destroyed after customization point return_void/return_value returns back to the coroutine. There is another customization point, final_suspend, which is invoked after user defined locals in the coroutine are destroyed. You can grab the adapters from pplawait.h and modify them as follows: Was: bool final_suspend() const { return (false); } void return_void() { _M_tce.set(); } void set_exception(const exception_ptr &amp;_Ptr) { _M_tce.set_exception(_Ptr); } private: concurrency::task_completion_event&lt;void&gt; _M_tce; }; Changed: bool final_suspend() { if (eptr) _M_tce.set_exception(std::move(eptr)); else _M_tce.set(); return (false); } void return_void() {} void set_exception(exception_ptr _Ptr) { eptr = std::move(_Ptr); } private: std::exception_ptr eptr; concurrency::task_completion_event&lt;void&gt; _M_tce; }; This changed will make sure that waiting thread will not be woken up before locals of the coroutine are destroyed. 
I don't doubt that the library solution has issues, the question is if it works well enough relative to how often people have to use it. Inheritance and virtual concepts basically just reverse the order of things. With inheritance, you stipulate that your type should follow a contract, and being out of sync with that contract is an error in your type. With virtual concepts, the type is "as-is", and the user of the type &amp; the concept must ensure their agreement. &gt; The remark about two types rarely having the exact same interface isn't necessarily a showstopper: adaptation/concept maps are part of the picture If you're going to write an adapter that's anything beyond the most trivial, then it basically erases this distinction. If Foo almost but not really conforms to your concept Bar, then anyhow you are writing adapter code, you can just call the adapter FooAdapter and have it inherit from Bar. FooAdapter can be specific to Foo, or in turn it could be templated and adapt one concept to another. The only time you actually gain something from virtual concepts is when things "just conform". For instance, suppose I have a vector of heterogeneous STL containers. I want to iterate through the vector, at each entry iterating through the STL container by calling begin and end. Obviously, inheritance forces you to use boilerplate that is not otherwise necessary. So the point is not that two types rarely having the same interface is a showstopper. It's just that unless they do, none of this buys you much. The real question is how often is this "just conforms" situation, compared to the other. In my experience, it's pretty rare. Mostly because there are an infinite amount of interfaces, so the odds that you will have conforming things from different libraries is rare, unless your interface is tiny, or unless all the libraries are aping the standard. If you have same interface things from the same library, they will typically inherit from a base class. The only reason not to at that point is because of performance, but with keyword final the only disadvantage of using a polymorphic object non-polymorphically is the vtable pointer. This is a pretty small price to pay in most cases. If you don't want to pay this price, you can trivially template your type on a boolean so that it "optionally" inherits from the virtual base class. This is a one liner that doesn't involve any of the boiler plate you've mentioned. In summary, I think the only cases you are left with where this buys you much are where the concept is very small (for instance, Callable of a certain signature) or follows a concept from the standard (like Container), or maybe in very very rare situations involving performance. This isn't trivial... but neither is the added complexity of all this. Maybe if C++ wasn't already so complicated I'd be easier to convince. But language complexity basically scales like N^2 with language features; as you have to deal with how things interact with each other. This is one more major feature, whose interaction with every single other feature has to be resolved. Even concepts which is arguably simpler than this has proven to be more challenging than expected in this regard.
yes, understood. our intention is to address these issues. This is the biggest push to clean up our setup story since I started here many years ago. This is just the first preview release and it has only limited workloads, so for the rest I'd say stay tuned and we are certainly tracking that user voice item. thanks for the feedback. - Steve the VC Dev Mgr
Thanks for working towards fixing this, I hope I can support VS in the near future. :)
and it has a debugger which is a common pain point we hear from Linux developers. - Steve the VC Dev Mgr edit: I no speak good english
Everypony knows this is an April Fool's joke, and nopony thinks it's funny.
I believe what o11c means is that ASLR doesn't actually stop attacks from happening; code that has exploitable bugs is still likely to be exploitable under ASLR; ASLR just increases the cost of "productizing" an exploit or decreases its chance of success. Compare with something like the stack protector ("-fstack-protector" or "/GS") which shuts down the stack buffer overflow attack vector completely.
Because there are *lots* of leaks about the addresses. And even if you don't know the exact address, you can guess with few negative consequences.
That was used as an excuse for trigraphs and digraphs, but that didn't stop them ...
Oh, so this is the time line that ends in the SG23 temporal cascade singularity...
Gor, thank you for quick reply and for the great job. Consider the following two functions: task&lt;MyValue&gt; value_task() { return create_task([]() { S s; return MyValue {}; }); } task&lt;MyValue&gt; value_coro() { S s; co_return MyValue {}; } Output when I run value_task().get(); OutputDebugString(TEXT("Got result\n")); is S ~S MyValue &amp;operator=(MyValue &amp;&amp;) MyValue(MyValue &amp;&amp;) Got result The frame's (== lambda) locals get destroyed **before** saving the return value. I assume I can never have 'Got result' printed **before** '~S'. Which is not the case with `value_coro`. Why did you decide to to make the change? I expect value_task and value_coro to be equivalent, which is apparently not the case.
I was going to just import it, but the modules system still isn't done. So I just copy-pasted it here.
Had to check which subreddit I was in. My goodness. 
Not at all - Visual Studio is fast enough for me. Visual Studio Code (which this is about) is very slow.
Yay finally! 
http://www.stroustrup.com/whitespace.html
This is great! 
Really nice.
We still have a few registration slots and hotel rooms open, although the group rate on the hotel rooms will not be available after April 8th.
Based on the screenshots, does this mean that the installation disk can be configured as well? aka disks other than c:\? 
They did it! https://www.reddit.com/r/cpp/comments/48n5ty/request_for_visual_c_only_please/
This is great, looking forward to reformatting Windows once it's done. :p
Look at the date: April the first
THANK YOU. Seriously. Assuming everyone wants blend thing (whatever that is) and other things that you could not opt out during visual studio install was one of the most retarded changes after vs2010. Lots of stupid things were done during that time. Good to see them being undone. Remember all-caps application menu bar? I was rejoiced to see it reverted to "the old ways" too. Keep it coming.
Very cool! The only way this could be even better is a zip file with VSshell and the MSVC compiler and a couple of batch files to setup a CLI build environment. However that, for me, is the holy grail so I don't expect it ;) I am *very* happy to see this slimmed down install though. I have been wanting this since they killed off the separate Visual C++ IDE back in like 2000 or so?
Would you please explain what blend is?
I personally like this (I'm biased, I have got similar idea before but keep in my head without telling anyone) but I think a lot of people will argue that it essentially add a third way to have method call and will be against it. We also can bike-shed on the syntax. I cannot explain why, but I will prefer something different than "." without finding something that I really like. .f(x,y); :f(x,y); $f(x,y); @f(x,y); f[x,y,z]; f{x,y,z}; call&lt;f&gt;(x, y); f.(x,y); f^(x,y) f@(x,y); 
none of the variations you suggest there have the desired feature for UFCS which is: putting one parameter up front (like 'subject verb object'; approximates infix notation; allows dot-autocomplete to narrow down suggestions based on knowing a type, and allows more readable function names e.g. with trailing prepositions that you know apply to subsequent parameters; and allows chaining without nesting e.g. ```a.foo(b).bar(c).baz(d)``` )
Maybe allowing a class to implement a special static "sizeof" function could allow not to need full definitions included when creating an object. // Header with partial declaration partial class MyObject { public: // implemented in cpp file static constexpr int sizeof(); // implemented in cpp file MyObject(); }; // Somewhere where we include only the header with partial declaration void dummy() { // We know the size on the stack without needing the full definition // thanks to "MyObject::sizeof" MyObject obj; // We know the size to allocate on heap without needing the full definition // thanks to "MyObject::sizeof" auto onHeap = new MyObject(); } 
The first line of my list of proposition is the version proposed in the paper (put for comparison). The three next are strictly equivalent with a different special character instead of dot. I don't understand why you think you cannot auto complete correctly with those syntaxes. In which way having this: .x.f(y).g(z); .g(.f(x,y), z); Is different from this ? :x:f(y):g(z); :g(:f(x,y), z); It's just more obvious that we use the new "unified call" syntax
I don't think that's a good idea simply because you have to invent another replacement syntax for `ptr-&gt;foo(42)` to make it work with pointers as well.
All caps menu and full gray IDE was indeed pretty rough.
Heh. OK, that's not what I'd expected. I'm glad you don't have performance issues with Visual Studio: we've done a lot of work to speed it up in the past couple of years. VS Code is a bit newer--it will improve in many ways, I'm sure. Thanks for the feedback! 
Ugly. Why not just pick something like extension method in C#? (I am learning D these days and I think the UFCS of D is nice.)
Oh, `[[suppress(...)]]`? I see. I'll wait for the lifetime safety profile to be filled out, but the exact semantics of suppressing the lifetime rules in one block seem rather murky. It'd be nice to have finer grained control eg. [[doesntinvalidate(p)]] v.push_back(42); so that you don't have to lose all checking if you don't want it for just one thing.
If this team had a policy of never using shared pointers I would avoid working there at all costs. Having said that, the use cases for using shared_ptrs is limited to safely managing the lifetime of shared resources. If the resource is not shared then a unique_ptr is preferred. If ownership is not a factor, then pass a reference. If you need a buffer of memory use a vector. But if you need a shared_ptr use a shared_ptr 
Thanks!
Of course :-) can be used in C++. int main(int argc, char* argv[]) { # define m(x) # define smile m( // Name emoticon suffix // ---------------- -------- ------ smile :-) ; } OK, I need a facepalm emoticon "m(" to use it, but does that justify a language change?
FWIW, I can't reproduce that with my Update 2 RTM install.
Did you see our RTM for the build tools sku? It's not quite a zip file but it is compilers and build environment only. Windows sdk requires msi and we require the windows sdk but it is quite minimal. (On mobile so no link but check the blog or the going native from Thursday)
it's good practice to disclose the recommended libraries that you authored yourself.
Ohh no! Thanks very much :) Will check it out. 
There is a lot of overlap. There is only a very small fraction of all this code that SHOULD be in C.
I don't use them so I can't tell. AFIAK, boost.fiber tries to provide classes very similar to std::thread, std::mutex, std::future and so on, but indended to be used with fibers (green threads). And boost.fiber is not in Boost at this time. 
I wrote a unit test framework that selects and prints a random positive message when your tests pass, and gives you encouragement when they fail.
&gt;There is no reason but inertia to use C. Interoperability with other languages is one thing that C has going for it. Granted, writing the library in C++ while providing a C interface for it is probably the way to go.
Hideous. The original UFCS x.f(y) -&gt; f(x,y) syntax was perfectly fine. It would not have broken *any* existing code, and it's been shown to work perfectly well in D. The objections in Jacksonville that killed it were without merit, and it's to great detriment to C++ that we won't have this incredibly useful feature now in C++17. The point of UFCS was that it was *transparent*. Requiring an alternate and unwieldy syntax (and one that will likely conflict with C out-of-order struct initializers if that ever makes it to C++) misses the entire point.
The proceedings reference is the one I've seen as well. If you're on Twitter perhaps you can reach out to the author for more information: https://twitter.com/zhendongsu
There's a "Who uses C++?" section in the [C++ FAQ](https://isocpp.org/faq): https://isocpp.org/wiki/faq/big-picture#who-uses-cpp In particular: - http://www.stroustrup.com/applications.html - http://www.lextrait.com/vincent/implementations.html
Noob here. Can't we make it like a compiler option? That way people that want to use can use it, and people that don't want to use can choose not to use it.
Where have you seen it? 
While it really doesn't matter for this example, and often the compiler can optimize in any case, I suggest you get in the habit of preferring prefix increment (++anger) vs postfix increment (anger++). Prefix might be faster in some cases, but more to the point it is likely more logically what you want to do. 
This is the first time I've seen it mentioned in at *least* a decade, so I am rather confused by your question.
&gt;Hideous. The original UFCS x.f(y) -&gt; f(x,y) syntax was perfectly fine. It would not have broken any existing code, and it's been shown to work perfectly well in D. The objections in Jacksonville that killed it were without merit, and it's to great detriment to C++ that we won't have this incredibly useful feature now in C++17. Rubbish. It introduced a lot of complexity, unnecessarily. 
&gt;putting one parameter up front Doing this has no benefits whatsoever. &gt;like 'subject verb object' Firstly, this isn't English. It's C++. Secondly, SVO is one of many word orders. There are plenty of languages out there with VSO word order, for example. &gt;approximates infix notation That's what operator overloading is for. &gt;allows dot-autocomplete to narrow down suggestions based on knowing a type This has nothing to do with UFCS and doesn't require it. Type `a.[tab][down][down][enter]` and you get `foo(a, [menu]`. &gt;and allows more readable function names Utter unsubstantiated rubbish. &gt;and allows chaining without nesting That's a bad thing.
That would be terrible. Some code would only work with the option, some would only work without. God that's terrible. Really, if people want this sort of thing they should write an editor that will understand the structure of their code and present it like their awful stupid `a.b().c().d()` even when it's actually `d(c(b(a)))` (i.e. when it's actually sane).
&gt;Both C and C++ are touring complete, so you can solve exactly the same problems in both. Completely irrelevant and moronic point that people for some reason feel the need to bring up every time languages are discussed.
&gt; Utter unsubstantiated rubbish. ```copy(a,b,c)```: no indication of what is going on; ```a.copy_from(b,c)```: - that is much clearer. a trailing preposition in the name applies to the parameters after it. Most functions are *not* commutative, so separating arguments with meaning makes sense. The asymmetry is actually helpful. The syntax is *so* popular that people risk the hazards of classes to get it. UFCS would give the benefits, hazard free. People conflate the hazard of classes with the syntax itself, which is throwing out the baby with the bathwater. &gt; That's a bad thing. It's a good thing; that's why people like infix notation. keeps the operators closer to the operands and you don't have to go back and forth balancing brackets &gt; That's what operator overloading is for. we only have a few operators , and we need huge numbers of distinct functions. Custom operators quickly get obscure. ```&lt;$&gt;``` etc, what the hell does that mean. &gt; This has nothing to do with UFCS and doesn't require it. Type a.[tab][down][down][enter] and you get foo(a, [menu]. It makes a complete mess of what you write, especially in a chaining scenario. Coherence between *what you write* and *what you read* (and hence *what you think about* when you see it) will make all programmers more efficient. *Communication between programmers* is an important function of a programming language. Readability matters. &gt; SVO is one of many word orders. There are plenty of languages out there English dominates the computing world already. If another word order is actually better? "yoda, heard him we all have. Improvement to english, suggest it you should". I'll stick with SVO. The nesting reduction has an objective benefit: more 'locality' (less temporary state whilst encoding/decoding) as you stream the expression in and out of your skull. It makes the man-machine and inter-programmer communication channel more efficient. Alternating 'subjects/objects' with verbs gives you maximum ability to encode a subset of a graph in a single stream. S-V-O-V-O-.. This efficiency is quantifiable as: the reduction of the number of extra symbols required (to create the correct grouping).
To save the complexity of a compiler lookup, we've shifted all of that complexity onto library developers who now implement classes that contain every possible feature imaginable, because they know that once the class definition ends, that's it. Subclassing doesn't solve much: the second two separate libraries subclass the same base class, they can no longer talk to each other with the same derived objects and functions between the two. UFCS would have allowed true encapsulation. Classes could become the core variables and functions that need to poke at private data, and classes could be extended at any time with extra functions. Library authors could have shipped very lean classes, along with a set of headers to extend various features. Library users could have chosen the features they wanted. People would have been far more likely to use the C++ standard library class types (std::vector, std::string, etc) instead of rolling their own like they do now. f(x,y) is not a solution. It turns C++ into Lisp, and destroys the beauty of C++: string.reverse().rtrim("#").lowercase().split("\n").strip(" "); Now becomes: strip(split("\n, lowercase(reverse(rtrim(string, "#")))), " "); Where the order of operations and function arguments are separated from each other, and no longer read cleanly from left to right. If you thought the feature was unnecessary, you didn't have to use it. I'd have even been fine with an -fno-ufcs flag like we have to RTTI and exceptions. C++ has always been a language that gave users lots of features and power, where you only pay for them if you want them. And that's why I've found it far more useful than C.
Apparently Borland Turbo C++ is still used a lot in schools in India.
Well, frankly speaking we could have gcc98, gcc03, gcc11 and gcc14 instead :)
It makes no sense whatsoever.
Linux is completely free with up to date compilers, better tools &amp; etc. 
You also gain cross-compiler ABI compatibility by writing a library purely in C (all of it, not just the interface). Once you mix C++, you can't link a binary compiled by MSVC2010 to a project in MSVC2015. Even though I'm a huge fan of C++, there's a unique feeling of lightness in writing a library in pure C.
Oh, now I understand why is there a variadic argument for coroutine_traits: // TEMPLATE CLASS coroutine_traits template &lt;typename _Ret, typename... _Ts&gt; struct coroutine_traits { using promise_type = typename _Ret::promise_type; }; So I can override it with more specialized version: template &lt;typename T&gt; struct coroutine_traits&lt;concurrency::task&lt;T&gt;/* no , _Whatever... here */&gt; { struct promise_type { auto get_return_object() const { return concurrency::create_task(tce_); } bool initial_suspend() const { return false; } bool final_suspend() const { if (ex_) tce_.set_exception(ex_); else tce_.set(val_); return false; } void return_value(T const &amp;val) { val_ = val; } void set_exception(const exception_ptr &amp;ptr) { ex_ = ptr; } private: T val_; exception_ptr ex_; concurrency::task_completion_event&lt;T&gt; tce_; }; }; This template gets chosen by compiler instead of the one supplied in `pplawait.h` header, cool. BTW, what is the use of `await_transform` and are there any other undocumented/unseen features? Thanks, Gor. 
That's why it's not a proposal, just regular C++
&gt;copy(a,b,c) no indication of what is going on a.copy_from(b,c) - that is much clearer. a trailing preposition in the name applies to the parameters after it. Rubbish. &gt;it's a good thing; thats why people like infix notation. keeps the operators closer to the operands People don't, actually. &gt;we only have a few operators , and we need huge numbers of distinct functions. We certainly do not need huge numbers of idiotic-looking `a.f(b)` 'operators'. &gt;making a complete mess of what you write, especially in a chaining scenario. False. &gt;Coherence between what you write and what you read (and hence what you think about) will make all programmers more efficient. Communication between programmers is an important function of a programming language. Readability matters. That's exactly why UFCS is awful. It kills readability and kills the correspondence between syntax and semantics.
Try not to be a moron.
What exactly does `make_shared()` have to do with anything?
So, I voted no and then strongly against UFCS at Jacksonville. After Jacksonville, Bjarne sent a mail to the list asking people who voted against UFCS if they could explain their position. I sent a reply summarizing my position and providing an example taken from my production codebase that I thought would become problematic with the prior (P0301R0 aka what we voted on at Jacksonville) version of this proposal. I also said: &gt; In retrospect, I realize I was remiss in my original post as I failed &gt; to explain what would be necessary to change my vote. &gt; &gt; I would vote in favor of a new unified call syntax proposal that took &gt; one of the following approaches: &gt; &gt; * A new distinct syntax (@f(x, y), f.(x,y), .f(x, y) / .x.f(y)), and &gt; overload semantics for this syntax instead of the priority/fallback &gt; semantics from the current proposal. I would prefer a syntax that &gt; would allow chaining (e.g. g(f(x,y), z)). &gt; * The syntax and semantics from P0301R0 (f(x,y) -&gt; x.f(y), &gt; priority/fallback for lookup) with an opt-in mechanism. &gt; * The original syntax and semantics (f(x,y) -&gt; x.f(y), x.f(y) -&gt; f(x, y), &gt; priority/fallback for lookup) with an opt-in mechanism. &gt; &gt; The opt-in mechanism could be one of the following, or both: &gt; * Class granularity (indicating any method in the class can be found &gt; with UCS). I would not object to the opt-in mechanism applying to base &gt; classes - e.g. if a derived class opts in, base class methods would be &gt; able to be found by UCS. &gt; * Method granularity (indicating that a particular method in a class &gt; can be found with UCS). I wanted @f() or f.(), but I can live with this. I voted strongly against UFCS at Jacksonville, but I will support this new proposal vocally and with my vote. I have not yet decided whether I would support this in C++17, or if I feel it should go into a TS. 
They're not pushing it. The library authors solicited feedback from people who voted against UFCS (like me). I feel that they addressed all of my concerns. 
What? `make_shared` doesn't make `shared_ptr` intrusive.
Because it breaks the fundamental paradigm of the STL. I could see one of the [SG14](https://groups.google.com/a/isocpp.org/forum/#!forum/sg14) technical specifications being for adding intrusive containers; particularly because they already exist in Boost.
It is a step in the good direction, but really a fully portable package of a reasonable size would be excellent. And btw I just tested the ewdk which does just that, and it contains among other things a windows sdk + msvc 2015 compilers. Sadly, vcvarsall has not been adapted so if your build requires it, it will just choke on not finding registry entries and then fail, but I guess writing a replacement that just uses env variables can be done.
&gt; If performance is so critical that you need to be using intrusive containers, maybe you should be writing in C. Why? The code will not run faster just because it is harder to write and debug.
I did not downvote you. You are doing a great deal of asserting things, and less backing them up. Could you clarify responses like "false" and "rubbish" what part of what is stated is not correct and why? Do you have examples of killed readability, to use your terms?
Typically, whenever I've had to use my own intrusive containers, it was because I just didn't have the luxury of using a memory allocator. I either had to get a single page from the OS, make a big stack frame up the call stack or statically allocate the space. Not that I didn't want the overhead of using the memory allocator, I just didn't have one, because including a memory allocator in the project would add complexity I couldn't afford. These projects were in C, involved embedded or kernel code. In these cases, if you're trying to wrestle C++ into the project, maybe try simplifying your tools. I understand speed, not memory is an issue for some applications and in those cases yeah, I see why they are useful, but I'll repeat the point about this not being the case for most C++ programmers. That being said, could this be added to STL and make sense? Yeah probably. I'm just saying why they haven't done it yet is probably because they don't have much appeal to most people.
Language versioning goes always upward. You propose something like side version. That could easily turn into horrid mess ('so, this library works with c++17, with X disabled, Y enabled, Z halfwayenabled, and if you want to use it, your code must use exactly same configuration.')
&gt;They are definitely needed for performance in the case where you have to manage millions of small objects partitioned from a big memory slab (memory pool). I've been there. I was coding in C in a kernel context and also inside a few embedded systems. That's exactly when I wrote or used an existing intrusive container. You are absolutely right, in that instance it's not a rare choice at all. I'm just saying this is a rare instance, at least in the C++ world. Very common in OS and embedded programming but those endeavors are typically done in C. I wouldn't add a dependency like boost to an environment where you're constrained to use a pool allocator. Personally, I would take a bit of time to write my own container, I don't imagine you need anything other than an intrusive linked list and maybe a hash map. Link list is easy to write (especially in C++) and you can write the hash map using the linked list you wrote. Maybe that's the C programmer in me that's talking but it's just my 2 cents.
The dialect of C++ that Turbo C++ supports is ancient. It far predates the first standard (C++98), so you can't even say that you are learning standard C++, let alone modern standard C++ such as C++11 or C++14. People that are taught like this learn "C with classes", not actual C++. It would be like teaching medical students using a textbook that predates germ theory and which talks about balancing the four humors via bloodletting. 
It wouldn't, assuming that they implement the same external API so that they work with standard algorithms. But I think it would be a tad confusing if there were two completely separate container libraries. That sort of gets down to the heart of the matter of what you really want in a standard library. There are tons and tons of things that you could potentially include that *somebody* would find useful, but if you're not careful you end up with a hodgepodge. The standard library I think has an extra burden to include components that are as universal and generic as possible. It's a balancing act. That's not to say that if somebody wrote up a proposal it wouldn't be considered, but back in the mid 90s during the push to standardization I think they had a lot more on their plate to deal with and having two separate container libraries would have been a bridge too far. 
we can have both. proper UFCS, and the new syntax for designated arguments
`make_shared` allocates the object and the `shared_ptr`'s control block together. It's not intrusive, but it's similar in the sense that it focuses on data locality.
Nice! That's a smart feature!
Ooh, good note; I wasn't aware I could and it sounds considerably easier to read. Thanks!
I am starting to like this new Microsoft. And this code editor works on the Mac. So great.
D E S I G N A T E D E S I G N A T E D
Just use GNU parallel extensions
What is your rationale for reinventing a part of boost instead of just using it ?
Can you give a more concrete example? I'm very skeptical that the things you are talking about can't be done with larger chunks of data instead of treating things as many small objects.
Possible reasons: 1. Not wanting to introduce an external dependency. 1. Working with a smaller subset of the C++ standard due to not having a production compiler for the target you're working on. If Boost uses C++ features you yourself haven't implemented in your system, then you can't include Boost. 2. You don't need a works-for-all solution like Boost containers, you just need a subset of the functionality. In a resource constrained environment, writing your own containers can be the way to go. These are all valid reasons when working in embedded projects. There are many many reasons not to introduce 3rd party libraries into your project.
Now let's just hope for constexpr switch. 
&gt; They don't require that you tailor the type specifically for the container SG7 (compile-time reflection) might be able to help with that.
386 level hardware? A rasberry pi zero costs 5 dollars. You still need a monitor, keyboard desk chair and power, all of which would cost more than the computer. Why would anyone use hardware that slow as a PC? A little critical thinking can go a long way.
Who says they do? Did you just hear of one school using it? India is a big place. Lots of people there.
&gt; It would not have broken any existing code. How situation where both x.f(y) and f(x,y) exist and do slightly different things (one mutates operand, one do not) are resolved? I did not follow papers, but it was my main objection when it was proposed first time.
Why valueless variants should be avoided at all costs?
Here's the Build Tools post with a link to the tools: https://blogs.msdn.microsoft.com/vcblog/2016/03/31/announcing-the-official-release-of-the-visual-c-build-tools-2015/
Pretty sure that's just on CLion. Makes sense that most people on CLion would use GCC, since if they want to use VC++ they'll just use VS. Nice info, but really doesn't matter to me how many people use what. I mean it says almost half the devs use C...
I fail to see how you can *not* discard the `static_assert`s from the nontaken branch.
Wish I could upvote twice.
I see. I will try a map of that key and value pair where they are pointers to those strict a. Thanks a lot man. 
It's the same issue as null pointers. It adds an implicit "lack of state" state to every single type, even when it doesn't make sense for the type to have one. Then any consuming code needs to either hope that it never happens or check for it.
Why not just make the conditions that would lead to a valueless variant cause an abort?
**Edit:** I think I was wrong! This [could](https://www.reddit.com/r/cpp/comments/4d608e/p0308r0_valueless_variants_considered_harmful/d1re716) actually be solved if C++ supported relocation. ----- Even though I authored the Relocator proposal, I don't think it can address the valueless variant problem. The valueless variant problem arises from, informally, nullability requirements imposed by existing move semantics in C++. To invent some ad-hoc terms, we can mean different things with "object": * There's *object-storage* &amp;ndash; the memory location where the object-concept currently lives. * There's *object-concept* &amp;ndash; an instance of a contract between object methods, constructors, destructors, and its data. * There's *object-value* &amp;ndash; the value that an object stores. Currently &amp;ndash; without relocation &amp;ndash; C++ supports moving an *object-value* to a different *object-concept*, which necessarily lives in different *object-storage*. The ideas of *object-concept* and *object-storage* are married, so there's no way to move an object-concept to different object-storage, without constructing a new object-concept, moving the object-value, and destroying the previous object-concept. Relocation is a proposal to recognize the distinction between *object-concept* and *object-storage*, implementing a way for object-concepts to be moved, without involving construction and destruction. The main use for this would be as an efficiency improvement in library containers. When moving objects to new storage &amp;ndash; or just shifting them in a vector erase or insert &amp;ndash; objects could just be memmove'd, potentially with some patch-ups. Several libraries already do so, but currently at cost of undefined behavior. Unfortunately &amp;ndash; this does not solve the problem of the valueless variant in the context of current C++ move semantics. The valueless variant comes up in situations where the moved-from object-concept is preserved, so relocation cannot be used. For example: std::variant&lt;X, Y&gt; a = ...; // guaranteed to have some "valid" value of type X or Y std::variant&lt;X, Y&gt; b { std::move(a) }; // bad_alloc here // does "a" store a "valid" value of type X or Y? does "b"? Relocation cannot be used here because object-concept "a" is preserved after the move, separately from object-concept "b". Relocation would require object-storage "a" to become uninitialized memory after the relocation; **and** for object-concept "b" to not exist *prior* to relocation; so that object-concept "a" can be moved to object-storage "b", while preserving object-value "a". ----- Ultimately, the problem with the variant is that it wants to be a non-nullable type; but the move semantics adopted by C++ "require" (if you want safety) the moved-from object-concept to be noexcept-nullable. This means the variant must either itself have a null state, independent of its value types; or, all of its possible value types must be noexcept-nullable. With current C++ move semantics, non-nullable constructions like the particular `std::list` implementation just seem like a bad idea to me. It's elegant, sure &amp;ndash; but it's cleverly at odds with C++ move semantics. And probably, that `std::list` predates them. ----- **Summary:** If C++ went a different route, and standardized only relocation (move of same *object-concept* to different *object-storage*), this would not be an issue. But instead, we have a different type of move (*object-value* moved to another *object-concept*), and this now requires nullability, for all types. I guess most people thought leaving an empty object behind would be a feature. :) C++ move semantics basically create another instance of the [billion-dollar mistake](https://en.wikipedia.org/wiki/Null_pointer#History) &amp;ndash; except now, with null object-concepts. This was standardized, and we have to live with this now. So, unfortunately, things like non-nullable `std::list` need to go out the window.
synth is not intended to be a documentation generator. Rather the goal is to be able to explore source code with (some of) the possibilities usually found in an IDE but (a) with greater accuracy because synth can take its time for parsing and does not need to adapt to changing source code and (b) if the project of interest already hosts processed source code, even without downloading (the source code or synth). A tool you could compare it to is https://code.woboq.org/. And as of now, it compares rather poorly. Apart from synth just being a prototype, the main differences are the license (synth is MIT while Woboq costs money if you want to use it for a commercial project) and that Woboq uses LibTooling while synth uses libclang, the C interface built on top of that, though this is still subject to change. The advantage of libclang is that it supposedly stays compatible across versions.
I know what particle systems are and how octrees work, in fact I have written my own and they have been very fast, but that still doesn't answer my question. For instance I would treat a particle system as a matrix of values with each attribute being contiguous in memory. Trees can be implemented with a vector per level (as well as many other tricks). Nothing dictates that you have to do an allocation every time you want to make a new node and in fact I can promise you that that will end up not only very slow on one core, but even worse on multiple cores as global memory allocations will block each other. 
There's already precedent for such limitations in the existing containers, though. Associative containers, for instance, can only be used with types that are compatible with the key comparison or hasher/key equality type on the container. An intrusive container could still require its embedded type to be "Linkable" or compatible with a link-handling type on the container, without requiring a specific field name or subclass on the value type. 
[removed]
I'm pretty sure the problem is trying to salvage an `std::list` implementation that isn't `nothrow_move_constructible`. C++ has adopted move semantics that are incompatible with types that lack a null-state reachable without exceptions. Given that C++ has adopted such move semantics, it should now not have library types that lack this (aren't `nothrow_move_constructible`). That's all there is to it. Maybe different move semantics would have been better, but they are the way they are. A requirement must be instituted for standard library types to be `nothrow_move_constructible`, and that particular `std::list` implementation needs to go out the window.
An "actual post"? Like, a self-post? Which is a link to text, posted on reddit? ;) I agree it needs more effort if it were to work as a standalone post, so no problem. I guess it's for the best. Also, I made an error &amp;ndash; nothrow_move_assignable is trivial with std::swap. The problem is nothrow_move_constructible.
Sure, but is that really a problem? You could also write yourself into a corner with an associative container with a value type that wasn't externally hashable or comparable, but the Standard solution is to avoid doing that. A type that is not under your control can have the necessary fields added through composition. There would necessarily be restrictions on the value type, but no requirement for a specific type, fields, or inheritance hierarchy, which seems generic enough to me. Even with the standard containers I've had to write value type wrappers before, such as to enforce the allocator on a nested container. 
&gt; It's not even real C++ as you need a separate compilation step to generate proper C++. No, you need a separate compilation step if you want to generate reflection info in other files. But g++ compiles your source files as they are, there is no "intermediate steps". All the Qt keywords are just blank macros.
&gt; Because we already have `constexpr(expr)` Are we? Never seen it, never heard of it.
If you use the signal/slot system of Qt (and pretty much everything in Qt builds on it) you need to run your files through moc (Meta-Object Compiler) bevor you can compile them with the actual c++ compiler. So for every real world Qt application you will need this intermediate step.
&gt; bevor you can compile them with the actual c++ compiler. No. moc generate *other* files that are to be compiled too, but your source files are compiled as they are by gcc/clang/etc. 
Where is the difference? Without running moc you're not able to build your program.
I think it's worth mentioning that this may be largely solveable with the `operator .` proposal. One could just write a proxy that contains the third party type in question, forwards all calls to it via `operator .`, and includes whatever other stuff (like inheriting from a class) to work with the intrusive container.
fwiw I think that the cpp reddit is a fairly reasonable place. There are occasional rude comments, but I actually quite consistently see the bad ones get downvoted. I think as long as you're polite you don't need to worry about stepping on eggshells. Btw, I'm guessing you're an HFT guy as well? I'm as well; feel free to shoot me a message sometime, would be curious to hear what your consulting business is about.
I've used intrusive pointers quite extensively in a pre 11 codebase, and it was not error prone at all. The necessary hooks were implemented in a base class (call it SharedBase), and any class that you wanted to be handled via intrusive pointer just inherited from that class (which also made it uncopyable). Intrusiveness is just that: it's intrusive, it affects the classes you write in ways that non-intrusive code doesn't. That doesn't necessarily mean it has to be bug prone.
https://sourcegraph.com it doesn't support cpp yet.
Why not just have a cell id stored as another attribute? If you need to go in the other direction there are options too. I'm still not sure what intrusive means and why this is a data structure design that seems so necessary.
* It might be nice to have a CSS class for *every* single production in the grammar. * Or better, generate XML and then use XSLT to generate the HTML with the limited CSS. * You should be able to automatically generate and link to the source for all external libraries, such as boost, and detect that they are separate projects. Remember the HTML docs for them might be on a separate HTTP server. * It's probably worth having a list of all C{89,99,11} and POSIX-{????,2001,2008} headers and symbols, and then also check for `#pragma GCC system_header` for implementation details. Emit this to the XML file even if you don't currently use it for the HTML transformation. * Is it possible to pass in flex and bison sources?
C++ is already full of faults detectable only at run-time, so what's the big deal? Throw an `empty_variant` exception and be done with it. 
The reason why C or C++ is still being taught with Borland Turbo C++ is that teachers know nothing better. They are stuck with what they know. There are mingw tools and code blocks if they are not comfortable with Linux. There is a need to update the education.
&gt; avoid a branch on its presence/absence in the destructor Is that really such a big deal? Hopefully there are also other reasons for the sentinel nodes!
Yes, but neither it is `constexpr if` nor `constexpr(expr)`.
I have approved your comment, but you have been shadowbanned. You should contact the reddit admins (I cannot affect shadowbans as a moderator).
&gt; If you have a moved from object, you have to explicitly assign it a new value before you can do anything with it again I'd like vector&lt;int&gt; v1, v2; // .... v2 = std::move(v1); v1.push_back(42); to still work
It's ok to disagree here. Consider what would happen if instead of `push_back` your example would use `back`: vector&lt;int&gt; v1, v2; // .... v2 = std::move(v1); // ... forget v1 has been moved from ... = v1.back(); // Undefined behavior (not obvious) It is not obvious when calling `back` that `v1` has been moved from, but if instead I would need to write: vector&lt;int&gt; v1, v2; // .... v2 = std::move(v1); // ... forget v1 has been moved from v1 = vector&lt;int&gt;(); ... = v1.back(); // Still UB, but a bit more obvious. then the UB becomes a bit more obvious. So I would prefer the compiler to give me an error message saying that `v1` has been moved from and to force me to give it a value. Not all types are as "well behaved" as `std::vector`, and the risk of using a moved from object without knowing that it has been moved from is not worth it. 
But there is always this risk when using back() or pop() or anything else. std::vector&lt;int&gt; v1; //... 100 lines of code if(foo) { // TODO FIXME v1.push_back(36); } //... 100 lines of code v.back(); // maybe ub what we need is not to restrict move, but to have better static analysis tools to detect such cases.
Sure, in that case static analysis might actually be able to catch it, but if it is able to do so, then why not make it part of the language somehow? I think it is ok to want to reuse the storage of a moved from object, but: - if you want to reuse the storage of a moved from object, why won't you explicitly restore its invariants? (e.g. `vector&lt;int&gt;()` vs `vector&lt;int&gt;(10)`) - if you don't want to reuse the storage of a moved from object, why pay for restoring its invariants? Just for fun I was thinking yesterday if one would be able to catch double moves in C++ some how, but for things on the heap I don't think static analysis tools can help: auto* v1_ptr = new std::vector&lt;int&gt;(10); std::async([&amp;]() { auto v = std::move(*v1); auto e = v.back(); }); std::async([&amp;]() { auto v = std::move(*v1); auto e = v.back(); }); A tool that can catch that would be great, but without it, the best thing is to "don't do that".
&gt; The language gives the guarantee that in a constant expression context a constexpr function will be evaluated at compile time. Yeah, my bad. I thought about constant-expression but forgot to mention it. &gt; To achieve that instead of writing: `auto e = foo();` one needs to force `e` to be a constant expression by writing `constexpr auto e = foo();` &gt; Allowing an equivalent `auto e = constexpr(foo());` &lt;..&gt; Well it is not quite equivalent: `constexpr auto` is const and `auto` is not.
&gt; Well it is not quite equivalent: `constexpr auto` is const and `auto` is not. Indeed! The C++17 `constexpr` lambda trick should work though. One can probably macro that out and play with it once it is available. Maybe it turns out to be truly useful, there is a lot of creative people using C++.
By using the tip from John Doe `visualcppbuildtools_full.exe /layout [Download path]` I've downloaded the full image `1,63 GB`. The list of files is [here](http://pastebin.com/ynFD08vA). Note the presence of `50.352.408 dotNetFx45_Full_x86_x64.exe`. With this amount of "technology" it surely takes a while to install. But hey, better than installing Visual Studio Community just for the tools.
&gt; v2 = std::move(v1); &gt; // ... forget v1 has been moved from &gt; ... = v1.back(); // Undefined behavior (not obvious) &gt; It is not obvious when calling back that v1 has been moved from, but if instead I would need to write: Why is `std::move` special here? Any other member of `vector` that removes elements (`clear()`, repeated `pop_back()`) could render `back()` UB. It's the caller's responsibility to either check `empty()`or otherwise guarantee safe access (e.g. calling `v1.at(v1.size() - 1)` and dealing with the exception).
I thing such a thing could be potentially a base for web-based C++ IDE. With clang tooling it's becoming more and more easy to develop one.
sounds like a teacher that needs to be retired, if they can't be bothered to keep their skills up to date.
UFCS will only add more confusion reading code. It is not needed.
The highlighting looks really good! I'm currently working on Coati, a developer tool that makes also use of clang, but goes one step further. Coati analyses the source code and puts an interactive visualization next to the hyperlinked source code. That way it's easier to see how classes and functions relate to each other, without having to look at 5 different files: https://www.coati.io/
C++ is trying to move towards better static checking and type-safety so that errors can be caught at compile time. Throwing an exception would be a half-measure.
The Build Tools include MSBuild. MSBuild is a C# app. That means we need to include .NET :/ 
I'm not sure I follow. Wouldn't you mark one const and the other non-const? x.f(y) const or f(const x,y) ?
Which implementation of `std::list` are you referring to, out of question?
None, `std::vector` behaves very well on move because its move constructor/assignment operator are not zero-cost. It's a trade-off. One can write a vector with zero-cost move constructor/assignment that still leaves the vector in a destructible state, but then only a hand-full of functions in the vector API would be callable on a moved-from object. 
Sorry, how could you implement a zero-cost move constructor or assignment operator? Wouldn’t it at least have to copy the pointer addresses of the source object?
Zero-cost meaning O(1) and noexcept rather than potentially O(N).
awesome stuff, the screenshots do looks sweet. i'll be waiting for the linux version, thanks.
&gt; Wouldn't you mark one const and the other non-const? Usually yes, but I do not always want to call mutating function even on non-const objects. struct foo { foo bar() const; }; foo&amp; bar(foo&amp;); foo x; foo y = x.foo(); //Can I be sure that x did not change? 
What about the target's existing data? It needs to be destroyed, hence O(N).
Ah, good point, that’s of course true for the move assignment operator and swapping them would at least require one temporary! But at least for the constructor, you shouldn’t have to do this?
I think it's pugi XML not pugy, which btw is cool.
Then thank you for your clarification, it is really much appreciated! :)
What is so special about "move"? What is so special about assign? Why does the core language need to have special case handling for these two mutating operations as opposed to any other mutating operations? &gt; The standard "hints" that destructors should work on moved from objects, which hints that "valid state" means "at least destructible". ... I kind of wish that the standard would very precisely define what a "valid state" is An object in a valid state behaves according to its specification; conversely, an object in an invalid state might not conform to its specification. "valid-but-unspecified" state means that the state of an object is indeterminate, except that it must be in some valid state. It's unsafe to call a function on an object in an unspecified state if the function has preconditions on the state of the object, since those preconditions may not be satisfied. &gt; Basically, you can do whatever you want as long as you document it as pre conditions on your methods (i.e. you can actually document the destructor of a type as "cannot be called on a moved from object" and the standard is fine with that). The standard library promises that any if its types are left in a valid-but-unspecified state when moved from, and requires the same of user-defined types with which it interacts. Outside the standard library is the wild west: C++ lets you do any insane thing you like.
&gt; For std::vector its move constructor/assignment leave the vector in a default initialized state. If the allocator is std::allocator, yes. Otherwise, not necessarily: move assignment between vectors with unequal allocators without POCMA results in element-wise moves. The source vector could be left empty, or it could keep its size and be filled with moved-from elements. &gt; My point is that for "less well-behaved" or "more complex" types having to keep in mind whether an object has been moved from and which implicit preconditions hold on every method is hard. Again, as others have already said, this is no different for the state resulting from a move operation than for the result state of any other mutating operation. It's a fundamental consequence of languages with side effects.
Windows support is now merged and working.
But isn't a call to the system's dynamic memory allocation process also atomic?
Nope, it's real alright :-)
You work in the VC++ team?
`libdl` is wanted because other loadable modules can be loaded at runtime if the user wants. That's something that can be disabled and is automatically on platforms where we cannot support dynamic loading for whatever reason. The compile time cost is not for `#include` it's actually for the instantiation of the `ChaiScript` object. If you use separate compilation of the ChaiScript object in your project ChaiScript should have very little impact in your overall build time. The first live coding video I did was on setting up a project like that: https://www.youtube.com/watch?v=6GRSyiFhTI4
The LGPL is an obnoxious license, because it requires dynamic linking or distributed object files. Consider using something like the Boost software license.
I'd suggest outputting DOT format graphs, and then using graphviz to create the images. Anything else is going to be very heavy weight.
Interesting, thanks. 
As minimal goes, I don't think you could go further. It would sincerly be appreciated by I'm sure more than just myself. Win10SDK/MSVC15 inc+libs is &lt; 2GB (assuming ARM support is kept), which for some, is about all they can use. It might also be useful for updates that don't include the actual editor etc. Thanks!
STL's lectures on Channel 9: * [Standard Template Library](https://channel9.msdn.com/Series/C9-Lectures-Stephan-T-Lavavej-Standard-Template-Library-STL-) * [Advanced STL](https://channel9.msdn.com/Series/C9-Lectures-Stephan-T-Lavavej-Advanced-STL) * [Core C++](https://channel9.msdn.com/Series/C9-Lectures-Stephan-T-Lavavej-Core-C-)
QuickQanava core is now MIT: https://github.com/cneben/GTpo. I will consider using something more consistent with Qan2 dependencies (most notably Protocol Buffer).
&gt; Anything else is going to be very heavy weight. That's what I usually expect from C or C++ but this time I need something super basic and fast to get up and running. Now I have written a class for loading and saving WAVE files and I am using Audacity do view them. That's what I am looking for. 
&gt; graphviz I believe you can use graphviz as a [library](http://www.graphviz.org/Documentation.php) too. Look at the examples. 
I went and looked up how C# does it. Seems that is indeed very similar to what I had in mind. I would suggest making the actual argument name 'this' however.
&gt; Now I have written a class for loading and saving WAVE files and I am using Audacity do view them. libsndfile to the rescue
This is a graph : http://www.greatandlittle.com/studios/public/blowup-images/Dart/.directed_graph_m.jpg This is a plot : http://scidavis.sourceforge.net/manual/pics/plot-gallery-line.png This is a waveform : https://raw.githubusercontent.com/phding/waveform-node/master/doc/stack.png what are you looking for ?
If the semantics of that pragma were referring to file *contents* instead of files themselves, it'd be easy. It wouldn't matter where the file came from, whether it's a link, whether's it's on a network, etc. The compilers would be free to use platform-specific ways of foregoing the hash computation, but a typical implementation I presume would use the tuple (size of input, hash) to determine whether the contents are identical.
Took just 10 min and I don't need another dependency.
Graph == connected points. Plot == connected points. Waveform == connected points.
do you handle multi-channels ? different bit size ? different sampling rate ? 
Well the top level comment seems to pretty clearly indicate that they're incompatible.
I have removed this post because beginner questions are off-topic for this subreddit. Please go to /r/cpp_questions as the sidebar advises.
Go to /r/cpp_questions I don't have time. Your post here has been removed. It depends on what compiler / operating system you are using. Contrary to languages like python, you not only need to include #include &lt;vortex_key-api.h&gt;, but you also need to tell the compiler where to find the library. Before that you need to compile the library (unless you downloaded the already compiled version).
Does this generate wt template code or what?
Can you use GitHub Releases for hosting the binaries? I'm not very trusting of sourceforge after the stunts they have pulled...
I had a bit of spare time so I threw this tiny example together for you. It doesn't do much more than plot pixels, draw lines, and spit out a ppm file, but it might help you along your way. [Here's the gist](https://gist.github.com/lithium-snepo/f8324f1e5cb791d8a286aaf5de806193). Includes a Makefile and a sample application. And [here](http://i.imgur.com/ZtVGNKP.png) is the output of that application converted to a png. Feel free to use (or not) however you like. I've only tested on OSX but it isn't using anything platform specific so it shouldn't require much (if any) porting. 
Cool, I see. Thank you!
I found it kinda hilarious that it took you, as the project owner, over ten minutes in order to compile a simple hello world example which is even described on the main page 😁 I don't want to offend you, I even listen to cppcast and liked your previous videos, but this video didn't leave me with the impression that I should try out Chaiscript anytime soon...
SF a little while ago started injecting their adware ridden installer into applications hosted by them (windows for sure, maybe osx?). They claim it's for applications that were basically abandoned, but... As for hosting. I think hosting binaries on github is a necessary evil.
I find the project very intriguing indeed and I plan to use it in a toy project very soon. The compilation time is a concern but I hope to be able to keep it under control using different TUs
Thanks for elaborating on your thinking. I will have to write a separate post/analysis of why we are still missing a crucial step here. Maybe that will turn into a WG21 paper and help the community attain a greater shared understanding. Let me however address a couple of points here though &gt; [...] the header is modularized (e.g., the header has a module map) Modularization is a software architecture issue, not the fact that someone wrote a Clang modulemaps for a header file. For example, I consider most implementations of the standard library containers and algorithms (what people call STL), as modularized by design. &gt; The only declarations supported would be the implicit ones generated in Clang's implementation of the _Transparent migration_ section. That is the thing! Nobody is proposing Clang's implicit modules to WG21. I've had several conversations on the topic, one no later than two weeks ago in Barcelona (during EuroLLVM). I was told "they are weird", "they don't scale". Implicit modules are what were original implemented to support primarily Objective-C, moving a particular build system logic into the compiler. Most of the recent work in Clang for the C++ benefits have focused on so called "explicit modules" so that other build systems could be used. &gt; Clang doesn't implement the syntax that would let you try this out. OK. You suggested repeatedly that there is a solution (see below) to the issues I am talking about, but you are also saying I can't try it. &gt; Right, they proposed certain syntax (`import legacy string-literal;`) with specific semantics, and then describe a conforming extension that implicitly treats some `#include`s as having those semantics. The semantics in question is (quoting the paper): module partition module-name; export module { #include string-literal } but the translation unit containing the import may not be part of a module (think the translation unit containing `main()`). There is a lot here that is left to the imagination and it is easy to fill that with idealized semantics in ways that are not necessarily consistent. That is why it is important to try out the exact proposed changes with a concrete implementation. I was expecting to try out concrete scenarios based on the actual proposed changes, especially given that the paper is _Proposed modules changes from implementation and deployment experience_. The suggestion of faking it via Clang's modulemaps isn't satisfactory: I am trying to figure out (and many in the C++ community) is how the proposed changes actually work in practice; how do they affect the codes we write, how do they affect software architecture; how does the transition scenario play out? EDIT: I'm not good at Markdown.
I've had my eye on WT for a while, but your tool makes it much more accessible. First tutorial was good (if a little basic in places), but looking forward to a more advanced exploration. Wondered if you have any more advanced web pages you've created with the designer. I know the WT authors created their own site with WT, which is impressive enough, but I'm looking for an example of what can be achieved through your WT designer point and click builder. e.g. Alternative layouts for mobile, portrait and landscape. Multi-page sites. Also interested in how it would work if I needed some hand crafted WT code to supplement your generated stuff. e.g. round-tripping implications or best practices. Really looking forward to this lightweight but still productive website generator.
Thank you sir ! Much appreciated instructions. Will try it soon.
Anyone know what happened to the London UG? They seemed to have ceased existing a couple years ago. A bit surprising given that there should be loads of C++ people around London, including big companies like Google.
They're on the source forge page.
Thanks for the clarification. I wrote somewhere above (or in another comment) that it is required but not sufficient, but "statically known at compile-time" makes it clearer.
&gt; What is so special about "move"? What is so special about assign? Why does the core language need to have special case handling for these two mutating operations as opposed to any other mutating operations? Nothing, which is kind of the point I guess. They are, as you correctly describe, just another side-effecting operation that changes the state of the vector. The issue I have with them is that "the moved from state" and the methods one can/cannot use on a moved from object are typically not well documented in user-defined types in the wild.
&gt; Wouldn’t it at least have to copy the pointer addresses of the source object? Yes, you are correct. I meant that it is not a "zero-cost abstraction", not that it is a no-op. For `std::vector`, e.g., `.size()` has no preconditions, that is, it can be called on a moved from object, so if the move constructor of `vector` changes its size (see /u/CaseyCarter comment above), it also must change the `vector.size_` data member so that `size()` returns a correct result. 
You want a text editor plugin for a text editor? o_O
&gt; Do I need to get an external compiler? Sure. Try asking in /r/cpp_questions for recommendations.
Hmm. Would love to see it come to life again!
Well, last meeting was in October 2015 http://www.meetup.com/Chicago-C-CPP-Users-Group/ Without having really monthly or at least quarterly meetings its hard to keep a group running.
&gt; .x.f(y) is equivalent to .f(x,y) Why is the first one even a thing? It makes no sense to me.
&gt; In your example, how can either of `a` or `b` ever end up valueless? Apologies - I made a mistake by showing move construction instead of assignment. The actual situation is: std::variant&lt;X, Y&gt; a = X(); std::variant&lt;X, Y&gt; b = Y(); try { b = std::move(a); } // bad_alloc here catch (std::bad_alloc) {} In this case, variant's `operator=` has to do something like this: b.y.~Y(); new (&amp;b.x) X(std::move(a.x)); If the move constructor throws, `a` is supposed to be untouched, but `b` is now valueless. The only way `variant` could allocate storage upfront is by first moving into a temporary: X temp { std::move(a.x) }; b.y.~Y(); new (&amp;b.x) X(std::move(temp.x); But now the second move-constructor can throw, with the same resulting situation. But hah! Because `temp` is going away &amp;ndash; this could, actually, be resolved if C++ supported relocation: std::aligned_storage_t&lt;sizeof(X), alignof(X)&gt; tempData; X* temp = new (&amp;tempData) { std::move(a.x) }; b.y.~Y(); new (&amp;b.x) &gt;&gt;X(*temp); This could work. :)
&gt; Modularization is a software architecture issue, not the fact that someone wrote a Clang modulemaps for a header file. For example, I consider most implementations of the standard library containers and algorithms (what people call STL), as modularized by design. I agree with this in general for one sense of the word, but in the statement I made I was using a different sense. Specifically, the solution P0273R0 posits depends not just on a library being modular in the software architecture sense, but that it actually opts-in to being _imported_ as a C++ module. That is what allows the compiler to resolve and avoid any ambiguity between an `import` of a module and an indirect `#include` of a header containing the same declarations. Also, the link I provided for trying out clang modules contains exactly this scenario: the main file imports a module foo, and then includes a (non-modular) header legacy.h, and that legacy header includes the foo's header. The ambiguity is resolved using exactly the semantics P0273R0 describes. (Though to be fair, even if legacy.h redeclares the entities owned by module foo clang's current implementation doesn't complain. This is not essential however, and P0273R0 is explicit about what it's proposing should happen in this case.) &gt; I was told "they are weird", "they don't scale". Implicit modules are what were original implemented to support primarily Objective-C, moving a particular build system logic into the compiler. Most of the recent work in Clang for the C++ benefits have focused on so called "explicit modules" so that other build systems could be used. I would certainly be interested in hearing more about this sort of thing. I don't have inside information to say whether they were really motivated primarily by Obj-C, but from my outside perspective this does not seem to be an accurate characterization. I wonder if your conversations have given you this inside perspective? Doug Gregor would know and I'm sure you've talked with him. From my outside perspective C++ was always the ultimate goal, but on the way there, and given the careful attention paid to providing a migration path, it was necessary to support C. Obj-C was added because it was convenient and easy, and because it provided immediate business value on the side. However I doubt that Obj-C would have gotten modules at that time without the ongoing effort to provide modules for C++. But regardless of the origin, they do seem to work well for C++, and I believe they'd work equally well for any language that uses the traditional C separate compilation model. I don't know how to weigh "they are weird," but "they don't scale," is certainly an important criticism about which I'd like to hear more. There's also nothing about clang's modules that constitutes moving some pre-existing build system logic into the compiler. Clang is simply taking the "on demand" approach to processing modules. Your own proposal P0142R0 describes such behavior in clause 5.12 _Separate Compilation vs. On Demand_. Do you intend to discourage this behavior? &gt; Most of the recent work in Clang for the C++ benefits have focused on so called "explicit modules" so that other build systems could be used. I think you're talking about clang flags such`-fmodule-name`, `-fmodule-file`, `-emit-module`, etc. These flags allow one to use the `Separate Compilation` for modules as outlined in 5.12 of P0142R0. However these still depend on module maps and clang's implicit behavior for wrapping headers as modules. There's no difference in terms of the benefits to C++, and the module semantics are the same as when building modules 'on demand'. So there are two orthogonal axes here: separate compilation vs. on demand, and implicitly wrapping headers as modules vs. explicitly writing module interface files using the new modules syntax. &gt; OK. You suggested repeatedly that there is a solution (see below) to the issues I am talking about, but you are also saying I can't try it. My understanding was that you wanted to look at the behavior of a translation unit importing one module and also including a legacy header which includes a header containing the module's declarations. [Here's](http://melpon.org/wandbox/permlink/qQdnx9rDpqGlNRRw) how one can write that test case in clang now (testing the situation where transparent migration _is_ supported). If you want to see the behavior for of the legacy import declaration when the Transparent migration extension is _not_ supported, you could write a test like [this](http://melpon.org/wandbox/permlink/CB7DiY4DCqHPYwej). &gt; The semantics in question is (quoting the paper): &gt; module partition module-name; &gt; export module { &gt; #include string-literal &gt; } &gt; but the translation unit containing the import may not be part of a module (think the translation unit containing main()). Where do you see the requirement that the import cannot be in a module? This is the opposite of my reading: the `import legacy string-literal;` _will_ appear in a module, specifically in the module that is wrapping the legacy header. (The Transparent migration section uses this and shows an example of the import declaration in a module.) In the translation unit containing `main()`, for example, the user will import the wrapping module the same as they would import any other module. Then they can also indirectly `#include` the header wrapped by that module and this will work due to the special semantics of the `import legacy` declaration. &gt; There is a lot here that is left to the imagination and it is easy to fill that with idealized semantics in ways that are not necessarily consistent. Perhaps some assumptions of clang's implementation are too deeply ingrained in my conception of modules for me to even see that I'm making these assumptions or that they're not explicitly laid out in these proposals, but in reading P0273R0 I don't really see that much room for imagination to 'fill in the gaps.' Of course I welcome any correction or opportunities to clarify my understanding of P0273R0. &gt; The suggestion of faking it via Clang's modulemaps isn't satisfactory: I am trying to figure out (and many in the C++ community) is how the proposed changes actually work in practice; how do they affect the codes we write, how do they affect software architecture; how does the transition scenario play out? Clang's module maps aren't _faking_ modules though. You can actually create real modules and see how modules interact with different software architecture. For the transition scenario there's certainly no better source for experience in how real software and libraries have been affected. There are certain things you can't try out with clang's implementation, such as the effects of textually including a module declaration from one file in another file. I would guess you gave that example because you're curious about the way clang associates modules with files and you wanted to try out what happens if the module declaration comes from a file other than the file containing the module's exported declarations. That's an interesting and worthwhile thing to want to try, but the inability to try it doesn't mean there's not other useful experience with the semantics of modules to be gained from clang's implementation.
BSL is not copyleft. People who choose LGPL often want copyleft but will allow static linking (e.g. [Qwt](http://qwt.sourceforge.net/qwtlicense.html), [FLTK](http://www.fltk.org/COPYING.php), [wxWidgets](http://www.wxwidgets.org/about/licence/), and [Free Pascal](http://wiki.lazarus.freepascal.org/FPC_modified_LGPL)).
As the sidebar advises, please send homework questions to /r/cpp_questions.
Provided that you know as much as you show off here, in which way gets C++ on the way? Because if you can make proper use of it, provided that you know the tool very well as you said (better than me at least and I have been using for a long time), how can you make such a clumsy use of C++ to the point of considering it worse than C? C++ is more strongly typed, you have classes, lambdas, templates and lots of things that help you write shorter code. You can manage memory through smart pointers... In *every area* except binary ABIs, C++ is superior, so, as I said, maybe you do not know this tool as much as you pretend. But I can help you: https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md
No, It does not work like WTemplate, although you can use WTemplate with WtDesigner. It generates C++ code directly, while the template files you mention are in HTML with special tokens that WTemplate uses to bind widgets. Give it a try, or watch the first video tutorial, it might be better than my poor explanation. https://youtu.be/ACOIjGnfIR4
Sorry, I forgot to put some screenshots here. I did put some screenshots in the SourceForge link though, or you can watch the first video tutorial which is much more visual. https://youtu.be/ACOIjGnfIR4
EDIT: this question seems more appropriate for /r/cpp_questions which is geared towards beginners/learners. This subreddit is mostly news and exciting new stuff. I'll comment on only the C++ side instead of software design. I only looked over for a couple of minutes and here are the ones that stick out. * Don't pass around things like std::vector and std::string by value. Use const references. Copying is uneccesarily expensive. * I see you dynamically allocate in hangman model. You don't need dynamic allocation here at all and mostly importantly you don't have a destructor delete'ing them so it's a memory leak. Always use stack allocation unless you absolutely need dynamic allocation. Even if you use dynamic allocation, your use case is perfect for std::unique_ptr (you shouldn't need raw new/delete nowadays). * I see in your header guards you have _Ha.... etc. Names that begin with underscore followed by a capital letter or any names with 2 underscores is reserved by the implementation. In most cases pragma once is okay (although it's not in the standard). Good luck!
this probably belongs in /r/cpp_questions anyway some random thoughts : in the model : Absolutely no reason to use pointers and dynamic allocation : just use regular member variables, it's faster and less prone to mistakes. Case in point you are leaking memory because you are not deleting them. The three getters are returning by value, meaning that a copy is made each time you are calling them. You should return a const reference instead Having "this-&gt;" everywhere only bloat your code (opinion)
Congratulations on finishing your program. However, as the other comments have noted, beginner questions (including feedback requests) are off-topic for this subreddit. /r/cpp_questions is better suited for that.
 the output operands and the command suffixes really helped me here and let me finish my project just wanted to say thanks for the help :)
Have you tried using [catchsegv](http://man.he.net/man1/catchsegv)? This won't help with the mystery of why the signal isn't being caught but it could help narrow down your problem.
The example where you couldn't fix `#pragma once` would be two libraries that you don't have control over which end up creating the conflict.
Specifically [gnuplot-iostream](http://www.stahlke.org/dan/gnuplot-iostream/). 
When I was a rookie, I carefully read the sidebar :) --&gt;
After commandline applications became too boring for me I started building small games or other graphic stuff using openFrameworks: http://openframeworks.cc/ I can also recommend cinder: https://libcinder.org/
Can you give a concrete example of how you get into this situation?
Good luck!
In the future, please direct such questions to /r/cpp_questions as the sidebar advises.
Why not use the type system?? Using [constexpr strings](https://github.com/crazy-eddie/crazycpp/tree/master/20141016-constexprstr): namespace message_types { template &lt; typename Prefix &gt; struct message_type { constexpr message_type(Prefix p) : prefix(p) {} Prefix prefix; }; template &lt; int Size &gt; constexpr auto make_message(char const (&amp;str)[Size]) { auto msg_string = make_string(str); return message_type&lt;decltype(msg_string)&gt;{msg_string}; } constexpr auto info = make_message("info"); constexpr auto warning = make_message("warn"); constexpr auto error = make_message("error"); } template &lt; typename Prefix &gt; void log_message(message_types::message_type&lt;Prefix&gt; const&amp; type, std::string const&amp; msg) { std::cout &lt;&lt; type.prefix &lt;&lt; ": " &lt;&lt; msg; } There's a myriad ways you could do this and improve it. You could use runtime polymorphism instead. You could use the constexpr string itself as the type. You could add priority information to allow masking. You could add special formatting rules for the different types... Lot's of stuff you can do. Point is to keep all the information needed to operate upon log types within that type rather than spread out in a bunch of if/else branches. This obeys both OCP and LSP, which enums worked upon by if/else or switches violate. User can extend the logging system with new types. The logging operations don't need to change. No chance of getting the wrong information into your logging operations with typeos or casting from int... Reworking your system to use enums rather than strings helps a little bit by preventing typeos, but it doesn't address the underlying problems.
&gt; /r/cpp_questions thanks, sorry
Going to have to start an South Austin, TX group. North Austin might as well be a different city...45 minutes by car. With ARM, AMD, Altera, NXP (and lots others), we should have a good enough representation. Hmm, time to get planning :).
with a language as versatile as c++, there are no definite answers, only lessons learned. thats what makes it so perfect. as i said, either way good luck! 
They definitely do. They get the strings from some source that they can't change for any reason and keep using them instead of converting them to something else.
I'm sure it has something to do with all of the scripting languages that it's commonplace for. Now I'm curious what the specialization for std::hash std::string looks like.
I'm not understanding your rebuttal. If you have a static string in you functions as in your blog, such as: if (type == "warning") Then you are already dealing with constant strings, and there is very much a chance to convert that to constexpr strings...though that was not my point. As to the visitor pattern stuff and the, "polymorphism causes so much overhead," stuff...I just don't have any idea what you're talking about. Virtual methods take a minimal amount of instructions as modern processors are built to deal with them and a visitor pattern on this problem would be so much overkill it's not even funny. Stop using raw types to represent your domain. That's my point.
Seems I started a trend xD
I don't buy it. That would only fail if `#pragma once` was daft enough to only look at the filename, and not the file path? And if they have the same name, then how are you `#include`ing them without conflict? Plus, in the library case, you can build them separately and then link them in, you only need the library headers. All of these examples break in both `#pragma once` and normal preprocessor stuff. I can't see a fail case that's unique to `#pragma once`?
Have you profiled? Edit: Funny thing is that the code I posted doesn't make use of virtual dispatch at all.
This is just a specialization of "[stringly typed](http://c2.com/cgi/wiki?StringlyTyped) systems should be avoided". Don't take my comment the wrong way. The article is good. But the article should acknowledge at the end the wider practice of using strings where stronger types should be used. Links to related topics (not enums) would be icing on the cake. :)
That'd be awesome considering I was bummed to see none in Chicago.
Library A and B both download and use the header only utility `my_cool_function.h`. #include guards handle this but you get multiple definitions with `#pragma once`.
I don't like your terminology here. Libraries don't "download" anything. I think what you mean is that two libraries both contain copies of the same third-party library header file. If the relative include path for this header is different for both libraries (one uses it as `#include "include/libC.h"` while the other does `#include "libC/libC.h"` or whatever), then they will include two different files. Since this file is placed in two different locations, they are literally two separate files from the compiler's perspective. `#pragma once` simply can't do anything about this, while include guards save the day. That's a fair scenario I hadn't considered, and is indeed problematic. Solving this in general would probably require a method to specify to the compiler when two files are equivalent. Like some sort of mapping file. I don't know of any compiler that does this currently. But let me present a counterpoint. What if the version of Library C used by A is different than the one used by B? In that case, there is no good solution at all. If C changed their include guard macros with each version (something nobody does!), then you have the same problem as `#pragma once` (multiple definitions). If the macro is the same, then either A or B will pick up the wrong version, whichever happened to be included first. Since the ordering isn't necessarily consistent, each translation unit gets something different. Ouch. The end result: this is a bad situation all around, regardless of whether or not you use include guards or `#pragma once`. For the special case when both A and B happen to use the same version of C, include guard can make it work while `#pragma once` has a harder time. I'm not sure the takeaway here is "`#pragma once` is bad" -- there's some best-practices that can help. If you make a header-only library, maybe `#pragma once` isn't the right solution. If you include a header-only library with your code, don't put it in a weird path. (Notice that this is the root cause of the problem. If both A and B just used `#include "libC.h"` then everything would be fine.) Or maybe we shouldn't be bundling header-only libraries at all, but that seems a bit too dramatic.
This does not explain *why* you voted strongly against, only what is required to what you want to want *for*. Can you comment on what exactly you don't like about `x.f(y) = f(x,y)`?
Sorry but this feature has been proven useful in C#, it's basically 'validated', so it's worth having.
While it's unfortunate, "bad situations" happen a LOT in the real world and are usually the result of the limited resources that developers have to work under. `#include guards` are annoying and frustrating for a variety of reasons, but they exist for a reason and solve real world problems. Having said all of that, the real problem is not `#include guards` but the file inclusion model that was inherited from C. Another band-aid like `#pragma once` is not the solution, but hopefully the modules proposal will become available in multiple compilers soon and the problem can actually be fixed.
I can't argue with any of that. In the end, I don't think C++ should standardize `#pragma once`. Maybe it's superior to include guards, maybe it isn't ... but that ship has sailed. C, on the other hand, will not be getting modules any time soon ... probably never, really. A discussion about `#pragma once` on that side might be more appropriate (though I foresee even more resistance there than here, even though changes are often more likely to be possible for C than C++). And *if and only if* it's standardized there would C++ get it as an afterthought.
What is the advantage of this approach compared to just having a reflective enum class? If you want to do something with the type system, an enum class can be pulled into a type quite easily by templating on it. And you can still do runtime dispatch. And it's less awkward to use. The black magic to define a reflective enum seems comparable to the black magic in your implementation, but less of the black magic bubbles up to the user in the reflective enum case. So... ?
Building a reflection system in C++ is actually quite a LOT of black magic. I done it and it was useful, but I wouldn't call it simple at all. Don't focus on the use of constexpr string. You can do this easily without. It's just an addition to reduce runtime cost...MAYBE.
Does VS Update2 also come with the full libc++ STL? Or is it the Clang compiler with the Microsoft/Dinkumware STL? I've been using tip-of-trunk Clang/libc++ for over 2.5 years now on Linux, and would be willing to try Clang/C2, but I really also need libc++. 
It's with Microsoft's library. I imagine this is largely because of the need for ABI compatibility. Also I think libc++ still isn't really targeting Windows, so the development and testing lags there. What do you need libc++ on Windows for? Microsoft's standard library implementation hasn't been lagging behind recent standards the way the compiler has been, so I wouldn't think there'd be the same motivation for using an alternative like libc++.
I am looking for up-to-date c++1z conformance, so far libc++ has been doing that a lot better than libstdc++ (pre GCC 6.0, so perhaps in a few weeks that will have changed). But you might be right, looking at the feature list it appears that the MS stdlib is doing quite nicely. I'll give Clang/C2 a spin then.
Saw this: &gt; C and C++’s philosophy Red flag.
Videos: https://www.youtube.com/playlist?list=PL_R5A0lGi1ADuZKWUJOVOgXr2dRW06e55
I am already at a fairly high level with C programming, and I am now learning C++. Do you think the one you recommended would still be a better option? Thanks for the feedback btw, this is very helpful.
C and C++ have different idioms. Scott's book is more about how you can use newer C++11/14 features as a better replacement to C++98/03 idioms (and some new stuff you can do). The author of the book he suggested is the creator of C++ and it looks to be aimed better at you than Scott's. &gt; however I am sometimes finding new material very difficult to understand Definitely go to Bjarne's. Just by looking at each of their summary of contents, I see that Scott's is entirely about only the new things introduced in C++11/14.
Maybe you should just get access to it and see if you like it, and then buy it, of course. There's a sample you can read: [http://cdn.oreillystatic.com/oreilly/booksamplers/9781491903995_sampler.pdf](http://cdn.oreillystatic.com/oreilly/booksamplers/9781491903995_sampler.pdf) And I'm sure you can get more somewhere...
Absolutely - if you are not yet experienced enough in c++ to be using things like templates and r value references , you are better off with Bjarne's book to start off with
I thought Herb Sutter would lead the C++ development at Microsoft?
Being useful in C# and being useful in C++ are utterly different things.
libc++ does not support Windows in any capacity.
AFAIK, Herb is an engineer (albeit a principle one) while Steve is a manager – MSFT really likes making a distinction between the two.
/C2 and /LLVM in the linked SO question is somewhat misleading, they target different platforms. If both target MSVC, the frontend behavior is indeed the same (both emulate MSVC bug).
Ok, so one-phase lookup behavior has been baked into ms-extensions when it shouldn't have. This should be reported to upstream Clang. If there's anything in VC's STL headers that is relying on this, I want to know so I can fix it.
Wait, why exactly is `float!` not the correct output? `i` is a dependent name, so the lookup of `f` should be delayed to instantiation time, in which case `float` is the better fit. Is it the case that `fun` only looks at names prior to its definition? What's the standard language for this?
First off, this is appropriate for /r/cpp_questions Second, *must* you use C++? I'd really using consider another language if I were a novice to (graphics) programming, unless there are external constraints you must obey. Third, I'd recommend SFML.
I've removed this post; as /u/fafasdf noted, beginner questions are off-topic for our subreddit.
It only looks for names after if it can use ADL to look them up in the namespace. I think. I don't understand name lookup.
As linked to in the sidebar... Does anyone even bother reading the sidebar? People should be reminded to glance at it before posting.
&gt; I think I can best demonstrate my thinking through a real world example. &gt; &gt; A production codebase at my organization has a class called IntVect, &gt; which represents a set of coordinates in Cartesian space. It has a &gt; method called min(). &gt; &gt; #include &lt;algorithm&gt; &gt; &gt; struct IntVect &gt; { &gt; int vect[3]; &gt; &gt; // ... &gt; &gt; // Modifies this IntVect by taking component-wise min with IntVect argument. &gt; IntVect&amp; min(const IntVect&amp; p) &gt; { &gt; vect[0] = std::min(vect[0], p.vect[0]); &gt; vect[1] = std::min(vect[1], p.vect[1]); &gt; vect[2] = std::min(vect[2], p.vect[2]); &gt; return *this; &gt; } &gt; &gt; bool operator&lt;(const IntVect&amp; p) const; &gt; &gt; // ... &gt; }; &gt; &gt; Perhaps .min() is poorly named, but that matter was decided before my &gt; time. The codebase is 20 years old and the name would be very &gt; difficult to change now. Whether or not .min() is a bad name for this &gt; function is not particularly relevant, though. &gt; &gt; Suppose the uniform call syntax wording from P0301R0 (which is what we &gt; voted on at Jacksonville) was adopted: &gt; &gt; using namespace std; &gt; &gt; // ... &gt; &gt; int main() &gt; { &gt; IntVect A{42,17}, B{17,42}; &gt; &gt; // std::min() is called (which does not modify A), instead of IntVect::min() (which does modify A). &gt; IntVect C = min(A, B); &gt; } &gt; &gt; If my users get into the habit of using free function call syntax, &gt; they will run into the sort of issue demonstrated above. If I thought &gt; these were just corner cases, I would not be worried. However, I don't &gt; think that's the case. In this codebase, there are many methods which &gt; have names that other 3rd party libraries (or my users) might chose to &gt; define as unconstrained templates - abs(), norm(), shift(), mask(), &gt; sum(), invert(), convert() and rotate(), to name a few. A quick audit &gt; of my other codebases revealed the potential for similar issues. &gt; &gt; If uniform call syntax was adopted, me and my organization would have &gt; to do one of the two things Nevin suggested: &gt; &gt; * Rewrite the library using free function call syntax. &gt; * Tell our users to not use this feature. &gt; &gt; The first option is likely not practical for us, &gt; &gt; This is why I voted against the adoption of uniform call syntax; if &gt; adopted, I believe I would have to advise my organization and users to &gt; avoid using this feature. Here was the original mail with my position.
It does work with boost, boost might have it's own build system but that doesn't mean you have to use it. [See](https://github.com/mkn/org.boost) I'm not so sure CMake or conan have the capacity to support so many targets as easily as Maiken can. Please feel free to counter.
It is from mine.
Yep, we've all been there. My suggestion would be to design the general structure of your program on paper first before starting to code (this works well with all languages!). One more recommendation I would have is to learn the tooling around C++. Basic GNU Make/CMake. Provide a simple README which tells you how to compile and/or use. I am always more inclined to try something out if I type `make` and everything is done for me. Cheers!
We had some talk in classes about this, although it didnt interest me that much, I have visual studio (or the thing within it) that takes care of it, however, I'll investigate it more ... some day. :D
boost (or any library) can be used without requiring a dependency, for instance lib: boost-system Would link libboost-system.a/so on nix, or boost-system.dll on windows. This of course requires boost to be installed appropriately. [Example](https://github.com/Dekken/tcpflow/blob/master/mkn.yaml) I would hope that header generation would only be required once (perhaps not realistic), that's where mkn.sh/bat come into play, if a dependency is missing, the first time it is retrieved this script is run to put the project in the desired state. I appreciate your comments, it's giving me some things to ponder. 
Hah! Most of us just end up giving our code that name when we're frustrated enough, I expect. Never had a good acronym to justify it. 
I think the paragraph you quoted is for ADL and corss TUs issue, e.g. // fun.hpp namespace adl { struct A {}; } void f(...); template&lt;class T&gt; void fun(T i) { f(i); } // tu1.cpp #include "fun.hpp" void tu1() { fun(adl::A{}); } // tu2.cpp #include "fun.hpp" namespace adl { void f(A); } void tu2() { fun(adl::A{}); } The above is indeed UB. The original case should be fine, otherwise I don't see any reason for 2-phase lookup.
&gt; I'm not so sure CMake or conan have the capacity to support so many targets as easily as Maiken can. That statement alone belies a fantastic amount of naivete about what you're doing; looking at your docs strengthens that belief; trying it myself myself confirms it. * Maiken requires the most cutting-edge/up-to-date build tools. Right out of the gate, you have excluded an absolutely enormous percentage of the development community. * Your Windows setup instructions don't even work. Obviously you're not testing this rigorously on all your platforms. (You're missing a `SET INCLUDES=%INCLUDES% /I%PWD%\ext\sparsehash\%HASH_VER%` in your make.bat, by the way.) * There is no mention of cross-compilation. You've just excluded another enormous percentage of the community. I could go on, but honestly it's not worth the time. This looks like a reasonably well-done hobby project, but it will never take CMake's place. You are a single developer, and you obviously don't see how huge the problem space CMake fills even *is*. Why would anyone trust that you can provide something better? 
I still think the quote applies here.
Yeah I upgraded from [this](https://github.com/sparsehash/sparsehash) to [this](https://github.com/sparsehash/sparsehash-c11) and clearly forgot something. Thanks. 
So the CRT/STL does not rely on -fms-compatibility. Do you know if the Windows SDK needs it? I saw someone on the boost mailing list saying that with VS2015 Update 2, the flag is not needed even by the Windows SDK headers.
It's not uncommon for some segments of the industry to reimplement function*, so it's a good thing to understand. *to enforce an inline/embedded functor size and to disallow "oversized" functors with compile-time errors, completely eliminating any ability for function to allocate memory; important in games, embedded, and some other similar environments.
That book is if you already know the old C++98. So you need to say which version you are learning. Many courses are still doing C++98 then that book wouldn't be any good at all.
[Hugo](https://github.com/spf13/hugo). Syntax coloring with [highlight.js](https://github.com/isagalaev/highlight.js). Haven't used them, but by the looks of the src.
Could you expand on what "installed appropriately" means? To me it looks like it has to download and build boost locally for each project. Also, it looks like it's tied to github if it works just by saying `org.boost` and `master`. If `master` changes, what happens? Wouldn't it delete and rebuild again? What happens if there's no internet connection, how do dependencies work? The way I see it, all the mkn.yaml files look terse and readable. But without a scripting language, I don't know if it's appropriate for the general case. You can't assume all software builds in the same way. For me, a build tool should let the user decide as much as possible how to go about things. The tool itself should only abstract away the differences between platforms and not more. This is why I find a build tool based on an existing scripting language to be ideal.
Thanks for the explanation.
`std::function` came from boost. The boost implementation uses free functions with `void*` manipulation to avoid overhead introduced by subclassing on many compilers. [It's a worthwhile optimization because theoretically you could have a whole shit-ton of these things.](http://www.boost.org/doc/libs/1_59_0_b1/doc/html/function/misc.html#idp219479664)
"installed appropriately" depends on the system and compiler. Running "sudo apt-get install libboost-dev" would put things in the right place for debian as GCC would use the default library paths. Dependencies can be updated through the -u\/U options, if you only want to stick to releases/tags you can do that also to disallow modifications, or just don't update those projects. Dependencies can be configured locally in the expected directory structure, or overriden per dependency with a "local: &lt;directory&gt;" field. Expected directory structure by default on linux would be ~/.maiken/repo/$PROJECT/$BRANCH . Which would need to be built once, for all projects with that dependency. Unless you update it or want to change how it was built. Not all software does build the same way, project profiles allow arguments to be given under some conditions, if it's a binary project, library project, shared or static, is BSD/Linux/Windows. Each project should handle itself, but of course that's not always feasible. Each mkn build uses a global settings file, which by default on linux is ~/.maiken/settings.yaml, and can be overriden per command with the -x argument. There you set what compiler you want. I have given [examples](https://github.com/Dekken/maiken/wiki) settings files for windows as it's not as straight forward as other systems, but I hope it is clear how it operates. The "-x" option lets me easily use many cross compilers on one system. Example cross compiler settings.yaml [MSVC on Debian with WINE](http://pastebin.com/eQFiCysk) [ARMv7-a on Debian](http://pastebin.com/LqieAnCK) [NetBSD on Debian](http://pastebin.com/NgxydL4J) Edit: Forgot to mention, dependencies can have an "scm" field which specifies where to find it. If it's missing the name is used to check against the configured list of URL prefixes, which in the default case is a single entry of "http://github.com/mkn/". URL prefixes can be overriden when building mkn, or in the settings.yaml, also if the scm tag is found but incomplete (missing "://" or "@") , it is used against the URL prefix list instead of the name.
Thanks.
&gt; If the function name is an unqualified-id and the call would be ill-formed or would find a better match had the lookup within the associated namespaces ... The set of namespaces associated with "float" is empty - builtin types have no associated namespaces - so the "lookup within the associated namespaces" finds no declarations of "f."
`std::function` also has the problem where its `operator()` is `const`, which violates the thread safety assumptions of the standard library.
Ok I agree with you. It's a bit crazy though that changing `float` to a class causes undefined behaviour.
Well... no, not really. Unless you're targeting some weird compiler. But if you're after MSVC, GCC, Clang, Intel, you should be generally OK. Besides, portability is only really relevant if you are writing software that's actually intended to be portable, like a commercial app that's going to be sold on multiple OSs.
Why not? Care to back up your argument? Because from where I'm sitting, both C# and C++ have member functions, and C# has a validated 'extension method' construct that is, quite frankly, awesome. It's not just awesome for extending user-defined types. It also allows pseudo-DSL applications such as the Maybe monad and others. Really awesome stuff. So I'm not sure what the argument is against having something similar in C++. For all intents and purposes, the fact that C++ has global functions makes things even easier.
Vital code from 1994?
A lot of the code is not getting wrapped. So I saw e.g. long if statements that were going past the border and into the background, being completely unreadable.
Everything about this is nonsense and should just be ignored.
&gt; Is compilers like guitars - they no longer makes them like they used to? No. There is nothing worth finding in older compilers other than the excitement of finding all the peculiar and horrible ways in which they deviate from the respective language standards. Some older compilers may be faster in some circumstances, but that's because they don't do half the optimizations we do nowadays or because they support only half as many features, that's about it.
You're mentioning a lot of non-obvious terms and conventions. Where are the docs at? In one or two sentences, what problem is maiken trying solve that other tools don't? Why did maiken have to diverge from CMake instead of working along with it? I'm really not seeing the big picture here. CMake documentation is terrible as it is. I can't seem to find anything on maiken... At this point, I would have to say that maiken has been over-engineered. It's way too big of a program for a problem that has already been solved. For me, CMake's weakness is documentation and the language. Neither of these excel for maiken. I hope you understand what I mean by that.
It has no advantages and is pure disadvantage. Wanting to use it is insane.
You should do some basic studying of embedded system programming and then attempt to do some hard research into the specific hardware you are dealing with. Most hardware systems with low level code have some pretty specific methods for input and output between the hardware and software... so, most likely whoever wrote the code was following some pretty defined standards created by whoever made the hardware. This stuff isn't rocket science if you know how to read the technical data sheets, but if you don't, and if you don't have a good understanding of programming at a low level, you have a couple options.... Learn how, or hire someone who knows how. Typically people who do what you are talking about are computer engineers or computer scientists. Of course, take my advice for what it is : the ideas of someone who was given very little information about your problem, and thus I may have bad answers for your specific situation. 
I assume you have access to the source code and you're not trying to reverse engineer this thing? Are you using an IDE? If working in Linux, are you comfortable with grep? If it were me I'd just start by looking at function names. First for anything that contains both "get" and "gps" (or "position", "pos", "location", "loc", "coordinates", "latitude", "longitude", etc..), and then just any one of those by themselves. What libraries are being used? What header files are included? Edit: Completely missed the fact that you're looking for time and not location, oops. Try just searching for "time" or "clock" instead of the other stuff.
Maybe, my questions are really way too vague to direct to anyone in particular at this point. I edited my post to add some new information. Take a look at that, and thanks for offering to help. 
embedded system programming? I know I can look that up, but in your words, what does that mean? 
Are you talking about Borland C++ 5.5 or something earlier? I used Borland C++ 5.5 as part of the Borland C++ Builder 5 Professional package. With just C++ and the included libraries it was okay. I found that trying to get it to work with other available libraries was a pain because the linker used a different format than Visual Studio and mingw. Compiling your own libs was, at times, an option but that was a bit of a pain too. This was back in the early 2000's though. I can't imagine trying to get it working with anything more modern. Really, if you want good compilers then GCC and Visual Studio are the way to go. Visual Studio 2015 Update 2 claims to be very standards compliant and GCC is very much a standard when it comes to open source software. If you like editors then Code Blocks is good, I've heard, and Visual Studio has all the bells and whistles a professional programmer could ask for. 
Kind of. Fairly. It's pretty important I at least make SOME progress. 
&gt; Borland c++ &gt;Recently there has been a surge in popularity of this compiler. [citation needed]
Yeah, it just searches for strings. For example, try: `grep -nr "time" /path/to/top_level_directory` -n means that it will print the line number before printing the file and line of each match -r means recursive (it will search all of the files in the target directory and subdirectories) And don't forget to replace "/path/to/top_level_directory" with the real path ;)
I'll look into that, thanks. 
Sure. I'm betting this is probably sensitive company material, but on the off chance that it's not you could throw it up into Github and I'd be happy to take a peek.
A few suggestions: 1) Run the code in a debugger. Find the entry point and start stepping through it line-by-line. Get an idea of how it flows and where it goes. I recommend doing breadth first, rather than depth first. That means use "step over" more often than "step into". 2) Log to file. Add lots of logs. Scatter them everywhere. Log at the start and end of every function, with the name of that function. Log inside functions when interesting things happen. Inspect your logs to get an idea of the program flow and what it does. 3) This really depends on what you're working with, but there is often documentation and samples which come with the device you're interfacing with that is provided by the manufacturer. Reading the docs and learning how the samples work can be very helpful for finding where the same things happen in the actual code.
Yeah half of it is open-source and half of it is proprietary, so I can't really be sharing it. Thanks though. 
You are giving off the impression that you aren't a programmer, or at the very least, don't understand C/C++/Java like languages, which every programmer does.
This is blogspam, copied from http://www.codeguru.com/columns/experts/load-testing-sla-first.html. Please do not submit garbage to /r/cpp again.
C# is an object-oriented language. Very, very object-oriented. C++ is not. 
I think the interface for `at()` is clear and unambiguous. The responsibility should lie with the caller to make sure implicit type conversions for the argument is actually safe. The `at()` function only checks whether the index with the expected type (`size_type`) is not out of bounds. It shouldn't care about type conversions and type safety.
Personally, I value consistency (or at least minimizing surprising behavior) when dealing with APIs. Indices are basic and prevalent in the C++ code. Using an API that has atypical validation - even if its theoretically better - means the user may hit edge conditions they wouldn't hit otherwise. To me, you are attempting to work around a fundamental part of the language. Why bother doing that - adding complexity, potentially surprising behavior, etc - without a really good reason? What if a user has an class that models an index (and implicitly converts to size_t)? Or what if someone is using a smaller time such as a uint8_t as an index because thats the starting type they have? Sure, they could explicitly convert them to a size_t but what is gained by requiring it that they programmers aren't already accustom to dealing with all over the place?
Compilers don't warn about any of those things. It may warn about the very obvious at(-1) because the -1 is known at compile time but they don't warn about at(i) where i is an int or other signed integral type. As I see it, if you are using at() instead of operator[], you are being explicit that you want overflow safety so I see the second option as the best one. All the guarantees without the run-time overhead. You still would have to static_cast&lt;size_t&gt;() but, as you say, C++ programmer should be used to manage size_t frequently. In my opinion everything would be better using signed int but that's another story.
Compilers are great at warning. The problem is that programmers are great at *ignoring* the warnings.
The most obvious thing is to decouple loading the ROM image from constructing the `Emulator` object. This might be a good idea anyway, since successfully loading the ROM isn't apparently a precondition for getting a usable `Emulator`. Barring that, I'd use the dummy file. (Aside: shouldn't this constructor take `pathToRom` by const reference?)
This seems like massive overkill to me. In fact some suggest not even using `at` in the first place: use `operator[]`, and your program logic should ensure that the index is in range (and the right type!!) . IOW this is the wrong place to be catching design errors. If you managed to write `at` with a `double` index you went wrong much before this.
Ah, thanks! I derp'd. Not sure why I didn't think to create another constructor. TIL about regular functional abstraction.
I don't think there's anything wrong with using data files as part of testing. In the long run it makes the most sense to store test data in files instead of code - collected in a 'testdata' directory, say. And later if you encounter bugs when loading certain files you can copy them there to catch regressions, etc. I can see where you're coming from not wanting to create a 'dummy file' but the alternative looks to me like a lot of unnecessary code that might actually introduce more bugs than it potentially prevents.
I'm guessing he deleted it because all the comments were along the lines of "wth is this crap?"
Well, IMO, the only difference between `at` and `operator[]` is that the former is beholden to _always_ throw on OOB. The latter should still check for OOB, just with `assert` or `Expects` or something.
Do you know any big projects using c++builder?
I'm not sure I followed any of your points. Can you elaborate what you're referring to?
I had assumed as much. Every few months, there is some poor student over in /r/cpp_questions that has to deal with a Borland compiler.
In release mode it shouldn't , it's an unnecessarily slowdown. You should use your program logic to make sure indices are in range, not guess and rely on the vector to catch it.
Why not just turn on warnings? All of the main ones should support warning on all the cases you are worried about &amp; you can treat them as errors too.
Err, I don't see how `std::min` could be called on an `IntVect` if there's no such overload. But the point you are highlighting - the one where 'extension' names can clash with member names - has already been fixed in C#. The rule there is simple: if you make an extension method for something an object already has (e.g., `ToString`), it doesn't count. As a result, neither should you need to rewrite the library nor tell the users to avoid the feature because, even if you had a global `min()` that also took two `IntVect&amp;` objects, the member function would still be called. Unless I'm missing something.
What I find more amazing is that the language allows for operator `[]`in arrays (and indeed vectors) to take _signed_ values. I'm guessing this is legacy, but that doesn't make it any less wrong.
I've got 'grep -rIn' in my muscle memory. The 'I' excludes binary files. Some old codebases have binary garbage in them. grep -rIn "time" /path/to/top_level_directory Also, be sure to have everything in GIT. Then your changes are seen clearly with 'git diff' and you can save them on the temporary branches. By the way, search by the file name for myfile.cpp is `find . -name myfile*` or `find . -name \*file.cpp`
This doesn't sound like a well-written C++ codebase. It sounds like an awful, old, legacy, C-style "C++" codebase. Or one where OOP has been terribly overused and not well-structured. You're deep in the shit. Sounds definitely too much for a beginner. Some people recommended grep - I would not recommend that to somebody like you from what I've read, except you're really a linux shell wizard already. But you didn't even know grep, so you definitely are not. So. Use an IDE. Use a good IDE. One you like. Try QtCreator first. It's one of the best on linux. CLion also, but I think it's not free (the EAP might be). An IDE will help you **immensely** navigating the code. (If you can run the software on Windows (it sounds like you cannot), use Visual Studio 2015. Period.) Debugging. Debug the code. Don't try to understand the flow and everything from just reading the code. Run it. Live-debug it in a debugger. The IDE will help you massively with that. You can set breakpoints, step through the code, watch variables in each step.
Regarding the need for `__declspec(dllimport)` etc. with clang/c2. In other recent news, you now have the ability to run Linux ELF binaries on Windows. This implies that the Windows kernel now has an actual ELF interpreter. Are there any plans to allow VC++ (either vanilla or with clang++) to emit ELF objects instead of PE-COFF? Being able to use ELF natively would allow for sanely using arbitrary templates in dynamic libraries, which would be a major win. It's the single biggest pain point when porting code to run on Windows. I know it would break the ABI, but it breaks with every VS version anyway, so I'd be happy to pay that cost.
&gt; I don't believe a data structure can exist whose index exceeds size_type. And again, compilers will emit diagnostics here. "The type size_t is an implementation-defined unsigned integer type that is large enough to contain the size in bytes of any object." So you're correct for things like arrays and vectors, but a data structure that holds its data in multiple objects (like a deque) should be able to have indexes that are bigger than size_t, from what the standard says ... but I don't believe that's the case in any implementation.
A mocking library makes it easy to test side effects. Testing only functions without side effects could leave tons of stuff untested.
Making the constructor load the rom add too much responsibility to it Additional good point to dissociate the loading is that for testing purpose you can mock your loadRom method
Really doesn't answer my main question. Why does maiken exist? What problem us being solved that wasn't already solved? Throughout this entire line of comments I've had multiple questions and most of them are still unanswered.
For the simplest example, consider `move_function`. Currently, it is *impossible* to pass a function-like object without a copy constructor.
The Windows SDK is huge so making a general statement is not possible. But I had success compiling stuff (mostly D3D9/D3D11/MediaFoundation) with Clang on Windows using `-fno-ms-compatibility -fno-delayed-template-parsing`. It did require manually declaring `IUnknown` (I included a file with a declaration via `-include Compat.h`).
That's why you build with `-Werror`.
It's actually quite the opposite, the fact that `operator[]` takes unsigned is the historical quirk that is now decried by nearly every member of the standards committee.
You don't need to go to India to see schools using ancient compilers. It happens every day in the US and Europe. In a lot of institutions and lectures.
5 dollars might be the monthly salary of a person working in a rural area in India. The 386 is already there, so is the keyboard. Costs 0$. I'm not saying using 386's is good or whatever, I also have no idea how poor people are in some regions of India, I'm just saying, you could so some critical thinking as well. It's very well possible that I'm not too far off.
How do you have more objects in your data structure than there are possible bytes of memory in the implementation?
The problem with `= delete` is i) it explodes the code size (imagine a similar function taking two such arguments for example), and ii) it's complicated to use to disable integer overloads because you don't know where exactly `size_type` sits in the list `unsigned`, `unsigned long`, `unsigned long long`. The error message is better, I agree, but a good `static_assert` is almost as good.
Hey, I didn't know that they controlled two-phase with a separate option. I'll add that to the STL's tests.
If `float` was a class type in the objective program, it would not cause undefined behavior. The global namespace either *is* associated with that class type, in which case ADL would find the overload of "f" in question, or the global namespace *is not* associated with that class type and ADL would not find the "late" overload of "f." You can't get UB from that rule in a program with a single translation unit. UB only occurs when the best match is declared in an associated namespace in some other TU but not in the TU where overload resolution occurs.
-Weverything is not intended to be used for anything other than testing and discovering potentially interesting warnings to opt-in to.
I wrote a utility that converts a binary file into a C literal array. Because I don't want a giant array cluttering my unit test file I usually #include a cpp file in my unit test that contains the literal array. Most unit test frameworks support a function that is run before a test and run after. In the before function I write the array to disk, in the after I unlink() it. 
Given: int x[2][2]; there's only one dereference when you do x[1][1] as the multidimensional array is a single memory block, not an array of pointers to memory blocks.
Why not just use an input stream object as constructor (and loadROM) parameter. This way you can just open file and pass it to constructor, and use stringstream for tests. No need for seperate RomLoader or anything. 
https://www.reddit.com/r/cpp/comments/4dyjj6/how_i_learned_a_vital_borland_c_coding_technique/ but I wouldn't say there's any _discussion_ per se.
Thank you for the enlightening answer however I am searching for the 3.1 and 4 version of the Borland compiler. Do you know where I can get those and if libraries produced are compatible between versions?
Version 3.1 and 4 is of special interest as I want to know what the hype is about. How can it be preferable to msvc 2015?
From fast glance at code: * Game class should be in separate header/source files * Invoking `Draw_Table` which invokes `Ask_Input` which then invokes `Draw_Table` which ... is very bad design. First - it's not descriptive, since name of function suggest that it will only draw table, but it instead loops until game is over. Second argument is that if you'd use stupid compiler, it might not perform tail recursion optimization and leave old data on stack which would quickly become overflowed. It's muuuuch better to write another method which would look more or less like (1) * You should minimize global state (variables etc.). If you don't have object which should have those data, create new class for object that will manage them (not always good approach, but leaving data dangling free is usually bad option too). (1): Game::RunGame() { InitializeState(); while(!isEnd()) { AskInput(); UpdateState(); // decide if game has ended DrawTable(); } DrawStats(); } There probably is more to fix, but for now you can fix what i mentioned and take a look at some existing small projects to see how others design applications/games/etc.
 Is it possible to use clang with clang codegen instead of MS? Mostly wondering because I'm been testing vs2015's auto vectorization.. and well.. it rarely works to say the least. Can't auto vectorize all sorts of code that it should be able to. Clangs support is likely better here, and if nothing else we can help to improve clang codegen with regards to auto vectorization, where as with MS, we are dependent on MS actually caring and devoting resources as such. 
You do realize you're talking about comparing software from ~1992 to that from ~2015, right? There is no 'hype' – no one is saying the former is better than **any** modern alternative, quite the opposite.
I'd not recommend anything other than Digital Mars' compilers as far as DOS work goes.
&gt; Automatic updates when the user enters a value _really_ is a vital feature. Yes, but that's not really what this is about at all.
&gt; don't understand C/C++/Java like languages, which every programmer does You'll find _lots_ of people in the ML/Haskell communities who develop software for a living who wouldn't know the difference between C, C++, and JavaScript if you gave them an hour. I love C++, but the world is a little bigger than this... ;-]
Because I've read the source for both and found DM's code much more sane than the other. If I had a problem, I'm confident I could debug it myself, unlike DJGPP's rather creative use of the preprocessor. Plus, I'm confident just pinging /u/WalterBright would yield results if I were having a real crisis, and that's worth something too. ;-] EDIT: If I'm having to work with DOS, I'm already having a terrible day, so I deviate from my usual priorities a bit...
I wish they would fix all of the inconsistencies in the ninja generator, as ninja seems quite speedy when paired with CMake. No color console output, no Fortran support, etc...
I know this isn't always possible for legacy code bases, but `-Wconversion` is your friend in this situation.
All I can say is: cool
Digital Mars' C++ compiler is C++98 compliant [except for export :-) ], but you wouldn't want to use RTTI or exception handling for DOS programs.
GCC and Visual Studio are moving towards C++11,14, and 17 compliance and are mostly compatible with C++11 and 14 already. 
I would suggest breaking up larger functions into smaller ones. For example if you have a block of code and felt the need to comment on what it's doing, that might be better as a function with an appropriate name. Also many of those for loops I would put into their own little functions. Makes the code much more readable and also reusable. There's a concept called "Single Responsibility" basically each function should do just one thing. That kind of improvement is part of a programming concept called 'Refactoring' which is worth learning about. It would also be handy to have some simple unittests. Verify that a winning solution is counted and such. Testing is vital for real programs so it's work getting into the habit. Even for small personal programs its quite useful often its faster than modifying some code then having to play the game a while to see that it worked. If there is a problem it's quicker to locate the source since you can test specific self contained functions. And when refactoring the code you know you haven't broken anything.
I know :-)
Not C++ related in any way.
"C++" is in the second sentence of the article.
That's not true. The absense of an explicit limit does not guarantee no limit. See Plorkyeran's reply, there really are (or rather, were) cases where the address space is bigger than size_t.
But why? The common-sense approach to an array, even if you treat this as a pointer offset, is to have strictly positive offsets. I understand that it's possible to point into the middle of an array and take `x[-1]` from it, but surely there's very few people who actually do that? Also, I hate to point this out, but other programming languages do not descend into pointer arithmetic and still manage to let people use arrays efficiently.
I much prefer to enable Weverything and then whitelist warning categories rather than the other way around that GCC forces you to.
This very much so! Had the STL been created created today it would use signed integers instead. The Google style guide has a good motivation and something Bjarne, Herb, Scott, Chandler all agree with!
In the example given by you, wouldn't AskInput(); and DrawTable(); be called infinetly even the user doesn't input anything? Or does the while loop wait until the "cin" has been set in the AskInput?
&gt; No you should not avoid them at all costs If I was asked this question in an interview, and it was phrased this way, I might be inclined to avoid the company at all costs...
If you are a library writer, you need to use header guards in case I need to completely replace one of your headers without altering the source package. This is not common, but sometimes it's necessary and pragma once makes me do it by messing with include path order, which is less clear to the programmer looking at source code. 
I've been programming in C++ for almost 20 years now, on large projects, with something like 20million lines of code. I have never once, ever, had a problem caused by include guards. All of the problems people have with include guards don't occur with enough frequency to bother changing. 
I had an interview like that during the last downturn (post 01), first they didn't take me and then 3 weeks later they decided to call me and offer me a lower paying job than what I had gotten. Good riddance to them.
\*_sigh_\* As expected, `testCppIO` does it completely wrong.
auto, variadic templates, chrono and other new features... ... and typedef ... Why people don't use "using"? using FuncMap = std::map&lt;std::string, std::function&lt;void (const char*, const char*, std::vector&lt;char&gt;&amp;)&gt;&gt;; Never liked typedef. It's syntax is so confusing to me.
The moment I found out about using, I dropped typedef like a hot potato.
Compatibility with older compilers. :-(
Well now you know :) Never use `typedef` if the compiler supports `using` please
My advice is to try to do it, and if you can't do it, break it down into smaller parts that you can. If you still can't do it, you are mistaken and need to think it through clearly. The problem is pretty straight forward, so you should google the keywords. You aren't going to learn by having someone "help" you with this. This is also not the correct subreddit for this, but don't look for any other one, just do it yourself.
`typedef` has exactly the same syntax as variable declarations, but with the word `typedef` on the front.
The committee has nothing to do with what you talk about earlier in your post. The commitee doesn't specify any particular library implementation, nor even that compiler and library be separate. In another post you apparently blame the committee for glibc which is completely unrelated. Performance is 100% at the implementation vendor's discretion. Why don't you go write your own C++ implementation that does it better than gcc and clang, and show all those idiots how dumb they are being. 
This is the top-level comment for **meta** discussion. Reply here if you have questions or concerns about this post. Example question: Should I enable "contest mode", which randomizes top-level comments and collapses their children? I'm not sure if the randomization would be beneficial or annoying.
If you think things like lambdas, auto, and the standard library - features that make C++ actually pleasant to use compared to things like C, are stupid, no, you're not on my side.
I love Catch! Header only, cross platform, single header, includes BDD style testcases. https://github.com/philsquared/Catch
There are some actual problems with how iostreams are defined. The C++ committee *mostly* acts as a clearing house though. That is, somebody implements something, writes a proposal to the committee, *then* the committee looks it over and decides whether they can approve it. But--if somebody doesn't write up that proposal, and have at least one implementation for them to look at, their hands are pretty much tied. The committee's charter is to codify existing practice. They've departed from that a few times, but only a few (and a large percentage of the times they have, they've regretted it, such as with `export`).
*Catch* - Header-only. - Supports unit-tests with bdd style (in this style you do not need fixtures anymore). - Easy to get started. - Does not support mock objects. - One single REQUIRE macro for almost any comparison. Easy to use. *Boost.Test* - Supports data driven tests since 1.60 (if I do not recall incorrectly). Very useful for random testing generating data. - Since 1.60, supports single BOOST_CHECK macro, same as CATCH. - No support for mock objects, though you can use boost.turtle. I used it together with boost/catch and it did a good job. - Supports header-only and library variants. You should use library variant in general. *Google Test* - Supports death tests (tests that would break the machine, such as segfaults). - Comes with google mock. You have mocking out of the box. - Does not support C++11 move semantics. I recall this was annoying at some point for my testing. - Must embed in your project as a source to compile with the rest of your code. - Many different macros. You have to remember a bit more than with Boost.Test and Catch in this area. Not a problem, though. All 3 frameworks support automated test registration. I would recommend any, but if you want a full solution and do not need random tests, I would go for google test. If data-driven testing is useful to you, you can use Boost.Test + Boost.Turtle. Catch is more beautiful, but also more lightweight, consider it if you just want to use a bunch of unit tests. It does the job well. I even integrated it with Boost.Turtle at some point, but was a bit of a pain. 
+1 for catch
Tested it with VS2015 x64 update 2. The results are not what I was expecting: Average c I/O took: 104.94ms Average posix I/O took: 103.82ms Average c++ I/O took: 368.99ms Average c++2 I/O took: 397.86ms The `std::filebuf` version is actually slower. 
Your post has been automatically removed because it appears to be help/homework related. If this has been in error please message the moderators. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
"C++" is only even mentioned in the second sentence of the article.
&gt;I spend half my day pouring over this to implement by hand features that the compiler does not offer You're describing a shortcoming in the compiler, not the committee. Any compiler vendor is free to implement whatever feature. You are free to make your own compiler (or a mod for an open source compiler such as clang) to do what you want. 
Yep, +1 for Catch. It's great!
* https://github.com/cpputest/cpputest * https://github.com/unittest-cpp/unittest-cpp
Such a thing will happen if someone actually implements what you are talking about. And then it may or may not be a fork. If someone implements this and everyone agrees it is good then it may become part of the standard. Most likely as something like the IEC60559 annex. The standard is for standardizing existing practice, not for being prescriptive. 
**Company**: [Grafitroniks](http://www.grafitroniks.fr/) **Type**: full time **Description**: We develop, market and sell both turn key software and solutions for industrial cutting and routing applications. Because of our small team size we are looking for developers who are able to take advantage of the flexibility we offer but can work independently. Despite being a French company we would encourage candidates who do not speak French but only English to apply as well. **Location**: Paris, France **Remote**: For the right candidate but we'd prefer someone who is local. Many of our projects entail interfacing with industrial cutting machines, something that can't be done remotely. **Visa sponsorship**: We are open to discuss this for the right candidate but we would prefer someone who is qualified to work in the European Union **Technologies**: Candidates can expect to own a particular project and are given the flexibility to pick the tools and technologies that they think will make them most productive. We are currently using Mac/Windows/Linux, some CGAL, some boost and mostly C++11. **Contact**: Just send us a reddit PM and we'll put you in contact with the right people.
Boost.Iostreams to the rescue! #include &lt;iostream&gt; #include &lt;boost/iostreams/copy.hpp&gt; #include &lt;boost/iostreams/device/mapped_file.hpp&gt; #include &lt;boost/iostreams/device/file.hpp&gt; int main(int , char*[]) { using namespace boost::iostreams; std::ios_base::sync_with_stdio(false); std::cin.tie(nullptr); copy(mapped_file_source{ "data.dat" }, file_sink{ "out_data.dat"}); return 0; } Should be faster than std::iostream. Can be tuned further by specifying an optimal buffer size for boost::iostreams::copy. Edit: benchmarks. 100 iterations, 200 MB random data: Clang with libc++: Average c I/O took: 150.07ms Average posix I/O took: 150.69ms Average c++ I/O took: 626.88ms Average c++boost I/O took: 161.96ms Clang with stdc++: Average c I/O took: 153.71ms Average posix I/O took: 149.79ms Average c++ I/O took: 154.22ms Average c++boost I/O took: 124.48ms GCC: Average c I/O took: 154.29ms Average posix I/O took: 152.29ms Average c++ I/O took: 155.54ms Average c++boost I/O took: 124.29ms Like I said, there's room for improvement. I simply benchmarked void testBoostIO(const char* inFile, const char* outFile, std::vector&lt;char&gt;&amp;) { using namespace boost::iostreams; copy(mapped_file_source{ inFile }, file_sink{ outFile }); } 
About mocking - do you know [FakeIt](https://github.com/eranpeer/FakeIt)? I've been using it with Catch for a while now and, as far as I can tell it has integration with Catch, Boost.Test, and gtest. How would you compare it with Boost.Turtle and gmock? Which one's easier to use?
&gt; So if I am a voice for change and I am followed by enough people, perhaps your statement falls out of context by democratic vote. I'm looking forward to that day. Those you represent may want to elect a new spokesperson, because you're fucking irritating. 
`using` has the same syntax as a variable definition, but with `using` instead of `auto`; also, typedef is harder to read, as the type is at the end.
Should've also profiled native calls to CreateFile() etc.
Article on this site is STOLEN from the original site I added a few minutes earlier: https://www.reddit.com/r/cpp/comments/4e9kf1/logical_expressions_in_cc_mistakes_made_by/?ref=share&amp;ref_source=link/.
**Company:** [Rohill](http://www.rohill.nl/company/careers/66-software-engineer). **Type:** Full time **Description:** We maintain a softswitch for Mission Critical Communications based on [TETRA](https://en.wikipedia.org/wiki/Terrestrial_Trunked_Radio). We are looking for developers with proven experience, but might look behind that if you are a fast learner. **Location:** Hoogeveen, in the north of The Netherlands. **Remote:** Preferably not. **Visa Sponsorship:** I'm not sure, but we do have experience with people from abroad. **Technologies:** New code is written in C++11/14. We do use boost libraries quite a lot. **Contact:** E-mail is listed here: http://www.rohill.nl/company/careers/66-software-engineer
Interesting. This seems easy to check for and could be added to compilers to produce more warnings. I just tested GCC 4.8.4 and the Rust compiler in version 1.9.0-nightly (e1195c24b 2016-03-31). Both do *not* produce any warnings. But in both cases the optimizer is smart enough to figure it out: bool test1(int x) { return x != 5 || x != 6; } // always true bool test2(int x) { return x == 5 &amp;&amp; x == 6; } // never true bool test3(int x) { return x == 5 || x != 6; } // x==5 is redundant bool test4(int x) { return x == 5 &amp;&amp; x != 6; } // x!=6 is redundant Looking at the assembly code, `test1` directly returns `true`, `test2` returns `false`, `test3` only checks `x!=6` and `test4` only checks `x==5`. So, these rules already exist presumably in the backend.
&gt;using has the same syntax as a variable definition, but with using instead of auto; Not true, e.g. `using T = int;` is valid but `auto T = int;` is not &gt; typedef is harder to read, as the type is at the end. The type may be in the middle, e.g. `typedef int X[5];` 
It seems `libc++` is also slow (~4x). Only `libstdc++` has a fast I/O. Why is this so? Different defaults? Missing *features*? 
Why? using Func = void(int, const int&amp;); vs typedef void Func(int, const int&amp;); *using* separates type name it's easier to read. Especially with function type declarations.
Why are people using `std::map`? It is for **sorted** keys, but you **almost always** just want to map from key to value, there is `std::unordered_map` a.k.a. hash map for you.
It wouldn't be annoying, but there's probably not enough competition within here for it to be necessary.
The fact that you can't see how your previous 10 posts might illicit such a response is why you're beyond hope. Now stop wasting precious nanoseconds arguing with me.
Never liked VC for it's error messages Clang main.cpp:86:15: error: missing 'typename' prior to dependent type name 'T::typeName' using Type = T::typeName; ^~~~~~~~~~~ typename G++ main.cpp:86:15: error: need 'typename' before 'T::typeName' because 'T' is a dependent scope using Type = T::typeName; ^
&gt; Because I am telling you guys how the world works I rest my case, ladies and gentlemen.
Yeah, I didn't mean to advocate for the use of typedef, just wanted to chip in why I sometimes use it. Besides, my Visual Studio isn't up to date so it may very well be fixed in the current version.
Most don't care about the difference, prefer the one with the shorter type name, or looked up something like "C++ map" at some point, ran into std::map, and called it a day. It's not that hard to figure out, really.
I know. But it is too magic. Not worth the trouble it could generate in some contexts.
I wanted to know on what basis you are saying using boost::asio might result in 100-200x slowdown. Have you benchmarked it against something? It would be nice if you could share the results.
So US based remote wouldn't be an option?
It looks like this is the case. Running on Darwin 13 after compiling with `clang++ -O3 -stdlib=libc++` (100 iterations on a ~44M file) gives: Average c I/O took: 502.83ms Average posix I/O took: 529.23ms Average c++ I/O took: 508.58ms I'm not sure the posix result being slower means anything, since my system wasn't particularly quiet while running this. 
Boost.Test support is coming in ReSharper 2016.1, which should be out this week.
Why you made me watch this ?? I blame you for making my youtube account look dirty. Edit: on the other side. That guy have to be kidding, no one can ruin someone name like that. Barney STAR-soup ? Yeah, this is a joke video.
I've watch a few of his videos. I can only say ignorance is bliss. It hurts my brain when he goes on rants about how bad C++ is when he just doesn't understand it.
I think the real problem here isn't POSIX (or the kernel as you stated in other comments) but your use case. You seem to work on a realtime application where every nano second counts (and you probably even want predictable/guaranteed timing). From what you've described I guess HFT? Normal OS are just not designed for such applications so you can't blame them if they don't perform well. If you need this kind of performance and predictability you should use a realtime OS, no OS at all or even FPGAs/ASICs.
Jane Street would like to have a word with you, Mr. "HFT Pro".
the guy that uploaded the video (and made the stupid title) is not the person talking
I know projects with 20 files and 5000 lines of code monoliths in each file...
Yes, but many _actual_ HFT professionals _do_. (Are you really making me point out the obvious here?)
Like I said, there are places we can improve perf here. At least in the binary I/O case anyway. I don't believe libstdc++ has the lifetime management issues we have (is unloading a library a common thing to do in Unix land?) but I could be totally mistaken.
Number of files could mean nothing. Looking at the [OTL](http://otl.sourceforge.net/), it's just one file. But it's 32.321 lines, about 1MB in size and a total nightmare (IMHO). If you are looking for smaller projects, you can have a look at some of my repositories at [https://github.com/taocpp/](https://github.com/taocpp/). Not sure if anything there fits your needs. Some like taocpp/operators are basically just one file + tests, others are a few files but not too much code as I like to keep things brief. If your metrics turn up anything interesting, I'd be interested to hear about it :) Note that some stuff there is still in development, so it might change anytime. Good luck.
So guy who is talking is just ignorant and the guy who upload the video is mocking him. Good to know.
Clang/LLVM has a warning called -Wtautological-compare which I thought was designed to catch issues like this but it apparently doesn't. The warning does catch a trivial tautological compare, however: see [your sample code](https://godbolt.org/g/N1zFbq) with Clang 3.8.0.
It is sad to see someone who knows a lot about one very specific thing and thinks that it means he knows everything about everything.
True.
QTcreator is, as far as I can tell, not free. Do you have any other suggestions?
 `-Wlogical-op` in gcc 4.9+ correctly warns for `test1` and `test2`. I'm filing a feature request for both clang and gcc to warn in the other cases. EDIT: done. [clang tracker](https://llvm.org/bugs/show_bug.cgi?id=22901) | [gcc tracker](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=70631).
Beginner/homework questions are off-topic for this subreddit. Please go to /r/cpp_questions as the sidebar advises.
I've tested this Win32 API version: void testWin32IO(const char* inFile, const char* outFile, std::vector&lt;char&gt;&amp; inBuffer) { auto in = ::CreateFile(inFile, GENERIC_READ, FILE_SHARE_READ, nullptr, OPEN_EXISTING, FILE_ATTRIBUTE_NORMAL, nullptr); if (in == INVALID_HANDLE_VALUE) { std::cout &lt;&lt; "Can't open input file: " &lt;&lt; inFile &lt;&lt; std::endl; return; } auto out = ::CreateFile(outFile, GENERIC_WRITE, FILE_SHARE_WRITE, nullptr, CREATE_ALWAYS, FILE_ATTRIBUTE_NORMAL, nullptr); if (out == INVALID_HANDLE_VALUE) { std::cout &lt;&lt; "Can't open output file: " &lt;&lt; outFile &lt;&lt; std::endl; return; } size_t inFileSize = ::GetFileSize(in, nullptr); for (size_t bytesLeft = inFileSize, chunk = inBuffer.size(); bytesLeft &gt; 0; bytesLeft -= chunk) { if (bytesLeft &lt; chunk) { chunk = bytesLeft; } unsigned long actualBytes = 0; ::ReadFile(in, &amp;inBuffer[0], chunk, &amp;actualBytes, nullptr); actualBytes = 0; ::WriteFile(out, &amp;inBuffer[0], chunk, &amp;actualBytes, nullptr); } ::CloseHandle(out); ::CloseHandle(in); } Built it with Visual Studio 2015 x64 Update 2. Results were: Average c I/O took: 102.03ms Average posix I/O took: 102.1ms Average c++ I/O took: 360.71ms Average win32 I/O took: 102.99ms
Thanks for the info. Nice!
For work I was handed a legacy fortran code that had 20,000 lines in a single file. That project gave meaning to the quote &gt;Always code as if the person who ends up maintaining your code is a violent psychopath who knows where you live. I've never personally met the author, but his name was all over it, and now I'm pretty sure I would have to restrain myself if we ever meet.
This is only his second semester teaching, so even with extra time after class it's a little challenging. 
Note: you can get rid of `dismiss` entirely with C++17 `uncaught_exceptions` (note the `s` at the end). libstdc++ and libc++ already support that.
That is insufficient to fill mt19937's enormous state (although it's obviously more than the single-seed-value constructor, and mt19937's constructor "fills in the blanks" so the state is initialized to something reasonable). If you really want to fill in the whole state with random_device entropy, you need to construct a seed_seq as big as mt19937's state_size.
VS 2015 RTM also supported uncaught_exceptions().
&gt; That is insufficient to fill mt19937's enormous state That's not the intent. The idea is to balance between using enough random bits to ensure bias is undetectable and being as convenient as possible given the existing API. The most convenient would be to use a single 32-bit value but that has too much bias; The least biased would be to fully randomize the state, but that is quite ugly to do directly or requires implementing or importing something like /u/ProfONeill's very nice randutils library. &gt; although it's obviously more than the single-seed-value constructor, and mt19937's constructor "fills in the blanks" so the state is initialized to something reasonable ... That's actually not what's happening in the code I show. I assume you wrote Microsoft's implementation of `seed_seq` and `mt19937`, so you know what's really happening and you're just simplifying the explanation. I'll just explain for anyone else. The code takes 256 bits from `random_device` and then `seed_seq` is responsible for performing an operation on these to come up with the complete state that is used to initialize `mt19937`. `mt19937` does not modify the state it receives, it just copies it*. The only time `mt19937` will 'fill in the blanks' is when seeding with a single `result_type` value. So in effect I'm using `seed_seq` to fill in the blanks rather than having the `mt19937` object do it itself. &gt; If you really want to fill in the whole state with random_device entropy, you need to construct a seed_seq as big as mt19937's state_size. Even that is less than perfect, given what `seed_seq` does. (Though I have written [examples][1] showing this as well.) \* mersenne_twister_engine does account for different integer widths when copying the data provided by a Seed Sequence, but that's unnecessary with `mt19937` because the widths are the same. Also, in a certain circumstance `mt19937` might change the state received from a `seed_seq`. This can be ignored since the odds are vanishingly small with a completely random state, and not practically worse with a `seed_seq` initialized as I show. [1]: http://stackoverflow.com/a/12323023/365496
&gt; that is quite ugly to do directly It's 5 lines. C:\Temp&gt;type meow.cpp #include &lt;stdint.h&gt; #include &lt;algorithm&gt; #include &lt;functional&gt; #include &lt;iostream&gt; #include &lt;random&gt; #include &lt;vector&gt; using namespace std; int main() { random_device rd; vector&lt;uint32_t&gt; v(mt19937::state_size); generate(v.begin(), v.end(), ref(rd)); seed_seq seed(v.begin(), v.end()); mt19937 mt(seed); vector&lt;uint32_t&gt; v64(mt19937_64::state_size * 2); // because 2 * 32 == 64 generate(v64.begin(), v64.end(), ref(rd)); seed_seq seed64(v64.begin(), v64.end()); mt19937_64 mt64(seed64); cout &lt;&lt; mt() &lt;&lt; endl; cout &lt;&lt; mt64() &lt;&lt; endl; } C:\Temp&gt;cl /EHsc /nologo /W4 /MTd meow.cpp &amp;&amp; meow &amp;&amp; meow meow.cpp 3189088385 4198905517423645860 1088755998 15898621895058742750 &gt; The only time mt19937 will 'fill in the blanks' is when it is initialized with a single result_type value. So in effect I'm using seed_seq to fill in the blanks rather than having the mt19937 object do it itself. You're correct here (I was confused).
I just pm'd you my details. It should be in your inbox! :)
Sounds very testable.
There is a similar implementation in the [GSL](https://github.com/Microsoft/GSL) library. Its called final_act and can be found in the gsl_util.h files (https://github.com/Microsoft/GSL/blob/master/include/gsl_util.h).
Strange, as `ifstream::read` is undoubtedly implemented in terms of `filebuf::sgetn`, and likewise for `ofstream::write` and `filebuf::sputn`; this seems to be a pathological case for VC++'s optimizer, as the `c++2` approach is consistently faster than `c++` with Clang/C2 (and thus the same stdlib code)...
You could make the seed atomic so the order of the the threads created would at least give consistent random numbers. If a program creates all its threads up front from the main thread it should be deterministic. 
Would it be possible to create an a random number generator using atomics? I would think that could be good enough for plenty of situations.
Well, as a basic bit, SyntroNet looks like a transport system for sending a variety of realtime data (audio, video, sensor data) around a network. SyntroCore is a library for sending and receiving data on SyntroNet. What are you trying to do, exactly?
So this thing is a big fancy microphone. It records audio data and stores it. That audio data then goes into an even fancier algorithm for analysis, but I'm not part of that. I'm only working on the part where it records audio and, in particular, exactly at *what times* the device thinks its recording (i.e., how does it know what timestamp to write down very each recording?) 
Ok. And is SyntroNet the code that runs the mic? Or is it custom code built on top of SyntroCore? Can you post a link to the code that talks to the mic and pipes the data into SyntroNet?
Guess I'll just stick to POSIX into my lib then. In the end i don't really care for what some code I've written and will probably never read again looks like.
Unfortunately the POSIX headers that come with VC++ use `unsigned` in a lot of places that are supposed to be `size_t` so it's not a totally portable solution. :-[
Yea, I did notice that. I don't think I'll ever use any 4+ GB files though... So fine by me.
Casey isn't some nobody that knows nothing about programming, if you look [here](http://mollyrocket.com/casey/about.html) he worked at RAD Game Tools on the Granny Animation SDK which has been around for the past 15 years, is still very popular today, and *written heavily in C++*. He used to be into everything C++, Object Oriented, etc but ended up realizing how it is not good or well designed. This is **not** someone ranting because they don't understand the language. It's just you haven't stopped drinking that kool-aid yet.
Contest mode is a very good idea as long as the posts are kept legitimate by the mods, otherwise you create a bias towards posts that are the oldest which is not a very helpful metric.
It's only a 3+ hour commute! I don't have any experience with phone development, though.
Anyone else use that one from SE where iirc you mix some seeded numbers and pid into some weird mix function that does a bunch of vector/matrix math (I'll find it if I have to)? 
Ok. PM me and we can discuss it more by email. I can probably explain whatever you need to know.
I sent an application to the email provided. I have a fair amount of graphics experience with some Android development, and of course C++. Hopefully, will look forward to hearing back! :) 
If you are into gaming and reverse engineering, I've open sourced a reverse engineered C++ API to the game Arma 3. It is relatively small. https://github.com/intercept/intercept
I definitely want to see those talks, but I'm saving up my pennies to go to CppCon this year. :(
In my projects I use [MiniCPPUnit](https://sourceforge.net/projects/minicppunit/), which is based on the same philosophy as JUnit. It clocks in at about two or three source files and a header, as I recall. I've run it on Mac, Windows, Linux, iOS, and AppleTV with no issues. Myself, I actually use a version I've tailored for my own projects (just getting rid of ansi output and has a little code cleanup), but regardless, I suggest trying it out if you get a chance. 
Hmm, ok. To create a level playing field for employers, I've enabled contest mode. Let's try this out.
&gt; Even when it does turn out later that the performance does matter, unordered_map has not been the correct answer often enough for me to feel that I should be just defaulting to it. It was be *so* nice to have a standard, open addressed hash map available in C++.
**Company:** [Optiver Asia Pacific](http://www.optiver.com/sydney/) **Type:** Full Time **Description:** Optiver is a tech-enabled trading (market making) company. We build high-performance software that is used by our own traders to trade a variety of financial instruments on exchanges throughout APAC. **Location:** Sydney, Australia **Remote:** No **Visa Sponsorship:** In certain cases i.e. very niche skill sets **Technologies:** All of our high-performance trading software is written in C++; this allows us to build fast, reliable, scalable trading engines. We write GUIs in C#, tools and prototypes in Python, and microservices in Golang. **Contact:** Please email us at careers@optiver.com.au with any questions.
Guidelines support library : https://github.com/Microsoft/GSL and I can't stop myself from dropping my own : https://github.com/haptork/easylambda well it is around 40 files but only a few are above a hundred lines. 
&gt; In fact, C++ was purposely designed in such a way that any program that is a valid C program, is also a valid C++ program. You can write a program that is entirely C, and a C++ compiler will compiler it. To a point. C++ is not a strict superset of C. Lots of things in C won't compile in C++ - old-style function declarations for instance. Another big one is that C doesn't require explicit casts. 
I'm not sure how well atomics are suited to the task. Atomics are great for making sure one data value is accessed atomically, but a prng state is many values. A mutex is a better fit. Whether you lock the mutex or the prng does is an entirely different question. 
If you don't need to store or return the List, a non-owning function type erasure can help: http://blog.miator.net/post/141640032964/take-callable-by-signature-rargs
I have a project in bash with an average of 4k lines each file. The biggest is 20k. I know the author, he is one of my bosses :(. (+_+) rip
It's completely free and open-source. Don't use the one from your distros repo though! It will be an ancient version. And don't build from source! Just use the binaries from qt.io. I think [this](http://www.qt.io/download-open-source/) is the link you want: Download the installer and just select QtCreator from it, nothing else - no need for any Qt libraries if you don't need them.
This might be a silly question, but why isn't `uncaught_exception` enough? I thought you'd just want to determine if an exception is being thrown?
The basic gist is inside a catch if you throw a different exception to the one caught there is more than one active exception. Explanation from a scope guard talk, starts at around the 28 minute mark. https://www.youtube.com/watch?v=WjTrfoiB0MQ
Well it's like this: If you know your game hasn't ended (meaning u still have a move to do, it's not over), then you "ask" player to give that input (using e.g. `std::cin`). If he gives you his input, then you can draw current game state after his move. If player gives you input, and it means that game has ended, you update game state and loop is ended after drawing table. I did miss _obvious_ `Update()` method there. If you really want this to look so good, then you should add something like `UpdateState()` after `AskInput()` which would choose if game is still in play, or has it ended? But i just assumed that it's obvious that it would go to either `AskInput` or `DrawTable` in this case. Either way, i've updated my answer.
why should the number of lines of code be a problem ? 
There's a typo in "Iterate with the speed and flexibility of dynamic **languages**"
It seems abandoned. No commit since 11 Oct 2015. travis-ci build is failing. Even the link to CERN binaries in the Readme is dead.
Looks like yet another C++ interpreter or Repl environment. The question is do we need another. 
can it run my project xyz ?
&gt; Actually, defensive programming dictates the callee DOES in fact take responsibility for validating its input doesn't violate preconditions. Well, yes, but there's a price to pay for the checking: * performance; in this case the price is small (albeit maybe not negligible for some), but not in general * complexity: in C or C++, how does a callee check e.g. that a pointer it received is valid?
Anywhere you can make the language more expressive without sacrificing readability, performance, or safety is a win, no?
Ah yep, my bad, that was for calling rand functions a lot with multiple instances of program. I normally just use that when I want something a bit better than normal rand() 
&gt; I suppose because you can write one yourself in a couple of lines of code, including it in the STL is unnecessary. But they have linear congruential generators too, which are also very easy to write and generally not as good. Also, there's a bit of boiler plate to write in order to plug it into a random distribution. 
Where is said rant? Couldn't find in/u/hftpro's profile.
Now if someone could just fix std::random_device always returning the exact same sequence in MSYS2 / gcc (yes, I'm stuck on Windows at the moment :( )...
Right. Herb is a partner program manager. I'm the dev mgr.
https://root.cern.ch/gitweb/?p=cling.git;a=summary Last commit was 29 hours ago.
Btw, there is one for metaprogramming: http://metashell.org/
This belongs in /r/cpp_questions, not here.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust] [x-post from r\/cpp: Logical Expressions in C\/C++. Mistakes Made by Professionals](https://np.reddit.com/r/rust/comments/4egxfj/xpost_from_rcpp_logical_expressions_in_cc/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
With Google Test you can generally limit yourself to the EXPECT_THAT and ASSERT_THAT testing macros, rather than needing to use/remember the panoply of comparison macros.
cling. The github page looks stale &amp; outdated (which is what I think you were looking at) which is slightly worrying since that's Vassil Vassilev's page who evangelized it a lot publicly. However, the actual CERN git repository for cling looks to be fairly active.
Scopeguard with `dismiss` is still my preferred choice for writing certain things. For instance, imagine writing the constructor for `std::vector` that takes a size. You'll first need to allocate memory with the allocator. After that, you'll want to ScopeGuard that memory while you construct all the objects (which themselves need to be guarded, that's another story though). However, after all the objects are constructed, you're finished with the constructor, and the memory now belongs to the vector and will be freed in the destructor. So you dismiss the ScopeGuard. template &lt;class T, class A = std::allocator_traits&lt;T&gt;&gt; class MyVector { MyVector(int size, A a = A{}) { auto ptr = std::allocator_traits::allocate(a, ...); auto ptr_guard = makeScopeGuard([&amp;] () { ...deallocate(...); }); int num_constructed = 0; auto obj_guard = makeScopeGuard([&amp;] () { for (int i = 0; i != num_constructed; ++i) { ptr[i].~T(); } } for (; num_constructed != size; ++num_constructed) { new (ptr + num_constructed) T{}; } ptr_guard.dismiss(); obj_guard.dismiss(); } (Probably has bugs, but you get the idea). What would you do instead? I definitely prefer this to catching, doing clean-up, and re-throwing. I agree with you though for strong exception safety it's not necessarily the best choice.
I would probably use a `unique_ptr` of some kind to manage the allocation because that's what they're designed to do (i.e. the familiar solution to a familiar problem). That just leaves the central initialization logic which I'll argue isn't any uglier to use a `try-catch` clause than it is a ScopeGuard. There are times when a ScopeGuard is handy, but IMO only if it doesn't require `dismiss()`.
Do you know if there are any plans to use libclang for intellisense instead of what I understand to be a custom build of EDG? I've been noticing discrepancies between what intellisense will flag as invalid and what VC.exe will flag as invalid more often lately since the advent of C++11/14 (manifested as extraneous or missing red squiggles). It would provide some nice consistency if the compiler and intellisense both used clang.
That's a question for /u/spongo2, he's the dev manager for the libs, FE, and IDE.
To solve the same problem that requires `ScopeGuard::dismiss` requires `unique_ptr::release`. In this case perhaps you could argue that you're going to use a unique_ptr as a member so you don't need release, but in other cases you will. Custom deleters with unique_ptr are much more annoying than ScopeGuard. If you're just planning to rethrow the same exception without changing the exception itself in any way, I'd argue that `try-catch` is almost always an inferior solution. When you're dealing with multiple points of failure, ScopeGuard scales much better (and this is one of Alexandrescu's key points in his talk about the updated ScopeGuard).
This is what we use at work. The integration with Google Mock (which is pretty awesome) is what sold us. It's been slow to adopt C++11 but the workarounds aren't awful compared to the suite of other great features.
Most new Google Test development appears to be happening in Google Mock. There is now (limited) [support for move-only types](https://github.com/google/googlemock/blob/master/googlemock/docs/CookBook.md#mocking-methods-that-use-move-only-types).
At least it was released.
And lisp is finally catching up in speed and ecosystem....
We are continuing to use EDG for intellisense in VS. I would LOVE to know what discrepancies you see. We fix these when we know about them.
Yeah. It's not that bad. There are countless improvements that are unrelated to that. But they're also trying to not reinvent the language. If you want something hip and new, go use nodejs or whatever the new hip language is. When you want to make something with a memory footprint that isn't the size of Montana, come back to statically compiled languages line C++. 
MSYS2 is more similar to Cygwin. It is environment that offers you a lot of precompiled open-source packages. Like bash, tar, awk, perl, .... It's main advantage over Cygwin is CLI package manager. Last time I used Cygwin it still used GUI to install or remove packages. MSYS2 uses pacman. You don't need to use MSYS2 to compile using Mingw. Mingw-w64 is just a standalone gcc compiler. Which can be installed in MSYS2 or it can be used stand alone. If you will use Mingw-w64 in MSYS2 you will get a lot of nice tools (if you care about it) - bash, awk, sed, grep, autoconf, automake ... Using it as standalone you won't have these tools, and if you are using GNU makefiles as your buildsystem, it could be very annoying without such tools. If you use something like CMake, then using just Mingw-w64 without MSYS2 will not be a problem.
I'm guessing hftpro went on his usual rant against C++?
If you're using MinGW, whether it's the original mingw.org or the Mingw-w64 fork, then you're programming for *native Windows*. You get whatever APIs Windows provides, and you don't get POSIX APIs. Cygwin is both a POSIX emulation library, as well as a distribution of tools built against that library. If you want to use POSIX APIs, you'll need to use an emulation library, but you should think long and hard about whether you really want to do that. MSYS is a fork of an extremely old version of Cygwin. So, like Cygwin, it is both a POSIX compatibility library and a distribution of tools built against that library. The major difference between the two is in the handling of paths. In Cygwin, all programs are expected to use POSIX paths, and there is no translation to Win32 paths performed. This works well for porting programs that already expect to use POSIX paths, but it does not work very well if you intend to also use native Win32 utilities (which expect to work with Win32 paths.) MSYS has logic that when invoking a native Win32 program, command line arguments will be implicitly converted from POSIX to Win32, with a bunch of heuristics. This means that MSYS is suitable for use as a build system where you have a mix of programs, some using the MSYS runtime and some being native Win32 apps (aka MinGW apps.) So MinGW is just a toolchain: compiler, assembler, linker, debugger, standard headers. All the tools are win32 native programs (i.e. they take Win32 paths and aren't linked against the MSYS runtime.) You can use MinGW without using MSYS. But MSYS gives you a build environment with a set of common Unix tools, such as the Bourne shell and Perl, which can be used in conjunction with the MinGW toolchain. And as already explained, the path translation logic in MSYS means that when invoking a native tool, the necessary translation is performed. Cygwin also has its own toolchain, all of which are linked against the Cygwin runtime and expect POSIX paths. The advantage of Cygwin is that it's more consistent. Everything is linked against the runtime, everything expects POSIX paths, and there are no Win32 paths or Win32 applications. It's the closest you can get to a pure Unix environment on Windows. If you're fine with staying in this environment, it's far superior to MSYS (which tends to have ancient versions of tools, and as it's based on an ancient fork of Cygwin, the runtime is quite outdated.) You can even integrate Win32 tools into a Cygwin path by using the `cygpath` tool to explicitly translate paths when necessary. But some people still prefer MSYS for whatever reason. But back to code that you write. Again, if you are using the MinGW toolchain (either by itself or in conjunction with MSYS), then you don't have access to POSIX APIs. You will need to port you code to Windows. MinGW is not a compatibility layer. The "Min" stands for Minimalist. If you want to write code that uses POSIX APIs, you'll need to either link against the Cygwin runtime or link against the MSYS runtime, although doing the latter is quite rare and MSYS isn't really built for that, so usually that means you'll use Cygwin (i.e. use its toolchain and not MinGW.) Code that links against the Cygwin runtime will require the Cygwin DLL, and will have some licensing requirements. The Cygwin code is licensed under the GPL with an exception clause that allows it to link against code under any OSI-approved license without the code becoming GPL. So essentially, your code has to be open source if you're going to distribute binaries. There is no such requirement when using MinGW, which uses a very liberal license.
JIT != REPL
Does anybody know why it took them 1 billion years to post videos from previous C++Now. I get they want to keep it exclusive, but to be honest it is a small enthusiast conference, I doubt attendance would drop that much if they released videos like 4 months after the conference. 
Difference is you can't forget or misplace the release(). Multiple points of failure suggests functions that should be broken into smaller pieces and thereby not so stateful.
We don't hold back the videos intentionally. The conference is run by volunteers and post processing the videos takes time. We get them up as quickly as we can.
Yes - this confused me at first. I initially thought it was an attempt to treat LLVM bytecode like JVM bytecode, which is not really useful because of conditional compilation.
**Company**: [Splunk](https://splunk.com) **Type**: Full time **Description**: Big data analysis. Our products help to make sense of unstructured data. Main parts are written in C++: indexing, search, distributed search, management, replication. Looking for different levels: from beginners to principal developers. **Location**: San Francisco, CA; Seattle, WA; Palo Alto, CA; Vancouver, British Columbia; Shanghai **Remote**: Preferable not, but there are always exceptions. We have few folks working remotely. **Visa Sponsorship**: It depends. **Technologies**: C++98/C. Linux/Darwin/Windows/Solaris. Other languages are: Python, JavaScript. We do some stuff in Golang (mostly prototyping), Java. **Contact**: You can apply directly at http://www.splunk.com/view/SP-CAAAGMJ, feel free to contact me about position at Seattle https://www.splunk.com/view/SP-CAAAGK3?jvi=o8ML1fwq
UFCS will make it easier to organise code. source bases evolve as demands change, so functions get moved in and out of classes.
Yep. In the future, please send such questions to /r/cpp_questions, as the sidebar advises.
Is it possible that your interviewer was looking for you to implement what std::next_permutation does instead? It does not use recursive backtracking (and indeed it cannot, since recursive backtracking requires linear space).
The Indiana Jones solution: C:\Temp&gt;type meow.cpp #include &lt;algorithm&gt; #include &lt;iostream&gt; #include &lt;list&gt; using namespace std; int main() { // list is inefficient, but demonstrates bidi list&lt;int&gt; l{ 1, 2, 3 }; do { for (const auto&amp; e : l) { cout &lt;&lt; e &lt;&lt; " "; } cout &lt;&lt; endl; } while (next_permutation(l.begin(), l.end())); } C:\Temp&gt;g++ -std=c++14 -Wall -Wextra meow.cpp -o meow.exe &amp;&amp; meow 1 2 3 1 3 2 2 1 3 2 3 1 3 1 2 3 2 1 
Cheers for the correction, I've learned something today :-) I'm new to C++, and was under the impression that compatibility with C went a lot further than it does in reality. The offending paragraph has been re-written.
When you don't have mixed insertions and deletions, binary-searching a sorted vector (or `boost::flat_map`) can be dramatically faster if the key is small due to the much better cache locality (and if your data fits within a cache line, even an unsorted vector is hard to beat). For certain mixed insertion/lookup usage patterns a btree can be dramatically faster for similar reasons. Even when a hash table is the best option, the various collision resolution methods can have a significant impact. Fortunately all of the major implementations of `unordered_map` are sufficiently similar that using it isn't an inherently bad idea in portable code, but I've seen a 10-20% speedup just from dropping in a boost::multi_index container instead, and an open addressed hash map can give bigger gains (or be worse, of course). Often the actual answer is "redesign the code to not need a key-value lookup at all", of course.
Stop "using namespace std;" and do your own homework.
 static inline size_t length(const char_type* __s) {return strlen(__s);} its the first line
That is a function definition, not an error.
http://reddit.com/r/cpp_questions
somewhere like http://reddit.com/r/cpp_questions
oh I forgot to make it clear, it's a single file with 6000+ lines of code.
You can do it by having your recursive function return a valid iterator to insert before. So as the stack unwinds you're returned a position to the current head, you insert your value before it and everything remains in place.. I've modified your example a bit, unless I'm mistaken, I think this works correctly: #include &lt;iostream&gt; #include &lt;list&gt; #include &lt;vector&gt; std::ostream&amp; operator&lt;&lt;(std::ostream&amp; s, std::vector&lt;int&gt;&amp; v) { for (auto i = v.cbegin(); i != v.cend(); ++i) { s &lt;&lt; *i &lt;&lt; ", "; } return s; } std::list&lt;int&gt;::iterator generate_perms_list_helper(std::list&lt;int&gt;&amp; list, std::vector&lt;int&gt; &amp; partial) { auto result = list.end(); // Are there still unused elements? if (!list.empty()) { auto iter = list.begin(); for (size_t count = 0; count &lt; list.size(); ++count) { int value = *iter; partial.push_back(value); list.erase(iter); iter = generate_perms_list_helper(list, partial); result = list.insert(iter, value); partial.pop_back(); } } else { std::cout &lt;&lt; partial &lt;&lt; std::endl; // assumes operator&lt;&lt; to be properly overloaded } return result; } void generate_perms_list(std::list&lt;int&gt;&amp; list) { std::vector&lt;int&gt; partial; generate_perms_list_helper(list, partial); } int main(int argc, char** argv) { std::list&lt;int&gt; list = { 1,2,3 }; generate_perms_list(list); return 0; }
Conversely he doesn't seem to have worked on any large scale applications, just highly separable technology projects. I'm not dissing that but large scale development is different problem set. It's a shame he's so willfully blinkered to that and deals with it through abuse because his vids are pretty good :) Now he's the "LOL C++ IS L4M3" guy
Not if you are just splicing one node (which is what OP's use case requires). The only case that takes non-constant time is "splice some from other".
That almost never happens. If it does, it is so rare that it certainly doesn't justify the cost of the change. 
Cool! That is encouraging.
Btw, if you run "cmake -G", you are going to notice that there are generators for MSYS Makefiles and MinGW Makefiles. From the CMake documentation: MSYS Makefiles: The makefiles use /bin/sh as the shell. They require msys to be installed on the machine. MinGW Makefiles: The makefiles generated use cmd.exe as the shell. They do not require msys or a unix shell.
Hi, thanks, but this is an assignment I have created for myself during the interview preparation. I have implemented the next_permutation() algorithm as well but this time I want to write an STL equivalent of the algorithm above. 
It happens constantly. You might think it doesn't because at present, the extensions come in a different syntax, which leads to people bouncing back &amp; forth throwing source bases away and rewriting. We have kitchen sink classes which need splitting up, and communities make extensions to core libraries which should eventually get absorbed.
Since GCC 5 it has conforming constant-time list::size(). And linear time is only for range-splice, not for single-element splice. 
There is also native Clang for Windows. [See here](http://llvm.org/releases/download.html)
This creates incorrect outputs for n &gt;= 4. E.g. for list = { 1, 2, 3, 4 }, 1 2 3 4 is output twice and 4 1 2 3 is missing.
This is something that I would have found very useful had I been able to use it with some legacy libs where nobody thought about the order of arguments. I'm not too keen on the syntax though, using dot for that kind of thing isn't very idomatic. Then again I don't have much better to propose. Maybe this could be a good way to use the "default" keyword once more?
Oh, thanks everyone! It makes much more sense now. Just about a Visual C++ / Cmake. So in most cases I could use one CMakeFile to produce 1. Makefiles for Linux and 2. Visual Studio project for Windows, as long as I don't use anything on the code itself from the Win32 api or POSIX api and if the libraries I use are available for Visual C++ too? Kind of new to whole C++ world and only experience is on the Linux side so CMake and stuff isn't so familiar. What I would like to do is be able to work on same code on Windows and Linux for which I wanted to use GCC on both but if it works like this I guess it's fine too.
`cl meow.cpp &amp;&amp; meow`?
What's wrong with GCC JIT?
What if the legacy stuff has different parameter names in the header and the cpp file?
I could take or leave the named parameter syntax, but designated aggregate initializers are something I've been pining after in C++ ever since I discovered them in C, and I'm glad to see them on the table again for inclusion.
Yes please.
I like much of it, but: Copy-initialization sounds like permitting lossy conversions(?). Those should be eradicated where possible, given that compatibility allows it. What I don't want is this: int x = whatever(); auto vec = std::vector&lt;int&gt;{.size=x, 23}; This is begging for problems as `x` might be negative. Apply the same rules that are in place for brace-initialization, and we are fine. But otherwise this would be a deal-breaker.
You put additional four spaces before lines of the code.
Hmm, sounds like an idea! Going to try that. Thanks :)
I agree 100%. Which is why I cant figure out why this code is working.
The comma operator will evaluate the left hand expression but return the result of the right hand expression. So `return cost, turnover, price;` is the same as `return price;`.
The only function whose return value you assign to a variable (retail_price()) happens to return the value you're interested in saving.
Yes, this is a standard way of "returning" multiple value, but it is not because of your `return` statement. Look at your function parameters- they are reference types. This means when you modify them in the function body, you are also modifying them in the caller's scope. Note that if you had passed your parameters as value types (`double`, not `double&amp;`) then this wouldn't have worked at all. The syntax `return x,y,z,...` is valid, but it doesn't do what you are expecting. The `,` operator you are invoking here does something else entirely. In effect, you are returning the last value in that "list".
I've now watched two of your presentations and am currently watching a third. I don't *really* know if Meson is useful for me yet but I do now that I'm now a fan of your work! :)
When I modify my code to reflect that, it breaks and stops giving me the correct answer.
I like the `.` syntax, even for function calls. For initialization I hope it's available only for public member variables (`struct`s are public by default).
Yep, removed.
Yes, I have the ability to sticky the meta comment. Done, thanks.
At least this is obviously wrong, unlike disappearing template parameters. To the author: code dumps are pretty boring unless they're written in a literate style explaining the Why instead of the How. Another good alternative is talking about some key design decisions by pulling out snippets and including the entire source as a zip file.
I'm interested in this RFC being extended to template parameters. Although the current practice of grouping function parameters into a struct is less than ideal, it is better than the workarounds for named template parameters. Best I've seen there is [boost's](http://www.boost.org/doc/libs/1_50_0/boost/intrusive/options.hpp) [solution](http://www.boost.org/doc/libs/1_59_0/boost/container/options.hpp), which is verbose and macro heavy.
Doesn't matter? I mean, using parameter's name to pass an argument is like specifying position of that argument, and parameter names can be different in declaration and definition, so..
Ooh great another self-promoting post by Jason Turner.
The Linux compatibility subsystem is not really meant for production code. It doesn't support any GUI stuff, for example. And you won't have access to any native Windows functionality. You could install a Windows cross compiler that outputs Windows EXE binaries, and run that on Ubuntu under Windows if you wanted to see how many layers deep you can go. That said, when I am on Windows I usually use the Qt Creator IDE (which comes with Mingw toolchain so you can use it easily) but I use it with the MSVC compilers that come with Visual Studio. I also use Msys2 for stuff that needs a unix-y build environment. I know what all the buttons do in Qt Creator, and it is available on Linux, Mac, and Windows so it's my default IDE on every platform that I deal with, even when writing code that doesn't involve Qt. (I have a half written blog post about using MSVC compilers in MSYS2 to build a library called ffms2 that I use. I'm not mainly a Windows guy so I got confused about some parts of the process, and I figured documenting it might help me remember if I ever had to do it again, and maybe anybody else figuring out how to get up to speed quickly on Windows. I should probably finish writing that. )
Of course, especially when they are smart :D
 - [Project home](http://mesonbuild.com) - [Github](https://github.com/mesonbuild/meson)
I'm confused; `next_permutation` *is* the STL. If you don't want to use that algorithm you could always implement what it does.
&gt;Since GCC 5 it has conforming constant-time list::size(). Ah, cool, they fixed it :D. &gt;And linear time is only for range-splice, not for single-element splice. If you wanted to do what the OP is trying to do you would want to do things like splice out from the current point in the list to the end of the list, so you'd be using range-splice.
[Implemented!](https://github.com/Oberon00/synth/commit/45f07a181052408c20a8de900b8530ea5ddf2133) :)
&gt; splice out from the current point in the list to the end of the list, so you'd be using range-splice This is also constant time because it does not change the number of elements in the list, so you don't need to recalculate size. See [on cppreference](http://en.cppreference.com/w/cpp/container/list/splice)
Ah, sorry, I misunderstood what kind of splice was needed. 
We release monthly so the mentioned ones are the only major new features. The rest is just bugfixing, minor usability tweaks and polish, polish, polish.
Awesome, looks like this is exactly what I need! Didn't know about splice :-)
Yes, can we have this in C++17, please?
There is a similar utility going through the committee right now. Latest paper is [p0052r2 - Generic Scope Guard and RAII Wrapper for the Standard Library](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0052r2.pdf) which links a reference implementation [here](https://github.com/PeterSommerlad/SC22WG21_Papers/tree/master/workspace/P0052_scope_exit/src). 
The problem with `boost::container::flat_set` is that it holds its data in sorted order then applies a binary search to that data. This looks appealing in terms of big-O, but still causes cache thrashing when dealing with large amounts of data. Significantly better is to store the data in breadth-first order and apply a linear search. E.g., Instead of using data `{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 }` and a binary search, use data `{ 6, 3, 10, 1, 5, 8, 12, 0, 2, 4, 7, 9, 11 }` and a linear search. This results in the same worst-case O(log n) complexity to find a value but plays very nicely with the cache regardless of data quantity. I've written my own solution for this, as I imagine many people have, but it really just needs to be in Boost.Container already... (Obviously all of this applies equally to `boost::container::flat_map`.) EDIT: This is all assuming you're searching far more than inserting/removing elements.
perhaps stdout does not get flushed before your program exits
The hard part with that is (1) then we need libraries to change in order to use it, and (2) they (the libraries) would have to do different things depending on the compiler you used to compile them, since legacy compilers won't know about this new syntax in their declarations. I don't really see the problem with automatically "exposing" their parameter names. They're already "exposed" now, if we can see them in the header; though obviously they're not used for anything today. If you really want to hide a name, then don't provide the param name in the declaration (after all, you don't have to, afaik). That type of "opt-out" would be backwards-compatible as well. And you could always make things more obvious by doing "internal_foo_param". Library authors could even change their param names without breaking compatibility by declaring the same function twice: once with the stable/earlier names, and another time with their internal name. (if they even need to - nothing prevents the function definition from using different param names than the declaration).
&gt; There is no chance that this is going to be in C++14. Hands down, no argument there
Just FYI, if you want that, and you have access to intel compilers then all you have to do is upstream this and maintain it there: https://github.com/steveire/CMake/commit/da05182d851b59c34a5a993f29567044ebaeccd4 
C++ likely isn't the right tool for this job. Consider using a scripting language like Python that has a large standard library containing HTTP and HTML libraries.
Thank you. I'm just curious though, where is Cpp's strengths compared to say Python or Java? And why does CPP complicated when it's involved with the web?
it looks like vaporware
The design decisions are listed in arbitrary order, no priorities implied. designatable - yeah, I know I've just created a word ;)
Overall I like the idea, but I have a few questions about it: * Since '=' operator is used by the caller, does it mean the assignment operator is called instead of the copy constructor? For primitive types it doesn't matter, but for custom types with overloaded operators it might. If it still calls the copy constructor, I would prefer a syntax without the '='. * According to 5.6 parameter-name based overloading is not allowed, isn't this exactly what is happening in 4.1, with the vector initialization enhancement? In my opinion, for the general case, when having a function void f(TypeA .foo); calling it with `f(5)` or `f(.TypeA = 5)` should result in the exact same behavior.
Thanks, fixed
What I would advise is to install git (which you might need). It comes with a nice terminal + bash + some common unix tools. Then you can install the visual c++ command line tools. If you need gcc/clang, you can add them later (mingw) as others have stated. I find this config very windows friendly (MSVC toolset) while having a nice term to work with.
Ugly syntax of C++ becomes uglier. No, for me.
I would also suggest a scripting library for this. I've used a Nokogiri module(or gem) in ruby. It had css selectors to get the desired data, later on I've searched for a similar library for C++. Although I couldn't find a single library to do all the work. You can get the web page with libcurl or curl++ (C++ wrapper) convert to XML with Tidy, and then extract the data from it with libxml2. I've used the latter 2 libraries for only once, so I am not sure how good these libraries are but for a trivial use they were enough.
I prefer `std::vector&lt;int&gt; v(N);` (i.e. without `resize()`) and then the loop with `v[i] = curr;`. Using the branching logic of `push_back` in a loop of known size can be slower than two sequential passes over the data. I use `push_back` only for adding data in an existing vector, not for extended initialization. The same for using `std::generate` with a raw pointer into the zero-initialized vector versus using `std::back_inserter` into an empty reserved vector.
Do you have a question? 
My bad I somehow I didn't put that, so after it creates the new list its supposed to copy the old one to the new one, delete the old pointer then make the original list pointer point to the temp. But when it gets to that it crashes and gives an error about unassigned memory or some shit
My bad I somehow I didn't put that, so after it creates the new list its supposed to copy the old one to the new one, delete the old pointer then make the original list pointer point to the temp. But when it gets to that it crashes and gives an error about unassigned memory or some shit
That's what I was gonna do, but it's for a class so I have to do it their way lol
this is HTML encoding for '&gt;' 
Those rare cases where: 1. Constant time amortized is not appropriate due to hard-real-time requirements. (trade throughput for consistency) 2. You really have a collection of N elements where N=huge, with many insertions and removals in the middle, and bidits are OK, and keeping iterators around is OK. 3. Test cases for STL algorithms making sure they work with bidits.
No.
&gt; I figured out how to fix this in Update 2 while preserving binary compatibility. The next major version of the libraries will contain a more comprehensive version of this fix, where I was able to delete 31 source files and shrink the STL’s DLL by 12 KB. Can you elaborate? That sounds intriguing.
I love these changelogs so much. I know they must take you a long time to write and we all appreciate the effort. Thanks STL!
You're welcome! Yeah, they take a while to write up. This time, there was an unexpected "reward" - writing up my atomic fix led me to discover the atomic&lt;long long&gt; bug (broken since atomic first appeared in 2012, but nobody noticed).
In Update 2, iostreams bypasses its slow and busted multiprecision codepaths for parsing floating-point, in favor of calling the Universal CRT, which is both fast and correct. This is similar to how VC 2010 and earlier behaved (when we called VC's CRT, which was fast but sometimes incorrect). There's an extra twist (where I use larger buffers and skip more busted code, to preserve the UCRT's correctness). However, Update 2 still needs to export the multiprecision code, for binary compatibility. In the branch for the next major version of the libraries, I deleted the multiprecision source files entirely, plus the extra busted code, hence the space savings.
EDIT: In debug mode, sorry Is it possible any of the changes to vector would cause a performance decrease during destruction? I noticed significant slow down after installing Update 2 in a tool I wrote for a passion project of mine where profiling revealed the destructor of a very large vector (up to about 12MB worth of `char`s) was causing a huge bottleneck and the tool was running maybe 5x slower than in Update 1. I changed the class in question to use a vector instead of a C-style array relatively recently but I'm 95% certain it was before Update 2 and the tool ran fine at that point. I'm going to try to put together a small repro solution this weekend, but I'm just curious if that's something that could have been impacted by the update, or maybe my code is crap and I should be using a C-style array. It could very well be my code, and normally I'd suspect it's something I did, but the timing seemed too coincidental. EDIT: Summarizing my results from further down: [Repro](http://pastebin.com/55JHMRWY) (created from normal Win32 Console App template, no SDL, no ATL/MFC, PCH enabled) Function | Update 1 (Debug-x64) | Update 2 (Debug-x64) ---|---|---- `read_texture` | 14ms | 14ms `read_texture_scope_test`| 17ms | 1329ms Running with x86 configuration it was slightly faster (~1250ms) in Update 2.
&gt; (broken since atomic first appeared in 2012, but nobody noticed). Did that please you or annoy you? :-)
So, I didn't know std::accumulate existed. However, after seeing it, I don't know why I'd want it anyway. The essay even points out that it does a = a + b instead of a += b which is sometimes worse. Plus accumulating on a container is such simple code that I can almost promise it will take most of us longer to figure out (remember, Google, whatever) the inputs to the STL algorithm than it would take to type out the for loop. Even the reverse iterator version is trivially simple with a for loop. It gives me the same feeling as I had when looking at the code for left-pad.js in the whole NPM javascript debacle. "Why don't you just take the thirty seconds to write it yourself?" Am I totally off base here?
(Also: the metaprogramming was incorrect; users are allowed to do nontrivial things in allocator::destroy and we were not respecting that)
I'll pull together a repro this weekend (side project, don't have much non-work dev time until the weekend) and submit a bug report if I can reproduce it. Just wanted to see if there was a possibility something changed in that area. I'll let you know what I find.
I don't think so. It can be helpful to have the name for clarity but decent naming of variables takes care of that. 
iostreams are infested with virtual calls, single-character I/O, and additional processing (e.g. we go through some "widening" code and buffer copying, even for chars). Some of this is the interface/Standard's fault, and some of it is our implementation's fault. As I mentioned, there are more improvements that we can make in this area, but I expect that iostreams will always be significantly slower than the CRT.
Awesome! That's exactly what we've wanted to hear from our customers.
In the repro, no. I have some issues to resolve with the FBX SDK to get my project to run in release mode, but I'm sure it's fine. The results of the repro mirror exactly what I was seeing in my project, so I'm sure release mode will be fine when I get it working. If I see any different I'll let you know. I understand that non-optimized perf isn't a priority and in this case may not affect anyone but me, that's fine. The difference in speed was large enough I thought you guys should know. Thanks for the quick responses! I can work around this by reverting to C-style arrays.
Thanks! I appreciate the quick responses. The update has otherwise been totally solid for me, so keep up the good work :) I can work around this pretty easily, it's not a big deal. Thanks again!
Just interesting, How do you find [CppFormat](http://cppformat.github.io/latest/index.html) design for you as STL streams replacement?
Haven't looked at it.
&gt; * &lt;unordered_meow&gt; My favorite standard header. I like reading these changelogs, I learn quite a lot about C++ each time. Thanks, and keep up the fantastic work!
Your post has been automatically removed because it appears to be help/homework related. If this has been in error please message the moderators. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
That's also true.
iostreams do a lot of things, which are actually same things as C I/O does, just not hardcoded: * access to a customizable external byte source or sink (array, file, serial port, TCP socket (see boost.asio)) * read/write/putback buffering layer on top of that (with repositioning if the external supports it) * conversion layer on top of that (with the limitation of being N:1, so UTF-8 external to UCS4 internal but not UTF-8 external to UTF-16 internal) * stateful typesafe parser/formatter layer on top of that, with endless customization points: state manipulators, system-supplied locales, and user-supplied facets. Perhaps too many - custom stateful manipulators (ones that use xalloc) don't seem to be popular, and I don't think I've ever seen all three callbacks in use. I don't think any one library can replace iostreams. It would require at least three - a buffer management library (which would have to support what boost.iostreams supports - compressed streams, tees, etc), a Unicode library, and a parser/formatter library, for which there are many options already. PS: I don't have ire towards iostreams, in fact, I point to the streambuf hierarchy is a perfect example of how to use virtual functions; as customization points, including both required (overflows) and optional (block transfers), which are not accessible via public interface but aren't parroting the public interface either, like in the facet hierarchies.
Python has good string handling, and libraries that are good at parsing crappy, syntactically incorrect HTML, and libraries intended for web scraping. C++ has adequate string handling.
Your post has been automatically removed because it appears to be help/homework related. If this has been in error please message the moderators. *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I'm surprised. The assumption that everything would "vanish" during compile time is so ubiquitious you get downvoted to your knees when talk about STL &amp; Modern C++. 
streambuf's main failure is that a locale is always attached to it, even if the stream to be used is purely binary. that's plain irritating. i can see your point about mostly keeping streambuf. For formatting something other than iostreams entirely based on utf8 only could be built on top. Of course the locale thing should still be dealt with....
I like how java does io with it's Input/OutputStreams. You can easily make your own similar wrapper. Then use boost.format which plays better with localization as a previous poster mentioned you can't change order with iostream.
so something with non-default formatting could be done like std::cout &lt;&lt; fmt( "{0:C2}-&gt;{1}-&gt;{0}", 17.4353f, some_str ) &lt;&lt; some_other_val &lt;&lt; "\n"; Heck I might go write a crappy version of this right now. I don't mind the .net string format specifiers as you get the positional information Update: screw it, boost did it and uses an example like this. 
There was a similar topic a while ago https://www.reddit.com//r/cpp/comments/48z6u9/guy_interested_in_web_search_engines_web_scrapers/
&gt; It does virtual calls on a per-character basis. Is this something mandated by the standard such that g++/clang also have to do? So what's the replacement for iostreams in Modern C++?
&gt; Folly Just looked up. From Facebook? It looks like every big firm developed their own libraries... 
&gt;Is this something mandated by the standard such that g++/clang also have to do? Much of it is mandated by the standard; e.g. how std::codecvt works. Much of it (at least on VS) could be improved in our implementation. &gt;So what's the replacement for iostreams in Modern C++? I don't believe there is one. I still recommend cstdio. :)
I heard that GCC/Clang are better at devirtualization, so that may be the reason for why their iostreams are faster.
I'd be curious to see your opinions of my (inprogress) https://github.com/seanmiddleditch/formatxx in comparison.
Nobody says otherwise. But the obvious cases like reading and writing a binary file shouldn't get a **~3x** speed penalty like we get with Visual C++ 2015 update 2. Both `std::ifstream`and `std::ofstream` are nice RAII classes and with two lines of code I can write or read a `std::vector`, which is neat. Kudos to GCC team for having their binary I/O as fast as the CRT / Win32 API. After publishing the article above I have learned how one can copy a binary file from one stream to another in one line, and doing so effectively: void testCppIO3(const char* inFile, const char* outFile, std::vector&lt;char&gt;&amp; inBuffer) { std::ifstream in; in.rdbuf()-&gt;pubsetbuf(inBuffer.data(), inBuffer.size()); in.open(inFile, std::ifstream::binary); if (!in.is_open()) { std::cout &lt;&lt; "Can't open input file: " &lt;&lt; inFile &lt;&lt; std::endl; return; } std::ofstream out; out.rdbuf()-&gt;pubsetbuf(inBuffer.data(), inBuffer.size()); out.open(outFile, std::ofstream::binary); if (!out.is_open()) { std::cout &lt;&lt; "Can't open output file: " &lt;&lt; outFile &lt;&lt; std::endl; return; } out &lt;&lt; in.rdbuf(); } This implementation is simple and super fast. It is fast on GCC. Tested both on Windows and Linux. Reading and writing binary files with Visual C++ 2015 update 2 is **~3x** slower. How fast / slow is the code above with Visual C++ compiler? **~10** times slower. It is true, iostreams are significantly slower than the CRT, but only if you use Visual C++.
The behavior of `pubsetbuf` (and the `setbuf` it invokes) is implementation-defined. Relying on it for consistent, sensible behavior across toolsets is broken to begin with. The only thing the _standard_ has to say here is that calling `setbuf(0, 0)` before any I/O takes place causes the file access to be unbuffered. Your overall point re: VC++ iostreams performance is very much agreed with though, although pedantry necessitates that I mention it's actually the Dinkumware stdlib to blame here, not the compiler. ;-]
Clang on Windows using VC++'s stdlib exhibits the same performance characteristics. The Dinkumware stdlib is to blame, not the compiler.
Hopefully someday we'll get libc++ on Windows...
For me, the bigger issue is that `iostream` is not designed *at all* with concurrency in mind. Even if you decide to roll your own version that is intended to be thread safe, the chaining of `&lt;&lt;` streaming operators will consist of discrete function calls, and does not allow dispatching the entire chain at once as a single "message". Whereas a single `printf` style function call, you can. If anyone has a solution for the above chaining problem, I'm all ears, as I'm looking for a thread safe `iostream` compatible alternative. edit: Thanks for the ideas. 
Thanks, that is clever! It is a slightly different algorithm (the order of permutations is pretty crazy), but it works. 
There was a proposal for output buffers exactly for that purpose. [C++ Ostream Buffers](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4187.html)
Holy balls, this sounds exciting. Could you describe to me a regular work day? Currently I'm an consultant doing different projects and talking to customers and such. How much do you code, and how much do you react to bugs and such? Aaand how fun is it? :) 
&gt;The essay even points out that it does a = a + b instead of a += b which is sometimes worse. The idea is to use it when it isn't worse, not all the time. You'll find that many times, it's just as good. However, when performance is a concern, the golden rule is *always profile*. Otherwise, performance should be a secondary concern. &gt;I can almost promise it will take most of us longer to figure out Readability always trumps writability when developing software. You write your code once and learn the STL facilities you use once, but your code may hang around for you, your coworkers and future maintainers to peruse for possibly years from now. As for readability, it depends on your definition of "us". If you're referring to, say, people new to programming/C++/whatever, yes it is something new to learn, but as a working programmer, I find it quite readable, shorter, and more expressive, because it reads to me as "accumulate all values from *range* [, using *function*]", and I'm confident all good C++ devs that I know would do the same. The key is to be familiar with the algorithm, and to think in terms of it, i.e. think "this is a fold" rather than "this thing is something that does this other thing". There are also other benefits: * It's more specific than a loop construct. As soon as I see an `std::accumulate` I already know to a more specific degree what to expect, compared to a loop. That's because loops are used for many, sometimes complex, things, whereas `std::accumulate` always folds a (possibly virtual) collection into a single value, 100% of the time (abuses notwithstanding, like exception throwing). This helps me quickly understand the intent of the code, and the rest of the code merely serves to 'fill in the blanks'. * It's also not as overpowered (no `break`, no `continue`, no `return`), which is good, because it feeds into the aforementioned principle of writing specific code, and building meaningful abstractions, as opposed to having all code written in terms of generalized while-if-switch-else-for-if-else jibber-jabber. Meaningful code aids comprehension, reduces fatigue and speeds up development. The up-front cost of learning things is oftentimes worth it. * Better const-correctness – the accumulated value would have to be mutable while being 'built', when using a raw loop. * Keeps the code at the same abstraction level. Whenever a raw loop is used, I find that it describes the *how* of something. *How*s represent drops in abstraction levels, which should be in separate functions, which represent extra code indirection for the maintainer to keep track of. `std::accumulate` in contrast, has a name that describes *what* it does (doesn't drop into the *how*), is part of the STL so it can be expected of the maintainer to understand it, and introduces no need for a separate function. &gt;Am I totally off base here? You're right about the fact that it may be something new to learn, but so are many useful things. I'd say the reason you're averse to folds is because you're already familiar with a way of doing what's presented, and I understand that feeling. However, when coming across something new, even if it solves an already solved problem, it's worth keeping in mind the fact that it's something that may have its own set of advantages which you may not yet perceive, in spite of whatever disadvantages you may already perceive. That's why it's worth framing your question more like "what are the pros and cons of folds vs. raw loops?", rather than "I don't know why I'd want this." (the implication of the latter is that you may have already decided you don't want this) – this kind of phrasing is why I think you got downvoted btw. My take is that maybe you're not (yet?) familiar enough with certain parts of the STL, particularly algorithms. However, the STL is as much part of the C++ standard as while loops are, and is well worth learning and leveraging. For example, you can define quicksort in terms of the partition operations found in `&lt;algorithm&gt;`. As a side note, it's also quite enlightening to discover the details of algorithmic complexity embodied by the STL – for example, some algorithms have particular iterator requirements, some may have different performance guarantees, depending on the iterators they're provided with. This provides a look into the way the standard expects them to work. 
I believe it's the opposite; they're tweaking &lt;atomic&gt; for backend bugs.
One way is like this: Console::Out() &lt;&lt; "Value: " &lt;&lt; value &lt;&lt; endl; where `Out()` constructs a temporary object implemented such that `operator &lt;&lt;` concatenates into a buffer. The destructor of the temporary object, then, must check if `std::unhandled_exception` returns true. If it does not, the destructor is being executed as part of normal forward progress, and can dump its buffer into the console. That's my solution, at least, for the multithreaded console output problem.
It is wrong to expect you can pre-plan for something like that. Demands change over time, for example, code bases can originate on one platform, then a new platform can emerge (which didn't even exist when it was architected) and you need to port it. Or someone might decide, "hey, this component of that sourcebase would actually be useful elsewhere, lets extract it as a library" (oops , it's too coupled because of it's classes, we can't, without farting around). A good language allows code to be refactored. C++ has this hazard where certain organisational decisions are baked into the syntax.. foo(a,b) vs a.foo(b), and everyone prefers the latter for reasons explained many times over. If we could just use that syntax everywhere, these problems would go away.
&gt; &lt;exception&gt; promises not to throw exceptions, except when it should.
Is this the real Billy O'Neal?
&gt;iostreams is also very slow compared to other alternatives Have you tried turning off synchronisation with stdio?
No inline asm yet, sorry. 
I like this. I might even make `Console` class return a temporary object for the first `operator &lt;&lt;` call, and then go from there.
If by "the real Billy O'Neal" you mean "the Billy O'Neal that did lots of work mentioned in that Update 2 blog post" then yes
Our iostreams implementation is mostly still Dinkumware's; but there are lots of places where we have diverged from Dinkumware now.
That is expected. Update 2's STL unconditionally attempts to use the `__make_integer_seq` compiler hook for Clang, as we support using the STL with Clang/C2 (currently 3.7 based but with this hook backported). If you want to use Update 2's STL with Clang/LLVM, you will need a recent build that also supports the hook.
What is the best way for us to report compiler crashes in codebases that are very large and we cannot distribute the source, if we cannot isolate it into a smaller test case?
Ok, thanks. I will, however, file a bug report for this regression when compiling with MSVC. `std::index_sequence&lt;I...&gt;` isn't deduced correctly in partial specializations: #include &lt;utility&gt; #include &lt;type_traits&gt; // first template parameter of foo is necessary to reproduce // the bug, which only occurs in partial specializations template&lt;typename, typename IncorrectlyDeducedType&gt; struct meow; template&lt;typename T, std::size_t... I&gt; struct meow&lt;T, std::index_sequence&lt;I...&gt;&gt; {}; template&lt;typename T&gt; struct meow&lt;T, void&gt; { using hiss = meow&lt;T, std::make_index_sequence&lt;1&gt;&gt;; }; // std::index_sequence&lt;I...&gt; isn't deduced from std::make_index_sequence&lt;1&gt; void instantiation(typename meow&lt;void, void&gt;::hiss) {} int main() {} It breaks a lot of my stuff :P 
Note that platforms with *good* streams libraries (like C# and Java) have split streams into separate libraries you mention. For example, in C# there's the Stream hierarchy which handles the byte blob abstraction, there's the TextWriter hierarchy which handles the locale-sensitive stuff and codeset conversion.
I'm still looking for a modern C++11/14 ncurses like library. 
`(a)[i]` is the same as `*((a)+(i))` for all possible `a` and `i`. (If one is ill-formed the other is ill-formed too).
&gt; Frankly, I think it was a mistake to make the index operations unsigned. A problem with signed indices is that if you have an object larger than `SIZE_MAX / 2`, you can't access any of it beyond that without writing weird code. 
What does that mean, can I now type regular expressions in C++11 syntax in the "Find" or "Find &amp; Replace" window of Notepad++?
Has anyone tried the C/C++ extensions in VS Code? How does it compare to a full fledged IDE like VS itself or Xcode?
Scintilla author here. Notepad++ uses boost.regex so this won't affect it.
Are you implying working with binary files is rare? o_O
By default they used boost regular expression library. Now since most c++11 compilers have worknig regex, they made it use that as the default and you can switch back with an option.
That's a classic book. The authors are sometimes called the Gang of Four. A lot of the patterns are really old, and aren't used much anymore or are very niche. While reading the book it can be hard to separate those patterns from the patterns that are still commonly used today. Of the design patterns books I've read, I like *Head First Design Patterns* best. It is very approachable, and uses modern techniques. Most of the examples are in Java, but that doesn't matter. Good OO design patterns work in any OO language. As for whether you should use a raw pointer or a shared pointer, that's not really an issue addressed in a design patterns book. Design patterns tend to focus on the bigger picture of the design and not on the finer details. If you want a book that goes into those details you may want a book more like *Effective Modern C++*. 
Why would you compare to iostreams, when you're just using standalone conversion functions in Python and not something equivalent to iostreams? Such conversion functions exist in C++, too (`std::to_string`, `std::stod`) – use those, and eliminate the borderline-trollish bias.
How about something like this: https://github.com/tobbez/string-splitting
https://blogs.msdn.microsoft.com/vcblog/2016/04/14/stl-fixes-in-vs-2015-update-2/ 
I crave challenges. Thanks. Will contribute. 
You could base your "contributions" on the following pull, as work seems to have been done to make the Python variants run much faster - a fairer benchmark. https://github.com/tobbez/string-splitting/pull/4 https://github.com/zwparchman/string-splitting 
It's not that working with binary files is rare. It's that almost all of iostreams' features are useless when you point them at binary files. At which point it makes sense to use a simpler library that exposes what you need to do; namely, cstdio.
Added two new tests, one simple C loop and one with sse 4.2. They both seem to perform well. Is this fair game or we're restricted to Modern C++? $ ./run_all.bash === System info Ubuntu 15.04 Linux 3.19.0-58-generic x86_64 GNU/Linux Intel(R) Core(TM) i5-2540M CPU @ 2.60GHz g++ (Ubuntu 4.9.2-10ubuntu13) 4.9.2 Python 2.7.9 === End System info 20000000 60000000 866116396 test_data ./split5.py Python: Saw 20000000 lines (60000000 words/806116396 chars) in 20.1 seconds. Crunch Speed: 996857.8 ./split.py Python: Saw 20000000 lines (60000000 words/806116396 chars) in 18.6 seconds. Crunch Speed: 1076444.1 ./split1 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 10.1 seconds. Crunch speed: 1982285.6 ./split2 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 18.9 seconds. Crunch speed: 1057108.1 ./split6 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 2.4 seconds. Crunch speed: 8239912.0 ./split7 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 2.0 seconds. Crunch speed: 10217541.9 ./split8 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 33.3 seconds. Crunch speed: 600356.5 ./split9 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 21.4 seconds. Crunch speed: 935157.4 ./splitc1 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 8.7 seconds. Crunch speed: 2291392.5 ./splitc2 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 7.8 seconds. Crunch speed: 2557167.3 ./splitc3 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 7.2 seconds. Crunch speed: 2764015.3 ./split_subparser C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 1.8 seconds. Crunch speed: 11272485.0 and the new tests: ./splithb1 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 1.2 seconds. Crunch speed: 17022604.6 ./splithb2 C++ : Saw 20000000 lines (60000000 words/806116396 chars) in 0.5 seconds. Crunch speed: 37585202.1 
I am not sure if it is independent in your terms but currently I am using g++ 5.3.1 and it has std::experimental::any
&gt; Head First Design Patterns I like that book very much too, but it's a bit too OO-heavy. Just keep in mind that (Modern) C++ is not a purely OO language and has many other techniques - generic programming, free functions, etc. You can't just translate the Java code 1:1 to C++ and get good C++ code. C++ can do much more. With that in mind, it's a great book. Read _Effective Modern C++_ in addition to that, watch some GN/CppCon talks, and you're good to go.
Those numbers look interesting, but I think the other top 2-3 C++ examples could be in the splithb1 time frame, if they didn't have the std::cin/getline overhead. There are a few issues with your code versions that may no necessarily be in-line with the benchmark: Initially your code will break when the line length is greater than 1MB (static buffer). Also your code is just counting chars and delimiters, where as the other code examples are doing so per line - technically if one isn't doing anything with the tokens then they're both the same thing, but the aim was probably to simulate line-by-line processing - In theory with your code you could just mmap the entire file and run the loop without any freads. Does the warm-up make much of a difference? I tried on both an SSD with 128MB of on-board buffer and a normal platter running at 10KRPM and could not see much difference. I'd love to see the SSE version made to work with quoted strings.
Sounds intresting, will it be FOSS?
If one doesn't care about loc, it's possible to get a huge speed boost by replacing the local object with a custom one like this; https://github.com/cdglove/daily/blob/master/include/daily/fast_iostream/fast_locale.h https://github.com/cdglove/daily/blob/master/src/fast_iostream/fast_locale.cpp Example: https://github.com/cdglove/daily/blob/master/src/fast_iostream/fast_locale.cpp 
I also have the book. You don't want to copy the given examples as you have better tools nowadays in C++11. But the principles still stand as they did at time of release. Some principles might be implemented already by libraries or such. But Design Patterns don't really age. 
I do not agree with everything he says, none the less a very informative [video](https://www.youtube.com/watch?v=rX0ItVEVjHc) about DOD.
If it were me I'd make the size of the SOO sizeof(vector), as storing strings and vectors in such a structure is a common usecase.
I'll have to check this out.
can you compare it with any experience of VS? I'm currently deciding between VS and Eclipse
For the record: about three years ago, I started the following GitHub repository to play with the code from the GoF book: https://github.com/BartVandewoestyne/Design-Patterns-GoF Feel free to use it, improve it, and contribute by sending pull requests.
In vtable_for_type, why do you copy the addresses of the members of vtable_dynamic and vtable_stack in to a vtable_type object instead of just having vtable_dynamic and vtable_stack inherit from vtable_type. Is that to prevent additional virtual method calls?
`sizeof(std::string)` IMO, as it's likely to be larger than `std::vector&lt;&gt;` due to its own SBO.
There's another great book, free to read, called [Game Programming Patterns](http://gameprogrammingpatterns.com/contents.html). The introduction mentions this: &gt; The goal of this book is *not* to teach you C++. The samples are kept as simple as possible and don’t represent good C++ style or usage. ... It does not use the standard library and rarely uses templates. This makes for “bad” C++ code, but I hope that by keeping it stripped down, it will be more approachable to people coming from C, Objective-C, Java, and other languages. I think you should look at the GoF Design Patterns book the same way. It's job isn't to teach you C++. It's job is to teach you patterns.
&gt; I like that book very much too, but it's a bit too OO-heavy. Yeah, I felt like a lot of the design patterns in that book were basically making up for Java's (then) lack of closures. Worth reading though, as long as you think critically about which parts are useful in modern languages.
Could you provide an example that triggers the bad code-gen?
Ahhhh, sudden realization! `atomic&lt;UDT&gt;` uses `lock cmpxchg8b` for everything, so alignment shouldn't matter there, ok? But... struct UDT4b { short a, b; }; atomic&lt;UDT4b&gt; foo; // assume 2-byte aligned foo.store(val, memory_order_release); // not atomic!! ...uses `mov DWORD PTR [foo], val` for the store and absolutely requires 4-byte alignment for correctness! ^^Sorry ^^for ^^pestering ^^you ^^with ^^this ^^stuff, ^^I ^^just ^^have ^^PTSD ^^from ^^dealing ^^with ^^lock-free ^^algorithms ^^and ^^x86 ^^atomics ^^in ^^kernel ^^code.
`vector` seems fine, as it's _always_ 3 words. `string`, however, is a bit more complicated: * On libc++ (clang) it's always 3 words (12/24 bytes). * On dinkumware (msvc) it's 6 words (24 bytes) on x86 and 4 words (32 bytes) on x64. * On libstdc++ (gcc) non-cow implementation it's the same size as dinkumware. Tips on the best balance?
cheers!
We use head first design patterns for my university's design patterns class. That book will always have a place on my shelf. 
&gt; If that problem can't be solved then I think C++ is better off without the feature entirely. I think throwing out a useful feature because you don't like the syntax would be a real shame. The Motivation section cites several tangible benefits designators provide, for efficiency, safety, and programmability. Why should syntax be a deal-breaker?
Because syntax matters. Having clear, logical, and consistent syntax also has benefits for safety, maintainability, learnability, etc.
What you mean debugs? Can i debug Windows programs with this extension? Accoding to the annoucement blog post [here](https://blogs.msdn.microsoft.com/vcblog/2016/03/31/cc-extension-for-visual-studio-code/), debugging is not supported on Windows :( 
**Policy-based design**, there's a great book explaining all the in and out by Andrei Alexandrescu ([Modern C++ Design](https://www.google.com.hk/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=modern%20c%2B%2B%20design)), this ressource is from 2001 so many ideas explained can be simplified with recent feature, but all of the concepts are still very much relevant For instance, most of the STL use policy-based design 
It's hard for me to compare the two, I do most of my programming on Linux, and when I occasionally have to code on Windows using C++, I always use the free versions of VS. This means I can't say what's not available in VS. These are some certain pros for Eclipse though: * The most complete support for the most current C++ standard. Right now that would be C++14, but if the past is any indication, Eclipse will probably always be the first to support them, half a decade before VS. * Cross-platform, Free (duh) * Feels lighter, you don't have to install a massive multi-GB editor, you don't have to remember your accounts in various MS systems and renew your license periodically. Some nice things to mention about Eclipse, without any comparison to VS (since I have no idea about the counterparts): * A very nice git integration, especially for staging chunks. Other features are nice too, like seeing line-by-line what code introduced when in the repo history. * Very powerful content assist. I really abuse my compiler with metaprogramming, using the latest features, but Eclipse can still give me content assist. It also sees through macros. Like, when a macro defines a new type with an argument as the name, eclipse realizes that and highlights that argument as a type. Eclipse's comprehension of the code also helps a lot with navigation, like jumping to definitions or call sites. * Decent refactoring and source generation support. Not stellar, but good considering how messy these things are in C++, like when a macro's argument is a new type name, if you rename the type somewhere in the codebase, that macro's argument is also updated. Cons: * Cmake editor is available as an external plugin, which is somewhat annoying to install. * The editor can be unstable sometimes. You can see visual glitches, or the indexer can silently crash and content assist stops working. In some rare extreme cases, while trying to get content assist for deeply nested sections of code using a combination of advanced features (like a generic lambda capturing multiple variadic template argument packs) the editor can outright crash.
C++ isn't just not a purely OO language. It isn't an OO language at all.
Qt
Thank you for writing such a thorough reply. You have me convinced. The thing that really resonated is the readability argument and the fact that when you read code and see a for-loop, it could be for almost anything. You see std::accumulate and know exactly what it does and there wont be surprises.
I selected the Win8.1 sdk but it doesn't install the debugger, I guess because it's an optional thing in the stand alone SDK installer. Any way to install it with the c++ build tools installer?
Things have changed since three years ago: https://www.reddit.com/r/cpp/comments/1iy7pq/c11_regex_support_is_poor_cant_be_used_in_cross/ C++11's regex can be used cross platform now!
I would note that policy-based design appears more in libraries and less in "final" user software.
At the risk of dampening your enthusiasm... I have seen many people learning about design patterns (via the GoF book notably) and then try to find whether they could use a design pattern (or combine various patterns together) to solve a problem. As the saying goes "when you get a hammer, everything looks like a nail". Design patterns are used to *communicate* (ie, describe a solution), for example, simply calling something `FooObserver` or `FooVisitor` will immediately inform the reader about the usage of the class. Also, while books will present code samples, those are *samples*. A Design Pattern is a *starting point*, to integrate it in your code or with other patterns, you will often need to adapt and modify it. As a result, whether a raw pointer is used in the sample matters little; maybe in one situation it would be best to use a reference, and in another situation a weak pointer, a shared pointer, ... What really matters is to understand the advantages and disadvantages of the pattern, so as to be able to use it when appropriate and leave it be when it's not.
Simple curses alternative: https://github.com/nsf/termbox
I would put it like this: C++ has many things and one of them is OO part
It depends on other headers, not just `any.hpp`.
Other than company-wide inertia/licensing costs, is there any good reason to stay on 2008 and not move to at least, say, 2012?
Not for companies with over 250 employees.
Honestly I don't want to hear such nonsense. To many Luddites in a project makes for sour community relationships. Just look at the stupidity of the people resisting the move to Python 3. Whining about a compiler from 2008 is just stupid. 
Usage For individuals Any individual developer can use Visual Studio Community to create their own free or paid apps. For organizations An unlimited number of users within an organization can use Visual Studio Community for the following scenarios: in a classroom learning environment, for academic research, or for contributing to open source projects. For all other usage scenarios: In non-enterprise organizations, up to five users can use Visual Studio Community. In enterprise organizations (meaning those with &gt;250 PCs or &gt;$1 Million US Dollars in annual revenue), no use is permitted beyond the open source, academic research, and classroom learning environment scenarios described above.
 &gt; one that's died out is Singleton what's the fashionable mechanism for dealing with the kinds of problem that Singleton solved. 
A very useful read... I've had exactly these issues already several months ago when VS2015 (Beta or RC or something) came out. Hmm, somebody writes in the comments: &gt; DependencyWalker still works good enough for me since ApiSets are not something any “normal” application will link to. If you are only searching for build inconsistencies it is still sufficient. But if you are after bugs in OS patches then things could get interesting That's not true, is it? I think any app compiled with &gt;=VS2015 will have this problem. It would be great if somebody would come up with an updated (or new) dependency walker?
Those headers can be easily replaced. They are there for convenience of the implementation. If you look at the source you'll see it's mostly stuff like using the any without any RTTI enabled, or using the `_t` typedefs from type traits. 
I think it still updates with each new windows sdk, but it's probably only to keep it afloat. A bug I had wasn't fixed in ages. Still, helped me figure out weird dependency issue with dlls, that linked to other dlls, that linked to a plugin dll that failed to load.
The goal of Singleton is to guarantee that there can only be one instance of a class and that instance is easily accessible. Guaranteeing that there is only one instance is rarely useful, but it does happen. Making the object easily accessible is convenient, but if the object is mutable, it effectively means there are hidden side-effects in any code that uses them. Simply passing references to the relevant objects as needed makes the side-effects easier to reason about, and avoids hard-coding the assumption that there can only be one instance into a lot of code. This can lead to many parameters being required for functions, but this problem can be reduced by proper grouping of parameters into higher-level objects. See this Stack Exchange post for more details: http://programmers.stackexchange.com/a/40610/63172
"not worth the effort" afaik.. there are multiple versions of the application, so I currently use VC 2005, VC 2008 and VC2012. I tell you, making bugfixes which are supposed to be merged into all of those version is so much fun
You would think that this and process explorer would be integral parts of visual studio but for some reason microsoft keeps piling on bloated features and awkward interfaces instead of polishing or maintaining essentials. 
That's unnecessary and highly counterproductive. Here are the features that VC 2015 Update 2 are missing (ignoring dynamic exception specifications, which are deprecated and widely reviled): * (C++98) Two-phase name lookup * (C++11) Expression SFINAE, partial support * (C++11) C99 preprocessor conformance * (C++14) Extended constexpr * (C++14) NSDMIs for aggregates * (C++14) Variable templates don't work in SFINAE (already fixed for Update 3) Our C++14 Standard Library support is complete, with very minor omissions (e.g. due to lack of extended constexpr), and 8 Library Issues which are bug-level things. Restricting yourself to C++11 still requires you to be aware of a few missing features, and you're giving up highly useful stuff like: * Lambda init-captures * Generic lambdas * auto and decltype(auto) return types * make_unique * Alias templates for type traits
It was. Then each and every release of VS , OS tools, SDK and DDK release it has to go through QA again. There is a point where the owner does not want to push it into another release. If you go to the dependency walker web site, you would read it is "for testing", which means users are supposedly lab rats testing the product. There is no Microsoft guarantee on the product's quality.
Sure, code is changed all the time, but the need to convert methods to standalone functions and standalone functions to methods never arises. 
disagreement over what should go into classes is continuous; and as source bases evolve, the issues around each decision can change, since the factors affecting each decision change. (i) Most programmers prefer using methods where possible: because autocomplete makes it vastly easier to use; discoverability - no one can hold a large source base in their head, and it makes it easier to move people between projects , and between different areas of the same project. Also the syntax is more readable. Most functions are not commutative, you gain value separating the arguments (a trailing preposition in the function name can apply to subsequent arguments rather than the first). They're easier to read &amp; write. (ii) but free functions are superior for decoupling (hence making portions of source bases re-usable). One scenario is writing a piece of code in one project, under the assumption it wont be needed on another (hence it's ok to use member-functions more), then discovering a new demand for a similar module, hence wanting to decouple it , hence needing to revisit that decision, hence needing to fart around reworking all the calls. If we had UFCS we could just use the superior method-call syntax *everywhere* and: (i) such refactoring would be easier to do (ii) we wouldn't have to waste time arguing over what does &amp; doesn't go into the class in the first place, since the refactoring cost if you get it wrong would would be vastly lower. It's so stupid that we have to waste time on this , when it is just syntax. Something else that happens is a decision toward or away from virtuals. They're unusable in some performance sensitive situations, but they're very intuitive for writing composable code without having to template absolutely everything
Anyone aware of an open source version of DependencyWalker?
&gt; list.size() It might be (and in most cases is) O(n) 
&gt; How would you solve const correctness in your reflection engine without needing to register both the const type and the mutable type? I'm not sure yet, I have different ideas in mind. Whoever, registering both alternatives is not a problem (Registration of types is done implicitly by indirect calls to MetaType::get&lt;T&gt;() (Most come from MetaObject ctors). So like it now calls MetaType::get&lt;std::decay_t&lt;T&gt;&gt;(), I could call MetaType::get&lt;std::add_const_t&lt;std::decay_t&lt;T&gt;&gt;&gt;() too. &gt; Also How do you handle existing objects whose lifecycle may not be managed by your MetaObject? The real implementation of metaobject covers more cases. For example, it has a `isReference` flag to keep track of references to existing objects (So the managed object is not released on MetaObject dtor). Existing non-managed objects can be simply wrapped as references. See https://github.com/GueimUCM/siplasplas/blob/master/include/siplasplas/reflection/detail/metaobject.hpp#L51 &gt; Is it possible for you to choose the correct method signature for a reflected method call depending on the input arguments (especially when considering overloaded methods ?) Good point. I'm working on a runtime mangling protocol to handle overloads. &gt; I used a lot of type traits to generate the reflection information which when using it for over 800 classes caused the compiler to crash because it used more than the allowed RAM for a single process. I wrote a tool based on libclang that scans for tags like: $(enable_reflection) class Foo { public: void f(); int a; }; and dumps all the collected class information to a C++ header (A template specialization per class). One of the first things I did was to track file changes with a timestamp database, so is not that horrible to run in most cases. The tool, called "Dynamic Reflection Library Parser" (DRLParser, a joke from biicode days https://github.com/biicode/common/blob/45e9ca902be7bbbdd73dafe3ab8957bc4a006020/edition/parsing/cpp/drl_parser.py :) ), will be covered in following posts. 