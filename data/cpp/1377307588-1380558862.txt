You don't need `then` to make futures a monad. Here's (more or less) how FTL does it right now (there are other ways of doing it too, particularly if you prefer the default launch policy to be async): Return/pure is simply a deferred computation returning the input: future&lt;T&gt; pure(T t) { return async(launch::deferred, [t](){ return t; }); } Map/fmap applies a function some time in the future: template&lt;typename F, typename U = result_of&lt;F(T)&gt;&gt; future&lt;U&gt; map(F f, future&lt;T&gt;&amp;&amp; fut) { return async(launch::deferred, [f](future&lt;T&gt;&amp;&amp; fut){ return f(fut.get()); }, std::move(fut)); } Bind sequences two future computations and puts them off to some future time: // concept_parameter is for extracting the "interesting" type parameter of a parametric type template&lt;typename F, typename U = concept_parameter&lt;F(T)&gt;&gt; future&lt;U&gt; bind(future&lt;T&gt;&amp;&amp; fut, F f) { return async(launch::deferred, [f](future&lt;T&gt;&amp;&amp; fut){ return f(fut.get()).get(); }, std::move(fut)); } Oh, and there is of course automatically derived implementations of `apply` and `join` as well, allowing one to e.g. write applicative style computations with futures. Combined with the currying facilities offered in FTL, it's actually pretty neat looking. The only issue is that the launch policy has to be hard coded, because unfortunately there is no way of extracting it from a future. Well, that, and the fact that futures aren't copyable, so you _have_ to use move versions as above. Anyway, I don't make a distinction between map and fmap in my library either, one's just the generalisation of the other after all. In general, I've tried to remove redundancies like those as much as I can.
If you change your ID3D11VertexShader to a ComPtr, try changing the &amp;solidColorVS in CreateVertexShader(...) to solidColorVS.GetAddressOf() and also change any place you use solidColorVS to solidColorVS.Get(). I hope this works for you. 
Right, I was just giving one example. The beauty of the "monad", "functor", et al design patterns is that they seem to occur naturally, and there's sometimes plenty of ways for a type to meet the criteria. For my code I simply required that Type&lt;T&gt; have a unary constructor and a bind() operation, so I used future&lt;T&gt;'s then() as a kind of bind() in my explanation. I love seeing functional programming ideas and designs make inroads into C++.
Well, it's a refinement of [Boost.Optional](http://www.boost.org/doc/libs/1_54_0/libs/optional/doc/html/index.html) which has been around since version 1.30.0 (2003-03-19). And apparently there was a [2013 Google Summer of Code project to implement Boost.Expected](http://www.google-melange.com/gsoc/proposal/review/google/gsoc2013/trademark/25002) which should be wrapping up relatively soon. There's also [this github repo](https://github.com/martinmoene/spike-expected#readme) that contains code and tests for a half-dozen variants on the idea.
Optional is a completely different thing. An Optional&lt;T&gt; function can return None, even when it succeeds. An Expected&lt;T&gt; function only returns nothing if there is an error. The two use cases are very different.
&gt; The two use cases are very different. I think they are sufficiently similar. Andrei’s slides shows them as alternatives. They solve the same kind of problem. Sure, they’re not completely equivalent but in practice they will act as alternatives for almost all use cases.
No. They both share an idea- that a T may or may not exist. The meaning of that nonexistence is completely different. In the Optional's case, I just move on because I don't really need that value. In the Expected's case, I fail myself. The logic you employ if there is no ~~spoon~~ T is very different. Also, I'm watching the talk again, and it really seems to me that he simply doesn't have a good core understanding of exceptions or error codes. I mean, he outright says at the start of the video that he doesn't really know that much about exceptions. So I'm struggling to see why anyone would take his advice. Functional languages use Expected&lt;T&gt; because they don't have exceptions, and Expected&lt;T&gt; basically boils down to exceptions but implemented as a library instead of as part of the language.
Calling them completely different is stretching it. The reason why Expected&lt;T&gt; isn't commonly used is because boost.optional is good enough for most use cases.
We use a class called Status for error codes + strings. We use StatusOr&lt;T&gt; for return values. It is a little tedious, but it works well.
One problem is that an exception alone doesn't help you when you want to pass data around between different threads, as the exception would go flying in the generating thread, not in the receiving one. `std::future` somewhat addresses it, but doesn't really fit when your data passing is based on request/receive style callbacks between the threads.
I think the Exception-template could use one more member: operator T() &amp;&amp; { if(gotHam) std::rethrow_exception(spam); return std::move(ham); } This should work thanks to the r_value-qualifier. Or does anyone disagree?
Stripping it down, isn't he basically asking why C++ isn't Java, that is to say, garbage collected and without explicitly stack allocated objects? Well, because garbage collectors are still a huge problem.
He's closing in on arguments for a "one true language" for all programming.
[I recommend reading the discussion on Proggit.](http://www.reddit.com/r/programming/comments/1kyz60/c_and_the_culture_of_complexity/)
That would render the type moot. At least make the conversion operator `explicit`, otherwise all the type safety offered by `explicit` is lost.
LLVM uses something similar: http://llvm.org/docs/doxygen/html/classllvm_1_1ErrorOr.html#details
I don't think so. The important advantage of this version is instead, that it would enable you to use the results of these functions as if the function would return T: Expected&lt;int&gt; a(); int b(); int x1 = a(); int x2 = b(); but: auto y1 = a(); auto y2 = b(); int z1 = y1; // works int z2 = y2; // does not work, you need to call get(). and: short s1 = a(); // works short s2 = b(); // works I think these are pretty much the semantics, that one would like to have.
&gt; int z2 = y2; // does not work, you need to call get(). Of course. But I don’t understand why you think it’s safe to allow the conversions from a direct call to `b`. It’s no more safe than the implicit conversion you want to forbid. *Both* need to be explicit to provide type safety.
Programmers in VB go the other way. =(
final.sln
I actually just recently added a bunch of tryGet() / tryOpen() functions to my game engine. Where get()s and open()s return either an object (or a reference) or throw in case of errors, try# versions return Expected&lt;&gt; of appropriate type. Before that design I used to use boost::optional for try# methods but it is not a good solution for when boost::none is a correct return value (as an iterator to end() is a correct result when using std::find). Expected&lt;&gt; clearly makes a distinction: either result is correct, or something "unexpected" (but not really;)) happened (I so much more like "unexpected" to "exceptional") Which I don't quite like about Expected is the very same thing I don't quite like with std::future: 1) you may easily miss the fact that something funny happened in the process (it is much harder to miss throw exception) 2) catch site of both Expected&lt;&gt; and future&lt;&gt; may be quite away from throw site. Unless get() is called immediately in the place where Expected&lt;&gt; is returned (which kinda misses the point), call stack is completely changed and it may not be easy to track the reason of exception being "thrown"
I liked the first half, then disliked the second half. I sometimes find myself adding *needless* complexity, I and other programmers should work to avoid *needless* complexity. Sometimes you just can't do a thing in a language like Java, C#, Ruby, Perl, etc... Sometimes you just need that complexity or the task just cannot be completed in the constraints required. For example, what other language lets you efficiently send bits directly to hardware and create an object oriented abstraction for that?
I have an answer for this... Applied in practice? Yes, I have a fully working C++11 version (as yet unreleased) with no drawbacks (e.g., those contemplated in Alexandrescu's talk) for some time now. My version works with all lvalues, rvalues, cv-qualifiers, and has no restrictions on type T or the type of exceptions it can capture. The code is one header file and is header-only. I consider the expected&lt;T&gt; code itself finished save possibly adding noexcept on some functions. Coding a complete and correct solution was not easy. My version is ~40 KB of code. I will be (i) releasing it as open source and (ii) I would like to formally propose it be added to the C++ standard (full source code and all) as it is really easy-to-use and extremely useful code, I believe I have covered every case that needs to be handled, and it should be in the Standard Library in my opinion. If anyone has any advice to offer concerning what is involved concerning (ii) beyond http://isocpp.org/std/submit-a-proposal, please do. Just in case, some might wonder about a use case. Here's one... When applying a function template over a std::tuple instance's elements, one can easily defer the handling of any thrown exceptions from within the function calls until AFTER all of the elements have had the function applied to them (since the return value of the function can be captured in an expected&lt;T&gt;). (NOTE: Amongst other features, I also defined expected&lt;void&gt; so void returns could be handled.) In effect, one can then easily guarantee the aforementioned function will always be applied on every tuple element even if exceptions are thrown. :-)
It does not decrease type-safety compared to returning the value directly from the function and throwing the exception fro it. In fact it behaves exactly like that, which is why I want it.
This conversion only exists if Exception&lt;T&gt; is an rvalue. ham is always an lvalue as it resides in the union. I agree that one might want to write operator T() &amp; too, but this would be even more controversial.
But it *does* decrease type-safety comparing to returning an `expected` value which cannot be implicitly converted. And thus renders using that type moot. Once you can directly assign like that, you might as well omit `expected` completely.
&gt; For example, what other language lets you efficiently send bits directly to hardware and create an object oriented abstraction for that? Ada, Object Pascal, Delphi, Modula-3, Oberon, Component Pascal, ... There are quite a few that can do that, sadly they share the sin, for some programmers, of following the Algol syntax instead of curly brackets everywhere. Please note that I know C++ since 1993 and when I need to choose between C and C++, unless obliged to do otherwise, my choice is C++. However, I would have preferred other languages with stronger type capabilities, modules and so forth would have taken C++ place. It is not what happened, so now we have to make the best of it. 
He's right about me. I'm attracted to complexity and I let it permeate my work for the wrong reasons.
To be fair, Delphi *is* Object Pascal, and rest (Modula-3, Oberon, and Component Pascal) are closely related. (just pointing out that these are not the best examples if you want to demonstrate that there are many different languages capable of doing what the previous commenter asked).
Not if you take care of all possible types T can have. :-)
He's got some points: - In C++ it's much harder to argue about what a piece of code does, you need to know much more context to understand it. x=y is the canonical example, but there are many many others - C++ is riddled with a less-than-fortunate heritage - After seeing some talks about the details, gotchas and pitfalls, I am myself not quite certain whether rvalue references were a good move (pun accidental) He's overlooking a few things, though: - he assumes accidental complexity is primarily a result of prgorammers attitude - he's viewing shared_ptr through java-colored glasses - he still needs us to write his fancypants garbage collector
I think that I get your point now. I think that you want to use Expected for something which it was not designed for. The type you want is more like this: template&lt;typename T&gt; class strong_typed { public: strong_typed() = delete; strong_typed(const T&amp; val) : value{val} {} strong_typed(T&amp;&amp; val) : value{std::move(val)} {} explicit operator T() const &amp; { return value; } explicit operator T() &amp;&amp; { return std::move(value); } private: T value; }; Of course this could be combined with Expected: Expected&lt;strong_typed&lt;int&gt;&gt; foo(); // completely untested, so I am not sure, if it works: // I'm not entirely sure whether the implicit conversion of Expected allows // the second from strongly_typed. int x = foo(); // this should never work: long x = foo(); 
&gt; In C++ it's much harder to argue about what a piece of code does Yeah but a lot of that is ultimately about garbage collection. You can't just make assignment universally alias the RHS because the thing might pop off the stack and destruct. But then again you might not want it to be a deep clone operation. You wind up needing finer control.
Take your strong_typed&lt;T&gt; example earlier on this page. Regardless of whether or not the cast operators are explicit, they will not work in general with arbitrary types for T with all possible use cases and what they should be permitted to be cast to in terms of correctness and efficiency. For example, strong_type's rvalue this cast operator will fail to work with strong_type&lt;T&amp;&gt; for example. The general solution for this is not simple because, in part, it also needs to address the various items needed to properly implement expected&lt;T&gt;: the solution must work with all types and code using it (especially since templates are everywhere in C++) otherwise using expected&lt;T&gt; becomes much more complicated to use efficiently and correctly instead of being brain-dead super-easy to use. Brain-dead super-easy was my goal and is what is needed for expected&lt;T&gt; to maximize the number of people able to use it --including new programmers to C++, i.e., nobody should have to resort to using C++ template metaprogramming or &lt;type_traits&gt; to use it at all. Anything less IMO is unacceptable if it is to be (potentially) included in the ISO standard.
This is the problem, not just with C++ books but with all language-oriented books in general: The author(s) are incapable of writing informative narrative backed up with meaningful assignments that allow you to practice and demonstrate the concepts they just tried to teach you. Instead you get endless chapters of yammering. It's like listening someone at a party who talks incessantly and never lets you get a word in edgewise. Therefore, you will need more than 1 book to learn C++ as it is an extremely complicated language. Like Haskell, I do believe that certain aspects of C++ are so complex as to be completely undocumentable. There are parts of the language that will simply never be fully understood. I'll warrant that Stroustrup himself would be hard pressed to explain what the best practices would be for some of them. Anyway, at some point you will have to buy Stroustrup's book. That's just the way it is. It is authoritative, and, about as *interesting* as a 25 lb bag of fertilizer. Nevertheless it is the definitive reference, so you better start saving up if you're on a small budget. Both of the Primer books are pretty good, as is the Sams "teach yourself" book (be sure to find the last edition I think it is 7). In general, the older the book, the more you will have to supplement it with a more up-to-date description. That is why I recommend that you go for a fairly recent beginner book, and then read Stroustrup for the final word. Good luck. You're going to need it. 
Luck? I got Physics. If I could get through Thermal with Kittel, I can get through anything they can throw at me. HAHAHA! Ah ha.... Ow, I just remembered Thermal Physics with Kittel. Few things can be worse than that. Anyways, would you recommend that I just flat out start with Sroustrup's book to save time and money? 
If you have zero other experience the best you can do is picking up some knowledge from a book and then practicing it a lot (just like in math and physics). After you have practiced some I suggest you have another, very careful look at your solutions with Scott Meyer's *Effective C++* close by.
Problems from past Topcoder competitions are at [various levels](http://stackoverflow.com/questions/3010376/how-to-get-started-with-topcoder-to-update-develop-algorithm-skills) and can be done in a number of different languages, including C++. The easier problems can generally be done in 30 minutes or less, and even the more difficult ones (if you can figure out how to do them) shouldn't take more than a few hours.
I know. That's the list this reddit recommends. I was saying that I don't know which one is appropriate for me. 
But I do have experience. Please read my post.
"C++ Primer" is very good in that it gives you a good introduction to C++11 at the same time. Quite importantly, from the books currently available on the market, I believe it does it best, since it integrates it smoothly across the entire text (not as a bunch of sections lazily tacked on at the end), see: http://www.drdobbs.com/cpp/c-primer-5th-edition-part-1-how-to-revis/240003977 (or, to give another example, the preference to use and introduce smart pointers *immediately* when dynamic memory management is introduced -- instead of using raw `new` for hundreds of pages (like "C++ Primer Plus", NOT recommended; or almost any other "general C++ introduction" book as of today, for that matter) which is nowadays considered a bad practice (doesn't express ownership semantics, prone to leaks) and [frowned upon](http://klmr.me/slides/modern-cpp/) in a general application code). As a consequence, "C++ Primer" will cover C++11 features like regular expressions or random number generators (working with new statistical distributions classes). Notably, however, it doesn't cover multithreading (even though that's also an important feature of C++11), so for that you'd also benefit from reading a topic-specific book. *// I'd recommend [C++ Concurrency in Action](http://www.manning.com/williams/).* Personally, I found it more easy-going than TC++PL (which is more reference-style at times; still, worth reading to get the big picture). "C++ Primer" also goes quite far (like implementing your own `operator new` and `operator delete` for `std::allocator`--in Chapter 19, etc.). OTOH, AC++'s pace is somewhat faster (it doesn't cover C++11 though, naturally), while "C++ Primer" is more in-depth (e.g., you won't be implementing your own `operator new`/`operator delete` for `std::allocator` in AC++ -- however, in contrast to almost every other C++ book, you *do* actually use `allocators` in both, (Chapter 11 in AC++, Chapter 12 in C++P), in your own classes--which only goes to show that AC++ for its size and language standard is absolutely unbeatable). For instance, AC++ introduces `static` storage specifier in Chapter 6 (around page 105), when it's needed for storing a certain pattern of characters (for (sub)string search purposes) while writing a function to find URLs in a given input text. "C++ Primer" introduces it in Chapter 6 (also, but around page 200), right next to local variables (and right after talking about automatic objects) in a function call counter example (the simplest kind of instrumentation). // Since you have programmed before, this probably won't be a big deal for you, but C++P example is arguably more "abstract" (general-purpose programming rather than simple-domain-specific-application example--this difference also occasionally shows up in the exercises), but at the textbook level this distinction is very slight anyway and it doesn't apply at all times (e.g., both books include "applications" like working on a Student/Quiz-style class). In comparison, TC++PL goes the most in the general-purpose programming direction (a function `f` calling a function `g` in order to do something with a variable `x`, etc. -- I think the most specific, down-to-earth application is the famous desk calculator example). NOTE: this does **NOT** mean that C++P goes slowly because it's for beginners new for programming. Quite the contrary -- it is a larger book, since it covers things in-depth / in great detail, talking about potential corner cases, good practices, advanced features, etc. -- things that will only be appreciated by intermediate programmers (so if by "absolute beginner" you mean someone completely new to programming, I think [PPP](http://www.stroustrup.com/Programming/) would be more appropriate). This is in stark contrast with books that go slowly only because they spend lots of time (and page count) on talking about simple things, like the syntax of a `for` loop and whatnot. In fact, C++P has you using third-party (source code supplied) classes already in Chapter 1, while AC++ doesn't introduce loops until Chapter 2; both, however, don't waste any time on talking about simple syntactical details for too long, move on after a proverbial page or two, and get you to actually use the stuff for coding (which is a good thing for not-new-to-programming audience).
Huh? You wrote nothing about experience with C++ (and Java and C experience might work slightly against you).
Thanks! I have one question though: I keep running into a C++ Primer + 6th edition online, written by someone else. I assume its a completely different book and not an updated edition the publisher had written by someone else? 
Well, it was technically a C++ course, except we never got that far into what defines C++ form C; we didn't get into objects.
Say away from C++ Primer Plus. It's a crap book that will teach you bad style. The one you want is C++ Primer by Lipman and Moo.
&gt; I keep running into a C++ Primer + 6th edition online Yeah, just like /u/bstamour mentioned, a completely different book, avoid, that's the one I've mentioned here: &gt; using raw `new` for hundreds of pages (like "C++ Primer Plus", NOT recommended (just one example of the bad style it uses).
I agree with him on the point of seemingly-needless complexity. I've been using C# pretty much since it came out, and it's all I usually work with. The past few months I've been getting into DirectX and C++ (I knew the basics of C++ but I'd never written anything purposeful in it, so I'm using a game engine to drive my learning) and holy shit are some things needlessly difficult. Off the top of my head: 1) Determining if a std::string begins or ends with a specific string. You have to use a weird and unintuitive function or Boost. 2) Casting std::wstring to std::string and vice versa. Another process that is easily achievable in a few lines of code but I can't think of a single reason why it wouldn't be part of the standard library. We have std::string.c_str(), why not string::w_str(), etc.? 3) Includes and libraries and setting up additional directories to support them. Pain in the ass. I long for some kind of module you can just reference. Another thing I don't get: typedef. This seems like it should be an 'evil' keyword that no one uses without good reason. You're just aliasing a type name and hiding its true type. Like, "okay, I see a variable of type 'matrix'... There's no 'matrix' type in DirectXMath, where the hell did that come from? Does he have a custom implementation or something I missed? OH IT'S JUST A TYPEDEF'D XMMATRIX? Go fuck yourself!" (which in reality is probably easier to figure out, but not so much when you're reading a book with no Go To Definition button)
Ah, what a dirty little trick!
You're welcome, but, respectfully that is strictly a conclusion of your own --not one of mine-- and it is also a completely different topic than the one here.
Whichever one you get make sure its about Turbo C++ because thats what everyone uses. (kidding!)
"The C++ Programming language" or Bjible as some call it is more of a reference to me. Yes, it can serve as a tutorial, but I would start with Bruce Eckel's free "Thinking in C++" books. This is what my first C++ course used, and I really enjoyed them. Next I would look into books on subtopics, such as generic programming(alexandrescu), templates and anything by Scott Meyers
I'd like to see the few lines of code than can turn an arbitrary `std::string` into a `std::wstring` and vice versa.
github repo: https://github.com/IntelLabs/NOVA 
What's the difference with ESXi?
;-) I actually don't know whether that was done on purpose, but... * C++ Primer: "1st Edition: Published June 1, 1989 by Addison-Wesley" // http://en.wikipedia.org/wiki/C%2B%2B_Primer * C++ Primer Plus: "Waite Group Press,U.S.; 1st edition (May 1991)" // http://www.amazon.com/Primer-Plus-Yourself-Object-oriented-Programming/dp/1878739026 ...so, the plot thickens! ;]
Unfortunately, I've seen things like '==' be overloaded to do unintuitive shit. One example: I was looking at some code that used '==' on a smart pointer and then checked if it was the same later. I thought this code couldn't be doing anything until I found an '==' overload that was doing something like "ptr1-&gt;id() == ptr2-&gt;id()". I flipped out a little. I like operator overloading, but you do have to keep people from changing what the symbols mean.
Until, that is, you're looking at a use of "UserNumbersList" and don't know WTF it is...like you assume it must be, I don't know...a *list*.
1) That would work well for starts with, but probably not ends with since you'd have to calculate n for an arbitrary string. The unintuitive solution I was referring to was: return mismatch(endsWith.rbegin(), endsWith.rend(), source.rbegin()).first == endsWith.rend(); Which I understand why it works, but I doubt I'd have come up with that solution in any reasonable timeframe were it not for google. This is probably because I'm not intimately familiar with the standard library, but to have such a common operation available for a std::string seems like a no-brainer for me. It's easier to read and figure out what the hell it does. 2) Yeah, between you and the other comment I now get why this isn't a standard thing. 3) The pain-in-the-ass I'm referring to is having to configure your project for each library. Some are header-only and you can just add its regular directory to your include directories, others want you to only link a certain directory within the library. Some require you to link specific libraries, some require various flags--there's no "standard" or easy way of adding a reference to a library. I have to read through the docs to make sure I've referenced everything properly so I don't wind up with linker errors. Defining a common module-like interface for referencing libraries seems like a very good idea. Is it rocket science to add a library to your project? No. Are linker errors indecipherable to the point where if you fuck up you'll have no idea how to fix it? No. But would it make things easier and simpler? Yeah. 
Indeed, but like I said, it was just a quick little example. I didn't really put to much thought into it and that isn't likely a name I would use anyway :)
Can some one explain to me why they are using #define macro(x) do { printf("%s", x); printf("%x", x) } while (0) Why not simply using this instead? #define macro(x) printf("%s", x), printf("%x", x) 
It is true suffix could be easier with a more fleshed out string class, but here's a slightly cleaner alternative to that mismatch solution: return std::equal( endsWith.rbegin(), endsWith.rend(), source.rbegin() ); http://ideone.com/Eq9E4k
&gt; Name a language that some other than you cares about. **Ada** Used by : - European Space Agency - Boeing - Airbus - European Train Control System - BNP Paribas - ... So it seems I am not the only one that cares about them.
Not if you do this: #define macro(x) (void(printf("%s", (x))), printf("%x", (x)))
You can actually redefine the meaning of operators on types that aren't user defined?
God this code is so ugly I was about to barf. F*ck macros.
In older MSVC's the default settings for compilation and debugging (specifically, "edit and continue" for debugging format) caused `__LINE__` to not be a constant at all. That's why it wasn't complaining about the value being a constant - because it wasn't.
If I do this in VS2010 it doesn't generate the warning: do { ... } while (false,false); As a bonus, it optimized away to nothing in Release.
Wouldn't targeting openmp for something like this be best?
I think he miss the point of the C/C++: it is more a portable assembly than a abstract description of the program. If your code is complex, it's because your program try to do something complex for the hardware and this complexity has reflected on the language (C++ only tries to compact and hide it a little, but you cannot escape it forever). But I agree, a more complete standard library could better, and better tools to test our program would be great, C++11 was a huge step, hopefully C++14 will be great as well.
If you want your macro to behave like a function call, use a fucking function.
Try asking over at /r/cpp_questions 
Of course they are. In fact, they're so easy that they're the only one of the Design Patterns that people talk about.
Hm. One really ugly bug has me healed from those singletons, problems might occur when destruction time comes... Also is the order of the static initialization/destruction in a program in C++11 still not really defined? 
It should be noted that neither std::dynarray nor runtime-sized arrays can be class members.
I remember a few years ago reading something by a guy named "Noah Roberts" that implemented polymorphic cloning using some sort of private inheritence scheme and maybe a pimpl pointer? Can't find it now unfortunately.
[Would be better with this](http://cpp-next.com/archive/2012/10/using-strings-in-c-template-metaprograms/)
Despite being an antipattern.
Not sure if the author is here, but this article would be a better read if the formatting weren't so totally terrible. First, each time he mentions a C++ construct within English text, it's surrounded by huge number of spaces - but then conversely, all the indentation is lost in C++ blocks so reading the blocks of C++ code is quite hard. A shame - this is something I'm interested in but after trying to puzzle my way through the first coding sample I dropped out. EDIT: actually, the issue with the C code is that long lines are broken up very badly. You could easily fix that by doing the breaking up of lines "by hand".
The expression "almost thread-safe" always make me think of another one, "a little bit pregnant."
The article makes it sound as if variable-length arrays were C++14’s biggest feature. I dearly hope that this isn’t the case. The usefulness of the feature is very limited to begin with, and providing it *twice* in almost identical variants is just not in the interest of most users of the language. [Somebody posted a nice summary on Twitter](https://twitter.com/martinfernandes/status/372378054141558784): &gt; @meetingcpp I don't get why we needed *one* of those features, but getting *two* of them just sounds like an elaborate troll to me. I somewhat agree. And C++14 has more important stuff. Small, to be sure, but mighty, such as the addition of `make_unique` (meaning we now almost never need to use `new` in code, yay!).
This is the wrong way to look at it. If you consider a raw pointer, then it is safe to copy it without the need of a mutex. The thread protection added to shared_ptr is only there to allow it to behave in the same way. If you actually want to *use* the pointer for anything, then you need some kind of lock, just like you would for a raw pointer.
Look at the other articles that Danny wrote for the site. He covers quite a few of the new C++ features, each in depth!
You can't always do that. For example, if you needed an actual loop in the macro.
Good point, I agree completely! And actually, when I browse coding things on my phone, I almost always do in landscape mode - when I have at least 80 columns of coding joy...
Ah, okay. That said, the title makes it sound as if this were a general article about C++14 features, and [I cannot find those other articles](http://blog.smartbear.com/author/danny-kalev/), everything else is about C++11 and C11.
Singletons are easy: don't use them. :P
Yes, the behavior is guaranteed: &gt; Otherwise such a variable is initialized the first time control passes through its declaration; _[C++11 6.7/4]_
Only when it's used to hide the fact that what the programmer really wanted was just a global variable, which is sadly the only thing most of us have ever seen it used for.
Stackoverflow answers: [1](http://stackoverflow.com/questions/923822/whats-the-use-of-do-while0-when-we-define-a-macro), [2](http://stackoverflow.com/questions/154136/do-while-and-if-else-statements-in-c-c-macros), [3](http://stackoverflow.com/questions/257418/do-while-0-what-is-it-good-for), [4](http://stackoverflow.com/questions/1067226/c-multi-line-macro-do-while0-vs-scope-block). 
Thank you
I could not disagree more. Especially when considering the MVC pattern.
You should be addressing startup and shutdown issues with singletons with an initializer and finalizer functions. You can call your intializers in the order you need them to be called. And the same thing at shutdown. This actually gives you two chances to do initialization and destruction. Some things are better suited for when the class instance instantiates, and some things are better suited for waiting a bit, for instance when dealing with resources.
C++11 move semantics eliminates a lot of temporary objects that you get in C++98. Expression Teamplates eliminate more by "recording" the intent and performing a single concatenation, which is very efficient.
Hands down the worse code I've ever seen was in Java, so...
They're not *too* horrible if you have, for example, a global audio manager. You should probably just pass it by reference, but on the other hand a singleton does let you have cleaner functions/classes.
Qt implements this since last year: http://qt.gitorious.org/qt/qtbase/source/b7f7edfae4d5dc7e7547c3377c73129a9c6fdae0:src/corelib/tools/qstringbuilder.h
What do you mean? I'm missing the connection with MVC.
Model - View - Controller. In this case, the model is probably going to be a singleton. So is the view. And so is the controller. For ease of use, all three singletons probably have an easy to use define somewhere to turn: ModelNamespace::ModelClass::getInstance() into TheModelClass One for each, your model, your view, and your controller. Hardware-accelerated applications will also use the singleton pattern to control access to the hardware resource. For instance, accessing your graphics card.
exactly and as long as the API is backward compatible there should be peace
So, I've been wanting *something* like this for a while, admittedly I am not an expert c++ programmer, but I'm going to ask if runtime sized arrays vs dynarrays vs std::array is what I'm looking for. Basically in my work I store lots of oscilloscope traces, which contains N unsigned shorts. I basically just have a Waveform class which performs algorithms, and I frequently store and use many instances of it. I have been using std::vector&lt;&gt; so far.. but I would never resize it. Additionally, in an interactive application I am building the number of unsigned shorts could change throughout the program (acquiring different data sets). So, in memory and performance considerations, are Runtime-Sized Arrays what I'm looking for?
Thanks for the correction, I did not know that. In this case, however, it makes more sense to just use an std::vector or std::unique_ptr&lt;T[]&gt;.
Operator auto: [http://herbsutter.com/2012/04/03/reader-qa/](http://herbsutter.com/2012/04/03/reader-qa/).
They mention Ubuntu there but it installs fine on Fedora 19 (just needs redhat-lsb installed)
Here's an example of how to perform efficient string concatenation using C++11 variadic templates. This approach requires you to have a fixed number of strings though (e.g. you can't use this to concatenate all strings in a container). http://ideone.com/WTEzbt I think it's very elegant and it fits into 50 lines of code.
Interesting, an approach similar to Expected&lt;T&gt; by A. Alexandrescu.
Intel favors for some strangeness RPM. It could have something to do that their Linux Distbution Moblin and later MeeGo also used RPM.
Why do they have to be singletons?
There are so many typos it's hard to read. He really should proofread his text before posting it.
I am glad to never have had to know the reason for why something failed when it did. Something like boost::optional is enough for me.
Will this conflict with http://en.wikipedia.org/wiki/C%2B%2B14#Optional_values ?
Just a quick sidenote: So far I have found a use for TMP in reflection of and automatic registration of classes for scripting languages. Not sure if I know of any other use for it.
I'm guessing English isn't his or her first language.
Not opensource. Would not use.
The only new thing is that the memory is allocated on the stack. That's good for short-lived objects, i.e. memory that you allocate and deallocate very often, since it reduces heap fragmentation. For long-lived objects, I believe `std::vector` is optimal, provided you indicate the size in the constructor instead of using repeated `push_back()`s. 
Why android needs a special C++ compiler instead of using normal icc?
The Linux standard says to use RPM. Nobody listens to it though.
That's a good question, C0+12. You could pass around a context to all of your functions. But that's the answer right there, isn't it? You then have to pass around a context for everything. I believe it adds too much typing. Too much of anything is a bad thing. The reason you shouldn't use globals or singletons is because, as one of my old college professors used to say, they're like insects and breed out of control. So, you end up using globals for everything. But that's the same problem as passing around a context to everything. Too much of *anything* is bad. Moderation is key. Because in this particular case, you'll have less than five singletons, I think it's the right approach. You make it easier for the developer to use, you don't pollute your namespace, and you end up with only one of each. That, in my opinion, is the best situation of all. I am enjoying this conversation. Thank you for making me defend my opinion.
&gt; You need an exception tree, if possible not too big, so that you can catch only the exception you are interesting it. But that just isn't true. 99% of the time, if you put a good error message into your exception that can be reported to the user, they can all be generic. Don't get me wrong - I'm not against creating new exception types but I don't do it unless I _need_ to. I have two projects ongoing now, one in C++ and one in Python. In the C++ project, I entirely use generic exceptions. In the Python project, nearly everything is a generic exception, except for a few parsing exceptions which contain a pointer to the exact place in the string or file where the error was.
A spell checker would have fixed the spelling errors at least...
nobody except intel
No, icc is the Intel C(++) Compiler. Which already produces x86 machine code, so zokier's question is completely valid.
Oh, I *love* this answer from the FAQ: &gt; Is this the same Intel C++ compiler Intel has been selling for several years? &gt; Intel C++ Compiler for Android OS shares a common base, and heritage, with other Intel C++ compilers – a heritage that focuses on performance, compatibility and support. The Intel® C++ Compiler for Android OS release is based on the Intel13.0 code-base. It shares many similar features of other Intel C++ compilers because this is a requirement for mobile computing. Licensing and packaging can vary from market segment to market segment but Intel’s goal is to offer a range of C++ compilers that are largely the same but whose differences enable use over a range of computing solutions. This makes it possible for developers to economically reuse code, where practical. That's marketing-speak for "yes"
&gt; The Linux standard says to use RPM. That is, the [Linux Standard Base](https://en.wikipedia.org/wiki/Linux_Standard_Base) specifies this.
And if the code doesn't run on anything EXCEPT Atom processors... all the better. For Intel, anyway. Meanwhile the Android market gets even MORE fragmented. This press release makes me sad. 
But why do you need to pass around anything to your functions? Aren't your MVC components instanced with whatever they need? For example, when you create a view object, do you not give it a reference (or pointer) to the model object?
Not necessarily. Think of the example in modern simulations using a component-based design. Each component you add to your base actor is going to be it's own object, each object having just enough intelligence to do it's one single thing. You typically only instantiate each one of these with maybe a string for a name or a reference to it's parent. But any and all of these may at any time need access to a property manager (in this case acting as the model), a resource manager, and the viewport. In all three of these, each one of these really needs to be a singleton. You can't have more than one graphics system running, and the graphics system is what provides the viewport. You don't want more than one property manager because that ends up causing all kinds of problems deciding what to put where. And you better not have more than one resource manager handling disk access to textures, bitmaps, and model files. Instead of retyping the same three singleton definitions over and over again in your code base, it's much easier and safer to simply use a define to make an easy-to-use accessor for each singleton. Typically, you'll see things like the following all through your codebase: ThePropertyManager-&gt;GetProperty( blah );
You've never need to know why something failed? And you got 4 upvotes? Come on.
I work in computer graphics. If something has failed the gfx card prolly just caught fire. Very few errors would be recoverable from software.
I'm sure you've never passed an incorrect parameter to an API function and wondered why it failed, right?
That's what assertions are for in debug builds. Graphics and other things are where performancs actually matters. Extensive error checking can be intolerable overhead. If something *unexpectedly* fails in release, get a stack trace.
See also: http://qt-project.org/doc/qt-4.8/qstring.html#more-efficient-string-construction
I commented on the blog, but anyway, let's write a version here :) - The code is not event close to compile ( http://pastebin.com/uztQuX2b )! - Don't use a bad technical tricks which breaks OO. Allocator are just not designed to solve this issue ! Implements a type Forename and Surname and then you do: Forename f = Forname("toto"); and you don't even have to compile to see your errors: Forename f = Surname("surname"); It is even better when you call a function/ctor: someclass.memberfunction(Surname("toto"), Forename("tata")); With C++11 and move semantics, it doesn't even have any overhead because of copies ! Moreover with your solutions you also make very hard interacting with standard string (you can't compare it, affect it, etc). EDIT: Formatting EDIT2: Add paste-bin link
But in this case you should note that std::string does not have a virtual destructor, so its IMHO better to implement the string as a member instead deriving from it.
Whoops! You're right! Thanks! :)
This is really just the strong/true typedef idiom. A more robust implementation can be found in Boost (of course) [`BOOST_STRONG_TYPEDEF`](http://www.boost.org/doc/libs/1_54_0/libs/serialization/doc/strong_typedef.html). Abusing `allocators` really is not an appropriate approach to this. Besides, needless incompatibilities between classes due to different `allocators` is actually disliked, for example [`SCARY iterators`](http://blogs.msdn.com/b/vcblog/archive/2012/04/06/10291485.aspx) try to work around this.
While true, this shouldn't stop anyone from deriving: This class is clearly not intended for traditional substitution but as value-type where you will never run into problems if you don't write unbelievable ugly code; remember that the rule about virtual destructors and inheritance has always been: “Require them if you have one or more virtual functions”. It wasn't “If you derive, you always need a virtual destructor”. Just look at std::vector: In most real implementations it inherits something like std::__vector_impl without any virtual functions and this creates no real world problems at all. Because noone is so stupid to rely on something like that. 
Well, no one? Trust me, if its possible, somebody will do this. For me its just wrong to teach the use of C++ in such a way. While technically you are right, why not use a baseclass and make the derived classes clonable? Adding virtual clone() to the from vector or string derived class seems easy. And in case of creating an email class, you might have data in regex classes or so that needs to be freed. I'd never trust the "no one is so stupid to do this", I've just seen too much realworld code doing weird stuff.
check out /r/dailyprogrammer, they have some interesting challenges on there. however, keep in mind that most of these challenges are aimed at procedural programming, so you won't get many opportunities to make OO code, which is what you're aiming for (i'm guessing).
OO code? Oh, Object oriented? 
yup.
See also: [http://preney.ca/paul/archives/1099](http://preney.ca/paul/archives/1099).
I understand. Our miscommunication is that I consider debug builds to be part of *ever*. 
The code formatting was corrupt in the article. It is fixed now. Sorry!
The basic thought seems right, but the implementation seems clunky. Could you not also create a class Forename: public std::string and inherent the entire default implementation? Forename &amp; Surname are bad names to use, [because your assumption is false.](http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/)
Hello! I made a slight change where `multi_tuple_eval` now uses the tuple constructor instead of `make_tuple`. This should let it correctly return references instead of a copy. Take another look, if you care about such details. Thanks for reading!
I should add that with my full implementation the normal use of expected&lt;T&gt; is different than what it might appear to be from Alexandrescu's talk or from those used to Haskell's maybe monad. For starters, one normally would not return expected&lt;T&gt; from a function and also would not even pass expected&lt;T&gt; to a function to use it! :-) Instead, one would normally use a helper function (i.e., I call it expected_run()) in almost all use cases. The reason why one would NOT use expected&lt;T&gt; as a return type or as an argument is that normally one does not care about exceptions and would not want to impose anything unnecessary on those pieces of code. So there is no conflict. If, for example as other posters on this page, want to use Boost.Optional they can and there is no issue. But if someone else wants to call code that might throw exceptions and capture them (e.g., so their function can be noexcept; so their program can use exceptions and be carefully and safely deployed on an embedded device) then one can use my expected_run() call to capture such or otherwise use the expected interface (e.g., write a try block, etc.). Truly, know that the use case, is NOT (normally) passing expected&lt;T&gt; as argument(s) to or return them from functions. It is most useful when writing code that one needs to absolutely guarantee that will run to completion regardless of whether or not exceptions occur --this is where expected&lt;T&gt; excels because it captures the returns or the exception and permits both to be processed later.
It is not about switching to it. expected&lt;T&gt; solves a completely different problem than Boost.Optional does. It is also not the same as returning a std::pair&lt;iterator, bool&gt; either since the std::pair does not capture exceptions. If boost::optional&lt;T&gt; is what you need in your code then I'd argue that is what you should use. However, there is plenty of code that has to "iterate" / process over a "set" of items and ideally must be immune to exceptions to guarantee all "set elements" (whatever those things are) are processed. In these cases, expected&lt;T&gt; is wonderful since one is able to capture all results be the result be a value (or void) or some thrown exceptions during the processing. After the processing is complete, then one can deal with the captured results and handle them ALL systematically and properly. Boost.Optional and using a sentinel return value (e.g., bool in std::pair&lt;iterator, bool&gt;) do not apply in this use case --expected&lt;T&gt; does.
True, for example in *many* cultures it would be perfectly reasonable to write child.surname = father.forename
Alternatively, use `reserve`. But even push back has amortized constant performance.
The next instalment builds on enclosing the type in a convertible class, as BOOST_STRING_TYPEDEF does, but without the nasty macro. The incompatibility isn't needless, it's deliberate to avoid accidental conversion between types. SCARY iterators is a different problem; incompatible iterators for types because of a container attribute (the allocator)
The other comments in this thread are correct - the way this guy is doing it... just ugh. I'd even go a bit further. Let's say you need to store phone numbers. Starting out you might just use std::string. But, as this blog notes, those can be mistaken for any other string. Having a class PhoneNumber encapsulating std::string not ONLY can help mitigate that, but you can also do other stuff with it. Validation is the first thing that comes to mind. You probably don't want phone numbers with underscores in them, and may or may not want to allow alphabetical characters. Extensibility is another. When your company decides to go international and all of a sudden you need to include country codes, and maybe default to US... it's easier to do that if you have a class ready to be modified. Or maybe using std::string worked fine when you were dealing with a couple of hundred phone numbers, but now that you're in the hundreds of thousands you want to look at a faster/more efficient way of storing and working with them. If you encapsulate it right, it's going to be WAY easier to rip that std::string out of your PhoneNumber class and replace it with whatever (what *would* be the most compact way to store 10-digit phone numbers? I'm leaning towards 3 unsigned int bit fields for area code, exchange, and subscriber number), than it is to change every piece of code that handles phone numbers to use whatever instead of strings.
"Needless" in this context means "I shouldn't have to worry about incompatibility between objects of the same class just because they've been allocated differently." Which is generally true. We all understand that it's being misused deliberately here. Use allocators for what they were designed for - to specify how objects of a class should be allocated. There are much better ways to accomplish what you're trying to accomplish.
Disclaimer: I'm still learning new C++ stuff myself. Hopefully nothing here is wrong. ___ You generally want to return by value and let the compiler perform [Return Value Optimization](http://en.wikipedia.org/wiki/Return_value_optimization) (RVO). If you want to return a temporary you can't return by reference (or rvalue reference) because the temporary will get destructed on function end. Here is an example of returning two vectors, gcc and clang both perform RVO on this: pair&lt;vector&lt;int&gt;, vector&lt;int&gt;&gt; getVects() { vector&lt;int&gt; a(100), b(1000); //copies happen for make_pair on gcc / clang if you leave out the move calls here return make_pair(move(a), move(b)); } Alternatively you could also pass results in by reference instead (this way doesn't rely on RVO since you move data directly into the results by hand): void getVects(vector&lt;int&gt;&amp; res_a, vector&lt;int&gt;&amp; res_b) { vector&lt;int&gt; a(100); //rvalue references can loosely be thought of as things without names //'a' has a name but move(a) and vector&lt;int&gt;(1000) don't res_a = move(a), res_b = vector&lt;int&gt;(1000); //you could still refer to 'a' down here, but as it got moved into res_a it's now in some unknown but valid state } I imagine tuple wouldn't be too different from the pair version See also: http://stackoverflow.com/questions/4986673/c11-rvalues-and-move-semantics-confusion
With `push_back`, you'll have several reallocations, and probably too much memory allocated.
The exercises of "The C++ Programming Language" are openly avaible at http://www.stroustrup.com/4th.html (there is a link to them) - But some of the exercises are only doable if you have the book. But it should give you some good ideas of what you can try.
Having gone down this road, there are a couple of roadblocks. You can do quite a few things using the return value optimization, but it is possible to make mistakes without noticing it. The RVO works by constructing into the class being assigned to - so you have to be returning one specific instance of the class that is only used for that purpose. If you have code like: if (someCondition()) return IDENTITY; Matrix m = stuff(); // More stuff. return m; the RVO simply isn't going to go off - and sometimes it won't be obvious to you. The other issue is that for efficiency, you still want mutators. It's much cheaper to say `matrix1 += matrix2` than it is to say `matrix1 = matrix1 + matrix2` because in the second case you're pretty well forced to create a new temporary matrix. The RVO is "nice to have" but don't rely on it unquestioningly!
learn by doing, find an open source project on github and contribute
Really?
Why did the one really useful sample get removed? Facebook is documented in the link, but the sample is no longer in the repository.
[tie](http://en.cppreference.com/w/cpp/utility/tuple/tie) [forward_as_tuple](http://en.cppreference.com/w/cpp/utility/tuple/forward_as_tuple)
say this to your self with a thick russian accent "Don't be bitch" and do it. Everything you don't know just look it up one step at a time
I'm well aware that mutators are often preferable (such as the axpy you mentioned), but in many cases, such as [special matrix construction](https://github.com/elemental/Elemental/blob/master/examples/lapack-like/Schur.cpp#L39), or returning eigen/singular values, or a QR/LU/etc. factorization, a tuple of matrices returned via RVO would be a large step towards the convenience of a language like MATLAB. I should emphasize that I'm already reasonable comfortable with how to make use of RVO (and the pitfalls, such as returning within a ternary), but I still haven't seen discussions of RVOs for composite objects. One option would be to set up a special structure for each combination of return values, and then to set up move/RVO semantics for each of these, but it would be nice if there was a simpler way to do this with tuples of structures which have RVO.
If you have boost available and want to do this sort of thing, [`boost::fusion::invoke`](http://www.boost.org/doc/libs/1_54_0/libs/fusion/doc/html/fusion/functional/invocation/functions/invoke.html) is a more generalized version of `tuple_eval` and [`boost::fusion::for_each`](http://www.boost.org/doc/libs/1_54_0/libs/fusion/doc/html/fusion/algorithm/iteration/functions/for_each.html) can be used for `multiple_tuple_eval`.
&gt; It's much cheaper to say `matrix1 += matrix2` than it is to say `matrix1 = matrix1 + matrix2` because in the second case you're pretty well forced to create a new temporary matrix. The temporary matrix can be eliminated by using expression templates for the addition. This is of course *way* more complicated, but boost.proto makes it sort of doable.
I think that the first example you gave essentially answered my question (assuming that it extends to [std::tuple](http://en.cppreference.com/w/cpp/utility/tuple)). I should probably mention that most of my code originally returned multiple objects through references, as in your second example, but that I've already coverted a fairly large library to support single object RVO (and I'm quite happy with the results). Multiple RVO via std::tuple and std::tie might be an even larger improvement.
What exactly is the model, view, and controller in your example? So far, you mention these things: * An actor * Components, which register with an actor. * A property manager (I'm not sure what this does) * A resource manager (I'm not sure what this does either) * A viewport I'm not sure which one is the controller. It's also not clear which one is the model. Also, is the viewport really the view? If the actor has, say, a static mesh component, wouldn't that be the visual representation of the actor?
sorry i was watching this video while typing the comment for context http://www.youtube.com/watch?v=N8IGDIujijQ
Won't the compiler fall back to move assignment in that case? I don't know, I'm asking.
Better, but a comment/link on the [reddit/comments](http://www.reddit.com/r/cpp/comments/1lbosi/using_strong_types_to_avoid_common_c_programming/) you had would have been nice. 
Second instalment is now online at http://reliablecpp.com/blog/2013-08-strong-types-updut
Ok, I added a comment with a link
&gt; an assignment of type radians to a type degrees converts the temperature appropriately. Don't you mean "angle" instead of temperature?
&gt; but macros are ugly and difficult with interactive debugging That’s a bit of a blanket statement. Macros are good at code generation, which is exactly what we need here. There’s no reason to avoid a macro here, and it doesn’t really make debugging harder either (only in that it obscures the type name slightly, but not more so than in your alternative). Using type tags to distinguish classes is of course an established idiom but you can implement it slightly simpler than shown in the article. Namely, there’s no need to *define* the tag type, it’s enough to *declare* it. And this means that you can declare it *inline* inside the type definition: typedef strong_type&lt;float, struct tag_degrees&gt; degrees; [Here’s a thread with more information about this technique](http://stackoverflow.com/a/14232406/1968), in particular in terms of optimising space usage – since specialising such a class for different type tags potentially generates redundant code.
&gt; The RVO is "nice to have" but don't rely on it unquestioningly! I disagree – you *should* rely on NRVO. If you don’t, you’re forced to return values via out parameters rather than return type. This makes your API un-idiomatic, harder to use, and the client code becomes more complex. These are substantial, unacceptable disadvantages since NRVO works *reliably* on all modern compilers. You just need to know when NRVO cannot be applied – like in the example you gave. And luckily these situations are relatively rare and easy to recognise.
[There’s an extensive discussion of this article elsewhere on reddit](http://www.reddit.com/r/programming/comments/1l6kan/error_handling_using_monads_in_c/).
Yes, return a tuple. If your classes have the necessary special member functions you will not incur copying, even when returning tuples. See here for an example: http://ideone.com/ms0Fn9 You may want to combine the `operator=` overloads into a universal one, which takes its argument by value ("universal assignment operator"). If you have measured (through profiling), that the move operations are your program's bottleneck (extremely unlikely), you may still chose to use "output" arguments (i.e. mutable references which will hold the result). Edit: as /u/m42a correctly points out, this *will* copy data if your classes contain unmovable types (e.g. `std::array&lt;&gt;`).
Yes, thanks. That's fixed now.
If you access a container millions of times in quick succession then performance plays a crucial role, even for a small number of elements. And the author says precisely that in those cases constants play a bigger role than asymptotic complexity. That’s anything but silly. Also, the author isn’t concerned about `begin == end` (his text explicitly says that he checks for `&lt;=`), he’s concerned about `begin &gt; end`, which would be invalid – but to be honest I’m not sure where such a situation might crop up.
&gt; The point about allocators is a valid one but I don’t understand how C++11 doesn’t solve this – what prevents you from using your IAllocator interface as an stdlib allocator? pre c++11 the STL containers didn't have to support allocators with state, it was purely optional. AFAIK all implementations do, with annoying bits of UB on std::swap&lt;container_type&lt;T, yourallocator&gt; &gt;. You can always drop it something like STLPort though if you need consistent behavior between platforms Almost all of these complains can be fixed with the boost libraries, and are being implemented in either c++11 or 14.
Well but I’m asking specifically why/how the author’s problem isn’t solved in C++11 as the article says.
Cheers, I'll have a look. :)
A while ago, EA wrote and released their own STL - docs: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2271.html source: https://github.com/paulhodge/EASTL It's been on my list of things to use for a long time, but I havnt started using it yet. Have you looked at this implementation at all? I know that the memory allocation issue was one of the things it addresses. 
Naming is an issue? really? That's why we've got namespaces, and everything inside of std is clearly defined. Ofc. you might get the issue if you use using namespace std;, but well, then you deserve that issue ;)
Company I worked for (multiplatform game development, mobile and current-gen) used it, they were quite happy with eaSTL (it decreased memory fragmentation which was big pain on Nintendo DS and 3DS).
&gt; You cited the naming of vector/map as downsides, but you bypassed a good thing, which is that they're in the std namespace and not pollutants of the global namespace. Sure, it's great, but also kind of a small deal these days, isn't it? Doesn't everyone take that for granted? Anyway, the naming thing is small peas. :)
Indeed, but it was cited as the very first complaint.
Yeah, I just wanted to get the petty stuff out of the way. It would be weird to end on that. :)
If Boost is a dependency you're ready to introduce to your project, those are great additions. :)
Well you can cook up an ad-hoc version yourself using less code if you know that you do not need support for 10 year old compilers. Also since flat_* containers are all drop-in replacements for the standard maps and sets you could provide a configuration option that controls some using aliases, and swap out boost for std::map. as with all things boost "I just wanted some steak but I got the whole cow"
Interesting. I know the Rare games Starfox Adventures and Grab By The Ghoulies used STL because I was working in the R&amp;D department and we developed the N64 and initial Xbox engine (the R-Engine fact fans!). Responses: 1. Namespace. 2. Given you write your own custom allocator by grabbing the full memory pool, add an STL interface to this for a free list (for example) is trivial. Degrade as required for allocation failure. 3. You know exact when iterators are invalidated - it's clearly documented. Most of the other issues are still present with pointers for instance. 4. If you're seriously worried about locality then you're probably talking about fitting things specifically into cache line - it which case your real issue is hot spots with structures/classes or using DMA access/ scratch cache or prefetching (depending on architecture) for more controlled access. STL might not be perfect out of the box, but all your arguments could be equally labelled at C++ compared to C (virtual function tables, lack of compiler optimisation opportunities with templates, runtime types etc).
Yeah I stopped reading after the first one. I used to work in games and the hate for STL was usually suitably justified, but when you open with that you really use all credibility. This document is the canonical "why I hate STL for games": http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2271.html
How is a slice class conceptually (or technically) different from an iterator pair? In C++, the obvious implementation of a slice is an iterator pair. C++ offers tons of algorithms and special syntactic support for it. I’m certainly not fundamentally opposed to other approaches but I’d love to hear which benefits they offer.
Yeah, though thankfully Boost is planning to drop support for the really old compilers.
The slice can be treated as an array, and iterated independently. All syntactical sugar is congruent with the primary vector implementation. It captures invariants. See Andrescu's work on ranges for more (I think it was him, anyway).
OK, so we're saying the same thing, just describing it differently I like a literal ArraySlice&lt;T&gt; class that initializes from a vector with an offset and size, which provides operator[] and everything. I don't want my code knowing that this is implemented as pair&lt;vector&lt;T&gt;::iterator, vector&lt;T&gt;::iterator&gt;, and I sure as hell don't want them having the flexibility to pass me iterators from different vectors. 
&gt; &gt; but macros are ugly and difficult with interactive debugging &gt; That’s a bit of a blanket statement. Macros are good at code generation, which is exactly what we need here. There’s no reason to avoid a macro here, and it doesn’t really make debugging harder either (only in that it obscures the type name slightly, but not more so than in your alternative). All the debuggers I've used don't allow you to debug the code in the macro. The macro just becomes a completely black box where lots of stuff happens, none of which you can really trace into.
They need an immutable type qualifier before they add pure. This qualifier must not be removable like const is. D is a good source of ideas here. 
&gt; Are those assumed to be unshared? Without enforcement of immutability, I don't see what guarantees [[pure]] could realistically offer you. I don't think it's a question of the guarantees [[pure]] offers you, the C++ developer. Rather, the benefit comes from the guarantee you are making to the compiler about the [[pure]] code. Once you make this guarantee, the compiler is free to optimize in a variety of ways it could not before. Of course, if you break your promise, you're in the land of undefined behavior. Certainly the compiler could catch many varieties of statically detectable impurities, but the proposal is still potentially very useful even if it can't statically prove purity. 
True but you don’t really need to debug into this macro, you can just regard it as a black box. *It generates a type*, not executed code. While debugging, the debugger will gladly let you step into the code that was generated by this macro, should that be necessary.
What? I thought purity was all about improving the way you reason about your code. In this sense, it is all about the developer.
The very first rationale given in the proposal: &gt; The ability to discriminate between well- and ill-behaved functions can be significant for the generation of high-performance code at and near a call site: If it can be determined at a point of call that the callee is well-behaved (and thus that its results are reproducible), additional caller optimizations may be applicable. This is especially important when the implementation of a function is not visible to the compiler (for example, if it's in another translation unit).
Sure, if calls to your "const" arguments can trigger observable side effects then optimizations the compiler might perform could leave your program with undefined behavior. So don't write "const" code that uses side effects! This is really not that different from C99's "restrict": "I promise there is no aliasing here, please optimize accordingly." That also requires that you know something about your arguments; it's an explicit, unenforceable contract with your caller.
I really hate the language used in this. Why start talking about "well-behaved" and "Ill-behaved" when pure vs. impure are perfectly good denominators. It's like someone talking about his new proposal for a spoiler for a car starting off with "Let's first define cars without a spoiler as ugly and conversely cars with a spoiler as beautiful. Now beautiful cars have the advantage over ugly cars that...." 
&gt; [[pure]] should guarantee some sense of _idempotence_ and I don't see that being done by this proposal. I'm not sure what I'm missing here, but the proposal seems to obviously guarantee idempotence to me. Any function with no side effects is necessarily idempotent (and even nullipotent), right? I must be misunderstanding you.
A very useful proposal. Walter Brown certainly deserves his fair share of acknowledgement in bettering C++. He has written many good proposals, some of which were unfortunately not accepted. Good to see he has not "given up" on the committee, so to speak. I would completely support this proposal, if it were guaranteed that the compiler check all the requirements for a `[[pure]]` declaration. I don't think it would be a good idea for the standard to invoke undefined behavior if a function declared `[[pure]]` would violate the requirements. There already are too many instances where UB (often combined with "no diagnostic required") is invoked in the name of "performance". Make it a strict, non-circumventable decorator (as it is in D, as far as I know), and it will do much good.
How about "real" functions vs. "fake" or "programmer" functions? :p
So the syntax will be something like pure double f(int j) noexcept {...} ? I kinda like it.
Yeah, I understand the fact that shared_ptrs aren't deeply immutable means the result is unpredictable in the presence of threads. But what does that have to do with idempotence? By that I'm assuming you mean... &gt; In the case of methods or subroutine calls with side effects, for instance, it means that the modified state remains the same after the first call. No state is modified by `square` in this case, so it applies trivially, right?
The Facebook Demo is still located under the Collateral folder: (Release-&gt;Collateral-&gt;Samples)
We added http listener in our 1.1.0 release. If there are some specific features you would like to see in our upcoming releases please let us know by opening an issue: http://casablanca.codeplex.com/workitem/list/advanced
The point of [[pure]] is to enable optimizations. The compiler may **or may not** replace int sum = square(spi) + square(spi); with: int tmp = square(spi); int sum = tmp + tmp; The question is, which one do you get? It's important because they are very different. This isn't a failure of the square() function, it's a failure of the datatype. The 'const' qualifier does not mean "immutable data", it means "immutable **access** to data". The emphasis on "only calling const member functions" is a red herring. That only provides the _illusion_ of correctness, it does nothing to guarantee it.
Great, thanks!
&gt;that doesn't prevent another thread You still need proper locking - this is not a problem with pure, but accessing and mutating state without synchronization.
Try adding "-o HelloWorld" to your command line. That tells it the name of the executable to create. And no, users of your compiled programs don't need Xcode themselves. 
I use a [Pure] attribute in C# for pure functions. The CLR doesn't use this attribute for optimisations at this stage. Whilst it does allow static code analysis to warn on calls to pure functions that disregard returned values, this rarely happens in practice. What I *do* find it useful for is documenting the nature of a method. Knowing that it's pure reduces the cognitive load when reasoning about a function's behaviour. It's like a short-circuit on a tree walk -- no need to look deeper than the signature. The C# compiler doesn't enforce the attribute's correct usage, so unfortunately it can be misplaced. It sounds like this proposal integrates the attribute/modifier deeply into the compiler so that this would be enforced and used as thoroughly as possible for optimisation. I suppose it may also be used for pulling repeated pure calculations with the same operands out of a loop.
No, it would be [[pure]] double f(...) noexcept { ... } The double bracket syntax for attributes [already exists in C++11](http://en.cppreference.com/w/cpp/language/attributes), so this would just be defining a new attribute rather than inventing new syntax or new keywords. 
I'm sure you're right, but I forgot how I did it and was hoping to find remnants of it somewhere. If it really comes down to it, I could do that. But I'm really trying not to. 
I believe the default output file is a.out. To run it put ./a.out. There's also an option to name the output file whatever you want. 
Ah got it! How do I run the executable then?
I am still quite new to cpp so I am not sure what this output file is &gt;.&lt;
It's the executable created once you compile the source. a.out is the default name, but like the other person said you can use the -o flag to name it whatever you want. To run it you just type ./a.out (or replace a.out with whatever you specify the name to be with the -o flag).
Oooh! Thank you! 
 ./HelloWorld
I would run it through ida pro and generate pseudo code. It should be helpful, unless you enabled optimizations or used something like boost.
You won't get the source code. The best you could do would be assembly.
If you haven't used -o option you can use 'ls' to list all the files in the directory. A common name for the executable is a.out. The .out is extension is entirely optional. Unlike windows unix-like systems don't use the extension of a file to deduce it's type, instead they read the first few bytes of the file. These bytes identify the type of file. to compile and run your program you can use this command : c++ Helloworld.cpp -o Helloworld &amp;&amp; ./Helloworld the &amp;&amp; (and) means that the second command command is only run if the first completed successfully (return code 0).
Isn't the C++ tradition to say, "you can do that, but it's undefined behavior" rather than "you can't do that"?
That is the old one, [here](http://softwaremaniacs.org/media/alenacpp/cppmap-2012.png) is the updated one.
That is insanely high res.
No, that's still terrible. Only a very few functions in most codebases will handle audio. A global is a complete waste. Also, "manager"? Owch.
Interesting. I did not know this!
At least we scavenged some useful things from the concepts shipwreck
Accelerated C++ (Koenig, Moo) or C++ Primer (Lippman)
If you read the sidebar here carefully even that might help you.
a little light on the content...
open source library for computational finance, with a random code snippet that doesn't describe half of what it does and poor overview on the time value of money.
Hang on, does that mean that if I actually want a container of iterable bools, and I use std::vector&lt;bool&gt;, I actually dont get an iterable container? 
Yes. That sucks so hard, that AFAIK it has been considered for deprecation, but nobody really knew what the deprecation of a template-specialization would imply. /u/STL, the maintainer of microsoft's STL-implementation called it “mankind's eternal nemesis” or something like that in official release-notes.
that was a good read, thanks!
So you forked SQLite and removed SQL? Any performance benchmarks ( against SQLite, LeveDB )? Finally, how do I pronounce this?
They're live streaming it during a week when I don't have access to a computer or (stable) Internet? Oh well. I'll have to just read about it in a week's time
Come lets not reinvent the wheel BOOST_STRONG_TYPEDEF(primitive type, name)
Relax, the videos will be made available afterwards.
Fair enough. Maybe I can watch them at work. That is, if I can convince the boss that I'm taking part in 'professional development' Edit: once I'm back at work, that is 
I am not sure what I should think about that: On one hand I would really love to watch that; on the other hand I should really practice for my math-exams.
Or just watch them any other time? Considering they are VODs you could watch them before work, after work, or what ever works for you... I'll be watching them while eating breakfast and before bed.
You can. Download its sources and integrate into your build system. You will need to understand how to build the library though, which can be complicated.
Yes, and it makes update, and distribution hard. It will also complicate external developer who might be interested in your code by hiding where/what are your adding. There is basically no reason to integrate your code into the SFML. If you want to avoid a dependency when the binary is distributed, just link against the static library.
If you are interested in the preprocessor, boost includes a preprocessor implementation that conforms to the C99 standard. It also allows you to do cool stuff like trace a macro expansion if you are debugging a macro. If you want to do cool stuff with macros, check out the boost preprocessor library. http://www.boost.org/doc/libs/1_54_0/libs/preprocessor/doc/index.html http://www.boost.org/doc/libs/1_54_0/libs/wave/doc/preface.html
Actually, if you are using a c preprocessor, you should check out [chaos-pp](http://sourceforge.net/projects/chaos-pp/). Which has an efficient recursion backend, lambda expressions, generic algorithms, and much more. Boost.PP is not as feature rich since it has to support MSVC's broken preprocessor.
You can integrate into build system but not #include a 3rd party lib generally. Example reasons why you can't or shouldn't, compilation errors due to duplicate global symbols and large compilation times (if #included, the lib and your incuding module form a single tanslation unit that has to be compiled again and again in your development cycle).
C++ does not have a 'library' system. Various ecosystems provide facilities to link in existing compiled code to new binaries, both at compile time and at runtime. Still, there's no standard way to do this in a theoretical 'pure' C++ ecosystem. A modules extension has been proposed for C++, originally for C++11. It has been postponed, but may resurface in the future. [Here](http://stackoverflow.com/questions/3596147/modules-in-c11) is a stackoverflow post with some useful information about the state of modules today.
There are header-only libraries, as others have noted. Oftentimes when I find myself writing a larger app, I end up writing a library and then several driver programs. And you're correct, when you're statically linking, you're essentially putting bits and pieces of whatever library into your code. It wouldn't make a whole lot of difference to the end result. There are still a few reasons you might not want to do that. It's one more thing to compile, for instance. And you might not have the option as you may not have the source code for the library available. There's also dynamically-linked libraries. When you compile a program with them, the build system sort of "promises" your program that this library will exist on the system when your program is run and that it will have the functions that your program expects. This means that when you distribute or deploy your software, you don't have to update the whole shebangabang every time. Let's say you write a game using OpenGL. If you statically link your libraries, that means every copy of your compiled program has a bunch of OpenGL code stuffed into it. When you give me a copy to play, I probably already have OpenGL installed, and now I have a redundant bunch of code in your program. Now a security exploit gets discovered and patched in the OpenGL library. I patch my system libraries, but then I still get compromised because either you didn't release a copy of your program including a patched version of OpenGL, or I didn't update to it. If you're using dynamic linking, that essentially just tells your program "hey ask the OS for this library and this function." If you do your program that way, then I just have to keep my system libraries up to date.
If you are saying you #include the source code in the library, others have answered the downside of header only library. If you are saying you put the source code to your project and compile them together with your project, here are the problems, 1, Some libraries have special project settings such as macro defines, which you don't want/need to include in your project. 2, Compiling time. SFML may need minutes to compile (sorry my desktop is too old...), the giant library such as Qt or Clang may need hours to compile. You don't want the extra minutes or even hours after you clean and rebuild your whole project. 
Something about wanting to remain elitists. The separation of the community is a bummer. With just only 1k people in the questions subreddit, defeats the purpose of having it.
I use Rational Purify Plus (the Purify and Quantify modules) for testing c/c++ non-managed code. As far as I'm aware there is no 'module' for static code analysis so a direct comparison probably doesn't make sense.
ah! Thank you very much for your reply!
I prefer void f(); To me less text is more readable than more.
When will the recordings be posted?
As I recall, yesterday it said 2 or 3 days.
The talk of Andrei is great
If ihad to use only vectors and had to use the vector after counting I'd copy the vector. Then id grab the first element and popback until the next element is not the same as the one I grabbed. Repeat till empty. If I didn't have to use vectors I'd use a set or something that allows unique items and counts the number of times for you. 
I'm not entirely sure how I would go through and count without also printing the words at the same time, since the program accepts any number of words.
This is of course only a couple ways of doing it. You could also go through the vector in a nested loop where you compare each item to the items next to it and then break after you found all the matches to avoid needless compares and skip forward the number of matches you counted. I'm speaking in generalities because I don't want to do your assignment for you.
Yeah I think a nested what I'm going to try. Thanks for the help =)
&gt;I'm using a vector to accept an amount of strings which isn't set and then I need to count the number of times that each word was entered and print them side by side. I'm not sure why you're using a `vector` here. std::map &lt;std::string, int&gt; count; To add a word: std::string const word { "bob" }; count[word] ++; To display the count: for (auto entry: count) std::cout &lt;&lt; entry.first &lt;&lt; " ---- " &lt;&lt; entry.second &lt;&lt; "\n"; ----- Two problems with your code: 1/ Add four spaces before each line to format it properly. 2/ `if (*it == *it++)` You're incrementing the iterator here, which is not what you want. In fact, the expression doesn't really mean anything, since you don't know whether `*it++` will be evaluated before `*it` or not. What you actually want: if (*it == *(it+1)) 
C++ is evolving waaay to fast for me to keep caught up. I'm still on C++11!
I usually agree with Bjarne, but I would **love** to see static if in C++.
&gt;we would probably rather write that logic in C++ (exactly like with templates) than use [...] the preprocessor static if would allow the replacement of ugly preprocessor conditions all over the place. In fact I think with templates (which we obviously already have) and static if the use of the preprocessor for everything but #include and leveraging code targeted at C would be obsolete. It's one of the biggest reasons I want static if.
&gt; The templates are already turning complete As is [Brainfuck](http://en.wikipedia.org/wiki/Brainfuck)
in c++ code I use void f(). In c code I use void f(void). 
C+++++
I want typed new &amp; delete operators, as well as compile time introspection for types, global and stack records. It would make compile-time generation of metadata for objects, globals and locals trivial, allowing for things like a precise garbage collector, automatically generated marshalling/unmarshalling object functions etc. 
Great, now I have to buy new set of textbooks again.
That's not the problem. The problem is compiler adoption. Using some features, then stopping because that's not in X compiler yet, then back again makes you forget what you can actually and should use and what you can't. If MSVC was as fast as gcc in their support, then most people would be able to keep up somewhat and only advance. Not go back and forth in features and idioms. 
Surely you mean the templates are... "turing complete"?
Not all gcc features are tested as rigorously as MSVC from what STL discussed in an article i read. (i'll try to find the reference)
I just turn my templates on their sides, since they have a complete 360 degree arc of rotation! (I fixed it)
No chance, STL is the man but honestly there's no comparing how absolutely brutal the bugs are in MSVC2012, especially compared to GCC. Furthermore I highly doubt that anyone can really compete with the level of testing GCC and clang get due to their open source nature. I'd love to read what STL's opinion on this matter was.
I may be biased but I think LLVM/Clang has some good code on a lot of points. Well documented, good "homemade" algorithms, uses of the STL, tested, not too difficult to understand, ... You can take a look at llvm::StringSwitch&lt;T&gt; I like the feature it provides: * http://llvm.org/docs/doxygen/html/StringSwitch_8h_source.html You may want to take a look at StringRef at the same time: * http://llvm.org/docs/doxygen/html/StringRef_8h_source.html * http://llvm.org/docs/doxygen/html/StringRef_8cpp_source.html If you want a collection of "generic programming" classes that are worth taking a look at: * http://llvm.org/docs/ProgrammersManual.html#picking-the-right-data-structure-for-a-task 
They've been in libraries such as boost, but c++ it's standard.
Runtime sized arrays really aren't all that useful, and they've been introduced for a very specific performance reason. `std::vector` is generally a far better choice in C++. Runtime sized arrays are more important in C because you can't write something like `std::vector` in C.
You might want to take a look at [codereview.stackexchange.com](http://codereview.stackexchange.com/questions/tagged/c%2b%2b) which offers exactly what the name promises. Apart from submitting code for review yourself, it’s often educational to look at what other people have posted, and what feedback they have gotten.
That's a nice summary. Will the C++ 11 standard smart pointers will be used in the Qt API, too ?
I think [dlib](http://dlib.net) has some of the best code around, but I'm biased :) But I think this because it's all done in the [design by contract](http://dlib.net/howto_contribute.html#1) style where every single function has precise documentation. This is important because the process of writing down all these detailed descriptions of what your code does becomes part of how you design software. For example, often you will find that when you go to write down the requirements for calling a function you are unable to do so. This may be because the requirements are so complex you can't think of a way to describe them, or you may realize that you don't even know what they are. These kinds of things are signs of bad design and are easy to avoid when practicing design by contract. Here are a few concrete examples. There is a pipe object in dlib and its [contract looks like this](http://dlib.net/dlib/pipe/pipe_kernel_abstract.h.html). Also, here are two examples of its use, one for [inter thread message passing](http://dlib.net/pipe_ex_2.cpp.html) and another for [inter process message passing](http://dlib.net/bridge_ex.cpp.html).
Sure you can, you just use `realloc`. The real difference is that runtime-sized arrays use the stack, but vectors use the heap.
Slide 8
If you haven't read Scott Meyer's Effective C++ I would highly suggest it if you love the language. It instantly became a favorite book for me and I try and make it a point to re-read it every few years, not just to refresh the ideas but because it is just that enjoyable to read. I had programmed in C++ for 20 years before the book first came out and considered myself a "10", it gave me a completely fresh perspective on the potential of the language and libraries like Boost and I definitely changed my coding style to try to achieve the same consistency and to employ the idioms that it pointed out that I had been doing on my own at times, but not consistenly or as well implemented because I hadn't realized the patterns were there.
This is a very good point. LLVM has a ton of useful utility libraries that are very well written. Clang code on the other hand. Mother of god, it makes me cry. Although clang does have useful utilities as well, they are dwarfed by the monolithic classes that permeate the codebase. I don't mean to say I could write clang better. I just wish they had!
&gt; Sure you can, you just use realloc. And do all the necessary work by hand.
&gt; IIRC the actual static if proposal states that it the compiler still has to parse and comprehend the code in each static if. "The guarded code is tokenized and ensured to be brace-balanced, but otherwise not analyzed". Tokenization doesn't involve symbol resolution, template instantiation, or, in fact, any sort of error-checking beyond completely trivial stuff (so it will fail if it sees the "$" character outside of a string, but not if it sees the line "false true &amp; &lt;booll&gt;").
&gt;But of course those const char[n] can automatically decay in a const char *. String literals can even decay to char*.
I saw this on HN but the stream was the day before and the uploads are pending (the going native talk ). One of the huge issues is going to be the lack of PDB support in LLVM. The debug info (not the technical term) in LLVM is pretty much tailored to DWARF. Sigh!!!! This effort from Chandler Carruth (who is awesome BTW) seems like it's in vain as Microsoft almost certainly wont release the PDB format and will most probably keep moving the goalposts if it's comprehensively reversed. Best case scenario imho We have a compiler but LDB doesn't work :( 
Microsoft's version of std::chrono is totally broken. In MSVC std::chrono::high_precision_clock has a resolution of around 8ms. EIGHT MS! That said libstdc++ and libc++ both lack std::wstring_convert and at least libstdc++ lacks std::regex. I really do not think GCC and clang can claim c++ compliance when they do not have the whole standard library. 
Not anymore they can't.
For me Modules are the single most important thing that is coming to the language. In most other languages I can stick a library in some predefined directory and the build system or compiler can figure out what I mean when I say import foo. Not so in c++ In c++ I have to download the package, find some cmake find module someone hacked together in a few hours, hope it works, tell cmake to link the libraries, and pray the libraries were compiled using settings that will permit me to link with them. Only then can I compile and spend two hours dealing with linker issues. This is a huge issue for someone learning c++ and it is a huge annoyance for experienced c++ developers. And when we get modules we will also get rid of the transitive nature of #include (if a #includes b then #includeing a also gets me b) and that alone would be a big win. Modules would also make generic code more natural, today templates feel like "second class" constructs since you need to define them in headers and make sure to distribute those headers with your library. Also, the little note about having f(x) and x.f be the same would be FANTASTIC. Imagine if when you used a library like cairo or xlib you were able to use the context structs like objects! These little quality of life issues add up and make a difference. On another note I would love to be able to define my own operators, and having a standard ++ operator for strings would be nice, + is addition not string concatenation. Bonus points if those operators can be Unicode symbols ;D.
to be fair the necessary work is not too hard, it comes out to a few dozen lines at most. It is annoying but there are other things in C that need to be greenspun with much more code (hashmaps anyone?)
&gt; There is nothing technical holding back the implementation of runtime-sized arrays Sure, now that it's specified. There's more to the specification than manipulating the stack pointer, however. IMO arrays were already bad enough, what with the restrictions on assignment, returning and passing by value, and their propensity to convert to dumb pointers at the drop of a hat. I think the committee should have stopped at `std::dynarray` and not added this weird new 'array of runtime bound'. In fact raw array syntax now has an added risk in that it might accidentally use this new type with all these new restrictions and that's another reason to never use raw array syntax.
I just can't count the number of projects that is plagued by multiple dll/so dependancy and the number of bugs due to the C interface. Module is the feature that could make a huge difference across the whole ecosystem and bring back sanity to some dev teams.
Indeed. C++11 appears to have nixed the conversion entirely.
&gt; Microsoft almost certainly wont release the PDB format and will most probably keep moving the goalposts if it's comprehensively reversed. What makes you think that?
&gt;Chandler Carruth (who is awesome BTW) He really is. I wish he gave more talks (or there were more of them on YouTube), because he has a very interesting and compelling speaking style.
&gt; as Microsoft almost certainly wont release the PDB format and will most probably keep moving the goalposts if it's comprehensively reversed. Let's say an alternative tool chain comes up that gains wide spread adoption. Now all of a sudden MS has to deal with bugs in someone else's PDB generation (and there will be bugs!), deal with back compat with someone else's tool chain, and so on and so forth. As it is, I am sure the PDB file format has lots of hacks just for supporting Visual Studio. I also think it is a matter of having one's own file format that can change as needed is really damn useful. It isn't a matter of moving goalposts to be annoying, so much as it is because needs change and features are added.
how can i get so much bad into so little code?
Microsoft is under no obligation to support 3rd-party PDBs. If anything, the 3rd-party vendors have an obligation to make their PDBs more compatible with Visual Studio.
I like ConceptDraw better than Visio, Dia or SmartDraw.
Depends on what the built up ecosystem would end up being worth. If it is sufficiently high to be of value, then MS could very well end up supporting it. 
For now, you can use GDB. I think LLDB support is the only viable option going forward. If it integrates with Visual Studio, it shouldn't matter too much.
Nice, these videos have been put up quicker than I was expecting. Channel 9 rules for media like this. Anybody want to suggest the stand out talks? I have been following them along live (in fact, I am doing so in the background at this very moment), but there are various ones I would like to revisit. Just wondering what others have though of the presentations so far. Of those that are currently posted, I really enjoyed STL's Don’t Help the Compiler, and Jim Radigan's Compiler++. This last one I'm really looking forward to revisiting, as I was cooking dinner as I was trying to watch it, and not doing a good job of it (watching, the dinner was alright). The last 1/3rd of Stroustrup's keynote were really good. The rest of it was good as well, I had just seen most of the same ideas before (last year, in TheC++PL 4th edition, paying attention to what is shaking up to be in C++14, etc.) From day 2, Carruth's talk on Clang was great. Seeing the debugging tools in action went smoothly and was great to see. From today, I really enjoyed Elliot H. Omiya's talk. If you can't tell from the other ones I have enjoyed, I tend to lean towards internals. Overall, all the presentations have been interesting, and if some of you haven't been following, most of them are worth watching. I've been bouncing between, and have been entertained by the apparently dichotomy, between the feeling that C++11/14 is getting simpler and easier to use, and how C++11/14 (still) has tons of gotchas and sometimes doesn't act at all how you expect.
LLVM can draw and save the CFG of the IR as a pass, not sure if this is what you need.
PDB isn't necessary. You can emit Codeview into the object files which will be correctly assembled into PDB files by the linker. This is less of a hurdle as Codeview has been released into the hands of third parties -- for example I believe Intel supports it for ICC.
&gt; A proposal to add coroutines to the C++ standard library I am excited :)
I too am interested, mostly just for finding clang binaries for RHEL5/6. They used to release binaries for RHEL5 but stopped for some reason.
So far I have Herb Sutter's talk marked for management at work and Chandler's for our team. I'm been playing with some simple refactoring using lexertl, but refactoring with Clang looks absolutely amazing!
What is this "*clicking*" of which you speak?
Hm. Wouldnt this lead to a whole new era of subtle, die-hard bugs? After some wild-west casting you obtain write access to something and bam. Ofcourse only in the release build, not the debug one...
My thought exactly...
Note from the [main page](http://en.cppreference.com/w/Main_Page): &gt; **Contributors wanted.** We're considering to expand the scope of cppreference to include general educational material. That is, things like [this article](http://thbecker.net/articles/rvalue_references/section_01.html), or [this book](https://en.wikibooks.org/wiki/More_C%2B%2B_Idioms), or what's currently here but explained in a way a beginner can understand. [Read more here](http://en.cppreference.com/w/Talk:Main_Page#cxx-book). &gt; Note: if you think you will be able to contribute, please mention yourself in the [discussion page](http://en.cppreference.com/w/Talk:Main_Page#cxx-book). This project won't be started if there's not enough interest, thus if you're willing to help, a simple "me too" message is very valuable. 
Also modules would (hopefully) improve compile times, another C++ weakness. For me it's the priority #1 feature C++ needs to implement, ASAP
~~I tried to edit cppreference once. The formatting syntax is dense and unforgiving. People will certainly need a bit of an intro before being able to assist.~~ apparently INSANE WIKICODE TEMPLATE knowledge is not needed for what they're wanting to do.
That bug is near the top of my list to fix. I feel super bad that I didn't get a chance to fix it in 2013 RTM.
2012's regex implementation is actually quite conformant. We fixed a few bugs in 2013 and there are a few more on our radar, but overall it does not contain dire correctness bugs. (Performance should be comparable to Boost.Regex most of the time, but we have difficulty with large alternations.)
&gt; I'd love to read what STL's opinion on this matter was. Me too. I don't remember saying anything like that.
There was [a post on HN](https://news.ycombinator.com/item?id=6342817) yesterday of a person writing some introductory articles on C++11. You could get in touch and see if they're willing.
2013 Update 1 to the rescue!
It's time we had a hybrid DFA/NFA engine. I'm totally prepared to help with the DFA side. It's laughable that in the 21st century regexes that don't rely on back references and/or assertions (i.e. regular expressions in the pure sense) suffer horrible performance! Oh and every man and his dog is clamouring for decent UTF-8 support. I will consider adding direct support for UTF-8 into lexertl when VC++ supports unicode string literals.
I used macro loosely here, the macro is that is spams a set of keys repeatedly for you (in random order). I put it together late at night rather fast, so it's pretty weak; cutting down its size with (real) macros would help a lot
I wonder when day 2 videos are going to be available. I have been wanting to see chandlers talk! 
I think you talk about a different 'macro' than most C++ programmers. :)
I'd recommend [Design Patterns](http://en.wikipedia.org/wiki/Design_Patterns) It is a bit old. But a lot of the design patterns are still used today. And it has a lot of case studies and examples. It is usually the "go to" book for software design. It includes: * class factories * decorator * Mediator * Observer * Singleton The examples, if i remember correctly, were in C++ too. Id definitely recommend it. It ha case studies for medium-sized projects. I wouldn't call them big, But I think their examples can easily be applied to bigger projects. Remember, its the design pattern that matters. Although the C++ examples might be dated. Design patterns haven't changed that drastically. All major mobile and GUI frameworks use MVC design patterns (although they differ in implementation).
software architecture changes based on the type of application. What type of applications are you looking to become an expert in their architecture? For instance there are a ton of books on designing good architectures for enterprise services, but obviously you are not interested in that if you are sticking to c++
The book may feel a bit dated, but definitely give it a read. Their take on the observer pattern alone is worth it.
[Code Complete] (http://www.codinghorror.com/blog/2004/02/recommended-reading-for-developers.html) by Steve McConnell. I link to a recommendation from Stack Exchange co-founder Jeff Atwood because 1) it links out to Amazon and 2) it says what's so great about this book very well. This is a book that is specifically about code construction: how to design and implement better code. It is sectioned very nicely and it cross-references frequently, so if you read it for either personal edification (cover to cover) or to get advice for a specific problem, you will be covered nicely. Its code examples come from several languages, as its advice is language agnostic, and they are written in ways to make them comprehensible whether you understand the language or not. That is how clearly McConnell codes.
With a few exceptions, I'd suggest that you don't look for books specifically about C++. Good techniques for designing and maintaining larger software applications don't tend to be language-specific, and so much of the best discussion of those techniques isn't language specific either. Having said that, the foundations evolve slowly, even if buzzwords come and go. You mentioned Lakos, and some of the ideas in there are still relevant. I also found Stroustrup's own writing useful, because one element that is relatively unusual in C++ is how much it mixes programming concepts that are more often used separately in other languages, and unsurprisingly Stroustrup has a lot of insights to share in that respect. Other than that, it's hard to make specific recommendations without knowing more about your existing experience and skill level. Have you read all the standard you-know-it books? If any of these aren't familiar, you might start there: - *Structure and Interpretation of Computer Programs* - *The Pragmatic Programmer* - *Code Complete* - *Refactoring* - *Design Patterns* Some parts of these have stood the test of time better than others, and some are more specific than others, but even if for example you're discussing why a Singleton pattern might not be such a good idea after all, it still helps to know how the idea evolved. For something a little heavier, not specifically about software architecture but full of comparative programming concepts and like C++ featuring a mix of styles, you might enjoy *Concepts, Techniques, and Models of Computer Programming*. You might also enjoy case study books like *Beautiful Code*. Although I'm not recommending specific books, I also suggest in general studying a variety of very different programming styles, such as Knuth's literate code idea, or functional programming that tries to minimize and control mutable state and side effects, or for a fascinating approach to modelling large-scale and long-lived systems, the way Erlang uses lightweight processes and message passing. I also recommend, where possible, studying the design of software that has evolved successfully over time, whether that's a big Open Source project or something like the BLAS and LAPACK mathematical libraries. These kinds of diverse influences will inform your thinking even if you wouldn't necessarily copy their strategies verbatim for any given project of your own. Getting back to books, the one thing I would caution you to avoid is anything too biased towards trendy Agile methods, which unfortunately includes a lot of work written in the past few years. There are certainly some good ideas in the Agile world, but much of the material available is focused on relatively small projects or low-level issues, without really taking scalability into consideration. There is also a **lot** of personal-opinion-stated-as-fact in Agile advocacy, and some endlessly repeated advice that just doesn't stand up to scrutiny. If you want to read people like Robert Martin, I suggest digging out some of their older and, dare I say, more thought-provoking work exploring ideas like the SOLID principles, and leaving the likes of *Clean Code* or almost anything with the letters XP in its title safely on the book store shelf.
I would recommend [The Architecture of Open Source Applications](http://aosabook.org/en/index.html) (You can read online or donato-buy) In there, developers describe the architecture of applications like LLVM, Eclipse or Audacity. I like that immensely, because software design is (still?) a bit of black art: there's no cookbook for success. We have to balance many aspects, something which you have to develop a good instinct for. As a bonus, this is an amazing example how top-level documentation should look like. 
It is a "standard book" which one should have read, but I would question the high educational value that is attributed to it. Design Patterns are just patterns we recognize in a design. It makes a good too for communication and documentation ("X is a facade for Y to ..."), but I doubt its value for actually *designing* software. I've never seen a software architecture described primarily in those terms, nor seem those patters catch the core of the design. (Neither have I ever thought in those terms for designing software - but that's anecdotical evidence and can easily be discarded.) I don't tink it's "bad" or "wrong" in any sense, I just think it's not what it is famed to be. A book listing nautical terms might not be the best way to lern how to set sail. 
It's a tour-de-force through all development, not particulary focused on design. Still something I'd make every budding developer read through back to back. 
Is there an equivalent book for C?
I agree on everything except the part on agile. Especially that you mentioned to read successful open-source('d) software and study different languages is really good advice. However, concerning agile, some books might have the drawbacks you listed, but for example, in Extreme Programming Explained, one of the standard works, it is specifically stated that YMMV multiple times. It even considers relatively thoroughly when and why some of the principles and techniques are, and are not, applicable. Just because it is hyped, doesn't mean it is devoid of reflection! Even if you end up not liking the ideas, it is still helpful to know how and why the idea evolved. Just as with singletons ;-) 
No problem! Also, I found two links you might be interested in to accompany the book: Book's source code: http://www.apibook.com/blog/source-code Reprint info: http://www.apibook.com/blog/errata
Thanks for the great advice! When it comes to programming and computer science in general, I am almost entirely self-taught. I started learning C++ with Stroustrup's "The C++ Programming Language", then went on reading some of Sutter's and Meyers' books. I even read "Modern C++ Design" by Alexandrescu. I feel that those books covered the language itself quite well for me. What I need now is to learn how to use this language to write good programs. Your comment will be (hopefully!) exceptionally useful for me. Thanks so much that you took your time to give your advice!
Read every book by Scott Myers. Then, go read them again. Then pick up his notes on C++11. Rinse, Repeat. I can not stress this enough. :)
That's pretty clever!
They're up now.
&gt; This will only work if unary operator* returns a reference, which should be true for both pointers and standard iterators, but someone has undoubtedly made a custom iterator that doesn’t. Input and output iterators are not required to return references. And, many output iterators don't return references since they must return a proxy that can be assigned to. Plus, its very common when using `boost::transform_iterator` to have a function that returns an rvalue rather than a reference. Of course, this is a problem, since now the iterator has to be demoted down to an input iterator, which is why it was proposed in the past to separate access and traversal. See [here](http://www.boost.org/doc/libs/1_54_0/libs/iterator/doc/new-iter-concepts.html). Of course, to stay on the safe side you can always statically assert that the iterator category is at least forward.
How is this different that the existing mpl?
Thanks, I should really review my assumptions before writing things like that. I changed it to use the tuple constructor for now, but that's not even a real fix. I think in the future I will write something that copies r-value references but still holds l-value references. Thoughts on that?
Looks like it's a HUUGE step up from VS2012, which in my opinion felt as if MS just dumped a buggy beta product onto its users. A lot of bugs are fixed, it has much better standard compliance, but it still feels behind GCC/clang although not nearly as bad as it used to be.
Not 2010, surely? That was the best in years.
I haven't really been following the 2013 development process much. Care to elaborate on what exactly has gotten better besides new features?
http://i.imgur.com/u85ZHzg.png
Apple isn't that much different.
Or, of course, the classic `vector&lt;bool&gt;`.
2010 intellisense was still objectively shit, but they thought it was good enough to make it difficult to [turn off](http://stackoverflow.com/questions/5400939/how-can-we-disable-vs-2010-intellisense). So that was wonderful. Aside from that it had the usual smattering of bugs in the 9th circle of hell, aka the black box known as msbuild. Of course, it will forever look great next to 2012, when they blew up the entire UI for no reason and introduced some truly amazing performance bugs.
To be honest, I haven't encountered any real bugs in VS 2010 or 2012. I suppose there was a background process that wouldn't complete at one point, but other than that any problems were simple errors in practice. Things like interdependent headers or forgetting to throw in an include guard after long coding sessions. Do you care to elaborate on what you've found? It'd be great to hear of anything that could come up in the future and cost me time. 
@Ben: Self promoting news isn't looked upon positively. If you like, message mttd periodically (at the rate he scours the internet for c++ stuffs he's either a c++ search engine or unemployed), and get him to post on your behalf. Otherwise posting your favourite bit of code that uses your library isn't really the mature thing to do. Also you always seem to neglect to mention the compile time hit that is taken when using your library. 
This paper: http://www.princeton.edu/~risteski/batch_dynamic.pdf provides a rough C++ implementation of a topological sort algorithm for DAG's described in this paper: http://www.doc.ic.ac.uk/~phjk/Publications/DynamicTopoSortAlg-JEA-07.pdf
[here](http://channel9.msdn.com/Events/GoingNative/2013). Scroll to the bottom. Herb's day 2 keynote is there, among others.
Does it work on linux?
Awesome. Thanks!
Remember folks, try out the RC but you're a fool to use it for mission critical work until it's released.
"@Ben: Self promoting news isn't looked upon positively." I thought this may be the case, but then the scores I'm getting for the articles suggest otherwise. I don't actually see what difference it makes who posts the news/article to be honest. "Otherwise posting your favourite bit of code that uses your library isn't really the mature thing to do." I'm posting snippets as I create them for genuine use cases. Either people will find them useful/informative or they won't. It would be great to get more feedback! Also, I simply don't get that reasoning. There has to be some way to learn about new technologies. If I read an article that's useful, I really couldn't care less who posted it. If there are compile time speed issues, I would like to hear about them. I haven't noticed any horrendous problems myself (we use the library at work for several scenarios) and I'm sure it compiles a hell of a lot faster than spirit. Also note that there is a code generating option for anyone who's finding that or the runtime construction a big problem for any reason.
i guess that's why some things get posted in journals and some things get posted on reddit
This is discussed in "The Design and Evolution of C++". Keyword arguments were going to be in pre-Standard C++, but Stroustrup dropped the idea after someone pointed out you could do it (less elegantly) using classes.
It is funny that an article about writing quality c++ would be written in such poor english.
It's codeproject.com. I don't expect anything but.
I grep for the assertions and found nothing. Using both stdlib assertions and assertions in your own code has been the number one way of finding bugs before they occur, thus improving the quality of the code before release. Stuff like ensuring that container and iterator usage is valid is crucial, the best part is that the cost is removed for release builds.
i really hope you're a troll
As long as he doesn't go hog-wild posting a million things that drown out all the other content on the subreddit I don't see anything wrong with it. I actually do like seeing some self-contained snippets that show how to use a library the right way to perform some real-world task.
Thanks. I'm unlikely to go 'hog wild' as I just don't do enough of this stuff to be able to do that! ;-) I will try and home in on topics that are likely to have the most interest. So far the JSON example seems the most popular (unfortunately we don't use JSON ourselves at work, otherwise I could maybe have a more interesting example.) It would be ideal if people could make suggestions (bear in mind until I have a complementary proper parser generator there are limits on how sophisticated the parsing can be!) I'm thinking it would be nice to have an example that tokenises blocks and then either applies std::regex or a different lexer to those blocks.
What do you place in the `kwget(width, w, args...);` ? 
That would be great.
Looks decent.
Unless I'm missing something, it looks like XP targeting is broke again. I'm using the 'Visual Studio 2013 RC - Windows XP (v120_xp)' compiler setting. Throws errors about an _except1 entry point missing when running the application, google has absolutely no hits for it.
Can I recommend taking this question to /r/cpp_questions. In general though, if you run a a console application from the IDE, it will pop up a command window and you'll see the output there. That window will disappear after the program finishes, so it could just be appearing an disappearing too quickly. Try putting a breakpoint in the code to pause the execution and keep the window open.
*facepalm* didn't know they existed. Thank you very much. 
This does not look as easy to use as the open multi-methods library posted earlier by Bjarne Stroustrup and the other guy.
I was excited to read about clang integration into VS including refactoring tool support and dynamic analysis support. Very, very cool!!!
Thanks, I've got my code up http://codepad.org/q4yjOA4u#output but think I'll take a look at breakpoints / trying to get it interactive. I feel like I'm dog right now. 
Boost Phoenix allows [something similar](http://www.boost.org/doc/libs/1_54_0/libs/phoenix/doc/html/phoenix/modules/statement/___if_else_____statement.html) using expression templates.
Oh, I did not know that you can initialize array to 0s in C++03 like this int a[9] = {}; Cool!
This is very average written code for me. For example, in StringRef_8cpp_source.html, I can see some magic numbers, variables named 'i' and 'j' (!), some 'return' and 'break' in the middle of a function and/or a loop, which completely breaks the readability of the code.
It worked for me when I tried it earlier today. This was using the dynamic runtime and running on XP SP3.
A quick clarification in the second paragraph: &gt; While boost libraries yet have no real task based concurrency included boost::asio can actually function very well for task-based concurrency. Once you've set up a pool of boost::asio::io_service threads, they act like a task-based thread pool (where boost:asio::strand defines serial tasks on the pool).
This is how it looks in VS: https://raw.github.com/markusl/GoogleTestRunner/master/data/vs_googletestrunner_screenshot.png 
You can only call the destructor of an object once, so you aren't allowed to call the destructor of the same object concurrently (because then you would be calling it twice). In the example from the talk when obj is deleted we first call the destructor of B, then we call the destructor of A. If a virtual function is called in a destructor the standard (12.7.4) says we call the version of the function defined in the destructor's own class (or one of its base classes). We do not use the function defined in a class deriving from the destructors class. So in this example if you were to call F during A's destructor you would not be calling B::F, you would be calling A::F. The data race comes because there is a separate thread calling obj-&gt;F(). This call might happen before or after the other thread enters A's destructor. This is a data race because the call to obj-&gt;F() isn't synchronized with entering A's destructor.
Why is throwing an exception **in different threads** going to cause the destructor of **one object** to be called **twice**. Whatever you're doing to make this happen, **it is horribly wrong**.
yes, I know that you can turn asio into a threadpool. Still, what I try to say is, that boost has no library doing this in a proper way for you. I'd like to have task groups, a scheduler that can give me the future to my task and maybe workstealing etc. 
That's not the cause of the race in the talk. The destructor is not being called in multiple threads. In fact, the destructor actively holds a lock to prevent the race. The race is unusual, and worth highlighting, because it's pointing out that vptr writes aren't protected by locks; and the empty destructor in a derived class _still_ does something.
&gt; Is this true? Yes &gt; Or phrased differently, is this something all major C++ implementations (GCC, MSVC, LLVM, ICC) struggle with? Yes, any reasonable C++ implementation would need to work this way. &gt; If so, my next question then is what happens when different threads throw concurrently, thus causing destructors to be called concurrently If you have set things up so throwing an exception can cause a destructor to be called twice, there's deeper issues with the code. A destructor must be called at most once, this is not related to multi threading either. &gt; Being careful not to throw in destructors, while not easy, seems at least manageable This is a pretty much a must. destructors must not throw, it can cause numerous problems, regardless of any multi threading issues. &gt; Or did I perhaps misunderstand something in the example? In the example there is one thread waiting (the Done() function) for the object to finish(be deleted), and another thread deleting the object. The Done() function tries to carefully apply locking , so it causes no race with the destructor (which also does the locking). However the destructor is virtual, and the locking is applied in the destructor of the base class. The destructor in the derived class has no code, but the compiler will generate some code that writes to the vtable pointer of the object - and there's no locking around that - which means there's a race. 
There is lots wrong here. Of course heap-based objects can be destructed - if you don't realise this then you don't quite get what `delete` does. Of course it is possible to share stack-stored objects between threads - you can pass a pointer or a reference (or, God help you, an rvalue reference) to such an object to another thread just as easily as for a heap-stored object. And I hope you're not using auto_ptr still...
This is great. Lets me know that Qt is still under active development. Anyone here use QtQuick/QML?
My initial reaction was something along the lines of "WTF, shouldn't they have at least a chunk of the Apollo guidance computer program?" &gt; Planetary: collecting and preserving code as a living object **tl;dr** What is happening here is the Smithsonian is doing something that, on the surface, the rest of us tend to take for granted. A *museum* has taken on the daunting task of preserving a piece of software, while maintaining the ability to use it on conventional hardware. What's extraordinary is that it seems that they're attempting to do so *into perpetuity*, which is something that is usually a side-effect of a program's success or ubiquity.
Oh hell yes. The sooner the better.
I do, for mobile apps. But haven't used it under Qt5 yet. Done some work under BlackBerry10, but thats a world of its own. What I don't like about QML: Its not strongly typed, typos in certain places don't show up, the code is not checked before startup, so you can hunt down your errors at runtime, instead of having a chance to catch them at compiletime. I like QML for UI, but everything else I'd like to do in C++.
You are right .. in C++ one has to worry about ownership anyways. I didn't think about this, when first seeing the code/explanation.
Yup I use qt daily and is my main framework. I love it.
(Couldn't (see '(that one) coming))
Yeah BB has their own UI stuff right? QML is meant to only be used for UI. It's very easy to expose different C++ types to QML so your business logic can stay in C++. The only thing I haven't figured out yet is how to do callbacks from C++ to JS. For example, a common pattern in JS is: doSomethingAsync(params,function(result){ processResult(result); }); I can't figure out why this doesn't work in QML.
I know it's sold for that, but IMO it's best at UI and UI interaction. Anything else needs C++. As for the model typos, been there done that.
QML is the language, QtQuick 1/2 are libraries, as is cascades.
If you thought qt were dying then you should do a quick search for qt-project. 
Nahh... I'd suggest going with Yaml! JSON is a subset of Yaml so any JSON document is a Yaml document. It's more readable than JSON - but more, it fixes some of the broken issues of JSON - particular flow control! (i.e. sending more than one thing in a single document, or in my case, over a TCP/IP connection). The [yaml-cpp](https://code.google.com/p/yaml-cpp/) library is solid and established. It's a crying shame that they moved to being dependent on Boost for tiny things - for an insignificant change, they require a 100MB download! - but I use the earlier 0.30 release which is very stable and doesn't require that.
Why did they never figure out a way to make delete and free exit safely if for some reason the object had allready been deleted or freed?
I'm in the [progress](http://plane9.com/content/blog/20130705-Config.jpg) of convert my WinForm configuration window to QML and I must admit, it's very nice to work with compared to all other GUI technologies I have tried. 
&gt; If we try to write this program recursively using the technique we mentioned earlier, we find that each recursive call passes the entire result array as an argument. As a result, the program's runtime will be quadratic in the number of iterations Given that we're assuming the compiler will perform tail recursion optimisation, this simply isn't true.
Ah, the "Gone Wild" flag
It's had /Gy unused function removal since at least VS2005. /Gw (the topic of the article) is about unused variable removal. I have no idea if the GCC ffunction-sections and fdata-sections are competitive with this. Going by the warnings in the gcc docs, it suggests perhaps not
const data.
Most upvoted comment in thread so far
So where are these jobs? 
The key to which language is best is to add: "for what?" For a quick data accessing web page PHP is certainly better than C++ in most circumstances. But to write the PHP interpreter itself then C++ is probably best with even potentially C beating even that. Personally I am huge fan of Python and some things like Numpy can make even plowing through huge data sets fairly fast but when I need to torture the machine to its limits then C++ is pretty damn good. So the question of where C++ is best would be answered: You need C++ when you are going beyond where others have been, way beyond. So if you are making the next great game engine, the most incredible vision library, the code for controlling a robot entire set of motors in real time then C++ rocks. But if you are using the C++ vision library in a more mundane (programmically) application then there are all sorts of languages where you will probably fight with the language less and thus develop faster. The other litmus test for using C++ or not is: Are all your programmers quite competent? With C++ you can hang yourself so quickly that one weak link in your chain can kill your project. With some languages I have seen fairly incompetent programmers only slightly annoy their coworkers and hardly damage the project. 
How exactly would you do that? You don't have any way of knowing if the pointer has been deleted already. That's one reason why the "set to null after delete" idiom is so prevalent. In that case a double delete is harmless, since delete null does nothing (though the program still should not double delete in the first). The only way to know if a pointer has been deleted or not is to track metadata about every pointer your allocator hands out. That's really expensive, and as such usually only done by dynamic analysis tools.
I was going to say: great a Gee Wiz flag!😝😝😜😅😅. However I see somebody offered up a better phrase in "Gone Wild" which seems to fit MS state of late. 
&gt; with even potentially C beating even that. ?? Why would C be a better choice? The only thing I can think of where C beats C++ is simplicity of language implementation, which means that C compilers exist for more platforms than C++ ones. And even that gap is shrinking with things like LLVM that provide reusable compiler components.
Sorry for the late reply, but what you described is probably the way to go. Or you could just always use the `iterator_traits&lt;Iterator&gt;::reference` type which would be a little simpler than trying to deduce the rvalue reference and removing it. something like this: template&lt;class T&gt; struct iterator_reference : std::iterator_traits&lt;typename std::decay&lt;T&gt;::type&gt;::reference {}; template&lt;class Tuple, int... N&gt; auto dereference_impl(Tuple &amp;&amp; t, std::integer_sequence&lt;int, N...&gt;) const { return std::tuple&lt;typename iterator_reference&lt;decltype(std::get&lt;N&gt;(t))&gt;::reference...&gt;(*std::get&lt;N&gt;(t)...); } 
I think this has a bug: OutputDebugString(str().c_str()); The std::string returned by str() might destruct before the C string makes it to OutputDebugString, at which point it's unallocated memory. Right? (I was on a small team where this problem bit us many times as we all learned C++ together. ;p)
Yes with LLVM things are getting interesting but C code can compile tighter and faster. When you look at things like drivers or core bits of most OSs they are in C and ASM. 
They've disappeared into the woodwork in my world - it's now only very low-level, hardware-only stuff. I guess at 'application' level, there's still games but... 
'task_group' looks extremely light on the interface side. A way to merge task_groups strikes me as an obvious omission... it is after all just a container of tasks. It also seems to repeat std::futures mistake of staying silent on whether or not there's any thread pooling going on behind the scenes. It just looks like it's already supplanted by the executors and schedulers stuff, and doesn't seem to offer a lot of value on its own. I like std::future.then()... I'm not entirely convinced that the lack of templated executors is ideal. I'm dubious about the choice to make it call std::terminate() too. I'm sure template based policy could make this more customisable. If the tasks under an executor are related, you might want to call a common handler or something, which decides whether the executer should continue or flush its queue. Executors also model container (a queue), with policy through subclassing rather than composition. The proposed base class has a tiny interface, and just reminds me a lot of the way std::iostreams are implemented using virtual functions and lack decent template policy. One thing I do like is how serial_executor takes another executor for it's output. Why not make all the executors chainable so pipelines can be created? Could be useful if different executors pin their threads to CPU sets. It's also *great* to see Christopher Kohlhoff propose something with a big picture view amongst all these fine-grained changes, although I'm not sure I like his proposal at all.
The author does call it out: &gt; Yes, this can throw an exception (both str() and OutputDebugString could throw), so you can avoid this choice,
I have the other one ;). Joking aside, there is good deal of those jobs i imagine (of course those probably wont be in webapps paradigm, where most of development appears [to me] to be right now). At least couple of big corporations do need people with low level/native skills (and this isn't all that C++ is being applied to).
How about a function which takes a map&lt;string, boost::any&gt;, called via universal initialization syntax: foo({{"key", value},{"key2", value2}}); And maybe with some macros to clean it up (help with the declaration of the function, and getting parameters out of the function during definition)?
[The product page](http://www.jetbrains.com/objc/features/cpp.html) says they are using the clang analyzer. Is all the backend C++ processing handled with Clang? I would love to escape from the last decade of C++ IDE and their pour support for advanced language constructs.
oOo... Just them shipping a precompiled window clang compiler would almost be enough to switch.
Looks quite cool for an early demo! I have two questions, though: * It seems that it's built on the IntelliJ platform. So it requires Java for running the IDE, right? (Nothing inherently wrong with that, just curious) * Why is it possible for them to use the Clang frontend, while the Qt Creator guys are struggling with its performance since years or at least month? (I think there's a experimental Qt Creator branch with Clang based parsing, but every time I read articles about the integration, it's still said to be slow and not really usable, yet)
If you click the link, it should bring you to 53:28.
clang now even supports Visual Studio. http://blog.llvm.org/2013/09/a-path-forward-for-llvm-toolchain-on.html
* There are two versions. A Java based one, which is what InteliJ uses, and a .NET based one, which is what ReSharper uses. * This might be lack of skills in the QtCreator team. XCode has clang integrated and works quite fast.
Meh, it's again one of their Java based IDEs. It will be slow and the UI will feel alien on every OS.
I understand your concern, but when I tried PyCharm it was orders of magnitude faster than Eclipse or VS both in startup and resource use. It was perfectly usable on a 4 year old, 1gb ram, 32bit laptop. Sure, it's not a text editor, but it's not trying to be. As for UI, I don't care if it is workable and works well. Looked fine on linux at least.
I must agree with you, even though IDEA is a very nice IDE. I tried using IDEA on my Linux box, but I cannot for the life of me get text to render nicely. Antialiasing etc is working fine, but the text is very fuzzy and odd looking, which in the end ruined IDEA for me.
Works fine for me. Try launching with: export _JAVA_OPTIONS="-Dawt.useSystemAAFontSettings=on -Dswing.aatext=true -Dswing.defaultlaf=com.sun.java.swing.plaf.gtk.GTKLookAndFeel" idea.sh
Well known JDK problem: http://youtrack.jetbrains.com/issue/IDEA-57233 It uses its own font renderer. So you need to install patched openJDK.
Yeah, don't do that.
[Here you go](http://i.imgur.com/J7SDACJ.png). Is it any better?
~~I don’t think “evil” is the correct word. Sure, this can fail in a lot of ways if you’re not careful but so can other custom allocators with peculiar requirements, such as a fixed-sized pool allocator. This is what allocators are *made for*.~~ EDIT: False. See /u/purevirtual’s answer. --- And using memory-mapped files for inter-process shared memory is an established technique, and also not something I would call “extra evil”. 
Somewhat, which font are you using?
Consolas. I also have this in my `~/.fonts.conf`, not sure if it changes anything: &lt;?xml version="1.0"?&gt; &lt;!DOCTYPE fontconfig SYSTEM "fonts.dtd"&gt; &lt;fontconfig&gt; &lt;match target="font" &gt; &lt;edit mode="assign" name="rgba" &gt; &lt;const&gt;none&lt;/const&gt; &lt;/edit&gt; &lt;edit mode="assign" name="hinting" &gt; &lt;bool&gt;true&lt;/bool&gt; &lt;/edit&gt; &lt;edit mode="assign" name="autohint" &gt; &lt;bool&gt;false&lt;/bool&gt; &lt;/edit&gt; &lt;edit mode="assign" name="antialias" &gt; &lt;bool&gt;true&lt;/bool&gt; &lt;/edit&gt; &lt;edit mode="assign" name="hintstyle" &gt; &lt;const&gt;hintfull&lt;/const&gt; &lt;/edit&gt; &lt;edit name="lcdfilter" mode="assign"&gt; &lt;const&gt;lcdnone&lt;/const&gt; &lt;/edit&gt; &lt;/match&gt; (... misc. font substitutions ...) &lt;/fontconfig&gt;
or just use qt, as all qt data types have serialization built in... I'm using qtcore more and more for non-gui stuff, it's just so damn convenient.
Qt also makes it easy to add serialization to your own classes. You just need to implement stream operators that return a QDataStream. 
I have no idea why you are being downvoted so heavily. QVariant together with QVariantHash make it hilariously easy to have a nested key-value storage with reasonable type safety and trivial serialization. 
No, it's not evil. It's *dangerous*... but used in the right context it can be a very powerful tool (EG embedded system, guaranteed same memory layout/processor, etc) Obviously you'd never want to use this as a generalized serialization solution but this type of system has been used in video games for fast restoration of save games. Note you also wouldn't want to use this with multi-processes unless you had a good system-wide mutex capability (Or can guarantee write once-read many)
Because pulling in Qt as a dependency when you just need serialization is a bit laughable. There are smaller, more focused, libraries out there.
Two things that come to mind: With Qt you have to call the serialisation function explicitly which is an O(num or items in container) operation every time. With this approach the backing file is updated automatically by the kernel. You don't have to write serialisation functions for objects, you just put them in the storage, which is especially nice if you don't have their source code or can't change it for any reason. PS I'm the original author of the article. If anyone has any questions just ask them here.
&gt; With Qt you have to call the serialisation function explicitly which is an O(num or items in container) operation every time. With this approach the backing file is updated automatically by the kernel. Sure, but in most cases you don't do it too often, and the overhead from serializing something isn't really something which I've thought about tbh. Cool hack, but I'll stick to Qt's file &lt;&lt; object; for now.
That's what I would stick with, too. :) It is the right thing to do 99.9% of the time.
The thing that makes this method evil is the requirement that the address be mapped to the same location in every process that needs to use the memory. It is *not okay* to pick an address randomly and/or hardcode it into the program. mmap on linux will replace any mapping at that address with your new one, so using fixed addresses with mmap can break literally anything (including the default malloc/new implementation). Address space randomization makes it harder (or impossible) to know ahead of time an address range that will be unused in a process. Further, the whole point of address space randomization is to prevent a specific class of security problems that can be caused when parts of a program or it's data have predictable addresses.
well, that's just one of the nuts it is good for.
I'm curious - not that this would be right or wrong (I have never attempted to study a rubix cube algorithm) - but did you think about a linked-list approach? Perhaps something like: class piece { piece * up; piece * down; piece * left; //... etc }; With that approach it might make it easier to debug because at any time you could print a map or navigate the map (though it looks like you have the ability to spit out a map already). Also - just an FYI - clock (AFAIK) doesn't return ms but the number of ticks the program took to complete. To get the "real time" you divide the delta of start and now clock results and divide by CLOCKS_PER_SEC to [get the number of seconds](http://www.cplusplus.com/reference/ctime/clock/). I believe C++11 also has some time stuff. See an explanation [here](http://stackoverflow.com/a/647613/195722). I would be really surprised if clock() spit out ms on every platform. Just a friendly warning. Also I'm curious on stats - runtime and memory usage. If you don't know how - Windows: you can use [this](https://www.dropbox.com/s/6l1jv745u92tb8q/timemem.exe) tool I found a long time ago or [this](https://code.google.com/p/time-windows/) one. I think they are both the same thing. Linux: use /usr/bin/time -v Oh and you really should apply a license to it (I'm a personal fan of Apache or MIT).
I'm always interested in "Elapsed (wall clock) time" (this is the total time the program took to run - which would include a call to sleep for example - std::clock does not time that. I don't think you use sleep but I'll trust the wall clock time over std::clock :) ) and "Maximum resident set size" (highest amount of memory used). The other stats are of course interesting as well if you are into benchmarking - but most probably won't apply to this application.
Also worth checking out [Cap'n Proto](http://kentonv.github.io/capnproto/). It's by the primary author of Google Protocol Buffers but fixes things. Still under dev though but the core seems done.
It looks like a cool project. I've just got a couple of point about the code rather than your implementation. From [this line](https://github.com/shafiry/rubik/blob/master/cube.cpp#L420) I'm assuming you're using C++11. * It doesn't look like your destructor, copy constructor or copy assignment operator for piece do anything other than the compiler generated ones. So I would just remove these (or use = default) and let the compiler generate them for you to avoid extra work if you add more member variables. Also specifying a copy constructor will automatically disable move constructors (not that it will make a difference for piece at the moment but it is something to remember). * You never use the member variable moves in the class cube so you could remove this (and then the copy constructor/copy assignment operator can be removed from this class as it will be the same as the compiler generated ones). * If you are going to generate copy assignment operators, the [Copy-And-Swap](http://stackoverflow.com/questions/3279543/what-is-the-copy-and-swap-idiom/3279550#3279550) idiom is a very nice alternative to checking for self assignment. * I would get into the habit of using preincrement operators over postincrement operators because it is just a (small) speed increase in most cases (I know compilers can optimise this in the case of integers but there's no harm in using the preincrement by default). * Is there any reason why the colours are not an enum instead of const ints? * I would reimplement your showcube() method as operator&lt;&lt;. * I would avoid committing any build files to your repo (*.o files etc). * Includes aren't in alphabetical order :) I'd like to see if you could extend the code to an NxNxN cube. EDIT: whoops! wrote = delete when I meant = default
Tried building in MSYS and got an issue with throwing domain_error. Turns out you didn't include &lt;stdexcept&gt; in cube.cpp and that fixed the error. Running cubesolved.exe afterwards results in the error: "terminated called after throwing an instance of 'std::ios_base::failure' "; I'm going to try to fix this. EDIT: Must have been a problem with MSYS / MinGW on this computer. Created a quick visual studio solution and it worked no problem. I have to say that some more commenting would have been nice. It's nice when code doesn't have too much commenting, but I'd rather try to read code with too much rather than too little. Sometimes it's better to write down not just what a function does, because that should be obvious due to the name, but rather How or Why it does what it does. 
Congrats on taking your time to really learn about the language and its main constructs. Most people, including myself, went the other way: learned a bit of c++ from tutorials, picked up an application-specific book, say on game programming, and then were repeatedly bitten by the holes in our core c++ knowledge. Once you have a solid grasp on the language, things become a whole lot easier. Just think about something cool that you'd like to build, learn about the main libraries/frameworks that you'd need to learn for that area, and just dive in. For example, if you're interested in games and graphics, you could take a look at DirectX/OpenGL (or lighter weight libraries like cinder and openFrameworks). If you're interested in developing UIs, you can pick up something like Qt (or use whatever library your platform uses). You could learn about sockets and network programming, and build some back-end service using those. You can go as high or low level as you want, and I'd recommend Casablanca if you want to get up and running quickly: http://blogs.msdn.com/b/vcblog/archive/2013/02/26/the-c-rest-sdk-quot-casablanca-quot.aspx If you're more interested in algorithms and simulations, there's not much you have to do, you already have all the tools under your belt! Personally, I feel that after I'd learned the basic language, working with existing large codebases for a few months during summer internships at software companies really helped me grow as a programmer. Reading others' code and going through code reviews is an excellent way to improve the way you code and design your own projects. A great way for anyone to do this would be to find an open source project that they're interested in (say, on GitHub), and just jump in by getting familiar with the codebase, learning about the design decisions, and fixing bugs. Most open source projects are not purely computational/algorithmic, so often you will get exposed to some application area (such as graphics, games, networks, UI, etc), and after a few months of this you'll have a solid foundation to decide what you want to do next. Good luck!
&gt;This might be lack of skills in the QtCreator team. XCode has clang integrated and works quite fast. Ouch.
A little off topic, but whenever I need some quick and dirty Object to disk code I use the following. Obviously doesn't do any deep copying but it's useful for basic classes void MyClass::write(const std::string &amp; filename) const { std::ofstream fout (filename.c_str(), std::ios::out | std::ios::binary); fout.write((char *)this, sizeof(*this)); fout.close(); } void MyClass::read(const std::string &amp; filename) { std::ifstream fin (filename.c_str(), std::ios::in | std::ios::binary); fin.read((char *)this, sizeof(*this)); fin.close(); }
&gt; I would reimplement your showcube() method as operator&lt;&lt;. Would it be better to create a getcube() that returns the cube as a string which then could be used in the &lt;&lt; operator and perhaps to store in a vector to output later as the steps taken or in unit tests? Edit: I suppose that one could use stringstream and use the str method to get a string.
You have a few options, depending on what you want. You could go with a widget library, such as [Qt](http://qt-project.org/), or [wxWidgets](http://wxwidgets.org/). These are cross platform and allow you to easily create windows with common controls (buttons, text areas, et al.). There is also [SFML](http://www.sfml-dev.org/), which is more oriented to games and doesn't provide high-level GUI elements (buttons, et al.). You could manually do everything with OpenGL or DirectX. This is overkill for a simple application with typical controls. You would still need a cross-platform window library, such as SFML. If you know HTML, Javascript, and CSS, you might enjoy using [Awesomium](http://www.awesomium.com/). It uses the same robust engine as Google Chrome, and you can easily link Javascript and C++ together (a button calls a C++ function, C++ sends Javascript commands to manipulate the page). This is probably the easiest, assuming you know at least basic HTML and Javascript.
Add #include &lt;cmath&gt; to the top of the source file, and see if that helps. See: http://www.cplusplus.com/reference/cstdlib/abs/ http://www.cplusplus.com/reference/cmath/fabs/
That seems to have done it. Thanks for the help. Not sure why he would have left that out.
Congrats on learning C++! Some other posts here linked some good tutorials, but I just wanted to give you some general advice. Instead of just trying to learn to make GUIs or something, I think you should pick a project. What are you interested in? Maybe make a simple game, or music player, or maybe a (musical) keyboard, or something that gets information from the internet and does something cool with it. Once you pick a project, then you'll have a good idea of what you need to learn to implement it. You may not be able to get it working right away, but I always find it useful to have some motivation
If he's good, he may have freehanded the code and not tried it before giving it to you. This is common, I had a professor who would occasionally write code on the blackboard, tell us to copy it down, and then to go home and make it work. In the real world, at least 90% of programming effort is debugging, and it's often someone else's code, so this is good practice.
I tend to like longer variable names in general, so I would have given the member functions for the different moves (lu, ld, mu...) the full statements of their names, what's currently the comment for each of them (leftUpwards, leftDownwards...). But using just the acronyms does work if you don't mind coming back to your code in a week or two and reorienting yourself. Though, that may not be an issue for you.
&gt;I've never seen a software architecture described primarily in those terms, nor seem those patters catch the core of the design. (Neither have I ever thought in those terms for designing software - but that's anecdotical evidence and can easily be discarded.) Perhaps you don't see it in the documentation, but I am willing to bet the authors of the framework do know it and try to use it. Java is probably the best example. It has an Observer object and class factories are used all the time. If you do Iphone development, one often encounters Singleton object. I also just read the RequireJs framework where it mentions how they went of out the way to follow the Modular Design pattern. Good programmers know how to design a program using patterns, especially if they are designing complicating software with a lot of relationships.
Yeah that would be fine imo. Then if you needed to have operator&lt;&lt; it could just be done easily as a non-member function: std::ostream&amp; operator&lt;&lt; (std::ostream&amp; stream, const cube&amp; cube){ stream &lt;&lt; cube.getcube(); return stream; } The only possible benefit I know of having operator&lt;&lt; rather than a function returning a string (except for the subjective view that std::cout &lt;&lt; cube; is nicer than std::cout &lt;&lt; cube.getcube();) is that if you have some strange templated function to do something with an object and print it to a stream, it will almost certainly call operator&lt;&lt; on it. Though that's really just a (weak) argument to have it as well rather than instead of.
I would like to point out that currently the method names conflict with the [Wolstenholme notation](http://www.topaccolades.com/notation/rubikscube.htm) and this could confuse any people who are familiar with it. Also if these functions returned a reference to the cube you could nicely chain them outside of the class (e.g. cube.FO().FA().DI(); using Wolstenhome notation).
Patterns have a valid place in the *details * of architecture, they are marginal in the context of OP's question, though. --- If I had a penny for each *"Design Patterns: The {Singleton|Class Factory}"* tutorial out there, I could probably treat myself to a great dinner. Singletons are, at best, a *technique* managing a global resource. Architecture and design, however, would be concerned which resources can, should or need to be global, and the different kinds of global. (More often than not they just seem to be a convenient *facade* for global application state). Class factories at least require a bigger context: If you have a certain type of object hierarchy, polymorphy often needs to extend to creating those objects. The topic of architecture - in my understanding - would be how well that class hierarchy models the original problem, what alternatives are there and what hidden cost - such as the need for factories - this choice carries. "I'll use an Observer here" I don't see as design choice as such. "All configuration properties use a publish/subscribe mechanism" would be. It has been argued more or less sucessfully that a large segment of design patterns could be renamed "workarounds for commn problems with OO languages", the visitor pattern probably being the most obbvious one in this group. --- I concede that Design &amp; Architecture needs a dedicated vocabulary (much as civil engineering or law), that Design Patterns are *part of* that vocabulary, and indeed probably ther most well-defined part. That appears to me only for the lack of something better. 
Thanks, I had indeed completely failed to consider that.
`Database`'s constructor should accept a `std::string`. Why is `Query`'s constructor public? Should the client code use it? Why is `Result` derived from `Query`? Does it make sense to convert a `Result` to a `Query`? 
Fixed. Thanks for the input.
And now there's no point for `protected` any more, right? Since `protected` means you're supposed to inherit from the class. (It's more of a pet peeve of mine though: as soon as I see "`protected`", alarm bells ring in my brain.) Even more nitpicking: add `Results::cbegin()` and `Results::cend()`. Also, in your example, add `const`: for ( auto const &amp; row : results )
&gt; Database db("test.sqlite", "johns", "1234"); //passing username and password How do you put a usename and a password on a SQLite database?
Protected to private - good idea. As for the cbegin/cend addition - is the range based for loop actually going to use these? Or would they only be used when used explicitly in a normal iterator based for loop?
Windows is a mix of C, C++ and C# apparently, also there is no intrinsic speed gain from using C over C++ and infact C paradigms such as mallocing everywhere and passing pointer around can prevent compiler optimizations. source for the windows build langs: http://social.microsoft.com/Forums/en-US/65a1fe05-9c1d-48bf-bd40-148e6b3da9f1/what-programming-language-is-windows-written-in 
Has something like this been proposed and rejected previously? It seems like such an obviously important thing to have that I'm left wondering whether there's some critical flaws with the concept or if it's just that everyone's minds have been poisoned by decades of using configure scripts for this.
not sure. There is an earlier paper from July on this (this is the follow up), but I agree that the idea is pretty good. I hope the committee shares this view.
Fabien, to better support queries beyond the standard SELECT, I'm thinking of making Results inherit from Row to allow for things like int count = db.Query( "SELECT COUNT(*) FROM test" ).column_int( 0 ) It's easy to implement but what would you think of this design? Terrible or useful? Or is there a better way to do a shortcut like this? 
No. The Clang analyzer is used to display diagnostic messages; parsing/reference resolution/refactoring etc. is our own implementation.
Why did you go and do that? What issues did you find with the Clang APIs? I was really hoping for something that had cutting edge template support, and understood every construct Boost, the STL and C++1y could throw at it. I wish you guys luck, but from what I have seen all IDE specific parsing efforts fall short. The C++ grammar is so complex, and only getting worse.
&gt; SELECT COUNT(*) FROM test Don't you mean "`SELECT COUNT(1) FROM test`"? (TBH, I'm not sure the difference matters in SQLite; maybe it's able to do the optimization itself?) &gt; what would you think of this design? Catastrophic. It pretty much means that a `Result` is a `Row`, which means, your API doesn't handle requests that return several rows. What you can do: `-` A function Row Database::QueryOneRow() (that throws if the answer isn't one row) would allow you to write: int const count = db.QueryOneRow( "SELECT COUNT(1) FROM test" ).column_int( 0 ) (Don't forget const-correctness, BTW!) `-` An operator Row Results::operator[] (int row_number) const; would allow you to write: int const count = db.Query( "SELECT COUNT(1) FROM test" )[0].column_int( 0 )
The Xcode guys might also work in the same building as th Clang team considering they both work for Apple. That probably helps quite a bit when it comes to integration issues.
I'd be adding this to the current possibility of iterating over the results using begin() and end(), but even that way I understand it could be confusing. However, adding the [] operator is not really an option since sqlite itself only allows for iterating over the results sequentially.
What exactly do you mean by "this"? (Really, the word "this" should be banned from Reddit, once and for all.)
Well, If you have a standard that says “I require A, B and C” in the one place, It is somewhat weird, if it says “If A is provided, the macro A_EXISTS shall be provided too”. Because technically it always requires the macro. Note that C++ is, unlike other languages, not defined in terms of an implementation, but in terms of what an implementation has to provide (-&gt; everything).
 class Results : public Row { // Row interface to fetch single row results easily public: begin() end() etc } so I can do int count = db.Query( "SELECT COUNT(*) FROM test" ).column_int( 0 ) but also still for ( auto &amp; row : db.Query( "SELECT * FROM test" ) ) { // like before } 
Therefore, as I said, a `Result` is a `Row`. If you want to allow multi-row results, you'll end up with a row that contains several rows, and your library is now incoherent. 
Or just implement `first()`. Thing is, I find that I often make queries that return only one line. So, if this was my code, I'd go with the `QueryOneRow` solution. 
Look at this code. What do you expect it to do? Why? Now run it. Did it do what you expected? Results res= my_db.query ("SELECT 1"); auto it1= res.begin(); auto it2= res.begin(); if (it2 != res.end()) { std::cout &lt;&lt; "The result is: " &lt;&lt; it2-&gt;column_int(0) &lt;&lt; "\n"; } else { std::cout &lt;&lt; "Nothing to show!\n"; } (I'm too lazy to fire my compiler, so, I admit I haven't tested the code.) 
True. Use either `shared_ptr` or `unique_ptr`, depending on the context.
I see what you're getting at. See bottom of README.
It's not unprecedented (__bool_true_false_are_defined), and __has_include is applicable for more things than just testing which parts of the standard library are available.
Yeah. I guess at some point I was thinking of giving access to the row index within the for loop when iterating over results. But that's not such a useful feature.
Maybe using a container-type interface is not the right choice. Yeah, I know, it's nice on paper, but it doesn't quite work. Try this experiment: remove the `Cursor` and `Results` classes, and use functors. template &lt;class F&gt; void Database::query (string const&amp; query, F&amp; f) { sqlite3_stmt *stmt = 0; sqlite3_prepare_v2( db, query.c_str(), (int)query.length(), &amp;stmt, 0 ); Row row (stmt); while (sqlite3_step( stmt ) == SQLITE_ROW) { f (row); } } Row void Database::query_one_line (string const&amp; query) { sqlite3_stmt *stmt = 0; sqlite3_prepare_v2( db, query.c_str(), (int)query.length(), &amp;stmt, 0 ); return Row (stmt); } And you could use it like this: int main() { Database db( "test.sqlite" ); db.query ( "SELECT foo FROM my_table", [] (Row const&amp; row) { cout &lt;&lt; row.column_string( 0 ) &lt;&lt; "\n"; } ); int const fourty_two= db.query_one_line ("SELECT 42"); } 
A way to improve matters a bit: put all the logic in `Results`, not in `Cursor`. Or, refuse a second call to `Results::begin()`.
I would go so far to say that I wouldn't really want either of those in the actual interface if it could be avoided – but internally if you are using a pointer I would always go that route.
Why would one use this in preference to leveldb?
I tried it and it worked but I didn't like how query parameters were passed after the functor. However I did add a query_single call that returns a single row. It should also be used for UPDATE and INSERT queries.
It's in now.
Google also supports clang development.
We already have an IDE platform written in Java, so we essentially had a choice: we could either reuse our existing IDE platform and write the C++ support from scratch, or we could reuse Clang's C++ support and write a new IDE platform in C++ from scratch. Given our existing investment into the platform and our experience supporting other languages on top of it, the former was a much better choice for us, although I do admit that it'll take us a while to have full C++ standard compliance. (And no, the standard compliance is not a matter of grammar at all; that's the easy part.)
&gt; It should also be used for UPDATE and INSERT queries. What's in the row those queries return?
&gt; I didn't like how query parameters were passed after the functor. Well, reverse the order of pameters in Database::query!
How do they work, though, when you're supposed to use `sqlite3_finalize` instead of `delete`?
 sqlite3_stmt *stmt; // call sqlite to initialize stmt here. std::shared_ptr&lt;sqlite3_stmt&gt; ptr( stmt, sqlite3_finalize ); I have an updated version which uses shared_ptr where it helps and I have also improved the API. Will push to Github in a few hours.
Thanks for the background. I definitely understand the desire to keep the platform clean. I don't know the details but I assume writing, maintaining and integrating a Clang java wrapper was seen as too much work compared to the gain.
Especially given that LevelDB has a permissive BSD style license see http://code.google.com/p/leveldb/source/browse/LICENSE And Vedis has a Sleepycat License (like GPL in that you have to release your source code if you use Vedis) and have to buy a commercial license if you want to avoid disclosing your sources. http://vedis.symisc.net/licensing.html
I this really not a fork of Redis? Looks like the same folks ( no pun intended ) who seemed to have forked SQLite and called it [UnQLite](http://www.reddit.com/r/cpp/comments/1li2yh/unqlite_a_transactional_keyvalue_store_for_cc/) Interesting FAQ ( question #10) on both libraries 10.Is Vedis a fork of Redis No, Vedis is a completely independent product written from scratch, developed by Mrad Chems Eddine (chm@symisc.net) at Symisc Systems. 10.Is UnQLite a fork of SQLite No, UnQLite is a completely independent product written from scratch, developed by Mrad Chems Eddine (chm@symisc.net) at Symisc Systems. Despite that, UnQLite share some SQLite low-level components such as the VFS (Virtual File System), the pager layer and the locking mechanism. 
In the title of an article is a question, the answer is usually negative.
Why is this in /r/cpp ? I don't see any code in this.
&gt; Did you stop using it because it's missing some critical feature you can't live without? Not to sound flippant, I've never started using lldb. GDB is an absolutely amazing debug utility, so I haven't had to look else where. Also - I don't use llvm, so I'm not tied into the llvm project's suite of tools. I think the question you should be asking if you want greater adoption of lldb is actually advertising it's features in the general sense. I don't know anyone (professionally or otherwise) that reaches for lldb before gdb (we're talking C++ here, not obj-c ..). I've read through http://lldb.llvm.org/lldb-gdb.html, and it looks like a debugger? Any unique features that it does that gdb does not? I'm just not familiar with it.
Tried it a couple of month (or even years?) ago, and to be fair it probably was a bit to early. But it already had some cool features at that time - I mostly just played around with it, following a few guides about the Python integration and such. Also, it's not packaged for my distro (see https://bugs.gentoo.org/show_bug.cgi?id=464354), so it's a bit of a pain to keep it up to date. But I would definitely use it again, especially with the features that are supported on Linux by now.
Weird, it didn't do that on my tablet.
Now updated with improved API. 
I tried it out a few months ago, but gave up after it crashed on me a few times.
Only thing I use GDB for really is aborting code, because I couldn't be bothered to hook up my stacktrace code to SIG_ABRT. Other than that, I don't really use debuggers. I've got unit tests, they tell me so much more. What's LLDB going to do for me?
To answer my own question, I stick with GDB because I can't work without the curses source layout (layout src). LLDB doesn't provide a nice source view yet.
I need to create a program to maintain information for a class of students. Each student has a name, 3 hw grades, and 2 exam scores. I am using a structure for each student and an array to hold all 5. I'm not sure why my array is not working correctly. Help will be greatly appreciated!
I am trying to figure out why my structure and array are not working. Any help will be greatly appreciated. Thanks! 
Some things I like in lldb: * Full C++ parser, so expression evaluation "just works" * Internal documentation (I find lldb's help command much more useful than gdb's) * Use the debugger as a library using the python API
it is unreadable use something like this to show the code: http://pastebin.com/
There are a lot of problems in the code; if you're trying to get help with a problem, start at the top of the compiler output and if you hit an issue you can't fix, post the error. This looks like an [xy](http://mywiki.wooledge.org/XyProblem) problem. You should explain the issues you're seeing instead of being vague about structs and arrays 'not working'.
http://pastebin.com/7zXYqMpw looks much better
&gt; Full C++ parser, so expression evaluation "just works" Do you have to use `clang` for that functionality or will it work with `g++` binaries?
Let me quote the sidebar for you: &gt; Discussions, articles and news about the C++ programming language or programming in C++. &gt; **For C++ questions, answers, help and advice see /r/cpp_questions.**
My favourite use of a debugger is the backtrace.
I still have his original code, in case you were really curious #include &lt;cmath&gt; #include &lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;string&gt; using namespace std; struct Angle { int degrees; int minutes; double seconds; }; typedef Angle&amp; AngleRef; const int MINUTES_PER_DEGREE = 60; const int SECONDS_PER_MINUTE = 60; const int SECONDS_PER_DEGREE = 3600; const int DEGREES_SYMBOL = 248; // Inputs the degrees from the keyboard. double inputDegrees(string prompt); // Creates an Angle structure. Angle createAngle(double fractionalDegrees); // Outputs an angle to the screen. // Formats the angle as: dÂ° m' s.s" void displayAngle(string label, Angle angle); // Converts an angle measurement in degrees, minutes, seconds to a // fractional degree value. double angle2Degrees(Angle angle); // Returns an angle that is the sum of its two parameters. Angle addAngles(Angle angle1, Angle angle2); // Returns an angle that is the difference of its two parameters (angle1 - angle2). Angle subtractAngles(Angle angle1, Angle angle2); int main() { Angle angle; double fractionalDegrees; fractionalDegrees = inputDegrees("Enter angle measurement in degrees [145.1134]: "); angle = createAngle(fractionalDegrees); displayAngle("In degrees, minutes, seconds: ", angle); Angle angle1 = {30, 30, 30.0}; Angle angle2 = {140, 45, 45.5}; displayAngle("\n\nComputing:\n", angle1); displayAngle(" + ", angle2); displayAngle(" = ", addAngles(angle1, angle2)); displayAngle("\n\nComputing:\n", angle1); displayAngle(" - ", angle2); displayAngle(" = ", subtractAngles(angle1, angle2)); cout &lt;&lt; endl &lt;&lt; endl; return 0; } // end main() // Inputs the degrees from the keyboard. double inputDegrees(string prompt) { double fractionalDegrees; cout &lt;&lt; prompt; cin &gt;&gt; fractionalDegrees; return fractionalDegrees; } // end inputDegrees() // Creates an Angle structure. Angle createAngle(double fractionalDegrees) { int degrees = (int) fractionalDegrees; fractionalDegrees = fabs(fractionalDegrees); fractionalDegrees -= abs(degrees); fractionalDegrees *= MINUTES_PER_DEGREE; int minutes = (int) fractionalDegrees; fractionalDegrees -= minutes; fractionalDegrees *= SECONDS_PER_MINUTE; double seconds = fractionalDegrees; Angle angle = {degrees, minutes, seconds}; return angle; } // end createAngle() // Outputs an angle to the screen. // Formats the angle as: dÂ° m' s.s" void displayAngle(string label, Angle angle) { cout &lt;&lt; label &lt;&lt; angle.degrees &lt;&lt; char(DEGREES_SYMBOL) &lt;&lt; " " &lt;&lt; angle.minutes &lt;&lt; "'" &lt;&lt; " " &lt;&lt; fixed &lt;&lt; setprecision(1) &lt;&lt; angle.seconds &lt;&lt; "\""; } // end displayAngle() // Converts an angle measurement in degrees, minutes, seconds to a // fractional degree value. double angle2Degrees(Angle angle) { return ((double) angle.degrees + (double) angle.minutes / MINUTES_PER_DEGREE + angle.seconds / SECONDS_PER_DEGREE); } // end angle2Degrees() // Returns an angle that is the sum of its two parameters. Angle addAngles(Angle angle1, Angle angle2) { double fractionalDegrees = angle2Degrees(angle1) + angle2Degrees(angle2); Angle sumAngle = createAngle(fractionalDegrees); return sumAngle; } // end addAngle() // Returns an angle that is the difference of its two parameters (angle1 - angle2). Angle subtractAngles(Angle angle1, Angle angle2) { double fractionalDegrees = angle2Degrees(angle1) - angle2Degrees(angle2); Angle diffAngle = createAngle(fractionalDegrees); return diffAngle; } // end subtractAngles() 
I find unit tests to almost entirely eliminate my time spent in a debugger.
thank you for your input. I was able to change everything! 
Why? Tossing in asserts, etc to verify things works fine. unit tests are designed to isolate you logic into testable units. I always compile and test in release mode and selectively compile some files in debug if I'm doing post mortem analysis. The software I work on is very aggressively and heavily multithreaded, injecting a debugger or compiling in debug mode very often allows the application or test to run successfully to conclusion.
Well said, the basic reason behind a bunch of the misra rules is reducing common "classes" of bugs. Dynamic allocation introduces a certain category of common bugs, so by avoiding it, you just avoid those entirely. 
You think eliminating STL and the heap increase risk for life critical embedded systems? You're crazy. They *reduce* the risk from several classes of hard-to-find bugs. How do you write a system immune from heap fragmentation? How can you be sure you've done so? Double free? Use after free? Leaks? Odd code paths encountering uninitialized objects? Lifetime management issues? Shared ownership? Remember, this isn't a word processor. Nobody is going to just keep opening documents or typing endlessly. The designers know up front exactly what structures and objects they'll need. These should be simple programs, and these restrictions aren't so onerous on simple programs. Especially when lives are at stake.
And you figure that learning to use a debugger is important when you already know how better to find errors in your code? It's like learning to drive a manual car when there are really good automatics. For all but the corner cases it's kind of useless.
I'm not sure what you mean by *slow motion*. Breaking on a failing assertion in a test and printing out the backtrace with all the variables in those call frames just takes 3 commands and is certainly a nice way to confirm your suspicions about the cause, if nothing else.
Debuggers show you what happens in your code with all insight in slow motion. They allow you to find out what variables turn into what and if you want to play funky you can fix a few to see if that helps. In the end though, nothing you do there will allow you to gain more insight than a decently written unit test will, none of them will actually help you fix your code &amp; check it quickly and automatically, and most certainly no debugger work will prevent that functionality from breaking in the next few modifications in that code. The unit test will point out what's broken now, the other unit tests on the same code illustrate what behaviour works and what doesn't and in the end, they ensure that that functionality keeps working. With or without all dependencies you want (although strictly speaking, it's only a unit test without dependencies).
&gt; Debuggers show you what happens in your code with all insight in slow motion. Did you read the comment you're replying to? Stepping through code is something a debugger can do, but if you already have a test failure to investigate you're just going to start by breaking on the failure. You won't be stepping through any code. &gt; In the end though, nothing you do there will allow you to gain more insight than a decently written unit test will, none of them will actually help you fix your code &amp; check it quickly and automatically, and most certainly no debugger work will prevent that functionality from breaking in the next few modifications in that code. No one here has suggested that a debugger in any way substitutes for unit testing. &gt; The unit test will point out what's broken now, the other unit tests on the same code illustrate what behaviour works and what doesn't and in the end, they ensure that that functionality keeps working. With or without all dependencies you want (although strictly speaking, it's only a unit test without dependencies). The unit test will tell you what is broken, but not *exactly* why it is broken. That's what logging or a debugger is for, and usually the debugger is going to be faster along with not requiring a recompile to insert or enable the debug logging you need.
&gt; unit tests Go back to the dynamic language hellhole you crawled out from.
I don't see how a universal reference counts as another way of passing the parameter - it cannot expand to anything not already covered.
&gt; Use the debugger as a library using the python API GDB has a python API as well.
Author of many $obscure_libraries in C++. Thank you very much.
GDB is really amazing. The big advantage that LLDB promises over GDB seems to be easier integration into IDEs. LLDB can be used as a library to that effect. If you want to integrate GDB into an IDE you have to use a pipe based protocol https://sourceware.org/gdb/current/onlinedocs/gdb/GDB_002fMI.html#GDB_002fMI
I don't use either - `gdb` looks overwhelming for my purposes and I've seen nothing that indicates that `lldb` has a gentler learning curve.
That's like saying everything in a Java app. should be "Integer" and not "int". Plenty of perfectly valid C++ is perfectly valid C. 
But valid C/C++ is rarely *good* C++.
I assume he submitted patches for these, or checked the latest repos to see if they had been fixed already? None of these are particularly "embarrassing". I've seen worse in the commercial products I work on. These are exactly why static analysis was developed and why you do code reviews. This can happen in pretty much any language (or similar bugs.)
So you agree that mixing C/C++ is not a bug?
"Valid C++" is a very loose expression.
You can supply your own allocator for C++ containers to avoid dynamic allocation. Plus, in C++11, you can use stateful allocators, thus allowing allocators to use the stack.So "don't use dynamic allocation" does not necessarily mean "don't use C++ containers". Also, of course, C++ algorithms can still be used, even if you don't want to use the containers with custom allocators. The algorithms do no dynamic allocation internally, and will work with C-arrays as well.
I fully disagree. There's nothing inherently wrong with the combination of C and C++ code. It's especially useful if you're maintaining an old, stable, legacy C codebase. Introducing one or two very simple C++ constructs - e.g. constructors/destructors/RAII instead of manual resource management - leads to better code. In other words, one of the perfectly valid uses of C++ *is to write better C*.
So you admit that you simply don't know how to use one? Debugger familiarity is essential. Saves tons of recompile cycles and you never forget to remove pesky trace statements. Beeing able to attach to a deadlocked app and immediately get stacktraces of the hang - no unit test can ever be more productive. 
&gt; I assume he submitted patches for these ... FAQ for those who have read our articles: [Did you inform the developers about the errors found in their project?](http://www.viva64.com/en/b/0132/#ID0E3C)
I dunno, man, describing 'serialization' as a nut seems to be a pretty serious understatement. It's going to impact not only immediate program correctness but also how you can change your program throughout development or post release.
&gt; So you admit that you simply don't know how to use one? Learn to read. I know how to use debuggers in detail, I just see no need / use for them.
Lots of stuff on JVM optimization, seems to be focused on running java apps. Solution to running ruby or python apps is to run Jython or jRuby... I'll pass :)
Slower is always better. :)
&gt; C++ is not about writing super-complex type hierarchies (as some people might have you believe). Yeah. Maybe this is just my limited experience, but I've never found type hierarchies to be very complex in practice. I mean, say, in Qt there's a type hierarchy of widgets, or in Java everything's a subclass of Object. If you start at the base class and actually trace out the *entire* hierarchy then, sure, it's going to *look* complicated just from its size. But you rarely need to know anything about the hierarchy besides the object you want to use, and maybe a few levels of parent classes up. &gt;Rather, it allowed us to write shorter code with less boiler-plate repetition and less chances for bugs. This is the main draw of C++ over C for me. I implemented a (Pascal) compiler in C not long ago, and the entire time I felt very limited. First of all, think about manipulating strings in C versus C++. And then think about using a struct to represent tokens with data of various different types in C (use void* and cast, or a union, or what else?) versus a Token&lt;T&gt; in C++. C++ is clearly more expressive, and way safer than C.
Am I the only one thinking that this is just pure advertisement?
Every time I've seen a project with this requirement it has been about speed, not safety.
Good question, definitely wrong category.
Most kernels are implement in C but with object oriented design in mind so there are just too many interfaces and boiler plate abstractions all around... which are supported natively in C++. And by the way - RAII, smart pointers, std locks, etc... makes C++ a lot safer than C. 
&gt;These should be simple programs, and these restrictions aren't so onerous on simple programs. I agree with everything you wrote except this sentence :D
&gt; I bet you never did enterprise C++ development. I'm not sure why you'd say that. &gt; Developers like to bash Java and J2EE bases architectures, however their design is no different that what enterprise architects were doing before with C++ and CORBA. Actually, that's precisely why they bash Java &amp; J2EE. ;-)
&gt; The reason C++ doesn’t have garbage collection is not because it can’t be done in an efficient way I disagree. GC is necessarily less efficient than reasonably written, near-optimal MM written by a human. &gt; but also because every time a reference count goes to zero on a piece of data a whole graph of pointers reachable from that object has to be traversed You only have to traverse memory you're actually releasing, whereas with GC, you repeatedly have to traverse memory that isn't being released. &gt; except for simple cases, you’ll never know which shared pointer will go out of scope last and trigger it The simple cases are pretty common, though. But in any case, I must second the recommendation to learn Haskell, though I don't think it directly competes with the C-level languages.
I knew those statements would be controversial, so I linked them to David Bacon's classic paper: http://www.cs.virginia.edu/~cs415/reading/bacon-garbage.pdf . The nice thing about Haskell is that you can call C code from it. So if you need low level optimizations, you can write them in C and use Haskell to deal with higher level abstractions, in which it excels. C++ provides some abstracting power, but it's severely limited. It's like a couch that is also a bed and a desk. It's not comfortable to sleep in at night, work at during the day, or watch television in the evening.
&gt; but many people will probably react emotionally and dismiss it. I can't even dismiss it because I didn't manage to get what exactly his point is. "Use Haskell for parallel programming" maybe?
You can call C code from almost anything. Haskell isn't special in this regard. This whole philosophy of "use a high level language for everything then optimize in C" isn't as easy as its proponents claim. The barrier to this transition is high enough that most folks rarely do it. Oh, sure, they *say* that that's how they do things, but I generally find most folks don't bother. It isn't just the ability to call a C function, but you have to rewrite all your data structures so they work in C and write marshalling code, and hope that marshalling code doesn't kill your performance. C++ is a language for programmers who know how to use it, programmers who know how it works, and would rather just write code in C++ and not have to worry about rewriting for performance later. They get their performance by default.
or XHTML?
For some projects, performance-by-default, correctness-by-hard-work makes more sense. Use C or C++. For other projects, correctness-by-default, performance-by-hard-work makes more sense. Use Haskell. I think the latter type of projects is more common, because most software isn't that performance-critical, and because you can get away with making 5% of your program perform well, and the rest just be OK. You can't really get away with making 5% of your program correct, though. 
You know, I was a good way through reading this longwinded article thinking "yes, but this is stupid and patronising, where is this all going?" and sure enough there's the first mention of Haskell. Haskell might well be fantastic but by god, people that write articles about how it's better than C++ are like the vegetarians that show up at your barbecue.
&gt; However, I'm not entirely sure that Haskell is a practical alternative. From what I read about functional languages, many domains (e.g. gaming) don't lend themselves very well to this paradigm. But I'm not really qualified to comment on this, as I haven't written anything in a functional language I was happy to see this [video](http://functionaltalks.org/2013/08/26/john-carmack-thoughts-on-haskell/) by John Carmack. I had some concerns myself about FP languages being used to develop games. This talk addressess some of those concerns. :) 
I must say, I did enjoy spending only a few minutes correcting trivialities when I switched to C++11.
&gt; how's the whole Python 3 thing going as of late? Pretty well? Most things support it these days and the transition is still on schedule (five years for it to become the "default" choice for new projects).
It's not as clear-cut. On my PC, the main CPU eater by far is Javascript code running in Firefox. How do we parallelize that? On my web servers, Apache just spawns a few processes, and I get parallelism without multithreading.
Fair enough. Do you have similar experience with parallel code? 
The thing is that there is no 'perfect language', there are warts on every single language out there; Lisp, C++, Java, C#, Pascal, Ruby, Python, Bash, Haskell, clojure, Eifle, ada, objective-c, objective-c++. they all have their advantages and their drawbacks.
I saw the video too and my impression was that Carmack is still pretty undecided. He seems to be like "let's see the cool stuff in Haskell and then go back to C++ with lessons learned". Besides I had a feeling that what he liked most was really just an idea of loose coupling - not a concept exclusive to Haskell.
Nope, I haven't done much parallel programming in either C or Haskell. It does seem like Haskell could blow C and C++ out of the water, in theory. But I've not had access to the kind of parallel machines that make this relevant.
Or improving cache usage like...Haskell absolutelly don't do with terrible data locality, and boxed values
In terms of syntax, I would use static matrix matrix::invert(matrix&amp; m) and matrix&amp; matrix::invert() The former takes a reference, creates a new matrix, and uses roi or move semantics to return ownership. The latter inverts a matrix and returns a reference to it. My reasoning is that object creation (assuming a mutable object) goes in the namespace as static, and self-mutation goes into instantiable methods. I wouldn't say invert() is generic enough to be a global template function - it is specific to the matrix, but not any one matrix. The only specialization worth thinking about would be the static member passed an rvalue, because then you can take matrix&amp;&amp; and do in place modification like in matrix::invert(), but that is redundant because you could just ::invert() the rvalue and confuses semanitcs because it means the static member is doing modification on the argument matrix. So I'd go with the static copier and the nonstatic self-modifier. It makes for really clean syntax too that is quite readable: matrix a { .... &lt;initializer list&gt; }; matrix b { matrix::invert(a) }; a is unmodified and b is the inversion of a. matrix&amp; c = a.invert(); c is a reference to a, and a is inverted. Implementation wise, I just did a google search and [found algorithms for sparce matrices](/http://mc.stanford.edu/cgi-bin/images/0/04/Li_phd.pdf) and [non sparce symmetrics](/http://www.mathworks.com/matlabcentral/fileexchange/34511-fast-symmetric-matrix-inverse) beyond the standard in-place symmetric flip of a X*X matrix. Since you couldn't assume symmetry (you at least don't mention it) you would get into the realm of non-symmetrics, and my matrix flip knowledge expires at a 4/5 on my high school AP stats exam. You would want to do in place flips on the class operation, and you could be more efficient with extra memory (since you are copying anyway) the namespace operation. Though of course, you would never want to write your own matrix library, and would be better off reading (and possibly optimizing) a communal one such as the [GGT](/http://ggt.sourceforge.net/) which most larger math libs have standardized on for vector and matrix math.
One rule of thumb I use when writing C++ and explaining it to others is that, when there's more than one way to do a thing (which happens all the goddamn time in C++), it's always better to do it in the least powerful way. Passing by reference can do strictly fewer things than passing by pointer, so pass by reference when possible. In the same way, `unique_ptr` &gt; `shared_ptr` &gt; raw ptr, `const` &gt; non-const, and `new` &gt; `malloc`. The problem with that principle for beginners is that they don't usually even know that the less powerful way *exists*, much less know how and when they can use it. It's a lot less to learn if you just learn the most powerful version and use only it.
Python 3 is going very well - what are you talking about? No one expected Python to be switched over overnight, or even in a couple of years. But steadily all the libraries you need are being switched over to Python 3 - and using `from __future__` and libraries like `six` let you write code that runs identically under 2.x and 3.x, which is what I use for my open source libraries. If you wanted to point to a controlled, systematic rollout of breaking changes, you'd be hard-pressed to find a better example than the Python 2-&gt;3 change. 
I think space-leaks due to laziness is the most cited burden Haskell brings to the table, and overcoming it is merely a matter of education. However, not having the right third-party libraries, tools, tutorials, examples, and other resources will always be a bigger obstacle to overcome for newcomers. There will continue to be a dearth of games written in Haskell as long as there is a dearth of games written in Haskell.
I feel that the writer just hasn't understood C++ at all. Let's start with his complaints about memory management. I write a lot of C++ and I can't remember when I had any non-trivial memory management issues - or, heck, even trivial ones. It's not that "I'm so smart" but that if you systematically use `unique_ptr` and the like, you will NEVER have memory management issues. Then there's his weird claim that using `shared_ptr` introduces all the issues associated with garbage collection. First, that simply isn't true - if the items that you're storing are light, no matter what you do with a shared pointer, you won't incur a huge cost deleting it. But more important - who uses `shared_ptr`? I have a large DSP codebase, and I literally _never_ use `shared_ptr` - I grepped to make sure. There are a small number of cases in which `shared_ptr` is the correct solution, but 95% of the time, there is a logical single owner for each object at any given time, which can be handled with `unique_ptr`. Haskell is a fine language, but you aren't going to get the performance you get in C++. Performance often isn't a big issue - I prefer to write in Python myself - but as I mentioned, I write digital audio software where performance _is_ important, and I honestly can't do without C++. And it isn't a small difference - the CPU consumption difference between my demo in Python and the final optimized C++ code is almost _two_ orders of magnitude, which means that the C++ code can actually do things that the Python could never do. 
Have a try to fix up the haskell code to be faster in [this benchmark](http://togototo.wordpress.com/). It just seems like everyone's _talking_ about how performant Haskell is for parallel stuff, but no-one seems to be able to show any examples or data.
Maybe we can throw some benchmarks around. The general arguments I've heard in favor of the functional/immutable style (as seen in e.g. Scala, Clojure, Haskell) is with regard to writing correct concurrent code. And it makes sense in theory: Write all your code with immutable data, and it's easily parallelized. But where is the evidence? From what I can tell, C and C++ are still clearly the performance kings *even in the multithreaded space*. Take [this comparison of C++ vs Haskell](http://benchmarksgame.alioth.debian.org/u32/benchmark.php?test=all&amp;lang=gpp&amp;lang2=ghc&amp;data=u32) from the benchmarks game. You can look at either one-core or quad-core; in either case, (1) C++ takes less time across the board, (2) C++ generally uses less memory than Haskell, and (3) *they use similar amounts of code*. Maybe someone can comment on whether the Haskell code used is actually easier to write than the C++ code (which seems to look like optimized C++ might look). Now I'm not saying, "Look! This one example is faster so C++ is always faster". But I would like to see benchmarks in Haskell's favor (or Scala's or Clojure's or ...).
I duno, iphone development is going strong and sometimes i feel like apple goes out of their way to break shit.
This might be true about general purpose Haskell code, which uses lots of small memory allocations and lots of pointers internally. But the data-parallel libraries like Repa or Accelerate are very cache conscious. A Repa array stores unboxed values. Laziness is also well controlled in these libraries. 
I work in a firm that uses both Haskell and C++. The C++ ended up using in-house data structures that can be translated to Haskell, to the point that you are required to use virtual function calls to access elements of an array (because Haskell is immutable and so on). Clearly not what you want to see in a high performance application. Also is impossible to debug anymore and has been quite a long time since we passed the point of no return. I think that the decision of mixing Haskell and C++ should be taken with care. I never had to deal with parallelism but, for my case, maybe CUDA should be the way to go. 
I strongly disagree - I don't see how my talk could be used to argue that. C++ is excellent at building high-level abstractions without incurring performance penalties. The purpose of my talk was to demonstrate, with real examples, how the language has various systems to help write correct and fast code, and how programmers should work with those systems to let them do their jobs instead of getting in their way.
Why don't you just look how the other (popular) LA libraries solved the problem? Google for Eigen, blitz++ or boost::blas and check their approach.
I conclude that C++ is good to write program that does random crap really fast.
&gt; I feel that the writer just hasn't understood C++ at all. Except you are talking about one of the widely known C++ gurus.
There is also a viewpoint that due to Amdahl's law desktop software is not so much parallelizable and hence making more than 4 cores doesn't pay off, and it's the reason why we don't really see promised growth of cores on desktops.
The funny thing is if your parallel code keeps allocating small objects you have those cache issues and need to pause all the threads to do GC quite regularly (or rather not "you need" but "runtime needs and does"). But if at least one thread doesn't allocate while some other does, then when another GC is about to happen whole program will stop to wait until non-allocating thread ends its computations and starts allocating again to notice it's time for GC. GHC 7.8 should have fix for that, but earlier versions had this issue. So it's like "don't allocate to have good speed but do allocate to avoid stalling all threads for indefinite time".
&gt; GC is necessarily less efficient than reasonably written, near-optimal MM written by a human. Yeah, I remember when people used to say the same thing about compilers vs. human generated assembly code. ;-) In practice, there can cases where the humans do better, but GC doesn't do too bad on its own. More importantly, the history of committees decisions about GC in C++ is well established.
And what kind of furniture is Haskell in that metaphor? Great read. As an experienced C++ programmer I actually feel sad that I do have to agree with lots of it :(
This guy seems to have a hidden agenda on C++ vs Haskell. I am unconvinced by his arguments. &gt; But you will need strict discipline and total control over your collaborators to pull that off because C++ is so permissive. Except it's not. The key is proper encapsulation into classes and generous use of private instance variables. The real problem is social: people want to get something done quickly, so instead of talking to the original author[1], they put a new method in the class (or declare a friend) and happily continue hacking, without thinking of far-reaching consequences of whether unrestricted (that's what public is) use of the newly introduced method violates class invariants in some way. [1] I'm aware that the original author might not be there anymore. Which makes a good case for documenting design and intent of the code. A QA matter. How does Haskell solve the social problem (directly hacking something into a module you don't own or fully understand)? &gt; What you don’t realize is that it will take you 10 years, if you’re lucky, to discover the “right way” of programming in C++ IMO, only novices and beginners look for "THE right way". Advanced people realize that the "right" way depends on the broad context and look for (the) best compromise solution among a spectrum of solutions. I believe this holds for other computing-related things (designing a LAN, database tables, etc.), as well as real life (sports, martial arts, even everyday things like sitting and walking as any person who has had a lower back problem will tell you). The problem is again social: novices and beginners who refuse to "grow up" and who know just enough to be dangerous: they can produce working but messy code, and refuse to learn other ways of doing the same thing in an appropriate context. (E.g. when to use while vs for, or why copy-pasting large chunks of code between different functions is generally "bad".) Somehow I'm not convinced that Haskell is the magic bullet which solves the underlying social/human issues. &gt; Haskell is not permissive, it won’t let you — or your coworkers — write unsafe code. He never defines what "unsafe" is. Mutation is not unsafe per se. &gt; Don’t be fooled: accessing atomic variables is expensive. I've seen recent slides somewhere on the web^TM where the measured cost of an uncontended locked cmpxchg on Haswell was ~20 cycles vs ~5 cycles unlocked. This is NOT expensive, unless unnecessarily you replace all memory accesses with their atomic equivalents. &gt; Most importantly though, threads are not a good abstraction for parallel programming No, but threads and mutexes are an essential and unovaidable building block for shared-memory concurrency. People have built high-level programming languages on top of assemblers. You should build high-level constructs on top of threads and mutexes. (Most useful one -- message queue -- is already provided by most widespread OS-es). &gt; Haskell is way ahead of the curve with respect to parallelism First, a computation can be abstraceted into a data dependency graph. Now, the reason that most of today's applications don't benefit a lot from the vast number of processors available is not that the underlying programming is somehow unsuited for parallelism. It is because that there IS NO parallelism available in these applications: data dependency graph is mostly serial, and if parallelism is available, it is on such a small scale that superscalar CPUs already make use of the large part of it (Amdahl's law). Also, with overly fine-grained parallelism you WILL end up in a situation where the overhead of atomic operations becomes non-negligible, even if it is only ~20 cycles. I believe that if people more often thought about their designs in message-passing terms, the dependency graph would also emerge, and they would see that there often is very little parallelism to extract through explicit parallelization. (Note that writing to a shared variable is just an extremely simple and efficient method of sending a message). This was also remarked on by Knuth: During the past 50 years, I’ve written well over a thousand programs, many of which have substantial size. I can’t think of even five of those programs that would have been enhanced noticeably by parallelism or multithreading. [http://www.informit.com/articles/article.aspx?p=1193856] So, IMO, easier/safer [race-free] coding for shared-memory parallelism is one of the worst reasons to switch programming languages. That's why I personally prefer Erlang-style concurrency (actors). Communication patterns and parallelism granularity are explicit, and communication cannot be decoupled from synchronization. [With shared variables, these two are decoupled.] 
I agree with you. I've never seen std::shared_ptr used either and I work in a C++ shop. So whatever the problems it may have, most of the code has simple ownership that only require raw pointers and std::unique_ptr.
There are cases where GC isn't too bad. But whenever your heap (legitimately) grows, GC will necessarily need to re-traverse it over and over. Manual memory management (in most cases) is not nearly as difficult as writing assembly code for the entire program. It isn't quite so manual, actually, it could be called "semi-automatic" :-) 
I think "desktop software" is too broad a classification to say anything meaningful. Desktop games can surely be parallelized. Photo/video decoding/editing can be. This means cheaper, slower, energy-efficient multi-cores might be able to do it rather than power hungry, expensive fast single-cores required as of today. Spreadsheets with complex computations can surely be parallelized. Word processing is computationally cheap enough that it may not *need* any means of efficient execution. 
Unless you apply "hard work" to it, and get a correct program that is probably faster (at least on uni-processor) than you would get with other languages, when hard work is equally applied.
As a Haskeller, this sentence makes no sense: &gt; to the point that you are required to use virtual function calls to access elements of an array (because Haskell is immutable and so on) Haskell isn't "immutable", it is "pure", which IMO is a misnomer for "typed-effects". You can do mutability just fine, as long as you are honest about it in your types. Also, even if you do use an immutable array, what does that have to do with virtual function calls?
You would pass it const, I just kept the whole signature down for brevity. It would also probably be nothrow, or even constexpr, since inverting a predefined matrix can be done at compile time. In your second question, that is mostly because std::string is meant to be immutable, but by being so close to the metal in C++ the abstraction leaks. In theory, due to its nature, std::string should never self-modify and would always return a copy, but in practice some cases facilitate an optimization that does involve self modification when you are certain the object is being discarded anyway (same reason move semantics exist). I imagine the C# DateTime class is also considered immutable, and if you wanted, you could make your matrix class immutable with a single: matrix matrix::invert() const; That returned a copy and never self-modified (and by declaring it const you guarantee that). C++ doesn't have a PEP. Even bigger software projects in c++ like boost, kde, and qt don't have guidelines this specific, they mostly just involve feature usage. Like I said above, the exact implementation is situational, but for a non-immutable data type the intuitive design is on-object calls are self-modifying and on-namespace calls are duplicating, but if the object is immutable you only need the duplicating call, be it in the namespace for consistency or on the object for brevity.
&gt; &gt; I bet you never did enterprise C++ development. &gt; I'm not sure why you'd say that. Because you stated you had a limited experience with the language. Maybe I read too much in that remark. :)
I for one agree with the article to a large extent (others have already pointed out the controversial parts, so I won't bother you). I'm not familiar with Haskell at all, but I can't help but think somebody should come out with a Haskell application that showcases all those advantages (and as I understand, John Carmack is already at it, right?). That should always make adoption, or hell, even transition, more likely and easy.
You're generally right. A funny thing is: photo/video decoding/editing looks like a good target for parallelization and yet it's hit by the same Amdahl's law too: very time consuming part of encoding/decoding is entropy coding (Huffman, CABAC etc.) which is sequential and not much parallelizable. And most frames can only be decoded after the previous frame is decoded. Then even if you make the process very fast you quickly meet hard drive speed limit and them RAM speed limit. In practice it turns out people doing video processing struggle to utilize 8 cores, they don't reach 100% load there. 
&gt; I feel that the writer just hasn't understood C++ at all. He does. He had a great C++ blog prior to his conversion to Haskell fanboy. Since then he became really obnoxious because he writes articles like these. Articles where you assume he doesn't know C++ at all. Though he does and he knows what good parts of C++ to leave out to make it sound more shitty than it is. 
Ok, so which books/tutorials should I (semi-experienced, 6 years, but just starting my professional career) read, to learn the good way of writing in C++?
Ironically, Dr. Bartosz is trying to promote Haskell, but it was his explanation of monads that finally convinced me I should stop learning Haskell and leave it to the people smarter than I am: "A monad is an endofunctor together with two special families of morphisms, both going vertically, one up and one down (for “directions” see Fig 5). The one going up is called unit and the one going down is called join." Full article here: http://bartoszmilewski.com/2011/01/09/monads-for-the-curious-programmer-part-1/ 
Are those written in C++03 or C++11?
My read on this article as a Haskeller who writes mostly C++ for his day job is that Bartosz is not saying that Haskell is it today. Rather, he's saying that parallel computation is the logical path forward when it comes to making faster systems, but that C++ (and other old-fashioned languages that still are mostly predicated on a PDP-11-like architecture) make evolution in that direction difficult. This is clearly true. Luckily on today's machines, it's not yet enough of a problem that C/C++ programs actually suffer performance: as everyone knows, there are relatively few benchmarks where parallel-ready languages like Haskell actually beat languages like C or C++ on current hardware. However, if we imagine an evolution towards more-GPU-like processors, made up of many many cores that are individually less powerful, and we imagine that on such systems more or less any non-trivial program will have hundreds or even thousands of threads, anyone who has done a lot of C/C++ programming should freak out, because managing that kind of program safely in today's C++ is an absolute nightmare. In Haskell however, well, it's honestly pretty easy, particularly with STM. So does that mean that Haskell will be the next C++? Maybe not, and in fact probably not. Haskell showcases a series of very important concepts in a programming language you can use today, but with a relatively academic bent (this is part of what I love about it.) As the hardware evolves, I expect "mainstream" languages to pilfer mercilessly from languages like Haskell. Because it does do certain things undeniably well in a highly parallel context, and C++ just doesn't. Witness the number of professional C++ programmers who don't actually know what the `volatile` keyword does.
&gt; He never defines what "unsafe" is. Safety (without qualification) in a programming language means memory corruption being guaranteed to not happen. Java, for example, offers such a safety guarantee (and Java allows mutation).
Exactly, for the most part what was described here is something that happens in any language. These aren't stupid programmers, but often errors introduced due to outside influences. For example being interrupted in the middle of editing for a hot issue or worst a "management meeting" that is an emergency. There are few jobs these days where one can focus on the specific task at hand all day long and not be side tracked by everybody and their brother. 
Certainly it is. However it does demonstrate the advantages of static analyst and other techniques to automate finding bugs. I'm not in the position to run commercial software but have run Clangs static analyzer against open source code. It has turned up bugs in some code bases along with some false positives. The thing here is the programmers are usually happy to see the reports. So anything that drives wider adoption of the use of these sorts of tools is a good thing even if it is advertisements. The reality is that no matter how well curated your code is even the best of us mis bugs, some of which may not show up for ages or simply cause the code to produce subtle errors. So I see it like this, yes this is advertisement but it also promotes a technique far too few programmers use. 
From an API point of view, (4) is indeed clearly the best way: unless there’s a *compelling* reason to make `invert` a member, don’t. `invert` is an *algorithm*; even if it’s not generalisable, the most C++-y way is to make it a namespace function. And returning a copy rather than modifying the source is good design as well. For efficiency, I would strive to make the code elide unnecessary copies when an assignment to self is made. That is: m = invert(m); Should work in-place (notice that this API shouldn’t necessitate a `move` in the client code). This is achievable by making `invert` return a proxy rather than a `matrix`. Then `invert` doesn’t actually perform the inversion, it just returns a proxy. The actual inversion is performed when the return value is reassigned to a `matrix`. You could then perform the in-place operation by the following logic: class matrix { … public: matrix(matrix_proxy mp) : matrix(mp.ref) { // Copy and inverse inverse(); } matrix&amp; operator =(matrix_proxy mp) { if (&amp;mp.ref != this) { // Copy source *this = mp.ref; } // Perform in-place inversion inverse(); return *this; } private: void inverse() { … } }; Notice that, since `matrix_proxy` is a thin wrapper around a reference, it’s passed by value. Downside: writing `auto other = inverse(m);` creates a proxy rather than a matrix. This should be caught somehow, i.e. by ensuring that `matrix_proxy` can only exist as an rvalue.
I should point out that I'm not posting this simply for the sake of attacking C++. The piece highlights an interesting (though obviously very biased) contrast between the two languages and I expect a lot of people here might find it really interesting. I'm sure a fair amount of you know who rob is and why he's such a big deal, so hopefully you can appreciate some of his frustrations with C++.
It ultimately depends how your Matrix class is implemented. You could implement it as a template Matrix&lt;X,Y&gt; that, for small X*Y products holds data inline, but for large values holds a shared_ptr to the data. In such cases an Inverted&lt;&gt; adapter that simply holds an non-inverted copy and then computes values on the fly would work. Copy-on-write can take care of efficient copies on large matrices, and you can even still perform vectorised computation in your conversion operator. This ultimately leads you to this sort of trickery, inverting an already inverted matrix: template &lt;unsigned X, unsigned Y&gt; Matrix&lt;X, Y&gt; invert (Inverted&lt;Matrix&lt;X,Y&gt;&gt; m) { return Matrix&lt;X,Y&gt;(m.data()); } There are many reasons to do this sort of thing. Identity matrices with millions of elements using O(1) memory for example.
Just to add to the other replies; there is no such rule in C++ (and it's arguably even a bad practice): &gt; But this goes against the idea that operations that affect the representation of a class should be implemented as member functions. In contrast with more OOPish languages, C++ is [multiparadigm](http://www.stroustrup.com/bs_faq.html#multiparadigm), in particular also allowing for [generic programming](http://www.stroustrup.com/bs_faq.html#generic) (often more powerful and more useful for many use cases, while maintaining zero run-time overhead) -- note, for instance, how the functions in the Standard Library that *do* modify the representation of a class (like `std::transform` affecting a representation of, say, `std::vector` through iterators) are non-member. This is not a coincidence -- this is a deliberate design decision; by the [Interface Principle](http://www.gotw.ca/publications/mill02.htm) the associated non-member functions *are* part of the class' interface (for instance, note how `std::begin` non-member function picks up std container classes via ADL). In fact, it is mostly when you *need* run-time polymorphism and `virtual` functions that you have to rely on member functions. There's plenty of other cases when it's a better design decision to avoid member functions. To see when &amp; how, read "How Non-Member Functions Improve Encapsulation" by Scott Meyers: http://www.drdobbs.com/cpp/how-non-member-functions-improve-encapsu/184401197
Actually there is some recent effort to port LLDB on Windows (both MinGW32 and MSVC build), as well as a Visual Studio debugger using LLDB underway. That should help for GCC/Clang adoption on Windows (until LLDB supports PDB, which would be great -- probably feasible given the modular architecture of LLDB)
As a c++ programmer, I have reached a level where I use a subset of c++ that makes sense for the kinds of projects I make, so I don't have any motivation to turn to GoLang.
Sure C++ is a powerful tool if you know how you use it, but surely its difficulty to learn really is a major negative? You're going to get less people using the language, and worse, tons of people using it poorly. C++ might be a decent language when used effectively, but the harsh reality is that most people don't do exactly that. Roughly of what proportional of professional programmers do you think actually use it effectively?
This all the way. Its no secret that Rob is being biased here, because hes one of the advocates that 'data structures are central to programming', and i dont think he realizes the full power of generic programming in C++ after C++98, because type heierarchies is really *not* what C++ was designed with in mind. It does support defining types through classes, but there isnt the limitation of a type heirarchy *at all*. Really, I think the biggest strength of C++ is providing these really powerful abstractions (templated structs, functions, possibly variadic) with compile time type safety. Rob (and many others at goingNative 2012 and 2013) have complained about the atomic memory model of C++11 (i.e the standard model) and their threads library, but I really dont think much could have been done by the committee at this point in time. The memory model of C++ was inspired directly from C, and that in turn was modeled in a time where concurrency really wasnt part of the specs. I think that becomes problematic when we want concurrency under a Go memory model. 
I'm a c++ programmer and I looked at go a while back. It appeared to do less than c++ in a less efficient way with a more ugly syntax. I saw no reason to use it. That's not to say that the world couldn't use a better native compiled language than c++, but so far I've not seen anything that's close.
&gt; surely its difficulty to learn really is a major negative? Possibly. That's why people who don't know C++ might be interested in Go. &gt; tons of people using it poorly. That's indeed a problem. Especially if you're hiring. That said, all languages can be badly used. Only niche languages, used only by enthusiasts, escape the problem.
Templates: C++ templates offer specialization, visible to the optimizer--not just generic programming. This is important. Range checking: if your arrays enforce range-checking, don't imagine you're in the same space as C/C++/etc. Somebody has to write your string-search library algorithm, and it's never going to be in go or java if you don't let the implementer control this. GC: needs to perform well with gigabytes of memory usage. I've yet to see this. Don't say it's 10% slower with a toy program--how much slower would it be for a browser with 20 tabs open? How's that java internet browser Sun made doing these days?
Not sure why ::vector_add can't be implemented as a compile time library or an addition to the STL. In fact the compile time library is the approach of at-least one package. The authors should address that there is more then a single kind of vectorization and it is often changing. For example, new Intel Haswells have AVX2 which increases the number of vectorization opportunities - as currently implemented on compile time I have noticed 40% performance increases with zero work. Vectorization is a changing thing, I can imagine a future where a massively parallel compute unit (GPU?) might be exposed as a host unit and it will enable vectorization opportunities for cases of branch divergent code (!).
My recommendation would be to provide an in-place variant as a member function, and a free function which returns a copy: `void matrix::invert() { ... }` `matrix inverse(matrix m) { ...}`
The only language I've seen that comes close to being a better C++ is Rust, but it isn't mature enough for serious use. I think that Go's authors are way off base describing it as a better C++. Anyone who prefers ease of use over zero-cost abstractions is already using something other them C++. Really it looks more like they are trying to create a better Java or a faster Python rather than a better C++.
Go lacks generics/templates and this leads to quite nasty code that is not minimizing programmers efforts at all. Say you want to sort some structures, according to go documentation example you have to do: type Organs []*Organ func (s Organs) Len() int { return len(s) } func (s Organs) Swap(i, j int) { s[i], s[j] = s[j], s[i] } type ByName struct{ Organs } func (s ByName) Less(i, j int) bool { return s.Organs[i].Name &lt; s.Organs[j].Name } I am ok with last 2 lines by for the gods sake why do you have to write first 3 lines for every single structure you want to sort. Go is a brand new language and it is 21st century outside so why do you need such boilerplate code?
I know it's minor, but Go's backward declaration syntax ("a int" instead of "int a") annoys me beyond reason. I spent half of my live doing C and C++ and "int a" is simply wired into my fingers. If you want me to migrate to your language, how about not messing with my muscle memory without real reason ("fixing" C function pointer syntax is not a good reason)?
Total dud in /r/programming, but really good talk.
This is bullshit. Monads represent the result of computations, and the only requirement to be a monad is that the result of the computation can be passed as input into another computation that returns a monad. As an example, let's say you have a computation that can either return a result of type 'a or throw an exception. We'll call its result a and give it type Exception&lt;'a&gt;, meaning "either a value of type 'a or an exception". The only monad operation, "bind", consists in providing the a to a function f that expects an input of type 'a and returns a value of type Exception&lt;'b&gt;. (Formally: f :: 'a -&gt; Exception&lt;'b&gt;) We define the "bind" operation as follows: if a is an exception, the function never receives the input, and the whole computation (a &gt;&gt;= f) returns the exception. If a is a value, that value is passed to f, which either returns an exception or a value b; the result is the return value of f. Going away from the exception example, a monad is any generic type M&lt;'a&gt; that supports an operation bind : M&lt;'a&gt; -&gt; ('a -&gt; M&lt;'b&gt;) -&gt; M&lt;'b&gt;.
&gt; an in-place variant as a member function Why as a member function?
&gt; Sure C++ is a powerful tool if you know how you use it, but surely its difficulty to learn really is a major negative? I think it's a major problem, but it's the sort of thing that would lead to people choosing to learn Go rather than to learn C++ as opposed to something that makes people switch from C++ to Go. I don't see "it's easier for everyone else to learn" as a huge deal, as while a team writing something in C++ absolutely needs someone with a solid understanding of C++, more than one often isn't necessary as long as the rest of the team is decent.
Yes, one can argue whether they succeeded in their goal of minimizing programmer effort or not. The point is that just by making the decision to not have zero CPU cost as a goal they are not a competition to C++ and shouldn't be surprised that very few C++ programmers switch to their language. I still think it would be nice to have a "better C++", but Go just isn't C++ at all.
I should have explained. All C++ classes have been designed to be translated to Haskell in such a way that you have always two pure abstract classes above. One for reading (all const methods) and one for writing. This is why everything (even for the array class) needs virtual calls. Maybe is only the design that could be better but I don't know how could you do it.
Copying garbage collectors (which I presume Haskell uses, by context?) aren't the only GC algorithm. I'm not convinced that MM is inherently that much faster than well done GC, and there's probably still performance to be gained by improved GC algorithms. And there are performance advantages to GC as well, a copying garbage collector helps maintain L3 cache locality, and a concurrent garbage collector makes it much easier to avoid lengthy delays when you de-allocate many chunks of memory. The real downside to GC programs is that they tend to require more RAM than MM. Which is irrelevant more often than not.
C declaration order makes the grammar unparseable without a symbol table, which is kinda evil. I'm glad they fixed it.
C declaration order makes the grammer unparseable without a symbol table, which is kinda evil. I'm glad they fixed it. FTFY
Frankly, I'm not writing compilers, so I don't really care about that aspect. As long as sufficiently smart compiler writers can figure out a decent solution, it's all the same to me.
I think I have a vague idea why symbol table may be required in C case, but in Go every variable declaration (with explicit type) comes with "var" keyword and function declaration with "func" keyword. Seems pretty unambiguous to me no matter what order you pick.
Do you mind explaining why? I've read about it before but never grasped it.
A stronger type system is obviously required. It's funny, because C++ programmers often feel that a C-like type system isn't expressive enough; template metaprogramming allows them to do things they can't do with C, and they understand the utility of the evolution. But when you tell them that they should go full-blown System F, they balk. I suspect we'll see an evolution towards something more typeful. Whether it will be C++ or something different (D maybe? or D++?) is hard to say. It's worth also noting that on the other end of the spectrum, Agda and Coq nerds are wondering how we manage to get anything done without dependent types. So there's a whole spectrum here. I liked your article very much, at any rate. Keep em coming!
Haskell uses a generational gc, copying/compacting young objects and only traversing old objects rarely. Whatever gc scheme you use, when you have a long running legitimately large heap with mutations, the gc will have to spend considerable resources figuring out whether parts of that heap became unreachable by traversing it repeatedly. Manual mm can use invariants the gc necessarily doesn't know about to save much of this work. Also manual mm can be cache friendlier using these invariants. Preventing long delays when manual mm discovers large amounts of memory to release is quite simple by postponing the cleanups. Memory constrained environments are still common, btw, though mobile tech is getting larger memories, it's still wasteful to spend two thirds of it on gc performance. 
I use "safety" in more colloquial meaning. What you describe is memory safety but there is also exception safety, thread safety, and many other safeties to consider.
Amen to that. And don't forget parallel *incremental* garbage collectors. 
Programmers need to understand the extreme advantage it is to have total control over how memory is used.
Thanks, I will certainly try to get it and read.
This article sounded like some kid crying in his soup because nobody will play with him. Seriously I didn't see one positive thing in here about Go other than it supports concurrency. Beyond that they know where their programmers or better actual users are coming from. A person programming in Python has a set of goals totally different than a C++ programmer. This should be obvious. 
I'm not a Haskell performance guru, as I don't use Haskell for performance-critical applications or deal with much parallelism. Most of the Haskell I write just runs fast enough out of the box. I did attack this problem for a bit, and cleaned it up a bit (removed about 10% of the LOC). Didn't manage to improve its speed by much, but I'm not sure what leak is being referred to in the article. Seems to work fine here. Note that this Haskell code is 50% slower than the fastest programs here. Even if this is the best result out of current GHC (which I doubt it is), this is quite a good result, if you factor in the nice guarantees you get from Haskell here. For example, the independent level generations are guaranteed to be independent because genLevel has a pure type that guarantees that. It may or may not be worth an extra 50% of your cores to save up development time and/or production bugs.
You should remove the semi colon at the end of the last if statement. Also, because if the first if statement is true you don't even need to check the other ones, you should make the two other if statement else if statements. There is a better way to do this however, I'm not sure if you are familiar with the concept of arrays or looping constructs yet?
The semicolon at the following line should not be there: &gt; (number3 &gt; number1 &amp;&amp; number3 &gt; number2); The next line is executed regardless, because the block after the if is 'empty'.
No, we haven't been familiarized with loops yet. My professor said that should be the subject of next week's class though. In due time I guess... Thanks for the tip. I'll fix that right away. Hopefully that's the only issue for today. :) 
Thanks for the tip. I appreciate it. Hopefully that fixes up everything! :D 
As others have pointed out it helps the compiler, but it is also more readable in English. For "a int" you can say "A is an integer", but "int a" is more awkwardly said "There is an integer called A". This obviously isn't a very strong argument, but it helps new learners and doesn't help you at all as you've said.
No. Sometimes, a bad parsing rule means that there are multiple valid representations and the language has to specify an [arbitrary tie-breaking rule](http://en.wikipedia.org/wiki/Most_vexing_parse). This isn't just difficult for compiler writers - it's difficult for everyday users of the language. A better syntax makes it easier for programmers to just write what they mean and get on with it.
I would like to see a new syntax for C++, but where the underlying language is retained. Compilers could then understand both syntaxes and it would be easy to convert between the two. It would mess up macros though.
One of Go's admitted goals was fast compilation.
From what I understand it is at least partially to get away from the insanity of the [Counter-Clockwise Spiral Rule](http://c-faq.com/decl/spiral.anderson.html) of declaration.
How would you try to convince someone to take Pike's opinion more seriously than their own? What if they had years of experience coding C++ and had read many books on the subject?
I agree, but programmers also need to understand the extreme advantage you gain from using a garbage collector. For example, in multithreaded code, it can simplify logic considerably when you don't have to worry about who will release the memory. This should not be an ideological divide, we should choose the right tools for the job.
Think about that "quite nasty" code in contrast to some of the nasty C++ you have seen. Doesn't seem so bad, does it?
Same parsing problem at function scope as well.
&gt; However, if we imagine an evolution towards more-GPU-like processors, made up of many many cores that are individually less powerful, I don't believe this will happen. I remember I was a teenager when transputers were the "hot topic". They came even with their own programming language which explicitly supported parallel programming (occam - based on CSP calculus), and... it withered. It gained some popularity only in facilities that had need for massively parallel computing. GPUs are in many ways similar to transputers. On several occasions I looked through ACM's and IEEE's digital libraries trying to find interesting transputer algorithms from the past, hoping I would find something applicable to today's GPUs and/or CPUs. What I found was.. that the field of parallel programming and algorithms is deades old, yet only a handful of parallel algorithms have been developed for specific problems. These have already been packaged in libraries. Trivially parallelizable problems can be trivially parallelized by OpenMP and (less trivially) CUDA. You don't see these in practice because your "typical application"^TM does not compute convex hulls, or nearest neighbors. Interestingly, there is a parallel algorithm for string matching, but I don't know how long the strings should be to overcome the overheads of parallelization. Also, note that effective parallel algorithms need a completely different approach. For example, if you need to sort a small number of elements (like, 30-ish), you typically call std::sort. Efficient parallel solution would be a sorting network coded with SIMD primitives, adapted to each element width. No programming language will ever give you this kind of insight.
Use [shared_ptr](http://en.cppreference.com/w/cpp/memory/shared_ptr) in C++
Two main reasons for not switching: - lack of shared libraries - lack of exceptions (see [here](http://voices.canonical.com/jussi.pakkanen/2013/06/10/what-exceptions-are-and-what-they-can-teach-us/) why you need them) 
Go has exceptions. It's panic/recover/defer instead of throw/catch/finally. But Go libraries don't abuse them for simple error handling. If you have to unroll large amounts of the call stack like in your XML example you can use panic/recover. It's just not good style to throw them over package boundaries.
What do you think about d-lang? (vs Rust and C++) And as for Go, I think it's best applied in data centers.
~~[feof()](http://linux.die.net/man/3/feof)~~ As pointed out by others, there are better ways to do this. See the comment from /u/Rhomboid But I think you'd be better asking this sort of question on StackOverflow
&gt; but surely its difficulty to learn really is a major negative? I don't believe C++ is hard to learn. It's certainly easy to learn than C. C++ is usually taught very badly. Also, I get the impression that some people think that C should be taught first, and C++ later. I would reverse this. C++ before C. The biggest mistake that is made in C++ teaching is the obsession with getting people to create their own new classes from day one. This is related to the fact that object-oriented-programming still isn't quite dead. :-) I would argue that, in terms of getting new programmers to write simple programs that work and that are easy to understand, C++ is far better than C. You can write C++ with few (or no) pointers, using references instead. And a combination of simple structs and static functions is easy for a new programmer to understand. And when you are ready to talk about shared objects and memory management, you can start with [shared_ptr](http://en.cppreference.com/w/cpp/memory/shared_ptr). Much of the 'hard-core' memory stuff can be delayed until the C course. TLDR: C++ is not "C with classes". It's "C without pointers and with a usable library".
You can't just used `shared_ptr` everywhere in C++ and expect not to have a bad time, especially if you're working with data structures. You still have to think about ownership or sooner or later you'll end up with cyclic references. Java's garbage collector will just cope by magic, but C++'s `shared_ptr` needs judicious use of `weak_ptr`.
Thanks this answered pretty much everything I wanted.
This is especially /r/cpp and not /r/cpp_questions. Read the sidebar, OP.
came back to this thread after leaving it overnight and it's become miserably stupid, bailing out
No – it’s not. Check the answer by /u/Rhombold. *That*’s what you need. `feof` is the *wrong* way of doing this, and may lead to undefined behaviour and an endless loop in case the file is somehow corrupt or reading fails, because then `feof` will never return true.
too late op, my pitchfork is out and i ain't putting it back.
Your assumption is right; frequency scaling already has hit the wall. But it seems that multicore scaling will also hit the wall, and that in the near future: http://dl.acm.org/citation.cfm?id=2000108 [if you google the title, you'll find a free version of the full paper] But the physics is also limiting the scalability of multicore scaling: threads must communicate with each other, and historically, latency has not scaled nearly as well as bandwidth. See these slides: http://www.ll.mit.edu/HPEC/agendas/proc04/invited/patterson_keynote.pdf [it's also the name of the paper, you might be able to find a free version online] &gt; In Haskell I will routinely spawn a hundred threads just because I'm lazy and I can. But do you get any *speedup*, and by how much? (With a bad/untuned STM implementation, you might actually end up with a significant slowdown relative to sequential program unless you have a large number of threads.)
Again, I don't think it can really be described as a better C++, it does make writing higher-level code easier, but it doesn't make writing low-level code any easier. Rust on the hand specifically targets making low-level code easier to write.
why would this be needed instead of const?
Ah, I tried out the feof and got my program to work with it, but with the risks you mentioned I'll get it converted over to the other way. Thanks for the heads up.
Agreed - editing my answer accordingly. And downvoting myself! In C++, it's [easier](http://www.cplusplus.com/reference/ios/ios/operator_not/).
Looks to be in the same vein as [string_ref](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3442.html) which, I believe, should be in C++14 and is similarly implemented in Boost 1.53+. I had a look over your Git repos though, you've got a lot of nice C++ in there. Particularly your handy string algo impls, which I think is one area where I think standard library could really do with some sauce.
Shared libraries are coming to Go (very soon). Take a look at this link. http://harmful.cat-v.org/software/c++/I_did_it_for_you_all
This is an advantage of shared_ptr IMHO. shared_ptr is a tool that gets you 90% of the way there... It plus a little thought saving you from non determinism sounds like a worthwhile trade off. That said, I'm not against garbage collection in general... It makes a lot of sense for small programs that run and exit relatively quickly.... But if I'm building something big or that needs to run forever, I want deterministic allocation / deallocation.
&gt; Manually check return codes at each function call, like in C? Surprising as it seems, but... yes, that's how errors are handled in Go. It's not as tricky as in C though, because Go provides RAII-like functionality with "defer" syntax. The panic/recover keywords behave more or less like exceptions, but the recommended syntax for recover just hurts my eyes. There's no explanation for such massive ugliness in a brand new language. See how "catch" statement looks like, straight from golang blog: defer func() { if r := recover(); r != nil { fmt.Println("Recovered in f", r) } }() It's supposed to be better than try-catch syntax. I can live with C-like error handling, because I wrote enough C code in my life. I only wonder why the fabled Python/Ruby developers don't walk away seeing this.
I prefer Rust to Go because slices are weird, and Rust does not force me to use the garbage collector. As for D, there were 2 strange versions last time I checked, and it looked like Java. I can write C++ that looks like D if I try. Also at work I use a lot of cross-compilers, and I can't use anything else.
Go has exceptions, they're just use different terms for it ("panic" instead of "throw"). I think the authors just didn't like overuse of exceptions in other languages, so presumably they named them differently to encourage different use and return error codes in cases of routine error handling.
Another aspect of using GC is that is simplifies library interaction. In my limited experience, when a pointer is returned from a library call, it is often unclear whether the library or the client code owns the memory. There seems to be quite a big amount of defensive copying going around when integrating libraries, especially when you want to get libraries to cooperate that have differing conventions. Or memory leaking applications. That means that in a complex application, due to the communication requirement between the authors of modules writing a near-optimal MM is impossible.
Just goes to show how *impossible* it is for a single language to compete with C++. Personally I've banished exceptions from my subset of C++ (and I'm [not](http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Exceptions)… [alone](http://llvm.org/docs/CodingStandards.html#do-not-use-rtti-or-exceptions)). But I can't switch to Go for other reasons (lack of operator overloading and lack of parametric polymorphism). C++ is so gigantic that it's impossible to meaningfully compete with it without creating an equally unwieldy language.
If you were a new programmer, just getting started today, would you like to read many books, or would you like to write many programs?
But what if c is not of type A?
link to the talk link to the talk [https://www.youtube.com/watch?v=JE17r3n1kz4&amp;feature=player_detailpage#t=2922](https://www.youtube.com/watch?v=JE17r3n1kz4&amp;feature=player_detailpage#t=2922)
Just use D.
&gt; transputers ... with their own programming language which explicitly supported parallel programming There was also C for transputers which supported paralelism with libraries. By analogy there will be (or perhaps is) attempt to deal with GPU similarly.
Nonsense. That's just patronising bullshit. Lack of generics in Go is makes some code much more cumbersome, and instead of strange "void*" that you'd see in "C++" code pike would be used to (I'm only exaggerating a bit, but Googles style guide forbids modern C++) you get interface{} and strange APIs to work around Gos missing features. That says, Go *mostly* allows you to make a nice &amp; clean interface. But sometimes when you'd want to make something it seems like you have to solve it with reflection on every line. And don't get me started on the fact that in Go you have to write code that is *both* exception safe **AND** check return values. Worst of both worlds.
&gt; Garbage collection (only) Stopped reading there. How many times has Stroustrup said he made C++ the way he did because he wanted to have controll over as many details as possible?
No, it's just a patronising troll post. He admits to not knowing C++ and is not allowed to use modern C++ at work. So... why should we listen to him on C++?
&gt; Personally I've banished exceptions from my subset of C++ Then that's just C with classes. Not C++. You can't use strings, vector, well.. all of STL. What aspects of the standard library do you think you can use? So yeah, you now have a much smaller language. It's called C, and it sucks as your program grows.
C as a language *is* simpler. That you have to churn out more code to do the same is a different story.
False. If you don't write exception safe code then you write buggy code. For example, you can't write: foo.Lock() blaha() foo.Unlock() Because if blaha() has a an issue that triggers a panic, then you don't unlock the mutex. And don't say "well I never recover, so it's fine" because the system libraries DO recover (for example, the HTTP server will recover from panics in your handlers). You need to write: func(){ foo.Lock() defer foo.Unlock() blaha() }() Do you? If not then you write buggy code.
http://en.wikipedia.org/wiki/Rob_Pike
[Yawn](http://www.reddit.com/r/programming/comments/1mue70/less_is_exponentially_more_rob_pike_on_why_c_devs/cccshd5). Like I said, he's not allowed to use modern C++ at work... because he works for Google. Also, from the post itself: "I admit I was never truly facile in [C++]". Do you disagree that "To [C++ programmers], software isn't just about getting the job done, it's about doing it a certain way" sounds like a troll? Or that it's patronising? If I wanted to use this kind of argument I'd say "Why should we listen to someone who thinks 8g, 6g and 5g are the obvious names for compilers for amd64, x86 and ARM? Why 5 for ARM?". What actually annoys me most about this post is that we have a language. This language has words, and these words have meaning. The word for the meaning that panic/recover describes is "exceptions". Computer science has defined the word "exception" to include a fairly broad set of concepts, and it definitely includes Go exceptions. So no, Go *does* have exceptions, and you have to write exception-safe code.
In this case you might be right. I haven't used lock also. With channels you don't have to (although I agree it's slower than lock). Returning errors from funcs works good enough for me. But a program is more than these issues. For instance, the C++ chess program Stockfish http://stockfishchess.org/ contains a lot of templates and other scary code. When I looked at it, I just couldn't figure out what was meant. When I look at the Go Chess program http://code.google.com/p/gochess/source/browse/ there is *nothing* I couldn't understand. So in the end, looking at the complete picture, what do you think has a healthier codebase?
Indeed, the C language is simpler (as in, the ISO standard contains fewer pages.) Programming in C is not.
I agree GC has major advantages. Not needing to deal with and document all of the memory life-time issues is one of them. I just also think GC has major drawbacks, as well.
If you open a file, you should use defer file.Close(). That's the way it is. In Go you probably won't write less code than in C++. That's the downside of a language that has little features. Personally I like it, however I can understand that you like a feature rich language. But in Linux for instance, when I look at a regular library, the amount of crap (really, with lots of preprocessor stuff), combined with the toolchains (cmake, autoconf etc) doesn't make life easier. Makefiles are most of the time a pain. The Go toolchain is a breeze. go get works perfectly with git. You want to test some scary stuff? All you have to do is change the GOPATH environment and you can test it without affecting the other code. Brilliant. It's not the language itself, it's the whole package that impresses me.
Small nit: &gt; I'm not against garbage collection in general You probably meant *automatic* garbage collection, which is what Java and Go have. Reference counting (shared_ptr) is also garbage collection.
&gt; If you open a file, you should use defer file.Close(). That's the way it is. Sort of. If Go didn't have exceptions then this code: f, err := os.Open(...) if err ... doSomething(f) f.Close() would be safe, and would arguably be prettier than: func() { f, err := os.Open(...) if err ... defer f.Close() doSomething(f) }() Someone not familiar with Go would be surprised and ask "why the lambda?". I would have to explain that defer closes at end of function (unlike C++ and CPython, for example, which runs destructors when the name is no longer accessible (out of scope and refcount=0 respectively). And that's why I had to create an anonymous function. I find I have to use this pattern quite a bit in looping, and I guess you're just going to have to trust me that the code belongs there and not in a named function called as the only statement inside the loop. And some resource acquisitions can't fail (lock example, above), by which I mean don't return an error, and it's then less obvious that must defer the resource release. Even if it's just two lines down that you want to release it, with no (visible) branches. A newbie who wanted to force Close before end of function and who actually believed what the Go authors say ("Go doesn't have exceptions") would expect that defer is just a convenience, not something you **have** to use in essentially all cases of resource acquisition and release. You mention other good things about Go, which I agree with. I code quite a bit of Go nowadays, and I like much of it. You left out ease of deployment, which is awesome. One binary, no dependencies. Just copy the file to a server and run it. Go is a younger language, and I've refactored code when Go1.1 came out because new language features (pointers to member function, and better code analysis understanding "if(foo)return 1 else return 0" without giving compile time error), so maybe in 10 years we'll have crufty Go1.1 code with Go13.0 code mixed in.
There is also a proposed boost library which does this: http://conststring.sourceforge.net/
I'm willing to consider it (or D) if it can turn a 2 hour rebuild into something more manageable.
It's easy to say "they just don't understand", and that's what this boils down to. I appreciate the substantive explanations of Go's motives, though.
I tried :-) I did read a lot about D, and I love the approach to templates. And nobody could dislike modules. But when I tried to get into D a couple of years ago, I couldn't get it all to work together for me. I wanted to try to latest functionality, with full support for ranges and so on. And I vaguely remember Tango/Phobos confusion. Maybe it's more mature now, but in the meantime I've gotten quite good at C++ :-)
Is there a good source to learn C++ that way? When I last touched the language over a decade ago, that didn't seem to be the case.
D has improved a lot since you've seen it then, but it is still suffering under enforced use of a garbage collector that is currently a bit slow in the standard D runtime. The two weird version thing is gone and now has 3 different standardized compilers (GDC, LDC and DMD that latter being the reference compiler). But no good cross compilers yet. Some D folk are looking at making a ARM compatible port or rather getting the GDC to talk to the GCC ARM backend (or indeed most backends). D is also still trying to get a compiler (GDC) into the GCC project.
C? Simple? My friend, here's a brainfuck interpreter: #include &lt;stdio.h&gt; int p, r, q; char a[5000], f[5000], b, o, *s=f; void interpret(char *c) { char *d; r++; while( *c ) { //if(strchr("&lt;&gt;+-,.[]\n",*c))printf("%c",*c); switch(o=1,*c++) { case '&lt;': p--; break; case '&gt;': p++; break; case '+': a[p]++; break; case '-': a[p]--; break; case '.': putchar(a[p]); fflush(stdout); break; case ',': a[p]=getchar();fflush(stdout); break; case '[': for( b=1,d=c; b &amp;&amp; *c; c++ ) b+=*c=='[', b-=*c==']'; if(!b) { c[-1]=0; while( a[p] ) interpret(d); c[-1]=']'; break; } case ']': puts("UNBALANCED BRACKETS"), exit(0); case '#': if(q&gt;2) printf("%2d %2d %2d %2d %2d %2d %2d %2d %2d %2d\n%*s\n", *a,a[1],a[2],a[3],a[4],a[5],a[6],a[7],a[8],a[9],3*p+2,"^"); break; default: o=0; } if( p&lt;0 || p&gt;100) puts("RANGE ERROR"), exit(0); } r--; chkabort(); } main(int argc,char *argv[]) { FILE *z; q=argc; if(z=fopen(argv[1],"r")) { while( (b=getc(z))&gt;0 ) *s++=b; *s=0; interpret(f); } } Ah, how simple! Elegant! *Jizzes hipster pants*
Then you'd have to indirect through two pointers to get to the string data. It would be more efficient to build reference counting into the string class itself. But any sort of reference counting is going to be a pessimization unless the strings are long enough to make copying expensive.
He didn't advocate its use everywhere, just as an option to manage ownership in multi-threaded code. It does that well.
A lot of teachers seem to be stuck in the mentality that object oriented programming is some revolutionary concept only suitable for advanced discussion. C++ is basically C with classes, right guys? Clearly more advanced. In reality, the additions to C++ make a lot of things simpler, taking care of a lot of the busywork necessary in C. I don't see how these teachers think that a language that is basically a hugely stripped down C++ without all of the useful shortcuts should be taught first. I mean, why not teach assemly first, by that logic? C is basically the least you could possibly do to assembly and wind up with something that's more than 50% or so readable. It sees your string of text, and, with the giddyness of a redneck reaching for his duct tape and WD-40, decides that it can represent that real good with an array and some integer pointers. And lo; the written word, passed down from the ancestors, became the memory address of some ASCII codes, sequential in memory, terminated in null. And C saw that it was good. "But how long is it?" the programmers questioned. From heaven, C thundered "It is exactly O(N) long, no more, and no less." Yet, the programmers were not contented; "What if our buffers the enemy overruneth, and so take from us the fruit of our labors?" And C answered, "Not my job." "And how do we concetenate?" At this, C became angered, and so struck them with the dreaded segfault.
http://bhattaca.github.io/cse2122/ There are some materials for a college class. I recommend trying to follow all the curriculum and do the homeworks.
Accelerated C++ dives straight into teaching how to use C++ correctly, rather than spending the majority of the book on a history lesson. It was an extremely good book when it came out, and unfortunately despite being somewhat dated now it's still probably the best book for learning C++.
If switching VS configurations causes a full rebuild more than once per configuration, then you have your object output path set up poorly. Of course, storing object files for a half-dozen combinations of configuration and platform eats up quite a bit of HD space...
Whether or not reference counting is a form of garbage collection is a matter of some debate. Manual reference counting is clearly not GC. Automatic reference counting behaves similarly to GC in some ways, but differently in many important ways.
Isn't there an option to completely disable the garbage collector? 
&gt; GC: needs to perform well with gigabytes of memory usage. I've yet to see this. Don't say it's 10% slower with a toy program--how much slower would it be for a browser with 20 tabs open? How's that java internet browser Sun made doing these days? The Go guys already said Go isn't a language for high performance and precision mostly due to the one-generation garbage collector and they will be working on a solution to this in the future. What their 'solution' will be I don't know.
ease of implementation and verifiability. i cooked up something like this but I allowed the contents to be replaced. so rvalue ref, swap, operator= and assign worked.
not in multithread context. dont ref count strungs please!
if you dont plan on using any d libraries.
I think it's rather a smaller footprint (you don't want system daemons to use 25 MB each for basic VM data structures) and a desire to make the libraries available everywhere, to both C/C++ code and Python/Ruby/Perl/PHP. And there's the usual desire to stick to tradition, which makes even C++ a fairly radical choice over C. I've seen quite a bit of C++ code form different authors recently, and most of it does not use RAII consistently, employs static polymorphism in a very limited fashion (which is often a good idea), and doesn't come with custom memory allocators or machine-specific code (definitely a plus in our case because we support server architectures beyond i386/x86_64).
&gt; Manually check return codes at each function call, like in C? Yes, that's how it is done most of the time. &gt; Ignore errors? Some types keep track of the error state internally, so that you can write correct code that ignores errors locally. For example, some code calls `bufio.Write` several times, without checking errors, followed by a call to `bufio.Flush`, checking its error return value. That makes it exceedingly hard to find missing error checking using a tool, and I don't think there is one right now. Fortunately, totally ignoring errors is only a (relatively) common mistake with functions called for side effect, like `Write`. Most code that accesses other function results also checks the error code. One really nasty problem in this area is that the I/O functions in the Go standard library use a different error return pattern than the rest of the library. Usually, it's sufficient to check the error code and bail out, but with the I/O functions, you have to check the other result first—they can return data *and* an array. 
Don't assume anything. Don't build on assumptions. When I was first starting out in CS, one of my professors mentioned this. His explanation is that until you know something for a fact, you dont know it at all (when it comes to programming.) His suggestion to the class was to first verify how something works, maybe spend a couple hours extra to really understand what's going on. Write an example program that isolates whatever problem is befuddling you. Basically first prove to yourself that whatever you're about to write will do what you want it to do. As time goes on and you code more, you'll build up some experience and won't need to look at the reference material nearly as much. Also get yourself a bookmark for Stack Overflow.
Your write/flush example is disingenuous. In most code, you really want to stop as soon as error happens (the fail early Unix philosophy).
It's just that you have been conditioned. There's quite a bit of languages that do it "backwards". what is wrong with "a is an int", and isn't it better than "here's an int, and it's called a"?
Don't try to guess your way from a buggy to a working program. Though trial and error is helpful in the start, you have to move from it towards methodical problem solving (both during program design and debugging) as soon as possible. This requires conscious practice. IMO, the most important part is to learn to use a debugger early on. It is THE key tool for scientific debugging (establishing *facts* for why the program does the wrong thing, and what you need to code differently in order to make it do what you want). I found it helpful to first write down the complete program on the paper. Syntax errors don't matter at this stage, the compiler will weed them out. This forces you to think about what you want to get done. After you have gained some experience, you can drop this stage. This paper-programming was forced on all students during the first programming course at the university. 
 &gt; &gt;If you have a pointer to a const string, there could be code elsewhere holding a mutable version of it. Yeah, but then you have a serious design error in your code. 
Yes, the class is movable. This is why the contained basic_string itself is not const.
Funny, when talking about D everyone seems to be mentioning templates, yet when I use D I barely use them at all. I use them more like generics in Java/C#, for specializing container types and stuff and nothing more. Yes, modules are awesome, being spoiled by D/Java/C# I just can't go back to C++ and those pesky .h/.cpp files. Tango/Phobos conflict is dead. If anything, the conflict now is between using GC in standard library and not using it, although this doesn't directly affect the end-user of Phobos. For me D is C++ with syntax of Java/C#, I hope it grows so that I can use it every time I'd consider using C++.
One final remark from Linus Torvalds about C++. http://harmful.cat-v.org/software/c++/linus
Which ones? asking out of curiosity. 
&gt; Java's garbage collector will just cope by magic This isn't entirely true. Java and .NET's GC can have trouble freeing long-lived objects. If an application is run for a long period of time and periodically does allocate these long living object before forgetting about them, then the application will behave as if it is leaking memory. There are rules to remember with a GC too.
Yea you can't even use new/delete haha
I've had it happen under certain use cases in Visual Studio 2010, but mostly on a Code:Blocks installation enforced by a embedded development toolkit.
Whenever you're writing a destructor in a non-resource class you're making a mistake.
You can as far as I know (and unless that has changed in the last few months) work without the GC but major parts if not all of the standard library is written assuming its presence and so you also opt out of a lot of stuff there. Finally there are a group of array and other operations as far as I know that don't work without the GC support. But D development is aware of it so it might have changed since their conference (I watched all their videos to see if I should go back to using D for my hobby projects).
You can as far as I know (and unless that has changed in the last few months) work without the GC but major parts if not all of the standard library is written assuming its presence and so you also opt out of a lot of stuff there. Finally there are a group of array and other operations as far as I know that don't work without the GC support. But D development is aware of it so it might have changed since their conference (I watched all their videos to see if I should go back to using D for my hobby projects).
... unless its an empty virtual destructor for a base class.
That might be the problem. I'll experiment with that. I am thankfully off that project now but the maintenance guys would probably want their builds accelerated too. I am using Qt Creator at the moment and with concurrent builds it is a joy to work with even on Windows and on a small and new C++11 codebase, build times in a 2-4 seconds typically.
Well, you *can* make new return a null pointer on allocation failure, so I think that should be fine.
To reduce risk of accidental misuse (`invert()` vs. `inverse()`). This is a case of weighting Effective C++ #23 against #18. `invert()` being a non-const member makes it very clear that it operates on `*this`, i.e. in-place. Having one as a member makes it more difficult to use the wrong function: void matrix::invert() { ... } matrix inverse(matrix m) { ... } vs. void invert(matrix&amp; m) { ... } matrix inverse(matrix m) { ... } The first one is harder to misuse, in my opinion. Maybe it would be even better design to have: void invert_inplace(matrix&amp; m) { ... } matrix inverse(matrix m) { ... }
I agree that because C++ is difficult to learn you get people who don't know much about it having to use it. I would know, half of my team is not very proficient in C++. And yet they can do their job. You do not need every member of the team to be an expert in C++, nor do you need everyone to be an expert in databases. 
I spend a whole lot my working day just recompiling C++ code. More importantly it breaks the flow.
As far as I know they are more than aware of it. A lot of the enthusiasm for D is coming from game developers and they really don't want a GC messing with their frame rendering. 
Define "*resource class*".
(Hey, why NOT reply to a question asked two weeks ago?) [C Interfaces and Implementations: Techniques for Creating Reusable Software](http://www.amazon.com/Interfaces-Implementations-Techniques-Creating-Reusable/dp/0201498413) Certainly not a new book, but it's the best I know of on the topic.
Build the parts that are important in C++, make individual programs - and string them together using bash. Once the individual parts work - you can start to code in the glue. [Use the unix philosophy](http://en.wikipedia.org/wiki/Unix_philosophy)
&gt; once you've learned [C++], […] you need very strong arguments to abandon it Ocaml was such an argument for me. Every single C++ line I have written to date would have been better off being written in ML, if only 1. My colleagues knew this "alien" FP stuff, and 2. The software we depended on wasn't already written in C++. In my domain, the only reason why C++ is still used is plain inertia. Which is often rationalized by a perceived need for performance, but this is crap: I have seen C++ programs be several orders of magnitudes _slower_ than my naive Ocaml code. The real killer is uncontrolled complexity, and C++ is hardly a solution here. --- I reckon, there _are_ some niches for which C++ is still the only reasonable choice, such as real time applications and embedded software… I just suspect the majority of C++ code out there would have been better off being written in a garbage collected language instead. 
And abandon *"total control over how memory is used"* in the process. Can't have it both ways.
Read [The Pragmatic Programmer](http://www.amazon.com/gp/aw/d/020161622X). I read this when I was first learning C/C++ and starting my first job as a developer (I did both of those things essentially at once -- it was a little crazy) and it set me down a path of true craftsmanship. There is incredibly good stuff in there. 
&gt; I just suspect the majority of C++ code out there would have been better off being written in a garbage collected language instead. Depends on what kind of code we're talking about. My C++ code relies heavily on static types (as in, if I have a `Foo&amp;`, then it's really a reference to an object of class `Foo`), and therefore, I rarely need to use dynamic allocation: I create an object somewhere in the scope, call functions with it as an argument (or its member functions), and let it die at the end of the scope. I suspect that, if you feel the need for a GC, you're using lots of inheritance polymorphism (as in, `Base* b= new Derived`). And then, I agree with you: Java is probably better for that, since it was made for that kind of programming. 
&gt; Maybe it would be even better design Probably, yeah.
A class that manages a resource, like memory, that must be explicitly released.
Now I'm curious: what does good Java look like?
&gt; Your write/flush example is disingenuous. I saw this in `encoding/xml`. I see it's fixed in Go 1.1. But the other problem remains, that errors and non-error return values are sometimes treated as an either-or choice (sum), and sometimes as independent information (product).
I was merely replying to &gt;&gt;&gt; in a garbage collected language I agree that FP is something completely different.
Last time I heard, the C++ standard is over a thousand pages long. Teaching such a behemoth first would be a _joke_. Also, C++ *is* C plus a few things (okay, a lot). A thorough knowledge of C++ necessarily implies a thorough knowledge of C. Whatever can bite you in C can also bite you in C++, if you don't pay attention. Now, one *could* learn only the C++ idioms, and learn to use iostream as if the `&gt;&gt;` operator was specific to iostream, as opposed to an overloaded bit-shift operator. But then, the language becomes as magic as Java or ML, except the abstractions are leaky. &gt; I mean, why not teach assemly first, by that logic? Actually, learning an artificial, simplified, stripped down assembly language is probably a good idea. My cousin started with Nand2Tetris a few months ago, and it's being a great help in understanding C right now.
awesome, thx
&gt; You're basically saying, "If you keep any const pointers to an object, then that object must be immutable everywhere. Actually, I am saying "*while* I keep...", which is different. That clearly is both desirable and practical. Here: void f(const type&amp; param) { use param... } You can't reasonably claim that param should change while used, or that it would be practical - what would be the point of that? It's just a massive lie to the user.
Yes, referencing the subject and content of the OP is a real "tangent."
Like D, Pascal and many others.
It's almost cliché to say this, but go get [Scott Meyers' Effective C++](http://www.aristeia.com/books.html) book. It just really really good for teaching you how to do things the right way from the start.
Thanks for all the advice guys! I was afraid this was going to get a whole bunch of flack, and people not helping me. Thanks again. -Cheers
&gt; It's funny, because C++ programmers often feel that a C-like type system isn't expressive enough; template metaprogramming allows them to do things they can't do with C, and they understand the utility of the evolution. But when you tell them that they should go full-blown System F, they balk. It's the classic problem of that it's difficult to judge how useful something is without having used it. Many things are a lot more useful than they appear, or vice-versa. IMO the core distinguishing factor between a Blub programmer and a non-Blub programmer is simply a willingness to waste time trying out programming styles and languages that sound useless.
I don't know of anything more recent that takes a similar approach. If you want to learn C++ I guess my recommendation would be to read Accelerated C++, then spend some time learning about the changes in C++11. Fortunately many of the changes in idiomatic C++ between 2000 and 2010 were incorporated into C++11 with a lot of the language changes just making it easier to do things that people were doing anyway, so C++11 has spawned a wave of useful articles without introducing as much more stuff to learn as one might expect.
This is so awkward. I really like how Python did it: lock = threading.Lock() # this just creates a lock object [...] with lock: &lt;do stuff&gt; You can of course use lock() and unlock() methods, but most of the time this works well, and you know that lock will be released as soon as the block is left for any reason (including exceptions).
[The Java language specification (warning PDF)](http://docs.oracle.com/javase/specs/jls/se7/jls7.pdf) is over 600 lines long. (The C++ standard includes not only the language specification, but also the standard libraries. The C++ language specification is only about 460 pages. The other 700+ pages in the C++ standard are library specifications. (Source: [n3337 - The C++11 final draft standard](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3337.pdf) (warning: also a PDF document)) [The C# language specification (warning PDF)](http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-334.pdf) (again, no standard libraries) is over 500 pages. All in all, C++ as a language seems less complex (if you're counting pages) than Java and C#. EDIT: Added information from the C++ standard.
Bandwidth limit exceeded
 std::pair&lt;std::vector&lt;int&gt;, int&gt; func() { return std::make_pair(); } 
- Make a class that encapsulates the vector and int and return that. - return a `std::pair&lt;std::vector&lt;int&gt;, int&gt;` - return a `std::tuple&lt;std::vector&lt;int&gt;, int&gt;` - pass the vector by reference and modify it in place and return the int - pass both the vector and the int by reference and modify both in place, and return void 
one of the above, that is
better ask such questions in stackoverflow. you might get answer directly as someone would have asked a similar question
So what you're saying is that this wouldn't be an acceptable solution? struct vec_int { vector&lt;int&gt; v; int i; }; typedef pair&lt;std::vector&lt;int&gt;, int&gt; vec_int_pair; typedef tuple&lt;std::vector&lt;int&gt;, int&gt; vec_int_tuple; typedef tuple&lt;vec_int, vec_int_pair, vec_int_tuple, int, void*&gt; ret_t; ret_t fun(vector&lt;int&gt; &amp;v, int &amp;i) { vector&lt;int&gt; res_v = {1, 2, 3}; int res_i = 4; v = res_v; i = res_i; return ret_t({res_v, res_i}, {res_v, res_i}, make_tuple(res_v, res_i), res_i, NULL); }
[Google Cache, sadly without images](http://webcache.googleusercontent.com/search?q=cache:http://fabiensanglard.net/rayTracing_back_of_business_card/&amp;strip=0).
The Go 1.1.2 Programming Language Spec http://golang.org/ref/spec is only in html format. When printing this to pdf, I get 68 pages.
Interfaces preferred over abstract classes and, most importantly, composition preferred over inheritance. Basically, the less types you have to traverse to figure out what something _does_, the better. And no code re-use via inheritance, that is the devil's tool.
Hacky solution: push_back the stand-alone integer to the returned vector and let the callers pop it.
Wasn't too difficult to make them. Very cool! Img 1: http://imgur.com/P9djObk Img 2: http://imgur.com/aU9JZP5
For this reason tuple/pair is probably the cleanest solution.
Buy a book. A good one. I recommend The C++ Primer 5th edition which covers C++11. I guarantee if you learn of online tutorials and such like you'll probably learn some pretty terrible style.
Since shared_ptr is completely deterministic and easy to understand in principle, I wouldn't say that.
Yeah, in principle. But in practice? No way: for instance, pick a program that uses `shared_ptr` in an actually shared, relatively long lived object. Then tell me when that object is actually freed. That's not exactly as easy as looking for the closing bracket. Most likely, it depends on the dynamic behaviour of the program. So in practice, you can forget "total control over how memory is used" whenever you use shared pointers. Those who advocate C++ for the memory control it gives you, then advise you to use `shared_ptr` are very close to contradicting themselves. When you actually take advantage of total memory control, you use custom allocators, and mostly forget about shared pointers. Now, sure, C++ lets you use both methods. But you need to make sure the parts using shared pointers, and the parts using custom allocation are properly segregated. At which point I'm strongly tempted to use a scripting language such as Lua. 
Yeah, I looked it up now and maybe the term *I* was looking for was "tracing garbage collection".
It's sort of pythonic. &gt;&gt;&gt; a = [...] &gt;&gt;&gt; a.sort() # returns void, sorts the list internally &gt;&gt;&gt; sorted(a) [...] # returns a sorted list, does not modify a
Wanting full control over memory, doesn't imply doing it all the time. For instance: If I have shared memory, I need to tell some of my data-structures to use it, but that does not imply, that I want to care about the exact point in time, when the data is released. And btw: shared pointers and custom allocators don't contradict each other, if done right there is no problem anywhere with that combination. That being said: If a program uses shared_ptr all over the place, it should be thrown away.
This should work with pair also because tie() just creates a tuple of references, which you can assign a pair to with no problem.
And if you're vector needs to be reallocated, you pay for a free store heap allocation. 
Hell no.
&gt; A thorough knowledge of C++ necessarily implies a thorough knowledge of C. Students learning it don't need a thorough knowledge. The only need to know sufficient bits to program, and in good C++ that leaves out most of the C bits.
Actually, C is not simpler, C is more simplistic. Simplistic != simple.
Oh no worries I am blaming the tools.
In C++11 you can pretty much get rid of that with template type aliases: https://github.com/socantre/LTR
I like that. It seems somewhat better, looks like you can't use it with function definition so you'd still be out in the cold with something like void (*signal(int, void (*fp)(int)))(int); 
You can't use it with the definition, but you can use it for a declaration: func&lt;ptr&lt;func&lt;void,int&gt;&gt;, int, ptr&lt;func&lt;void,int&gt;&gt;&gt; signal; A function definition requires the standard syntax, at least at the top level. And with only one level of non-LTR syntax it can be quite readable, arguably more readable than the pure LtR syntax above: auto signal(int, func&lt;void, int&gt; fp) -&gt; ptr&lt;func&lt;void, int&gt;&gt; { // ... }
Which happens more often than you'd think.
Then don't start with C++. Pick Scheme, or ML, or Lua, or Python… If you don't intend to teach low-level stuff in the first place, don't use a language that does low-level. Besides the fundamentals, students can also learn computer architecture, for which C can help. Once the above is done, you can talk about interpreters and compilers. Every single programmer should know about them. I think the fact they don't is a major failing of our industry, which likely cost humanity $trillions in sheer wasted time. _Then_, you can mention C++. At this point, it should be easy to learn. 
libstdc++ does [actually have a written manual](http://gcc.gnu.org/onlinedocs/libstdc++/) in addition to the auto-generated mess. You might as well bookmark that instead, as it links to the doxygen stuff under "API and source documentation." Better yet, just bookmark [`gcc.gnu.org/onlinedocs/`](http://gcc.gnu.org/onlinedocs/) instead. (I always scroll down to the "current development" section at the bottom, as that gives links to the latest version without embedding a version number in the URL.) 
Read many books, and do many simple small projects. http://www.amazon.com/Tour-In-Depth-Series-Bjarne-Stroustrup/dp/0321958314
&gt; If you don't intend to teach low-level stuff in the first place, don't use a language that does low-level. Teaching students C++ as an abstraction building language, and then to write high-level C++ programs using the abstractions they build, doesn't mean that the students don't learn about the low level things that they are abstracting. But it does mean that the programs won't look anything like C.
Well done Marshal, Howard and all the rest !! hopefully some implementations will allow for any std errors to be improved before finalisation. Great work.
Yeah, nice to see a full implementation before the standard is finalized. I've always liked whatwg's idea of requiring two interoperable implementations of things before they can be declared standardized, since it helps ensure that the standard is sane and avoids things like export templates.
In this example the QML is only used for the GUI. Business logic and the adapter layer are written in C++ (although combined in this overly simply example).
I just like the idea that the LLVM/CLang team is still going like gang busters. The participants deserve lots of credit. 
sudo apt-get install libstdc++6-4.7-doc
I'd really like to see N3716 (A printf-like Interface for the Streams Library) added to C++1y.
I would have dropped entirely the compatibility with printf non-DRY format specifiers (d, u, l, x, p, s, etc) as C++ type system already has the information, and kept only positional arguments (and fields stuff). More like [this](http://code.google.com/p/nicola-bonelli-repo/source/browse/trunk/codes.cpp0x/print.hpp?spec=svn612&amp;r=612)
Looking forward to reading the updated paper on what was formerly known as polymorphic allocators (now polymorphic memory resources). If you want high performance and you want to use the standard library extensively, custom allocators are essential. They're fairly easy to use in an isolated context, but if you have different allocation policies for different objects, having to do type conversion manually between them is a big hassle and potentially a performance cost, especially if you'd otherwise be able to pass by a const reference. In other words, it would be nice to be able to say, "Here's a const string for you to read. You don't need to know how it was allocated because you're not modifying it." 
Boost.Format also fills this role
&gt; More to the point, If I wanted to teach abstraction building, I wouldn't chose C++. So you claim to know what building abstractions means after all. &gt; I maintain that overall, C++ is a terrible teaching language. Your original point that I was disputing was that the only sensible way to teach students using C++ is to either teach the thousand page spec, or to not explain things like operator overloading. My contention is that this is not true, that one can choose much better things to skip over such that one is neither teaching too much nor too little. Whether that makes C++ a non-terrible teaching language I don't care to argue.
Which is (at least partly) supported by the proposal.
Oh hey, what a coincidence. I wrote the question that led up to this one. EDIT: lead and led are different. derp. Also cake day (woohoo)
Doesn't work on vs2012. It looks like it tries to compile it, but it's getting confused with the different types of calling conventions. EDIT: works on MSVC 64-bit, since there is only one calling convention. 
I'm dubbing this the "anonymous function pointer" idiom. I guess you don't need the operator+ in a lot of cases, but when you do... sweet.
Yes... yes they do. Could be worse, though. 
Yes. Could be doing x86 16-bit code. ;-) Also, could be using VC++ 2.0. ;-)
Oh, I was using VC++ 1.52c back in the day. I've been there. In fact, I still have the CD-ROM. 
There must be one calling convention that it is choosing to go with. I wonder if you can do something evil like declspec(auto) to get desired behaviour.
Why is support for unary plus so widespread in languages? I've never actually seen it used in real code in any language, and it seems like every language has one or more wtfs related to it.
This pretty much explains it: [http://stackoverflow.com/questions/14845706/whats-the-default-calling-convention-of-a-c-lambda-function](http://stackoverflow.com/questions/14845706/whats-the-default-calling-convention-of-a-c-lambda-function) 
Forgive my late reply - and kudos for putting your money where your mouth is and feeding back to the repository. Re: performance - I wasn't expecting Haskell to be fast(est), to be honest. I mentioned above that I don't see any evidence of it "blowing [algol] languages out of the water" - and I like to do my little bit to dispel this here-say. For what it's worth, I can see the value in having a language force guarantees of correctness - I just think that it's possible to write code in this style without throwing the performance baby out with the bath water. Now doing this requires competent talent behind the keyboard, of course - but I'd argue that Haskell is complex enough in getting performance out of a program that you'll need that talent anyway - but your program will come out slower. I just don't see the performance niche that Haskell is trying to fill that can't be done elsewhere with appropriately competent coders. 
Please use an explicit cast in such situations, don’t confuse future maintainers of the code. (That said, I can’t think of a situation where either would be useful – normally, the implicit cast should suffice.)
The implicit cast is only sufficient when you declare the variable as a function pointer rather than the lambda closure type using auto.
Well I’m saying that in cases *where it matters*, you will pass the lambda to a *function* with appropriate parameter types anyway, or at least to a non-local variable whose type has to be made explicit anyway. Why would you explicitly create a *local* variable of type pointer-to-function rather than lambda?
&gt; you won't fine tune your algorithm repeatedly until you've squeezed out ... FYI: I'm the author of the C++ in that benchmark - and I spent probably four hours on it (and the previous single threaded benchmark) - hardly tuning it repeatedly. &gt; Real world performance is hardly quantifiable - programmers discussing the &gt; performance of languages are like fishermen comparing the yields of various &gt; bodies of water; no directly relevant hard data exists, you go by personal &gt; experience, intuition, and the currently accepted body of knowledge. I'm not sure I agree with this - surely as computer scientists we can do better than feeling? I presume you've seen the programming language shoot-out? Does this not count as evidence / data? No other languages get a wavy hand Jedi mind trick when it comes to performance - why is Haskell magically different?
Here's the code that I wrote that triggered my initial quesiton (written within the context of a 2D Rectangle intersection test): auto compare = [](T a, T b) { return a &lt;= b; }; if (!includeEdges) compare = [](T a, T b) { return a &lt; b; }; //do a bunch of things with compare I had to change it to: std::function&lt;bool(T, T)&gt; compare = [](T a, T b) { return a &lt;= b; }; if (!includeEdges) compare = [](T a, T b) { return a &lt; b; }; //do a bunch of things with compare 
Thank you for pulling out the context for me. In the first quote, the key word is "first". I was specifically answering [this](http://www.reddit.com/r/cpp/comments/1ms6mv/less_is_exponentially_more_rob_pike_on_why_c/cccz9g1): &gt; I don't see how these teachers think that a language that is basically a hugely stripped down C++ without all of the useful shortcuts should be taught first. Which I interpreted as "C++ is a better first language than C". In my opinion, this is plain silly, and based on faulty assumptions. The "shortcuts" for instance, while very useful for actual programming, don't really help learning. Student's don't get a deep understanding of VTables by writing C++ class hierarchies. That understanding is better acquired by emulating the whole thing in C. I reckon I may have been a bit carried away, though. I don't like C++, and I hate when it is chosen by default on domains it is clearly not good at. Nearly five years of inappropriate C++ made me a bit… sensitive. --- &gt; I agree with Stroustrup that "Within C++, there is a much smaller and cleaner language struggling to get out." (And no, it's not C.) That's one of his many good quotes. And I agree, it's not C. If I were to attempt this, I think I would start with: - Remove classes and methods. Replace them with Lua-like syntax sugar to avoid repetitions like `my_obj.my_func(my_obj, arg1, arg2)`. - Keep, and maybe rework, templates. - Remove the `const` keyword. Make everything const by default, and let it be overridden with `mutable` or something. That should help with const-correcntess, and force people to be mindful of [uncontrolled side effects](http://loup-vaillant.fr/articles/assignment). - Tweak the syntax so that we can actually parse this damn language. - Replace the current header files system with something saner (I don't know what, but I hear other people are working on it). Of course, backward compatibility would be thrown out the window. I suspect this is a requirement, or we would all use that cleaner language by now. 
If it stops being surprising when you replace the unary plus with a clearer equivalent, then the surprising part rather obviously is directly related to unary plus.
&gt;No other languages get a wavy hand Jedi mind trick when it comes to performance - why is Haskell magically different? FYI, I hold my opinion in the case of *all* languages. The only benchmark that could possibly be reliable here is to have several teams implement the same real-world project each in their own language. We could measure man hours, correctness and implementation performance. Even then though, it would be hard to control for developer experience - Bjarne Strousup would hand his ass to a college undergrad regardless of the languages either of them chose. In essence, I agree that performance does vary significantly between languages, but getting a clear view of how is probably not easy (beyond "C++ is faster than Java is faster than Python".)
Oh hey, it's Omni! Good luck on Starbound, I'm sure it will be amazing!
Thanks!
Look around for the Intel compiler, if you're running an Intel system (you probably are). It's free [for students?] and probably the best one out there.
But that shouldn't compile, because the types of the two branches of the operator do not match. You'd have to do: auto compare = include_edges ? +[](int a, int b) { return a &lt;= b; } : +[](int a, int b) { return a &lt; b; } *Update*: Okay, your version totally compiles fine... all that is missing is any semblance of my own understanding as to why. Care to enlighten?
That's the kind of thing where you ought to burn it in effigy.
I recommend gcc, just download cygwin or mingw.
I'll check it out thanks! Why'd you get a downvote?
Any reason you dont want to use VS2012? Visual Studio 2012 Express for Desktop should let you do anything you want to do with a C++ compiler. At a minimum it will let you do anything you could do in VS2010. I have had some issues finding the correct express edition in the past so I hope this helps - http://www.microsoft.com/visualstudio/eng/products/visual-studio-express-for-windows-desktop
Probably because it's not *actually* free (it's a limited trial).
The types of the two branches do match: both types are "pointer to function with two int parameters which returns a bool." (pretty sure)
They don't need to be of the same type; they just need to be convertible to the same type. A particularly silly example of this: #include &lt;cstdio&gt; #include &lt;cstdlib&gt; struct Foo { operator int() const { return 0; } }; struct Bar { operator int() const { return 1; } }; int main() { printf("%d\n", rand() ? Foo() : Bar()); } 
I guess I was unaware that the terniary operator would magically select a conversion to a common type. I thought you had to be explicit about the casting to make it work.
Hmm... I guess the catch is that they have to have only one common convertible type.
sorry to be blunt, but if you can't manage to simply uninstall a couple of programs after messing up you PC for unknown reasons (why would you prefer 2010 to 2012? mystery) I doubt you'll be able to do anything useful with any compiler. 
Actually, you can use it for console apps.
Really? I might just get it again then. When I got it I was just following my friend's directions. Like I said, I'm just barely getting into this stuff. Thanks!
I would recommend clang Being a student unless you have the student license, I would also go as far as to recommend codeblocks for your ide that way you can switch compiler quickly 
I was replying directly to this question: &gt; Why is support for unary plus so widespread in languages? And the answer is that no such “widespread support for unary plus” exists. That’s all I was referring to when saying “it’s not surprising”.
But why should we *let* it become an idiom? It doesn’t solve a problem. Idioms are usually created because they help us solve a problem. Whimsically abusing notation can of course create idioms if it’s widespread enough – but should it?
Personally I would avoid MSVC when starting out because it won't actually teach you to write C++ code anyway (IE how to orgranise files, Makefiles....) it just teaches you how to use MSVC; thus locking you into windows (which is why it's free).
I really like all the work behind *N3753 - Centralized Defensive-Programming Support for Narrow Contracts*. My definition of *Defensive Programming* seems to slightly differ from the one used by the authors. In my comprehension, *DP* is about doing everything we can for a program to never end abruptly. When, we are considering abortion through assert (or equivalent here), it doesn't match precisely my definition. Anyway, this is not important. I have a question for the authors of the proposal (in case they read this message...). Why restricting the proposal to preconditions only ? Shouldn't it encompass all DbC contracts instead ? i.e. invariants and postconditions as well. It sounds a bit odd to use **pre**_assert for postconditions and invariants. And yet, I don't see any reason to not use this new feature on other DbC contracts, as I've done in this little [pet program](http://ideone.com/DOCWOy) for instance, but with plain assert(). It could become dbc_assert instead.
Clang is great on Linux or OSX, but getting it going on Windows is not easy. They've made made a lot of progress on that front. They recently released an installer, so you don't have to get the source code and build it yourself, but you still have to do a lot of manual setup before you can compile "hello world!". If you're starting now, grab Visual Studio 2013 Express RC (http://www.microsoft.com/visualstudio/eng/2013-downloads#d-2013-express)
Here [http://nuwen.net/mingw.html](http://nuwen.net/mingw.html) is great GCC distro containing also useful libraries and tools in one bundle.
I haven't talked to the authors about this paper. But I guess the reason for focusing on pre_assert is simply that usually bigger parts are split up into smaller packets for standardization, as its a long process, and the paper is already quite big.
Once there was a proposal for adding contracts to the core-language; it looked nice and would certainly have been nicer than adding even more macros. I would be interested if someone knows what happened to it.
Your answer initially seemed slightly confusing, and now extremely confusing. So you're saying that in an answer to the question "Why is unary plus so widely supported", you answered "It's not that surprising", because you believe that unary plus isn't widely supported? 
What I said is: There is no *special handling* of unary plus to allow these “surprising” semantics.
Ahh, then your confusion makes sense, sort of. Plorkyeran wasn't saying that anything surprising about unary plus lead to the results he just phrased the question: "Why is support for unary plus so widespread in languages?"
&gt; don't see any evidence of it "blowing [algol] languages out of the water" - and I like to do my little bit to dispel this here-say Well, I did qualify it with "in theory" :-) The reason I believe this, is because Haskell makes finding safe parallelism opportunities so much easier, and makes automatic translation to things like GPU code possible. I think GHC is being hyped more than it deserves at the moment, as there are so many basic things that could be done to *vastly* improve the performance of its outputs. Nested data parallelism, for one, sounds very promising. It's not something you can practically do in most languages. &gt; For what it's worth, I can see the value in having a language force guarantees of correctness - I just think that it's possible to write code in this style without throwing the performance baby out with the bath water There's a big difference between writing in this style with a compiler guarantee and without: * When working with a team, the lack of a guarantee is going to be a problem (paraphrasing Carmack: "Anything syntactically accepted by the compiler will wind up being committed into your code"). * The optimizer will have a lot less room to work with your program and optimize it. The guarantees don't only help you reason about your code (safely), they also help the optimizer do things it otherwise can't. &gt; I just don't see the performance niche that Haskell is trying to fill that can't be done elsewhere with appropriately competent coders. Complicated STM code that works in Haskell is going to be hard to reproduce elsewhere. Clojure comes closest, but it only has runtime guarantees, and only if you only used wrapped IO actions that verify not being in a transaction. If your logic is easy to express with STM, Haskell is going to make it easier. As I mentioned above, nested data parallelism is not something other languages will be able to reproduce. Immutability guarantees really help with message passing concurrency, too. Also, Haskell offers a variety of concurrency and parallelism tools, so it's more likely to have a good solution for various needs. Most other languages focus on having just one tool (e.g: Go chans).
He was expressing wonder that unary plus is actually a part of languages, since its behavior on most types is by definition redundant. This particular behavior isn't surprising once you've seen the why, but the motivation for even defining a unary plus operator in the first place is somewhat confused.
Are you a non English speaker per chance? Widespread support in languages means that many programming languages support it, not that it is implemented with special or unusual/surprising elements. But we've found the root of the confusion at least, you didn't understand the post and was arguing based on a different interpretation of what was said.
&gt; Widespread support in languages means that many programming languages support it Sure, I get that. But the *context* of that sentence was that the unary plus has gotchas (and additionally doesn’t seem to have uses). And those gotchas – I assert – don’t really exist. The existence of unary plus itself, without any context, is of course a valid question in itself. Maybe I over-interpreted the question. But then by the same token we might ask why languages allow us to write both `.2` and `0.2` etc. Unary plus is there for symmetry with unary minus, and that’s its use (i.e. when you want to make the contrast between the two glaringly obvious in code).
I actually never heard of this, and I use Qt every day! 
Out of curiosity, what do you feel Qt is good for? As I understand it, its a cross platform GUI framework. Does it require a paid license? Is it easy to work with? Thanks
It is open source, does pretty much everything, very consistent api, and excellent documentation.
Licensing isn't mention, and in the repository I find a license directory with all kind of licenses in, anyone know under what this is then licensed?
Not just gui. Lots of collection classes, utility classes and the signals / slots mechanism is great. 
It'd be nice if in addition to listing best/average/worst performance and space, it also listed whether or not each sort is [stable](http://en.wikipedia.org/wiki/Category:Stable_sorts). That can be very important. *Edit: I'll go make this comment on that page too, in case the author didn't post this here.*
Excellent IDE, too. I find myself using it more and more over visual studio.
AFAIK your options are Visual C++, GCC and Clang. They are all good enough for learning C+.
LGPL for the most part. Examples mostly in public domain and bsd, iirc. http://techbase.kde.org/Policies/Licensing_Policy Also, the source files themselves should have a license header.
Nice, I thought its LGPL, but couldn't find a proof... And I don't want to look into the sourcefiles to know the license. So its more or less what Qt does, LGPL for the libraries, and BSD style for the examples.
I have heard this argument before, and frankly its bull. What it comes down to is some of what you do in C++ (such as: writing the code, application design, ect) is portable to all environments, and some of what you do is not (System calls, build systems, using different debuggers, and more) The same argument could easily be applied to using g++ and emacs vs clang and xcode That being said - the environment you use is just a set of tools to do what you really want to do, Write code. As a beginner I would suggest picking something where you can find help for online, and sticking with it until you feel like you understand why you are pushing the buttons and adding command line switches. Once you understand why its much easier to transfer to a different environment. 
You can use clang in Visual Studio now too (though it is only alpha support) One of the most exiting things going on with C++ now imo is how the different tools are working better together, and learning from each other. 
So does using [RCU](http://en.wikipedia.org/wiki/Read-copy-update) data structures or tagged pointers. I don't think the synchronization overhead imposed by the GC can ever be less than that of using RCU to prevent ABA. Also, [lockfree "pop-all" stacks](http://www.boost.org/doc/libs/1_53_0_beta1/doc/html/atomic/usage_examples.html#boost_atomic.usage_examples.mp_queue) don't suffer from ABA at all and can be quite easily used as queues. These exist in two variants: busy waiting enqueue, wait-free queue - or - wait-free enqueue, busy waiting dequeue. The latter will even amortize because of the multiple enqueues per dequeue.
You can play with the "current" implementation now: https://github.com/bloomberg/bsl/wiki#the-bde-standard-library
Do you mean projects created in VS2013 RC may not work in VS2013 release copy?
And you can easily use Qt Creator for non Qt C++ projects as well
No, I mean you're likely to run into bugs in any prerelease software. If you spend time "fixing" a bug that isn't even yours, you're wasting your time.
No I simply don't agree, And ide is just an interface aimed at dumbing down the tools you use. Tools that are well worth learning. Ask someone who has used Eclipse or VS all their lives to link together some objects with either the MS linker or the gnu linker. It doesn't matter they'll struggle. Ask someone who is familiar with ld to link with LINK and they shouldn't have any problems. Also your most important tool, you editor, is infinitely more portable when you aren't tied to an IDE. I personally like vim, but I don't care what others use, but speaking for vim, it's work for just about any language (you can add you're own with ease too) and works on any platform.
Pros: - Qt is what MFC should be/should have been. - It's more than just a GUI toolkit; it's more like an application toolkit. Comes with an entire STL-parallel cross-platform library to do pretty much everything you're likely to need -- networking, threading, internationalisation, xml, signals/slots, collections, streaming, opengl; as well as it's superb GUI facilities. - It's LGPL so you can use it however you wish - Is possibly the most pleasant library to write for you will ever use. - Documentation is excellent. It's drawbacks: - it's pretty hefty. You'll have to distribute quite large dynamic libraries with your application if you can't rely on them being available already. - It's also got it's own build system. It's not a bad build system, but you will probably not want to roll your own (although there is quite good `cmake` support for it). - It's got a pre-compiler stage. I understand why, and don't really blame them for needing it; but it can put you off if you like your C++ pure. Being fair though: it's highly non-invasive. 
Seems like a baby step towards immutable data structures. &gt; auto bar = concat(foo, x); &gt;then when I'm done, the value of bar is an NcArray whose elements are all of the elements in foo followed by a copy of x, and as a side effect, I must promise that I will never again use the value of foo. Don't we do something similar every time we modify a variable inside a loop? string s; for (int i = 0; i &lt; m; ++i) { s = words[i]; ... } Inside the first iteration of the loop, you can use s to access words[0], but after the second iteration starts, you can *never again* use s to access words[0]. Likewise, after the third iteration of the loop starts, you can *never again* count on s to contain the value words[1]. Instead of becoming unusable, the variable is intended to be reused, but for a different purpose every time. If you declare s inside the loop, the "never again" becomes explicit, because different iterations of the loop no longer see the same s. 
~~I don't think the author means that you can never use a variable after you modify it, I think he's trying to point out an unintended side effect: the original concat will add the second array to the first then return the first array.~~ ~~Now, sometimes you may want that but most of the time an API that does this would result in lots of user complaints.~~ On further read, I think you're correct; the author is pushing for immutable types, and is using an example of how not to do immutable types. 
I think he literally means never again. See his later example: auto bar = concat(foo, x); auto baz = concat(foo, y); The value of bar is unintentionally changed when foo is used a second time. I think the contract of the concat function is that something unspecified happens to foo when you call concat and there are no guarantees at all if foo is used again after that. I doubt an API author would document (and therefore commit to supporting) the behavior you get in the code above, for example. 
On further read of the author's previous articles I think /u/SublethalDose is correct -- the author is trying to push for immutable types and he's using an example that's not immutable. 
Could you compare and contrast the three categories? What distinguishes them?
Do you expect so much traffic that three distinct subreddits are needed?
every thread is by you. lol.
"I’m about to force a floating point number into an integer and all those decimal places (that are probably quite important) are going to be lost" Actually, I never say this when I'm using static_cast, instead I'm saying... "I’m about to force a floating point number into an integer and all those decimal places (that are *definitely not* important) are going to be lost" Stop trying to put words into my mouth, the only thing you achieve is a reduction in credibility.
&gt; Type casting in C++ is a form of what is known in computer science as type punning Type casting in C++ is *sometimes* punning. In particular, `static_cast` *isn't* punning (unless you're dealing with pointer types). It's just an implicitly-defined function from one type to another. It may be a partial function, but that doesn't mean it's an abuse of the type system. Likewise, it may throw information away, but that's perfectly valid for mathematical functions too. 
I didn't downvote, but... All float type conversions are available implicitly, but not at any point in an expression. If you mix integers and floats in an expression, the default casts will tend to be to floats. You may need explicit casts within the expression to choose integers instead. That said, you could break up the expression and store the key intermediate results in variables, thereby forcing the result to take the same type as the variable. Using `static_cast` for downcasts when you already know it will work (as opposed to `dynamic_cast` when you want the run-time check) makes sense. And I know what you mean about `void*`, though personally I don't see the point of the verbose C++ casts when you're dealing with C code anyway, especially as sometimes you need to cast constness too - even in templates, it's usually easier and just as safe to use C-style casts to/from `void*`. In any case, just because *you* use `static_cast` mostly that way doesn't mean that applies to everyone. Even Stroustrup publicly states that no-one knows how most people use C++, and that means there is no definitive "the use case". 
See /r/cpp_questions
Sorry, and thanks, moved there.
Moved to /r/cpp_questions, thanks for the advice tho.
&gt; There's much less room for personal taste vs. doing the right thing when it comes to static_cast Just because you're doing more interop with C code than e.g. using user-defined casts between class types doesn't mean everyone else is doing the same. Sometimes, what you're doing *does* affect which trivial and well-defined features you use and how you use them. 
Reinterpret cast reinterprets anything - as long as it's two items of the same size, you can bork it to your heart contents. Which is why you don't want it. As for the need to cast - I am with Raymond Chen of oldnewthing fame: [if you need to cast, you can't afford it](http://blogs.msdn.com/b/oldnewthing/archive/2009/10/23/9911891.aspx). Seriously... the older I get, the more I see a design error lurking behind any given cast - programmer borked up his types elsewhere. 
I just wanted to point out that physically typing 'static_cast' for integral conversions is wrong in my experience. "user-defined casts between class types" are implicit by default in C++. Wherever they're explicit (a good thing) you can use the construction syntax as well, since these 'casts' still create a destination type temporary.
I like N3766. They should also add SGI's `std::project1st`, `std::project2nd`, `std::select1st`, and `std::select2nd`. Meanwhile, you can use [my implementation](https://github.com/alnr/cpp/blob/master/alnr_functional.hpp) if you like.
It probably should be `select&lt;N&gt;` and `project&lt;N&gt;` instead of hardcoding the position in the name.
The blog post isn't very helpful, sorry. It implies that `reinterpret_cast` is the 'right' way to cast to a superclass. &gt; reinterpret_cast is used ... &gt; To convert a pointer-to-type into a pointer-to-different-type But this is quite incorrect. Maybe the author just didn't communicate very clearly. struct Base { int m_b; }; struct Derived : public Base { int m_d; virtual void f(){} }; int main() { Derived d; Derived * d_ptr = &amp;d; Base * b_ptr = d_ptr; // OK Base * b_ptr = (Base *) d_ptr; // OK Base * b_ptr = static_cast&lt;Base *&gt;(d_ptr); //OK } The reason these casts are OK in that `d_ptr-&gt;m_b` and `b_ptr-&gt;m_b` will refer to the same `int` - check their addresses. But a `reinterpret_cast` would be incorrect. Base * b_ptr_reinterpreted = reinterpret_cast&lt;Base *&gt;(d_ptr); // WRONG Using this pointer, for example accessing `b_ptr_reinterpreted-&gt;m_b` is undefined behaviour. It won't access the int you think it will access. `&amp;(d_ptr-&gt;m_b) != &amp;(b_ptr_reinterpreted-&gt;m_b)` The root cause is that, due to the that Derived is virtual and Base is not, the memory layout of Derived is not what you might think it is. In this case, the Base subobject is *not* really the first thing inside Derived.
I believe the topics are distinctly different enough to warrant separate subreddits; traffic is not the only concern when categorizing such things :)
Numeric conversions *are* available, but just because I can, doesn't mean I should. But *sometimes* I should, and when that happens, sticking a cast does two things: * tells the compiler to trust me and stop whining * diverts my colleagues' and future me attention that something not quite kosher is going on
Agree. Personally I hope that most of the nice libraries get adopted to boost (if not already present), so that you don't depend so much on your compiler version.
&gt; You just shouldn't use static_cast&lt;int&gt; and think it's any better than an implicit cast. It is better because it stands out like a sore thumb, and that is a quality when something **isn't** kosher. (I am coming from a perspective that any cast isn't). GCC doesn't warn about "possible loss of data"? Didn't realize that before.
We did not discuss string_view in LWG this week. I think it may be moved to a fundamental s TS for C++14. It may come up to us in Issaquah in February.
I have long long long thought that C++ could have an interesting place in web development. A simple thought that keeps occurring to me is that while C++ can be more complex capacity gains are potentially worth it. That is that if you compare a C++ system with say a PHP (fairly fast) system that you might end up with the PHP requiring multiple servers sooner than with the C++ system. Also deploying 3-5 servers is so much easier than deploying 20 servers. So while the coding is harder the server(switches etc) architecture is simpler. But the other benefit is in overall architecture possibilities. C++ will allow you to do things that you might not attempt with traditional web programming languages. You might have huge memory structures that would allow you to do things that would simply be too hard and too slow in other languages. A simple example would be if you have 10,000 logged in users that you might have things that are computationally too hard to do in real time but with C++ you would go ahead and do it. 
It kind of had it in the begging, at least in the Windows world. First with COM components in ASP pages, later with ATL based IIS modules. But it was done in C++98 with lots of C style code as well, ugly Windows macros and COM boilerplate. Eventually Microsoft moved their focus to .NET and it faded away. The way it is so easy to write unsafe code in C++, given its C roots also does not help. Funny enough nowadays, with C++11/14, one could try again to provide such frameworks, in modern safer C++. Not sure if many will use it though, given the actual language landscape and how developers tend to be unaware of safe C++ idioms.
Good work! My only remark, please don't use C style strings in the examples. 
Thanks! The output format string is passed to the C API of Mongoose anyway, that's why I kept it as char*
Shoo.
Why are you using C-style casts?
&gt; you might end up with the PHP requiring multiple servers sooner than with the C++ system. Or you make a [PHP-to-C++ translator.](http://en.wikipedia.org/wiki/HipHop_for_PHP) 
Why do you do the following in the [example](https://github.com/catnapgames/TestMongooseCppSite/blob/master/TestMongooseCppSite/main.cpp): `struct mg_callbacks callbacks;` `memset(&amp;callbacks, 0, sizeof(callbacks));` ? Wouldn't default initialization be better (if anything, simply due to less typing; it's a POD anyway, but you don't appear to be making any use of the alignment, etc.): http://stackoverflow.com/questions/1998752/which-one-to-use-memset-or-value-initialization-to-zero-out-a-struct The following also feels somewhat out of place in a C++ code (esp. while making use of C++11 features--it's not as if C compatibility is possible in such a case anymore): ` int request( struct mg_connection *conn, const struct mg_request_info *request_info )` Why not simply use type names themselves (without the unnecessary C-style redundancy `struct`) and references (or at least [const-pointers-to-const](http://www.parashift.com/c++-faq/const-ptr-vs-ptr-const.html)) for [const-correctness](http://www.parashift.com/c++-faq/const-correctness.html)?
In the most basic sense, you just need a table of regular expressions that turn into tokens. A shitty algorithm that should work would be to go through your table checking each regular expression against the current input and take the one that matches the most characters. That should get you on the right track. 
I highly recommend going through the LLVM "Kaleidoscope: Implementing a Language with LLVM" tutorial. Even if you don't want to use LLVM in the end, this gives you a broader knowledge of how things interact with each other. http://llvm.org/docs/tutorial/
Doing this from scratch, you'll need a state machine that uses several deterministic finite automata (generated from your grammar). There's a great course on it at Coursera (https://www.coursera.org/course/compilers). They go through the theory behind the front end of a compiler, but eventually they'll start using GNU tools for the lexer generator. You'll have to use what you learn to write this yourself.
Doing this from scratch, you'll need a state machine that uses several deterministic finite automata (generated from your grammer). There's a great course on it at Coursera (https://www.coursera.org/course/compilers). FTFY
You don't need a lexer *generator*, like lex, you just need a *lexer*. You can easily hand-code a lexer, which is honestly what most production compilers and interpreters do. I've written a few. Here's [one in Java](https://github.com/munificent/bulfinch/blob/master/src/com/stuffwithstuff/bulfinch/Lexer.java). And [one in C++](https://github.com/munificent/magpie/blob/master/src/Syntax/Lexer.cpp). As you can see, they're pretty straightforward.
Would it work like an iterator? I.e. become a dangling reference if the string is destroyed, possibly also when it is modified? Or is it like std::weak_ptr and invalidates itself safely when the string goes away? 
I didn't say it was a good solution, those were all taken when I made this suggestion. 
More details: The landing page is here https://bitbucket.org/brechtkets/graphicscpp Discussion: http://groups.google.com/a/isocpp.org/group/graphics/
I actually welcome this because it seems that the only way of really doing portable graphics at the moment is to use bloated library like Qt which I begrudge doing. However I think we really need decent UTF support before we do this.
Text rendering is extremely hard to do properly. I wonder if it's possible to do it without some kind of bloated library (even if said library becomes standard). 
And here I thought OpenGL already did 2D graphics pretty well, among other things ;) EDIT: I should make it clear that I'm not being entirely serious here. Bringing up an OpenGL context depends on so many other things, and actually drawing anything is far from straightforward.
Sure, I've done it. It's possible. You should use a bloated library.
I think this is for kids and noobs to learn programming with. "Draw a blue circle with radius 4 at (50, 60), draw a green line from (20, 20) to (30, 30)," draw a snowman, draw a stick picture of the teacher, add crude genitalia and get sent to the principal's office, that kind of thing. 
The lexer generator is what I need, I want to create one. I know how to make a static lexer.
Well for my purpose that won't work, but thanks for your reply.
&gt; However I seriously think it's far more important right now that UTF is properly supported in the standard than having a 2D graphics library. Personally, I think [modules](http://isocpp.org/blog/2012/11/modules-update-on-work-in-progress-doug-gregor) are 10 years overdue, so let's haggle :) Regarding unicode rendering, the graphics library could make the unicode rendering "implementation defined", and it could fall back to platform standards (I'm not sure if there are toasters out there which have a high resolution display but no unicode support, but mandating unicode support for a particular library seems against the flow.) Regarding unicode manipulation, well, yeah, kinda. On one hand, I'd love the C++ universe to standardize on UTF-8. OTOH, that's exactly what C++ can't do, so it will be a massive library. --- A well-designed 2D graphics library would be a top hit in education. If we could combine the appeal and ... tang^\* of simple graphics with teaching modern C++ concepts, we'd make it much easier finding and raising next gen devs. Something that IMO beats every other library long term. --- Likely it's a false dichotomy anyway - it's not necessarily the same persons working on these things. ^\*&amp;nbsp;that's&amp;nbsp;the&amp;nbsp;noun&amp;nbsp;of&amp;nbsp;tangible,&amp;nbsp;right?&amp;nbsp;Right?
False Dichotomy: Presenting two alternative states as the only possibilities, when in fact more possibilities exist. ^^Created ^^at ^^/r/RequestABot ^^If ^^you ^^dont ^^like ^^me, ^^simply ^^reply ^^leave ^^me ^^alone ^^fallacybot ^^, ^^youll ^^never ^^see ^^me ^^again
I am not talking about unicode for the 2D graph lib, I am talking about about having UFT8 supporting the library as a whole. It *IS* embarrassing that our language can't in a standard and platform independent way handle the encoding that is most wildly used/useful in a multi language world. Modules are great, they'll make development much easier, but let work getting the basics down before we work on "comfort features". Better yet, do both. Also from a education point of view, I really would think 2d graphics are all that important, I think it's far more important to teach kids the importance of a good command line programs, so they can think of programs in terms of input and output, the figure out how to compose programs out of multiple programs tied together, i.e. scripting.
Minor nitpick: I wouldn't call this "Microsoft" effort; while Herb now works for MS, this activity is certainly in his function as ISO C++ big hat. 
Being sent to the principal's office seems to be a vital part of an early CS education. Nice to see more options for that. 
That comment was about the C API (which is not exactly the native API for LevelDB), and even so is a bit of a head scratcher. If you use the API properly there should be no problem with multiple concurrent reads.
&gt; get sent to the principal's office, Because of the genitalia, or because the job was crude?
&gt; tang I think you're looking for "tangibility".
Sounds a lot better than *tangibleness* - but now I've grown enamored with *tang*. 
If you want to "kindle imagination" then teach them python and have them up and running in a day. I still consider unicode to be far more important that modules. modules will make development easier and that's great. But unicode support will make the programs themselves better.
&gt; I think this is for kids and noobs to learn programming with. "Draw a blue circle with radius 4 at (50, 60), draw a green line from (20, 20) to (30, 30)," draw a snowman, draw a stick picture of the teacher, add crude genitalia and get sent to the principal's office, that kind of thing. If it isn't for a C++ developer, why is it being pushed into the standard?