He did a video on it a while back: C++ Weekly - Ep 7 Stop Using std::endl https://www.youtube.com/watch?v=GMqQOEZYVJQ
So performance issues? That's not the point of the flush, if you care about performance issues you're probably in an enterprise environment and probably have a more advanced logging system than simply printing to stdout. The issue with having unflushed data is that it gets *incredibly* confusing if your program crashes and you're missing your stdout logs. If I write a mock program, the first thing I do isn't not hook up logs, I typically use std::out for logging until the program actually serves the proof of concept. The issue is your program could crash at the exact same line and each time it crashes the logs you see will be different each time because a different amount of stdout has been flushed. For beginners, this will be incredibly frustrating, you cause some issue but you can't figure out why because printf/std::cout doesn't do what you think it does, and you've been told std::endl is evil even though it would solve your problems.
I was excited to give this a try for the application I maintain at work until I saw it was LGPL. Header inclusion is probably the biggest legal grey area when using an LGPL library and Verdigris is _header only_... no way will this pass my company's open source policies.
Doesn't Eigen specifically warn you about that? I barely used it and I think it was featured pretty prevelantly in their docs.
Blog post: https://woboq.com/blog/verdigris-qt-without-moc.html
I didn't see anyone mention that when you #include "file.*", all that the compiler really does is replaces the #include line with the contents of the file that you are including. Note that i called the file "file.*", and not "file.h". This is because you can #include anything you want... The compiler doesn't care. All it does is copy the contents of that file. If you don't believe me, try changing the file extension of your header file and you'll see that it still works. If you only declare your classes and functions in the included files, then all you're basically doing, when you #include that file, is forward declaring those classes and functions within the source file that uses those classes or functions. On the other hand, if you define your classes and functions in the header file and #include it multiple times, then you are redefining your class and functions every time... and redefining class and functions is a very bad idea.
Fork of Qt 4? Doesn't sound too enticing, considering the current Qt release version is 5.10.1.
No, that's copperspice. Verdigris integrates (drop-in) with Qt 5.
Right. I got confused by the Introduction chapter, which started describing something that was a Qt4 fork.
I have to say... Verdigris is awesome! I always hated running moc, and the macro syntax is surprisingly nice. 
Wow, you are right. I looked that up again. That wasn't my understanding of how modules were going to be done when it was first proposed. I was under the impression that it was going to be like java (or all header files in C++). And the interface (header) part would be extracted out by the compiler.
AnkhSVN. It's somewhat flaky (i.e., doesn't show all changed files even when you click on refresh).
git, used outside/separately from visual studio.
Updating an atomic invalidates the whole cacheline, which is bad for performance. Reading is usually fine, but one writer with many readers will usually lower performance of every reader by a lot, [which can make even a reader writer lock much slower than a mutex](https://m.youtube.com/watch?v=GzLsHTgZ-XI). Synchronization is always bad for performance, atomics are not an exception.
I've found myself liking staging/committing locally from within Visual Studio since I'm already right there, and then still going to command line for everything else.
At work we use TortoiseSVN for the old codebase and TFS for newer projects. At home i only use git
Or better yet..... just use cmake ;D
But beginners don't know about RAII and won't know how to set up such an object. If you're making a video on teaching beginners I would suggest to them to use endl instead of "\n" for the reasons I listed above. For most simple programs, logging using stdout, it's a better idea to use endl.
There is enough junk code out there that new programmers emulate the style of. It matters once it is linked on cpp reddit.
Exactly this, for me as well.
So, C++ is going for runtime cost free compile-time reflection that will *permit* you to write a runtime reflection library for a (set of) types. In effect, it will let C++ programmers and library writers to write what Python the language writes. In one sense all Python code is codegen, as it the interpreter translates your Python code at runtime into code to be run. There is more than one *kind* of thing you could call code generation. Templates could be called code generation, but it isn't code generation in the same sense as printing into a string and calling `eval`. Generics is another kind of code generation. C# reification yet another, or even manually outputting bytecode and loading it dynamically. In a sense, C++ templates are no more code gen than a for loop or list comprehension in Python is code gen, or fiddling with the class pointer is. No new tokens are created by C++ templates: they are simply function and class factories built into the language. 
Yes, that's the entire point.
SVN? That's a blast from the past! I used the standalone client then (TortoiseSVN). Tried ankh, but found it poor, dropped it (that might have changed sunce). Nowadays you're better off with git or TFS/TFS Online.
"Separate interface from implementation" is a logical statement; it does not imply a specific physical layout. It means your interface is supposed to be sufficiently abstract that it will continue to function even if you decide to completely change the underlying datastructures and algorithms. Where you actually put that implementation in the code is irrelevant, as long as it is clear what is part of the interface and what is not. It could certainly reside in the same header, but separated in a 'detail' namespace, for example. 
Regarding the `tuple` issue, you may be interested in how tuples are implemented in libc++ (which comes with Clang). The base idea is NOT to use recursion, but instead use *numbered leaf nodes*. In essence: template &lt;std::size_t I, typename T&gt; struct leaf_node { T mValue; } template &lt;std::size_t... Is, typename... Ts&gt; struct tuple_impl&lt;std::integer_sequence&lt;std::size_t, Is...&gt;, Ts...&gt;: tuple_leaf&lt;Is, Ts&gt;... {}; template &lt;typename... Ts&gt; struct tuple: tuple_impl&lt;std::make_integer_sequence&lt;sizeof...(Ts)&gt;::type, Ts...&gt; {}; This in turns leads to a *fixed* template instantiation depth. *Note: there are further tricks, such as leveraging EBO for zero-sized Ts, that I won't expand on here; I invite you to read Howard Hinnant's code, it's really cool.*
&gt; if you care about performance issues you're probably in an enterprise environment If you're using C++ then you care about performance. If you don't, then why are you using C++? There are better languages for your task. It took me a while to accept this viewpoint (I found it on Scott Meyers's blog I believe), but it makes a lot of sense if you think about it. Given that, `std::endl` is poison for your code if you're using it by default, especially if you're writing to any kind of flash memory; flushing a single string's worth of data causes the system to rewrite an entire block of memory. This is not fast at all and will cripple both enterprise and embedded code. If you're that concerned about logging crash data you should be using `std::cerr` instead of `std::cout`; `std::cerr` flushes by default, not to mention it better captures your intent. 
&gt; "HTML 5 in native WebView + Qt C++" Had a question regarding this. If you decide to take this approach, would you use [QtWebkit Bridge](http://doc.qt.io/archives/qt-4.8/qtwebkit-bridge.html) for the interaction of HTML and c++ ?
&gt; spending some cycles twiddling atomic counters is no problem Don't forget about the cache misses you'll be encountering too since the underlying object and the reference count may not be stored near each other (unless `make_shared` was used to create it), as well as doubling the memory usage of each pointer (vs `unique_ptr`).
&gt; unless make_shared was used to create it We only allow `make_shared`. `new` is forbidden in conjunction with smart pointers. &gt; as well as doubling the memory usage of each pointer (vs unique_ptr). How many pointers do you have that the size of them matters? Yeah, if you're using `shared_ptr` on an embedded platform where you're scrimping for every byte, fair point. But the stuff we're pointing to vastly dwarfs the size of the pointers themselves.
If you are doing any non trivial amount of printing, the overhead of endl is *huge*. Adding a flush or two after you crash isn't going to be hard. Dealing with an abysmally slow program is going to make everything else harder. Effort is fungible. Not using endl leaves your program *correct*; only when you mess up elsewhere does it cause a problem of incomplete output. A fix consists of adding some temporary flushes, fixing the crash, and debugging. Using endl everywhere makes your program *slow*, and there is no fix short of rewriting the code. Do not premature optimize, but don't premature pessimize either. 
Marking all your code as `inline` frees up the compiler to bloat up your final executable, as called out in the remainder in that paragraph. Some implementations may sometime treat `inline` definitions as weak symbols and merge them at link time, but as there's no such thing as a "weak symbol" in the standard that's totally a QoI issue. Even if they do, you're just back to compilation efficiency problems, only now you've also made the linker do even more work.
If I had used it for more than debugging (needed eigenvalues/eigenvectors, and we didn't have that capability in our home-grown matrix classes), I'd have wrapped what we needed out of it (or tried to). Its types are so ugly/non-intuitive (to me) that I could easily have made that mistake.
Perforce with niftyp4. If you set it up to use environment settings, set up a p4config file, and drop one in the root of your workspace, combined with a good p4ignore file, you basically never have to worry about it until you go to submit. 
This looks great! The rationale is solid and something I'm glad is being pursued. I look forward to using this. 
How does this compare to [Boost.GIL](http://www.boost.org/libs/gil/), aside from the I/O (which GIL notably lacks OOTB)?
I have not done any proper comparisons, but my best guess is that Selene has a somewhat simpler design, I/O in the box, is implemented using C++14, and is probably lacking a few features that GIL has at the moment. That said, it may grow with a slightly different focus (more I/O formats, more algorithms for computer vision pre-processing applications, such as geometric transformations), and is intended to be developed further as my time allows or collaborators join. As far as I can see, the Boost.GIL codebase (or at least the documentation) has not been touched since 2007, for better or for worse.
No, the mutex was just faster than multiple atomics (in this specific case).
I'm not sure I understand. When I say that we only allow `make_shared`, I certainly don't mean that we disallow `weak_ptr::lock()`. I mean that we do not permit `shared_ptr(new Thing)` and equivalents.
What if someone calls `IncrementCount(-9001)` in their code?
`make_shared` allocates the control block and the object together for cache efficiency, which is typically a win, but if `weak_ptr`s typically outlive your `shared_ptr`s and your object size is large then it almost certainly isn't.
!removehelp
&gt; If you were put in charge of a greenfield project to build new design tooling for a game engine, what would you do instead of the usual embedded Lua approach? Unfortunately the best answer I have there is going to be a boring "it depends." :) Remember, "make games, not engines." An ideal choice for a shooter is not necessarily the ideal choice for an MMO nor for a story platformer nor for an open world RPG. :) There's then also team composition to consider; I know my team right now would *love* C# (both designers and engineers) but that might be a poor choice if working with a team that prefers more data-driven scripting. All that said, the answer would most probably be either C#, JavaScript, or a visual scripting system, depending on specific needs and use cases.
The Youtube description says "Patrick Stewart presents an Academy Award of Merit to Mark Elendt and Side Effects Software" So "Mark Elendt" ?
I've seen people replace atomics with non atomic ints, because "atomics are slow". Now their code has a bug, but is faster. So it is true that atomics are slower than non-atomics. Just buggy. Mutexes are also 100x slower than non-atomics (since they are built from atomics). A mutex, in some cases, can be faster than atomics, particularly if it is one mutex vs multiple atomics, and depending on the shape of the contention. Also, a mutex is (relatively) fast, because it tends to be a single atomic operation - what is slow is contention. ie waiting is slow. A free mutex is pretty fast. Atomics are rarely worth the trouble. The above may be obvious to some, but doesn't seem to be in practice.
"It depends" answers are always appreciated when they enumerate a set of possibilities and reasons as you did :) Thanks!
Code generation is how you get efficient net message serialization in Unity's net transport utilities. It rewrites the assembly in a post-compile step to convert reflection based serialization functions to direct read and write calls of the reflected fields. This is objectively better than leaving the slow path in.
Why would they be? Part of the reason generics exist is to break dependencies – using the tools available isn't ludicrous.
Awesome! It looks like something I've been wanting to write for a while now. Look forward to trying it out. OpenCV's type design is pretty lacking. The interpolated pixel accessor is really cool, especially with the border access policy. Looking at the examples, does `sln::convert_image&lt;&gt;` need the first template parameter specified? Can't it be inferred from the argument? Actually, I think if you switch the order of the template arguments from `&lt;src, dst&gt;` to `&lt;dst, src&gt;` you can set it up so that a `convert_image`call only needs the destination type specified. template &lt;typename Dst, typename Src&gt; Dst foo(const Src&amp; x) { return static_cast&lt;Dst&gt;(x); } int main() { double x = 1.23; return foo&lt;int&gt;(x); } 
I'm one of those new programmers. Hence the question.
Sounds like a great project! As a side question. Could it technically work with Qt 4.8.6 and a C++14/17 compatible compiler? Or are there major changes in Qt between 4 and 5 that make it impossible to get it working with Qt4?
Why would this be used instead of stb image libraries? They are C, but I can't remember anything lacking due to C except for possibly very simple wrappers for memory management.
Yep, makes sense. I remember contemplating that but not going for it at the time, probably because of consistency with the other functions that take both `src` and `dst` images. For these, you also have to specify both "semantic" pixel formats (since they're logically decoupled from the Pixel[Src|Dst] types), and it would be odd to switch the template arguments there. Will reconsider this - thanks for the tip!
This post is number one of all time? What the fuck is this subreddit?
I don't think stb are really type safe. And from a modern C++ perspective, they're horrible old-style C code. Pointers everywhere. Not neat, clean, easy-to-use (and hard-to-misuse) RAII types.
Why header only? Could've been made to allow you to choose between header only and static library.
Looks like it uses lib{png,jpeg}: https://github.com/kmhofmann/selene/blob/6f85adac5a8dee03dde3f3e48a0bc2563b7f205c/src/CMakeLists.txt#L7-L32
Ah, I see, I had a mental model mismatch. Images cover up to the "pixel abstraction" but the "component abstraction" (RGB, BGR, etc.) is separate and external to the image? So you'll never have an `Image&lt;RGB&gt;`, only an `Image&lt;Pixel_8u3&gt;`? In your design did you ever encounter the idea of have pixel format be a part of the image type, and if so what pushed you away from it?
Doesn't seem like such a bad thing. If the project had its own implementations I would treat it with great suspicion. Also, I wouldn't consider it *just* a wrapper since the real "value added" is type safety, not image I/O.
&gt; I've seen people replace atomics with non atomic ints, because "atomics are slow". Now their code has a bug, but is faster. So it is true that atomics are slower than non-atomics. Just buggy. I'm not sure why an anecdote about a nonsense mistake from someone making a change they didn't understand is relevant either. Atomics are there for synchronization - none of what you have talked about has anything to do with performing any sort of synchronization faster. 
It will work just fine with Qt4, as long as you change the binary format of metadata to match Qt4. 
I am not a Qt expert, but QtWebkit (or the successor QtWebEngine) does not support mobile platform afaik.
Yea definitely! Totally agree with you.
I like the quick access to blame, but I do everything except that on the command line.
Git. Once you're used to your working dir being a real repo, you can't go back. DVCS is a game changer, and git pushed out everything else in the space. 
There's no way to teach C++ basics in an "article". Even a book may be a bit on the short side: it's a big language, and even the basics can be tricky to sufficiently get across. The data structure can be explained in a reasonably short reddit comment. No pics needed, although good ones would help.
Can’t wait for meta classes to replace moc.
You can use QT with a "designer window" or without. Or only using it a little bit to design small modules which you can re-use directly from code. I'd suggest not making assumptions about how the most popular option for what you want to do works. Instead, try it and try to see why it's the most popular.
Unless you have a specific job which wants certifications, I'd say you're probably wasting your time and money.
Wow, here I though VS was the mother of the spoon-feeding IDEs.... You definitely don't have to use Qt Designer if you don't want to. As far as Qt is concerned you can program in notepad. That said, designing complex UIs by hand, in code, is the recipe for ugly and strange stuff. But who am I to stop you.
They end up being one or two function calls, which is my whole point. It isn't necessary to be a modern C++ zealot for such a simple API. I'm normally first in line to champion ownership semantics and type safety, but it just doesn't matter much here. You pass in a buffer of bytes. It isn't a string class that you'll use over and over or a GUI framework full of little components, there isn't much room for error as it is.
Yeah, the article mainly outlines a technique for reflecting upon the class and chaining initialization code. The "runtime reflection" that you're stuck on is just an example of something you can do using the introspection technique. It's not the focus of the article, I'm very hand-wavy about it. &gt; Not sure if by this you mean, calling a member function of the class, or doing something similar to your getter and setter stuff. The former, member functions are more complicated to reflect over, but it is possible in principle. But as to the latter, you can do it very easily without using compile time counters or daisy chaining (lazy initialization is handy but still optional. I don't really understand how this is possible. You're expanding a macro inside a struct declaration- how are you going to populate a hashmap without initializing each struct out of line manually? That's what I'm trying to avoid-- scattering initialization code between declaration and implementation. I want it to be in one place-- the header file, the struct declaration.
C++ is complex for many reasons, much of it doesn't have anything to do with the complexity of the problems it solves. Even if one takes your argument at face value, it doesn't say that that is the ONLY reason it's complex.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7xeypy/taking_the_cpa_exam/du806o9/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
moc works fine for me, but I am impressed by the necessary degree of insight about Qt internals, to achieve this! (and I have 10 years of Qt behind me).
If you're working in teams and whatnot, you're not sharing "your" source. Well, you can, but I would argue it's wrong: Rather, your source is living in the source code repository and I refer to it. Ideally, I only refer to your built artifacts (`*.a, *.so, *.lib, *.dll) should you have them, so that I don't need to build them myself either (why would I? Build server is made for that).
to be fair, I think that Olivier Goffart (the author) is the main maintainer of moc itself, so he certainly is knowledgeable of his stuff.
Yeah its Mark Elendt. (Hi Bryce &amp; Tony, love your talks :) ).
Thanks! - Since I don't use Visual Studio for most of my development, I had simply not heard of them yet. What would be the impact of adding them on the codebase? If it is relatively minor, I'd be happy to take a pull request. :) - It's wrapping libjpeg/libpng (libtiff support planned, but that API is crazy). These libraries are stable and pretty battle-tested, and reimplementing reading &amp; writing complex image formats from scratch is not really on my mind. Having clean wrappers is already a big win, IMO. I support probably most of the respective image data reading/writing APIs, but excluded things like handling optional tags (such as EXIF), palette based images, etc. - I have to admit I don't really buy into the header-only craze, which seems like a poor substitute to good package management to me. I'd rather make it as easy as possible to build and use my library in all kinds of scenarios. Using CMake seems like a good start; I also envision further package manager support. Also note, for example, that libjpeg/libpng includes happen only in the respective translation units, and not in my headers. This is by design!
I'd stick to python. It will give you a feeling of success much quicker and keep you motivated. It also sounds like you don't really need the performance advantages c++ potentially offers (I say potentially, because it is just as easy, to write slow programs in c++ as in Python). That being said. There is rarely a reason to write sort, trees and lists by hand in c++ other than to learn how such algorithms / data structures work. Regarding the build system: it is just as feasible to manually compile your program now as it was 15 years ago which is to say: I don't know why you'd want to do it for anything but toy examples. You don't have to learn cmake though (the basics of which shouldn't take you more than an hour at most) just use an IDE like qtcreator or one of the alternative build systems. I don't have experience with the libraries you mentioned, so I can't help you there. I'd recommend to just look at some example code and compare it.
So, without getting into which macro generates it, let's say that your macros end up creating a struct like this: struct Foo { int x; double y; auto reflect_tuple() { return std::make_tuple( std::make_pair("x", std::ref(x)), std::make_pair("y", std::ref(y)) ) } }; Hopefully you agree that this is relatively straightforward to write a variadic macro for. Now, let's be clear: reflect_tuple itself already has enough information that you can implement your set-by-name function. You just linear search according to the string in the tuple, and when when the first element in the pair matches, you set to the second member. So, the "runtime information" you are referring to is really just a form of caching to a) make the member lookup more efficient, and b) handle the type erasure for you at that point instead of dealing with it later. There's lots of ways to deal with caching, but a simple way is a static local member: template &lt;class T&gt; unordered_map&lt;string, std::function&lt;void(T&amp;, std::any)&gt; make_setter_map(); template &lt;class T&gt; void set(T&amp; object, string name, std::any value) { static auto setter_map = make_setter_map&lt;T&gt;(); setter_map.get(name)(object, std::move(value)); } Hopefully that makes sense.
&gt; If I stick with C++ how necessary will it be to learn CMake in order to build my projects? Even in college for bigger projects, I just compiled everything at the command line. Is this no longer very feasible? This was never feasible if you had more than one or two files. At the very least learn the basics of GNU Make. *** &gt; Is there a reasonable equivalent for data science in C++ as python? I have a few little data sets to analyze. I've seen a few posts about possible alternatives Will these be much more complicated to learn than numpy and scipy? Would I be shooting myself in the foot in other words trying to do this in C++? If you're just getting started with something, I would suggest just picking up whatever library/framework/tool has the most tutorials/documentation available. The language is unlikely to be a barrier to you learning the topic. You can decide for yourself later which ecosystem you like more. *** &gt; Also looking into a small webapp at some point. Is Wt a reasonable alternative to use instead of say python/django? AFAICT, Wt has very few resources available online to help you compared to the popular choices. I would recommend you pick something like Python/Django, Ruby on Rails, Node.js, or Go etc. People should have a good reason if using Wt since they will forego tutorials/documents. And when you're starting off, that's the most important thing. *** &gt; How much of these libraries have been updated to use the new "modern" C++ style with auto pointers and whatnot, or are most of these projects still using the older style and you end up with a semi-balkanized project with old and new pointers (excuse the pun) being used side-by-side? Have a peek into the source. The README often lists the "target" standard. C++11 is nice, C++14 is great. Otherwise, look at the source cpp files and/or example files and look for things like `auto`, lambdas, etc. *** Good luck and happy learning!
If by "auto pointers" you mean `std::auto_ptr`, I have some bad news... It's been removed from C++ and replaced (by the similar `std::unique_ptr`). But basically a lot of C++ has changed since you originally learned, and a lot hasn't: * Regular pointers, std::string, main(), std:: vector (and the other containers), all still exist. * Containers like std::vector got an upgrade. You can now write `std::vector&lt;int&gt; my_int_vector = {1,2,3,4};` to create a vector containing 1,2,3,4. * `nullptr` is now preferred over `NULL` * Smart pointers have largely taken over from new/delete - `std::unique_ptr` for objects with a single owner and `std::shared_ptr` for shared objects. To free the memory simply assign `nullptr` or allow the unique/shared ptr to itself be destroyed (local variable at the end of the function, member variable when the containing class is destroyed). * `std::move` is now a thing - it allows efficient transfer of memory owned by one object to another. It's also invoked automatically on return from functions. Some types can only be moved (e.g. `std::unique_ptr). * C++ finally gained "for each" loops in the form of ranged for. This largely avoids the pain of using iterators: `for (int&amp; x : my_int_vector)` * You can now specify default values for member variables directly, no need to write a constructor. * Lambdas - a slightly more advanced feature, lambdas greatly simplify passing custom predicates/comparators to std:: algorithms There's much more, but it gets more and more complex. Much of the above simplifies things.
Thanks for this advice. Yeah maximal performance is probably the least of my problems at this point. It certainly has been easy just typing `pip install &lt;my package&gt;" to get something to work and run it with "python myfile". (I was mostly kidding about the re-writing data structures. I definitely don't have time or energy to go back into that. I'm glad to see they're all part of the STL :) Maybe my memory is foggy and in retrospect I was just using the IDE's build for bigger projects that was making a script automatically. I can't remember the IDE I was using then though (maybe a student edition of Visual Studio 5? I imagine it would compile all the project files together and link them) Sometimes my ambition goes over my capacity. I've been trying to use Linux again too and thought "maybe if I learn C++ I can also figure out system calls and read low-level hw input and figure out how to get my laptop's trackpad working correctly...but I think that's as likely to happen as me reupholstering my couch. 
Interesting. I think my own preconceived notions are preventing me from grokking. So `Image` doesn't know colorspace semantics and `ImageData` *can optionally* hold colorspace semantics? Is there a compositional relationship between `Image` and `ImageData`, is one upstream or downstream of the other, or are they simply alternative ways of representing "images"? Poking around the Doxygen I can see a way of creating an `ImageData` from an `Image` but not the other way around. Tangential note, can you configure your Doxygen to produce namespace-centric navigation? I see the navigation bar provides class-centric navigation but it doesn't help with discovering what free functions you have (like [`to_image_data()`](https://michael-hofmann.info/selene/ImageToImageData_8hpp.html#a1f556c6333171655a839ad60b6f8b4bb)).
You can convert from one to the other, and vice versa (`ImageDataToImage.hpp` and `ImageToImageData.hpp`). - `ImageData` is dynamically typed (similar to OpenCV's cv::Mat) and becomes useful e.g. when reading image data from disk. Since you don't know what formats to expect, you need a container to be able to hold everything. - When actually working with image data, however, you in pretty much all cases want/need to know what data you're holding. That's why the `Image` class is statically typed and should be used for all kinds of actual image processing tasks. - At the moment, ensuring proper conversion is up to you. I can envision (and will likely add) convenience functions that can directly read into an `Image` class, even if the representation is different, and automatically convert if necessary. - You could indeed envision "more strongly typing" the `Image` class on the semantic colorspace type, but as of now the type is storage-centric. Hope that clarifies things a little. Regarding Doxygen, there's only one namespace (`sln`) in the library, so I thought that a namespace-centric view might not be that useful. I acknowledge that Doxygen is actually a pretty terrible way to read API documentation. Do you know any better alternatives, by any chance? (I considered *standardese* to be not production ready yet...)
&gt;If by "auto pointers" you mean std::auto_ptr, I have some bad news... Haha! Looks like I missed a whole generation of pointers. Now I really feel old. Thanks for the summary. I was doing a c++/unreal online class with the kids and it started getting over their heads way too quickly. But some of it was all the macros being used, some were the pointers, and some of it was the build system being used. I hear some of C++17 makes things even simpler (but I imagine implementing these features will be slower on these larger projects with large code bases already) Python they seem to be picking up a bit easier. 
Yeah I think I was leaning in that direction as it was. Seems like you can do a lot in python. Probably much more than I'll ever get to before I start running up against a wall. 
It's about choose the right tool for the job. If you don't need insane performance, which you probably don't, go with Python. C++ is like cutting yourself with a razor blade. It's surprisingly addictive. Python, on the other hand, is like working on a motor home. You could probably install a swimming pool, though I'm not sure how.
I would definitely go for an IDE for build management. No need to mess around with this stuff unless you need to. Just remember to learn how to setup your IDE for the language standard and get yourself comfortable in its workflow. About the Linux thing, I think you'd quickly find the need for C as well :)
Don't get me wrong, I like programming in c++ - not only when performance is important - and it is where I'm most productive in many cases, but that's because I have a fair amount of experience with it. I think, if your primary goal is to write a program that does X and you don't know any language particularly well, python will get you there faster. If you are primarily interested in learning a programming language as a hobby sure, why not go for c++.
Why? Are jpeg and png complicated to implement? 
vs2017 express (free for commercial application): http://aka.ms/vs/15/release/vs_WDExpress.exe
&gt; He also mentions in it running a dataset in python taking 3 or 4 days and his code on C++ could run it in 10 minutes. This is most definitely due to right vs wrong data structures and their implementations. Bjarne knows how to approach a computing problem and how to write an efficient implementation, some other person may not and C++ won't help, especially as it is harder to get things right in C++ vs Python. If you want to get better at solving computing problems (and at programming interviews), your first step should be studying data structures, algorithms design etc, not a specific language. "Algorithm Design Manual" by Steven Skiena is a good starting point. &gt; If I stick with C++ how necessary will it be to learn CMake in order to build my projects? Depends on you project, Chromium is a huge one which doesn't use cmake. C++ language knowledge is orthogonal to build tools knowledge. Python is worth learning anyway because it is a good scripting language and it is often used for building and integration.
Thanks for the clarification, that makes sense to me. I actually don't mind Doxygen, but I don't know of any easier way to browse free functions other than the namespace-centric view, unless one was very careful to reference the free functions all over the class documentation. I've explored [Sphinx](https://breathe.readthedocs.io/en/latest/) and [breathe](https://breathe.readthedocs.io/en/latest/), but never liked how they turned out. [Standardese](https://github.com/foonathan/standardese) I haven't explored yet. There's also the [cppreference](https://github.com/p12tic/cppreference-doc) documentation. Another orthogonal thing to consider is creating something in a format suitable for [devdocs](https://github.com/Thibaut/devdocs) (see &lt;https://www.devdocs.io&gt; and enable C++).
&gt; the very least learn the basics of GNU Make. That's like saying " If c is too complex for you at least learn assembler"
It depends on the level of functionality. If you knew you just wanted to read some 8-bit RGB image produced by some program it's not terrible to whip up a reader, but I imagine getting wide interoperability (interlacing, RGB, RGBA, gamma, compression, *animation*, EXIF, color spaces, metadata, etc.) to be an extensive exercise in handling edge cases.
And eats resources like nothing else
Maybe I'm crazy, but basic makefiles seem very easy to write and use.
4. workaround for the enum issue; or use enum class: enum class Color { Red, Green, Blue }; Color color = Color::Red; 
Auto doesn't play well with expression templates. There is a [paper](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0672r0.pdf) in flight which proposes to fix this.
Learn both. There is always time to learn two languages.
It would be so nice if c++ could allow me to do: private namespace { } so I wouldn't have to do: namespace detail {} 
For desktop apps, Qt is the best. You don't have to use QDevelop. I certainly never had. For web apps, [Wt](https://www.webtoolkit.eu/wt) seems to be very good.
And to make it beyond pascal and increase safety and performance even more, array accesses could be made safe by combining numeric ranges and arrays. 
You can declare unnamed namespaces, but it makes sense in a .cpp file only: namespace { ... }
He mentions `enum class` in the article.
And how would that work / what would be its effect?
Hiding implementation details 
First of all, I have to thank you for such a great software. Second, I have a question. I couldn’t find any information about integrating this to cmake based qt code base. Would you mind to add tutorial for this? Again, thank you for such good software and I hope you guys keep it updated ;)
WRT using raw pointers with legacy/library code. Worrying about interfacing the two misses the point of what a unique_ptr does. With a unique_ptr it's not the memory address that's unique, it's the ownership that's unique - you use the unique_ptr's ownership of a raw pointer to manage its lifetime. So you assign your raw pointer to a unique_ptr that 'owns' it and then can spin off as many 'weak pointers' (i.e. raw pointers) to pass down the stack to your legacy code and everything plays nice. :) --- numpy/scipy is easier than C++, but IMO Python doesn't permit the same degree of vectorisation that C++ does which almost always ends up being a fatal flaw. If you only have little datasets to analyse Python wins every time - but the nontrivial cases can get painful. --- Compiling everything at command line, without a build-system, is Bad Form because you have to document the build-procedure. (You come back 6 months later and what was the command again? Which libraries do I need to -L? Which order? Why doesn't this work any more...). Build-systems are a nice way to make the build-process self-documenting, (./configure &amp;&amp; make &amp;&amp; make install) /Which/ build system you use is largely irrelevant, when dealing with other people's code you pretty much have to learn their build system anyway. CMake is becoming a defacto-standard for a lot of stuff just because it's so portable. There're other options but I don't think they work as well on as many platforms. It's one of the worst parts of C++ is having to marshal build-systems. Most places I've worked have had a really crappy cludge of perl.
in my experience it's quite common for inlining and constant propagation to cause expressions and code to not look like humans would write them (given the constants). The toy example of if (x &lt; x + 1) happens when the function is inlined and what was a variable turns into the constant 1.
My main languages are all C (something) C/C++/C#. I get why python is popular but i have a self diagnosed condition whete i have a mental breakdown staring at code where indention is part of the syntax. I really enjoy c# on the high end because the syntax is very similar and it lets me interact with c++. Its the biggest advantage c# has over javs, the ability to go unsafe and pass pointers or redefine structs do you can communicate with c++ libraries.
Yes and the headline says "workaround for the enum issue" which is a non-issue with enum class (even the article agrees!?). So it is a reason to use namespaces if you are working with a compiler older than 7 years. I think a time to upgrade, YMMV. 
The second two are legit, I don't think the first one is relevant to reading and writing since working with images is different and very separate from getting them from in to memory. 
Consider this before spending time on SVN : https://trends.google.com/trends/explore?date=all&amp;q=%2Fm%2F05vqwg,%2Fm%2F012ct9,%2Fm%2F08441_,%2Fm%2F08w6d6,%2Fm%2F09d6g&amp;hl=en-US&amp;tz=&amp;tz=
&gt; `nullptr` is now preferred over `NULL` "preffered" is not a good word here, I would say "has replaced" 
namespaces are utterly unable to hide implementation details. You can always reopen the namespace after.
&gt; I thought there was better optimization in python Some things can't be optimized by the interpreter, but the Python one has still a lot of things to improve. For example, the interpreter [does not](https://stackoverflow.com/questions/13591970/does-python-optimize-tail-recursion) optimize tail call recursion. [example](https://i.stack.imgur.com/52QrO.jpg). There are interpreted languages which have better performance, but I guess it's just a matter of time for Python to catch up. The other thing are templates... some things in C++ can be viewed as "O(0)". ___ 1\. CMake is only "required" if you want a multiplatform setup. For own projects, you can rely on your IDE or unix makefiles (which, huh, in most cases are generated by the IDE) - they are interchangeable between any IDE except Visual Studio 2\. I don't get this question but for any task in C++ that is not covered by standard library - grab an external library that suits your needs. C++ allows to write very good libraries. 4\. Don't use an old library if you don't really need to. If you don't have any other choice, write a wrapper around that library to make everything RAII and such &gt; I guess my question is, if you only had time to learn one language and were more of a dabbler in 2018 would you all as C++ programmers still think it's worth the investment? Or would I save myself a lot of headache by sticking with python? If you learn C++, you will know practically any programming language because others are just a subset of features with some assumptions for convenience. Then you need to just learn the syntax and language philosophy to write good code in it
Has somebody readed his book and could share opinions?
&gt; You don't have to use QDevelop. I think you meant "Qt Designer" here.
We don't have access-based namespaces, but we will soon have modules! Just don't `export` stuff in your `detail` namespace, and users who `import` the module won't be able to name them. Effectively, those names are private to the module... just via a mechanism that is more or less exactly unrelated to namespaces. 
You don't have to learn CMake. I think it is currently the most widely used build system, but there are a lot of alternatives. If you have experience with python, maybe have a look at [meson](http://mesonbuild.com/Tutorial.html). It seems to gain a lot of traction at the moment, has python-like syntax and seems very clean.
GIT!!!! VSTS at least has integrated client but I wouldn't bother, use some external client. Since it's Windows the least-hassle choices are either GitKraken or bash shell + git. There are other clients as well but I have personally found those two least hassle.
There are far more scenarios where speed is important, like embedded programming (you don't have a great hardware so you have to get the juice ice out of it), high demand server programming (you have a great hardware but so many requests that a millisecond makes the difference), real time applications, etc.
By the way, what's the status of the proposal to allow `decltype(this)` in member declarations? With it it would be possible to eliminate `AUTOPROP_ENABLE`. Or I guess we will have reflection before that anyway...
Contents of anonymous namespaces have internal linkage, not external linkage.
Anonymous name spaces doesn't hide names from appearing when you include a header that contains one. I think the purpose of a private namespace would be to make names only accessable from the enclosing namespace. You could use them in headers without exposing names to the translation unit that includes it.
Hey.. I'm a C++ programmer for last 18 years and seen its transition from C++98 to C++17. I love C++ as performance wise its still the fastest..though some languages like "Go" are inching closer in that direction. So here is what I think about your questions... Ans[1] : It's not necessary to learn CMake as you can build with your own scripts. The Command line is very much feasible and hardcore programmers are still using command line. Ans[2] : Clear "No", there are libraries, but the number of people using it and the community around it is limited. Even "JavaScript" has better libraries for Data Science than in C++. Ans[3]: nodejs + JavaScript will fit as a reasonable alternative to python/Django Ans[4]: Almost all code will be mixed (at least as of now). This is not the fault of the language, but the programmers. Most of the time, programming habits are hard to change.. :) Is C++ worth the investment? Depends what you want to write the code for.. "Yes" if you're planning to write a gaming application or applications with hard real time requirements. If your intention is write more of a front end applications... then C++ is definately NOT a right choice
OK. So atomics are slower than non-atomics. That was what I was responding to - the "atomics are slow" part. Yes, for synchronization, non-atomics don't matter, if they are incorrect. But it isn't that cut and dry. You can spend a lot of time/effort turning a 3-atomic algorithm into a 2-atomic algorithm. (Or tweaking the memory orderings from seq-cst to acg/rel to relaxed.) So it does matter that non-atomics are faster than atomics.
&gt; I think the purpose of a private namespace would be to make names only accessable from the enclosing namespace That's exactly what anonymous namespaces do. They are like an inlined namespace, they inject their contents into their enclosing namespace only. &gt; You could use them in headers without exposing names to the translation unit that includes it A translation unit is always exposed to everything being compiled. Including any mythical private namespace feature. The point I'm making is that C++ already has private namespaces. They were a bit broken in C++ 03 such that they got a bad rap and people avoid them, but they were fixed in C++ 11 and now they're perfectly serviceable as the "private namespace" language support the OP wished for.
One thing that is important to note (Bjarne mentions this is his talk as well) is that one of the features that make the c++ language so great are its libraries. The same can be said about python; Imagine writing python code without importing any modules... you wouldn't be able to do very much at all! People have this misconception that "python can do so much more than c++ with a lot less code". This has nothing to do with the python programming language. Instead, the fact that it is very easy to do complex things like plotting, machine learning, etc., in python is due to the many well-designed modules that you can easily "import". C++ will soon be getting modules as well. In the meanwhile, before you try to write something in c++, you should make it a habit to look for a 3rd party library that already provides the functionality you are looking for. Search GitHub or SourceForge. Using a 3rd party library in c++ is very much like importing a module in python... it is essential if you want to minimize the amount of code that you have to write to get a really cool feature. Linking to a library in c++ comes with its own challenges, which brings us to using CMake. Learning CMake is beneficial, but i wouldn't say that it is essential. You can always set up your projects by hand using an IDE. CMake will help you automate this process, but if you are only writing code as a hobby, then i wouldn't worry about learning CMake. 
I was talking to a colleague recently who was experiencing performance problems in a Python application due to excessive string copying. They ended up writing a new string implementation a bit like LLVM's implementation of borrowed fragments of other strings, using Python memory views, and got a 200x performance improvement. Python's a deep, deep well of bespoke and arcane knowledge. No wonder C++ folk feel at home there.
&gt; It's one of the worst parts of C++ is having to marshal build-systems. Most places I've worked have had a really crappy cludge of perl that nobody in the company has any ownership of. Agreed. My current work contract its build system is a series of very long and complex Windows batch scripts which tie together no less than five completely independent build systems, some of which were forked from their original in the 1990s and heavily customised. Build takes two and a half hours for any change to the core libraries because the batch scripts can't tell what actually needs building from a change upstream, and so rebuilds everything downstream. Yay.
&gt; IMAO Python is a horrible language because beyond simple scripts C++ projects of the same size are easier to maintain I don't know where you get this from. Python has a very decent modules implementation and package dependency manager. I've worked on million line Python codebases before (Zope) and it's a much better experience than with C++ where you must deliberately hobble the C++ features you use between your "modules" to prevent ABI breaks. Moreover, Python doesn't require recompiling everything when you make a small change to a header upstream, you just launch and it goes. I'd recommend Python over C++ any day for multi million line code bases.
&gt; I don't know where you get this from. I had misfortune of working with Python on a "real" project. And like said it is my opinion and I do not care if somebody does not agree since I do not want to waste my time discussing how shitty Py is. You like it? Great use it. It is just not true that every person on the planet agrees that only drawback of Py compared to C++ is speed.
Does the standard say that unnamed namespaces make their names only accessible to the enclosing namespace? That's new to me! I had thought that they only deviated names within them with internal linkage.
No, items with external linkage are merged into a single instance by the linker. Internal linkage can multiply, external linkage unify.
Good one! 😁 In e.g. C#, there's conditional compilation and partial classes (better matches what you're explaining); didn't try though.
It looks very interesting. Do you have a link to the version without std::function involved ? I made very little adaptations, and it seems to be compatible with c++14 : https://wandbox.org/permlink/qtA1CbGcv2TWif4O It only needed to replace the if constexpr of NextValueForCounter with std::enable_if
I'll second the CppRestSdk. It was easy enough to integrate into a small app for basic web interfacing. Much, much easier than it was to use ASIO at the time.
Easy: Use the time filter. 
Ya, I could probably accomplish something similar with partial classes and #if in C# -- but not exactly the same because each conditional partial class would not have its interface enforced by the compiler. There's probably way through inheritance, but then I'm jumping through hoops to accomplish something that is easier in what is arguably a more primitive language.
Nice, I really like this! Have you considered using different classes for image storage and image view, like std::string -&gt; std::string_view? 
Section 10.3.1.1 of the C++ 17 standard: &gt; An unnamed-namespace-definition behaves as if it were replaced by &gt; namespace unique { /* empty body */ } &gt; using namespace unique ; &gt; namespace unique { namespace-body } 
The only part I'm not clear on is how a macro could generate that at all. As far as I know, macros can't recurse at all.
Exactly, therefore don't put them in a header, that's the rationale.
FemtoLisp, Scheme dialect. Fits into 200K, used as embedded in Julia project https://github.com/JeffBezanson/femtolisp
Where does it say that you can't access declarations inside an unnamed-namespace from outside of it?
Writing a macro that expands MY_MACRO(int)(x), (double)(y)) Into: int x; double y; auto reflect_tuple() { return ... } Is very easy using BOOST_PP. An example that does similar, and even more sophisticated things to generate reflective enums: https://github.com/quicknir/wise_enum.
3\. I would say that Wt is a good alternative, but then again, I'm a bit biased :-). I wish I could share my "introduction to Wt" talk I gave at FOSDEM with you, but the video is delayed because of audio synchronization issues. [Here are the slides, though.](https://fosdem.org/2018/schedule/event/web_development_in_c/attachments/slides/2443/export/events/attachments/web_development_in_c/slides/2443/Roel_Standaert___Introduction_to_Wt_4___FOSDEM2018.pdf) As far as [Wt's documentation](https://www.webtoolkit.eu/wt/documentation) is concerned: I'm doing my best to improve it. We have a link to an introductory tutorial that's a lot like the example I use in the talk. Apart from that, the best way to explore the features of Wt is to look at the [widget gallery](https://www.webtoolkit.eu/widgets), and the examples in the [examples directory](https://github.com/emweb/wt/tree/master/examples) in the Wt source.
Why choose one? Write time sensitive stuff in C++ (You might want to check out all of boost as well as the Eigen math library) and use boost::python to export object interfaces to Python. I've realized incredible performance increases in speed by rewriting scripted (ruby/perl) code in C++, but a good chunk of that was just improving the actual design of the program. I had some math-heavy scripts that did a lot of extra file accesses and forking off of processes that usually took 35-45 minutes to run. Switching to C++, pulling the data it needed in from the database just once and keeping all the computation in memory and in the same process it took less than 1 second to run. That's a pretty dramatic speedup, but it's hard to say how much of that was just how badly the program was designed originally.
What's the default linkage for items declared in a normal namespace?
Everyone already knows that anonymous namespaces do this. What everyone is talking about is this: namespace A { private namespace B { void Func (); } Func(); // legal } Func (); // illegal
"Modern C++" is like a whole new language. In a good way.
An issue I have with the way computer science is taught: In calculus they make you write proofs for dirivation and integration, and then at the end of that 1-2 weeks tell you what to memorize so that you never have to do that again. It shows you how it works, and follows up with how to use it practically. In intro computer science courses they don't follow up writing stacks with telling you to just use #include&lt;stack&gt;. Or that all those algorithms you have to write like bubble sort are in &lt;algorithm&gt;. //End mini-rant. 
I have considered this but decided against it. For example, it would be much more difficult to write functions that are agnostic to whether a passed image is a view or owned (without excessive templatization or overloading on both).
Actually I don't agree with the general notion, that the only advantage of c++ is speed. I like statically typed &amp; compiled languages. But if all you need it for is some hobby program, python will get you there faster.
&gt;I do not want to waste my time discussing Then why bother posting? Nobody cares.
&gt; Everyone already knows that anonymous namespaces do that. Dunno, some of the other comments would suggest a fair bit of confusion on the topic. &gt; The purpose is so that you can declare a "detail" namespace in a header file that cannot be accessed by users of the header. That's not possible in any implementation or language for the exact same reason as why `private:` member functions are always part of the public API. Sure, you can *obscure* the contents, make it slightly *harder* to access them. But the detail always leaks out, and until now nobody has bothered to persuade the committee that it's worth adding extra shallow compiler checks for trivial use of internal detail. Now, all that said, would some sort of shallow checked `private namespace` like `private:` class member functions have a chance of succeeding at WG21? Personally, I'd vote for it *if* the semantics and consequences were well understood. So the devil would be in the precise details, and messing with name lookup requires a very high bar at the committee. Someone with enough motivation and time probably could get it through, eventually. 
A very basic cmake file requires two lines: project(my_project) add_executable(foo main.cpp) It will work on Linux, Mac and Windows with whatever is your default compiler and allows you to create debug as well as release builds. Writing a simple function in assembler is also not difficult, but it is still easier and more portable to do it in c.
Nobody is looking for perfection either. There is always a possibility of just reopening a namespace and accessing whatever you want. People just want a way to say "please don't use the contents of this namespace" in a way that is enforced by the compiler in some way rather than by conventional naming schemes. Therefore I'm sure the shallow check would satisfy most people interested in the private namespace feature.
I appreciate there are tradeoffs, but you might find this interesting: http://neugierig.org/software/chromium/notes/2011/08/static-initializers.html
Well boost::signals2 is thread-safe which is not the case of this library, so it's not really comparable.
I'm honestly mostly a C++ programmer, but when writing new code, if ultimate performance is not my goal, C++ is rarely my first tool of choice. My first attempt is Python, using NumPy. My second attempt is Cython. And my third attempt is re-writing performance-critical sections in C++ and wrapping them to Python. I mostly work on C++ codebases and FOSS projects, but when I need to get a task done succinctly and cleanly, and where developer "performance" is faster than code performance, favor Python to C++.
I don't see how this infringes on the /r/cpp rules. There's no: career advice, coding question, tutorial/blog/book suggestions. I was merely asking people regarding their experience in taking the exam.
&gt; why would you want to learn how to write make files? I don't want. I don't see much sense in doing it manually, there are many tools for it. &gt; What possible advantage would that give you over cmake? CMake does not support everything AFAIK. At least I have encountered multiple places where makefiles were written manually. If CMake does support it, then I had bad project managers - so much time had been wasted. &gt; If you think c++ has all the features there are, you probably haven't used any other language than c I did not state that C++ has all the features. C++ either supports feature X directly or requires some code to make it (may be a lot), but every feature is possible to have (with higher or lower convenience). Many other languags (especially high-level ones with GC) do not support going lower levels to memory allocations and pointers.
Heh, slick! I never thought of that. Nice. 
I second the recommendation of GIT. In addition I use sourcetree as the GIT user interface. Command line git makes my skin crawl.
Right, but you're writing your kernel in C or C++ Also GPU is only one option for vectorisation and doesn't map onto all problem spaces whereas multithreading is much more general purpose.
* Announcement blog post: https://research.fb.com/announcing-tensor-comprehensions/ * Paper: "Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions" - https://arxiv.org/abs/1802.04730
Do these projects accept only students (i.e. currently enrolled at a university)? Or do they accept others too?
14ned's point is that you can't get ODR violations using unnamed namespaces. Of course code bloat would still be an issue, though. :)
I do too. I wouldn't be here if I didn't. OP isn't looking for that sort of thing though, and I'm not going to be biased towards a thing I love, but instead try to steer the OP in the direction he is interested in.
Per https://developers.google.com/open-source/gsoc/faq, it looks like the answer is...yes. "You must currently be a full or part-time student (or have been accepted for the fall term) at an accredited university as of the student acceptance date"
 You can get ODR violations easily. namespace foo { namespace { auto do_nothing() { return [](auto&amp;&amp;...){}; } } template&lt;class F&gt; inline auto maybe_do_nothing( std::true_type, F&amp;&amp; ) { return do_nothing(); } template&lt;class F&gt; inline auto maybe_do_nothing( std::false_type, F&amp;&amp;f ) { return std::forward&lt;F&gt;(f); } } now, calling `maybe_do_nothing( std::is_same&lt;T, int&gt;{}, [](auto&amp;&amp; x) { std::cout &lt;&lt; x; } )(t)` is an ODR violation because you *used* something in an anonymous namespace (with internal linkage) in the implementation of something inline without internal linkage. 
Yep. Read that after I asked the question. A shame, IMHO. It seems like it would be great to have this framework of mentorship and guided projects (within a defined outcome) even for working/jobbing programmers/dilettantes and such. Oh well...
I have bought his book, because I was interest by a particular chapter: "Functional data structures". I found this chapter clear and helpful.
I love the comedy aspect of some of them too. Adds positivity to the c++ community.
While CLion far from perfect, I really like its refactoring and search (indexing) features. Feels like the most productive IDE. It's a little resource hungry on the downside.
&gt; C++ won't help, especially as it is harder to get things right in C++ vs Python Citation needed. Most computations in python are going to be full of expensive indirection, type checks and bounds checks.
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7xktv4/how_to_learn_more/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
They aren't as bad as people make them out to be, but there are quirks
Preface: Any choice of language must first answer the question "which ecosystem has the best library support for my task" No, I would only use this language if there was an existing codebase that had features I wanted to use on which I wanted to develop extensions. Its benefits have mostly been subsumed by other languages when considering new development. Its main win comes from the ability to let you write high level template code while retaining the ability to optimize in cases that you care about. If you don't have a need to heavily generalize your code, as in, you're not having to deal with a bunch of types and their interactions while also *requiring* that the code be super fast, then IMO all the other costs and pain points do not make up for this benefit. Regarding CMake, you should be fine with a simple makefile. At least, that will let you repeatedly build your work, and you can use it for more than just code compilation. If you really need to build cross-platform, I would explore CMake then. If you're looking for something completely different, try Common Lisp, or Racket :-)
Do you think CMake is a good forward looking build-system so future projects don't end up the unholy mess you're describing from the past?
I'm still enjoying the textbook so I'll probably keep doing a little at a time when I've got a bit. It obviously can't hurt. My one reason for staying away from some of the other interesting languages I've read about (like Go, Rust, etc) is that eventually I'm going to want to do things (whether it's a gui for a game with the kids or a web app, or some data analysis) and I'm going to be desperately dependent on well developed and documented libraries (I don't think I'm ready to write my own matplotlib and stats package for example). How do some of these other languages (like Haskell) compare in terms of useful (to me) external libraries? 
I'm comfortable with both Python and C++, but I'd say stick with Python until you've got a real need for the speed. Python's a million times easier to write at first, so just keep with that and do your processing that way, then once you've got your problem areas identified, you could just write a tiny C++ thing to handle that, if you really needed to. I'm of the mindset that you don't actually gain much from learning about the ins and outs of structs or memory or anything like that (mostly because I don't know what I don't know) but I get far enough without needing to worry about any of that at all, and just use `std::vector` and `std::map`, and have all I need really. That being said, if you're deadset on learning C++ again (because you find it fun), it's never been easier, with `auto`, lambdas, and for-ranges, so it's pretty close to Python in certain senses. auto my_name = "RedditBot"; auto print_names = [](std::string name){ std::cout &lt;&lt; "My name is " &lt;&lt; name &lt;&lt; std::endl; }; print_name(my_name); print_name("DiggBot"); Even thanks to `dynamic_cast` you can safely convert from type to types without things going too badly.
Thank you for the advice. As to your reply: &gt;Clear "No", there are libraries, but the number of people using it and the community around it is limited. Even "JavaScript" has better libraries for Data Science than in C++. Why do you suppose that is? My understanding is that most of the python modules for data science have a C or C++ module it's calling actually running the analysis. So if the guts are already C++ is it just that the front-end is too complicated to make/use? 
Yeah I'm with you. I remember that exact feeling after finishing and switching career paths thinking "Wow I know all of this theory after months of hard work and studying...but I can't make a Window pop up with some buttons and do anything actually useful" I suppose the same was true for all the biochemistry I took. I memorized every chemical structure of all the light and dark reactions that went on in photosynthesis, but I can't seem to keep a few potted plants from dying...
Why are you the build guy at work?
hey man no! I missed all of these happening too! I learned in the 80s and I'm 40 now and you're not old. I've been using the videos by Cherno on youtube (The Cherno as he goes by), he put up a nice little c++ series. He reveals a lot of cool stuff that even I didn't know about the internals of the compiler etc. It's neat.
Totally agree with this. The new features of the updated language are super helpful. I dont recall if templates existed in the 80s but if they did I didnt knoe about them, he he. But I love those and these special pointer objects are cool. I still have a soft spot for manual pointers though, takes me back ha!
&gt;Sounds like you're wanting to learn a lower level language but worried about the pains of old build systems and old libraries. Haha! Yes this exactly. I guess I remember kinda liking it. My only worry about Rust is that it won't have all the libraries I'm very much going to be relying on to do the things I'll want. Seems like most of the Top 5 languages have some implementation of everything (even if it's not the easiest to do).
qt creator
Have you tried trying?
I'm interested though the "ideas list" sounds like it's way over my head... 
yes honey she knows it a multipass. And I got tired of the way configure and make were being misused and read up on the tools just like you're supposed to do with all your other tools. Except too many people just wave their hands and say "its all black magic". I've learned a lot and have a healthy appreciation for ./configure now, as well as what linking is all about and how to resolve errors and why they happen, etc.
I personally always end up going back to vim, but I do own a current license of CLion and occasionally go back and try it out. I briefly tried QTCreator and didn't really like it, I thought CLion was better, so there is that. OTOH, if you're happy with it, then why switch as long as it works. 
I would be interested if it had CUDA support.
Yea, couldn't agree more. Since VS is free, there is literally no reason to pay for CLion for a personal license. If it was free too for personal use, that might be a good reason to try it and potentially use it. It does have the appeal that it works on Linux and macOS as well.
It's basically an [LL(1) parser](https://en.wikipedia.org/wiki/LL_parser) that builds straight assembler (normal compilers would generate an abstract syntax tree, convert that to some intermediate assembler, optimise and register allocate). The most clear example of it being an LL parser in the implementation of the statement function. The hex things you see are x64 opcodes. Instead of efficiently using as many registers as are available, everything happens over EAX, EBX and the stack for pushing intermediate values. The constructor of Function shows the setup of each function : 1024 bytes of 0x90 (which is the NOP or no-operation). With the first few instructions and the last one already fixed. First we preserve EBX (which is a requirement of the [x86 calling convention](https://en.wikipedia.org/wiki/X86_calling_conventions), then we save the current stack pointer in EBP so we can independently refer to variables from that known position and reserve some stack space for those variables. The end of the function does everything but in reserve. The function primaryExpression() handles a number of variable reference. If it is a number, we directly MOV that one in EAX. If it is a script argument, we move it from the predetermined register (see above mentioned calling convention) into EAX. Finally, if it is a variable, we move it from it's known location relative to EBP. Term expression handles adding two terms. First we handle the left side through primaryExpression, we push that value on the stack, then we do the same for the right hand expression but leave it in EAX. Finally, we pop the left hand in EBX and add the two. Same for the subtraction but we need to NEGate the result of course. expression handles the assignment. Here we do a dirty trick. The left hand side is evaluated and it's value is stored in EAX. But we don't need its value, we need to know its location so we can store something else there. Luckily that location (in this case, relative to EBP) is the one but last opcode we added. We steal that one and use it to store the right hand side (evaluated into EAX) in that address. 
It's a shame thesw things are for students only, scylladb is a marvel of engineering, I think a lot of c++ lovers would kill to work wirh that team. I'll definitely be sending this one on to soms people, good luck to anyone applying. 
I get it for free as a student but after I graduate I wouldn't pay for it. It's a great project and maybe I'd ask my employer for a license but for personal things emacs is just as good.
&gt; scylladb is a marvel of engineering What makes you say that? 
&gt; Why do you suppose that is? * When code is IO-bound, as many data science applications are, the appeal of C++ is weaker * Anecdotally, C++ tends to appeal more to systems-programmer types than data scientist types. * Anecdotally, "Not-Invented-Here Syndrome" is rampant among C++ developers. i.e. "Why use a library when it's more fun to do it yourself? Why use pandas when I have std::unordered_map and &lt;algorithm&gt;?"
I am interested in this But I don't know how to apply for this
You can use C in C++, and you can wrap the ugly structures in a `std::uniqueptr` with a custom deleter, making it so much nicer than the regular C code.
I'm happy to pay that money for being more productive. I tried to get used to VisualStudio (2015, after skipping some versions) again, but I ended up using it only to Windows compilation and doing all edits in CLion. One reason is definitely personal preference (I'm also using IntelliJ and AndroidStudio). CLion feels just smarter (especially Alt+Enter intention actions and refactoring).
What's the Alt+Enter equivilance in Emacs? :)
Visual studio isn't exactly free. It gets real expensive real fast if you use it in commercial projects.
C++ is absolutely not suitable for data science. I'd use Python or R instead
As an aside, you can totally give yourself user flair identifying that you work on Factorio. :-)
That was just a matter of time (EAP builds aren't the releases with the appropriate quality), as it was in development. Available now in the EAP build. Feel free to try it.
Have you looked at LuaJit or LuaLlvm? I don't think they are written in modern C++ but maybe they will help you squeeze more performance. 
I think it would be impractical both for you and the Lua developers to maintain a C++ port. It will probably have a bigger footprint on target platform (stdlib and stuff) while benefits are not obvious. From a practical point of view it's really easy to write a good wrapper for any C library, so why bother? I also admit that it could be a great and fun exercise - making something already good, so much better and cleaner. The thought alone is enough to make my hands itch :) ^^^P.S. ^^^Bots ^^^are ^^^superior
A while ago I wrote a [PEGTL](https://github.com/taocpp/PEGTL) grammar that corresponds to the Lua 5.3 lexer and parser. If you get the [PEGTL](https://github.com/taocpp/PEGTL) you can find it in `src/example/pegtl/lua53_parse.cpp`. Should you decide to go ahead with a rewrite, and try to use this grammar, I'd be happy to help get you started with the PEGTL, and to iron out any bugs the grammar might still have. (It parses the official Lua test-suite, which is a good sign, but without any semantic actions it's not enough to be sure it's correct.)
Google updated it to work with 2017.3, not 2018.1 yet. (Mind, JetBrains doesn't maintain the plugin)
I know it is not exactly what you are looking for but you might want to check http://luajit.org We are using havok script which is just highly optimized lua implementation with few language extensions. Also kudos to you for being able to troubleshoot lua internals. I didn’t have to do it in past 7 years but i still remember the pain.
Today was actually kinda rough but I really appreciate you coming back to tell me about this. Thank you
Yes you are right, but getting code especially using STL anywhere close to a Linux kernel driver in mainline is not possible. Personally if I am to do any work near the kernel I do want the work to be usable by more than me and integrating with the kernel build setup means that there is neither C++ nor STL. I presume that access to the Kernel is needed unless the bugs are in an input library layer tied to X or Wayland (both of which do not provide C++ or STL in their build/code either). 
&gt; I think it would be impractical both for you and the Lua developers to maintain a C++ port. As far as I can tell there is no C++ port of Lua. There's only the C implementation which you can compile under C++ but it's still C. &gt; It will probably have a bigger footprint on target platform (stdlib and stuff) while benefits are not obvious. I don't really care about the footprint - I care about performance and being able to easily modify the code while knowing that the modifications done aren't going to break other things. &gt; From a practical point of view it's really easy to write a good wrapper for any C library, so why bother? Because it's still C - I can wrap it all day long but that just adds additional overhead to interfacing with the Lua engine. I want to be able to change how the internals of the Lua engine work without changing user-facing behavior and doing that right now with the C implementation of Lua is incredibly difficult because the way C works it goes against everything I try to avoid when writing C++.
Good point, I guess the only good side is if you don't care about merging your code in the kernel because it's very specific and your company doesn't want to open-source it, you're good. And even if Stallman will hate you for it, it's not illegal to use GPL code in your proprietary application if you're the only one using it. 
Additionally: I want a proper C++ interface with the Lua internals where I can just do: for (auto&amp; value: L-&gt;get_table("...")) and iterate the key/value pairs in a Lua table directly instead of the massive hacks that is standard Lua iteration: * Push nil onto the "lua stack" * Call "find first" on the table which pushs the first key + value onto the "lua stack" * Call a function which parses the Lua stack values one at a time into C values you can use in C++ * Call "find next" which does a hash lookup using the previous found key to find where in the table it's located then steps to next and pushes that onto the "lua stack" * Repeat until you iterate off the end of the table Lua iteration through the C API is stateless and it adds a ton of overhead to do it and provides no way to do stateful iteration if you don't intend to change what you're iterating on. You're just stuck with the slow way.
We looked at LuaJit but it's based on the 5.1 standard and is still C meaning all interaction with it is going to be through the C API which I've grown to dislike for how inefficient it is.
versioned releases is also easier for internal use where I work, easier to talk about in conversation, and easier for non-tech people to understand. An obvious downside it requires more work to organize. I +1 the idea for more frequent version updates. anything monthly or longer per update I think is good. 
Why not use a wrapper over the C API ? You don't seem to need anything that require a complete C++ Lua implementation.
You can use VS community on commercial projects, if you have less than 5 concurrent developers, 250 people in your company and 1million revenue per year. _Then_ it gets expensive. 
LuaJIT has an FFI, which is far from inefficient (perf and work wise), and IIRC Paul did put some time in getting it to work with C++ symbol decoration from GCC/Clang/MSVC. Also I do believe there is a pull request to update the parser to 5.2, but its actually rather easy to extend oneself (I added custom parse/compile time operators with almost zero effort). IMO you'll be hardpressed to beat LuaJIT's perf, VM or JIT'd, but because Paul took great care in making it fast (but the code ain't "pretty"). I'd say your time would be better spent extending it, rather than rewriting. Something to note give a comment further down, don't do Lua in C++, do the Lua in Lua and use the FFI to invoke C++ if needed (this is something Paul mentioned a few times on the mailing list), also, benchmark, because it does sound like a few assumptions about the speed of things has been made.
Almost all of the "Lua" we do in C++ is iterating tables and extracting data into C++ format - should we do it the other way around: iterate the table and call a C++ function for each value - it would execute the same code except now we'd be adding a bunch of Lua -&gt; C++ function calls to the operation. I do a lot profiling on different interactions between the game and executing Lua and already implemented several changes to the version of Lua we use to mitigate what I found. One example: setting a meta-table performs an (at worse) O(N) operation to un-link the table from the standard lua object list and put it into the meta-table objects list.
Hi, I've done quite a bit of work with Lua's C API and think your project sounds fun. It would be a lot easier (for me) to contribute to Lua if it was in C++ and developed on github instead of their private close-knit community of developers (I think you can get patches merged but the development of Lua isn't exactly open).
&gt; But from what I can find nobody has done it - which also makes me wonder if I'm missing something. The stated goal you have is to ease hacking on its internals. OK, one point for rewriting Lua in something that doesn't make your eyes bleed. :) Let's go through the reasons against. Your rewrite won't be the official Lua and never ever will be. It won't receive patches or maintenance by the Lua developers so you'll be entirely on the hook. It won't even have community support necessarily since the community is built around the official Lua and its C implementation. When Lua 5.4 comes out, your rewrite would need to be updated separately, and any custom additions or extensions your fork adds may then be incompatible with future upstream Lua with no recourse. Lua has no spec and no public test suite for you to develop against to ensure your rewrite it correct, so you'll also need to write and maintain a test suite. Unless you maintain the very C API you might be having problems with, popular community Lua C modules or interpreter hacks will be incompatible with your fork. Without copying many of the implementation details of Lua, even popular Lua debuggers and "IDEs" would be incompatible. Basically the question you have: is addressing the pain you endure when delving into Lua's crusty C implementation worth the cost of writing, testing, releasing, and maintaining a parallel implementation with zero upstream support and negligible community support?
A couple of guys have apparently implemented lua VMs in go: * https://github.com/milochristiansen/lua * https://github.com/yuin/gopher-lua (more active) And there are a couple of what appear to be aborted or dormant attempts at writing VM's in rust. I suppose there's no C++ version because C++ devs generally don't encounter that much friction from using the standard C version.
&gt; Basically the question you have: is addressing the pain you endure when delving into Lua's crusty C implementation worth the cost of writing, testing, releasing, and maintaining a parallel implementation with zero upstream support and negligible community support? To me? yes :) We're currently using Lua 5.2 from 2015 with custom patches to make table iteration deterministic and to fix some massive performance problems. Our current version of Lua we use is already incompatible with virtually all Lua IDE/Debuggers because of the changes we've made so I have no problem maintaining my own version with my own tests. My hope was someone had something already going that I could take and run with :D But if that's not the case I don't mind being the first. At the very worse I give up on it having learned why nobody else did it. Any other result I see as a win.
If modules come along, there would be no need to have a detailed namespace for hiding code, only for organizing code. 
No, I meant QDevelop.
The namespace would be private to its parent. I.e. only its parent would be able to access it. Of course, I'd like to have private and public declarations on a per namespace basis. 
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7xpapd/learn_advanced_modern_c/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I'm not sure it's what you're looking for, but there's a number of really neat, modern C++ lua bindings. I can personally recommend sol2: https://github.com/ThePhD/sol2 There's also Selene: https://github.com/jeremyong/Selene 
That is really generous! That's really nice &amp; great.
At the end of the day there are &gt; 1 million players and far less mod-devs so a little extra work on the mod-dev side for a faster gameplay experience for the million+ players sounds like a great trade off to me.
Well, moc does not support templated QObjects, headers with raw string literals and plenty of other modern C++ features
Section 6.5.4 of the C++ 17 standard: &gt; An unnamed namespace or a namespace declared directly or indirectly within an unnamed namespace has internal linkage. All other namespaces have external linkage. A name having namespace scope that has not been given internal linkage above has the same linkage as the enclosing namespace ... So, you're just wrong on this.
Inline is totally orthogonal to the concepts of internal vs external vs no linkage. Inline simply means "ignore multiple implementations of this external linkage symbol". Also, types and templates have no linkage, neither internal nor external. They don't appear to the link stage at all.
Kaguya is also pretty great https://github.com/satoren/kaguya
If the faster gameplay experience is worth it, you could look at replacing Lua with something like [Chaiscript](http://chaiscript.com/), which has excellent C++ integration, but I can imagine the mod devs not being too happy with having to rewrite all their existing mods.
&gt; Except it's an r-value - if it was C++ I could just give it the string and it doesn't have to heap allocate anything. If you put a `std::string` into your value/cell `union` instead of a `const char *` you can easily `move()` your r-value string into it, at the cost of greatly increasing the size of the value/cell, seeing that modern `std::string` implementations are often the size of 3 or 4 pointers so that they can store small strings in-object. This might of course be an acceptable trade-off for your use case, or if your library uses a smaller `std::string`... ...although now you can't bit-copy your value/cell anymore, to be clean you need to call the `std::string` methods for all operations. Do you have any other idea on how to approach this issue that requires neither a heap allocation nor depends on the details of your `std::string` implementation?
The value/cell union in Lua already does a small-string optimization as well as storing the size of the string so it essentially is a std::string except worse because you can't move into or out of it and have to manage all of the logic of allocating/de-allocating the string in the cell. I don't want to start down hacking the C version of Lua to do stuff like this when I'm calculating a full re-write of the language into modern C++ to be near the same amount of work and would allow for far more tweaks.
For one, it interfaces with an existing client and configuration model (that of Cassandra's) almost perfectly. Secondly, it realistically scales to hundreds of machines, whilst this is theoretically achievable by most databases, good luck setting up a galera cluster or one of the potsgresql m-m hacks on a hundreds of machine without the whole thing breaking down once it hits production. Thirdly (and this is where Seastar comes in), it is able to use more of the resources of a machine by essentially throwing away the kernel and hugging all the resources. This approach is very good because it allows amazing performance, especially netowrking performance (since it essentially means you can allow trusted devices, such as the other DB's in the cluster, DMA), but also amazing performance in terms of what you get from the CPU, since the CFS doesn't have to switch various threads between processes at all times (leading to cache invalidation, leading to a huge loss of speed). Fourthly, it's a very good project in terms of accomplishing some amazing goals that's written in C++17, which is nice, because it showcases how awesomely powerful newer OSS can be over the ancient behemoths written in pre ANSI C, maintainable only by a select few madman. Granted, this may not be a reason to call it "marvel of engineering" but it's still a reason to like it. Granted, I've never had the type of data which catered perfectly to scyllaDb, when it comes to modern databases I often find myself using Clickhouse, so I've only read about it, been told about it and use it in test scenarios (on dozens, rather than hundreds of machines). I've also haven't browsed the code that much, from my limited experience getting seastar to compile I would say that it's way more annoying than it ought to be. Also I'm not a database programmer, so I can't really speak as any sort of authority here, maybe I'm attracted to it's capabilites and to it's relatively tiny codebase (considering the scope of what it can do and it's performance) but inside of it there well may lie poor design choices I can't spot. I'd genuinely love to hear about other databases which hold up a candle to ScyllaDb when it comes to it's ability to run queries on large amounts of data, ingest large amounts of data and distributed over several machines. It's one of those rare pieces of software that seems to be "better" than basically all competitors without sacrificing much, a leap into a new era of code that ends up being much better than the old. (To be noted, there are obviously cases and cases, for example lag cursor queries on heavily indexed tables that involve a lot of complex joins are still something one should do with stuff like Mariadb or potsgres)
&gt; P.S.: Didn't dare to use QtCreator yet. You didn't try the (arguably) most recommended IDE which is free and instead bought CLion? Are you working for jetbrains or are you do they pay you to advertise? I have tested both (I'm a student) and see no (not enough) reasons to use CLion over QtCreator to be honest.
Templates were introduced in the mid-90's. Borland was criticized back then by shipping an early implementation, while ANSI/ISO members were still trying to come up with how they should work at all.
Lua is a library you link with. If you do it in C++, wouldn't you have to extern "C" everything? Seems like you would need to include the source in your project for it to work purely in C++. 
It would be another set of classes you'd simply include and use in your code base - no extern "C" anything - pure C++ - which is the entire point of it - full integration with the rest of your codebase and super easy to change anything should you want to.
I don't see the point in rewriting Lua from scratch just because you happen to dislike the language it was written in, but if you want a more C++-like interface to interact with Lua, you can use [sol2](http://sol2.readthedocs.io/en/latest/)
&gt;You can use VS community on commercial projects, if you have less than 5 concurrent developers, 250 people in your company and 1million revenue per year. _Then_ it gets expensive. It's *and* though. It's *or*. If you exceed any of those things then it's no longer free. It's not hard to exceed 5 developers or 1 million in revenue.
Factorio could stand to gain signifigantly from improved lua performance.
they don't want parity with the C lua API, they want to be faster.
&gt; Lua has no spec and ***no public test suite*** for you to develop against to ensure your rewrite it correct This sounds llke a bad idea for any language product... 
ChaiScript is your C++14 embedded scripting language. It's missing a boatload of things you'd consider a standard library like file IO (something I'm actually working on as a separate project). It's a place to start when evaluating what modern C++ can offer an embedded scripting language. 
Each EAP build has fixed expire date. in EAP release cycle, (ex. 2018.1 EAP -&gt; 2018.1 RC -&gt; 2018.1) They release EAP build before last EAP build is expird, but it is not garunteed after they release normal one. For example, GoLand(= go IDE) 2017.3 released on 11/30 2017, but first EAP build(=181.2784.36) is released on 1/22/2018.
It's hard to use anything except QtCreator for two simple reasons: 1. It can handle MASSIVE codebases without a hitch... CLion is just not built for that. 2. It is FAST, probably because it's written in C++ and not Python or JavaScript So for toying around with C++, CLion and VSCode and all those are just fine. But when you have to get real work done on a large codebase, there is no substitute for QtCreator. Maybe Emacs or VIM, but those are not exactly IDEs.
Have you ever considered to switch to some other programming language for mods? Or even create one... Some time ago I created couple mods for factorio and I found lua as a programming language to be ... How should I put it... My internal "Wat?" rating of lua was quite high. There are many confusing and inconvenient things in the language. Also mapping C++ classes to lua is cumbersome and inefficient. Bluntly speaking I have no idea why lua is so popular. At some point I was going to offer the programming language I was working on as a replacement for lua in Factorio, but for personal reasons I had to suspend the work on that language :( As counter reason for "One does not simply replaces one programming language with another" - there were changes in the mod API that were breaking compatibility. Yes, you will have to support old and new api/language for some time. But switching to another language that is more efficient, easier to support and easier to use by modders might be, in the end, for better. 
You'll likely need a [test suite](https://www.lua.org/tests/) to guarantee it didn't break.
I don't know if this has changed in recent versions, but in Visual C++, the size of a member function pointer isn't always the same. It depends on whether it has multiple inheritance, for one. I once had a bug where different compilation units had disagreed about the size of the member pointer, I think because one only had the class forward declared.
That first one that lets you do: DECLARE_GETTER((QMap&lt;QString, int&gt;), property1); seems like a lot of work and obscure macros to avoid: #define ESC(...) __VA_ARGS__ DECLARE_GETTER(ESC(QMap&lt;QString, int&gt;), property1); 
&gt; if you have less than 5 concurrent developers, 250 people in your company and 1million revenue per year. Not and, it's or. &gt;[For all other usage scenarios: In non-enterprise organizations up to 5 users can use Visual Studio Community. In enterprise organizations (meaning those with &gt;250 PCs or &gt; $1M in annual revenue) no use is permitted for employees as well as contractors beyond the open source, academic research and classroom learning environment scenarios described above.](https://aka.ms/vslicensing) Which means that for a smallish office like mine with only 20-30 people in total, our revenue is over $1 mil which means we would have to get an enterprise licence. CLion is a lot cheaper. CodeLite or KDevelop are also free and available for WIndows.
&gt;People have this misconception that "python can do so much more than c++ with a lot less code". This has nothing to do with the python programming language. Instead, the fact that it is very easy to do complex things like plotting, machine learning, etc., in python is due to the many well-designed modules that you can easily "import". This is so true and so many people don't seem to realize that. That is also why I am hoping (against hope, I know) that the C++ community will eventually get some sort of a more ubiquitous build system. I might be completely wrong and I might misunderstand all the info I read, but my understanding is that modules (even if some people seem to think they will) will not help at all with that. Having great libraries is mostly useless if you have to become an expert at a new build system to use it. Then, everyone on the team has to either learn it as well or will rely on you to install on their machine. And then, you have to install on the deployment system or static link or... Anyhow, I was glad someone mentioned that Python alone (and most programming languages for that matter) are actually productive not by themselves, but because it is easy to reuse *other people's* code.
Love love loooooove the clang-tidy integration using Alt+Enter. That's the real good shit right there. 👌
Isn't this https://www.lua.org/tests/ the test suite?
ChaiScript actually belongs to the slower group of scripting languages, certainly far behind Lua in the speed department. Its selling point is easy C++ integration. Not sure if I would call it excellent though. 
I look at it this way, why ask? Implement LUA in C++ and see how it works out. These days I'm not sure why one would implement any language in C. C++ and a few other languages would be a huge improvement in reliability and safety.
Chaiscript is horrible for performance though
FWIW if you did this I would strongly consider to use it in my game hobby project, send bug reports, and possibly contribute code and bugfixes. The most important win from my point of view is that it would (hopefully) make it possible to use lambdas and such as functions that get passed to lua, and it would be a lot simpler to write code that uses both exceptions AND lua. Many gui libraries that I have tinkered with rely on exceptions so it makes it really awkward to use them with lua.
The "fix" used nowadays is expression-templates. But if you use almost-always-auto, sometimes the expression isn't evaluated when you want it too. I've seen a proposal for extending `auto` so that `auto foo = &lt;expr-template&gt;` would evaluate the expression template, but something like `expr foo = &lt;expr-template&gt;` would leave it unevaluated (I can't remember the specific details, though). That would allow for what you described, as well as mixing between multiple operators efficiently.
In our project we need a UI mostly for the debug parts (i.e. the UI won't be shipped to customers), so we have kinda more relaxed requirements. For me FLTK (http://www.fltk.org) works the best: it's not as nice maybe as Qt, but very easy to learn and very lightweight (statically linked to the app is ~500KB extra), and supports Windows/Linux/OSX.
Ah, I think I'm fully getting what you're trying to do now. So you want to reimplement a lua interpreter actually, but in C++... I'm sure that would be awesome to have but existing C implementations are actually quite optimized I've heard, so I don't think you can get anywhere close or even reimplement it in two weeks. But... why not try as a hobby project if you like it :-) Great things have started this way and you seem to know your way around it very well!
Please define MASSIVE and how CLion does not scale. I think the relevant performance is developer productivity, which correlates heavily to IDE performance but also to its smartness. Would you say QtCreator can keep up with CLion with the latter?
&gt;has anyone re-written it in modern C++ Hell, I would take one rewritten in modern C. I wouldn't put Lua's codebase in my "pretty C code" bucket^^^(a very small backet).
&gt; faster As I said, there's no guarantee a C++ version of Lua would be significantly faster than Lua itself, even considering the API overhead. &gt; easier to use There are wrappers around Lua's C API that make it pretty easy to use.
How will a re-write in modern C++ alone improve performance?
Luajit still uses nan-tagging, so it still has the MAP_32BIT set, but I haven't encountered any issues with it personally. Using any kind of jit, and especially the FFI, exposes your surface area to funkiness and crashing but for me at least the performance is worth it
The 2.1 beta has a GC64 mode that supports a 47-bit address space (maybe a bit larger, idk)... though that capability needed a lot of internal changes so it may be a bit buggy right now.
Well what I am saying don't use the C API, the FFI can easily bind onto functions and generate calls from within Lua, with close to native performance, you might even find it easier to do everything in Lua via UD+FFI and then pass the final results to a callback. Thats why I am saying lift the current front-end from Lua 5.x and replace the VM/bytecode gen with LLVM. Its like a lower level transpiler, else you could just make a higher level transpiler and use one of the dynamic C++ techniques... 
I only brought it up as it's interface is fairly nice. I didn't bring it up because it's fast. I use it mostly because it's minimalist and has a small surface area for implementing user scripts. I like it for many of the same reasons I like Catch. *shrug* 
I really, really liked angelscript. Where did it even go
Don't waste time and use the string builder approach: reserve beforehand and use append. 
&gt; I really, really liked angelscript. Where did it even go http://www.angelcode.com/angelscript/ ??? Last updated Dec/16/2017 according to the website.
I couldn't and didn't say that it would, I said that increased lua performance would benefit factorio.
As someone who started off with C#, would it be reasonably accurate to say that `unique_ptr` is equivalent to C#'s `using` keyword?
Exactly. It doesn't take much of a company to reach $1 million in revenue. I'd have a much harder time getting my company to shell out $539 per year for visual studio than I would getting them to shell out $199 for the first year, $159 for the second year, and $119 for following years of clion. And if I couldn't get them to shell out any money at all, I'd only have to pay ($90/71/53) for the individual clion license vs $539 a year for Visual Studio. 
Removed as duplicate.
This does not solve all cases though
I stand corrected. Thanks!
As /u/encyclopedist pointed out, I was wrong on that point. :) It's not necessarily a test suite suitable for compatibility testing between competing implementations, but it does exist. :)
I have it free too for being a student, but same as you, I wouldn't pay for it after I graduate
Breaking mods' scripts would affect the &gt;1 million players who might be using mods and not _just_ affect the mod devs, yeah?
&gt; We're currently using Lua 5.2 from 2015 with custom patches to make table iteration deterministic and to fix some massive performance problems. Our current version of Lua we use is already incompatible with virtually all Lua IDE/Debuggers because of the changes we've made so I have no problem maintaining my own version with my own tests. Fair 'nuff. We're in a similar boat at my workflow - Lua 5.2 with a homegrown IDE to support our special integrations and such. I still come from a "just replace it all with V8 and be done with it" stance though. :)
Putting a `std::string` directly in the value cell is fair deal. Making it bit-movable isn't free, but you'd use the move constructor for that, and and then on certain C++ runtime libraries you might wish to forgo the destructor call (where it does nothing much for such strings).
You can only use CLion if you have an SSD, because with an HDD it's so sloooow
They use Lua to support third party modding. 
https://blog.tartanllama.xyz/writing-a-linux-debugger-setup/ https://github.com/MattPD/cpplinks/blob/master/executables.md http://sourceware.org/gdb/onlinedocs/gdb/Remote-Protocol.html https://eli.thegreenplace.net/tag/debuggers https://github.com/llvm-mirror/lldb/tree/master/source from the QR Code at the end
Hah, I mentioned all this to our core tech lead here (he worked on custom Lua stuff back at Monolith and owns the scripting layer here too) and he's actually pretty excited by your idea of a new C++ Lua rewrite (it helps that he's a huge Factorio fan). Not that we would necessarily be able to use it in our project due to time/risk factors, but he already wanted a GitHub link if it was available so you might have your first contributor lined up. :)
Zdar, wow all these answers are incredibly unhelpful haha. Dealing with C if you are in modern elegant C++ mindset is very counterintuitive and frustrating. I know the pain. Would it really be such a straightforward job though? Developing Factorio really seems just as fun as playing it sometimes. 
I am starting a small project with a group of CS undergrads, and would like to set the codestyle. The project is expected to be large(?) but will be written and maintained by several different students. I wanted to tackle this issue by dividing the problem into several sub-tasks, each defined by an interface, implemented and delivered as a .so for the other projects to link. Since the projected are small, .h implementation is sufficient and a common include directory can be shared across projects. I got almost unanimous negative responses, is this such a bad idea?
thanks!
So actually you invent a new lua based language which includes everything which has been left out of lua on purpose?
Hey, I just wanted to let you know that I understand *perfectly* where you are coming from. I was the lead programmer for Starbound, which has an extensive Lua modding API, and interacting with the Lua C API has... an enormous impedance mismatch, to put it mildly. No, LuaJIT does not help, and not every platform allows you to JIT. It would take me a long time to put into words all of the issues I have with the Lua C API, but you've mentioned a lot of them. Stack manipulation incurs a lot of real overhead, and I find myself just wanting to hold onto actual internal pointers. It makes interacting with the garbage collector and holding values outside of the Lua state difficult, requiring constructs like the registry, which incurs even more overhead. Also, to the extent that you care about safety, Lua's error handling strategy brings in even more problems. I currently do most of my work in Rust, and making a Rust bindings layer for Lua that is safe has taken [far more work than it should](https://github.com/chucklefish/rlua), and the result is far slower than I would like. The Lua C API, ESPECIALLY with the whole __gc madness, will just longjmp on you at the drop of a hat! Also, you have to constantly call lua_checkstack, and (surprise!) luaL_xxxx calls will use stack space that they 1) don't document, and 2) require that you call lua_checkstack to protect against. I'm POSITIVE I could do a better job speed wise with rlua, but I question HOW much of a better job is possible, if you limit yourself to an actually safe API. I do, however, think that the Lua C API is ultimately very clever. Ultimately, it IS quite a lot simpler than other language's APIs, and I'm sure if you're writing nothing but C and the occasional setjmp / longjmp is not a problem for you, it's quite nice even. However, when you care about safety, or hit an optimization wall because of the overhead of the API, it starts to.. lose its charm somewhat. I haven't interacted with the Python C API in quite a while, but as I remember it, it was much more the style that you would want when binding a large complex API to it. You get pointers and manually inc / dec the reference count on them, and this allowed you to, with very low overhead, maintain handles to internal Python objects. Also, the error handling strategy is NOT based on setjmp / longjmp, but rather error codes and a global error state like unix errno. I'm ultimately not sure whether the fact that the Lua API is *C* is the problem, or whether it's the specific choices that went into it that are the problem. I could see, at least theoretically, that a C API in the style of Python's could be quite low overhead, if only because you can more or less take the C++ API you're describing (or any C++ API really) and, with great effort, write something that's the moral equivalent of it in C. If such a Lua interpreter existed, and had a C API, I might use it! (Full disclosure, I have actually seriously considered writing my own Lua interpreter in Rust for precisely the reasons you describe, and am STILL seriously considering it.)
&gt; Cherno Thank you for this suggestion. I've started watching it. In a few videos he explained so much so well! Definitely going to keep watching it and going through Bjarne's book. 
&gt; so I'm wondering whether you want to put a std::string directly in the value/cell. I do.
FYI, if you're using Windows 10 (as far as I know it might only be on version 1709) you can enable the Bash on Windows subsystem. This will let you use apt to install pretty much any native linux application (like GCC or eclipse), meaning you don't need Cygwin. If you get an X server like xming, you can even run X applications on Windows.
&gt; Have you ever considered to switch to some other programming language for mods? Or even create one Yes, but at this stage in Factorio development we can't realistically switch the script language mods use without the community crucifying us :P
We did the same exercise for our JSON library, it has a `std::string` in a union, and it works well. I'm mostly curious how the performance characteristics will be with a scripting language, compared to Lua, in particular when `std::string` is 3 or 4 pointers large.
That would definitely make C++ integration nicer. And as said [here](https://www.reddit.com/r/cpp/comments/7xohqu/lua_written_in_modern_c_14/dub4ng5/) I'd be quite interested in seeing the performance characteristics of this approach when used with a scripting language.
&gt; I still object to pointer tagging strategies as a philosophical matter. I object to retrofitting them onto architectures that don't natively support tagged pointers. In old-school architectures where every pointer had some explicit tag bits, they were awesome.
They are a lot of cools and important proposals * char8_t http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0482r1.html * Freestanding Proposal http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0829r1.html * wide_int http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0539r3.html * Text formatting http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0645r1.html * Maths constants http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0631r2.pdf * Contracts http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0542r3.html * Stacktrace http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0881r0.html * Timezone http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0355r4.html I'm however annoyed by [The 2D graphics proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0267r6.pdf) and [Direction for C++ - Section 5 ](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0939r0.pdf) which advocates for that 2D graphics library. An incredible amount of work went into making a library, that, at the end of the day won't be used. For the simple reason that its way limited that already existing solution. What's a 2D graphics library that can't display text ? And displaying text is a hard problem C++ notoriously lacks Unicode support. Focus should be put on Unicode rather than on displaying triangles. And yeah, improving teaching is a noble goal, but Qt and Qt creator is WYSIWYG at this point. And either you teach to young children and you won't use c++, or you teach in college and you take the time to tell them about libraries. In the same fashion, HTML is not important ( HTTP is ). Give me Unicode, a text manipulation library and i will manage with `HTML`. Take a long hard look at Qt and QML. there is a good reason why they don't do UI in c++ anymore. Give me the cake, not the cherry. Give me executors that handle co routines transparently like go, give me processes, give my dynamically loading libraries, give me Unicode, give me pattern matching, reflection, etc. Fundamentals things other people can build on and that can be used on most hardware by most industries. Let UI to projects that do that well and in which were poured thousands of man hours so that a high dpi rotated watch screen can display right to left Unicode collated text with color emojis. Stop pretends the standard needs UI., it needs a lot more thing a lot more. And if you thing it would be cool to have UI because there is no user friendly package manager, maybe work on that instead ? If we are willing to consider computers have touch displays, surely we can standardize some tools ? 
Hmm? And lose it's main purpose? A portable language, just compile it and works on *any* machine compliant to ANSI C. Quite useful for embedded systems...
Agreed, why even bother to rewrite an implementation if you could look for other scripting languages
It's a big shortcut to say that Qt doesn't do C++ UI anymore. QtWidgets are now matured compared to QML, that's why the changes are not so important in the latest releases. For me, QML is more focused on mobile / web UI, but if you want to build a solid desktop app with consistency with the OS guidelines and complex controls, widgets are still the way to go. Anyway, thanks for all the links you've posted!
From the paper &gt; We recommend • Use TSs for library components. • Don’t use TSs for a language feature unless the feature is a mostly self-contained unit. • Never use a TS simply to delay; it doesn’t simplify later decision making I wonder if the authors were thinking about coroutines and concepts? These were 2 major language features which got pushed into TS's and got their general use delayed by at least 3 years.
How likely is the committee to accept the backwards incompatibilities in the`char8_t` proposal? I'd really, really like to see this proposal make it, but fear the backwards incompatibilities will kill it. 
It is a common use of namespaces, whether you consider it a good mechanism or not. I think everyone using it knows that it's a convention, not a language feature. With modules, there is no need for such a convention, but all other uses of namespaces will continue, as far as I know. 
Thanks for the comment! It's nice to hear that someone agrees with me. I'm not sure exactly how modules will work in c++. I am under the impression that they will make the lives of those who write compilers much more difficult, but I'm really hoping that they will make developer's lives easier... But we'll just have to wait and see.
Huh. I've been developing with Qt for more than 10 years now, and this is the first I've heard of QDevelop. Bizarre. You have my apologies. 
That's fair - I vaguely remember on the DreamCast, they had the memory map set up so that effectively the high bits were in control of things like cache modes. So you could do stuff like int x = *addr; // cached read int t = *(addr + uncached_base_addr); // Same value, but bypass CPU cache Obviously a game console like that can be guaranteed not to get more RAM over the lifetime, so the vendor can allow funky bit interpretations. My philosophical complaints are mainly focused on stuffing functionality into "nobody is currently using this bit so I can get away with it right now" kinds of things that explode as soon as somebody installs more than X MB of RAM in their computer, or the OS decides to slightly rearrange an undocumented loader so things happen at different addresses that nobody should notice.
Is this 2D Sutter vanity bit mandatory? Or can it be standardized them deprecated after getting the public roasting it really seems to want?
Modules! I hear Google might be trying to make a new testing framework? (Not that it has to do with the Standards Committee directly, but for the love of God please. I don't like gtest much.)
Thanks, forgot mention it. It is Directions for ISO C++ http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0939r0.pdf 
Just do itttt..
Bots vs belts, eh? :) I think factorio community can accept **justified** change. Also, possibility of transpiling lua to something else always exists :)
Stroustrup is advocating for the 2g graphics proposal because he believes it will make it much easier to teach c++: https://www.youtube.com/watch?v=fX2W3nNjJIo 
Well I've seen the the 2D library proposal had a universal textExtents function and that's great if you ever had to deal with rendering text on multiple platforms. Rendering text tends to be quite painful in general, and having something well supported and with a good and clear documentation would be awesome.
They already have plenty of automatic testing for the whole application itself, and can probably set up some for commonly used mods to ensure the consistency.
Well, people have tried to fix JavaScript with TypeScript and other variants.
This isn't a real answer, but Jesse hasn't published to his blog or academically for a couple years now. His GitHub repositories have been mostly hands off for at least a year, though he still reviews requests. I don't think he has said anywhere on what he is currently working :-/ Maybe things got super busy at Esri if he is still there?
I'm working right now to get the travis tests back up and running. Figure I should do my part if I want it to stay/be current.
I believe there is a happy middle ground where Lua is still easy to use but the compiler can actually help with enforcing proper code quality.
This is a wonderful explanation, thank you very much!
really enjoyed this, thank you :) 
On the Seastar page, it reads "Proposals accepted starting on March 12, 2018" I think that they are not accepting applications until after that date.
The point is not to make a "new" language, nor change it, just leverage the infrastructure that already exists (LuaJIT is 1:1 with standard Lua, but its a version or two behind, but can be updated easily). Basically don't rebuild the whole wheel when you just need better tread.
I don't see why we need a language change when you can just as easily write a function std::string join (std::initializer_list&lt;std::string_view&gt;&gt;); that first measures all the string views, then reserves enough space, and finally concatenates them. I mean, ultimately you are going to need this function anyway, somewhere at the bottom of your proposed stack of template magic, so why not just go and call it directly? Also, the same question came up last week and the consensus was that it was basically not worth it as a general feature; that there are no data types other than string that benefit from this, and that a solution as described above is sufficient for strings. 
RemindMe! April 20th, 2018
I will be messaging you on [**2018-04-20 07:02:52 UTC**](http://www.wolframalpha.com/input/?i=2018-04-20 07:02:52 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/cpp/comments/7xg7y5/relearning_modern_c_or_give_up_and_go_to_python/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/cpp/comments/7xg7y5/relearning_modern_c_or_give_up_and_go_to_python/]%0A%0ARemindMe! April 20th, 2018) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Write a C++ wrapper around LuaJit if you can afford to spend the extra time. That is currently the approach I use in my C++ game engine. LuaJit performs really well. What is your current approach to embed Lua? I am a huge fan of Factorio. Keep up the awesome work!
No, it is just not public.
You checkout Agner Fogs information on http://www.agner.org/optimize/ As far as I as I know, you can get this information out of Intel's VTune as well.
Is there any project more serious than cldoc example which uses cldoc?
I have the same fears, but assuming c++20 fill finally be the next major upgrade we are all hoping for, I hope they will be more permissive with regard to breaking backward compatibility.
Why in the world would a paper being considered for inclusion in an international standard not be publicly accessible?
copypasting this subreddit?
[removed]
I agree with most of your rant, but setting up Qt in your project is not quite as simple as including a header and adding a library (thanks to moc &amp; friends) - but usually not much more difficult.
You're absolutely right. MOC is kind of a beast to deal with. I considered using gtk+ or wxWidgets as the example, but those don't come bundled with an IDE and working compiler tool-chain for windows, as far as I know.
Regarding the "no one will use it" point: I had a few embedded projects, where I would have very much enjoyed to have a standard, lightweight 2D api. Whether the cross-toolchains for those platforms would have implemented it is of course another question.
N4719 is the version of the modules TS approved for publication. ISO does not allow it to be made publicly available (after all, it needs to sell the TS...)
That paper doesn't add Unicode support, but just introduces a type to distinguish utf8 encoded strings.
What does Unicode support mean, then? What features is that spec missing?
Indeed, but for beginners, reading a compiler errors sometimes can be frustrating 
Nice try, Spectre exploit writer.
My favorite one: "P0896R0 Merging the Ranges TS"
&gt; Just ask yourself, when's std::auto_ptr going to be removed? Do you want to repeat that for 50+ pages, instead of 2 pages of standardese? Already is.
&gt; I'm however annoyed by The 2D graphics proposal and Direction for C++ - Section 5 which advocates for that 2D graphics library. and &gt; And if you thing it would be cool to have UI because there is no user friendly package manager, maybe work on that instead ? If we are willing to consider computers have touch displays, surely we can standardize some tools ? Standardisation of something happens because there is at least one person willing to lead out that thing being standardised. 2D graphics has not just one person willing to lead it out, but several. All the other stuff you want does not. Simple as that. If you, or anybody else, wants what is standardised to be different, lead by example. Stand up and lead out that feature's standardisation.
At least, a way to work with code points instead of 8 and 16bit values and easy translation from one Unicode encoding to another. One could argue that "true" Unicode support also needs things like normalization, but that would significantly bloat the standard library and is imho best left to 3rd party libraries.
lol
[Standardese](https://github.com/foonathan/standardese) looks similar but has active development.
+1 to this: 1) Pet project (make a game!) 2) Read books (check out the book guide on SO!) However, I actually disagree with picking dev enviornment while you're still starting. Building C++ projects and libraries is the worst part of C++ programming. I would suggest to just go with Visual Studio that does that part for you and somewhere during the summer (when you have more time) check out how builds work.
At first, this was mainly a prototype to see if it can be done. But then people wanted to use it, so why not.
I would. Used both and CLion is god awfully slow. Every alt+Enter makes me wait for God knows what. I am yet to see a wait in at Creator when doing contextual/refactoring actions.
And you don't need text ? buffering ? animations ? images ?
I totally agree on the set of vocabulary types, I would actually be in favor of that `&lt;geometry&gt;` is something that would be very useful and actually is doable because every body can agree on maths and maths don't need to reinvent themselves each time a new gadget comes about.
The book guide on StackOverflow is awesome. I tend to agree with you. I started with Visual Studio and XCode. It helps to have a mature IDE when you begin. It’s easy to forget the steps I took to get to my current workflow.
There is no such thing as simple graphics system if you want to handle things like text. `paint("🌸 Hello, World, my name is 桜 🌸");` should do the "right thing", and that already thousands of line of code. Are you really going to tell the Arab kids they write in the wrong way ? Or the Chinese kids that splitting a string is just too complicated if you language has more than 255 characters ? Are any teacher willing to explain that to display a cherry emoji you need to tile in `"\xF0\x9F\x8C\xB8"` ? And then there is high dpi, image format ( no gif ? electron has gif. etc), screen rotation, mouse, touch, keyboard input.... where do you stop ? 
&gt; (Full disclosure, I have actually seriously considered writing my own Lua interpreter in Rust for precisely the reasons you describe, and am STILL seriously considering it.) If you're working on new tech, why even take on the baggage of Lua? In addition to getting rid of crud like __gc you could make something with local scope by default, distinguish between hashtables and arrays, integers that aren't an afterthought, better error handling mechanism, change `~=` to `!=`, start arrays at zero, et. al. (Full disclosure, that's my pet project \^_\^)
Are you able to describe what your Lua workload looked like? At my new job there are vague rumblings of moving to LuaJIT, it's sort of just sitting there as a project for somebody to pick up. There are lots of interleaved callbacks between the C++ and Lua though, so I'm wondering if it would all be call overhead anyway.
Agree. Some people seem to be looking for auto-magic way to migrate all their header-based code to modules. I personally learned pretty early that magic does not work, not in the long run. To put it another way, the choice is not between "Modules TS" vs "Auto-Magic Modules" but "Modules TS" vs no modules. Or, perhaps, "Modules TS" vs "Auto-Magic Modules" that, because of the complexity, will take 10 years to beat into anything usable. In fact, "Modules TS" is already fairly complex and the jury is still out on how soon we will have a truly "usable in production" implementation (and I can tell you from actual experience, none of the existing ones currently are).
C++ has /enormous/ library support, just not as part of the standard library and not with a universal build system, etc. (projects like Conan are working toward that). However, languages like the ones we're discussing have central repositories and tools for automatically searching for and installing modules, and they typically have many, many of them.
Is there a reason to jump straight into C++ instead of starting with a neighboring easier language?
Aren't both coroutines and concepts "mostly self-contained"? If they're not, I don't know how to understand that bullet point.
You can access your CPU's Performance Monitoring Unit (PMU) output using performance counters; there's a couple of choices, recently discussed here: https://www.reddit.com/r/cpp/comments/7kurp6/recommended_c_tools_for_linux_profiler_static/drhpyfh/ One reasonably cross-OS^* choice is Processor Counter Monitor (PCM): "PCM works on Linux, Windows, Mac OS X, FreeBSD and DragonFlyBSD operating systems." ^* - cross-OS, but not cross-platform, since it's Intel-specific; there are cross-CPU projects (Performance Application Programming Interface (PAPI) - http://icl.cs.utk.edu/papi/, perfmon2 - https://sourceforge.net/projects/perfmon2/) for Linux, though. https://github.com/opcm/pcm More information also here (previous version, should be close enough): https://software.intel.com/en-us/articles/intel-performance-counter-monitor 
&gt; but still a little sad it doesn't include pattern matching, and/or language native sum/union types. It's the first "Concrete Suggestion": &gt; Pattern matching: List comprehension, and not just for algebraic types. This could simplify code and strengthen type safety. It would eliminate a lot of ad-hoc selection, eliminate unsafe use of unions, low-level use of variants, and eliminate the visitor pattern (which is an expensive workaround that confuses many) And then again later: &gt; However here are some areas of general concerns – often cutting across application domains – that we should not ignore: [...] Alternatives for error-prone and unsafe facilities (like std::variant as an alternative to unions or pattern matching as an alternative for std::variant)
There's also [P0955R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0955r0.pdf) by Bjarne, which fortunately strives against the support for macros.
No it's not included in the list of priorities for c++20, only as a medium term goal (3-10) years later on.
&gt; What's a 2D graphics library that can't display text ? The 2D Graphics Proposal has a placeholder section within it: "Chapter 10: Text rendering and display", stating that text rendering and matters related to it, such as font support, will be added at a later date.
&gt;But we'll, maybe we are at a point, where the only reason to use c++ at all is because we have to work on a legacy code base and we should pick another language for any new project. I want to believe C++ will be easy enough to use so starting new projects or even making small "scripts" will be feasible. I truely believe that a compiled language leveraging zero overhead abstractions can in fact become rival to those scripting languages out there. A good module system is the first step, an easy to use concept feature + natural syntax is the next step, then ranges, then add coroutines, networking together, and after all that we can think about adding reflection and generative features.
That... wasn't what you said tho? ::shrug::
I usually implement one liners directly in the class.
&gt; There is no such thing as simple graphics system if you want to handle things like text. There is no simple *any* system if you want to handle Unicode, period. Try doing Unicode I/O in the console on Windows. And we still use console I/O all the time. So its perfectly understandable if the graphics api messes that up too. I mean, the situation is no better in the rest of the standard library. &gt; And then there is high dpi, image format ( no gif ? electron has gif. etc), screen rotation, mouse, touch, keyboard input.... where do you stop ? No need to stop. Add as much as is feasible, and more in the next standard as things evolve. I mean, thats what we do with rest of the standard library. We are adding networking, filesystems, soon fibers and what not.
I started to learn RoR and JavaScript on my own but in college they teache you c++.
Hmmm... I'm not a fan of the new reflection by constexpr value. I really wish there were not a single `meta::type` that has some kind of compile time type erasure (which in my opinion is not needed) but kept the original design that generate a meta type for each type. With the new proposal you end up with since kind of strange object that is only usable in constexpr context, allow a bizarre syntax that reverses the type erasure (reverse reflection) and don't try to use them at runtime, because the reverse reflection operator cannot possibly work. It complicates reflection so much I cannot believe that! With the old "A design for static reflection" proposal, the reflection operator just simply yeilded a normal constexpr object. Just a plain object you could write yourself! But generated by the compiler on the fly to contain meta information. In fact, the compiler were not forced to generate the whole meta object, because it created no storage really, but only static constexpr members function and member types. This is lightweight, easy to wrap or mind around and on top of that, easy to use! Oh you tell me there was no ready to use reverse reflection operator in the old proposal? Oh come on! Since every meta objects were distinct types, it could have simply have a member alias or member reference to the reflected entity. The standard library could even had shipped a reverse reflection alias: template&lt;auto const&amp; meta&gt; using reflected = typename std::decay_t&lt;decltype(meta)&gt;::type; Then use that: std::reflected&lt;$int&gt; i; // i is int std::reflected&lt;$i.type()&gt; j; // j is int To make that work, simply make `$x.type()` return a reference to a global (generated) object. Think about that. It would be easy to implement the reverse reflection operator as a library utility. In two lines. Let the compiler never output the reflection objects to the binary unless ODR used. If you don't like the `$` operator, okay fine. But let the interface be heterogenous, but it's concept homogenous. All reflected type have the same concept, all reflected variable have the same concept, etc... The original proposal was elegant. Don't break it. Please. Sorry for the rambling here.
I totally feel you! If there was ever such a project I would fork it and make array indices start at 0. I would call it Lua++ then :-) So, DO IT!
&gt; Are you able to describe what your Lua workload looked like? Not well. :) Basically, a c++ webserver where some URLS are configured to have a Lua hook to customise some behavior, but I couldn't tell you what % of URL's or anything like that. (Presumably, because it's proprietary internal data that I wouldn't be encouraged to share, but also mainly because I just have no idea off the top of my head and I'd have do do some digging. :) ) Because it was a callback tied to a particular URL setup, it would be rare for a particular URL's callback to get super hot because after running, some other URL's would usually get accessed before it ran again. If any particular URL was accessed so often that it single-handedly got to be the majority of what was being served, it was probably just a request for a file that didn't have a special callback attached to it. &gt; There are lots of interleaved callbacks between the C++ and Lua though, so I'm wondering if it would all be call overhead anyway. Best guess is that JIT will show itself as a winner if you are CPU bound inside Lua, with some consistently run chunk of Lua that looks like an inner loop. If you are mostly marshalling data between c++ and Lua, and calling a bunch of different small callbacks, it may not be a huge win. But my intuition is always a less accurate measure than testing so /shrug The original port from mainline Lua to LuaJIT happened before my time at the company, but the port back to mainline Lua from LuaJIT was apparently surprisingly easy. I didn't do the work personally, but apparently the API's are similar enough that starting the project and having it running in production all happened within a single sprint.
About [p0824r1](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0824r1.html) "Summary of SG14 discussion on &lt;system_error&gt;". &gt; In any enumeration type E satisfying either is_error_code_enum_v&lt;E&gt; or is_error_condition_enum_v&lt;E&gt;, the enumerator value 0 must be set aside as a "success" value, and never allotted to any mode of failure. In fact, the enumeration E should have an enumerator success = 0 or none = 0 to ensure that this invariant is never broken accidentally by a maintainer. IMO this is simply wrong. `&lt;system_error&gt;` is designed to facilitate usage with any existing error enumerations (which can have any value reserved for errors and successes). Instead `make_error_code` should not make `error_code` with `value` 0 for enumerations representing errors, but "source" enum can have value 0 with different semantic from "success". `error_code.value` is never required (I think) to be equal to enumeration value. This is one of the reasons `make_error_code` exists and why equivalence is checked through categories and conditions. In other words `error_code.value` should be private state of category and if you ever check `error_code.value` outside of `error_condition` or `error_category` you doing it wrong.
Indeed the "reflection by constexpr value" solution requires a non-trivial change in how constexpr works. But it seems that the committee believes the benefit of this change will outweigh the complexity (by enabling more powerful and nice-looking metaprogramming).
I did not state anything about C++ hardness. Only that it has complicated type system that allows large expressiveness
True, but it was just given for comparison. 2 `const` still remain.
What is wrong with vTune? I've used a number of tools to drill down deep into performance issues, and vTune has always provided the best combination of ease and power. I've never used it on Windows so maybe there are quirks there I'm unaware of. Of course, it costs enough I'll never open my own wallet to use on a personal project, but it's been beneficial enough my employers all either already had a license or were happy to spring for one.
Tangential question, do you get to work on making these presentations at DRW or is this all in your own time?
https://i.imgur.com/Yno5MHQ.png :) If I don't give up on it in a week I'll push it to github when I have a general idea of where I want to go with it.
Extended Grapheme Clusters, not codepoints. Codepoints are almost as bad as working with bytes in a multi-byte encoding. 
Sure, that's one use case, but I also don't want the public visibility namespaces offer. 
Please let's _not_ make that happen. Here's why... Creating POD objects using malloc is both existing practice in countless places, as well as a vital part of the C-compatibility that C++ still strives for. Yes, the definition in the standard formally disallows it, but that should be considered _a bug in the standard_, rather than _a feature of the language_. To solve it we must fix the standard, not the language. Fixing the language, i.e. adding a method to 'bless' an object into existence, means that all software that acquires any kind of object from malloc, or from any C library, will be decreed to be UB until a programmer goes and adds `std::bless` in _every_ location where such is necessary. I believe the amount of affected software is likely to be very close to the full set of software written in C++. You cannot ask for a more massively breaking change than this. And what for, exactly? It all works fine as-is today. What we therefore need to do is not to add some kind of magical `std::bless` function that will compile to precisely zero assembly instructions but is somehow necessary for a program to not be considered UB. Instead we should fix the standard so that existing practice, as used in billions upon billions of lines of C++, is still considered valid. 
&gt; You misunderstood my example. The library doesn't have to directly support any image format. It just has to give me a way to put pixels on the screen. I can write the rest of the code to read a jpg file and display it. Here's the thing, I can already do everything in standard C++ currently, except the display part. This library solves that. You're going to write a jpg decoder yourself? Or are going to install one as a shared library? If you're willing to write or install a jpg decoder, why not a graphics lib?
The irony, is easier to code an Spectre exploit than code a profiling tool to get that information. 
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/7y01y0/need_a_book_recomendation/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
vTune's fine, even great. But it's not exactly cross-platform, except to Intel chips. And as you mention, it ain't cheap, whilst Linux perf and oProfile are excellent and free of cost.
vTune's fine, even great. But it's not exactly cross-platform, except to Intel chips. And as you mention, it ain't cheap, whilst Linux perf and oProfile are excellent and free of cost.
I'd prefer named parameters. Anything with more than 2 defaults quickly becomes unwieldy. f(~,~,~,7)
It is? Clearly I missed that, so I apologize. C++14? Or the newly released C++17? I'm really happy to hear it was removed officially. Thanks for letting me know.
I started with C++ way back in 3rd grade. I definitely didn’t know what I was doing at the time. You can’t really get anywhere useful until you start to grasp OOP design, patterns, and other software engineering principles. Up to that point I was just throwing away projects over and over, because everything was coupled all over the place, and I didn’t know how to successfully manage the complexity.
Sounds interesting to me. I partly use this (the git submodules part) but not as elaborate as he proposes it. Does anyone have experiences with this workflow?
`dynamic_cast` uses a string comparison?! Shouldn't it just walk some trees internal to the RTTI implementation?
True, I guess I was thinking of cross-platform as merely cross-OS. I guess that makes it glaringly obvious the variety of CPU architectures I've (not) done targeted optimizations for. 
I'm here since I was mentioned and just skimmed the video. If you want fast formatting you should note that branching isn't always a good idea; [a branchless variant](https://www.reddit.com/r/programming/comments/7ljzty/how_to_print_integers_really_fast_with_source_code/dro1umo/) can be almost as fast as a single branch miss. (NB: That linked code isn't up to date, nor is it optimised for 32 bit values.) Those misses won't show up in your benchmark unless you test effectively, but your production routine doesn't care.
The ability to ignore a value in structured binding is needed. Many languages with structured binding (or pattern matching) allow you to do that. C++ should follow the suite and not try being different in this area. I use structured binding quite a bit and every time I need just one value from std::pair, I ask myself, why should I name the second value if I never going to use it? My code could be cleaner if I could just ignore that second value. However, `f(~, 4)` is something completely different. It's not related to structured binding. It's not ignoring a function parameter, but using the default value. Indeed, named parameters would be a much better solution to this particular problem.
For me, the hilarious thing about your comment is that on my machine, for the non-ASCII characters in your example, they just show up as boxes for me, because my system doesn't have Unicode turned on. Which is an amazing example of why baking things into the standard isn't the right way to do it, just as you were trying to convey :-)
Thank you for explaining. I appreciate it.
Did you actually read the paper?
There is a link on the right side of this page, but if you google "definite c++ book guide" it will come up. Visual Studio is an IDE, while VS code is an editor. Compiling with g++ manually is cool for single-file programs, but once you start coding a project, it will be hard. You will probably have a couple of libraries that you depend on, which you will need to link. Also, you will have multiple source files and you need to compile and link all of them. Visual studio does building compiling, linking and running with one button press and there is also a built-in debugger. If you don't know what I am talking about, just start programming your video game project and once you google up a tutorial, it will contain all of this.
I sounds great in theory, and may even work for some projects, but in my experience, it's far from a solution to C++ dependency management woes. For one, it's hard to reach platform independence (which the article conveniently skips by more or less ignoring Windows), especially if you encounter a dependency with plain broken CMakeLists.txt that either has no install target, or worse, ignores the CMAKE_INSTALL_PREFIX and always tries to copy its binaries and headers to C:\\Program Files\\OhGodWhy. Why I'd ever want a library there, I don't know. Then there's the problem of compiler options. For header-only and dependencies with a pure C-API (no C++ data structures at the API boundary), that doesn't matter too much, but I remember pulling my hair out when dealing with a mess where one library used CMAKE_CXX_FLAGS that caused runtime errors when passing data to another library with another set of CMAKE_CXX_FLAGS. And neither of them had a mechanism to substitute my own flags that would have made everything work. In the end, I had to patch one of the dependencies' CMakeLists.txt as part of my build process. In short, CMake works great for managing simple dependencies. It can even work for more complex ones, if used correctly. Unfortunately, CMake makes it really easy to use it incorrectly, partly due to a whole bunch of outdated information on the web, partly due to its flexibility. 
You are using a structured binding to get only one element of a pair?? Why not just get it directly?
&gt; You're going to write a jpg decoder yourself? Maybe. Or a decoder for a custom image format. Or something else, like a graphing tool. The key thing is with this library I can display what I want on screen, earlier I couldn't do that. &gt; Great, so I'll have to install graphics related libraries on my non-embedded headless servers to support the compiler bundling a graphics library. Thats the best part, you don't have to do anything extra. It comes built in with the rest of the stdlib. You already install the components in the stdlib you don't use, so nothing changes. &gt; It's way too hard to download a single installer and click go? It's the same level of complexity as acquiring the compiler to begin with, just more of the same. I use VS. Configuring QT to work with VS is one more extra step. And what if my choice is GTK, or wxWindows, or whatever else? Why go that extra step? &gt; What constitutes a book for beginners? I learned C++, starting with 6 months of C98 experience, by jumping into the deepend on Qt graphical application development at work. I did that without a book, just the online documentation and trial and error. Good for you. Don't assume thats a good fit for someone else. &gt; I agree, why are we including networking or filesystems? I'd rather not see those in the same document as the language standard either. I'd much rather see those be separate standards, even published by the same ISO working group, but not with the same force as the official language and standard library standard. Thank god Stroustrup or the rest of the committee doesn't think like you do. &gt; No page limit, of course. I seriously disagree that a brief standard is not desirable. I really want a brief standard. I want a featureful standard. C++ is probably the wrong language if you want brief anything. &gt; Same document, same people doing the work. So technically you're correct, but in actual outcome, I don't see a difference. In most cases, the implementers for language and library features are different. The committee even has different groups for language and library changes. &gt; And on what platform do you have a compiler installed where you don't already have access to one or more graphics library? &gt; Visual Studio comes with numerous choices for graphics None of them cross platform or standard C++. &gt; The amount of work it takes Windows and Mac to get a cross platform compiler + IDE + graphics API is less than the amount of work it takes to get the official compiler and IDE from the OS vendor (Visual Studio vs. XCode). Not everybody wants your choice of IDE or compiler or graphics api. &gt; What's next, we'll include a standard IDE in the language's standard library? Will your choice be emacs, or vi? Neither are IDEs. 
But then implementers won't be able to implement their mythical optimizations and therefore (and more importantly) no one would be able to fix bugs by blessing memory! And yeah - they propose to treat malloc and memcpy as markers for object creation.
I use them for everything that represents an index or a quantity of things. In fact, anything that isn't logical to have a negative amount. There are sometimes APIs that takes signed ints as parameter, then I usually use signed ints in that case, even if negative don't quite make sense.
I use execute_process to run a .cmake file at configure time, containing an External_project add command, which downloads and installs the dependency. The dependency doesn't need to be a cmake project. This way is portable and less error prone but more verbose. I'm on mobile but you can see examples here https://github.com/DavidAce/DMRG/tree/master/cmake
&gt; stl containers using unsigned indices is considered a mistake Wait what? STL implementations of [] take the unsigned size_t type.
Might have replied to the wrong comment, woops 
Which is unsigned and generally considered to be a mistake.
He means historically. The LEWG group wishes they could go back and change that decision.
Yeah sorry that's what I meant. Long term all features are included :)
It makes for loops much cleaner if you only need the key or value. for (const auto&amp; [_, value] : map)
Use signed integers if you plan on doing extensive arithmetic operations on the values (e.g. representing time, price, quantity) even if the values can't logically be negative. You'll then be able to do things like timeC = max(0, timeA-timeB) without worrying about edge cases. Use unsigned integers if you don't plan on doing arithmetic operations on the values, for example for unique identifiers.
I have never run the profiler on any code, I'm ALWAYS surprised where the time bottle neck turns out to be. The renders all time spent on discussions of "performance" a waste of time. If want fast code - always profile.
Good point. My idea was to reuse a concept, but you're right that the semantics are very different and it might be counterproductive to use the same syntax. 
Let's put it like that: All those features would have been nice, but not necessary. Text via bitmap fonts was sufficient and I believe text rendering will certainly be added to the standard sooner or later, but we have to start somewhere.
Ah, interesting. But why? Won't arr[-2] crash your program just as much as overflow to arr[4294967294] would?
Yes, but the concern was that GUI drains ressources that would better be invested in e.g. Unicode support and other than what /u/Ayjayz implied, Unicode support is far from done if and when that paper gets accepted.
In case it helps you: I've put up bindings for LuaJIT 2.0.5 which supports Lua 5.1 and generated the Rust bindings (https://github.com/fschutt/rlua/tree/luajit - https://crates.io/crates/lua-jit-sys). Then I forked rlua and exchanged the interpreted Lua for the JIT-compile Lua (https://github.com/fschutt/rlua/tree/luajit). Now I had the problem that rlua expects Lua 5.3, not 5.1. So I ported some functions to Lua 5.3 (https://github.com/fschutt/rlua/blob/luajit/src/jit_compat_51.rs). The commented-out parts are not yet ported, because for my needs (a configuration script), interpreted Lua is OK. I recently made a small JIT interpreter that can interpret Rust code (well, one simple function), as a test (https://github.com/fschutt/gsr-jit). It is way harder than I expected to make a JIT, even if you already have a parser. You find yourself solving stuff like better register allocation, basic memory allocation, assembly optimization (this is fairly huge). So I can only advise against writing your own JIT. 
Qt is the only Library I use that requires me to change my build process. Everything else litteraly just requires adding include directory and lib binary or just a `target_link_library...`&gt; Can you give an example?
Only the OP knows, but sometimes people need that sort of low level detail when developing for games consoles or mobile phones from Windows. I've certainly spent a week tuning a FFT for a very specific ARM chip in the past, one we knew most of the customer base would be using.
&gt; Maybe. Or a decoder for a custom image format. Or something else, like a graphing tool. The key thing is with this library I can display what I want on screen, earlier I couldn't do that. Nothing's stopping you from doing any of that now, except that in your hypothetical example you're willing to write a custom decoder for an image format but not download, configure, and install a graphics library from one of the dozen or so highly popular implementations of that concept. There's nothing stopping someone from doing the things you describe. Including a graphics library in the standard doesn't enable new capabilities for the language, it only adds yet-another-graphics-library to the existing ecosystem of graphics libraries. Only this one we'll *all* be stuck with, bugs and all, for a very long time. &gt; Thats the best part, you don't have to do anything extra. It comes built in with the rest of the stdlib. You already install the components in the stdlib you don't use, so nothing changes. Sorry, I think you misunderstand my complaint for this quote. How can the compiler have a graphics library, if the system it's installed on has no display? For a Mac OS or Windows machine, one could potentially argue that the compiler can simply assume the existence of various userland libraries that come with the OS. Of course, Windows now has "Server Core" which somewhat violates that assumption. For a Linux, BSD, Solaris, or QNX machine you have no such convention. What underlying mechanism is this graphics library going to use to draw pixels to some display device? I certainly hope they're planning to use operating system facilities to do so. * Windows, it'll use the win32 api somehow. * Mac, it'll use Cocoa, right? * Linux? Are we going to draw directly onto the frame-buffer? Nope, that breaks XWindow's network transparency. So we implement XWindows, right? Nope, you're forgetting Wayland. Ok, so we implement for Wayland, nope we're forgetting about NCurses. * What about QNX? As far as I recall from when I did some work with it, they don't use XWindows, and have their own display technology. * Android? Surface flinger. Their own beast. * Raspberry Pi's official debian version had some experimental direct framebuffer drivers for a while way back when. If they *aren't* going to use some kind of operating system abstraction layer that makes use of the underlying host's existing support, what are they going to do? Implement all of the logic to interface with directly with the graphics card, using the lowest level API available on the platform? Not likely! You'd end up having AMD and NVidia fighting over which specific version of which specific API that they'll each support in their driver. So, it's safe to assume that any kind of graphics support is going to, inherently, require interfacing with other parts of the computing environment that we install on. That means that in order to install gcc, I'm going to have to install at least one thing out side of gcc. Maybe GCC's implementation will provide a stub "do-nothing" implementation, so that's not the case. But that's not very useful, is it? So more likely in my opinion is that any implementation of this graphics API will require additional libraries, ones that aren't provided by the compiler implementer, to be available on the machine. Roughly 3/4ths of my computers are display-less Linux machines, but they have compilers installed so I can build packages on them. These machines have no graphics capabilities (many of them are VMs without anything but a virtual serial terminal). I object to any world where installing a compiler requires that I install XWindows. That's just insanity to even get close to that possibility. &gt; I use VS. Configuring QT to work with VS is one more extra step. And what if my choice is GTK, or wxWindows, or whatever else? Why go that extra step? Then you already have an existing graphics library that comes with visual studio. Use that? Exactly! Why have everyone install an under-capable graphics library that no one currently uses? Just because it's in the standard doesn't mean it'll be very good. I won't be using it, I prefer Qt, GTK, wxWindows, etc. Why force me to install it? I find it very unlikely that those other graphics libraries will make use of the one in the standard. Qt already has 3d everything, with graphics card acceleration. Switching to a simple 2d library is a step backwards for them. GTK is implemented in C. Isn't wxWidgets implemented in C? &gt; Good for you. Don't assume thats a good fit for someone else. Sure, so why assume that a complex, 100+ page graphics library proposal is a good fit for someone else? The resources for teaching beginners are available anywhere you want to look. If they truly are not available, there's good money to be made selling a "Getting started with C++ w/ Graphics" textbook, complete with a 1-click install for your graphics library of choice, plus your IDE of choice, for platforms. I know my Alma Mater would be willing to buy potentially 100 or more licenses per academic year. We don't have to involve every single c++ developer in that process by putting the graphics library de jure into the standard itself. I'm a c++ professional, and I'm saying it's not a fit for my needs, and that adding this graphics library to the STL will force me to spend extra effort removing it from my companies version of our compiler in the future. We don't want it. We don't want it so much we'll delete it from our STL when we upgrade our compiler. Please stop making me do extra work. &gt; Thank god Stroustrup or the rest of the committee doesn't think like you do. I may be the one objecting here, but I'm not the only one. I think they're taking us in the wrong direction, and I hope they stop it. I'm sad you disagree. &gt; I want a featureful standard. C++ is probably the wrong language if you want brief anything. I want a capable and expressive language. That doesn't necessarily mean featureful. The more stuff we put into the standards document, the more difficult it becomes for compiler venders to actually implement. A more capable and expressive a language is, the more we can do that would have been impossible without. Adding stuff to the standard library does not do this. By definition, things in the library are things that can be implemented without the language standard being involved. &gt; In most cases, the implementers for language and library features are different. The committee even has different groups for language and library changes. No real reply to this. &gt; None of them cross platform or standard C++. So...? There are significant advantages to using platform specific APIs. Better platform integration, less runtime overhead, less programmer effort to get the same or better features. If you want a cross platform graphics library, go get one. They exist, they're free and open source, and they come with additional runtime overhead, and less seemless platform integration. Choose your compromise. &gt; Not everybody wants your choice of IDE or compiler or graphics api. And not everybody wants to be forced to have a graphics api bundled with their compiler. That was my point. If you don't want the C++ standards committee in 2030 to say "Gee, you know, we really should make sure there's an IDE available in the standard, because it's really difficult for beginners to get started with C++, since getting an IDE is so difficult, and no book teaches a standard IDE." Then lets not put a graphics framework in the standard? &gt; Neither are IDEs. Do you mean IDE's aren't cross platform? Or that not everyone wants someone elses choice of IDE, compiler, or graphics API? If you mean IDEs aren't cross platform, then neither are most graphics APIs. That doesn't stop someone from making them work in a cross platform way with proper abstractions. And the majority of IDEs available *are* cross platform. It's just Visual Studio and XCode, I think, that are the major non-cross-platform ones. If, instead, you mean that not everyone wants someone elses choice of IDE... yea, that's my whole point. I actively am putting in effort to inform you that I not only don't want your choice of graphics API, but if you put it into the standard so that the compilers I commonly use for work start including their implementation of that API, I'll put in work to remove that API from my system. It's so much not my choice that my boss is going to tell me to put in man-hours to remove it before we deploy it to the rest of our developers. We're not going to do that for filesystems, because while philosophically we don't like the idea, we see the point of it, since 90%+ of all platforms in existence have filesystems that work roughly the same, so it's fine. We probably won't remove the networking stuff, for basically the same reason, though we might. We have our own networking code that won't work with the one in the standard. Might be better to chuck it so that an intern doesn't use it by mistake. But we'll be removing a graphics API. It's actively harmful to the way we deploy things to include that much extra stuff.
On the contrary. Things that are hard should be in the standard, so that it a) only has to be implemented by a few experts and then can b) easily be used by others.
Yep, but the reasoning is actually performance and causing unsigned to infest the rest of the code base. Chandler Carruth covers it a bit here: https://www.reddit.com/r/cpp/comments/567ckz/cppcon_2016_chandler_carruth_garbage_in_garbage/ IDK where it is in the video, but I think it was there. 
This isn't really an excuse for Qt, because the MOC is horrible, but one way to work around needing to change your build process is: https://www.reddit.com/r/cpp/comments/7x94jk/verdigris_a_pure_c14_replacement_for_qt_moc_has/
If you don't want negatives you also typically don't want wraparounds. So using unsigned hasn't won you anything except being less likely to detect it. Unsigned ints model modular arithmetic, not non-negative integers.
Thanks for the detailed answer, problem is static_casts can be unavoidable if some 3rd party lib needs signed parameters. More problematic than 3rd party libs, my own libraries like math/utility stuff that I wrote in the past often lacks overloads for unsigned values and it would be time consuming to correct all of these, @whichton's answer with core guidelines convinced me: i am getting rid of unsigned values for good, no matter if I don't want negative values.
Functional unicode rendering, brought to you buy the makers of auto_ptr. Done better by a small group of experts than a company with large profit motive to sell an operating system to billions of non-ASCII character set users. The same experts who have "char", "char16_t", "char32_t", "char64_t", "wchar_t", but until the proposal listed in the OPs link, no "char8_t" ???? I'm not implying that the standards committee are not members of the best and brightest that the computing community has to offer. I am trying to convey that even the best of us get it wrong sometimes. Less is more. We can always acquire unicode handling from somewhere other than the standard library, just like we do today.
It'd only require code changes if you had an existing codebase already using Qt. If you're already using Qt, then Verdigris wouldn't help you. If you assumed a project that had no graphics handling at all, the Verdigris project would allow you to use the Qt framework without adding MOC as a step to the build.
Deprecated in C++11, removed in C++17.
Thanks!
I kind of got carried away, honestly. My response makes me seem like an *unsigned int evangelist*. I think your response to avoiding them is warranted; they do make your life as a developer a little more difficult. There are times when you don't want to use them, and there are others when they are just a real pain...
I think it is like std::stringstream: it could be fast -&gt; however nobody caring about speed uses it because performance sucks -&gt; no reason to optimize it. 
What you said makes a lot of sense (re: situations where LuaJIT would be a big win). I really appreciate you replying, thank you. Measuring is everything, but of course there's some time investment in doing the measurement (especially in a big old codebase where not everyone is neasuring that part frequently) so hearing your experience is really helpful. Especially since it sounds like our Lua usage patterns might not be that dissimilar. Cheers!
Is this a jab at Windows though? I don't know either where platforms I don't use put their stuff.
I just downloaded the Lua code from [here](https://www.lua.org/download.html), and basically search and replaced a bunch of stuff, and it just kinda worked, no errors not even warnings.. That was really surprising to me, here's a [Github link](https://github.com/dextercd/luacpp) so you can see what I did. Only `src/Makefile` has some manual adjustments.
I do enjoy VSCode.... They just need to get the SymbolTree working (which I think has been on their TODO list for a while now). And I mean a real, language-aware symbol tree. My fingers are crossed!
There are multiple reasons, at various levels: - *Developer*: `-1` is obviously a bad index, while `11` is maybe right? - *Developer*: Underflow/Overflow on signed integers occur for really large values (in magnitude) which are likely far away from the normal range; compare to unsigned values, where Underflow occurs at 0, the most used value ever (this ties back to the first point). - *Optimizer*: by presuming that no overflow occurs, some optimizations enabled by range-value-propagation analysis can be applied; of course, the pendant is that overflow is UB. - *Linter/Checker*: if static analysis/run-time instrumentation observes an underflow/overflow on a signed integer, it's 99% sure to be a bug; if it observes an underflow/overflow on an unsigned integer, it may be intentional and will likely NOT be reported. In short, I'd advise using unsigned integers *only* when you want modular arithmetic.
That concern is based on an assumption that the people working on the GUI proposal would volunteer to work on something else. The committee doesn't work that way; it is comprised of volunteers that work on what they are passionate about. And yes, good Unicode support will require a ton of work. We're just scratching the surface right now. `char8_t` is just a fix for a fundamental issue that, in my opinion, hobbles good designs for Unicode support.
Depends on what you are doing and with normalization (which is already extremely difficult) it gets you pretty far. Unless you do actual Textprocessing you usually don't need to interpret more than ascii delimiters, check for equality and have some consistent ordering. But as I said: &gt; At least 
I'm a Windows user and I don't even know where Windows users put their stuff. I just dump everything in a folder C:\cpp In Linux there are clear conventions for where dependencies should be placed.
My understanding is that it's really hard for ABIs to agree on RTTI implementations and comparison between them...but everyone can agree on "does this string match this other string".
They put their stuff the same place as on Linux - under some common location, which with CMake is accessed via the CMAKE_PREFIX_PATH. The only difference is, on Linux that place is defined for you by the platform, whereas on Windows you define it yourself and you put your dependencies compiled from source there. And once you realise that CMAKE_PREFIX_PATH is not a constant and doesn't *have* to be /usr/local/ or whatever, then even on Linux you can define alternative locations for your dependencies. Perhaps you don't have root access. Or perhaps you think it's actually a bit daft to get sources for a library not included with your distro, install it system-wide with root permissions even though you're the only user of the system using it, and then only to statically link it into your executable. And regardless of any of the above, you really shouldn't be putting platform-specific paths in a cross-platform CMake build script anyway. 
Yes and no. Afaik at least some of the people involved e.g. were also concerned with more efficient data structures and at some point this also becomes an editorial and implementation issue. Bjarne also wrote a paper talking about the importance of prioritizing certain features.
As the author, I also have those fears :) The good news is, you can help! I've been planning to explore useful backward compatibility options, but time has been limited. I'd love for anyone with time to build the gcc prototype implementation [1], run it against some real code, and report back with what kinds of things break (file issues against the github repo). Searching github code for 'u8' [2] looks to be a reasonable way of identifying open source code bases that use UTF-8 literals. [1]: 'char8_t' branch at https://github.com/tahonermann/gcc/tree/char8_t [2]: https://github.com/search?l=&amp;q=u8+language%3AC%2B%2B&amp;ref=advsearch&amp;type=Code
You're probably right actually...I'm always careful to build my whole codebase with the same compiler &amp; settings... Any expert able to chime in and explain why strings are used?
Man, it reminds me a lot of routing for speedrunning.
So, while this is now working, I'm curious if there is a better solution, which does not involve to calculate the position of each letter.
That is what cquery is for...
this seems fine. `create_font` shouldn't `else-if` though. also, I suggest you don't expose that `operator T`.
This is the jab: &gt;And don’t get me started on Windows. I cannot understand how people can program there.
Fine. I don't care. My point is I won't insult you for it.
The module system is about exposing names to other translation units. It will be part of your build system. It's probably not part of starting small projects or scripts. A nice package system would be good for that. But C++ across interfaces generally requires everything to be built together, and that's hard. 
First, why not use [ExternalProject](https://cmake.org/cmake/help/latest/module/ExternalProject.html)? It does something the article do by hand in several steps, and allows for a bit more choice (i.e. not only Git). Second, if you only depend on libraries that can be built with CMake, you might as well go and use [Hunter](https://github.com/ruslo/hunter) package manager. I had some good experience with it, building dependencies for 4 platforms (Windows, MacOS X, Android, iOS). Very nice. There is also [conan.io](http://docs.conan.io/en/latest/integrations/cmake.html)([list of packages](https://bintray.com/conan/conan-center)\), which I haven't tried, but it seems it can download binaries if they are available (Hunter always builds everything which makes the first build quite long, especially for multi-arch platforms such as iOS and Android).
I think the issue is that `if (ec)` is used as "did an error happen?" while it is implemented as "is error code non-zero". Thus, you should reserve 0.
Why would you needlessly complicate it like that instead of just: typedef uint8_t u8; enum class font_attribute : u8 { italic, bold }; struct font { font_attribute attr; font (font_attribute attr) {} bool italic () const { return attr &amp; font_attribute::italic; } }; font_attribute operator| (font_attribute x, font_attribute y) { return static_cast&lt;font_attribute&gt;(u8(x) | u8(y)); } int main (int, char **) { auto x = font (font_attribute::bold | font_attribute::italic); return 0; }
Bitflags are essentially a way to implement small sets with usual set-theoretic operations. Your implementation is missing a lot of that, though i see no reason those couldn't be added, and not needing to repeat enum type name to specify multiple flags if pretty neat. Downsides are that you have to tediously declare all the accessors and it can't be used in `switch` cases or template value parameters.
Your approach is [widely known](https://softwareengineering.stackexchange.com/a/204566). What I wrote was just an idea, I'm not using it in production at all.
The advantage of having a single type to represent a reflected entity is that we can then use standard containers and normal functions to manipulate them, instead of being stuck with tuples and function templates. This, in turn, offers a significant simplification in the way the new proposal will be used over the previous proposal. With the new proposal, you will be able to basically write normal C++ code that is evaluated at compile-time to perform type transformations, and only use `unreflexpr` at the few boundaries where you need to go back into type-world.
You got the point. Indeed, my first motivation was to write this: create_font( decoration().bold().italic().underline() ); Instead of this: create_font( font_attribute::bold | font_attribute::italic | font_attribute::underline );
Thanks for your response Louis! I'll try to investigate more about this and see what's going on there, I'm still skeptical about that as it introduces many new things, but also curious as why all of this is needed. It's kinda strange to me that we need to hide stuff trough type erasure even for stuff that the compiler already knows about at compile time.
I really really like languages with arrays that can be indexed from the end with negative numbers.
You don't _link_ headers btw.
One place where unsigned integers shine is bit fields. When you need every bit so that you can do struct packing to make sure alignment and / or atomics will work, you don't want to waste bits on making numbers signed. 
It's even worse than that. Most `dynamic_cast` implementations are wrong and binsearch the object file's header which is very likely at least one TLB miss away from the current code location. See [Arthur's](https://www.youtube.com/watch?v=QzJL-8WbpuU) talk for details.
i don't think this would scale well when you have lots of flag values. each new value needs both a foo() and has_foo(), and the integer literal for that flag is duplicated in each. that makes it prone to copy/paste bugs that either result in foo() and has_foo() using different values, or reusing the value of another flag. the enum approach is simpler because it only needs one definition for each also, what about clearing bits? your example could never un-bold something. you could add a un_foo() for each value, but that makes the solution even more verbose
I assumed it was just the author admitting he was a bit clueless? It's a nice contrast to the all-to-common nerd bragadoccio: somebody who's acutely aware of the significant limitations in their abilities and is unafraid to admit them publicly.
Sounds about right. It doesn't help that any library that actually requires heavy use of something like `dynamic_cast` will always relent and find a way to code up a faster version of the same (e.g. Qt).
The perceived benefit from the optimizer doesn't actually exist, though. The problem with this type of code is the use of an offset size that does not match the pointer size, thus causing the compiler to perceive any possible iteration as (potentially) being over only a subset of the array. It is true that UB can be used as an excuse to simply ignore the fact that a subset was specified, but specifying an offset that is of the correct size allows even better optimisation to be applied. I'm not going to copy the whole answer here, but check my (long) reply here: https://www.reddit.com/r/cpp/comments/7w6vu3/video_arvid_norberg_integers_in_c/ 
But what if I'm putting a lot of headers in a program? How do I link all the headers in a package to be used by a program?
As /u/Aistar has already said, ExternalProject will do this in a more standardized and maintainable way. This approach would quickly become untenable for larger projects, where Conan becomes much more appealing. It's also considered bad practice to put package find behavior in CMakeLists.txt - it should be in custom FindXXX.cmake modules if you insist on doing things this way. 
Thank you! Makes perfect sense!
For those interested here are some implementations for itoa using various techniques: https://gist.github.com/anonymous/d89506220647db47f42dedd2674dfb1b 
Thats basically how cget works to create a local environment.
&gt; always tries to copy its binaries and headers to C:\Program Files\OhGodWhy. This can happen anywhere, even with pip and python setuptools. Issues like this should be considered a bug, and it shouldn't be the package manager's responsibility to fix these problems(which I commonly see happen unfortunately).
I add every dependency as a submodule and have them built with CMake. I don't bother with the other stuff though: * no find anything - the repo's copy is always built, even if you have another copy of it installed globally. No point having the dependency in the repo if you don't use it * no submodule update - this is something you should do yourself, in my view. It's not for the build process to do * no install step - where do you install it to? Just use the repo's copy... I also have each dependency as a full-on non-INTERFACE cmake target with source files list and everything, so it gets built as part of the build process. (When I do a Debug build, I then get a Debug build of the dependency.) Sometimes I have to write a CMakeLists.txt to make this work... but that's fine. It's a one-time cost. The main goal for me is that when I load the Visual Studio solution or Xcode project, I want to see source files for *everything*, so that code browsing and code completion works as best it can. And on all platforms I want symbols for everything so I can step through all of it in the debugger... Additionally, if I need to add some debugging stuff into the dependencies' code this is also very easy - the source code is all there, and I can just modify it and it will get built on the next build. 
Return value optimization would be my guess.
Yes, but not necessarily quite incompatible enough to 100% guarantee they won't appear to link and then explode at run time. I mean, if you try to mix MSVC and gcc, the ABI's will be different enough to be a non-starter. But if you try to mix slightly different versions of a library that are almost but not quite ABI compatible, some things may work while others explode. the definition of "ABI compatible" is a large topic.
I'm no expert but it doesn't make sense to me either. The c++ versions copy the 64-byte result directly back to the caller's buffer using an unaligned move. The C versions copy the result to the stack, then copy it to the caller's buffer in smaller (32 or 16 byte) chunks using unaligned moves.
The reasons why Lua's API are the way it is are pretty much why you guessed. Not exposing pointers to C is primarily about garbage collection and secondarily about isolating C from Lua. Lua doesn't use reference counting so it cannot use a system similar to the one used by Python and Perl. (switching the original GC to reference counting could also have a negative performance impact, since reference counting has nontrivial overhead while the program is running). And if you don't want to have reference counting things start to look less apealing. One option is to have the GC inspect the C stack like Ruby does but this will result in a conservative GC and would also need nonportable assembly code. Another option would be to have multiple different reference types, like Java's native interface but that adds a lot of complexity as well. As for the stack manipulation, I think the main reason for that it avoids needing to store function arguments and result lists in garbage-collected tables. I agree with you that lua_checkstack is super annoying though... As for the problem of speed, one avenue I am currently exploring is to have a third language to sit between Lua and C. The programmer never gets to manipulate Lua's internals directly (which is good for simplicity, safety and version compatibility) but the compiler is allowed to generate code that does that (which can be good for speed). But at this point it is still in very early prototype stage...
there's a good sub for questions like this at /r/cpp_questions/
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7y2hq8/how_do_i_link_headers_that_are_not_in_usrinclude/dudd4go/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
One thing to be aware of is that this test suite is made to stress test the PUC-Lua implementation and isn't particularly designed to ensure that a separate Lua implementation conforms to the official specification. To do that we will need a more comprehensive test suite, which is an idea that has been floating around in the Lua community for a while but that noone has actually jumped ahead to do so yet.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/7y3kzg/c_largest_of_three_numbers_cant_print_string/dudd8z1/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Yeah, I think it's just the same normal C behavior that causes C programmers to "know" that large structures cannot be returned by value and instead must be returned via an out pointer passed as a param. If I implement the OP's example with a C-style scheme then the output is about as short: clang: https://godbolt.org/g/rf2wvL gcc: https://godbolt.org/g/nMrX1P Note that both compilers don't even require that I mark the result pointer with `restrict`.
Between you and raevnos, that seems to make a lot of sense. The named RVO means C++ is allowed to place that variable directly into that buffer right from the start. In C, depending on where the return value goes, that could result in naughty side-effects that might be visible to other threads while the function is still running, or to the same thread if the function exits some other way than that `return' (the only possibilities I can think of right now is if that function terminates its own thread, or if it calls `longjmp'). The last time I used C, if you wanted to refer into your callers object inside a function, you explicitly passed a pointer - RVO doesn't do anything you can't already do, it just makes it implicit. I like RVO a lot, but I also understand why C might not want it. The not-an-expert warning still applies - I have some idea what that object code means, but without checking what all the suffixes on those vector move instructions indicate, I wouldn't have a clue without your explanations. 
This is called [Copy Ellision](http://en.cppreference.com/w/cpp/language/copy_elision), or RVO (Return Value Optimisation). You can have exactly the same code between the two if you disable this, by putting -fno-elide-constructors in the C++ compile flags.
&gt; naughty side-effects that might be visible to other threads while the function is still running C doesn't make any guarantees that this won't happen. And even if it did, the C++ code is better; the result is copied to the destination with 1 instruction. The C compilers copy it with 2 or 4. And aliasing isn't in play since both input buffers are completely read before the output buffer is written. So even if the output aliases an input, the C++ version is safe.
Thanks for the clarification! This indeed seems to be it.
I thought of this initially but don't avx operands have to be aligned? How is it ensured that the pointer is aligned or is there no alignment nesscary in this case?
Why can't C compiler do RVO?
vmovdqu32 means "*Unaligned* load of 512-bit scalar integer"
Look at the -O0 output for both. They're doing _slightly_ different things, mostly due to C++'s copy-elision rules I'd wager. The C version is allocating stack space for the `res` variable in the function prefix and then `memcpy`ing it on return in the function suffix. The C++ version expects the caller to pre-allocate the space, in which case the C++ version just uses that space and has no need to copy anything on return. Additionally, it looks to me like the C++ version is making assumptions about the alignment of `res` that the C version is unable to make. Effectively, this is a difference in ABI or calling convention between C and C++ on the target platform. This is (one of) the reasons that you have to `extern "C"` definitions of C functions in C++ code: not only is name mangling disabled, but the default calling conventions on the platform may differ wildly between C and C++ code. I played around with some of the [GCC function attributes](https://gcc.gnu.org/onlinedocs/gcc/x86-Function-Attributes.html#x86-Function-Attributes) for calling conventions but couldn't convince either GCC or Clang to do return copy elision for the C version.
I wanna second Hunter. The documentation was a little hard for me to understand, and i still havent really figured out how to add custom packages, but otherwise it's just fantastic. It takes the submodule workflow and makes it better be effectively namespacing all the dependencies so you dont ever have target name collisions. It also makes changing dependency versions (or other compilation options) a breeze and lets you feed in a toolchain files through your top level project which will automatically propagate through all the dependency (debug/release/static/dynamic/compiler-versions/target-platforms all can be changed instantly)
Ah I see, thanks for the clarification.
Afaik, c and c++ have usually the same calling conventions
Search using which string equivalence rules? (and where do GCs even come up?) Trim what to length, measured in what units? Cutting for what purpose? There is nothing special about GCs, they are just of of the several meaningful groupings of code points used by Unicode.
&gt; There are no side effects to avoid. Are you sure? You think the compiler writers are just lazy and don't know the language as well as you?
I'm going to argue a point where I'm not confident at all in the hopes of at least learning why I'm wrong. I guess I can imagine ways that C might be allowed to cause side-effects that are visible while it's running in a threaded program - I really don't know that much about memory barriers, but with no explicit locks, I assume there are none and that the compiler is free to re-order as it pleases. Honestly, I didn't think about it that deeply. But I still believe that's assuming it knows, absolutely for certain, that the return will definitely be called. Modifying memory that you don't know yet if you should modify it seems fairly obviously bad. As we've learned recently, even modifying the cache for speculative reads can be problematic (not the compilers fault, of course). If the return is never called, in C, AFAIK there is no excuse for corrupting the callers buffer. In this case, obviously that return *will* be called (ignoring things where the mess would be expected anyway - e.g. some other thread terminates this one before the function terminates) but there's still some static analysis to prove that which AFAIK isn't especially encouraged in C. There's also static analysis needed to implement the RVO, but that's different static analysis and quite strongly encouraged by the standard and culture - in C++17, one case of RVO (I forget which - maybe NRVO, maybe not) is even guaranteed. Of course the RVO is better if legal - that's why it's called an optimisation and why C++ has it. But if there are consequences that the standard promises won't happen, no amount of faster outweighs memory corruption. 
woops sorry thought I was on that sub
[removed]
Hey didn't saw your message during travel.. though it's late but here is what you should consider Most of the python is written in C. Even the popular JavaScript engine is mostly C++ We have created these high level languages because programmers want to concentrate more on what to do rather than how to do.. if you are a data scientist and good in both C++ and statistics..then you can use C++.. unfortunately most of the time a good algorithmic person or a person with sound understanding of statistic may not necessarily know C++ to it's core.. that's why python or R is a better choice.. spend your time where required.
Thank you
Which I think is the right call. Modules should finally get rid of the order dependency of header files and the infectious nature of macros. Sometimes you have to use a macro, but if you want to provide a macro to someone, just use a header. If a macro needs to be defined in all translation units, use your build system to define it. Otherwise modules can't really provide much benefit over header files.
I really want a proper package system for C++. A lot of recent proposal try to work around the bad package situation, e.g. the 2d and html proposals. I think something like Haskells stack would be a good fit. It allows you to use multiple compilers/compiler versions, but it has separate package installations for each of them. By default packages are default in a project specific workspace, but you can also install them in a compiler specific workspace to share them between projects. Also packages are compiled from source by default. The current package set is frozen and can be used to reproduce the exact same build on a different machine. I know, there is Conan, but it still seems a bit rough around the edge and it seems more focused on providing binary packages than allowing you to easily setup an environment, but I don't have much experience with it. It is also a bit cumbersome to integrate with Visual Studio. I think a proper package system is something C++ desperately needs to be easier to teach and adding 2d drawing and touch input to the standard is solving the wrong problem in my opinion. I'd prefer that to be a curated package instead of forcing the implementation on compiler writers.
Another reason metaclasses would be awesome...
- indices - ids - large values (e.g. squared ranges, or 2^31 lol) - flags - colours And basically anything else I wouldn't expect to perform any arithmetic on.
I was about to say something like "it's probably better to do this in QML", but then I remembered this is /r/cpp :-)
C++ integers make me want to vomit.
You've described [`build2`](https://build2.org) approach to packages. Plus it supports C++ Modules. If you want to see how the two work together, there is [lightning talk](https://www.youtube.com/watch?v=PxFrhYAYF3M) from CppCon 2017.
C compilation *doesn't* do it?! I find this strange. There's no difference between C and C++, in principle : copying that return value around is potentially expensive. (In practice, it's more expensive in C++ because of the unknown cost of copy construction or assignment; but it could be less expensive, too, with move semantics, perhaps?)
This is just bit of candy to inter-operate with (typically old) APIs who use bits in an integral. It's neither bad wrong or crazy to me.
Copy elision is more than about constructors. It is about identity. Two objects elided together have the same identity afterwards; the same address. In many cases this is academic, as there was no way to get the address due to language grammar: but in others it permits otherwise distinct addresses becoming the same. bar foo( bar* ptr ){ bar y={7}; printf("&amp;y=%p ptr=%p\n",&amp;y,ptr); return y; } bar x = foo( &amp;x ); If y and x are elided, they are the same object. 
Google is using something similar to p0947R0 in production on O(100,000,000) lines of code. Also p0947R0 isn't about providing a "Auto-Magic" way to transition to modules. It's about providing a sane way.
The only problem with Hunter I encountered so far was Android. There exists an... "interesting" combination of mismatched versions, bugs, and bad decisions that leads to Android Studio+CMake+Hunter requiring quite a lot of work to get working properly. But I hope very much that the next version of Android Studio (or, rather, Android Gradle plugin) will fix this. However, if you're content with building your C++ code separately, or with a custom command in Gradle (instead of using built-in CMake support, which doesn't work without hacks), then Hunter is also easy on Android.
For me, all the exponents in the formulas don't show as exponents, but as factors, which makes it hard to read.
char8_t isn't actually about storage (other than fixing the aliasing rules), it's about interfaces. One of the main use cases is being able to distinguish between the system encoding and utf-8 in the type system.
Google has enough resources and tightly-controlled environment to make pretty much anything work. *Have you, personally, successfully used Clang modules on any non-trivial codebase? All this time I've only seen exactly two reports of using Clang modules: at Google and by [this user](https://www.reddit.com/r/cpp/comments/759nff/facebook_has_been_working_on_c_modules_support/do7l67e/).
This is bad advice. Unsigned values should only be used when you *need* them, not when you just "don't want a negative". They can still be negative, they'll just overflow into positive and break everything. If you need a larger range, use a larger type.
//c++17 // Get the size of string-like args int strlen(const std::string&amp; s) { return s.length(); } template &lt;typename... Strings&gt; std::string strcat(const Strings&amp;... ss) { int totalsize = (strlen(ss) + ...); std::string res; res.reserve(totalsize); (res.append(ss), ...); return res; } 
kinda like fmt::format?
[Bitmask](http://en.cppreference.com/w/cpp/concept/BitmaskType) is a very simple concept. It is widely used as is even by std lib. And your wrapper around it is more complex and confusing to new developers who will read such code for the first time. Everyone understands what `font_attribute::bold | font_attribute::italic | font_attribute::underline`means. With `decoration().bold().italic().underline()` a new developer will need to look up this `decoration()` - is it a type or a function, does it have any side-effects or not.
This doesn't have anything to do with C++
That was on e very special case in a single benchmark and because a 32bit unsigned values was used on a x64 bit machine and the compiler didn't know the start value of the loop index. I really wonder how relevant that is in day to day programming.
Well, show me your QML! ;)
I can configure a Dell for the cost of a compact sports sedan with 1.5TiB of RAM. If size_t is 64-bits in most cases I’d just leave it that way :-)
Adding the '-fno-elide-constructors' switch seems to produce identical code.
You have any specifics about the issues you've had? I actually need to build a lib for a client with ndk-tools. Its a bit of an unknown area for me. I thought the Android NDK is basically a custom gcc-tools (so just a different toolchain file for Hunter), you build a lib and then that links into your Android project (that part im not responsible for :P) Are you saying Hunter gets upset when you have AndroidStudio run CMake to build your lib?
What do you mean by undetectable? Just as I can check for negative values, I can check for values that are too big. If we ever.g. talk about an index:with signed numbers I have to do two checks: &gt;=0 and &lt;size. With unsigned I only have to do one: &lt; size. One real advantage however is that I can let the sanitizer check for integer overflow when doing arithmetic.
Nice article, but std::function as a function pointer? o.O // Getting the function pointer std::function&lt;int(int)&gt; lambdaFn = [](int value)-&gt;int{ cout&lt;&lt;"Hello World"&lt;&lt;endl; return value + 1;}; 
Author here, yes I'm actively working on it. You can see example output of standardese here: http://foonathan.net/doc/type_safe/index.html https://ned14.github.io/outcome/reference/ /u/catskul please let me know if you have any issues with it (the develop branch is the version that will soon become the basis for the new release). 
Fortran array indexing and general use is god tier. I'd sacrifice a goat for that in c++ but I guess it's technically impossible. 
You can use the math.type function to tell if a number is into or float. The type function was kept unchanged for backwards compatibility.
You can write your own container and overload [] so that: signed user_index = ...; unsigned index = user_index &gt;= 0 ? user_index : size - user_index; Sprinkle out-of-bounds checks as appropriate. :) Interesting puzzle: figure out cheaper way to compute the unsigned index. 
I've used Python. It's one aspect of the language that I find quirky: - index from the front: 0-based, - index from the back: 1-based. **WAT??** I understand the *reason*, but at this point I'd prefer a wrapper type: array[rev(index)]
Not what I meant at all. Fortran allows things like arbitrary slicing A(2:7) = B(3:8,2) striding A(1:5:2) = B(1:3) as well as passing around any of those to functions expecting contiguous arrays and treating them natively. Nothing in C++ comes close to the expressiveness and clarity of this. Various linear algebra libraries have the functionality, but in horrifically ugly templated ways.
It's that time of the year again when C++ makes someone vomit.. what is your grievance with them? Bad semantics? Complain to the hardware vendors; they couldn't even agree if we should 1's or 2's complement just to get started. Of course, more "elegant" languages sidestep the issue by deprecating stuff that is hard to deal with. Look at Java and unsigned to get started on this track. (I am not saying it was a bad design decision, just saying C++ sticked with what C was already well established and worked out in practise). If your beef is unsigned integers, too bad, because that's what the hardware has and C++ is willing to expose. It's rough life to be a C and C++ programmer these days; critics vomiting left and right. LOL. 
Ah, completely different thing. I haven't done any Fortran since 1980's (I'm old geezer). I used to have interpreter for it for Commodore 64. :) I was big on template meta programming in early 2000's but came into my senses; it was a passing fad. If the code is too hard to follow it's probably not a very good thing. :) 
Yeah I started on it yesterday and this is what I have as of last night: https://github.com/Rseding91/LuaPlusPlus
Hello meetingcpp, Thanks for your comments. Well I just thought stating "function wrappers" may confuse people...if you think it's grossly incorrect and doesn't conveys the meaning.. will change the same to "Polymorphic function wrappers"
I think using auto instead of confusing std::function in a lambda article would be better.
Good to know, I'll have a look!
So where does the x * n / n occur here? Which bounds check is being elided? If I run variations of this code through compiler explorer I cannot get it to produce different code, whether I use ptrdiff_t or unsigned int. 
How is that an answer to my question?
&gt; I suggest you don't expose that operator T I'd suggest he has a way to get the value as T but not through the operator as that can lead to implicit conversion. Something like get_value() that returns T for when you really want it.
Thanks a lot for your suggestions, will modify appropriately.
I don't understand, why you make things so complicated. Sooner or later the function has to write the result into the output buffer and both versions do that, except, the c-version does some more work before that. The c++ version doesn't use the output location for storing intermediate results or something like that which could be detected by a racy program (would anyway be UB).
+1
So now we can tell everyone with a straight face that c++-code is faster than equivalent c code ;) Usually I'd just put it of to some heuristics that are tuned differently between the c and c++ optimizer engines. Optimization ist an exact science after all and when optimizing c++ code the engine light be tuned for different patterns than in c. But the fact that both gcc and clang produce the exact same code makes me wonder if that is actually the case or if there is a language reason behind it. If this is important to you, you should probably put this on SO and/or submit a Bug
http://en.cppreference.com/w/cpp/utility/bitset + index into bitset with an enum
`std::bless` is for situations not covered today neither by the C standards nor the C++ standards. It is not about `malloc`.
Why having a single type is a problem? As long as it enables normal looping at compile time and more reuse I cannot see a single disadvantage. It is way easier to use than TMP.
What change is needed? Not sure. Forgive my ignorance.
I'm not talking about single type vs TMP. Single type wins that's for sure. I'm talking about single type vs generated type. The generated type would have tuples of members. With tuple based for loops, it's quite easy to iterate and du stuff. With single type, the information is "hidden" in type erasure, because it uses compile time vectors. You have to downcast the compile time objects, and it also surprised me that any of them are pointers. What I find strange is that to order to make reflection easy to use, we must hide information in your erasure and get that information through a special reverse reflection operators. That information in known by the compiler, yet hidden to the programmer until downcast. In my opinion, I think we should make other tools easier to use. For example, if tuple had `operator[]`, some examples in the paper would look pretty much the same as with single type. Again, I'm saying that because I'm not convinced yet. Committee members seem in favor of single type. As Louis said, it makes using reflection much easier than many types. I'll try to compare the old cppx and what the new code would look like.
great article explaining the syntax and scope...but *why*, in a practical sense, would these be used? there's some overview in your article about this, but it's kinda vague and hand-wavy to me. i'd like to see examples of a concrete benefits: does this speed up execution of compilation? if so, how. does this make code more maintainable? if so, how. what does this gain over an actual function with appropriate use of const, immutable, et. al? 
Or ` absl::StrCat`
You wouldn't say that if you knew how deep into the insanity I've been. Of course I do use templates, yes, of course I still do use the type system for control flow. I just don't go full retard about it and think that it is a good thing or something to be proud about. It is not a hallmark of a good design how complicated it is, that's all I am trying to say here. :) 
I would never hold a lambda in an `function&lt;...&gt;` actually. I think the compiler will have to do a conversion actually, with possible slight overhead. Just use `auto` or `const auto`. The type will then be of an internal lambda-type or something like that but you will never have to bother with it. I think the only reason to use `std::function&lt;...&gt;` to hold a lambda might be if you need it as a class member variable.
There are also memory allocation implications of casting it to a `std::function`. You should mention the instances where a lambda can actually be cast to a real function pointer though-- when it is stateless. Also, might want to mention a couple more things: - `mutable` keyword. - default arguments. (C++14) - capturing `this` - capturing `*this` (C++17) - closure copy constructor (C++14) - closure move constructor (C++14) - capture initializers (C++14) 
Lambdas are useful when you want to associate some functionality locally that won't be reused elsewhere. It's why we push variables close to where they're used in C++ (as opposed to the C convention). Also, when cast to std::function, their state can be captured and the lambda is used polymorphically or parametrically polymorphically.
&gt; The renders all time spent on discussions of "performance" a waste of time. If you don't know *why* code is fast, you can't produce fast code. If you don't know *why* code is slow, you can't prevent yourself from making it. Profiling is definitely an important and necessary part of performance optimisation, but if execution speed matters to you and you're still frequently surprised by the performance characteristics of your code either your code is not written to be amiable to performance introspection or your mental models of computer execution need work. You wouldn't expect a scientist to *just* run experiments, you expect them to use experiments to verify and disprove their theories. Computers are predominantly deterministic machines, and they work in fairly easily analysable ways. Something that runs tens of billions of instructions *has* to be slow, and if you've written your code to have visible abstractions you should be able to see where they are. You can definitely optimise from first principles if you spend the effort to get an accurate understanding of how computers work and how your code translates. This isn't a skill that people practice much nowadays because it has been many years since computers are fast enough that almost everyone gets away with slow code, and abstractions have become so towering and openly embraced that most people end up overwhelmed by random incidental costs instead of fundamental ones. The video's example is a good example of this. It might well be surprising that `stringstream` wastes its time in `dynamic_cast`, but this is incidental to the issue. A cursory understanding of how C++'s IO works and performs in practice should make it obvious that most of the time was going to be spent on the incidental complexity, rather than malloc-dealloc costs, which, whilst far from free are of mere reasonable cost. The same applies to `sprintf`, albeit to a lesser extent. You can even guess at the absolute costs with a bit of practice. His last attempts lose out about a factor-2 in performance because his first principles aren't strong enough that he knows to focus on predictable throughput instead of latency. Doing so, [as I mentioned below](https://www.reddit.com/r/cpp/comments/7xzhf1/worked_example_of_performance_tuning_in_c/duckvyw/), is going to be faster; in my tests it reduces time taken to 70% of the original, and formatting backwards improves that to 55%, almost twice as good as his final variant. I am fairly sure that tuning, especially on the actual dataset, can reduce that even more. 
Small correction to "Do you see the antialiasing?" Aliasing is the cause of the artifacts (sometimes called jaggies). Antialiasing is what you call a technique for fixing it.
Well, I was being ironic...
&gt; [..] what the compiler could/should prove about the function in isolation becomes irrelevant - what matters is what the compiler can prove in the context in which the function is used. On the contrary, what the compiler can prove in isolation for that particular function is the ONLY thing that is relevant for this question/reddit. &gt; It's not necessarily true that sooner or later **a** function has to return a value [... very, very long list of reasons, none of which apply here ...] Please read again, my claim was that &gt; Sooner or later **the** function has to write the result into the output buffer I was not talking about functions in general or their compilation to an abstract machine, but about the concrete example in this thread. All your explanations in the first 80% of your post about why functions could return prematurely are certainly informative, but also completely irrelevant to the topic at hand. The compiler has complete insight into that function. It doesn't have to deal with maybees, what ifs or could bees. And again, my assembler skils are a bit rusty, but as far as I can tell, the binary emitted by the c++ compiler doesn't do any stores that the c version doesn't do as well, so fear of unallowed side effects can't really be the reason. &gt; don't worry too much about these artificial in-isolation compilation results. You're probably making a big fuss about nothing. Are you seriously writing a page long post in which you claime that I'm making a fuss, because I'm writing a 5 line post saying that you overcomplicate matters when analyzing what happens in a ~5 line function? Are you mixing me up with someone else? I also don't understand, why you think I worry about something? How did I give you that impression? At most, I'm curious about this and a bit dissatisfied with some reasonings I've read here, but I'm neither a c programmer nor do I have to deal with absolute high performance code, where such things would be a problem. &gt; Anyway, compilers aren't required to apply every possible optimisation in every case where I or you can prove it's valid, though failing to do so might be considered a bug. In the case where the style of code isn't really encouraged by the culture of the language (which AFAIK in C is still that they like their pointers, and prefer to use them explicitly to guarantee a particular behavior rather than hope for the optimiser to do that for them) that may not be considered important. Now that is the important part here (and mirrows closely what I said in another post). Optimizations are heuristics and the engines are by now way perfect, so it could just be a fluke cause by different optimization passes and parameters. But the way I understand the OP, this is mostly about satisfying curiosity not about investigating a actual performance problem. 
&gt; The “anonymous” namespace you have created will only be accessible within the file you created it in. Just to be clear, the members of the anonymous namespace will only be accessible within the *compilation unit* they are defined in. As of C++11, they also will have internal linkage (which allows the compiler the freedom to do a lot of optimizations). Importantly, an anonymous namespace in a header file will not behave as the article implies. Also, that could lead to longer compile times.
So, I did some investigations on clang. First of all, running `clang -O2 -mllvm -print-after-all` dumps the LLVM IR after each pass, which is very useful to understand what is going on. First thing, both the `C` and `C++` version generate an LLVM function with the exact same signature (`void @test(%struct.v* noalias sret, %struct.v* byval align 8, %struct.v* byval align 8)` ). This means that there is no ABI difference, at LLVM/assembly level both functions are interchangeable. The main difference is, like other said, because of RVO. Now, RVO is a quite peculiar optimization, because it is allowed to bypass the `as-if` rule. This means that a copy-constructor can be optimized away *even if it has side effects*. Now, in clang `RVO` is implemented in the compiler frontend, not in the optimizer. This is probably because it is much easier to do it in the frontend, rather than teach an optimizer that it can optimize away certain side effects and not others. So, this answer the question `why does the optimizer produce different results in C vs C++?`. The answer is that the optimizer starts with a different version of the function, because RVO is done in the frontend. Unfortunately, the optimizer is not smart enough (yet) to elide the obvious copy, [*even in C++*](https://godbolt.org/g/3PUYrm).
UTF-16 is a variable width character encoding. You can't just iterate over the UTF-16 code unit and print them one by one like that. This will not work with emoji for example. Not to mention the composite character. I gues the right API to use, with Qt, is QTextLayout::glyphRuns and QPainter::drawGlyphRun. This will also help to keep the spacing as different character may have very different width.
I think he meant RVO isn't a C thing. 
What is interesting is that parts of the system by themselves all work: you can use AS+CMake as long as you don't need Hunter. You can use CMake+Hunter outside of AS to build Android projects (I build some libraries this way, works like a charm - but you can't *easily* build APKs without Gradle). But bring all pieces together, and you get a perfect storm! If anyone ever needs to illustrate the conception of Integration Tests, this is a perfect example.
Someone has posted some more insights to this, apparently this is not ABI dependent in this case: https://www.reddit.com/r/cpp/comments/7y39ek/why_does_this_snippet_produce_that_much_better/duefafd/
&gt; Unfortunately, the optimizer is not smart enough (yet) to elide the obvious copy, even in C++. Your C++ link has the compiler passing the result into an out param instead of actually returning it. If you can the function to return the result directly then it does seem to [elide the copy](https://godbolt.org/g/GY4ePP).
As has been mentioned, it's not scalable, but your heart is in the right place. I have implement a `EnumSet&lt;E&gt;` class at both of the jobs I held; I just find it that useful. My approach is to: - declare a `constexpr std::array&lt;std::pair&lt;E, const char*&gt;&gt; values(Id&lt;E&gt;)` which enumerates the values of the `enum E` (preferably use macros for that), and from there use meta-programming on the enum (including formatting, parsing, `foreach`, ...). - use the above to implement `EnumSet&lt;E&gt;` on top of `bitset`... though I personally tend to re-implement a `BitSet` because the `std` one is not `constexpr` :/ For bonus points, have the size of the `BitSet` adjust automatically depending on the number of inputs: you don't need to reserve 32 bits when you only use 3...
Actually, I meant "rvo was a C++ thing", but autocorrect changed it to C.
&gt; Just one little detail I'm curious about is the definition of side effects here, as in does this include purely moving memory around on the stack or is this rather explicit code executed by the copy constructor? Basically this: class foo { foo( const foo &amp; ) { std::cout &lt;&lt; "Hello" &lt;&lt; std::endl;} foo bar() { foo x; return x; } //... foo x = bar(); Can print "Hello" 0, 1 or 2 times, depending on whether RVO/NRVO is applied. All the 3 outputs are allowed by the standard.
Yes that was my point, maybe I was not clear. RVO/NRVO aside, clang optimizer is not able to elide the copy from a stack variable into an output parameter. The version that takes a reference also generate a function with the same signature in LLVM, (LLVM basically implements C++ references with pointers).
Yes, that is a good idea, though I currently only plan for text characters. Also the drawText call is from the example, didn't thought about alternatives so far. Will look into this.
&gt; Lambdas are useful when you want to associate some (trivial?) functionality locally that won't be reused elsewhere. If it's trivial, isn't the code clearer if you leave the lambda out and just implement the solution directly? The special case that you've given as an example - where you want to pass a simple function as a parameter - makes sense to me. But beyond this usage, I'm having trouble seeing any advantages over using normal functions or implementing the solution directly without a lambda or function call. Are people using them for anything other than as parameters?
`using namespace std::math_constants;`
When the code base is big, 3 ~ 4 gb, CLion will be very slow to load, and unresponsive for minutes when editing.
In my experience lambdas are mostly useful as parameters
Here's another (contrived) example. Imagine this reads the first row in a database and dynamically parsers the subsequent rows. Check out the second instance of a lambda. It contains `index` which is mutable. To do this otherwise would require a class definition and an overload of `operator()()`. It's debatable whether this is cleaner or not, but I'd rather not define a class for each field. #include &lt;vector&gt; #include &lt;map&gt; #include &lt;functional&gt; #include &lt;sstream&gt; #include &lt;iostream&gt; struct Person{ std::string first; std::string last; int age; }; using FnMaps = std::map&lt;std::string,std::function&lt;void(Person&amp;,const std::string&amp;)&gt;&gt;; void parseColumns(std::istream&amp; data,std::function&lt;void(const std::string&amp;)&gt; fn){ std::string input; while (std::getline(data,input,',')){ fn(input); } } int main(){ FnMaps columns = { {"Age" , [](Person&amp; p, const std::string&amp; s){p.age = stoi(s);}}, {"First", [](Person&amp; p, const std::string&amp; s){p.first = s;}}, {"Last" , [](Person&amp; p, const std::string&amp; s){p.last = s;}} }; std::stringstream firstRow{"First,Last,Age"}; std::vector&lt;std::string&gt; columnOrder; parseColumns(firstRow,[&amp;](const std::string&amp; s){ columnOrder.push_back(s); }); Person p; std::stringstream data{"Joe,Blow,53"}; parseColumns(data,[&amp;,index = 0](const std::string&amp; s) mutable { columns[columnOrder[index++]](p,s); }); std::cout &lt;&lt; p.first&lt;&lt;","&lt;&lt;p.last&lt;&lt;","&lt;&lt;p.age; } 
The world would be a better place if everyone stopped using FindXXX modules and switched to exported targets instead. 
Thanks for the fully fledged out example! I tried external_project once and found it too hard to get hard. &gt; This way is portable and less error prone but more verbose. Why less error prone? 
I think they are just for parameters. Functional programming loves them.
The exceptions that an function can throw are just as much part of the API of that function as its input and output parameters. Hence, yes, the exceptions should encapsulate or abstract information as necessary.
FYI, you are shadowbanned. You’ll need to contact a reddit admin.
You’ve gotten some responses; I’m going to remove this now as help posts are off-topic for our subreddit.
There's no reason to employ type-erasure for `parseColumns`, so realistically that would be `template&lt;typename Fn&gt; void parseColumns(std::istream&amp; data, Fn fn)`. Were `parseColumns` itself a lambda, that could be shortened to `auto parseColumns = [](std::istream&amp; data, auto fn)`. Succinctness and locality are practical, worthwhile advantages on their own.
Actually, I'm pretty sure RVO was NOT part of the language standard prior to c++17, but was implemented already in pre c++11 compilers. Are you sure you are not mixing the specific optimization RVO, with the general concept of copy elision, which was part of the standard from the beginning and which allows RVO even when copying objects has Side effects?
But does this answer a question to why RVO is not taking place for C code? As far as I understand C compilers should be free to implement RVO even though RVO is not in the semantics of the language as it is in C++.
Ah, guess I fell victim to Poe's Law.
(N)RVO was *allowed* but not *required* prior to C++17.
Neither of those seem to apply t `ESC`.
 for(auto i = v.size(); i &gt;=0; i--) { ... Having fun with auto keyword :D
Watch out for i wrapping around at zero; that loop will never terminate.
I know, this is why I put this piece of code. Be careful when using the auto keyword :p
I am very careful; I am a Turing Complete code generator. ;)
OpenCV is really useful and all, but if anything it's a shining example of terrible design. It's not really a good place to be looking for inspiration.
_Auto_ itself isn't a problem here – `for(auto i = v.size(); i--; ) { ...` is fine after all.
The answer is 'the optimizer is not smart enough'. Even if it is the _same_ optimizer for C and C++. It works in C++ because RVO happens _before_ your code even makes it to the optimizer (at least on GCC/clang). RVO/NRVO is implemented in the frontend. You can check that OP's code compiled with -O0 has a `memcpy` in C, but not in C++.
I repeat: I'm pretty sure (N)RVO is not a language concept (at least I don't find it in the standard). It is an optimization that wouldn't require any special blessing by the standard for trivially moveable and destructable types, because it is already covered by the as-if rule. However, in cases where the copy/move constructor has side effects, the as-if rule is not enough and this is why the standard explicilty allows copy/move elision for function return values (but also in other cases) since probably c++98. And yes, c++17 turned that permission into a requirement under certain circumstances which don't apply here. Now, as copy construction can't have any sideeffects in C anyway, I don't see a reason, why the c compiler wouldn't be allowed to implement (N)RVO - irrespective of whether it is mentioned in the standard or not - the as-if rule should apply here. It obviously doesn't implement that optimization (most likely because it is a less common requirement in C), but I haven't yet heard a compelling reason, why it could not. 
Finally someone who is doing some actual investigation into what is going on. Thank you very much.
&gt; Capture lists?.. What, so we actually can alter outer scope? You can capture by value. And if your types are functional, then even if you capture by reference, there still won't be mutation.
&gt; 1- Library based architecture Most large scale projects do that. &gt; 2- Modularize by namespaces Most large scale projects do that. Without this, you will have name collisions and too many names to remember. &gt; Avoid multiple inheritance It's a good point. But usually it's common in GUI applications. If you follow religiously, eventually you will have same code repeated every where or too many compositions. &gt; Avoid defining complex functions function as a logical unit is very common, but eventually decays due to hard deadlines. Since OpenCV is an open source project, they technically don't have any deadlines. &gt; coupling and cohesion everyone knows this and very less know how to do it. &gt; Defines data model as POD types it's not possible everywhere. You need a lot of encapsulation and exposed interface should be less primitive.
Right, the standard doesn't use the terms RVO or NRVO, but it talks about cases where copy/move elision of return values is allowed. (N)RVO is just a convenient way to refer to that. You're also right that the as-if rule should allow this optimization all the time in C. This reminds be of an old GCC bug/missed optimization I reported: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=63537
Hello. Sorry to hear about the same. Same case everywhere, I don't know how to show the same inside code block. Couldn't find anythings
RVO and NRVO have been part of C++ long before we had a C++ standard, e.g. C++98. C++ books from the 1990s wrote profusely about it. C programmers “know” that you don’t idiomatically return structures by value - you pass a pointer to where to store the return value; just look at most C APIs :-) So this particular optimization does not impress them. Note also that in the 1990s, not all C++ compilers were able to do NRVO - some, like GCC, invented syntax to make up for their inabilities to perform that transformation; if you look into GCC archive, you will find it. Fortunately, that was removed when the folks at CodeSourcery (notably Mark Mitchell) introduced a higher level representation in the G++ front-end.
Of course, you can. You can also write pure free functions, which don't have any of that stuff and are still a better fit for FP, if you forget about little details like errno. The thing about functional programming is not that you CAN write pure functions, it's that you CAN'T trivially do otherwise, that you're forced to write a different code. Lambdas not only allow writing code which is controversial for FP, they encourage you to do so by having a very handy capture list syntax - that's my point. It's like writing an article that a large diesel 4x4 off-road vehicle is good for inter-city trips. Okay, it might be comfortable for the people inside because of quality suspension, and cos there's a hell lot of space, and nobody objects that you can use it to go from one city to another. But hey, it has half the mileage of a smaller low-clearance sedan, can't really go faster than 100mph / 160kph, it's power steering system is too sensitive on high speeds, and generally speaking, the truck was designed for somewhat different activities. Don't you think a test-drive for that kind of car should have targeted off-road capabilities first? 
I would be fine with it if std::math_constants were an inline namespace.
I don't think this is the case here. Iterating over a QString is fine since it uses QChar which correctly represent a utf-16 character. 
Julie Hockett is working on one to add to the list of clang tools, see the RFC: http://lists.llvm.org/pipermail/cfe-dev/2017-December/056203.html This review thread is here: https://reviews.llvm.org/D41102
Now it's "function reference", wow. OP, that thing is called "function object", don't use the word "reference" that way. By the way, regarding function pointers, there's a very important difference between lambdas which can be assigned to a C function pointer, and those which cannot, and it's not something chosen out of readability concerns, at least when you know what you're doing.
&gt; for(auto i = v.size(); i &gt;=0; i--) gcc 7.3.0 gives a warning here: &gt;&gt; comparison of unsigned expression &gt;= 0 is always true [-Wtype-limits]
Vague statement, just like your initial post. If you wanted to talk about how lambdas help writing FP code, you should have started with a notion of functions as first-class objects in FP languages, gave an example of function composition through returning a new function object, an example of calling a generic STL algorithm with a function / predicate argument, maybe something else based on your taste, and then showed how these cases can be simplified if use lambdas instead of C function pointers or structs w/ operator(), where context is necessary. And then proceed with syntax, details, pitfalls (for FP paradigm as well), runtime overhead and how to mitigate it, etc etc etc. That would be an appropriate and structured way to deliver a meaningful message, unlike what we see now.
More like an advert of Cppdepend's tools. If anything OpenCV is a messy library with its own, unclear memory management practises. 
&gt; It obviously doesn't implement that optimization (most likely because it is a less common requirement in C), but I haven't yet heard a compelling reason, why it could not. As I said, it *could*, but the difference is that it is not required.
Even in c++17, the c++ compiler is not required to implement NRVO which is what is happening here.
True indeed, didn't tried my sample with all warnings enabled.
It's not the worst, but Visual Studio has for me always been a frustrating experience. It keeps freezing up on large projects and IntelliSense breaks randomly. It's not acceptable that you have to keep doing incremental builds to catch compilation errors. Libclang based IDEs seem far more robust
Every IDE from JetBrains I can describe as slow heavy useless unintuitive painful piece of shit. Codeblocks is ~light, but it's unintuitive and non-customisable as hell. 
I'd like to give an example of horror design I found in opencv the other day. Unfortunately I can't remember the function name, or I'd link to it, but I found a function where the documentation suggested there existed an overloaded version with two parameters reversed. Strangely, the two parameters which were reversed were both ints, which isn't really possible with c++. To solve this problem they redesigned one of the ints to a vector of ints, and assert that it only holds one value. All so you can call f(a,b) and f({b},a)...
Nice to know someone shares my feelings for JetBrains stuff. And for Java, now VSCode has proper Maven support with the Red Hat plugin ill never have to touch IntelliJ again... I hope...
Yeah, visual studio is by far the best but also the worst at the same time... 
Hitting Build regularly during editing to see if the changes made compiles.
Ah, you mean, VS intellisense is bad compared to some other IDEs? Yes, true. Don't hit build, hit Compile though :-)
Visual studio, for me, is by far the best. Make sure you use at least 2015 preferably 2017, these two new version are much better and handle larger projects pretty good (you need to let it build the initial intelisense database and then you're good). I use it everyday at work with a large code base and run only into minor issues.
It seems promising...! Some comments. A. IMHO it'd be much better if it were a header-only .h library - because then I could just drop it into a project and have it all work. Doing so would not increase compile times significantly for most projects, I believe. Usually, you read the command line arguments in one module and use them to fill in your own proprietary structs and classes. B. Also, in reading the code, the fact that there's no way to tell whether a variable is a local variable, a global variable or a class member. This is not only hard for the reader, but it increases the chances of [shadowing](https://stackoverflow.com/questions/19621785/shadowing-variables), where you have a collision in names between different scopes that results in unexpected results. I strongly suggest using a naming convention. The convention I use is that member variables end in an `_` - so you'd get member variables like `args_`. This is very common, and also uses the least typing, so it's a big favorite of mine. C. The fact that the first thing you see on reading the code is three classes marked "Internal use" is a distraction. Use forward declarations for those classes, and then actually define them at the start of your implementation (in the .cpp file in the current implementation, or later in the .h file if it were header only). D. [`ArgStream`](https://github.com/dmulholland/options/blob/master/src/options.h#L19-L26) is a class with exactly one member - a little bit of a code smell. Consider replacing that class by this: using ArgStream = std::deque&lt;std::string&gt;; and replacing those three methods by pure functions on `ArgStream`. E. You keep "renaming the wheel". You should try to use exactly the same names as in `std::`, if they are applicable. For example, [`numArgs()`](https://github.com/dmulholland/options/blob/master/src/options.cpp#L218) should just be `size()`, etc. F. Or even - why not just expose those members read-only and get rid of all those methods? class ArgParser { public: using Arguments = std::vector&lt;std::string&gt;; const Arguments&amp; arguments() const { return arguments_; } private: Arguments arguments_; }; All this cruft just vanishes! The API user never has to learn your weird ;-) names like `hasArgs()` but just uses plain old STL directly. Have fun with it!
no sublime text?
Sublime is awesome, but it's really more of a text editor than an IDE. 
Sublime is text redactor, not IDE. There is big difference between IDE and text redactor. 
IntelliSense is just dumb at times. Recently I discovered that in large header files IS would underline every function as "not implemented in cpp file". Turns out, if there are any classes/structs/enums defined inside of a class they need to be defined AFTER the constructor and the destructor. Now I do that, and IMO it is terrible, but it is better than IS not working. To be fair I haven't had the opportunity to try VS2017 yet, this happens in VS2015. Maybe IS is better now.
Do you use any addons like Visual Assist X or Resharper++? I have always wondered how a big flagship product like Visual Studio can be so bad at code surfing.
Quite right. I've used both, first Assist X, then the company switched to Resharper++. With Resharper, I am very disappointed by the performance. It has huge problems with file includes over network. It takes forever to parse through your code base (if it's on the larger side), and ever so often causes out of memory problems during parsing (memory usage climps up to 2-3gb, and then VS crashes). Resharper has nice features, but boy is it unstable... Visual Assist X on the otherhand was fast and very stable, but has fewer features... Still, would recommend to use one of them.
For me the worst has been eclipse cause I kept having issues with it (i.e. eclipse crashing because the indexer runs out of memory). I've been using qt Creator quite successfully lately, so I indicated that was my favorite, although I don't have much experience with the others. Only problem I've run into so far is that after I switched to the clang back end, it uses ungodly amounts of ram to function, and code assist is a bit slower now. The quality of the code assist is a lot better though, which is why I put up with it.
Sorry if this is dumb, but shouldn't you not do "using namespace std;" for libraries like this? I had thought it was a community norm or whatever to just do *std::fart* as opposed to just *fart* when writing a DLL or whatever.
There are only two places where the author uses `using namespace std;` (namely [here](https://github.com/dmulholland/options/blob/master/src/example.cpp#L8) and [here](https://github.com/dmulholland/options/blob/master/src/options.cpp#L12)). First one is the example, so it's fine. Second one is the .cpp **implementation** file. Doing `using namespace xyz;` is very bad if written in the **header** file, because including it would spoil the name visibility in the user code. Doing it in the implementation files is totally fine.
&gt; It has huge problems with file includes over network. File includes over network? Why would you ever do that? :) But on the subject, I think Resharper takes a long time to index your code. It can be pretty tiresome. But once it is done, Resharper is the best choice for me.
It's totally fine if it's only in the implementation file. I personally use `std::` everywhere for find/replace purposes, but it's only when it's used in header files that people get (rightfully) annoyed.
It may depend how many new C++ features are being used. VS is by far my favorite IDE, but there are generally a few features that I know they haven't yet fully implemented, both in terms of compilation and Intellisense. The development experience goes best if the code evolves along with the ongoing dev/debugging experience, whereas if people aren't exercising IDE features throughout development (e.g. if the code was written in a text editor, or different compiler was used) sometimes certain things just don't work. I've also had to 'correct' a project by reorganizing the include file strategy for a project that did fancy things in its makefile - otherwise, lots of things weren't working. That isn't unexpected, but it can happen.
I suspect Microsoft may have some sort of deal with the developers of famous plugins like Visual Assist (Whole Tomato Software) this is why the code refactoring stuff sucks so much with VS compared to other IDEs, so that everyone buys plugins. 
You might want to take a brief look at Clara which is used in Catch library and is also available as standalone separately. 
No I don't, a few other colleagues do though along with another plugin called "FastFind". Visual Studio 2017 comes with a whole new suite of improved search features, better filtering and more filtering options in intellisense which make it much easier to browse code.
OutputArray and InputArray template stuff is a mess and break const-correctness. Some functions accept const cv::Mat (without compile nor runtime error) and still modify it...
Have you done lot of customization to key shortcuts? For me they are horribly inefficient, hard to use and unintuitive. I never bother to spend multiple hours just changing shortcuts. And have they finally made mouse back&amp;forward keys working? I prefer QtCreator
Where is turbo c++ :D
&gt; Unfortunately, CMake makes it really easy to use it incorrectly, partly due to a whole bunch of outdated information on the web, partly due to its flexibility. This is the bane of cmake. I find the official documentation hard to understand, maybe because it doesn't care about the bigger picture (how does the command work vs. how should I do something). There are millions of answers for specific things on stack overflow but they are from vastly different times using different paradigms, so it's hard to grok "how should I do this in 2018".
Lots of style guides ban this though (llvm, Google). At work we do the same, no using namespace ever, even in cpp files.
Agree for me it's quicker to read std::transform than transform. The std:: feels like part of the name
Lots of good advice here. About B., shadowing is typically an error nowdays so it's a non-issue. -Werror is fantastic. Also, not being able to tell local variables from member variables is an indication of too large functions. A naming convention to tell them apart feels like a band aid, not fixing the real issue.
I haven't used it exentisvely, most certainly not for c++. I just get in touch with it every now and then because the work we do is for app security, but I am certain that Xcode is the single worst IDE known to man. 
That's true. The reasoning of the corresponding LLVM rule doesn't look great to me: it's basically saying "hey, don't omit `std`, but omit `llvm`, because it's cleaner". While I partially agree, I'm not completely sure about this rule as a whole. As for me, decent naming and code structure is way more important for readability and elegance, doing `using namespace xyz;` in implementation files is not as crucial.
I like and use CLion on my machine at work on all my projects. It can be slow and needs stout hardware to be useful. For other OS projects I use codeblocks on my slower systems.
QtCreator and VisualStudio, both really high up for me. On the other hand Eclipse and XCode are the worst experiences of my professional life.
Please consider “Unix as an IDE” type setups as well. A good terminal emulator, tmux, tiling window manager, neovim with plugins, cmake, plus clang tools make for a really powerful IDE like workflow. There’s a greater learning curve and initial effort involved, but it allows infinitely more customization and tweaking. I have a feeling that this setup is preferred by a large number of developers especially in the linux community. This is what I personally use. After lots of experience with Visual Studio and CLion, all the monolithic IDEs just feel very bloated. By excluding “Linux as an IDE” type setups from your poll you might skewing your results.
Once you break Xcode’s code parsing there’s no coming back. Xcode absolutely grinds to a halt on large codebases on my 2017 MacBook Pro.
It’s for good reason, to avoid name clash hell.
I really hope we will finally get the wide integers. While they are not usable for production-software in my field of interest (cryptography), they would at least make scientific prototypes much nicer. (For production we would need a guarantee that the implementations are side-channel-resiliant, which would slow down everybody else.)
underware-brief
CLion is by far my favorite. Really nice CMake integration, great customization features and active development. Love it!
The library is too limited for me. And, for how limiting it is, it should definitely be a header-only drop-in, and not require .cpp / implementation in my opinion :-)
Doing it in the implementation file isn’t great either. You never know what someone will do with your file, unity builds for example!
It's a toss up for me. I, personally, use Qt Creator and it works fine, until you need to see the internals of an object of a class, derived from Qt class: it just freezes up, no amount of waiting was enough. XCode was meh experience for me, I guess I would need more time with it. Visual Studio wasn't very usable the last time I used it extensively, unless you used Visual Assist, this might've changed since then. I've never used CLion, but my friend said that it was a resource hog. Oddly enough, NetBeans worked relatively well, but doesn't offer much functionality and might work worse on larger projects. Eclipse I've never managed to get working, easy choice for worst one.
CLion lightweight? It feels much slower to me
I use visual studio code: it is multiplatform, has a debugger, understands c++ enough to make good predictions, has a pretty decent project manager, there is not much else lacking. Furthermore, I can use the same IDE for typescript or python with best results. 
Slightly related: what's the best way to "fix" anonymous namespaces in implementation files when using unity builds? (i.e. two `.cpp`s define the same functions/types with different implementations in top-level anon namespaces (or `static`))?
&gt; Are people using them for anything other than as parameters? Initializing the value for a constant.
&gt; File includes over network? Why would you ever do that? :) Someone's never used Clearcase Dynamic Views for version control...
The OOM issues are solved by editing eclipse.ini and giving a couple GB's of memory. Before switching to visual studio code, I used Eclipse CDT to work on the chromium codebase (several gb's of code) without issues. If you want to give Eclipse CDT another try you can check this https://chromium.googlesource.com/chromium/src/+/lkcr/docs/linux_eclipse_dev.md#Setup
Eclipse based IDEs get much better if you have lots of RAM and you make an effort to go in and edit the launch files.
Any legacy code base is going to have problems like that. I've found OpenCV very easy to use. I'm using in in conjunction with ffmpeg. If you want some legacy library misery, try writing something with ffmpeg sometime.
I haven't found VAX necessary since VS 2015 (and 2017 was even better), before that it was a must-have though.
I am frequently surprised when I profile other people's code. I end up shocked that that specific person would write code that inefficiently.
I couldnt disagree more about JetBrains products, they offer a quality that many others only can dream about, it has every i need and more, it is well integrated and if something is lacking you can develop a plugin It uses more resources than others ide but it is nothing critical, is responsive as any other Ide on the market, so what? I dont really care about the extra ram and if you want to use less CPU you can activate the power save mode Even Google noticed tha high quality and switched from eclipse to IntelliJ for Android
My current projects have a third party code base of close to a million LOC. Thanks AUTOSAR.
Very true. I'm a KDE developer so by all rights I should use KDevelop or QtCreator exclusively. But those weren't available when I started so even now I'm still a "Linux as the IDE" developer, with Vim, a Konsole with a gazillion open shells, and a good web browser.
I honestly don't know how someone feels this way about JetBrains products. They are by far the best IDEs I have ever used. They are very customizable and literally every action is key-bindable. It's like vim, if you use it once, you get mad because you can't figure out how to quit and you tell everyone it sucks, but if you actually figure out how to use it is light years ahead of things like nano.
The worst I’ve ever used is no longer availabile or relevant: the Windows IDE for N64 development. Instead of a project file, it just parsed the linker file (which you had to write by hand) in order to determine which cpp files were used and where they were located. It would list those paths in a small, fixed-size GUI window. So, if your dev path was too long you couldn’t distinguish between files when you want to open one. On top of that, it somehow frequently failed to correctly parse these simple strings and some listing would randomly be garbage characters. That meant there were source files you simply could not open. Too bad. If you determined you needed to debug on of those, the work-around was to find some function in a different file that called into the needed file, place a breakpoint in that function then step into the needed file. For one release it had an off-by-one error in the variable watch window. It would display each variable reinterpret-cast as the type that was declared next after the actual type of the object. The work-around was to locate the type declaration and typecast a pointer to the object as whatever type was defined just before the actual type. But, hey. We had it better than the PS1 team. The same IDE for them was literally useless for anything besides launching the game on the device. They had to log-debug exclusively.
In addition to what others have said, a couple issues that would affect me as a user: Consider using CMake instead of makefile. If I'm writing a program that has to be cross-platform -- which is most of the time -- then a platform-specific, compiler-specific library is a non-starter. Don't catch your own exception or write to cerr and exit. Just let the exception propagate and let the caller decide how to respond to errors.
&gt; (...) throw an exception (...) I thinks it’s more about the decision „UB or not UB?“. But yeah for library it probably means throwing exceptions. I think you’re right. It’s been a while since I’ve watched that particular video. I might have confused them. 
The worst I've been forced to use was Dev C++ for a computer science. It's old, unintuitive, doesn't get updates, and is buggy as hell. For me, the best editor will always be (Neo)Vim + some plugins.
There are plugins (such as [vim-refactor](https://github.com/LucHermitte/vim-refactor)), but one of the best tools is [eclim](http://eclim.org/). It lets you use Eclipse features in Vim. It's not as smooth as using something like Eclipse directly, but it's a small price to pay for the increased efficiency that Vim gives you.
Would really like to try Visual Studio, but I can't bring myself work with Windows. With WSL it's somewhat bearable but after a couple days I just can't stand the many terrible systematic design mistakes. It can't work with any filesystem besides NTFS and (ex)FAT, it's stuck with weird character encodings when literally everything else is UTF-8 all the way. And then comes the small stuff, like having absolutely no decent terminal, weird Window placement on multiple monitors, a total mess of new and old system settings and no way to do that on the command line. At least for that you can actually read a description instead of needing screenshots and translating everything to your locale. Also choco really isn't a match for integrated package managers and knowing my Linux works in whatever CPU you have available That said I'm really happy with Linux, alacritty, tmux, NeoVim and YouCompleteMe. No matter if I'm coding in Python, Go, C++ or writing a letter in LaTeX it all feels the same and does exactly what I want it to do.
Man, I'm glad it's not just me. Took forever to figure out XCode, has to be the most unfriendly user interface I've used.
Suppose you're right, didn't think a lot of people actually used sublime as an IDE
I have 2 best IDEs for C++: QtCreator for programming, but debugging in it is unusable for larger projects ... so the second one is Visual Studio for debugging, which is unusable for programming (talking about version 2015 as there is no express version of 2017 and I cannot use 2017 community because of its license terms).
Not really, it depends on how you like to work. Here's a sum of what I use frequently: CTRL + F - find in current file CTRL + SHIFT + F - Find in entire solution/project (can use regex here) CTRL+TAB/CTRL+SHIFT+TAB - Switch between tabs CTRL+Shift+V - Cycle paste history Ctrl+; - Highlights solution file find (very handy) F12 - Go to definition Worth noting and something I thought was integrated into Visual Studio but I actually use this plugin: https://marketplace.visualstudio.com/items?itemName=SamHarwell.MouseNavigation It allows you to go back and forwards from where you were previously editing/searching (it's a different shortcut for ctrl++ ctrl+-)
Renaming an interface is indeed a bit messy. There are plugins for that, but I had a better experience with Visual Studios built in rename. On the other hand most of my interfaces are only used in 2-5 files, so even without a plugin renaming is quite fast. Other refactorings, like changing function arguments, are a lot easier to do for me in vim. Change the function, hit compile and your at the first compilation error, make your changes, jump to next by either recompiling if the change produces a lot of errors (like templates) or by jumping to the next item in the error list. You can navigate the error list in Visual Studio, but it is a lot more work as compiling doesn't automatically jump to the first error. Is there a better way to do that in Visual Studio, where you need manual work for the changed argument?
We use tools like rtags: https://github.com/Andersbakken/rtags Literally everything you like about your IDE can be done in a good Emacs/vim setup. 
5000 lines is the size of some functions in some projects, I work with. No, your functions shouldn't be that long, but sometimes code grows organically with age and requirements. 5000 lines is something I would consider a small project (although line count is not the best measurement for complexity).
For me it feels like that, because you can comfortably work in CLion with just one window with code and line numbers, while in VS you always distracted by unnecessary functions to the point where even keybindings configuration becomes impossible.
Then you will have to add a `using my::thing;` to fix the build errors. Sure, it is some work to fix it, but you can usually fix it in less than 5 minutes and you probably save more time, if you don't have to specify each namespace every time. Typing std:: is not a lot of work, but sometimes it's mildly annoying.
And here goes the bikeshedding... 
You have a lot of other issues, if someone does that with your code and the using namespace can easily be fixed with a tool (does clang-tidy support that?). The bigger issues are static variables, anonymous namespaces, anything that relies on the reduced visibility/internal linkage from implementation files.
I really like this one: https://github.com/docopt/docopt.cpp
I don't like VS for few reasosns - pushing a lot of non-standard features (why the fuck precompiled headers are from `#include "stdafx.h"`? Other compilers just use commands) (Warning 4996 - some standard functions are marked as deprecated even though they are not) - bloated, installs and launches long (Eclipse was faster) - no good themes - seriously - I have browsed most of the market and almost all themes still leave 50% of the code white. Eclipse has &gt;30 colors, including setting different colors for overloaded and non-overloaded operators, parameters vs members - gitignore for VS has over 100 lines
It looks like on both polls results are similar. Both show which IDE is used more often. It would be better to make a poll which features people like and which one dislike.
Pffftt idk about u casuals but notepad is the shit
I didn't mean to suggest that was my only problem with it, I just wanted to give an example of some of the problems I had. Sorry if that wasn't clear on my initial post. Some of the other problems I had: problems with certain c++ language features (c++11,14 features), autocomplete not working on certain types, go to header sometimes going to the wrong file etc. At some point I decided to try a new editor (qt Creator) which didn't have most of these issues. Some of these had fixes, some did not. Note that this was back in the eclipse Luna days. I hope they've fixed some or all of these issues by now.
I learned c++ with DevC++! It's my pick for worst IDE as well
sed &amp; awk
&gt; it just freezes up, no amount of waiting was enough. Encountered this strange issue as well, though it doesn't freeze up for me, just the debugger inspect view becomes non-interactive until I move to the next instruction.
I use `Unix as an IDE` (mostly) but can't say I love it or anything. I waste so much fucking time playing the `why isn't neovim working properly` game that it drives me insane. Also, I fucking LOVE autocompletion. The better the autocompletion the happier I am. I view it as documentation-while-you-type more than save-you-key-strokes as I frequently work on libraries/languages/projects that I'm not quite familiar with and so those little bubbles of docstrings and extra potential functions/methods I could call are educational while I work. That being said, YouCompleteMe is probably the worst completion system amongst Xcode/Atom/VSCode/VisualStudio/Clion.
FFmpeg is a C library, which makes it quite an apples to oranges comparison with respect to software engineering and development. Legacy code bases become hard to work with when people refuse to address problems when they see them. This doesn't necessarily need to be the case. For example, anyone familiar with the Chromium project will see how aggressively developers can be in routing out cruft. They even appoint devs to do just that as "Code Wranglers". I've been looking for computer graphics software lately and it's really impressive how two "equivalent" packages for raytracing can be so polar opposites when it comes to software engineering practices. Notably, LuxCore is a nightmare to work with whereas Blender Cycles is a dream.
Good to know. I like being wrong when someone makes me learn in the process. Thanks!
My favourite is Qt Creator, though my 2nd favourite must be VS(2015+). I really miss it's in-line go-to definition/declaration feature. It helps a lot when you are trying to figure out how some huge project functions and constantly need to look up what certain functions do without switching from the context of the current file.
May be on Itanium. I'm a lot more familiar with Windows where there's a big difference between __cdecl, __stdcall, __thiscall, __fastcall, __vectorcall, etc. It has corrupted my thought process in the topic perhaps. :)
I don't have any strong issues with JB IDEs regarding the usability/combustibility, but god they feel painfully slow/laggy! Doesn't matter whether I'm on a mid-range i5 laptop with 6GB of RAM or on 4GHz quad core Haswell Desktop with 16GB of RAM, the delay (in typing, completion, highlighting, etc.) is still noticeable. And they are pretty resource-hungry to boot.
JB products are just so damn confusing. I haven't found any feature of a JB app that would make me want to use it over vim.
Best "fix": don't do that. :p Most seriously, though, it's common in a unity/blob build to have to spend some time finding the right combinations of TUs that will build together. If you have two TUs that clash like that, they'll need to be in different blobs, or be modified to not clash. That said, these days, just skip unity builds. Especially if you have the option of making large source changes: just get rid of "common" large headers pulled into most TUs, put the stdlib and other very-common headers in a precompiled prefix header, and just stop parsing/compiling the same header content in every TU. That's all a unity build is really doing anyway. (There's some further potential optimization benefits in a unity build but link-time optimization can do *most* of that these days, and even in a perf-sensitive world I've yet to find the delta in optimized performance between the two to be meaningful in relatively recent toolchains.)
Congrats on your library. There are obviously a lot of libraries in this space, but this is a good problem to tackle to learn about library building, creating good interfaces, etc. I'm sure you looked at a number of other libraries before writing your own, but I'll just mention here a command line argument parsing library that I came across a few months ago that has quickly become my favorite, go-to tool for this. It's called [clipp](https://github.com/muellan/clipp), and it has, imho, a very nice interface that is driven primarily by powerful composability of basic concepts. It's worth checking out as motivation of how one might consider handling certain features. It's also nice in that it is a single, header-only library, which makes it very easy to include in any project without complicating the build at all.
That's still the most recent edition of Effective C++ AFAIK. Effective *Modern* C++ is a different book, still in its first edition AFAIK, covering techniques specifically for "modern" C++ (using C++11 and C++14 features). I don't own Effective C++ (any edition) but it's not really a case of one book replacing another - some things described in the older books aren't the best way any more, but you may want both. 
Are you on ssd or spinning disk? I've experienced no delays since I mostly use ssd everywhere and I've used jetbrains ide of some sort daily for the last 5 years. But one laptop at work had spinning disk and idea was insanely slow to start, like 20 minutes on that one.
What was the N64 IDE called? And are you saying the PS1 used the same IDE? I'm interested in seeing what it looked like. Did the N64 have an SDK or something? Or did you just write everything you needed yourself?
&gt; A. IMHO it'd be much better if it were a header-only .h library - because then I could just drop it into a project and have it all work. Just make it an option. I personally find the idea of saving 5 minutes of setup at the cost of permanently slower compiles to be abhorrent. Header-only is an anti-feature. Optionally-header-only is a different story. :) &gt; All this cruft just vanishes! The API user never has to learn your weird ;-) names like hasArgs() but just uses plain old STL directly. Leaks the abstraction of `vector` into the public API. Would be better if it returned a `span` or some other construct at the least. :)
I am sure Visual Studio could be Nice, but I wonder how you guys customize it? Compared to CLion, it is absolutely horrible to customize. I found it non-intuitive how keyboard shortcuts are assigned (I find the simple json file in VScode so well suited, I would love it in VS). How do you add multiple cursor without an extension? If you guys could share a nice setup of VS, maybe we (other IDE developers) would take a shot at using it.
I always find it awkward when the README starts by pages of text explaining how to install (and what are the dependencies), and then another blurb on how to use it. I valiantly struggle past all this pointless (for now) information, to finally reach the example section, and what do I see: &gt; You can check your build using the test binaries that were (by default) built and emitted to `$DEEPSTATE/build/examples`. For example, to use `angr` to symbolically execute the `IntegerOverflow` test harness with 4 workers, saving generated test cases in a directory called out, you would invoke: &gt; $ deepstate-angr --num_workers 4 -output_test_dir out $DEEPSTATE/build/examples/IntegerOverflow &gt; The resulting out directory should look something like: &gt; out &gt; └── IntegerOverflow.cpp &gt; ├── SignedInteger_AdditionOverflow &gt; │ ├── a512f8ffb2c1bb775a9779ec60b699cb.fail &gt; │ └── f1d3ff8443297732862df21dc4e57262.pass &gt; └── SignedInteger_MultiplicationOverflow &gt; ├── 6a1a90442b4d898cb3fac2800fef5baf.fail &gt; └── f1d3ff8443297732862df21dc4e57262.pass Wow! Now I'm in! Looks so cool! *SIGH* --- I heartily recommend reorganizing the README. Push down the Building/Usage sections, people will get to them if they wish to, and instead put an *example* front and center. **Not a link to an example**, a simple but complete example. Like, *inline the code from IntegerOverflow.cpp* (and simplify it!) and then *inline the result*. And if it's not self-explanatory, a small blurb about what we are seeing and how it's supposed to help us wouldn't hurt. **I am not going to download and build all your stuff JUST to see what the output looks like.**
I'd say it's still a very useful book, but bear in mind it's pre-c++11. Scott mentions "TR1" in it (pre-c++11 experimental/"early access" stuff)- a lot of this will now be part of the language. You should still aim to study C++11 in something like "Effective Modern C++" but it will be hard going if you don't have a solid grounding in the earlier stuff first.
CodeBlocks works well for console applications, like for school. I'm sure most prefer Visual Studio for larger projects.
Valid point. There are many aspects of the Google C++ Style Guide I strongly disagree with. This one though I think they're right. It's like with everything else, use the smallest scope necessary and only import what you use. So better just use "using std::map;" locally in the scope where you use them, if your code becomes to verbose.
Same here, luckily the professor let us change it as he knew how shitty it was
I like `cscope`. Vim can read its tags and so you can jump from any variable name to its definition; from types to their class definitions; from function names to their definitions… you can do symbol usage lookups, all kinds of different things. I wouldn't try to refactor a complex C++ codebase without `cscope` and in fact I usually set up a post-checkout git hook to rebuild my cscope tags in the background. Makes Vim **really** feel like an IDE to me, more so than pretty much any other plugin. Although a Vim-integrated debugger would blow my mind.
I thoroughly enjoyed Visual Studio. We used Netbeans in a college class and it was buggy. I wouldn't reccomend the later. 
I also like to use terminator to have multiple displays and I use htop to monitor how the program runs 
Obviously you should get "Effective Modern C++" from Meyers. But his other two older books, Effective C++ and More Effective C++, are still relevant, they complement "Effective Modern C++" quite well. Just keep in mind while reading them that C++11 and upwards didn't exist yet back then, but they show many best practices that are still relevant nowadays. And the ones that are **not**, these are covered in Effective Modern C++. I'd get &amp; read all 3! (And possibly read "Modern..." first).
&gt; Ever wanted to isolate cv::Mat? Yep, a k-means implementation is tightly coupled with it. That's really good to know, thank you. And much worse than I thought, lol. :-O
Sounds like the SNSystems debugger. But that wasn’t an IDE, really. Unless you’re talking about the CodeWarrior IDE, though I’d be surprised if they shipped something with an off-by-one error, at the same time automated software updates weren’t a thing back then either, so I suppose it is possible. I never did N64 professionally, but the PS2 tools by SNSystems were still fairly Windows 3.1 looking back in 2001.
I think people don’t like Xcode since it’s not a clone of Visual Studio. It just takes some time to get used to.
&gt;Wouldn't you have all the platform-specific build files such as the solution in a separate directory such as "build" anyway? Yep, my gitignore for VS is just: /_build* Works for UNIX too!
Honestly, the very worst is Xcode. Because for certain projects, you're sort-of forced to use Xcode and don't have a sane alternative. Like for an iOS codebase purely written for iOS (so no cross-platform stuff, no CMake etc.), set up as an Xcode project. *shudder*. Xcode is so bad, and you can't even easily switch to some other IDE easily to build, run and debug. I really don't get how an IDE of such a big company with huge resources can be so incredibly bad, both from a usability as well as a functional perspective. At least on any other platform you usually have a choice of using another IDE, for whatever you're doing.
I second [cppcon](https://www.youtube.com/user/CppCon) talks. For reference, there's obviously [cppreference.com](http://en.cppreference.com/w/Main_Page) - there's another reference site that people often use which seems to often have errors, cppreference is maintained well and has a very low error rate in my experience. The obvious books are Stroustrup 4ed (which I have) and Josuttis, The C++ Standard Library, 2ed (which I don't). But Stroustrup 4ed is already seeming a tad dated - it covers C++11, not C++14 or C++17 - maybe there's a new edition soon. Also, I've not actually read that much of it. For getting a fairly brief overview of new features in C++11 and C++14, the Leor Zolman videos from cppcon2014 are useful. For the C++17 equivalent there's Alisdair Meridiths C++17 in breadth talk from cppcon 2016. There's vast amounts more, too - very likely including a better overview of new C++17 features, but I haven't watched much of that yet. 
It does sound doable but i still dont think its easier than intellij where you can review all your changes and just click on the ones you want to keep. But maybe i just need to see someone with a workflow like yours in action. Personally i have to pair at work so the company kind of enforces an IDE and its easier to get everyone up to speed with intelliJ than it would be with an advanced vim setup (when the configuration is actually so personal from my experiences so far). Nevertheless its interesting to know how others work and ive definitely attempted to get proficient with vim.
&gt; When you unroll the loop manually the SROA pass (scalar replacement of aggregates) manages to remove the alloca call for res, replacing it with 16 individual registers. With the loop in the middle this does not happen ( not sure why, need to check the code ). Very cool. Thanks for the explanation. This answer led me to the [list of optimizations](https://llvm.org/docs/Passes.html) implemented in LLVM so in case anyone is interested ...
SNSystems have got soo much better so then. 
&gt; Lots of style guides ban this though (llvm, Google). So? The Google C++ style guide bans exceptions!
Well there is a clear difference. The llvm code is all inside the llvm namespace so if you in your cpp files just open namespace llvm { ... } then you never have to import it.
I think, with semantic autocompletion, CTags browsing, and error highlighting, my Vim is basically an IDE at this point. 
[removed]
SSD on desktop, SSHD on a laptop. Helps with start up/indexing, not much with general latency.
Personally I also agree with not using `using namespace std;` - mainly because it is relatively short anyway. But I think, the possible impact of violating this rule tends to be overdramatized and overargued. I mean, look at this thread - ~30 Posts in total and maybe half of them are arguing whether one may use `using namespace std;` in source files (a big chunk of those is from me though ;))
Indeed. A colleague once wanted to compile a minimal version of an imgproc algorithm which only depended on cv::Mat in core. (The plan was to keep the binary size on mobile devices down.) It was exceedingly difficult to get the core library just down to a basic version of cv::Mat and nothing else. Everything's entangled in there...
Maybe startup by a few seconds but the actions and manuevering is quicker for large projects
For all intents and purposes, the software no longer exists. For PS4 development everything is integrated with Visual Studio. Thank God.
Rtags is great! All the power of IDEs but without the bloat
I'm going to say the best and pretty much the worst (I have blocked a few from my mind) is Visual C++. Visual C++ is hands down the best IDE in my opinion. But Windows is one of the worst OSs in which to develop. In linux or mac you apt install somelib, brew install somelib, or make install somelib and then it usally just works. With visual C++ there is almost always a battle to get something to compile when using an interesting library. Usually an annoying but short battle. But sometimes the battle turns into such an Odyssey that I switch to another library. 
&gt; no submodule update - this is something you should do yourself, in my view. It's not for the build process to do In my company most people do this. I think it's just another pitfall when seting up someone's project. I prefer if code is buildable by the standard *mkdir build; cd build; cmake ..; make* routine. &gt; no install step - where do you install it to? Just use the repo's copy... Could you explain this, please? Your comment about having the code all available rings true to me. Much more efficient for quickly checking how something is implemented.
Parts of XCode just rock, but I do end up fighting with it more than I should. QtCreator just end up lacking after you use Visual C++ for any length of time. For Qt though QtCreator is the only way to go. 
Thanks for checking it out! It’s an alpha release to coincide with a conference. The readme, like the rest of the code, is a work in progress. We’ll take your comments into account as we continue to work on it. By the way, PRs are welcome and we send bounties! $50-200. Documentation counts. Finally, you may be interested in our paper on this library which doubles as documentation at this stage: https://www.cefns.nau.edu/~adg326/bar18.pdf
That's a very good point that I did not consider. And optional component would be even less portable than a 3rd party lib; thanks
Can we get some love for Visual Studio Code? The C/C++ extension from Microsoft does an excellent job!
Interesting. Didn't know that. Care to share a few examples. There are a few subsets of -Wshadow that you can disable/enable, maybe one of them cover your use-case?
I think it must be something like this. IMO Xcode's UI has always been better than VS and some of the most significant improvements in VS for me have been because they introduced features from Xcode, such as single clicking to view files and the ability to usefully break windows out of the main VS window.
&gt; large values? not really, if you have a problem with range then twice as much won't buy you a lot. Just use bigger type Um, sometimes you can't just go to a bigger type unless you want serious performance hits. Embedded platforms being one of those.
Indeed it's pretty low on the list of priorities. It's only just above all the whitespace conventions.
I didn't know about hunter, sounds very interesting, thanks. I tried to setup a dependency as ExternalProject from the documentation you linked and stopped after two days of frustration. Granted, it wouldn't have worked with the workflow described on the blog neither, but I found ExternalProject to be too complicated.
It's still good. I learned what the typename keyword does from it when using nested types.
I'm sorry but if you use vim/emacs (even nano) you've cucked yourself. Using your hands to hammer nails doesn't make you talented, it makes you a fool. Use the right tool for the job ❤️
YouCompleteMe is the only vim autocompletion that works for nvim for me, but it leaves some features to be desired. YouCompleteMe needs to have tabbed insertions like Visual Studio and Xcode. Clang’s language server tool would be awesome to incorporate. Any terminal editor like vim is limited by what a terminal can do, but I can’t live without a modal text editor. The vim plugins for Visual Studio really suck. I also hate working in Windows. If Microsoft released Visual Studio for Linux, I would switch tomorrow. They are moving in the right direction with native support for cmake projects. What we need is a modern terminal standard. The idea of fusing terminal and editor is a good one.
It requires Python 2. Sigh.
Unicode handling can, in fact be done with the standard. I think there are of course an enormous complexity and a few challenges : we need tables which are heavy and an issue for embedded devices. However, a consensus is reachable because, from a standardization point of view the work is mostly done, by the Unicode consortium. The names exist, the expected behavior is well define, the test cases exists. So the potential for bike shedding is manageable even if the task is enormous. However, in a graphics library, Every thing is a prime candidate for bike shedding nothing is binary, nothing is proper or definitive and the possibility are endless. Which is not suitable for the ISO process. 
Thanks for checking it out. I would be happy to send anyone a bounty for Python3 support. It's only a few hundred lines of Python so it should be straightforward. Note that even if the library supports Python3, there might be issues since many of the underlying tools like Manticore and angr use Python2.
That's a recurring issue with Google. they do things that work for them but they can afford things most can't.
I’ve never seen someone make this assertion who has actually mastered an IDE. Sure most people have tried them but if you haven’t used one on a massive code base while leveraging advanced approaches like conditional debugging or structural refactoring, you’re just not making a fair comparison. Conversely I’ve used vi and vim (yes both) for 25 years and as a test of my own biases refused to allow myself to use IntelliJ to write some Go recently. I easily sunk 30 hours of messing with configs and plugins including the excellent vim-go and rtags. After weeks in that setup, it wasn’t close to IntelliJ. Not by a country mile. So no, “literally” everything can not be done in a good vim setup and you’re misusing the term “literally”. 
You can use __COUNTER__ to generate unique names. Non-standard but most preprocessors support it
I gotta say, Orwell Dev C++ has been my goto-IDE for the time it was actively developed (until 2015 (plus the time i still had hope)) because I found it to be really nice and small/simple, Visual Studio; which is what I switched to, is just to big and cluttered (or feature-rich) for my personal taste. But in the bloodshed time it really was unusable if you think about it. Sometimes if the IDE crashed it'd just delete all your protect files and even the code folding didn't work but would mess up your code.
I hate VS when messes up .*proj and .designer.cs files just by changing one little thing... oh, it seems that you clicked on a button in your form... let me flip over all the lines in the designer file so that your commit is messed up... -.-
Doesn't sound appealing...
Maybe managers in the team were bad with it. I would say yes, they commited dlls and other binary stuff.
The question was about c++, not go. I have yet to see a feature in a c++ ide that I can’t do the same for in Emacs. But you’re right, I’m only dealing with 10-100k loc codebases, not millions, so maybe things are different at that scale. I’d love to know what I’m missing though so I can implement it. 
Writing just 'hello world' with some modern graphics APIs is going to get you over 5000 LoC. Anything where performance is critical tends to have pretty big code bases in my experience. For a game engine honestly 50-100k LoC probably is what you would hit for a small custom made engine, just to have all the necessary features implemented reasonably. 
&gt; do you know any compiler that - by default - uses a different calling convention in c and c++ mode Nope. :)
I haven't seen much about C++ builder on here so.. I've been using Embarcadero C++ Builder for about 5 years now. I have a love/hate relationship with it. Pros: - Build Applications easy and fast - easy to use - has most things you would expect from and IDE plus some - multi platform (I don't use this feature personally) Cons: - Linker bugs - Only supports full C++11 with clang based compiler - Really slow compile times with clang compiler. - lack of third party library support for bcc compiler
It's not even 5 minutes, if you have a single .cpp file the setup time is ridiculously low.
Just the `cv::Mat` constructors are already confusing enough because most parameters are ints, but they are for the dimensions themselves or for the number of dimensions. Plus you have overloads where you supply your own pointer with data in it.
The overall design (modularity and stuff) is good, but the actual code sucks. The documentation is also good sometimes and horrible most of the time.
&gt;Note that even if DeepState supports Python3, many of the underlying tools like Manticore and angr require Python2. You should not have used them then.
&gt;First one is the example, so it's fine. I would argue that makes it worse. If there are multiples, the reader now has to figure out which comes from which. Recently, I was using an old boost library and the example section had "using namespace boost::library::sublibrary1" and using namespace boost::library::sublibrary2" and was using a few things from each. It was annoying. 
Does vim support a clang-based code model? If not, then it won’t be able to do a lot of things trivial in IDEs with a full C++ parser (e.g. those using clang). C++ parsing is anything but trivial. 
They could use a hashing function of the name and store this instead of the name in the DLL (or store both). 8 bytes should be enough to avoid collisions and can be compared easily.
get over it
Those are some really sucky APIs then. It takes well under 5k lines to implement a hello world running on top of raw OpenGL ES, with state-of-the-art shader-based font rendering, where shader draws splines and fills them. With a toolkit, similar functionality may fit in a couple of lines. Literally m. 
Xcode project files can be built from command line. You can use whatever IDE you wish with them, as long as you either import the Xcode project file or make a plugin to expose the file structure of the project to the IDE. In most IDEs, it’s not super hard to do. 
I should get over people using technology that is a decade out of date to develop new things? Sure, I'll get over it by not using this library.
&gt;python 2 isn't a decade out of date You're right, Python 3 was released on December 8, 2009, so it is 9 years 2 months and 10 days out of date. &gt;it still gets updates So does COBOL. &gt;plus dude made something thats good enough Good enough for what? People to use? Then use Python 3. &gt;no one's asking you to use this It is implied that I am be asked by posting it to a subreddit I frequent.
I use visual studio since in recent years it has become pretty ubiquitous for game development. UE4 and Unity both use it as a default and primary editor. What takes it from "okay" to "good" (but not great) for me are the addition of [VsVim](https://marketplace.visualstudio.com/items?itemName=JaredParMSFT.VsVim) for general code navigation/editing and [Visual Assist X](https://www.wholetomato.com/) (or VAX for short) which is an improved intellisense replacement. Lastly at my workplace [increadibuild](https://www.incredibuild.com/) and its visual studio plugin helps to massively speed up build times.
Doesn't applying point D make it more difficult/extend to change `ArgStream`'s implementation? When wrapping `std::deque` into a class, you're allowed to present a restricted view of the features that you want an `ArgStream` to support and the right to change the implementations of `append()`/`next()`/`hasNext()` as needed (not to mention the internal representation of `ArgStream` itself). You can also later extend `ArgStream` to have features that aren't inherently supported by `std::deque`; although, you can just provide free functions that implement functionality for a `std::deque` (taking an `ArgStream` as an argument), so maybe that's a weaker argument. I have a similar thought for F. As /u/SeanMiddleditch pointed out, you would be leaking the way that `Arguments` inside of `ArgParser` is implemented. As I said above, presenting an abstracted view over the implementation allows the maintainer further freedom to change the implementation without the user noticing (e.g. changing `std::vector` to a different data structure). 
The tools you’re talking about have decade person-years of very advanced effort put into them. It’s non-trivial to rewrite or forward port them. What matters more than language runtime is whether they work, and they do, very well. We want to find bugs and security vulnerabilities in programs. Arguing over anything else is a distraction.
If you're using neovim, you should be using deoplete along with LanguageClient-neovim and clangd. It's significantly easier to set up than YouCompleteMe and works much better in my experience.
I'd recommend Visual Studio Code with the C/C++ extension. The extension will reach 1.0 some time in March or April
vim and visualstudio express(2015, 2017). vs express edition doesn't support vim feature(vsvim), so i am implementing my own editor.
Vim and emacs have extremely good clang integration. Which shouldn't be a surprise because all of clangs tools are accessible on the command line.
Little late to the party, but I think named parameters would be best. Since the standards committee loves keyword reuse, I wouldn't be surprised if we got something like this: auto [delete, result1, delete, result2] = some_function(); function_with_default_args(default, default, result1, true); 
I love using VSCode. I have WSL and the integrated console is super convenient. 
I don't know, I still didn't look by myself, bit quite frankly I don't believe it. This is so egregious (IMO) that I just can't.
What was the hiccup when supporting all platforms? I was a bit lazy to read the whole Readme if you mentioned it there, Sorry!
Vim unfortunately doesn’t even come close to leveraging all the functionally made possible by LLVM clang-tools and related programs and libraries. Because it’s running in terminal with ncurses there are some limitations, but you can still do some really powerful things with clang-tools. Clang language server has the potential to make the ultimate auto completion engine. I wrote a small neovim plugin that controls lldb and allows me to set breakpoints in vim directly. I envision a modern terminal standard that allows more than just grid based text and curses style graphics. Imagine if you build a modal editor that combines that with all the functionally that the llvm libraries expose, but that’s the classic problem of Better IDE -&gt; better editor -&gt; better terminal emulator
I don't think you should really blame the IDE for incompetent people being unable to follow a simple process. Of course it passing a code review before being committed is even worse.
&gt;File includes over network? Why would you ever do that? :) IT and Engineering were in different cost centers so cheap NFS servers that add six hours to the build time meant big bonuses for IT!
Last time I checked out the development status of clangd it was still missing some important features. Thanks for the reminder. I’m gonna take a look at it again. 
Squirrel, a sane lua.
no, just go to cppreference
visual studio should be fine, but with its shitty intellisense makes it the worst.
No one I've heard make this assertion had actually mastered vim/emacs++ setup. If you need an IDE to refactor your code without error you should really revaluate your career choices. And visual debuggers are nice, but if that's what's holding your productivity back, your coworkers are reevaluating theirs.
What's the deal with `typedef struct {...} X;`? Since you're doing C++ the struct namespace is the same as the non-struct namespace, and this prevents forward declaring them. Just use `struct X {...};` instead. Let me add my own elf loading code too - it's in my linker project: https://github.com/dascandy/ZeLDa . Has both 32-bit and 64-bit implemented and usable concurrently.
There's so many packages for sublime that you can turn it into an IDE
Interesting I'll have to try that out. I would still prefer to turn -Wshadow into an error and selectively disable it around cases like these
&gt; This isn't good either as it destroys that ability to trace code to the library that defines it. What? Are you telling me you have a code editor that can't even trace symbols in a project with three files in a single directory and without external dependencies? Actually, even grep should suffice here. Not mention the fact, that c++ programmers should recognize (at least most) names from the standard library.
They could, but they didn't, and I don't think they will change their ABI anytime soon. Besides, given that most string comparisons end after a handful of characters (at the first non-matching character), and given that the compiler could very well make sure the strings are properly aligned and padded to allow for 64 bits to be compared in each step anyway, I'm not convinced it would make a major difference. Which leads us to the interesting observation that here we have a situation where the naming of the class in the source code actually has an effect on performance: if you name all your classes cClassInMyProjectWithMySpecialRules_ClassName (i.e. a long stretch of identical characters at the start) you'll trigger longer-running string compares, slowing down dynamic_cast. Ten years from now people will wonder where the "use short class names for better performance" rule came from. There will be bitter fights between believers and non-believers. Remember: you saw it here first! ;-) 
Yes there is no practical difference between a header only library and one that is one or a a few source files as far as setup time goes.
Effective and More Effective C++ are still good books. I'd recommend reading Effective Modern C++ first though. That way when you read the older ones you will know which parts to "take less seriously or ignore" because you know the modern way of doing it. Old ones are still great though.
Elfio (http://elfio.sourceforge.net) is another small header only, standalone, ELF reader and writer which I've used successfully in my own projects. 
No, a simple function like this: to_string(T&amp;&amp;... args);
There was very abnormal review (people who did it cared much more about indent and whitespace instead of what the code was actually doing). The most sad part was that there were no restrictions when compiling. Several thousand lines of warnings every time you compile.
There's no real reason to use C for anything in 2018. Anything that can be written in C can be written better in C++.
WoW runs quite well with 50+ addons. And I think it's even single-threaded.
https://github.com/Rseding91/LuaPlusPlus
I don't have a full answer, but I do have an anecdote. std::sort is roughly 2-3 times faster than qsort() on primitive types, because the compiler is able to inline your comparator, while qsort() has no choice but to call it through a pointer.
It's just making the problem worse though.
Colors are often represented as an RGB triplet where each value can go from 0 to 255, so it makes sense to represent them as 3 unsigned bytes *when the representation matters* (i.e. because you have to store them in a specific format). However, depending on what you're doing, you can see an RGB color as a 3d position in an RGB space and I really advise AGAINST specifying coordinates in unsigned terms (it breaks a lot of geometric operations you may want to do), so even there it makes sense to use a signed value. 
Because of the crazy performance requirements of their game.
Imho the page offers a very poor quality regarding... * ... the didactical approach - you could better read the original gang of four book; which is imho also not a really motivating one, but rather has *reference* style. But this site wants to be a learning platform‽ The examples are... well... kinda artificial - one does not understand the so simple strategy pattern only by componentents calles ``strategy_a`` and ``strategy_b``. You need *fitting* examples of a domain everybody can understand (like the different behaviors of *ducks* 😉) * ... the code: Very poor C++ code; pointer abusement und memory leaks en masse. No modern C++ - a factory should imho return a *smart pointer*! No language specific technical abstractions are chosen - in Java it is quite common to favor *interfaces* over *abstract* classes. * ... informations about built-in support for patterns or *special* variations of patterns within the presented languages (like *policy* pattern in C++ as design time substitution for *strategy*, *command* and sometimes *factories*; or *delegates* as language built-in in C# for the *obeserver* pattern, and much more!) * ... the design! (look and feel ist a bit early 2000s style) One should better read the famous book by O'Reillly's *Head first Design Pattern* and on top the original GoF book! 
That doesn't really help, unless I am misunderstanding. I think what you mean is: I can use whatever text editor to write the code - but I have to build with Xcode on the command-line. If the build errors, the errors will not show up in my editor/IDE. I will have to read the command-line and manually go to the file's location in the IDE. This is cumbersome, idiotic, and unnecessary manual work. And what about debugging - after I've built the iOS app on the command-line, can I run it on the phone using the debugger in, let's say CLion? And stuff like breakpoints in the IDE and variable watch window will work flawlessly? And for sure I am not going to write a plugin to make this work. I have to work on my work project's code, not spend time on developing some IDE plugins.
"It depends." (IIRC, I gave two choices - one was to use the same names as in the STL, and the other was to just expose the underlying STL data structure.) Sometimes hiding and delegating is a good choice; other times it's just cruft. Remember, each of those delegating functions needs to be documented, and then all the users have to read that documentation. Generally, in the early stages of a project, I will be more likely to expose the underlying data structures - later on, when I'm sure I have the right interface, I'll be more likely to hide it and provide methods. But either way, methods like `numArgs()` or `hasArgs()` are a no-go. Either delegate to the STL class, or use familiar names like `size()` or `empty()`.
No.
Im not that familiar with the Lua API, but out of curiosity: why do you need to push `nil` first?
Have you ever thought "man, it sucks that I have so many tools at my disposal, I wish I was working in a limited language so I can reimplement linked lists any time I need one?" Me neither.
* Best: VS &gt;= 2010 * Worst: VS &lt;= 2008
You can ask MSVC to default to __stdcall, however on 64-bit there is only one convention.
Sort a few (dozen) million random ints with qsort vs. std::sort.
which is why you do early access.
Cool. Is this only for single argument functions or can more complicated expressions work? How efficient is it in evaluation? Does only having h(4.f) compile down to many more arguments in assembly than what simply writing log(4.f) would do? If it manages that then I think it could be very useful for lots of stuff that requires chaining equations. At the very least as a way to simplify long expressions before their evaluation. Follow-up to that. How easy is it to define a function by itself if you know what it is? And do the derivatives and main function know if they have been evaluated already? Many functions have themselves as arguments for the derivatives so this would be very nice to have. I am thinking especially of the exponent, but also polynomials and many other special functions. I can easily see that defining functions so that they work with other expressions can become quite cumbersome. And does it handle only scalars? What about std::vectors? Do they compile down to code that is vectorized correctly if the right native environment variables are set? Either when set as arguments or when looped over?
TIL. This is great!
You can get Pro. If you can't spend ~$2500 on productivity tools while having &gt;$1M revenue, you have a problem.
I didn't downvote you, but I disagree with the prices you mentioned. VS Pro costs $500 for a perpetual license.
It's a pun in the style of "Ze goggles - zey do nothing". And yes, I should get around to that... but so should I for at least 10 other repos that I end up linking people to on a weekly basis.
rtags is so weak compared to what a real IDE provides that it's not even funny
According to procurement, no I can't. I'm not the one in charge of the finances and it's tougher to sell management on VS with only being able run it on Windows when our projects are moving towards a more multiplatform base. Hence why CLion was preferred and CodeLite or KDevelop were supported.
When I was in university, our "Introduction to Programming" was given in [Oberon-2](http://www.ethoberon.ethz.ch/). The IDE that came with it was straight out of hell. All hotkeys were Mouse button combinations, even the compilation command. The IDE would crash when dragging windows too quickly, and you could get compiler errors such as "Error, too many errors." or "Not enough registers, please simplify expression." If you wanted to compile a file, you had to place a "marker" in the file, and then click the build command. It was straight out of hell. It taught me how to code though. 
That too of course. Why else call a *link*er Zelda?
Just in case the link didn't reach you, from elsewhere in the discussion- https://github.com/Rseding91/LuaPlusPlus
This article makes it as small as 45 bytes and is an interesting read : http://www.muppetlabs.com/~breadbox/software/tiny/teensy.html
&gt; &gt; The member eof() shall return an implementation-defined constant that cannot appear as a valid UTF-8 code unit. Why not U+003 ? This is consistent with the `char_traits&lt;char16_t&gt;` and `char_traits&lt;char32_t&gt;` specializations. 0x0003 is a valid UTF-8 code unit value, so it can't be used to differentiate read of a valid code unit vs end-of-stream.
python2 was last updated few months ago, whats out of date?
I've got all the Meyers. They are all relevant, but be sure you know C++11/14/17 before, because some advise could be dated with the new specifications and it is better to know how to spot them.
Most property-based tools like RapidCheck only provide random testing, not symbolic testing, and most symbolic tools are limited to a single backend. DeepState lets you define a single harness for your app, then lets you switch effortlessly between different backends, whether they use random or symbolic testing. Many great tools are research prototypes with numerous bugs and may not always work. DeepState lets you avoid making a choice of any single framework, and lets you effortlessly adopt new tools as they make advances. Check out the [paper](https://www.cefns.nau.edu/~adg326/bar18.pdf)! It's very readable.
Name mangling might get disabled sometimes, but I guess that is a little different than calling convention.
You should get "Effective C++", too. "Effective Modern C++" is not a replacement of the former but augments it with topics around the more recent language features.
A call to "next" on a table with nil as the key is the signal to the "next" function to get-first.
Not who you're replying to but, in my experience, it isn't that I need an IDE to refactor my code for me. I just want it to do it for me so I can get back to coding. Same for the visual debugger. I work faster with it than without it. As with most jobs, you should be using the right tools for the right job. For many, that is an IDE provided by someone else as opposed to a custom environment with vim/emacs/whatever. You probably shouldn't so quickly dismiss someone's opinion and imply, because they disagree with you, that they're terrible at their job. Maybe they'd be terrible at your job or in your work environment but so what? Their experience is going to be different from yours and that doesn't make it demonstrably right or wrong. Just different.
You missed the point: why do you even need to rename things across a million lines? a) Failure of Encapsulation: local changes should have local effects; why is one change cascading over millions of lines? b) Failure of Planning: your global API had the wrong name or signature. This should be an extreme corner case -- how often do you fail to account for something used a million times? c) Failure to show Restraint: a global change for cosmetic reasons; maybe your time is better spent not changing "Init" to "Initialize" or vice versa. Every time I've needed to do a large refactor it was: d) Language or Library change: IDEs don't necessarily do this for you, and if they do there's probably a clang tool you can use from the command line. I can write a sed or other script to rename global API, or change NULL to nullptr, and admittedly that probably takes a couple minutes more some menu item, but the entire point is that such an exceptional case it adds up to maybe a couple hours of work per year? You're optimizing the wrong path. I spend almost all my time finding and making small changes, and an light, scriptable editor optimizes that case very well. You may not like the implied condescension in what I said, but notice the first sentence of my post was the same as the parents. I didn't come here to tell people IDEs are bad, but I replied to someone who came to tell people unix editor++ was (because he spent 30 hours trying, as if you could "master" an IDE in 30 hours). If you want to put other people's tools down, be prepared for a defense. If you want to talk about visual debuggers, it how IDEs hide things from you that you should probably know, I'm game for that discussion too.
&gt;Finally, pointers! In this last case, we’re in the open sea of UB, first of all because we start with a null pointer and try to increment it: this itself is NOT UB, because we’re not dereferencing Incrementing a null pointer is UB.
One approach would be to create a strongly typed bitset, where each bit is accessed by a type, specified as a template parameter. It allows the following usage: int main() { struct bold; struct italic; struct underline; using decoration = bitset&lt;bold, italic, underline&gt;; decoration d; // turn on bold and underline d.set&lt;bold&gt;(); d.set&lt;underline&gt;(); std::cout &lt;&lt; "bold=" &lt;&lt; d.get&lt;bold&gt;() &lt;&lt; ", italic=" &lt;&lt; d.get&lt;italic&gt;() &lt;&lt; ", underline=" &lt;&lt; d.get&lt;underline&gt;() &lt;&lt; '\n'; std::cout &lt;&lt; "mask=" &lt;&lt; d.mask() &lt;&lt; '\n'; return 0; } [Here is a gist](https://gist.github.com/skebanga/9f1abc3b7db7a81f8f2026f0b7c8b68c) showing the implementation details. A production ready version would have a lot more features, such as choosing storage size appropriately, depending on the number of bits you want to store, allowing turning on/off multiple bits at the same time, constexpr all the things, etc
From https://stackoverflow.com/questions/29825352/is-incrementing-a-null-pointer-well-defined The evaluation of the postfix incr is UB if final pointer not belonging (or past last element) of an array. TIL, I would have thought that pointer arithmetic (just need type and address), and thus incrementing NULL (as 0, not nullptr, of course), is not UB unless you accessed the pointer. Are you referencing §5.7/5 too, or is there any other place in the standard defining this?
Keep in mind though - Effective Modern C++ is not a good book for learning modern C++, it describes quirks and corner cases which might be handy to consult after you learn modern C++ from other sources.
I have been fretting over this the past day, as I am currently coming over to c++ from Python. I have been using code::blocks, and I actually really like it. Is there any reason I should be *avoiding* it? E.g., will it not scale well with larger projects? Is it bad for refactoring or debugging? Is there something that is clearly just *better*? I like it partly because I work on both Windows and Linux, and it runs nicely on both. The interface is kind of old-fashioned but whatever I'm focused on code. It is very easy to use, not a ton of stuff going on that is confusing. I was going to start a thread on this topic, but thought I'd bring it up here.
My intent was not to put down someone else's tools. If that was the message that came across, forgive me. My intent was to suggest that experiences vary across the industry. In my experience, I need these tools because of legacy code. I've spent my entire development career working on software which was developed by physicists. I've also spent my entire career working to bring "best practices" to those code bases and in some cases that requires large refactors which benefit from the tools made available in the IDE. I agree that in the ideal world, you shouldn't need to modify code across the entirety of a million LoC project. I don't work in the ideal world. The current project I'm working on was first released in 1990. I don't know if there are parts that still exist from then but I do know that for the majority of this projects life, it was maintained by one person. They created a very good product in terms of its usefulness for users, but the coding practices used were informed by their times and, as such, need some major work to bring forward. With that being my experience, it makes sense for me to want an IDE which provides whole project refactoring tools. If I had been working on different projects, I'd certainly use the tools appropriate. I am, in no way, implying that everyone everywhere should be using an IDE. The entire point of my post was, in fact, to suggest that instead of returning someone's dismissive/insulting attitude, you should strive to rise above and not fall into fighting at their level. But I could also just be bored at work and browsing reddit when I should be doing something more productive and decided to pick a fight. I had not heard that about the wages with regards to editor preference. I couldn't find anything after a few quick google searches but I feel that there are probably some interesting correlations to investigate.
Eclipse gives me testicle ache
Yes, why would one do that. I ask myself the same question. Let's just say that we're talking about a company with ~30 yrs old legacy code, handled in a process and a versioning tool that is probably even older. You'd like to change that? Oh boy, the only one who wants change is a baby in a wet diaper - or so my colleagues say. Thanks for letting me vent that.
It's less effective but still in effect. It will *a*ffect you.
How about Camerun?
Some employees work remotely, how come this is not possible with most positions?
Is the salary real? How come it's so higher than the average for a software developer? Is this a scam?
Do I get paid in Monero?
What does "transfer Visas" mean? That I already need to have a visa?
Yes. 
If you find such library (to query over a boost graph - I assume basically providing a different interface over the traversal algorithms), I'm interested, but I don't know of any such library. If true this means that: 1. There might be an opening/market for a new library doing this. 2. Maybe a better boot.graph-like library would be more useful before doing fancy query stuffs over it. I mean, boost.graph is old, although it's ideas are powerful. Maybe a new one would be easier to write today and would be a better candidate to build traversal tools over. 
Pointers are weird :) &gt;Are you referencing §5.7/5 too, or is there any other place in the standard defining this? Yes (§8.7/5 in C++17)
Correct, we are able to transfer any visas that you already have, as well as extend where available. We can also apply for new visas if you have a current visa while we do so. By that I mean, we can apply for an H-1b during the F1/OPT time period. 
My comments were to the parent's reply alone -- how vim with plugins *can* do everything an IDE can, although with different tradeoffs. My clang or sed based script takes longer to write, but it also serves as repeatable documentation of the transformation set used. The transformation itself can be code reviewed, and repeated again in the future. By all means use whatever tool is right for you at the time, but let's not pretend IDEs represent a missing master weapon elite developers somehow volunteer to forgo. The parent talked about million love coffee bases, but I guarantee you Google doesn't use a developer in an IDE to make sweeping company wide changes.
If the author is here, the signed integer UB comes from [\[expr.pre\]](http://eel.is/c++draft/expr#pre-4): &gt; If during the evaluation of an expression, the result is not mathematically defined or not in the range of representable values for its type, the behavior is undefined. How this is handled for unsigned integral types is that there's no overflow in the first place - adding 1 to `UINT_MAX` is modular from the get-go and produces 0, so there's no actual overflow and the result is in the range of representable values.
Very good, thanks
This is good, thanks! One question: does the FindRapidJSON.cmake example you used work under Windows? If not, what's the best way to write a truly cross-platform Find*.cmake file?
Huh, interesting. I didn't realize you could that easily use target_link_libraries to depend on other targets and their dependencies. Now it seems so obvious :-). Thanks a lot 
There are 255 unique values between 1 and 255 those two values included. The 256th value is the zero. Maybe a smaller range will help to demonstrate. Here are all possible 2 bit unsigned numbers: 0, 1, 2, 3. As you can observe, there are four of them - not three even while three is the largest value. Not sure if got trolled or not but hope this went to a good cause (actual help or at least entertainment). 
If you need `dynamic_cast` on a performance critical path, you are probably doing something wrong. People have been using virtual functions or member variables (in case they need to avoid all indirection) if they want fast dynamic casting for a while already anyway.
[removed]
Don't. Find modules are nuts, trying to feel around looking for something that looks like what you want god knows where. Use CMake config files. If CMake builds don't export their installed targets, patch them so they do. Write config files for things that don't build with CMake - it's easier than writing a find module. Put them somewhere on your prefix path. Job done, easy, deterministic.
This is **the** single best blog post about CMake that I have ever read. Thank you so much for this excellent, detailed and quite comprehensive write-up/tutorial. Absolutely brilliant, and will be of much help to my projects! Everyone share this to spread modern CMake use!
Thank you :) I was indeed very vague in the description of the find module. In the example I rely on the fact that rapidjson uses pkgconfig. If they do the same on windows it will work (not even sure if pkgconfig is available on windows). Additionally, afaik functions like find_library() will inspect standard system locations at fallback, meaning that under normal circumstances this find module will work cross-platform, but I need to test it. In reality is very hard to say how to write proper find modules as it depends on several factors: does the package you are trying to locate use pkgconfig? is it header only? does it have components? do you need it to work on all possible platforms? etc. All of this could be avoided if maintainers provide proper configs 
[Reminds me a lot of Mathieu Ropert's talk at CppCon](https://www.youtube.com/watch?v=eC9-iRN2b04)
What's the requirement ? Being good at C++ or replying to the question "what does being good mean?". Because it is such a good question! 
Done.
Really? Xcode is rock solid for C, and VS is complete garbage to me, it doesn't even suggest variables as you type...
There's nothing internally *wrong* with find-modules, we should just strongly prefer config-files. In a perfect world, every C++ project would install cmake config-files and export their targets, but sadly that's not the reality that we live in. There are always going to be projects that refuse to use cmake, often for bad reasons which the authors will never be convinced out of. There are also cases of libraries that get installed with an operating system which you can't really expect to have config-files for. Find-modules fill in that gap, and they do a reasonable job of it.
I also really appreciate that it's a blog post. I can't be the only person that hates watching long videos when reading text is a far better way to communicate information like this. 
[removed]
What's the recommended approach for multiple leaf targets that need the same compiler options/features? You will likely still need to use different options for different compilers; setting that up on a per-target basis seems rather cumbersome.
How much of a pain is it to build libcurl to use LibreSSL (or even BoringSSL if anyone is familiar with it)? I must admit I stopped investigating too hard after being so discouraged. :P
Are you doing this on Windows? If yes, I think vcpkg has them both (and ready to be used with CMake), so no need to build it by yourself. If you're on Linux or mac, just use one from a package manager? :-) Or maybe check conan.
Your problem is libraries not using cmake. My personal favourite apart from OpenSSL is ICU. If you are on windows install the libraries using vcpkg. You can edit the portfiles to target specific versions. 
Fully agree with this. Specially in case where no cmake is used but pkg-config is. Then a find module makes perfect sense imo.
&gt;Note that there is no reason to manually append -std=c++11 to CMAKE_CXX_FLAGS, let CMake do that for you! Stay away from variable land, model your requirements via properties. That's just pointless complexity. Writing `-std=c++11` is trivially easy, and everyone understands what it means. You should avoid these sorts of build tools as much as humanly possible. A simple `Makefile` is more than sufficient.
[This talk](https://www.youtube.com/watch?v=bsXLMQ6WgIk) is really worth watching though. And [these slides](http://www.steveire.com/embracing-modern-cmake.pdf) are great too.
&gt; A. IMHO it'd be much better if it were a header-only .h library - because then I could just drop it into a project and have it all work. Booo. Please stop telling people to do this, C++ people.
My package manager's versions were too old (Debian Buster). Plus, it's good practice to check in external dependencies, since you can get hermetic builds, which is nice... depending on system libraries can make for a pain sometimes, when you install a new system and things are out of whack. :(
Personally in that case I just prefer to hand-write a project-config.cmake file I can manually install alongside the problematic library. 
Whatever happened to "if it ain't broke, don't fix it"?
I would love to have something like Rust's cargo but for C. Just stick source in a folder and compile it + do package management + document generation. 
Visual Studio has the option to show exactly this: the included headers in hierarchical way. I once used it to help me to figure out the headers for precompiling headers.
I'm not familiar enough with it yet to be able to separate the good and bad tutorials, so I can't find them for you, sorry. :( For instance, the few tutorials I was following were recommending practices outlined in the first page of the blog post as bad...
Not interested. Call me lazy but recorded talks are a nonstarter, as are slides. I say that as someone that's given a lot of conference talks. Conference talks are for verbal discussion, papers are for one way communication. I'm interested in documentation that I can read quickly and reference it at my leisure. 
That's an abuse of the config-file concept. Config-files should only be installed by the package that provided the library to begin with. If anything, you should be installing a find-module for that dependency, not a config-file.
I blame the documentation. It's all reference. There are no examples. I've been learning CMake recently after many years away from C++, and this was the main pain point. Learning by reference means looking at other CMakeLists.txt. It's the blind leading the blind. 
CMake is kind of frustrating because it's a really powerful tool which can make your life substantially easier, and the modern style espoused by people like Daniel Pfeifer is nice, and when it flows you can faintly see the outline of something great. The trouble is that greatness is obscured by layers and layers of horrible legacy cruft. The language you have to write CMake in is weird and unpleasant, there's usually a bunch of different ways to do the same thing, most of them are bad, and those old and bad ways are always the ones documented in tutorials. It's just far, far too easy to write bad CMake code. What it really needs is to be stripped down to the core, the old nasty syntax implemented as a layer on top of a clean core, and the clean core exposed with a language that doesn't have gross syntax, and only exposes the good way of doing things, and guides you in the direction of doing the right thing.
Particularly in this case. The language standard should not be something that gets modified in some sub-sub-leaf CML.txt. It's perfectly fair for the sub piece to say 'No, you must supply a c++11 compiler'. But affect everything? Or worse, not affecting everything, leading to weird mixed build problems. 
Why?
It's like learning a foreign language by reading a dictionary. :-(
Comprehensively selfhood is consonant with concurrently thou.
Fair enough; that makes sense. &gt; Generally, in the early stages of a project, I will be more likely to expose the underlying data structures - later on, when I'm sure I have the right interface, I'll be more likely to hide it and provide methods. Good point -- there's no tagged releases or version numbers for this project, so it should probably be considered non-final. However, I would say underlying data structures should be exposed as little as possible once a project hits a "1.0" release (or whenever you would make promises to clients about API backwards-compatibility in future releases). &gt; But either way, methods like `numArgs()` or `hasArgs()` are a no-go. Either delegate to the STL class, or use familiar names like `size()` or `empty()`. Totally agree with you there.
Because you'd be using a config-file as a find-module. The concept behind a config-file is that it is a file installed **by** the package for which it is providing configuration information. It should contain **explicit path(s)** to the library or libraries of the package that it was installed for, and it should know the path(s) because it was generated during the installation of that package. The only credible authority on the path(s) for the package is the installer. **You** are **not** a credible authority on those explicit paths if you are someone who is acting independently of the installer. The concept behind a find-module is that it's a script which searches for the package on the system. The find-module is installed separately from the package that it searches for, but it it is written with the goal of finding that package without prior knowledge of the exact path that the package might lie on. tl;dr: Config-files should know where a package is, and that knowledge is hard-coded into them by the installer. Find-modules search for a package without prior knowledge of its location, and they are installed separately from the package that they search for.
Interesting idea!
I blame there not being standard ways to do common things. There should be one-liners built in to cmake to do common things the right way.
&gt; build It used to be, but they fixed that when 3.0 was released: https://cmake.org/cmake/help/v3.11/manual/cmake-buildsystem.7.html
Right, but if I know the explicit path to the library what’s the point of writing a find module as opposed to just writing a config file that hard-codes the locations in? Obviously the best solution is just to modify the CMakeLists of the library in question, but sometimes that can be tricky when libraries do horrible things with macros etc. 
This rant seems oddly apropos in an /r/cpp comment. ~~CMake~~ C++ is kind of frustrating because it's a really powerful tool which can make your life substantially easier, and the modern style espoused by people like ~~Daniel Pfeifer~~ Bjarne Stroustrup is nice, and when it flows you can faintly see the outline of something great. The trouble is that greatness is obscured by layers and layers of horrible legacy cruft. The language you have to write ~~CMake~~ C++ in is weird and unpleasant, there's usually a bunch of different ways to do the same thing, most of them are bad, and those old and bad ways are always the ones documented in tutorials. It's just far, far too easy to write bad ~~CMake~~ C++ code. What it really needs is to be stripped down to the core, the old nasty syntax implemented as a layer on top of a clean core, and the clean core exposed with a language that doesn't have gross syntax, and only exposes the good way of doing things, and guides you in the direction of doing the right thing. Ah, I kid. Mostly. At least 30%. C++ has more than the faint outline of greatness, and even hairy recursive templates with lots of dependent names are easier on the eyes than your average CMakeLists.txt. But the rest of it rings true :)
Man I really wish they do a syntax 2.0 in the future. Most of the ideas are sound and it works pretty well, but the syntax and that everything is a string somehow always gets in the way. Also it feels like a lot of legacy baggage is carried over and that could be removed if they made a clean cut. Backwards compatibility is less of an issue with a build system as that tends to be small part of a project and they could probably keep the old syntax around for quite a while if the new syntax is put in a separate file e.g. you use CMakeList without an extension for the new syntax or an 'use syntax 2.0' in the first line, etc. Of course that will most probably never happen, but one can dream, right?
It's fragile and it's not portable. It's fragile because 1. If you **or anyone else** ever uninstalls the package and then reinstalls it somewhere else, then **you** are personally responsible for remembering to remove the old config-file, produce a new config-file, and install the new config-file alongside the new package location. 2. It's easy for a human to make a mistake when hard-coding a path. 3. Sometime later, one of your dependencies might begin to provide config-files, and then there could become conflicts between the config-files provided by you versus the one provided by the dependency. It's not portable because 1. If you want to share your project with anyone, you cannot reasonably expect them to manually write config-files for your project's dependencies. 2. You cannot know where another person might install dependencies, so you cannot provide manual config-files for them. &gt; Obviously the best solution is just to modify the CMakeLists of the library in question Sorry, but no, this is not a good solution, because this is not portable. The best solution is to modify the CMakeLists to install a config-file *and then submit a pull request to the upstream developers, convincing them to accept the changes*. You should **not** modify the dependency's source code on your local computer and then act as if that dependency can be assumed to provide config-files, because then you are forcing users/collaborators of your project to only use your personally modified version of the dependency. This kind of needless forking is a disease, and the cure is find-modules. If you cannot manage to convince the upstream developers of your dependency to install config-files for their project, then you should use find-modules. In some sense, it's a last resort, but it's the most sane thing to do in this situation. Writing your own config-files is not sane. Forking a dependency just to give it a config-file is not sane. Find-modules aren't ideal, but they're preferable to insanity.
&gt; The best solution is to modify the CMakeLists to install a config-file and then submit a pull request to the upstream developers, convincing them to accept the changes. That is what I meant, sorry I wasn't clearer. With regards the original discussion, I'm taking about local installations only. Personally I prefer to remember to change the config file if I move the library. You're of course right about anything that needs to be shared. In that case a find module is the only thing to do until upstreams can be cleansed.
There's no reason that Windows can't support Makefiles now that it was WSL.
I'm legitimately confused about why you'd prefer to manually write and manage config-files instead of writing a find-module. It's genuinely easier and safer (not to mention, correct usage of cmake) to write find-modules rather than trying to manually install personally-made config-files alongside dependencies. And if you go with find-modules from the offset, then you're ready to share your project without any additional trouble.
Removed as off-topic.
QtCreator has been amazing for me. C++ is just a hobby for me, and Qt just feels great to use. The IDE lets you do everything by hand or use the designer tool if you're lazy or just want to have a window done quickly.
I find the caveat at the end, suggesting that parsing time is correlated to actual compile time, is not convincing and not backed up with any evidence. You can't compare parsing time of a header with compile time of a cpp file which instantiates 20 instances of templates within that header, doing SFINAE and other slow things. Transitive includes make the information the tool provides much less useful, too. What if all of your "slow" headers include one particular header which is the real cause of the slow compilation. You won't be able to avoid it. I don't think this is of any practical interest, unfortunately. 
I don't know what for. You need to talk to a reddit admin, it's reddit-wide. As a subreddit mod, I can individually approve your comments (which is how I notice) but I can't affect the ban itself. You can verify by viewing the site incognito, and attempting to click on your profile name. Someone shadowbanned will display "page not found".
Well at least Microsoft changed their projects file syntax completely with VS2010 (or was is 2012), moving from a simple text file to XML. The GUI usually does a nice enough job though it is perfectible (like why do you have to repeat settings shared across every configuration while you could write it just once), and the way the target files work is a clear improvement. I'm guessing you won't see this from CMake but rather a whole new build system. Breaking changes are usually frowned upon and quite rare.
No Idea how easy it is to use LibreSSL with libcurl sorry. But LibreSSL by itself builds nicely even on Windows.
I just have an irrational hatred of them I guess after trying to use a whole slew of them that are poorly written and poorly documented. Every one you find in an existing library has a different set of PROJECT_ROOT/LOCATION/HOME/INSTALL variables that are maybe read from the environment or maybe not. I just plain don’t like anything that perpetuates this madness. However, you’ve convinced me. Custom find- modules are the sanest thing to do and at least if I’m writing them myself I can ensure they’re properly done and documented and generate sensibly names targets that hopefully just be replaced by an upstream package-generated config at some point in the future. Thanks. 
Yep, one nice thing about find-modules is that if you write your own, then cmake will always use *your* find-modules instead of anyone else's (just remember to add your project's find-module directory to the `CMAKE_MODULE_PATH` variable).
Lol, well thank you for your honesty!
I usually don't leave the selection of the c++ standard to the whims of cmake. The decision, against which standard(s) to code is a project level decision. For one, automatic deduction makes it far too easy, to let different components being compiled in different modes and second, if I decide that a project is supposed to support c++11, I want to get a compile error, if I accidentally use a c++14 feature. That being said, I use cxx_std_11&amp; friends on libraries to communicate my requirements.
I usually don't leave the selection of the c++ standard to the whims of cmake. The decision, against which standard(s) to code is a project level decision. For one, automatic deduction makes it far too easy, to let different components being compiled in different modes and second, if I decide that a project is supposed to support c++11, I want to get a compile error, if I accidentally use a c++14 feature. That being said, I use cxx_std_11&amp; friends on libraries to communicate my requirements (but for the tests I use explicit flags again).
Makefiles existed since forever for the MS build chain.
There is no reason to directly use an assembler style build tool like make files on any platform except as a matter of last resort. If you want, you can set compiler flags like ` -std=c++11` in pretty much any build tool just fine but just because you need full control over a single loop in your program you should not write your whole program in assembler.
Right - and that is less complexity than just using cmake?
Sounds like you develop on Windows, you unfortunate soul.
Because makefiles never break on Linux.
For sure, the only time you should do this is when you're making very rigorous use of PIMPL or something conceptually equivalent.
Pc gobbledygook also break on Linux unless you use the the same dist and compiler config as the lib developer.
Or Android. I think there are three STL options to choose from in the NDK, although one was recently standardized in late 2017 I think.
Very true for cmake &lt; 3.0.0. New documentation does include more examples and best practices, but definitely not a lot. I blame the lack of interest of upstreams to modernize a system because it "works" and it's so ugly written that nobody wants to touch it.
&gt; I think you are viewing this backwards I think the direction you choose to look at it from depends on whether it's more important to target certain platforms or whether it's more important to be able to leverage certain potential dependencies. You might decide to disregard support for older platforms if there's a dependency that will make your life much better at the cost of requiring a newer standard. But I do agree that it's important to be conscientious of what standards your codebase is able to support, and to carefully weigh the choice to increment it if it's something like a library that's going to be distributed to users. &gt; of course, when you are working on toy/short-lived projects that is less of an issue Also, if you're developing an application rather than a library, it's typically not much of an issue. As long as you can export the binaries to your target platforms, it doesn't typically matter what standard you're building against. If it's important that end-users be able to build it from source, then it might matter somewhat. &gt; The problem is that many libraries have compilation switches that depend on available features Fair point. That kind of cruft can definitely cause headaches, especially when they occur opaquely due to unexpected compile flags.
&gt; This is definitely a totally reasonable desire. And now that you mention it, I don't know of a built-in way to achieve this with CMake. It would be nice if cmake could offer some way to explicitly enforce this kind of thing. Indeed - there are two areas I find cmake lacking: Selection of language version (even the cxx_std_XX flags are a pretty decent addition iirc and is there any way to disable dialect extensions like gnuc++11?) and header only libraries (although this has become better)
Not sure if you are being serious. Not only is using target_compile_features nicer and shorter to write, but is actually safer as it's compiler agnostic and portable. And Makefiles, please...
If I'm developing an App, I chose the latest standard possible with the compilers I need to support - there is imho no reason to code c++11 if 14 is available and the same with c++14 vs 17. Main problem is -in my experience - support for old Linux distros except if you are willing to ship all dependencies yourself.
&gt; is there any way to disable dialect extensions like gnuc++11? Indeed! I recently discovered the [CXX_EXTENSIONS](https://cmake.org/cmake/help/v3.5/prop_tgt/CXX_EXTENSIONS.html) property, because I was having issues where cmake was needlessly choosing to compile with gnu extensions, and this conflicted with boost in an ugly way. Just set `CMAKE_CXX_EXTENSIONS` to false somewhere early in your root `CMakeLists.txt`, and it will disable compiler extensions. &gt; header only libraries I've actually found header-only libraries to be handled very nicely by `INTERFACE` libraries.
The problem with INTERFACE libraries is that you can e.g. not add the headers as source files, which e.g. means they don't appear in the IDE and there where a couple of other useful properties that (at least in the past) were not whitelisted for INTERFACE libraries, but I can't remember which exactly right now.
Using modern C++ still involves using most of the old C++, but Effective Modern C++ deals only with topics that were new or substantially changed in C++11. The chapter titles are... 1. Deducing Types 2. auto 3. Moving to Modern C++ 4. Smart Pointers 5. Rvalue References, Move Semantics, and Perfect Forwarding 6. Lambda Expressions. 7. The Concurrency API. 8. Tweaks. Also, there were other Effective books between editions of Effective C++, but all up to and including the third edition published by Addison-Wesley. And there's what Meyers has said in a few talks - he still wants you to buy all the books, and ideally several of each I guess. 
&gt; it's so ugly written that nobody wants to touch it. Are you kidding? [It's beautiful.](https://gitlab.kitware.com/cmake/cmake/blob/master/Tests/BuildDepends/CMakeLists.txt) 🤢
Ah, good point about the IDE, although I wonder if it should maybe fall on the IDE to display files from directories that are listed as include directories of INTERFACE targets.
That’s basically what cget does 
Oh boy. Even MSBuild .targets looks better than that.
Find modules are for upstream dependencies that don’t support cmake users and config files are for downstream dependencies.
&gt; What if all of your "slow" headers include one particular header which is the real cause of the slow compilation. You won't be able to avoid it. Then the tool will show you which header is the slow one. Most of the time these sorts of includes can be fixed or avoided by code reorganisation and refactoring. I have done this several times.
Does it also show how long each of the headers takes to process?
Yes and it would be nice if cmake supported pkgconfig natively with `find_package(... PKGCONF)` like how Daniel discussed in his talk. So, we wouldn’t need a find module .
I didn't even select a bad one. Are you going to tell them their baby is ugly? Cuz, I don't want to be the one.
Both have the legacy cruft for the same reason. If old CMakeLists.txt files suddenly stopped working, people wouldn't move to the new version. We saw this happen with Python 2.7.
Thank you so much for actually uploading the complete project as a reference! This will make it so much easier to understand it.
Far less complexity, yes! 99% of the time, everything just works normally with Makefiles and standardised flags that every other compiler in the world understands. In one particular special case, you have to do a bit more work. That's okay. Windows is idiotic.
It doesn't have to map them one-to-one. If MSVC needs you to specify things that you don't need to specify on literally any other operating system in the world, then that's (yet another) Microsoft bug.
It certainly is not nicer or shorter to write. -std=c++11 is completely compiler agnostic and portable, with the single exception of a compiler that doesn't actually support any version of C++ (MSVC).
It's the current reality, and it's not changing terribly soon, because it relates to fundamental design differences between UNIX-like systems and Windows.
I am between cmake and meson just for the fact that cmake has CLION, Qt Creator, VS Studio and Android Studio support and project generation. The default targets in meson are great but there is so much good stuff so difficult to ignore in cmake. The scripting language is a hell... but the IDE support is a killer feature of cmake.
Meson documentation is light years ahead.
The guidelines recommend gsl::index but it didn't seem to be in the library. As a big-enough signed integer is it just an alias for ptrdiff_t?
I highly recommend looking at [Hunter](http://github.com/ruslo/hunter).
Dear @hikolu3, Indeed the salary is real and there is no scam. We value our developers, are looking for the best of the best in their field around the world and like to reward them properly. We know that our success is based on their skills and dedication. That's it! You can read more about or way of working at think-cell here: https://www.think-cell.com/career/overview.shtml#worklife
std::unique_ptr is a very obvious improvement on std::auto_ptr once move semantics were available.
Are there anywhere similarly good article about cmake External_Project?
If you haven't seen them yet you might be interested in [uniqueness types](https://en.wikipedia.org/wiki/Uniqueness_type), too.
CMake needs to be replaced by a more object oriented approach. It needs debugging tools and a syntax that guides you to design better code.
but there totally are, honestly! It's just impossible to tell because they're all terribly documented -.-
Not sure but most likely no.
BoringSSL does use CMake. It is 2min job to clone and build BoringSSL via CMake. I wish it didn’t have Go dependency too.
I still don't see a _good_ way to have CMake deduce the list of sources based on directory structure or rules of some sort to avoid listing them explicitly in the build file. It wasn't even addressed. Without this CMake is inferior to bash as a build system, and I don't see any purpose in learning anything else about it - I can bludgeon it into working if I need to for some silly reason.
How? Yes. Howard Hinnant, [N1856](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1856.html#Addition%20-%20Class%20template%20unqiue_ptr).
Yea totally agree - good reasons to include them.
&gt; Without this CMake is inferior to bash as a build system What!? If you think having to list your sources explicitly (you don't by the way, you can use file glob but shouldn't) is the worst thing about cmake you have a long way to go. 
This does not seem to perform any validation. Its trivial to find inputs that segfault your parser. For me this seem s like a huge security issue
The workaround for that in CMake is to make a static library with a single file that has a dummy unique symbol. It will work exactly the same, except you get the headers in the IDE! And no, you can't have a static library with only headers, as lib and ar will complain when trying to create an empty library usually.
But only very partially open-source. And the parts that _are_ open source, are GPL licensed. That's a deal-breaker for even many open-source projects to use JUCE.
u mean breakpoints on function return address
Which games did you develop?