My condolences.
Hey Andrew, always appreciate your insight. I have definitely noticed the speed of the effort to catch up as well as the effects of the newfound openness. Do you think that MSVC itself would ever maybe be open-sourced like .NET to match GCC and LLVM? Perhaps that wouldn't jive with larger business goals or would introduce incredible overhead, but I guess it's worth asking. Having to support old Windows platforms at work is a pain... I can't imagine years and years of old programs reliant on oddities in a compiler. I'm not envious of the challenge! It's also good to hear about the goal you mentioned elsewhere in the thread to conform to C++17 "this year". Excited to see Structured Bindings make it into a VS2017 Preview!
Great, thanks! The fact that you already started on the preprocessor gives hope that the priority is not too low.
There are definitely [other things](https://en.wikipedia.org/wiki/Compatibility_of_C_and_C%2B%2B) too.
[removed]
Thanks, I'll check it out. Does the automatic vectorization help in the serial case? e.g. this is in an MPI context where every process is tied to a single core, so I explicitly don't want any instructions to use multiple cores. Each process is usually affinity-locked to a specific core. 
Being a monad is a concept that certain types adhere to. If C++ wants to start sprinkling monadic constructs into the language and standard library, I feel it would be best to leverage concepts, as they're the natural way of expressing such things.
[VS 2017 has a lightweight installer](https://blogs.msdn.microsoft.com/vcblog/2016/08/22/the-lightweight-visual-studio-15-installer/) that allows you to install far, far less than VS 2015. Give it a try if you haven't already!
(no comment)
I never looked at D. C++ has a lot of legacy stuff, but at the same time a lot of people know it. The idea of cleaning the language up, without really inventing a new one (which would make current knowledge obsolet) is somewhat interesting, hence the question.
Hah, I know it was an impossible question. I'll hold out hope :)
Yes. I recently interviewed James Gosling on his life and times. Thanks for the feedback, I was overwhelmed by the stature of the person. I will try not to do that in my subsequent episodes. But it was amazing speaking to him.
What effect does C++ now being a design by committee have on the language, and how does he feel about the progression of the language? Oh, and how old was he when his first tooth fell out?
How does he enjoy working in private industry full time now
I think that this "Linus Torvalds on C++ rant" should finally have only historical value and there are a lot of reasons why C++ is not adopted into Linux kernel, e.g. allowing C++ in GCC was adopted after quite a long time and debates of what it will bring to the table and even then only few features of C++ are used in the compiler. Of course now C++ and its compilers are a lot better than 10 years ago, but it is still questionable that mixing modern C++ code with old C code will have benefits for maintainers.
&gt; you can grab daily builds off a NuGet server to try out our compiler as it is right now Is there any documentation anywhere on how to do this?
Looks like I don't need VS 2015 anymore. I just noticed they released a VS 2017 version of the extension: https://opencppcoverage.codeplex.com/discussions/663329 Unfortunately, OpenCppCoverage is quite crashy and I cannot get it to work at this time. Being able to see what statements are covered by my tests is extremely useful. It would make so much sense if VS could measure code coverage out of the box. 
Here's one I found, you have to skip the first two minutes or so for the interview to start. https://www.youtube.com/watch?v=TTVCaZVUvC0
What are those periodic up-spikes in selection and bubble sort?
That's a Qt failing, not a C++ failing. See [CopperSpice](http://www.copperspice.com/) for Qt using modern C++, sans moc.
Many of these failures involve not only proprietary languages, but also proprietary platforms.
notice me senpai (´・ω・`)
Those critisisms are very old. He's been asked this over and over the past decade.
Timestamp https://youtu.be/TTVCaZVUvC0?t=2m38s Very nice watch (has subtitles).
[removed]
How do the original ideas come to him? Does he dream it up while going to sleep, then struggle to recover the good ideas the next day, or does he simply research and evolve ideas through studies? 
You are right, but we talking about Stroussoup... What has he not been asked in the last 2 decades? 
What if he replies to this thread.
Yep, the most powerful of the vectorization optimizations take advantage of paralleling a loop on the same logical core. A single HT core is constantly executing a dozen instructions at a time, so selecting which those are is very useful for performance.
That's C++ adapting to the lack of MOC functionalities
I think recent progress shows that Microsoft can invest in conformance if they want to. And, in fact, they do want to.
You can already build that yourself(or use Agner Fog's vectorclass or one of the various similar libraries)-- But in order to do so you need compiler support for the intrinsics-- which is what this is.. If you mean a standard implementation of this stuff, there is this [proposal](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0214r3.pdf). I haven't looked very carefully at it though. 
Oh, go find some bridge crossing billy goats to harass.
Danemark? The small country in Urope, or a misspelling of Denmark?
BONUS QUESTION: What's his opinion on the W3C EME/DRM suicide specification?
Nothing controversial has ever happened regarding concepts. 
He had a problem: he needed OO and low level access to solve it. He looked for alternatives, failed to find them. So he wrote C with Classes to solve the problem. C with Classes grew in users and demand that he decided to shelf the original problem to do some polishing, hopefully to boost user numbers another 10 fold (from 100s to 1000s), then he could recruit from those users someone to maintain it. He overshot, and never got back to the originak problem he wrote the language for. Citation: cppcast episode 100. 
There are a few cases where I have wanted 128 bit divides (supported by gcc/clang and x64) but didn't have access to them in msvc, making it impossible to make the code compile. I could have used inline assembly... but ohh, according to Microsoft "you don't need inline assembly in x64". That means I will have to switch to using masm and sacrifice any chance of that code being inlined. Not ideal.
Since the measurements are made in nanoseconds, the program is very sensible to the numbers (random) and the OS. In Linux there is a property called niceness, that is the amount of processing power a process can acquire. I tested it with the maximum of niceness so my computer was almost only sending CPU to the sort algorithms test. Maybe use a library that tests CPU time is better, I couldn't find one by the way.
MSVC is necessarily a platform tool as it only targets Windows. But that's not incompatible with it being standards-conforming. We mostly want standards conformance so that [Visual C++ can be a great cross-platform tool](https://blogs.msdn.microsoft.com/vcblog/2015/04/29/whats-new-with-c-cross-platform-development-with-vs-2015-rc/) [for mobile targets](https://blogs.msdn.microsoft.com/vcblog/2014/11/12/cross-platform-mobile-development-with-visual-c/), [Linux](https://blogs.msdn.microsoft.com/vcblog/2017/05/10/linux-cross-platform-and-type-visualization/), etc. Some of these efforts are new, some old, but we want you to be able to use Visual C++ and Visual Studio for all your code. 
But isn't an intrinsic itself just an `__asm` block? And in that case, who cares about what the compiler supports or doesn't support through intrinsics? I mean, sure, they make life a little more convenient, but if you are library builder, it shouldn't matter, should it? Or am I trivializing things?
Thank you for some great questions. I will make my show MappingTheJourney - https://www.mappingthejourney.com/, a show run by people of Reddit. Community driven Podcast.
See [Stay up-to-date with the Visual C++ tools on NuGet](https://blogs.msdn.microsoft.com/vcblog/2016/04/26/stay-up-to-date-with-the-visual-c-tools-on-nuget/)
Hilarious! Thanks for sharing.
French spelling.
Qt long predates any of the C++ features that would allow it to exist without the need for `moc`. CopperSpice is hardly the right solution: it produces heavier code and has larger memory occupation than Qt, and it's a fork of Qt4 thus missing a lot of important improvements introduced with Qt5. A much better solution would be [Verdigris](https://woboq.com/blog/verdigris-qt-without-moc.html), which is full y backwards compatible with Qt, although it requires C++14, and _still_ has a worse syntax than Qt. (And yes, requiring C++14 _is_ an issue in some projects, due to the need to support older compilers or to integrate with other stuff such as CUDA.)
 No it is not an _asm block-- With intrinsics register scheduling is done by the compiler, with asm you have to do it yourself. Because the compiler understands intrinsics you can build abstractions around it(such as Agner's vectorclass), that you almost certainly could not with _asm. Also MSVC doesn't support _asm in x64 anyway, so that is a no go. 
http://www.stroustrup.com/hopl2.pdf
By "features", are you including the predatory license terms? I'm not sure it's even worth talking to someone who actually thinks string based dispatch at runtime is a desirable thing, or that moc syntax is anything but an attrocity...
I agree with you that it's easier to be productive in C++. I think the idea that C is easier comes from the learning curve; I can pretty easily learn all the features of C in a week if I did it full time. I still learn something new about C++ pretty much all the time.
You are right. My mistake.
I wouldn't say it was only of historical value - I highly doubt that Torvalds has changed his opinion on the language - but on second thoughts yeah it's not something to ask Stroustrup. It has nothing to do with him. I'd still be interested to see if he's seen the 'C++ was a joke' fake interview though, if nothing else but for the giggles.
&gt; By "features", are you including the predatory license terms? In case you missed the memo, Qt has been free software since 1998 and GPLv2 since 2000. That's 11 years before the ratification of C++11, and 15 years before its widespread adoption. &gt; I'm not sure it's even worth talking to someone who actually thinks You seem impressively angry about something in this thread, as if you had some kind of personal involvement in it. &gt; someone who actually thinks string based dispatch at runtime is a desirable thing So, where did I say that exactly? &gt; moc syntax is anything but an attrocity... Yet you linked to CopperSpice as an alternative, whose syntax is even worse. 
yes, plus ability to detect if given function is declared noexcept. Unfortunately committee decided to use one keyword for two things (to declare a promise and to enforce it) -- as result (for inlined functions), if compiler can't prove that you don't throw -- you pay. And if it can prove it -- no effect, there is no point adding it to function declaration.
When will there be a new edition of "Design and Evolution of C++" to look forward to? 
if it inlines -- you will get slower code too (if it doesn't believe your _noexcept_ promise)
You want a safer language practically the same as C++? Compile your code turning warnings into errors, and enable as much warnings as possible to let the compiler catch potential bugs early, also run an static code analyzer tuned for low false positives, whose warnings count as compilation errors. If no static code analyzer seems to be good because they show too many warnings I have very bad news for you about the quality of the code being analyzed. 
Gotcha, that's what I thought. I have lots of compute intensive vectorizable stuff. Do some of the vectorizable options try to use multiple cores though? If they do, can I selectively turn those off? (Although I think the OS locking their affinity would prevent them from going off core...)
If there is only one thing you do for C, _please_ let it be equivalent functionality to GCC's `__attribute__((cleanup(func)))`. Plain C deserves destructors too and not having support for that in MSVC is making it hard for cross platform projects. People really, really want to use that but if they need to support MSVC they can't. As an example [glib has destructors based on this](https://github.com/GNOME/glib/blob/master/glib/gmacros.h) and people really love those. But currently they are GCC/Clang only.
The [C++ Core Guidelines](https://github.com/isocpp/CppCoreGuidelines) could be a worthwhile topic, such as how far have they gone since the reveal at the opening of CppCon15 and their future roadmap.
Have you tried to use the Clang with Microsoft Codegen compiler from Visual Studio? I've used it a bit and the biggest issue I've seen is code that has a bunch of ifdefs for MSVC. (Clang with Microsoft Codegen still reports _MSVC so there are some oddities relating to that.) Edit: Apparently they're stopping work on this project, but maybe it's already mature enough for you? According to Andrew Pardoe's comment here: https://blogs.msdn.microsoft.com/vcblog/2017/05/10/c17-features-in-vs-2017-3/
The best I can suggest is that you enter a [UserVoice suggestion](https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/category/30937-languages-c). However, we're going to be primarily interested in standard features and secondarily in copying non-standard vendor extensions. 
Enter a [UserVoice suggestion](https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/category/30937-languages-c). Also, paging /u/AugustinPopa.
Thanks Andrew! [I had noticed some signs of life](https://www.reddit.com/r/cpp/comments/6gshou/vs2017vc141_is_binary_compatible_with_vs2015vc140/djjmvd1/?context=2), but couldn't guess whether that was once-off or a resumption of regular updates. Glad to know it's the latter with so many new language features around the corner (I can't afford the disk space of a full VS preview just to try out the compiler). :-D
Yes, it's back alive. The dogfood server might be sketchy from time to time, but now that we have a path forward we're going to keep the dogfood server going until we can replace it with something real. 
Please ask Mr. Stroustrup the following: Some of the recent C++ proposals such as unified call syntax, default comparisons, and the dot operator, as well as the C++17 feature of structured binding, seem to have caused a lot of discussion or controversy. Are you working on any new "controversial" ideas that you haven't yet described to a wider audience? 
Have you ever played Super Mario Bros?
I put my idea to rest in piece at the idea graveyard: https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/20060866-statement-coverage-for-c I realized that I should try to help with OpenCppCoverage.
&gt; Do some of the vectorization options try to use multiple cores though? That would be parallelization rather than vectorization; ICC supports both, and they are configurable separately. N.b. ICC, GCC, Clang, and MSVC _all_ do automatic vectorization; ICC just does it better.
We do look at those, and we do implement many. See our release notes and blog posts about fixes from developer suggestions. But yes, there are more ideas than what we can act upon. Getting them votes is the best way to get action. Or paging the proper person (as I have above.) Thanks for recording it!
What you guys think about Lazarus: http://www.lazarus-ide.org/ I've just discovered it. Cross Platform. It's free. LGPL License. Easy interactive builder. Can have Windows, Linux, OSX, Android or IOS native looking. http://wiki.lazarus.freepascal.org/images/5/5d/Windows_10.png http://wiki.lazarus.freepascal.org/images/d/dc/Lazarus1_6_Linux_Mint_17_2.png http://wiki.lazarus.freepascal.org/images/f/fd/Lazarus_1.6_on_macOS_Sierra.png Or also can look like GTK. What do you think? There are many other guis and frameworks, so many that is difficult to get discovered by people.
I just want intrinsics to calculate Z-order curve positions. 
That haven't been asked a million billion times before? No. 
oh damn this is a really great idea! Never thought of that before but it'd be so useful ;~;
I don't believe any will try to use multiple cores. I used some shifty words above simply because I'm not sure :)
That sounds interesting, thanks! I'll definitely check it out to see if we can use it.
Yea since it costs more to calculate that than the benefit you get from the cache-friendliness, usually. :(
Does he do something technical at Morgan Stanley or he's there just to attract talent?
Cget can also download and install non-cmake projects as well. * `cget install -X header` will install a header-only library * `cget install -X binary` will install a binary * `cget install -X boost` will install the boost sources There is also a bunch of [recipes](https://github.com/pfultz2/cget-recipes) for installing other libraries as well.
In range-v3 and in the terminology of the Ranges TS, a `View` is permitted to have mutable iterators.
You can use clock(), clock_gettime(), getrusage(), times(), etc. to measure cpu time. 
I strongly recommend using Clang/LLVM 4.0.1 instead of Clang/C2 3.8.
Linked here seven and a half years ago, and on my saved list ever since: [spear operator and associates](http://rsdn.org/forum/humour/3686634.flat). The description is in Russian, but the code and outputs are in English.
foo(Concept, Concept)
Needless virtual dispatch. Manual connectors: should be compile time data driven, so `Weapon&lt;Self,s&lt;'-','-','&gt;'&gt;,Target&gt;` produces a spear and `Weapon&lt;Self,s&lt;'-','-','-','E','-'&gt;,Target&gt;` a trident. Send it back for another iteration.
On a related note, is anyone else disappointed that the new C++ coroutines require an allocation? (Yes, it can be elided in some cases but not if they're being stored e.g. as a class member.) A value type for coroutine state was definitely possible, but didn't happen..
Wow. I've never even heard of `std::error_code` (and I nearly have 100k rep on SO, it's just never come up) . I implemented an error code class in my most recent large project but perhaps using `std::error_code` would have avoided reinventing the wheel. 
Tried techniques like these? Bet you have but worth a shot: http://www.volumesoffun.com/implementing-morton-ordering-for-chunked-voxel-data/ helped me make it very very nearly worth it. I had issues getting it to work because I had just started programming C++ when I was using it, but I think someone more talented (i.e anyone but me) could use it!
Yes, I've read that site. I tried plenty of different techniques, it always ended up being slower (at least on my CPU).
It's not trolling if they're respectfully making genuine points
We started using Conan on our project at work. The migration is an ongoing effort, but so far Conan meets all our expectations for a good C++ dependencies manager. A solid tool that I recommend. Some background : our project has 20 dependencies, it builds on Windows and Linux, using the SCons build tool (also an excellent tool). I have experience with 2 dependencies manager for Java (Maven and Ivy), and I could not find a counterpart for C++ until Conan came up. Also, the dev team is reactive, they fixed an issue the day I reported it to them.
Did you (MS) think about switching to llvm/clang stack over time?
What fresh hell is this?
C++. Pay attention.
Touché.
&gt; MSVC is necessarily a platform tool as it only targets Windows. But that's not incompatible with it being standards-conforming. True but for a long time Microsoft didn't seem to care about standards when it came to MSVC. I'm really happy to see Microsoft refocusing on standards but honestly I gave up on the platform sometime ago. &gt; We mostly want standards conformance so that Visual C++ can be a great cross-platform tool for mobile targets, Linux, etc. Some of these efforts are new, some old, but we want you to be able to use Visual C++ and Visual Studio for all your code. That would be nice but you do have significant competition with other platforms / IDE's. Catching up would be good though.
Can anybody explain when/why `std::error_condition` should be used instead of `std::error_code`? (maybe `error_condition` is "platform independent"... but doesn't that imply this -- and most -- examples of `error_code` are wrong?)
Oh no, I tried it already, and kudos for that! I remember 2015 requiring all the .NET frameworks even though I only really wanted C++ and Python, so again, FANTASTIC work! Unfortunately, for all the wonderful it is, it was still in the neighborhood of about 6GB larger than MinGW, gdb, emacs, and Python installs :/ Though it goes without saying MSVS is without a doubt lightyears ahead of the above in terms of user-friendliness and ease. Had I the extra space, it would be without question my choice. And again, I cannot stress this enough, *this is very much a me problem.*
Please don't do this.
`std::error_code` is supposed to be specific, right down to the specific source (category) of the error. `std::error_condition` is the general case. Multiple `std::error_code`s can map to the same `std::error_condition`. E.g. you could have 2 very specific I/O errors from two libraries that you want to distinguish for diagnostic purposes. However, you may still want to respond to that *family* of I/O error in a single way. `std::error_code`s map to `std::error_condition`s and that's where the distinction becomes important.
It was `boost::error_code` for a long time. Maybe you heard of it under that name?
Yeah, that became clear later, as misguided as I consider those arguments. But the initial flippant nonsense wasn't clearly anything but a troll, at almost an "I know you are" level...
I must have missed the memo. That, or you wouldn't know a [predatory license](http://doc.qt.io/qt-5/licensing.html) if it started chewing on your face. And the worst aspect of moc is the silent acceptance of input that will fail at runtime...
**Company:** Wargaming Sydney - [Wargaming Careers] (http://wargaming.com/en/careers/) **Type:** Full time **Description:** Wargaming Sydney is the Australian branch of Wargaming.net - an award-winning online game developer and publisher. Wargaming Sydney is the software studio behind BigWorld Technology, the cutting-edge online game engine that powers World of Tanks, the smash-hit MMO with over 100 million players, and other record-breaking titles. We're looking for **C++ Software Engineers** to join our fun, talented and multinational team that helps the other Wargaming studios deliver great online gaming experiences to millions of players. Your primary responsibility will be to work on our Linux-based server back-end, a distributed processing software architecture that can handle millions of concurrent players in an online world. **Location:** Sydney, Australia **Remote:** Partly **Visa Sponsorship:** Can be considered **Technologies:** C++, Linux/Unix, TCP/IP, UDP/IP, sockets and network programming **Contact:** jobs_sydney@wargaming.net
A sweet reminder that one can write a lot of silly code in C++ and get away with it. And a reminder about the things that people do, that have to be supported by each standard because they wrote production code like that. 
I thought this was about [--&gt; operator](https://stackoverflow.com/questions/1642028/what-is-the-operator-in-c) ;P
&gt; That, or you wouldn't know a predatory license if it started chewing on your face. And apparently you wouldn't know how to get your point across effectively if someone literally spelled it out for you. Let's see..: If you actually think what you have to say is interesting/worthwhile then stop being so hostile towards everyone that bothers to reply to you. You might have genuine pearls of wisdom to share but _no one will notice — no one cares_ what you have to say when you're antagonistic for no reason; you stop being part of the conversation and become some distracting trifle that no one takes seriously.
Why did Bjarne choose C as the language to add object oriented features of Simula? His favorite language was Algol68.
&gt; Plain C deserves destructors too and not having support for that in MSVC is making it hard for cross platform projects. FFS, just use C++ instead.
Not as nice as the [backwards-arrow operator](https://www.slideshare.net/phil_nash/c-extension-methods-18678294) :)
In fact, when the OP article stated the problem introducing the two related error "codes" FlightsErrc and SeatsErrc, I expected it would eventually introduce a common error "condition". This would have been very instructive, since "code vs. condition?" is the top FAQ on this subject. However the article ends without getting to that... a missed opportunity or maybe material for a "part 2" article?
/u/STL He overloaded the comma operator! Aaaaaah!!
These are the type of posts I very much enjoy, but am not really knowledgeable enough to actually contribute to the discussion!
He didn't, he just uses the built-in one.
Wholeheartedly agree - that is why there's a disclaimer in the post.
woops - will admit I kinda "skimmed" this one
I remember the "--&gt;" joke on stackoverflow. 
That's the operator-goes-down-to, related to the ----&gt; operator-goes-quickly-down-to. This is operator-----&gt;, far pointer dereference.
I do. Many people and projects either can't or won't.
I didn't expect this to wnd up on reddit (what's more, in two sub-reddits). Just for people that do not realize this (there seem to be a few), this is mostly a joke post - do not treat it otherwise.
&gt; Sadly, the clang/c2 toolset support for vs is terminated, since they consider msvc will soon be as standard complete as others. Wait, where did you hear about this? I never heard about it being cancelled.
Just out of curiosity, when doing the `&amp;lvalue(42);` where is the 42 on the stack? Is it the address of the parameter passed (wouldn't that be UB?)? Does it get introduced as an anonymous local variable?
Google Docs this time around, though I've used Graphviz for similar stuff before.
It's Windows-specific because the problem basically doesn't exist in any *nix. DLLs/Windows are just their own special hell.
https://twitter.com/stephantlavavej/status/871861920315211776
Any chances of us getting Modern C++ In-Depth Series with new &amp; up-to-date books on C++ and its internals, where all books would be hand-picked by Bjarne?
Much better, now you only need to replace `,` with `;` to get the same error as originally. If this really bothers you id try to go with lambdas probably: template&lt;class Lockable, class Function&gt; void critical_section(Lockable&amp; m, Function&amp;&amp; f) { std::lock_guard&lt;Lockable&gt; sg(m); f(); } void safe_increment() { critical_section(g_i_mutex, [&amp;] { g_i++; }); } though its probably not worth it. Also lock_guard creating a temporary is a fiasco. It would be nice to get some kind of control over this in the language.
1. Have you ever visioned other programming language(s) that is not C++? 2. C++ is misunderstood by so many. Can this situation be changed? How? 3. How did it feel like when you realized C++ was such an achievement? Was that happy? stressful? 4. Do your kids program in C++? Do they like it?
What about (ab-)using templates instead of the comma operator? [godbolt](https://godbolt.org/g/ynxbhq) The function `with` takes any number of arguments and invokes the last one. Temporary arguments like `lock_guard` are kept alive while the function runs.
Almost always, if you are ever storing a `error_condition` then you are doing the wrong thing. Store `error_code`, compare to known `error_condition`'s
Yes it's the address of the parameter passed. It's fine as long as you don't keep the pointer after the value goes out of scope - i.e. after the current line.
If I call a function a new stack frame should be created and such the address should contain the new parameters or the return address, shouldn't it?
In this case both the return value and parameter are references, so reference the original value back in the parent scope.
&gt; I really don't like wasting brain cells needlessly making up names for stuff Ok, the *name* ``g_i`` clearly showed that 😈
But if I do stuff like `foo(69, &amp;lvalue(42));` wouldn't the address that we get from the `&amp;lvalue` contain 69?
No. Passing a _real_ rvalue to `lvalue` creates a temporary, and the address of that temporary is passed to `foo`. The temporary dies at the end of the full expression containing the function call so there's no UB here as long as `foo` uses the address it's given immediately (as opposed to persisting it and attempting to use it later).
We evaluated it. The best I can say is that every option has advantages and disadvantages. 
I just got home and checked with godbolt and gcc. Seems like the initial information was wrong, 42 gets put in the local stack frame, so it's not the address of the parameter.
More importantly, how do you declarate an array in an object-orientated way?
cppreference has this to say about [`error_condition`](http://en.cppreference.com/w/cpp/error/error_condition): &gt; `std::error_condition` is a platform-independent error code. Like `std::error_code`, it is uniquely identified by an integer value and a `std::error_category`, but unlike `std::error_code`, the value is not platform-dependent. If you look at the three enums that are supported for error conditions ([`std::errc`](http://en.cppreference.com/w/cpp/error/errc), [`std::io_errc`](http://en.cppreference.com/w/cpp/io/io_errc), and [`std::future_errc`](http://en.cppreference.com/w/cpp/thread/future_errc)), you can see that `errc` maps to POSIX errors and the other two have "implementation defined enum values). An error code only has to be consistent for the platform you're using, but an error condition has to map to different platforms (even if they are using different underlying libraries, etc). Presumably there is more overhead when you add that platform independence, so unless there is cross-platform communication happening, it makes more sense to stay with error code
&gt;Also lock_guard creating a temporary is a fiasco. It would be nice to get some kind of control over this in the language. What I am doing now for locking is creating consoles (As I termed them): (I am using them currently only in python so see the following as a sketch not working code.) class A { friend AConsole; public: AConsole get_access(){ return AConsole(this);}; private: void do_something(int i); std::mutex m; } class AConsole { public: this( A* parent ){this-&gt;parent = parent; this-&gt;parent-&gt;m.lock();}; // delete copy and copy assignment constructors this (AConsole &amp;&amp; other) { this-&gt;parent= other-&gt;parent; other-&gt;parent = null;}; AConsole&amp; operator= (AConsole &amp;&amp; other) { this-&gt;parent= other-&gt;parent; other-&gt;parent = null;}; ~this(){if(this-&gt;parent) this-&gt;parent-&gt;m.unlock(); // else it was moved out }; void do_something(int i){ this-&gt;parent-&gt;do_something(i);}; private: A* parent; } int main ( ){ auto a= A(); a.get_access().do_something(1); // fine grained locking (equivalent to what Boost describes as internal locking) { // coarse grained locking(equivalent to what Boost describes as external locking) auto a_acc = a.get_access(); a_acc.do_something(1); a_acc.do_something(2); } // a.get_access(); // a.do_something(1); // no access, no chance } The Console is basically a class specific lock_guard but also all critical methods can only be accessed over the console. For OPs example this is of course overblown but for some objects it is worth the pain to reimplement all necessary methods with argument forwarding for the security to avoid (certain) race conditions. 
Pretty sure he said he isn't the author of the website.
I'm not very familiair with scripting languages for C++. But how does this compare to ChaiScript? And are their any reasons you would use a scripting language like LUA over those 2?
To initialize the parameter of type `int&amp;&amp;` a temporary object of value 42 is created ([\[dcl.init.ref\]/(5.2.2.2)](https://timsong-cpp.github.io/cppwp/n4140/dcl.init.ref#5.2.2.2)), the temporary object persists until the completion of the full-expression containing the call ([\[class.temporary\]/(5.2)](https://timsong-cpp.github.io/cppwp/n4140/class.temporary#5.2)).
Thanks alot. I figured it out already using godbolt. Love that thing.
If it was a value parameter and not a reference, it would be as you thought.
It will put 42 on the stack, yes, but that's _in order to_ take its address – you can't form a pointer to something that isn't in memory. See e.g. [godbolt.org/g/6iyzbK](https://godbolt.org/g/6iyzbK): `foo`'s arguments are passed in `rdi` and `rsi`.
Having C++ exceptions in C code is undefined so it's hardly portable if you want to handle that in callback functions - it's probably a good idea to provide a C++ version of API which accepts a template function in addition to C version accepting `extern "C"` function pointer, for convenience of using a library in C++ programs. Something like that I suppose (also add C++ detection in your headers obviously). #include &lt;stdio.h&gt; extern "C" void accept_callback(void (*callback)()) { puts("Accepting C callback..."); // code callback(); // more code } template &lt;typename T&gt; void accept_callback(T callback) noexcept(noexcept(callback())) { puts("Accepting C++ callback"); // exception safe code callback(); // more code } extern "C" void c_callback() {} int main() { accept_callback(c_callback); accept_callback([] {}); } If you just want to have some sort of RAII in C, not necessarily compatible with C++ exceptions, then use `goto`s. There is nothing wrong with using `goto` in C, it's by far the clearest way to free resources used by a function in C.
I'm aware of that. The question was whether it would be put in the local stack frame or the address of the parameter. It's all clear now though. Thanks.
It's on the FAQ of his website.
Just had a look. From what I saw it just says that it wasn't real - which we know. http://www.stroustrup.com/bs_faq.html#IEEE
I agree that it's annoying naming things, but lesser of two evils man. If a co worker was writing a lot of code like this and got it merged, I'd surely be fantasizing about torturing them.
Is there any chance for Visual Studio (not code) on Linux? I heard it is deeply integrated with the compiler; is there any chance that would change? I've worked on Linux w/ gcc/clang my whole career and among people who have worked on windows, VS has a legendary reputation. Would love to be able to try it.
http://doc.qt.io/qtcreator/creator-writing-program.html
Using OS-specific APIs. Or you can use a big OS-agnostic library like wxWidgets or Qt, which handle interfacing with the OS for you. That said, not sure why your IDE would in any way be relevant to the problem.
VS for Mac exists but has no C++ support yet. I think Linux would be come far later than that if it were to come. 
Does he prefer coke or pepsi?
Item 1: Removing Concepts. 
Link to subreddit: r/cpp_review
I'm fantasizing on it becoming a successful idiom :D
This was my first design but once I realized of the comma I just thought: scratch that!
I had a feeling that's what this was gonna be based on the title. The more real-world situation I find is getting a `std::vector&lt;std::unique_ptr&lt;T&gt;&gt;::iterator`and accessing with `(*it)-&gt;`. It would be interesting if `it-&gt;-&gt;` or some such thing were allowed, but maybe I should just be doing `(**it).` instead
Go to stackoverflow with this sort of thing!
THIS SR IS NOT FOR C++ QUESTIONS! Use /r/cpp_questions
Ah yes, I forgot to mention when I sketched that and abandoned in favor of comma, not only commas won't require any utilitarian, but also, evaluation order is guaranteed! With the `with` statement it isn't.
As I've commented in that blog series some years back, sadly [not even books cover it](http://blog.think-async.com/2010/04/system-error-support-in-c0x-part-1.html?showComment=1423271831644#c3029788187789243763).
Permit me to point out the review facility provided by the Boost Library Incubator www.blincubator.com. To get listed in the boost library incubator, a library should fulfill some basic reasonable requirements such as having documentation, tests, a repository, etc. If it passes that bar, the author just fills out a form describing the library with a pointers to the above. At that point anyone who wants may write a review of the library. This review is also a form where different aspects can be rated and commented on. The review has been designed to mirror the requirements of a review for inclusion to Boost. There is also a facility for gathering statistics so libraries can be rated by "stars" for documentation, code quality, etc. One can check out the review(s?) for the safe numerics library to get a feel for what the review looks like. Hopefully, having what to my mind is a concrete example of something similar to what you're proposing might help keep the discussion more focused on a practical idea rather than our usual habit of going off into space with more speculative, unrealistic and utopian proposals. more realistic and focused. Unfortunately, this facility is almost never used. I'm sure that one reason this is the case is that writing a useful review is actually a lot of work - much more than most people anticipate. There may be other reasons as well. I'm not sure what to do about this - but I'm happy to receive suggestions and comments on this or any other aspect of the boost library incubator.
This mostly reflects what is needed for boost inclusion, imposes things like directory structure etc. I want r/cpp_review to be bit more liberal with these things. Having six pages of requirements for submission only. Also I decided against hosting this as a community myself, for various reasons. One of them was that there already is a thriving C++ community on reddit, which could participate in such reviews easily. Most people don't want to create an account on some website, just to be able to write a review. Also I'd like to split the review as a group effort from a single persons decision if the library should be certified/accepted or not. To write a review its required to login to the page, which seems not to work. I don't see a registration form for a user, but when trying to add a review, all it says that I should login as a registered user. The page offers some good content, but its not very user friendly in my opinion.
There is now a place where you can submit your libraries: https://www.reddit.com/r/cpp_review/comments/6ilf8q/rules_stuff/dk66cce/ My User Group in Düsseldorf will host a [discussion and first review](https://www.meetup.com/de-DE/C-Dusseldorf/events/241245563/) of kvasir::mpl next week wednesday
Lua is good for a C project. Great language. My only gripes are 0) 1 based indexing, 1) no += /= |= etc. I havent used Angelscript enough to complain, but I think whatever issues it has are offset in its ease of use with C++. Haven't used Chai. The claimed type safety in binding looks interesting though.
Yes, C++ is cancer and this is the proof. Real valid code can look like this any you will have no idea. I would have believed if the disclaimer was absent. Kill C++ with fire.
Depending on your aims, how large of chunks of code you want reviewed, etc., one might also consider the Stack Exchange [Code Review](https://codereview.stackexchange.com/questions) site.
Please focus on standard C features. `restrict`, array dimension using `static`, complex numbers, etc. That makes supporting MSVC for many open source C libraries more easy.
"VS for Mac" isn't VS in anything but name – it's actually Xamarin for OSX, .NET-only through and through. Just shows how thoroughly confusing this whole branding strategy is... ;-]
Lua is good for more constrained environments (it's incredibly lightweight) or C codebases, ChaiScript is better for C++ speficic integration and AngelScript is in-between (there are others). Also ChaiScript is the most recent scripting language in C++ but it's the only one I know that focuses on being used with C++ (except Falcon but it's not not maintained anymore)
In python there is the term "pythonic" describing if something is in the spirit of the languages founding fathers. As in: "The most pythonic way to write this down is ..." or "Map/filter don't feel very pythonic." What is the equivalent term or expression for being in the spirit of C++? The article, while a joke (just like C++ originally, look it up), feels very C++y.
True, that sucker probably got more gender study credits than computer science, if he even went to college. 
Very good point ...
There is a 2013 article on a Russian programmers web site (in russian only, sorry): https://habrahabr.ru/post/184436/ The author used the same long arrow operator '---&gt;' to create a safe RW-locking pointer and a CoW pointer. 
Actually just taken from http://en.cppreference.com/w/cpp/thread/lock_guard.
My first thought was "Why do we need this, there's already the boost library incubator, and also SO Code Review". But actually this is pretty cool! Very nice concept, best of luck to the project!
Thanks!
&gt; 69 https://imgflip.com/s/meme/Futurama-Fry.jpg
I had a problem using that once because of licenses. They won't accept links to code in a repo, and want it posted there (which makes sense, because the repo could go away or change and make the review there worthless). Code you post on Stack Exchange will be MIT licensed, so unless you're cool with licensing your code that way it's kind-of a no-go. When I had my issues, I was using the MIT license but they hadn't switched to it yet, and I was uncomfortable implicitly licensing it differently on Stack Exchange.
This is a pretty helpful criticism. I've had running difficulties getting the website to function as I desire. Having to mess with wordpress, php, etc, etc makes me much appreciate C++ much, much more. It is focused on Boost Inclusion. But I think the principles required for boost inclusion are either easy to adjust to (directory structure) or good practices that any serious open source library should reflect ( tests, documention, etc). The reason for "requirements" (I never thought it was six pages) is to exclude the library which is "this is some really cool code, here it is" which is way, way too prevalent and wastes everyone's time. One can't expect others to invest effort and time in his code without investing time himself. I understand that the site registration is a pain, but there is a whole issue about keeping out spam, reviews disguised as ads, etc, etc. "Also I'd like to split the review as a group effort from a single persons decision if the library should be certified/accepted or not." Hmm - I'm not sure I understand this. The Boost Library Incubator review section just lists the reviews, and summarizes the results in number of stars along different dimensions. To align with boost it does have an accept/reject button, but that's not a determinant for acceptance - only boost authorities can do that. "but its not very user friendly in my opinion." I'm sympathetic, but it's not as easy to address as it first might appear. Of course if any php/wordpress gurus want to lend a hand, I would appreciate it. 
We got numbers like 42. It's basically number foo bar.
Trust me, I know your pain with WP and php. It is great from you building this site, but IMHO in the time since its start, boost should have taken over the project and improved it where needed. But I don't want to get started on the problems of boost here... Even if Registration is not a pain, it keeps lots of people from ever participating. Ofc. choosing reddit also locks lots of folks out, but as it happens, the interface of Reddit is pretty much what I think is needed. So why reinvent the wheel. Splitting reviews: oh yes, that was not what I meant. Now how to say what I actually mean... I want to split the long parts of reviews into dedicated discussion threads. The review at r/cpp_review should only be accepted/conditionally accepted/declined and a list of things you like/dislike on the library. I don't want any discussions in that thread. In every other thread, there is a dedicated topic on the library.
BMI2 can do bit interleaving; that exists already.
Nice, thanks! Seems libmorton [implemented it recently](http://www.forceflow.be/2016/11/25/using-the-bmi2-instruction-set-to-encode-decode-morton-codes/). I'll have to try it out (when I get a CPU that actually supports it :P).
Check [this response](https://www.reddit.com/r/cpp/comments/6mnrx2/microsoft_visual_studio_2017_supports_intel_avx512/dk6o0uo/). :)
Ok. Didn't like it either way 😉
Oops.. I should probably have pasted some example code to help getting up to speed: #ifdef MANGO_ENABLE_BMI2 static inline uint32 u32_interleave_bits(uint32 value) { return _pdep_u32(value, 0x55555555); } static inline uint32 u32_encode_morton(uint32 x, uint32 y) { return _pdep_u32(x, 0x55555555) | _pdep_u32(y, 0xaaaaaaaa); } #else static inline uint32 u32_interleave_bits(uint32 value) { value = (value | (value &lt;&lt; 8)) &amp; 0x00ff00ff; value = (value | (value &lt;&lt; 4)) &amp; 0x0f0f0f0f; value = (value | (value &lt;&lt; 2)) &amp; 0x33333333; value = (value | (value &lt;&lt; 1)) &amp; 0x55555555; return value; } static inline uint32 u32_encode_morton(uint32 x, uint32 y) { x = u32_interleave_bits(x); y = u32_interleave_bits(y); return x | (y &lt;&lt; 1); } #endif That won't compile as-is but should give a jolt to the right direction.
I expect VS for Mac will become more like VS and less like Xamarin Studio. But as I said, there's no C++ there right now so my team doesn't have a lot to do with VS for Mac.
How do I update the version of Clang to 4.0.1? I have the most recent version of VS, but Clang is still version 3.8.
You are awesome guys. I don't want to use and learn other language to set up our website ;)
This feels like a more open approach to reviewing libraries. I myself am someone who does small bits of c++ as part of my job (integrating libraries as plugins for vfx host applications) and probably more of a c++ 'serious hobbyist'. From my perspective, the c++ community leaders could do with listening more to the day to day users and their experiences. Making the language and libraries easier to use is very important I believe to the future of c++. I refer to a talk by Timur Doumler called "Readable Modern C++" (there is a lightning and full version on youtube). This talk probably gets across the feeling I have about the language. I hope that this new review community gives a louder voice to those who aren't prominent committee/corporate/boost members. 
Clang/LLVM from llvm.org, not Clang/C2 that comes with VS.
We are focusing on standard C features. We haven't ironed out details yet--what version of the standard to target, [whether to implement VLAs](http://www.clarkcox.com/blog/2009/04/07/c99s-vlas-are-evil/), etc. For the time being, we're tracking our progress using GCC's feature lists. * https://gcc.gnu.org/c99status.html * https://gcc.gnu.org/wiki/C11Status
You are using memory management, look up [RAII](https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization). It's one of those things that you'll know when you need more. The STL is a powerful creature, use it's features when you can. *edit:* But if you're comfortable with what you've got now and want to learn more, you might be due for a good book. There's a comprehensive list in the sidebar.
Manual memory management is generally only done these days if you implement your own data structures. If you don't do that it's still a good to know how to do it, but not essential for wiring C++ as you've experienced. There are more topics like this. I couldn't be bothered to learn the internals of iostreams, for example. Something you should really look into is template metaprogramming, that is an advanced feature that you do not want to miss out on. My suggestion is that you pick up a good book on C++ so you at least know what other C++ knowledge you might be missing out on, so you can refer to it when you actually need it!
You are doing everything right. No one should be doing (explicit) memory management unless they really have to. And if you are not sure then you don't need that. 
### You are doing the right thing. Explicit memory management is difficult and error-prone. RAII is the way to do it.
&gt; It's way lower priority due to way lower usage How do you measure usage?
Do you use standard library stuff like std::vector, std::list, std::map, etc.? If you do then you are using complex stuff 'new' and 'delete' or 'malloc', 'calloc ', 'realloc', and 'free'. This is the beauty of "RAII" you do not have to think about memory allocation. Even Bjarne Stroustrup the creator of C++ says that using 'new's and 'delete's should be avoided. The most complex thing I could see is using placement new (writing object directly to a block of memory) . I could be useful in video games where allocating and deallocating memory could lead to huge bottlenecks or small scaled embedded systems where memory is a premium. C++ is a general purpose language and therefore is can be applied to many domains the trick to using C++ is keep your code simple.
It's not too late for coroutines! Come and discuss in #coroutines on cpplang.slack.com. Write a proposal. Email Gor. Make your voice heard!
So much time and off hand knowledge.. 
We measure usage in many ways. A lot of it is just talking with developers and being out in the community. I don't read /r/C_AT much, but it's the only C-language subreddit I know. And if you look through UserVoice suggestions, Stack Overflow questions, bug reports from developers, app crash data, etc., it's pretty consistent. It does make a difference when you reach out to us. The next time I need to push on getting C support I'll pull out this thread and use it as evidence that C is important to our community. 
Just to put things in perspective... You have the luxury of considering only MSVC, GCC, and Clang. In that prospect, one of them will always clearly be the third of the three. Some have, in their professional life, to deal with other compilers. And when you do that, when you realize that it's not only C++11 support that's missing, but sometimes even something like C++98 isn't fully/correctly supported (I am NOT kidding), then you LOVE it when you have the opportunity to work with only the top three. Any one of them. No matter the version. As long as it is not 10 years-old. The best moment in your professional life is then when one of those crappy (but otherwise prestigious) compilers is "decommissioned". 
Obliviously depends on the project, but as a student games programmer, there's so much good that can be done with using the heap memory. Especially as far as console programming goes. But if you've managed to get this far without, I'd say you're doing pretty well and probably shouldn't worry! Still, it's an opportunity to learn if you're interested. :-)
As someone who tried many solutions (python, lua, squirrel, angelscript), here my comments. For me it was not clear how Angelscript had to wrap my objects. I recall having some kind of intrusive reference count, but could not get it. What got most in the way for me was, actually, that Angelscript is statically typed. I really think that for my scripting purposes (at least) it is better to have a dynamic language. For a static one I already have C++ after all. Angelscript advantage seemed to be that it had been tried in lots of platforms and is similar to C++ (from a C++ POV). I also tried lua + bindings, it is nice, but lua looks so irregular to me compared to other programming languages (indexing at 1, no classes at all by default, etc.) It is brief though and resonably easy to bind. I did not try it in mobile yet so I cannot speak for this part. As for python, I just cannot use it for all platforms easily (mobile for example). Squirrel: tried and it looked nice, but at the end I left it bc doing bindings through its c++ wrapper through sqrat. At the end I went for Chaiscript and did not move from there since then: dynamically typed, similar to javascript and good integration with C++, very good indeed and its strongest point for my needs. Slow to compile though.
Well if you use the Standard Library then you are using some forms of memory management you just don't know it. Otherwise you are doing the right thing, the crafting of your own memory management solutions should be avoid when ever possible. For one it is error prone. Second; you spend a lot of time getting custom memory management working correctly. Instead you use the features supplied by the standard library as much as possible. Like others here I suggest reading up on RAII which is a form of memory management that you may not recognizes such. C++ does have some good features even if the language is really starting to show its age, RAII has been around a very long time though and is well regarded.
Note: My understanding is that VLAs were removed in C11 in large part to concerns about people blowing their stacks with them and/or making it easy to create other kinds of exploitable vulnerabilities. Even if you use C99 I'd avoid this feature if at all possible.
No. Your locals have to go somewhere and disallowing use of noncopyable and/or nonmovable types within coroutines would not have resulted in something generally useful. Being disappointed by this allocating is like being disappointed that `vector::reserve` is allocating.
Hmm, I suppose I misspoke when I said "value type." I just meant "the persisted locals struct, rather than a pointer to it". I didn't mean to imply that unboxed coroutines would be movable or copyable. If coroutine types themselves were noncopyable and nonmovable, nothing would prevent using noncopyable/nonmovable types as persistent locals within the coroutines. Having unboxed coroutines being noncopyable and nonmovable wouldn't impede much or surprise anyone. If you need to copy or move your coroutine, box it.
That's no where near as controversial compared to the other non-controversies.
Your memory management "keywords" are `std::make_shared` and `std::make_unique`. Your linked list is `std::list` and it's reserved for the 10^-40 % of cases where it is actually faster than an `std::vector` (or when the iterator invalidation semantics are convenient). If you're using (the right) 5% of C++, you're doing it right.
accept operator dot 
" I just declare a variable/object in the appropriate scope and leave it at that" Perhaps it's the best Modern C++ definition I ever read
It's great, if you are able to write good and relatively complex software without such sort of things( manual mem. allocation etc.) Straustrup would be proud of you ). But the point is that it's very useful to know about how it works inside. It will be good if you will know the classical C++, but it's up to you.. And yes, you are doing things correctly, cause of what you're doing is main goal of modern C++
&gt; [...] unless they really have to. Want to change that: Noone would ever need memory management, unless they want to build something that actually manages memory.
Or more likely, you have inherited a twenty year old mess and you need to fix it
Unless you do something really interesting, you don't need to manage memory yourself. Stack allocation, or heap allocations bound to the stack (unique_ptr, shared_ptr) are your best tools since they do all the necessary deallocation and express ownership very well. I remember one project where explicit memory management was necessary. A lexer implemented in C# where we found that most of the time was taken by GC, because we were throwing away maybe 90% of what we allocated. The solution was to implement a memory pool. Code [here](https://github.com/SusirinkeLaborams/LaborasLang-aka-ForScience-/tree/master/Lexer/Containers) for those interested.
To be a good Redditor I don't like to upvote these posts until I've listened to the podcast. But by that time the post is hard to find. I almost always like the episodes and if there are others like me these posts would have a lot more upvotes. Just saying.
I think it's normal. I rarely do any manual memory management, and most of the code I write is fairly simple. But I understand how to manage memory, how to implement many types of data structures, and I have a good understanding of algorithms. Classes by themselves are more than enough to solve an enormous range of problems cleanly and efficiently.
I think templates are a major contributor as to why C++ is being considered a "difficult" language. Eventually there's no way around using them if you want to write effective code, but one needs to know a good number of rules relating to them ( ADL, two-phase name lookup, template argument deduction, ..., later on SFINAE ) to work with them in an efficient manner that isn't going to end in trial-and-error programming. As for doing things right, C++ is a language that shouldn't be thought of as an efficient language. Lots of languages are efficient, and I can also write code that's efficient in C or even Fortran. In all three of them I can do manual memory management for data structures or write my own containers and hash maps. However while using RAII, templates and so forth, I can retain the same efficiency while writing effective code. In general, if your code is both, effective and efficient, that's when you're doing C++ correctly - and that is pretty much C++ strong point over C and Fortran. Finally, C++ has some corners that are really confusing, but that you for the most part can avoid. In this category you'd for instance have locales/facets, the std::atomic consume-release modes with std::kill_dependency, the C++11 minimal garbage collection support or even goddamn std::valarray (never, ever use this abomination). You won't need these in practice but each of them is somewhat difficult to learn. If you're looking to improve your C++ skills, I would strongly recommend a book (see sidebar) instead of just doing learning-by-doing. Especially in relation to advanced concepts, having read a comprehensive treatment first will make getting into features correctly and without tripping over rules all the time much easier.
First, the good things: you're not doing memory management. That's always a plus, you should not see memory being allocated and deallocated. Use [RAII](https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization) as often as possible. That's good stuff. However, by the way you talk about it, it may very well happen that you actually see from time to time the allocation keywords, and you probably are not sure how memory management is done. And if we were working together your description would make me rush to your code and check it multiple times for memory leaks. The point is that ignorance in this case is not the proverbial bliss. Ignorance can be very dangerous. First issue, don't mix malloc and delete, new and free. While behind the scenes they do pretty much the same thing, the difference is way more subtle, and you might not realize it. In general, use one method of allocation. Respect the method of allocation that the code you call uses. For example, if you have calls that allocate memory for you you need to make sure you free that memory area properly. Second, data structures should be used based on needs only. You generally don't need linked lists - unless you're working from both heads, somehow. But even then you can implement that easily in a vector. Even so, you need to understand the strengths and misgivings of data structures. When to use a map, an unordered map, when to use something more sophisticated. Almost never should you try to implement your own data structure - unless your concern is motivated by performance or sophistication of algorithms involved. Third, github is not the place where you would find high standards for code quality. While you might find projects with draconic leaders that impose high quality choices, it's hard to see them from projects with lax quality standards. Fourth: don't try to use every feature of C++. In fact, more than half of the language is only useful for library implementers and for very specialized jobs. However, don't ignore your ignorance when it comes to the language. It's worth trying to get to know all its features. The more you know, the more options you get for solving your problems. Maybe you can solve a lot of stuff from compile time (that's what template metaprogramming is for). Maybe you can write your code in a more generic fashion. Not knowing your tools, using only a small percentage of the language, will give you the impression that every problem is a nail, and all you need is a hammer. And it's not the case. Also, see the advice you receive from the other posters. Use STL, understand what RAII is, read the books from the sidebar recommendation.
I appear to be missing the point. What part of this is "without the ugly"? All of this appears to be ugly. &gt; But I believe that borrowing the concepts of functional programming can nudge us towards the terse and expressive writing these languages tend to have. This does not strike me as an inherently good thing. Code has to be self-explanatory to a variety of readers who will be new to it for purposes of review, reference, and maintenance in the future. Not everyone has an IQ of 180. If a code snippet is so lacking self-explanatory properties that it takes a blog post, it should most likely not be seriously used, unless its advantages are so pervasive that it is advantageous to make it part of core knowledge that is taught to all team members.
I had no idea the comma operator even existed... does this mean it's normally frowned upon/to be avoided or is it esoteric for other reasons? I haven't quite taken the time to understand it yet, but it looks like this is a good use case.
I agree. When you have to say, "Don't panic, here's how to read this", you are failing at team programming. 
All in line with you guys on the fact that code should be as self-explanatory as possible! And thanks for taking the time to read. The exhortation to the reader not to panic is in the library part, the implementation of `make_multiple`. It uses C++ techniques that seem relatively advanced to me, hence the disclaimer. I suppose that it wasn't hard to understand for you though. Clients of `make_multiple` are not supposed to see this anyway, since its library implementatation. My goal was to make the resulting client code as simple as possible though, by hiding the technical stuff. Do you have an opinion on how the resulting client code looks like? I'd be interested in hearing it.
I honestly don't understand the details. That could be partly because my IQ is not 180, and partly because I lack functional programming experience. I'm familiar with functional programming in theory, I'm keen on the idea of no-side-effects, and I think that's a useful principle to maintain in imperative languages. However, things like monads seem to me functional workarounds for things that imperative languages support natively. I find it hard to wrap my mind around them because I've never actually constrained myself to the limitations of a functional language. So in other words, unfortunately, I did not understand the full extent of the post, other than that you introduced a way of generating functions. If I was asked to maintain this code, and if there's just one or two usage cases for this generating function, I would rewrite it in a way where it is clear what happens by reading the code imperatively.
Fair enough.
Stop bragging about how great your code is.
Every time I read such posts on functional programming in C++, I think of the "I can write C in any language" syndrome. I am not sure if this is the case, or if this is bias on my part :(
For me there was 2 somewhat recent talks which enlightened me on system_error (std::error_code, std::error_condition, ...). It may be of interest to those who did not "get it" yet: - Mongrel Monads, Dirty, Dirty, Dirty - Niall Douglas [ACCU 2017]: https://youtu.be/XVofgKH-uu4?t=12m51s - C++Now 2017: Charles Bay "C++11’s Quiet Little Gem: \&lt;system_error\&gt;": https://www.youtube.com/watch?v=w7ZVbw2X-tE
And you are stuck with c++98 or older and can't upgrade it :(
Is your recommendation because of improvements from the clang version upgrade, or because something in Clang/C2 is problematic or less convenient?
*I upvoted both you guys and I have zero intent to bash VS as IDE.* No, it is not, not even close. With recent improvements to CMake yes it becomes a viable one, but not as much as QtCreator+ClangCodeModel combination. Microsoft written IntelliSense engine is horrible in compare to what clang provides. They could have used ClangD (or libclang), but they don't and TBH their own parser and IntelliSense is not even close to what clang does provide. I hope they reconsider this decision. 
That's ambitious, and sorry, but this might sound kind of mean: When I can install Clang as a toolset in VS &amp; compile &amp; link for windows, what benefits would MSVC offer? It would seem, at least to me, at that point Clang does everything Visual C++ does and more. It doesn't make sense at that point from a developer point of view to support MSVC unless there's some significant upsides. LLVM / Clang also isn't doing nothing while MSVC is trying to catch up.
Reading through the code, I found the code really hard to read too. Now, as I tried to point out in the post (but my comment was removed), it does not mean that Monad have not their place in C++. On the contrary. If you look at range-v3 documentation, https://ericniebler.github.io/range-v3/index.html#range-views, you can see two very interesting range views: * view::transform, a lazy std::transform * view::join, which flattens a range of range into a range When these two range views are both combined, they form the (&gt;&gt;=) operator of Haskell. So the range-v3 already implements the Monad concept! There is no need to: * use cartesian_product to do it * or implement a custom flatten to do it So I think the author did not recognised the Monad inside the range-v3 and proposed an implementation that is less readable, and also much less efficient: range-v3 does the job lazily, while the code in the post creates whole huge vectors in memory. For an example of doing "range comprehension" right instead, I encourage you to refer to this article of Eric Niebler: http://ericniebler.com/2014/04/27/range-comprehensions/.
Copperspice is one the projects I really want to see its improvement. I don't have anything against Qt. But I do really like the idea of abandoning MOC and having Qt level GUI library for pure C++ without taking an extra step.
Monads do not have necessarily their place in C++ the way it is shown in this post. By they have their place in C++ and are already there (see my comment above: range-v3 already implements the Monad concept through view::transform and view::join). In the end, we have to be pragmatic as you point out: * Sometimes, concepts do not translate easily between languages * Sometimes, concepts do translate, but not with the same shape / syntax I think that Monads, in the form in which they appear in Haskell, do not have their place in C++. But Monads have their place as a design pattern to build great libraries. You can check this great talk of David Sankel that demonstrate this: https://www.youtube.com/watch?v=DiisKQAkGM4. If you want some examples of API that take inspiration from FP concepts, you can refer to some examples on my blog: https://deque.blog/2017/04/20/building-on-the-stl-random-header/
You could try [Verdigris](https://woboq.com/blog/verdigris-qt-without-moc.html). &gt; Verdigris is a header-only library that can be used with Qt. It uses macros to create a QMetaObject that is binary compatible with Qt's own QMetaObject without requiring moc. In other words, you can use Verdigris macros in your Qt or QML application instead of some of the Qt macros and then you do not need to run moc.
Nice, I was really looking for something like that a couple of months ago. It's quite difficult sometimes to develop a library without much feedback on the code and the design decisions when you're doing the development alone. It'll be useful for the general quality of smaller C++ libraries. I'll try to add one of my libraries for review in the next few weeks (still some work to do on some parts).
This community will for sure host a best practice section for lib devs. It aims at reviewing mostly mature libraries, the (long time) goal of certification is, to have a list of high quality libraries, where C++ users can choose from. But we'll see where thing go.
Reads like you're pretty much doing the right thing. Reads almost a bit ironic to be honest. Make sure you use std::vector though. Almost always use vector. Don't use list, queue etc, unless you know what you're doing. But don't use custom containers if you can avoid it.
Yikes. I find this to be "programming for geniuses". Let's look at this beautiful snippet here: auto game_gen(int max_player, double map_size) { return apply_gen( to_object&lt;Game&gt;(), //Combine into a game round_gen(), sorted_map_gen(max_player, player_name_gen(), coord_3d_gen(map_size)) ); } What's the return type of this? It looks like it's `Game`. Is it? But what's the return type of the call to `sorted_map_gen`? This is just so loaded with meaning. How do I step through it in a debugger? How can I imagine the generated assembly? Can my compiler even turn this into efficient and correct code? How can I be sure this use of features does not trigger compiler bugs which cause the generated code to be suboptimal or buggy? Where you see "expressive!", what I see is "I seem to have no control over what happens here", "way too much is going on in these few lines", and "I have no understanding of this code's performance".
Fair enough, it might not be the best example :) I think there is indeed an interesting debate to have between "more control" and "declarative-ness" (I do not like the word "expressive", as it is overloaded) and where to apply it. The way I see it, each time you do a step in the direction of having more declarative code (which does not mean readable - readable is too subjective), you give away some control. This is a trade-off. In some sense, using std::future::then (a Functor in FP) takes away some control from you and give them to a task scheduler. The more leverage you get from the "smart thing behind the curtain" (which might do a better job than you doing it manually), the more interesting it is to give that control away. In the case of the random generator, the main leverage you get is describing the shape of the generated structure, instead of specifying the steps on how to do it (and potentially failing at it) and re-using part of that structure for some other definition. In some other cases (more interesting ones), you can get better default performance out of it too (like future). After that, there is very likely a matter of taste: I always felt like C++ is "programming for geniuses": we do have to keep a lot of things in our head (like STL iterator invalidation for instance, it is non-trivial) that declarative approach can get rid of. But it almost always comes at a cost for sure.
&gt; the WTF operator ??!??! [2] (which no longer works in C++17 BTW) Sorry, my bad.
&gt; When I see people compare C++ to other languages they always talk about how difficult it is but to me there doesn't seem to be much difference aside from additional hassles like declaring the variable type and having to compile. It's because they only know C++ from 1995 usenet blogposts and poor college teachers
For completness, here is how you could implement the code in Haskell (the reason why I mentioned this is like "list comprehension"): [ f3 x | a &lt;- map f1 [1, 2, 3] , b &lt;- map f1 [3, 4, 5] , x &lt;- f2 a b] And here is the sketch of a possible implementation of this using range-v3 (cartesian_product, transform and join views): std::vector&lt;int&gt; xs = {1, 2, 3}; std::vector&lt;int&gt; ys = {3, 4, 5}; auto ring = view::cartesian_product( xs | view::transform(f1), ys | view::transform(f1)) | view::transform(tupled_args(f2)) | view::join | view::transform(f3); But am I not sure whether it is interesting to write it like this. Overall I feel like the solution given by Eric Niebler in http://ericniebler.com/2014/04/27/range-comprehensions/ is both more idiomatic for C++ (closer to for loops) as well as more flexible.
 std::vector&lt;int&gt; results = std::vector{1,2,3} | and_then([](int a){ return std::vector{3,4,5} | and_then([=](int b){ return f2(f1(a),f1(b)) | map(f3); });}); hmm.. % grep '[a-z0-9]' -o example.cpp | wc -l 90 % wc -m example.cpp 170 when almost half of your characters are special characters, maybe it's still a little bit ugly
I agree, that's very reasonable. I am "special" (perhaps idiosyncratic?) in that I value *knowing* what's happening over how well it's happening. Performance is important, but what I value more is that when something is going wrong, there are few black boxes that obscure understanding of the problem. To this end, I discourage cleverness for the sake of cleverness, and recommend using clever solutions judiciously. Declarative programming appears to be especially clever to me, in that it multiplies the work needed to understand what a particular snippet of code is doing. But it can be very powerful if used judiciously, and can reduce errors, I agree.
I don't think that's a particularly interesting metric, but at the very least you gotta give me `_` and whitespace as non-special. Now I'm up to 121/170. And even then, are braces and parens and namespace qualifications special? 
Next article: &gt;The Vector Monad, Really Without the Ugly Stuff (thanks to Rust) And we'll have a flamewar between /r/cpp and /r/rust.
Really, this person is living the dream!
This is the most important question. For my own work my target is always an Ubuntu or debian distribution of my choosing. This means that I work very hard to make sure that all my requirements are satisfied by available apt packages. I also build my own software as debian packages with correct dependencies so that things match up. This is hardly a new method, but I find it pretty good. Though it obviously makes strong use of my target system so really you'll have to take that into account for your own work.
More than frowned upon, I think it's actually well underestimated, hence it's not taught much or people care about it. I'm deviating the norm on its use recently and going quite liberal about it, reason why this has come to me. Another use I'm making of it is this, instead of: if (some error happened) { sprintf(stderr, "Some error Foo happened"); return EXIT_FAILURE; } I'm doing this: if (some error happened) return sprintf(stderr, "Some error Foo happened"), EXIT_FAILURE; I already employ braceless `if` statements both because I always use clang_format, which removes any indentation risks, as well as I'm well aware of macro mistakes, but still I'm not dogmatic about prohibiting it.
Both. Clang/C2 had some FE/BE bugs that aren't present in Clang/LLVM. Also, Clang 4.0.1 (and soon 5.0)'s newer features are important for the STL going forward.
and_then, and_then. its like watching the movie "dude, where's my car"
I'm having trouble understanding `and_then`. What is the type of `VU`?
`map` takes a `vector&lt;T&gt;` and a function `T -&gt; U` and returns a `vector&lt;U&gt;`. `and_then` takes a `vector&lt;T&gt;` and a function `T -&gt; vector&lt;U&gt;` and returns a `vector&lt;U&gt;`. Note the difference in the return type of the function parameter. 
Ah. Thanks
**Company:** Cohesity Inc. See the company [info](http://cohesity.com/overview/) and [LinkedIn](https://www.linkedin.com/company-beta/3750699/). **Type:** Full time **Description:** Hyperconverged Secondary Storage. Cohesity makes large organizations productive by consolidating, protecting and sharing your non-mission-critical data assets. Your essential data is instantly available when you need it, where you need it. Our ground-breaking distributed systems technology hyperconverges all secondary storage workloads into an efficient, agile and infinitely scalable resource pool. This greatly simplifies both your infrastructure and the resources to administer it. We are looking for world-class engineers to develop our disruptive, converged storage architecture. This position includes everything from building the core of an infinitely scalable file system to exploring huge information sets to presenting complex information in an easily digestible format for our customers. Our team has worked on technical feats including the Google File System, Google Search and Ads, hyper-converged scale-out systems, Netflix Real Time Bidding &amp; Cloud and Veritas Data Protection. **Location:**Santa Clara, CA, US. **Remote:** No **Visa Sponsorship:** H1 transfers **Technologies:** C++14 on Linux. Many internal and open-source libraries. Networking, storage, distributed systems. **Contact:** Redit PM and we'll take it from there
It's not mean, but it's also a slightly more nuanced argument than you are presenting. Trust me, we talked about it at length. We decided against, at least for now. 
Which decision precisely are you hoping we reconsider? 
Is there a recording somewhere?
I'd love to watch the talk as well..
This is just in my experience so ymmv but... Most C++ developers make things much harder on themselves than they need to. I'm not talking about playing with the language, especially the new stuff, to see what it can do and such. They tend to make a mess too sometimes...but at least they're trying and will get better. No, what I'm talking about is the C with classes kind of people. Most C++ devs I run into are scared of templates for one...if not many of the other features that make C++ a good language. Just f'n terrified of them. No desire to learn about them. Little interest or desire in continuing to learn new language features. Most C++ devs are stuck on legacy code written for old compilers, using ancient idioms, and are scared of moving beyond that. Even the young ones that you'd sort of expect to really want to learn...don't. They want to only know what the "need" in order to work on such code bases...learning a better way just isn't a goal. I actually have trouble understanding it. I have a family and that takes up a huge chunk of any free time I'd have for continuing to educate myself...but I still do. I have to work on legacy crud too, but I try to learn more about that balance between too much change and not enough. To have zero interest...it's hard to understand it. I have to say though that this seems to be the norm. You'd expect something completely different if you went by the Internet community. You'd expect to see RAII used everywhere...pushes to get the fuck out of 03 and into the modern era... Incredibly smart people who spend a lot of time in groups like this learning and teaching new stuff... That's not the norm in the real world though. I think you really have to end up in one of the biggest firms and/or someone completely new in order to actually use modern C++ in your every day development. The rest of the industry though...mired in legacy environments and terrible, half assed attempts to be "agile". Filled with "the average developer" who from what I can tell just doesn't give a fuck. Most code is just garbage. I'm not talking about how you look back on your own and see mistakes. I'm talking just simple basic shit that's demonstrably bad in every way. C++ is scary for such people...like cleaning a loaded gun or something. That's because they don't leverage the type system, don't leverage scope and values...in fact they spend a lot of time trying to bypass such things through direct, manual, and extensive use of dynamic memory and pointers...and then see how many and how often they can violate SOLID principles. Yeah, C++ is hard when you do that.
I didn't know that `andThen` was an alternative name for `bind`. Given that bind can be written in terms of map and join, it would be nice for our `andThen` to somehow support this. Writing `map` on a vector is easy. Writing `join` on a vector is also easy. Writing both together can be slightly more efficient, but we don't always need that. Another minor complaint is the fact that you have to double-enclose the lambda, once with `{}` and another time in `()` to pass it to `map` or `and_then`. If `and_then` was an adapter instead of a holder of a function: template&lt;class Proxy&gt; struct pipe_adapter { template&lt;class C&gt; struct proxy { Proxy const&amp; p; C&amp;&amp; c; using T=decltype( *std::begin(std::declval&lt;C&amp;&gt;()) ); template&lt;class F&gt; decltype(auto) operator|(F&amp;&amp; f)const&amp;&amp; { return invoke_pipe(std::forward&lt;C&gt;(c), p, std::forward&lt;F&gt;(f)); } }; template &lt;class C&gt; friend proxy&lt;C&gt; operator|(C&amp;&amp; c, Proxy const&amp; self) { return {self, std::forward&lt;C&gt;(c)}; } }; struct map_t : pipe_adapter&lt;map_t&gt; {}; static const map_t map; struct and_then_t : pipe_adapter&lt;and_then_t&gt; {}; static const and_then_t and_then; template&lt;class T, class A, class F, class U=std::invoke_result_t&lt;F&amp;, T const&amp;&gt;&gt; std::vector&lt;U&gt; invoke_pipe( std::vector&lt;T,A&gt; const&amp; v, map_t, F&amp;&amp; f ) { std::vector&lt;U&gt; res; res.reserve(v.size()); for (auto&amp;&amp; e:v) res.push_back( f(decltype(e)(e)) ); return res; } template&lt;class T, class A, class F, class VU=std::invoke_result_t&lt;F&amp;, T const&amp;&gt;&gt; VU invoke_pipe( std::vector&lt;T,A&gt; const&amp; v, and_then_t, F&amp;&amp; f ) { VU res; for (auto&amp;&amp; e:v) for (auto&amp;&amp; r : f(decltype(e)(e)) ) res.push_back(decltype(r)(r)); return res; } which gives us std::vector&lt;int&gt; results = std::vector{1,2,3} |and_then| [](int a){ return std::vector{3,4,5} |and_then| [a](int b){ return f2(f1(a), b) |map| f3; }; }; with less "crud" at the end of the statement. The "pipe adapters" tell us how we want to pipe the container (or whatever) to the function. As a bonus, *anyone* can extend `and_then` or `map` by overloading `invoke_pipe( lhs, map_t, F )` or `invoke_pipe( lhs, and_then_t, F )` in the namespace of `lhs` or `map_t`/`and_then_t`, and the boilerplate per new piping operation is significantly reduced (both use the same `pipe_adapter&lt;Derived&gt;` utility class). [Test code to play with](http://coliru.stacked-crooked.com/a/855ed1dc7f393364). I think we can write a generic and then and map in the above syntax to use the other. I think I'd want to have these "default" versions have a different name that SFINAE dispatch to the "real" versions if and only if it exists, and otherwise tries to use alternative solutions. So we can ask end users to write `map` and `join` and `and_then` would work for free. 
Exactly. People often code in C++ as if it was Object Pascal or C :(
I sometimes write `return foo();` where `void foo();` just to save on typing - and people get unnecessarily thrown off by that. Compare: if (condition) { foo(); return; } vs. if (condition()) return foo();
Agreed, though, even though I'm not that thrown off by it, the intention is a bit misleading since you're just returning, but it looks like the intention is to return a value.
That's the right thing to do because as I said the current use of C for Windows development right now is for cross platform C libraries/frameworks.
It only looks like the intention is to return a value if you never knew that returning `void` is possible :)
Or just `pacboy sync clang:x` (:x means 64-bit version).
This is a surprisingly complete summary, pretty great.
If I'm not mistaken, in case heterogeneous computing makes its way to language, we will be able to utilize some parts of GPU without the need to use OpenCL, Vulkan etc.? That would be incredible.
Holy crap, if C++20 will indeed have all that was mentioned (except ranges, damnit) then that will be amazing. The STDLIB by then will seem to be large enough to compete with languages that have huge standard libraries themselves.
ah sure. One can make it explicit/clearer with: if (condition()) return foo(), void();
I would much prefer modules without macros and just #include the one or to files which actually need them.
Yep. I actually like that idea.
Sorry I wasn't clear enough at the end. I meant solution for their IntelliSense engine. i think it would be awesome to use clang based auto completion in msvc.
"const callable &amp;&amp;" - can you explain the purpose of the "const &amp;&amp;"?
No, the const shouldn't be there, as its an RValue ref, while technically they exist: http://www.codesynthesis.com/~boris/blog/2012/07/24/const-rvalue-references/
How is that gcc is the compiler the most used but most C++ devs are using Visual Studio? Also where are the infos about how many people took this survey?
there was another one there for "cb", I think
Indeed. Just being used to const correctness a bit too much...
I found that odd too. Considering VS project is also the most used too, how is gcc winning? It's the sum of everyone else?
we did it!
https://www.youtube.com/watch?v=NvWTnIoQZj4
If you're going to use operator overloading, why not follow convention and use &gt;&gt;=? vector&lt;int&gt; results = vector{1,2,3} &gt;&gt;= [](int a) { return vector{3,4,5} &gt;&gt;= ... etc
Multiple choice, for one. We used VS for all our editing, VC for every build, and also GCC for all our server builds. Not at all uncommon. Also keep in mind that VS has out of the box support for compiling with GCC or Clang these days. 
I dunno about convention. Haskell does it that way. But, there's at least three reasons I dislike `&gt;&gt;=` for C++. In no particular order: 1. It associates the wrong way. So when you want to chain things, in Haskell you can write `m &gt;&gt;= f &gt;&gt;= g &gt;&gt;= h` but in C++, you'd have to write `((m &gt;&gt;= f) &gt;&gt;= g) &gt;&gt;= h`. That's awkward. Using `|`, or if possible a member function, avoids the extra parenthesizing. 2. It doesn't follow the C++ convention for what `op=` means at all. Which itself is kind of a subset of ... 3. It's just way more confusing than using a named operator. I find it confusing in Haskell too - particularly since `&gt;&gt;=`, `&gt;&gt;`, `&gt;=&gt;`, and `&lt;=&lt;` all do totally different things, despite being closely related. I'd rather take the more typing and then having to parenthesize the lambda than use the terse operator. 
It concerns me that so many libraries (e.g. Cairo) may be built directly into the core of C++. Having a big standard library is a recipe for disaster (hard to version, hard to update). They should focus on modules, and provide a very easy way to build code and then allow the community to build the appropriate solutions on top of a solid foundation.
hmm which other languages have huge stdlib?
A recording should be released sometime soon. I'll tweet it from @TonyAlbrecht when it is.
I'd like modules too (my number 1 hopeful feature from C++20). But I still think there's a good argument to be made for a rich stdlib. Python is my other daily driver, and easy module support does not solve all your problems, especially with deployment. I know I can always depend on the stdlib to be there. I can't depend on a user being able to pip install some module. (I deploy mostly to other developers on managed linux systems)
Python is the first that comes to mind
I'm reminded of this list : https://www.reddit.com/r/cscareerquestions/comments/2q7oo6/a_strong_understanding_of_c/
[removed]
We do the same but it still don't addups, except if what you say about vs working well with GCC. Did you mean even the debugger?
There's also the variation: if (condition()) return void(foo()); Both avoids the issue of `return foo()` for the case where `foo` ever changes its return type.
I'd like to hear the argument for including Cairo in the C++ standard library. The surface area of that is huge, increasing the chance of defects and reducing the portability of the language, e.g. for embedded development or in critical systems. Deployment issues (e.g. installing the right dependencies) are an infrastructure problem, not a language problem, and they won't be fixed by making the standard library fatter. In fact, if anything, they make it worse - it becomes impossible to version things.
I'm not saying it should be. I'm not saying boost should be part of stdlib or any other specific package. I'm just saying that there is room between adding nothing and adding everything. And don't expect modules to cure all that ails you.
I'm not expecting modules to cure every problem, I just think it's more important to make a foundation on which dependencies can be built and installed, and that simplify/standardise the process in which C++ code is distributed. A lot of `boost` IS being merged into the standard library. I think one of the main drivers for this is the poor support for 3rd party modules -the complexity of building them on different platforms/compilers, etc. If you look at really successful modern languages, you'll see that they invariably expose a robust foundation for 3rd party modules, and let other people solve the problems of the "standard library", rather than trying to do everything in house. Even Ruby recently started turning their standard library into gems: https://stdgems.org which IMHO is a brilliant move.
&gt; I think one of the main drivers for this is the poor support for 3rd party modules -the complexity of building them on different platforms/compilers, etc. I completely agree! This is the problem I'd like to see modules take a bite out of. However, I'm pretty skeptical that what we'll get in C++20 is going to do that. Macro syntactic sugar might help modestly with compilation speeds, but it still isn't going to have any impact on the actual dependency problem. We need a whole ecosystem to achieve the kind of developer workflow you get from Ruby, or Python, or pick your favorite modern (dynamic) language. The workflow needs to manage cradle to grave; from downloading a package to configuring, building, installing, and then utilizing a module as a dependency. That's just not something I think we're likely to get basically ever. I don't know of any compiled language offering that kind of environment. I don't see the community every even agreeing on a build system. Though, CMake does appear to be making headway. 
Ruby and Python I feel. They include things like compression, CRC, JSON, and HTTP over TCP sockets.
Java.
Pretty much everything else? Java, C# etc. They have meaningful base class libraries to accomplish real work. C++ gives you a string, vector, some pre-built looping algorithms, and an awful legacy of iostreams. I feel the rise of Java/C# weren't entirely related to their runtimes which offered GC, or web-friendly environments, etc. It was also due to them having a central set of useful abstractions to build much larger things with. QT, boost, and the efforts of the language committee (the energetic committee of the past 7 years, not the one before that) should've all gotten together in the early 2000's and sorted some things out... As it stands QT is still way off on the side, Boost is a mix of 3 classes of code: deteriorated, semi-modern, and code ready-to-become-a-standard, while the standard is awash in gory details as the complexity of the language is growing out of control (concepts and syntax etc.)
and different inode
Same here, I really don't get the macro love.
Should I send you some pms? I'm a size C++.
Almost all of them. A big C++ mistake was to follow the C's culture that the "runtime" should be the OS APIs. Which incidently lead to POSIX, because everyone wanted to use C's libraries outside UNIX.
If they are on the standard, there is a certain quality level and the expectation they will be supported everywhere a compilant C++ compiler is available. Community solutions most of the time lack support for specific OSes, compilers or the base quality one is expecting to have.
And all the os APIs are crusty old c style
Game consoles and embedded development with plugins that are only available for Visual Studio is quite common. Those areas tend to have quite a few gcc forks.
I've gotten size R... I could use some C++ right now you know.
I think modern C++ is way over represented in that kind of survey. I don't know anything about their survey methodology, but 12% of people on C++17 (which is technically not a standard yet) seems over estimated (unfortunately). I'd be tempted to think that people taking these surveys are naturally self-selected to be much more interested modern C++ than most folks.
Honestly, while I appreciate the sentiment, there are a ton of things in the C++ standard library that are badly designed.
&gt; We do the same but it still don't addups The numbers seem to be well within the bounds of reason to me. &gt; Did you mean even the debugger? Yup. Not only does Microsoft now ship its new Clang-based compiler for Windows, it also can remotely compile on non-Windows platforms with GCC or Clang and remotely debug via GDB or LLDB, all out of the box. Here's one of the many articles describing it: https://blogs.msdn.microsoft.com/vcblog/2016/03/30/visual-c-for-linux-development/
I don't think it has much to do with macro love, instead Google/Clangs proposal features macros so that you can gradually move from non-modular code to modular code. Much of the discussion on the proposals is how important macros is for transition, wherein Google supposedly found a strong need for it when they tried to port code. Without macro support you basically have to modularize from lowest level upwards to application code. You can't easily just wrap a lower-level library. Which leaves your application in mixed state of modular and non-modular includes/imports.
Whereas *my_header.h* used to be a mix of macros and code before, it gets split into a few files. #ifndef MY_HEADER #define MY_HEADER #include "my_header_macros.h" import my_header; #endif Easy, one translation unit at a time. Macros still available to *my_header.h* users, without having to explicitly support macros. 
depends, just doing `namespace foo::bar` would mean that you are in C++17-territory. 
No programming language has a perfect standard library done without mistakes. Mistakes do happen, and some of them are eventually corrected. It was this attitute that made C++ lose the GUI and web space, replaced by programming languages with better options on their standard libraries, guaranteed to work anywhere there is a compiler or vm available. Now on the GUI space, it is left to pushing pixels around at the lowest level, while everything else that could be done with nice C++ abstractions gets written in JavaScript, Swift, Objective-C, Java, C#, QML, .... 
Well, this is only about specifying the language so it's possible. You'll still need a dedicated toolchain to compile your code for GPUs, and you will have to load the code using a separate program. No magic attribute to say "oh yeah, run this on the GPU somehow".
Just a question. Why do you need zlib in your project?
Let's pick Ruby on Rails. Is Rails built into the Ruby standard library? No. In terms of GUIs - every modern GUI that I can think of is underpinned by C++.
Even if you know that's still what it looks like.
For the lock guard you can cheat with a macro and generate a unique identifier (using `__COUNTER__`).
In VS we now have NuGet. In Unix we always had `make install`. So it *is* extremely useful. Yes, you can have your project build its own deps, but why do that? This is why NuGet was invented. However, that's the developer perspective. The user perspective is that `make install` allows building installable packages. `make install` copies files into a sandbox, you package the sandbox , end-users can then install that package.
I knew all that but it's so new I didn't consider it used that much... Ok I see.
*Underpinned* is the keyword here. Cocoa, written in Objective-C and little pieces of Swift nowadays. No C++. Android GUI, written in Java with APIs 100% Java only. C++ is only used for the OpenGL bits on SurfaceFlinger. And the Android team quite keen that it stays that way. Brillo was supposed to have a C++ GUI framework, replaced at last minute by a Java one taken from Android, and got renamed into Android Things. Tizen, inherited the C++ GUI from BadaOS, dropped and replaced by a C one based on EFL, now getting a .NET Core one instead. Web &amp; Electron Apps, HTML5, Canvas and WebGL 100 % JavaScript. C++ is used for the VM implementation, not the actual GUI code. WPF and UWP it is mostly .NET. C++ is used for the rendering code, there is C++/CX and C++/WRL but besides Microsoft on their infrastrucutre code, hardly anyone uses them. Rather using the .NET UWP APIs. Even at BUILD there were almost no sessions about them. Qt the C++ flagship GUI framework, if one wants to target embedded or mobile devices the answer is QML, with C++ widgets being left to classical desktop applications. There used to be OWL, VCL, MFC, wxWidgets, Gnomemm but they are hardly relevant nowadays. As for Ruby on Rails, I never seen a request for a Ruby Project that wasn't a Ruby on Rails contract, so effectively it is as if it is part of Ruby's standard library. As I already mentioned multiple times, thankfully the negative voices aren't part of the ANSI/ISO process.
I know but in my XP a lot of people use GCC outside gamdev and they dont want to use VS at all, so it's surprising to me. I mean gamedevs are not the only people answering these surveys isn't it?
&gt; Cocoa, written in Objective-C and little pieces of Swift nowadays. No C++. From my experience debugging Cocoa (e.g. looking at stack traces), a lot of the actual UI toolkit is written in C++. &gt; Android GUI, written in Java with APIs 100% Java only. C++ is only used for the OpenGL bits on SurfaceFlinger. And the Android team quite keen that it stays that way. The Java VM is implemented in... oh, C++. &gt; Brillo was supposed to have a C++ GUI framework, replaced at last minute by a Java one taken from Android, and got renamed into Android Things. Never heard of it so can't comment. &gt; Tizen, inherited the C++ GUI from BadaOS, dropped and replaced by a C one based on EFL, now getting a .NET Core one instead. Same as above. &gt; Web &amp; Electron Apps, HTML5, Canvas and WebGL 100 % JavaScript. C++ is used for the VM implementation, not the actual GUI code. All major JavaScript interpreters are written in C++, and that includes most of the rendering engines, including Canvas, HTML rendering/compositing, WebGL, etc. &gt; WPF and UWP it is mostly .NET. C++ is used for the rendering code, there is C++/CX and C++/WRL but besides Microsoft on their infrastrucutre code, hardly anyone uses them. Rather using the .NET UWP APIs. Even at BUILD there were almost no sessions about them. Right, so fair to say it's underpinned by C++? Also don't forget MFC, but I'm not sure how popular that is these days - I'm not a Windows developer. &gt; Qt the C++ flagship GUI framework, if one wants to target embedded or mobile devices the answer is QML, with C++ widgets being left to classical desktop applications. Yep. &gt; There used to be OWL, VCL, MFC, wxWidgets, Gnomemm but they are hardly relevant nowadays. Sure. &gt; As for Ruby on Rails, I never seen a request for a Ruby Project that wasn't a Ruby on Rails contract, so effectively it is as if it is part of Ruby's standard library. At a very high level, I see where you are coming from. But at a language level, this isn't really a valid argument. &gt; As I already mentioned multiple times, thankfully the negative voices aren't part of the ANSI/ISO process. I'm not sure I understand what you are trying to say here. Coming back to your original assertion: &gt; It was this attitute that made C++ lose the GUI and web space, replaced by programming languages with better options on their standard libraries, guaranteed to work anywhere there is a compiler or vm available. I'm going to assert that based on the above evidence, I don't think that it's "lost the GUI and web space". As a matter of fact, I'm implementing right now an API server in C++ because performance is more important than maintainability - and I think that's a critical point which does sort of support your assertion - yes, Ruby might be the go-to for a CRUD web app, because no one cares much about performance (developer time is more expensive than processing time and all that). But that has nothing to do with having a standard library that includes Cairo, which if you recall, was my original point.
Module with macro seems to me essential if we want support idioms like assert and others debugging tools in templated code.
That's also a point of concern for me with the Rust language. Stuff like (most of) the itertools crate belongs in the standard library, but the team is extremely conservative with stdlib. With good reasons, I admit, and some interesting novel approaches like the recent "officially sanctioned" external library effort (sorry forgot the actual name).
The cool thing is that you cannot properly use the GPU as is: the compilers are garbage and they routinely garble up things. My experience with AMD-OpenCL has been nearly a nightmare; I will pretend OpenGPU and assembler, very disappointed to see this is carrying on considering it is not solved even on "native" apis.
I would say the number do add up: 34% use the Visual Studio IDE and 33% use the MSVC compiler. Do note that the compiler choice is a multi-choice survey: GCC + MSVC would already cover 100%, and the others add up to another 60%.
&gt; I'd be tempted to think that people taking these surveys are naturally self-selected to be much more interested modern C++ than most folks. I would expect so too. After all, taking the jetbrains survey means that you *care* about C++ to a degree, which in itself makes you more likely to be following the latest developments.
I've been out of C++ for a while. What happened to the 'calendar' utilities? Are they going to be a library only? Are they already in? I've seen them being presented at c++con 2015 /methinks.
Yes, but even doing so means you have a very recent compiler, which is usually not the case in my experience and in what I've heard from others.
You must be fun at LAN parties. 
&gt; the C++11 minimal garbage collection support C++ has that? I always thought that had been cancelled...
Can't you use constexpr and other new features to implement assert without macros?
The main reason why Hana has its own tuple is for compile-time performance. A fully standards-conforming tuple implementation is very difficult to make compile fast, so Hana drops some standard support. Hana's tuple also has stuff like `operator[]`.
Yes, see [N2670](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2670.htm). It's an optional feature, so [std::declare_reachable](http://en.cppreference.com/w/cpp/memory/gc/declare_reachable) and related functions might just do nothing at all. In fact, I don't think there is any implementation actually shipping a GC for C++, at least none [listed at cppreference](http://en.cppreference.com/w/cpp/compiler_support) do. I reckon one might as well use the functions if one is indeed working with XOR linked lists or similar patterns, since there's only a single call before and after involved. However, for all practical intents and purposes, this doesn't matter. The mentioned consume-release modes fall in the same category btw, [no compiler is known to implement](http://en.cppreference.com/w/cpp/atomic/memory_order#Release-Consume_ordering) them, i.e. they're always being mapped to the more relaxed acquire-release semantics. I think it's the same for this feature; if appropriate one might use it, but it doesn't matter at all in practice.
What does it matter if the low level code is written in C++ if we cannot **write GUI code in C++** and are forced to use blessed APIs made available in other languages?. You seem to be happy to have C++ just shifting pixels between CPU and GPU, or manipulating hardware registers, so that you can point out to lowest tiny rectangle on the architecture diagram and state " see there is C++ being used!". As I said, really happy the ANSI/ISO guys disregard such mentality.
I think you've missed my point which is I don't think `-lcairo` should be part of the standard c++ library. You certainly CAN write GUI code in C++ if you want. There are a ton of options available - and none of them should be part of the standard library IMHO. It simply doesn't make sense for a heap of use cases that C++ is currently used for.
Interesting how CMake is only 34%. So much for the de facto C++ build system claim. I think what we have instead is a very vocal CMake user base.
Does postmortem crash dumps work yet, I'm guessing not
From the blog post: "For this post we welcome John Morgan from Intel Corporation as guest author on the Visual Studio Blog. John has been with Intel for nine years, but his contributions to the Microsoft compiler stretch back through two decades and three other companies. He gratefully acknowledges help with this post from others at Intel and Microsoft." We asked John to write this post as he's the expert and did a ton of the work. You should not be surprised that Microsoft and Intel have different blogging styles. We didn't feel it necessary to force John to adopt our more casual blogging style. 
You're welcome, but remember a lot of credit goes to our Intel business partners as well!
There's additional info on that on the post's first link at the end.
is there a place that lists the difference with ASIO ? 
[]&lt;&gt;(){}
&gt; pick your favorite modern (dynamic) language What's `(dynamic)` has to do with anything? &gt; I don't know of any compiled language offering that kind of environment A lot of newer programming languages come with their our package manager/ecosystem. See Haskell with cabal/stack, OCaml with OPAM, Rust with Cargo, D with DUB, .NET family with NuGet and etc.
\_\_VA\_OPT__ seems like a good solution, i've used the ##\_\_VA\_ARGS__ hack to make the preprocessor remove the comma when there are no arguments provided, but it's not in the standard to do so as far as i know. Having the target endianness provided would be really nice, i've never managed to get compile time deduction of it to work. I do have one question though: is there a repository somewhere that contains all of these in-development features in a compiler to try them all out with? I know there are experimental versions of some of these features in several compilers, but i'm not aware of any single compiler that lets you try them all out at the same time. Overall looks like a great report, i'm hoping to see more features wrapped up for inclusion in the months to come!
Just that I don't know of any compiled language with an ecosystem.
Who says that CMake is the *de facto* build system?
Better than all of that, we are working on - not yet finalized - adding functions to std::string!
It's not that big of the problem for Rust since it's extremely easy to add and install those dependencies.
Oops, maybe the IDE imported it without me noticing, I'll remove it. Thanks for noticing!
What kind of functions are planned to be added? Is there a working draft of the proposal available?
So if a feature is voted in C++20 like now will we see them officially (not like experimental features) implemented in the compilers or we'll have to wait for ISO to publish C++20?
please trim() :D Modules and Networking, yes yes yes!
Compilers do whatever they want. Some implement features early, some late, some never. Some implement features that don't exist. ie extensions. And by 'some' I mean 'each'. But being in the working paper tends to mean it is implemented early. 
I don't think this is valid C++. I don't believe you can specialize it.
Will we not need https://github.com/imageworks/pystring anymore? :)
Those I listed are all compiled (AOT or JIT, doesn't matter).
Just starts_with and ends_with. Which will also eventually be ranged-based generic algorithms, but they read well and are easiest to understand as member functions. 
Thx guys, awesome work. Any proposal for modern style pattern matching (OCaml style) in C++20 ? - 
It depends what you mean by officially, and it will depend on your compiler. In gcc and clang most features will become available before C++20 is published, but you'll need to pass -std=c++2a on the command line to enable them. There is a period of time between the committee approving the final draft and ISO publishing it; if any compilers make a release during that period they may add -std=c++20 as an option.
Did we get the terse Concepts syntax?
Don't worry, we'll propose something to make sure that is valid. Missed opportunity otherwise. 
I'm assuming that terse syntax for Concepts didn't make it, since there is no consensus on repeated ones in the arguments list. Is there anything else that was changed?
Maybe proper C++ interface wrapping C Posix signal handlers?
No, no consensus there. However, they will try to add it in C++20.
There is work ongoing, nothing finished yet. Not sure about C++20. 
No, not yet. We don't yet have consensus on exactly what it should look like and how it should behave.
Besides postponing the terse syntax, there were some changes to the syntax of concept definitions and the rules regarding subsuption and declaration / definition matching. Respectively: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0716r0.pdf and http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0717r0.pdf 
Looks like [this](https://github.com/chriskohlhoff/asio/blob/master/asio/tsify.pl) is the script that does the transformation from ASIO to TS implementation. 
[removed]
That's not the only concern people have with the terse syntax. We also simplified the subsumption rules and a minor change to redeclarations.
We would need someone to write a paper.
hahaha, nice. so I guess ASIO must be evolving to follow the TS instead ? 
It is quite amazing that we have gotten threads into the language that we all love, and pretty soon a heap of other fantastic additions (look at that impressive TS list above) will be added - but we are still left with one of the poorest string classes ever seen. Or is that about to change now? That would be awesome!
Is there any progress on extension methods? This would be great. Or rather is unified call syntax completely dropped or just delayed? I could also accept any practical solution as adding an attribute [[extension_method]] to a free method to allow resolving a call of x.f(y) to f(x.y) 
[]&lt;&gt;(){}();
I don't think the repeated type problem is very controversial. There were other issues: is it important to easily recognize templates? What about forward references? 
Different compilers implement features in whatever order and timing they choose. There's no single compiler with a strict superset of the others.
There seems to be pretty wide agreement in Library Evolution that we don't want a new string class.
There hasn't been any discussion of unified call syntax since Jacksonville.
String has too many member functions. Free algorithms are more useful. 
This sounds like real progress :) Great work! And very clever to start merging early. Maybe most of the major features are already in the working draft end of 2018 leaving two years for fine tuning :) - What are the major areas that got finally unblocked by the fact the concepts are now part of the official working draft? - Did the modules TS got accepted to pdts without changes? Will the modules TS get affected by the concept merge? - How was the feedback on herbs metaclasses? Will this be part of the upcoming reflection ts? I really hoped for an official reflection TS this meeting. 
Additional questions: - still without macros? - progress in "modulizing" std &lt;header&gt;'s? 
This is troubling: vector v{vector{1, 2}}; // Deduces vector&lt;int&gt; instead of vector&lt;vector&lt;int&gt;&gt; This seems inconsistent with how initializer_lists constructors are preferred when using list initialization. For example: struct A { A() { cerr &lt;&lt; "A()\n"; } A(const A&amp;) { cerr &lt;&lt; "A(constA&amp;)\n"; } A(std::initializer_list&lt;A&gt;) { cerr &lt;&lt; "A(std::initializer_list&lt;A&gt;);\n"; } }; int main() { A a{A{}}; vector&lt;vector&lt;int&gt;&gt; return 0; } outputs A() A(std::initializer_list&lt;A&gt;); 
You are absolutely right :) And thanks, this answers all my questions. By the way shipping toolset versions separately from visual studio and allowing all of them to be installed at once is a major improvement. Great work!
&gt; Not only does Microsoft now ship its new Clang-based compiler for Windows That's already abandoned in favor of proper LLVM Clang.
Msvc is also very active. It would be very helpful to add the TS implementation effort in the compilers to the official conformance status page at isocpp.org. I think a TS feature is ready once gcc, clang and msvc are working on an implementation and at least two of them have an working solution. 
No; assertion failures are required to include "*the text of the argument, the name of the source file, the source line number, and the name of the enclosing function — the latter are respectively the values of the preprocessing macros `__FILE__` and `__LINE__` and of the identifier `__func__`*". Basically all of this is impossible without making `assert` a macro.
Simple reflection is much farther along than metaclasses, so I don't expect it all to be in the same TS at the same time. Concepts unblocks Ranges at least (although it could move forward without concepts if it really had to) More importantly, Concepts unblocks *discussion* on concepts and other features. *I* think we were starting to stagnate/spin without making much progress. I don't think there is much overlap between concepts and modules.
CMake users I would presume. Here is an [example from C++Now](http://sched.co/A8J6).
Is the committee dragging its feet on [_Generic Scope Guard and RAII Wrapper for the Standard Library_ (pdf)][1] or what? The last couple of versions of the proposal have only had minor tweaks that don't justify delays. Why is this not in C++17? Why is this not in C++14? [1]: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0052r4.pdf
&gt; Did the modules TS got accepted to pdts without changes? There were changes but none of the "major" proposed changes that I suspect you're referring to (feel free to clarify if there's something specific you're interested in). Those will likely come up again. IIRC, the bulk of the changers were editorial/clarifications. The biggest change was to require syntax to differentiate the module interface from the module implementation. So interfaces will have e.g. `export module M;`. From here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0629r0.pdf
&gt; still without macros? The PDTS is without macros. My sense is that there's a general sentiment that this issue is still unsettled and this is a place where, once we have implementations, community feedback will be super critical. 
If you're referring to [Extending `&lt;chrono&gt;` to Calendars and Time Zones](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0355r3.html), there's a repo [here](https://github.com/HowardHinnant/date), but I'm not sure on the proposal's status.
Obligatory link to Herb's old column http://www.gotw.ca/gotw/084.htm
You mean like embedded development? Then I don't want *std::filesystem* on ANSI C++ given that many embedded devices don't have a file system. Lets also throw the Networking TS on the garbage can, because many embedded devices targeted by C++ code lack a networking stack. The goal of the 2D library is to have a *guarantee* any C++ developer can take advantage of basic drawing code, instead of a TTY like if we were living in the 70's. The goal is not for replacing Qt. As for writing GUIs in C++, try to write a Cocoa GUI in *pure* C++ or an Android one for that matter, with native OS widgets, mimicking them with OpenGL doesn't count.
I also mentioned embedded devs.
I'm not happy with the designated initializer proposal and I don't think it should have been accepted. It implements one special case for initializing members by name but not the underlying logic. I.e., instead of being able to type whatever accessor 'expression' you can only use ". _identifier_". Not only is this unfortunately inconsistent, it also skips what in my opinion is the primary useful use-case for designated initializers in C++: sparsely initializing arrays: int arr[10000] = {[23] = 12, [6758] = 2}; // better than int arr[10000] = {}; arr[23] = 12; arr[6758] = 2; Designated initializers encourage C-style design and C++ offers better alternatives*. It's not even much good for C compatibility due to the differences between this proposal and the C feature unless the programmer is deliberately targeting the intersection of C and C++, which they can do without this feature anyway. This proposal also further breaks the rule that user defined types should have access to the same syntactic features as built-in types. Universal initialization fixed up some problems that existed in C++98, and now this proposal is breaking things again. --- \* The c-style design I'm referring to is leaving initialization entirely up to the user of the type, rather than the designer of the type strictly defining appropriate initialization. Designated initializers make using c-style structs easier, but don't make them a better idea. The alternatives I view as generally better are constructors and in-class initializers.
\*sigh\* *unzips*
So, how do we stand regarding http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0592r0.html and especially, modules. Do they have any chance of being merged before 2020 ?
Ah, nice, designated initializer lists can be used with aggregate classes and not just PODs, so that means they can be used to mimic named arguments, right? 
&gt; That's already abandoned in favor of proper LLVM Clang. Oh? Is that planned for VS vNext? Visual Studio 2017 definitely still ships with Clang/C2. It uses Clang/LLVM only for Android, though one can install Clang/LLVM for Windows (not from Microsoft) as well.
As far as I understand macros are the pure evil for future development of smart tools. Thus I'm very happy to hear that. I have read most of the public information on modules, but as on outsider I might miss some information. Up to now I only recognized the discussion weather to include macros or not, but I never saw a specific list that categorizes macros usage in the context of modules. There was a recent blog post "The year is 2017 - Is the preprocessor still needed in C++" and from that it seems most of macro usages can be omitted. The post lists 'passing configuration options' and 'conditional compilation' as the major drivers. Maybe the unsolved discussion on the macros reveals an deeper problem and the fact that modules solve only part of the modularization process and that a true solution also need to tackle open questions on how to also standardize the build process? 
During CppCon Mr. Alistair Meredith talked about possible development of std2, which opens new possibilities for fixing some of the long standing bad design in std v1, such as size_t -&gt; int as size type, fix string to be a simple non-templatized UTF-8 ready text type accompanying bytes (which is like what python3k did), a concept ready STL etc. Is there any discussion on this direction?
Not sure on specifics, just piecing things together – "*We are unlikely to be updating Clang/C2 at any time*" from [here](https://blogs.msdn.microsoft.com/vcblog/2017/05/10/c17-features-in-vs-2017-3/#div-comment-333265) and "*The Clang/C2 experiment for C++ is ending. Our STL is now tested against Clang/LLVM 4.0. (VS 2017′s second toolset update might need 5.0.)*" from [here](https://twitter.com/stephantlavavej/status/871861920315211776) (Clang/C2 is 3.8). ([also relevant](https://www.reddit.com/r/cpp/comments/6mqd2e/why_is_msvc_usually_behind_the_other_compilers_in/dk4ywhm/?context=3))
I think this proposal is about maintaining compatibility with C. At least it doesn't cause any new incompatibilities (like complex).
That's the output with C++14; in C++17 you only get `A()` due to guaranteed copy elision ([demo](https://wandbox.org/permlink/fWiyy0nsg69zvdzp)).
~!!+[]&lt;&gt;(){} Not very useful, but cryptic enough
Oh, very interesting. Would have been nice to get some official messaging about that on the MSDN blog. :)
No you don't. `A a{A{}}` does not collapse that down. That specific rule is in [\[dcl.init\]/17.6.1](http://eel.is/c++draft/dcl.init#17.6.1) but before we even get there we'd have to jump into list-initialization due to [\[decl.init\]/17.1](http://eel.is/c++draft/dcl.init#17.1) - so we avoid that prvalue rule entirely. I think that's a compiler bug. On the other hand, `A a(A{})` would indeed just do `A()`. Edit: See [CWG 2137](http://wiki.edg.com/pub/Wg21toronto2017/CoreWorkingGroup/cwg_defects.html#2137).
export import. New users of c++ will never get them the wrong way round. /s
Then MSVC (v19.11.25506) has the same bug. :-]
To be fair, I only know this now because I was originally arguing to vaughn in the opposite direction :)
I wonder what is the expected shipping vehicle for static reflection (P0194)? Is there an intention to merge it into C++20 or will it need to go through TS first (that would push it back to C++23 or sth)? The Herb Sutter's trip report mentions it under "on track for C++20" header, while the report here lists it as TS. Was there any discussion or decision made in that matter?
There was some work on text and unicode stuff. There is (or will be) a proposal on having a dedicated UTF8 char type.
Cool! Will find_first_not_of also get added as a range-based generic algorithm?
It could go into a TS, and still go into C++20. That is true of all the TSes.
Only if someone writes a paper. Or if it is already in Ranges. Or, it might be easy to express it with ranges, without making a dedicated function.
We're just waiting for someone to write a paper to move it from Library Fundamentals v3 into the C++20 WP.
&gt;such as size_t -&gt; int as size type Yeah for all those vectors with -2 elements. 
what have we become
Do you mean you're waiting on someone in particular, or just in general? Also, I don't see any published version of Library Fundamentals v3 so I don't know what's in it, but the generic scope guard proposal hasn't been part of Library Fundamentals so far. Certainly it wasn't in v2. The scope guard proposals have all been separate and the wording makes them sound like they're proposing adding the feature directly to the C++ working paper. &gt; The following formulation is based on inclusion to the draft of the C++ standard. However, if it is decided to go into the Library Fundamentals TS, the position of the texts and the namespaces will have to be adapted accordingly, [...] Maybe I don't understand the process correctly, but it seems like all that's needed is committee approval of any one of the proposals that have already been written, not yet another proposal to accept an existing proposal.
Oh wow. vector a {vector {1, 2}, vector {3, 4}}; // vector&lt;vector&lt;int&gt;&gt; vector b {vector {1, 2}}; // vector&lt;int&gt; Good job. Well done. Makes total sence. ----------------------------------------- &gt; tuple t {tuple {1, 2}}; // tuple&lt;int, int&gt; &gt; vector v {vector {1, 2}}; // vector&lt;vector&lt;int&gt;&gt; &gt; We find it seems inconsistent and difficult to teach that vector prefers list initialization while such similar code for tuple prefers copy initialization. "Fix tuple? Nah, lets fuck up vector instead."
Heading corrected. I didn't mean to imply it would be in C++20 -- I started writing that section with stuff we plan to vote in at the next meeting or two but then "refactored" the article and other things crept in.
Think of how the feature is meant to be used. In general, the point of C++11's adding uniform initialization using `{}` was to have one way to initialize things. For example: X x{init}; // construct x from init No vexing parse, etc. -- wonderful. If `init` is also of type `X`, that's a copy (or move): X f(); X x{f()}; // construct x using copy/move X x{X()}; // construct x using copy/move X x{X{}}; // construct x using copy/move I think it would be quite strange and inconsistent (and a regular pitfall to explain and teach) if that broke when `X` happened to be `vector`.
No macro required for location information with this proposed feature in library fundamentals: http://en.cppreference.com/w/cpp/experimental/source_location Given that modules won't exist until c++20 at the earliest, I'd personally lean towards evaluating modules and their necessary features in the context of the other features potentially in c++20 instead of evaluating them against today's standard. :)
IMO most of the issues I hear about regarding `std::string` aren't about `std::string` itself, but about lack of `string_view`. And we added that in C++17. C++17 `std::string_view` should now be the default (not only) type used for function parameters that want input string data, and it can accept data owned by any string type without making your function a template and without conversions.
That's not the reason for preferring signed types. Some of the reasons are: - sensible arithmetic. Sizes may only be non-negative, but what about, e.g. differences between sizes: `a.size() - b.size()`. If `b` is larger than `a` then this should be a negative number. - mixing signed and unsigned types is error prone, so the signedness of values you need to represent is not all that matters. The signedness of every other value your value may be mixed with also matters. The frequency of values that required signed representations is far greater than the frequency of values that require unsigned representations, so obviously this weighs on the side of using signed. - Even though wrapping behavior for unsigned types is well defined it's still usually undesirable. Keeping the boundaries of the representable range far away from the range of values expected to be used is generally good. - Performance. Unsigned semantics are more strictly defined and so disable some optimizations. I see recommendations from Intel, Nvidia, etc., as well as compiler optimizer devs, that signed types should be preferred. For some compilers there's also an option to make unsigned overflow behavior undefined so that you can get the same optimization benefits as with signed types. In short, you should not used unsigned types without a good reason, and they should be localized to the region where that justification applies, not put into broadly used interfaces. And 'this value should never be negative' is not a good reason.
I would argue that modules _and some other new not-terrible-like-preprocessor-macros feature_ are required for those things, but that's me. :) There's not a particularly huge number of things that preprocessor macros can do that other C++ features can't. The things remaining just need proposals/effort to correct. Thought ultimately, I'd be a huge fan of getting "hygeinic" macros into the language for which some of the reflection proposals lay the groundwork.
So you are telling me that for this example: std::vector&lt;int&gt; a; std::vector b{std::move(a)}; `b` should be deduced to `std::vector&lt;std::vector&lt;int&gt;&gt;`? I think it would be wrong. You want to move construct, not move into a vector of vector.
What about 'text of the argument', i.e. the text of the expression itself? I suppose it could be implemented as a metaclass...
Badass.
&gt; We added the following features to the C++20 draft: Question to clang guys (Richard Smith and others), maybe also to MSVC/gcc folks: Do you plan to start implementing something from that list?
You've got it the wrong way around, it's: "I can write in any language, in C++". 
&gt; sensible arithmetic. Sizes may only be non-negative, but what about, e.g. differences between sizes: a.size() - b.size(). If b is larger than a then this should be a negative number. Why are you subtracting two sizes if the answer can be negative? In what world does that make sense? If you want to figure out say how many spots there are left in a buffer (i.e. you want to compute a size) in what world does `size - capacity` make sense? A buffer can't have -3 things remaining, so if you were going to get a negative result odds are your computation is bogus, not the type. &gt;mixing signed and unsigned types is error prone Which is just as much an argument against signed types as unsigned types. &gt;Performance. Unsigned semantics are more strictly defined and so disable some optimizations. I see recommendations from Intel, Nvidia, etc., as well as compiler optimizer devs, that signed types should be preferred. For some compilers there's also an option to make unsigned overflow behavior undefined so that you can get the same optimization benefits as with signed types. So you want to crap on the semantic meaning of your program for a few nanoseconds? I thought the point of modern C++ and C++ in general was that you shouldn't be trading semantic meaning for performance. If unsigned arithmetic is slower then there should be a solution which preserves the elegance of unsigned types mapping representable values to the problem domain and performance, the answer isn't just to jam a hack in and call it a day.
How would you "fix" tuple? It doesn't have any `initializer_list` constructors, so the deduction of `t` to just `tuple&lt;int,int&gt;` falls out from the language rules we already had. In particular, there's no reason for `tuple a{tuple{1}}` and `tuple b(tuple{2})` to deduce to different types, but there would have been a reason for `vector v{vector{1}}` and `vector w(vector{2})` to deduce to different types. People decided the latter difference wasn't useful - don't wrap unless explicitly called for. This is arguably inconsistent with initialization rules, but also arguably one fewer place where `{}` and `()` do different things. 
Thank you for clarification. It somehow felt too good to be true. Though hope never dies. :-)
Agreed! I didn't appreciate how much unsigned can break things until I wrote a nontrivial OpenMP-parallel loop on an unsigned (size_t) index. The output was wrong and I had no indication why. Eventually I cracked the OpenMP spec and learned that unsigned index =&gt; UB... Makes sense in retrospect, but it was hard to solve and impossible to google. Now every time I see "for(size_t i..." I flinch.
Note this is started by u/nlohmann , previous thread mention here https://www.reddit.com/r/cpp/comments/5w52tq/json_for_modern_c_version_211_released/de7jfgw/ The [discussion on proposals mailing list](https://groups.google.com/a/isocpp.org/forum/?fromgroups#!topic/std-proposals/JNZzOvC7llo) is well worth a read. Even as a bit heated, the mentions to have each of DOM, StAX and SAX be addressed are valid. I think i've only ever seen one c++ StAX parser
hooray for `endian`!
And you blame unsigned types here, and not OpenMP..?
The asio master branch has been more or less in sync with the draft TS throughout the process. Now that the TS is done, these changes can finally go into a stable asio (EDIT: and Boost.Asio) release too. Keep in mind that the TS is only a subset of asio functionality. Probably about 25%.
Not at this time. At a coarse-grained level, you can compare [this table](https://chriskohlhoff.github.io/networking-ts-doc/doc/index.html) to [this table](http://www.boost.org/doc/libs/1_64_0/doc/html/boost_asio/reference.html) to get an idea of what functionality has or hasn't been carried over into the TS.
Surely you meant *Boost.Asio*
Both.
Well, yes, there's also the position that terse shouldn't exist at all because it's unclear it's a template. That's certainly more defensible with the adoption of familiar template syntax for lambdas, as without that there wasn't any place to put the Concept. But there does seem to be a deep split, with both sides saying their position is intuitive, that in void foo(Iterator b, Iterator e, Output o); whether b and e must have the same type. I'm not sure about the forward references? 
Oh, very good! I was never really comfortable with a Concept having always to have the bool noise. This also makes me feel a lot better about the committee not pushing them in so close to the end of the '17 cycle. 
Well, when the other highly reported choices are VS Project, handwritten Makefiles, and autotools, 34% is fairly significant. It's the only one of those where I'm likely to be able to integrate someone elses library into my build, since I am not on a VS system. There's a bunch I don't like about CMake, mostly from back before they really realized just how hard a problem it is. Which is also why autotools has stuck around for so long. The one thing I really miss is being able to reconfig a build cleanly. Build directories that can't be moved is also inconvenient. 
Except not; Boost.Asio is still on the Asio 1.10 branch, it's Asio 1.11 that tracks the network TS. You had it right the first time. ;-]
Does he think C++ is dead?
&gt; What about 'text of the argument', i.e. the text of the expression itself? There's a few additional concerns on top of that. For example, you generally want lazily-evaluated expressions for debug utils so that they can be turned off in release builds. Lazy arguments and expression stringification could indeed possibly solve all or most of those needs. I bet they're just a paper* away from getting into C++. (*) Well, a paper, a committee meeting, 7 revisions of the paper, 3 or 4 more committee meetings, a TS, 2+ compiler implementations, and then another round of papers and meetings, 2+ standard revision cycles, and some hurt feelings... but still, all in the realm of possibility. :)
Find a program you want to create, and then write that in C++. Learning a language without a goal is kind of pointless and will cause you to lose interest fast. So think of something to write and go from there. Maybe a web server? Maybe a game? Maybe a desktop app with Qt? Or go find an open source project, find a feature that is wanted, and try to implement it. There sure are no shortages in that category.
May I ask what you use C++ for?
Asio uses an even minor number to denote stable branches, so the next stable asio release will be 1.12.0, and this version will go into Boost as well (hence the "Both.").
Oh, I'm sure it _will_ be, but it isn't yet, which is what I thought we were talking about ("*throughout the process*").
Sorry, I assumed Vinnie's comment referred only to the latter part of my comment i.e "finally go into a stable asio (Boost.Asio) release". I'm pretty sure he knew the Boost.Asio master branch wasn't tracking it :)
Yes.
That's a feature I like from D
I use C++ to make all the things you listed, except not bloated laggy bullshit.
I've started learning it to help me make games in Unreal. Lovely language, I like it alot. My background is mainly JS, with a little C#. As IPA already said, find a little project and just go at it. Start small.
If recent history is any indication - you should have much reason for optimism. Clang just got its C++2a mode for us to consider starting to add stuff under: https://reviews.llvm.org/rL308118 
please, let it be split... That thing has been a pain in the ass as I have seen.
Yes, once I finish producing the new working draft and get back from vacation :-)
Do these kinds of comparisons never -O3 or use link time optimizations or does Copperspice really produce such immense binaries with no way around it?
Have you tried Unity? How would you say it compares to UE4?
Usually you want to do your own memory management when doing things that do not fit the existing idioms or would be too taxing otherwise (a few horror stories here). Also C++ is by its own def a multi paradigm language so C with classes (and templates, functors, lambdas) is perfectly valid way to use C++. 
Try unreal, the game engine. It has a C++ api you can use to make games in C++.
I inherited a C project at work that I converted to C++, and also maintain a C++ Qt GUI on top of it.
I saw this trip report so soon after the meeting and thought "must me good news". I'm glad concepts finally made it in. It looks like C++20 is going to be packed with concepts, ranges, networking, contracts, ..., if these TSs make it in. I almost wonder if they'll feature freeze in a year.
If you're using Clang or GCC you can expect these features to be implemented within the next year or so. If you're using MSVC, you can expect a half-assed implementation in 2022 or so.
Do a lot of people use pystring? I work at Imageworks and never realized people used pystring outside of work.
I had same exact problem before. I suggest to check graphic software and cad systems. Those things are almost exclusively written in c++ and you can find plenty of open source projects.
 std::string s {"s", "s"}; // no warning or error, UB at run time std::string t {"s"}; // perfectly fine 
&gt; Designated initializers encourage C-style design and C++ offers better alternatives. Could you elaborate?
I'm in! 
[removed]
Not sure why you're downvoted, you're spot on. Make illegal states unrepresentable == no negative sized containers...
Modules will totally revolutionize and modernize C++ by eliminating the archaic and annoying notion of header files. It's a shame that modules weren't included in C++17. There are working reference implementations. My understanding is that Google held out to get modules to include preprocessor macro definitions. Let's hope modules are adopted into C++20. 
Something that doesn't even remotely resembles C
This would be a great feature. The include system is quite hacky.
The difference in size between two containers most certainly can be negative. 
Sorry computing the difference in size of containers is very common and there is nothing invalid about it. Together with all the implicit conversions makes it error prone.
&gt; Better than all of that, we are working on - not yet finalized - adding functions to std::string! That seems a strange use of `operator+`. What is `"hello"s + printf`? 
Attributes are not supposed to change legality of code, in that if ignored the program should still function.
And in what situations is that meaningful? I write code every day and I can't remember the last time I wanted to subtract two sizes and plausibly wind up with a negative result.
&gt; Sorry computing the difference in size of containers is very common and there is nothing invalid about it. I never said there was a problem with it, I merely questioned why that result would be negative: What would it mean?
A simple example is that delta_size is the amount a container needs to be resized to be the same size as the other one.
&gt; A simple example is that delta_size is the amount a container needs to be resized to be the same size as the other one. `a.resize(b.size());` Next.
You asked for what a negative size means, not how to resize. That's just avoiding the issue.
I found the link in the blog post much better than the post itself: http://clang.llvm.org/docs/Modules.html#problems-with-the-current-model It at least acknowledges these issues: &gt; [...] the system headers themselves may require some modification, if they exhibit any anti-patterns that break modules. Such common patterns are described below. &gt; &gt; [...] &gt; &gt; Missing includes - Headers are often missing #include directives for headers that they actually depend on. [...] &gt; &gt; Headers that vend multiple APIs at different times - Some systems have headers that contain a number of different kinds of API definitions, only some of which are made available with a given include. [...] There is no sane way to map this header to a submodule. Thanks, at least someone noticed the elephant in the room, even if this only happens near the end of the otherwise optimistic and forward-looking text. I can't comment how it is for Linux folks, but both these things occur in spades with Windows Platform SDK. Until someone does a humongous job of reworking the headers to be amenable to modules, the entire feature on Windows will be just something you do with your own code or maybe with non-platform-specific libraries.
Designated Initializers will be very nice, if I understand them correctly. For one, it gives named parameters: foo({ .arg1 = value1, .arg2 = value2 }); Which would make the named parameters of this form unneeded (as an example of the sort of thing I've seen): // A hack: arg1, arg2 are actually objects with // overloaded operator= s foo(arg1 = value1, arg2 = value2);
write a cross-platform, client app (pick 3: ios, android, windows, mac, linux). if you care about making it fast, you'll end up with C++.
And `c++1z` mode will become `c++17` come November?
Why does this website have to change how my scrolling works?
&gt; Desktop applications would be easier with JavaScript or Java, web applications easier with NodeJS, PHP, or Python, and even games are easier to make with C# and Unity Engine. Getting something to show up on screen will certainly be faster with the languages you mention. Getting something *correct* to happen (i.e. according to your expectations) is imho easier in C++ thanks to compile-time verifications &amp; computations that are much more powerful than any of the languages you mentioned.
I think I recall author's name. Thank you very much. A colleague joked on me about C++ still not having calendar utilities out of the box. Sad to see he was spot-on.
Non-member vs member functions is not the issue. I want easy to use string operations (as members or non-members) without having to write those algorithms myself. Most other languages trying to be somewhat useful have a feature rich string class, e.g. Rust: https://doc.rust-lang.org/std/string/struct.String.html Just because the string can be seen as a collection of characters doesn't mean it must be treated like an STL container... ;-)
string_view is a welcome addition but it does not address the issue I was talking about, namely the feature-poor string interface. And whether that interface is realized by members or non-members is not an issue either. Most other languages trying to be somewhat useful have a feature rich string class, e.g. Rust: https://doc.rust-lang.org/std/string/struct.String.html and Python: https://docs.python.org/3/library/stdtypes.html#string-methods
A PDF. Let me print it before reading...
"The format is human readable and very lightweight." Very sneaky, but you're not fooling anyone. Just because the format definition itself is very lightweight certainly does not mean the implementation is. A good implementation likely uses a single unordered_map. Every level we go down in the 'object' our hash time increases as the object names are concatenated. This is further compounded by our human readability feature encouraging descriptive names for variables that aren't later compiled out. This is hardly lightweight. I don't know why it says the library can be implemented with C++11/14/17, if you want this to be in the standard its not going to be in C++11/14/17 standard; you will have all the tools of C++17. This is especially frustrating when much of this can be simplified using `std::variant`. Since our motivation is the serializability of JSON, we have a very limited subset of datatypes an 'object' can hold when compared to JSON in JavaScript which can have functions which are not preserved during the serialization of a JSON object. This simple set of datatypes makes this a perfect use case for variants. JSON stands for JavaScript Object Notation. Why would we add support for an object notation specific to another language. It's a decently made proposal, but JSON doesn't make sense to be standard in C++. There are many widely used 'things' in the realm of data formats. XML, MD, INI to name a couple. You don't see these in the standard library or being added to it. There are many great libraries for parsing and interacting with JSON in C++. I cannot see a good reason for this to change. As a side note: instead of trying to add support for everything into the definition of C++, an effort towards something like npm for C++ could be more useful. I know there are package managers for unix based systems (pacman, apt,...) which are essentially this, but there is no cross platform solution. It would certainly not be easy to create such a system and even harder to maintain and curate it, but it may make C++ more competitive with tools like nodejs/npm which have an advantage with ease of deployment of one code base on many differing systems.
It would be great to have a standardised way of dealing with JSON in C++. I started off with CppJson but recently switched to nlohmann which is much nicer to use in code. Neither of them, however, can support a decent debugger view of json elements, which is a major issue for me.
At least within Visual Studio, you can get the debugger to call .toString() or .serialise() or whatever your library uses to serialise the json from within a debugging session in order to see a pretty view of the fragment.
I agree completely. I don't think C++ should necessarily follow the model used by other languages in that everything should necessarily be in the standard library. We have an enormous ecosystem of libraries already; the only problems being that many of those libraries are actually C libraries (i.e. usually not particularly type safe, nor very RAII-friendly), and installation of those libraries in your development environment (which is usually a complete pain). That last problem can be attacked using a decent package manager (vcpkg for Windows is great in that sense), and the first problem is something we can slowly work on improving by providing wrappers or rewriting libraries in C++ from the ground up. Either way, I think libraries such as this, while being absolutely great, should _not_ be in the standard. Instead they should be part of whatever package management system for C++ libraries will eventually emerge. 
I just finished an implementation of `unique_resource` for a lib at work. The specifications of `unique_resource::swap` are really vague. You can easily guess what it must do though.
Yes, I should try that before complaining.
&gt; JSON stands for JavaScript Object Notation. Why would we add support for an object notation specific to another language. It originated from JavaScript. That's about it. We already have regex in the standard, so having JSON isn't really too far-fetched.
Heh, I love u guys ;) 
I like json but I don't really think it should be part of c++ language:/ example: hard to image someone developing firmware for microprocessors using it.
&gt; and my new metaclasses paper. please please please please be in C++20. &gt; Add designated initializers. Draft C++20 now allows code like: `struct A { int x; int y; int z; }; A b{.x = 1, .z = 2};` I wonder how it looks for inheritance, e.g. struct A { int a; }; struct B : public A { int b; } B val{ .A = {.a = 34}, .b = 45} ??
I find that C++ really shines as a full-stack application language, since it's by far the most pragmatic language around. The kind of application where you create anything yourself, rather than piling frameworks. Just the standard library alone is worth the effort. I tried writing Snackis (https://github.com/andreas-gone-wild/snackis) in several languages, before settling on C++ as the only viable choice for me.
Why single-out JSON for support by the standard library? There are a range of ASCII data formats (JSON, XML, YAML, ...) that you could potentially aim to support. Why does JSON deserve special treatment over the rest?
In scientific computing, the area I work in, index arithmetic is very common. Just open any math textbook and you will easily find formulas having `i-j` in them. The fact that negative numbers are not representable in size_t only makes things worse. Negative indices are easily detectable and make perfect mathematical sense. There in no surprise that many mathematical libraries (like Eigen) and computational frameworks (like OpenFOAM) use signed ints as their size and index types.
Btw, thanks for your support 😉☺
My understanding is that modules were never on the table for '17. It's a big enough feature that a TS was necessary first.
Don't use unreal to learn C++. Unreal Engine does a lot of stuff to emulate languages like java or C#. (Garbage Collection, Super, Custom Collection Classess, UCLASS and USTRUCT and more).
Just be aware that Unreal has been around long enough that it's built around custom libraries and memory management that is a little at odds with modern C++.
Unity doesn't use C++. Generally speaking Unity is more user friendly. But if you wan't to learn C++ I don't recommend using Unreal Engine. They do a lot of stuff to emulate higher level languages. You won't be learning proper C++.
&gt; Why is the state of software development so awful? Is it? The IT infrastructure of the world seems resilient against the worst we can throw at it, and apart from a few high-profile failing projects, most projects actually seem to be doing ok. Those failing projects typically seem to suffer from badly written requirements (being far too broad, or changing rapidly after work has begun), as well as implementation by companies whose primary business model is not so much producing software, but rather fleecing the government. That's not something you can change on a technical level though, that's an organisational problem. 
&gt; As a side note: instead of trying to add support for everything into the definition of C++, an effort towards something like npm for C++ could be more useful. I know there are package managers for unix based systems (pacman, apt,...) which are essentially this, but there is no cross platform solution. There is [`build2`](https://build2.org) which is a cross-platform C++ toolchain (build system, package manager, etc). Its aim is exactly this (full disclosure: I am involved with the project). &gt;It would certainly not be easy to create such a system and even harder to maintain and curate it, but it may make C++ more competitive with tools like nodejs/npm which have an advantage with ease of deployment of one code base on many differing systems. Yes, we have started such a package repository, called [`cppget.org`](https://cppget.org). It already, for example, contains a decent XML parser and serializer library for C++. 
The following might be too technical, but then again, hearing the answers to these might actually be interesting... Why is there so much UB in the language? Sure, I understand why some UB is more or less unavoidable if you want performance, but falling off the end of a function without returning an appropriate value is UB. Don't tell me compilers cannot detect that one, at zero runtime cost. Similarly, this program is already UB: char c; std::cin &gt;&gt; c; if (isalpha (c)) // oops, user might type high-ASCII character -&gt; UB Why are templates instantiated automatically? You might find yourself instantiating the same vectors, maps, etc. in thousands of translation units. I think I would have preferred just declaring a point of instantiation myself, and letting the linker put everything together. Are there any plans for adding facilities to the language for indicating conformance levels inside the source code? I.e. #pragma not_using_old_style_casts #pragma not_using_octal_numbers #pragma no_unmarked_fallthrough #pragma use_c++20_keywords (I write these as pragmas, but they could of course be anything) Why is the switch statement still based on integer literals, and can we not have a switch that (internally) expands to a series of if-statements? (jump tables are still fine as an implementation for situations in which it is appropriate) Why is there no constexpr switch, similar to constexpr if? How does he feel about the occasionally heard sentiment that 'OO has failed'? Will C++ always follow a file-based storage model for source files? Has anyone ever considered other forms of storage, like a database? (I'm asking because it seems to offer intriguing possibilities for basically completely doing away with forward declarations and headers, leaving only one definition for any object, and possibly making compilers a lot faster as well) 
I blame OpenMP and GCC (no compile-time warning! Unsigned works in simple cases but silently breaks some loops. Why did I have to dig into the spec to learn about this restriction?), but the experience drove home the point for me that unsigned int can be subtly disastrous; that, as bames53 put it, &gt;'this value should never be negative' is not a good reason to use unsigned. There are cases where the compiler may silently break that expectation (here, under a #pragma omp).
Is the OP the author and posting this because they are soliciting feedback? Because there's plenty wrong here, but I don't want to spend time writing out a critique for no reason. 
I'm so glad to not have to write `concept bool`. :-)
Could you point me to one of those alternatives? Plenty are discussed [here](https://stackoverflow.com/questions/11516657/c-structure-initialization) but none are any good. I like that sparse array initialization syntax! I hope it gets considered for addition at some point.
To the point that the std::string class isn't rich enough, it's list of member functions goes on and on to such an extent that I think most people would say it certainly is too rich. Oddly enough though, recently I was coding something in C where I had to use a loop because the C library does not have something close enough to std::string's find_first_not_of . So right now I'd argue that all algorithms that are useful in std::string should also be implemented as general algorithms. At least starts_with , ends_with , find_first_not_of, and find_last_not_of ... 
Not in a year. We will feature freeze later, when we go to Committee Draft (appx 2 years from now).
Try an example from something like CLRS. Or, take a project you've previously done in something like Python and port it over to C++. Use the cProfile to profile the Python code, then use the &lt;chrono&gt; header and std::chrono namespace to profile the C++ code. You'll see notice the increase in performance when running the native C++ code. This will get you to appreciate running native C++ versus interpreted Python.
In what context do you ever do this other than subtracting a known smaller container size from a larger one? (Something in fact I never do anyway either)
recomend a way to learn c++ from scratch??
Not a great reason, but JSON and YAML are really simple formats that are used a lot for configuration and such. Personally I don't think it's a good idea to standardize because it will probably be a half-baked committee product...
This is my feeling. I primarily do Java dev, but it frustrates me when the JDK pulls in things that are really not widely applicable or already have really good third party libraries. They did that with XML in XML's heyday. However, XML usage is waning. Yet now we are saddled with the extra bulk of XML handling in the JDK.
Your argument is nonsensical. Not everyone who uses C++ develops firmware for microprocessors. Also, 75% of Standard Library is probably not good enough for that task.
&gt; Why does JSON deserve special treatment over the rest? No one is giving JSON special treatment. It's just that nlohmann wrote a proposal specifically for JSON. If you want to see XML/YAML/... in the Standard Library, you can write a proposal for them.
adding json to the standard so you can get a better debugger output is like flying on an airplane because you like peanuts.
I think libraries like RapidJSON or ChaiScript are great examples of how to do libraries in C++ and don't have a package management requirement. The problem is I think the complexity of the build and link process and having a package manager doesn't really help solve that problem...
I was in LWG as we went over the wording for those facilities. It just turns out that getting a correct specification is surprisingly difficult.
If you can't tell if a function is a template, then you can't tell if `foo(X &amp;&amp; x)` is an rvalue reference of a forwarding reference. It is just another 'is that a template' problem, but is important enough to highlight on its own - and could possibly be solved on its own (`&amp;&amp;&amp;`?) My opinion is that there really isn't a 'deep split'. I think the vote will (if it ever happens) come back quite lopsided. But I could be overly optimistic/confident. And it is not that 'deep'. Everyone sees value to both sides of it. (Almost) everyone I talk to seems to agree with me, but I admit there is probably a self-selection bias at work there. (But it does include a number of people that I expected to be on the other 'side', thus my added confidence.) We may end up with a terse(ish) syntax where it is always explicit making the question moot.
That concatenates the function onto the end of the string. Obviously. 
OK, I'm hearing a lot of people asking for more string functions. Anyone want to gather a list of top requests? Expect the end result to be free function algorithms, not member functions. Hopefully ones that fit well with Ranges. And yes, you could say "it's obvious, the committee should be able to make that list and should have made it a decade ago". But the committee is just a bunch of volunteers. Making that list (even via a reddit thread maybe?) would help. 
Clearly it prints the string.
It turns it into a monad, actually. (that might not contradict what you said)
Don't expect metaclasses for 20. You don't rush things of that magnitude. I suppose it could happen, but don't expect it. 
Sadly, the last page contains the following lines written in small green font: „I think before I print. Save the trees”. Printing it will cause your printer to return -EOMGITSAPDF
&gt; They did that with XML in XML's heyday. However, XML usage is waning. Yet now we are saddled with the extra bulk of XML handling in the JDK. That's sort of like saying you shouldn't add anything ever though, since while we don't use XML as much today, it was very much in vogue some years ago. I don't know that something should be avoided only because in a decade's time something better will be found.
The game's not over until the im-comfortable-with-my-body-thanks lady sings. So there is a chance. My impression is that there is still work to do however, and we'd really like to get implementation and usage experience before moving forward. So please give it a try and let us know
That point is often raised, against any standardization effort. I think the fair argument is : web. Web speaks JSON and some XML mostly, having decent facilities for 'speaking web' in a language out of the box makes it very much more useful. Other languages have evolved and kept up. Python for instance, has had CSV of all things in standard library forever, and JSON got added only a couple years ago. YAML or TOML or even XML are still not part of stdandard library. 
As I see it, the initialization is logically ambiguous. It could either be a copy or an initialization through an initializer_list. C++ resolves this ambiguity by preferring to use the initializer_list when list initialization is used and a matching initializer_list constructor is available, as demonstrated in my example. I believe this makes sense, since you can use parentheses to indicate you want to resolve the ambiguity the other way, and if you want copy initialization you can use an equals sign. vector v = vector{1,2};
Yes, there is actually plenty of C++ firmware in microprocessors that does JSON. Often poorly, and dangerously via questionable custom parsers or half arsed libraries, i might add. 
hah it seems I was wrong
There is an often raised concern that 'we should not be adding all things to standard library' which i cannot quite comprehend. The argument is that 'in 10 years it will be poorly maintained and irrelevant' .. well yes, then you deprecate it ? ~~&lt;sstream&gt;~~&lt;strstream&gt;, anyone ? There are other not so great parts that have ended up in standard library, nobody is complaining about having them though. 
In this era of package management and the internet, why wouldn't you avoid it? Anything brought into the standard library must be maintained for the rest of the language life. Further, if the feature does anything "special" it can impact things like compiler development (see java serialization). Json may or may not stick around, and there are certainly competing serialization standards. So why stick it in the standard library? Are there not freely available json libraries of high quality?
I feel you, I feel just the same. However, it's clear that the library support is monopolized by people that don't really know what an easy to use API is - and they insist bastardizing operators and creating their own syntax out of what's barely allowed in the language, to make sure that they make their library as unusable as possible. Maybe in a few years' time there will be better libraries, I hope so, really I do. But who knows?
That's not really it. It's more like this: some things are core language features, so they go in the standard, and others are more of a support feature that some people need and others don't. Those things don't _have_ to end up in the standards document. Why should there only be a binary choice between standard and non-standard? As it is, things are already on a gliding scale anyway: features might be standard but just not available in a compiler, or they might be non-standard but so widely available that many consider them to be standard anyway. And we have things like Boost, which is already acting like a semi-standard of a sort. As I see it, while C++ could certainly use some kind of 'officially blessed support ecosystem', that doesn't necessarily mean that ecosystem must therefore also be part of the language standard. In fact there's much to be said for keeping those things separate: less bulk in standards documents, for one thing, and potentially a more agile approach to developing those support libraries as well. 
We could attempt to feature freeze (at the feature level, not the detail level) earlier, so that we have more time to work on library implications, for example. *See:* guides, deduction. I at least think/hope that having a rich and large feature set early means that we can more easily say no to last minute features. 
It turns a C feature that always fails to compile in C++ to a C feature that might compile today, but fail tomorrow. It's probably more fair to treat it as a novel C++ feature inspired by C.
The compiler/toolchain can be downloaded separately from MS I think since a year or so. I know this only kind of covers one of your use cases, but it's still worth noting. Visual Studio Build Tools https://blogs.msdn.microsoft.com/vcblog/2016/11/16/introducing-the-visual-studio-build-tools/ It also shows how to install them "silently" (GUI-less).
Why wouldn't it? As long as you can, somehow, 'emerge' a package and have it ready to go in your compiler as soon as the operation is done (i.e. include path set up, link path set up, etc.), the problem is solved, isn't it? I've struggled quite a bit in the past with compiling versions of libraries for Windows, and I imagine I'm not the only one (a good example is OpenSSL, but also Cairo, Pango, etc. Building these yourself is not entirely trivial). But now, vcpkg makes it completely trivial.
While it's true that MSVC tends to be the one that lags, I think it's unfair to blame them too much. One: they're not lagging in everything: they have modules and co-routines already. Two: they also need to support their users, they can't break too much in one go. And three: they've been bitten before by implementing not quite standard stuff that got dropped at the last second by the standardization committee...
&gt; the C library does not have something close enough to std::string's find_first_not_of `strspn`?
You probably still need administrator privileges to install those and they aren't portable installations. You could also probably copy the Windows SDK in the same bundle to make things easier. The tricky part after this is go get programs like CMake to work with them, there are quite a lot of paths to set. But when it's done, you can get any buildagent autoprovision themselves as part of the build script, which adds a lot of flexibility!
Wouldn't it make more sense for the C++ Standard Library to have support for JSON than for JSON to have support for the C++ Standard Library? Seriously though, widespread adoption of package manager technology would alleviate the need to shove everything into the Standard Library. It strikes me as a little odd that XML was never added to the Standard Library, and now we're talking about adding JSON?
Pretty well every project I do these days involves JSON somewhere within it. It's been years since someone asked me to send XML over the networks, and I'm sorry, but I really don't believe anyone anywhere is streaming .ini or .md files across the web. JSON is the lingua franca of the internet. The time has definitely come for a JSON within the C++ standard. 
I would strongly recommend against using this script, for three reasons. * One (a), Microsoft already provides this functionality in multiple ways. /u/sumo952 [recommends the VS Build Tools](https://www.reddit.com/r/cpp/comments/6nlmtr/create_a_portable_visual_studio_installation_with/dkal3m2/), which are the "official" supported way to install the VC++ compiler without the IDE. I believe it does require admin access, however. * One (b), the Visual C++ team provides NuGet packages that contain an entire drop of the VC++ compiler toolset. More info at http://aka.ms/DailyMSVC. NuGet packages are just ZIP files with a little metadata, or you can use the NuGet CLI. No admin access is required to install them. * Two, your script names individual files. It is **extremely** fragile. While we are very unlikely to [change the layout of the toolset again this decade](https://blogs.msdn.microsoft.com/vcblog/2016/10/07/compiler-tools-layout-in-visual-studio-15/) it is certainly possible that necessary files may be added, deleted, or moved around. Scripts are fragile when they aren't maintained by the team who produces the tools. * Three, note that options 1 and 2 are properly licensed as a [Supplement to Visual Studio](https://www.visualstudio.com/license-terms/mlt553512/). This probably isn't a big deal because I don't imagine anyone who can't use a free VS Community license would adopt an install script off of GitHub, and your script presumes a VS install, but it's nice to have things all tidy for those lawyers, ain't it? 
&gt; deduced to `std::vector&lt;std::vector&lt;int&gt;&gt;`? Yes, because for a `vector`, or any type with `initializer_list` as a parameter of a constructor, {}-initialization is not the same as ()-initialization. For those types, its an array-like (in a sense that we can initialize `vector&lt;int&gt;` as we can initialize`int[N]` with `{1, 2, 3, ...}` + the preference for `initializer_list`). And what do we specify in an initializer of an array? It's elements. Then, `vector v {elements...}` literally says: "`v` is a `vector` of `elements`". Ok, whats the actual type of `v`? Its a `vector&lt;T&gt;`, where `T` has to be deduced from the initializer, from `elements`, so we look at the `elements`, see if they're all have the same type, or have some common type, idk, and set it as `T`. So, where does dependency of deduction of `T` to amount of `elements` fits here? Why does it matter if types of `elements` happen to also be `vector`? In case of `vector`, {}-initialization is an array-like. Then treat is as one! `vector {vector {...}}` is vector of vector(s). Its simple, its obvious, and it is easy to teach. So, yes. vector a {...}; vector b {move(a)}; `b` is a vector of vector, because of `{}`. If you want to copy/move construct, use direct-initialization `vector v(move(w))`, or better yet `vector v = move(w)`. --- Regarding `tuple`. Whats a tuple? - A bunch of elements. Whats an array? - Its a tuple where all elements have the same type. Whats a `vector`? - Dynamically sized array. All these things are of the same sort. But `tuple` and `array` are aggregates. `array` is a proper one, `tuple` is emulated one (`vector` falls here too, I guess). And there is an aggregate-initialization, which is also done with {}. And it has this same problem: array a2 {array {1}, array {1}}; // array&lt;array&lt;int, 1&gt;, 2&gt; array a1 {array {1}}; // ??? tuple t2 {tuple {1}, tuple {1}}; // tuple&lt;tuple&lt;int&gt;, tuple&lt;int&gt;&gt; tuple t1 {tuple {1}}; // ??? `array` is initialized via aggregate-initalization, `tuple` is initialized via constructor call, hence the "emulated", but conceptually it's an aggregate. And well, the code says: "arrays of arrays, tuples of tuples". So what are the types of `a1` and `t1`? `array&lt;int&gt;` and `tuple&lt;int&gt;` _obviously_. If initialization of `array` and `tuple` would've been simmilar to initialization of `vector` above, then it would've been simple and obvious, what are the actual types. --- But of course the problem here is not with arrays or tuples. - Its with braced-init-list. braced-init-list is many things, its an aggregate-intializer, its an array-like, its a universal initalizer, and those things are overlaping, so we're ending up like this. To solve this problem, a question has (or had) to be answered: "What is the braced-init-list?" It has to be one, not many, at least within the limits of one class. edit: rephrasing
yay! :D I anticipate I'll have less use for this in my planet renderer than in my voxel thingy, but I *have* been meaning to port that voxel project to Vulkan like the rest of my projects...
We obviously work in different places but yes, bad and changing requirements are major problems. But I want to know Stroustrup's opinion on that.
I'm of the same mind, with one mental stumbling block: C++ doesn't quite even speak 'internet' by itself. HTTP, TCP/IP(4 6), DNS .. none of those layers are anywhere in standard libraries. And quite honestly, i haven't seen a single library or abstraction that I'd be happy to see adopted. 
What if JSON could be defined by the standard, but be optional like annexes in C. Standard library developers could choose to implement it and if not, outside libraries could be plugged in (possibly via package managers) to support the functionality. This would result in consistency across different codebases, allowing everyone to look at code and immediately understand how ````std::json```` works just like how you already look at ````std::string```` and its member functions and immediately know what it will do.
The regex library was a mistake. The boost version is way better than the current implementation in GCC regarding performance and generated code size.
I'm all for package management, but i've learned the downsides of it as well. Case in point, NPM, where an average packaged up piece of functionality is about 3-4 functions maybe. The average quality of the code is horrible, the dependency trees are miles deep, there is no reasonable filter apart from popularity to select your dependencies etc. With C++, it's actually even a bit harder, and i do spend much more time evaluating my C++ deps than node or python, especially for embedded work. Does it use exceptions, allocation strategies, which parts of standard lib does it drag in, what system facilities does it depend on etc. As for JSON sticking around .. TCP/IP or HTTP will likely not stick around either forever, but that's not an argument for not supporting either ever. Having a standard JSON in library will certainly not hurt anything much, just like having a std::string has not prevented anyone from using their own, often much better targeted string classes for particular uses. 
I was really hoping for what the title says: portable visual studio installation. It should mention that it is only for the compiler/tools. It might be considered click-bait otherwise. :-(
Just looked vcpkg up for the first time .. I think conan.io looks a bit more promising for a crossplatform way of doing this. EDIT, Wow, lol : "Why not Conan?" FAQ section https://github.com/Microsoft/vcpkg/issues/478 
"module" as a keyword broke a library I was using while testing in Visual Studio. The library had used "module" as a variable name. Is there some easy workaround to this that doesn't require modifying the library?
Or using the difference to index off another pointer: middle[a.size() - b.size()] where middle is arranged such that `middle[-1]`, `middle[-10]`, etc. are meaningful. (The above may appear to work even with unsigned sizes, but it's undefined behavior.)
Npm is fucked up. node_modules might be the dumbest implementation of package management I've ever seen. Library A depends on library B, and library C depends on B. You're telling me I need 2 copies of library B?
Package managers like apt and rpm rely on a consistent ABI for everything they install. That's difficult for C++, because the ABI depends on, for example, the standard library you used, the compiler options you choose, the size of every object you pass by value. If any of those change, you will, if you are lucky, be unable to link. If you are unlucky, you will discover the problem at runtime with difficult to understand crashes. Other languages avoid the problem in different ways, for example by being interpreted, by very late binding and indirection, or by having only one implementation of the language. Or by being small. C++ systems run into the 10s to 100s of millions of lines of code. And every translation unit has to use exactly the same definitions of everything. Having version 1.2, 1.7 and 2.3 of the 'same' library can be a disaster, and deferring until link time, or upgrading via shared library, often fails spectacularly, at least not without unceasing effort. That's why most C++ 'package' systems focus on being portable build systems, so that you can bring a library into your environment. Doing so in a cross platform way is tricky, since the build system then needs to work with unfamiliar compilers. There are more than 3 compilers out there. All of this is why there are pressures to move common things, even if they aren't used by all clients, into the standard library. Things in std:: are stable, and make good vocabulary types. Good development practices makes using 2nd party libraries (the ones from other groups in the same org) possible. 3rd party libraries are much more difficult. So, while C++ is probably never going to have as large a library as say, python [batteries included], it is going to be getting bigger, as we move up the stack of things used to build things into things useful directly by an app developer. 
Once upon a time, JSON was evaluated in JavaScript by eval(). These days, even JS uses a json parser. 
&gt; well yes, then you deprecate it ? Have you not observed how paranoid the standards committee (and vendors, et cetera) are about breaking existing code? You add something to the standard and so much as one person writes code that depends on it, you're supporting it basically forever.
Great post! But, ...ummm, I cannot figure out why the `when_all::execute` works correctly. It looks like that the `cs` are likely referenced without validity by another thread if the right-most lambda finishes its work before others and the call of `when_all::execute` itself. It doesn't matter when the following children are all of field-less types but so does when there is any stateful node like the `when_all`. Sorry for my Engrish.
I'm pretty sure this changed with C+11 and now with 17 there is a pretty significant shift. And I'm not new, been writing C++ in Turbo C++ since 93 or so. Just some example deprecation things that are being done: `register` keyword, `auto_ptr`, `throw()` specifier etc 
It turns out that you actually do, because the dependencies are incredibly fragile between versions. So your average node project will include at least 7 versions of lodash, at different nesting depths
Just not quite enough to agree on how to replace it. 
You can 'emerge' a package and have it ready to go for the system compiler and std:: library, which today is probably still --std=c++03. And, no, you can't mix c++03 and c++11 or 14 in the same binary. 
I bet if you put a logging statement in the "scope_guard&lt;fn_t&gt;(fn)", that it is *not* getting called. There is no way that the compiler is ever going to guess that's what you meant. 
There are some alternative approaches as well. CMake + Hunter, Conan.io , CPM .. each has their downsides. Just looking at Conan, having package definitions in procedural ( python ) rather than declarative form is probably not a good call. Python's own packaging has struggled with this for years. I read your FAQ, and it would be real nice if you re-consider your Github hosting point - just for the pull requests. 
Don't quite understand what you meant by "might compile today, but fail tomorrow". Can you elaborate?
Have you heard of Networking TS (which is based on ASIO)?
Implying everything else in the STL is the best of its kind...? Most people don't need top performance from regex.
No i had not, [looked it up](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4588.pdf). Seems like it's stripped down Asio. A tad scary because i think Asio is can be a tad convoluted way of doing the simplest things .. 
The only reason I ever used designated initializers in C was to deal with structs whose member order is unspecified (even the standard has a few of those). They make it possible to write code that is portable to situations where the order differs. The initializers introduced in C++ do not support that use.
&gt; where middle is arranged such that middle[-1], middle[-10], etc. are meaningful. I consider this less an argument for signed types and more an argument for code review that doesn't allow insane code to get pushed through.
Make an http webserver from scratch -- that's always a fun project -- and you'll learn a lot while doing it.
&gt; You asked for what a negative size means You've just hit the core of the issue though. "*[T]he amount a container needs to be resized to be the same size as the other one*" isn't a size at all. It's a delta. Sizes are unsigned (since they can't be negative), deltas are not. So again: Sizes should be unsigned because the type says something about the value it represents. A lot of modern C++ is about making invalid states unrepresentable (e.g. [`gsl::not_null`](https://github.com/Microsoft/GSL/blob/be43c79742dc36ee55b21c5d531a5ff301d0ef8d/include/gsl/gsl#L72)). A negative size is invalid so the type chosen should make that value unrepresentable.
And yet there are shipping modern compilers built today that support all the things you give as an example.
Why is that insane?
Sure, but I'm pretty confident there will be compilation profiles ( std=c++17strict or whatever ) soon where deprecated features will throw errors or at least emit warnings. It's a natural cycle, same happens with Python and other languages.
Re good news: Yes, but the reason I posed it immediately was more because I'm trying to get the trip report off my plate. :) I used to wait for the post-meeting mailing so that accepted paper links would be available, but (a) it's easier to write it up in the moment while I'm there than page it all back into my brain two weeks later, and (b) the great [wg21.link service](https://wg21.link) (thanks again to Maurice Bos for inventing that!) lets me write the links now and they light up when the paper (or its revision) are available.
starts_with, ends_with, trim, split, join, icompare, replace(string, string), ireplace, prepend (to be symetric with append) come to my mind. substr returns std::string although string_view would be more efficient. We can't change it so what about to add string::operator() (pos, len) ? And make it work for negative indexes like in python. Alternatively name this as slice(). Also some simple unified extraction like std::parse&lt;int&gt;(str, size_t* pos=0, int base=10) would be nice so it becomes symetric with std::to_string. Existing std::stoi/stol/stod/etc. feel like from the past century. I better use boost::lexical_cast than that. I am not sure if making them free functions only is a good idea. There are many existing member functions which are heavily used but won't become range algorithms. Examples are length(), substr(). Now imagine mixing new free functions with these member functions. One has to remember which one is what and for reasons most C++ programmers don't follow or consider marginal. 
Did you forget the title?
I don't understand what your point is. People with older code bases, using ill-advised features which are being deprecated/removed just won't set that flag and will expect vendors to keep supporting them.
Thank you, but that's way premature -- we are still early in proving out that the basic approach can work (is it efficiently implementable? can it solve the problems it aims at?), and this week we only had an initial presentation to a study group to get an initial read of interest in further development in this direction (is it aiming at the right problems? is the current way we surface the capability, or programmer's [UX](https://en.wikipedia.org/wiki/User_experience), acceptable to users and committee members?). The feedback was that the committee liked the core capability (if it proves out that it works) but wants some specific changes in the way it's surfaced (UX -- primarily, to explore whether we really need a whole new metaclass concept or could it be integrated more closely with existing concepts in the language).
Ooops
Principle of least surprise: People don't expect array subscripting to be used with negative indices. It's also exceedingly infrequently used (I've never seen an intentional use of it) which means that it's harder to maintain/reason about. You're taking the bounds checking/correctness problem and doubling it, since now you have to worry about running off either end. I'm sure there's a handful of instances where this is actually the best tool for the job, but if I saw it in code review odds are it's just someone being unnecessarily clever and they can rewrite the code in a way that's clearer but doesn't stroke their ego quite as much.
As far as package managers go, [vcpkg](https://github.com/Microsoft/vcpkg) is far better than NuGet (for me). It takes a few commands to set-up and even fewer lines to add/remove a package and it's instantly available for use (and links automatically). Plus, the more people that use it the more packages we get added to the list of libraries available.
I dont think that's the case. Most of the really old code that i have to maintain and compile i do with .. old compilers, too - at my own peril. You cant expect to come with C++ codebase from '95 and expect it to compile and work flawlessly with a '17 compiler, and nobody does. If you have to maintain and modernize a library, you'll maintain and modernize it, assuming it's valuable and it's worth the resource investment. If not, keep compiling it on the old systems/platforms and deprecated features won't be an issue. You made a statement that breaking existing code is somehow a big deal for C++ specifically and things won't ever get deprecated - my point is that it's not really that much of an issue in real world, and C++ committee seems to have embraced that mindset too by _actually actively deprecating features and removing old cruft_ 
JSON is defined by the ECMA standard, so by adding it to the C++ standard we make C++ dependent on another language's standard.
We're working on getting a command-line environment script built into the NuGet package. It's meant to be used from inside of VS, but you can of course run the tools off the command line. For the time being, here's what I do. I open the matching VS command prompt (e.g., 64-bit hosted and targeting), change into the directory that contains the NuGet toolset, and then run this very fragile and limited script that you should replace with the real one as soon as we start including it instead: @echo off if not exist lib goto HELP if not exist build goto HELP where cl &gt; NUL 2&gt;&amp;1 if errorlevel 1 goto HELP set _PATH_ROOT=%CD%\lib\native set PATH=%_PATH_ROOT%\bin\Host%VSCMD_ARG_HOST_ARCH%\%VSCMD_ARG_TGT_ARCH%;%PATH% set INCLUDE=%_PATH_ROOT%\atlmfc\include;%_PATH_ROOT%\include;%INCLUDE% set LIB=%_PATH_ROOT%\lib\%VSCMD_ARG_TGT_ARCH%;%LIB% set LIBPATH=%_PATH_ROOT%\bin\Host%VSCMD_ARG_HOST_ARCH%\%VSCMD_ARG_TGT_ARCH%; set ATLMFCDIR=%_PATH_ROOT%\atlmfc set IFCPATH=%_PATH_ROOT%\ifc\%VSCMD_ARG_TGT_ARCH% echo Set environment to NuGet tools located in %_PATH_ROOT% set _PATH_ROOT= goto :EOF :HELP echo Run this script from inside of a developer command prompt and in the directory of an MSVC NuGet echo package, e.g., \Projects\Foo\packages\VisualCppTools.Community.VS2017Layout.XX.YY.ZZZZZ-Pre :EOF Now, for the time being, the MSVC tools in the NuGet packages are no newer than the ones included in the last V[S Preview release \(15.3 Preview 4\)](https://www.visualstudio.com/vs/preview/) just [released on July 12th](https://www.visualstudio.com/en-us/news/releasenotes/vs2017-preview-relnotes). Both the Preview release and our current daily builds are at compiler version 19.11.25505. So the better answer is to just use the VS Preview right now. It contains command scripts. As for your last question, we've talked about adding "known directories" to the compiler in addition to allowing environment variables. This would allow the MSVC compiler to have the same sort of "magic" built-in knowledge as to where it should find its libraries. For now, we're not going that way. It can get really fragile when the compiler finds the wrong include files or libraries. But I totally understand the appeal of the way a Unix-based compiler finds dependencies. Thanks for suggesting it!
I totally agree, and the VC++ team will be using vcpkg for libraries going forward. But NuGet works for tight VS integration so we're using it for the MSVC tools. Thus OP's question. 
&gt; from '95 and expect it to compile and work flawlessly with a '17 compiler 95 is rather early to talk about standard C++. Sadly I am still working in c++98 mode right now for various reasons and I would expect the code I write today to still work next week. 
That's a fair point too. Standard library does not need to be _the best of the best_ whether its in regards of performance, syntax or footprint. Adequate, well behaved, consistent and documented will do, there is always room for specific libraries to do better for one or other use case. Case in point: just how many std::string alternatives are out there ? 
&gt; Principle of least surprise: People don't expect array subscripting to be used with negative indices. Depends on the domain. And people also don't expect modular arithmetic except in specific domains, so by the same token unsigned should be avoided (except in those domains). Again, the valid range for a value not including negative integers is insufficient to justify using unsigned.
But that's the core of the problem. Types don't exist in isolation, they need to be considered together with the the operations on them. Unsigned types model modular arithmetic but what people expect when they take the difference between two unsigned variables is standard subtraction between two non-negative numbers. If the subtraction operator between two unsigned returned a signed there would be no (or very little at least) problem. To make "invalid states unrepresentable" you need to use something from foonathan/type_safe, not modular arithmetic when you mean non-negative integers.
&gt; Also some simple unified extraction like std::parse&lt;int&gt;(str, size_t* pos=0, int base=10) Do you mean something like [`std::from_chars`](http://en.cppreference.com/w/cpp/utility/from_chars)?
&gt; And people also don't expect modular arithmetic except in specific domains, so by the same token unsigned should be avoided (except in those domains). I'm going to go ahead and say that this is a bogus claim. Most people who have had any sort of formal computer science/software engineering training expect that unsigned integer overflow involves wrapping to zero (i.e. modular arithmetic). The one that most people find surprising is that signed integer overflow doesn't just wrap to `std::numeric_limits&lt;T&gt;::lowest()` and is instead UB. &gt;Again, the valid range for a value not including negative integers is insufficient to justify using unsigned. This argument could equally be made to justify not using non-const references and instead always using pointers.
Evolution sims and non-gpu AI experiments.
&gt; You cant expect to come with C++ codebase from '95 and expect it to compile and work flawlessly with a '17 compiler, and nobody does. As /u/josefx has pointed out 1995 is a bit of an early year to choose with respect to C++. People seem to routinely do exactly what you're talking about with C.
That's my whole point - i also work with plenty of legacy code. It won't stop working, but at some point you may either be required to put in effort to modernize it slightly if you want to use modern toolsets, or just keep using your current/old version of tools, including a compiler. If i ever need to recompile my old Symbian code, i actually have to pull out some _really old_ clunky tools running in virtual machines. There is nothing particularly specific about this for either C++ or C, other languages go through the same cycle of deprecating and modernizing. 
The more complexity you add to the Standard library, the harder maintenance gets. Specially because 1) modularization isn't something C++ is good at, and 2) there isn't any central implementation (like Java's, Rust's, Python's etc). Why continue bringing more mistakes to the std, rather than fixing the essential problems first (modularization, package managing)? &gt; Most people don't need top performance from regex. And yet a few people use the standard version. Some people don't even know there is a regex library in the std.
&gt; If the subtraction operator between two unsigned returned a signed there would be no (or very little at least) problem. But then you would run into the issue you have with pointer subtraction: For two pointers to a single contiguous memory block (such that the statement `a &gt; b` is defined) the following code is of ambiguous defined-ness: std::size_t diff(std::max(a, b) - std::min(a, b)); Due to the fact that the result of subtracting two pointers is `std::ptrdiff_t` which is signed and it's possible that the distance between two pointers is greater than `std::numeric_limits&lt;std::ptrdiff_t&gt;::max()`.
[fmtlib](https://github.com/fmtlib/fmt) could be the first step.
Macros? #define module my_variable_name #include "../library/header.hpp" #undef module
&gt; Why continue bringing more mistakes to the std, rather than fixing the essential problems first (modularization, package managing)? Maybe it's possible to do more than one thing at once. I think framing things as 'mistakes' in std is not productive either. You can always make an argument for one or other facility to be less than perfect. The language itself is never perfect in the first place, it's just our methods and paradigms evolve. Who exactly is hurt by having a less than ideal std::string in the library, that does not do all the things for all the people, but works okay for basic needs ? Also, removing "mistakes" that have outlived their usefulness is not that big of a deal in a decently managed lifecycle of a library. 
The only thing I wish vcpkg had was a way to add libraries without having to PR into the repo. Right now my team is using a gradle setup with maven dependencies for C++, since it needs to be cross platform. It's a giant PITA to set up, but I do wish there was a way we could at least publish the windows packages to vcpkg easily, so users would have easier access. BTW, part of the reason above I was asking about the independent compiler is Gradle actually does that with its native builder. It actually removes the vcvars added variables, and just invokes the command line directly. It also manually finds and adds the include directories. Its kind of horrible. It also build slightly incorrect libraries, mainly involving which symbols get exported. Sorry about making this long winded, but any chance MSVC would add an export all symbols argument to the linker? I know things like cmake are adding this functionality to make shared libraries cleaner, and having that functionality built into msvc would be awesome.
Precisely, there is no easy way around it so we cant change the subtraction operator. For memory addresses you probably want modular arithmetic anyway. At the end of the day the only sensible way is to use unsigned when you want modular arithmetic, and use signed when you want integers, non-negative or not. And if you really want to make the negative state unrepresentable you need a custom type.
Initialization order of data members is important in C++ due to destruction order. I think this is align with existing requirement in the standard and a sensible choice.
A more important question than whether or not people are still streaming .ini files is whether or not people will write new code that streams .ini files. Getting JSON support on std would remove a dependency from many future projects.
But again, and admittedly I'm playing devil's advocate here, why prioritise the web? I'm pretty sure XML support is part of the python standard library...
The alternative is to have package management be NP complete and possibly unsolvable...
&gt; I'd like to hear the argument Aside from the original papers proposing the effort begin, a recent paper exists where the authors attempt to address that question: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0669r0.pdf &gt; reducing the portability of the language, e.g. for embedded development or in critical systems. C++ already differentiates between library components required by "hosted" and "free standing" implementations, so portability is not a major concern. &gt; Cairo in the C++ standard library Note that "Cairo" is not being standardized; they're building a new library, which while originally derived in spirit from Cairo, is making changes based on the feedback of myself and graphics experts in SG14 (e.g. Guy Davidson who is now a co-author of the graphics proposal) towards a more sensible interface closer to those used by all the folks who already ditched Cairo in production. :)
&gt; For memory addresses you probably want modular arithmetic anyway. I don't agree with that. There have been very few situations wherein I've ever wanted the defined-ness of unsigned arithmetic. The only situation I can think of where I actually wanted that had nothing to do with memory addresses. The issue is that people are conflating implementation/standardization artifacts/details with some kind of meaning. As far as I'm concerned, except in very narrow cases, any code that causes an integer overflow of any type is broken. From my understanding unsigned types have defined overflow semantics just because it was natural: There are not, nor were there ever (to my knowledge) competing representations of unsigned integers which had different overflow semantics. The same cannot be said of signed integers (1's complement vs. 2's complement). The solution to this problem isn't to just wantonly run off and use signed integers for everything, especially (but not solely) because sizes don't fit into signed integers as a general rule. This is especially obvious on a 32 bit system. The solution is for there to be some way to tell the compiler that unsigned overflow should be undefined as well.
&gt; I'm going to go ahead and say that this is a bogus claim. Most people who have had any sort of formal computer science/software engineering training expect that unsigned integer overflow involves wrapping to zero (i.e. modular arithmetic). The fact that most programmers know about modular arithmetic and know that unsigned uses it is irrelevant to the fact of how often they write arithmetic expressions without the intent to use modular arithmetic, or read arithmetic expressions without noticing some erroneous use of modular arithmetic. &gt; The one that most people find surprising is that signed integer overflow Overflows are typically unintentional and modular arithmetic wouldn't be the desired behavior anyway. Saturation or raising an error would most often be the desired result. Fortunately in signed types the boundaries where unexpected behavior is triggered have sensibly been put as far away from zero as possible. &gt; This argument could equally be made to justify not using non-const references and instead always using pointers. Well, take each of the four points I made that lead up to the conclusion that the valid range of a variable is insufficient to determine that unsigned is appropriate and see if they analogize to pointers vs. references.
There's also [RFC 7159](https://tools.ietf.org/html/rfc7159) that deals a bit better with JSON in the wild. But, in any case, depending on other standards is not a bad thing. Unless you're arguing that there ought to be a C++ON? 
&gt;Fortunately in signed types the boundaries where unexpected behavior is triggered have sensibly been put as far away from zero as possible. It's not "*unexpected behavior*" that's placed so far from zero, it's "*undefined behavior*." Calling it "*unexpected behavior*" presupposes that there **is** a behavior, but this is not required. Speaking about the behavior of signed overflow is erroneous. &gt;they write arithmetic expressions without the intent to use modular arithmetic Which is irrelevant. You're critiquing unsigned integers because they have properties which the users thereof may not intend to use in every case. Which is fair and is one of the reasons frequently performed or particularly complicated expressions (using signed or unsigned integers) should be ferreted away in a function where they can be analyzed, verified, et cetera all on their own. But this ignores the fact that (see above) signed integers have properties which the users thereof may not intend to use in every case, and that if unsigned integers are flawed because of their unwanted/-used properties, then signed integers are also flawed for the same reasons. When someone writes an unsigned integer expression they may not intend to take advantage of modular arithmetic, true. But equally when someone writes a signed integer expression they may not intend to take advantage of the fact that overflow is undefined or that negative numbers are representable. &gt;or read arithmetic expressions without noticing some erroneous use of modular arithmetic. And the same can equally be applied to signed numbers. If I decide to use signed integers for sizes everywhere I may not notice a computation yielding a negative number in the same way I may not notice an unsigned computation which is (mathematically) negative (and which therefore wraps to a very large number). Ultimately both computations are incorrect and will produce (in likelihood) incorrect behaviors which must be found and debugged. And in both situations the generated values will be trivially easy to recognize as incorrect. The difference is that when you write erroneous code with unsigned integers the computation is at least guaranteed to produce a value which you can examine to see its effects. With signed integers you have no such guarantee. If you write an erroneous computation with signed numbers which wanders off the end of the range the compiler/computer is not required to do anything sensible. Also from a code review point of view it's easier (in some cases) to spot these kinds of unsigned bugs as opposed to signed bugs. If I'm looking through a peephole at some computation where someone chooses a signed integer and the computation can conceivably be negative, I can't conclude that's a bug without examining the rest of the program/surrounding code (which takes time, has mental overhead, et cetera). Whereas precisely because of what you said (that people don't intend to use modular arithmetic in most cases) with an unsigned type I can identify a possible bug very quickly by imagining a plausible situation wherein the result could be mathematically negative.
Ditto. This is totally incorrect: &gt; the overloaded member functions like shared_ptr::operator-&gt; access the pointer in the manager object to get the actual pointer to the managed object. The deref and arrow operators perform single indirections, not double indirections. (The control block's pointer is used for deletion.)
I forgot to ask. Any information on the outcome of P0684R0 "C++ Stability, Velocity, and Deployment Plans"?
Maintaining standards based code isn't quite the same as a regular library. You don't get feature requests at the same rate, or at least ones that you have to deal with immediately. There are, of course, the occasional bugs to fix. The sand isn't shifting out from underneath you, though. You're not relying on parts that are moving out from underneath you, and C++ rarely makes non-broken code broken. If that were to happen in a standard library implementation, I'm pretty sure that's exactly the sort of thing that would cause the committee to back off from that change. I think the real concern is that the size becomes a barrier to entry. 
Simple question, do you want cpp to be a more general purpose language with wider audience and community? I'd yes, then web kind of needs to be an area of focus
Thanks! Hopefully `when_all::execute` is correct. Here's my reasoning: * `cs...` are part of the same computation chain, and accessing them will be fine as long as the computation chain is alive. * `.execute` can return before the scheduled computations are complete, but it is assumed that the entire chain will be alive until all computations are completed. Ensuring this is the job of `.wait_and_get`. Do you have any particular scenario in mind that could cause the issue? By the way, I added [some randomized tests](https://github.com/SuperV1234/vittorioromeo.info/blob/master/extra/zeroalloc_continuations/p2_parallel.cpp#L370-L410) and ran them [under various combinations of sanitizers and optimization levels](https://github.com/SuperV1234/vittorioromeo.info/blob/master/extra/zeroalloc_continuations/runtests). While this doesn't prove anything, it increases my confidence in the possible correctness of `when_all::execute`.
It's already dependent on C standard, isn't it?
Not so surprising when the rust compiler and clang use llvm as backend 
If it's a goal to make {} and () not do different things, I think there's a better proposal to be made. 
Qt and other similar frameworks handle quick to screen.
My wishlist: * https://github.com/nlohmann/std_json/issues/5 In my opinion, there should be several pieces: 1. Json lexer ([gason](https://github.com/vivkin/gason) is excellent for this.) * One advantage is that the lexer could handle arbitrarily long ints and floats. The parser would later flag an error when data do not fit the datatypes. (Technically, very few Json parsers satisfy the [JSON standard](http://json.org/), which does not limit numerics.) * Also, lazy parsing becomes possible, which in some scenarios leads to much quicker sparse access than with most Json parsers available today. * It is also possible to stream the lexer to the parser. 1. Json parser, templated on `&lt;Int, Float, String&gt;` and maybe also list and dict types. * If list/dict are also template arguments, then STL allocators are automatically included. 1. An API for providing converters between array-of-char and those template-arg types Several lexers, specializations, and converters could be provided. There could be obvious defaults, so convenience should not be the issue. (For the record, I maintain [JsonCpp](https://github.com/open-source-parsers/jsoncpp), which is not even close to what I would want in standard C++.)
Thanks for the first actual on topic comment in here, rather than the usual brawl over what should and what shouldn't be in standard, and why we still don't have working g package managers
Why is package management NP complete?
&gt; It's not "unexpected behavior" that's placed so far from zero, it's "undefined behavior." Calling it "unexpected behavior" presupposes that there is a behavior, but this is not required. Speaking about the behavior of signed overflow is erroneous. My comment encompasses both signed and unsigned. I.e., the unexpected and undesired behavior that occurs with both signed and unsigned types is "unexpected". And in any case "unexpected" is entirely appropriate to describe the behavior of signed types here because I am not talking about the behavior defined by the standard, but the actual, literal 'behavior' of the program in the real world. The language standard does not define any behavior; that does not mean that the reified program doesn't exhibit behavior. &gt; But this ignores the fact that (see above) signed integers have properties which the users thereof may not intend to use in every case, and that if unsigned integers are flawed because of their unwanted/-used properties, then signed integers are also flawed for the same reasons. The relative likelihood of the two possibilities matters, and weighs in favor of signed types. &gt; And the same can equally be applied to signed numbers. If I decide to use signed integers for sizes everywhere I may not notice a computation yielding a negative number in the same way I may not notice an unsigned computation which is (mathematically) negative (and which therefore wraps to a very large number). It's not equally true, because "some erroneous use of modular arithmetic" can occur due to things implicit type conversions, and other things related to using unsigned. So in plenty of cases negative results and intermediate values are perfectly valid and expected, but modular arithmetic still screws something up somewhere. &gt; The difference is that when you write erroneous code with unsigned integers the computation is at least guaranteed to produce a value which you can examine to see its effects. Having well defined behavior can actually be a negative in some cases because it may be preferable to be able to use tools like ubsan or -ftrapv. And even if similar tools exist for unsigned types you then have to deal with the fact that other, non-erroneous parts of the program may intentionally use unsigned overflow, preventing you from being able to just get straight to the problem.
commenting for future (pun intended) reading
Writing embedded firmware on very low end devices, for one. Try it: get a STM32 demo board for $15 and see what it can do
You are probably referring to heavily templatized and sometimes arcane toolsets like Boost. You wouldn't be the first person to be put off by that. However, you have a lot of choice with C++ to pick your own tools of trade. For instance, Poco is excellent, straightforward toolkit for many networking apps, Qt does GUI apps decently too.
&gt; I.e., the unexpected and undesired behavior that occurs with both signed and unsigned types is "unexpected". In what universe is the undesired behavior of unsigned "*unexpected*" though? Can you find a single formally educated software engineer or computer scientist on the planet who doesn't expect unsigned integers to overflow by wrapping to zero? The only plausibly unexpected behavior here is signed integer overflow. Most formally educated software engineers/computer scientists who don't understand the vagaries of C/C++ would expect that it wraps when it's undefined. &gt;The relative likelihood of the two possibilities matters, and weighs in favor of signed types. How do you figure? In both situations a wrong calculation yields a wrong answer. In both cases it's a bug. I don't see how it's fundamentally easier to detect `-1` vs. `18446744073709551615` and say "*oh wow that's wrong!*" I'd actually make the argument that it least in terms of context free values I'm more likely to think negative one is acceptable. With context they're both clearly bizarre and immediately obvious. &gt;It's not equally true, because "some erroneous use of modular arithmetic" can occur due to things implicit type conversions, and other things related to using unsigned. And the same is true of signed. Implicit integer conversions in C/C++ are a complete mess both for signed and unsigned types which is why I practically make a religion out of ensuring that operands always have the same types. It seems like that's a better recommendation than just avoiding unsigned types altogether. And none of this cuts to the core of the issue which is that seeing something declared as an unsigned type communicates something about the intention of the programmer. Someone made a conscious choice to use an unsigned type which means they understood something about the problem domain which means that the value-in-question can't be negative. It's the difference between seeing `int *` and `gsl::not_null&lt;int *&gt;` or `int &amp;` which is a problem I deal with on a constant basis since my place of employment insists on using the Google Style Guide. I can't count the number of times I've had to ask someone "*hey can this be `NULL`?*" and gotten "*idk*" as an answer because the information that the original programmer had about the problem domain is lost because someone chose a type which doesn't communicate/preserve that knowledge. Also particularly `std::size_t` gives you a lot of nice guarantees which not only signed types don't have, but which they can't have.
I must use php at work, and I miss templates in it. I mean, i do feel insecure without type-safety
++++++++[&gt;++++[&gt;++&gt;+++&gt;+++&gt;+&lt;&lt;&lt;&lt;-]&gt;+&gt;+&gt;-&gt;&gt;+[&lt;]&lt;-]&gt;&gt;.&gt;---.+++++++..+++.&gt;&gt;.&lt;-.&lt;.+++.------.--------.&gt;&gt;+.&gt;++.
With regards to adding a package without PRing our server, are you just looking to run a local vcpkg? Or am I not understanding your scenario? Regardless, you should enter an issue at https://github.com/Microsoft/vcpkg/issues and discuss what you want. With regards to Gradle, yes. It tries to force MSVC into a Unix model. And the issue you're seeing is what happens... As for the linker, do you want to force everything you wrote into /EXPORT or get all the libs exports with /WHOLEARCHIVE? Sorry, again I'm not getting it. 
Yeah the gradle issue is definitely a pain. I wish they would have done differently. For the exporting, the equivalent of this cmake flag would be nice built into the linker. https://cmake.org/cmake/help/v3.4/prop_tgt/WINDOWS_EXPORT_ALL_SYMBOLS.html Only exporting files specified in a def file or explicitly marked dllexport just seems like an artifact from the past, and not having to have the build tools have a custom setting to export everything to a dll would be nice. 
TIL "reader/writer iterator" == StAX, thanks! I don't understand the impassioned defense of SAX in that thread. Every time I've had to use that interface it's been painful. If you want SAX, build it on top of StAX. Nicol Bolas is on point in that thread.
They were maybe on the table for C++17 in 2013. IIRC it was still considered a possibility when C++14 was being finalized, but not for very long after that.
Way too late, due to `&lt;regex&gt;`. C++14 [intro.refs]/1: &gt; The following referenced documents are indispensable for the application of this document. For dated references, only the edition cited applies. For undated references, the latest edition of the referenced document (including any amendments) applies. &gt; 0. Ecma International, ECMAScript Language Specification, Standard Ecma-262, third edition, 1999. &gt; 0. ... Likewise for C++11.
It's equivalent to a SAT-3 solver, which is NP-complete. Roughly, you're trying to satisfy all of the constraints about allowable versions. Someone recently formulated a mapping from package management to SAT-3, which is not terribly surprising, since package managers have been using SAT resolvers for a while. https://research.swtch.com/version-sat 
You're saying this proposal is half-baked?
That's what the `save` button is for.
oh, i didn't know that functionality existed.
Found the Java programmer! 
The library is not the same as the language. As long as you don't include that header it shouldn't cause any effect on the code. 
&gt; Qt the C++ flagship GUI framework, if one wants to target embedded or mobile devices the answer is QML, with C++ widgets being left to classical desktop applications. Speaking as a KDE developer, we're also using more and more QML at the desktop level. C++ is often tied in, but we have increasing numbers of desktop apps which are pure QML, built on Qt and KDE QML libraries.
That's a good point that I failed to consider. I still feel like the whole thing could be done differently
Yikes, compilation time on that Kitchen Sink benchmark was also really rough for CopperSpice: Qt 5 (moc) Verdigris CopperSpice Compilation time 1:57 1:26 16:43 Binary size 1.32 MB 1.36 MB 115 MB 
Thank you for kindly explaining! I overlooked the job of `.wait_and_get` as you explained. Now it's clear.
It should be easy to see that satisfying dependencies is in NP: A successful answer can provide listing of packages and versions as certificate, and it can be verified in polynomial time. As to NPC, you can convert it to SAT, and convert SAT to satisfying package dependencies with multiple version* Quick sketch: consider SAT formula with N clauses. If we create a package M(aster), that depends on N packages C1..CN (representing clauses 1..N), that in turn depend on packages X1..Xm (variables), with versions consisting of {true, false}, then successfully installing M also gives you answer to your original SAT problem. Converting package management to SAT is left as an exercise to reader. (Its just lot of busy work to turn numbered domain into binary one.) * Assuming you want single instance of dependency installed, and not multiple ones. 
Thanks for the answer. Alas, SAT solvers are quite effective these days, so finding a satisfying assignment should not be "unsolvable".
Could you provide an example implementation which could be used as inspiration for an API?
Ok, I kind of am loosing hope here, but here goes my question: What is up with the process of adding C++ **language** features that depend on the standard **library**? This makes writing STL replacements a nightmare. Some areas really benefit from replacing the STL library, be it with a consistent implementation between compilers, or providing a trimmed down implementation that favors raw speed over memory safety and/or correctness for all edge-cases.
We really could use a simple way to install various versions of compiler toolchain from command line without any fat graphical installers and countless of hours of hunting for actual setup packages. Much like "vcpp compiler for python 2.7 package". This would b immensely helpful for build servers that need to produce binaries built (from command line/script) with various vcpp compilers and runtimes. Any chance we will see this in near future? These nuget packages look vaguely like what I described but not exactly.
Who cares. This post is inappropriate for this forum as this article is not about C++ or a topic of any particular interest to C++ programers. I know that Rust advocates think that C++ programmers SHOULD be excited about Rust, but the fact that you think C++ programmers should care does not mean they do.
It doesn't even tell us which compiler is used for the benchmark... I'm not sure I'd trust those numbers.
1) Your 'json' class appears to require that client code either use it directly or write other code to translate between native data types and it. The first would be a bad design (what if my application has to support something besides JSON in the future?) and the second is both a developer headache and a performance drain, since all the data would be copied twice in any serialization/deserialization operation (and to/from a dynamic data structure at that). If the standard is going to support JSON, it will need to be possible to convert directly between JSON data buffers and native classes/structs. Apologies if your proposal does support that and I just misread. 2) I think you're being overly specific by proposing only JSON. As noted in other comments, there are a myriad of other serialization formats and by the time any such proposal makes it into the standard, there may be another new hotness. It wasn't so long ago that *everything* was going to become XML. What you should be shooting for is a generic framework that can support multiple formats via something like template parameters. Then make JSON and one or two others (XML? SQL?) the initially supported formats. That would make standardization worth the effort. 3) Any effort to standardize serialization-related functionality should probably follow and integrate tightly with reflection. That way, schemas can be easily derived from plain-old structs and classes and serialization logic auto-generated. Perhaps with optional attributes to define/customize the schemas inline with member definitions, a la Java. Although maybe that can all come in a "serialization v2" spec, as long as it's planned for in advance. All that said, good on you for making the effort to put something out there. Don't let the negativity get you down, just try to filter out the good advice and come back with something even stronger.
Hey, thanks for checking in here. The only C++ StAX lib that i have seen was LlamaXML, which is still available [through SourceForge](https://sourceforge.net/projects/llamaxml.berlios/) Obviously both Java and C# have a full suite of SAX/ StAX / DOM parsers, too. Here is a [simple comparison](https://dzone.com/articles/parsing-xml-using-dom-sax-and)
&gt;The only thing I wish vcpkg had was a way to add libraries without having to PR into the repo. You can package your own libraries using [this tutorial](https://github.com/Microsoft/vcpkg/blob/master/docs/examples/packaging-zlib.md). Though it won't be available to everyone in the world, you can share you configs for your packages with your team and they'll be able to retrieve, install and use your packages.
however I like qt, it's still slower to setup than opening a text file and putting `&lt;html&gt;foo&lt;/html&gt;` inside
I am not sure that desktop apps would be easier with JavaScript or Java. Do you know of Qt? Similar for games. C++ is not used much for web apps. I see things thus: C++ is not easy, but it can give you unbelievable results due to the level of control you gain over details, and details are where the quality comes from. For me, you seem to be looking for "easy" too much. If so, don't do C++.
debian has some solutions: https://people.debian.org/~dburrows/model.pdf
&gt; As it is, things are already on a gliding scale anyway: features might be standard but just not available in a compiler I think that someone should sue when a compiler advertises "ISO C++" support but does not support all the mandatory features.
&gt; but I really don't believe anyone anywhere is streaming .ini or .md files across the web. you know that there is aworld besides the web, right ?
:) :P Just in case you were serious: The expression "I can write C in any language" is not a critique (or joke) about C. It is a reference to programmers who know one language well, then try to port its paradigms and ways of thinking to the next language they learn, without considering that a good technique for one language may be a poor fit for the paradigms and APIs of another one.
I learned a lot of C++ by writing some code for Clang. You can find a bug or two to solve. I would advice by starting something small, like clang-tidy or clang-format.
So they are building a new library but because it's part of the standard it will be: - unable to be used with older standard compilers. - hard to version as a dependency because the APIs are part of a standard. Why can't what they make be released as a library? Why must it be part of the standard? It seems to me making it part of the standard makes it objectively worse in every way except that it might be slightly easier to use and have more uptake.
You're mistaken. My point is just the opposite. Besides 1 major feature (the borrow checker), I do believe rust does not bring a lot new compared to modern c++. I've some decent code base in modern c++ and was wondering if it was worthwhile to explore rust or not. Before that I was using Ada 2012 but Ada has his issues (a niche and poor offer in 3rd party libraries to address modern needs). The day c++ has some decent and generally accepted fixed support that is close enough to Ada's, I would be delighted. All I wanted to share is that it is possible to write c++ in the rust way without any issue and with the same "compiler" (llvm) yield similar or even better performance (about 10%). Unfortunately, gcc yields in slower code (about 20%). Something important to keep in mind in case someone claims rust is faster than c++. So much depends on the compiler's optimizer.
Like /u/savuporo said, there's not much prior art in C++. Though I don't know which StAX parser they're referring to. The [jsmn](https://github.com/zserge/jsmn) C library is sort-of StAX, but it will attempt to scan ahead to determine the size of an array or object, which sort of defeats the purpose. The best example I can think of comes from C#, the JsonReader in JSON.Net: http://www.newtonsoft.com/json/help/html/T_Newtonsoft_Json_JsonReader.htm `JsonReader.Read()` is the iterator step function, then you can query the current state of the reader using `reader.TokenType`: http://www.newtonsoft.com/json/help/html/T_Newtonsoft_Json_JsonToken.htm which is similar to the SAX interface in RapidJson: https://github.com/miloyip/rapidjson/blob/master/example/simplereader/simplereader.cpp (excluding the `{Start,End}Constructor` tokens, which have to do with the library supporting deserializing derived classes). As noted in the thread, the API difference is centered around about processing parse events by reading from a queue or pumping an iterator as opposed to using callbacks. I imagine a modern C++ StAX/iterator style API could provide an iterator that dereferences to some kind of variant which is one of those parser state types. Users could also switch on a state/token enum as in the JSON.Net Reader class. // enum switch style for (auto &amp;json_token : std::json::reader(input)) { switch (json_token.kind()) { case std::json::token_kind::begin_object: std::cout &lt;&lt; "Begin Object {\n"; break; case std::json::token_kind::end_object: std::cout &lt;&lt; "} End Object\n"; break; case std::json::token_kind::begin_array: std::cout &lt;&lt; "Begin Array {\n"; break; case std::json::token_kind::end_array: std::cout &lt;&lt; "} End Array\n"; break; case std::json::token_kind::number_literal: std::cout &lt;&lt; "Number: " &lt;&lt; json_token.get&lt;double&gt;() &lt;&lt; "\n"; std::cout &lt;&lt; "Alternate Number: " &lt;&lt; json_token.get&lt;double&gt;() &lt;&lt; "\n"; break; case std::json::token_kind::string_literal: std::cout &lt;&lt; "String: " &lt;&lt; json_token.get&lt;std::string_view&gt;() &lt;&lt; "\n"; break; case std::json::token_kind::bool_literal: std::cout &lt;&lt; "Bool: " &lt;&lt; json_token.get&lt;bool&gt;() &lt;&lt; "\n"; break; case std::json::token_kind::null_literal: // maybe lexeme info also? std::cout &lt;&lt; "Null: at " &lt;&lt; json_token.position() &lt;&lt; "\n"; break; } } // variant visitation style for (auto &amp;json_token : std::json::reader(input)) { std::visit(overloaded { [](begin_object_token &amp;json_token) { std::cout &lt;&lt; "Begin Object {\n";}, [](end_object_token &amp;json_token) { std::cout &lt;&lt; "} End Object\n";}, [](begin_array_token &amp;json_token) { std::cout &lt;&lt; "Begin Array {\n";}, [](end_array_token &amp;json_token) { std::cout &lt;&lt; "} End Array\n";}, [](number_literal_token &amp;json_token) { std::cout &lt;&lt; "Number: " &lt;&lt; json_token.get&lt;double&gt;() &lt;&lt; "\n"; }, [](string_literal_token &amp;json_token) { std::cout &lt;&lt; "String: " &lt;&lt; json_token.get&lt;std::string_view&gt;() &lt;&lt; "\n";}, [](bool_literal_token &amp;json_token) { std::cout &lt;&lt; "Bool: " &lt;&lt; json_token.get&lt;bool&gt;() &lt;&lt; "\n";}, // maybe lexeme info also? [](null_literal_token &amp;json_token) { std::cout &lt;&lt; "Null: at " &lt;&lt; json_token.position() &lt;&lt; "\n"; } }, json_token); } An important advantage would also be partially consuming the consuming the tokens, but also being able to pass the reader object off to another routine.
Captain obvious strikes again.
When I started `rpclib`, I named it `callme`. The reason was that I initially [planned to ship a "maybe" type](https://github.com/rpclib/rpclib/issues/20) and saw a great pun opportunity for creating the best example code in the world: ``` auto heres_my_number = []() -&gt; callme::maybe { }; ```
Meh, I doubt most projects have a business case for this level of "safety."
Now do it for cryptographic keys with virtualizing obfuscation.
Okay I'll try☺
Nice non-argument. The language-level problems that cause exploitable flaws in software written in C and/or C++ are shared between the languages. C++ adds a few of its own but all the significant ones already exist in C. For balance, C++ allows you to elide manual bookkeeping (and thus reduce errors in the manual bookkeeping) but over a hundred different types of undefined behavior remain and are still consistently found in the wild in programs written by experienced and disciplined C++ programmers. For the purposes of a discussion of safety it's easier and simpler to just discuss C and C++ as a single tightly-knit family instead of as meaningfully distinct programming languages.
Let me link you to what [happens](https://www.nytimes.com/2017/06/27/technology/ransomware-hackers.html) when you don't have this level of [safety](https://www.theverge.com/2017/5/12/15630354/nhs-hospitals-ransomware-hack-wannacry-bitcoin).
No?
Those were bugs in the OS
I agree with the author's premise. The "bad developers are a liability" argument has a point. Almost by definition, if there are developers who are bad, they will have higher defect rates than those who are competent. Otherwise, what would make them "bad"? But, putting "bad developers" aside &amp;ndash; I don't know anyone, including myself, who has not created at least one exploitable memory safety issue over a period of, say, 10 years of active work. This might have been acceptable 30 years ago, but now that memory safety issues result in [this](https://www.nytimes.com/2017/06/27/technology/ransomware-hackers.html) and [this](https://www.theverge.com/2017/5/12/15630354/nhs-hospitals-ransomware-hack-wannacry-bitcoin), it's not acceptable to have this problem in computing. The problem can be fixed, so we should.
I agree with you. I've even worked at a place where crashes were just another input to the process, meaning that they developed systems robust enough to withstand crashes to core functionality. They didn't need fuzzing. Multi billion dollar company that isn't a Googbook
...? Are you one of the incompetent programmers that the OP post speaks of? What do you *think* caused those bugs in the OS?
Oh boy, here we go again... 
They have removed `register`. Fixing an old codebase with `register` is easy -- delete the keyword. Maybe, I could be wrong, replace with `int`. `auto_ptr` was also removed, but only because it was seen mot just as obsolete but actively harmful. The process of replacing with `unique_ptr` is nearly mechanical and should reveal or remove bugs and result indentical binary output. Basically you highlight automatic move-from with `std::move`. It isn't perfectly safe, but what was there before was amazingly unsafe. Json seems unlikely to be that massively bad, so yes I would expect indefinite support. 
&gt; programming in C and C++ for over 25 years &gt; Mozilla &gt; I cannot consistently write safe C/C++ code &gt; I don't know anyone else who can. No shit! They have new/delete all over their code. https://github.com/mozilla/gecko/blob/central/netwerk/cache2/CacheFileChunk.cpp#L309 mRefCnt = 1; delete (this); // wut? https://github.com/mozilla/gecko/blob/central/netwerk/cache2/CacheFileChunk.cpp#L553 https://github.com/mozilla/gecko/blob/central/security/manager/ssl/nsNSSCertificate.cpp#L83 { nsNSSCertificate* newObject = nsNSSCertificate::Create(); if (newObject &amp;&amp; !newObject-&gt;InitFromDER(certDER, derLen)) { delete newObject; newObject = nullptr; } return newObject; } 
Without examples of what the author is talking about (ie, what specific code he wrote last week that turned out to be unsafe), it is kind of hard to say anything sensible about the post. "I cannot consistently write safe C/C++ code"... mmm, OK?
Is one exploitable memory safety issue per 10 years of active work good or bad? I don't know how close this ratio is to reality, but if it is, then mine could easily be completely safe because it is likely in a side branch of some feature that I managed to throw away - or in some small internal utility that I wrote to perform a minute task which nobody but me even could have run...
Nah. strspn expects a set of chars you're looking for, not the ones you are *not* looking for. It makes a difference when either the set you're looking for compared to the set you're not looking for have significantly different sizes... 
[Uhh](http://i0.kym-cdn.com/photos/images/newsfeed/000/992/407/776.jpg)
Oh yeah, and strspn doesn't "find" (for certain values of find) the *first* occurrence of the set you're looking for (it finds the last one)... So it's enough different to really not fit the bill...
[Oh no you di'nt](https://media.giphy.com/media/v98g8s9mNEDcI/giphy.gif) These are humans. The only thing that matters is how you deal with it. The cost of making the OS bug free on release would relegate it to academia for life.
&gt; In what universe is the undesired behavior of unsigned "unexpected" though? In the universe in which the programmers write code not expecting to hit the boundaries and then do. &gt; And the same is true of signed. Implicit integer conversions in C/C++ are a complete mess both for signed and unsigned types which is why I practically make a religion out of ensuring that operands always have the same types. It seems like that's a better recommendation than just avoiding unsigned types altogether. &gt; How do you figure? Because the normal mathematical behavior is common and more likely to be what's actually desired. You say that signed integers have properties which might not be desired in some particular case, just like unsigned integer have properties which might not be desired in some particular case. The point is that those cases for signed integers are less common than for unsigned, because signed integers are closer to normal arithmetic. &gt; I'd actually make the argument that it least in terms of context free values I'm more likely to think negative one is acceptable. Right, because negative one is _more likely to be acceptable_. That's the point. &gt; And the same is true of signed. Implicit integer conversions Your example was with "signed everywhere." I'm not arguing for mixing signed and unsigned. I've made two points: don't mix them, not even with explicit casts, so the only way to do that is minimize use of one or the other; And the one that should be minimized is unsigned, because all those reasons that 'argue equally against' both actually argue more against unsigned. E.g. you said yourself "I'd actually make the argument that it least in terms of context free values I'm more likely to think negative one is acceptable." &gt; And none of this cuts to the core of the issue which is that seeing something declared as an unsigned type communicates something about the intention of the programmer. So there are bad reasons to use unsigned. It's not a good thing that unsigned therefore communicates that the programmer had one of those bad reasons in mind when he chose to use unsigned. E.g. if they use unsigned are they communicating nothing other than that the valid range is the non-negatives? Or are they communicating that they've carefully considered every other possible type that might appear in an expression anywhere with the value and concluded that it never makes sense for any of those other types to ever be signed for any reason? Are they communicating that they want modular arithmetic? Or that they're working with a bundle of bits rather than an integer? If seeing an unsigned type reliably communicated one of these good reasons, then I'd take that as a point in favor of unsigned. Not enough to outweigh the other problems though. If you want to communicate integral ranges through types then use a ranged integral type.
Memory management used to be a lot more difficult than it currently is, mostly before shared_ptr came into common usage. C++ forces you to think about memory in a lot of different ways to get things right, especially if you care about performance. Garbage collection is often thought of as the silver bullet to solve all memory related problems, in my opinion this is just pushing the problem somewhere else. You still need to think about ownership related issues &amp; lifetimes. And instead of hunting down leaks &amp; cyclic dependencies you're instead trying to massage the GC to not screw you over. Personally I work in games, and currently for small to medium sized projects Unity3D is fairly popular, which uses C#. In my view we spend more effort on making sure the GC is behaving decently than I ever spend in C++ with memory related issues. In this field you can't have the GC unpredictably consuming precious miliseconds.
The NuGet packages will improve a bit, with a command script. And as time goes on, more of then will be available because NuGet.org doesn't let you delete anything. But if you're asking for a real package manager in Windows and a catalog of old compiler toolset versions then no, I don't see that coming soon. 
Argument from authority: The post. Instead of swanking about his own competence, I would have rather read about common sources of unsafety. I have gathered exactly nothing from reading this post; no essence, no valuable insight, nothing.
&gt; For the purposes of a discussion of safety it's easier and simpler to just discuss C and C++ as a single tightly-knit family instead of as meaningfully distinct programming languages. Sure it's easier but also completely wrong. Claiming C and C++ have the same safety properties means not understanding either C, C++ _or_ safety. Plain C is just as terrible as the original article suggested. It is plain impossible to create reliable programs with it because it does not allow for creation and enforcement of safe constructs. C++ as done in the "C with classes" style is marginally better than C but not by much (see code snippet posted above by Abyxus for an example). "Modern" C++ using constructs such as RAII, move-only objects, arrays that always know their sizes and all that good stuff is light years ahead of plain C in terms of reliability. It is not as safe as Rust and no-one is claiming it to be Rust but claiming it to be in the same ballpark as C is just plain misguided.
`delete this;` makes sense when you are implementing a shared pointer: https://github.com/gcc-mirror/gcc/blob/master/libstdc%2B%2B-v3/include/bits/shared_ptr_base.h#L138
Not package manager, just a convenient way to install only compiler/sdk/libraries without any of IDE stuff which is not needed at all on dedicated build servers. Self-contained msi package would just suffice. Basically [this](https://www.microsoft.com/en-us/download/details.aspx?id=44266) except for later vcpp versions.
So `strspn` returns one past the last occurrence of the char that is in the set, while `find_first_not_of` returns the first occurrence of the char that is _not_ in the set. Sounds similar enough.
Indeed, but `mRefCnt = 1;` before `delete this;` means that a destructor would check mRefCnt for some reason.
Most exploitable safety issues are caused by UB related to integer overflows, use-after-free, concurrent modification (iterator invalidation, insufficiently synchronized multithreading), complex ownership semantics and missing pointer validation (`unique_ptr` won't save you from `p-&gt;foo()` when `p == nullptr`). None of these have comprehensive solutions in C++, (and some don't have any solutions at all for that matter), except running your program under valgrind and/or UBSan with a debug-version standard library that's 100 times slower. Furthermore, the problem just keeps getting worse because the standard committee feels like they have a blank check to sprinkle more undefined behavior wherever they like. We recently had a bug that went undiscovered for a long time because [`std::uniform_int_distribution`](http://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution/uniform_int_distribution) has *undefined behavior* when constructed with an invalid range (b &lt; a) which only occurred when loading a certain data set defined by a designer and which presented reasonable (but subtly wrong) results in testing. I'm a huge advocate of using modern C++ instead of C or "C with classes" but even modern C++ doesn't come even close to making it possible to write large pieces of software without security flaws.
Please check your calendar. It's 2017. `unique_ptr` with move constructors was around since VC++2010/GCC 4.3. Different kinds of `scoped_ptr` were used since early 2000s. There is no excuse for not using them.
Intrusive ref count sanity check?
OMG. C/C++, what the hell is that? I only know C and C++ and they are completely different languages.
It's very easy to write safe C code but almost nobody does because it's a lot of extra work and forces you to avoid many core constructs of the language such as pointers, arrays, and direct memory management. C++ IMNSHO is too complex and fragmented. It offers safer mechanics (especially RAII) but all the bad stuff is till there. EDIT: by this I mean in high security environments, not in general. 
&gt; They're not a panacea. Actually, they are. &gt; Clearly there are reasons (or "excuses") for why companies choose not to use them. No. Manual memory management is only a remnant in legacy code. Moreover, you're making an "argument from authority" fallacy. Just because big companies write bad code doesn't mean that the bad code in question is good. &gt; If these better pointer types eliminate all vulnerabilities, including buffer overruns, which they don't, then I'm sure these companies would all be furiously rewriting their code. The reason they don't do this is because it comes at huge cost. Rewriting code bases of xxM LOC costs millons.
What you linked to - it's not the destructor. And that particular assertion runs before that `mRefCnt = 1; delete this;`.
Yeah, but usually mRefCnt is not exposed from an intrusive ref-counted base class.
Maybe sensible, but inconsistent with C++'s own mem-initializer-lists (to be fair, in a way that permits a future extension)
&gt; We recently had a bug that went undiscovered for a long time because `std::uniform_int_distribution` has undefined behavior when constructed with an invalid range (b &lt; a) What's your proposed alternative? IMHO, undefined behavior is the right choice here. Your Standard Library implementation should `assert` in debug mode - it's a quality of implementation issue. 
I don't really get it. Could you elaborate on how this `mRefCnt = 1;` is useful in any way?
No one uses that anymore, everyone moved on to C++/CLI. &lt;/joke&gt;
Have you considered the following: 1) Mozilla is a proponent of pushing the rust cool-aid, and wouldn't you know Mr. Ocallahan is a proponent of re-writing all things in rust google site:ocallahan.org rust 2) Dunning-Kruger hath been declared, therefore this 3 paragraph ~250 word blurb with nothing to back it up is the truth all-mighty. Bollocks. Meanwhile /u/Abyxus has pulled a few examples where only an inexperienced c++ developer, or a bad one, would write such code. If you have 25 years of experience and you pepper your code with news and deletes outside of smart pointers then you are a bad developer. That's the harsh reality.
&gt; because std::uniform_int_distribution has undefined behavior Wow, it seems like a pretty silly thing to do. I wonder what was the rationale (if any) to not provide 'Throws' section instead of 'Requires' for such a trivial precondition.
I always heard of that the back-compatible is a heavy burden for c++ committee to add new features I am wondering whether c++ committee has a plan B that to create a TRUE new c++. Because it must be much easier to rewrite sth in a new cpp than rewrite it in a new language like Rust for a c++ developer.
So we have NS_PRECONDITION(0 != mRefCnt, "dup release"); mRefCnt = 1; delete this; Just how does it work?
I started integrating the ideas into my library called [Yavin](https://github.com/SteveWolligandt/Yavin). The presented code is used in the [VBOHelpers](https://github.com/SteveWolligandt/Yavin/blob/master/Yavin/VBOHelpers.h) header. It is still in progress but works pretty well until now. There is an [example] (https://github.com/SteveWolligandt/Yavin/blob/master/test/window_test.cpp) showing how the library is used. I tried to use the naming conventions of the std library. Hope you like it :)
And I disagree with your IMNSHO and say that you are spreading unreasonable doubt about C++. This gets us nowhere and would be a pointless exchange. Let me try to explain why I think C is a bad idea in modern code. As an example I work with several large C code bases that have to run 24/7 because they run to keep stuff that matters from breaking which would cost money. These code bases are vetted constantly, every change reviewed, all code is run through rigorous testing both as units, then in several levels of integration tests until they end up in a device where they are then tested first by us and then in an often months long field test. And you know what the company conclusion on C is after all of this. It is not a good language, but we had to use it because that was all the compilers we could get. Software written in C forces you to: 1. "remember to do things": - Call that function to deallocate - Call that function to update that value - Call that function to do this and that. 2. If you modularize your code which you have to do to keep it from becoming spaghetti you end up passing around structures and pointers to structures, now you have to check for null. 3. And you have the array situation in C, get an array, but wait no length, that has to be a parameter. Uh better remember to check that. Oh wait you forgot. And we are lucky, we don't even have a dynamic memory situation, which is just a magnitude worse. So we have turned to C++ because we have compilers for that too on all platforms we use allowing us to get rid of all those weird cases were we have to remember stuff through operator overloading and RAII. We use references instead of pointers and suddenly don't have to worry about null pointers. We have actual strong type check which has also caught a lot of bugs. And we have templates which allows us to generalize so much code that in one case an application turned into a main function with an event loop using general and well tested templates. And we have an array that knows its size at compile time and can always be queried about this. And it has worked. We have metrics that show our bug rates have started to drop. You bring up that C++ is complex and fragmented. I don't see much fragmentation but yes it is complex. But like most places we have treated C++ like we did C, we have a set of rules for how you code which you have to pass during review. Yes that set of rules is bigger for C++ but since we have turned most of the stuff our C guys had to repeat and do again and again our application is often so terse that actual language or safety concerns are not the concern when we review rather it is if the application behavior is correct, which is actually the important bit. Could there be a better choice in language. Maybe in time, but right now all other options would be laughed out of the room as not having enough tools, compilers or a syntax that we can teach our developers. Until I sleep more soundly using C++ over C because our code is safer and thus better for it. 
People who refer to C and C++ as the almost the same language are bound to not know C++ very well. C++ is light years ahead of C when it comes to safety. I am sure the author knows this and this mini article was an attempt to generate strong impressions towards the languages. 
Throwing instead of defining undefined behavior is a terrible idea, as it defines a **wide contract** compared to a **narrow one**, limiting flexibility and imposing overhead on the user. If your precondition is undefined behavior, you can always implement a throwing wrapper over it, while the opposite is impossible. You should watch "Defensive Programming" by John Lakos. 
&gt; Who is more qualified to make security decisions: Certainly not the guy writing raw new/delete in his code, regardless of his employer and experience. &gt; buffer overrun is a classic example of something that isn't covered by what you claim is a panacea That's because `std::unique_ptr` is not the correct type to store an array (despite it offering a `T[]` specialization). At this point I'm convinced that you don't even use C++ let alone smart pointers. &gt; By definition, it is not a panacea if it does not remedy all difficulties. Smart pointers solve the problem of missing or multiple `free`s and other issues that come with raw pointers. Buffer overflows are an orthogonal problem with a different cause; you would neither expect smart pointers to solve the issue of data races or UB arising from sequencing, right? Smart pointers tackle one niche of problems and solve it without overhead, therefore they're a panacea. &gt; I don't agree that they make it easy to suddenly write safe C++. It's just easier to write safe**r** C++. Speaking of strawmen, here's one. Nobody would ever be as foolish as to claim that *any* system programming language is *completely* safe. My and Abyxus' point was that the author of the blog post went on a rant about C++ while his company patently fails to adhere to even the most fundamental of safety practices. That's the embodiment of hypocrisy.
&gt; None of these have comprehensive solutions in C++ Not true. Integer overflow can be caught by turning on the relevant exception mechanisms. Use after free and pointer validation can be solved by using c++ lambdas and the visitor pattern. Iterator invalidation can be solved by using cleverer and more expensive iterators. 100% machine checkable multithreading synchronization is an unsolvable problem, but there are work arounds like the no sharing approach. This is not a problem of C++, it exists in all languages. 
It should throw an exception. The overhead of the check is a branch that is correctly predicted virtually every time; in practice nothing. Furthermore you could amortize that already minimal overhead by constructing the `uniform_int_distribution` once and using it several times. If someone who doesn't understand superscalar CPUs and branch predictors starts screaming about performance, they could have a special constructor overload with a `struct i_am_stupid_and_want_UB{}` tag type that has the current behavior. But it should be opt-in, not opt-out.
&gt; so even if there are great smart pointers these days, they weren't available back then. sorry but no. Boost smart pointers have existed for much longer. `tr1/shared_ptr` was already useable in 2007.
Um, yes, this code is typical of old codebases. Yes, it's utter shite by todays standards of how it's done, just like a 1997 car is utter shite by today standards. The difference is that cars tear and wear, whereas code, much less so. Wanna rewrite/retest all of that? :-)
Show me one single mature C++ code base that does all of these. (EDIT: In production, not just while testing/debugging.) They don't exist, because all of those require extensive code structure changes or have other unacceptable overheads either for performance or programmer productivity. (You'll note that I did mention using UBSan and valgrind. EDIT: And debug-mode stdlib, which gets you those fancy iterators.)
&gt; &gt; &gt; Manual memory management isn't just for legacy code. If you are coding a kernel/device driver you have manage memory manually. There are also a couple of other low level reasons to manage some memory manually. people are able to write whole kernels in OCaml and Rust so I really doubt your assertion. 
&gt;&gt; They're not a panacea. &gt; Actually, they are. [Panacea, as per wikipedia.](https://en.wikipedia.org/wiki/Panacea) IMO, it's a huge stretch to call modern C++ a panacea in solving direct memory access and other problems that cause the lack of safety in C++.
This isn't unreasonable but incompatible with the current design rationale of the class, which is explicitly thought of as a zero overhead abstraction. Constructing many distributions is supposed to be *free*. In fact, your suggestion to reuse distribution objects, while reasonable, isn't how you're supposed to use them.
I never said that you should only used shared_ptr. unique_ptr existed too at this time with -std=c++0x.
True, but they don't use `unique_ptr`s in new code. They're just writing legacy code. Btw, rewriting is totally possible, Google regularly writes clang plugins to massively rewrite code in Chromium. (Such a pain for people who maintain forks of Chromium).
... yes, today. In 2007, -std=c++0x was already a compiler flag and c++11 was far from being released.
I *completely* disagree. * An exception's overhead might be more than the cost of the branch. There might extra work involved depending on how exceptions are implemented *(i.e. they're not really zero-cost on some systems, therefore unusable for performance-critical applications)* and they may increase the code size. * The Standard Library should strive to be exception-agnostic to be usable on as many architectures as possible. * Exceptions widen the contract of a function call and impose an unnecessary overhead *(however small it might be)* on the user. You can always create your own wrapper over `uniform_int_distribution` that throws an exception, but you would not be able to do the opposite. * Exceptions and similar error handling mechanisms are for **errors that you can recover from**. Precondition violation is not one of those things. Your Standard Library implementation might, by all means, decide to throw an exception on **assertion failure** during debug modes, but this should only be a debugging aid. Also your attitude is very childish. How about a `struct i_think_i_am_better_than_anyone_else_and_i_dont_understand_UB { };` tag instead?
&gt; That doesn't make sense. It might have been an experimental flag, as c++0x is the name for the standard before it was finalized. No one would use experimental c++ in their release code. so what? the current proposition is to instead use other languages that are not even standardized. there are ten times more change in two years of rust than in ten years of "experimental" C++ standard and yet it doesn't prevent people from using rust.
I took a different approach with my library. I avoid the variant structure all togethor and geneate a static serializer/deserializer for each class(e.g. like a to_string, from_string type function). So a class just needs to provide a mapping of json member name and type and the local member or functions to get/set struct A: public daw_json_link&lt;A&gt; { int a; string b; static void json_link_map( ) { link_integer( "a", a ); link_string( "b", b ); } }; A blah = from_json_string&lt;A&gt;( some_string_value ); There are helper macros to generate the easy stuff, but for complicated cases one just needs to provide callbacks that will take the underlying json type(e.g. string, integer/real, boolean, object, array... and whether it is nullable/optional or not)
Even if you construct it in a loop with the same arguments, you can be certain that the branch will be hoisted by the optimizer. This particular case of "zero overhead" is theoretical wankery with no practical performance effect in any realistic scenario. Furthermore, you are actually wrong about the intended use. &gt; Complexity &gt; Amortized constant number of invocations of `g.operator()`. In order to reach the `amortized constant` complexity guarantee, you *have* to re-use the same distribution object; there is no guarantee that the random generator produces the exact required number of random bits for every invocation of the distribution object.
&gt; Rust is stable https://killercup.github.io/bitrust/
Not a fan of CMake, for a lot of reasons. I think Premake is a lot better, but obviously doesn't have the momentum of CMake so will probably never overtake CMake for library devs.
&gt; This particular case of "zero overhead" is theoretical wankery with no practical performance effect in any realistic scenario. Please prove your claims. A simple "quick-bench" and "godbolt" shows that there is overhead for the check, even when the exception is never thrown. * http://quick-bench.com/jML0F7JQlE9AzeMuBfsdxGPCRDE * https://godbolt.org/g/1VbNso I know that this doesn't mean much, and that this overhead *could* be insignificant. But it's still there and the user cannot get rid of it. If this is used in a very hot loop, it could make a significant difference.
&gt;Throwing instead of defining undefined behavior is a terrible idea I hope you would not advocate for std::regex to behave in the same way. Also, how do you propose we define behavior of working on nonsensical data? If designers were unable to provide an interface which is difficult to misuse, they should admit that and provide some basic error checking. I understand that sometimes it may not be feasible to provide the check on runtime, but this here is not the case, especially since distribution object is supposed to be created once ahead of use. 
&gt; Wouldn't it be nice if C++ could do that? And then we're back to the original argument. One could have been using tr1 or boost smart pointers for the last decade without breakage, but does not. That's like using `unsafe` everywhere on rust code and then complaining that stuff breaks. &gt; As you can see here, only 16% of Rust users have seen their code broken by a soundness hole being closed. and C++ have had trigraphs only removed on C++14 because of [some confidential IBM customers who still don't have ASCII](https://isocpp.org/files/papers/N4210.pdf), so certainly less than 0.001% of the global users. And yet there were tons of debates on removing them. There will absolutely never be something in C++ that breaks for 16% of users, it's a massive number.
IMO there are some rules in C that catch people unawares and make it less “easy” to write in Officially Portable C, especially when it comes to basic arithmetic stuff that appears to work sensibly until suddenly it doesn’t. Signed overflow, division/remainder of/by negative, and bitwise operations on negative are all left floating for the sake of sign-magnitude and ones’-compliment machines, but those’re all perfectly reasonable and useful operations on a two’s-complement machine. Same problem with the off-by-one baseline minima (e.g., −32767 for `INT_MIN` instead of −32768), which make it hard to quickly and portably bounds-check some things.
I'm very skeptical that it could ever make a significant difference in a non-artificial scenario. First off, if the arguments don't change, then the check is hoisted out of the loop: http://quick-bench.com/IZCO0jHqJtpJeLfwyTc8K4fUzEo Secondly, if the arguments *do* change, you are either doing some predictably optimizable change to them (such as incrementing both by one) which still allows the optimizer to elide the check, or you are reading the data from memory, in which case with such a hot loop you are almost certainly bound by memory bandwidth. Never mind that even in this extremely artificial benchmark, there is only a &lt;4% difference.
Nope. http://eel.is/c++draft/macro.names#2
&gt; these chunks of code that were linked to were last touched 4+ years ago, which is just when C++11 support was getting to be mainstream among compilers. Smart pointers have been around for almost 20 years: http://www.boost.org/users/history/version_1_10_3.html &gt; My point exactly is that they don't solve all problems. [...] Tackling one set of problems does not make a panacea. It just solves a set of problems. I might have been mistaken about the exact definition of the term 'panacea', I concede that. &gt; They do have overhead, however small. [Nope.](https://godbolt.org/g/nfVohu) Replacing an owning raw pointer and new/delete with `std::make_unique` has 0 overhead. `std::shared_ptr` does have significant overhead but that's a semantically different construct altogether. &gt; It is easy to write safe Rust. I've had a suspicion that you're just trying to shill for rust from the very start... &gt; Even something as simple as an index going beyond the end of an array. You have no idea what you're talking about. [`std::array::at`](http://en.cppreference.com/w/cpp/container/array/at)
Hahahaha, good one
&gt; You can't use a standard language's automatic memory management in the kernel, because they rely on using the OS's usermode API under the hood. can't you ? you can use whatever memory management scheme you want with shared_ptr / unique_ptr (or even handle things that are not memory : gpu buffers, file descriptors, whatever). It's trivial to have smart pointers use kalloc / kfree (or fopen / fclose or whatever) instead of malloc / free. By the way here's an example of linux kernel module in C++ : https://github.com/korisk/fpw Since it reimplements new / delete in terms of kalloc / kfree, unique_ptr would "just work". To downvoters: could you explain ? I'm using automatic memory management on AVR micro controllers without any hint of operating system, so I fail to see what's the problem.
Which feature do you mean?
Mr. Madison at no point did you come close to an argument resembling rational thought. Everyone is now dumber for having listened and may God have mercy on your soul.
Name any language that's "safe" from shit like heartbleed or poodle? 
I'm not sure I've written many programs that can be compiled by two different languages.
&gt;Could there be a better choice in language. Maybe in time, but right now all other options would be laughed out of the room as not having enough tools, compilers or a syntax that we can teach our developers. Until I sleep more soundly using C++ over C because our code is safer and thus better for it. Ehhh, I wouldn't call C++ the best language for all situations. These days I only really see it used for performance (or interacting with 3rd party code)
You're totally right on both counts. The conceptual design goal of zero overhead abstractions in C++ simply doesn't account for “modern” CPU features such as branch prediction. Regarding the runtime, I didn't mean to suggest that the distribution object should be reconstructed inside a loop, for precisely the reason you mentioned; merely that constructing many *with different arguments* should be cheap: if it weren't, the classes would need member functions to reset their distribution parameters.
I agree. I have even met devs with credentials similar to his write appalling exploits, so I am suspicious of him invoking Dunning-Kruger. I also think its telling he made no concrete assertions and didn't link to any code. I do agree with him that we need to fuzz more.
I get that it is old code. And C++ is hard, no doubt about it. But coming at C++ so hard when I see code like this: NS_IMETHODIMP nsNSSCertificate::GetEmailAddresses(uint32_t* aLength, char16_t*** aAddresses) or a bit further on: NS_IMETHODIMP nsNSSCertificate::GetClassDescription(char** aClassDescription) { *aClassDescription = nullptr; return NS_OK; } I'm just glad the C++ code I work with from day to day looks nothing like this. Can I write perfect code? Certainly not. But my code will be less error-prone than it would be if I'd have to work with ancient constructs like above.
What's `concept bool` and what is the issue about? I haven't been keeping up with concepts.
&gt; 0.001% is made up it is, actually the number is certainly even smaller. Some research was done by people in the c++ commitee : Case study The uses of trigraph-like constructs in one large codebase were examined. We discovered: 923 instances of an escaped ? in a string literal to avoid trigraph replacement: string pattern() const { return "foo-????\?-of-?????"; } 4 instances of trigraphs being used deliberately in test code: two in the test suite for a compiler, the other two in a test suite for boost's preprocessor library. 0 instances of trigraphs being deliberately used in production code. Trigraphs continue to pose a burden on users of C++. They couldn't also find a single usage of trigraphs through Google Code Search when existed.
Since JSON is commonly parsed from untrusted sources, it seems like encountering malformed JSON should be a more or less expected result, and having the library either throw an exception or call `abort()` seems a bit annoying to use. Especially in light of the recent efforts to make the standard library more useful with exceptions disabled. I also can't see any way to influence maximum recursion depth etc. Related to this, I don't see why const-overloads for operator[] are provided, the precedence set by std::map seems simple enough - .at() for the throwing, const version and operator[] for non-throwing non-const.
&gt; Interesting how CMake is only 34%. the answers are not exclusive. I use CMake to produce Makefiles and .sln, so I use the three of them. And the next meta-build-system in the list is autotools, at 16%, so half than CMake.
I completely disagree with this. Exceptions aren't for catching coding errors. What are you possibly going to do to resolve the error when you catch an "invalid uniform_int_distribution arguments" exception? I can't see any reasonable response to that exception other than not catching it and letting the program crash. asserts are supposed to validate that you've followed the API contract, not exceptions. The assert crashes the program when you test it and the test doesn't get compiled into the release version. How is that not entirely reasonable?
I think arguing from experience _is_ an appeal to authority. When Sean Parent talks, he doesn't need to mention explicitly his years of experience. You listen to him and his experience and expertise ooze out of his words and code. You see his code and then go, "Well fuck, my code sucks now." Any time anyone says, "Well, I've been doing this for X years so I know better", it means they're largely incompetent and a poor leader. Your ideas have value because they're valuable. You don't get respect and leadership just because you kept your skull together while beating your head against a wall for X years.
You can wrap existing headers into modules which is far more likely than anyone actually changing existing headers mostly meant for C consumption. Many of the *different api depending on defines* are pretty legacy but even that could be solved by making each form of include a different module. For example `Win32_multibyte` vs `Win32_unicode` instead of option being defined in the IDE which in turn defines a macro.
Any chance to see std::span in C++20 ?
No intention to defend them, but there's value in consistency. A mix of practices spanning 20 years can be seen as a mess.
All input is to some extent untrusted, or should be. But there probably should be a way of picking up after a failure in parsing the input. A failure to correctly escape a string, for example? Real world encoded data is often messed up in various ways. I'm thinking of something like how iconv gives you a way of proceeding when there's a code conversion error? The library also probably needs to deal with unicode, so keeping an eye on [Text_view](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0244r2.html) is probably a good idea. 
At the moment the biggest offenders are typeinfo and initializer_list, but it seems more are to come.
Agreed on exceptions : can't use on embedded
It is not possible to write safe code in either C or C++. This is feature from the design of the original C which has yet to be addressed. It will not be remedied by smarter programmers. The arithmetic model used by these (and most other languages) is fatally flawed from the beginning. Certain operations will silently return arithmetically incorrect results and it is impossible to check for them without distorting one's program to the point that it is completely opaque and unreadable. This easy to verify by checking out the [CERT guidelines](https://www.securecoding.cert.org/confluence/display/c/SEI+CERT+C+Coding+Standard). Help is on the way however. Boost has recently accepted a library - Safe Numerics - which promises to permit one to compile his C/C++ program with little or no changes and be guaranteed that it will never produce incorrect results. Information on the library can be found in the [Boost Library Incubator](http://www.blincubator.com).
I have some questions/problems with the article Boost is not a monolithic library, it's split in many small parts. Being header only says nothing about the size or complexity of a library. " C++ is only portable before compilation" that may be true for the language itself and the relative small standard library, but as soon as you want to interact deeper with the target system that portability is gone. Examples are GUI or network programming. What makes your packet manager better than conan.io ? And how do you want to prevent the numerous (security) problems that npm (which seems to be your inspiration) has? For example the leftpad debacle 
not arguing to change the contract of uniform_int_distribution, just commenting on what I believe is a common error: &gt; errors that you can recover from. Precondition violation is not one of those things It often is. Those values passed to the constructor may have been derived from the contents of a hacked TCP packet or from data read from a hacked file (game save file, video stream, or even xml), or otherwise depend on the state of the execution environment. Recovery could be dropping the malformed packet and moving on to the next one, rolling back a transaction, or telling the user the file they just tried to load is corrupt. Not crashing and losing their unsaved work.
In previous proposals, to write a user-defined concept, you'd have to write `concept bool MyConceptName /* blah blah */`. I always thought that that was kind of silly, since concepts are predicates on types (does this type satisfy this concept?) and thus the bool should be implicit.
Seems like Mozilla's "security advisories for Firefox" would be a good place to check out https://www.mozilla.org/en-US/security/known-vulnerabilities/firefox/
My assumption, based on my experience, is that most professional developers don't partake in online communities or read magazines. They might ask a terrible question on SO in desperation, but that's about it. Is that what others experience, or is that just me being arrogant and cynical?
&gt;In 10 years it will be poorly maintained and irrelevant I personally think this is a good example: https://isocpp.org/files/papers/N3888.pdf It was a proposal to add Cairo as a 2d graphics library to C++. Personally I believe this is a terrible idea. The library exists today - why bring it into the language. It doesn't represent a common use case other than introductory to graphics (games and even modern guis will use ogl/vulkan or directx for performance while modern business apps will be html). Additionally this library is actually pretty huge in how many dependencies it needs. 
Hi snsmac, good questions. &gt; Boost is not a monolithic library, it's split in many small parts. Agreed, we ported the libraries individually so that people can cherry-pick only the parts of Boost that they want to use. However, the way that people tend to use Boost is very much all-or-nothing, particularly on Windows. http://buckaroo.pm/search?q=boost &gt; ... interact deeper with the target system that portability is gone. Portability is definitely possible, provided the library designer takes care to abstract over platform differences. A good example of such an abstraction is SFML (https://github.com/SFML/SFML). This is what VM developers are doing to support multiple platforms. However, you cannot guarantee portability when distributing pre-compiled libraries. For example, on Debian the change from GCC 4.9 to 5.0 lead to std::string ABI breakages. &gt; ... For example the leftpad debacle Buckaroo is more decentralized than NPM and Conan. The perferred way for users to create a package is directly on GitHub, which they control. We prevent unrequested package upgrades by fixing the dependency resolution using a lock-file. 
Well, 6 years ago I would say this would be very difficult. C++11 added a lot of really nice features: shared_ptr, unique_ptr, make_shared, range-based-for, auto, more containers. These as well as safer printing and formatting functions (ex: snprintf/_snprintf_s) have made it possible to largely program without the need for manual memory allocation, out-of-range errors, pointer arithmetic, etc.... Better warnings from all the compilers + clang-based static analysis tools (and other freely available ones such as CppCheck) help keep guard and tell us when we screw up. I'm not going to say things are perfect and that it is possible to be completely safe, but it is far, far easier to write safe code today than it was 6 years ago.
Thank you for writing this up. I'm in the process of applying myself, and it was nice to read about what your first meeting was like. Looking forward to being able to help out!
&gt; You can't use a standard language's automatic memory management in the kernel, because they rely on using the OS's usermode API under the hood. No, they rely on having an implementation available to link to. That implementation can be in the kernel just as easily as userspace. Of course at some level you need to write that implementation, but that doesn't mean you can't use it for the entire rest of the kernel and all its drivers.
You do not have to manage memory manually in a kernel or driver. Smart pointers can be built just as easily on top of kernel-level memory allocators as userspace ones.
I think the standard could side step this by simply requiring its own ABI. The current ABI is actually the operating systems ABI and has nothing to do with C++ since C++ doesn't mention linking. The current compiler maintainers have a lot on their hands which is probably why it won't happen but I don't really see a technical reason why we couldn't have an official C++ symbol table and defined linking model and then just have the compiler work within the OS to do whatever they need to support that on top of their existing model. For example: on windows you have your native exports - you could have a new one that simply is called `cpp_symbols` that then exposes a standardized structure. There are tons of other issues I'm glossing over such as mixed abis on Linux but the basic idea is that we need a language defined ABI instead of forcing the OS onto the language. 
Because http://docs.conan.io/en/latest/howtos/define_abi_compatibility.html is so error prone. If a package owner doesn't properly know about ABI differences, then you'll end up with incompatible binaries. Good luck with that! If you end up rebuilding everything yourself, every time (possibly with some caching), you'll end up with a correct build without thinking about it. And correctness is above all the most important quality of a build system.
I stand by my point. Precondition violation is not something you should be able to recover from. All the scenarios you mentioned are cases where you need to sanitize user input making sure that it won't break preconditions when passing it to another function. You're not recovering from the precondition itself, you're preventing the precondition from being broken in the first place. Example: // Precondition: `p.valid() == true`. void handle_contents_unsafe(const packet&amp; p); // Precondition: none. bool try_handle_contents(const packet&amp; p) { if(!p.valid()) return false; // or throw handle_contents_unsafe(p); return true; } You provide `handle_contents_unsafe`, which has `p.valid()` as a precondition. This is a potentially unsafe function that results in UB if the precondition is not met, but doesn't impose any overhead or error handling mechanism on the user. If you are sure that `p` is valid *(maybe you already checked it in your code path)*, you can safely call `handle_contents_unsafe`. If you just got the packet from the wire, then you use `try_handle_contents`, which does the check for you. Everything must obviously be documented properly. I also recommend watching John Lakos's "Defensive Programming" talk.
Nice writing and interesting. But a question, why does ISO forbid recording?
Why not start with the elephant in the room: 0. Buckaroo is written in Java and is tied to a build system that requires *both* Java and Python. What is so wrong with C++ that we have to write our tooling in Java? BTW, if you are interested in the package/project dependency management, I highly recommend [So you want to write a package manager](https://medium.com/@sdboyer/so-you-want-to-write-a-package-manager-4ae9c17d9527) by Sam Boyer. I wanted to have a talk/discussion on this in the context of C++ at CppCon but unfortunately it wasn't accepted (*Building C++ Modules* was so see you all at CppCon).
Here's an example of unsafe code that isn't super obvious: #include &lt;stdio.h&gt; void main(int argc, char** argv) { if (argc != 3 || !atoi(argv[2])) return 1; return abs(atoi(argv[1])) / atoi(argv[2]); } Integer overflow exception vulnerability here. Or: #include &lt;stdio.h&gt; void main(int argc, char** argv) { if (argc &gt; 1) printf(argv[1]); } This one has arbitrary memory read and writes, you can hijack the entire process by passing in the proper inputs. 
I don't think anyone would claim that Mozilla is bad or incompetent because they still have `new` in their code, what with a lot of it being legacy. I think the claim would be (from the perspective of many on this sub-reddit), that you cannot argue that modern C++ is not sufficiently safe, when you haven't actually tried it.
&gt; Maybe it would be useful to have an online coding exercise where you are given some apparently-simple task, you write a C/C++ program to solve it, and then your solution is rigorously fuzzed for exploitable bugs. See [PexForFun](http://www.pexforfun.com/).
Agree they are not a panacea. That said, I think you are attributing way too much rationality to people/companies. I've had quite a few developers, who are objectively not stupid people or terrible developers, dislike using smart pointers ("I like new/delete because they are "explicit" "). Also, whatever the real barrier is to using code outside `std`, In practice I've seen it be a major psychological barrier. Many companies in 2008 would have been better off trying to use scoped_ptr and shared_ptr in many places, but chose not to, because enough people disagreed, and in the absence of consensus inertia tends to triumph. I think that with good devs on a modern codebase, memory issues should be quite rare. Not non-existent, but rare. That's definitely my experience, but a lot of people seem to think this is absurd, and inevitably end up saying things like "chrome still has memory issues; are you smarter than Google?", etc.
&gt; Just looking at Conan, having package definitions in procedural ( python ) rather than declarative form is probably not a good call. Agree, but given the hairball of build systems they have to deal with, I doubt they had any choice. In `build2` we have one uniformly sane build system to so we can be declarative. &gt; I read your FAQ, and it would be real nice if you re-consider your Github hosting point - just for the pull requests. Let me see if I can setup a mirror of some sort.
Well in my case it is one of only two viable choices. I would be interested in other options, but I have 2 different instruction sets and 3 levels of MCU and CPU types to support with code that should work the same on all of these. My options for languages are ASM for each platform, C or C++. We've picked C++ for all low-level stuff for PC it is a 80/20 split between mostly C# and C++ with some Python coming in now too. Performance aside from compiling to native is not really a concern except for in specific modules and there well that is done on a case by case basis sometimes we algorithm optimize sometimes we have to go for assembly (although due to our wish to have generic reusable code we try to avoid it except when we have hard real-time stuff with very small reaction times (milliseconds) and the like).
Yup. RAII is super cool. It helps avoid leaks in the presence of exceptions just like all the nice containers. But things like containers and smart pointers don't really solve use-after-free issues if you mix them with non-owning raw pointers, references and iterators. Ever ran into an iterator invalidation issue? Even innocent-looking, "modern C++" code can involve holding on to a reference that becomes invalid sooner than expected. And you don't really want to abandon iterators, references and raw pointers and replace them with weak_ptr or checked iterators (incurring a runtime overhead). Raw owning pointers are icky. But raw non-owning pointers *are* acceptable in "modern C++". And you *can* screw up lifetimes dealing with those. Addendum: Basically, C++ supports "ownership" via destructors and other customizable special member functions. But "borrowing" is still unsafe.
Not only that, but [it's not even valid javascript](http://timelessrepo.com/json-isnt-a-javascript-subset). Vaguely similar to "C/C++", that is, they don't have a sub/super-set relationship.
Unfortunately, working in finance, all of the code I work on is proprietary. What I can vouch for is this: I worked for a few years on a C++ codebase that served as a module in a larger codebase. It was somewhere in the 100K-1M LOC range. It was started pre 11. It used scoped_ptr, shared_ptr, and intrusive_ptr, extensively. It was able to use them in such a way that their overhead was never an issue. Because of the limitations of not having unique_ptr, there were specific situations where raw pointers were used also. We didn't have any memory issues in this codebase for the entire time I worked on it, despite the fact that we were not even running asan, msan, or valgrind as part of our nightly build. One day when I did run valgrind, it came out 100% clean through out entire unit test suite (&gt; 90% coverage). None of this was magic or required genius programmers. It was just following best practices of the time, avoiding both premature and post-mature (not a word but who cares) optimization, and that was it. Post C++11, with `unique_ptr`, this is way, way easier. You don't even have to do some of the (relatively minor) gymnastics as before, where performance is an issue, since `unique_ptr` doesn't have overhead. You also can use far less `shared_ptr`, which I tend to avoid in any case because shared ownership is so much more complicated to reason about. Sorry I didn't really answer your question. A decent way to discover such a codebase might be to see if some companies that have large open source codebases have given CppCon talks, that's usually a good correlate for following best practices.
"sanitizing" input is not always meaningful. If your input is a DSL (e.g. you're a browser processing some Javascript), sanitizing it is indistinguishable from processing. (don't get me wrong, It's great when it is possible. and I have no problem with unsafe overloads. I am just objecting to the idea that precondition violations cannot be recoverable in principle) on a side note, if there is a design problem with uniform_int_distribution is that it takes two arguments of type int rather than one integer interval.
&gt; Any time anyone says, "Well, I've been doing this for X years so I know better" Sometimes the 'years of experience' just mean that his code looks like from years ago...
Oops I forgot to add "embedded software" to my list :P My work is higher level than yours.
!autoremove
&gt; just something you do with your own code or maybe with non-platform-specific libraries I'll take it.
I figured ;) Most software work is higher level than what I work on and there C++ these days makes most sense if you care about performance.
Concerning the first sample, anyone who doesn't check for overflows has extreme confidence in their control of the input. Concerning the second sample, the same applies, and Clang always throws a warning over missing format specifier as I recall. No one is perfect, that's why I use the analyzer in Clang to help me catch stuff like this, in addition to habitual check for the validity of inputs.
Is this not the correct place to be asking this question? It would be nicer if you asked me to delete it because I happen to have that option :)
Honest question what "compile-time verifications &amp; computations" does c++ have that java and c# don't? (I realize that constexpr brings in a lot of power, but I have yet to find a usage for that in my own code)
Ah, I see, so you mean it for people who will try to write Haskell 'in any language'. Gotcha, went over my head. 
Tell me about it. I'm working with 20 year-old C++ flavored C code. I want to rewrite in modern C++, but my manager made the same point you made.
Yea, tooling helps, but even in the second example you could add parameters and still be vulnerable. If you pass user input as a format string parameter, you're always vulnerable (assuming you didn't sanitize the input first) C and C++ just have lots of ways you can make subtle mistakes in code that would never show up until an attacker finds and starts exploiting them specifically. 
I fail to see the vulnerability in your first example. You can trigger an integer overflow, but that does nothing other than cause it to give an incorrect return value. No "exception", no vulnerability?
Signed integer overflow is undefined and an exception will be raised. Unsigned integer overflow is defined and ok :) Also even if these were unsigned, depending on how the data is used, it could be a precursor to the rest of the exploit. Perhaps it was to calculate the size of a buffer to allocate, it could allocate a 0 or small sized buffer which you leverage later for a buffer overflow.
Run it with `INT_MIN -1` as input, it will most likely crash with with a division by zero exception.
Is there an option to use system library instead of one of dependencies? It would be nice before creating debian package from scratch.
On the site on each package page there should be list of available versions.
&gt; Signed integer overflow is undefined and an exception will be raised. That's not the security issue. This is a "Basic Integer Overflows" article: http://phrack.org/issues/60/10.html 
C doesn't have a division by 0 exception
Why don't you write a TS and present it to the committee?
What about take already developed e.g. program_options from boost to standard? 
&gt; (I realize that constexpr brings in a lot of power, but I have yet to find a usage for that in my own code) ensuring fixed sizes for arrays at compile time (can't do in java, there is no `template&lt;int N&gt; class array { }`), single ownership enforcement (`unique_ptr`), `static_assert` on generic code... a nice library for instance is [boost.units](http://www.boost.org/doc/libs/1_64_0/doc/html/boost_units.html) : it does compile-time units arithmetic, e.g. `meters x = 5; seconds y = 3; decltype(x / y) == meters_per_second`.
The exception is not from C; it's from the CPU. Integer overflow is undefined behavior in C. In particular, there are two sources of UB here: the first one is `abs(argv[1])`, wherein the input argument will fall outside the [acceptable input range](http://en.cppreference.com/w/cpp/numeric/math/abs). Many implementations will, nevertheless, simply return `INT_MIN` anyway. The second is during `INT_MIN / -1`, which will once again fall outside the acceptable range of `int` at the output, and this one will actually trigger a CPU exception on x86.
There's also tclap. If you want something lighter weight. Though personally for me, whatever the difficulties are in setting up boost, I'll always jump through those hoops. There's always something I'll end up using, in any non-trivial size project. BOOST_PP, BOOST_FUSION, inter-process, and many others.
He's saying 'Security in C++ is still unfixed. Even smart people with a lot of experience make holes, and that's not good enough. Maybe we should look at Rust or other languages, maybe we should have a standards committee for safety.". Disclaimer: I'm not him. And, we do have a standards committee for safety. 
Did you actually read the paper I linked? In its entirety? Here's one of the FAQ entries that more closely addresses your question: &gt; The authors have already received encouragement from educational institutions about the teaching potential for this library. As mentioned earlier, writing code which produces graphical output is a traditional route to learning the art of programming. By providing a standard 2D API this approach and its benefits become available to the student immediately and consistently between institutions I agree with you in general. I'd _much_ rather we just have a standard package tool that works across vendors and institutions and the teachers can spend the 5 seconds it takes to teach students to run `cxx-std-pkg-thingy require https://github.com/isocpp-extra/graphics` (or their IDE's equivalent) and make it easy for classes beyond CS 100 to use the same workflow (maybe requiring `https://github.com/cs200/graphics-framework` or whatever) and make _everyone's_ life better. Remember primarily that this is all a volunteer effort. The folks working on graphics care about that specific problem. They don't necessarily care much about general packages. Nobody on this earth can force them to work on packages instead of graphics. If you care more about packages, _write a paper_ and make your argument formally to the committee and not on Reddit. The committee does not review Reddit comments when voting on the standard. :) All I strongly care about in this case is that if they're going to work on graphics, they do it right, and they seem to be doing that these days, so I'm content. :)
While conan has room to grow, I think this misses the biggest "innovation" of conan, integrating with third party build systems. This lowers the cost to make dependencies "buckaroo compatible" and lowers the cost to migrate an existing code base to natively use buckaroo.
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. a C++ question or homework related. Help posts are off-topic for r/cpp; this subreddit is for news and discussion of the C++ language only. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/6nuhta/how_to_make_an_aimbotautoplay/dkcig2h/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
From that thread: Python's: https://github.com/imageworks/pystring/blob/master/pystring.h I'm glad I learned about that. 
&gt; It should be easy to publish new packages. Yes it should! &gt; Publishing a Buckaroo package is trivial: &gt; 1. Write a Buck build file to your project Learning new obscure build system that noone uses is not trivial. Writing build scripts for big and complex projects in such newly learned system is even less so. This is the reason why this is going to fail. Too bad, c++ could use tool like this.
 std::basic_string&lt;T&gt;::split
oh, I totally agree; taken liberally, writing 'safe' c code means utilizing libraries for basically everything. This isn't commonly done though outside of very mature projects as a very large proportion of C code was written before this became standard practice.
&gt; You bring up that C++ is complex and fragmented. Re: fragmentation, C++ has certainly undergone a tremendous amount of drift, from "C with classes" to iostreams to STL. Functors to lamda etc. Complexity is objectively measurable. In particular the grammar is incredibly complex.
Thank you, that's what I figured. 
A few things: 1) Trim+Split: I'd like to see an exact replica of C#'s Trim and Split functions (with the same parameter options; trim chars other than spaces, remove empty split groups, etc.) utilizing string_view as much as possible (including having the operations directly on string_view as well) 2) StartsWith/EndsWith/ToUpperInvariant 3) A single, non-templated, UTF-8 (or UTF-16) encoded string class to remove the need for basic_string&lt;T&gt; and manual encoding/decoding/length calls altogether. 
`trim` should be usable as `trim_left` and `trim_right`, too.
`to_lowercase` and `to_uppercase` that work with unicode would be nice
There's also this - https://llvm.org/docs/CommandLine.html
The problem is, without ranges, it is not clear what the signature should be. I guess you could have it return a `vector&lt;string&gt;` but then half of people probably wouldn't use it because it is too slow. You could have it return an InputIterator, but there doesn't seem (?) to be much precedent in the STL for that. Edit: also, to be clear, this absolutely should not be a member function. You can accomplish all this using the already existing public interface of `string`, and so of course you should in a free function: http://www.gotw.ca/gotw/084.htm.
Good enough for most would be nice though. Standard library does not need to meet all needs of everyone.
Sure. There needs to be a paper that advocates moving `program_options` into the standard first, though. Roughly speaking, all the committee does is vote on accepting proposals for changes into the standard document. Someone still needs to actually write a proposal on which they can vote.
My understanding is that it's to both protect industry participants who might be sharing proprietary technology for discussion as well as to protect individuals who might be voicing unpopular opinions. Essentially, it's to foster communication by industry representatives who might opt out of contributing to the ISO process if they had to operate more openly. In other words, ISO focuses on garnering industry experience rather than open discussion.
How would you handle chars that do not have an lower/uppercase ? For example _ , ß, € ? You also get localisation problems. For example the carcater 'i' can be 'I' or 'İ' in turkish
and if possible a way to do case insensitive string compare *without* having to supply a std::locale object. The way it is now goes against the simple things should be simple to do philosophy.
That is probably 'A Bad Idea'. Id rather the standard did not create a false impression that it can handle Unicode outside of just storing stuff.
Even just with ranges, there's problems. Consider a line like so: auto results = temp_string().split(' '); With ranges, `results` would likely just have a dangling reference unless it takes ownership of the result. There's going to be an allocation somewhere no matter what one does if the interface is to be safe. It would be possible perhaps to override `split` on whether the object is an rvalue, but that would make the interface complicated and fragile IMO. C++ _really_ needs lifetime extension (a la [P0066](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0066r0.pdf)) in my opinion. Ranges are going to make life great in the simple case but lead to so much confusion, so many crashes, so many vulnerabilities until that's sorted out, IMO.
Depending on LLVM just for the Command Line doesn't look attractive for an arbitrary noncompiler-related C++ project. FWIW I think it's really good and is even better than anything I used before (boost::program_options and tclap). Doesn't support subcommands, though, but that's due to the nature of LLVM project and how tools are shaped, I guess.
ẞ (capital Eszett) exists. But for characters that don't have one, ignoring is the way to go since it does not apply to them (for the Turkish letter, that's another problem).
If i would get python in this discussion i always really like using https://docs.python.org/3/library/argparse.html Something similar for C++ would be very nice
Yes, this issue just came up elsewhere on reddit in the context of `std::max`, I almost mentioned this issue pre-emptively but then I didn't. I think the simple solution is just to specify that `split` is only valid when called on lvalues, and require calling it on rvalues to be a compilation error, and be done with it. Can be as easy as: output_type split(const string&amp;) {...} output_type split(const string&amp;&amp;) = delete; Overloading it based on lvalue vs rvalue I agree seems like a bad idea, in this particular case. It is fine in other situations where the output type differs only in its const and ref qualifications. Anything more than that I think is "too clever". This may be a bit less ergonomic with regards to chaining or what not, but having to assign to an intermediate variable is not the end of the world. Also, it absolutely should not be a member function, should probably add a note to that effect.
&gt;We find it seems inconsistent and difficult to teach that vector prefers list initialization while such similar code for tuple prefers copy initialization. Oh dear god no. Don't make the rules even more complex :/ I find initializer list story pretty sad, but lets not make it worse.
Yup, ran into the same problem as well with string_views. I chose to exclude incoming r-value ref's (not sure I got the syntax fully correct, but it works for my project, yay c++!) for my split and therefore force the caller to 'root' the object in place while being split over.
Yeah, it's a bummer that there's nothing comparable to Python's `argparse` or Rust's `clap-rs`. I enjoyed creating CLI applications in those languages because of these libraries a lot. Would love to have something similar in C++.
And because that's true I'd expect it to be difficult to get something nice in the standard. I'd like to see something that's header-only get added to Boost, though.
They are also pretty great for readability.
There is a trivial way to get rid of it, which is to not construct the object inside your hottest loop. A single branch is a very, very small price to pay to avoid yet another potential UB, and I believe C++ should make a strong attempt to remove all of these trivially avoidable UBs from the language. I was stung badly by isalpha() being UB if you pass it a high-ASCII value. How could you possibly get this wrong (as a language designer, I mean)!? There are 256 possible inputs and half of them _crash your program_! Worse, since these functions could quite easily end up in some kind of validation path, it is trivially triggered by specially crafted user input. Or, you know, just accidentally. So you end up with the completely ridiculous notion that, before you are allowed to call a character classification function, you must pre-classify the character yourself to ensure you are not accidentally passing 'wrong' inputs. And that check is pretty much mandatory, but even so, getting it into the standard is beyond any kind of discussion (I tried). I'm sure someone will quote the standard to me on this one and claim I'm a lousy programmer for not simply reading the fine documentation, but I really don't see why this particular bear trap has to be there in the first place. 
&gt; Exceptions and similar error handling mechanisms are for errors that you can recover from. Precondition violation is not one of those things. So what's this, then? http://en.cppreference.com/w/cpp/error/logic_error
I don't think extra methods are that useful as long as std::string only understands ASCII. I think a std::utf8_string would be much more usefull, and then the extra functions would make a lot of sense.
This one might be small enough to not even need a TS.
Because the standard doesn't say "do like library XXX does". Somebody needs to take the time to fully specify the interface of program_options in sufficient detail that standard library maintainers can independently implement it (if you want such a thing in the standard). 
Subcommand are supported since last year (http://llvm.org/docs/doxygen/classllvm_1_1cl_1_1SubCommand.html)
I believe that interface is already documented properly by developers of program_options ..
What level of case-insensitivity do you want? Do you want accented characters to be considered equal? You probably want I and i be considered the same. What about İ and i? Or I and İ? (After all, if I == i and i == İ, then by transitivity I == İ.) But then what about A and Ȧ? What I'm trying to say is that a case insensitive string comparison is _not_ a simple thing to do without a locale.
What should `std2::string` have? Proper Unicode support. For example, iteration by code point or by grapheme cluster, normalisation, case conversion, conversion between UTF-8, -16 and -32 representations without having to use the godawful `codecvt` API, etc, etc Split, trim etc would certainly be useful, but it would be better to put them in the Ranges TS rather than make them `string`-specific. 
&gt; No shit! They have new/delete all over their code. Mixed with realloc() too!
Since people are recommending libraries, here's a pretty simple, 1-file header only lib I've used https://github.com/Taywee/args/ 
Yep. If it ain't broke.... On the other hand, if it is broken, then you may have an argument that rewriting will provide benefits. Then the question becomes will the benefits outweigh the costs.
If you want easy you can just use Boost, it has a case insensitive compare. I've been doing C++ since before '95, so yeah, I'll just acquire the std::locale object I want, etc. . But I don't want to have to explain to a C++ newbie how to to caseless compare in a standard-conforming way! 
Ah, you're right, they are, I forgot that. I remember I had issues with them, but now that you reminded me I recall that even though they are supported, they were not robust enough for my case (I did clang-refactor prototype with subcommands and that was quite a pain).
So, do you suggest the C++ standard to say "lookup the specs in Boost documentation"? FWIW documentation and specification is not the same.
This, so many package managers try to create their own build systems instead of using the current libraries' build scripts. Of course, on the other side, very few build systems are package manager friendly.
I don't know "a lot", but a few gamedevs at least do. In non-performance sensitive areas (i.e. most), it helps a lot with the subpar string handling experience that C++ offers.
The developers of `program_options` document how end users use the thing, not necessarily signatures / etc. of everything.
We did wording review on it in LWG. I think it will likely be ready.
I'd call it pretty obvious. In fact, I'd almost go so far as to say that *any* use of `atoi` should be viewed as a problem. For a rarity, this one's so obvious that even many coding standards get it right and recommend against using `atoi`, `atof`, etc. `strtol` has been around nearly forever (they weren't new when they were included in the C standard in 1989). There's really no excuse for using `atoi` and its ilk in C++, except, perhaps, in throwaway code where you're making no attempt at validating input at all. At least in my experience, the big problem isn't that it's impossible to write safe code in C++. The big problem is from "production" code that doesn't even make an attempt. In the end, it's true that 100% safety is really hard--but avoiding the problems shown here isn't hard at all. I'd be rather surprised at a beginner committing the second as well--most programmers are *fairly* advanced before they kind of realize that a `printf` format is just a string like any other, so they don't actually have to use a string literal for it.
How is std::array::at an answer to what you said? It looks to me that it doesn't stop an index from increasing beyond the end of an array... just throws an exception but still.
..and other times it means he doesn't really have 25 years of experience at all. Many have more like six months of experience, just repeated 50 times over.
Anything requested here is automatically in consideration for Ranges. 
My language accepts all input, and outputs 42. But given the subsystems it runs on, I still can't claim it to be safe. It does, however, by specification (not necessarily implementation), have no undefined behavior. 
&gt;But, it seems like an odd choice to use against a guy who has probably been programming for longer than you have. So, what? Number of years is not a reliable metric. It gives you a rough estimate about the skills / experience of the coder, but it can't be used as an only metric to compare two coder.
You also have to install Java and Python (2 or 3?).
There are only two hard problems in Computer Science: cache invalidation, naming things, and off-by-one errors. 
"and off-by-one errors" is out of context in the post as well as it's not part of the original quote.
There is nothing peculiar to Windows when it comes to using a particular library from boost. The build script lets you build whichever library you want. It's not at all monolithic. 
This is very cool. Though my system has cross language dependencies (meta / codegen) and while this is awesome it would be nice to integrate with other modern build system like buck,bazel, pants. I currently use Ansible to untar, make, install, etc all third party code but that's less ideal. It would be nice to simply add a git_repo via bazel/buck and have it do the right thing. I guess this is just monorepo problems. 
+1 for tclap. Surprisingly modern, extensible, easy to use, and easy to integrate
I tell people the same thing; just assume you're going to need it for every project. There's just too much useful stuff in there and once a project uses even one small bit from it, the initial integration cost is already paid for. 
We have a service for this already, called wg21.link
atoi() was used just to get the point across, it's not what I was trying to demonstrate. I was just saying dividing two integers can be a problem (INT_MIN / -1). I probably should have just hard coded the values. Basically, you need to be validating your integers before dividing them, or guaranteeing that they wont become invalid if they aren't part of user input. I'm not sure what you mean by the second point. There are no string literals in C? Just don't pass user input into the format string parameter was supposed to be the point. 
Access to character properties such as the Unicode general category, case, and bidirectional status of characters. https://en.wikipedia.org/wiki/Unicode_character_property 
Maybe this is all it takes, but I find it hard to accept that the only reason something like this does not exist in the standard is because nobody has bothered to formally propose it. It must have been discussed at some point.
You are asking a pretty loaded question. The short answer is that it relies on Buck which already requires Java and Python. If you're using Buckaroo, then you're using Buck. And if you're using Buck then you already don't care that your tooling isn't written in C++. From the looks of it, this project is really a package manager for Buck rather than one for C++.
I think you will run into many different approaches for handling argument parsing. Conventions in Unix/Linux differ significantly from Windows. How do you handle these differing convensions in one API. Do you change how you pass the arguments in based on the OS or do you force one convention onto another platform where the users may not expect it? Some things to consider for each OS * What do you use to initiate a non-literal argument (Linux only uses -, Windows uses / and sometimes allows -) * Can you specify multiple things in one argument (Linux lets you do -ab, Windows won't let you) * How do you handle paths (I don't actually know the conventions here, /opt=123.txt or /opt 123.txt or /opt:123.txt)
Honestly, at this point I am not going to stop using my personal library of string functions that I've accrued over the past half dozen years, as they are never going to add enough functions to make it obsolete, so I kinda don't really care.
Meh, asking for more `std::string` functions when the vast majority of the existing functions are broken or dangerous with utf8 is missing the point IMO. Any codebase that does internationalization needs to be really careful using string as anything more than a raw bytes buffer, usually doing actual string operations with a real utf8 library... not fun and IMO it's this way just for legacy. A few random ideas about my ideal `string`, which is more about removing functions from `std::string` than adding: - no copy operator: copying strings gets out of hand way too quickly. In my experience, the copy operator on `vector` and `string` is called by mistake 99% of the time. The preferred way to make a copy should be calling `copy()` - better integration with a (new) `string_view` type. `string2` should be a strict superset of its `string_view2` (eg. including `copy`) above. No methods that take `string` directly thanks. - use iterators and ranges for `find` methods instead of `size_t` indices. It's just gross that `size_t` as indices is still there after all the rest of the STL moved to iterators. This is very important on a UTF8 string because accessing an iterator returned by `find()` is constant time, but `[](size_t)` is not. - no [] and random addressing. You aren't going to need it, after the change above, and it would be O(n). - ability to explicitly return the internal container as bytes, when you actually want to work with bytes. Ideally this method would return just a old-style `std::string_view` - `size()` returns the number of UTF codepoints rather than the byte size. And maybe a bunch of utilities that every language has like `starts_with`, `ends_with`, etc... Basically, I would really like if people realized how incredibly broken-by-default and behind the times `std::string` is, and if it was entirely deprecated. It actually kinda blows my mind that everyone seems to just use `std::string` today without major complaints.
Why not returning a `vector&lt;string_view&gt;`? I don't think that the arguments below against `string_view` is not "safe" make any sense because any collection returning an `iterator` suffers exactly the same invalidation issues. `string_view` is basically just two iterators anyway. Or you could make a version that takes a `void(string_view)` lambda to avoid any allocation.
The same thing would happen with an iterator grabbed from a temporary map, vector, list, etc... basically any existing collection suffers from this issue. That's kinda the way C++ is, it's not really an argument against `split` written that way I think.
I use Google's [gflags](https://github.com/gflags/gflags) for all CLI programs and love it. IMO, it's in the same category of quality and ease of use as argparse.
A vector means making heap allocations, that in many many use cases isn't necessary. The better thing to do is to return an InputIterator that dereferences to string_view. If you have such an iterator you can trivially construct a vector from it and be no worse off, but if the function returns a vector you can never recover that overhead. 
Because getopt exists? :-P
 std::basic_string&lt;T&gt;::is_palindrome Ideally AVX-powered for best performance.
What's blocking this proposal? From basic overview it looks like logical evolution of const T&amp; semantics.
Having followed several other much smaller quality of life proposals through the committee it truly is finding someone who cares enough to write up a formal spec and propose it.
To be clear, getopt is not part of the C standard, it is part of the POSIX standard. A lot of things that we take for granted in C come from POSIX rather than the C standard itself.
Even if it was discussed before, I don't see a proposal recently, and the landscape has changed. I'd expect, for example, a modern option parser to travel in terms of string_view. POSIX standardized it once, but then everyone promptly extended it. In incompatible ways. Which might be the highest barrier. But we did get filesystem, and that's a mess of real world issues, too. 
Maybe someone should write the package system to not be terrible to use with a build system. Or, I don't know, a bridge between cmake and some lesser used systems to allow them to communicate at some level. Not that this is really possible in any sane way. 
https://stackoverflow.com/questions/3478046/why-isnt-main-defined-mainstdvectorstdstring-args
&gt; The same thing would happen with an iterator grabbed from a temporary map, vector, list, etc... basically any existing collection suffers from this issue Sort of. It's much harder to accidentally do this sort of thing with an iterator since algorithms require both a start and end iterator, and you can't easily grab both. e.g., it's very hard to write: `auto iterator = find_if(temporary.begin(), the_same_temporary_as_before.end(), predicate)` due to the difficulty of writing `the_same_temporary_before`. It's certainly possible with some gymnastics, but rarer. Compare with the trivial ease of writing `auto result = ranges::find_if(temporary, predicate)`. Practically writes itself.
&gt; but I find it hard to accept that the only reason something like this does not exist in the standard is because nobody has bothered to formally propose it Well, formally propose it, then do all the followup papers after evolution provides feedback on direction and then again after core is ready to review standard wording. "All it takes" is entirely accurate and yet misleadingly simplified. :) The process would involve: - proposing the feature be added and getting approval from LEWG (I suspect you'd get near unanimous support) - proposing the _shape_ of the feature, e.g. what the interface should be (I suspect you'd get resistance on just copying `program_options`) - this might be part of the same proposal - getting approval after any and all evolution sub-committee feedback - proposing wording to LWG - this might be part of the original proposal for a small and simple feature, but unlikely for something as complicated as option parsing - making any requested tweaks or changes by LWG that they require before voting it in - _if_ it goes into a TS instead of the standard proper, eventually making the follow-up proposal to move the wording from the TS into the standard (which should be much more straight forward) It's a bureaucratic hellscape, but it is what it is (and some people even prefer it that way).
&gt; Use boost.program_options is overkill if project should use boost due to this case. If all you use is program_options, then all you pay for is program_options, not all of boost. I don't think this is a good reason to avoid program_options. ~~It's header-only, though, so you don't pay for what you don't use.~~
As far as I get it, it's not *just* writing the paper. It was my impression, and I would love it if I was wrong here, that you'd also need to be able to leave work and pay to go to one of these locations where they meet every so often. Or you'd need to find somebody else who will support the paper for you. Right? 