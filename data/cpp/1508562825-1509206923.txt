Sometime after 15.5 right?
Furthermore, Jason Turner is co-host of a pretty rad podcast (cppcast). I've learned a good deal of information from their discussions on the podcast.
Previous post of this video - https://www.reddit.com/r/cpp/comments/772f9h/cppcon_2017_victor_ciura_bringing_clangtidy_magic
To be fair, just because a GPU shouldn't be a statemachine doesn't mean that an Audio channel shouldn't. They are very different things. 
Didnt spot your original post. No harm posting it again for weekend video watchers as this video is well worth the time.
You are correct that source location information may change the IFC in the Visual C++ implementation. Ideally, only semantics-relevant changes should trigger recompilation keyed on the IFC.
&gt;A standardized BMI would be even better Agreed. &gt;but can that happen or is this even possible? Is there a current effort on this? It takes a whole village to accomplish that :-) But my hope is the community at large recognizes this is an opportunity to accomplish something very useful and change C++'s reputation in the build area (and elsewhere!)
&gt;I recall /u/GabrielDosReis saying that MSVCs module interface format is based on his earlier IPR library, and that they were planning on open sourcing it at some point. That is correct. Microsoft's C++ Team is committed to openly documenting its IFC format and is willing to collaborate with any tool vendor interested. Hopefully, it is useful enough for the community at large. This is an opportunity to do something unprecedented for C++. &gt; IDEs will also need to be able to understand BMIs if they're going to be able to provide completion suggestions and error squiggles etc, so it seems very likely we'll get a standard format (perhaps per-platform) at some point. The Visual C++ team is working on that too.
Well, the current spec allows expression of component boundaries, explicit statement of dependencies, isolation, massive compile-time improvements, and enable semantics-aware development tooling. It is however true that my approach is to make all this mostly transparent and not require heavy syntax -- to say "look ma, I am using modules." Ideally, for an existing well structured modular components, you don't want formal introduction of modules to require excessive source code changes. In that sense, it is different from say `constexpr`.
It's very simple to express with a simple utility function template: https://godbolt.org/g/YW76GT.
Note that the Module TS does not impose any of the 'single' vs. 'multiple files' thing. It supports both equally.
How do you like the idea of having a compiler option that can be used to print a "location-independent hash" of the interface during its compilation?
&gt; could/should the BMI be embedded in a module's compiled object file? One of the items on my long TODO list is to embedded the IFC in shared or static libraries, so that they (the libraries) are independent self-describing entities. Useful for verification, binary analysis, validation, etc. And of course, for the IDEs
Just to clarify, `fetch` fetches the list available packages from all the repositories you have added to the configuration (with `add`). You also don't have to issue `build` for every dependency if you specified your dependencies in a manifest (I showed an example in one of the parallel discussions here somewhere). I've also watched the video you have linked and I now understand better where you are coming from. In your model the dependencies live in the tree of your project (maybe not in the repository but at least on disk). The `bpkg` approach is different in that the dependencies live in the build configuration. This has some advantages like being able to build against several versions of the same library or having several projects share a library.
As an occasional library developer and full-time app developer, I appreciate the "no dependencies beyond your OS" approach to vending a library. 3rd-party dependencies are a huge risk and practically automatic technical debt. I'm much more willing to adopt a library which doesn't compound the problem by introducing a transitive dependency.
&gt; The bpkg approach is different in that the dependencies live in the build configuration. This has some advantages like being able to build against several versions of the same library or having several projects share a library. I think it would be great to have a clear explanation of this in the documentation for `build2`. Especially if it comes with several examples. 
He's not an FP person, he's an "expressive programming" person. All of his posts are about names, filled with tangential padding. The only FP thing in here is him talking about functional languages a little, which is relevant because he's talking about a functional construct.
I’m very happy about the warm reviews I received for this project. The feedback and the community excitement around this talk/tools well exceeded my expectations. This fueled our team to continue developing these tools. We have big plans going forward. 
Sharing is caring :)
Thanks for sharing! We look forward to community feedback and contributions. I’m very excited about this project; gaining some serious traction...
- - - **Abstract:** “A 14 year old code base under active development, 2.5 million lines of C++ code, a few brave nerds, two powerful tools and one hot summer…”, or “How we managed to clang-tidy our whole code base, while maintaining our monthly release cycle”. Did I mention that we’re a Windows-only dev team using Visual C++ ? That’s right, we’re going to continue using both Visual Studio (2017) and Clang tools on the side, to modernize and improve our code quality. I’ve just come back from an interesting journey … and I want to share with you some of the most exciting experiences my team and I had along the way and a few things we’ve learned that you may take with you on your next “travels”. It all started a year ago, at CppCon, with a simple but life changing decision: we would stop worrying about whitespace and started our addiction on smart C++ tools with clang-format. We didn’t realize this at that time, but this was just the first leg of our great journey; next we decided to hop on the clang-tidy train and set out to modernize our aging code base and find hidden bugs along the way with clang-tidy static analyzer. The hard part was getting all our code to compile with clang, using the correct project settings (synced with Visual Studio) and Windows SDK dependencies (our code has a fairly wide Windows API surface area). After that, clang-tidy was a breeze to use and we immediately integrated it in our workflow. I still cannot believe the code transformations we were able to do with its ‘modernize’ modules and some of the subtle latent bugs we found and fixed with its static analyzer and ‘cppcoreguidelines’. Luckily, we took a lot of pictures and kept a detailed travel log, to share this fruitful journey with you, now. We’ll also share some tools we developed, to help you with this workflow: automation tips &amp; configs (Jenkins, MSBuild), open-source PowerShell scripts (clang-tidy on Visual Studio projects), free Visual Studio extension and more. **Slides:** https://goo.gl/iw9Xz2 - - - GitHub project: https://github.com/Caphyon/clang-power-tools PowerShell scripts: https://github.com/Caphyon/clang-power-tools/blob/master/ClangPowerTools/ClangPowerTools/clang-build.ps1 https://github.com/Caphyon/clang-power-tools/blob/master/ClangPowerTools/ClangPowerTools/sample-clang-build.ps1 Visual Studio Marketplace - "Clang Power Tools" https://marketplace.visualstudio.com/items?itemName=vs-publisher-690586.ClangPowerTools Version history: https://github.com/Caphyon/clang-power-tools/blob/master/CHANGELOG.md --- 
[removed]
For C++ questions, answers, help, and advice see r/cpp_questions or [StackOverflow](http://stackoverflow.com/). This post has been removed as it doesn't pertain to r/cpp.
Is Microsoft IFC that same thing as Binary Module Interface (BMI) that Berium talks about? Two points to consider - Instead of binary with description it’s also possible to have description with binary - Think of IFC as a service For both cases IFC file should be a container consisting of standardized and custom partitions. The module interface description should be one of the partitions (hashed so that build system can easily retrigger compilation) This would allow the community to build a rich infrastructure around modules AND embed their results into the file. So we have one bag that carries around all information. 
By definition a wellformed program is a program that adhere to the syntactical rules (and possibly the statically checkable semantic rules). So ill-formed problems will be rejected at compile-time. But most compilers, for imperative languages, also accept invalid programs and catch those problems at run-time. In C++ those run-time checks have been replaced by UB. So yep, UB implies an invalid program, but UB does not imply an ill-formed program. All ill-formed programs should be caught at compile time, if not that would indicate a compiler bug.
You can have only mop files if you only use modules.
Even if it's possible, it might have different meanings, so I am not sure how much this would buy you (on a new platform, it might compile, but do something different ...)
This is what I was lead to believe too. I'd be interested to know what's correct here. I wonder if /u/ldionne has an answer for us
I believe that what happens is that the static constexpr const function is being called, not operator(). This is constexpr in all cases and is perfectly valid C++. 
I believe that what happens is that the static constexpr const function is being called, not operator(). This is constexpr in all cases and is perfectly valid C++. The reason you can't get a constexpr value from operator() is because it's not static and is therefore using "this" implicitly. "this" isn't constexpr and therefore, as the parameters to the function aren't known at compile time, the full call isn't possible. This, of course, doesn't need to be the case (as GCC demonstrated), but as far as the language is concerned - there is no syntactical guarantee and therefore it should not work. This is obviously something that will change in the future, I can't imagine why this wouldn't be beneficial.
I believe that what happens is that the static constexpr const function is being called, not operator(). This is constexpr in all cases and is perfectly valid C++. The reason you can't get a constexpr value from operator() is because it's not static and is therefore using "this" implicitly. "this" isn't constexpr and therefore, as the parameters to the function aren't known at compile time, the full call isn't possible. This, of course, doesn't need to be the case (as GCC demonstrated), but as far as the language is concerned - there is no syntactical guarantee and therefore it should not work. This is obviously something that will change in the future, I can't imagine why this wouldn't be beneficial and I doubt it'll be too hard to change the standards requirements for a constexpr expression.
I believe that what happens is that the static constexpr const function is being called, not operator(). This is constexpr in all cases and is perfectly valid C++. The reason you can't get a constexpr value from operator() is because it's not static and is therefore using "this" implicitly. "this" isn't constexpr and therefore, as the parameters to the function aren't known at compile time, the full call isn't possible. This, of course, doesn't need to be the case (as GCC demonstrated), but as far as the language is concerned - there is no syntactical guarantee and therefore it should not work. This is obviously something that will change in the future, I can't imagine why this wouldn't be beneficial and I doubt it'll be too hard to change the standards requirements for a constexpr expression.
I believe that what happens is that the static constexpr const function is being called, not operator(). This is constexpr in all cases and is perfectly valid C++. The reason you can't get a constexpr value from operator() is because it's not static and is therefore using "this" implicitly. "this" isn't constexpr and therefore, as the parameters to the function aren't known at compile time, the full call isn't possible. This, of course, doesn't need to be the case (as GCC demonstrated), but as far as the language is concerned - there is no syntactical guarantee and therefore it should not work. This is obviously something that will change in the future, I can't imagine why this wouldn't be beneficial and I doubt it'll be too hard to change the standards requirements for a constexpr expression.
Woops. Phone decided to die and post a million times
Honest question: Why can't you just tell your RHEL 5 customers to compile CMake from sources so that they have a recent version? It doesn't take more than 5 minutes, and since your library is a C++ SDK they're going to have a C++ compiler to compile it anyway.
That's the point, if it wouldn't work it wouldn't compile. But it would be a standard way that would work on 99% of machines. Just like intXX_t and the like.
&gt; because it may interfere with optimisations relying on strict-aliasing The next question, then, is whether strict-aliasing is really that useful. In HFT, where performance is critical, C++ code is compiled with `-fno-strict-aliasing` because strict aliasing inhibits a whole suit of optimizations, such as `reinterpret_cast` buffers of bytes as various types and efficiently storing various types in `union`. Now, I am not saying that aliasing information isn't important. However I am afraid that type-based aliasing is the wrong way to go: - it's extremely coarse: it provides no facility to indicate that two `char*` are not aliasing, - it's superficial: `T*` and `U*` may be guaranteed not to alias, but no such guarantee is made on their `char*` data member, - it's inhibiting important low-level optimizations/bit-twiddling. At this point, I think we should seriously consider scrapping the idea and move on. C's `__restrict`, possibly with a "depth" indicator, is a possible candidate for example.
Note that C++ allows access of the "common prefix subsequence" via non-active union members between trivial-layout types. That is, for: struct X { int tag; char* string; }; struct Y { int tag; double number; }; union U { X x; Y y; }; It is always well-defined to read `u.x.tag` no matter whether `x` or `y` is the active field.
I would like `restrict` as an addition to current strict-aliasing (with possible type-punning on unions like said before), but I don't see it as a complete replacement of it (and there are probably good reasons for it but I can't think of them from the top of my head).
Following [the Keynote provided by Titus Winter](https://www.reddit.com/r/cpp/comments/73108j/cppcon_2017_titus_winters_c_as_a_live_at_head/), They just updated their Tips of the Weeks question with 3 new tips
It would probably make syntax more complex. `abc?lol : 3;` is this `abc ? lol : 3;` or label `abc?lol` and a non-op `3;`.
Question: what's the technical/theoretical reason not to have constexpr arguments in C++? Is it because then they can't be runtime functions?
What do you mean by "work"? This code could do a lot of different things to the float, all well defined, but different ... so which of these different things would be allowed to compile?
&gt; If strict-aliasing was to be completely removed then restrict would have to appear next to almost everything to not change meaning of the code (meaning to the compiler/optimiser, not necessarily the behaviour). As with any optimization quest, measuring first is necessary. To be honest, it is unclear to me how much type-based strict aliasing really helps optimizing performance. As I mentioned, in HFT where performance is king, `-fno-strict-aliasing` is used because it allows crafting code which performs better. I find this rather disturbing. If there are cases where strict aliasing: 1. Has demonstrable performance gains, 2. Has large-scale performance gains (ie, gains that cannot be recoup by a handful of `restrict`). I'd really like to know about it. I do not make any claim as to whether they exist or not, I just haven't seen any myself. Otherwise, it seems to me this could well be one of those undefined behavior we would simply be better of without.
by "work" I mean it would be implementation defined instead of undefined behaviour
Sorry but no. the operator() is being called. It is constexpr-valid because "this" is not used inside the function scope, it's used outside of it and given as first argument. check this code snippet, it compiles fine with clang &amp; gcc. https://godbolt.org/g/FuhwFn 
They have to have runtime counterpart and that's the issue. The issue is that for each call with a new set of constexpr arguments a runtime version must be produced where this new set is found in the mangling of the function. The issue is how do we determine what constitute a new set of constexpr arguments. The "operator==" is the desired answer but there is no linker today that would be able to handle that and would require the linker to be C++ conscious =&gt; not gonna happen. In short it has all the same issues of [N3413 abitrary literal type for non-type template parameters](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3413.html). Once we've said all of that and seeing the lukewarm reaction from the committee the last time N3413 was mentioned (during CppCon 2017 "roast the committee) It's fair to say it's not gonna happen any time soon. 
&gt;Is Microsoft IFC that same thing as Binary Module Interface (BMI) that Berium talks about? Yes. &gt; Is Microsoft IFC that same thing as Binary Module Interface (BMI) that Berium talks about? That is my expectation, but IFC aren't distribution format though.
We can definitely consider that. Devil is in the detail: what would the 'print "location-independent hash"' look like. We are building utilities to manipulate the IFCs, along with APIs.
I was giving more thought to this as well, and I have some ideas for how one could make an audio API that acts more like Vulkan. Namely just baking state into audio "pipeline" objects (name pending, ofc), and only allowing some vague dynamic state. Declaring how many emitters you expect to have in a pass of some kind, but changing the audio objects bound to those emitters (like how we declare samplers bindings ahead of time, then bind different textures to the sampler locations)
`using std::string` in a header file [like this](https://github.com/evilsocket/librestd/blob/23f3d1d7f57f011c22e21e6ddaf4881285e80af5/include/crash_manager.h#L23) is a no-no for libraries. The good news is that it's easy to fix!
Please don’t use link shorteners, the spam filter hates them. I’ve manually approved your comment.
I really appreciate your effort sir, thank you very much for the library!
Are the four tips on this site all that are public? If not, link to a full list? I haven't been able to find any others.
Yeah I think it's a pretty compelling idea. I've written audio streaming and mixing software before for a number of platforms (though not console) so I'm pretty familiar with how the systems all fit together with the operating systems. I'm actually starting to put together some code around this concept. Might be another week or so before it remotely resembles the proposed architecture.
Thanks, STL
It's hard to justify using std function here instead of just storing the capable because you don't need type erasure, and the author doesn't attempt to. The use of macros is also hard to justify for such a small saving. Keeping in mind it's relatively rare to need defer at all in c++. For instance, in the example given, close is called automatically. I love scope guard but I only need to use it rarely, about half the time is when I'm working with c routines, but not enough calls to the same stuff that it's worth wrapping. 
Putting N3413 aside, why can't we say if a function has `constexpr` arguments it is not allowed to have runtime counterpart? It is almost like macros, they will be syntactically substituted with a literal in compile time.
Neat project, how did you come up with the design for this? Did you just look at other projects and come up with this? Also any plans for encryption? Looks like all data transmission is plain text.
who can resit abusing a new language feature? hopefully this is not too much text #include &lt;tuple&gt; template&lt;class V, class F&gt; struct Mono { V v; Mono(V v, F f) : v(v), f(f) {} ~Mono() { f(v); } private: F f;}; template&lt;class A, class B&gt; auto defer(A a, B b) { return Mono&lt;A, B&gt;(a, b); } template&lt;class A&gt; auto defer(A a) { return defer(a, [](auto){}); } namespace std { template&lt;class T, class U&gt; struct tuple_size&lt;Mono&lt;T, U&gt;&gt; { static const size_t value = 1; }; template&lt;size_t I, class T, class U&gt; struct tuple_element&lt;I, Mono&lt;T, U&gt;&gt; { using type = T;};} template&lt;int N, class... T&gt; auto get(const Mono&lt;T...&gt; &amp;d) { return d.v; } #include &lt;fstream&gt; #include &lt;functional&gt; #include &lt;stdlib.h&gt; int main() { auto who_owns = (int *)malloc(sizeof(int)); { /* ... */ const auto &amp;[bound_to_me] = defer(who_owns, free); } std::ofstream f("./file1"); const auto &amp;[fref] = defer(std::ref(f), [](auto r) { r.get().close(); }); const auto &amp;[leak] = defer(malloc(16)); return 0; }
If you want your C++ library to be widely used, I would recommend against using the GPL or LGPL. Once I saw the license, I stopped looking any further, and there are other people who are are going to to do the same. In addition, there are several libraries for HTTP(S) that are under more liberal licenses. 
I'm learning a bit of Rest for services in small devices. What would the advantages be when using a library like this one instead of, say, FastCGI programs with a JSON library?
Not really an answer but may I ask why you like trailing returns? I only use it when a trailing return will decrease verbosity (such as an internal typedef used as a return type of a member function), and in 99% of functions I find it just to be more verbose.
Because it reminds me of the mathematical notation for functions. I like to see functions as "this goes to that"-things and because of that i like writing the return type after the parameter types and the arrow.
I always read about type erasure for std::function. Is that about two instances with the *same* signature having the same type and lambdas being type disjoint even with the same signature? If not i would very much like a link for further reading...
If you prefer the haskell way of type annotation though be aware that return types unfortunately are not part of the type signature (which is important with respect to overloading)
As a recent example, I needed to cast a large structure into an array of `uint_32`. Making an entirely separate copy of this temporarily seems excessive.
No, you're exactly right. Every lambda has unique type. One implication of this is that if you store a lambda directly, you don't heap allocate, and the compiler knows exactly which lambda is being called so inlining is easy. With `std::function`, the type is only based on the signature, and it can store any callable compatible with that signature (it need not be exactly the same). Because the lambda has unique type, and because you are storing that type in the scope guard object, the scope guard must be templated on this unique lambda type that can't easily be gotten (you have to use decltype). So prior to C++17, the best interface looks like this: auto guard_foo = makeScopeGuard([&amp;] { ::free(foo); }
I though LGPL was fine. Is there an issue? But yeah, whenever I see GPL I just skip the whole thing.
While watching this, I can't help but imagining /u/STL cringing at the indignity of what is done to some of those cats.
Please switch to the Boost Software License
That is not true, return types are part of the type signature, they are just not considered for overloading. Making return types part of overloading would be a real mess. Haskell doesn't have C++ style overloading afaik so they simply don't have this issue.
That's all that are public. The numbers imply there are more because the source is an internal Google tip of the week series. They are publishing them over time. 
Hm... Didn't think of SFINAE, you are of course right. Your argument about haskell overloading (i.e. type classes) not having this problem is not really true though. They are just saved by a more strict type system in the end...
Ah, thanks for the clarification. I just recently noticed this unique type problem trying to explicitly (template) instantiate some foo(FuncT&amp;&amp; func) function for compile time optimization... Not having to worry about types has its costs.
This sounds and looks like a destructor. 
This library is intriguing to me, but I want to see it mature a bit more before I consider bringing it in as a dependency. CMake support is obviously a must, but I see there is a PR for that in now. In the keynote they teased new versions of gflags, glog and gtest. Gflags and gtest are both already very good libraries and glog isn't awful, so I'm staying tuned.
so go users consider this to be an advanced feature?
I cannot help but think,"why not just use unique_ptr here?
Hooray. A few weeks on from the keynote and I was beginning to think these were going to be overlooked. Will the release of further tips be more consistent?
That's true, you can overload based on the return types in Haskell: class Read a where read :: String -&gt; a This lets you define `read` multiple times for reading different things from a String. Thanks to Haskell's global type inference the ergonomics of return type overloading is identical to that of argument type overloading.
It looks like several of the things in the library, like `string_view`, are being merged into C++17. In general, I recommend the [Guideline Support Library](https://github.com/Microsoft/GSL), since it has many similar features. The idea is to write code in such a manner that either memory errors can't happen, or they always properly trigger exceptions. So, the C++ core guidelines encourage things like using smart points of object management instead of calling `new` or `delete` directly. That way, when an item goes out of scope you never have to worry about, "did I remember to delete this?"
What's the fix? You can't forward declare them I thought
Replacing `string` with `std::string` would be the easiest.
This is not the first time seeing someone "implementing Go defer in C++". But why do so? I've never seen a justification. All these attempts are using RAII *but yet* ignoring the benefit of RAII and its superiority over Go defer.
&gt;It looks like several of the things in the library, like `string_view`, are being merged into C++17. The point of those are that they work under C++11 so those features can be used if you can't upgrade. If you then upgrade to C++17 then all of your usages of absl::string_view etc just become std:: versions
Have you tried looking into the hunderdths of checks from clang-tidy, see if this has been already added?
this or boost, which to learn?
Oh I'm sorry, I misread it as simply using the string rather than writing literally that. My fault.
&gt;That is not true, return types are part of the type signature Thought it depends.
my thoughts exactly
Because it's a totally different idea altogether. The constexpr argument idea is that the user can add a constexpr specifier to any function argument argument. I doesn't imply that the function is constexpr. Furthermore a constexpr function is, in C++, an inline function that might be evaluated at compile time, a runtime version I required to exist. A constexpr argument should only be seen as a generalised template non-type parameter given in the argument list of a function. Ex: auto print_n(constexpr size_t i) { std::cout &lt;&lt; std::get&lt;i&gt;(*this) &lt;&lt; std::endl; } auto stuff2(constexpr auto, int i, constexpr auto) { ... } //i is a runtime value constexpr decltype(auto) operator[](constexpr size_t I) { return std::get&lt;I&gt;(*this); } //if there is no runtime version this can't be called on a non constexpr object! I had this idea a while back and did post it on the future-proposal forum but It didn't garner much attention and some of the points are invalid. I'm working an alternative to constexpr arguments. Mainly the idea is : //allways_inline constexpr function were the arguments are implicitly constexpr. //allways_inline because no mangling/linking issues. //If an argument is required to be constexpr and it's not =&gt; error; otherwise ok. //If a non constexpr function/method/value is used, the code will be injected it in the caller's assembly. //a real alternative to macros without the context awareness. inlined_constexpr decltype(auto) operator[](size_t i) { return std::get&lt;i&gt;(*this); } int main() { std::tuple&lt;int, long&gt; t = {0x5, 0x34}; auto &amp;x = t[0]; //produce the exact same assembly as "std::get&lt;0&gt;(t);" constexpr auto&amp; x1 = t[0]; //error t is not constexpr constexpr auto x2 = std::tuple{0x5, 0x34}[0]; //ok }
are you sure you're not going to reinvent some kind of patching music software like PureData, Max/MSP, etc ? look also for instance at the [Gamma](https://github.com/LancePutnam/Gamma/blob/master/examples/effects/echo.cpp) library or [Faust](http://faust.grame.fr/examples/2015/10/01/organ.html), a DSL that generates efficient DSP code (for instance in C++ or whatever language you like) that you can then just use in your software.
Boost is a collection of libraries which are distributed together. You don't "learn boost", unless you're insane. 
The author really needs to learn more about C++11 onwards. typedef, NULL, no use of std::unique_ptr where it would be useful, etc etc. 
&gt; There are languages where functions are actually objects, C++ is not one of them. so std::function isn't an object ? :p 
The problem is - the defer statement is not local scoped, it's function scoped. And it's used in that way, for example inside - if blocks which add defer statement and it executed at the end of the function. So to recreate it in C++ you have to create variable at the start of the method/function and store your scope guards there (like vector?). Why would you want it tho? Function based scope would be extremely counter intuitive to the any C++ code and developer because with RAII we are used to local scope. 
The guideline support library doesn't even have a fraction of the features abseil has. Afaik, the only really useful class is gsl::span (and maybe gal::byte, which is now part of c++17). Multispannung is also interesting, but I didn't have a use case for it yet. And you certainly don't need the gsl in order to use smart pointer.
Boost if anything. This one will die like every other Google's library.
probuffer?
This looks very similar to scope exit, which IIRC there has been a C++ proposal for.
It's useful for things you only need to do once where wrapping the resource in a class is not worth the effort.
Very much. I've never met a vegan who was so forward about it. On the other hand, it's fairly regular that a meat eater, once finding out (through questioning about bacon or something) that someone is a v-word, will spout a never ending torrent of bullshit about bacon and vegan memes. Go and make fun of people with ideologies based around their imaginary friends, rather than those based around ethics and more concrete concepts. Also: Programming. C++ is great.
Right, boost already has optional and string_view though. I know Google's version is going to likely be a bit more efficient, but it needs a few more features before I think it will see widespread adoption.
std::function is neither a function nor a part of C++ language.
I've tried doing this myself. I started by trying to write a regex that matches on function signatures, but I'm no expert, and it was really hard to try and come up with one that worked for everything. I don't even know if it's theoretically possible to write such a regex.
And throwing in a destructor then, if it happens..
&gt; the C++ core guidelines encourage things like using smart points of object management instead of calling new or delete directly. What are you going on about? We are talking about absail, and that has nothing to do with using or not using smart pointers.
The author here. Useful feedback, thanks! I am close to what I write so there are lots of things that I don't see. I think it's one of the points of public writing, exchanging ideas and feedback. So if you have more feedback keep it coming :)
I think its not possible. When a function declaration has an argument which is a template it needs to match the &lt; and &gt; which is a typical aⁿbⁿ grammar which is not parsable by a regex
I did and i havent found anything
&gt; std::function is neither a function so what is it then ? &gt; nor a part of C++ language. the standard library is part of the language. 
Just curious: what are the libraries you are considering for doing REST on small devices? You mentioned FastCGI, and I think CGI based solutions are too heavy for small devices. Though I've no experience with programming on embedded devices, so my point might be wrong. Would be interesting to get a comment from experts in the domain.
It is not really a small device, Aarch64 but I would like to know if the efficiency exchange for modularity would be worth, as the service might share resources with more important stuff where performance is critical
There's a proposal for `std::bit_cast`, which would do all (or at least most) of this with a natural interface `std::bit_cast&lt;double&gt;(uin64value)`: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0476r1.html
I hate the modules proposal. It's better than nothing, but falls short on so many fronts. Personally my biggest issue with it is that the committee is forcefully trying to keep modules and namespaces separate. Modules *should* introduce a scope, in fact they should become a *replacement* for namespaces. If they don't, you're locking yourself out of a sane import system like Haskell's forever. By default, I want to import names unqualified. Why do i need to clutter my code with all those std::, boost::asio::, ... when I rarely have name clashes? Talk about paying for what you don't use on a syntactic level... The way this is modules proposal is going I see myself putting a "using namespace" at the top of every single file to achieve the same convenience. Namespaces are now useless.
Now I just need a slack bot to auto post them to my team. 
Sounds like you prefer verbosity over terseness. I don't.
I'm sure it's possible to do some sound synthesis/filtering on gpu as well. I'm not sure it's necessarily worth it on systems with discrete gpus, but on mobile devices and other integrated systems you can just make shared buffers that are accessible by both so it's pretty easy to jump back and forth without copying a bunch of memory. That said I suppose the goal right now is to work out an architecture that allows for the submission of something akin to a command buffer to a command queue that allows you to work with an audio graph in a less stateful way than most existing apis provide for.
How to describe code: * If it is longwinded but I like it, it's "explicit". If it's longwinded and I don't like it, it's "verbose". * If it's is short but I like it, it's "concise". If it's short and I don't like it, it's "terse". :-)
To everyone who wonders how trailing return types look when rolled-out on an entire (small) code base: https://github.com/Robbepop/clpp/blob/master/include/clpp/device.hpp#L113 Personally I liked this syntax a lot but I guess all in all the best is to be uniform within the same code base.
To the best of my knowledge there currently is no check for clang-tidy to convert a code base to trailing return type. Also one could argue if this is more a concern for a code formatter such as clang-format. However, it would be awesome to have such a tool!
"Unicode aware" is one of those terms that mean different things to different people. It doesn't mean much in itself. It looks like the goal of this `CsString` is to be able to treat strings of different encodings uniformly. void f(CsString_utf8 a, CsString_utf16 b) { if (a == b) // ... } The cost of this is that every single operation on a `CsString` requires looping through every single code point, including `size()`: template &lt;typename E, typename A&gt; auto CsBasicString&lt;E, A&gt;::size() const -&gt; size_type { size_type retval = 0; for (auto item = begin(); item != end(); ++item) { ++retval; } return retval; } So the 3500 line `cs_string.h` is basically a bog-standard string implementation, backed by a `std::vector`, that uses an encoding template trait to return code points. Personally, I fail to see a use-case for this: internally, your program should stick to one encoding (and one timezone, and one system of measurement, etc.) and convert when interacting with its environment.
At your service: template &lt;typename F&gt; decltype(auto) scope_guard(F&amp;&amp; f) { auto ff = [&amp;f](char*){f();}; return std::unique_ptr&lt;char, decltype(ff)&gt;(reinterpret_cast&lt;char*&gt;(1), ff); }
Because it is a destructor.
&gt; I'm sure it's possible to do some sound synthesis/filtering on gpu as well. It is, but not in a real-time context. For audio you really want at most a few microseconds of latency for a given computation (so that you can be below a few milliseconds when a lot of stuff occurs). Some reverb effects were made to run on GPUs a few year ago but the latency overhead of cpu -&gt; gpu -&gt; cpu was &gt; 50ms iirc which made it totally useless for real-time stuff.
Yeah, there's no perfect solution right now. Having to inherit to compress opens a whole class of problems. I know that GCC folks were working on a `[[gnu::empty]]` attribute to make empty members take 0 space instead of inherinting, but it probably has its share of problems too.
Please provide more information how it is better than icu::UnicodeString. What features are missing? Some benchmarks would be nice. It's certainly doesn't look like an improvement in terms of style and case.
For example if you need to modify it to suit your needs and your company policy prevents you from sharing non-bug related changes. Also, if you want to statically link to it plus some other corner cases. (I know about object files. It's ridiculous.)
You can always put nginx as a proxy that provides TLS and HTTP to HTTPS redirection. Though I would suggest looking at alternative libraries with more features.
I always wonder why people use struct sockaddr_in address; memset(&amp;address, 0, sizeof(address)); instead of struct sockaddr_in address = {};
Both QString and icu::UnicodeString actually understand unicode, and support, among other things case insensitive comparison, normalization ( and collation, but that's a more exotic feature). QString uses utf16 as it's internal representation and a lot of work has been done to use SIMD instruction wherever possible. ICU is exactly why there is no Unicode support in C++. you need mega bytes of table (unsuitable to performance critical applications) to do that properly, and those tables change more frequently than the standard.
Or even just `sockaddr_in address{} `
Er, more that you don't need the scope guard at all,you can use the custom deleter of the unique-ptr when creating the file object. That said, ofstream's destructor calls close already, doesn't it?
That, doesn't answer my questions...
That doesn't answer my questions...
Besides raw pointers (which smart pointers should manage anyway), what resoueces-acquiring constructs aren't already a class, though?
&gt; ofstream's destructor calls close already Of course it does, don't do this with a stream object! Mine is just an example of how to use unique_ptr if you don't have any suitable object to "own".
There is a nice presentation about Unicode featuring this library from CppCon: https://youtu.be/ysh2B6ZgNXk
&gt; cpu -&gt; gpu -&gt; cpu was &gt; 50ms iirc which made it totally useless for real-time stuff. ...except on, e.g. mobile devices and other systems with integrated gpu/cpus that share memory since you can have 0-copy buffers between cpu and gpu. Anyway I'm not interested in doing that. &gt; that's literally how most "professionnal" audio software work. Some good read on this : http://www.rossbencina.com/code/real-time-audio-programming-101-time-waits-for-nothing ... that link has nothing to do with what you just said. I'm aware of how audio subsystems work. I've worked with low-level audio on mac/ios, windows, and android. I've written real-time audio mixers before. What I have not done is spent a lot of time with middleware such as Fmod (which appears to be industry standard) but just looking at sample code and having worked with the lower level systems on these platforms before I figure (and so does the OP who brought this up) that there must be a better way to design a cross-platform api for audio playback and mixing. Whether or not I manage to create that "better way" is inconsequential. I'm giving it a go for funsies and I will succeed or fail. So far you have not managed to convince me that what I am doing is redundant. You have given me examples of software that are either not really relevant (synthesis, which is not the primary goal here), or too low level (wrapping the OS-level audio subsystems is the point). 
Lots of OS-specific stuff like HANDLEs or resources from C libraries.
You can at least use the feed http://feeds.feedburner.com/abseilio
Also note the cppcast episode http://cppcast.com/2017/10/titus-winters/
ICU is also a good example of the kind of problems you get porting from Java to C++. Lots of microallocations, which cause cross thread contention with typical system allocators... Converting into icu::UnicodeString pretty much locks your entire product into these bad architectural decisions, because the cost of not treating it as an all-or-nothing solution is staggering.
yeah but it takes like, 5 seconds to write a class for that. class winhandle { public: winhandle() {han = CreateHandle();} ~winhandle() {CloseHandle(han);} operator HANDLE(){return han;} // for convenience private: HANDLE han; } Which you should if you're doing `lots of OS specific stuff`, ie more than once, with the handles.
Yes, it is useful in this case. So? Does it justify "implementing Go defer in C++"? Just in case if I'm not clear: This very blog post, at one step before going into the implementation of `DEFER`, demonstrates scope guard idiom, which already handles this particular use case. All "implementing Go defer in C++" does is unnecessarily wrap the scope guard into a macro for the sake of making it looks like Go defer. I don't see the justification. And this blog post is worse than your comment. You at least points out a use case that potentially justify `DEFER` (which doesn't; it only justify scope guard). The example in the post defers a `ofstream::close`! The OP doesn't even border to do a `fclose`?
String concatenation is such an interesting problem because it's so conceptually simple yet so difficult to do correctly and efficiently. There's a reason absl::StrCat was highlighted at the keynote.
I don't like ScopeGuard because it has too much syntactic overhead. The DEFER macro is a bit better but... a macro. Both require an #include. I'd much rather have it as a language feature. Make simple things simple.
IMHO do not use std::cout &lt; ... at all and use [fmt::print(...)](https://github.com/fmtlib/fmt) instate. 
&gt; problems you get porting from Java to C++. Wasn't icu originally a port from C++ to Java? 
It can be useful to time code for instance: int main() { using namespace std; using namespace std::chrono; using clk = high_resolution_clock; { auto t0 = clk::now(); for(int i = 0; i&lt;1000; i++) { } std::cout &lt;&lt; duration_cast&lt;milliseconds&gt;(clk::now()-t0).count() &lt;&lt; '\n'; } } vs int main() { using namespace std; using namespace std::chrono; using clk = high_resolution_clock; { auto t0 = clk::now(); DEFER ( [&amp;] { std::cout &lt;&lt; duration_cast&lt;milliseconds&gt;(clk::now()-t0).count() &lt;&lt; '\n'; } ); for(int i = 0; i&lt;1000; i++) { } } } I find the second case clearer: the rest of the code isn't polluted by the timing code which is all in the same place visually
&gt; I don't see the justification. He says it in the blog post: you have to give a name to the scope guard. Giving names sucks. I hate C++ every time I have to have find a name for `std::lock_guard&lt;std::mutex&gt; whatever(m_myMutex);`
THIS! I finally moved started moving from our custom formatting library to fmtlib.
No, it's from Java to C++
Some companies still do not allow bringing in LGPL code as a matter of policy. At my work I would need to demonstrate there are no other viable alternatives under a more less restrictive license. (Freedom from licensing compliance burdens is always preferred)
That was also my first response, but there is a subtle difference: If you `friend` the collaborators in your namespace they could potentially modify all private data members directly. In particular, you can no longer tell whether what you think are invariants of a class you're looking at actually are invariants or if some of the friends invalidate them by modifying data members directly, even if all provided member functions maintain a certain property. If you only allow access to a restricted set of member functions to namespace members then it is much easier to guarantee that invariants really hold, as you only need to look up the definitions of the member functions of that class and not also all (member) functions of friends.
kinda wasted effort -- we need to get rid of iostreams library
I think the memcpy version is not undefined. The C++ standard is not silent on what happens when the pointers given are to objects of different types: It states, by reference to the C standard, that "The **memcpy** function copies **n** characters from the object pointed to by **s2** into the object pointed to by **s1**," where s1 and s2 are the first two arguments. So the standard specifies exactly the desired behavior: The destination object's representation is modified, but without in any way disturbing its existence, meaning that it clearly remains legal to access the destination object. It's less clear that the version that used reinterpret_cast to directly write `unsigned char` values is free of undefined behavior. The potential problems I see are that it's unclear that the lifetime of the original object continues after writing `unsgined char` values into its storage, and that strict aliasing rules are 'one way': you can read an object's value through `unsigned char*`, but reading the value of `unsigned char` objects through an expression with, e.g., type `float` is an aliasing violation.
I don't believe you.
iostreams will stay around for years, so informing people of its pitfalls is important. Replacing it is just a good longterm goal.
&gt; there has been a C++ proposal for. Thanks for mentioning that. I went ahead to search for it. This seems to be the latest revision: [P0052r5](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0052r5.pdf). The paper links to an example implementation [here](https://github.com/PeterSommerlad/SC22WG21_Papers/tree/master/workspace/P0052_scope_exit/). Boost's scope_exit also has some [comments on alternatives](http://www.boost.org/doc/libs/1_65_1/libs/scope_exit/doc/html/scope_exit/alternatives.html) 
yeah, if we don't yank it out -- it will indeed stay around forever. It's pitfalls are known for decades, all these videos do is rehash old stuff. This lib is like MFC -- over-engineered, very hard to use properly, inefficient and hard to get rid of. It takes place that could be occupied by a better solution (or combination of those).
I haven't thought of that use case. 
I mean, don't endl into an fstream, but with cout it kinda makes sense to flush after a line break. std::cout &lt;&lt; "Doing X\n"; trivial_five_liner(); std::cout &lt;&lt; "Doing Y\n"; the_works(); "Huh? Why is X taking so long?"
Which turns into a "why is this so damn slow": for ( int i = 0; i &lt; 100000; ++i){ trivial_function(i); std::cout&lt;&lt;"processing "&lt;&lt;i&lt;&lt;"/100000"&lt;&lt;std::endl; } Some people try to log everything. In this case its also a good idea to pipe the output directly to /dev/null . 
Usually you should create specific classes to clean up HANDLEs, so that you only write the code once instead of every single time you deal with that kind of resource. 'defer', 'scope_guard', or whatever you want to call it is still good for things you only do once though.
&gt; No, it's from Java to C++ It was originally C++ according to the [Wkipedia page](https://en.wikipedia.org/wiki/International_Components_for_Unicode) TL/DR ICU is descended from C++ frameworks produced by Taligent in the mid 1990s. ..After Taligent became part of IBM in early 1996, Sun Microsystems decided that the new Java language "was missing international support.. 
On sane systems, stdout is line-buffered, and \n causes a flush here anyway (since std::cout itself is unbuffered by default, \n moves to C layer right away)
Did he really say, he couldn't assign the result of std::bind to a std::function?
It is and I have just implemented constexpr parsing of standard format specs: [implementation](https://github.com/fmtlib/fmt/blob/dd0b72e1ee393f1eeac7393dc37f909b76239317/include/fmt/format.h#L3403), [test](https://github.com/fmtlib/fmt/blob/dd0b72e1ee393f1eeac7393dc37f909b76239317/test/format-test.cc#L1708). Still need to integrate it with formatting.
Done in the `std` branch which will be the basis of the next major release.
Is there an OS that doesn't line buffer standard output to a terminal?
I wasn't aware of fmtlib, but it looks great!
While it is wonderful that that can be done, and it's great that C++ is flexible enough to make that happen, it is completely hideous and impractical. That seems to be the trend with "reinvent the wheel" projects though. Proof of concept.
Did you mean std::cerr?
Yes, Windows. Don't know if they fixed it recently, but until recently that was the reason printing UTF-8 to console worked through puts, but not through std::cout.
It's unbuffered unless you sync_with_stdio(false)
Ah, that's the distinction then - buffering in cout *and* buffering in stdout
Doesnt ICU use UTF-16 internally too?
What are the problems with std::cout?
No way I will ever replace `lock_guard` or `scoped_lock` with `DEFER`. But I digress. Think about this. Let's say I agree giving name sucks. Is it a justification of `DEFER`? Or something else? Like "unnamed non-temporary local variable"? "Compiler-named object"? This blog post bundles preprocessor-generated name and a scope guard class together under one macro. But what if I don't want to use the scope guard class? What if, just like your example, I want to apply generated name on `lock_guard`? What if someone somewhere in the world wants it on some class else? What if someone figure out a non-RAII application of generated name? Here's the problem of every "Go defer in C++" I see: Cut down a good general idiom into a narrow use keyword, and then disregard the general idiom, and then pretend the narrow keyword solves the general problem perfectly. And the post is really pretending that. `DEFER(f.close())`? Huh? And it is doing one more bad thing: Narrow the use of generated name into scope guard, disregarding other use cases, say, your example, `lock_guard`. I like RAII. I'm OK with scope guard. I don't oppose generated name. I'm even fine with composing scope guard and generated name together. What I disagree is the attitude behind every "Go defer in C++" I ever see: disregarding RAII or any general features or idioms without justification.
I agree. Do you have a specific library you prefer? A guy above mentioned fmtlib when talking about cout.. I'm also a fan just looking for other opinions.
I do without iostreams unless circumstances force me to use it. It is trivial to write a simple "streaming" logic and there are simpler (and faster) ways to format output or open files.
Thanks for the response. Good to hear some other opinions.
I think this would be in clang-tidy domain. clang-format, as the name suggests, formats the code.
In case you are not joking, there's a blog post from 2001 or smth written by the founder of StackExchange in which he wrote about how difficult this problem is and how common is to poorly implement it. He wrote about many other things in this post, this is just one of them. Good read, totally recommend.
Maybe I can make one for you. I will just look into it. Let me know if you find another solution for your problem.
I'm not sure which documentation he's referring to, but the form he finally comes up with is almost exactly what http://en.cppreference.com/w/cpp/utility/optional says to do: auto create2(bool b) { return b ? std::optional&lt;std::string&gt;{"Godzilla"} : std::nullopt; } 
I don't know if that was deliberate, but I find it strange that he doesn't mention Herb Sutter as the author of the proposal even once.
I wonder if that was the case back when he gave the presentation.
https://xkcd.com/927/
[Image](https://imgs.xkcd.com/comics/standards.png) [Mobile](https://m.xkcd.com/927/) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini\-USB\. Or is it micro\-USB? Shit\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/927#Explanation) **Stats:** This comic has been referenced 4913 times, representing 2.8703% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20{thing_id})
Well, the default encoding is utf8, which you imho should use anyway so the overhead is not that big. And the fact that you have to loop over the string is true for any unicode encoding except utf32. I wonder though, why they don't cache the size (on the other hand, I don't think that is a common operation (most of the time you need the actual number of chars/wchars or work with iterators)
This was really interesting. Did anybody else catch his “oh f*** I need some water” comment? Totally came out of nowhere. Made this talk instantly memorable. 
At the end she shows a very practical approach to still using this with regexes in strings though. And it optimizes very well. So it definitely isn't impractical for some uses. I imagine embedded systems might benefit from this for example.
What if I do want essentially line buffering, then it's no real harm right?
[Static constructor](https://llvm.org/docs/CodingStandards.html#include-iostream-is-forbidden)
Seems like a really cool idea. Too bad, it's 200$ for a license :(
I think it's telling that the introduction is almost entirely about `bpkg`. To me, `build2` feels more like a package manager with some necessary evil build-system stuff, than an actual build system. The point of the article, if I read it correctly, is that when you mixing the packaging system and the build system is either going to result in a package manager that duplicates the one your OS is using (poorly, which is also what tools like `gem`, `pip`, etc. are doing), or a build system that is OS-dependent which results in duplicated efforts writing build files for multiple target systems. Therefore, it is beneficial to have a full separation between packaging system and build system (with a standardized interface), so that existing package managers can communicate with build systems. It seems that instead of solving this problem, `build2` aims to be yet another package manager (like `gem`, `pip`, etc.) making the same mistakes those tools made but for C++.
Chevrons
It was added at 12:39, 2 October 2017‎ so you're probably right.
Well, according to the 'Background and History' section at http://userguide.icu-project.org/ it's the other way around...
&gt; the fact that you have to loop over the string is true for most operations on strings anyway (find, split, copy, compare ...) That's incorrect. Many of these operations can use low-level functions such as `memcpy` or `memcmp` for which most compilers have highly optimized intrinsics. Because `CsString` can work with strings of various character sizes and (I guess) to simplify the implementation by not trying to detect them and use intrinsics, it has manual loops for everything, including `append()`! It doesn't even attempt to reserve anything on the internal `vector`. &gt; I wonder though, why they don't cache the size The cache would have to be updated on every change, and that's an O(n) operation. Maybe they thought it was inefficient enough. In short: it's a slow-ass piece of crap that tries to solve a non-problem. Stay away from it.
You only need to write `"^(?:[abc]|xyz).+$"_pre`. I don't see how that's hideous and impractical.
I also found that omission deeply weird.
How much time would it need to save you for it to be worth paying $200 for?
The first time I see MFC being referred to as over-engineered! The library that feels pretty empty when compared with OWL and its successor VCL. The library that was originally called AFX and was considered too high level by internal teams, thus becoming a thin Win32 wrapper instead.
You are correct and the Wikipedia article has been fixed now.
Like I originally sarcastically said when this bad proposal came up that c++ should deprecate the struct and class keywords this video proves once more I can not even joke about something without somebody thinking it is a good idea... If somebody sent me const _struct or int meaning const int for code review I would jump out of a window. What is metajoke is that C++ ISO butchers the future standards in the name of backward compatibility while code like these slides will turn the language into present compatibility hell...
Currently I have Not found anything.
Also (as opposed to some other metaprogramming libraries), this can be totally hidden in a single TU behind a non-template interface, so the compiler can take its time and do whatever it takes to compile this, but the rest of the program will benefit from these optimized regexes without additional complexity.
This (as opposed to some other usages of TMP) can be totally hidden in a single TU behind a non-template interface, so the compiler can take its time and do whatever it takes to compile this, but the rest of the program will benefit from these optimized regexes without additional complexity.
&gt; I think it's telling that the introduction is almost entirely about bpkg. I think it has now been established that the current introduction, well, sucks ;-). &gt; To me, build2 feels more like a package manager with some necessary evil build-system stuff, than an actual build system. Oh, no, it is the other way around: the build system is where we spend 90% of our effort which should (and does, IMO) make the package manager relatively straightforward. 
Agree. That introduction clearly needs a rewrite.
depends, they have free licences for students and FLOSS ppl.. What worries me more is that I am concerned it may get buggy on MSVC codebases, that it does not understand Protobuffs/Thrift... 
From the build system's point of view it doesn't really matter what exactly this has is: all it needs to do is to store it and compare it to the one obtained on the previous invocation. So an options (e.g, `/module:printIfcHash`) that print the hash to STDOUT during the module interface compilation could do the trick (in case of `cl.exe` there is a bit of a problem in that it by default prints diagnostics to STDOUT, not to STDERR, so this could be a good opportunity to fix that ;-)). Also, we would need to make extra sure that none of the position information from the interface affects the consumer (e.g., in debug info somehow).
Do you mean this: https://www.joelonsoftware.com/2001/12/11/back-to-basics/ ?
There are compatibility flags for MSVC in clang and we make project setup easier with our VS Extension: https://marketplace.visualstudio.com/items?itemName=vs-publisher-1208751.SourcetrailExtension Unfortunately, Sourcetrail can't follow cross-language bindings so far.
The central repository looks a bit empty to me, doesn't it? https://bintray.com/conan/conan-center
I got completely thrown by that slide's use of `operator@`... For anyone else that's confused it's just a placeholder for `operator&lt;`, `operator&lt;=`, etc. Only `operator&lt;=&gt;` is new.
This actually works in VS 2017 if you comment out operator""_pre() and operator""_fpre() in pregexp.hpp. e.g: #include "../compile-time-regular-expressions-master/include/pregexp.hpp" int main() { using namespace sre; RegExp&lt;Begin, Number, Number, Number, Number, End&gt; rx; if (rx.match("1234")) { puts("Match"); return 0; } else { puts("NO MATCH"); return 1; } return 0; } I look forward to P0424 being implemented in VS!
or you could use one generic RAII wrapper for all the things that require a single line/call for cleanup instead of rolling a whole class everytime (with the only difference being that one line). Often the rest of the interfaces are fine/usable and there's no need to wrap all that (or at least not worth the time).
He probably didn't realize it comes across as odd after spending so much time reading the standard. The standard uses `@` as an operator placeholder pretty consistently. 
to my knowledge unique_ptr still needs his template type T (presumably the resource/handle you want to wrap) to satify "NullablePointer", so this doesn't work for all generic handles. See [reference](http://en.cppreference.com/w/cpp/memory/unique_ptr), at the bottom of the 'Notes' section. It works for generic Windows api HANDLEs (typedef'd to void*) as an example, but not for Windows api FILE handles (that's a struct I think, without having looked it up though).
Yeah, there are two parts of the library: regexp matching + parsing of pattern into "type representation".
I hope it'll get [standardized](http://fmtlib.net/Text%20Formatting.htm) (p0645).
How does this compare to [Boost Xpressive](http://www.boost.org/doc/libs/1_65_1/doc/html/xpressive.html)? The talk does not mention about the performance wrt to std regex. That's what everyone wanting to know!
Thank you for showing me this. I've never heard of it until now.
This is a great little demonstration. Thanks for sharing! I’m not very familiar with asio but this is a clear win for coroutines as far as readability! What about performance? Does the coro version have more overhead? Or less? In the readme you mention how the coroutine wrappers are separated out so they can more easily be reused. But the current code seems quite specific for this demo... calls std::terminate on exception, looks kinda hard coded. Or maybe that is just my inexperienced view of coro helpers?? Do you know if there are any std lib proposals to standardize on more fleshed out wrappers for asio?
I think your last sentence resumes crudely but accurately how I feel about CopperSpice as a whole. They forked Qt to solve a non-problem and without understanding fully why Qt was the way it is. 
Number of packages too low. Also, I could be wrong, but C++ attracts a lot of language purists for some reason.
Hello, is the compilation data base generated the same as the one, say cmake would generate on other platforms when using g++? 
With my experiments, the coroutines version compiled to a binary with was a fair amount larger (65k -&gt; 75k, I think). I didn't do any performance verification of it, but I wouldn't expect much in the optimized builds; the cppcon talks I have seen about coroutines showed some quite impressive optimisation by clang. Nevertheless, it's probably worth benchmarking :) The Task class is very primitive, I pretty much just tore out the simplest "do nothing" coroutine wrapper type I could think of, but I believe the intended pattern for coroutines is that there will be a small set of these types defined by clever library authors, and us mortal programmers can just use them without having to think too hard about what happens with an uncaught exception. In this case, it may be better to try and kill only the offending connection and keep the program alive rather than terminating, but I didn't want to pull too much focus away from the echo demo :) I expect that if coroutines are accepted into the standard, asio will start to support it. Until then, I don't know of any effort to provide some. It would be really great to see one though, as it would give people an opportunity to try out coroutines in practice.
Unfortunately your Catch template doesn't compile under VS: using namespace sre; using RE = RegExp&lt;Plus&lt;Catch&lt;1, Number&gt;, Optional&lt;White&gt;&gt;&gt;; RE re; re.match("12 34"); Any chance you could look at that?
I will look at this asap.
I didn't know about Boost Expressive a few weeks ago. I didn't have time to do any measurement. But I tried to compare egrep and libc++'s regexp and this one. And results depends on complexity of RE: RE: "ABCD" on very big file of several gigabytes: compile-time-RE: 20.6s libc++: 6m 43s egrep: 1m 2s RE: "ABCDE|DEFGH|EFGHI|AAAA+" compile-time RE: 21.6s libc++: 63m 18s egrep: 3m 39.7s
Anyway ... there was not much time to talk about performance. But I'm planning to have normal sized talk next year.
well, with a proper module system, we could create a new textual binding for C++ without breaking use of existing module. Since modules are binary, syntax don't matter. It would impact new code that uses that textual binding. This would allow to throw away all the old stuff we're carrying, and make the syntax, if well designed, much better. Disallow C style cast, const by default etc.
It is, because it's a curated repo and they are understaffed at the moment. But check out the bincrafters repo: https://bintray.com/bincrafters/public-conan
"horrible"? If that is your biggest concern, consider yourself lucky. I doubt "old style" function declarations are going away, so get used to them. Spend your coding time somewhere else.
&gt; crudely Really? Which part? The "slow-ass" or "piece of crap"? &gt; CopperSpice as a whole Honestly, it's the first time I've heard of them. Reading through [their slides](http://www.copperspice.com/pdf/CsString-CppNow-2017.pdf) is funny: &gt; Companies like Microsoft may have selected a text encoding without really thinking things through, they elected to adopt UTF-16 as the native encoding for Unicode on Windows And looking through their github repo doesn't make me feel better. I mean, who in the hell would think _forking Qt_ is a good idea?
I am pretty happy with my codebase and I realy like the new function syntax. I know the old function syntax will not magically disapear in all c++ projects and i dont even want it to, but I would be a even more happy programmer if it disapears in my project.
So this is basically an C++ implementation of the Haskell `maybe` monad?
The flash presentation actually touches that subject. He says that std::endl is now tainted by the wrong usage so when you look at code, you can never be sure the author wanted the flush effect or just the return. So you should then do it explicitely : use `&lt;&lt; '\n' &lt;&lt; std::flush` to be explicit this is the behavior you wanted.
How do you find something like this? Running 'strings' on the compiler dlls? Btw: best unknown MSVC flag in my opinion is still "/d1reportAllClassLayout" and "/d1reportSingleClassLayoutXXX"
Much better indeed.
Would **love** to have more detailed statistics about compilation time/complexity.
&gt; c++ should deprecate the struct and class keywords I really think that it's a good idea. C++ should become "imperative lisp" imho.
Well, packages in a central OSS repository is just a small fraction of what conan is. For enterprise users, the central repository is completely useless, because they will not want to rely on it (I hope!). Instead they package and build their dependencies in-house.
Perhaps. http://en.cppreference.com/w/cpp/language/union is a somewhat hairy read. I'm not on the committee or a serious language lawyer or anything, but I would guess that the bulk of the future work will be pointed towards improving std::variant or considering the different pattern-matching ideas, which would more or less obviate the need for unions except as implementation details for things like variant.
Summoning /u/STL !! 
I agree completely. Trying to read their documentations is painful too
Actually, /u/AndrewPardoe would probably know what's going on with this.
So what's the harm of an additional flush that `endl` causes, since `\n` has already flushed the stream and the additional flush has therefore nothing to flush?
fmtlib is awesome, no need to consider anything else IMO. It's fast, effective, safe, easy-to-use, and has a good chance of being standardized in C++20.
We really need compile time reflection support very soon !
&gt; Languages change all the time and even if they would teach modern C++, it will probably be obsolete by C++30 or so anyway... Hmmm.... I wonder what would be the correct course of action for a university to take to ensure they stay relevant... I guess it's just an impossible situation for them, and we should all thank them for what they're able to accomplish being stuck with 30 year old technologies. I hadn't really thought of it that way. Must be a tough situation.
Another roundtrip through the C runtime library, including stdout's mutex lock/unlock. Not as big of a deal as with file I/O, true.
That's all written in C. Saying that string concatenation is difficult in C is very different than saying it is a difficult problem in general. The use of a '\0' as a string terminator is closer to a simplistic encoding format than a data structure. 
That showing up on the video's preview frame was the whole reason I watched it; the actual operator was kind of a disappointment after that
Oh my god, can't wait to try this on our code base. Been annoyed there's no good way of profiling compilation for a long time now. 
Instead of using ```cpp DEFER ( [&amp;] { std::cout &lt;&lt; duration_cast&lt;milliseconds&gt;(clk::now()-t0).count() &lt;&lt; '\n'; } ); ``` couldn't you just use ```cpp auto _ = std::ScopeGuard( [&amp;] { std::cout &lt;&lt; duration_cast&lt;milliseconds&gt;(clk::now()-t0).count() &lt;&lt; '\n'; } ); ``` ? 
Is there an optional flag that makes it a good compiler?
I mean if you really want to overengineer this, you should just make a recursive function template that writes directly into the resulting std::string, and not create the intermediate buffers for digits. Just extend the result string by 32 chars, use it as scratch space and then resize as needed.
`/permissive-` ??
Does that give it feature parity with the good compilers?
Do _you_ have an optional flag that makes you write constructive comments?
This is going to take me a while to get through. As an aside, allowing submissions in Markdown is a welcome step, but I wonder if it would be possible to run such papers through a Markdown-&gt;HTML converter prior to putting the links up online. For example, I find [this paper](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0784r0.md) quite hard to read in its "raw" form.
From Rust experience, an issue that crops up is iterating via pointers. On 0-sized elements, it just doesn't work, since you add `0` at each stage...
The latest versions of VS 2017 are actually pretty good in that respect, but in some ways [yes](https://blogs.msdn.microsoft.com/vcblog/2017/09/11/two-phase-name-lookup-support-comes-to-msvc/)!
I'm going through some of them. * Make the "module" keyword context sensitive : +1 * Bloomberg complains that there is no good way to migrate a 250K files code base from headers to module. I guess they are not wrong but, I mean, I feel like the best place to solve that is at the version control/tool/process level. * Apple complains that modules are not extensible. I agree. However, if you try to make them open-ended, I guess the proposal will be delayed by another decade. * Modules and macros, lots of comments about that, for obvious reasons. but, personally, I really don't see a better solution than having all your exported macro in an isolated header, along your modules. Side note : if you are a Qt user, I would strongly suggest you use Q_SIGNALS and Q_SLOTS over signals and slots. * [Module Names](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0778r0.pdf). What if c++ was a build system too ? * [Monades](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0650r1.pdf) What if c++ was a meta monoid ? * [Lambda functions with a name](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0839r0.html). Why not, it's C++. Please cast your vote using the following expression `auto yes = []no(){};` * [reflection for functions](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0670r1.html). This should be a thing, yes, please. * Features macros : pretty please. * concurrent queue and flat_map would be welcomed additions. * [An Adjective Syntax for Concepts](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0807r0.html) . "Does template &lt;Foo X&gt; have an unconstrained non-type parameter or a constrained (type or template) parameter?" Who the heck cares ? The compiler knows, the dev either know or don't care and the tools know. What are you actually trying to do, for me ? Because I don't care that c++ resembles the English language, but I do care about typing less, more readable code. * [Metaclasses](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0707r2.pdf) C++17.06, right ? * [Modern main signature](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0781r0.html). I'm very much in favor of that, but maybe either provide an alias or a wrapper class so that Teacher don't have to explain template syntax day 1. * [operator try()](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0779r0.pdf] This actually proposes to add a new kind of macro with rust/d semantic like it's nothing. I would say they need to speak to Herb. Did I miss anything ?
&gt; [Modern main signature](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0781r0.html). I'm very much in favor of that Ooooooh, nice. When I got to `int main(const some_container&lt;const some_string_type&gt; args)` I immediately thought "well you can't use std::vector or std::string, that would be horribly wasteful in allocations - how about std::initializer_list&lt;std::string_view&gt;? Minimal number of allocations, and may even be implementable with no allocations" - and then I read "The author of this paper suggests std::initializer_list&lt;std::string_view&gt;" and decide I like this proposal a lot :)
Or OCaml's Option, or Rust's, or F#'s. Or Java's Optional. Pretty much all languages that allow functional style to some degree have something like this.
&gt; Who the heck cares ? The compiler knows, the dev either know or don't care and the tools know. The human reader?
Those are good, but if I had to keep one, it'd be `/d1PP`.
What do you want to know? It's an undocumented compiler back-end option, as indicated by the `/d2` prefix. (The compiler front-ends are `c1.dll` and `c1xx.dll`, and their undocumented options are `/d1` prefixed; the backend is `c2.dll` and it uses the `/d2` prefix. The backend is also known as UTC internally.)
&gt; I really think that it's a good idea. C++ should become "imperative hybrid compile-time run-time lisp" imho. I agree, but that is not enough... I want the language to enable us to control codegen. I mean it should be meta so x64 or ARM should not be baked in the language, it should be purely library built on top of language support. 
Oh, Catch is generic capture class, you need to choose from these: * OneCatch&lt;id, SubRE...&gt; * StaticCatch&lt;id, size, SubRE...&gt; * DynamicCatch&lt;id, SubRE&gt; OneCatch is StaticCache of size 1, which will only capture first pass. StaticCatch capture max `size` passes. DynamicCatch is using std::vector internally to catch all passes.
yes, the `_` is what I generally do but I really dislike it :p 
I find it's proposed that mathmetical constants go into `std::math_constants` namespace in P0631R1. Makes it a bit too long to type in math-heavy code, so we'll probably end up with `using namespace std::math_constants;` in a lot of code - even the example in the paper uses it. And due to that I wouldn't be able to be sure if the constants come from the standard library or somewhere else. I don't think the `using namespace std;` argument is very strong. People using it *should* know the horrible problems that come with it. If the proposal goes through as it is right now, then this will probably wind up as the `using namespace std;` of any math code.
*(author of the blog post...)* gief us moar undocumented back-end options plz, kthxbye :)
Very nice, kind of what the standard version should have been.
So... One would imagine that `map` in its simplest form is implemented as: auto map(auto func) { return has_value() ? func(value()) : nullopt_t; } However, because this is C++, there are 16 (!) different versions of map in that file (none of them implemented in terms of each other) and they look like this: template &lt;class F, detail::disable_if_optional&lt;F&gt; * = nullptr, detail::disable_if_ret_void&lt;F, T &amp;&gt; * = nullptr&gt; detail::get_map_return&lt;F, T &amp;&gt; map(F &amp;&amp;f) &amp; noexcept(noexcept(detail::invoke(std::forward&lt;F&gt;(f), std::declval&lt;T &amp;&gt;()))) { using result = detail::get_map_return&lt;F, T &amp;&gt;; return has_value() ? detail::invoke(std::forward&lt;F&gt;(f), **this) : result(nullopt); } Here is a breakdown of all the versions: * when `this` is a non-const`lvalue` * `func` is not an optional, and it doesn't return `void`. (+1) * `func` is an optional, and it doesn't return `void` (+1) * `func` is an optional, and it returns `void` (+1) * `func` is not an optional, and it returns `void` (+1) * when `this` is an rvalue (+4 and the value is now `std::move`ed) * when `this` is a `const lvalue` (+4) * the next 4 are guarded by a macro `TL_OPTIONAL_NO_CONSTRR` (presumably const rvalue). I repeat none of them is implemented in terms of each other. btw, I am not complaining. I write C++ applications for a living, but writing C++ libraries is a different level of masochism. 
Does he really ? In the current C++ standard, `void f(Foo &amp; foo);` is a function where foo can be: * a class * a class derived from another class or set of classes, which may be templated * an alias for a native type, a function, a class, a template instantiation The reader can either not care or refer to the definition of "Foo". The sensible things is to use an ide or an indexer, but I guess grep works too. I really don't see the point of having all kind of warning sign decorating the function `!!!!!&gt;&gt; This method use the c++20 concept feature &lt;&lt;!!!!!!`
Well, I am glad that we have the possibility to implement this as a library feature. I'm sure this could end up in the STL someday.
IMHO, this implementation can be improved a lot. First, dealing with null returns is straightforward, you write: template &lt;class T&gt; constexpr auto void_or_nullopt() { if constexpr (std::is_same_v&lt;T, void&gt;) return; else return std::null_opt; } And then: return has_value() ? ... : void_or_nullopt&lt;result&gt;(); If you don't have 17 then adjust appropriately, but even so this prevents this logic by being multiplied by cv and value category qualifications. And you can do something similar for invoke; `detail::invoke` should be the one to handle the case where `F` is an optional. Once you push these details down instead of let them bubble up, you are down to your standard const &amp;, &amp;&amp;, and &amp; overloads. As for const&amp;&amp; overloads, their utility is incredibly low, even for library code. I'd be curious for the author to explain why `func`'s being optional (which, honestly is a dubious use case unless you're getting too functional for your own good; C++ isn't Haskell) and `void` returns are handled this way, maybe there's something I missed.
Important one that has all changes between c++ 14 to c++ 17: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0636r2.html
I like the idea, and in fact I'm suggested the same myself a couple of times on Reddit. My worry is that `std::initializer_list&lt;std::string_view&gt;` is a lot to type, and moves beginners from wondering what pointers are to wondering what templates are. I wonder if it would be better to have a dedicated `std::command_line_args` type instead. This would be a magic type that only the compiler could create (like `initializer_list`), and would be implementation defined, except that: * It must meet the requirements of a random-access range * Its `value_type` must be a random-access range of `char`s, convertible to `std::string_view`. From a user's point of view, this would still allow easy iteration over arguments: int main(std::command_line_args args) { for (std::string_view arg : args) { // do something with arg } } From an implementors point of view, it would allow implementing `command_line_args` as something like a GSL's `span` over the existing `argc` and `argv`, without needing to calculate the length of each argument until (or unless) it is used. 
Finally!
Please no more camel case. We had enough of it for this year.
Exactly! Great article!
For those who are interested, here's a non intrusive implementation that uses | instead: using std::optional; namespace detail { template &lt;class T&gt; constexpr auto void_or_nullopt() { if constexpr (std::is_same_v&lt;T, void&gt;) return; else return std::optional&lt;T&gt;{}; } template &lt;class T&gt; struct is_optional : std::false_type {}; template &lt;class T&gt; struct is_optional&lt;optional&lt;T&gt;&gt; : std::true_type {}; } template &lt;class T, class F, std::enable_if_t&lt;detail::is_optional&lt;std::decay_t&lt;T&gt;&gt;::value, int&gt; = 0&gt; auto operator|(T&amp;&amp; t, F f) -&gt; decltype(detail::void_or_nullopt&lt;decltype(f(std::forward&lt;T&gt;(t).operator*()))&gt;()) { using return_type = decltype(f(std::forward&lt;T&gt;(t).operator*())); if (t) return f(std::forward&lt;T&gt;(t).operator*()); else return detail::void_or_nullopt&lt;return_type&gt;(); } Example: int main() { optional&lt;int&gt; x(3); x | [] (int x) { return 2*x; } | [] (int x) { std::cerr &lt;&lt; x; }; return 0; } I wrote this in 17 cause I"m lazy but it's trivial to change to 11/14. Live example: http://coliru.stacked-crooked.com/a/535ce4e05c69d562.
&gt; The latest versions of VS 2017 are actually pretty good in that respect "*[A]ctually pretty good*" isn't going to make my deduction guides or fold expressions compile.
One of the existing problems with the C++ syntax is that meanings of expressions change dramatically depending on whether a name refers to a type or a value. Even in C, x * y; is ambiguous unless you know whether `x` is a typedef name or a variable name. C++ (and especially template specialisations) makes this a lot worse: template &lt;int&gt; struct x { template &lt;int&gt; using y = int; }; template &lt;&gt; struct x&lt;4&gt; { static constexpr int y = 0; }; auto z = x&lt;sizeof(void*)&gt;::y&lt;4&gt;(0); If you've ever wondered why writing a C++ parser is so hard, consider that the expression initialising `z` has two *entirely different parses* on 32- and 64-bit systems. And then consider that instead of `sizeof(void*)`, the template argument could be call to any arbitrary `constexpr` function... This matters because programmers have to effectively function as "human parsers" when reading code. C++ already requires carrying around a huge amount of "mental state" when we are reading. Allowing more multiple meanings for the same syntax, depending on what kind of entity a name refers to (type, value, concept...?) would only make this worse. There are only so many different ways we can write things (ALL_CAPS, CamelCase, snake_case) to try to make the distinction clearer to the reader. (That said, I'm generally in favour of the "terse syntax" for constrained function templates. But I imagine I might think differently if I wrote both types and concepts in CamelCase...) 
That does sound sane - I hope this goes somewhere!
Ok, next version will be snake_case.
Have fun making your code unreadable as fuck.
&gt; First, it does not have a random access iterator. This case is believed to be fairly minor, since the ordering of parameters is typically more important than ordinal location TIL! And this is a strict no. I wonder how the author came up with the `typically more important` part. &gt; Again, if the alternate is completely manditory, the exisitng signature is still available I feel that any proposal which relies on "we can always go back" puts itself on a very very slippery slope. 
Better than getting fucked by a `default` clause in a switch.
Can you add a bit more info on the output? What's the criteria for an "anomalistic compile time"? What's the "most hits" list? Can any of the stuff in RdrReadProc help narrow down the reason for a slower compilation?
&gt; it does not have a random access iterator I have no idea why the author wrote that. `initializer_list&lt;T&gt;` is backed by an array of `T const`. Its iterator is a `T const*`. 
This is huge. My team has been dealing with an ever increasing code base that is taking way to long to compile. We've tried unity builds, forward declarations, etc. Nothing seemed to work. This may be exactly what we need to figure out what's going on. Thank you.
You might want to try the [Markdown Viewer](https://github.com/simov/markdown-viewer) plugin for Chrome, it does a good job of rendering pages like that. (You may need to go into its options page and check "Allow all", I think it only works on local files by default.)
Even if the cleanup is just a single line, it's still worth writing a class that encodes the relationship between the resource and the cleanup. This way the knowledge is encoded in one place and taken care of every single time you use the resource. Contrast that with using a generic RAII wrapper like `scope_guard`: using `scope_guard` means you still have to write that single line of cleanup over, and over, and over. `scope_guard` should mostly be used only when it's doing something that's not reusable.
fold expressions are in last weeks' 2017 15.5 preview1 if memory serves... Haven't done that myself, though, so can't test it.
this one led me to code that compiled fine with gcc &amp; clang, but didn't with msvc... It's almost shocking to have a ms compiler be the strictest out of of those three, but there you go...
Good debut presentation by a talented engineer. The library is tightly focused on solving one specific problem, the application of regular-expressions at compile time. This is a worthy problem and a valuable addition to our knowledge and tools.
He does know, and he'd refer you to /u/aras_p's blog post as the best available documentation. He would also point out that undocumented switches are subject to change or removal without warning. But what if you wanted this switch to be documented? Should you give !RedditSilver to /u/aras_p? No, there's an easier way. Just ask! Please go to [UserVoice](https://aka.ms/uv-c) and enter a suggestion. Then post that suggestion ID back here--and maybe on /u/aras_p's blog post--and ask people to upvote it. And we'll start a conversation on our team about what it would take to document and support that switch (and other cool ones like it.) Thanks!
Does Rust really have 0-size elements? In C++ the compiler guarantees that two objects that share a type will have different addresses, so despite the ability to compress empty objects through inheritance, a class always has a size of at least 1.
why do you need std::forward in the trailing return type? Just for consistency or?
Is there something similar for clang? It looks quite useful!
Please, no macros exported by module. What's wrong with making a header to define macros? It makes it clear you may have macros when you include. It makes it explicit. It's nice. Want to modularize Qt? Import some modules, and include the `&lt;QObject&gt;` header and you're done!
No, the type that you get from calling a function on rvalue, or lvalue, of the same type, could be different, and often is. I want my return type to be correct so I have to use `std::forward` there too.
Apart from the CamelCase it is a very nice library! 
Very nice! I'd like to share my own ultrasimple approach to chaining optionals, to show that it can be done cleanly and concisely without extensions. First note that short-circuiting and left-to-right evaluation order are guaranteed for non-overloaded &amp;&amp; (C++ standard 1.9.18), then just write, e.g., if (optional&lt;int&gt; x, y; (x = f(3)) &amp;&amp; (y = f(4)) &amp;&amp; (x = g(*x, *y)) &amp;&amp; (x = h(*x))) { // we can use *x } else { // we know that something failed } This uses only bare-basic language features (with the new if statement initializers to scope the return and temporary variables) and is easy to adapt with specific error handling logic, so it's handy in a pinch.
That would be another useful use case for namespaced expressions if that was even a thing: `auto foo = std::(abs(c) + pow(pi, 2));`
If I understand correctly, this is incorrect. GCC landed support for filesystem *in C++17*, which differs substantially from the TS in a few places.
Is there a list of the differences since the TS somewhere?
I don't know of ones off the top of my head; you can look at the list of papers touching it after it got merged into '17. The biggest user-facing changes I know of are: * the "magic dot" was changed to the "magic empty string" when decomposing paths; as a result there are knock-on effects in almost every path decomposition function * `copy` was substantially overhauled * What `absolute` did functionally became `operator/`, what `system_complete` did became `absolute`. 
Thanks!
&gt; This means that, for example, path("a/b/c") / path("C:/foo/bar") is path("C:/foo/bar"), not undefined behavior. Having become more familiar with python's `pathlib` and rust's `Path`, I am glad to see consistency in behavior Interesting to note that this was taken advantage of in the [underhanded rust context's winning solution](http://blog.community.rs/underhanded/2017/09/27/underhanded-results.html). &gt; In case folks weren't aware, "magic dot" was how directories were disambiguated; "a/b/c/" decomposed as {"a", "b", "c", "."} in the TS, but decomposes as {"a", "b", "c", ""} in C++17. There are knock-on effects because, for example, path::has_filename says there is a filename in the TS but not in C++17. Interesting. This is an area where things diverge. Python will treat `a/b/c/` the same as `a/b/c` and does not track the intent of the trailing item being a directory (unsure of Rust) . I think this is reasonable considering that in`a/b/c`, it is unclear if `c` is a file or directory. ```python pathlib.Path("hello/world/") == pathlib.Path("hello/world") ``` See also https://github.com/python/cpython/blob/6f0eb93183519024cb360162bdd81b9faec97ba6/Lib/pathlib.py#L51 I assume they don't treat `path("")` the same as the hidden `""`?
Ah C++ abominations... :) BTW (TIL thanks to your void_or_nullopt) that I can do[return void;](https://stackoverflow.com/a/2249281/700825);
&gt; I think this is reasonable considering that ina/b/c, it is unclear if c is a file or directory. Unfortunately on POSIX the trailing / is significant, because if the last path element is a symlink, the trailing / says to follow the symlink, and no trailing / refers to the symlink itself. &gt;I assume they don't treat path("") the same as the hidden ""? The extra `""` is only visible in decomposition. It is the same as `path("")`.
&gt; pathlib.Path("hello/world/") == pathlib.Path("hello/world") Not in C++. That gives the wrong answer on POSIX systems. 
For those who are new here: are all CppCon talks 5 minute length? Or it's like some kind of fast-talk part of the conference? 
Billy is correct. I added support for the Filesystem TS *years* ago. What I committed today is the C++17 support.
I probably missunderstood the focus of your original post: No, I don't think interoperability between different encodings is the primary goal of this library. Just as I wouldn't widly mix different specializations of std::basic_string in my code, I wouldn't mix different specializations of `CsBasicString`. I think the easy interop ability is just a side effect of their decision to provide a unified interface that provides code points. That said, I totally agree that the library lacks a lot of optimizations - in particular for the case where two strings with the same encoding are involved - but I don't think that is a general design issue, but rather a question of how much time the authors can spend on it and how important any performance benefits are to him. 
Those are the main things. Also new functions for finding relative paths, string view interoperability, new `fstream` constructors (not in GCC's lib yet), and directory entry caching (for performance when iterating over directories).
First post!
Eh?
Wow, even by phoronix standards that post is bad. &gt; The GNU Compiler Collection (GCC) today merged their initial support for the C++17 file-system technical specification! No, not for the Technical Specification. &gt; ISO/IEC TS 18822:2015 defines a set of standard interfaces for C++ for dealing with file-system operations. This TS, which is based upon the Boost library's file-system implementation, is optional in C++17 and is now supported by the GCC C++ compiler. No it's not optional. This is different to the TS (which GCC has supported for years). &gt; The C++ file-system library introduces the concept of paths, directory entries, file information, easy copying of files, symlink handling, file renaming, checking of file types, and other easy helpers. A paragraph without any false statements! &gt;As of today in GCC is the initial file-system library support. Why "initial"? &gt; Additionally, the library bits have also been merged into libstdc++. Eh? There are only library bits. They aren't additional. &gt; More on the technical specification via open-std.org and cppreference.com. LLVM's Clang compiler has supported this file-system TS since Clang 3.9. No, that's the TS, Clang does not support the C++17 version yet. 
What's that?
Optional for standalone vs hosted?
OK, it's not required for freestanding implementations, but that's not what phoronix is talking about. They're just confused about the difference between the TS (which is not required for conformance to the standard, i.e. optional) and stuff that is part of C++17.
Most of the CppCon talks posted here are about an hour long. This is one of the "lightning talks".
There's been some pushback on any use of nested namespaces lately, so I expect change here.
Hasn't GCC always been ripe with compiler extensions and less than standard code?
For a brief moment there, I was excited. I wonder when libstdc++ will support standard `&lt;filesystem&gt;` header, without having to explicitly link `-llibstdc++fs`.
Dunno. It's seemed more strict and correct than msvc for many, many years, but your experience may be longer than mine....
In this mailing even! [No More Nested Namespaces in Library Design](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0816r0.pdf)
Comment distribution here is so bizarre. &gt; Here's a UDL that does optimal regex parsing for you! &gt; Terrible library. CamelCase is shit. Can't please everybody I guess... 
When it's stable enough that I'm willing to commit to exporting the symbols from the libstdc++.so shared library forever and ever. While C++17 support is experimental and still evolving, I'm not willing to guarantee to export those same symbols forever. The separate static library means nobody's binary depends on symbols from the shared lib for `&lt;filesystem&gt;` features. Why is needing to use an extra option such a big deal? You already need to use `-std=gnu++17` or `-std=c++17`. If you don't want to use the extra option then wait for it to be stable, let early adopters experiment with it.
Uh that's fine `&lt;filesystem&gt;` can wait. I'm more curious about `std::make_array` which lies under `&lt;experimental/array&gt;` header. Is it close to stable yet? If so, why not take it out of the experimental header?
After watching this, I realized how limited my knowledge of C++ is.
AFAIK `make_array()` was only ever part of the Library Fundamentals TS. It didn't make it in to C++17 because template deduction guides for constructors rendered it needless.
I'm constantly amazed that I am learning new things, even C++11 which is 6 years old, I just discovered &lt;valarray&gt;... Anything in particular that you feel you should know better?
Exactly right. It's in `&lt;experimental/array&gt;` because the Library Fundamentals TS says it should be there. It's not in `&lt;array&gt;` because the C++ standard says it's not in there.
I have to disagree with this. Putting everything in one namespace is how you end up with [`std::remove`](http://en.cppreference.com/w/cpp/algorithm/remove) vs [`std::remove`](http://en.cppreference.com/w/cpp/io/c/remove) and [`std::move`](http://en.cppreference.com/w/cpp/utility/move) vs [`std::move`](http://en.cppreference.com/w/cpp/algorithm/move). I find it very strange that that paper doesn't even *mention* namespace aliases: it's really not that hard to put namespace fs = std::filesystem; at the top of your source file to avoid a lot of typing, but still qualify everything with `fs::` so as not to be subject to the whims of ADL.
Where I work, we are fighting to be allowed to use C++11. Upto a year ago, I would get comments on pull requests "This is not backwards compatible". Full use of C++11 is not there yet. I don't think any projects are on C++14. 
&gt; /d1PP something related to preprocessor?
This was very informative, thanks! During the questions you mentioned pair’ piecewise forwarding constructor. Every time I’ve tried to do something similar for my own types I struggle with SFINAE / template errors. I was wondering with c++ 14 or c++17 if I can just inherit from std::pair and throw some using pair::pair to bring in the constructors. Will that work? Any pitfalls?
You can always do `namespace math = std::math_constants;`.
Thanks for taking time to answer :)
This was an incredible talk. I learnt so much and Vinnie was a great speaker!
Thanks for the kind words! Typically, it is not a good idea to derive from standard library classes. I know what you mean about struggling with the template errors. I struggled myself to get the declarations right (you can see them at https://github.com/boostorg/beast/blob/860b764ba4ea1caa7649b3d8bbbdf88bf24d5ef6/include/boost/beast/http/message.hpp#L847). The best advice I can give is to try to find someone who knows these things really well and have them mentor you. I got a lot of assistance from Howard Hinnant, Peter Dimov, and a few other kind folks on the cpplang Slack. There is really no shortcut here, to be truly effective you need to find a source for good information and learn the best that you can to understand how these things work.
The difference between C++03 and C++11 is really huge. It was said by someone that C++11 represents the biggest leap forward / paradigm shift in the language's history and I agree with that. You aren't missing much from C++14 or c++17 but you do lose a lot without access to C++11. Beast targets C++11 because I don't want folks who are stuck with older toolchains to be left out. The last survey I looked at still shows C++11 as having the highest percentage of users.
Thank you for the kind words
I am very surprised to see this calm response, given my lack of explanation and slightly provocative tone. Thank you. Here a link, in case you need to justify it to others: https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#Rl-camel
`/d1PP` preserves macro definitions when preprocessing with `/P` and is extraordinarily useful for investigating macro mischief.
It would be interesting to see https://github.com/google/re2 in that list.
/permissive- is about disabling MSVCisms. It's orthogonal to our feature work. But we are planning on full conformance soon. 
"Second mover advantage" comes into play here. The fact that MSVC just recently implemented some of these features or fixed some of these bugs means we looked at the language specification after the feature was well understood. A new implementation acts as a check on existing ones. And yes, GCC is full of GCCisms. But they are generally well-controlled with options. 
I would like to kindly ask you you to [unlicense](http://unlicense.org/) it. As far as I am concerned, the scope of the project implies that it's not licensable in the first place, but providing that statement would put some managers at rest. P.S.: It was very helpful to me, thanks!
In this case, it's really not. It's not because names are unique and whether the parameter of a function is a concrete type or not, the compiler logic to resolve the type of that parameter is basically unchanged. Also, putting all sort of syntactic decorators on the function signature does not help with call site ambiguities.
Meanwhile how many random npm packages are your front-end dev pulling from git ?
Go explain to beginners why they sometimes need to use import and sometimes include when they want to use a lib.
&gt; operator try() This actually low-key proposes to add a new kind of macro with rust/d-like semantic. I would say they need to speak to Herb. Herb's proposals work at the macro-scale, the C++ macro functions proposed in that paper work at the micro-scale. There is some overlap, of course, but the scope is opposite. And for the record, **any** other better solution to injecting boilerplate other than (a) C macros (b) adding new keywords with expand into boilerplate via the EWG is really what the paper is asking for.
I disagree, depending on your design you might decide not to include a default case and then get a warning/error for not handleing some cases.
Great talk! Your enthusiasm and way of explaining things is super exciting for me. Whenever .66 comes out I am going to have a look at Boost.Beast and remember this great talk. Keep it up my man. Would love to see more from you.. especially if you make it as interesting as that!
I totally agree on principle, I was just not expected such a big solution to a very limited problem. The discussion about macro need to happen, and rust/D are definitely on the right path. But there is no reason for macros and meta-classes not to share some common syntactic constructs. We also need a good story for platform specific code, who need to be live in the AST and visible by tooling but ignored by the compiler, including being able to recover from parsing errors. It's a lot of work. If this paper starts a discussion, I would be thrilled. 
Beginners have no problem dealing with that in languages supporting modules and file imports, for example Turbo Pascal since version 4.0. I really don't get the macro love when we have better mechanisms in C++17. 
Go explain to beginners why they sometime need to use `&lt;...&gt;` and sometimes `"..."` when they want to use a lib. Not something I'm proud of the language right now, but I think it's possible because we are doing it right now. But for module vs include, I'll explain that by teaching them (after teaching them about module) that there's a preprocessor that run before the compiler. That it's not really part of the language, but rather look like a language of it's own. Then I'll explain that you can transform the current source file by appending another file in it, and do textual replacement in the code using macros. Then it's really easy to explain why they are not usable with imports. Because macro only exist in the context of the preprocessor. Not while compiling. The compiler don't even know about macros. Then now it's easy to understand why you should include macros from libraries. But that for sure, I'd rather teach with a library that avoid macro altogether. Modern libraries rarely need the user to use their macros, and when they do, it's usually for reflection, which we might also get on c++20. Also, exporting macro would require preprocessor tokens to be language entity. The export keyword would need to recognise a macro and export it. It would require the **compiler**, not the preprocessor to know about macros. So the preprocessed file would still need to be preprocessed, because macro would still be there for the compiler to read them. This is illogical and would just break the language as we know it. For clang folks, it's easy to do because their module implementation is basically precompiled header with the module ts syntax. They based their whole module-ts machinery on the precompiled header code. GCC on the other hand uses as much as possible their logic from their LTO code. Macros are not representable in that form, or really hard to do, and patchy. All this because some don't want to use the processor... In order to use the processor?
I can't speak for everyone, but I'm supposed to run my code on an Ubuntu LTS version - and right now, that still includes 14.04, so that means gcc 4.8.4 or clang 3.9. It's nice that there are cutting-edge versions of gcc and clang out there, but we cannot use them unless they become part of a distribution baseline. The Windows-only code we write is using more modern C++ features than the code shared with Linux, despite the incessant complaining of some people about Visual Studio being less advanced than their Linux counterparts... 
[removed]
Your comment has been automatically removed because it appears to contain profanity or racial slurs. Please be respectful of your fellow redditors. If you think your post should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Vulgar%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/784jzg/cppcon_2017_matthew_avery_xmacro_how_to_avoid/dot4jja/?context=3.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Sure! The `func` returning `void` is an unfortunate problem due to `void` not being a regular type. You can't have a `std::optional&lt;void&gt;`, because you can't store a `void` object, and you can't use a `void` return to construct something. As such, I need to evaluate the function for side-effects, then fixup the return to `tl::optional&lt;tl::monostate&gt;`. This allows users to `map` side-effect-only functions, and preserve whether or not the original `optional` had a value. The optional func is indeed a case of me getting too functional for my own good. This implementation was originally to support [p0798r0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0798r0.html), and the first version of that proposal supported optional callable arguments, since it made the type an applicative functor as well as a functor and monad. Eventually I decided that, while the consistency was nice, there weren't any real use-cases I could think of, so I removed it from the proposal. It's still in the implementation just in case someone finds a use for it, and because it'd take more effort to remove :D
&gt; Go explain to beginners why they sometime need to use &lt;...&gt; and sometimes "..." when they want to use a lib. I'm personally advocating the banishing of `"..."` to a lower plane of existence. Just always use &lt;&gt;, from the root of your lib and add it to the `-I`. This way the day modules come we can just do `s/\//./g` and `s/#include &lt;/import/` and `s/.hpp&gt;/;/` with sane libs, to get e.g. `#include &lt;whateverlib/bananas/stuff.hpp&gt;` into `import whateverlib.bananas.stuff;`
Great talk Vinnie! Both informative and entertaining - keep it up! :-)
OK, this means that at least one of your slides is wrong. I've got both StaticCatch and DynamicCatch working now, but I have raised an issue in github for DynamicCatch on the way you calculate the pointers for begin and end.
They were supposed to be included, [but they don't work](https://developercommunity.visualstudio.com/content/problem/131610/c17-fold-expressions-not-working-in-visual-studio.html).
Yes, breadth first search using this technique would be the killer blow I think (Hana mentions on the github page that depth first search has it's caveats). Honestly though, I'll take depth first search at this stage. We use std::regex at work anyway...
The really promising thing here is the use of variadic templates (and maybe constexpr?) to cut down on compilation times. And of course being able to compose regexes from strings just leaves other solutions in the dust.
I see, my bad. I didn't expected I will have the talk so I prepare slides only few hours before the talk on the other talks.
My question was also on the fact that I used the generic ScopeGuard instead of the specific DEFER macro.
What's the code?
I completely agree, I think it was one (if not) the most entertaining talk given in CppCon this year! The content is very informative and useful, and the enthusiasm of the speaker make the whole talk very enjoyable Highly recommended
Fair enough. I added the [suggestion on UserVoice](https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/31999147-document-d2cgsummary-and-other-internal-compilati). I couldn't link to either this thread or Aras' blog post because the spam filter is removing my suggestion after a few seconds. Same thing with adding a comment to my suggestion.
So much useful information, and with very succinct delivery. Great talk. 
This stats being a matter of philosophy. By that standard the op's defer-like keyword does exactly what you want then. I personally don't like that. You can even use a one-line macro (like in the op) to prevent duplication of that one line, but you don't need to write n classes just because you want to use it with n 'resources'. At some granularity or becomes a matter of taste.
Link to UserVoice item added at bottom of the blog post.
but that's exactly what the DEFER macro does, except it saves a name.
thanks
Oh, I didn't know that, thank you.
Well we keep ducking the issue of replacing preprocessor macros with something better. And I'm getting annoyed by it. So sure, the operator try paper proposes something very big which would solve not just it, but co_await and everything like it. As covered in the paper, the syntax ends up not quite as nice as it could be, so something even better surely exists. I'm a libraries not language guy, I'm definitely the wrong person to propose that something better. But macro functions are not terrible, and as you point out, I closely cloned Rust. If it starts a discussion I too would be thrilled.
I just posted that to keep the discussion going ! https://www.reddit.com/r/cpp/comments/78ewm2/the_road_towards_a_macro_free_world/
Wow Vinnie, how many accounts do you have on here? :-) 
I use macros to generate variable names for scoped blocks (for things similar to `std::lock_guard`). It could be solved with some syntax to extend life of temporary until the end of the scope without giving it a name.
Wow, that looks so clean compared to what I see in the std library implementation for MSVC (no offence to those devs - the leading underscores and capitals they put for good reasons just make it so hard to read!!) Hope you don’t mind a follow up question: what do the unnamed integer sequence arguments accomplish?
&gt; are there any other use case for them Backwards compatibility. So even if all current uses could be replaced with new and hopefully superior mechanism, it still would not be possible to get rid of macros. Better get used to them och switch to another language altogether.
You're not alone. One of the compiler that I use at work is gcc 4.4.7 :(
i dont understand the `static inline` function part :( reading your comment it makes either sense to: * declare a free function only `inline`so it has external linkage and everyone is using the same identical function * declare a free function only `static`so it has internal linkage and everyone is using his own function. how does `inline`correspond with static in the same declaration?
&gt; #include and #pragma are probably okay, as long as the actual included header plays nice. They are okay, but it **is** technically possible to replace #includes with modules. 
Compare two error messages: * &gt; Body requirements not met * &gt; 'user_type' has no member named 'value_type'; did you mean 'vaule_type'? Which one do you like better? 
nice and fluent(!) presentation. slides-style and code snippets were really well chosen and colorized. towards the end - where everything got put together - it got a little bit to fast (for me) and i was happy i can pause the video to take a longer look at the code (but tbh, even tho i heard and read about the used techniques in the past i never used them myself activly to write production code. so i may be a little bit slow getting into it). good work!
LMAO so true
Yeah...I went a little bit too fast :( the talk was supposed to go for 50 minutes.
Something like the [P0577R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0577r0.pdf)?
The `index_sequence` provides the list of integers {0, 1, 2, 3, ...} used to extract each tuple element using `std::get` (e.g. `std::get&lt;0&gt;()`, `std::get&lt;1&gt;()`, ...) on the tuple.
I'd hate to be the one who'd have to update the Windows headers to remove all macros, and deal with the C/C++ standards deviating on this.
The proposal you linked (for the try operator) is just another "let's implement the 'maybe monad' in C++" paper, but it tries to present it differently. Please don't take your design hints from it. 
I wished someone created a clean C++ version of the Windows headers without all that macro non-sense. Even a subset would be enough for most people.
Il y a aussi [markdown-viewer-webext](https://addons.mozilla.org/en-US/firefox/addon/markdown-viewer-webext/) pour firefox
Looks very nice! Possibly it also helps to resolve the issue of preserving temporary for ranges (e.g. `for (auto &amp;val : some_modifiying_view (register get_vector ()) )`)
still the other points remain valid :P even if it was fast, the initial problem was introduced very clearly and with the classes growing incrementally you could still follow what's going on and grasp the idea to solve the problems. i enjoyed the whole talk until the end (which is not always the case :D)
You could avoid std::function by using C++17 class template deduction guides
Thank you very much for the kind words!
Read to the end of the paper
/permissive-
I fixed that after giving the talk :-)
_This is reddit, so I don't have to :P_ Seriously, I hadn't noticed the paper addressing this (I had stopped reading about half-way through).
It is called C++/WinRT.
It seems a lot cleaner to write a function that accepts a lambda and executes it. I.e.: template &lt;class M, class F&gt; void with_lock(M&amp; m, F f) { std::lock_guard&lt;M&gt; l(m); f(); }
s/Michael/Roland/ https://youtu.be/QB0oY5f6q-E &lt;-- Correct link
While macros suck, and are a known wart on the language... None of the solutions are here yet, and none of them will be here for a while. You can do amazing, quite maintainable things with BOOST_PP with very few lines of code. It's a tool that should be in every C++ dev's toolbox.
To be fair, those packages are often transpiled to versions that can run on very old browsers. The entire point behind babel is to make modern javascript specs available for development, and then converting it to ES5 (or whatever target you want) and polyfilling missing browser features for the browser.
Doesn't AddressSanitizer do this already (and in a more general manner - catching out of bounds writes to all arrays)?
And if that's too ugly and you don't need to capture anything you can just write: `with_lock(m, &amp;foo);`
Another use-case of macros is for assertions and logging facilities. While use-cases involving macros like `assert` are probably going to be covered by contracts or similar proposals, the logging use-case still requires macros. For example void f(int x) { LOG("In f, value of x is %d", x); // ... } the `LOG` macro would expand to a call to a logging function on debug builds and to `void(0)` on release builds. This allows release build to avoid evaluating potentially costly expressions, which could cause an unnecessary performance hit. On the std-proposal group I had proposed a language extension to allow lazy evaluation of function arguments, so that the `LOG` above could actually be implemented as a function, while still avoiding runtime evaluation of the unneeded expressions.
Did not the phrase "with added native C++ macro functions!" in the paper's title not give it away?
&gt;code that compiled fine with gcc &amp; clang, but didn't with msvc
Ah, nicely done. I somehow never noticed until now that cppreference is a wiki
Thanks! These were the two links I wanted, not any links on UserVoice. 1. I can find the internal bug and push a conversation. 2. Anyone reading this thread or /u/aras_p's post can take a second to upvote this suggestion.
Well most likely you won't have multiple cases of mutex locking/unlocking (including nested usage) in the same function, I'm no expert on that. However with other cases of applying similar pattern I've had that. And having one useless name is probably fine, having several gets annoying very quickly (i.e. people start using name + number for them and I can't blame them). You may argue that everything should be separated into a myriad of small functions but I'm not really sure that this is a way to go in all of the cases.
https://en.wikipedia.org/wiki/X_Macro
I honestly would have trouble imagining how you would have so many of these guard variable type things in the first place. Still, this is still better: struct with_helper { template &lt;class ... T&gt; with_helper(T&amp;&amp; ...) {} }; template &lt;class F&gt; void with(with_helper, F f) { f(); } with({std::lock_guard&lt;std::mutex&gt;(m1), std::lock_guard&lt;std::mutex&gt;(m2)}, [&amp;] () { // do stuff }
&gt; Not yet possible &gt; &gt; Creating types and variables by concatening strings : Some of the reflections proposal as well as the meta classes proposal go a long way. I think this is the wrong mentality. Macros are bad in part because they allow stupid crap like token-pasting to happen -- leading to new and unpredictable other macros to be invoked and new and unpredictable other variables and functions to exist. And before long the programmer doesnt know what code is actually compiling. The goal should not be to duplicate all of the different forms of macro evil within the mainstream language. Then we may simply have to ban the new features as well in sane code bases. Instead what i want is, if joe programmer wants to solve a complicated problem with macro magic and token pasting, he shd be obligated to go write a nicely annotated python script that generates what he wants, and integrates that into the build system. I dont want him to never do this, but i want it to be painful so that he doesnt abuse it. And i want things to be nicer for the person debugging the code than they are for joe. I dont really want joe to say, "look, now i can do it in c++20 with reflexpr and it doesnt violate the coding standards!"
Well they are locked simultaneously in your example. Why not guard lock1; // code guard lock2; // code Maybe some even more complex structure: guard lock1; // code { guard lock2; // code guard lock3; // code } I mean you can fragment functions all you want but then naming functions becomes a problem at some point too. As for usages that's a separate question: you may use them for saving/restoring some states, temporary disabling signals or even pushing/popping things on/off the stack. Basically using such construct is not a big deal for me to separate their usage into distinct function all of the time.
So we have `assert(a &lt; b);` which is if (a &lt; b) {} else assert_failed("a &lt; b", "source.cpp", 123); And `TEST("some") { assert(true); }` which is void unique_name_123(); bool unique_name_123_ = add_test("some", &amp;unique_name_123); void unique_name_123() { assert(true); } How do you do that without macros?
Hunter.sh is another growing C++ dependency manager - cmake based. I like it's simple straightforward design.
That's a very good point of which I hadn't think of. It gets complicated. I guess you would need something to declare a type-safe macro whose arguments are valid, un-evaluated expressions that gets evaluated within the body of the macro. template&lt;expression.... Exprs&gt; void LOG#(Exprs.... exprs) { if constexpr(compiler.debug) { -&gt; Logger.log(evaluate(expressions)...) } } where expression would be an AST node representing either a variable, or a function call ( including new statements and constructor call ) Or some other syntax doing the same thing. Is that something we should have ? I don't know... 
Well, once we have modules, you could do `namespace m = std::math_constants;` in your interface without any problem.
There doesn't exist any language called C/C++. An as expected the example tells us why strcpy is bad. That doesn't matter, I use std::string. And that's an example how to harden an C++ programm.
Nothing is named in my example; you have `with` which is always the same name over and over, and you pass in lambdas which are anonymous. Littering your code with macros to "solve" these things which aren't even really issues in the first place, and have plenty of alternative solutions, is kinda gross, to be blunt.
Thankfully, Rust does not offer such a (dubious?) guarantee, and therefore it really features 0 size objects. Examples of 0 size types: - `()`: the empty tuple, and any "unit" type, - functions and non-capturing lambdas (which cannot be named), - ... Anytime there's no state, the type has size 0. This requires some adaptation of the code, for example a `Vec&lt;()&gt;` is nothing but a glorified counter: it knows its size and will panic if accessed out of bounds, but will not allocate on the heap (pointless). It's really cool though to get "pair compression" and "struct compression" for free out of the box, it makes using stateless behavior generic parameter truly free without any wizardry!
Yes, it's roughly the same. Except the Visual Studio extension also adds some clang compatibility flags so that clang knows that this compilation database comes from a Visual Studio project :)
I think the assert macro could be replaced with a function that uses something like if constexpr (has_variable&lt;compiler_options&gt;("NDEBUG")) { } to check if the function should perform the check or if it should do nothing. The function call to an empty function will still exist in NDEBUG builds, but this should be easily removed by the optimizer. The bonus is that you won't get unused variable warnings in NDEBUG for variables that you only use in calls to `assert()`. The downside is that if the compiler can't prove there are no side effects, the argument expressions to `assert()` will not be optimized away nicely every time.
Ok so it goes to lambda nesting then. How much lambda nesting is really ok? Personally 3-level scope in function for me looks ok, but 3-times nested lambda, I'm not that sure. The other problem I see is that if user isn't aware of the pattern he may not suspect lambda is actually called in this case and may be surprised. Personally I don't feel bad for using macro instead of missing language feature, you may as well hold other opinion.
I might be missing something, but why not just do: template&lt;typename SdlType&gt; struct sdl_deleter; template&lt;&gt; struct sdl_deleter&lt;SDL_Window&gt; { void operator()(SDL_Window* win) { SDL_DestroyWindow(win); } }; template&lt;typename SdlType&gt; using sdl_ptr = std::unique_ptr&lt;SdlType, sdl_deleter&lt;SdlType&gt;&gt;; 
I don't belong to this ideological school: "if you are true C++ programmer, you must hate macros" 
AdressSanitizer has a big runtime overhead compared to stack smashing protection via guard variable which is nearly free
I always have that problem giving presentations. I've always found it helped to practice with a timer to get a sense of when everything should happen and then put my cell phone on the podium running a timer during the talk.
You can get the same class of errors in C++ too, for example when writing over the end of an collection
It’s much simpler than that. In my proposal, when the compiler sees a “lazy” parameter, it silently passes a &amp;-capturing lambda producing the value instead of the value itself. Those kind of lambdas can always be passed as a couple of pointers (remember `reference_closure`?), so the function need not even be inline. The function then invokes the lambda only if needed. In case the function is inlined, I expect the compiler to be smart enough to optimize away a lambda if it detects that it is never invoked.
Asserts are better covered by the contracts proposal. I wouldn’t bother eliminating them, although my proposal would easily allow that among other things. 
You forgot about `offsetof`! There really should be a constexpr equivalent, that also work with pointer to members and is aware of Inheritance. Something like `offsetof_in&lt;Derived&gt;(&amp;Base::member)`
Thanks
Can you provide a link to the std-proposal topic ?
If you want, you can easily generalize so that nesting is not necessary (any more than introducing new scopes would be, which is still a form of nesting): with(guard(m1), [] { // some code with(guard(m2), [] { // more code }, guard(m3), [] { })); Etc. Still better than macros which aren't namespaced, expand to surprising and opaque things, can cause expressions to evaluate more times than expected, can often break when commas are introduced in surprising places, etc etc. But honestly, whether via `with` and lambda, or macros, introducing this much complexity just to avoid giving variables names, which a large fraction of the time would actually be useful, is just a bad idea. Even in the use cases you listed, `restore_foo_state`, `pop_from_stack`, `guard_foo` are all perfectly fine names for these things. If you don't have a good name, call it something short, and it won't matter anyway.
Eric Brumer here, a dev lead on the C++ code generator team. We do have plenty of /d2 switches. Some of them collect data that's too useful to leave undocumented... such as /d2cgsummary. I am working on a blog post (possibly multiple posts) to go over compile-time measurements, documented and undocumented switches, and improvements that the MSVC team has made over the past few releases. Spoiler alert: /d2wpasummary has a detailed breakdown of the time spent in the whole-program-optimization phase of the compiler. This can be useful if you compile with link-time-code-generation (aka LTCG, the default with Release mode projects in VS). /d2cgsummary gives lots of useful information, but it is specific to the backend (c2.dll). As /u/aras_p points out in his post referencing the /Bt switch, the backend may not be the dominant part of compile time. It can also be tricky to make sense of the data and take appropriate action because the backend &amp; optimizer are multithreaded. The format of the output will change in future releases to make it easier to understand and take action. To answer some specific questions from the comments: * We compute the average codegen time per function. The anomalies are the [decorated names](https://docs.microsoft.com/en-us/cpp/build/reference/decorated-names) of functions taking longer than avg + 2*stddev. * The "Reader Cache" is a compile-time saving feature which showed up in Visual Studio 2017 15.3. It saves compile-time when inlining the same function multiple times. We'll get into what this technology is in a future blog post, and go over the output.
First of all, your `unique_ptr` now potentially has performance overhead, because the type of the deleter is a function pointer. So it's no longer trivial for the compiler to inline the deleter call. But, this is also overly complicated, and has worse properties than much simpler solutions, because it's encouraging you to centralize all of the cases in one place. Why bother, exactly? How about this instead: write a small helper function that handles the type inference part. template &lt;class T, class D&gt; auto make_unique_deleter(T * p, D d) { return std::unique_ptr&lt;T, D&gt;(p, d); } And then just write overloads for each type you want to handle: auto make_unique_wrapper(SDL_Window* p) { return make_unique_deleter(p, [] (auto x) { SDL_DestroyWindow(x); }); } This is simpler than the original solution, faster, and not as intrusive. If you want a convenient alias to deal with the lambda thing (for most applications you can probably just use auto, but if you want a vector of these things or something), you can replace the `Ptr` class with a simple `Ptr` alias: template &lt;class T&gt; using Ptr = decltype(make_unique_wrapper(std::declval&lt;T*&gt;())); 
This leads to everyone's code having a different abbreviation. Perhaps there could be standardized abbreviations? Probably a terrible idea, but it would solve the problem.
&gt; Apple complains that modules are not extensible Extensibility is a major feature lacking in C++ right now, IMO. I'd love to see extensibly modules (but standardize the existing proposal first, assuming it leaves room for such a thing) as well as unified function call syntax (or even operator. which, as I understand, allows for a weird, limited version of UFCS).
Well you nest second `with` inside lambda, that's what I was talking about. I understand that macros are no good that's why my uses of them are very local and simple and often can be superseded with simple language constructs. I mean we may as well channel this discussion into "do you consider proposal [P0577R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0577r0.pdf) to be bad, unc++like or unnecessary?" and if not I don't care because people can as well avoid using this feature if they want and write such code your way. Well calling destructor and constructor-like functions was considered to be explicit and good for documentation purposes before too. Eventually no one takes the ability to name such variables if you wish to do so even if the above proposal would make it to the standard somehow.
This is excellent information, thanks!
 //static std::uniform_int_distribution&lt;int&gt; distri; // note: uniform_int_distribution doesn't support ([[un]signed] char types)! //using parm_t = typename decltype(distri)::param_type; static std::uniform_int_distribution&lt;&gt; distri; // note: uniform_int_distribution doesn't support ([[un]signed] char types)! using parm_t = std::uniform_int_distribution&lt;&gt;::param_typ e;
the commented-out stuff didn't compile w g++ and clang
I use begin/end and algorithms. And that's an example how to harden an C++ program.
That's so beautiful. I hope that we will get a "`&lt;compiler_traits&gt;`" library like that in the future. But it should also include things like platform info, endianess, etc.. all under a common interface like it is done in `&lt;type_traits&gt;`.
Great talk! Very well organized and clearly thought out motivational examples.
[Here it is](https://groups.google.com/a/isocpp.org/d/msg/std-proposals/JH-xC84dvW4/4Efwe5lfAQAJ). I actually improved the proposal since then, but I did not post it back to std-proposal since the reception of the first draft has been quite lukewarm.
The lazy parameter version is also a bit more flexible. I want logging to be enabled/disabled based on runtime configuration, not compile-time configuration, and I only want those expressions evaluated when it's on. The realistic macro version looks like so: #define LOG(channel, fmt_string, ...) do{ \ if (channel.enabled()){ \ channel.format(__FILE__, __LINE__, __FUNCTION__, (fmt_string), #__VA_ARGS__); \ } }while(false) Or for perf-sensitive cases, a structured logging approach instead of a string-based approach, but the gist of the macro is identical either way. Note that the above only actually invokes any of the expressions if logging is enabled (at runtime). There's variations of the above that are _super_ useful too, like "error contexts," which are basically just RAII constructions that push a logger callback onto a stack; when an error occurs, the stack of error context information is included in the log/exception/dump/whatever. These are a _bit_ trickier because you usually want lazy eval for perf reasons, but sometimes you need immediate eval for correctness reasons. It's an inversion of the defaults that features like `lazy` attempt to address; instead of immediate-by-default and lazy on request, it's lazy-by-default and immediate on request.
Same here, they have very useful use cases. I don't really care if they where replaced with something that offered all of the same functionality but if it reduces compilation speeds while offering nothing other than "now you don't have to use macros" then I'm probably not using it.
Interesting, never heard of that
You can `use integral_constant`: using Unique_SDL_Window = std::unique_ptr&lt; SDL_Window, std::integral_constant&lt;decltype(&amp;SDL_DestroyWindow), &amp;SDL_DestroyWindow&gt;&gt;; 
I'm looking into something like that, Stay tune !
Yes, but it is not meant for production use since it increases the attack surface of a program.
I don't hate macros - at least not normal ones. Their biggest problem is that they don't work in the AST since they are just text replacement. I don't have an issue with `#if COMPILER == CC_CLANG` what I hate is macros that are expected to be used like `FOR_EACH(magic_variable, container, item){ do_thing(item);}` These types of macros are typically very hard to reverse from macro'd C code into modern C++. They do way to much all hidden by other macros and without type checking they tend to introduce subtle bugs down the line.
OK. 5 minutes later, it's done. We're advocating making C++ janky forever in order to avoid a tiny bit of time explaining something to someone once? An explanation that can be dropped the second we have the C++ language constructs to replace all remaining macro uses and major C++ libraries upgrade and we can start considering preprocessor includes truly legacy? And we're expecting beginners' first exposure to C++ to be Qt or a 250k file Bloomberg or Apple codebase where they can't get anything done until we explain the meaning and purpose of a special-case inclusion syntax and their custom library-specific language hacks and macros? Really? And learning the difference between `import foo` and `#include &lt;foo&gt;` is going to in any way be a burden compared to learning all the rest of C++ itself?
So this is just a wrapper to make std::bind a little more clean?
**Company:** [Ciere](http://ciere.com/hiring/) **Type:** Full time **Description:** We are a small consulting company that solves problems to make our clients successful. We primarily employ software; however, many of our solutions include custom electronics and mechanics. &amp;nbsp; We are full stack developers. We write software for small little devices with 4k of RAM, embedded devices with and without OS's, workstations and servers, and 400+ core distributed cloud systems. We work in the medical, consumer, business operations, industrial control, financial, and scientific fields. &amp;nbsp; We are looking for a full-time member of our technical staff. As a small company, we wear many hats to solve problems for our clients. Coding, data analysis, technical leadership, advising/mentoring, electronic design, running/maintaining large cloud deployments, and more. We craft solutions primarily in C++, Python, and Javascript. Our team has a diverse technical background and software is our artisan tool. We love solving problems and seeing our clients succeed. **Location:** El Dorado Hills, California **Remote:** This position is for on-site at the El Dorado Hills, California office **Visa Sponsorship:** Not currently sponsoring a visa **Technologies:** We are using C++14, C++11, and C++98/03 depending on the requirements of our clients. Most often we are able to use the C++14. Nearly all of the software we write will target multiple platforms and support more than one compiler. We use contemporary C++ (some people call that modern). We are actively involved with the Boost community and use Boost libraries when it makes sense. In additional to C++ we may be solving problems with some assembly, C, Python, or Javascript. **Contact:** If you are interested in joining our team and becoming part of the success story of our partners, please submit your CV (as a PDF) to [hiring@ciere.com](mailto://hiring@ciere.com). Include a few sentences describing why you would be an asset to the Ciere team and our clients.
My initial reply irked modbot, reposting: True. If the default case by design wouldn't *f-word* you, then it's a moot point. I'm thinking more about a situation where you have enum switch checking enabled, so people become inclined to just add defaults to appease the compiler.
std::bind it's not a currying, but it's can be use for *explicit* partial application. Example: int foo(int v1, int v2, int v3, int v4) { return v1 + v2 + v3 + v4; } // std::bind auto c0 = std::bind(foo, _1, _2, _3, _4); auto c1 = std::bind(c0, 15, _1, _2, _3); auto c2 = std::bind(c1, 20, 2, _1); auto rr = c2(5); std::cout &lt;&lt; rr &lt;&lt; std::endl; // output: 42 // kari.hpp auto c0 = kari::curry(foo); auto c1 = c0(15); auto c2 = c1(20, 2); auto rr = c2(5); std::cout &lt;&lt; rr &lt;&lt; std::endl; // output: 42 For different functional abstraction we needed implicit partial application.
I don't see the practical difference beyond the second version hiding a possibly expensive function call.
X Macros are really unreadable and hard to debug. It is very understandable to use X Macros, since they solve a very widespread problem, but they are least elegant way to solve any problem.
Well, that's what conventions are for
&gt; he lazy parameter version is also a bit more flexible. I want logging to be enabled/disabled based on runtime configuration, not compile-time configuration, and I only want those expressions evaluated when it's on. Precisely. Actually you can have both run-time and compile-time checks at the same time. Something like: template &lt;class Args...&gt; inline void log(Channel channel, [] Args... args) // [] introduces lazy arguments { if constexpr (/* debug mode */) { if (channel.enabled()) { channel.do_log(std::evaluate(args)...); // force evaluation of lazy arguments } } } (unfortunately, __FILE__ and __LINE__ still need to be used in a macro, but you get the point)
All these things is one of the reasons why the standard library is being written in the first place.
Sort of unrelated but why do i see someone on a dirtbike in the thumbnail of that link. Always happy to see a husqy in action but how are these thumbnail images determined 
BECAUSE REASONS!!!
I would like to one day no longer need Macros. So when will I be able to do the following without Macros? struct vertex_attribute { gl::GLint num_of_objects; size_t size_of_object; gl::GLenum type; bool normalized; constexpr vertex_attribute( gl::GLenum type, gl::GLint num_of_objects, bool normalized ) : type(type), num_of_objects(num_of_objects), normalized(normalized), size_of_object(get_size_of_object(type)) { } }; namespace vertex_attributes { #define CREATE_ATTRIBUTE(type_name, type_id, size, normalized) constexpr vertex_attribute type_name ## size (gl::GL_ ## type_id, size, normalized); #define CREATE_4_ATTRIBUTES(type_name, type_id, normalized) \ CREATE_ATTRIBUTE(type_name, type_id, 1, normalized) \ CREATE_ATTRIBUTE(type_name, type_id, 2, normalized) \ CREATE_ATTRIBUTE(type_name, type_id, 3, normalized) \ CREATE_ATTRIBUTE(type_name, type_id, 4, normalized) #define CREATE_ATTRIBUTES(type_name, type_id) \ CREATE_4_ATTRIBUTES(type_name, type_id, false) \ CREATE_4_ATTRIBUTES(u ## type_name, UNSIGNED_ ## type_id, false) \ CREATE_4_ATTRIBUTES(type_name ## n, type_id, true) \ CREATE_4_ATTRIBUTES(u ## type_name ## n, UNSIGNED_ ## type_id, true) CREATE_ATTRIBUTES(byte, BYTE) CREATE_ATTRIBUTES(short, SHORT) CREATE_ATTRIBUTES(int, INT) CREATE_4_ATTRIBUTES(float, FLOAT, false) CREATE_4_ATTRIBUTES(floatn, FLOAT, true) CREATE_4_ATTRIBUTES(double, DOUBLE, false) CREATE_4_ATTRIBUTES(doublen, DOUBLE, true) #undef CREATE_ATTRIBUTES #undef CREATE_4_ATTRIBUTES #undef CREATE_ATTRIBUTE } I run into this kind of design pattern a lot, where I need a variety of predefined objects and don't want to type out the (in this example, 64) discrete objects one-by-one, not least of which because I'm deathly worried about typos. This is probably the "Least Evil" code I can imagine, because the macros are immediately `#undef`'d after use, but it's still an uncomfortable construction which IDEs struggle to represent, and which, in my opinion, demonstrates the need for proper compile-time reflection as part of the core language.
In talk I noticed statement that pointer is a value of a function. Yet in example $f.pointer() is used instead of $f.value() (f is free function). Latter better match with reflection of enum and is more verbose. What do you think?
Reddit pops up as thumbnail the first image it finds in the mobile layout, I think, which happens to be the repo owner's profile pic. https://github.com/BlackMATov It's one of the reasons I despise browsing Reddit on my phone and just don't bother. Every code-related subreddit is just choke full of screen-sized profile pics of grinning mugs, brogrammers, neckbeards, and other nonsense irrelevant to the content. :p
There are people still targeting Windows 7 though...
I think I would prefer `constexpr for` or `for constexpr` like `if constexpr` to `for...`, in terms of being visually distinct. 
[operator try](wg21.link/p0779r0) is one of the best committee papers I've ever read. Bold ideas presented well.
If your programming language requires you to use **another** language just to get things like reflection, it's not a good language. C++ is shit right now, and while I think it is poorly designed from the ground up and can't really be fixed now, these proposals bring it a **lot** closer to usability.
Any updates on that? :)
actually, `#if COMPILER == CC_CLANG` is pretty bad. The problem with that is that, since this conditions live outside the AST, you can't write refactoring tool that just work. Think about renaming a variable or a type that is used inside of one conditional macro. The tool won't see it. 
He's skirting around the Actor programming model, but never actually *gets* it. Carl Hewitt needs a publicist.
You don't mention __COUNTER__. I have used it for type safe aliases: using Id = Ts(int);
congratulations Vinnie!
[Platform endianness doesn't matter](https://commandcenter.blogspot.com/2012/04/byte-order-fallacy.html?m=1). The endianness of the *data* (on the network or disk) is what matters. 
/u/blelbach's comment about [operator try](wg21.link/p0779r0) is one of the best comments I've ever seen. Strong comment, stated strongly.
This is an awesome use of `integral_constant` that I've never thought of before! And with C++17 you can even do `template &lt;auto T&gt; using integral_constant = std::integral_constant&lt;decltype(T), T&gt;;`
I don't find them unreadable. They just have a learning bump because they're not the intuitive solution. Other than that, they're consistent across use cases and easily recognizable. Of course, I'm still looking forward to a proper replacement for them. Reflection should handle most use cases.
&gt;obligatory pedant remark about it being pronounced "val"-"grinned"
&gt; By that standard the op's defer-like keyword does exactly what you want then. No, it doesn't prevent duplication of the cleanup code: every time you want`defer` to handle cleanup, you have to write `defer`. `defer`is a utility that makes it convenient to re-write that one-line cleanup over and over, which is what I was saying should be avoided. We don't want to write that one line over and over. &gt; You can even use a one-line macro (like in the op) to prevent duplication of that one line, No, that doesn't avoid the duplication. The duplication I want to avoid is having to say, over and over, 'do cleanup here'. You can write `#define CLOSE_FILE(X) DEFER((X).close())` and that just means instead of writing `DEFER(f.close())` over and over you write `CLOSE_FILE(f)` over and over. That's not an improvement; You're still writing the code over and over. The improvement I want comes only when the type that provides access to the resource also encodes the rules for cleaning it up. There's a huge difference between using `defer` to run `fclose()`, and using a type such as `fstream` which automatically closes the file when it's destroyed. `fstream` is what we want, not `defer`ed `fclose()`. &gt; In the end that is what templates are for: prevent duplication of lines that are basically the same with just a different type. But different resources are not managed 'basically the same' but with 'just a different type.' Creation and destruction APIs for different resources can be very different, such that it would be quite silly to try to smash them all into a single template. `fstream` doesn't handle the Win32 API for creating and destroying GUI windows, for example. And there's really no code to be shared between a type that manages opening/closing files and one that handles creating/destroying GUI windows, so it'd be silly to try to write a template that's shared between these resources.
I really hope that [p0834 (Lifting overload sets into objects)](http://wg21.link/p0834) (especially &amp;sect;2.1, i.e. Passing overloaded functions) could make its way into C++20. It's a constant annoyance that we can't pass a overloaded function as an argument, especially since it's quite hard to tell the problem by looking at the compile error and there's no clean and macro-free workaround.
Holy shit! I took my Intro C++ II course under him at LSU in 2010 I believe. Incredible!
That is defintely something that fits in the `ifc.exe` utilities. Like I said earlier, the devil is in the detail.
Why is OC downvoted? The question is legit.
This is a good idea - lazy evaluation is very useful in certain contexts. From your linked thread, I like Hymen Rosen's idea of prefixing [] to parameters. Have you seen [this paper](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4360.pdf)? I am not sure if it was discussed at all, I can't find any follow-up, nor reason for rejection. I wouldn't call the reception to your proposal lukewarm. Given the number of times it was discussed on std-proposals, there seems to be a real interest in this.
It's a decent overview of what's used for deep learning; but the "with c++' bit is a bit of an overstatement. Even though C++ was highlighted in the title, c++ was totally incidental to the content of the talk. Python had a far more important and prominent position. At the end, he even says that using Python is essentially better than C++ for this anyway.
Thanks for the pointer. Yes that paper looks very similar. I’ll try to contact the author and see if we can push this forward together. PS: I actually prefer the use of `inline` instead of `[]`.
You don't even need to specialise `sdl_deleter` for different types, just give it an overloaded `operator()` and use the same deleter type for all `SDL_Foo*` and `SDL_Bar*` pointers.
[here](https://www.youtube.com/watch?v=eH1CxEC29l8) it is!
You have a good memory. Thanks for the heads up!
I'll be happy to answer any questions!
&gt; US work eligible candidates only. Does this relate to the whole Microsoft (US engineering jobs) or just your particular team? As for the remote - are you aware of any other teams at MS (world-wide) open to this?
isn't the usual combo python for the user-facing API, C++ for the heavy lifting in the back?
u/hkaiser say hello
I hate macros, but they are still currently needed and I don't believe in huge ugly contortions to avoid them, such as the big multi-line code fragments in the OP. I particularly dislike any suggested solution that relies on the optimizer, where there are no guarantees for inlining or code/data stripping. So... I still use them. Upcoming features like static reflection will help chip away at macro usage, and a C++ equivalent to C#'s ConditionalAttribute would help with logging/assertion usage. It'd take a powerful general facility like AST-level transformation to really make a dent, though, given usage of macros to generate entire classes and control structures. And it's not clear they can reasonably be replaced at all for platform-specific compilation, where too many definitions can be missing for tools to have any hope of parsing the inactive code blocks. 
The second one is easily solver with lambdas. add_test("some", []{ ... });
you probably mean `??? ??? = add_test("some", []{ ... });`
Why would you need the result of that as a named variable?
Because you can't call functions in namespace scope (or whatever it's called).
Can anyone explain why the `(void)0` was needed as the last parameter to the void_t in the metafunction? timestamp: https://www.youtube.com/embed/WsUnnYEKPnI?start=1804
If anyone is interested, you can find the library here: https://github.com/iboB/dynamix Don't forget to prepare some water before you open that link!
To disable any comma operator overload shenanigans. Casting the argument to `void` means any defined function taking a `void` argument would be ill-formed (5.2.2/9 [expr.call]). Since no viable function exists that takes a `void` argument, the operator is assumed to be the built-in comma operator (16.3.1.2/9 [over.match.oper]).
Anything equivalent for clang?
I wouldn't say least elegant, but I grant that they're not the clearest construct. But they do serve a useful role that isn't quite accounted for by template metaprogramming as it currently stands. I guess the reflection proposal will be able to cover similar ground though? Honestly I kinda wish there was a sort of namespaced, cleaned-up macro language proposal for C++. There's a bunch of stuff that you can do with macros that's frankly clearer than the template equivalents, because simple text replacement is easy to get your head around. But the awkward points of the preprocessor and the lack of namespacing are kinda fatal.
As I said above, X Macros are ok in principle, they are just too dirty to my eyes, so I can't use them, and I warn my collegues if they use it. But yeah, it's not covered by metaprogramming, so it's fair game. I know it will sound even worse to some people, but I normally use a simple python script I use to generate code. I basically embed python between `&lt;?` and `?&gt;` tags (like PHP lmfao) and write inline python code that generates C/C++. Then, in my makefile I generate the code and work on it. Some people are probably screaming right now reading these lines, because a lot can go wrong with code generation if you don't do it right. But this is a very restricted use case, I use it maybe once eveyr other month or so and didn't get any problem so far.
It shows a lack of effort. Googling for "currying" would have immediately revealed the difference compared to "binding". Also, there's a difference in saying &gt; what's the use of this? We have std::bind and lambdas compared to something like &gt; what's the use of this? How does it differ from std::bind and lambdas? 
I wonder what will happen if the table is declared `constexpr`.
On mobile so I haven’t tried it yet but I imagine a few constexpr keywords would be a quick work around as well. It would be interesting to see if that would fix the compile times or just the code Gen.
https://www.reddit.com/r/cpp_questions/comments/76u45k/c_practice_questions/dohd5fg/
Good point! A quick try with `constexpr` indeed fixes the compile times (but makes it not build with VS2010 as expected). I'll update the post soon, thanks!
Usually it is CUDA+GPU
No unittests for a crypto libary?
Check out [Mudlet](https://github.com/mudlet/mudlet) perhaps - there's CI and code reviews, uses C++11 with 14 on the horizon, and mainly works with text so it's relatively easy to hack on.
i think it's not needed here. further on i did some research and played with different expressions and came to the following conclusion: what you can do with `decltype(xxxxxxx, (void)0)`is to chain the comma seperated expressions (if you dont overload the comma operator). in the end, decltype will give you the type of the LAST expression. here you get the type `void` (0 casted to void). this matches your base template default argument. if you want to test just against one attribute (for example if body has just a write function) without the `void_t`to `void`mapping, this would be helpful. imagine your function returns an `int`. then the specialization would not get picked, since `int != void`. but with the comma seperated expressions we are always getting `void`, regardless of what gets returned by the function `write`. in the end this is all not nessesary, because we are mapping and arbitray amount of templates arguments simply to `void`with the help of `void_t`. i hope someone will correct me if i am misstaken
That's fine - they just talk about "the C/C++ language".
Doing it via a separate tool is definitely simpler. My only concern is the need to start an extra process after each module interface compilation just to obtain the hash. Especially since process creation on Windows is fairly expensive.
You've already missed the deadline for the badges ;)
Wow, you guys are strict, but I understand the point.
FYI - ticketshop link is broken for me
Too bad it's so far north, and at noon (:
ah, f...ixed. :)
Wow!!! Unexpected lol
This is not intended to be a real crypto library; besides, many hash algorithms are naturally constant time so that note is mostly a warning about table-based implementations of functions like Whirlpool (for which Botan also uses a table-based implementation which is not constant time). You might consider using it over a full-fledged crypto library if you only need a digest and do not want to introduce a link time dependency on an external library. Adding a header-only library to a project is often easier. Unittests and benchmarks will be added later; for now I am more interested in critique of the API.
If you remove `(void)0` then this will not compile in VS2017 https://github.com/vinniefalco/CppCon2017/blob/master/main.cpp#L122
That's a fair point. A better title may have been "engineering challenges of deep learning". Note however that everything below the layer layer, from computation graphs to kernels to hardware, is C++. It's usually the frontends that are in Python. I'll update my slides to make this clearer for future talks. At the same time, I'll likely not want to dive deeper into C++ specifics, mainly because discussing implementations of graph scheduling systems or tensor manipulation routines would take away time for the higher-level algorithms/topics (especially since the implementations won't be super unique to deep learning, they're necessary for many HPC applications).
isn't it confusing to use `*` for composition? `|` seems to be the preferred choice by other similar libraries (boost adaptors for instance).
maybe you're right, but symbol * look like composition operator in mathematics :) ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8dd56d1296cf6b100fd8671c5c589dbe85742df3)
Wouldn't the process creation concern hold even if it was within the compiler?
what is the error message? i tried the following snipped on [godbolt](https://godbolt.org/g/ySQXkS) and with [this](http://webcompiler.cloudapp.net/) visual studio online compiler(dont know which version it is exactly). #include &lt;type_traits&gt; #include &lt;iostream&gt; struct maybe_B { using value_type = double; static void write(std::ostream&amp;, value_type const&amp;); }; template&lt;class B, class = void&gt; struct is_body : std::false_type{}; template&lt;class B&gt; struct is_body&lt;B, std::void_t&lt; typename B::value_type, decltype( B::write(std::declval&lt;std::ostream&amp;&gt;(), std::declval&lt;typename B::value_type const&amp;&gt;()) ) &gt;&gt; : std::true_type{}; int main(void) { if (is_body&lt;maybe_B&gt;::value) std::cout &lt;&lt; "is B"; else std::cout &lt;&lt; "is not B"; return 0; } 
I know, but they're not _the same_.
It's the same. Example: auto f = _ + 2; auto g = _ * 2; std::cout &lt;&lt; (g * f)(4); // g(f(4)) = (4 + 2) * 2 = 12
But yes, I can added pipe operator with other behaviour :) Like this: (g | f)(x) = f(g(x)) So... I think I'll add it. Thank you)
I mean they're not the very same in math either - you use two different symbols: a filled dot vs a circle. In this particular case, writing `(_ * 2) * 2` forces the reader to parse and type the code in order to understand what's every star about. But hey, my 2 cents.
In Haskell it can be written like this: (*2) . (+2) $ 4 But in C++ I can't use custom symbol operators :)
I can't reproduce it now :( 
If you are only reading `data`, sure. But what about creating `data` it self? You **need** to know the format used by `int` or you can't decompose it.
Good point and a better solution in this case where there are only a few types that need to be deleted.
There are times when this is needed though to invoke compiler specific functionality versus the `FOREACH` style macro which is honestly just hiding complexity. Refactoring tools are great but I also need to have high perf code that works now and typically that involves something like: #if CC = CLANG # define FORCE_INLINE __attribute__((always_inline)) #elif CC = MSVC # define FORCE_INLINE __forceinline Probably not the greatest example but you get the idea. 
Your post has been automatically removed because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20False%20Positive&amp;message=Please%20review%20my%20post%20at%20https://www.reddit.com/r/cpp/comments/78ownu/help_with_solving_for_pi_with_classes/.) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
This is the sequence of steps if the hash is produced as a byproduct of compiling the module interface: cl.exe /modules:printIfcHash ... foo.mxx store hash, if foo.cxx changes, then: cl.exe /modules:printIfcHash ... foo.mxx compare hash to stored, if unchanged, then no need to recompile module consumers If we have to use the `ifc.exe` utility, then it becomes: cl.exe foo.mxx ifc.exe foo.ifc store hash, if foo.cxx changes, then: cl.exe foo.mxx ifc.exe foo.ifc compare hash to stored, if unchanged, then no need to recompile module consumers 
What would be the point of VS2010 support? Windows 7 and 10 allow you to download VS2017, and I doubt Windows XP is worth supporting.
This could be worded in a more constructive way.
I think clang's concept ts implementation is at its early stage, while msvc is still busy with basic std features. The only (relatively) stable one is from gcc.
..Which is done from C/C++. But this is actually rare. Most use cases don't call for CUDA/GPU.
First: Interesting talk, well organized, overall well done. Secondly: Not sure if you mentioned this,zero but are networks trained/developed in pytorch translatable to the caffe2 framework?
Yeah, sorry
Are you referring to LLVM 5.0.0 or trunk? There should be a big difference. Also, don't rely on libc++ from OS repositories unless they track trunk.
Thanks! Take a look here: http://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html
why do we need concepts already ? :p 
I'd note that calling it `deque`-like is a little misleading, since it's very explicitly not a queue and orderless. I understand what you mean (it's chunk-based) but I'd just stick with that and drop the references to `deque` entirely, especially as that quality of `deque` is an implementation detail and not formally part of its interface or purpose in any way. :) This seems like it has a lot of similarity in purpose to plf::colony, though with the addition of thread safety, so it may be worthwhile to talk to [Matthew Bentley](http://plflib.org/colony.htm). A good place to discuss these kinds of data structures - where performance is a key element - would include the [SG14 mailing list](https://groups.google.com/a/isocpp.org/forum/#!forum/sg14).
Microsoft is claiming that the Concepts TS is being targeted at the upcoming 15.5 release of their compiler (as well as being C++17 complete by 15.6, fwiw) according to "MSVC Conformance" (6th) slide on [these cppcon slides](https://msdnshared.blob.core.windows.net/media/2017/09/CppConTalk_2017_share.pdf).
Using old languages is not a problem. If you teach algorithms and data structures, the specific language is really not important. You may as well use pseudo code which has the additional benefit that you don't have to explain specific aspects of C and especially (modern) C++. Knowing RAII, templates and all the other fancy stuff is not important if you are learning about sorting algorithms. Universities should and do teach the fundamental principles first and foremost. These don't change as fast as specific technology stacks. Turing machines and lambda calculus date back to 1936 and many of the core concepts and algorithms are from the 1940s to 1960s.
For most people, probably no point in VS2010 indeed. In our particular case (Unity game engine), several reasons: - We still do support Windows XP in fact! (mostly for China) - It's a huge codebase, with a ton of 3rd party libraries; many of them coming in binary forms or built with custom build processes. Since VS2015+ and VS2010 .lib formats are not compatible, this means all these libraries have to be rebuilt. Not rocket surgery, but quite a bit of work. - The usual work of upgrading codebase from old compiler to a more recent one - e.g. new compiler bugs (have ran into a couple), resulting code runtime performance regressions (have ran into a few), code build time increase (what my blog post is exactly about), etc. - Not *that much* incentive to switch to a more recent compiler. E.g. we still can't use C++11 anyway (due to some other platform compilers not being up to par); and people here can already use any VS IDE version as code editor/debugger; just the build itself goes through VS2010 at the moment. That said, yes, we are switching away from 2010 to 2017 right now, and this blog post is due to one of "oh, builds are slower now" cases I've noticed.
Can you explain how to use Empty and Identifier/Id?
 It is faster then plf::colony, and it work differently. The more fragmented plf::colony become, the slower iteration become. My benchmark, https://github.com/tower120/plf_colony_test (at 90% of plf::colony erase it becomes slower more than 5 times at best scenario). SyncedChunkedArray compact each chunk in unodered way as only this becomes possible, if chunks become too small - they are merged. It is not JUST thread-safe, read/mutation of each particular element is safe! This have HUGE performance difference, against manually lock of each element in container. The locking itself, have literally no observable performance impact (in single threaded env. it have speed of vector [well, almost :)]). I don't think SG14 group will be interesting in this. Moreover, it can be ORDERED, but it will be slower to erase. All elements emplaced at last chunk, reuse of half-filled chunks forbidden, merge is more agressive. Allowed operations will be emplace_back , erase, iterate; you can't swap, insert in the middle. If there is need in such a thing I probably can make a prototype of it. 
`Empty` is Usable when doing something like `Select&lt;OptionalRE, Empty&gt;`. `Identifier&lt;ID,NUM&gt;` is usable to check which path of RE was matched by using call `.getId&lt;ID&gt;()` and it will return NUM if path contain went the specific `NUM` way.
Maybe a MS-Employee will correct in a moment, but my impression is that MS is nowhere near a usefull implementation of concepts (neither the TS version, nor the c++20 version). So either we'll have to wait quite some time for 15.5 or the slides are just a bit too optimistic (which is what I believe). Personally, I think it is much more important that MSVC finally complies to c++11/14 (c++17 is the icing) than trying to implement a moving target. Concepts are nice to play with, but outside of personal projects, I'm not sure if there are a lot of usecases for them as long as they are not fully standardized (ETA 2020). Of course I'd be very happy when they are supported accross all compilers much sooner, but I think the priority should be on fixing SFINAE, ICEs and the preprocessor.
Congratulations, you've rediscovered the [Curiously Recurring Template Pattern](https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern). That's not a bad thing. The sad part is that nobody pointed that out to you in your quest for the answer.
Man... Every time I read/watch something about the new coroutines I get excited and then immediately depressed, because our platform is CentOS, so it will take forever until I get to use it. 
&gt; Personally, I think it is much more important that MSVC finally complies to c++11/14 (c++17 is the icing) than trying to implement a moving target VS 2017 RTM is already C++11/14 complete in both language and library, sans two-phase lookup. Which is coming (mostly) in 15.5. It's already partly C++17 compliant and expects to have that complete by early 2018 in the 15.6 update. MSVC already supports the Modules TS and the Coroutines TS (in fact, led directly to the creation of the TSes!) which I'd say helps to illustrate the importance of Visual C++ getting Concepts sooner rather than later. :) It's still taking me for a loop too, but we've gotten to a point where our enterprise-upgrade-speed is often leaving us with better C++ support on Microsoft's toolchain than we have available on Linux. :p We're literally right now (I'm waiting for tests to complete) going through the process of getting bleeding-edge Clang installed across all our build nodes (replacing the bleeding-edge version we started requiring all of one single year ago) just to support the code our developers are writing for Windows in VS _2015_. :)
VS 2017 still supports targeting WinXP via the v141_xp toolset (unfortunately for me).
I'm unclear if its use within the infrastructure of an educational institution is considered commercial or not. I understand that students using it in their own PCs is (in general) non-commercial, but the institution itself is another matter.
&gt; sans two-phase lookup Do they compile 6 year old C++11 libraries like... range-v3 yet?
I didn’t write this article. Just wondering what people though about it.
I love this and I would like to use it on this giant dinosaur of an open-source library that I sometimes contribute to. Has anyone tried this "at scale"? how big of a project has been thrown at it successfully?
I managed to run it on my 2 mln LOC Windows Project. Indexing took like 40 minutes (normal rebuild takes 4 minutes in VS), and the tool worked very well. I could see all the types, relations.
Fair enough. Either way, what's described is a totally valid pattern, and it's very widely used.
niiiiiiiice
Yes, students are good with non-commercial use. For teachers and trainers it's not so easy. Education can be hard to define. We decided to offer free licenses to teachers/trainers, but they have to apply for it ( see offer in the lower section of our store: https://sites.fastspring.com/coatisoftware/product/buy_0?action=adds )
&gt; I really can't understand how anybody can claim that MSVC can compile C++11 code with a straight face Easy. It compiles C++11 code, sans some compiler-straining examples like ranges-v3. We're happily using all of C++11 and some of C++14 in our many-million-line codebase which all compiles quite well with MSVC. :) Granted, some of those features have bugs or limitations restricting how far we can push/use them, but then so do GCC and Clang and their respective stdlibs. We have hacks in our code to avoid miscompilations or ICEs or conformance bugs and other problems for both of those implementations (most of which can hopefully go away after we finish upgrading, but then the year-old versions weren't supposed to have any of those problems either). :)
Can you use clang on CentOS?
&gt; sans two-phase lookup Not quite: As I said, the preprocessor still handles variadic macros in a non-conforming manner and afaik the remaining SFINAE issues mentioned towards the end here(https://blogs.msdn.microsoft.com/vcblog/2016/06/07/expression-sfinae-improvements-in-vs-2015-update-3/) are still present in 15.4. And even in code that MSVC would compile correctly, I get the occasional ICE. MSVC has been "mostly" standards compliant for quite some time now, but when you write template heavy code (which is exactly where you usually want to use concepts) "mostly" is just not enough and I certainly prefer a correct&amp;complete implementation of c++14 to a broken implementation of c++14 together with a broken / incomplete implementation of concepts. Honestly: I believe the ability to use ranges-v3 is much more important for readability and correctness of my code than concepts are. Also, what people tend to overlook when they ask for concepts support is that there currently isn't "the" concepts language feature: There is the version in the concepts TS, the version of concepts that made it into the current working draft, the version that will be in the final c++20 standard (there are still quite a few papers in flight) and I'm not even sure if the different versions of gcc have a completely compatible version of concepts or if that evolved too. &gt; MSVC already supports the Modules TS and the Coroutines TS (in fact, led directly to the creation of the TSes!) which I'd say helps to illustrate the importance of Visual C++ getting Concepts sooner rather than later. :) Sorry, I don't understand why them supporting modules and coroutines means it is important to also support concepts.
Has your team looked into the "special interest group" [devtoolset](http://mirror.centos.org/centos/7/sclo/x86_64/rh/devtoolset-7/) packages? They're more up-to-date dev tools for RHEL-based systems. The last set is exceedingly recent, and they seem to be releasing new sets every 6-12 months pretty regularly.
I used it on a massive open source game (https://github.com/CleverRaven/Cataclysm-DDA/) because I wanted to contribute there and thus needed to understand the games' architecture, and it worked **very** well.
That's a good solution I think, I mean handling it on a case by case basis.
Is that actually a lot of work when it comes to maintaining the STL? The "only" things that come to my mind are threads and atomics that would use stuff that's not available in Windows XP. I could be wrong though.
Sadly we have a few closed source packages which only support the base version. Which also rules out clang. Until that changes were stuck. 
We have a few closed source dependencies that limit our options severely. Until we can address that somehow were stuck.
It's primarily responsible for degrading our multithreading efficiency on all platforms, yes.
You might check out [Cataclysm-DDA](https://github.com/CleverRaven/Cataclysm-DDA/), a post-apocalyptic 2D-roguelike game. They have quite the active development community, and use C++11 for their project.
That article is from 2009. /u/VinnieFalco just gave a nice talk at CppCon (["Make Classes Great Again! (Using Concepts for Customization Points)"](https://youtu.be/WsUnnYEKPnI?t=25m55s)) where he presented a modern version of this with `static_assert` and template metaprogramming.
I'm thinking to start a user group in Herzliya. Do you think there'll be much interest? 
Might P0091R3 (template argument deduction for class templates) and P0127R2 (declaring non-type template parameters with auto) be in the 15.**6** (not out yet) preview, or are those further off?
There's also the official (gcc images)[https://hub.docker.com/_/gcc/])
What you call a compiler-straining example is what we call our standard library. range-v3 has concepts, ranges, views, optional, variant, proxy iterators... All of that plus clang-modules is what we call "modern C++". Flipping a compiler flag does not a C++14 code base make. 
I actually did watch that a few days ago, great talk.
It's the same with every other exiting new proposals for C++, like proper support for meta-programming with meta-classes. It will probably take 3-6 years until it would perhaps get standardised. For me personally, this is way to long, there are languages which have evolving faster (eg. Rust).
See e.g. [mettle](https://jimporter.github.io/mettle/): suite&lt;&gt; basic("a basic suite", [](auto &amp;_) { _.test("a test", []() { expect(true, equal_to(true)); }); for(int i = 0; i &lt; 4; i++) { _.test("test number " + std::to_string(i), [i]() { expect(i % 2, less(2)); }); } subsuite&lt;&gt;(_, "a subsuite", [](auto &amp;_) { _.test("a sub-test", []() { expect(true, equal_to(true)); }); }); });
C++ coroutines have been in standardization process since 2012 (http://open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3328.pdf). It has been 5 years already. So, given you prediction 3-6 years, they should be merged into the standard in 2018 :-) at the latest.
What is it - LISP?
I was thinking about the case where an institution (say, a well defined university, no doubts on its purpose) wants to install it in machines belonging to the organisation (classroom computers, etc) for use in lessons by whichever teachers and students are assigned to it. Those installs wouldn't belong to a specific person. We face that situation often, for example with Unity 3D, where students and teachers can have non-commercial licenses, but university-owned computers needed a paid license (may have changed recently, not sure).
Great talk! Q: In the message struct: what is the benefit of using `Body::value_type` over a simple wrapper class `BodyWrapper` with a data member `body`? The difference is that BodyWrapper must be instantiated and the write function can therefore not be static. Is that wrong? Or is there another reason? Or doesn't it really matter?
Correct me if I'm wrong but doesn't SDL use this parent-destroys-children model where an object destroys all the objects it created? I.e. template&lt;typename T&gt; using sdl_ptr = std::unique_ptr&lt;T, sdl_deleter&lt;T&gt;&gt;; sdl_ptr&lt;SDL_Renderer&gt; create_renderer() { sdl_ptr&lt;SDL_Window&gt; window = SDL_CreateWindow(/*blah blah blah*/); sdl_ptr&lt;SDL_Renderer&gt; renderer = SDL_CreateRenderer(window.get(), /* blah blah blah */); return renderer; // but window goes out of scope here // and its deleter also deletes the renderer }
&gt; What you call a compiler-straining example is what we call our standard library. Which is weird, because it's most definitely not the C++ standard library, at least as of C++17. :) &gt; All of that plus clang-modules is what we call "modern C++". That is a very forward-looking and creative definition of "modern." :)
No. Bad slide, or bad slide-reading. The TS listings on the slide do not imply that the TS implementations are complete. MSVC will NOT have concepts in 15.6.
[Much of two-phase name lookup is complete in MSVC](Much of two-phase name lookup is complete in MSVC). There are broad categories of "doesn't work yet", and those are indicated in the linked blog post.
/u/SeanMiddleditch is correct: Range-v3 uses the heck out of modern C++ features. [We have a fork of Range-v3 for VS 2015 and forward](https://github.com/microsoft/Range-V3-VS2015) that is kind of out of date. /u/caseycarter intends to make a new fork Real Soon Now (TM) that he will merge with Eric Niebler's official Range-v3. He's just waiting on a few more bug fixes from the compiler. 
We are working on a new, conforming preprocessor implementation. This will be our long pole for conformance, however. We have been addressing those remaining expression SFINAE issues, but you're right, some still remain. 15.5 should have a bunch more fixes for expression SFINAE. 
Those are preferable! Though there doesn’t appear to be official images for clang, which my images use.
Status of concepts in MSVC: nothing useful yet, nothing useful expected for a while. [Use GCC for concepts, even in Visual Studio](https://blogs.msdn.microsoft.com/vcblog/2017/02/22/learn-c-concepts-with-visual-studio-and-the-wsl/). We started implementing them some time ago, under the /experimental:concepts switch. For quite a while, only an identity concept (e.g., `requires(T)`) would compile. But it looks like Jon Caves has been coding behind my back. I've now gotten most--but not a useful portion--of my Concepts Hello World to compile: #include &lt;iostream&gt; template &lt;class T&gt; concept bool EqualityComparable() { return requires(T a, T b) { {a == b}-&gt;bool; {a != b}-&gt;bool; }; } // bool is_the_answer(const EqualityComparable&amp; i) { // return (i == 42) ? true : false; // } int main() { // if (is_the_answer(42)) { std::cout &lt;&lt; "42 is the answer to the ultimate question of life, the universe, and everything." &lt;&lt; std::endl; // } return 0; } I can compile the concept with today's compiler, but I can't actually call the concept (missing type specifier on `i`--the compiler doesn't realize that `EqualityComparable&amp;` is the type of `i`.) 
Why? There's nothing constexpr about it.
Ah, alright. Thank you for clarifying. :)
So glad you're making use of libclang as opposed to writing a custom parser. Jetbrains is wasting so much effort and money reinventing the wheel for their CLion IDE. Their parser is not even close to what libclang can do (especially when it comes to c++ templates)
i'm only speaking for my team here :) for more broad hiring, I'd suggest going to the official careers site.
 &gt;Jetbrains is wasting so much effort and money reinventing the wheel for their CLion IDE. Their parser is not even close to what libclang can do (especially when it comes to parsing templates). I imagine JetBrains don't want to interface their IDE written in Java so closely with a native C library because native stuff in Java is never fun. Also it might be that libclang doesn't do what they want in the way they want it because JetBrains IDEs already have very extensive abstract source code manipulation across a large number of languages which clearly works for them. Having to bend over just to accommodate libclang in that model might be also too much.
 &gt;We face that situation often, for example with Unity 3D, where students and teachers can have non-commercial licenses, but university-owned computers needed a paid license (may have changed recently, not sure). I think that is very much a marketing thing to essentially make money off institutions too who would be more inclined to do so if there are teachers already familiar with the platform and students' demand. Kind of similar to how Adobe didn't go too crazy over piracy of their products because people who learn the software to actually work in the industry will have the company still buy the product at the end of the day because that's what most professionals have learned one way or the other.
True, but it is a compile time expression. Visually, it's more distinct than an ellipsis. 
`&lt;filesystem&gt;` is also going to be extreme pain.
Same thing. You define an endianness for the data and serialize to that. 
Speaking of Preprocessor ICE's, found this a couple days ago: https://developercommunity.visualstudio.com/content/problem/138552/ice-in-preprocessor-expansion-used-for-template-ar.html Just got around to reporting it a couple hours ago. Also, any updates on this? https://developercommunity.visualstudio.com/content/problem/109571/intellisense-cant-resolve-some-base-type-members.html Driving my team and I up the wall.
Did the WinAPI change that much in regards to filesystem operations between WinXP and more "modern" Windows variants?
He is also a legend in the Boost community.
Thanks for both of these! I'll have them looked at. 
What about existing IFCs produced from previous compilation or already installed?
**Company:** [Akuna Capital](http://www.akunacapital.com/) **Type:** Full-time, also internship **Description:** We're a booming proprietary trading firm specializing in derivatives market-making. We are the leading options market-maker to successfully enter the industry in recent years, with sustainable growth. We use C++ for a large majority of our backend software, much of which is latency-critical. We hire developers proficient in C++ from all levels of experience, whether you are still in school or have been working in the industry for 10+ years (no prior financial experience required). **Location:** Chicago, Shanghai **Remote:** No **Visa Sponsorship:** Yes **Technologies:** C++11, transitioning to C++17, Linux, Boost, _templates_ **Contact:** PM me your name after you apply [here](http://www.akunacapital.com/careers#careers). Fun fact: I landed my job at Akuna through this subreddit.
Hmm.... good question! I guess the reason is to not introduce new instances of user-defined objects. In the current design the Body type is never instantiated. Another reason is that the wrapper might force callers to write additional forwarding constructors. The real beast *Body* concept has more requirements: http://www.boost.org/doc/libs/master/libs/beast/doc/html/beast/concepts/Body.html. A wrapper would still need the nested `value_type` so that the associated `reader` and `writer` constructors could utter the type using uniform notation. At that point the wrapper doesn't add anything over just using the nested type directly.
Im in the same boat. Devtoolset v7 is your friend particularly if you also add the clang (llvm) bits. Much better dev tooling, build for centos7 and though I havent yet tried, I think you can configure the std c++ lib to be compatible with shared libs from earlier gcc versions albeit with slight c++ 11 conformance issues eg string SSO
I see, but if HR doesn't sponsor visas for MSVC team, I doubt they will do so for Notepad.exe team either :) And what about remote jobs at MS? There is an impression that MS is more or less open to it, but it is never mentioned on careers site.
This seems like a very useful little library
I was so close to greatness.....
Been slowly pushing C++ in a much larger C codebase for space flight software. The nice thing is that it is embedded programming, but space systems still tend to be fairly beefy, at least in terms of memory and binary size restrictions so a lot of the traditional "complaints" about C++ and template programming go out the window.
Templates can even yield smaller binaries if used well. AFAIK, they are pretty effective on embedded systems.
I assume this freestanding comment was in response to me haha. Thanks so much, I really appreciate it! The intellisense one is shockingly painful. I would have to imagine it's the bizarre include structure in the reflection system causing the problem, but I'm just not sure. We're happy to make workaround changes as long as they don't inhibit functionality.
Thanks for the confirmation. Don't get me wrong: I'm already very happy with MSVC as it is. Great work you did there over the last couple of years.
Does the long version work? The abbreviated template syntax didn't make it into the draft (yet) anyway right?
&gt; [Much of two-phase name lookup is complete in MSVC](Much of two-phase name lookup is complete in MSVC) Did you miss a link?
Slightly off topic, but why does it take so long to get std::future::then in the standard? I remember talks about it in 2012/13.
I very much hope so. I'm not sure if c++ could live down another c++17 release with very little progress.
&gt; Which is weird, because it's most definitely not the C++ standard library, at least as of C++17. :) Sure, its the C++11 backward compatible implementation of the STL2, which will land in the C++23-26 time-frame. Using the C++98 STL1 in 2017 looks like a pretty backward-looking definition of "modern".
&gt; Range-v3 uses the heck out of modern C++ features. Which modern C++ features is MSVC missing? Two-phase lookup is from C++03, expression SFINAE is from C++11. It is 2017, all other compilers implement those.
Wow! Awesome talk :-) 
&gt; Also, what people tend to overlook when they ask for concepts support is that there currently isn't "the" concepts language feature Yes, I want concepts to use the _real_ STL2 instead of range-v3. But chances are that if MSVC can't compile range-v3 yet, it won't be able to compile the STL2.
A compile time lexer is a total game changer for me. Can you give a small example of the use of Identifier please?
JetBrains is offering class room licenses for some of their products for that. But I'm not sure we really have that use-case. I can see how teachers/trainers could use Sourcetrail to teach programming and explain patterns. And students can use it to read into some code base or work on their own projects/assignments. But why would a whole class use Sourcetrail within a lecture? I'm open to request of that kind, just wondering if it makes sense.
Seems the call for more cross community interaction is already paying off? 
That's cool, thanks ! I'm feeding it the LLVM sourcecode too test it out !
Isn't there a switch where I can say I don't need windows compatibility, so just use more modern API primitives?
It also really depends how controversial a feature is, and even if it's not controversial, if there are "competing" ways of doing it. I think there's many different ways to do coroutines and 2-3 groups of people trying to push through their view of it, so they have to converge on a solution, with sometimes can only be done through implementing it in a compiler and let people get some experience with it. See e.g. the two competing modules proposals &amp; implementations that are converging towards each other (hopefully). Or concepts, which became concepts lite. Btw, this competition is quite good, to a degree :-)
Oh come one, the picture is really not that dark. The jump from 14 to 17 might not include some of the "big" things many people would have liked, but C++17 allows you to write so much better code out-of-the-box. `&lt;optional&gt;` (no boost needed anymore), `&lt;string_view&gt;`, `&lt;filesystem&gt;`, structured bindings - that's already enough for me but there's so many cool small things more that help immensely in writing good, simple, maintainable code day-to-day.
the hero we need
It's surprisingly reactive and smooth. The indexer seems to do a good job. The search feature could be improved by having the ability to filter by type of symbols ( like Qt creator does ). I had a few freezes though. An outline view to show all the symbols of a given file / class / etc would be welcomed.
A small comment on expanding to `void(0)` (or other no-op constructs): One problem that you can run into in this situation (and I have) is what happens if you put things with side-effects inside one of these macros. I've seen people do things like: LOG("The result of foo() is %d",foo()); in code where `foo()` has side-effects. Solutions to this I've seen attempted: - Bad idea: ask programmers not to use functions with side-effects in these circumstances. (And do you really trust everyone on the team to remember?) - Good idea: in non-debug mode this would get reduced to `foo()`. If this is a pointless function call, hopefully the optimiser can figure it out and remove the call. Pipeline dream idea: Some compiler enforced method of proving that a function does not have side-effects (e.g. a const member-function, which does not take non-const pointer or references as arguments - to use the easiest case to prove) and have some way of marking a code-block as "no side-effects" in the language. Maybe I should write a paper up on something like: const { // Code here can read, but not write, any variable outside // the enclosing scope. }
**Company:** [think-cell Software GmbH](https://www.think-cell.com) **Type:** Full time **Description:** think-cell is a kind of company developers truly like. In fact, of our now 20 full-time developers, in 15 years of think-cell, only one ever quit his job. We are highly profitable, so we can give you the time and resources to write beautiful code. There are no meetings. All management (the two co-founders) are computer science PhDs, so no demands from people who don’t understand the trade. We are working on revolutionizing the way presentations are made, reinventing the user interface and largely automating the slide layout. At the same time, we integrate this product into Microsoft Office, which means reverse-engineering and disassembling the innards of Microsoft’s code. And we do this all based on our very large, home-grown C++ library, which we have the liberty to perfect along with the rest of our code. think-cell is the only German company funding a C++ ISO committee delegation, so there is a good chance that components we invent will find their way into the standard. We are growing the team and would like to hire up to 10 C++ developers in the next year, either junior or senior.You can find the [full job description here](https://www.think-cell.com/career/jobs/development.shtml). In general, developers sit in offices with one or two other developers and their offices open to a great hall. You don't work in teams necessarily, though it may happen. In general, you discuss your tasks and projects with the Technical Director, according to where your interests and skills meet what is needed by the product and company. This is an amazing opportunity since we are not hiring for one project or objective in mind, but for multiple. There is no exposure of developers to clients, since there is a support team that works closely with them. However, they are much closer to the client than other companies, with only one “layer” of people in between, instead of 3-7 “layers” or intermediaries. The company has 41 employees already, 20 of which are in development. Nationalities at our company are varied, including German, Chinese, South African, Italian, Argentinean, Russian, etc. **Relevant questions:** Do you believe in beauty when it comes to programming? Do you have a vivid interest in elegant algorithms? Are you fluent in C++? If so, we would like to meet you. **What we offer in a nutshell:** * A wide array of extremely challenging C++ development tasks * An international team of brilliant minds * A working environment that makes this team stay and grow * Enough time to make sure that every detail of your solution is perfect * A flat organization and plenty of room for your ideas * No scheduled meetings * Family-friendly working hours, no deadlines, no overtime * A competitive salary from the start and a raise to EUR 120,000 annually after only one year **Location:** Berlin-Mitte. Chausseestr. 8/E, 10115 Berlin, Germany. **Remote:** No, we prefer to work at the same office. Since there is a no-meeting policy at the company, it’s good to have all colleagues nearby during office hours. However, these hours are flexible if the developers need to run an errand, they have to simply notify the others. **Visa Sponsorship:** Yes, we support candidates by sponsoring their VISA if they need one. Besides, to relocate a candidate, instead of a one-fits-all package, our CEO speaks directly to the candidate about his/her needs to relocate and if they are reasonable requests, we work to provide it. In general it may include support moving and accommodation when they first move to Berlin. **Technologies:** Please, see a detailed description of the technology we use under the subtitle ["about our software"](https://www.think-cell.com/career/jobs/development.shtml) **Contact:** Send us your CV/resume per email to hr@think-cell.com
From [N4690](http://wg21.link/N4690): &gt; The `future.then()` feature was expected to be controversial but in the end it wasn't, as the vote was very much against. Everyone wants it, but everyone agrees it needs more work. There are pieces that are ready and will move forward 
Is it just me, or any source including boost headers make the parsing extremely slow?
Hey there Eric, one of the things I'd be very interested in seeing from the MSVC compiler team is integration with a profiler that allows more general poking around, such as [easy_profiler](https://github.com/yse/easy_profiler) or [brofiler](https://github.com/bombomby/brofiler), I became interested in seeing this sort of data after Jonathan Blow showed off [telemetry](http://www.radgametools.com/telemetry.htm) during a [video talking about compile times of his language](https://www.youtube.com/watch?v=rJ7-j1nK9gk). Being able to hook into VS in a "debug build times mode" and watch/look at a build and see this sort of data in an easily consumable/browsable way would be absolutely amazing. Of course I don't know how easy any of these tools make it to show different processes like they're used in MSBuild and such.
That's indeed a much simpler approach, that didn't occur to me at all. Thanks! :)
This is a nice alternative solution, and I like the idea of using `decltype`/`declval` to define the `alias`. But I don't buy the performance argument here at all. The destruction functions like `SDL_DestroyWindow` are implemented in a dynamically linked library, so there is no way the compiler could inline them anyway. But even if it could, I don't see why it would matter. In my use case, I'm only creating a small number of SDL objects, and they are destructed very rarely. So any small performance gains thanks to inlining won't make any practically noticeable difference. It's very easy to make claims like "this is much faster", but until you actually measure the difference and prove that there is one, claims like that aren't very helpful. I also agree that your solution is less code and allows you more flexibility in how you structure/distribute it, but I disagree that mine is "overly complicated" and "more intrusive". I can see that subclassing `unique_ptr` is not ideal, and it's actually not required with the approach proposed by 0xa0000 above. But what matters to me in the end is how the usage code looks like. And with my solution, I can have one of these pointers as a member of a class, and then initialize it in the constructor with minimal boilerplate: class Foo { private: sdl::Ptr&lt;SDL_Window&gt; mpWindow; }; Foo::Foo() : mpWindow(SDL_CreateWindow(...)) {} With your solution, I'd still have to add the `make_unique_wrapper` in there in the ctor. So I would argue that from the point of view of the clients, my solution is *less* intrusive. &gt; it's encouraging you to centralize all of the cases in one place. Why bother, exactly? Why not? It's not too uncommon with smaller C libraries to have a single header that you always include, with SDL for example, you'd usually include `SDL.h` and that's it. So for my particular use case, it's totally fine to have a single wrapper header, which includes that and then adds the `sdl::Ptr` stuff on top. 
Not that I'm aware of. If that's indeed the case, do you have a link to the documentation mentioning this behavior?
You can filter search results by symbol type if you enter the symbol type first e.g. class. I opened a feature request for outline view: https://github.com/CoatiSoftware/SourcetrailBugTracker/issues/497
&gt; because our platform is CentOS why do you restrict yourself to CentOS's compiler ? building GCC from source takes half an hour and you get access to allthe last stuff
Honestly, I would never follow the CPPCoreGuidelines here. It's just an opinionated piece of advice that gives no actual benefits, just as I have opinionated feelings here. The document itself points out the consistency trumps all, but when I start a new project, I reach for PascalCase regardless. Although, I find this part hilarious: Enforcement Impossible. As I have considered writing a clang-tidy check for PascalCase for our CI just as a gag, though if I did I'd of course write rules for camel and snake_case and such. It's at least theoretically possible to check.
The best part of the session was the one dedicated to questions
Yeah. There are decent C++11 deep learning APIs that really are meant to be used from C++ rather than python. You would think a talk about C++ and deep learning would talk about some of them.
I came across [this](https://blog.jetbrains.com/blog/2015/08/06/jetbrains-way-to-cpp-the-inside-story-of-our-journey/) article after a bit of research &gt; However, the main reason for not using libclang was our caching needs. For the IDE to perform efficiently during refactorings and other operations, we needed symbol caching. And it’s not just about refactorings: finding context-aware usages of a symbol, building proper hierarchical views, providing navigation actions, and doing all of this in a reasonable timeframe requires good code context awareness and symbols caching. Clang does a good job of caching while working with individual files, parsing incrementally. But if we talk about the entire project, it starts parsing every file, just like a true compiler should do. And that’s where it can become problematic from inside the IDE. Maybe this line of argument is outdated considering the article was written in 2015. But I tried using CLion on a project with 500k+ lines of code and the indexing completely failed in certain areas. Qt Creator (which uses libclang for its code model) took some time to index but its index and refactoring tools have yet to fail me.
&gt; We have a few closed source dependencies that limit our options severely. Until we can address that somehow were stuck. - [me](https://www.reddit.com/r/cpp/comments/78p6mf/cppcon_2017_gor_nishanov_naked_coroutines_live/dovus4e/)
No, the longer version does not work. MSVC has started parsing concepts. That's all. We're a ways from having anything work. 
I'm typing this without compiling it, but something like this: ``` enum class Type { number, id }; using Lexer = RegExp&lt; Select&lt; Sequence&lt; Plus&lt;Number&gt;, Identifier&lt;0,Type::number&gt; &gt;, Sequence&lt; Plus&lt;Range&lt;'a','z'&gt;&gt;, Identifier&lt;0,Type::id&gt; &gt;, &gt; &gt;; Type getLex(const std::string &amp; str) { if (Lexer lexer; lexer.match(str)) { return lexer.getId&lt;0&gt;(); } else { throw NotMatch(); } } ```
Yes, it was in response to you. The iPhone Reddit app presents some UX challenges for me.
I see this same pattern every few months (and do it myself from time to time in our codebase). I *really* wish there was language level support for it because every implementation I've seen has drawbacks.
There's no questions at the end of the video unfortunately.
For `.ifc` produced from previous compilation the build system will store the hash (e.g., in the same place it stores the extracted header dependency information). For installed `.ifc` we normally don't expect them to change often so there we can depend just on the modification time. I am not against having this functionality in a separate tool especially if you already have one. I am just suggesting that it may also make sense to duplicate it in the compiler to avoid spawning a separate process in the most common cases.
(Grumble grumble iPhone.) Thanks, fixed!
Contains Javascript =&gt; I rather die then use it
It's not a compile-time expression (there are requirements on the range/tuple to expose compile-time information, but that's not necessarily an expression [members of a class, arguments in a pack, etc]). The semantics of the construct are to expand into a sequence of statements. That's what ... does.
&gt; So any small performance gains thanks to inlining won't make any practically noticeable difference. I actually forgot to mention the more significant performance drawback of your solution. Output the result of `sizeof` on your pointer type for a nasty surprise. Having a `vector&lt;unique_ptr&lt;Foo&gt;&gt;` where each unique_ptr is double the size it should be will impact performance. &gt; It's very easy to make claims like "this is much faster", but until you actually measure the difference and prove that there is one, claims like that aren't very helpful. Well, it's easy when it's correct. Writing a whole bunch of code, profiling it carefully, and then rewriting it to fix the issue takes time. If you can avoid throwing performance away at minimal cost, then you should do it. &gt; But what matters to me in the end is how the usage code looks like. And with my solution, I can have one of these pointers as a member of a class, and then initialize it in the constructor with minimal boilerplate: The thing is that this not how smart pointers are created idiomatically in C++ anyhow. SDL_CreateWindow is equivalent to a `new` call, calling a `unique_ptr` constructor this way can lead to memory leaks. The idiomatic way is to call `make_unique`. So, if you want the most idiomatic interface, you should really be aiming for `make_unique_wrapper&lt;SDL_Window&gt;(...)` or something like that. You can do this by writing a template function with perfect forwarding and an implementation struct that gets specialized on the type being created. &gt; Why not? It's not too uncommon with smaller C libraries to have a single header that you always include, with SDL for example, you'd usually include SDL.h and that's it. So for my particular use case, it's totally fine to have a single wrapper header, which includes that and then adds the sdl::Ptr stuff on top. This is just a very short sighted approach. All this machinery can be re-used easily across different C libraries. I don't see any value in having to write `sdl::Ptr&lt;SDL_Window&gt;` versus `curl::Ptr&lt;SDL_Window&gt;`. You may as well just have these utilities (`Ptr`, `makePtr`, whatever) defined in your code's namespace, then you can have a header that includes SDL and specializes the appropriate structs, a different header that includes curl and does the same. Even if you don't feel this way, someone might use your code, and be using libcurl, and want to take advantage of this approach. The thing is that I've pointed out things in your design that pure downsides, with no upsides, like the performance, and like the centralization of the cases. Then you say: "this is ok for my use case". But if you're going to put it on a blog as a technique for others to use I think it's worth thinking about things that lie just outside your use case; if you can do something to improve your technique with *no* downside, you should do it in the context of a blog post you want others to read and derive utility from.
That works fine (if I throw in a Static catch then I can also fetch the position). I can't begin to tell you how cool this is. Out of interest, do you have a string syntax for this too?
**Company:** RandomControl, S.L.U. We develop GPU-accelerated light simulation technology. We are currently in the process of scaling the company, and adapting our technology to new markets, such as (very especially) Media &amp; Entertainment, production, and VFX. **Type:** Full time **Description:** We are looking for a Developer to join the group of highly skilled engineers in the plug-ins team. You will be developing, improving and supporting Arion plug-ins for existing and future DCCs used by prestigious VFX facilities around the world. A deep knowledge of several DCCs will enable you to provide a tight, native integration while empowering the users with the ease of use and performance of Arion. You will also coordinate with the other plug-in developers to maintain interoperability and user experience coherence. **Responsibilities** - Integrate Arion core features and ship them in a Maya plug-in. - Support and integrate new DCC features. - Build and release on Windows, Linux and Mac. - Write robust, readable, maintainable, reusable and extensible code. - Implement or improve core plug-in features, such as IPR. - Refactor legacy code. Produce unit tests and documentation for any new code. - Debug and profile existing subsystems, identify problem areas in the code, provide both quick temporary workarounds and high-quality, lasting solutions as needed. - Debug production scenes and communicate findings and workarounds with both customers and other developers. **Requirements** - Excellent C/C++ programming, debugging and profiling skills. - Good understanding of computer graphics and FX. - Deep knowledge of one or more DCC SDK (i.e., Autodesk Maya). - Experience with Arion and/or another production renderer(s). - Good English communication and writing skills. - Familiarity with VFX in Film &amp; TV. **Bonus points** - VFX production experience. - Experience with DCCs such as 3ds Max, Maya, Houdini, C4D, or Katana. - Cross-platform development experience (Windows, Linux, OS X). - Familiarity with Microsoft Visual Studio on Windows. - Familiarity with CMake. **Location:** Madrid **Remote:** No, unless you are very good, a perfect fit and trustworthy. **Visa Sponsorship:** No **Technologies:** C++(11), Qt **Salary:** One you don't find everyday in Spain, interesting that is. **Contact:** Contact us by email at jmguerra_at_randomcontrol_dot_com or by skype (using the same email). 
&gt; Two-phase lookup is how templates have always worked Actually, [templates originally did not require implementations to check non-dependent names early](https://blogs.msdn.microsoft.com/vcblog/2017/09/11/two-phase-name-lookup-support-comes-to-msvc/). Microsoft's implementation really wasn't suited for two-phase name lookup so we opted to do the perfectly standards-conforming thing and not support it. Then the standard started to require two-phase, and we couldn't very well move because of our very 1980's compiler design. *Just as a small digression, I'll note that I hear people in the C++ Standards Committee arguing from time to time whether GCC or Clang's implementations can support proposed features. The origins of this problem are not unique to MSVC.* &gt; MSVC does not support writing modern C++, not even a little bit. Quite the opposite, it encourages writting classic C++. [We have been making incredible efforts towards making our compiler completely conforming](https://blogs.msdn.microsoft.com/vcblog/2017/03/07/c-standards-conformance-from-microsoft/), starting with a [complete rewrite of the compiler's parser](https://aka.ms/CompilerRejuvenation). We never hide the fact that we are still the last major compiler to fail to compile modern libraries--I had a lovely discussion with Eric Niebler on Twitter about Range-v3 just last week. But I find it slightly uncharitable and possibly unfair to claim that our compiler "encourages writing classic C++". If you truly believe that it does, maybe you should look again at our progress, including adopting our `/permissive-` standards-conformance switch that removes MSVC-isms from our compiler. We had to get the whole of Windows's public surface area to move in order to make this level of conformance possible. Moving legacy code is expensive (and doesn't add user-benefiting features.) **We are committed to standards conformance.** But it takes time to turn around a compiler that has to support millions of projects that are already out there and developers that don't want to change their code. You should find that our expression SFINAE implementation has improved greatly in the [Visual Studio 2017 "15.5" previews](https://www.visualstudio.com/vs/preview/), and it should be complete in next year's "15.6". There will be bugs. There are always bugs. But our expression SFINAE is getting close to correct. *(Another amusing digression: our /u/STL yesterday checked in some implicit deduction guides and noted that his testing of them had revealed two bugs in a major compiler and one in the Standard. This happens all the time. There will be bugs.)* We appreciate your passion about our product. It's great that developers are concerned for our success! Feel free to reach out if you have any other comments or questions about MSVC's progress.
Fixed, thank you. I can't stand the iPhone Reddit app! 
I only skimmed the slides, but I do have one question. How do you justify mixing tests with prod. code, when doctest invokes UB, especially given the several UB talks that were given this year (again)? :-) To elaborate, forward declaring (also defining or specializing) things in the `std` namespace is considered undefined behaviour, which is a very bad mojo in production code. _This is actually meant in good spirit_
I was supposed to go to this but picked up a terrible flu a day before. So glad to catch this now.
I know, I was there. Herb Sutter got to the last slide and just left.
Exactly. Especially when used to enable or disable wide swaths of functionality (in a safer way than ifdefs). 
Yes, but also something different, which isn't that visible if you haven't been to one of the last two or three Qt DevDays/World Summits: This is not a pure dev event (anymore). Its full of booth space, has lots of non technical, business/design related talks. So while many Qt devs come there too, you also get a lot of the decision makers and team leaders and QML/JS/UI Focused people. Reaching this audience for C++ is an awesome chance.
I won't say that's a stupid stance, but... Javascript is not a bad language to write small functions, bindings and small expressions in order to have nice UIs. It's what it was designed for, and that's how you are supposed to use it in QML All you logic, processing, etc should be written in C++, then you push your data to QML using signals and properties bindings. 
Unfortunately, that means there was not a lot of technical, in-depth talks. Still, it was a great event and I'm very happy that I was able to attend, I did meet lots of great people.
doctest provides [a config option](https://github.com/onqtam/doctest/blob/master/doc/markdown/configuration.md#doctest_config_use_iosfwd) to force the inclusion of the appropriate header for the forward declarations. Also Boost.DI does the same, but that doesn't mean its safe... And since I'm doing this hack I need to test properly - so that's part of the reasons the build matrix is so huge - over 300 (or 350) different configurations (compilers, debug/release, OS-es, valgrind, sanitizers, standards, etc.) But it is a legitimate concern none the less! I tend to lean towards the practical side though...
I realized that the example section in README was not completely synchronized with the latest API version. This has been corrected, all examples are now compilable.
Idk if it is on ios, but the Reddit is fun app is the best one I've used. Used it for years now and much Better than the official app. 
Thank you. There is no string syntax for captures or identifier yet. I tried to be as close possible to perl regular expressions. I'm not sure how to do it properly and in a compatible way. Btw I found you on cpplang slack, you can as question there, it will be quicker.
Please do. I had the same opinion until I tried to actually make something, then I loved it, not necessarily the Javascript part, but overall is a pleasant way to develop apps even for mobile platforms.
Visual Studio is an IDE, not a compiler. Most large open source projects compile with MSVC (the microsoft compiler) if they run on windows. The majority would compile with MSVC and Clang and GCC. 
Agreed. If a software maintainer uses Visual Studio, the rest of the contributors are not obliged to use VS as well. They can still use whatever they like.
Microsoft itself actually has a decent number of open source projects on their GitHub including a pretty cool flight simulator for drone automation.
You can use CMake to generate Visual Studio project files, and merrily hack away in that IDE if it suits you. Most open source projects have at least a few contributors, so different developers might use different IDE's or editors, especially if it is a cross platform project. Projects will often set up build servers that will build the code on other platforms automatically, so a user working on Windows can check in teh code and see if they broke anything on Linux before they try to merge their branch. You might try asking something like this in /r/cpp_questions, which is mentioned in bold text on the sidebar as the place for it.
Some open source projects include a vcproj project file in tree somewhere. Usually it's "community contributed" though. It's pretty atypical to require that new pull requests that add new files must update the vcproj file, since the majority of open source devs won't use vc. And also, travis-ci can't use the vcproj.
Heh, that's pretty much my feeling. But QML seems a lot saner and more internally consistent than doing similar work as a web page. And any complicated behaviors, you can always just implement a behavior in C++ and make use of it in QML. No system is perfect, but the Qt stuff is still my favorite for doing the kind of stuff that it does well.
or just use js and html5
Nothing wrong with VS, if you know how to use your tools. It's just a code editor. You don't even need to use the compiler if you don't want to.
Was there too, but only on the first day. Yes, less technical talks, but for that there is Meeting C++ ;) Herb seems to have repeated mostly his keynote from CppCon, so I did not miss much... 
And if you're willing to move into unsupported and discouraged territory, it's actually possible to build working XP SP2 (possibly even earlier, I thankfully haven't needed that) applications that compile with both /std:c++17 and /permissive-. This isn't meant as a recommendation, but rather a data point for anyone else unfortunate enough to need this info. Kudos to the MSVC team for making it possible.
Except you can still write to .c_str(), .data(), and the like
You're welcome. BTW I tried commenting the above on your blog (I saw it elsewhere before it was posted here), but either you're behind on your moderation queue or wordpress ate it.
The more interesting question is usually whether the project depends on the posix API or not.
I think Herb is on an evangelist mission to convert the world to meta classes. He his gonna do this talk daily until the committee approves the feature.
I'm agree that 'someone could do potentially do this other stuff in theory' while being competitive with only the std::unordered_set should be called 'The Holy Grail'. 
Can you comment on your turnover rate?
/u/8bitslime is right, although I'd say it's more than a code editor : ) But VS supports multiple compilers and build systems. You don't have to use MSVC or MSBuild. It's very configurable. 
I've not really looked into it, but is dependency on the POSIX API a good or a bad thing? I've encountered it quite a few times and never had any trouble with it, but I'm wondering if its considered good/bad practice? 
I mean, up until a month ago I developed primarily with VS (now just vs code on ubuntu lol) and when it came to situations where posix API vs Windows API came up I was still able to get some semblance of the posix code in place using preprocessor macros. Thankfully, that's starting to disappear more and more: `std::thread` came first, but now we've got `std::experimental::filesystem` replacing more OS api calls. Mostly just leaves DLL handling stuff as the most commonly encountered OS api calls (maybe aligned allocation? or has that been wrapped with the standard library too?)
Pretty much everything said about Modules TS in this talk is wrong. Specifically, modules are hierarchical and the only reason there is no `std` module yet is because the standard library modularization proposal is just a first draft. Even the module names are not final (`std.threading` or maybe `std.concurrency`). But if you want `std` right here and right now, there it is, you can compile it and it will work: export module std; export { import std.core; import std.io; import std.threading; import std.regex; import std.filesystem; } 
https://www.reddit.com/r/cpp_questions/
you don't need to use CMake to generate project files in 2017 anymore. We recommend that people use the open folder experience and just do file-&gt;open folder and point at the folder that has your CMakeLists.txt file.
forgot about that sub 
What are you trying to achieve with the conditional?
The condition that you've written can never be false, it's always true. For example, if `senior` is `'Y'` then the first test is false and the second test is true, and `(false || true)` is true. Essentially you've written `if(true) ...`. You do not want to use an 'or' here. 
If senior is not 'Y' but 'y' it still executes. You probably wanted if( !(senior == 'Y' || senior == 'y') )
Good luck binding that to C++
it looks cool, but would not it crash if the input file had a few empty new lines at the end? doesn't seem terribly robust
We can't eliminate the overhead because it would affect layout and create binary incompatibility when mixing third-party libraries with different settings.
Whether a project is open-source and the tools its contributors have used to contribute to it **COMPLETELY** orthogonal. Those two things are **COMPLETELY** unrelated to one another.
&gt; Usually if the project aims to be cross-platform OP asked about open-source, not cross-platform.
Paging /u/Rusky, the developer who's looking at your ISense bug. As for the ICE, it's ill-formed code so it's not going to be high priority for a fix. But it's always good to fix ICEs. Thank you!
It's not as easy as qt but writing v8 bindings is not terribly difficult. I'm curious if qt and qml offer any other benefits though
As an outsider, but follower of Qt since the KDE 1.0 days, it looks to me that the project have been trying to pivot to QML/JS/UI as a means to keep Qt relevant to new generations, as an alternative to the current Electron craziness. 
Highly recommended
I don’t think you can boil it down to being a good or bad thing. It’s just a question of what makes sense for the particular project.
Awesome material! I guess we shouldn't be surprised that ILP can trounce algorithmic complexity; I so loved Robin Hood hashing though :( Is this ever going to be open-sourced? (A quick google search didn't turn it up...) --- There is one potential improvement that was not mentioned: bounded probe-length. I'll mention the downside first: on `insert`, you have to check against the upper bound of the probe-length, and resize if it's reached (or refuse the insert...). This may cause some issues with particularly large probe sequences. However it's possible to really take advantage of it by reserving space not for Capacity elements, but for Capacity + upper-bound (+ maybe some spare, if you process elements 16 at a time). This means that on look-up: - bounds-checking is unnecessary: you have a guarantee that there is an empty element (or 16) before running out of bounds, - wrap-around is unnecessary: see above. Now, for 2^N size the wrap-around is not too costly (just bit-masking), but for other sizes it can get a bit more complicated, so when experimenting with non-power-of-two sizes, it's something to keep in mind.
He mentions during his talk that it will be open sourced in Abseil, potentially in End of this year, but most likely Q1 or Q2 2018
Ok I was just making a point, I know it does literally everything under the sun ;)
No problem, and thank you! I sometimes wonder if it does too much....
Could you explain when I should use POSIX and when I shouldn't. 
The point here is precisely that expression must not be evaluated, under certain conditions, even if they happen to have side-effects. It’s responsibility of the programmer to not depend on those side effects.
&gt; I'm curious if qt and qml offer any other benefits though a way saner DOM, and it's one of the only "mainstream" languages with true reactive bindings (eg the expression 'x: y + sin(width) * 2' means that whenever the value of y or width change, the value of x is updated. eg paste this in qmlweb.github.io (or better, in a .qml file and run it with qmlscene) : import QtQuick 2.0 Rectangle { width: 500; height: 200 color: Qt.rgba(x / 20, y / 20, (y+x) / 20, 1) x: 10 * Math.cos(elapsed) y: 10 * Math.sin(elapsed) rotation: elapsed property real elapsed: 0 Timer { onTriggered: elapsed++ running: true repeat: true interval: 32 } } 
Let's not ignore that any C++ class written with Qt is available with minimal hassle to the QML side of things. All you need is a single function call, and it will expose properties (with getters, setters, change signals), signals, slots and functions. In addition to this, it's very easy to create data models that connect to databases that can be inserted into QML items. This isn't even half of it, most of the C++ side of things integrate quite nicely. Is there any readily available solution for this with V8?
Yes. For example, symlinks were added in Vista so every API we need to talk with them needs to be guarded. The current experimental implementation gets around this by outright not supporting symlinks at all.
You can see your problem without having to introduce static and dynamic libraries. You compile a cpp file that export a module interface. That cpp file is then compiled into an object file. Since that cpp file don't use the template, there is no instanciation done. Yet, other TU uses your exported template, link and work. What's happening here? Since the cpp file exporting the template don't instanciate the template, other TU must do it. And other TU knows how to instanciate it! A compiled interface is a binary file that contains all the data needed by the compiler to use that interface later on. When exporting the template, it must serialize all the information needed to instanciate the template. So it effectively serialize the template AST and other info into the module interface binary file. Does it look like C++98 `export template` feature? Yes! But it's extended to every declaration, not just templates, and added an `import` and provided a way to export a block with `export { }`, and fixed every other problem `export template` had. So it's not really an extended `export template` feature, but kind of the right way to implement the exportation of declaration, and the right way to consume those exportations.
Let's not forget the performance implication of that declaration - could be noticeable if called often enough. I had made similar changes, but was investigating runtime perf: https://stoyannk.wordpress.com/2016/12/10/static-variables-in-functions-bite/
For clarity I've reversed the order of your quoted sentences below, apologies: &gt;So it effectively serialize the template AST and other info into the module interface binary file. &gt;And other TU knows how to instanciate it! Sure, I get that part. The confusion on my part (I think) doesn't happen until libraries become involved. Does all of that serialized template AST in the module interface files end up embedded into a static library (libfoo.a, foo.lib) file? How can that happen without the static library linker [which on Linux is just `ar`, a primitive relative of `tar`] growing the know-how as to how to do so? Or does the build system for the library alternatively have to install the serialized AST files somewhere, where it can be accessed by clients that want to link against the library? In the case of a dynamic library: does the AST conversion to a template instantiation (thence to object code) happen at link time, or at run time? If the former, then it breaks the long-standing ability of users to swap out differently-implemented but API-compatible DLLs at runtime. On the other hand, if the latter, then we'd seem to need to be able to do the AST de-serialization and conversion to new template instantiations even on systems with no compiler or linker. Thanks, Kevin
It could make sense if/when those teachers and students perform those tasks you described on computers owned by the institution, not their own. No big deal, it just jumped at me because I've had to analyze licenses for this sort of case recently, and I got curious after reading yours.
&gt;Does all of that serialized template AST in the module interface files end up embedded into a static library (libfoo.a, foo.lib) file? How can that happen without the static library linker [which on Linux is just `ar`, a primitive relative of `tar`] growing the know-how as to how to do so? A compiler could embed the interface information in the static library, but usually, a separate file is outputted, and the path to that file must be provided in the compiler arguments.
Nothing changes. A precompiled module is essentially just a binary representation of the public parts of a header. Your templates must be in the precompiled header if you expect consumers of that module to be able to instantiate the template. &gt; So the foo() template is now defined in its own .cpp file that exports its definition in a Foo module. This is the part you've got wrong. The templates cannot move to any ol' file. Public templates must be fully defined in the module interface unit, just like they had to be defined in the header before.
I'm going to assume this video was taken down because of the audio sync issues, and we be back soon!
How bad is the overhead? I had a program written in PyQt that took seconds to load a QStandardModel table with 600k entries, in c++ it was instant. 
I don't see anything inherently hierarchical in the Modules TS. Sure, there's the implicit expectation that there will be some kind of hierarchical naming conventions for modules, but I don't see anything stopping you from doing something like this: ``` export module banana; export { import std.threading; import std.filesystem; } ``` Which I feel is kind of the point; it's yet another "this must work with _any_ project structure, better make it _really_ general", which essentially means throwing away all guarantees and conventions (implicit or explicit), ending up with headers, pretty much (but with a different syntax).
Thanks, you and /u/gracicot helped a lot. Let's see if I can rephrase this correctly after a little more mulling it over. A good mental model for a compiled module interface (.ifc file, or what-not) is *NOT* anything like an object file. It's a lot more like a header file that was auto-generated from the contents of the module interface .cpp file (if one were to imagine a sufficiently intelligent header auto-generation tool), and then pre-compiled. In the pre-module world, source code for "libfoo" might contain foo.cpp and foo.h, which would get built (on Windows) to foo.lib (perhaps + foo.dll). The author of "someprogram" would need foo.h and foo.lib (perhaps + foo.dll) in order to code with the library. And the author or other distributor of "someprogram" would distribute someprogram.exe and possibly foo.dll to end users. Whereas in the brave new module world: the library author would only write foo.cpp, which gets built to foo.lib (perhaps + foo.dll) and foo.ifc. The author of "someprogram" would now need foo.ifc [rather than foo.h] and as before, foo.lib (perhaps + foo.dll) in order to code with the library. And just as before, s/he has to distribute only someprogram.exe and perhaps foo.dll to end users. Either with or without modules being involved: when there is a runtime DLL, all template instantations in "someprogram.exe" that came from library templates would be embedded in the object code of the .exe, and *not* in the object code of the DLL if they were not already instantiated there. Hope I got that all right. Many thanks for the clarifications!
What I meant by this is that as long as the code is using only the standard library and other cross platform libraries, the IDE/compiler you are using is not really important. If the project doesn't use cmake or the like, It might take some work until you have all the right include paths, flags and project structure set up but those are purely additive changes that don't interfere with the files under version control. However, when the project starts to rely on platform specific APIs (and a lot of projects assume a posix OS) using visual studio and development on Windows in general becomes much more difficult.
Sadly, another one of those rare reasons for those ugly macros: namespace util { //! Equivalent to std::make_optional (cond, value) except that //! evaluation of value is deferred, which is often a requirement //! when value is dependend on cond and would require `cond ? //! std::optional&lt;T&gt; (value) : std::optional&lt;T&gt;()` instead. #define UTIL_MAKE_OPTIONAL(cond_, how_...) \ util::detail::make_optional (cond_, [&amp;] { return how_; }) namespace detail { template&lt;typename Fun&gt; auto make_optional (bool cond, Fun&amp;&amp; fun) -&gt; boost::optional&lt;decltype (fun())&gt; { using T = boost::optional&lt;decltype (fun())&gt;; return cond ? T (fun()) : T(); } } } with auto const opt = UTIL_MAKE_OPTIONAL (data.count (key), data.at (key)); Sorry.
You might want to use italics instead of bold caps to yell at someone.
The main problem is - as you said - that the expectations where much higher and they will be even higher for c++20. It is not quite an even comparison, but if you think about the changes between c++03 and c++11 (~8 years), c++14 and 17 did little more than tweaking the language. Most of the goodies you could actually have much earlier with a library like boost anyway. FYI: The two main features I'm using with c++17 so far are constexpr if and std::optional, but I didn't have a chance to use c++17 in production code yet, so maybe I'm underestimating it..
I'm in a parking lot, but try passing in a `std::mem_fn(&amp;user::userInput)` for the first parameter.
The point is to load the model in C++ and only handle the GUI logic in JS, do the JS is simply passing references/pointers around and calling C++ code. For small segments like this the performance impact is almost nonexistent. Of course you *can* abuse it, but generally the easiest course of action seems to also be the correct one with QML, making accidental abuse unlikely. My only gripe with qml is that qtquickControls still aren't as fleshed out as qt widgets.
&gt; [...] usually, a separate file is outputted, and the path to that file must be provided in the compiler arguments. You must do this for libraries [...] A little more research turned up the [GCC Wiki page](https://gcc.gnu.org/wiki/cxx-modules) for modules. Which interestingly has this remark: &gt;Compiling the interface TU generates a Binary Module Interface. [...] The BMI is not a distributable artifact. Think of it as a cache, that can be recreated as needed. If in the GCC authors' envisioning, the binary AST files are not something to be distributed, then I wonder what their plan is as to how this will work for libraries! (I know that it's early days yet...)
I think the community could gain a lot from having, in each compiler, a translator from their binary interface file to something like IPR, and from IPR to their binary interface file. This would still allow the compiler to have it's own format, but would allow someone to ship the IPR files with their libraries, and even embed the interface into libraries. It might someday become like that. We still don't know, a lot of stuff can happen.
Createan instance in a place that'll make the object instance outlive the call to your threaded function. Then use std::bind(&amp;Foo::Bar, &amp;instance) and use the returned callable. I hope I understood your question correctly.
As a member function always needs to operate on an object you need to specify both. The easiest way is to pass a statefull lambda. https://godbolt.org/g/VSCRnu
Seems right to me for the most part. &gt; The author of "someprogram" would now need foo.ifc [rather than foo.h] and as before, foo.lib (perhaps + foo.dll) in order to code with the library. Realistically, they'll need the module interface unit's source, just as they need header sources today. That file may be called something like `foo.mpp` or whatever instead of `foo.h` but either way it'll be C++ code composed of various declarations/definitions. There is not yet any stable cross-vendor binary module format. Microsoft appears keen on the idea of the IFC format but there's not yet been buy-in from Clang or GCC, or any other compiler vendor for that matter (that I know of). The binary formats used by each compiler may not even be stable across minor compiler revisions (or command line parameters!) so shipping them to developers in lieu of module interface sources is likely to be more trouble than it's worth.
I skimmed the code a bit, and it seems that you are right in that SDL_DestroyWindow doesn't implicitly free the renderer, **but...** SDL_CreateRenderer does this: `renderer-&gt;window = window` where `window` is the `SDL_Window` passed to it, so it can't be good if you call `SDL_DestroyWindow` and then try to use the renderer. There doesn't seem to be any reference counting.
you can invoke a member function on an object like this: std::thread t (&amp;YourClass::Memberfunction, std::ref(yourclassobject), functionargument1, functionargument2,...); but i think its easier with a lamda :-p
I've been using lua in fusion (composing software) and I hated it. This talk opened some interesting parts of it. Should also read a book probably..
Okay, a better answer (with help from Peter Dimov): `(void)0` is needed &gt; when you don't already have `void_t` outside of the `decltype`. The `void_t` makes it void anyway, so no need to make the `decltype` yield `void`. although, to be on the 100% safe side, you could put the `(void)0` after each expression to guard against an overloaded comma operator 
Choosing the right scripting language can bring in new users that would otherwise pass. If you're working on a role playing game that allows modding and don't want to write your own language, Lua is a very good choice. Particularly because there are lots of people who have written mods for World of Warcraft in it. ;)
Those puns are on point.
I see, thanks. That seems unfortunate. If a source .mpp file in a library contains complex template definitions that are expensive in time and/or RAM to compile (think Boost.MPL or Ranges TS), then it seems like being able to reliably build a program against a precompiled binary AST representation, instead of the .mpp file, might save a lot of resources on the library user's machine, no? Or is most of the resource usage on the template instantiation side of things instead and therefore unavoidable? Another thought I have about this, concerns the definitions of non-template, non-inline functions and methods that get compiled to normal object code, which we want to export from a module. Say that we have a function `really_long_definition()` that is either really long itself, or else calls lots of single-use locally-defined (unnamed namespace or static) free functions. It seems to me that we want to have only the function declaration, not its entire definition, be in the .mpp file. So that the .mpp file and/or any binary AST file built from it are not bloated with the entire function definition. But then we are forced back to the DRY violation of having to repeat the function signature in a separate module implementation .cpp file where the function definition lives (that gets built to object code), as well as in the exported declaration in the .mpp file. This is the very same function-signature duplication we have in pre-modules C++ with declarations in .h files and definitions in .cpp files. I'd hoped that DRY issue was one of the things modules were supposed to help us get rid of... :-/ Apologies for delving so deeply into all the nitty-gritty of this ... when not busy writing C++ I am one of the dorks who maintains the build infrastructure where I work, hence my interest in understanding how Modules will affect this eventually.
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/78yyc2/how_to_pass_member_function_thread_c/doxzf56/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
I can say the same about C++ compilers. Just build LLVM 5.0.0 with static dependencies and ship everything statically linked. Only the kernel versions and glibc have to match unless you manage to build it with static musl (if you do, PM me!) cmake -GNinja \ -DCMAKE_BUILD_TYPE=Release \ -DCMAKE_INSTALL_PREFIX="/opt/llvm" \ -DLLVM_TARGETS_TO_BUILD="AArch64;ARM;X86" \ -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD="WebAssembly" \ -DLLVM_INCLUDE_EXAMPLES=OFF \ -DLLVM_INCLUDE_TESTS=OFF \ -DLLVM_ENABLE_WARNINGS=OFF \ -DLLVM_ENABLE_PEDANTIC=OFF \ -DCLANG_DEFAULT_CXX_STDLIB="libc++" \ -DCLANG_INCLUDE_TESTS=OFF \ -DLIBCXX_ENABLE_FILESYSTEM=ON \ -DLIBCXX_ENABLE_SHARED=OFF \ -DLIBCXX_ENABLE_STATIC=ON \ -DLIBCXX_ENABLE_STATIC_ABI_LIBRARY=ON \ -DLIBCXX_INSTALL_EXPERIMENTAL_LIBRARY=ON \ -DLIBCXXABI_ENABLE_SHARED=OFF \ -DLIBCXXABI_ENABLE_STATIC=ON No license issues that I'm aware of.
I’ve started using it wherever possible, pre c++17 many of the things were doable before but now I don’t need to. I just include variant or optional or string view and go to town. Inline variable. If constexpr. Etc... Earlier today I tried to use a fold expression and was so disappointed because it was not supported on one of my target platforms... like wtf. I spent half an hour doing a workaround with basic templates. At least experimental/string_view is available on iOS. I think that in a few years most devs will be demanding popular libraries move to c++17 for the better APIs and compile time that it will enable.
Thanks for that btw. I’ve used the experimental implementation quite a few times and at least for my use it has worked great! (Mostly for creating and managing log files, data dumps, etc)
When you say 3rd party libs, if I write a basic app that only depends on the windows SDK + dx9 (imGui) and built from source, would I feel the pain?
Funny enough my company still distributes some app updates that are built with the XP toolset but the installer blocks windows XP anyways because of unrelated problems on that platform. I haven’t had time to move it to a newer toolset....
During the talk there seemed to be a slide about doing it with coroutines but it seemed like the speaker dismissed it outright. I have almost no experience with asio so I’d really appreciate hearing the pros and cons on coroutines for the future of asio from someone who has real experience with the library... I’ve only seen toy examples like the one in gors recent talk. They look awesome in that context.
Ok but im assuming you dont want to maintain parallel build systems and projectfiles. If you can get a VS projectfile from cmake, great, but afaik without a good projectfile, its going to be a pretty heavy weight text editor in comparison
So we have a function, doing two entirely orthogonal and unrelated things, just to check off template argument deduction and structured bindings? Why not just: ifstream in_file(argv[1], ios::in); if (!in_file) throw ...; oftsream out_file(argv[4], ios::out | ios::trunc); if (!out_file) throw ...; Does that not count "expressive"? 
VS is big, yes, but it is the most powerful editor for free right now. The build system is completely configurable so you can do it however you like. I prefer to work in MSVC and test compile in GCC every now and then and I enjoy that workflow. Editors are personal preference though, don't forget that.
*Bill forwards thanks to his predecessor(s) :P
Aww yiss
I'm talking about STL data structures, so if you don't include &lt;thread&gt; etc. these concerns are irrelevant to you.
&gt; Which I feel is kind of the point; it's yet another "this must work with any project structure, better make it really general", which essentially means throwing away all guarantees and conventions (implicit or explicit) Module re-export is a generic mechanism that can be used to assembled modules out of submodules (which *is* the convention) as well as re-export an unrelated module from another which, from experience actually modularizing something real, *is* something that makes sense sometimes. Yes, you can use it in a stupid way (just as pretty much the rest of C++). But someone may also find a novel way of using that the authors of the mechanism haven't even thought of. Always remember, *"the imagination of nature is far, far greater than the imagination of man."* &gt; ending up with headers, pretty much (but with a different syntax). I am really bothered by this knee-jerk reaction to modules. Even if you really believe that C++ must have strictly-enforced and locked-down hierarchical modules, think how can this be implemented *practically*? How will the compiler ensure that module `std` re-exports all the submodules that are all defined in separate translation units? And will the resulting complexity really be worth it?
Is there anything like this for Windows?
&gt; Module re-export is a generic mechanism that can be used to assembled modules out of submodules (which is the convention) as well as re-export an unrelated module from another which, from experience actually modularizing something real, is something that makes sense sometimes. Sure, my point was just that modules as defined by the Modules TS are _not_ inherently hierarchical, as opposed to the other examples given in the talk where they _are_. Since they aren't specified to be hierarchical, you can't assume that they are, which means you can't take advantage of any hierarchy that might exist (in the general case). &gt; I am really bothered by this knee-jerk reaction to modules. I don't think it's a knee-jerk reaction. My impression is that modules don't improve things much when it comes to code organization, apart from making more explicit what the public interface of a "module" is (which most projects already accomplish by placing this interface in an include directory, or placing implementation details in a detail namespace and/or directory) and improving separation by not allowing macros to "leak". &gt; Even if you really believe that C++ must have strictly-enforced and locked-down hierarchical modules, think how this can be implemented practically? I'm not trying to argue that it should be hierarchical, just that it isn't hierarchical right now.
Count yourself lucky if you can use the latest and greatest. I have some projects, where I have to stay c++11 compatible. That being said, I've been using stringview and optional for years now. With c++17, it's just the prefix/spelling that changes.
Those 1 based indices uuuff I prefer javascript over Lua, but I used the second because of the excellent sol2 library to glue Lua and C++: https://github.com/ThePhD/sol2
There already is a portable C++ ABI. All the goals in that document are already how most of the world works. The problem is that Microsoft doesn't follow it, and don't publish how their implementation works.
There are more compiler vendors out there besides clang, gcc, and Microsoft. http://en.cppreference.com/w/cpp/compiler_support Secondly, we don't have languages ABI, rather OS ABIs. And in that regard, Microsoft has the best C++ like ABI via COM/UWP, even though it doesn't support all features. C ABI only works, because currently most OSes are written in C. OS not written in C, like IBM and Unisys mainframes, don't have C ABI when using the native environment, only when targeting the POSIX environment.
so in my underststanding you can write 'void_t&lt;bla,bla,bla...&gt;' or 'decltype(bla,bla,bla... (void)0)', both results in sfinae "errors" and match the type `void`otherwise. that would mean, its not needed in this example since the use of `void_t`? if its nessesary for vs2017 it may be a bug? (i just wanna get this right for my understanding :-P ) 
This idea has one main drawback, you expose yourself to a memory DoS attack. The attacker only need to insert 'bounded probe + 1' elements to cause a rehash and can fill up the memory this way. It is possible to mitigate the problem, but it is not a good idea for a general purpose hash map. The advantage of the idea is really negligible, on the tests I did with my Robinhood implementation, the difference between wrapping around or not was so low that it was difficult to even measure the difference. The best is to grow the hash table when some max probe is reached but only if the table also has some minimum load factor. It's what I have done for most of my hash map and it's also what Rust's Robinhood hash map is doing. 
I agree, sadly I feel the C++ committee's main focus isn't on the practical features the community needs for years like: modules, package manager, reflection and so on.
I'm currently evaluating scripting languages for the purpose of embedding one into a large, serious application. What in particular didn't you like about Lua? And are you aware of any languages that would do better? 
Indeed zerexim, I can! Apologies if I haven't answered before, I must admit I am not very active on Reddit. The reason we are constantly hiring is because we want to grow the team and we want to find the right talent for it. Because of that, we are in no hurry and we wait until we find it. This is an ongoing search for us, since we would like to hire as many high level C++ developers as we find. Regarding your turnover question, that is not true. We do not have a high turnover rate, only two people have resigned in 15 years and one of them because of relocation to another country. What we do take very seriously are the 6 months of trial period to evaluate if there is a good fit between the employee and the company. I hope to have answered your question and don't hesitate to ask more if you need or write us to hr@think-cell.com. Soledad on behalf of think-cell's HR team.
I agree it's much, much more than just a code editor. I, for one, have used it to edit .ini files as well ;-) 
What I can isn't the question. 
You may assume such, but at least in our case you'd be wrong. We use the project files not only as a build system, but also to further organize our source code (by putting subsystems in folders). This is immensely useful, but not something you can do with automatically generated project files, AFAIK. Since our workflow is pretty much "build and debug in VS first, then compile on Linux", we do indeed maintain two build systems, but in reality that mostly means adding the occasional .o file to a makefile, which isn't much of a hardship. 
Is n approaching 0?
Am I the only one to feel that he actually sounds rather defensive about what appears to be an amazingly useful feature that is enthusiastically received, at least here on reddit? Is there actually any significant opposition to this idea, in the standards committee perhaps? 
How about a new keyword? `doubleconst int x = 0;` (semantically equivalent to `const int const x = 0;`) (not to be confused with `double const x = 0;`
His articles have usually been great, but this one is terrible advice. There’s *no such thing* as a “function alias”. Instead, what’s being created here is a global, modifiable (!!!) function pointer. Such a thing could be created in C++98/03 too, just with a typedef instead of `auto`. Please don’t do this. Please don’t tell people to do this, *especially* without explaining what they’re actually doing. It can lead to incorrect code (if the function pointer is modified) and raises security concerns (all long-lived function pointers are juicy targets for attackers, which is why we use `EncodePointer`).
&gt;C ABI only works, because currently most OSes are written in C. From what I've read most of Windows is actually written in C++ these days, but they still use a C abi both for legacy reasons and because they haven't stabilised a C++ one.
C++ is supported on the kernel since Vista, but the code is mostly C still. https://www.reddit.com/r/cpp/comments/4oruo1/windows_10_coded_in_c_or_assembler/d4g9we9/ What they did was to re-implement the C runtime in C++ and export the entry points as *extern "C"*. https://blogs.msdn.microsoft.com/vcblog/2014/06/10/the-great-c-runtime-crt-refactoring/ Windows has a stable ABI for all Microsoft languages, COM.
Not an OP but do check also Duktape and JerryScript. 
The fact that it's just `&lt;optional&gt;` and not `"boost/optional.hpp` anymore is actually huge, at least for certain scenarios. Sure we were able to use `optional` for the last 10-20 years but once it's in the standard it becomes practical to use in so many more scenarios. (same for string_view, variant, filesystem, etc.).
[Sumatra PDF](https://github.com/sumatrapdfreader/sumatrapdf)
Would `constexpr auto&amp; alias = some_fn;` be more okay?
I think you are not quite right, we do have language abi such as the IA 64 cxx abi, and an os abi, they can be different, hence the old stdcall function decorators. Feel free to correct me if I am wrong.
Yes there was, we did a lot of work on this at BlackBerry, but it never reached WG21. I did try to summarise the main themes into a paper at https://arxiv.org/abs/1405.3323 and the filesystem part of that work became AFIO v1 which was rejected by Boost peer review. AFIO V2 is the complete rewrite which I hope to submit for standardisation as the File I/O TD next year. The standardized component object model for C++ 17 aka *actual* Modules as proposed by that paper remains a long way off. Nobody wealthy is willing to sponsor its development unfortunately, it would cost quite a few million dollars in total.
Paging /u/ubadair ...
OK, can you comment on your turnover rate during that "trial" period i.e. before one receives 120K permanent offer. I believe this is one year period, right? Your constant advertisement of engineering position leaves the impression that you "hire" people only for that one year period and then replace them, right before offering 120K... 
&gt; I guess we shouldn't be surprised that ILP can trounce algorithmic complexity; I so loved Robin Hood hashing though :( I don't understand why you say this. He uses SSE instructions to check multiple bytes at a time in the meta data, but what about this excludes Robin hood hashing? 
How is SSE going to help with Robin Hood? AFAIK there are two main variants of RobinHood: a) store array of hashes + array of payloads. When probing check if hash matches, hash distance from ideal is greater than current or hash == empty. Since hashes are usually 64 bits, SSE won't help all that much here. b) store array of distances + array of payloads. When probing check if distance is greater than current. SSE can be used to compute the probe length (load 16, cmp gt current distance, movmask, count number trailing ones) but it is questionable if this is going to be faster than walking the array of distances. Perhaps you have something in mind?
C++ has had a generally implemented ABI based on IA64 for something close to 10 years per my recollection. This is why you have a chance of writing C++ shared libraries and the ability to cross compile C++ with mangled symbols. Now, is everything working with that ABI? I doubt it, but the ground work is done. Lead a horse to water... 
Those are both quite interesting, thanks! Javascript has a particular advantage in that it is easy to find online resources, and easy to find people who know it, two features greatly favored by my customer. Having multiple implementations of a language also offers some hope that if one of them ceases development in the future, another might be put into its place without having to replace all the user scripts. 
&gt; One Definition Rule (ODR) violations are prohibited by the C++ standard and are automatic undefined behaviour territory, yet they are widespread in any C++ codebase of any size whether hidden via static storage, anonymous namespaces or DLL symbol visibility boundaries I disagree with this one. When you're at the DLL boundary, you have entirely left the areas that C++ covers anyways and are only dependent on what your operating system does with dlopen / dlsym (or equivalent). The C++ standard does not care about what's in a compiled library or executable. It's "undefined behaviour" in the sense that apple pies are UB because their recipe isn't defined by the C++ standard either. I think that the ideas in this article are basically what you get in environments like Smalltalk (or TempleOS :p) where the operating system, the applicatoins, and the programming language all interoperate at the source code level in permanence. One needn't say, they aren't very successful.
1. Microsoft doesn't use that ABI 2. Itanium only covers the language ABI, it doesn't cover the library ABI. std::string from one toolchain version isn't guaranteed to be compatible with std::string from another toolchain version. For example, gcc 4.x and gcc 5.x have different, incompatible string layouts.
&gt; We have a few closed source dependencies that limit our options severely. I don't understand the problem. "closed source library" built with gcc 4.2 still works with gcc 7.
The best would be `using func = otherFuncName;` I think there's a proposal for it.
Thanks. This is exactly my point and I could not have said it better myself.
&gt; &gt; &gt; Thankfully, that's starting to disappear more and more: std::thread came first, but now we've got std::experimental::filesystem replacing more OS api calls. Mostly just leaves DLL handling stuff as the most commonly encountered OS api calls (maybe aligned allocation? or has that been wrapped with the standard library too?) why wouldn't you use boost for this ? It had all of this since decades. For instance for DLLs you can use Boost.DLL which allows loading dlls in a cross-platform way. Or if you want, you can use Qt to the same effect.
Oh right. I had completely missed that. Thanks so much, I worked your remarks into the post. So do you find that using const pointers mitigates the problem of incorrect code (even if it wouldn't solve the security issue)? Also, are you saying that a function reference is better than a function pointer (otherwise what would be the point of `auto&amp; r = func`)?
Funnily enough my reddit showed me this post as I am debugging some race condition involving QTimer! 
If there is interest for it, is finding a few millions really that hard ? Lots of company are heavily invested in C++, starting with google and Microsoft.
It's a very big ask for that sort of funding commitment from *any* organisation, no matter how wealthy, when the payback is not immediate and its effects are diffuse. It's not how multinationals work unfortunately. The only reason we were allowed to build out working prototypes etc at BlackBerry was because it was thought at the time we would be extremely flush after the BB10 launch and would need big scope projects to spend the free cash on before another division got there first. So it was defensive R&amp;D on the part of my division so we got our slice of the pie and didn't get cut out from the bonanza. In any other circumstance, it never would have gotten even as far as it did.
We have played with bounded probe-length. Our experience thus far is that it does not improve performance for good hash functions. For bad hash functions, you are better off identifying and fixing the hash function then letting the table balloon in size.
That most likely was PyQt's fault. With Qt you can still populate the QStandardModel in c++ and (eventually) write the view in QML. I said *eventually* because TableView is not available in QtQuick Controls 2, yet.
I have done multiple implementations using JS as a scripting language in C++, using V8. It works great and the customer is usually more inclined to adopt scripting with JS, a language that is way more popular than Lua. I don't doubt that Lua is a mature and more cohesive language though.
AFAIK the C++ ABI is different, no?
it hasn't been broken since gcc 3.4 if you use the c++03 abi https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html. GCC 5 introduced a new ABI but it is here only if you use -D_GLIBCXX_USE_CXX11_ABI ; else it's still compatible.
Sounds promising. Have to take a closer look then. Thanks!
Except hackers don't care what you compile it as. It's still exploitable.
Except hackers don't care what you compile it as. An array is still an array, it'll never be not exploitable with direct memory access.
So hackers make my vector accessed out of bounds? Through pure will?
Through WriteProcessMemory, DLL injection + memcpy, etc, there's many different ways.
&gt; GLIBC: A per-thread cache has been added to malloc. wow, that's big. interested to see how it compares to tcmalloc/jemalloc
&gt; This idea has one main drawback, you expose yourself to a memory DoS attack. Maybe. Unbounded probe-length exploiting a weak hash function is a well known attack vector which can slow down a server to a crawl by turning what ought to be a O(1) look-up into a O(N) one. Some such attacks have been demonstrated with *thousands* of elements hashing to the same bucket. Unfortunately, from experience it is much easier to spot/deal with a dead service than a slow service; therefore, I would rather have a process crash over having a process slow to a crawl. But would the process actually die? It will depend on overcommit and how quickly you grow: - if overcommit is off, then aggressively expanding memory will indeed cause the process to die from memory exhaustion, - otherwise, as is I think the most common, the process will consume more and more address space (but not real memory) and will finally die when it either runs out of address space or the memory allocator throws its hands at the impossible size of the allocation size. I am not sure, thus, than such an aggressive growth would be significantly easier to DoS. 
&gt; I don't understand why you say this. He uses SSE instructions to check multiple bytes at a time in the meta data, but what about this excludes Robin hood hashing? I never said this *excluding* Robin Hood, however Robin Hood hashing with backward shifting deletion has regularly topped the benchmarks up until now and the claim here is that an algorithmically simpler linear-probing implementation (with SSE for probing) performs better. Therefore, it seems this new contender steals the spot light. --- I would note that one could perfectly think of mixing Robin Hood hashing + SSE. However, Robin Hood hashing has a relatively complex insertion and deletion process, shuffling elements around to minimize probe length. It essentially attempts to trade-off a slower insertion for a faster find. When using SSE instructions to check *16* buckets at a time, reducing the probe length by less than 8/16 simply may not do much to improve find-time. So such an implementation would be slower to insert (Robin Hood logic) with no significant gain to find... until probe sequence *really* degenerates; at which point you may want to check your hash function. Of course I would not be surprised to learn than a Robin Hood implementation would allow significantly higher load factors (90%? 95%?); in some cases it may be worth sacrificing some CPU for lower memory requirements.
Is there a better viewer for cmake targets than just the CMake-&gt;Build Only menu? It'd be nice if there was an added explorer for viewing those options when using Open Folder. Love that the Solution Explorer actually shows the Folder on disk by default in the mode too :) Any plans on allowing users to point to a CMake version separate from what VS uses once all the MSVC changes are merged/the cmake daemon is fully unified?
You can turn the question on its head though: How does the lack of abi/proper modules costs google each year ? Probably a lot. But I understand these works are not easily funded. I remember funding being a hot topic after openssl heartbleed. On an unrelated note, AFIO looks interesting !
I'd recommend you just use objective-c instead. They are both pretty similar except for a few changes that sometimes makes the coding a bit more complex. But don't worry as there are programs that can help you, such as checkmarx and others that works the same. Good luck!
Yeah, your comment somehow ended up classified as spam without me noticing - sorry about that! I've restored it now, and also wrote a follow-up post where I explained the functor-based approach.
And when they infected a DLL they need to access my vector out of bounds?
The main reason that would be not just better, but *way* better, is because it would cover function templates. Right now the only way to have easy access to a function template is to actually write a function that forwards to it :-(.
That is a nice improvement of the dense_hash_map. However unless I am mistaken, there is still a peak memory usage of at least 3x (6x if the resize occurs at a 50% load as dense_hash_table does) when resizing. I wonder if Google is planning a similar improvement for sparse_hash_map?
I can't think of ways to break const pointers or references, since they aren't reassignable. However, the reference is better, because of how it interacts with address-of (and `reference_wrapper`). Consider what happens when someone says `&amp;func` - they get a function pointer. Same for `&amp;func_ref`. However, `&amp;func_ptr` returns a pointer-to-pointer-to-function, which is not useful - so that's a way in which it breaks down as a function imitator.
I think so, but I haven't done that so I can't recommend it.
So also: - V8 is Chrome's JavaScript engine (also default in node.js). Super optimized but more heavy (vs Duktape's "can run on platforms with 160kB flash and 64kB system RAM"). If you are on PC/Mac or mobile you can use that if you need performance - There is also ChakraCore (https://github.com/Microsoft/ChakraCore) made by Microsoft, MIT licensed, powering Edge Browser and Windows I guess. Also performance on the V8 level and JIT can be disabled so you can run it on iOS - JavaScriptCore is WebKit's (Apple) engine. Have seen it embeded on non-iOS apps but probably not so popular choice (at least outside of Apple world). I'm not sure but QtScript seems to be using it. Works great on iOS though (built-in) and if you want JIT performance on iOS you have to use that (iOS security rules disallows to execute code from allocated memory in runtime which JIT needs to do) - There is also Mozilla's engine but I don't know much about it Those options + Duktape/JerryScript are probably most popular and safest to use (from https://en.wikipedia.org/wiki/List_of_ECMAScript_engines). I've just noticed that JerryScript is now "jerryscript-project". It's Samsung's project (but in commits history now I see even Intel's people). They moved repo from their organization. https://github.com/Samsung/iotjs &lt; this is using it for example. It looks now like I'm sort of JavaScript fanboy and lover but I wanted to just point out Lua alternative. New JavaScript is not that bad language (and most of the features can be transpiled to old dialect). I've found [this](https://realmensch.org/2016/05/28/goodbye-lua/) article and stopped considering Lua.
There are many posts like this. This is my usual response: Use the STL. Smart pointers, containers, and especially algorithms. The algorithm library cover much more than you think. Yes you could use raw loop or raw arrays, or raw pointers, but there are tools that do all the hard stuff for you. Don't fall into the trap of using `std::shared_ptr` everywhere. Try using a value first. If it won't work, use `std::unique_ptr`. If you absolutely need shared semantics, then use a shared. In C++, you should think about ownership of resources. Whether it's memory resources, a database connection, a file handle and so on. You should define who is the owner of which resources and design your application accordingly. Even with smart pointers, *non-owning* raw pointers are file. They are observer to a resources. OOP is great, but not everywhere. Java fits every problem into OOP. If the only tool you know is a hammer, everything look like a nail. In C++, you could exclusively do OOP, but knowing you have other tools, you could have make your code much smaller, or easier to use. Learn templates. They are nothing like Java generics. They are one of the most powerful tool in the language. Templates and constexpr allows you to compute stuff about your program, and allows you to make decisions and logic about your own code while the compiler is compiling your program. Also, templates are a form of compile time fuck typing.
&gt; then it seems like being able to reliably build a program against a precompiled binary AST representation, instead of the .mpp file, might save a lot of resources on the library user's machine, no? I communicated poorly, sorry. :) The compiler will still have to compile the module interface unit into a binary representation used as input to all further TUs importing the module. To clarify, a module _interface_ is just the .mpp file, not the entirety of all the .cpp files that get implement the module. I meant only that you can't _distribute_ the binary module if you ship your library to third-parties. The binary file is like a precompiled header: the build system can produce it and reuse it, but you can't package it up and hand it out to others. &gt; It seems to me that we want to have only the function declaration, not its entire definition, be in the .mpp file This is the case. A module interface is akin to a header. That's by far the easiest (over-simplified) way of thinking about it: a module interface is just like a standardized precompiled header except that it only exports precisely what you ask it to.
The talk mentions that on processors without SSE2, the implementation defaults to using 64 bit arithmetic limiting to 8 items per group. I think that it would be great to add to absl::uint128 support for the instructions used (cmpeq, movemask, etc...) which would use sse if available or default to 64 bit arithmetic. That way the hash table could simply use absl::uint128.
Sure. That paper was basically a summary of all the ideas we were throwing around at BlackBerry at that time (2012/2013). Some were much more fully baked than others. Many of the C++ leadership reviewed or read that paper linked, and most didn't agree with it in detail. But it's a *position paper*, it is literally one opinion on one possible future. Gaby indicated he intends to take Modules forward in a direction I don't disagree with, we've since realised that ASIO's `io_service` isn't fit as the universal event dispatcher for C++, so Gor has taken that on with Coroutines. I'm doing File I/O and on top of that the standard transactional graph database suitable for bootstrap querying of which Modules to load and/or JIT compile etc. Apart from Reflection, we'll have the parts in place if all goes well by 2025 or so to choose to build on top a proper Modules implementation if that seems desirable by that time. All the above is not a formal plan nor agreement. Just people talking to each other, expressing their plans, and others choosing to focus on various niches knowing the plans of others.
&gt; You can turn the question on its head though: How does the lack of abi/proper modules costs google each year ? Probably a lot. Google don't themselves think so. I tried to argue that their whole need to invest into LLVM and clang stemmed from the lack of proper modules, and therefore it has cost them very dearly indeed. But they choose to not interpret the situation in that way, and it would not be helpful for them to choose to interpret the situation with Googlebot in any way other than they currently choose to. &gt; But I understand these works are not easily funded. I remember funding being a hot topic after openssl heartbleed. OpenSSL is a hundred times easier to get funding for than full fat Modules for C++. Investment has clear and direct benefits to the investor. You literally set out a budget, and deliver the results gained. Easy. Full fat C++ component objects is a much more diffuse ask. If you read up on how Microsoft COM was funded, it basically was funded through lies and subterfuge. Key engineering directors wanted them, could never get the funding, and so sponsored its development and deployment basically by stealing funds for other tasks and mislabelling development to sneak it past their managers. Once COM got deployed, it was an amazing boon to productivity, indeed Gates himself said that COM was one of the key technologies which made Microsoft scale out so quickly and faster and cheaper than anybody else. But in any normal corporation, those managers would have been fired and possibly charged with fraud. It was possible at Microsoft at that time only because growth was going through the roof, and a wild west mentality prevailed at that time. It would be unthinkable today. &gt; On an unrelated note, AFIO looks interesting ! Thanks! My current big push is https://ned14.github.io/outcome/ partially because it is Boost's counterargument to the wisdom of the Expected approach, and I am under a lot of pressure to deliver that as soon as possible. Lots of people are waiting on it, and AFIO is utterly dependent on Outcome throughout, so Outcome needs to succeed for AFIO to be viable. But even I came away impressed when I ran AFIO on Optane storage. AFIO is so low overhead you can measure the PCIe latency variance with it. During the summer we found a bug in the Windows scheduler with it. That sort of thing. I hope to submit it for the Rapperswil 2018 meeting, with a devastating opening graph comparing i/o with iostreams to i/o with AFIO, then showing how you can build a constant-time expanding `vector&lt;T&gt;` (where `T` is trivially copyable) with it that can resize capacity at almost no cost because AFIO lets you manipulate the virtual memory underneath via page remapping, so no `memcpy` needed. Deploying such arrays could reduce GCC's compile times by about 40%. It's a major win for that niche of use cases.
Was "compile time fuck typing" a Freudian slip? :P
I would love that to make it into the standard or a TS or something that compiler vendors are forced to implement though :)
Our testing indicates load factors of .9375 are possible while still have &gt;99% of finds in the first group. So I don't think there is a lot of room to find higher density...
`std::any`: fuck all the types.
Peak memory usage is growth_factor (currently 2) * max_load_factor (currently 7/8 soon to be 15/16)/2. Meaning an overhead of a bit over 2x. We are experimenting with lower growth factors.
You mean a bit over 3x, since you copy the old table (1x) to the new table (2x), right?
"Legacy reasons" rather are: Windows uses a C interface for the same reason POSIX does: that's how OS (userland) API was published and used forcyears. There's a bunch of system APIs that are in fact COM, but that's still C at its base, however ...
please check out the latest commits. Now functions `is_equal`, `is_not_equal`, `lesser_than` etc can be replaced with operators: ``` auto idLesserThan10 = storage.get_all&lt;User&gt;(where(c(&amp;User::id) &lt; 10)); auto cuteConditions = storage.get_all&lt;User&gt;(where((c(&amp;User::firstName) == "John" or c(&amp;User::firstName) == "Alex") and c(&amp;User::id) == 4)); ```
!removehelp
OP, A human moderator (u/blelbach) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/794l6a/how_can_a_java_developer_break_into_c_development/doz76li/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
Wow, that is big
I really like the idea that filesystem structure should be enough to describe the way a C++ project should be built. It's also true that it won't fit every possible needs, and that's where turing complete build systems should be used, but not in the general case. I wrote a few projects aiming in that direction, although it didn't get a lot of interest at the time. The first is https://github.com/automeka/automeka and which goal was to achieve this in pure C++. The main issue was that there's no way in C++ to tell the linker which library should be used when a module X is used (although there were linker flags included in the initial Clang module implementation, the one with modules map files). The other is https://github.com/berenm/CMakeBuildPackage, built on top of CMake with the same idea but assuming that CMake is available and that it would be easier to have CMake scripts doing the project structure discovery than having to install another tool, and relying on declarative dependencies rather than trying to discover them.
&gt; , we've since realised that ASIO's io_service isn't fit as the universal event dispatcher for C++, interesting, do you know where the relevant bits of discussion around this could be found ? 
My memory might be faulty, but I believe that was realised in a discussion when we sitting on a park bench eating barbecued meats in Aspen during a C++ Now. Gor was there in a some sort of Hawaiian shirt if I remember, he argued (successfully) in favour of approaching the problem of implementing universal reactors and event dispatch via resumable functions, not by assuming some `kqueue` equivalent implementation as ASIO effectively does. I was won over, anyway.
The defensiveness is probably necessary. This could potentially replace moc. The Qt crowd is rather jumpy about this topic. Additionally, meta classes, heck even templates, are not what Qt is about. It's more like 90s OOP. Don't get me wrong, it does its job. But it is far from "modern". I imagine it is rather difficult to talk to the audience which lives in the past about the future. This may sound rather harsh against Qt, although I like it for GUI stuff, despite its quirks. I just wish, it would evolve more towards modern C++.
Javascript is there just for oneliners. It could have been anything that supports some basic operators and types.
tbh i did not know about boost.dll in some cases, I am restricted to what I can use: boost is template heavy, and that's not ideal for some of the embedded work I've had to do. that and we have to stick with the standard, usually, for a whole bunch of other reasons
Facepalm
You are correct, it is not needed inside the `void_t`. It is only needed for type lists outside of the `void_`. Just to reiterate, I am by no means an expert!
https://isocpp.org/std/submit-a-proposal
nyuk nyuk
Why do you think going from 8 to 16 per group by paying 2+ times the cost of matching will be a net gain?
We can't avoid wrap around because we do quadratic probing. With upper bound and extra reservation you have to do linear probing and we have found that it is slower in our production workloads.
my build script for gcc 7.2.0 was already using glibc 2.26 and I didn't even know about this! :D
!removehelp
OP, A human moderator (u/STL) has marked your post for deletion because it appears to be a "help" post - e.g. asking for help with coding, help with homework, career advice, book/tutorial/blog suggestions. Help posts are off-topic for r/cpp. This subreddit is for news and discussion of the C++ language only; our purpose is not to provide tutoring, code reviews or career guidance. Please try posting in r/cpp_questions or on [Stack Overflow](http://stackoverflow.com/) instead. Our suggested reference site is [cppreference.com](https://cppreference.com), our suggested book list is [here](http://stackoverflow.com/questions/388242/the-definitive-c-book-guide-and-list) and information on getting started with C++ can be found [here](http://isocpp.org/get-started). If you think your post is on-topic and should not have been removed, please [message the moderators](https://www.reddit.com/message/compose?to=%2Fr%2Fcpp&amp;subject=Help%20Post%20Appeal&amp;message=My%20post,%20https://www.reddit.com/r/cpp/comments/796485/what_should_be_the_next_step_for_me_currently/dozg5f1/,%20was%20identified%20as%20a%20help%20post%20and%20removed%20by%20a%20human%20moderator%20but%20I%20think%20it%20should%20be%20allowed%20because...) and we'll review it. #####&amp;#009; ######&amp;#009; ####&amp;#009; *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cpp) if you have any questions or concerns.*
If I remember correctly, the proposal was never about compiler abi (how are exceptions implemented, what calling convention is used...). Those things tend to be fairly stable anyway, but about type layout/library stability.
Python?
Indeed, the type trait is `[boost::callable_traits::remove_noexcept](http://www.boost.org/doc/libs/develop/libs/callable_traits/doc/html/callable_traits/reference.html#callable_traits.reference.ref_remove_noexcept)`. It won't be released with Boost until December's 1.66 release, but it works as a standalone library in the meantime.
No.
I don't believe the C++ language should standardize any build systems, package managers, or the like. 
I think there should be at least one wildly used, but not a standard one. There always will be projects which have special needs, and those projects should still be able to have their special configurations
Of course, I am not proposing to standardized the build system or a package manager, which just be an enormous task to be effective. Rather, just to standardized a way to describe the toolchain and a way to describe the package and its dependencies. And that may not come through the C++ language per-se, as this could easily apply to other language as well.
Then you agree with the post then.
COM is not really an ABI in the generally-understood sense, like the 'C', 'FORTRAN' or 'Pascal' ABI. It's an IPC framework, somewhat like `dbus` in Linux (which uses a very comparable 'object-like' model internally). Maybe you can use to replace an ABI somewhat but that's not what it's really built for, it's just hacking things together.
A package manager should not take away the ability to build and install dependencies manually, especially to help those projects with special configurations. Of course, I see no problem in having many different package managers as long as there is a way for them to interoperate together.
C++ will **never ever** be able to unify a standard package manager and a build system, it's a mistake that needs time travelling back to the past to solve. *[Downvotes swarm in]*
I think /u/jonesmz was referring to the headline of the Reddit post.
As with any proposal to Evolution, you could convince me. But as with any proposal to Evolution except for those submitted by Richard or Tony, I start off with a pretty strong opinion :)
It would sure be handy when trying to port stuff to Windows.
you do not have to be ashamed
Regardless, it is the official way to write OO components on Windows that can be shared among any language that speaks COM. With UWP ( kind of COM 2.0) C++ even has a center role as the foundation of it via C++ WRL, C++/CX and the upcoming C++/WinRT.
This looks similar to the Rust stdlib hashtable. Would be interesting to see how to optimize it for delete heavy work loads.
I doubt that the cost of matching an extra 64 bits already in the cache would make a significant difference, but obviously I can't be sure. 
Microsoft's profiler in visual studio and Intel's VTune. It's rare I would say this but I think microsoft's interface is better.
With modern c++, a c++ library should be a fully portable small hand full of header files and compilation units. This is not a problem calling for complicated infrastructure. If the library isn't in the standard OS install it should probably just be compiled and statically linked into everything. The whole "package" problem seems like something that's going away anyway, to me.
Yes, I was referring to the headline. 
Would definitely helped if C++ was developed with something like `cargo` in mind from the start. But at this point - it's not really the question of need, but if it's even possible to create some solution that would seamlessly integrate with all existing libs/tools in the C++ universe (or it won't be universal).
A few years ago I did work on binary c++ libraries across compiler and stdlib boundaries. https://m.youtube.com/watch?v=BbbqBJ94-_E https://m.youtube.com/watch?v=a4iFJuNBx7U With this you could pass and return string, vector, tuple, function across components, even those written using different compilers and stdlibs. For example, GCC with libstdc++ and MSVC. It was portable to Linux and Windows.
As much as I'd love a modern header only version of OpenSSL... The program in question is in C anyways. I had high hopes of using nuget, but its version of openssl is years old and the command that's supposed to install it doesn't work even after I get rid of the invalid command line args. Plus I can't find the copy of cmake that VS2017 was supposed to include... the whole porting process is a hot mess. I did get it working with MinGW and MSYS2, but that won't make a static executable for some reason...
I rarely use the pimpl idiom anymore and when I do, it usually contains public member variables only. I never duplicate the interface. I will to my best to make stuff work with declarations only, but I'm not systematic and I won't add boilerplate to avoid including something. I've been using templates a lot more since C++11. The combination of `auto`, `enable_if`, variadic templates and perfect forwarding makes it much more convenient. Since I don't usually work on largish projects, I find that compilation speed is not often an issue for me, unless I run into compiler defects that I need to work around. Personally, I think the pimpl idiom was a crutch that has been rendered mostly obsolete by better compilers and faster computers. YMMV.
I’ve thought about this before, but haven’t come up with a good template-based way to do the traditional Pimpl idiom without reflection/injection being added to C++. However... having a pure virtual interface, which is implemented by your concrete implementation in many cases actually accomplishes the exact same thing that the Pimpl idiom is intended to do. You still have to double declare all of the methods, but you don’t have to write the forwarding implementation of each method in the interface class, like you’d have to do when using the pimpl idiom.
Looks good, but calling `std::abort` in a library is a bit of a no-no, isn't it?
Won't modules ts solve part of this universal problem? I really like the Go way to get and build everything up just from urls. cget follows a similar idea and it's simply build on cmake. So I don't think it would be too much problem for a universal resolution for most use cases.
Username checks out
Not with that attitude
Use vcokg, not nuget. It fetches a modern OpenSSL and builds it with you given toolset.
Is that part of VS2017? It's not being recognized at a command prompt (Or vcpkg in case you made a typo).
https://github.com/Microsoft/vcpkg
&gt; However... having a pure virtual interface, which is implemented by your concrete implementation in many cases actually accomplishes the exact same thing that the Pimpl idiom is intended to do. When doesn't it?
There is a main requirement for timer actions: they shouldn't throw. If a timer action throws then a timer engine doesn't know what to do with this exception. It is especially bad if timer_thread is used. Timer actions are called on the context of that thread. How this thread should react to exception? Ignore it? Transform it to something else? [There is a special section](https://sourceforge.net/p/sobjectizer/wiki/timertt%201.2/#exceptions-from-timer-handlers) in the documentation where the behavior of timertt in a case of exception is described.
The only counter-example I can think of is a situation where you would want to be able to actually have the lifecycle of the interface object and the impl object be slightly different. For example, if the impl object does something asynchronous, it could keep itself around somehow while waiting for it to complete, even if the interface object has been thrown away and actually deallocated. I’ve considered using this pattern before but never tried it, so I’m not sure how effective it would be.
This is about 2x faster on lookups and uses less memory than Rust's hashtable. The one in Rust is Robin hood with linear probing and it stores the 64 bit hash for each element.
Agree. And I would say it's not even *shouldn't* but *can't*; there is just not enough existing, agreed upon to be best practices to standardize. I also think many don't realize what an enormous task it will be. Just look at how long it took to standardize ASIO as Networking TS -- this will be 10 time that. It's easy to write a blog post with a "*let's (someone else) do it*" call. It's entirely different matter to painstakingly figure out and specify all the details. Take a look at the [`bpkg` manifest documentation](https://build2.org/bpkg/doc/build2-package-manager-manual.xhtml) (still a draft) to get a feeling.
While they can be massively useful, a curated package repository will, invariably, not contain the one package you desperately need. Having your favorite package included therefore relies on the whims of the package curators. I don't think such a central gatekeeper should exist. At the same time, life would be much, much better with a unified way to deal with external libraries. If I were to consider my ideal environment: - It would let me download libraries in a standardised format. - The installation process would be in the form of a single command, which should take care of all the necessary compilation, as well as integrating the library into the compiler environment. This process should not rely on external tool chains or languages to be installed (i.e. don't make me install msys, cygwin, python, perl, or cross-compile gcc just to build your single library). - The package itself should contain a manifest containing instructions for looking for and installing updates. Updating an entire local repository to the latest versions of all those libraries should be a single command. - The package format itself should be system and compiler agnostic. - The package format should solve dependencies automatically. - Support for multiple versions side by side is a necessity. - Creating a packages library should be straightforward, and support for this system should be so widespread that any library author wouldn't even consider any other way of distributing his work. Did I miss anything? I don't believe we have anything that comes even close today. We have curated repositories, which are certainly great, but I don't see why a central repository like that would be necessary in the first place. We also have build tools like cmake, but they are only a part of the answer. Would a system like this be a 'package manager'? Well, certainly not in the classical sense, but I do believe having it would be incredibly useful. 
&gt; Having your favorite package included therefore relies on the whims of the package curators. This is an interesting topic that we've been thinking about in the context of [cppget.org](https://cppget.org). We definitely want a more fluid, natural selection-like process compared to, say, Boost. So the idea is to have a formal set of rules of when a package is included into the repository and when it is kicked out. For example, any package that builds on at least one core platform/compiler combination can be added to `testing` (you can read about the [repository sections and their policies](https://cppget.org/?about)). Then, after some time and if it builds on a some percentage of core platforms/compilers, it is migrated to `stable`. If you stop maintaining your package and it starts to fail-to-build on sufficient number of platforms/compilers, then it is moved to `legacy`. Would that work for you?
Can you explain why you think this is not necessary? 
The operator dot coming in C++17 (N4173) and will give you an elegant way to solve that without any code generation mechanism! This new operator overloading is meant to be used in PIMPL, proxy and smart references implementation.
The post concluded that we should standarize on some existing practice. 
vcpkg is not a cross-platform tool. 
I'm curious what is the low-level mechanism for the timer and how accurate are they? On Linux given its scheduling the guarantees can be pretty bad. Do you use signals internally? This looks like a useful lib but I would need to know the impact on the rest of the application. I didn't see these answers in the docs but might have missed them. 
If we are speaking about [timer_threads](https://sourceforge.net/p/sobjectizer/wiki/timertt%201.2/#timer-threads-basics) then they use `std::this_thread::sleep_for`. It means that timers will be as accurate as `std::this_thread::sleep_for` on your platform. It we are speaking about [timer_manager](https://sourceforge.net/p/sobjectizer/wiki/timertt%201.2/#timer-managers-basics) then it is your task to count time intervals. So it is depends on you how precise you can be. However there are two important moments which needs to be taken into account: 1. This library was designed to work with big amount of timers (it should handle hundred thousands and millions of timers). But there wasn't a demand for high precision of timers. Typically timers are used for working with timeouts in seconds, tens of seconds or even minutes. If timers will be inaccurate in tens of milliseconds it is not a problem in such scenarios. 2. timer_wheel engine is not an accurate by design. It uses a time ticks with fixed length (for example 250ms or 1s or 10s). It means that you can't have timer more accurate than length of time tick. It is the price for ability to effective handling of millions of timers.
&gt; Won't modules ts solve part of this universal problem? C++ modules (at least as currently specified in the TS) are agnostic to packages/libraries. So they won't solve this problem directly. Indirectly, though, they may shake up the C++ build ecosystem enough that we will get something interesting our of it.
Operator dot was not included in C++17.
&gt; It's just so much easier to develop for Linux and Unix systems. Everything just works, and dependencies are easy to manage. Or you could turn the table around and say it's hard on Windows because developers mostly care about Linux builds. Dependencies on Linux are easy as long as you don't have to include 3rd-party binary components that are linked to a newer glibc than the one you want to support or if a non-packaged dependency needs another one that is incompatible with the system's. Then you're in for a lot of fun, esp. with glibc incompatibilities. -- I tried to make an embedded product based on CentOS 7. I gave up and went for Windows. It was a larger one-time upfront cost of building dependencies, but the net savings afterwards (re testing, deployment, setting up new dev machines, etc) were worth it. Though I agree that Linux is "easier" when you just want to compile and play with some code maybe hack on it.
No, C++ should take modules a step further and standardize a representation of modules as binary blobs portable across compilers and platforms (say, compact representation of an AST). Like a "pre-object" file. Stroustrup has even a paper on this. This would solve most of the problems that package/build managers attempt to solve.
Location: has good flights from Seattle (direct) Date: around CppCon ;)
This is an operating system problem. My operating system has a very powerful package manager that I always ignore package managers like `cargo`, `pip` etc. I see no reason for there to be a package manager for C++. Since it'll complicate things even further, I do not want this to happen. Sorry for Windows users etc...
Ha, I missed that one. Too bad.
Unfortunately I see only 16 Packages on cppget.org :( By the way about build2, I think that installation process is way too complicated. Especially if you just want to look at in and decide if build2 is usable for you or not. On download page it is not clear what you really need to download. Also Bootstrapping hat too many steps as well. It would be great you one could download simple (*.zip or *.7z) file extract it and run some *.ps1 script or something like that. 
Splitting interface and code might be useful in some cases, but I really don't want to do that. However with the slow compilation of c++ this will only practically be possible if the compiler is smart enough to prevent constant compilation. So this must be solved better now than later, or a lot of people will be really disappointed with the result. Couldn’t you split the IFC file into immutable and mutable data? I guess a lot of data will be immutable by design as it is tied to the generated binary (function signatures, etc). While debug information is static data (can be changed but doesn’t make a lot of sense). Other data in the file can be dynamic (Comments and documentation of functions, etc). Thus recompilation of depended modules should only be triggered only by a change of the immutable data? Or can problems arise from other parts of the compilation process (e.g inlining)? 
Uh... are you talking about the `fast_hash_map` presented here? It seemed to me that the SSE code presented assumed linear probing.
Wow! That's an impressive number, thanks for sharing. I must admit being surprised at such a density; at 93.75% I would have imagined some long probe chains even with a good hash function (because of clustering effects). It's encouraging that you could reach such high densities while still maintaining low probe chains length.
Can you link to that paper?
Compare that to auto const opt = data.count (key) ? std::optional(data.at(key)) : std::nullopt; Sure, that's ten characters more, but no macro. Or are you thinking of boost::optional and C++11 without template argument deduction and no return type deduction?
Yep, I'm sadly stuck in that world at work. Also, there was some context in which some compiler didn't like having `boost::none` on one side and `boost::optional&lt;T&gt;` on the other of the ternary, as seen in my snippet, making code really ugly.
@mattkulukundis the insert/constness gotcha you mentioned, are you sure that's really an issue? I think the template&lt;typename P&gt; insert(P&amp;&amp;) overload should actually be triggered in that case. Admittedly, that will then trigger the emplace case. Not sure whether that's what you were referring to then.
Rusts hashtable is way more generic and secure in the worst case cases. Robin Hood Hashing is way better at handling heavy clustering in the table.
I am not sure what you mean by generic. I have an implementation of SwissTable for rust which I will open source soon. It implements the same interface.
Yes I am talking about this one.
Yes, it could. I am a bit undecided what I like better for such cases. The latter is shorter, of course, but the former clearly separates initialization from condition checking. Thus, I am leaning towards the former. 
A similar approach (CMake and pkgconfig) is used in this project (collection) http://staticlibs.net/ Can anybody share some experience with it? Do I understand correctly, that you could just static compile an EXE without any dependencies using these libs?
&gt; Unfortunately I see only 16 Packages on cppget.org But they are all awesome ;-). And [build](https://cppget.org/?builds) everywhere. There are also a few more (`libpkgconf` and `libpq`) [being stages](https://stage.build2.org/). Take `libpq` (PostgreSQL client library), for example: you will pull all your hair out if you try to follow official build instructions for Windows. &gt; By the way about build2 [...] Yes, we realized the build procedure is too complicated. It may be *right* but that's not what people who just want to check `build2` out need. They need a quick and dirty way to try it out. So we are going to work on that (and rewrite the intro while at it). And thanks for the feedback, always appreciated.
I think you are talking about this example: void BM_Fast(int iters) { std::unordered_map&lt;string, int&gt; m; const std::pair&lt;const string, int&gt; p = {}; while (iters--) m.insert(p); } void BM_Slow(int iters) { std::unordered_map&lt;string, int&gt; m; std::pair&lt;const string, int&gt; p = {}; while (iters--) m.insert(p); } With this as reference: http://devdocs.io/cpp/container/unordered_map/insert Assuming C++11, the first matches overload (1) and the second matches overload (2). Overload (2) creates a node on the heap and then probes the table. If it found a node with the same key, it drops the newly allocated one. This is not a bug in the standard - this is a quality of implementation issue. Granted it requires quite a bit of metaprogramming to decompose arguments into key, value in order to avoid the creation of a node in insert and/or emplace. Once we open source the code it is likely that standard library implementations will implement the idea as well.
It's easy to make such broad statements without actually testing if they actually check out :-) The tradeoff between "security" and "performance" is such a grey area, I don't think it is reasonable to expect we are going to convince anyone, especially on reddit. One important thing to keep in mind though is that in general slower hashtables are more secure than faster ones. Take it to the extreme: a hashtable that takes 1 year to do a lookup is quite secure: it takes more than a lifetime to perform a DOS attack ;-)
Yeah, that's what I am talking about. From the talk I understood as if they were talking about the valuetype&amp;&amp; overload. But yeah, at the end of the day it's not perfect in either case. Cool, looking forward to the opensourcing. Though, I'd say try_emplace and insert_or_assign is the proper way to go forward anyway.
Although openssl doesn't provide a portable build script with cmake, libressl does. You can do `cget install pfultz2/cget-recipes libressl` to easily install libressl on windows. &gt; Plus I can't find the copy of cmake that VS2017 was supposed to include... I didn't know VS2017 was going to include cmake. I always install the latest version on windows.
Yeah fully agree with that, that was kinda my point. Your statement seemed kinda broad especially given that there is nothing open source yet so that people can fake their own benchmarks. In regards to micro benchmarks, I am interested in how swisstable performs with heavy clustering compared to a robin hood style implementation. 
Nice, can't wait. Let's hope before end of the year was a good bet.
http://www.stroustrup.com/gdr-bs-macis09.pdf
&gt; While they can be massively useful, a curated package repository will, invariably, not contain the one package you desperately need. Having your favorite package included therefore relies on the whims of the package curators. I don't think such a central gatekeeper should exist. That is another good point as well, but the fact that we dont have a universal build system or package manager, I dont see a central repository happening. Of course, any package manager should be able to install outside of its central repo(although vcpkg doesn't which is pain). For example, with pip you can just point it to a git repo to install a package. &gt; This process should not rely on external tool chains or languages to be installed Ideally, a good package manager can see what build system in needs to use and will install the build system. &gt; The package itself should contain a manifest containing instructions for looking for and installing updates. Updating an entire local repository to the latest versions of all those libraries should be a single command. This implies an integrated build, but most package managers do not support this workflow because its difficult to deal with different build systems(although Meson has wrappers to try to help with that), and you can't support binaries easily with this workflow. &gt; The package format itself should be system and compiler agnostic. I agree, don't you think the format I showed is system and compiler agnostic? &gt; The package format should solve dependencies automatically. The package format? The package manager should solve dependencies. The package format should just describe those dependencies. &gt; Creating a packages library should be straightforward, and support for this system should be so widespread that any library author wouldn't even consider any other way of distributing his work. Yes it should be simple, and something more standardized, but supporting a package format doesn't mean distributing. Distributing still happens with providing a URL to a source tarball, but now with a package file there, users can easily put that in there favorite package manager to start using the library. &gt; but I don't see why a central repository like that would be necessary in the first place. Actually, one of the reasons I have in the package file a dependency list to map a package to URL, so it could be easy to install the package without needing some central repository.
Yes, it would be awesome to have something like `pip install` or `npm` that just works on all major platforms.
&gt; A similar approach (CMake and pkgconfig) is used in this project (collection) http://staticlibs.net/ Can anybody share some experience with it? That's not the same approach. They are trying to wrap everything(including the build) into some structure they have defined. This post is proposing a standard format to describe a package(mainly for package managers to read), and a standard toolchain description that build systems can use. None of this should require changing the build scripts, nor is any of this related to usage requirement formats like cmake's or pkgconfig. &gt; Can anybody share some experience with it? It doesn't seem to use the common configure, build and install. I tried to install staticlib_config which doesn't have any dependencies and it failed with "No rule to make target `install'". So I dont really understand how those libraries are supposed to be used. They claim a "uniform CMake build interface" but can't even be installed using cmake's install. &gt; Do I understand correctly, that you could just static compile an EXE without any dependencies using these libs? Thats what name implies.
&gt; Sorry for windows users I couldn't care less about windows users. Linux/*BSD themselves have their own share of issues. As the Linux desktop marketshare continually rises, we should examine and explore the solutions to this problem. First of all, I'm not saying that having different package managers is an inherently bad idea. However the problem is that many distros have named different packages differently. For example consider that you are a software developer and your project requires jsoncpp parser (https://github.com/open-source-parsers/jsoncpp) to handle JSON files. Obviously, you want your install script to search for jsoncpp in the system. You would: 0. Ask the system package manager if it's been already installed or 1. Ask the system package manager if it can be installed from the known package repository or 2. Resort to git-subtree/sub-modules as the last option. The problems we face are: 0. There are multiple package managers that your scripts should cover. This problem is easily fixable by creating an abstraction over known package managers. Each package manager is an object of a package manager class that has certain functions as attributes. For example, pacman has search/install/remove/build attributes. Apt, zypper, dnf, eopkg etc share the same attribute as well. This could be an easy weekend project: Abstracted Package Manager Interface (apmi). (Sounds cool) However, 1. Different distros/repos have named their packages differently. This jsoncpp is named jsoncpp (as it should be called) in Arch and OpenSUSE's repo. For some reason, Debian (and Ubuntu as a result) decides to call it libjsoncpp-dev. What Arch calls xf86-input-libinput, Debian calls xserver-xorg-input-libinput. This is a serious problem. The developers are forced to let users hunt down dependencies by themselves and compile big projects. We know how well C/C++ handle external dependencies (they can't) hence the users create 3rd party scripts that don't work half the time. What we need is a standard package naming scheme, which all (or at least the majority) of the distros follow. Otherwise we have to create separate database to map associated libraries to their distinct names given by the distros. This is not reliable. Lastly, I know some of you will tell me to use Appimage (portable Linux Apps) or sandboxes like flatpak or snaps. But this is just going around the big problem we have.
Would you like to elaborate?
&gt; I dont see a central repository happening. Neither do I, but I believe we don't need one. By analogy: we don't have a central repository for internet pages; we have a standardized mechanism ("HTML spec") for delivering them from a great many sources ("websites") - and multiple implementations of browsers. I believe C++ libraries should be delivered to my computer in the same way: the mechanism should be standardized, but there is no need for a unified repository containing absolutely everything. &gt; Ideally, a good package manager can see what build system in needs to use and will install the build system. That's just additional complication I'd rather do without, especially since it inevitably introduces an OS dependency. I don't really care about how this gets build; I just want it to happen. We might as well choose one of our many build systems and stick with that. Note that this particular build system is only for external libraries, not for your entire project (which could use any build system you want). It only ever runs when a library gets installed or updated, i.e. irregularly. &gt; The package manager should solve dependencies. Of course, but the information is obtained from the libraries themselves, not from some central database. Anyway, my point was mostly that we seem to have a strong focus on centralized repositories, and I think that shouldn't be the goal to begin with. The mechanisms I describe can _also_ be used to build centralized, curated repositories, of course - but in principle the goal should be to standardize a single _mechanism_, and let library authors do the rest themselves. Also note that this is not exclusive to C++. The exact same system would also work out of the box for the C community.
&gt; I really like the Go way to get and build everything up just from urls yuck, I always found this to be terrible. Sure it's fine for five minutes for your small 10k LOC project, but at scale it becomes a pain. What if you want to build some dependency with a different set of flags ? Or load them dynamically ? etc etc
Is there any plans on shipping the vcpkg with the VS? P.S. right after the proxy issues fix, of course
Go go `#git-include https://github...` or something for the standard =P
&gt;Need Can*. And the answer is, and forever will be a big **No**. (Grapes are sour too).
Most software in your computer is written in C, C++ or python. Python already has their package manager, which I personally never use; instead, I use pacman to install everything. Now, you say we will bring a C++ package manager that will replace pacman? Or will it supplement it? What will happen to C software? Will we package C stuff too? Most distros have "all-or-nothing" type of package managers. For example, pacman *owns* /usr/local. There is usually no practical way to use multiple package managers. So, you'll basically have to work around your package manager everytime you want to use C++ package manager; just like you need to do this for pip or cargo, since pacman will override them. So, you now have even more problems. You need to check what is shipped by your distro, choose which one to keep. And then you'll check whether the source of C++ package manager is as reliable as the one in pacman or AUR. This is relatively managable when you're dealing with pip or npm since libraries will not be as system-wide. But when we start shipping boost or libgmp with our C++ package manager, things will start to get a little too fun. What will happen to backports my distro puts to libraries? If this sounds easier to you, sure go ahead, I won't stop you. The greatest issue I'm having here is I never had any sort of problem with installing and deploying C/C++ programs/libraries since if you know your way around, everything is very easily scriptable. But when stuff like npm get into the scene, suddenly everything becomes much more complex and messy. And if you apply that whole business to your whole system, I cannot see how will you get an easier-to-use system.
Sorry if I seem to be rude but would you like to post a TL;DR? I can't see a call-to-action or question(s). 
By the way the example given at https://stackoverflow.com/questions/27331047/c-std-regex-crashes-in-msvc-during-long-multiline-match/30128172#30128172 works just fine with this library. I extended the example to include it in a compile time lexer: using namespace sre; using Lexer = RegExp&lt; Select&lt; StaticCatch&lt;0, 1, Sequence&lt;Plus&lt;Number&gt;, Identifier&lt;0, Type::number&gt;&gt;&gt;, StaticCatch&lt;0, 1, Sequence&lt;Plus&lt;Range&lt;'a', 'z'&gt;&gt;, Identifier&lt;0, Type::id&gt;&gt;&gt;, StaticCatch&lt;0, 1, Sequence&lt;Char&lt;'/'&gt;, Char&lt;'*'&gt;, NGStar&lt;Anything&gt;, Char&lt;'*'&gt;, Char&lt;'/'&gt;, Identifier&lt;0, Type::comment&gt;&gt;&gt; &gt;&gt;; Lexer lex; std::string input = "1hello2there/*\naaa\naaaaaaaaa\naaaaaaaaa\naaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaa\naaaaaaaaa\naaaaaaaaaaaaa\naaaaaaaaa\naaaaaaaaaaaaaaaaaa\naaaaaaaaa\naaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaa\naaaaaaaaa\naaaaaaaaa\naaaaaaaaa\n*/3and45"; std::size_t index = 0; PositionPair range = {}; do { if (lex.match(&amp;input[index])) { range = lex.getCatch&lt;0&gt;()[0]; std::cout &lt;&lt; lex.getId&lt;0&gt;(); } else { range.end = 1; std::cout &lt;&lt; ~0; } std::cout &lt;&lt; " '" &lt;&lt; input.substr(index, range.end - range.begin) &lt;&lt; "'\n"; index += range.end - range.begin; } while (index != input.size()); 
&gt; "well, it's UB so we made it crash on purpose" If it's UB it can do a different thing on each run of the program. 
Yes, we have plans to expand VCPkg inside of VS. Paging /u/roschuma for details. 
The standard at least seems to claim it should be equivalent to this: shared_ptr().swap(*this); Which doesn't look to be what you observe. 
Obviously, but this is particularly galling if it's really UB to do this. I mean, we tell people that they should use std::shared_ptr as a replacement for T* and that it can deal with complex situations. It's also possible to implement a shared_ptr that does the right thing in this case. I mean really, I have a pre standard shared_ptr that works fine in this case. I'm also not totally sure it's UB. I mean, is this UB? #include &lt;iostream&gt; struct something; void print(const something&amp; item); struct something { something() { a = 4; } void print() const { std::cout &lt;&lt; a &lt;&lt; std::endl; } ~something() { ::print(*this); } int a; }; void print(const something&amp; item) { item.print(); } int main() { something a; } 
Once the dtor of shared pointer starts to run, there is no longer an object there from point of view of the standard. So when you copy the object in ~test, that is UB. To fix this crash, the copy ctor would have to check, is the ref count zero? Even though the pointer is not null? If so then dont actually copy and assume its this broken situation. That branch has a cost. In c++ you dont pay for what you dont use. Its not beligerent adherence to standard, its that its not worth making everyones code slower to accomodate this janky code example.
He makes a copy of the shared_ptr, not the object. 
And that's intentional, at least for now. There is [an issue about cross-platform support](https://github.com/microsoft/vcpkg/issues/57) on their repo now with quite a bit of discussion. 
The OpenSSL that the VisualCpp account added to NuGet was added only as a dependency of another library. NuGet makes it impossible to delete packages. We kinda wish it wasn't there. But...use VCPkg instead! NuGet isn't really a C++ package manager because C++ is all about source and NuGet is all about binary distribution.
Accessing a partially destroyed object (the shared_ptr) is UB no matter what shared_ptr's implementation does. It could go out of its way to prevent a crash, but that would just cover up the UB, not solve it, possibly adding overhead in the process.
Paging /u/SEGFALT_WA for info on CMake in VS. I know that VS2017 has very flexible and well-integrated CMake support but I think you're supposed to install CMake yourself. 
To make it work all the shared_ptr needs to do is free the object before it decrements the count. That would give the desired behavior. Also, this is an example that illustrates the issue. The real code where I encountered this issue was in ancient Win32 event callback logic with execution paths in and out of windows event routines. It was much larger than this toy example and involved a much more complex sequence of steps where this wasn't at all apparent. I mean, we tell people std::shared_ptr can deal with complicated patterns of things pointing to other things. It's irritating that things that could be made to work don't for pedantic reasons. Honestly, C++ is my favorite language and I tell people all the time that it's really easier to use than they think. But this kind of "well, the standard lets me make it shitty so I did" thing here is totally counter productive to selling other people on the idea that C++ is something they should be considering. 
It was also UB with your old shared pointer as well, it just happened to appear to work.
&gt;&gt; [..] triggering an infinite recursion of calls to ~test() The crash maybe due to running out of stack memory.
Note: #include &lt;functional&gt; namespace N { struct S { static std::function&lt;std::function&lt;void()&gt;()&gt; x; }; decltype(S::x) S::x = [] { return std::function&lt;void()&gt;{}; }; } works. Only the returning a lambda inside it will break. The error message is likely just a follow up error from that.
So it should check the count, if it's 1, free the object, and then decrement it? That's two accesses of the count. Isn't the count an atomic?
Ah, good point. Maybe you can't do it without a small amount of additional overhead. :/ 
I don't think C++ package manager is necessary. Even if is necessary, it's not doable language-wise. As I stated above, this is a problem that should be solved by the operating system, not by the language. Sorry if I trigger anyone.
For those that use CMake, https://docs.hunter.sh/en/latest/ is an awesome dependency manager at least, and fully cross platform. But yes, I say it really needs one.
It's a nasal demon.
Yes, but maybe it shouldn't be.