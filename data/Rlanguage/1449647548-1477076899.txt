Hi FlyingGeo, Thanks for highlighting the error, However I misspelled the uploaded file, but the variable names are correct. I reduced the file size by sub-setting it and the code runs fine on the reduced count of observations.....
could try this: tmpDF1%&gt;%group_by(ID, Season)%&gt;% summarise(NumAppearances=n())%&gt;%select(ID,Season)%&gt;%group_by(ID)%&gt;%summarise(Season.Count = n()) 
I'm having trouble visualizing what your desired output needs to be since all of the observations in the sample you provide are unique there's not really anything to summarize in that sample, ie. ID A123 appears twice with the same type and category but with a different season so both of those observations receives a count of 1. Maybe first try this: df1 &lt;- subset(as.data.frame(table(df)), Freq &gt; 0) df1 &lt;- df1 %&gt;% arrange(ID) and see if that's what you're looking for. If not, if you could edit your post and add a table of what the output you want would look like for the sample you provided it would help a lot.
Have you tried manually installing the package instead of just using the install.package() function (i.e., doing a hard search for ggplot2 under the Packages tab in RStudio)? Depending on how your permissions are set up, you can use this as a workaround. Another option, if available and what I suspect is the primary issue, is running RStudio as administrator. You can either do this manually each time or set up RStudio as you would with any other application by enabling "run as administrator" every time you run the program. There are a ton of ways to accomplish this automatically; here is a link with such options: http://www.eightforums.com/tutorials/9564-run-administrator-windows-8-a.html Good luck! 
How do you want to compare them all? Find the largest? Plot them? Take the average? 
Perhaps pulling out the first column vector and then using the apply() function over the rest. 
I think comparing the average with the first column would work for me, but i don't know how to do it 
I would look into moving it to AWS or Google Cloud Platform. It's hard to say what solution (EMR, BigQuery, Redshift, etc) would work for you with the info provided. You might want to talk to a data engineer...
SQL might make for some smarter indexing, but it all depends on what you are trying to do and with what data. My goto is keeping indexes in the ram with an innodb engine to prevent table locking.
Essentially the data is data collected for a manufacturing process. I want to be able to visualize and analyze the data to make decisions for improving the process. I am hoping to do this in a shiny application so the user can pick "i want to look at x, y, and z" than maybe compare it to " f g h". Example: So lets say we have to drill a simple hole on a plate and I have the data from 1,000,000 holes drilled. I want to know what drill parameters I can tweak to increase throughput speed without sacrificing quality (offset and size of hole). Therefore I would change parameters (drilling speed/rotational speed), collect data for a few days/weeks, than reanalyze and measure success. Some index-able information would be the tool name, operator, date, bit #, plate temperature, etc. So I would want to maybe pull this process data in a specific date range... than plot the hole diameter by rotational speed or whatever. In this example each of my individual .rds files would be like a set of 50 plates processed with the same parameters set0001.rds. I am looking into SQL but I have never setup a SQL db and would have to learn how to set it up, maintain it, and search it. This is the primary reason why I am working with a plethora of .RDS files and some index files to poke through. However, I am willing to learn SQL if it might be a better solution. Especially when my data set gets larger than few hundred GB. 
I would say SQL would make more sense for what you are doing.
The SVM is trained using an iterative algorithm, until some stopping criteria is met. To prevent it from training forever by accident, the algorithm will stop after a certain number of iterations, even if the stopping criteria has not yet been met. In such a case, the algorithm has stopped, but has not yet reached an "optimal" solution, as defined by the stopping criteria. So it issues a warning. It's saying "hey, I'm returning an answer, but it's not the best answer I can find. If I run more iterations, I can probably find a better solution." See the help page for details on the stopping criteria used and the default maximum number of iterations. You can probably increase the maximum number of iterations. I'm on mobile so I can't check at the moment.
Does anyone use rstudio as an alternative text editor? Rmd as an alternative to LaTeX?
Haha true. It's not the greatest service anyway, I always found rpubs to be a bit weird. I feel like it isn't super streamlined, their new Connect will probably feel a lot more functional
Yes - I do. It's a lot faster than Tex and sufficient for most purposes.
Depends - you can make nice, small and static tables with markdown or you can use knitr::kable() to produce tables from within R code chunks. 
The best thing I found when getting familiar with R (an even now) was to have a project to do with an end goal, which you have, and then breaking it down into what I knew how to do and parts I didn't. Then I'd get the stuff that I knew how to do working and incrementally add the parts I had to figure out. Stackoverflow is awesome for learning from other peoples problems, and any time you copy/paste figure out what the other person did and why they made certain choices. As for what your looking to do, I normally only work with one input file at a time, but it sounds like using ggplot2 with geom_tile might work? Your y axis would be categorical (merge all the files together and have the name in a column). Not too familiar with that type of data but it sounds do-able.
Andy Field's Discovering Statistics Using R.
I have no idea what you're trying to do.
So how can I really showcase my knowledge on my site, in that case? I'm guessing I'll just have to supply the data as well as the code.
Yeah, you could possibly provide the code for some analysis you did of public data. 
The forecast package is a good place to start
Others have addressed the forecast package and `arima` functions, but you haven't mentioned whether your data is already in R or not. If it is, then go right ahead with reading/getting help on `arima`, but otherwise, you may want to take a look at the [`readr` package](http://blog.rstudio.org/2015/04/09/readr-0-1-0/) and the [`readxl` package](http://blog.rstudio.org/2015/04/15/readxl-0-1-0/), which will help you get your data into R to then plug into the `arima` function of your choice. As for ready made scripts: # Load the libraries ----------------------------------------------------------- library(stats) # not really necessary (loaded automatically), # but this is where the arima function is located # library(readr) # library(readxl) # Load the data ---------------------------------------------------------------- data("USAccDeaths") # included in the arima package. # When using your own data, you'll want to use the readr/readxl libraries # and the read_csv/read_excel functions. # Plot the data ---------------------------------------------------------------- plot(USAccDeaths) # Clear seasonality # Fit the model ---------------------------------------------------------------- arima( # data vector USAccDeaths, # AR order, degree of differencing, MA order order = c(0, 1, 1), # seasonal part of the arima model, # with components order and period # order has the same components as before # period defaults to frequency(x) seasonal = list(order = c(0, 1, 1)), method = "CSS")
Rob Hyndman wrote the forecast package in R. He also wrote a free online text here: https://www.otexts.org/fpp There are a few, short chapters on ARIMA that are helpful.
Are you aware of help? help(t) or just ?t it works for all functions and I cannot imagine operating without it...
This is very good advice. When I teach R workshops one of the very first things we go over after getting R installed is how to find help and read the documentation. I'm more forgiving when their first questions are about functions like 't', 'c', 'q' that are very difficult to google. In general sticking "R CRAN" in front of your google query can help get results. I know there are search engines tailored for finding help with R like [R Seek](http://rseek.org/) but I usually just use google if the documentation (using ? or help) doesn't fix my problem or I don't know which documentation to look at.
I'd be inclined to use a data.table for this. Something like the following should work: dataname &lt;- data.table(data.name) new_dataname &lt;- dataname[ a!= b &amp; a != c &amp; b != c]
~~Question for you that I'm having trouble finding the answer to... is there an easy way to update the db with new values than refresh the keys? Using the example in the link there is a flights dataset, what if we want to update that db daily using R.~~ db_insert_into 
I think this works, note 100% though df_new &lt;- df[which(df$col1!=df$col2&amp;df$col1!=df$col3&amp;df$col2!=df$col3),] 
I think [my solution](https://www.reddit.com/r/Rlanguage/comments/40f0pq/problem_with_deleting_a_row_in_a_loop/cyuq3wm) should work. I wish I had the data to test against.
I want to do a loop and define the first two terms of my sequence to be 0.
Build with VS 2015?!? It's a late Christmas miracle! I'm hoping this becomes the official platform for R development from MS, so I don't have to touch the VS 2008 shell ever again. 
hehe. i feel your pain.
No. Base can do more things, it's just a giant pain to use compared to ggplot
No. I HATE the way ggplot2 looks by default, and it's way harder to customize than base. It's better for a few things, but I'd rather fight with base than have one graph in a presentation be stylistically different.
They claimed an R wrapper for Tensor Flow was coming... I haven't checked in a bit, but I've heard they are doing a lot of good and fast work on Tensor Flow in general. This makes me think it will be available soon. 
If you really want to do deep learning, you should probably just bite the bullet and pivot over to python. theano, torch, lasagna, tensorflow... doesn't compare.
Found [this](https://github.com/terrytangyuan/rflow), though if you're installing scikit-learn and tensor flow, you might as well use tensor flow.
Of course not. But I use it 90% of the time. 5% base. 5% other graphics packages
+ theme_classic() helps
Being Friday night, I'm walking my dog and drinking a cold beer. I'll see if I can make any headway with that approach soon. On a side note, it'd be interesting if RStudio kept track of how many hours I've used it. That way I'd have an idea of how many hours it took me to get to this point in the learning curve.
Just saw [this post](http://www.kdnuggets.com/2016/01/top-10-deep-learning-github.html?utm_content=buffer1f617&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer) on twitter, good timing for you (top 10 Deep Learning project on GitHub, some of them are R packages)
If I knew how to do that, I wouldn't have needed to post this question in the first place haha
 x &lt;- c('won', 'lost', 'lost', 'won') x[x=='won'] &lt;- 1 x[x=='lost'] &lt;- 0 That should do it? edit: wait, i just reread your code, why are you only loading in the sales outcomes after you made your model? What is it testing on?
Because the actual outcomes aren't one of the factors that influence the Status of a sale. I just load them in after to compare with the predictions, so I can make a confusion matrix
No, it was just chosen as the best variable to use since once you can get a probability for it, you can apply the thresholds to it and then compare it to the actual outcome of the sale for determining accuracy. Thanks for trying to help and all man, but i'm past this question now. It's not causing me as much hassle as it was. Thanks though :)
check out the function colMeans. you can use it on your whole dataframe. 
what about filtering out NA and what if I wanted values in a specific range?
Many R functions can filter NA. Read the [R documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/colSums.html) of the function you want to use to see how. Also, by "range" do you mean a part of the matrix or do you only want to include numbers in a certain interval?
like I want all values &gt;2 but &lt;10 for example.
you can subset a vector with a logical statement that contains the vector, i.e. mean( apples[ apples &gt; 2 &amp; apples &lt; 10] ) and with the apply function you can call such vector operations for every column in your data.frame (or matrix or array)
well, you'd have to make a properly formed filter. I'll demonstrate how to do with with `data.table` since I'm a huge fan: dataTable &lt;- fread('your_data.csv') dataTable[Apples &gt; 2 &amp;&amp; Apples &lt; 10, lapply(.SD, mean)]
I had to look up what MRO is, and it looks lime it's a different version of R that makes use of multiple processors (I may be way off). RStudio is an IDE, so it just acts as a overlay to let you do things in R easier, while MRO is a different version of R that RStudio can run on top of. When you install RStudio it detects what version of R you're running, so it probably detected MRO intead of regular R. I'm not 100% certain about this though.
Rstudio is environment which gives a nice GUI and interaction ability. The engine is either CRAN R or a recent Microsoft R/RRO. Microsoft RRO is maintained and managed by team now owned by microsoft. 
Thanks for the response, I had to ask because it's really clearly written when I search about it. This is actually the first time that I knew that there are other versions of R besides the CRAN one. Thanks again.
Thanks!
That seems like a pretty basic task for R. At the sizes you laid out your computer should have no problems with it.
Use assign() iside the loop
Lots of good stuff there, thanks!
Markdown! It's incredibly efficient for making presentations, reports, etc. all from within R in one workflow. If you do academic or scientific publishing then LaTeX integration with knitr is next. Both make collaboration and reproducible research much easier and less tedious. 
I can see this for sure. I've moved from "taking notes" in code to taking notes in markdown. Now that I've got the main features down like embedding code and whether to evaluate or not, headers, etc it's a very fast and clear way of displaying the principles I'm learning. My one complaint is RStudio-specific I guess, which is that spacing and tabs inside #' lines get a little screwy and I have to manually make spaces for functions. 
I'd definitely recommend learning some base R, it comes in handy from time to time and gives you a better understanding of the language. That being said just make your life easy and use any/all of the packages from Hadley as often as possible. At this point, in my opinion, they are synonymous with R (at least as far as data science is concerned) dplyr will change your life. 
Base R: * basic plotting * common distributions, d/p/r functions (pdf, cdf, random sample) * sampling * apply functions * vectorized operations Additional libraries: * [These](https://gist.github.com/dmarx/d08d9e7950f58dddcc73) are some of my favorites. A few in that list are a bit esoteric, but most of them are pretty popular/common. I should update that gist with descriptions of those libraries, but at least you've got a jumping off point for your research. This lis is unfortunately incomplete: there are a few newer packages I need to add, but also a lot of dependencies that get installed along with explicitly listed packages that are immensely useful but just not didn't get their own separate installation line in that script. * You're gonna wanna acquaint yourself with the [Hadleyverse](http://blog.revolutionanalytics.com/2015/03/hadleyverse.html)
Excellent! Thank you very much. Looking forward to checking all those libraries out. Just checked out Stan, wow, hadn't heard of it but it looks very interesting. Now I just need to learn stats! I can sometimes be overly picky on how I learn things, but coming from mathematics I'm positive there's approaches that will, and won't, sit well with me haha.
The MOOC you're looking for started two weeks ago: https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about It follows this book: http://www-bcf.usc.edu/~gareth/ISL/
?'[' my_data_frame[,c("columns","I","want")] or my_data_frame[,-c(3,4,5)] if you want to remove 3,4 and 5 (removing doesn't work with column names as far as I remember)
 columnsToKeep &lt;- c("name1", "name2", "name3") myDF &lt;- myDF[,(names(myDF) %in% columnsToKeep)]
library(dplyr) my_new_df &lt;- select(my_other_df, nameofcolumnIwant, nameofcolumnIwant, etc...) 
The Y axis needs to be values, not categories. Let it remain 1, 2, and 3, but turn off the axis labels. Then plot your own custom axis after the plot using the axis function. edit: misunderstood the question.
factors are internally represented as integers. Now since x is a vector of numerics, plot() assumes that the second parameter will also be numeric and screws up. plot(factor(y),x,xlab="Booksread",ylab="Grade") On the other hand recognizes that the first variable is a factor and therefore calls boxplot() If you do want the individual data points ploted you have to use stripchart: stripchart(x ~ factor(y))
A giant if else decision tree to parse everything into a POSIX class, then define a period of time for each season and then use if statements to classify each date. If you have date ranges, maybe 2 columns, one for the start and one for the end of the time period?
If they are actually in date format, you can pull the month using months() and then compare that to the actual season definitions with if/else statements. You may need to do some splitting since it looks like you have some that overlap seasons.
/u/a_statistician This is incredibly detailed and helpful thank you. I will take some time process this and implement it or something similar. Thank you! 
I agree. You should write a blog about this I'm sure it would help lots of new R users. :-) 
Take the `colour` argument out of the aes. You're not mapping it to your data. i.e ggplot(data.frame, aes(x=test)) + geom_line(aes(y=one), colour="blue") + geom_line(aes(y=two), colour="red") + geom_line(aes(y=three), colour="green")
Seems like the data should be reshaped into long format as well which would avoid the multiple geom_line statements.
Yes that makes perfect sense, cheers!
Customizing the coloring is a bit of a pain, if you can live with the ggplot defaults, then what you should really be doing is melting the data first and using ggplot how it was intended like this: ggplot(melt(data.frame,id.vars='test'),aes(x=test,y=value,colour=variable))+geom_line() If you must control the coloring, this is how: myColors &lt;- c('blue','red','green') names(myColors)&lt;-c('one','two','three') ggplot(melt(data.frame,id.vars='test'),aes(x=test,y=value,colour=variable))+geom_line()+ scale_colour_manual(name = "variable",values = myColors) 
Sure the other examples work, but this is the most "textbook" response. 
As I understand it (I'm not a Julia user), Julia has type hinting. http://docs.julialang.org/en/release-0.4/manual/types/ For what it's worth, I often find myself getting caught up on errors that would be solved by a stronger type system.
&gt; If we had types everyone learning R would have to learn and use type hierarchies. It would quickly become a burden. We already have to do this. Adding static typing doesn’t *add* a type system, it just makes it explicit. If anything, static typing makes it *easier* to reason about types. It never makes it harder. Haskell seems harder to you not because it has a static type system, but rather because it’s a much more powerful language. Your argument about type hierarchy doesn’t really hold water either: &gt; For example you have a function that returns a diagonal of a matrix. So a vector. Does it have any special type instead of being just vector? Well it should - not all vectors are diagonals of some matrix. No, no, no. There’s nothing wrong with a matrix diagonal having the static type `vector` — that’s what it is. Your argument is confusing instance with type; it’s akin to saying that `3` should have a different type from `integer` because not all integers are `3`. You’re right that symmetric matrices potentially need special handling because they support operations not all other matrices support. But this is a solved problem in statically typed languages. And numeric vs character matrices are not a problem at all, this is neatly solved by generic types, which `matrix` would obviously have to be.
Thanks for the thoughts, can't wait to read again when I'm awake! What's the umbrella term for S3, R6, (and S4 and apparently others)? I don't quite get what they are. &gt;we’d have to disallow modifying existing objects Replaced with, for instance, creating a new object out of the one we want to "modify"? (which I think is how Haskell does it?, if I remember)
aes has the group parameter and you can use expressions, so x%/%2 or something like ifelse(x %in% c(1,2), "first", "second") should work
cut might be worth trying inside mutate since it would give you the values of the split rather than just an integer representing a group (not familiar with using integer division). Wouldn't be any faster though.
Without knowing more about the situation, it's really difficult to answer the question. Still, it seems like this is a job for tidyr or reshape2. 
The data tables package can do all of this quickly and succinctly https://rawgit.com/wiki/Rdatatable/data.table/vignettes/datatable-intro-vignette.html
It seems like you should make a class that describes a single sub-list. For the big object holding all the little sub-lists, you could then make a superclass that inherits from the first class. 
Thank you for your response. I was thinking this, but the slots on the superclass just be @1 @2 @3 ... @count(sub-lists). And that would be variable depending on the instance of the class. I cannot find any examples of an S4 class like this online, and I can't think of how I would define the class due to the variable amount of slots (which could go into the thousands range). Would I just have to define every possible slot?
Yes! Thank you. This is what I ended up doing. Thank you for all your help!
The Hadley Wickham way to do this would probably be to use dplyr with his other package tidyr. TidyR is kind of like the reshape2 package but more simple and intuitive, it's main usage is for converting from wide to long formats and vice versa (which is what you are doing). Try this out: library(tidyr) # fake data d &lt;- data.frame(personID = c(1,2,3), other = c("F", "F", "T"), native = c("T", "F", "T"), black = c("T", "F", "T"), white = c("T", "F", "T")) # pipe the data set to tidyr::gather() new data &lt;- d %&gt;% # the syntax here is: OUTPUT "key" column name (i.e. your labels), OUTPUT "value" column name (i.e. true, false, numeric values, characters etc.), and the INPUT column names gather(race, value, other:white) %&gt;% # now filter out rows with false dplyr::filter(value == "T") Have to specify `dplyr::filter` as there is another filter function in the `stats` package. "X must be atomic for sort.list" usually means the function is expecting a **single vector** as an input, not multiple vectors. You're trying to pass the results of more than 1 column to the factor function (columns are technically vectors; data frames are technically a list of vectors with the same length), in this scenario, to a single output column/vector `data$race`. Factor is only expecting 1 vector as an input. 
[Introductory R](http://www.introductoryr.co.uk) is my favorite beginner R book. As others have mentioned, swirl is an interactive beginners course in R that is really helpful for getting comfortable within the R environment. The DataCamp R course is pretty good although I've found the in-browser console to be a bit buggy which can get frustrating. [CodeSchool](https://www.codeschool.com/courses/try-r) is another free interactive website that you may want to take a look at. The R course is relatively short but a good introduction to the core concepts. Microsoft also offers a free [Introduction to R Programming course](https://www.edx.org/course/introduction-r-programming-microsoft-dat204x-0) on edx that is probably the most in depth option. The interactive portions of the edx course are run through DataCamp so you can consider it a more detailed version of the DataCamp course. 
I'm a huge R fan, but know going in that R is great for *doing* things; less great for *building* things. It's a great platform to do analyses, visualizations, research, academic writing, etc. but it's not what you'll use to build the backend on a website, for example, or do commercial database management. 
ok great! thanks so much! 
Great! Thanks for the links and I just started Roger Peng's course :)
Kaggle has a Titanic dataset for learning that you might like.
Yeah I checked them out but the first column never has a header. The only way I know how to create a plot is by using headers (e.g. plot(variablename$column1header, variablename$column2header). Plus when I use file.edit() it won't allow me to edit the file so I'm totally lost
This could be a good opportunity to learn how to assign headers to an existing data frame object. Then once you do that, you can plot them as you normally do.
&gt; Yeah I checked them out but the first column never has a header. There might be a reason for that. Maybe the first column doesn't have real data in it, such as row numbers.
`?apply` would have prevented that entire blog post.
The first column is often the row names, rather than a real column. [For example](http://www.r-tutor.com/r-introduction/data-frame), the mtcars dataset has data on a bunch of cars. Rather than having a "car name" column, the name of the car is in the row name. You can interact with the row names using the [rownames() function](https://stat.ethz.ch/R-manual/R-devel/library/base/html/row.names.html). You might want to read over some tutorials on manipulating data frames -- they're a really powerful way to store data, but a bit tricky if you're used to something more basic. The built-in datasets are a good way to explore how to manipulate data frames. See if you can create a new data column and fill it with the row names of the same data frame.
I think his criticisms are fair. R has a very steep learning curve because even the most basic problems have multiple solutions. I had nowhere near the amount of trouble learning C as I did learning R for that reason. 
I use R on a daily basis, and I find the documentation on apply to be unreadable, try it out, if you don't already know what it says, it's garbage. Even something as simple as the definition of X is totally arbitrary. &gt;X an array, including a matrix. Now does that mean that X is an array which includes a matrix or does that mean that X is an array which may include matrices? Someone who doesn't already understand it won't be able to make sense of that. The whole article is unreadable from the perspective of someone who is learning about R. 
You have a point, but in these cases it’s necessary (and possible) to follow the trail of information. In this case, `?array` and `?matrix` will answer this question. But granted, it’s not an easy read. That’s why it’s indispensable to actually *learn tools properly* when using them. This (very common) attitude of learning by doing doesn’t work well for complex intellectual activities such as programming. Unfortunately it works *just* well enough that people think they can get away with it, and that’s why we get so many bitchy blog posts.
The comparison is completely unfair. C is an intentionally primitive language and consequently simple to learn (but not to use). C and R serve completely different purposes, and R is a much bigger language than C. C’s design is obviously a joy to behold — very few languages have such a clean, well thought-out design. But achieving such a thing is much easier for simple languages.
So, in other words...?apply would not have prevented that blog post?
&gt; I guess you can say I want to replace them with blanks What’s “blank”? In R parlance, a blank *is* an NA. Removing NAs is just that: a complete removal of the values from the data, and since individual fields in row/column cannot be missing the whole corresponding row (or, rarely, column) is removed. This is done because *it’s the only sensible way* of removing NAs. You can of course substitute NAs with any other value that you’d like (the other answers show how) but you need to be aware that 1. This isn’t replacing NAs with a blank, it’s replacing a *blank* with another, arbitrary value. 2. Subsequent computations on the data will yield potentially wrong results, since the replacement values will influence the computation. For instance, if you subsequently calculate the mean of a column, NAs that have been replaced by zeros will skew the result. Conversely, if you had left the NAs in the data, you could instruct the `mean` function to ignore NAs for the sake of the computation (`mean(the_data, na.rm = TRUE)`).
Why do you want to do this? Unless you have a very good reason to do this (and if you did you would probably know how to replace NAs with the blanks you want in the first place) I would suggest not doing it and instead describe what you really want to do. It would be unusual for somebody to say something like "I want to replace NAs with blanks" and have that be the last thing they do - no more analysis - no more anything. So what are you intentions? Because most likely keeping the NAs as NAs is the better route for whatever you want to do.
Say I'm trying to get the median of some column, all it would give me is NA every time. I figured if they were blank then maybe it'd ignore them, but maybe I could leave them and find a way to just have it ignore NA every time it sees it?
Thats' exactly what I needed! Thank you so much. 
Awesome, that's so easy. Thanks man/woman. 
It would make sense to use a plotting package which easily handles the generalized case of this but my response answers the OP's question. *shrugs*
Side note, you can also provide integers (or factors) to col (there are 8 basic colours so it does modulo 8 on them) In you case using col=mtcars$cyl/2 will give you what you want (numbers might not match the specific colour). If I want different colours I usually have them in an array and then use the integers to pick them e.g. col=c("red","blue","green")[mtcars$cyl/2 - 1]
Might make more sense to do just do char_vector &lt;- unlist(strsplit(text, "")) in place of lines 2 and 3. Might also be useful to call either tolower or toupper on char_vector depending on the goal.
Thanks, but how do you exclude whitespaces and other random characters (e.g. commas, parenthesis, periods, semicolons, etc.)
I think you're missing the spirit of the argument in the post in which the author states that while there are many roads to chieve the same result, sometimes due to the nature of the way R packages are constructed it can be very confusing for the intermediate user. Lets face it, novice R users will remain novices until they have banged their heads against the wall for a while and learned by doing. The advanced R user will still run into these strange piutfalls. The intermediate R user face a further uphill battle when multiple advanced R users state that there are several different ways to do something because of the sheer power of R.
Awesome, thanks man/woman. I've been busy with my job that pays me so I haven't started this yet but will it count upper and lower case letters separately? 
I haven’t missed that point at all, and it’s completely intangible to the argument I’m making here. It’s one thing to (legitimately) complain about the lack of uniform, well-designed API. This is a point that I entirely agree with (in fact, I’ve written another answer where I acknowledge that the author makes good points). It’s another thing to complain about an entirely *sensible* design and reveal that the problem is actually caused by not having read the relevant documentation. By their own assessment, the OP is *not* an R novice any more. There’s really no good excuse to not having read the documentation of something you’re using for work routinely. (Truth be told, I also don’t always read the documentation of every single thing I’m using. But then, when I trip up, I lay the blame on myself rather than blaming the tool.)
Yes. Each unique character. If you want to count them as the same you should convert to all upper or all lower char_vector &lt;- tolower(char_vector)
Thanks for the advice!
Great answer. Most functions have ways of dealing with NA, without needing a data subset. An alternative would be writing an alternate custom function that remove NA from calculation, e.g. medians &lt;- function(x) median(x, na.rm=T) 
In the long term, give all of your files and functions descriptive names. I don't know if you use RStudio but if you do control+. a little search box opens that allows you to search within your RProject by function name instead of by filename. If you don't yet, setup an R project and add that whole folder of files to it to enable this functionality. I structure my exploratory code in the form of a main control script, and one or more other files that house all of the actual functions, so that it is easy to search for the functions in this way and to share them across projects. There are a lot of options for making packages; creating a little function library and having a load file to source everything in it; etc but it really depends on what your existing files are like and your most common use cases.
Thanks for sharing. I do use RStudio, but instead of the build-in user interface, I'd created a search function to dump the lines or chunks from the search string into an output directly instead. More intuitive to browse all search directly than relying on the GUI in my opinion. However, it still doesn't really solve the issue, due to inconsistencies from most of the scripts, collated from multiple co-workers. I'm guilty of some of the inconsistencies myself, be it due to new packages used, on-the-fly analysis or mood-swings. 
That's a nice structure. I think the more common terms would be "run.R, clean.R, data, output, utils". But it's great to see you're organized like this You could also look into a Makefile if you have multiple inputs/outputs
So I want to remove all the whitespaces and punctuation from this text file and only graph letters and numbers. Here's my steps: text &lt;- readLines("~/path/to/file.txt") text &lt;- paste0(text, collapse = "") gsub(" ", "", text) gsub("[[:punct:]]", " ", text) But when I remove the punctuation then the white spaces come back and I'm stuck in a loop of choosing whether or not I want the whitespaces removed OR want punctuation removed, NOT both. How do I remove both the punctuation AND whitespaces? 
I just converted all the "" to NA then used the is.na command to delete them :[
table() and barplot() for your second part, and you can eliminate "" before or after as dasonk describes
First, when you post a question like this it's almost always more likely to get answered if you also post the code to make some generic data that mirror yours. Most people aren't going to take the time to set up fake data to provide you an answer, and rightly so--it's your question. That said, just from reading your question you could probably get pretty close by using a for loop, selecting each relevant subset in an iteration of the loop, and plotting the result. Something like this: for(j in unique(variable)){ temp&amp;lt;-data[row==j, col] boxplot(temp, ylim=(,) }
What are you trying to plot with? This seems like it would be pretty straight forward with ggplot2: http://docs.ggplot2.org/0.9.3.1/geom_boxplot.html Since you are a beginner, I would recommend getting comfortable in the hadleyverse to make your life easier. http://blog.revolutionanalytics.com/2015/03/hadleyverse.html https://www.rstudio.com/products/rpackages/
Look into faceting with ggplot2 and reshape2 to do this. I came up with the following (x is the name of your data frame): library(ggplot2) library(reshape2) x.m &lt;-melt(x, id.vars = c("Patient", "ACCOM")) p &lt;- ggplot(data = x.m, aes(y = value, x = ACCOM)) + geom_boxplot() + facet_grid(.~variable) p This will plot everything together, and you can adjust the groupings by changing the .~variable bit to .~ACCOM and x = ACCOM to x = variable. You can set a lot of different things in ggplot as well like colours/fill inside the ggplot argument but answering that goes into using ggplot2 (http://www.cookbook-r.com/Graphs/). As /u/fastrmastrblastr mentioned, it is always better to post a bit of code you've tried and something simillar to the data you have, you can do that by setting up like this to make a random data set (this is what I did to test): x &lt;- data.frame(Patient = 1:20, COSTS.T1 = sample(10:40, 20, replace=T), COSTS.T2 = sample(15:50, 20, replace=T), ACCOM = sample(letters[1:6], 20, replace = T)) 
So, I'm not actually sure if there is a way to force an intercept in the traditional lm model without a workaround (and I'm not sure why you particularly want to, either, but I know there are sometimes reasons). If you want to do that, I think the following code should work: intercept &lt;- 10000 Y_Prime &lt;- Y - intercept lm(Y_Prime ~ x + x^2 + 0) Where 0 will suppress the intercept in the lm model, and it will effectively fit y prime to the data set. 
As much as I love bayesian methods there is no reason one would need to resort to bayesian methods with a strong prior just to "force" a term. The workaround given works just fine. If you want there to be an actual term for the intercept instead of just subtracting it and fitting a no intercept model you could do something like... yint &lt;- rep(1, length(y)) lm(y ~ x + offset(1000*yint) + 0) offset terms automatically get a coefficient of 1 but you can multiply to make any coefficient you want. In this case we construct the 'predictor' that is used for the intercept and then manually add it in as an offset term specifying a coefficient of 1000 (through multiplication) and then tell lm not to fit the intercept (since we already added it in). With all of that said I don't see much reason as to why one would force an intercept in such a manner.
Can you be more specific? Do you store it in a variable once, create the gridlines, then create it normally and it should work? 
barplot(my.data, main="frequency vs letter", xlab="letter", ylab="frequency") grid()
while we're at it, is there a way to add tick marks to a 2nd y axis using the minor.tick command?? 
http://stackoverflow.com/questions/6142944/how-can-i-plot-with-2-different-y-axes You just have to add an axis on the opposite side (with the other scale). Doing two on one side is possible, by shifting the second further out but it also makes everything more confusing. In general try avoiding two axis plots if possible, especially in the case of barplots there isn't really a reason not to plot them next to each other.
Well I have a barplot of frequency (y axis) of letters and numbers (x axis) in a document--since there are 30+ bars in the graph I thought it'd be nice to have 2 y axes? Is there a better approach?
Sorry if you mean two identical axis just one on either end, then do it as the link describes with axis(4) (or what ever the number for the right side is). I was under the assumption you wanted two different scales in the same figure (as shown in the link). Which is generally not as good, because one has to match between the data and the correct axis.
there is the option panel.first, you can give it R code that is executed after the plot device is created, but before the actual plot is drawn. e.g: plot(1:10,1:10, panel.first=grid(NULL,NULL, lty=6)) should work EDIT after quick testing for barplot you need panel.first={par('xpd'=FALSE);grid(NULL,NULL)} because he otherwise draws the grid to far out.
No problem, I'm glad it helped! Good on you for going beyond what the class is asking for too. R is really useful to know and ggplot2 is really well documented.
Thanks! If I'm understanding this correctly, I have to rearrange the pixels into the correct matrix? So am I correct to assume that my matrix should be the size of the expected output image, i.e., 640x480?
If the data doesn't line up then you need to figure out what you want the X axis to be. i.e. do you want the line to end after the data runs out, so d would have the longest line but c would be shorter? Maybe this helps: http://stackoverflow.com/questions/15562289/plotting-2-datasets-with-unequal-lengths-using-ggplot2 
Of course. I tried the code below to get red assigned to teams with &lt;30 wins and green to everything else (x was the variable I stored my.data in). I kept on getting this error message: Error in dotchart(x$Price, x$Team, col = ifelse(x$Wins &lt; 30, "red", "green")) : 'x' must be a numeric vector or matrix 
look that error helps, your x$Wins is composed of character strings (or possibly factors). So: class(x$Wins) to make sure it is "character", and if yes then using as.numeric(x$Wins) will do the trick
Awesome, thank you. If i wanted to set a range of values, say &gt;30 but &lt;40--how would I do so?
What are you looking for? You can use sessionInfo() to get a lot of information. To just grab packages you could use external_packages &lt;- sapply(sessionInfo()["otherPkgs"],function(x) { names(x) }) Probably a better way, but it works. 
Is this coming from the perspective of an RStudio server admin?
use &amp; to combine to logical operations see ?'&amp;'
To get comfortable with R, I would suggest spending as much time as possible playing around with the data structures(vector, lists, data frames ). The "datasets" library has some sample data sets that you can work with. Once you are absolutely sure that you have got your head around them, try looking at ways to subset them or add/modify them. The majority of time spent in R coding is in managing data. Look at [this](http://www.studytrails.com/R/Core/StartingR.jsp) tutorial for a starting point.
Well yeah. You were pretty vague in your set up. Often if we're talking about monitoring our 'users' I would assume there is an integrated ecosystem all users are hooked into. In the case of are R this ecosystem is most commonly RStudio server. In the case of RStudio server all users are hooked into a common compute server and access it via their web browser, and as you've found it allows for a bunch of configurations and monitoring features. Now if instead your 'users' are all running R on their local machines then it would be a little more work to get that same data. 
You definitely need to be more specific, and ideally provide a [minimally reproducible example of your code](http://stackoverflow.com/help/mcve). This is particularly important because the code on the Stack Overflow question you’ve linked to works like a charm. If it doesn’t work for you then there’s something else wrong. That said, the snippet you’ve posted simply doesn’t include the relevant code to produce the lines anyway, so maybe just add that?
[grouped bar plot](http://stackoverflow.com/questions/17721126/simplest-way-to-do-grouped-barplot)
Google Excel grouped bar chart and show how far you've gotten so far and what part you are stuck on. There are many good YouTube videos of various Excel functions. You might try looking there as well. That should give you a good foundation. Then translate to R.
Hi, Thanks for the reply. I've updated the post with a more complete representation of what I'm working with. I realize that I didn't include a command to draw any lines because everything I've tried hasn't worked and normally fumbles up the graph. When I add the suggested line from the post I linked: geom_errorbar(stat = "hline", yintercept = "mean", width=0.8,aes(ymax=..y..,ymin=..y..)) It graphs all the data just fine but I get this: "Error: No stat called StatHline." and no lines appear on the graph. If I remove the stat = "hline" (which supposedly no longer exists), then I get: "Error: Unknown parameters: yintercept."
have you tried to copy the full code there? It's exactly how it is in your OP. http://i.stack.imgur.com/RARzx.jpg Can you post your code and output?
Code: ggplot(nfl, aes(factor(Team), Price, fill=Wins)) + geom_bar(stat="identity", position="dodge") + theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5)) Output: http://imgur.com/JU0RBX6
I dont' know the format of your data
as it was in the comment below: nfl=the variable where I stored the .csv file that contains... Team=name of NFL team Price=Average price of tickets during the 2014 season Wins=win percentage of the 2014 season 
Awesome, that's exactly what I was missing. You just made my day, thanks. 
This is the code: &gt; install.packages("ggmap",lib="C:/Program Files/R/R-3.2.3/library") also installing the dependencies ‘RJSONIO’, ‘maps’, ‘sp’, ‘RgoogleMaps’, ‘png’, ‘rjson’, ‘mapproj’, ‘jpeg’, ‘geosphere’ Packages which are only available in source form, and may need compilation of C/C++/Fortran: ‘RJSONIO’ ‘maps’ ‘sp’ ‘png’ ‘rjson’ ‘mapproj’ ‘jpeg’ ‘geosphere’ These will not be installed
You will need Rtools to compile R packages on Windows. https://cran.r-project.org/bin/windows/Rtools/
either mapply, or apply with an inbetween 'wrapper' apply(the_data, 1, function (x) your_function(x[1], x[2], x[3]) ) depending on your result you may need to use lapply and you'll likely need do.call('rbind', the_result) to rbind it all together
Thanks for the reply! I'm not sure I actually didn't try mapply because it sounded really intimidating. I also wasn't sure how it would address the issue of knowing that the "vector" i'm supplying is just one element for each input in the function, as opposed to the "vector" being the first element of the function. Could you give me a little more detail on how you would go about it? 
Installs fine for me. You might want to elaborate on what "no dice" means in your particular case. And in case I need to spell it out - post what you ran and what you got in response. My guess is that you're running windows and don't have the necessary tools (Rtools) needed for compilation.
Yeah, I've used almost all the https CRAN mirrors--I've been going down the list.
I'm not 100% sure on the details but apply (and other functions from the family) have some assumptions on the returned value of the called function and in general no side effects are allowed to happen (they screw stuff up). So my guess is that something along those lines is the problem (each result is probably only allowed to be a single row of the result), and then mapply also wont help (for the record you would have to pass it each of your columns separately). You can try lapply(1:nrow(x), function (i) testfunction(x[i,1],x[i,2],x[i,3])) that returns a list of results (and has no assumptions about how the results look) which you can then combine with the do.call I mentioned above, or you can always use a for loop, but may need to preinitiate your return variable, if you have lots of inputs.
I'd run remove.packages(pkgs, lib) to remove rncl and any dependencies then try a reinstall. You could go into the library folder and ensure it's gone. Otherwise, I'd specify a new library and ensure that it works. FWIW, I was able to install the package to my computer and I'm running 3.2.3 so I think it's a issue on your PC and not the server. 
Try setting the installation library to a folder that doesn't have the R version number in the path ("3.2" here).
I would use a pack called matrixStats and use the function colMeans() 
Awesome, I actually found the data.table package and have been using that. I've been using this formula to find the average # of home runs a year: hr1980 &lt;- DT[year=="1980", mean(HR, na.rm=TRUE)] DT =my .csv file as a data table year= year HR=home runs Would I be able to create a command to continue doing this for 1981, 1982, 1983... and so on without having to create individual variables and then somehow combine them?
Try: hr &lt;- DT[, mean(HR, na.rm=TRUE), by="year"]
Thanks! I'll do that.
I'll be trying it when I get home--it just means I'm appreciative of the advice. I'll be sure to let you know, though!
I also asked the original question :). Do you just use comment lines to structure it then? It gets the job done I guess, it's just like later on if I wanna "turn off" section 2 say, there's not a super smooth way to do that. I guess you could also put it in a giant conditional statement and then have switches at the top, but that feels a litlte cumbersome.
This would be simple with dplyr. require(dplyr) baseball.data %&gt;% group_by(year) %&gt;% summarise(Annual.Home.Runs = mean(Home.Runs, na.rm = TRUE))
I was going to say use dplyr as well. I run into this problem a lot when I'm trying to find information about a subset. However the group_by function sometimes gets a little funky for me I like to create my own dfs withit 1980 &lt;- filter(baseball, year == "1980") mean(df$year) 
What are the issues with group_by that you've encountered? To use the approach you just proposed, you'd probably want to use an apply function over all the years. Something like this: years &lt;- c(1980:2015) sapply(years, function(x) baseball.data %&gt;% filter(year == x) %&gt;% summarise(mean(Home.Runs, na.rm = TRUE)) )
Ok, dumb question. How would you add the average to the data frame as another column? I was trying to do this a while back and all I could come up with a was cbind. That works, but I thought there might be some sort of summarize/mutate hybrid. 
You could use lists in your functions. You could also save your datasets as Robjects. I typically have all my functions organized in separate files and most of the main data loading is done in a "main" script that sources the other functions. I also make heavy use of dplyr and tidyr. 
Great answers everyone. Is there a way to do this with ranges of dates? Like 1980-1985 and 1986-1990, etc. 
EDIT: I was wrong, read the other guy's post
x &lt;- data.frame(x1 = c(1:10)) y &lt;- subset(x, x1 %in% c(5:8)) z &lt;- y$x1
 x[5:8] EDIT: I guess I was downvoted for providing the simple answer on how to get the values between the 5th and 8th positions. If you are looking to find the vector positions for values that are between 5 and 8 (integers only) you can do this. x[x %in% 5:8] If you are looking for a vector which contains all of the numbers between those two from the x vector, there is what goat mentioned. 
The comment seemed ambiguous and I took it the wrong way. Edited. 
Number of home runs is a count variable, which is not appropriate for linear regression. If you simply are interested in bivariate correlations, use Spearman's rank correlation between steroid use and number of home runs. You can then filter or split your file by year, such as data from 2010 and then run your correlation. Next, select data from 2011 and so forth. I strongly recommend that you do not dichotomize your data - you will lose valuable information. Also, only look at it by year if you have an adequate sample size (at least 100 data points or so each year) 
To modify axis ticks, you can try to use yaxt="n" in your call to plot, and then use axis function: year &lt;- sample(1975:2000, 200, replace=T) salary &lt;- sample(5e5:4e6, 200, replace=T) df &lt;- data.frame(year, salary) plot(df, yaxt="n") tick_positions &lt;- seq(500000, 4000000, 1000000) axis(at=tick_positions, label=tick_positions/1000000, side=2)
Thank you. So lets say now I have a data set with all the players' individual salaries, the year, and whether they were in the National League (NL) or American League(AL). Let's say I want to make a line chart showing the averages over the years by whether they were in the National League or American League. Would I use aggregate() to get the total salaries for each year by AL or NL or would I use melt()?
What is this called? I have another project I'm working on where I'd like to add another variable to the by="year" part of the command but unsure how do to it. 
Ok - I don't use barplot, so I'm not going to be much help, but according to the below blog post, using the arrows() &amp; segments() functions will get the job done. The post also has a solution for ggplot2 that I quite like. http://www.r-bloggers.com/building-barplots-with-error-bars/ p.s. - Rat neuroscientist?
[**@michaelhoffman**](https://twitter.com/michaelhoffman/) &gt; [2014-11-10 14:55 UTC](https://twitter.com/michaelhoffman/status/531822555465007104) &gt; 2/Bar plot with error bars is a sign of this kind of hidden data. Reviewers should not allow. @pknoepfler @RetractionWatch ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I don't think so, but if you are thinking about creating your own template, you should read the source code of ggthemes' templates and do your own thing. It's not that hard.
Check out http://www.ggplot2-exts.org/
Yeah but how do you know which color combinations that are more "visually aesthetic?" Is there a systematic way to it?
Well, you can read some visualization authors like Tufte and Stephen Few.
You could use [inflation data from the Fed](https://research.stlouisfed.org/fred2/series/GDPDEF) to figure out what 1985 values would be if they only grew at the rate of inflation. Just figure out the ratio of each year's deflator value to 1985's deflator value, multiply that by your 1985 salary, and add a line to your graph. They release the data quarterly, so be sure you're consistent about which quarter you keep from every year. I'd go with the last yearly observation but as long as you're consistent it shouldn't make much of a difference. Good luck!
So on a level of 1-impossible how challenging is this for a beginner to do? 
From what I found in a quick search, the main issue with doing linear regression on count data is having lots of zeros that throw it off. That's not an issue with this data, so are they any other reasons it would be inaccurate with count data?
I think excel is easier if you need to do something once and only once. If you need to make multiple graphs (or repeat the process at a later date), R gives a HUGE time savings.
not sure, but again if you want an R^2 it's trivial to get using a validation set. 
This might be the best place to start with those questions http://www.r-bloggers.com/joining-data-frames-in-r/ I typically only keep data that shows up across the board (inner join). 
Good luck!
I guess one option is to store it in a graph db like Neo4J and output the queries into dataframes
Using your example here, salary &lt;- data.frame(year = 2006:2011, value = c(2295547,1859555,2364383,2812141,2335314,1986660), team = "ARI") win &lt;- data.frame(year = 2006:2011, team = "ARI", wins = c(76,77,90,82,70,65)) # gets you the combined data set merge(win,salary) # a simple correlation cor(salary$value, win$wins) Does that help at all?
Basically my goal is to create something like this: http://pix-media.s3.amazonaws.com/blog/795/NFLMoneyWins.jpg I'm not sure if this method will help though? 
[Look at the top links](http://www.lmgtfy.com/?q=r+plots+with+regression)
Using dplyr and ggplot2 the following should work: baseball = inner_join(salary, win, by = c('year', 'team')) ggplot(baseball) + aes(value, win) + geom_point() + geom_smooth(method = lm, ci = FALSE) Then you can get fancy, such as adjusting the axes so that they match the ones in your picture, including the R^(2), etc.
use `dplyr`! it makes joins so easy you will laugh at your computer screen in joy.
The SSL/TSL certificate on the r-project.org website begins on 08/18/2015 and expires 08/18/2018. Perhaps you have another certificate in mind, maybe of their email server? There is no vulnerability implied even if there was an issue with their SSL/TLS certififcate. CRAN downloads are signed with the a GPG key ("Michael Rutter &lt;marutter@gmail.com&gt;" with key ID E084DAB9" and that has no relation to the SSL/TLS certificate.
No need to use a loop or an if statement here. Cost &lt;- ifelse(Toll &gt; .4, 10000, 20000)
We would need example of the data to help. Sample of it that produces this "error"?
+1 but please don’t use the abbreviations for `TRUE` and `FALSE` (*especially* when teaching beginners) since they are not reserved and can be reassigned (`T = FALSE`).
.... It might be. I forgot about sample. I'll check when I get back to a laptop later, thanks.
Do you want 5 of each letter but placed randomly? Or do you want 25 random draws from `a` to `e` then arranged as a matrix (in which case, for example you might get 8 'b's but no 'd's) First one: matrix(sample(rep(letters[1:5],5)),nr=5) second one: matrix(sample(letters[1:5],25,replace=TRUE),nr=5) 
Who would do such a thing. I don't see harm in using abbreviations. 
The second one was 100% what I was after, thank you very much! I guess I would use similar code if I wanted to replace a random square in the matrix with a random letter from a - e too? (then put it in a loop to repeat 100x or whatever)
Ah ok, I apologize I see what's happened but I'm not sure on the solution. It apparently takes the whole ALLDATA table copies it into the column QCAnalysisNumber. It looks like a copy of the dataframe is stored in QCAnalysisNumber, as I get an error about writing a dataframe in a dataframe when I try to write the table to file. For example, see this result: Where the "log" columns are the last in the sheet, then the table headers are copied with the QCAnalysisNumber.xxxxxxxx naming convention. Unfortunately this code is quite long and pulls data from an access database on my computer so I'm not sure I can copy a repeatable case. LogComboResult LogResult QCAnalysisNumber.SiteNumber QCAnalysisNumber.SiteRegulatoryID
This error pops up in R-studio: Warning messages: 1: In is.na(e1) | is.na(e2) : longer object length is not a multiple of shorter object length 2: In `==.default`(ALLDATA$LongName, ALLDATAQC$LongName) : longer object length is not a multiple of shorter object length 3: In `==.default`(ALLDATA$SampleDate_D, ALLDATAQC$SampleDate_D) : longer object length is not a multiple of shorter object length
Thanks anyway. I appreciate it.
You could do it that way, though I'd be trying to see if I could do it without loop.
I actually remembered it incorrectly, in aes you usually write by which column of your dataframe the colour should be chosen. Add a scale_colour_manual() to set specific colours (see http://docs.ggplot2.org/0.9.3/scale_manual.html). That said, there are lots of scale_colour_... functions with different default discrete and continues scales which might be simpler to use.
ggplot is already 2.1.0. I don't ever suggest linking anyone old versions of docs since some things might be deprecated. http://docs.ggplot2.org/current/scale_manual.html .
I get comparable times for the two on my system. But the code doesn't really make sense. I'm guessing you made up an example for sharing but it doesn't really make sense. Why apply mean to each value separately?
So you want read in this csv file, normalise it, and save it as a table in an SQLite database? Doesn't sound like a huge job, and you can always loop through files in a directory if need be. Create a database connection: con &lt;- dbConnect(SQLite(), "name.db") Read in your csv, and normalise it. Write your normalised data as a table. dbWriteTable(con, value = norm_data, name = "normalised_data") Disconnect from database: dbDisconnect(con) 
The write your normalizeddata... that's the part where I am asking if there's a faster way other than subseting Dataframes into separate tables.... the rest I already had.... 
The pacman package might meet your needs. There is a p_load function which tries to load a package and if it isn't installed already goes ahead and installs it.
install.packages() is what everybody should use to install packages from CRAN. If the packages are only on github etc. you can use the devtools package to install from them. In general making your script a small package might be the simplest way (if it only relies on CRAN packages), since when installing a package all dependencies are installed automatically. (There might be a way to get at that functionality directly.)
Yeah for now, I ended up just appending the script on top with a simple statement that sees if (!required(....)) returns true or false, if false, I use install.packages to download it. 
oh cool, I'll take a look at it. For now I made my own simple loader that uses the !required and the install.packages() functions. I'd really like to use something more robust so I'll definitely take a look at pacman. 
You are looking for packrat. https://rstudio.github.io/packrat/ Installs packages to your project folder (make sure you add the lib folders to your .gitignore), has version control, and syncs very well with rstudio.
&gt; he colon returns a vector from a to b (only works on integers) Oh really? &gt; 2.5:6 [1] 2.5 3.5 4.5 5.5 
Each of the numbers in the "Estimate" column are your beta coefficients. Your model follows the form: Y = a + b1X1 + b2X2 + b3X3 ... bnXn So if you wanted to find an expected insurance charge for somebody, it would be: charge = -4745.546 + age*263,242 + sexmale*-491.179 + bmi*115.035 ... smokeryes:BMI30obese*19794.852 I'm assuming some of these factors (sexmale, smokeryes, regionnorthwest, regionsoutheast, etc) are binary, either 0 or 1. If that's the case, then you can interpret the coefficients as, for example: somebody who is a smoker will pay 13402.363 than somebody who isn't, all other things being equal. &gt;My second question is how can my median be -1251.6 when everyone has charges in my data set. That is the median residual. Your residuals are the observed (actual) values of your y (in this case insurance charges) minus the expected y values. &gt;I am trying to see if the insurance charges go up if someone has these qualities. If the coefficient for a factor is positive, then that means it is associated with higher charges, all other things being equal. 
I was assuming that the regions were binary, are they not?
I'm not sure of your data set (nor am I an expert), but make sure you are testing your model against other data sets, or splitting your data into training and testing models. Otherwise you run the risk of over training.
Why don't you write *your own* REPL, using [R's expression-parsing abilities](http://adv-r.had.co.nz/Expressions.html)? You'd still be constrained by the syntax of R (so no "open door"), but it would be possible to parse / check validity / modify input before evaluating it. You can write a fully functional REPL in R using `readline()`, `parse()` and `eval()`. `parse()` returns an expression tree that you can access as a nested list: &gt; p = parse(text = "a = 1; b = 2; print(a + b)") &gt; p[[1]] a = 1 &gt; p[[1]][[1]] `=` 
&gt;(so no "open door"), but it would be possible to parse / check validity / modify input And actually this sounds even better because I was already imagining a layer between input and evaluation to try some (very amateur level) NLP, partly to see what techniques could avoid the old: "sit chair" ERROR "sit in chair" ERROR "sit on chair" ERROR "sit in throne" ERROR "sit on throne" OK
Cool idea, I hope you'll keep us updated on your progress.
Well, you can still add a layer that would [parse natural-language commands](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) and turn them into R expressions. I don't know what for, but you can ;-)
Looks wonderful, thank you :)
Perfect! Thanks so much. 
Ok thank you. Do you know what the -.75 log odds probability means ?
If you report your coefficient in regular odds ratio form, then each coefficient is interpreted as a multiplier for your baseline odds. So odds ratios greater than 1 increase your likelihood of a 1 on the DV, and odds ratios less than 1 make a DV of 0 more likely. Your output instead reports log odds (which is actually natural log), and the log of a number greater than 0 but less than 1 is a negative number ranging from 0 to negative infinity. So your coefficient means that variable names the outcome less likely. Depending on your regression package, it might be easier to interpret odds ratios. At least most intro material focuses on odds ratios as being more intuitive to understand.
Best way is to subscribe to R-bloggers articles and check old articles online. They frequently provide explanatory articles with code to learn and practice. 
I actually am glad to hear that! I like the idea of using Swirl because it gives me structure. I just wanted to be sure that it would help me achieve my bigger goal. It sounds like R-bloggers will be a great resource for when I'm done with Swirl. What intermediate goals would you suggest?
To build fundamentals: http://www.r-bloggers.com/?s=exercise For topics specific to heatmaps: http://www.r-bloggers.com/?s=heatmap I find it easy to search in my gmail inbox with r-blogger + &lt;whatever topic i want&gt; to find relevant articles from their news feed.
Have you tried prefetching your token and hard coding it into the script?
The `duplicated` function shows all elements that are duplicates of an element that occurs closer to the start of the vector (left to right). It also has a `fromLast` argument that applies the same logic right to left. To see *all* duplicate elements, we can combine the two. Here's three different ways to apply this: subset(df, duplicated(name) | duplicated(name, fromLast=TRUE)) library(dplyr) df %&gt;% filter(duplicated(name) | duplicated(name, fromLast=TRUE)) df[duplicated(df$name) | duplicated(df$name, fromLast=TRUE),] 
the which() function will clean out NA's, so it can be useful for that. [See this stack exhange answer.](http://stackoverflow.com/a/6918894) I'll recreate his example of the difference below: &gt; dfrm &lt;- data.frame( a=sample(c(1:2, NA), 10, replace=TRUE), b=1:10) &gt; dfrm[dfrm$a &gt; 0, ] a b 1 1 1 2 2 2 3 2 3 4 1 4 NA NA NA NA.1 NA NA 7 1 7 8 1 8 9 1 9 10 2 10 &gt; dfrm[which(dfrm$a &gt; 0), ] a b 1 1 1 2 2 2 3 2 3 4 1 4 7 1 7 8 1 8 9 1 9 10 2 10
Some parts of it seem sort of hacked together, but that's just what happens with open source sometimes!
Thanks for your replies. I really do appreciate them Assuming this is my data set Name | Year | Distance run ---------|----------|---------- John | 1850 | 2 James | 1853 | 3 John | 1854 | 1.5 Jeff | 1851 | 4 The data above is saved as “Distances”. What I want to be able to do is pull out the instances where I have similar things. For example, I want to be able to see all the occurrence of John which would be: Name | Year | Distance run ---------|----------|---------- John | 1850 | 2 John | 1854 | 1.5 How do I do this? 
Have you tried Knime? 
I'm assuming when you say smooth, you mean loess. If that's the case, I'd use ggplot2 and use stat_smooth which defaults to loess. If you want to use base plots, try [this](http://stackoverflow.com/questions/3480388/how-to-fit-a-smooth-curve-to-my-data-in-r).
I feel like R is the only language in the world that has done vectorization right. Also, even though it's not the first to have strongly typed optional values, I feel like NA is extremely easy to deal with in R compared to its peers. Unfortunately, I also feel like R is the only language in the world that didn't manage to get programming right. And I mean this in a broad sense, as it refers to things like compatibility (how much this language is dependent on its S forebear and how many problems, questions, and nightmares are based on "Well that's how S did it") as well as runtime experience. R borrows heavily from PHP in the sense that if something goes wrong, it will do its best to keep going and not tell you. Array-out-of-bounds doesn't exist, and so even though x[0] means literally nothing in every instance because this is a 1-indexed language (which isn't even really a bad thing), the language does absolutely nothing to warn you at runtime that you did something brain-dead stupid. Furthermore, there are warnings and errors, but not exceptions, so you can't TryCatch individual problems. All of this adds up to a language that is definitely ranked up there in terms of development speed from devising questions to solving answers, but whereas Python is often lauded as another great prototyping language, R has *none* of the language features that make programming in Python fun. R is carried along with its gigantic toolbox of useful built-in features and an impressive array (probably one of the most impressive in most languages) of packages, but developing large-scale projects in R is a goddamn nightmare. I'll use R to do analysis and anything that requires math before taking the numpy route, as long as the question I'm solving has let's say less than 2GB of data. But if you actually wanted me to *program* something, I would never choose R as my first choice.
`df=data.frame(x,y)` `ggplot(df, aes(x,y)) + geom_line() + geom_point() + geom_smooth()` remove any of the geom_() layers you don't need, or use geom_smooth(method = "lm") if you need a linear model instead of loess. To add error bars, you need to have a vector of error sizes. 
You need to post some samples. Do you mean "2001.2" or "two thousand and one point two", for example? To give a general (and probably quite unhelpful) answer: packages tm, stringr and stringi will probably help you.
&gt; Furthermore, there are warnings and errors, but not exceptions, so you can't TryCatch individual problems. Could you expand on this? In my experience, conditions are hugely versatile and useful for exception handling - I think I'm not really understanding the point you're trying to make.
Actually, I have already tried that code but it just kind of made the line really skewed, almost like a vertical, hardly passing through any of the points...
for somebody from a non programming background, it was pretty easy to learn, with all the resources available. and i'm finding out that it's pretty easy to teach too. i started an internal class at work, and people have started doing wonders with very little startup work.
Here's an [example](http://www.johndcook.com/blog/2014/06/20/benchmarking-c-python-r-etc/) of R, Python, C++, Julia, and Java under various computation settings for solving an economics problem (stochastic neoclassical growth, if anyone cares to explain what that is). I've found that you can use languages like R and Python to compile C code (I mostly just use Theano in Python for neural networks) or otherwise interface with another language, and then it becomes a matter of how quickly you can program and how efficient the interface is. R is top notch for visualization, though, and I pretty much use it for almost all of my non-interactive visualization tasks. I've used Shiny to do stuff in a web browser, but it seems a bit too gimmicky to use unless giving a presentation on my own computer.
I played with it for about 10 minutes on a buddy's computer. It didn't have the same feel as Alteryx so I immaturely said some swear words and walked away from it. Would you recommend I give it a serious chance?
&gt;oes this mean out of 20 League of Legends games (assuming that the chance of you losing a game is equal to the chance of winning) to get exactly 11 wins there is approximately a 16% chance of this happening? Yes &gt; In binomial probability is each game independent of each other Yes &gt; I am confused as to why the likely hood of winning 11 wins out of 20 games with a 50% chance of winning seems so high. It's not clear why you think this is high. But in any case, here's a handy little rule for you: the probability of getting the median (k=n/2) in a binomial(n,0.5) is approximately 1/sqrt[pi (n-0.25)] (which for large n is essentially 1/sqrt[pi . n] So you'd expect dbinom(10,20,0.5) to be roughly 18% (it's actually a bit less). Now the peak is not super narrow, so dbinom(11,20,0.5) will not be a lot smaller -- as a result, 16% seems entirely plausible. rbinom generates random binomials. Look at the examples at the botttom of the help pbinom is the cdf. you give an x, it gives you p = P(X &lt;= x). It's the sum up to x of dbinom values qbinom is the quantile function. You give a p, it will give the smallest x for which p &gt;= P(X &lt;= x) 
Your interpretation is completely correct. Run this command: `sapply(1:20, function(x) dbinom(x, 20, .5))` This will give you the probability of getting exactly each number out of 20 games. Try summing up the result. Do you see that it sums to 100%? Plot the result. It should look similar to [this graph](https://upload.wikimedia.org/wikipedia/commons/7/75/Binomial_distribution_pmf.svg). The reason that 16% seems so high may be because you're thinking of the probability of winning at least eleven games where this is telling you the probability of winning EXACTLY 11 games. (Could have been 10, 12, or any of the other numbers). If you want the probability that you win AT LEAST 11 games, you can sum up the first eleven members of the vector above that we produced or you can run this command: `pbinom(11,20,.5)` That would be about 75%, does that seem better to you? RBinorm gives a random number of wins. Try running `?qnorm` for a qnorm explanation
Thank you
Don't forget x=0. And I'm not sure why you try sapply instead of just using dbinom directly: dbinom(0:20, 20, .5) 
I would highly recommend. It has an open source community edition which has a full suite of features. And in addition to all the preprocessing nodes it has, it also has a very fully baked 'data science' set of nodes-some natively implemented, some community implemented, and quite a few dependent on Weka implementations. I believe it's worth a fair look. Cheers! 
Thank you for your input also one more question: If I want to see the probability of winning more than 11 games is this the correct code? pbinom(q=11, size=20, prob=1/2, lower.tail=T)
i wasn't sure each test in binomial distribution was independent of each other that's ultimately why I thought the probability was high because i subconsciously sort of believed each test was not independent idk why. Actually I know why because i never learned probability 
i understand I can substract from 1 without using the 'lower.tail' command but I was just wondering if I properly coded it with the 'lower.tail' method? Sorry I'm new to R
Doesn't quite answer your question, but you may be interested in quantmod - it pulls stock data and returns it as a time series. (By default it is xts, but you can make it be a time-series class of you want. library(quantmod) getSymbols('AAPL',src='yahoo', from = "1980-12-12", return.class="ts") The above will return an mts. If you just one one of those series, you can do something like: AAPLts &lt;- AAPL[,1] By default, when you use getSymbols, quantmod will create a variable named whatever the symbol name is. You can set auto.assign to equal FALSE if you do not want that behavior.
Wow that's brilliant, thanks!!
I'm just having a hard time seeing any way to do that any more quickly. Slick man. 
Try the [lahman](https://cran.r-project.org/web/packages/Lahman/index.html) package for MLB. There is also [a book about it](http://www.amazon.com/Analyzing-Baseball-Data-Chapman-Series/dp/1466570229).
It worked, thanks.
Hmm, maybe, although I'm not sure I totally understand. Maybe with a concrete example it'll be a little clearer. Say I have two datasets of restaurants. Each dataset has 4 columns, "restaurant name", "street number", "street name", "phone number". I want to figure out, for each row in data1, what is the corresponding row in data2, by looking at all of the columns. So, for example, the way I think I'll do it if I can't figure out anything better is to look at row 1 of data1. Make a new table with column 1 as the distance between data1[1,name] and data2[,name], column 2 as the distance between data1[1,streetnumber] and data2[,streetnumber] etc. Then, make a new column with the sum of all the other columns, and take the row that minimizes that that column, and call that the match for data1[1,]. Repeating that process for all the rows gives each row in data1 a matching row in data2. Does that make sense? Maybe your proposed solution already solves that in which case let me know :). Also, amatch: http://www.inside-r.org/packages/cran/stringdist/docs/amatch
My solution is only for 1 column in each. If you really want to compare the name, street number, name, phone #, then calculate the edit distance as the sum of the edit distances for each column from row 1 of the data frame to every row of DF2, and select the lowest result. You just need to tweak my function a little more to do it. I don't store intermediate data unless I need it. You're looking for the index of the minimum difference in edit distance space between two elements that each have 4 features. Just calculate it...
Smells like homework to me. You need to subset the data frame using, say `data[sex=="female","age"]`(you could [very easily have googled this part](http://lmgtfy.com/?q=r+filter+a+data+frame&amp;l=1)) then run whatever test you want to run (you didn't specify the test so you're on your own there) on the results.
Its because i have a dataset of indicidence of specific diseases. V1 is the herd, V2 is the date, V3 is the disease type. What i want to find out is how many percent within the herd got treatment for the specific disease at that time.
How large is the data set and how varied is the data? This could work, but would be horribly slow. reddit &lt;- read.csv(paste("~/R/reddit/","import.csv",sep=""), header = TRUE, sep = ",", quote = "\"",stringsAsFactors = FALSE, encoding="UTF-8") names(reddit) &lt;- c("herd","date","disease") output &lt;- matrix(, nrow = 0, ncol = 4) for(herd in unique(reddit$herd)) { for(date in unique(reddit$date)) { for(disease in unique(reddit$disease)) { subset &lt;- reddit[which(reddit$herd==herd &amp; reddit$date==date),] output &lt;- rbind(output,c(herd,date,disease,nrow(subset[which(subset$disease==disease),])/nrow(subset))) } } }
i am getting this error: cannot modify grouping variable
I have approximately 10k observations. Started your script, since i dont bother waiting if it works
Thanks! it worked
This is an aes change isn't it?
Your `Revenue` column hasn't been loaded as a number, it's a factor. Use `stringsAsFactors=FALSE` when you read your data.
Holy shit ggThemeAssist looks amazing. Simple, but a huge time saver. 
Your data file probably has missing values that are causing problems. You might want to clean it first by replacing or removing NAs. The `as.numeric` call won't do what you're expecting-it gives you the raw factors [rather than converting the label to a number](http://stackoverflow.com/questions/3418128/how-to-convert-a-factor-to-an-integer-numeric-without-a-loss-of-information). 
Yeah that looks pretty awesome. Can't wait to try it. 
One addendum to this that I wanted to share, since I've just been learning about it: purrr is quite useful for some of the things we've discussed. &gt; purrr::quietly(log)(-1) $result [1] NaN $output [1] "" $warnings [1] "NaNs produced" $messages character(0) &gt; purrr::safely(log)("a") $result NULL $error &lt;simpleError in .f(...): non-numeric argument to mathematical function&gt;
You are to vague for anyone to provide you any real assistance.
This helped me when I first started, https://learnxinyminutes.com/docs/r/
Thank you guys, you really helped me! I checked out that website and it is great! And I will do what Semantix said :) Thank you!
Worked like a charm. Thank you so much. This community, though small, has been super helpful. edit: was going to use coord_flip, but thankfully read that ggstance link first
The way you display the data will depend on how you interpret those descriptions. If you create categories for the descriptions based on severity or duration or some other ordinal way then a nice plot is possible. If you have an idea how you want to categorize the descriptions, then I can provide some suggestions. 
Functions don't modify their arguments (except in special cases). You're calling View on the original untransposed version. 
I would recommend giving it a try. But if you want to modify the dataframe then you just need to save it to an object. You can save it to the same name if you want weekend_8 &lt;- t(weekend_8)
It worked!! Thank you!!!
just as a note, if you are transposing it, it is likely a matrix (at least afterwords), since a data.frame's structure is columns that each can have a different class (while matrix is all the same class)
Yeah once Asthma is grouped into something more meaningful, I imagine a mosaic plot (or a matrix of mosaic plots) would be ideal https://learnr.wordpress.com/2009/03/29/ggplot2_marimekko_mosaic_chart/ 
You can get the output of the console if that is what you are asking. https://stat.ethz.ch/R-manual/R-devel/library/base/html/system.html
I meant something like some_statistic &lt;- simulation-script.R(parameter, otherparameter) Basically like function, but instead of a function inside of the main script, you're calling an external script.
Out of the languages I've worked with (Matlab mainly, but also C/C++, Python and Java), R definitively seems to be the quirkiest so far. I'm motivated to find out what it has to offer, though, since statistics start to play a bigger role in my daily work.
I can't see the benefit of this over the common boxplot + jitter or violin + IQR. Especially since it's a piece of cake to overlay the geoms in ggplot.
This is a good point... Why should we sail the seven seas (of data viz) with pirate plots, when jitter + box plots do just that? Really, what I'm interested in knowing is why we should keep the bar chart at all?
you are a lifesaver. Thank you!
Use the debugger to step through the code of sqlSave. You'll be able to find the point where it's dropping the spaces and amend the code so that it doesn't. This may also be a feature of your ODBC connection though, as RODBC gets some of its information from the connection. I'll be interested to know how you get on, because my forays into RODBC's innards haven't been too pleasant. It seems like pretty poor package to me (why do errors in sqlQuery cause it to return a character vector by default, and with the optional errors setting, return -1, rather than just signalling an error condition!?) and its creator hasn't been responsive to emails.
so you want a variable coded 0 or 1 based on whether it's a certain brand of OJ? create a new variable with the ifelse() function, something like: data$oj_dummy &amp;lt;- ifelse(data$ojbrand=="whatever", 1,0) but the way you worded your question makes is seem like you're just looking to subset and then perform regression. not the same as a dummy variable. 
I'm not at my computer to write up a potential code snippet but I would do something like this. - Create a vector where you randomly rearrange the numbers 1:n where n is the total number of entries in the matrix. You can do this by creating the vector 1:n and then sampling n values from it, without replacement. - Create a vector that is a random sample, with replacement, of n actual values from the matrix. Then replace the item in location 1 of the first vector with value 1 from the second vector, etc. with items 2 through n. You can probably do this without having to create an explicit for loop. Because you have a copy of the original matrix and of both of these vectors you can easily recreate any intermediate stage k of the randomized process by only using elements 1:k from those vectors. This approach has the benefit of processing the elements in a random order as you desired, while also guaranteeing that each one of them gets processed. It would be easiest if you just leave the matrix as a vector up until you're finished doing the randomization process, and then shape it into a matrix. That will simplify the calculations a lot.
From what you said in other comments, I would first create categories for Asthma. I would also make a percent jogger stat. When is comes to visualization I'm a big fan of keeping it simple. I would just use several dodged bar charts. How you dodge and divide the charts is up to you and how the Asthma categories works out. Something like: ggplot(df, aes(x=Gender/Asthma, y=PercJogger)) + geom_bar(position = "dodge") + facet_grid(. ~ Gender/Asthma)
I mostly run RStudio on my home desktop and login through the web. Working on an alternative web based platform (also based on the ACE editor though), but it is a bit out still. 
I think this is what I'm looking for, I'll test it out a bit later. Many thanks!
A colleague of mine sets up an AWS instance of Rstudio server whenever we need to tackle something big and computationally intensive. I personally have yet to hit anything that my macbook couldn't handle but that is what I would do if my laptop wasn't up to the task for something.
I have an iMac 5k Retina 4GHz quad-core i7 32GB DDR3L RAM 3TB Fusion Drive AMD Radeon R9 M395X w/4GB video memory I use VMWare Fusion for virtualization, and have a PostgreSQL Database setup on virtualized CentOS that I use for data storage. 
You have a list of dataframes, you could try: sapply(names(mydata), function(x) write.table(mydata[[x]], file = paste(x, "csv", sep = "."))) This should save the files in your working directory, though I'm not sure how it will handle the nested list you have as the second element in your list.
Try using unlist() to flatten it?
I just saw the imgur link. Try what _Wintermute suggested. I'm not sure how the data is organized. It looks like a database with linked tables, which I've never manipulated in R. You could try exporting each table separately: write.csv(mydata$bric, file="mydatabric.csv")
You hav four dataframes. The object is a list of two, when the first item is another list of one and it has one table, and the second item is a list of three, each with a table. You could export them to CSV by addressing each sublist one by one with [[]], like export.table(data[[1]][[1]], file="bric.csv") export.table(data[[2]][[1]], file="russia1.csv") export.table(data[[2]][[2]], file="russia2.csv") export.table(data[[2]][[3]], file="russia3.csv") ...I think 
Change your ylims to c(-250, 250) and c(-6,6) ?
Hi, I finally got around to testing out that bit of code - but figured it isn't quite what I'm looking for so was wondering if you'd have any idea about the following: So what I need to do is replace a totally random part of the matrix (without selecting it specifically, i.e. not specifying it to be bob.matrix[2,3], but having it almost as a sample of the bob.matrix, which is then replaced). This is where I'm having most of my trouble - selecting something random, as with sample, but then replacing what I've just selected, as opposed to just recalling that information ([2,3] = "e" or whatever). The next issue I seem to be having is that doing bobmatrix.v2[2,3] &lt;- sample(bob.a) doesn't replace with a letter from bob.a (a,b,c,d,e) but a number, I think related to the position in the vector (5 or 4 or whatever instead of e/d). Any ideas? Seemed so simple in my head when I started it...
Making the assumptions that: 1) You can load the data into a dataframe df 2) the columns have headers &gt;dfsub &lt;- subset(df, select=c("column2", "column5","column8")) &gt;write.csv(dfsub, file = "dfsub.csv",row.names=FALSE) 
Replace c("columnA","columnB") with c(1, 3, 5)
facepalm thank you rookie errors :) 
Can you post the script?
&gt; Seemed so simple in my head when I started it... because it mostly is bobmatrix.v2[sample(length(bobmatrix.v2)) &lt;- sample(bob.a) the second issue is that bob.a is a factor (integers that have assigned names), while bobmatrix.v2 is a character matrix make them use the same class...
well any of the more args could have changed and since I don't know the Fragman package, some internal states could also change.
thanks for the response, the ...more args, are the same for every command (I can post the full thing if you think it might help to pinpoint the issue though). Would any change of internal state not be picked up by the: identical(before, after) Or does it not work like that? 
Yeah I got it in the end, possibly via some help from stack overflow too, the bit I was missing in by the time I'd had a play was defining the length as the whole matrix. Thanks very much for the help!
Thank you, that was what I needed.
Where's the script?
Take my upvote. Will surprise my bosses with this one eventually.
What /u/_Wintermute said... some packages for python and other languages have the best support in a linux/unix environment, and that is what Mac's OSX is built on.
Well I know what I'm going for the rest of the night
I have not done text mining myself, but you might want to check out the TM package https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf Also, I sort of feel like you might be able to do what you want using a Bayesian analysis with the Sentiment package. Here is an intro for that. http://www.r-bloggers.com/intro-to-text-analysis-with-r/ Hopefully something in those can help you.
How did you only enter the .txt name and not the entire URL. My computer didn't download the file directly.
Looks about right to me, depending on what exactly you want. For the line and regression output related stuff, look a little more closely at the objects "mylm" and "summary(mylm)" using the str function and bracket indexing. Those will help you pull out the parts of the results you want, such as the significance level, betas, etc.
To add this this, check out the [broom](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) package. It's very good at extracting data from models into a sensible dataframe. You can also use it with dplyr if you're doing multiple models with `group_by` etc.
I'm seeing p-values... # Call: # lm(formula = Distance ~ Hang + R_Strength + L_Strength + R_Flexibility + # L_Flexibility + O_Strength) # # Residuals: # Min 1Q Median 3Q Max # -17.5529153 -9.1989060 -0.0663009 6.4325479 20.0259748 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -31.26258727 73.04680402 -0.42798 0.68360 # Hang 2.60809195 27.19209255 0.09591 0.92671 # R_Strength 0.27588886 0.49348347 0.55906 0.59635 # L_Strength 0.03802752 0.61793233 0.06154 0.95293 # R_Flexibility 1.24223416 1.56421785 0.79416 0.45736 # L_Flexibility -0.41338711 0.82547970 -0.50078 0.63436 # O_Strength 0.21354313 0.17616188 1.21220 0.27100 # # Residual standard error: 15.8115 on 6 degrees of freedom # Multiple R-squared: 0.8146591, Adjusted R-squared: 0.6293182 # F-statistic: 4.395462 on 6 and 6 DF, p-value: 0.04727903` 
Has she redefined the `c` function? (Did you check what was in her `a` and `b`?) 
Thanks guys, it was helpful. Especially article provided by Cosi1125. 
Thanks. Tried this and it doesn't change anything--both work for me, neither for her.
Could it be a namespace issue? have you tried: graphics::plot(a~b) 
Haven't tried this. Will have her give it a shot and report back. Thanks!
The c function is used in constructing a and b so those later lines would be beside the point. But if you've checked `a` and `b` (i.e. compare `str(a)` and `str(b)` on both machines) then that's not the issue. One possibility is that (somehow) `plot.default` or one of the functions it calls -- perhaps `unique` as mentioned in the error message -- has been redefined at some point. 
There are packages that can make use of multiple cores: https://www.rforge.net/doc/packages/multicore/mclapply.html Adding cores doesn't seem to have any diminishing returns (as long as you have enough memory for every forked process).
Thanks. So far I've got ggplot(data, aes(x, y)) + geom_density_2d() I've managed to generate a plot, but I can't figure out how to use colours instead of just lines, and I need to create a legend which gives what colours correspond to which values of z. Sorry, my R isn't great. Tried to use the following argument: ggplot(data, aes(x, y)) + geom_raster(aes(fill = density)) + geom_contour(colour = "white") But got the following error: "Don't know how to automatically pick scale for object of type function. Defaulting to continuous. Error in data.frame(fill = function (x, ...) : arguments imply differing number of rows: 0, 104" Btw, I can't express z as a function of x and y. 
I think the problem might be that you're using density as a variable name (for fill) since density is a function. Try changing it to another name and see if it works. You don't have to express z as a function of x and y in this case.
I've found that R is actually very easy to parallelize, given the right circumstances. Are you able to provide some more details about what you'll be doing?
Sure, please post it or send it to me in a message.
Because in 99% of the cases it would make stuff slower. A lot of overhead is needed to split the data to the different cores and then join it again. Edit: also not all vectorized functions can be split at all trivially, think stuff like cumsum() Note vectorized functions are things like '+', not apply
PM'd
omg NO. R is barely usable for shiny R is a really powerful EDA, basic modeling, and data analysis language but it's absolutely HORRIFIC at anything beyond that.
I understand where you're coming from, but I do think Shiny added a LOT of value to R. It shouldn't be used as a replacement for web technologies to build web apps, but it does have many helpful utilities. For example, I recently created a package to help analyze some biological data. The first iteration involved programming only, and soon enough I realized that any scientist that isn't comfortable with R just won't be able to use it. By creating a shiny app that is simply a point-n-click interface to the R code, it makes it a lot more accessible to a lot more people
I completely agree with you. I have developed several shiny apps for sharing my results and models with scientists, engineers, and even our sales department. But it's *excruciating* to work with as someone who has a background in UX with modern web application frameworks. Just as an example...try and add a vertical scroll bar to one of your components such that it dynamically adds the scroll bar if your shiny panel or dashboard box gets bigger. It's a *nightmare*. And don't get me started on the whole "lets just share input and output as a global list accessible everywhere". It's easy and it works, but is completely garbage for any sufficiently complicated problem. I still use R and shiny in my work daily - but unless R adopts some real OO paradigms and frameworks, or unless we can invent some sort of native web application framework, then R should stop at the edge of the statistician's desk.
I didn't explicitly say it, but I was trying to say the same thing as you. I just don't agree that we should try to write production code with R. If you want to write web apps, use python and flask (or better yet, Java and Spring). R lacks the OO framework to be a "real" language in the web application framework world.
http://pastebin.com/ the script?
Shiny is great for analysts to manipulate data visually, and it allows us to get quickly off the ground. And on those 2 criteria alone I find it way more flexible than Python + Flask. 
Just like EVERYTHING related to R, Shiny was designed to work and be practical (that's how I feel at least). I'm actually quite impressed with how much Shiny can do and how convenient it is, I find it quite elegant to be honest. If you have some very big-picture ideas that could be improved, I'm sure Joe Cheng would be happy to hear about it, he's very open minded and open to criticism/suggestions about shiny
sure, that's totally fair. The issues with shiny is that it's not a native UI, it's a wrapper around javascript. So in order for it to be approachable, and (as you put it) elegant and convenient, it has to be stripped down and black-boxed. So if you're sharing a few plots and want to give your audience a UI to adjust your hist binning or poly fitting - it works great! But it completely falls apart as a web application framework. It's really not a tool for dynamic UIs. My point is: R and shiny are great at what they do, they're just not the be-all and and-all of programming languages and web application frameworks. Shiny has some real fundamental problems (like accessing events via input/output variables) that will prevent it from being a serious production web application framework. (And I don't think that chatting with Joe would solve that)
I agree. Shiny is good for what it is, but hopefully it stays in the space it's in now and doesn't try to become a full fledged Web app stack With MS acquiring revolution, I think we'll start getting better VS support and integration with the .NET stack. You could have the Web devs use .net as the Web framework, and have r integration within the app 
I will post in here once I get a couple projects I have been working on into the wild. I connect directly to the websocket and keep most of my logic on the javascript side making it easily extendable and also have integrated it with Nodejs. 
Your plot is two dimensional: * x-axis: OBP + SLG * y-axis: Runs_Scored Your model is three dimensional: * Predictors: * OBP * SLG Your confusion is a consequence of R's [formula syntax](http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf) for model specification. If I understand what you are trying to do correctly, this is what you want: df = data.frame(x=OBP+SLG, y=Runs_Scored) mylm = lm(y~x, df)
For me R is about visualisation and exceptions *first*. The way I work now is to use plots and curves to tell me how clear the approach is and how pure the data is. With a spreadsheet or database it is back to front. The interface, tables and the formula-query come before the discovery.
R can help you pull information and trends out of data in an easy to interpret manner. Going to be hard to use if you do not have basic stats knowledge though.
Well, when I said "like SQL" I was more taking into account the idea of using a higher-level language to describe a very complex action. Joining data.frames is hard, and you're right that full SQL probably has more capabilities for it, however here is why I don't want to be doing this in a SQL environment: * the target platform is a browser. Although I suppose it's not entirely dissimilar, sending output through a PHP POST (by way of `write.csv()` in R and then `fgetcsv()` in PHP) is slower but simpler than creating a PDO and executing the query natively. * I can sand-box R much easier than SQL. * R can load arbitrary files into memory much easier (again, not necessarily faster, just easier) than SQL * SQL has multiple dialects. Shall we support MySQL [link](http://stackoverflow.com/questions/3709560/mysql-join-three-tables) [link](http://stackoverflow.com/questions/10257433/mysql-query-to-join-three-tables) or SQL Server and Transact-SQL [link](http://stackoverflow.com/questions/15987321/joining-multiple-tables-in-sql) [link](http://stackoverflow.com/questions/8758223/sql-server-multiple-table-joins-with-a-where-clause) [link](https://technet.microsoft.com/en-us/library/ms191430\(v=sql.105\).aspx)? * this isn't 'just' a SQL problem. Although that's my most pressing use-case, I have other uses for such a function as this. It would be versatile, as I mentioned, with two named examples being able to both compute SQL joins as well as power towers. Isn't that interesting enough, to warrant investigation?
What you want to do, is basically writing a wrapper for the `Reduce()` function: foo = function(op, ...) Reduce(op, list(...)) &gt; foo(`+`, 1, 2, 3, 4) [1] 10 &gt; foo(function(a, b) a ^ b, 2, 3, 4) [1] 4096 My question is: Why aren't you supposed to do it in R? ;-)
Right, I specified that wrong--I meant that even if she creates an lm() with an existing dataset she still gets the error. /u/joe_gdit's suggestion below worked out...does that suggest that plot.default has been redefined?
Yes, or something it calls -- probably by some package that's been loaded, but possibly also at the command line (it's easy enough to do)
Thanks for your suggestion! &gt; My question is: Why aren't you supposed to do it in R? ;-) Because I was thinking I was going to heavily abuse dots (...) in a way you're not supposed to. Dots "shares" all arguments down to future calls. I was planning on only sharing a subset of the arguments. Your `(2 ^ 3) ^ 4` example is encouraging. Is it mentioned in the documentation that Reduce scans left-to-right? I wouldn't want `2 ^ (3 ^ 4)` or any other pairwise combination, such as `(2 ^ 4) ^ 3`, etc. edit: making it clear I really appreciate the help 
I've already got the list version sort of set up, but I don't want to use a function that is `item &amp; [list of items] -&gt; item`. I want to make the API as clean as possible, conforming to a `item &amp; item &amp; ... &amp; item -&gt; item` call. As in, sum(1, 2, 3, 4, 5) vs. sum(1, c(2,3,4,5)) The first is significantly "cleaner".
Solid. Thanks!
Yeah, I don't think that is going to work, as someone else pointed out, I think it is an issue with concurrent shiny sessions. I don't know exactly *why* you're trying to load a shiny app from another shiny app, but you'd probably have better luck integrating the two apps into one or getting a server where two concurrent shiny sessions are allowed.
Hey, this code is almost Pythonic! ;-) A few functions specific to R: `rep(x, n)` - yields a vector of length n filled with x (a bit like `str(x) * n` in Python, but the result is a vector). `i:j` - like `range(i, j + 1)` in Python (note that R's indexing is 1-based!). `paste()` - the example provided in your code is awful, but basically it's like `a + b + c` in Python, where a, b and c are strings (string concatenation). `table()`- if applied to a vector, returns counts of its elements (in your example TPvector1T probably contains percentages of occurrences of elements in TPvector1). `unlist()` - converts a list to a vector (used because `strsplit()` returns a list). `strsplit(string, sep)` - like Python's `string.split(sep)`. `names(v)` - elements of vectors can be labeled; since TPvector1T, -2T etc. are tables of occurrences of elements from TPvector1, -2 etc., `names()` returns a list of these elements. 
Alright, thanks a lot!
http://mathesaurus.sourceforge.net/r-numpy.html
Cook distance is used to check for influential points. Check on wikipedia about cook distance. https://en.wikipedia.org/wiki/Cook's_distance I think it is pretty helpful to evade cases like the 4 case of the anscombe quartet https://en.wikipedia.org/wiki/Anscombe's_quartet Its a pretty good test to see what happens to these plots, make a linear regression, and explore these plots for each case of anscombe quartet 
I like the idea of including a shiny app in your package (just conceptually, I have never authored a package)
I'll start by creating a dataset to work with: # Make dataset events &lt;- data.frame(year = c(rep(1970, 3), rep(1971, 4), rep(1972, 2)), continent = c("Africa", "Africa", "Asia", "Europe", "Asia", "Asia", "Asia", "North America", "Europe"), event = paste0("Event_", 1:9)) It's usually considered good form to ask questions and provide a bit of code to create a sample dataset. Not a huge deal (since you at least provided what the dataset looks like), but something to know as you continue to learn R. :) table(year, continent) would do the mathematical operation you want, but that will be in "wide form": &gt; table(events$year, events$continent) Africa Asia Europe North America 1970 2 1 0 0 1971 0 3 1 0 1972 0 0 1 1 To get it into the form you want, I'd probably use `dplyr`, so let's skip that and just use the `dplyr` package from the beginning: # load libraries library(dplyr) # Useful for summarizing and manipulating datasets events %&gt;% group_by(year, continent) %&gt;% # split the dataset up by year and continent summarize(events = length(unique(event))) %&gt;% # count unique events ungroup() # Remove remaining grouping variable: year Which produces: &gt; events %&gt;% + group_by(year, continent) %&gt;% # split the dataset up by year and continent + summarize(events = length(unique(event))) # count unique events + ungroup() # Remove remaining grouping variable, year Source: local data frame [6 x 3] year continent events (dbl) (chr) (int) 1 1970 Africa 2 2 1970 Asia 1 3 1971 Asia 3 4 1971 Europe 1 5 1972 Europe 1 6 1972 North America 1
If I'm understanding how your data seems to be, you just need to run a one way ANOVA. All you would need to do is arrange the data into two columns. The first column should be categories of smoke and the second should be the associated age of death to the correlating first column category. Like this: Column 1: nonsmoke nonsmoke, etc. Column 2: 87, 98, 67, etc. do this for each category and data pair you have Then run your ANOVA in r. Here is a great tutorial on how to run a 1 way ANOVA and its assumptions and whatnot: http://www.stat.columbia.edu/~martin/W2024/R3.pdf
thanks! also, what does %&gt;% mean?
Thank you so much!
What do you mean by "two-tail" when it comes to ANOVA with more than two groups? Any kind of deviations from all means equal lead to large values of the test statistic. Are you looking for some kind of ordered alternative?
group_by() creates smaller data frames, each with one level of any of the variables you pass in. ungroup() basically undoes this grouping, since not all functions in R can handle grouped data frames. In this example it doesn't do that much, but you can end up with very odd errors if you tried to use a grouped data frame somewhere else - that's what I was trying to prevent. 
That’s *multiprocess* computing though, not multi-thread. It uses the POSIX `fork` command for that, which has its own advantages and problems. In particular, as far as I know `mclapply` only uses a single core on Windows (because Windows has no `fork` syscall), no matter the actual number of cores. Furthermore, memory-intensive computations may fail with this kind of parallelism (even though forking a new process doesn’t copy memory until actually necessary).
What does "event" refer to in length(unique(event))?
wait, then this will only count unique events, which isn't what I want. I want a count of events. If two cases have the same event in them (in my case, maybe 1970 Europe had 2 bombings, and so the event column will have "bombing" for two entries), the code will ignore one right? so it will undercount by one. I should just remove the unique nesting right?
I think they're just used to talking about "two tailed t-test" and don't realize that ANOVA is typically a one-tailed test.
For it to be *two way* you'd need a second categorical variable, beyond the smoking category. Since you only mention one, I doubt it's that.
There's also a CRAN Instant Answer in the making, but I'm too busy to work on that myself. Feel very free to pick up the ball and finish it. The stuff that remains to be fixed is not that hard. https://duck.co/ia/view/cran https://github.com/duckduckgo/zeroclickinfo-spice/pull/2644
You should post this in /r/rstats - a more active subreddit for R.
Thanks for the recommendation, I'll crosspost it there, too!
It's a pipe from the magrittr and dplyr packages. These two are equivalent. "2015-01-01" %&gt;% as.Date(format = "%Y-%m-%d") as.Date("2015-01-01", format = "%Y-%m-%d") While you're at it you should look into some of the other pipes in magrittr. This is my favourite; these are equivalent. some_var %&lt;&gt;% as.Date() some_var &lt;- as.Date(some_var) I like this because the less repetition I have, the less likely I am to mess up changing it later.
Thanks very much Rlangthrow!
When the open-source software in question is as solid and mature as R, I would need serious incentive to start using some proprietary implementation/alternative. Even if I take their arguments at face value, they don't really cut the mustard for me.
I am a little worried about that, but I was already a curmudgeon about it on /u/mulderc's other post.
I'm ready to dismiss it because I have not seen anything negative from Microsofts embrace of R and trust the core R team. If MS does something then I will reassess but right now I see no reason to have any negative feelings about MS and their support of R. 
I guess I am confused because I haven't specified any columns. I just told R to do the entire data set. 
That doesn't mean that the function you're calling isn't calling [. train() or something further down the chain will be calling [ somewhere. You can use the debugger to step inside the function and see what's going wrong.
This guy's blog post has great info to get you started on decision trees: http://trevorstephens.com/post/72923766261/titanic-getting-started-with-r-part-3-decision
Check out [Orange Data Mining](http://orange.biolab.si). While it isn't necessarily an R Language tool, it is certainly worth having in your data science toolkit. Specifically relevant is the interactive decision tred visualization widget.
You can run all tests from the command line with `R -e "devtools::test()"`. There are also options in RStudio projects to run tests after each save, and you can always set up a github repo and use travis.
Why would I download an app and give away my personal details when there are already 20 other established services that do the same thing (and will probably still exist in 6 months time)?
I have never experienced that issue when upgrading in the past. Everything on CRAN should be fine.
Maybe its because I use RStudio but there are a lot of packages that weren't compatible with the latest R version. 
You are absolutely right. However, sometimes, one wants a message to persist in the server so it can be followed up later. That is not always possible in IRC. The group is intended to be a substitution of IRC at freenode. Everyone is free **not** to join, but for those that already uses the platform it might, perhaps, be useful.
Time to take a deep breath, upgrade, and pray that packrat will work.
TIL; IRC is still used. Man does that take me back 
\#R
Thank goodness for the logo update. 
Did you try xmlToDataFrame()
I would be interested in setting one up or invigorating the existing one.
It was somewhat active the last time I was on it. If you think that channel is quiet, then you haven't been around much on other IRCs then. I think some have gone to use more modern or newer avenues like gitter or public slack channels. I havent used them so cant say they are better or not.
Oher IRC channels I use are much more active and other programming IRC channels I have looked at appeared to be more active. 
Update: looks like the site clock is GMT, so the deal has now expired :(
Looks like it's expired, but still the 5th here!
Ok, since you are a newbie to R I highly suggest using Rstudio instead of an interactive session in Cygwin, you get the documentation inside Rstudio and it is continuously being developed. It also offers a lot of features, which are difficult to get in Cygwin. Also, if you like emacs, then ESS (emacs speaks statistics) is a great tool.
Don't use Cygwin. If you want to Linux from Windows do it in a VM instead. Yes, I just made Linux a verb.
I think that what you need is to replace data parameter with data=subset(d,d$year!=2008), that should work.
if `year` is a separate variable (not a column of `d`): reg = lm(y~x+z,data=d[year!=2008,]) Alternatively if `year` is a column of `d`: reg = lm(y~x+z,data=d[d$year!=2008,]) 
Or with data.table: data = d[year != "2008"]
When I deal with this in I always append an "A" to the start of the number. It means the whole cell is always treated as text no matter what. When I start working on the column again I strip out the leading A's. I've had issues with every other approach as its too easy for a simple opening and closing of the CSV to mess up the data.
1. Check and make sure it's actually saving w/out leading 0s. Excel has a habit of stripping them on open. 2. Convert your column class to 'character' as mentioned before 3. Use sprintf to pad 0s if they're not reading as padded. 
I did not get the last bit after you call the expression with D for "x". I think I'm missing out on something very simple. Please explain. 
Oh cool. Can I plug in values of x (lets say, a vector/list) into that derived function and get back values of y'?
Yes, you can use the function "deriv" to do that. See "?deriv".
Okay, I'll check that.
R can do some basic differentiation (see `?D` which has the help for several functions related to symbolic differentiation -- for a simple class of expressions) but it is *not* a computer algebra system (though there may be some add in packages that cover what you need). There are other programs - including free ones - that probably do what you want. 
What format is the month name in? Number or Abbreviated? (01/02/03 v Jan/Feb/Mar)
I'd use sqldf `library(sqldf) df &lt;- read.csv.sql(file, sql = "select 'PM10' where 'Month' &gt;= 5 and 'Month' &lt;= 9", header = TRUE)` http://www.inside-r.org/packages/cran/sqldf/docs/read.csv.sql The format of your dates will affect the SQL but the sqldf package should be able to help, just format the query to match your data.
Hi, can we talk?
Thank you, I'll make sure to let you know how it went!
A histogram+summary stats is the best option. It will give you the distribution of downloads. Use the following functions on the numeric vector: [hist()](https://www.datacamp.com/community/tutorials/make-histogram-basic-r); summary()
Check out the ReporteRs package. I am using it for powerpoint generation / manipulation, but it has word functionality as well. What may work is creating a template doc with your list and then writing a new doc based on the template and then combining that with the other doc.
You've heard of this newfangled thing called Google, right? http://3.bp.blogspot.com/-XZe7sFinWVQ/UWQaH6l6MHI/AAAAAAAAHUk/pRD0MxvRtB4/s1600/npw12.1.violinplot.2013-04-09.png
I am the author of qdap. I would not use qdap here as it was designed more so for text scraping (usually we don't care about preserving exact doc structure, such as list numbers, in text mining). pandoc is designed to do exactly what you want (docx to html conversion). You'd need to install pandoc if you already do not have it but install is easy. If you want to do this within R the following command works: system('pandoc -f docx -t html -o test.html test.docx') 
Nothing. Just one box. 
Seems like you have very highly skewed data. You may want to log transform it and then plot it out. 
"Good" is funny. What you see is this: http://www.r-bloggers.com/good-r-packages/
Thanks a lot for the explanation, really useful.
The same problem here as well. But with Linux Mint 17 Rosa, which is based on Ubuntu Trusty Tahr. Changing mirrors did not help at all. 
Does [this](http://askubuntu.com/questions/41605/trouble-downloading-packages-list-due-to-a-hash-sum-mismatch-error) solve the problem?
Might want to try to make a shiny app for this one.
I tried it, and it works! Thanks!
Good idea. Added to original post.
Ok. I will try to take a look either later this evening or tomorrow. I just recently managed to read tables out of word docs and import them into R, so I'm at least a bit familiar with Word's perversion of XML :).
Anyone ever actually go through any of these courses? Curious about the quality. 
The only one I've taken was "Automate the Boring Stuff with Python," which was quite good. I have previewed quite a few others. The quality is incredibly inconsistent, since Udemy just offers a platform to individual content providers (not unlike YouTube).
If you look at their webpage yo have to do one by one downloading, they have a button that says, download to a spreadsheet.
I believe it's called quantmod, a package that connects directly to Yahoo finance data. Download and use that. You'll prob have to do your own code to deal with the name errors.
Eh... I should have said this beforehand: I apologize if this is repetitive, common knowledge or I'm being to vague. Obviously, I'm a total newb here. Give me the word and I'll scrap the post. Thanks in advance for any help.
Thanks for your responses. It turns out I was being a moron. Definitely should have posted my function at least. My input was just a dataframe containing a list of 100 positive numbers. My function was z = (mpg-mean(mpg))/sd(mpg) ..... which is exactly the default function of scale. Not only glaringly obvious that it must have been the scale command but I missed it in the ?scale help section. Coffee only does so much good. Sometimes it's best just to walk away for a bit. Sorry. The environment was a bit glitchy on several occasions so I was convinced R Studio was hanging onto something.
You're right. Scale was doing the same thing that my function was.
For my future reference, would it suffice to just summarize the input and output? ie. Input: list of 100 positive numbers; Output: list of standard deviation for those numbers; and the function? Is it standard practice to link a .txt of the .R and the .csv inputs in this forum? Just curious.
phenomenal. Thanks for sharing this.
This is exciting. I can't wait to try this out and see how soon I can finally ditch Tableau.
Well you can just do the math by hand, looking at the roc will also help in that regard. The basic is, that you are either predicting everything as 0 (TPR=0, FPR=0) or everything as 1 (TPR=1, FPR=1). And it is good that trivial predictions do not get a high AUC, otherwise the score would be less popular. So next time, before assuming a bug, maybe just check that you understand the basic math first.
You could read it in without headers, set the column names to the 2nd row, and then drop the first two rows. colnames(df) &lt;- as.character(df[1,]) df &lt;- df[-c(1,2),]
After finding out what a nested cross tab is, I thought, why not add 'R' to my google search, my second hit was: http://rstudio-pubs-static.s3.amazonaws.com/6975_c4943349b6174f448104a5513fed59a9.html It mentions several packages/functions to do tabularization and if I understood the examples correctly, provides a function that is capable of nested cross tabs. Otherwise I would search in packages that make tables for reports (e.g. rmarkdown, etc.), since to me that seems to be the most frequent use case for such a function
Nice use of a lookup table. I just read about indexing in that capacity. It's great to see it out in the wild. 
I once done something by using divs and a stylesheet to get it to do what I want. You could use radiobuttons for each year and then just do something like this: sidebarMenu( div(div(radioButtons(foryear2002), class = "SBS"), div(textInput(foryear2002), class = "SBS"), class = "sidebyside"), ... ) With the stylesheet containing something similar to this: .sidebyside { display: inline-table; width: 100%; vertical-align: top; text-align: justify; min-height: 75px; } .SBS{ overflow: hidden; width: 50%; display: table-cell; vertical-align: middle; align-items: center; justify-content: center; padding-left: 10px; padding-right: 10px; } You had to play with the stylesheet-classes to get the result you want (especially padding and min-height), but it worked well for me.
Column width was not an issue I think. the input box was not aligned so it was all out of whack. Rhandsontable looks really neat tho! Even if I wont use it for input control I think I will use it in general display of data :)
I don't know how to do that, but I know you can fill the cells with color, so I bet you can remove the borders -
I think you mean exists with one 'i'. Exists is base. Try ?exists
Given that I have thousands of CustomerIDs what is the best way to input them into the for loop without copy pasting all of them? I currently have them imported via csv: CustomerList.csv &lt;- read.csv("C:/Users/me/Desktop/CustomerList.csv",header = T)
I'm not sure about the first part. Does it change if you do lm(y~x, data= OrderedSales.csv[OrderedSales.csv$GroupID==custids[x],])$coeff? or conversely lm(OrderedSales.csv$y[OrderedSales.csv$GroupID==custids[x],]~OrderedSales.csv$x[OrderedSales.csv$GroupID==custids[x],])$coeff For seeing all coeff, just leave that as $coeff
Something like the following should work and will detect all of the unique customer IDs automatically: outlist &lt;- unlist(by(dataset, dataset$CustomerID, function(x){ lm(x[ , "Purchase Amount"] ~ x[ , "DateID"])$coeff[2] })) 
how about the following: for (i in c("question1", "question2", "question3")) { assign(paste0(i, "_copy"), eval(as.name(i))) } EDIT: if you have a lot of variables to loop through and don't have a way to loop through their names, you could use the following to generate a character vector of question variables: ls()[grep("question", ls())]
Thank you! This is so close to being exactly what I need. I actually wanted to preform a polynomial regression so I have modified the code as such: outlist &lt;- unlist(by(OrderedSalesAll.csv, OrderedSalesAll.csv$GroupID, function(x){ lm(x[ , "TotalDollars"] ~ x[ , "DateID"] + x[,"DateSquare"])$coeff })) --------------------------------- This is so close to being exactly what I need, but I'm trying to figure out how to export it to a CSV in the format of: customerID,Intercept,DateID,DateSquare customerID,Intercept,... ..... -------------------------------- I've gotten this far: write.csv(outlist,file ="C:/Users/me/Desktop/RegressionOutput.csv", append = FALSE, quote = FALSE,sep=",",eol="\r\n",na="",dec=".",row.names=TRUE,col.names=TRUE) But started getting several errors such as: http://i.imgur.com/0ag5f47.png And the results dont look great: http://imgur.com/a/Fb7do Any ideas where I'm going wrong? Thanks! 
Hard to say without getting my hands on it but try turning outlist into a dataframe before exporting it: outdf &lt;- data.frame(CustomerID = names(outlist), Intercept = as.numeric(outlist)) EDIT: You say you want to include "DateID" and "DateSquare" values in the output, but wouldn't there be multiple DateID and DateSquare values for each coefficient?
Or, simply: question_copy = question But yes, the fundamental solution here is “use vectors”.
Oh oops yeah that makes sense. Also the code I gave you is inaccurate. I said Intercept and I should have said Coefficient. I'll give you updated code later this evening.
thank you for the reply. The problem is that I have 18 question variables (i.e. question1, question2, question3, etc.) doesn't the code you posted just loop through the first 18 elements in the question vector and assign it to question_copy? I don't see how that's capturing all 18 question variables. 
The point is that you *shouldn't* have 18 numbered variables, that's madness. You should have one vector instead.
care to explain? I've only used SPSS/STATA, so all of my logic/reasoning is based on how to do things in those programs. I just installed R a few days ago! each one of the variables represents a question on a test (there were 18 questions on the test) and whether they got that question right or wrong (0 or 1). how would I create one vector that contains all that information? 
If you have one test result with 18 questions then you can use a vector. If you have many test takers then you use a data frame. In that case each column will hold the answer to a question. Data frames are like excel or SQL table. Edit- &gt; forvalues i =1/18{ &gt; gen question_copy`i' = question`i' &gt; } Correct me if I am wrong, but this says copy q1 to qcopy1, q2 to qcopy2, etc. You can do this by &gt; for (i in 1:18){ &gt; assign(paste0("questioncopy", i), get(paste0("question", i))) &gt; } But no one ever does this. Create a vector if you have single values or a data frame if you have a table.
I think you've stumbled upon the different terminology used between the programs. Variables in R are not variables in a dataset table, but variables used to store something or other, i.e., how you would talk about a variable in C or Python, or whatever. That seems to be the confusion, at least given your example Stata code above. You're want to create new column vectors that are copies in that data frame, you do not want variables. Anyway, that's scratching the surface because R and Stata are very different beasts indeed, and you're going to have to check in your Stata macro programming concepts at the door, unfortunately, because they are only going to hinder your understanding of R. But when you DO think variables in R, think locals in Stata. So, this is going to be an uphill battle, because I am assuming you are going to want to treat data frames as Stata datasets. In Stata, you can only have *one* table at a time, you have *no such* restriction in R. That means one way or another you're going to need to reference your dataset when you manipulate it. I'm assuming you have loaded your questions from a csv file into a dataframe q and your filename is questions.csv. Then, to create a copy of the current questions, in a relatively close equivalent to your Stata code... q &lt;- read.csv('questions.csv', stringsAsFactors=FALSE) for (i in 1:18) { q[paste0('question_copy', i)] &lt;- q[paste0('question', i)] } That will generate a question_copy1 : question_copy18 in data frame q. By the way, whereas the Stata regex functionality is frustrating as hell, R actually has real regex support so you could loop through all column vectors that contain 'question' relatively easily and then substitute question for question copy, thus freeing you from having to think how many questions you might have. Assuming your data frame is q: for (qst in grep('question', names(q), value = TRUE)) { q[gsub('(question)(\\d+)', '\\1_copy\\2', qst)] &lt;- q[qst] } That will create a copy of as many question# columns as you have into question_copy# column vectors.
I agree with you. The data is hierarchicaly clustered: level 1: time (dates), level 2: customer. You can perform this analysis with a program called HLM.
Glad I could help. Thanks for the gold :)
that, of course makes sense, I was to much into his example...
Sounds like you need to reshape the data. I'd recommend the [reshape2 package](https://cran.r-project.org/web/packages/WDI/WDI.pdf). Also, you can get most WB data directly from here: https://cran.r-project.org/web/packages/WDI/WDI.pdf 
I am not sure where the WB data lives, so it's kind of hard for me to exactly visualise what you mean. If you have the exact dataset in mind I'd probably be of more help, but what I think you're asking for sounds like an easy job for *dplyr* (specifically, summarising the mean via *summarise*) and *tidyr* (for getting the years into rows via *gather*).
As mentioned elsewhere, I would start with reshape2, specifically the melt function. The "long" to "wide" transformation is handled by the cast function, "wide" to "long" (like your data, it sounds like) is handled by the melt function.
Okay, so I tried this and it's just totally not working out for me I have no way to diagnose what's wrong. It's just giving me errors and NAs &gt; D(expression("4x^2"), "x") [1] NA The same if I try to find the value of the slope at a point: &gt; D(expression("4x^2"), "2") [1] NA 
I use `data.table` which may not be the answer you want. But as a `data.table` you would just write: `myData[ , newColumn := mean(Growth), by = Year]` In base R, I would do it by doing using `split`. I believe this works: `myData$newColumn &lt;- sapply(split(myData$Growth, myData$Year), FUN = mean, na.rm = TRUE)` P.s. Three points to help you in the future: First, use `dput` to output your data here so people can easily load and test methods on your data. Second, use better test data because there are only trivial cases in the example you provided. Third, your title has little to do with your question as there isn't any logical arguments in the question you ask.
That's because you must not feed a string to expression(). The following works: &gt; D(expression(x),"x") [1] 1 Also, R won't understand "4x". You'll have to write "4 * x", as in: &gt; D(expression(4 * x^2), "x") [1] 4 * (2 * x) 
That looks more like a random collection than a series of progressive, well-thought exercises.
Beggars can't be choosers
if you want a step by step instruction: swirl package if you want interesting exercises/common tasks (independent of language): rosetta code
Oh that's so weird. My understanding of atomic data structures is that s was a string, therefore I can put a string in in. That's really unintuitive, but thank you EDIT: I tried your original code, and turns out it doesn't work. You passed in s as a string, and that didnt do anything, which is where I think my confusion came from. It's really weird now that I see that expression has to take an undefined variable x, whereas every other programming language would say that is erroneous. What did you mean to say in your original post? 
This looks interesting. How would I recreate this for R? Can I just fork it and translate to R? Edit --- Started my own. https://github.com/luisd303/r100Exercises
&gt;Shiny by RStudio &gt;A web application framework for R &gt;Turn your analyses into interactive web applications &gt;No HTML, CSS, or JavaScript knowledge required http://shiny.rstudio.com/ That pretty much sums it up.
I think geom_raster is the one you want (from the link).
Could you please provide the link for the C++ repo you mention?
Will try that.
The melt() function from the reshape2 package is exactly what you're looking for. I've used it many times specifically on data from the world bank.
Do you use lm()? If so, the resulting model has it's residuals saved as lm &lt;- lm() residuals &lt;- lm$residuals so you can sum the SSR up yourself with sum(residuals^2) The total sum of square should be given using var(data) And therefore the explained sum of squares is var(data) - sum(residuals^2) But maybe has an easier formula to it.
[Do you have all of the dependencies?](https://cran.r-project.org/web/packages/Rcmdr/index.html)
You're fine, you'll just want to see what OSX dependencies there are. /u/mattindustries left a link with more information above.
I tried changing for the syntax you mention and an error occurs: lines.df&lt;-SpatialLinesDataFrame(lines,data=df) Error in SpatialLinesDataFrame(lines, data = df) : row.names of data and Lines IDs do not match
well is df a data.frame that has rownames that match the names of lines? That error is an API error that explains the problem.
The title is a reference to Game of Thrones "a girl has no name". Don't take it seriously 
yeah, that's what I thought too, but how can I solve this? Cause, in my input I have points (about 7) and I want to merge them in a line and export it in shapefile. somehow during the script something changes the number of items. Sorry, i'm a beginner 
With dplyr: join(q1, q2) %&gt;% group_by(channel) %&gt;% summarise(sessions=sum(sessions))
Dplyr is the way to go
The hadleyverse doesn't hugely change the way R works... it just simplifies it and imposes a bit more structure. You can do the same things with `apply`/`sapply`/`lapply`/`mapply`/`tapply` that you can with `dplyr`, but `dplyr` is going to be easier to learn from the get-go and (imho) easier to debug. This is doubly true with `ggplot2` - if you learn it early, you'll still be able to use base graphics and understand them, but you'll have a much wider range of things you can do early on. It'll also be easier to think about things in vectorized notation if you're actually plotting them that way. Hadley is also very good about enforcing uniform syntax (with the exception of arguments in `readr` vs. `readxl`, but that's a very small beef), so it's going to be easier to use `stringr` functions than it is going to be to use base functions for string manipulation. I'd say go ahead and learn the hadleyverse. It'll teach you good habits early on, and I don't think you'll have problems understanding other people's code later (you'll have to learn what the functions do, but that's always something you have to do when reading other people's code).
I work with both base R and the Hadleyverse. Hadleyverse packages are huge productivity boosters for day-to-day analytical tasks, but I do recommend learning base R. * You will need base R if you plan to use it as a programming language, rather than a Super Excel * Knowing base R will let you figure out what's going wrong when something in the Hadleyverse returns unexpected behavior (which will happen outside of obvious cases) The question for you is, how soon do you need to be productive using R? If you need it for work next week, start with the Hadleyverse. If you're learning on your own time, starting with base R will make learning the Hadleyverse easier.
Base R is great. The Hadleyverse is also great. Stuff you should absolutely focus on learning from base R: subsetting, joins, general data handling Stuff from Hadleyverse that I find are absolutely great and better than base R: plyr + doParallel, ggplot2, reshape2, stringr, and some parts of dplyr (the summarize function is great). The database interface packages like RSQLite are absolutely fantastic. Stuff I like to avoid from the Hadleyverse: dplyr (all of the subset/filter/etc. feels very clunky relative to base R), tidyr (reshape2 does this better).
&gt; It will fade into oblivion if R persists as a serious product. I can't see that happening myself. Like it or loathe it base R is R. Have a look at the dplyr code base, it's essentially a lot of `vapply` and `lapply` sprinkled with c++.
There's nothing conceptually wrong with your code, if that's what you're asking. It's cleanly written and (relatively) easy to read. From an R style standpoint, I'd remind you that you don't need to use semi colons to end each line of code, unless you try to put more than one line of code onto a single line of text. I'm just curious if you knew Map, Reduce, Filter, Find, Negate, and Position are all part of the base R language to begin with. Type `&gt; ?Map` into your R console for more details. As for your code, I would recommend considering more parallel implementations of map and reduce than using a while loop. It might help your code execute faster. Your out-of-the-box `map` function takes about twice as long to run as the `sapply` C primitive. &gt; system.time(sapply(1:1e6, function(i) i^2)) user system elapsed 1.137 0.023 1.166 &gt; system.time(map(1:1e6, function(i) i^2)) user system elapsed 2.161 0.052 2.235
Nice catch on data1. I did some pruning before putting in github, so I must have missed that. I spent 15 minutes looking at a reference guide online, but it looked outdated (web page looked old) and there were no for loops in it. That is why I used while loops in it. Are there any cases where omitting a semicolon would result in unintended behavior? In JavaScript, for instance, omitting a semicolon can sometimes wreak havoc.
That makes sense.
Do you recommend any resources for advanced topics like that? I am mainly learning R because SQL Server 2016 is supposed to support it, and there are things that are difficult to do in a readable manner in SQL. R reminds me of a cross between very limited subset of JavaScript and Python.
as others pointed out, you are basically writing C code in R, which doesn't make sense but some important pointers: you call things "list" but then treat them as vectors, those are two distinct things in R (vector has one type, list can have different type elements)! in filter(...) you do the common beginners mistake of adding elements to a vector in a loop (which leads to repeated copying of the vector), '[' is quite mighty in R and can make it a one liner (the other functions can also be compressed, even if you skip using the functions that provide exactly that functionality)
Hadley Wickham is a name you should become very familiar with. Here is his book, Advanced R, on the internet: http://adv-r.had.co.nz
In R, you should never ever ever ever be writing for loops. Everything in R can be done in a vectorized manner where everything gets analyzed at once. stuff like this is the typical replacement for for loops vector &lt;- someFunction(vector) matrix &lt;- apply(matrix, margin = 1, someFunction)
datacamp.com is a fantastic resource. 
I recommend datacamp.com. Thats what i've used to learn R. They have tons of different courses like credit risk modelling, or text mining etc that you might get a lot out of.
Hmm, I guess I wasn't totally clear. I've done some datacamp, some coursera, and I'm more interested in projects out in the wild, not snippets written for optimal instruction. Again there's a bajillion of these just a search away, but asking here is a nice way of narrowing down that search! :)
Use the package qgraph: [link](https://www.jstatsoft.org/article/view/v048i04/v48i04.pdf)
Thanks!!! I don't fully get it though. A data frame only with NA's and the same amount of rows as the old data frame? Or should I just extend the old data frame with NA's?
Please share if you find a good answer to this somewhere else
For python look for scikit-learn and for R look for caret package.
&gt; In R, you should never ever ever ever be writing for loops. For loops are not so bad if you use them sensibly, i.e allocate the vector length beforehand rather than growing it each iteration. &gt; Everything in R can be done in a vectorized manner where everything gets analyzed at once. Not *everything* can be done in a vectorized manner, and it's also misleading to say it gets analysed all at once, the loop is still there it's just written in C.
If you're having trouble getting a classification tree instead of a regression tree, it might be because the variable representing the classes is of class integer instead of character or factor (assuming you're using R).
Wrong sub-reddit, but why not. If you're set on using a decision tree, check out [`sklearn.tree.DecisionTreeClassifier()`](http://scikit-learn.org/stable/modules/tree.html#classification). Though you're likely to get better results from an ensemble method, so have a look at random forests and gradient boosted trees in sklearn as well.
What do you want to do with your model?
&gt; singular gradient matrix at initial parameter estimates That is a pretty clear error. Your `start=list(var = blah, etc)` doesn't work.
Try out this quick bit of code: df &lt;- read.csv(file = url("http://pastebin.com/raw/rmhGAfKb"), header = TRUE) model &lt;- glm(formula = UnitPrice ~ Complexity * Quantity, data = df) summary(model) Use `?glm` to find out how to set appropriate parameters. 
This might be what I am looking for! Except when i run ?glm, it tells me that this is used to fit linear models. What can I do to set this for a non-linear model? 
I may not fully comprehend your question, but no package should be needed beyond base R. If you want to use a package, one quick way would be with data tables package and the shift() command. Assuming annual time series (for quarterly or monthly, change the 1 in the shift() to 4 or 12): Pct Change: dt.data[,var1_pct_chg:=var1/shift(var1,1)-1] Diff Change: dt.data[,var1_diff_chg:=var1-shift(var1,1)] Another way would be to create a lag of the variable of interest and then do the pct change and difference between the lagged variable and current period. 
thank you 
The answer might change depending on what you want to do with splitnum, but this will work: num &lt;- 100 splitnum &lt;- seq(0, num, length.out = 6) 
You don't need to use non linear regression to solve your problem. As i can see you should only pay attention to the fact that you have one categorical variable (complexity). Just fit the model and take your conclusions according to that
This is the whole code: #Input files settings ids &lt;- read.table("C:/Users/c13144a/Desktop/R_FrequencyTables_ResearchNow/id_mosaic.csv",header=TRUE,sep=",",stringsAsFactors=FALSE, na.strings = "") second &lt;- T first &lt;- T #Defining and beginning of the big loop (includes all operations) m &lt;- list.files(path="C:/Users/c13144a/Desktop/R_FrequencyTables_ResearchNow/survey_data",pattern="*.csv") for (i in m){ ailment1 &lt;- read.table(i, header=TRUE,sep=",", stringsAsFactors=FALSE, na.strings = "") work_table &lt;- merge(ailment1, ids, by="SUBSID") v &lt;- colnames(ailment1[,-1]) # Little loop for (r in v) { a &lt;- data.frame(unique(work_table[,r])) a &lt;- a[!is.na(a)] q &lt;- data.frame(rep(as.character(r), times=length(a))) qa &lt;- cbind(q,a) work_table_ansfreq &lt;- subset(work_table, !duplicated(subset(work_table, select=c("SUBSID", r)))) table_ansfreq &lt;- table(work_table_ansfreq[,r]) names(qa) &lt;- c("Question", "Var1") qa &lt;- merge(qa, table_ansfreq, by="Var1") unique_answer &lt;- subset(work_table_ansfreq, !duplicated(work_table_ansfreq[,"SUBSID"])) table_uniqueans &lt;- sum(table(unique_answer[,r])) qa[,4] &lt;- table_uniqueans qa[,5] &lt;- (qa[,3]/qa[,4])*100 #Loop inside the first one makes calculations and assigns them as a next column to the data.frame created above m &lt;- unique(unique_answer$MOS_GROUP) m &lt;- m[!is.na(m)] colnames(qa)[3]&lt;-"Answer_Frequency" #mini Loop for (k in m) { mos_sub &lt;- subset(work_table_ansfreq, !is.na(work_table_ansfreq[,r]) &amp; work_table_ansfreq$MOS_GROUP==k &amp; !is.na(work_table_ansfreq$MOS_GROUP)) freq_mos&lt;- data.frame(table(mos_sub[,r])) qa &lt;- merge(qa, freq_mos, by="Var1", all.x =T) qa$Freq[is.na(qa$Freq)] &lt;- 0 colnames(qa)[ncol(qa)] &lt;- k } # For some reason the loop does not sort alphabetically the results that go out of it so i sort them myself qa1 &lt;- qa[,c(names(qa)[1:5],sort(names(qa)[6:16]))] qa1[,c(1,2)] &lt;- qa1[,c(2,1)] mos_subA &lt;- subset(unique_answer, !is.na(unique_answer$MOS_GROUP) &amp; !is.na(unique_answer[,r])) freq_mos&lt;- table(mos_subA$MOS_GROUP) #I create a 6th row that needs to have 3 empty cells and other calculations follow in the 4th to the end of the table #row6 &lt;- c("", "", "","", "Unique Distribution",rep("", times=11)) #qa1 &lt;- data.frame(lapply(qa1, as.character), stringsAsFactors=FALSE) #I change the first 2 columns to character thinking it would fix the error #qa1 &lt;- rbind(qa1, row6) #binding the data.frame and the 6th row qa1[nrow(qa1)+1, 6:ncol(qa1)] &lt;- freq_mos #assigning the values to the cells in the 6th row qa1[nrow(qa1), 5] &lt;- "Distribution of unique" if (first){ results &lt;- qa1 first &lt;- F } else { results &lt;- rbind(results, qa1) } #Then i write the file with a name that is a combination of the big loop's variable and RESULTS_ #Closing the big loop } if (second){ final1 &lt;- results second &lt;- F } else { final1 &lt;- rbind(final1, results) } } names(results)[1] &lt;- "Question" names(results)[2] &lt;- "Answer" names(results)[3] &lt;- "Answer frequency" names(results)[4] &lt;- "Unique people answered" names(results)[5] &lt;- "Grand mean" write.table(final1, file ="C:/Users/c13144a/Desktop/FINAL.csv", append = FALSE, quote = TRUE, sep = ",", eol = "\n", na = " ", dec = ".",row.names = FALSE, col.names = TRUE, qmethod = c("escape", "double"), fileEncoding = "UTF-8") 
If you can throw the data up on Dropbox or something similar and post the link I'd be happy to take a look. Can't really do much without at least a sample of the data. 
There's probably a better way, but I usually export results by putting them into a data frame and writing it to a csv 
In R you need to save your calculations to an object, like so: result &lt;- rlnorm.rplus(10000, MyMean, MyVar) After this you could save it a csv file like this: write.csv(result, "name of you file.csv")
Hi again. I sent you an e-mail with my contacts. I just think it would be easier if we could chat or talk about it so we can fix the issue faster. :-). 
And you are right. I just ran my code again and it produced the same error as you said. I really don't know what the hell is going on anymore :-DDD It also repeats data. I can see all results from the first file 5 or 6 times? In the FINAL file... Strange. This is probably caused by the error we are seeing?
How can I go about learning this level of code-writing in R? I've taken a handful of R courses, but none of them have taught me how to code things like loops and what not.
&gt; What kind of things are they going to get you to do? Find that out and practice. Don't know yet but with past tests I got no info at all. Like, literally all I was told was that the test was on "statistics" and I would not be allowed any aids. &gt;You need to memorize things for sure What things though? 
I'm just making this up, but I would expect someone who claims some R experience to be able to subset and apply a function across columns of a dataframe without any trouble. On the other hand there's no point expecting someone to know all the options of something like `glm`, as it takes all of 2 seconds to `?glm` to find what you need. If you've described your experience as basic, they're probably just checking you aren't making up things to put on your CV.
Lol'd
You could use match(min(dist(df)),df)
That still returns a single integer location unfortunately.
Is `which.min` what you're looking for?
For clustering, as in the code example provided above, perhaps having a set of (x,y) points and I want to begin by clustering the two points which are closest to eachother. Then I would want to find the min of the dist object, but I would need to know which of the points the min corresponds to. Admittedly, I'm aware of `hclust()` and that probably handles things better than me doing it myself, but in the case that I wanted to manually handle the clustering for some reason. 
This does it! I'll have to step through what's happening after some sleep, but I tested on another seed and it seems to work. Thanks a lot, interesting solution. I'll also have to see how it holds up for situations ie integer valued, where multiple min values could be present, and higher-dimensional data, but it's a great start, thanks. Seems a little bizarre that given what `dist` does, that it's not obvious how to extract the points involved in some distance calculation.
Love to know how it goes. I haven't used the dist function, but enjoy helping others. It really is a bit off the wall of a solution isn't it, haha.
Very :) It makes me think about an "Obfuscated R" contest, like the obfuscated C contests. It does fail on multiple minimums. And I realized that higher-dimensional data doesn't make a difference, because `dist` will always give me pairwise distances between datapoints, regardless of their dimension. (I think) In any case, I'm just exploring to learn, so it's not a critical matter by any means. Thanks for the help though, somehow I didn't realize `combn` even existed, so that's cool.
Wow, that's pretty nice, and the resulting (x,y,dist) dataframe will be very easy to work with. Thanks a lot! I changed it slightly, because which.min was still returning a single row. combinations[which(combinations$dist == min(combinations$dist)),] This returns multiple minimums if there are any, as in the case of `df &lt;- data.frame(x=1:5, y=1:5)`
Try opening R and reinstalling those two packages: install.packages("devtools") install.packages("tcltk") Otherwise, I'd try installing R using your package manager rather than going through anaconda. Why make things complicated?
So far this is the closest I've gotten to getting it to work; when I went through my package manager, I got a different issue with grep==1 or something like that. I got so far as to install devtools, but still getting the issue. tcltk is definitely installed, as I checked using capabilities()["tcltk"]. I've seen some workarounds where init.tcl is linked to the folders where it states it is missing, but I'm not quite sure how to do that.
try locate "tcltk" in an OS command window. That will identify where the libraries are. Then you'll have to link those libraries to wherever R thinks tcltk lives on your OS.
I've actually done that (I'm on linux, find tcltk) but I'm not sure where to go from there.
When I use find, I get &gt;/usr/share/tcltk/tcl8.6/init.tcl &gt;/home/niklas/anaconda3/lib/tcl8.5/init.tcl &gt;/home/niklas/anaconda3/pkgs/tk-8.5.18-0/lib/tcl8.5/init.tcl How can I link these to the folders mentioned in the error?
I've tried it through apt-get, same error. 
speed and dist are within the cars dataset, so you need to refer to them as follows tabber(cars$speed, cars$dist, cars) 
The solution was to use paste0 and create the formula, thanks murgs. Here's the code I got working: data &lt;- cars xtabs(~speed+dist, data, exclude= FALSE, na.action = na.pass) tabber &lt;- function (var1, var2, dataset) { mytab &lt;- xtabs(as.formula(paste0("~", var1, "+", var2)), dataset, exclude = FALSE, na.action = na.pass) return(mytab) } tabber(speed, dist, cars)
R is more library oriented than framework oriented. I posted this elsewhere, but here is how I keep things structured Project Name | run.R | | ---|---|----|---- | **scratch.R** | | | | **import** | data1.csv, data2.csv, puppies.csv | **export** | formatted.csv, cuteness.csv, etc | **common** | format\_tables.R, format\_graphs.R,etc If you are going to share weekly/monthly results you might want to structure it as project_name/export/presentation/date.pdf or something. If you are creating a web interface then you probably want to structure things differently. [Not sure if this helps](http://rstudio.github.io/shiny/tutorial/#client-data). Like /u/fang_xianfu mentioned, your question is pretty vague. 
Try [ProjectTemplate](http://projecttemplate.net/getting_started.html). It's a package that structures your projects with a few directories. It has a nice feature to support caching data pulled out of databases. This works, but it makes monolithic projects so rendering individual plots makes it a pain; you still need to write scripts that load the project and run the scripts that generates the plot. You can alleviate some of that pain using Makefiles to provide a bit more abstraction. The other advantage of Makefiles is you can do your munging in other languages and have Make handle orchestrating each step in the pipe when files change. There's a neat article on using Makefiles with data projects [here](http://www.oliversherouse.com/2016/04/07/makefiles.html).
 do.call('rbind', lapply(0:42, function(i) your_df[,(i*2)+(1:2)]) should do the trick if I understood your question correctly (not sure if I would call the solution 'elegant' but it is concise)
Can you provide a reproducible example and code?
1. seq(1,41,by=4) 2. You'll need to explain that a bit more... 
2. Not sure, but maybe you mean: (1:100)^2 (?)
/r/homeworkhelp
 send.mail(from = "sender@gmail.com", to = c("recipient1@gmail.com", "Recipient 2 &lt;recipient2@gmail.com&gt;"), replyTo = c("Reply to someone else &lt;someone.else@gmail.com&gt;") subject = "Subject of the email", body = "Body of the email", smtp = list(host.name = "smtp.gmail.com", port = 465, user.name = "gmail_username", passwd = "password", ssl = TRUE), authenticate = TRUE, send = TRUE) You don't have to use gmail, but you do need to find the SMTP config for whatever provider. Using a real address makes it not go to spam.
Right now its looks like this 1 c(0.00056206, 8.9506093483, 0.0780648, 0.09992363) 943 and i need 1 0.00056206 8.9506093483 0.0780648 0.09992363 943
 cbind(rbind(old_mat[,1]), old_mat[,2]) I suspect the old 'matrix' is actually a data.frame (just FYI) and you might need to wrap the above in as.matrix to transform it into an actual matrix
If you're on Linux (haven't tested, but should work): system('echo "This is the message body" | mail -s "This is the subject" mail@example.com') If you're not on Linux, look for another solution.
thanks. but still does not split the vector into new columns
Could somebody also explain the benefit of using s4 over s3? All my code is S3 and I'm reluctant to change it without some really compelling reason. 
This is a good question!
&gt; procedural programming style that should be re-written in OOP “OOP” and “procedural” are completely orthogonal. R is a *functional* programming language so you really should write *functional* code, not procedural code. You still can use OOP but I strongly advise against S4, which is a badly conceived framework that’s based on a misunderstanding of what OOP means. Use S3 instead. See [this comment](https://www.reddit.com/r/Rlanguage/comments/43dm4t/r_but_with_strong_static_typing/czimg20) for more details.
try df[,x]
 paster &lt;- function(x) {paste(x)} df &lt;- data.frame(a = c("a","b","c"), b = c(1, 2, 3), c = c(2, 3, 4)) df$b &lt;- paster(df$b) # Just one column df &lt;- apply(df, 2, paster) # All columns df &lt;- apply(df, 2, as.character) # Using the available tools df &lt;- apply(df[, c("a", "b")], 2, paster) # Subsetting then applying If you want to remove factors, then your best option is to do so when the data frame is created. 
Wow, thank you for all the quick anserws. I will try what works best for me =)
Thank you for this answer, it made clear to me that my function is redundant and there are much simpler ways to do this. It is definitely true, that I should avoid factors in the first place. I load the data frame from a SPSS .sav file, with read.spss. The problem is that in this file the interval variables (1 to 7) have their extremes not just as integers but as "1 = lowest" and "7 = highest", and so R makes it a factor.
Why not R6? I'm a very experienced programmer in a number of languages (python,perl,R,java), and both S3 and S4 feel like awful attempts to force OOP onto R. R6 seems to work far more like what I'd expect, and feels much cleaner to read/write. Why would I write in S3 instead of R6?
Nameforyourmodel&lt;- glm(y~x1+x2+...+z3, family=binomial(logit or probit), data=dataframe where your variables are). Is this what you want?
Why do you need all dummies to show up? It kind of sounds like you aren't quite interpreting the default model correctly.
I don't think anybody pointed out your main problem yet. It's this: df[x] Data frames always have *two* dimensions. Subsetting them using [] requires you to give *two* numbers, separated by a comma. Leaving an index blank means "all". So if you want all rows in column x, you do df[ ,x]
Factors are effectively enums from many languages or variants in OCAML/F#. They allow you to express a logical label without having to specify a mapping to a concrete type. Let's say you want to express males and females. You could use the strings "Male" and "Female" but strings can be complex to parse, store, and compare. You could use numbers. 0 is male. 1 is female. But these are harder to understand for a human. Instead, you just assign the factor "Male" or "Female" to the values and the system handles the rest. This has a few advantages: * They are faster. They don't take memory or string compares. * They are human readable. You don't need a lookup table to understand them. * The runtime can enforce the types. In the above example I could just use a new string, "Bear" or a new number, 3, and the compilers or runtimes would happily accept them. With factors, you can't. If I try to add a person with a gender of "Bear" it will complain unless I explicitly change the factor levels to include it. * Finally, as a few others have noted, they can be ordered if desires. The corollary is that if they are not, the system knows that they are unordered and can act appropriately.
the characters "[ ]" are used to subset data, of any type. You need to give the correct number of indices for the data structure. Data frames have two dimensions. You need to give two numbers (or blanks), separated by a comma. Vectors have one dimension. You only need to give one number. 
Purrr would be a better fit here - rowwise is on its way out. 
Thank you for your answer. I do understand that a data frame has two dimensions. Still, I ask you to try the commands "df[x]" as well as "df[,x]" on a data frame of your choice with at least x columns. The output is quite similar except for formatting. But I get that the notation with comma is more correct in a way.
I agree, my wording was off. John Chambers probably knew what he was doing. I guess the problem is that S4 came at a time where OOP had just entered mainstream programming and was confusing to many people. Coupled with the fact that Java was becoming synonymous with OOP and Java’s success, this may have led people to try to emulate this style of programming in R, using S4.
For a test that has N observations of P variables, why aren't you modeling that as an NxP matrix (or dataframe, if the data types of each variable aren't the same) with N in rows and P in columns? If you want any help with the rest of it, you probably want to post a reproducible set of data so we can help you out. I don't have a spare matlab file in your particular format, so it's really hard to tell you what to do. 
eval() is almost always a terrible idea, and this is definitely not one of the exceptions. Don't write code in strings. The right solution to this is to use a better data structure. Is there a reason you can't just have a dataframe with column for test, a column for observation, a column for order, and a column for the integer? Failing that, perhaps a list with sub-lists where one item is the vector of integers and the other items are the metadata? There are other options too. The best data structure will depend on what you want to do with it. What do you want to do with it?
&gt; Don't write code in strings. What is the harm in doing so? Why is it a terrible idea if it sorts my data into a structure I can work with? Working out the data structure has definitely been the hardest part - it was the hardest part when I did the work before in Matlab, but I got it to work in an admittedly clunky way. I'm basically going to be using a series of signal processing techniques to extract features from my signals, and apply machine learning techniques to detect damage in an engineering system based on the extracted features from the data under different operating/damage conditions.
This is where you should start. Data camp will go over syntax and data types. I guarantee those will be on the assessment. Assessment will more than likely include: * Syntax * Data Types * reading data * writing data * subsetting &amp; extracting data (get the coefficients from an lm() output; lists of lists kinda thing) * how to write a function More than that will be more than a basic understanding of the language 
Yeah, I'm working on switching over, but haven't totally converted to purrr yet. 
I'd make sure you know how to read in data, do basic cleaning (e.g. convert characters to numeric variables), and plot the data. I'd also make sure you know how to write a function and that you know how to look up the syntax/arguments for basic statistical tests (e.g. a t.test)
I'd love to hear what it was like after you've taken it
I hoping that's basically what it is. I am pretty comfortable doing that 
I'll let you know how it is 
Have you tried the [tm package](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) yet? It builds term-document matrices.
I just updated the post. The assessment was pretty simple 
Knowing the people involved, I'm almost certain that is not the case. 
Hence my saying that “John Chambers probably knew what he was doing”. I don’t know who else was involved, I just see the upshot of it. And that upshot is the Java-style OOP/procedural programming as it is evident in much of Bioconductor, and as it was taught to us in undergrad — by people who were deeply involved in Bioconductor.
I mostly I use stringsAsFactors=FALSE to simplify handling of string variables in my code. When producing output, particularly ggplots, I apply factor(x, *level*) to those variables whose string values need to be in a particular *order*. For example, the rows in [this](http://p1analysis.com/2016-dis-gtlm-4/index.html#pits-and-stints) chart has information about the drive times and and pitstop times of each of the cars in a race. By applying orderedcars=factor(carnum, levels=*finish_position*) and I can then assign this factor to ggplot's aes(y=orderedcars) to order the rows by *finish_position*. I also use factor to apply my own [scales](http://docs.ggplot2.org/0.9.3.1/scale_manual.html).
Post some data samples and what you have done so far.
You would need to be certain the selection is 'fair', and know the min/max, but the pnorm function is probably what you are looking for.
apply coerces it's input to a matrix so this is a bad idea
apply coerces it's input to a matrix and is thus a bad bad bad idea. 
Oh wow, okay, I never appreciated that until now. Thanks for the heads up.
I'm sure you're busy, but can you expand on this? I see data frames as fundamentally two-dimensional, and therefore always use two subsetting indices. Am I misunderstanding something?
So a truncated discrete normal distribution?
They are confusingly both one dimensional (if you think of them as a list of vectors) and two dimension (if you think about the rectangle formed by observations and variables)
Imagemagick is an image converter. Not sure it can be used to plot graphs.
Normal curve from 0-100 would need to have a mean of 50 and an sd that will get as close to 0 and 100 without going over. x &lt;- as.integer(rnorm(1000000, 50, 8.8)) max(x) min(x) hist(x) 
For remote servers Image magick allows you to view images saved on the remote server on your local machine's screen. It basically just opens a window on you local computer so that you don't have to copy the image over before viewing it. You have to make sure that you allow X forwarding when you log in to the remote server. For me, I just do 'ssh -X username@remotehost ' , the '-X' is what allows forwarding. Then on the remote machine, the command to view an image is "display", so to display img.pdf it would be 'display img.pdf'.
round() plus rtnorm()
Try: ?Devices 
What OS are you using? On ubuntu at least, calling plot opens a new window with your plot.
Does it? I'll have to try it.
This is a great resource. Once you're ready to design your own themes, you can easily reverse engineer the themes in this package by using, for example, print(theme_economist) or-in RStudio anyway- View(theme_economist)
[magrittr stole the operator from the F# language](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)
We independently invented around the same time and then when we figured it out decided to collaborate on function. 
Scale a beta distribution
If you need an explanation like me: http://www.r-bloggers.com/more-fun-with-and/ Still not sure how I understand how to use it, but it's enough for me to nod along and upvote. =D
Because I'm old 😜
I've just tried it. You don't even need to open the device using `x11()`. Simply issuing some drawing command opens the device by itself.
bruh. basically the way to think about it is this: `x %&gt;% f(y)` = `f(x, y)` you know how if you want to do multiple functions in a row, you have to nest the functions over and over? this allows you to put them in order, and you can daisychain functions by inserting the output of one as the input of the next. 
It's a great addition to R when it comes to interactive use. When programming, delayed evaluation makes debugging a bit hectic to say the least.
I like [cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html).
you can always add a %T&gt;% print() %&gt;% which will chain a print but keep going as if nothing happened. For example dplyr::mutate(...) %T&gt;% print() %&gt;% dplyr::select(...)
get the fuck out of here 
And you can use it with anything x &lt;- 1 x &lt;- x + 3 x %&lt;&gt;% `+`(3) or with strings x %&lt;&gt;% toupper() x %&lt;&gt;% c('foo') 
hng
or, if i'm not mistaken, use `require(purrr)` and then `%&gt;% walk(print) %&gt;%` 
oh nice! I prefer that to %T&gt;%
i like how you just taught me %&lt;&gt;% and i just taught you walk!
No, because the main feature is inserting the argument alongside the next arguments. The y in my example would be the necessary args of the next function. Yours isn't wrong, it just doesn't show off the main feature of piping, imo. 
No unfortunately not. But you can use it with ggplot1! See https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Towards-a-grammar-of-interactive-graphics for more details
IMHO, yes, to prevent your R process (or worse, the OS) crashing due to a wrong bit. Any process where it's critical to not crash, you'd use ECC. Because it will probably take you a decent amount of time (or NEVER) to restart your R process where it left off, i.e.., if you have to first reconcile the previous state of your order book with the current state of the order book out in the market (i.e., executions and other updates may have flowed in while your process was down). In my experience, this is often horrendous and I wrote "NEVER" because sometimes the automatic reconcile fails and you get stuck the rest of the day doing manual reconcile. May not be such a huge issue if trade just 2 stocks :-)
Make your own with theme_update and customized colour palettes # Colour Palette - Colours for charts palette3 &lt;- c("red", "green", "blue") palette4 &lt;- c("grey60","red,"green", "blue") palette5 &lt;- c("grey40", "grey50, "grey60", "grey70", "grey80) # ggplot themes theme_update( plot.margin= unit(c(0.25,0.25,0.25,0.25), "cm"), title = element_text (colour="black", size=12), panel.background = element_rect(fill="NA"), panel.border = element_blank(), panel.margin = unit(1, "lines"), panel.grid.major.y = element_line(colour="grey90"), panel.grid.minor.y = element_line(colour="NA"), panel.grid.major.x = element_line(colour="NA"), panel.grid.minor.x = element_line(colour="NA"), axis.text.y = element_text (colour="black", size=10, hjust=1), axis.title.y = element_text (colour="black", size=12, angle=90), axis.text.x = element_text (colour="black", size=10,angle=0), axis.title.x = element_text (colour="black", size=12), axis.ticks = element_blank(), legend.text = element_text (colour="black", size = 12), legend.position = ("right"), legend.title = element_blank(), legend.key = element_blank() )
Yes, that would work!
You can use the assign() function to create variables on the fly within a loop. 
Not quite sure exactly what you want for a final output, but this should get you close. This will create a character vector with the image file names. It appears that the actual images have a similar url pattern so I used a paste and gsub, which might not be a perfect match for the real urls. Note, somewhere is a missing file name as the character vector has 912 and the full table has 913. Also, there's probably a better way of doing this, and someone else can probably jump in with better code. vg &lt;- read_html("https://en.wikipedia.org/wiki/List_of_works_by_Vincent_van_Gogh") jpg &lt;- vg %&gt;% html_nodes("table") %&gt;% .[[1]] %&gt;% html_nodes("img") %&gt;% html_attrs %&gt;% rapply(function(x) head(x, 1)) jpg.link &lt;- paste0("https://commons.wikimedia.org/wiki/File:", gsub(" ", "_", jpg)) 
Also, /r/rstats might be another place to post this request
I'm having trouble understanding how is it different from poisson disc distribution? https://en.wikipedia.org/wiki/Low-discrepancy_sequence ?
Welp, I tried your code and I think you are onto something but ultimately it did not work. I think I just don't know enough R or enough HTML/CSS to figure this out. However, I did a freebie run on https://www.import.io/ and got what I wanted. Whew!
Try this and references therein from Zachary, et al. http://chemists.princeton.edu/torquato/research/hyperuniform-materials/
Checkout Chrome selectorgadet add-in along with Hadley Wickham's web scraping guide. 
Oh happy day.
did you actually start the R code? Have you read any tutorial on R? once you have read in the data you might want to look at ?rle for a easy way to extract continues heatwaves
Nah mate.
I have problem at writting down the code for detecting the hot day. Since I can not do it, it nothing works :((( library("forecast", lib.loc="~/R/win-library/3.1") data&lt;-read.table("LJUBLJANA.txt",header=TRUE,sep="\t", dec=","); head(data) # pisanje v indekse j&lt;-1; nad29.5 &lt;- 0; temmax&lt;-1:46816; dim(temmax)&lt;-c(46816,1); #data for 1879 - 2016 for(i in 1:length(data$leto)) { if(data[i,1] &gt;= 1879 &amp; data[i,1] &lt;= 2016) if(data[i,4] &gt; 29.5){nad29.5 &lt;- nad29.5 + 1;} ## a start of the new year if(data[i,1] &gt; j+1878){temmax[j,1] &lt;- nad29.5;nad29.5 &lt;- of data[i,4];j &lt;- j+1;} } #grafically summary(temmax) cas.niz&lt;-ts(temmax, start=1879) ma5&lt;-ma(cas.niz, order=5) ma10&lt;-ma(cas.niz, order=10);
I see two questions here: how to read access files and how to manipulate data. There's a HUGE amount of literature on the second one, so I won't attempt an answer there. For reading access files, tell me if you can get this working for you: library(RODBC) myDataConnection &lt;- odbcDriverConnect("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=C:/your/path/here/yourfile.accdb") myData&lt;- sqlFetch(myDataConnection , "YourTableNameHere", stringsAsFactors = FALSE) close(myDataConnection) rm(myDataConnection)
Thanks! appreciate the help. Two things were missing tho. 1. I needed to switch my version of R to match the MS Access 32 bit. 2. I needed to install the necessary access drivers. 
https://www.reddit.com/r/Rlanguage.json Reddit has a neat api with a decent documentation. It should be pretty easy to find it.
There are some really great tutorials in the swirl package. They're fast (10-20 minutes) and most importantly free. This should get you started. Do a bunch of lessons. They cover importing data, working with data, and a bunch of other topics. Once you have some basics down and need specific questions answered then come back. http://swirlstats.com/students.html
If the '&gt;' doesn't appear it means it is still running the code (you can always check if the process is using cpu%). If it expects more code it shows the '&gt;' since that is the input prompt From briefly looking over the code, could this piece be true for ever? while (U.pls[i,i] == 0) { U.pls &lt;- rbind(U.pls[-i,],U.pls[i,]) } And just a pointer, two of your while loops are for loops.
Here is how to add a blank row: # with an existing data.frame called “data”… # create a one-row matrix the same length as data temprow &lt;- matrix(c(rep.int(NA,length(data))),nrow=1,ncol=length(data)) # make it a data.frame and give cols the same names as data newrow &lt;- data.frame(temprow) colnames(newrow) &lt;- colnames(data) # rbind the empty row to data data &lt;- rbind(data,newrow) # with a previously-defined character object containing the colname # (“newColName”), add the empty column to the data frame and populate data[,newColName] &lt;- NA data[nrow(data),newColName] &lt;- “newValue” [Source](https://gregorybooma.wordpress.com/2012/07/18/add-an-empty-column-and-row-to-an-r-data-frame/) You'll probably need to do something like that inside of a loop to create this table. I've needed to add single blank rows before, but it seems odd to need so many... 
if it is for external use, you can always print it with cat and set the line separator to "\n\n\n"
~~I can't help you with your ggmap issue and I'm not sure if this is of any use to you, but perhaps you could consider using the [OpenStreetMap package](https://cran.r-project.org/web/packages/OpenStreetMap/index.html)? I've used it with success.~~ Never mind. I see that ggmap has OpenStreetMap calls as well, so you probably really need Google Maps data.
Thanks, I'll have to try something like this out. I'm surprised it takes so much work to do something so simple.
I haven't tried it but it looks like `miscFuns` has a `bin()` function for this. [https://cran.r-project.org/web/packages/miscFuncs/miscFuncs.pdf](https://cran.r-project.org/web/packages/miscFuncs/miscFuncs.pdf)
ok, so the issue is that mutate_geocode() in auburngrad's branch doesn't work with the key parameter. i'm trying to pass the key in the ... ! someone please help me. edit: since mods deleted my comment - i fixed it and commited here: https://github.com/adhi-r/ggmap/commit/c0918ea60fb8ad94f45ebf3ade3aa960eea8a099
oh shit i found the error in geocode(), fixed it, [and committed here](https://github.com/adhi-r/ggmap/commit/c0918ea60fb8ad94f45ebf3ade3aa960eea8a099) this is my first github commit EVER. even though it was a minor fix, i'm so fucking happy. this shit was fucking me up for two days.
I'm a little tipsy but would transformDivideThousand &lt;- function(data_frame, listofvars = TRUE){ data_frame[,listofvars] &lt;- data_frame[,listofvars]/1000 return(data_frame) } do the same thing as your function? It should be quite a bit faster since it's vectorized.
Forecasting: principles and practice is free on OTexts. 
It's a strange approach, having methods in search of a problem... Writing your own implementations is a great learning tool, but sorting algorithms are bread and butter computer science so any method you read about has likely been optimised and written in a low-level language already - and will be orders of magnitude faster than what you will write in R.
 j[,1] &lt;- vec1[j[,1]] j[,2] &lt;- vec2[j[,2]] should do the trick, not the nicest, but does what you want
awesome stuff (code, not the drought...)! might be worth looping that gif. dunno the command in magick for that off the top of my head though
Great thanks. Setting the row and col names in the matrix also works. I must have done something wrong the first time I tried.
Yes, I tried doing this originally and thought it didn't work but I did it on the wrong matrix... hahaha, thanks.
Nice! Never knew about the magick package. Certainly it should be a bit safer than using system() to invoke an Image Magick command. 
But why did they shut down the blog? I'm perfectly fine with Microsoft contributing to an open source project, but I just don't understand why a decent source of knowledge is gone. 
I don't know about the blog, I've never used inside-r. I imagine it has something to do with being owned by Revolution Analytics which is owned by Microsoft. I was more replying to the comment: &gt; I really hope R survives this monetization wave and keeps its openness spirit...
It's a complicated issue (redirecting the blog aside). The creator of R and the rest of the community built something amazing. Revolution squirrels away their own project then says hey guys look we made this! then sells it for a shitload of money to Microsoft who will in turn milk gullible company managers into doing the same. "But it's supported now!" doesn't mean jack shit. Have you ever tried to have something fixed on Windows? Meaningless. So yeah. Embrace, extend, exterminate. People poured their lives into this for the benefit of others not for the monetisation of a multi billion dollar company. Shrug. But that's the license.
Sorry, this was tough for me to follow as I'm new to this and not really sure how functions work. These are the actual vectors I am trying to put together, the others were just an example. Could you give me the info on how to make this into a function? Would really appreciate it! (I'm trying to make a model to predict percent chance of an outcome in a baseball game if your curious) names(Team1) &lt;- c("1", "2", "3", "4", "5") names(Team2) &lt;- c("1", "2", "3", "4", "5") RSPS &lt;- ((Team1["1"] * 4.45 * 162 * .666) + (Team1["2"] * 4.45 * 162 * .334)) RAPS &lt;- ((Team1["3"] * .666 * 162) + (Team1["4"] * .334 * 162)) + Team1["5"] RAPS2 &lt;- ((Team2["3"] * .666 * 162) + (Team2["4"] * .334 * 162)) + Team2["5"] Wins &lt;- (RSPS^1.83) / ((RSPS^1.83) + (RAPS^1.83)) Wins2 &lt;- (RSPS2^1.83) / ((RSPS2^1.83) + (RAPS2^1.83)) HomeP &lt;- ((Wins) * (1 - Wins2)) / (((Wins) * (1 - Wins2)) + ((Wins2) * (1 - Wins))) * 1.08 EDIT - Sorry if this is super complicated, but this has been bugging me and I have been trying to figure this out for a while. If you can tell me how to make this a function, in which I can just plugg-in the initial variables (Team1, Team2) to get my final calculated percentage number I would certainly give some gold. 
I totally agree with /u/FlyingGeo . You want to write a function that returns the desired output. Check out this [guide to writing simple function](http://environmentalcomputing.net/writing-simple-functions/)
it worked! Thank's dude, that is really huge. Have been doing these calculations by hand, this was really helpful, I appreciate it big time. 
Please stop indexing your vectors using numbers as strings. Instead of A["1"] you should just use A[1]
thanks, that actually simplified this a lot, was unaware you could do that. 
[Relevant](https://xkcd.com/323/)
I think you're looking for [cbind](https://stat.ethz.ch/R-manual/R-devel/library/base/html/cbind.html).
that would work except i would have to rename the variables on the fly and somehow add the new variables to the cbind(x, x2, x3, x4)
c() them together in the loop and the. Do a cbind() when you exit the loop
Could you please show me an example of this? I'm new to coding and I can't seem to visualize this
Thank you!
rbind()
What you're looking for is how to create a function. So if I want to make a function that just multiplies two numbers and adds one to the result: times_add_one &lt;- function(x, y) { ans &lt;- x * y + 1 return(ans) } Then you can use `times_add_one(2, 3)` which will return 7. Should be enough for you to figure it out. 
Are you also not interested in creating multiple columns, one for each level? Because it seems like the two options are either a) have a column for each level (a binary column for A, B, C, etc), or b) treat each possible combination as its own level within one column (A is a level, B is a level, C is a level, AB is a level, etc), but this seems prohibitively complex depending on how many categories you have and if the order matters. Can you provide any additional information about your data and what you're looking to do with it?
Ah so each row is a movie, and one of the columns includes the actors in that movie? So one entry might be "Tom Hardy, Jake Gyllenhaal" and another might be "Marion Cotillard, Leonardo DiCapiro, Robert Deniro" or whatever? If that's the case, what kind of analysis do you want to do on the data?
I have the box office data for a movie theater and I want to know if the presence of an actor affects the performance. 
If you want to check for a specific actor you can pretty easily subset the data for rows that include that actor's name. You could also create a list of all unique actor names in that column pretty easily and then use that list to loop through your data and gather stats for each actor. Is this the kind of stuff you had in mind?
Yeah makes sense. I could just compare the gross for the subset of data for the actor and compare it with overall performance of movies at the theater. Thanks! I think I wanted to have a comprehensive data set with all the columns needed to run the analysis but this is more efficient.
Thanks for the tip. Just getting into R programming. I plan on buying the Art of R programming. Would that be a good resource to know about coding the right way ?
Not sure if this is what you're after, but this may help. https://www.r-bloggers.com/populating-data-frame-cells-with-more-than-one-value/
Read up on the concept of [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf) [PDF]. Your use-case should be much simplified by using this. Instead of storing multiple categories per row, you create a table with one row per category. Movie | Actor ---|--- Movie 1 | Tom Hardy Movie 1 | Jake Gyllenhaal Movie 2 | More Actors Movie 2 | Too Bored Movie 2 | To Think Movie 2 | Of More There are several packages for R to manipulate such data effectively; foremost ‹dplyr›.
Thank you very much! With some tinkering, I was able to get what I needed- mod &lt;- function(x,y) { mds &lt;- x ubd &lt;- y vec &lt;- numeric(ubd) vec[1] &lt;- 10%%mds for (i in 2:ubd) { vec[i] &lt;- (10*vec[i-1])%%mds } return(vec) } 
Not quite what I was asking for but thanks!
Thanks for linking the paper. I will work on converting it to tidy data.
Oh, this is a data viz book. I thought they meant igraph/sna.
If you just want Gibbs sampling, [JAGS](http://mcmc-jags.sourceforge.net/) and [rjags](https://cran.r-project.org/web/packages/rjags/index.html) are probably the most popular ways to do it. If you aren't looking specifically for Gibbs sampling, I would recommend Stan and rstan as /u/Icota mentioned. I think the documentation is better and HMC/No-U-Turn methods seem to have better performance on correlated parameters.
Make a dataframe with all the items out need to use and loop through the script replacing the letters with that item... I'm on mobile so hard to explain but I've got some stuff at home that's does similar things
Using the dplyr package: library(dplyr) filter(data, Position == "A") Try googling these basic questions first, there a lot of tutorials on this. 
Look at the leaflet R package - it allows you to create really cool interactive maps and has good documentation/tutorials.
Thanks. I'll be sure to look more into the different packages
It sounds like you actually need to put the code in a function, then call it each time changing a variable for the letters in question. Can you post some/all of your code?
that makes perfect sense actually. here is what I want to repeat &gt; BKGURL &lt;- 'http://www.marketbeat.com/stocks/LON/BKG/' &gt; BKGwebpage &lt;- read_html(BKGURL) &gt; BKGhtml &lt;- html_nodes(BKGwebpage, "td:nth-child(5) , td:nth-child(4) , td:nth-child(3) , td:nth-child(2) , td:nth-child(1)") &gt; BKG &lt;- html_text(BKGhtml) &gt; BKG [2] &gt; BKG [4] &gt; BKG [6] &gt; BKG [7] &gt; BKGprice &lt;- 'http://markets.investorschronicle.co.uk/research/Markets/Companies/Summary?s=BKG:LSE' &gt; BKGpricewebpage &lt;- read_html(BKGprice) &gt; BKGpriceHTML &lt;- html_nodes(BKGpricewebpage, "#wsod td.first") &gt; BKGgbpp &lt;- html_text(BKGpriceHTML) &gt; BKGgbpp [1] I then want BKG to be replaced with AZN (for example). Thanks for your help, much appreciated edit: code doesn't display very nicely in a reddit post it seems :'(
There is probably a cleaner way, but this works: x = c(19, 65, 72, 41, 24, 86, 42, 42, 64, 91) diff_x = diff(x) signed_x = diff_x &gt;= 0 seq_length = numeric() for (k in 1:length(signed_x)){ seq_length[k] = 0 for (j in k:length(signed_x)){ if (signed_x[j]){ seq_length[k] = seq_length[k] + 1 }else{ break } } } longest_seq_start = which(seq_length == max(seq_length)) sub_sequence = x[longest_seq_start:(longest_seq_start + seq_length[longest_seq_start])] print(sub_sequence) # [1] 42 42 64 91
for (name in names(TagsPivot)) { write.csv(a, file = paste('Companies/',name,'.csv',sep = ''), col.names = NA,row.names = FALSE) ##your code here using name where you want to replace anything } 
That's a nice little function, rle! Thanks for the tip.
i am not sure I fully understand this but will try have a fiddle with it. much appreciated! thank you
Similar idea, but more efficient myFunc &lt;- function(x) { change &lt;- diff(x) subseq &lt;- rle(sign(change) != -1) lengths &lt;- subseq$lengths values &lt;- subseq$values longest &lt;- max(lengths[values == TRUE]) length.pos &lt;-which(lengths == longest)[which(values[which(lengths == longest)] %in% TRUE)] end &lt;- sum(lengths[1:length.pos]) + 1 start &lt;- end - longest x[start:end] } max_pos_length = function(x) { a = rle(diff(x)&gt;=0)[[1]] b = sum(a[-(which.max(a):length(a))]) + 1 return(x[b:(b+max(a))]) } foo1 = function(z) {x = sample(1:100000); myFunc(x)} foo2 = function(z) {x = sample(1:100000); max_pos_length(x)} microbenchmark(foo1(), foo2(), times = 1000L) &gt; microbenchmark(foo1(), foo2(), times = 1000L) Unit: milliseconds expr min lq mean median uq max neval foo1() 10.610437 12.37062 12.54137 12.54808 12.72396 15.26859 1000 foo2() 9.497912 11.04273 11.23399 11.15426 11.29303 98.69924 1000 
Hi! Sorry for the late response. That worked for me. But I wanted something more elaborated, só I talked to the teacher supporting me, and he gave me a code that does the job. The code he gave to me, makes all the conversions real -&gt; binary, binary -&gt; real. But, anyway. Thank's a lot for your time. Cheers!
I'd love to see it. I assume it's in C or C++. Care to share?
Damn forgot about which.max, nice work.
Very cool, though I wish very specific packages like &gt;dataRetrieval: Package to retrieve USGS and EPA hydrologic and water quality data, officially supported by USGS. The vignette gives several examples of downloading interesting data sets. Wouldn't use such generic names. 
r/askprogramming *I am a bot; I link the subreddits mentioned in the title for easy navigation*
To expand... for(i in 1:length(correct)){ text=gsub(incorrect[i], correct[i], text) }
Neural networks are not R's strong point. If you're set on using them it might prove easier to try python modules such as keras or neon which are a lot more popular and have a lot more documentation.
How would you use lapply for this ?
I'm not sure either. Not to hijack your thread or anything - but this is something I've been curious about as well with the split-combine - apply stuff. I get the baseball part and the general concept just fine - But when it comes to a list of models made at a group level to the corresponding groups in a test set - how do I do that?' For example in the baseball model - supposed I grouped every all the data by league and team and playerid lm(HR~RBI) for a random subset of years (training dataset) If had my test set of data for the other years held out, how do I ensure that my plyr code assigns the right linear model to the right player/team combination?
You can also look at ropensci tjey have very high quality packages for opening up data
"Warning message: algorithm did not converge in 1 of 1 repetition(s) within the stepmax" This is the error I keep getting
At the moment you're just using the default parameters in `neuralnet`. If you type `?neuralnet` you should be able to see the documentation. You could maybe try increasing your `stepmax` parameter to a higher number such as 1e7, or increase `rep`. Though you're probably just going to overfit such a small dataset. Honest advice would be to use caret and a more simple statistical learning tool such as a random forest. A neural net is likely going to give you poor predictive power with such a small dataset, as well as many parameters you will have to adjust to get anything useful.
Yep! path &lt;- "~/R/project/" for(file_name in as.vector(list.files(path = path, pattern = "*.txt"))) { df &lt;- read.csv(paste0(path,file_name), header = TRUE, sep = ",", quote = "\"") ggplot(data=df, aes(x=columns_x, y=column_x, fill=column_fill)) + geom_bar(colour="black", stat="identity", position=position_dodge(), size=.3) + # Thinner lines scale_fill_hue(name="Legend (Fill)") + # Set legend title xlab("x label") + ylab("y label") + # Set axis labels ggtitle(file_name) + # Set title theme_bw() ggsave(file=paste0(file_name,".pdf")) }
man I'm trying to work through the baseball case now, and I cant even get through the first part. I'm solid until &gt;d_ply(baseball, .(reorder(id, rbi / ab)), failwith(NA, plotpattern), .print=TRUE) Everytime I run this i get the error &gt;geom_path: Each group consists of only one observation. Do you need to adjust the group aesthetic? I haven't gone that in depth into ggplot2 yet, so I'm not sure what I've done wrong. sorry for not answering your question, but I'm just trying to work through it in order 
Ah, whoops. I read that wrong and thought they had a bunch of arduindos outputting txt files.
the easiest is likely readLines(...) it returns a character vector of each line of your file. You can then split it with lines[ y + 8*(0:(length(lines)/8-1)] which gives you every 8th line with an offset of y (the variable you want to extract). And then you can split those lines with strsplit(...) and with sapply and '[' you can separate the parts for the different lines.
'1:length(x)' has unforeseen results when x is length 0. Consider 'seq_along'. In your CreateCustomTimeSeries, what happens if interval isn't provided and values is a single value? It looks like interval gets a sample of length 0, and your loop doesn't behave correctly. 
Wow, I didn't expect anyone to be this helpful. Thank you so much.
Cheers. Purchased at top level, looks like some solid reading. Love these bundles. 
Can anyone give reviews of any of these? 
Firstly, I found this to be extremely helpful: http://r-pkgs.had.co.nz -- I bought the e-book copy but refer to both that and the site version very regularly. Read it cover to cover then go back and review your own package. I'll add a few more points here as they're not specific enough to be in Issues. - The title for `GetWindowVariancePDF` is "Plot the PDF of Variances ..." but it doesn't do any plotting. - `Query` has a `library(xts)` call that isn't necessary (also flagged by `check()` - Dependencies in `Imports` should be `Depends` (this is a hairy distinction) - You could use some examples in your major user-facing functions - One of your tests fails because it relies on another package (`TSTestDataUtil`) which isn't defined as a dependency - Consider following the `testthat` templates described here: http://r-pkgs.had.co.nz/tests.html - `paste0` is essentially a `sep=""` version of `paste` - `GetLengthInSeconds` seems oddly specific; do you always want seconds? You could make this more flexible by replacing it with GetLength &lt;- function(timeseries, units = "secs") { difftime(time(timeseries[length(timeseries)]), time(timeseries[1]), units = units) } - You've used `is.pip` as a variable, but it looks like a function (well, I looked for it). This is especially confusing to a dev reading your code as you've used things like while (!is.pip[k]) { k &lt;- k + 1 } - Consider turning on RStudio's syntax checker - you have a lot of missing whitespace (e.g. `abc&lt;-x+2`) which is a) prone to breaking if accidentally changing to `abc &lt; -x + 2`) and b) harder to read. If you can't fit the entire block of code onto a page, consider that your block might be too long. - Add links to the GitHub repo in `DESCRIPTION`, i.e. URL: https://github.com/joshmarsh/TSPatternQuery BugReports: https://github.com/joshmarsh/TSPatternQuery/issues - I see you're using TravisCI, which is great, but consider also using [AppVeyor](https://www.appveyor.com/) for Windows testing, and once your tests are in order, [CodeCov](https://codecov.io/) for code test coverage. Furthermore, `devtools::check()` will give you lots of useful information. It's a requirement that it produces no `ERROR`s or `WARNING`s (idealy no `NOTE`s either) if you're going to submit to CRAN. At the moment it gives an `1 error | 3 warnings | 5 notes` and points out the things you need to look at: - `.travis.yml` should be listed in `.Rbuildignore` - `testdata` shouldn't be a top level directory - Lots of 'global function definition' notes. This is very common, especially if using non-standard evaluation, but it gets flagged because of how R does scoping. e.g. `GetLengthInSeconds` refers to the function `time` without defining which namespace it's from. Either use `stats::time` or add `@importFrom stats time` in the roxygen header - Ditto for `approxfun`, `cor.test`, `density`, and `var` - I know, we use these all the time from auto-loaded packages, but it's best to be precise As for whether the package works or not, well, that's for you to decide. I'd be adding more specific testing around what inputs will break your functions. I've been saved by this several times (make a change and a test fails). Any time you get a bit of code failing, you should be writing a test that captures that scenario. Best of luck! I've found package building to be a great way to focus my programming skills and really tighten the way I use R on a daily level. The best way to learn what a good package should look like is to check out some of the very popular ones on GitHub and see how they do things (e.g. anything estabilshed from Hadley's tidyverse). This is by no means a perfect example, but [I got this to pass the CRAN process recently](https://github.com/jonocarroll/ggghost).
Art of R programming is good for a statistician looking to go deeper into programming via R or a programmer going deeper in the matrix and vectorize ops of R. Not an advanced programming book by any means but a very good introduction and survey on s3 and s4 classes. I'm currently going through the Python automation one. I'm a little less impressed by it. There's a few typos in the code and it spends a lot of time on the basics (why would I be looking automating processes if I didn't know what a loop was?) But the examples are decent and the writing is clear and consise. Art of R is really the only reason I even bought the bundle, it's a great book. I didn't mind spending 15 bucks on it alone, but with the humble bundle I get a bunch of other books and without DRM.
that was way too easy, thank you for the hint, i don´t got it by myself.. It works now, great
Fivethirt--Oh. 
Yeah I know! that's all I was sure about too. I mean, they have a github page.
check out grepl()
Yeah, adding lines to connect the bars might be a pain in ggplot, although definitely possible. If it is a one off plot I would just add them in illustrator or paint. 
Do i understand you correctly that I cant specificly search for the *? Sadly ill have to wait till monday to try the recomendations :( (maybe not the best idea to post on friday)
You can, but in a regex (which grep and gsub use) by default it means the previous letter can appear 0-Inf times. So you either have to use the option fixed=TRUE to deactivate the use of regex or show that you mean the actual symbol * and not the regex meaning by writing \\*
Every time I see something like this, all I can think is that there's a GIGANTIC pile of money out there for some group/company/institution to grab and dash. The potential exists for some accredited institution to offer online college degrees for $1000 (arbitrary) number or less, wiping out huge amounts of the competition, but yet nobody is doing it. Online classes already exist. Most courses rarely change - below the graduate level, only minimal changes would ever be needed to adjust the curriculum. But yet, the colleges and universities just want to keep on following the same (greedy) outdated models. 
http://r4ds.had.co.nz/strings.html
I would do a video on simple data manipulation before going towards the harder functions. I'd split the cbind etc. video in two, as apply and dplyr are, in my opinion harder to understand and use correctly.
Thank you for this very clear explanation. It hit me like a ton of bricks while reading your post that the output "Won 1 Oscar" occurs because nothing was replaced. My misunderstanding ultimately was that I though sub() only output any elements for which changes are made, but I realize now that it outputs the entire vector of strings regardless of whether a substitution was actually made in a particular element,. Thanks again for your post! I get it now!
Swirl is a good place to start if you like interactive learning. All the lessons take place inside your R environment: http://swirlstats.com/students.html
I'm interested in this answer as well. 
per my edit above: it appeared to be that website I was using. If I use [this one](ipinfo.io/ip) instead, then it seems to work better. It even picks up on my VPN ip address when I use RCurl.
Uh-oh **why_not_tho**, it looks like there's **1** broken markdown links in your post. I've listed them below: Fixed Link | Original Markdown | Fixed Markdown :---------:|:----------:|:----------: [this one](http://ipinfo.io/ip) | [this one](ipinfo.io/ip) | \[this one\]\(http://ipinfo.io/ip) *** ^(I am a bot, and this action was performed automatically.) [^Feedback](https://np.reddit.com/message/compose?to=lucadem1313&amp;subject=Link%20Fixer%20Bot "Contact to report issues, ask questions, or leave some feedback") ^| [^Formatting ^Help](https://np.reddit.com/wiki/commenting "Reddit.com markdown guide") ^| [^Subreddit](http://np.reddit.com/r/thelinkfixerbot "Subreddit for bot info") ^| [^Bot ^Code](https://github.com/lucadem1313/thelinkfixerbot "Code on GitHub") ^| [^Original ^Comment](https://np.reddit.com/r/thelinkfixerbot/comments/4vo43z/list_of_all_posts_with_broken_links/d6re8lp "Record of original comment") ^| [^Delete ^Comment](https://np.reddit.com/message/compose?to=thelinkfixerbot&amp;subject=Delete%20Comment&amp;message=delete%20comment:%20https://www.reddit.com/r/Rlanguage/comments/4yxdir/what_ip_address_does_rcurl_use_default_options/d6re8ow "Just Click Send")
Why would you want to do that, if i may ask?
What, in particular, are you trying to plot?
BI intern i use R because i want to - to be honest other languages would have been better. everything we do is in salesforce so i've gotten good at querying that data, manipulating it, and then visualizing it in tableau. so really, i just used R to author an ETL library - importing salesforce data into my SQL server, and then connecting to that with tableau. i've done very little in the way of statistical analyses, but that's only because i was the company's first BI/data hire ever so i had to focus on more operational stuff.
Marine biologist...fish stock assessment, satellite tracking, mapping, spatial statistics, etc. 
Fisheries Biologist: I'm starting to Learn R today! (so I subscribed to this sub). I hope to use it to analyze the general fisheries data that I collect at work. 
I don't really have a title. I was formerly an analyst at a major media publication company (edit: my focus was on newspaper advertisement and direct mail optimization), but was poached by one of our clients to bring them the gift of data-driven decision making. Just off the top of my head, I use R to: 1) Automate repetitive reporting for finance, HR, operations, accounting, and other departments 2) Hypothesis testing (simple anova stuff) for various departments 3) Predictive analytics. One example: model that predicts which employees are becoming flight risks. 4) Clean really shitty data that we get from a dozen sources 
Used it occasionally at my previous job as an analyst when looking beyond just getting the data, but am using it almost daily in grad school. 
Web analyst here, I use to get and cleaning data and create dashboards
Also curious on real time aspects! 
Data analysis and analytics in regards to marketing. Use R to create models and determine the impact of marketing on consumer response.
I understand the certification part of the question but not the specific reference to r-project.org, plus the link to Swirl.
You should get in touch with /u/owls_with_towels
Energy Trader/Analytics Almost none but heavy SQL user, so scraping/modeling aspect is critical
I hope this isn't a shitty answer. I don't know what regex is, but dplyr has filter which can be used in chaining which looks similar...
 x = c(1,2,3,4,5,6,7,'as','fr',1,2,3, 4,5,'53j35') x[grepl('[*1-9*]', x)] -&gt; [1] "1" "2" "3" "4" "5" "6" "7" "1" "2" "3" "4" "5" [13] "53j35" edit: guepier actually answered your question. I'm just pointing out that grepl doesn't have to be ugly. Maybe you're not writing it as succinctly as possible.
for a vector you can use grep(...,value=TRUE) if you are after all rows of a matrix/data.frame/... where one column matches a regex you will have to go with cubby13579's solution for base R (data.table and hadleyverse have their own subsetting functions)
I don't think any of the popular Excel packages (openxlsx, XLConnect, readxl, etc.) can do this with their standard functions. If you can, I'd recommend that you make a copy of the xlsx file, read the conditional formatting formulas that were used to color code the texts, and write an equivalent formula in R or Excel to create a proper column containing the information that was encoded as a color. If you can't recover the formula or rules they used, you'll have to find a VBA solution to the problem.
Use an anonymous function. E.g. mclapply(X, FUN= function(i){do_stuff(i)} )
I've been trying to expand my role into the HR analytics space. Just curious, what factors do you look at in #3?
The tidyr package should handle this. There are plenty of guides and cheat sheets online.
No guarantees, but if anything can do (or will eventually be capable of doing) this, then the [jailbreakr](https://github.com/rsheets/jailbreakr) package would be my best guess. 
Your example of the end result is missing a column, which is the "A/B/C" value that separates the assessments. But this process is very easy, you just need to use tidyr::gather(). Read [the vignette](https://cran.rstudio.com/web/packages/tidyr/vignettes/tidy-data.html) or [the documentation](https://cran.rstudio.com/web/packages/tidyr/tidyr.pdf).
Yep. I know that it's supposed to eventually have that functionality, but whether it currently exists or not is anyone's guess. 
It's pretty straightforward with Rcpp. 1) Run Rcpp.package.skeleton (or Rcpparmadillo.package.skeleton), move your c++ files to the src package. 2) Run compileAttributes('pkgdir') to generate the export files. You can do all of this manually, but in my experience, it's much easier to use Rcpp. A good reference is https://github.com/cran which contains a mirror of cran. Just look at any package that links to Rcpp.
A question about breaking down the data as above. Isn't the value of having the combination of actors lost if I use the above format? 
Huh. That sounds surprisingly .... simple, and actually .. doable. The solutions I was finding involved some fuckery with bioconductor, but I have no clue how to do that. Yours might actually work, and yeah it all depends on what threshold I take for black. How would you suggest I go about the "argon" image, since the black ratio seems to be very close to 1, or would it not really matter if that is the case, in your opinion? Also, since you seem to be familiar with image stuff in R: I need to copy/paste a scale to the bottom right of each image. Assuming my scales are "10x.jpg" "20x.jpg" "30x.jpg", how do I make it go through the home folder, check for "10x, 20x or 30x" as part of the string in the image title, and weld the small scale jpg to it? Thanks in advance, you've already helped me a lot.
The regression code was literally just: fit &lt;- lm(Y~x1+x2, data=data)
Finance Specialist / Controller: I analyze key business metrics for the upper managements / board of directors, or more generally anything they happen to ask.
Very, very basic, but yes, Python looks like the way to go. Well, it's an excuse to finally learn it - I could never get the point of learning programming just for the sake of learning it, but this clearly creates a needs-based problem, which will make it much more interesting. I don't think R is very strong when it comes to image editing/analysis just yet, but figured there's nothing to lose in asking. Thanks for your feedback once again :)
Glad to help :) Good luck writing your script! :) 
If your variable is continuous, lm() will need to estimate two coefficients, an intercept and a slope which you can envisage as a best fit line through a scatter of y~x. If your variable is binary then it dummy codes one level as 0 and one level as 1. It will still estimate an 'intercept' and a 'slope' but because the first level is coded at x=0 then the intercept is essentially the mean of the first level, and because the second level is coded as 1 then y=slope*x+intercept becomes y=slope+intercept, so the 'slope' is basically the difference between the means of both levels. If the variable is categorical with &gt;2 levels then it is effectively an extension of this, with the first coefficient the mean of the first level, and then each subsequent coefficient the difference in means of that level and the first. Hence for a univariate model with a categorical explanatory variables you'll get as many coefficients as levels in that variable. This makes sense since you could not put a best fit line on a plot y~x as x has no set order or fixed intervals.
Check out scikit-image in Python as well. Your images will be numpy arrays of greyscale values (0-255), which should allow for some straightforward quantification.
First, I appreciate the help. I used the code as recommended, but did not obtain results as expected. I"m looking for something like below: So the distance between 'location2' and 'location2' is zero while distance between 'location2' and 'locationx' is its respective value, e.g., 38, 81, 18. Could you provide additional guidance? 2 3 4 5 6 2 0 38 81 18 0 3 38 0 44 37 71 4 81 44 0 76 114 5 18 37 76 0 0 6 0 71 114 0 0 For instance, should I create an empty matrix then apply some distance formula with input values obtain from the table in original post? #Generate empty table DistTable &lt;- table(stations_dist$ID,stations_dist$ID) DistTable[DistTable==1] &lt;- 0 # Distance function distm(c(lat1, lon1), c(lat2, lon2), fun = distHaversine) 
I figured out the problem. All of my data was listed as type "factor" instead of "numeric". Once I changed the type I got results I expected since I didn't intend for the data to be categorical. What is an easy way I change all of my variables to type numeric?
What about more of a webservice environment so they don't have to install anything? Shiny lets you make a webservice that calls your R scripts: http://shiny.rstudio.com/ You could host it on the linux box you're using. Unless they need to do some local processing? Then you can create a batch file to run the script. But then you need R installed on their machines and it needs to be added to path. You could create an app in python that calls your linux R server via R serve: https://rforge.net/Rserve/doc.html but that sounds like even more work than shiny. If you just want to display pop up windows I would go with shiny. If you're trying to do something local on their laptops, we'll probably need more info to make a recommendation. 
I'm probably not explaining myself very well. Shiny doesn't run on the client, it runs on the server. The visuals created by your process are rendered on a webpage for your end users. Like this: http://shiny.rstudio.com/gallery/movie-explorer.html This lets you share 'dashboards' with your users instead of them having to install anything. So yes it's heavier on the server box, but much lighter on all the others. Plus you're already talking about getting all this stuff installed on your user's boxes, and they will need to have shortcuts created, R included in their path, and they can't mess with any of these things or the whole process will break. This sounds like an administrative nightmare!
thanks for the response. i know it's late.... but i used the Amazon SES service to setup a SPF record. so all good now :)
That is what I was hoping to do, Dr. Wickham. I am hoping to see a concrete example though---not planning on changing the C++ code at all
&gt; Dr. Wickham
Just look at any of the packages that depend on Rcpp
You should be fine as long as you use the package rather from doing any from scratch and keep it pretty simple. Here is the first in a nice series of videos https://m.youtube.com/watch?v=b5hgDPa7a2k
A potential solution using `dplyr` library(dplyr) df %&gt;% group_by(status, geo) %&gt;% tally %&gt;% group_by(geo) %&gt;% mutate(ratio = n/sum(n)) This will give you ratio of subscriber status in a geo as fraction of all subscribers in that geo.
i would just use the package. is there any book youd recommend as a crash course? im a book reader. i just want to crash course the basics so i can understand that package asap
I'm ignorant how to find this information (which is the motivation for this post). How do I search for packages that depend on Rcpp? Sorry if this is really elementary, but at the moment, I don't know how to do this. 
Oh thank you. I didn't notice that my formatting screwed up.
I did! It worked out just fine! I compared it to the results without the significance levels (cor(data, method""spearman")) and the results were the same. Thank you so much!
https://cran.r-project.org/web/packages/Rcpp/index.html Look under reverse dependencies.
Thanks!
This worked great! Thanks for the insight into dplyr. I haven't used this package before. 
I used it on the titanic dataset from kaggle, but I didn't see a fantastic improvement in my prediction percentage. Very cool package though! 
While R and python are pretty awesome tools, other open-source software for image analysis are available too, like ImageJ (FIJI). I'm a biologist, and I frequently use FIJI in conjunction with R for my more complex analyses. But for black and white ratios, I suspect it's very straightforward. I'm on my phone rn, but we use the built in threshold ingredients between contrasts for cell colony counts, and that can easily be manipulated to layer thresholds and compare them. I'll add that I tend to lean towards using R or python alone for image analysis too, but depending on your programming background and time constraints, you might have more luck using fiji. Best of luck! 
So there are two halves to this type of question. The first half is conceptual: "What sort of activities should I undertake, what sort of output do I want to aim for, what sort of visualisations do I want to create?" The second half is practical. "Now that I know the target I'm shooting for, how can I accomplish this with R?" This is a language sub, so most of the discussion is about the second type of question. Your question is the first type: they are much more general and have nothing to do with R in particular. So you may want to try a different community - perhaps, and forgive me for rampant speculation, you might find /r/homeworkhelp useful? So, with that said, these sorts of things are very simple to accomplish in R. Categorising and creating summaries of data is very easy in base R; I also like the workflow provided by the dplyr and tidyr packages so you may want to read some of their vignettes. For graphing, base graphics are quite powerful and there is an extremely good package called ggplot2 that provides excellent graphics as well. For predictive models, it's essentially just a matter of deciding which methodology you want to use, downloading a package that implements that methodology, and using that package's documentation to work out what functions to call. If you settle on a methodology, this sub would be a good place to ask for help with either finding a package, or using a package you've found.
data$Month means the Month column of "data", so you're basically saying "the subset of 'data' where 'data$Month' is equal to 2"
It is raw luck that "Temp" &gt; 27 works for you. I doubt that it's doing what you expect. 
Okay perhaps a poor example on my part, yes try to avoid using spaces where possible! Lets say you want to specify reading something called "beginning" in a book called "learning". You'd write that as learning$beginning - "learning's beginning". 
noted !
I think you could write the second line more simply by just dealing with the individual vectors as: data$Temp[data$Month==2] I think the first line could also be written more simply but not 100% sure what it's asking for, what is column 2 in your data frame as indicated by the **,2]** ending?
r-exercises.com
You should be writing your code in the top left pane, aka the editor. You might have to create a new file using the file menu. Then just select the code you want to run and click on run. Not sure about the last question.
Hah ;)
cast the numeric as a character or visa versa. Char to num casting may fail because numberic can be character, but not the other way around.
That did the trick! Thanks.
Good to know....
You don’t lose the combination of actors in this data presentation though — so nothing is lost: the exact same information (including relations) is present, it’s just in a different format (“tidy data”).
Do you have a function for performing lemmatizing or are you looking for how to do that as well? You might not be able to find too many people with experience with both. If you have a function you can use apply to quickly run the function across the columns.
Check [combs(v,k)](http://svitsrv25.epfl.ch/R-doc/library/caTools/html/combs.html). Is this what you are looking for? Or do you want the algorithm presented to you?
expand.grid(...) or two loops/applies/...
This isn't very elegant, but would something like this work? v1 &lt;- c("a","b","c","d","e") # elements v2 &lt;- c(rep(1, 2), rep(2, 3)) # 2 groups of size 2 and 3 ranGroups &lt;- sample(v2, length(v1), replace = FALSE) # randomize order of v2 assignGroups &lt;- data.frame(v2, ranGroups) After this you just subset using ranGroups = 1 and ranGroups = 2
Solve people's problems on stackoverflow [r] tag
I am using rpy2 with R-3.3.1. Previously I used it with R-3.3.0. Could you be more specific with what is not working for you? 
http://shiny.rstudio.com/ it's very intuitive -- for sake of ease I just stick to the templates and cut out what I dont need. 
As far as I know you always need an R session to run in the background for a shiny app to work. Shiny has its own severs where you can upload your app or you can use your own server, but I think the answer to both your questions is no.
You can also run your own framework for working with data. Shiny is just a bunch of helpers for dealing with JS/HTML and websockets.
Are you sure a line with those values should pass through those points? Anyway, the way I understand slope-intercept form, the following function should do it. line_segment &lt;- function(slope, yintercept, xstart, xend, ...) { ystart &lt;- slope*xstart + yintercept yend &lt;- slope*xend + yintercept lines(x = c(xstart, xend), y = c(ystart, yend), ...) }
simple solution: '[[' and '[' ie ZCP$text[[1]][1] ( '[[' works like $) technical details: '[' returns an object of the same type you are indexing, so when indexing a list (which you have) your get a list (of length one with your [1]) with '[[' you retrieve an individual element (i.e. without the list(...) surrounding it) just a heads up, many functions in R can operate on vectors that can simplify for loops away, compressing and speeding up code
Yes, the lines are more or less for visualization, its just with the way the data is I figured slopes would be easier. Regardless thank you very much for the code.
[This is what I mean](http://imgur.com/NTgU5eI); a line with that slope and intercept doesn't seem to pass through those points.
If the code is exactly the same, there is likely a discrepancy between your data set and your professors
segments() would make the code slightly simpler, and would automatically vectorise the function. 
Would you mind explaining how that would work? Wouldn't you still need to calculate the `y` values given only a slope, intercept, and x values for the two points? Edit: nevermind, I see what you mean. Thanks, I didn't even know about that function.
The solution is to use geom_segment!
I'd compute the summaries with dplyr and then plot. It's much easier to see what's going on that way. 
can you elaborate? how would i apply the data.frame function here?
The code in the error isn't the code you gave - it's not that line that has the problem.
I figured out, was a very simple mistake. It should be ...german_creditlr2$response instead of $Y
I've run into this problem with different types of analyses before, and what I did in the past was narrow down my dataset to the most informative subset that I could, and then work with that manageable subset of the data. Alternatively I've also run big analyses on a compute cluster and just requested an adequate amount of memory, but this only works if the analysis is only memory intensive. Often it will be both memory and compute intensive, and in that case things get more difficult and you either need parallel computation or you have to wait ages.
Great, keep me updated as well if you have any info! I asked this question on the /r/bioinformatics subreddit as well so keep track of replies on both threads if you'd like. https://www.reddit.com/r/bioinformatics/comments/52foia/anyone_have_good_ideas_on_where_to_host_data/
Thanks. So did you average out the estimates? Or did you always work with the model generated on that one subset? 
Statistical solution: Do you really need all 1.4m observations? Running regression on a sub sample may be enough. You might not even to run regressions across multiple samples and take and/or check their averages. Computational solution: try using a different regression package that supports larger datasets such as 'biglm` or use a different method of learning with support for larger datasets. 
Yes, that is one of the options I am considering.
Damn son, 100% test covarage! Looks very finished! I haven't checked the package in r yet but you seem to have all the right stuff.
`dcast` is a function in the package `reshape2` that converts data from "long" format to "wide" format. The `reshape2` package author has a newer package called `tidyr` that does mostly the same thing but I find easier to use. You can see examples of both being used [here](http://www.milanor.net/blog/reshape-data-r-tidyr-vs-reshape2/) and more on why you would want to move your data between long and wide orientations [here](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).
Example: dat &lt;- data.frame(my_values = c(50, 60, 70, 80), num_of_times = c(1, 2, 3, 4)) data_with_rep &lt;- rep(dat$my_values, dat$num_of_times) data_with_rep result: 50 60 60 70 70 70 80 80 80 80
Oh, ok, managed to use it for the calculations I needed then. Mostly basic stuff for an introduction to statistics class, so it's pretty standard, the only problem was the giant number of observations we had from our sequences, but this helped a lot, thanks!
well, write a function that solves the problem for one client and takes the client info as input, then run it for each client (via a table or list of clients). And the main function I would tackle by the order the tasks are needed, because that usually makes the most sense (ie. first implement the downloader, then the processing, then the adding to the master DB).
This is an interesting article! Could you give us a screenshot of what the output would look like?
Nice, thanks. Came in handy tonight. 
The Data Scientist Toolbox course on Coursera was really just an introductory course that summarizes and goes over general concepts in data science. There's no real in-depth learning for R there.
ah that's what I thought too, i'll follow it for a while anyway if they have some surprising tips
thanks
For me R courses in DataCamp are much better than Coursera for beginners. Coursera one is a bit too complicated and hard to follow for me
Oops I should've read all of the post. I skipped to the questions portion lol. I like Hadley wickham's "R for data science" available free online. 
Could you post a bit of sample data and what you what your are dealing with? If you had some NA values, you could do something like this df[which(is.na(df$TargetColumn)),]$TargetColumn &lt;- NewData
In the same exact boat as you. As with most coding and computing skills, the best way is to learn by completing projects. Now the question is how to find them heh.
Paste the output from executing head(length) 
First off, as a stylistic thing, you probably don't want to overwrite the length function by typing `length &lt;- length(csvlists)`. You will need to refer to the original function `base::length()` once you overwrite it in the namespace, or restart your R session. Anyway, let's look at your last line of code: data &lt;- lapply(length, function(x)(csvlists[[x]]$whatever)) You're applying a function to each element of a list called `length`. For each value `x` in `length`, you will index into the x'th data.frame of `csvlists`and get the `whatever` column out of that data.frame. Question, and something which /u/maximumbay is trying to lead you towards: what is the value of `length` inside that lapply? In the previous line (and what I alluded to in the first paragraph), `length` is going to be a number. `length(csvlists)` is just going to be equal to `length(&lt;vector of file paths&gt;)`. It's a number, not a list. Think of the difference between your lapply and my lapply: lapply(csvlists, function(df) mean(df$whatever)) Mine returns a list with the same number of entries as you have data.frames in csvlists, but each entry in the list is a vector of length 1, and it is equal to the mean of the `whatever` column from the data.frames. To get the "mean of means", you might be interested in the function `unlist()` which turns a list into a vector.
Create a data frame with the data. Say col_a has some of the numbers you want and col_b has some of the others. Then use: ifelse(is.na(df$col_a), df$col_b, df$col_a) Where col_a is empty, use col_b. You can do this in SQL with coalesce. 
thank you! this is exactly what I needed
1 .Primitive("length") Not sure to understand honestly
lapply(csvlists, function(df) (df$whatever)) Extremely simply, again thank you ! 
I just started myself. Checked out a Lynda.com course while doing some googling. Wanted something more hands on. Found data camp through this subreddit. Excellent crash course material. It was created by the team at Facebook to get more people into data analysis with R and/or Python. https://www.datacamp.com
Thanks for the quick reply - I've tried both cases and while I don't get any warnings, I no longer see any red lines. The summary data on my model seems reasonable, so I don't think it's an issue where the line is being drawn out of view.
hmm, this worked when I tried it with both of these toy data sets: # sample 1 data(cars) x &lt;- cars$speed y &lt;- cars$dist test.fit = lm(y~x+I(x^2)+I(x^3)) plot(x,y) lines(sort(x), fitted(test.fit)[order(x)], col='red') # sample 2 x &lt;- runif(50) y &lt;- runif(50) test.fit = lm(y~x+I(x^2)+I(x^3)) plot(x,y) lines(sort(x), fitted(test.fit)[order(x)], col='red') Maybe it's something particular to your data? Can you provide a small reproducible example?
I'm an idiot.... I am doing some indexing here and accidentally was using the index as my "x" instead of using it to index my data. All is good now! Thanks a lot for your help!
Ah, yes there is! By some strange occurrence, I'm actually doing exactly that right now. img_path &lt;- "..."; the_png &lt;- PNG::readPNG(img_path); rasta &lt;- grid::rasterGrob(the_png) Now you got yourself a raster array that can be used in grid.arrange along with any other grob object!
How neat is that! Thank you, I'll check it out then.
Is it possible to save base R graphics as grobs?
Thank you so much! :D 
I looks like you are giving the string "files" as an argument, and not the files object you just created. Run the function with just the object called files, not a string called "files". Ie, get rid of the quotes.
&gt;I wouldn't recommend learning a new programming language for it unless you know what you need and you're sure that R is the best way to do that. Although I'm not knowledgeable in network security, I think it very likely that you could apply statistical concepts to look at various aspects involved. As long as data is generated and captured in some way, R seems like a good way to start exploring the data and eventually doing more advanced statistical analysis. OH and as far as programming languages go, R is not very difficult to learn.
More info: ?rnorm ?mean ?for
You might want to look at the standard deviation for players, maybe averaging the batting average for each time the player was on and finding the difference between those averages would be interesting. Simple scatterplot with team and player can look like this once you get it formatted. &gt;p &lt;- ggplot(data.frame(df), aes(team, batting_average)); &gt;p + geom_point(alpha = 0.3,aes(colour = player))+ scale_color_gradient2(midpoint=mean(df$batting_average),low="#23c500", high="#e61e00", mid="#ffd600") Something like that might work.
A simple for loop. NumSamples &lt;- 100 SampleMeans &lt;- NULL for(i in 1:numsamples) { SampleMeans[i] &lt;- mean(rnorm(100,0,1)) }
You could avoid a loop entirely. (I'm not sure if this is homework and a loop is required.) n_rows &lt;- 100 n_cols &lt;- 100 dat &lt;- replicate(n_cols, rnorm(n_rows)) dat.means &lt;- apply(dat, 2, mean) 
And i am a damm *lazy* looper
If speed isn't an issue, then sometimes I'll use this function: is_error &lt;- function(x) inherits(x,"try-error") Then within the loop: if (is_error(try(read.rwl(list.rwl)))) { rwl.bad.files &lt;- c(rwl.bad.files,list.rwl) ## assuming you define this vector ## outside of the loop } else rwl.files &lt;- read.rwl (list.rwl) 
That bottom one: Error: object 'rwl.bad.files' not found ...means that you didn't initially define it. Outside your loop do: rwl.bad.files &lt;- character(0) # an empty character vector I'm not familiar at all with the dplR package, but I can't imagine why you would want to use a loop and set the output of read.rwl() to the same object. It will be overwritten each iteration of the loop.
You could also use readr::read_file instead of your custom function.
Got it! Didn't even see a change in order of operations. Thanks! 
You can apply multiple functions, one way to return them is as a following. ########################## df = data.frame(Student= c(1,2,3,4), Semester= c("9a","9b","9a","9b"),Grade = c(90,91,76,87),Name=c("J","B","G","P")) pd = aggregate(Grade~Semester, data = df,FUN = function(x) { a = mean(x) b = sd(x) cc = min(x) op = c(a,b,cc) })
You're a lifesaver. Thanks man.
This might help (on mobile, sorry for errors) Arr &lt;- array (dim=10) for (i in 1:10) { Arr [i] &lt;- i } print(Arr)
I would use a for loop that looks at ever index and compares the value to the value of the previous index. If you need more help can give you a rough outline of how to write a for loop like this, but I'm pretty new to R so this may not be the most efficient way. 
Thanks. How did you figure this out? Is this a skill that I should know?
Chrome developer tools, looked at the network tab, found a xml files. Also look for json files. 
I see you got help for your problem but if you want to learn the fundamentals of r you should try the swirl package.
Learn this now: Character matching is slow and you should avoid it whenever possible. Data &lt;- switch(Data, "E"=1, "S"=10, "D"=100, "M"=1000, "B"=10000) Dif &lt;- Data - lag(Data, 1) BDCount &lt;- sum(Dif == -9900) BECount &lt;- sum(Dif == -9999) BMCount &lt;- sum(Dif == -9000) SECount &lt;- sum(Dif == -9) SDCount &lt;- sum(Dif == -90) SMCount &lt;- sum(Dif == -990) 
Hello everyone ! Thanks for your answers. I've solved this problem on my own this morning and it's working fine. 
Wow, that's awesome /u/mattindustries! That's something I've had on my *figure out how to do someday* list for a bit. Cheers!
Thanks! That helped with what I was doing a lot.
It would help if you could produce an actual reprex since obviously the example.com URL doesn't exist. It's hard to reproduce the problem without having an actual file to download. There must be example XML files out there that you can use?
I am using a dataframe, and I am trying to apply the formula above to each unique code from each site, and store each value from each site into a vector. 
 # Get a column anno$Annotation # Convert column to character, it is presumably a factor right now as.character(anno$Annotation) # Split each character string in the column along ' (', that says space left parenthesis. # Double \\ is used to scape the ( strsplit(x = as.character(anno$Annotation), split = " \\(") # Iterate over every element of the list and extract first element of each list. # [[ is the subset function in R. Imo, it is meant to be used in the form x[[1]] # but a lot of people have been using it in *apply and higher level functions # The 1 argument is passed to [[ as i indicating that you want the first index lapply(X = strsplit(as.character(anno$Annotation), " \\("), FUN = "[[", i = 1) # The above line does the exact same thing as this more readable code lapply(X = strsplit(as.character(anno$Annotation), " \\("), FUN = function(x) x[1])
Thank you for your response! Although I'm still a little confused. I'm just trying to see how I can use this in the future, because I find it very useful and how I normally work around this is by converting the list to a dataframe and then taking whatever column or vector I want (which now seems dumb and tedious). So what exactly does the "[" do then? Let me ask the question in the form of an example: Like say I had a vector (v) of strings "5-0-1" and "4-7-9", it's easy to see how to strsplit them strsplit(v, "-") but what lapply command would I use in then to take the first, second and third item out?
Nevermind, understand it now with /u/rtyuuytr answer. Thanks though!
[removed]
Thank you! https://31.media.tumblr.com/f1d7436dac815e25ed26430aac72b007/tumblr_inline_n6u90aGbTS1qmjwd3.jpg
I still have no idea really what your data structure is, or what you want to end up with, and most crucially, why you want to do it. You need to describe the data structure you have (str(data_frame) is a good start but a fabricated example created in code that can demonstrate the principle is even better) and a similar construct showing your desired output format. Without that, I'm left trying to second-guess exactly how your data is structured and what you want, and that will get us nowhere. I'm not trying to be a dick, but without some concrete details and dare I say code that describes what your situation is, what you're trying to accomplish, and why, it's extremely difficult to suggest solutions to your problem. I could probably tell you more or less how to do the task you've outlined, but there is nearly definitely a way to do it far more efficiently.
You might want to check out Ari Lamstein. He wrote the choroplethr package for R and he's trying to build an R mapping community. Haven't checked out his classes, but his blog is nice. These days I typically use leaflet for mapping and the Google maps api for R for geocoding, but I'm not aware of specific communities for those. 
Leaflet is pretty fun. And customizing your own maps in mapbox is very easy.
Awesome, I definitely need to talk to this guy! Mapping and geospatial data certainly go hand in hand and complex satellite file formats like hdf I need all the help I can get. As an R newbie I also want to learn some of the statistical capabilities. An important thing I need to look into is data aggregation of several images and being able to combine pixel/wavelength values for analysis. I'm definitely in over my head but your help is useful, so thanks!
/r/homeworkhelp Even there though, just posting your assignment won't get you anywhere. If there's something specific you're having trouble with, ask about it, but very few people will just straight up do your homework for you.
Okay, then lets start from the beginning, shall we? I extracted a data set from csv and it set up as a dataframe. It has one column of data set as "site" in the form of a factor. I wish to perform that each of the sits are taken out with the rest of the data frame and put into a vector for each of the sites. Each vector has within it the data from only the "site" data that matches it. I tried to automate it through this way, where the factor being used to split up the data is the site. &gt; for (i in site){ vector[[paste(names, i:site , sep = ".")]] &lt;&lt;- subset(taxa1, site.c = site) } I need help finishing it up in order to do this task, or you have suggested, have a much better way of thinking about it. 
Use openxlsx. It's much lighter, no java. 
Sorry, new to posting in reddit, thought I had written text with my specific question but now checking seems not. Ill try repost. thanks 
I'm not a pro at this, but I do know that the instructor that's teaching me R and Machine Learning users the 'readxl' library, and has no issues reading and writing to Excel from within Rstudio. Maybe that will make things easier.
Kind of hard to say what it could be without seeing the data, but "quote = TRUE" may solve your problem as already suggested. Additionally, try coercing all of your columns to character class and check to make sure the names of your columns aren't duplicated.
http://stackoverflow.com/questions/2676554/in-r-how-to-find-the-standard-error-of-the-mean Don't use the direct formula like that, use the standard deviation and make your own function! Best suggestion from the link I provided up top: &gt; std &lt;- function(x) sd(x)/sqrt(length(x)) &gt; std(c(1,2,3,4)) [1] 0.6454972
Thanks for your response! This wasn't quite right, but your advice pointed me in the right direction to get what I wanted. You're correct that this iterates from the "last" (rightmost) column, and moves to the left. The tricky part is that the number of rows will decrease by 1 for the first iteration for each additional period you sample. I ended up changing the code to read like this, which works perfectly. averagePickup = function(data, day, periods) { # data will be your Pickup Data # day is the day you're forecasting for (think row number) # periods is the period or range of periods that you need to average (a column or range of columns). pStart = ncol(data) pEnd = ncol(data) - (periods-1) row = (day-1) new_frame &lt;- as.data.frame(matrix(nrow = 1, ncol = periods)) q &lt;- 0 # Instantiated a q value. Run 0 will be the first one. for(i in pStart:pEnd) { new_frame[1,1+abs(ncol(data)-i)] &lt;- mean(data[1:(day - periods + q) , i]) # Added a subtraction of q from the row number to use. q &lt;- q + 1 # Incrementing q, so the next time will use one less row. } return(sum(new_frame[1,1:ncol(new_frame)])) } This also eliminates the need for the `row = (day-1)` argument, since I'm just calling `day` directly now. 
Looks good to me. Happy to help. :)
I was absolutely shocked that standard error isn't a base function in R. For a while I implemented my own, but then I had to remember to source that file or copy/paste that function into nearly every script I wrote. So I found a package that has one. The `plotrix` package has a `std.error` function that I use regularly.
Interested in knowing more on this. 
Can you throw some more of the code up?
If I recall correctly, it's just a REST API with JSON. Take a look at Python packages that pull it to get an idea on how they do it. I don't recall anyone compiling an R version...but I've thought about it before. Could be a fun colab project?
 amountbyfactor &lt;- function(df, criteria){ levels &lt;- levels(df[,criteria]) bycrit &lt;- unlist(lapply(levels,function(x)sum(df[,criteria]==x,amount))) result &lt;- as.data.frame(cbind(levels,bycrit)) names(result) &lt;- c("categories","amount") result$amount &lt;- as.numeric(as.character(result$amounts)) return(result) }
My guess its insufficient user rights. Ask IT to run as administrator as a test.
Specify your .libPaths() to C:/Program Files/R/.versionhere./library/ .libPaths("C:/Program Files/R/...")
Something that helps me occasionally with installation issues is to go to the CRAN page and download the zip. Then use the Rstudio package installation wizard to install from zip. 
Then something is still running somewhere. Start in Safe Mode or your platform's equivalent.
Well, I should have said that I haven't used rPython, so I don't know what the standard method would be. The reason I suggested it is that it seemed like what you want to do is load some R code, which then loads python code. You could write a package for that, but it seems like overkill, unless you need the name space for some reason or you want to put it on cran. `source` is pretty standard for loading R code from one script into another (for example a long script of function definitions). 
If you want to know how to create an R package in general, you should read [Hadley's book](http://r-pkgs.had.co.nz/) on the topic. The first thing you need to do is be sure you actually need a package. Packages are useful because they have versions, they can have dependencies, and they (should) include their documentation. They also crucially keep all their inner bits in a separate namespace; things in a package can be accessed using package::function() or (package:::function(), carefully!) without having to import the entire namespace. Even if you do import the whole package with library(), you only import those functions that have been explicitly marked for use; purely internal stuff remains hidden. Packages are, however, a lot more work than maintaining an R script that you just source(). So with that said, in your particular case, creating your package is very simple. After creating the usual package gubbins like a DESCRIPTION and so on (RStudio automates some of this btw), you will drop you python code somewhere in the package's inst folder, have some very simple R code to call it, exactly like the one you quoted, and then export those R functions from the package's namespace. That should be all that's required. If you don't understand that terminology, read the book.
I don't think you can test this. And personally I don't think it makes sense. If H0 is cor(a,b)=1 then this does not allow any variation. Say you have two vectors: a and b and cor(a,b)=0.98. This already has p-value of 0 since it's impossible to obtain such data under H0.
Oh I didn't know that a correlation of 0.98 has already a p-value of 0. In my research, the correlations are very high (~0.9) and I thought this might be another way to show it. Thanks mate! 
If you have packages loaded from your `~/.Rprofile` on startup, suddenly your script is no longer reproducible for colleagues. 
Hmm, I'm not sure, but maybe the [rank](https://stat.ethz.ch/R-manual/R-devel/library/base/html/rank.html) function could be the one you need? And maybe the answers to [this](http://stackoverflow.com/questions/29534780/r-calculate-rank-sum-automatically) SO-Post help? Otherwise I'm out of ideas.
The [googleway](https://github.com/SymbolixAU/googleway) package might be able to do what you're after, but you'll need a Maps API key. 
Assuming Windows 10 - https://www.google.com/amp/www.digitaltrends.com/computing/how-to-set-default-programs-and-file-types-in-windows-10/amp/
Not true, you simply host your package on Github and they can then install your package using `devtools`. Whether they choose to load it globally on startup via `~/.Rprofile` remains their choice. This is actually more structured and organised than sharing your code as a text file allowing version control and others to collaborate and enhance your function. This is described in the linked book.
Yes, make a function. You don't need to explicitly throw an error, you can just define the function argument and not give it a default. R always throws an error when functions are called without arguments with no default. Also, it may be useful to make a package instead of just a script if you plan to distribute your code.
You should look into setting arguments on the command line with commandArgs(TRUE)
Give users as few things to fuck up as possible.
It won't make a difference. Your p-value is p-value &lt; 2.2e-16.
Yeah but can I use this p-value? I'm not sure since my data is not normally distributed and a t-test assumes a normal distribution.
Have you thought about transposing the csv? You could read it linewise then, reading 100k lines to memory, retransposing it, if needed, and work with it.
if the rows are long: readLines() with skip and nrows + strsplit to split the columns and as.numeric or whatever you need to transform it into the format you are working on. read.csv will be much slower because it tries to figure out what data type each column has. if you have lots of rows do what owls_with_towels said
Thanks! 
Is your issue that you're running out of memory, or us there a reason you want to segment the analysis otherwise, because the memory problem can be solved
I am relatively new to programming too. Started learning r from the data science specialization last summer. I am not really confident with programming yet but i tried to do something interesting and fun. I am glad you liked it and i think its easy for you to modify the code and do something that you like. I thought of doing 2 wordclouds for 2 rival companies to see how they are marketed amongst their customers. Although, i dont really know how to exclude tweets from that companies. Anyway, you learn programming by doing projects so dont be afraid to have fun with R.
**My own solution** The `na.omit` option of predict will remove a row if there are `NA` values, but perform the prediction for all other rows. In the real data set, there were many columns, but only 2 were used for the decision tree (Var1 and Var2). If you inspect the fit result `rpart_fit`, you can find the fitted columns by vars_to_keep &lt;- subset(rpart_fit$finalModel$frame, var != "&lt;leaf&gt;", select "var") print(vars_to_keep) ## var ## 1 Var1 ## 3 Var2 You can then perform the training again, now with only the necessary columns. subset_training_data &lt;- subset(training_data, select = c("Class", as.character(vars_to_keep)) subset_rpart_fit &lt;- train(Class ~ ., subset_training_data, method = "rpart") This gives the exact same tree as before. We can now make predictions with `na.omit`. predict(subset_rpart_fit, newdata = test_cases, type = "prob", na.action = na.omit)
I think you may need to be more specific than what you've mentioned so far, medical statistics is likely a rather specialized field. I imagine you'll need to find resources specific to that topic. As for more resources for general R, make sure to read Hadley Wickhams papers when relevant, he is a major developer for R and has a lot of libraries you'll commonly use.
I would pick any of those. I'd also consider [swirl](http://swirlstats.com/). The point is that no matter which one you choose, be ready to put a few weeks of consistent effort into it. R is worth learning, and just like anything else it takes time.
You can't use IIS. It doesn't work. I'm just running an rscript on a dedicated virtual server hosting to a local/online address. But I don't expect more than 2 concurrent users or more than 3-4 apps. Idk what you're situation is.
I saw that with the university distance-learning courses. Might be able to find something more specific to what I'm looking for in one of those courses. I'll check out ggplot as presenting and publishing my data in visually is a must. Thank you
Exe for the R script? You mean you are using Apache or something? Same situation for me, it's basically a simple dashboard for 5 people but needs to be refreshed every 5 minutes. 
No. I am running the R code that contains the runApp() function with Rscript.exe so it opens a cmd window to service the app without an IDE. Note that this method is not scaleable if you expect more than like 5 concurrent users on any number of your apps. The only reason I'm doing it this way is because you have biz intel software that people have full-time jobs developing, and I don't know anyone using Shiny that is an actual CS dev. 
I will check it out!
Why aren't you using shiny server?
Because it would be nice to have AD auth and SSL and I don't want to pay for Pro for something so simple. And I know nothing about Linux.
How would you say your methods stack up to those featured in optim?
I'm no security wizard I'm afraid but can't you restrict a domain behind a proxy? I didn't knew anything about Linux either when I set up my server but following the guide was pretty easy. You also have an Rstudio server version so once you're set up you won't have much dealing with Linux
I would typically use something like NewData$pred&lt;- predict(Model,newdata=NewData) is that giving you the error? What's the actual error code?
You're saying the *file* is just rows of text data like that? Read the text file as strings and wrap sets of JSON data as {event: dataTable.dataRows, id: 233, data: [{nested JSON}]} Create a vector of strings in that format. Then parse using the [jsonlite](https://cran.r-project.org/web/packages/jsonlite/index.html) or [rjson](https://cran.r-project.org/web/packages/rjson/index.html) packages. Depending on how badly I misunderstood your post, you may be able to use those libraries to read this data directly but that is certainly one interesting non-conforming dataset you scraped.
I should clarify that the data comes from a GET request I make once I am logged into the site. The response headers indicate the data is Content-Encoding:gzip Content-Type:text/event-stream. Does that change anything?
Do you get one dataRows per GET or do you get a collection of them? Could you show an example response? It would help understanding what needs to be parsed better.
It depends on what I request with the GET. If I don't filter down the requested data I get multiple chunks of dataRows with the get if it is a large request. For smaller requests I might only get 1. I will get an example asap. I have to strip out the data.
I'd build another service that parses the Json into a format that's nicer to work with in R. Check out nodejs and express - it's very easy to set up a service that can take your Json data and return a nice csv string, or whatever other format you might want.
Honestly if you are about speed, learn ggplot2. Once you understand the grammar of graphics you'll fly through everything you just listed. It's well worth the time invested. 
This is the meat of your problem: &gt; Warning message: 'newdata' had 3143 rows but variables found have 162 rows Does countrystats have all of the variables in your model? **Make absolutely sure the variable names in countrystats match the variable names in the model formula exactly.** If it does, do the variable columns have a lot of NAs? 
Sorry, on my phone now. I guess I'm just confused why the number of rows is relevant at all. I literally just want to multiply the coefficients I already have by these new columns. It would be so trivial to hardcode in a big for loop where it is like new-prediction[i] &lt;- intercept + coef1*columnA[i] + coef2*columnB[i] ... And just manually type the coefficients or refer to them using Clinton.Predict$coefficients[1]. This seems like it should be really easy and just don't get why the number of columns is relevant...
The number of rows isn't relevant in the prediction task, it's relevant when you are trying to construct a new column on the dataframe. Your original training data was 162 rows: the predict function isn't scoring off of the newdata parameter, it's just returning the scores from the training data. The error with the row counts is telling you that the predict call didn't return 3143 records like you were expecting, but instead only returned 162 (i.e. the scores associated with the training data), and R doesn't know how you want to assign those 162 values to the 3143 row dataframe. It is extremely likely that the reason this is happening is that the newdata isn't compatible with the model. You probably didn't subset the data the way you think you did. Another sanity check you can do would be something like: all(sapply(names(training_data), function(x) x %in% names(newdata)))
I'm not trying to construct a new column on a dataframe though. Can't I just output a vector of 3143 rows where each row is essentially the sumproduct of the coefficient vector from my model and the matching entries in the columns of countystats? I just did this manually below since there are only 6 coefficients (plus the intercept) so it wasn't too difficult and it gave me exactly what I wanted, but I feel like there is a more proper way to do this... Clinton_Pred_frac_byCounty &lt;- double(nrow(countystats)) for (i in 1:nrow(countystats)){ Clinton_Pred_frac_byCounty[i] &lt;- Clinton.Predict$coefficients[1] + Clinton.Predict$coefficients[2]*countystats$PercFemale[i] + Clinton.Predict$coefficients[3]*countystats$PercOver65[i] + Clinton.Predict$coefficients[4]*countystats$PercWhite[i]+ Clinton.Predict$coefficients[5]*countystats$PercCollegeGrad[i]+ Clinton.Predict$coefficients[6]*countystats$PopPerSQM[i]+ Clinton.Predict$coefficients[7]*countystats$HomeOwnRate[i] } Those columns PercFemale, PercOver65, etc. are the exact columns that the lm Clinton.Predict was run on originally, and they are in both the training data frame of 162 rows and the countystats data frame of 3143 rows. The 162 row data frame was defined using a which statement on the countystats data frame as given below, so the columns all match exactly. training &lt;- countystats[c(which(countystats$state_abbreviation == "IA"), which(countystats$state_abbreviation == "NV"), which(countystats$state_abbreviation == "SC")),]
&gt; I'm not trying to construct a new column on a dataframe though. Then what do you call this? countystats$Clinton_pred_frac &lt;-predict(Clinton.Predict, newdata = countystats) &gt; Can't I just output a vector of 3143 rows where each row is essentially the sumproduct of the coefficient vector from my model and the matching entries in the columns of countystats? Absolutely. The problems with this aren't technical, they're stylistic and architectural. * Your code won't be nearly as easy to read. * Your code is more likely to have some disguised bugs. * Your code won't be nearly as portable. The latter point I think is the more important one. If you change your model, say add or subtract some terms, or change their order (since you're indexing them numerically), or change the underlying model (maybe you want to use a non-linear regression), you have to change significantly more code. And the more code you change, the longer it will take and the more likely you are to make mistakes. You should figure out why this fairly simple operation isn't working. It's entirely possible that whatever the problem is will still be a problem if you calculate your predictions the "hard" way. One last point: that for loop in your demo code is bad form. If you wanted to do it that way, you could totally vectorize the operation, which would be way faster. ---- All that said, try changing your code to something like this: training_ix &lt;- which(countystats$state_abbreviation %in% c("IA", "NV", "SC")) form &lt;- "your ~ formula + here" mod &lt;- lm(form, countystats[training_ix, ]) y_pred &lt;- predict(mod, newdata=countystats[-training_ix, ])
I was under the impression that the scraper was a separate service that his R code was calling. Not sure why I made that assumption.. You're right.
if you are starting with ggplot2, this addin will prob help you: https://github.com/calligross/ggthemeassist 
There's no real contribution here, this looks more like a class project on building an R package. You've thrown together a few of other people's packages without any real documentation of when one might be better to use than the other or better than optim for that matter. When asked, you don't seem able to answer the question either, which makes one worried whether you even understand the functions you are calling well enough to be able to properly interface with them. On top of that, there are several other packages that try to provide a unified interface to a host of optimization libraries. A proper contribution would be to try to contribute to one of those projects instead of just cluttering the field with yet another interface.
RStudio is good, have built-in editor, console, help and variables scope. Also have some useful shortcuts.
I have Shiny Server running on Ubuntu 16.04 now. Used VirtualBox first and had weird issues but VMWare Player 12 is working great. I looked through the admin guide but can't figure out yet how to get a different "location" working and if I can just put my ui.r and server.r files in the respective folder.
How do you want to handle collisions such as x[1,1] = 1 and y[1,1] = 2?
I am creating a package with R functions. These R functions use python via rPython. However, I need python libraries loaded first for those functions to work. The code (when using rPython interactively) is: library(rPython) python.exec("import python_library") How do you do this when creating an R package? 
Knew someone getting their PhD. I wanted to ride bicycles, but they had to write R scripts to analyze their findings. I learned R so because it was going to take me a lot less time to write the needed code. I went to [learnxinyminutes](https://learnxinyminutes.com/docs/r/), looked up some other info for subsetting, the rest of the apply functions, parallel packages, etc. and have a job writing R scripts now. I like it. 
Took courses on both Python and R. Loved Python and wasn't a big fan of R so ignored it; preferring to use Python for everything. However, I often deal with tabular data and data visualization, as well as statistical analysis. So I finically got off my high horse of "R isn't a nice language" and started learning it and how to use it. Turns out it's pretty nice for data analysis (especially with tidy verse!) I'm only at the beginning of my usage of R but I'm reading guides and blogposts and applying and expanding what I learn there to my own data on a regular basis. 
Started a job using matlab to do some image processing, though the data extracted was analysed in R. Picked up R which seemed relatively sensible compared to matlab. I then used R throughout gradschool, convinced others that they should give up excel/SPSS. Really got into it, started teaching others, creating packages and attending R users meetups. Started another job which used python for everything. Picked up python and realised R has plenty of warts. I now mainly write python code, though still write and maintain some R packages.
I'm fairly certain your job opportunities won't be very good in 10 years if you don't know how to write code (in certain fields). I have finance background, and it's already absolutely obvious that you have huge advantage in the job markets if you know your way with the tools. I work in management accounting, and pretty much every job ad screams "please please PLEASE have knowledge about R or Python, and SQL". So I kinda just figured to get it over with.
Only ~25% of responders to the last O'Reilly annual data scientist salary survey had a PhD. About 40+% only had a master's 
That's interesting. I wonder if they're also classifying "data scientists" in an odd way. I for example got my UK BSc this week and I've been a data analyst for 9 years ;) But I would consider myself to be very far from a data scientist.
Most likely - it's become a buzz word, so there are plenty of "data scientists" out there that just use Excel all day. They created a few clusters of the respondents, and went over the tools used in each cluster. There was definitely one that was similar to what I described, but I can't remember the proportion off the top of my head. Regardless, you should check it out, it's pretty interesting! 
I already had a basic understanding of programming from being self-taught in web development. When I started grad school in psychology, pretty much everyone used SPSS, and that's what I learned statistics in. But eventually I heard about R, and as I looked into it I realized I could use it to deal with many of the frustrations I had with SPSS. A couple summers ago I took the time to learn R -- the best way I found was to re-analyze a dataset I had already analyzed with SPSS and try to duplicate all the results with R -- and fell in love. I've since suckered a few other grad students into learning it, and also gave a one-day workshop for students and faculty. But I now use it for about 95% of my analyses, and have authored a couple of packages as well in my spare time.
It's nice to read these stories. I am 32, decided to back to school, and am in a statistics class that has an R lab component. It's been difficult but I'm starting to feel more comfort and will hopefully be able to use R to pull meaning from data.
Under the hood, hadley's stuff is using the lazyeval package. You can read its docs to learn how it works and go to town. You could only use this to replace the variables vector and not the countries vector to my knowledge, though. Also, you might not actually need to even use lazyeval in this case... can you just pass the dots through to select? spii_sub &lt;- function(dat, ..., start = 1970, end = 2014, countries) { #V0.1 #variables must be a vector of strings of columns of original data #start and end must be years in the form of integers #countries must be a vector of country names with precise capitalization new &lt;- select(dat, year:country, ...) %&gt;% filter(country == countries) %&gt;% filter(year &gt;= start &amp; year &lt;= end) } Perhaps this will just work?
thanks for the suggestion of lazyeval and the dots which should work! is the issue with the countries vector the fact that i'm using it in a single boolean? do you know of a different way to do this where i can use lazyeval?
Thanks for the response. By the way, I saw that post of yours the other day, though I can't remember where. Did you post it to dataisbeautiful?