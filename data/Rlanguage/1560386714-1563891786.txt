It sounds like this could be a function.
Okay people, this started up the Python vs R arguments. Honestly as someone with ~5 years experience with both (which isnt much, but most people try to learn both and decide that picking a 'side' in the argument is easier than actually learning both languages). I would say anyone who says either language is better than the other is wrong doesnt know what they're talking about. They both have their strengths, you should use them in entirely different cases, and if you cant find the strengths/weaknesses of both then get out of the kitchen.
Well, to be fair, R being an evolution of SQL is about as accurate as the pictures on the top being used to depict biological evolution...
How do these companies stay competitive?
You want to group by the ID and Date and then summarize using the mean before plotting: df %&gt;% group_by(Date, ID) %&gt;% summarize(daily_avg = mean(Score)) %&gt;% ggplot...
I think you may have misread the post. I'm looking for a solution for how to combine multiple IDs into one. Would I just rename values or is there a way to aggregate them so that I can maintain their identities? Thanks
Can you give an example of the output you want to see?
I don't think I misread it, I am just not understanding what you're asking. Are you asking for the averages of A, B, C, and D for each day? That's what that code will do... If you want just a daily average of all scores on a day, only group by Day and ignore ID altogether.
I may have phrased it poorly--that's my bad. So I'm looking for the averages of [A and B] and [C and D] for each day, rather than A, B, C, and D as individuals.
The x-axis of the [box]plot would have two levels, the combined observations of [A and B] and [C and D], with the values being the average scores for those two groups each day.
How does any dinosaur stay in business? Uncompetitive markets, captured consumer base with high momentum, regulation making market disruption difficult, lack of interest from startups &amp; VC, etc etc.
I think you want the forcats library and its function fct_collapse().
Does removing ID from the group by in the other response help?
Fair, some parts seemed interesting but when they said it was Excel I was surprised, it was an excel test to give charts and tables.. It seemed very easy but its part of a government
Thanks for the recommendation, will try it :)
Add a new column that creates a new ID where it’s “A” for A or B, or “C” for C or D. Group by that column with dplyr and summarize by average. df$new &lt;- df$ID df$new[df$ID = “B”] &lt;- “A” df$new[df$ID = “D”] &lt;- “C” df %&gt;% group_by(date, new) %&gt;% Summarize(avg())
OK gotcha df %&gt;% mutate(grp = ifelse(ID %in% c("A", "B"), "AB", "CD") %&gt;% group_by(Date, grp) %&gt;% summarize(daily_avg = mean(Score))
Yep -- this will do it. Though if you want to keep it "tidyverse" and piped, use mutate with an ifelse like I did in my other response
Sweet. That’s the output I’d want. Ifelse works well here since there’s only 2 groupings being made. But, the example I gave was just a subset of my full dataset. If there are multiple groupings, how would I go about doing that? Thanks
Nice! So for multiple there are a couple options. you can use multiple ifelse: df %&gt;% mutate( grp = ifelse(ID %in% c("A", "B"), "AB", ID), grp = ifelse(ID %in% c("C", "D"), "CD", ID), ... ) %&gt;% group_by(Date, grp) %&gt;% summarize(daily_avg = mean(Score)) Alternatively you can bust out some forcats::fct_collapse df %&gt;% mutate(grp = fct_collapse(ID, AB = c("A", "B"), CD = c("C", "D"), ... ) %&gt;% group_by(Date, grp) %&gt;% summarize(daily_avg = mean(Score))
Thanks again. I’ll play around with those options.
^^^^^ this answer is much cleaner.
I'm not sure if this is similar to what you are trying to accomplish or not. d &lt;- data.frame(text = rownames(mtcars), stringsAsFactors = FALSE) d$raw &lt;- lapply(d$text, charToRaw) d$encode &lt;- lapply(d$raw, base64enc::base64encode) d$decode &lt;- lapply(d$encode, base64enc::base64decode) d$text_back &lt;- sapply(d$decode, rawToChar) head(d)
If they're integers: df$variable &lt;- 1-df$variable Otherwise first make them integers and then do that.
What Package are you using GGPlot2?
I don't know which function you're using but can you not set the `zlim` argument to the limits you want?
Looks like base graphics. Like `fields::image.plot` or something.
Looks like base graphics. Like `fields::image.plot` or something.
I am using 'Spatstat' for the kernel density mapping.. Nothing special for the actual final map making. Was just using par(mfrow ) to organize it
I need to jump into GGPlot2 I keep reading about it.
I will try that. Thanks.
The data is a point pattern stored as ppp. the circle is a the boundary I need stored as owin.
This works fantastic, like 11/10 goodness right here. Can you explain how this works?
Subtract 1 and then multiply by -1?
Is it possible that your default.stringsAsFactors() has changed from TRUE to FALSE?
Is there any way to add a unique identifier column to each file? Ie, file 1 would have a column added that contained all 1, file 2 would have a column added that contained all 2, etc.
Yep you can add the .id argument. .id =“file”
No, that definitely hasn't changed and was the first thing I check : /
Chandelier is right. Just set the zlim argument the same for all figures zlim=c(min(of all figures),max(of all figures))
My guess is the website you are retrieving data from has changed their formatting. I use R and read.csv() regularity and haven’t encountered this problem.
I assume the order in which this function goes in is dependent on the order of the strings in the "files" variable? That would be useful to know so I can get the date the file was run in there. Also, can you explain how this code works?
I thought so too but I checked it against data that was downloaded over a year ago and is stored on our server. I wrote a function to output the variable name and class to build a data dictionary and when I check it against that (which I built earlier this year) I can see many variables were stored as factors and are now characters
Use the seed argument to ensure you are using the same resampled data each time. Otherwise, you are using different data each time the resample function is executed.
Datacamp.com is your best friend for the next week...you can do most of the intro modules for free
On the phone so can’t really explain. Google ‘read multiple csv files map_dfr .id’
I think I get it, would adding the name of the file work? We just need to change what we iterate along then. test &lt;- seq_along(files) %&gt;% map_df(function(i) { df &lt;- read_xls(files[i]) df$file &lt;- files[i] df })
That's perfect! How does your code work? I am noticing that the variable "df" has to be called out at the end.
Yes, in a function the last line of the function is the thing that returns. This is the same thing as writing `return(df)`. The way that this works is like this `for` loop: result &lt;- list() # list to accumulate our data frames for (i in seq_along(files)) { df &lt;- read_xls(files[i]) df$file &lt;- files[i] result[[i]] &lt;- df # add to list } test &lt;- do.call("rbind", result) # stack all data frames Does that make sense? Can I elaborate more?
Would you say a for loop is a less efficient way to handle something like this?
Yeah, I would say so. In general, when you have to grow an object in size, like we did by appending to `result` in the `for` loop version, this is slower and less efficient. I also just realized we didn't need to change what we iterated along. This works too and is easier to read/understand: test &lt;- files %&gt;% map_df(function(file) { df &lt;- read_xls(file) df$file &lt;- file df }) Not sure what I was thinking lol.
This is why I import everything in as a character, and then change to my uniform format in my code after it has downloaded. I agree, it must be on the sites end that is causing the change.
It's the best way
It's just a meme.
I highly doubt read.csv has changed significantly. I suggest using read_csv instead anyways and manually setting the cols variable to avoid something like this in the future. I am going to assume that something in your data set changed and caused R to guess the column types incorrectly. Even a seemingly trival change in your data set could cause R to guess the column types differently than before and I bet that is what happened. Even though your data may seem similar to what you had before sometimes it does not take much to cause R to behave differently. So do your self a favor and always manually set the column type when reading in csv data.
I've updated my code to do just that but what I'm saying is that I've even imported data from a file that hasn't been changed in a year with a script that hasn't been changed in a year and it is behaving in the same way as my current data and current script. So I really don't think it could be a change in the file or the script as I can see in their properties that they haven't been used/updated in a year.
Specifically you can use: set.seed(*insert an integer here*)
MFW I use DBI w/ ODBC
Laughs in small data
I've been on Reddit a long time. Been a while since I've seen someone so utterly deconstruct an argument. Nicely done.
Interesting, maybe there was an update to that function, I will need to look at the change log for base R. Did you change R versions since then?
Lol I recognised 1C from their games distribution, had no idea they did office stuff too.
It’s possible but I can’t recall
I think it reflects a transition I made and I assume a fair amount of other people made when it came to tools for analyzing data.
You’re using `subset` wrong. Look up some examples online. I usually use `dplyr::filter`, but you can do it the old fashioned way: `data = data1[ data1$var1 %in% c(703:709, ...), ]`
 raw.df %&gt;% filter(Location1 == "Indoors", Sent == "Y") %&gt;% mutate(Vgrade_group = fct_collapse(Vgrade, B-0 = c("B","0"), 1-2 = c("1", "2"), 3-4 = c("3", "4"), 5-6 = c("5", "6"), 7-8 = c("7", "8"), 9-10 = c("9", "10"), 11-12 = c("11", "12")) ) %&gt;% group_by(Date, Vgrade_group) %&gt;% summarize(total_sent = n()) %&gt;% ggplot(., aes(x=Vgrade_group, y=total_sent, fill=Vgrade_group))+ geom_boxplot(notch=FALSE)+ labs(y="Problems climbed", x="Vgrade", title="Average number of problems sent indoors per session")+ stat_summary(fun.y=mean, geom="point", shape=20, size=3, color="red", fill="red") + theme(legend.position='none')
`set.seed(1337)` for life.
I actually started my engineering career 25 years ago with writing code. When spreadsheets became popular I moved to Lotus 123 and Excel. A few years ago I returned to writing code, using R.
It’s evolution of the data analyst, not evolution of the tools
Yeah for the most part it doesn't make sense, but I applied for a job at a fortune 20 company a few days ago in analytics and they said they use SQL for their analytics, and that I wouldn't need R. SQL is great for what it does, but trying to use it solely for analytics grinds my own gears because it's so limited
Excellent! It doesn't quite do what I want, but it gives me enough to where I think I know where to go from here. Thanks!
Lol in Lolcode
It's like comparing a hotel to a plane. You might prefer spending time in one or the other but your holiday will be pretty crap if you remove either.
No I have multiple projects and my boss will ask for me to tweak my analysis all the time or ask for new analyses of the same dataset. I get kind of anxious opening up a huge .Rmd file so I end up with a bunch of scripts but maybe a huge .Rmd is the way to go. &amp;#x200B; Maybe I will write a package for it. In my experience I end up spending so much time making the package compared to writing code. But I only wrote 1 package so maybe that was just me getting up to speed.
It may be worthwhile to write your "scripts" in a way that is conducive to tweaking. Since you know your work and your boss better than I do, that would be up to you to figure out. I can tell you in my personal situation, I have written a package that encapsulates many things I seem to repeatedly do so then I can focus more on the "one off" requests. I try to write my analyses using functions as best I can to help anticipate the "tweaks" but it's not always possible or worth the time to think through all of that when you just want to get the analysis done.
Subjective of course. It was interesting to read the part about language unity and Tidyr. As a relative beginner, I think the Tidyverse makes a lot of sense and is very intuitive. However, I do understand what he means by language unity potentially pushing away people from using R's basic functions. I feel now that I'd rather use Tidyverse functions than try to figure out a way to do the same thing in base R. I never really looked at Tidyverse as "having influence", but rather the Rstudio team making R a better place. I do want to check out the Art of R programming now, though, to see how Matloff utilizes base R.
tl;dr I like both languages, both have strengths and weaknesses, but this post doesn't really tell you much about them.. &gt; Object orientation, metaprogramming &gt; Slight win for R. Hah that one is crazy! I have no idea how object oriented programming works in R (admittedly haven't tried extensively), but in Python it is very easy to create objects and apply inheritance. I'm not creating my own objects very often when working on data science, so at least for me it isn't a big deal, but Python well beyond wins out here. &gt; I'm annoyed by the fact that I cannot print a function to the terminal, which I do a lot in R. As a nitpick `ipython` you can easily ~~print~~ page the whole function / class / module (!) with two question marks: import seaborn seaborn.matrix.ClusterPlot?? # view whole class on how a clustered heatmap is drawn -- Also the available libraries section is pretty poor. In Python most basic data science tasks (clustering, machine learning, regression) is accomplished in the `scipy` stack. So its weird for the author to complain that they couldn't find a nearest neighbor code on PyPI, when the first google result (for me at least) for "python nearest neighbor" comes right up with [scipy's well written documentation on all nearest neighbor implementations available](https://scikit-learn.org/stable/modules/neighbors.html). R has many more packages for doing "data sciency" type stuff than Python. Don't get me wrong, there are some really fantastic R packages and I'd argue that R has a wider selection when it comes to cutting edge statistical or biological procedures. But this section is way off point.' -- I also think the "learning curve" section is poor. It isn't very difficult to get up and running with `numpy` matrices. I don't know what it means to say R is ready to run out of the box when there are so many dead simple tutorials and installations for Python that includes the `scipy` stack. Also it's weird they jump to saying "huge win for R" when their previous section points out how "sleek" Python is. Further, I always feel like there's a bunch of different ways to do things in R, while in Python there's usually just one or two. I suppose in both languages you figure out best practices for yourself and stick with them, but I strongly feel the learning curve is much lower in Python than R - particularly in base Python - it's become my go-to data and file wrangling scripting tool. looping through files in Python: import glob files = list() for file in glob.glob('*csv'): files.append(file) list comprehensions: critical_files = [x for x in files if 'important' in x] This stuff is so intuitive in Python (admittedly to me because I've learned it). I've struggled to do similar things as easily in R. I know `tidyverse` gets you a long way, but this isn't base R, which the author raves about as fantastic (as opposed to Python where you *gasp* have to install numpy first). Having to install a few packages in this day and age is a moot point. You're going to be doing so anyway. Not to mention the standard library available to you in Python, along with the built in types (lists, dicts, sets) that are so useful. I feel like this person isn't much of a Pythonista not to bring this up. They are certainly correct that the built-in statistical functions available in R are fantastic - that's absolutely where R shines. This article had the potential to highlight some serious pros and cons of each language. Instead it's a contrived list of one-off examples that is barely a superficial survey of the languages. There's not even any mention of plotting!
One of the biggest reasons tidyverse exists is to correct base r’s lack of language unity. The classic example is base r’s apply functions. The arguments are named differently and appear in different orders. Tidyverse was built, in part, because the cognitive load of having to look up syntax every stinking time slows down development. Tidyverse unites naming conventions, argument names, argument ordering and function philosophy. It doesn’t make r less United, it makes it more United.
This is great OP! Dr. Matloff is a great resource for very reliable information. For those beginning in R, his book is one of the best resources for getting started. You can get a free version [here: The Art of Programming in R](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://diytranscriptomics.com/Reading/files/The%2520Art%2520of%2520R%2520Programming.pdf&amp;ved=2ahUKEwim8ITZm-niAhWkKH0KHeA5Bi0QFjAOegQIBRAB&amp;usg=AOvVaw247jGdrwjRoVHMtJqNtwpD). His point about a unified language is very important. This was a topic of lunch discussion at a recent R conference I attended. I seemed to be the only one anti-`tidyversev::`. He said it much better than I could; however, a knowledge of base R is essential, especially when transitioning to development in R. Many others at the table spoke about how readable the tidyverse is; I totally agree. Notably, ggplot, dplyr, and stringr, are the highly efficient for the purpose they serve. If youre doing mostly plug-and-chug analytics and scripting, the tidyverse is a great resource. However, when developing a package or functional pipelines be wary of a reliance on the tidyverse packages (ggplot, dplyr, stringr, ...). Why? * Non-standard Evaluation (NSE): Try writing a wrapper function around any of the tidyverse...have fun. * Quoting: this became such an issue there had to be another package created to address it `glue::` * Masking: the tidyverse share function names with other packages so you need to explicitly call `package::function` * Performance: `purr::` rears its head here * Extensibility: You want to develop things in a manner that decouples and can be used across environments. The more inherent the better (IMO)
Okay, thanks for the advice
Discussion in /r/rstats from when this was posted there yesterday: https://www.reddit.com/r/rstats/comments/c03k97/r_vs_python_for_data_science
Aah, I only searched for reposts on this subreddit. Thanks! Is there an official R subreddit by the way?
Not really, the community doesn't have a huge presence on reddit. I think rstats is a little more active than here.
Python ftw!
I think just changing your `group_by()` to `group_by(Date, ID, .drop = FALSE)` should preserve the zero count groups.
Very simple solution. This is nearly perfect, except for one thing. My IDs are actually aggregate groupings of sub IDs. For example, ID "A" is actually the aggregate data of a1, a2, a3, and "B" the aggregate of b1, b2, b3. I achieved this by doing mutate(ID = fct_collapse(subID, "A" = c("a1", "a2", "a3"), "B" = c("b1", "b2", "b3"))) I thought by intentionally leaving out certain IDs, for instance, the ID "C", I'd be subsetting the data to only include A and B. But when I put in: df %&gt;% filter(Win=="Y") %&gt;% group_by(Date, ID, .drop = FALSE) %&gt;% summarize(count = n()) %&gt;% ggplot(., aes(x = ID, y = count))+ geom_boxplot()+ I get a box plot with the ID "C", although there are no values that go along with it. I thought to try: df %&gt;% filter(Win=="Y", subID !="C") %&gt;% group_by(Date, ID, .drop = FALSE) %&gt;% summarize(count = n()) %&gt;% ggplot(., aes(x = ID, y = count))+ geom_boxplot()+ but that doesn't change anything either.
hmm, try inserting the line `droplevels() %&gt;%` between `summarize()` and `ggplot()`. Or else maybe between `filter()` and `group_by()`. Also, you don't need the "." as the first argument to `ggplot()`; the `%&gt;%` automatically pipes the left side as the first argument on the right size.
I tried your suggestion about droplevels(), but it ended up messing some other stuff up. I looked more closely and realized I could just add a filter after the summarize function to remove "C". Thanks for the help with the syntax and the .drop=FALSE!
Jfc stop calling it a debate
Thanks for your suggestion! But still is not working. How would one use dplyr::filter in a ggplot function for selecting observations in this case?
Thanks, it worked! :) I don't have prior programing knowledge, and I keep having these problems with synthax. Do you know any sources focused how to "write the code"? Many learning sources I see focus on "how to solve X problems", or "how to clean data", etc, without going into specifics of code synthax and I am struggling to find such material
The post made this point, but I think it's an important one. R is basically two languages these days, just pretending to be one. This strikes me as a massive problem. As much as I respect Hadley's work, we may have been in a better situation if he has started by forking the base language and creating an R++.
Heres some similar base R code to the Python you wrote: Looping through files in R (no loop necessary!): files = list.files(file_dir, pattern='\*\\\\.czv') List comprehension: critical_files = files[grep('important', files)]
Cool! I'm just not familiar enough with R to pull that off the top of my head.
use unlist() to convert the matrix (or dataframe) to a single vector. Then compute the mean of the vector.
Let's say your matrix is stored in an object called m, you simply have to do mean(m)
Yeah no worries, just wanted to add it on for those interested
I think OP (and so do many beginners) confused the term matrix with dataframe.
&gt; I previously used a command like &gt; &gt; DF[DF == "Dont know"] = NA &gt; &gt; to replace "Dont Know" with NA for certain analysis Ok &gt; I later changed a date column from character to date class so this is completely unrelated to what you have done previously &gt; and that command didn't work because POSIX is not an 'unambigious date format' so you don't have a POSIX column to begin with &gt; I started using gsub, this command to be specific: &gt; &gt; datafile1[] &lt;- lapply(datafile1, function(x) gsub("N/A", NA, x)) &gt; &gt; and now it is changing my POSIXct date column to a character. So here you seem to be using an entirely different list (possibly a data frame) named `datafile` and lapply'ing gsub on every single element of said list (or every single column of said data frame.) I do not understand what the chain of events is here or what exactly you are asking. You really should provide some example data and expected output. But for now, here's a bunch of possibly related details - gsub will always return a "character" as stated in the documentation. Any input `pattern`, `replacement` and `x` is expected to be a character, if not, they will be coerced into a character. - lapply will apply a function to every column of a data frame (or every element of a list). So the output of that lapply can only be a list of character vectors. - If you ultimately want to convert a vector of characters into POSIXct, first you need to set anything you don't want to be dates to NA's then `as.POSIXct` will do the job - Note that `Date` and `POSIXct` classes are not the same.
gsub() will coerce anything you pass to it to character, it's for regex substitution and it is not the appropriate tool for this task. If datafile1 is a data.frame your lapply is going to traverse all the columns coercing them to character. The right time to handle NA values is usually at the time you load the data into R, e.g. by using fread()'s 'na.strings' parameter or read_csv()'s 'na' parameter. As.POSIXct also has an 'optional' parameter which you can set to TRUE to make non-date values to NA instead of erroring.
Can you be more specific? In R an attribute is descriptive metadata associated with an object and accessed with the attr() function, but that does not sound like what you are describing. What structure does your data have: vector, list, data.frame? Is your data one or more variables? What "database" are you referring to?
I disagree. You can even see the impact in this sub, where a decent number of questions are posted in tidyverse flavor, which makes it difficult to (a) run the repex if provided if one doesn't have the entire set of tidyverse packages installed: (b) difficult to interpret if one is not familiar with tidyverse syntax, and (c) unclear if the OP even wants/can make use of a solution in base R. My biggest gripe is that tidyverse does not offer any clear objective benefits for the amount of overhead, but since it seems to be easier for most, and notably newer users, it has taken on a life of its own. It wasn't so bad when the decision was base vs dplyr vs data.table; users still had to understand base to some extent, so there was an expectation of shared common ground. With tidyverse, you basically have a sub-dialect/bubble, and I don't think that is doing any favors in terms of uniting the user base.
Beginners will always have questions. The fact that they are asking questions and asking specifically for the tidyverse answer should say something. Every time someone says they can’t understand the “objective value of tidyverse” and think that it has “objectionable overhead”, I just want to direct their attention to the simple fact that every single new user exposed to tidyverse prefers it. Almost everyone I know who teaches r now prefers teaching it. I don’t know of anybody who is bogged down by the overhead of tidyverse. Anyone who uses big enough data to be bothered by milliseconds simply writes in r and uses compute somewhere else. I’m not saying that there is never case to optimize to milliseconds but python users and r users alike turn to solutions that allow them to write code in their favorite language while the model is trained in something faster underneath their code. It’s fine if you personally don’t want to switch from whatever you prefer. But can you really not see why everyone is flocking to it?
&gt;* Non-standard Evaluation (NSE): Try writing a wrapper function around any of the tidyverse...have fun. `rlang` has greatly improved the process of NSE. Yes, it's a hurdle to learn, but once it clicks it's quite straightforward to implement. &gt;* Quoting: this became such an issue there had to be another package created to address it `glue::` `glue` makes it easier, but you don't need to use it, and I don't think it was such a difficult problem. &gt;* Masking: the tidyverse share function names with other packages so you need to explicitly call `package::function` AFAIC, that should be best practice when programming with R anyway. &gt;* Performance: `purr::` rears its head here Is that such a huge difference. In my experience, `map()` is marginally slower than `lapply()`. &gt;* Extensibility: You want to develop things in a manner that decouples and can be used across environments. The more inherent the better (IMO) Sure. To me the other issue, which you haven't mentioned, is the possibility of breaking changes to existing packages. Packages like dplyr and ggplot should hopefully not go through those anymore, but you never know.
I like the [R for Data Science](https://r4ds.had.co.nz/index.html) book.
&gt; By contrast, R is rapidly devolving into two mutually unintelligible dialects, ordinary R and the Tidyverse. Reminds me a lot of trying to learn vanilla JavaScript towards the end of the jQuery craze.
Yeah, a few thoughts. 1. The NAs will be coerced when they are passed to a character vector 2. When going from character to date you're going to have to specify `format()` `as.Date` 3. Remember NA is not Null, you could convert to a long dataframe or call format so the NA are caught with `as.POSIXlt`
Sure, I am sorry. I am dealing with survey data which has 123 attributes (columns) All attributes has a name, for instance, What is your name? What is your age? What is your profession? &amp;#x200B; Since there are 123 questions it is hard for me to select certain attributes by their number. using commands such as data\[,3\] to select what is your profession? attribute. Thus I am trying to assign a number to each attribute, in order to recognize them by number. Desired result: 1- What is your name? 2- What is your age? 3- What is your profession? [https://www.kaggle.com/osmihelp/osmi-mental-health-in-tech-survey-2018](https://www.kaggle.com/osmihelp/osmi-mental-health-in-tech-survey-2018) This is the data original data
i found that very helpful as a person who has used neither but is trying to understand the tradeoffs between the two.
I have a background where I learned other languages before R. Vanilla R is super illogical to me. Sometime if you apply an operation to a vector you get a new vector where it has been applied to each element. Sometimes the function evaluates the entire vector and returns scalar. Data$test = ifelse(is.na(data$value), "a", "b") When I was new to R it made no sense to me why this code wouldn't work. The tidyverse functions are much more like what I know from other languages. Chaining function calls is really elegant to me and really intuitive. I think it depends on your experience. I think tidyverse is more like other languages. Particularly the C# linq namespace has a lot of functions like the tidyverse ones.
If I understood your problem correctly, this can be achieved with paste0 and simple for loop, here is example. &amp;#x200B; data &lt;- data.frame(matrix(ncol = 123)) names(data) for (i in c(1:123)) { names(data)\[i\] &lt;- paste0("atr" , i) } names(data)
Defending a position on the basis of popularity isn't coming at things from an objective position, but I do understand the core points I (think) you are striving for. On teaching: I can understand the draw of tidyverse for workshop-style sessions; if you have only a few days to get a range of mostly new (to R) users maximally productive in the language, tidyverse has some clear benefits. On optimization: I should have clarified re: overhead. I was not (primarily) referring to literal computational overhead, but rather the fact that one has to learn what is effectively a divergent sub-dialect/style, without any clear objective benefits. I understand why dplyr exists; I also understood why reshape2 was created. What I do not understand is the standalone value of e.g. purrr, although I do understand its existence in the context of the tidyverse suite. I want to emphasize that I DO understand why many are flocking to tidyverse. I am not debating its merits in a vacuum. I will also continue to maintain my stance that in the context of the general R user base, it is creating a divide rather than unifying.
Thank you for your guidance. It solved my problem. Here is the last code version: for (i in c(1:ncol(data))) { names(data)\[i\] &lt;- paste0(i, "- ", names(data)\[i\]) }
You're on the right track. Use sub on data.gender with OR clause | data.gender &lt;- sub(''male|Male|^m", "male", data.gender) Etc etc
Use case_when in dplyr: df$gender_clean = case_when( df$gender == “male” ~ “Male”, df$gender == “MALE” ~ “Male”, df$gender == “M” ~ “Male”) And so on.
One of the stated benefits of map() over the apply() family of functions is the unification of syntax though. The apply family has argument that do the same thing but are named differently and appear in different orders. There is much more overhead to remember or have to look up the arguments to use apply. In addition, map() is designed to be so easily extensible. Case in point is furrr() which takes map and applies “futures”. This unifies r with python, scala, etc in a big way. Or you can apply this idea to any backend... map() can be used to perform lambda functions against a database backend, spark backend, etc. I wild think map() is almost the perfect example of why people would want to use the tidyverse.
Rather than solving the problem with certain cases, I was looking for a solution with regex. Because there are more than 60 factor levels, I do not think that I will be able to detect them one by one.
This is how I achieved the currently desirable results. If you could provide the less complex or better-resulting code I would appreciate. Thanks \# Define gender attribute data.gender &lt;- data.fact20\[,13\] data.gender &lt;- tolower(data.fact20\[,13\]) \# Trim whitespace trim &lt;- function (x) gsub("\^\\\\s+|\\\\s+$", "", x) data.gender &lt;- trim(data.gender) \# Regularize recognizible data.gender &lt;- sub("f|female|woman", "female", data.gender) data.gender &lt;- sub("m|male|man", "male", data.gender) \# set "other" for neither male or female for (i in c(1:length(data.gender))) { if(data.gender\[i\] != "male"){ if(data.gender\[i\] != "female"){ data.gender\[i\] &lt;- "other"} } } \# print results data.gender
This \*may\* work hahah &amp;#x200B; \`male\_female\_clean &lt;- function(x, ...) { x &lt;- tolower(x) x &lt;- gsub("\^\\\\s+|\\\\s+$", "", x) if(any(grepl("\^fe.\*", x))) { x \[grepl("\^fe.\*", x)\] &lt;- "female" } else if (grepl("\^me.\*", x) { x \[grepl("\^ma.\*", x)\] &lt;- "male" } else if (length(list(...)) &gt; 0) { x&lt;- gsub(...) } &amp;#x200B; x\[! x %in% c("male", "female")\] &lt;- "other" &amp;#x200B; return(x) }\`
With sub(m|male|..) you will be substituting all the m im female with male i.e. female &gt; femaleale ^ m you need the carrot to specify at the start if the line.. I assume you are getting a large number of false others at the moment
I’m trying to work this out as well. I’m building a shiny app that will allow edits for email drafts and then send them. I want to output an image of the draft and if they want to make changes have a pop up box that allows for changes. When they make changes I want them to alter that aspect of the email template. So far I know that I need to set certain elements in the email body as objects and then call those objects given user input but i haven’t figured out how to do this quiet yet in shiny ace.
Yep, this should work. Using summarize() you can sum up the number of times a column fits a condition. raw_df %&gt;% group_by(Date, Vgrade_group) %&gt;% summarize(proportion_sent = sum(Sent == "Y")/n()) Hope this helps, let me know if you have any questions.
Perfect! Thanks!
&gt; I have no idea how object oriented programming works in R (admittedly haven't tried extensively) OO in R is really weird. There are three implementations: S3, S4 and R6. The default is S3, and base R uses this to no end. Quoting from [Wickham's Advanced R book](http://adv-r.had.co.nz/S3.html): &gt; R’s three OO systems differ in how classes and methods are defined: &gt; - S3 implements a style of OO programming called generic-function OO. This is different from most programming languages, like Java, C++, and C#, which implement message-passing OO. With message-passing, messages (methods) are sent to objects and the object determines which function to call. Typically, this object has a special appearance in the method call, usually appearing before the name of the method/message: e.g., canvas.drawRect("blue"). S3 is different. While computations are still carried out via methods, a special type of function called a generic function decides which method to call, e.g., drawRect(canvas, "blue"). S3 is a very casual system. It has no formal definition of classes. &gt; - S4 works similarly to S3, but is more formal. There are two major differences to S3. S4 has formal class definitions, which describe the representation and inheritance for each class, and has special helper functions for defining generics and methods. S4 also has multiple dispatch, which means that generic functions can pick methods based on the class of any number of arguments, not just one. &gt; - Reference classes, called RC for short, are quite different from S3 and S4. RC implements message-passing OO, so methods belong to classes, not functions. $ is used to separate objects and methods, so method calls look like canvas$drawRect("blue"). RC objects are also mutable: they don’t use R’s usual copy-on-modify semantics, but are modified in place. This makes them harder to reason about, but allows them to solve problems that are difficult to solve with S3 or S4.
My point exactly :) No way R "wins" in OO programming over Python! I guess that's where the author brings in "metaprogramming"as a strength in R. But c'mon he has like 5 sentences on the two topics, I'm not being convinced or really learning about anything from this post.
The article also made a good point about the corporate aspect of R Studio. Not to paint R Studio as the bad guy because they are doing a lot for the community, but I totally agree with you- they are covertly creating a new dialect within the R verse. They could've improved some of the paradigms and improved debugging, as opposed to making a new syntax. They also sponsor a ton of conferences- which, intentionally or unintentionally makes their packages ubiquitous.
I have another question: &amp;nbsp; Now, I'm looking for the average Vgrade attempted and the average Vgrade sent per day. I first changed the Vgrade of "B" to "0", filtered out "makeup", then I changed the variable type to numeric. &amp;nbsp; For "average Vgrade attempted", I want to group_by Date, then do (Vgrade*Attempts)/(Sum of Attempts). For "average Vgrade sent", I want to do the same thing, but with the conditional statement that Sent == "Y". I can then select for unique observations of Date to get the output I want. Is there a better method than the one I have here? Also if you have a method for selecting only the observations for Sent == "Y", I'd appreciate it; it seems like an easy enough task, but I don't quite understand the syntax yet. average_vgrade &lt;- raw.df %&gt;% filter(Location2 == "CRG_Hadley") %&gt;% group_by(Date, .drop = FALSE) %&gt;% mutate(Vgrade = replace(Vgrade, Vgrade == "B", "0")) %&gt;% filter(Vgrade != "makeup") %&gt;% mutate(Vgrade = as.numeric(levels(Vgrade))[Vgrade]) %&gt;% mutate(avg_vgrade_attempted = sum(Vgrade*Attempts)/sum(Attempts)) %&gt;% mutate(avg_vgrade_sent = ??????) distinct(Date, .keep_all = TRUE) Thanks!
R has a build in whitespace function, `trimws`, which might be helpful for you in the future
Assuming you get the quotes straightened out, you can use a JSON parser to pull the structure apart. JSON requires the keys to use double quotes, so you'd have to do some sort of substitution to fix, but you can use fromJSON in rjson.
So your code leading up to the last few lines might be slightly different than mine, but the general idea is to create a Vgrade\*Attempts column, then sum that column only when Sent == "Y". average_vgrade &lt;- raw_df %&gt;% filter(Location2 == "CRG_Hadley") %&gt;% mutate(Vgrade = replace(Vgrade, Vgrade == "B", "0")) %&gt;% filter(Vgrade != "makeup") %&gt;% mutate(Vgrade = as.numeric(as.character(Vgrade))) %&gt;% mutate(Vgrade_Attempts = Vgrade*Attempts) %&gt;% group_by(Date) %&gt;% summarize(avg_vgrade_attempted = sum(Vgrade_Attempts)/sum(Attempts), avg_vgrade_sent = sum(Vgrade_Attempts[Sent == "Y"]/sum(Attempts[Sent == "Y"]))) Btw typically using group\_by() then summarize() is the same as doing group\_by() then mutate() then distinct(), so I don't think you need that extra step. If this doesn't work though, let me know!
Easiest route might be to use either the stringr packages and first get rid of the 's and "s (str_replace) then extract (str_extract). Regardless, I don't think you'll need to go to far down the regex rabbit hole on this one. This is a good reference for both regex in R and common functions for working with strings: [http://stat545.com/block022_regular-expression.html](http://stat545.com/block022_regular-expression.html)
Thanks, that worked, though I had to mess around with the formulas. The one I supplied weren't correct. To make a graph comparing the avg_vgrade_attempted and avg_vgrade_sent, I usually proceed by making the dataframe longform, by using melt(). Is there a better way with dplyr or is that the standard method. Thanks again!
Yep, no problem. So the new tidyverse version of melt() is gather() from the tidyr package. If melt() works for you there isn’t necessarily a reason to change though
Good to know! Thanks again.
!RemindMe 1 day
I will be messaging you on [**2019-06-17 00:43:11 UTC**](http://www.wolframalpha.com/input/?i=2019-06-17 00:43:11 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Rlanguage/comments/c141h0/outside_personal_preference_why_especially_in_a/erajb80/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Rlanguage/comments/c141h0/outside_personal_preference_why_especially_in_a/erajb80/]%0A%0ARemindMe! 1 day) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
For me personally, the dplyr and ggplot2 APIs are just so much nicer to work with than anything I've seen in Python for data transformation, analysis, and visualization (especially, I can't stand matplotlib). R's built-in stats functions are also more complete and better designed than scipy's, and R's formula API for generalized linear models is nice too. I use Python a lot for scripting/automation and web server stuff, and I would use it if I needed to build a neural net or something, but I feel R is just better for general exploratory data analysis.
I feel like R is better for pure statistics. Also, for traditional ML (i.e., not deep learning) it seems to do well. At my previous employer, R was my clear favorite for statistics, it was probably a tie between R and Python for traditional ML, and then Python obviously takes the cake for deep learning.
I tried replacing all of the single quotes to get it into JSON format. Using jsonlite I keep getting an error. json &lt;- stringr::str_replace_all(new_df$Location, "'", '"') mutate(json = map(json, ~ fromJSON(.) %&gt;% as.data.frame())) %&gt;% unnest(json) &gt; Error: lexical error: invalid char in json text. &gt; 18.413", "human_address": "{"address": "", "city": "", "stat &gt; (right here) ------^ Any idea why I might be getting that error?
At my lab, the breakdown is basically as follows: 1) Stats and visualization: R 2) Text parsing, general "programming" tasks: Python (some bash, honestly) 3) General pre-processing: Dealer's choice, maybe decided by what you feel lost comfortable with 4) ML/NN: Depends. NN are pure Python, basic ML depends on the user.
Others have covered the machine learning side of things so I will skip that, but I will share my thoughts as someone who uses R in my work everyday. &gt; that R doesn't scale up well, and isn't what you want to turn to when working with datasets of a significant size This actually not true at all, but that won't stop certain python users from claiming it anyway. The main limitation you face with large data sets in R *and Pandas in python* is that they work out of memory, so you are limited by how much RAM you have for very large data sets. Assuming that you do have the memory for both however, [R blows python data analysis packages out of the water in terms of performance](https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping). If you look at [this chart](https://raw.githubusercontent.com/wiki/Rdatatable/data.table/bench/grouping.2E9.png) you will see that when dealing with massive data sets (100 GB+), Pandas throws memory errors, where data.table and dplyr operations are able to handle all grouping/data manipulation tasks without issue. To address your question more generally, I think the most obvious area that R is superior to Python is in general data analysis tasks. While data aggregation, visualization and presentation are all things that Python *can* do, once you have worked with both its clear that the equivalent tools in R are objectively superior. I covered aggregation tasks with above, for visualization ggplot2 and even the base plotting graphics (for ease of use) are superior to python's visualization tools. For presentation, Rmarkdown is excellent and there is nothing in the python universe that can compete with Shiny apps. I believe an underappreciated reality in the python vs R debate, is that python does not deal with data frames natively. I struggle to think of any data analysis tasks that don't involve working with tabular data. In the R language, the data frame is the cornerstone to it's foundation, and everything in R is designed around that framework. Obviously the pandas library was created to bring that functionality to python, but in my usage of it, it always *feels* like an add-on, that you need to work around, rather than within. Personally, this is especially obvious is in interfacing with databases, which is built in to the RStudio GUI and is effortless. I have found it to be a real PITA in python/pandas.
The issue with R is that it is kind of hard to go outside existing packages/functionality, but for graphics and data analysis with data.table it beats Python. Also, with tools like Bioconductor it is very used for bioinformatics/genomics. Some of the nice everyday tools like Jupyter notebooks or conda were designed with Python in mind, but work fine with R. The reverse is not true for RStudio, for example.
`data.table` is beautiful. And RStudio is a fantastic IDE.
I've come to like Rmarkdown for reports. It handles Python, shell, and a few other languages as well.
It works! However, it would take my time to understand code if I return back to it after a time
I understand the issue you are mentioning, But it did not happen I am not sure why, I do not understand regex totally. If you like to simulate Here is the data: [https://www.kaggle.com/osmihelp/osmi-mental-health-in-tech-survey-2018](https://www.kaggle.com/osmihelp/osmi-mental-health-in-tech-survey-2018) My results: Before &gt;&gt; [https://i.imgur.com/qN0Go9I.jpg](https://i.imgur.com/qN0Go9I.jpg) After &gt;&gt; [https://i.imgur.com/pH7Rpnb.jpg](https://i.imgur.com/pH7Rpnb.jpg)
Thank you for your reply. I revised the code by your feedback. # Define gender attribute data.gender &lt;- data.fact20[,13] # Pre-Adjustments Before Fix data.gender &lt;- tolower(data.fact20[,13]) # all lowercase data.gender &lt;- trimws(data.gender) # Regularize recognizible data.gender &lt;- sub("f|female|woman", "female", data.gender) data.gender &lt;- sub("m|male|man", "male", data.gender) # set "other" for neither male or female for (i in c(1:length(data.gender))) { if(data.gender[i] != "male" &amp; data.gender[i] != "female"){ data.gender[i] &lt;- "other" } } # print results data.gender
You should use the ```other``` line and get rid of your for loop. That line says for values of ```x``` that aren't in "male" or "female", assign "others" to those elements. That's what the ```gsubs``` are largely doing as well.
Use dplyr library to 'gather' the data to get the years in one column and then plot. You can use esquisse library to make plotting of graphs easier.
Try table(data.gender) to see the final output.. it gives a better summary
RemixAutoML on github for R is better than any AutoML on Python.
 [R blows python data analysis packages out of the water in terms of performance](https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping) &amp;#x200B; That chart shows data.table beating both dplyr and pandas 5/5 times and pandas beating dplyr 3/5 times for groupby aggregations. I don't really think you drew a fair conclusion of that "R blows python data analysis packages out of the water" based on that benchmark alone, especially considering dplyr is more popular than data.table ( [https://www.reddit.com/r/rstats/comments/9nk2d6/popularity\_of\_rs\_dplyr\_and\_datatable\_packages\_oc/](https://www.reddit.com/r/rstats/comments/9nk2d6/popularity_of_rs_dplyr_and_datatable_packages_oc/) ). Talking about speed in the R vs Python debate really boils down to which problem you're solving and the specific implementation.
 [https://github.com/chasemc/electricShine](https://github.com/chasemc/electricShine) bundles R and packages into a desktop app. Looks like only available for Windows current
That's a fair point, and "blows it out of the water" is probably a bit hyperbolic, and would have been more appropriate for the second chart. I was mainly trying to address the idea that R is bad for big data, and you should use python instead, which is frequently claimed and I think I demonstrated, outright false.
What do you mean by "hard to go outside existing packages/functionality"?
Data.table melt/cast + ggplot2 but there's more than 1 way to skin a cat.
Thank you so much, this was the solution I used. Worked out pretty good :)
Thank you for your reply, I ended up using gather() from dplyr library. But yes, I realized there are many ways to do what I needed!
Scripting is way easier with Python, at least for me. With R I feel I am bit stuck looking for a package that matches my exact needs rather than trying to write something in my own...
That's pretty subjective, I write R scripts all the time. It's a scripting language, lol. And unless you're a very advanced statistician you're usually better off using a package anyway no?
Dplyr is more popular than R because most users aren't working with hundreds of gigs of data.
gather() is from the tidyr package, not dplyr. This is an important distinction if you're working with database backend because dplyr functions have methods for tbl_sql (via dbplyr) while tidyr functions do not.
gather() is likely to be deprecated in favor of the pivot_ family of functions within the next year.
Welcome to the wonderful world of joins! If you've never used joins before, you can read this - make sure you understand the logic, not necessarily the language https://en.wikipedia.org/wiki/Join_(SQL) Once you understand what's going on, here are a few R functions to perform the same joins, from the dplyr package: https://dplyr.tidyverse.org/reference/join.html I'd suggest joining on Name, then you'll have one dataframe with all the columns. If you do a left join of dat to data.df, it'll fill in all the NAs for you for the data.df columns that don't have a matching name in dat. If you are more interested in having a full data.df table, I'd suggest left joining data.df to dat - that would drop all the rows in dat that aren't in data.df so you will have complete observations (a quick glance made me think that your top dataframe contains all the names that are in the bottom one, sorry if that's wrong). Then you can do whatever you want as far as ggplot goes.
You could use merge, specifying they x and y var names you're merging on and telling it to keep all of the x rows but not all the y: merged_data &lt;- merge(dat, data.df, by.x="Player", by.y="Name", all.x=T)
Absolutely incredible! Funny thing is I was doing a course where "join" was used, but it was a little bit hand-holdy so I didn't get to see the full power! Solved!
If you want to simplify things, (in my opinion) you can do plot(yourLm, which = 1) To do a residuals vs fitted plot, you can add to it to change the color of the dots based on a third column by Plot(yourLm, which = 1, col = data$column)
plot(fit, which =1, pch = FishEgg$sept) I tried this for different symbols but they didnt change respectively. any ideas?
Never mind it worked perfectly! Thank you so much
Thanks for the correction... I forgot which of the two libraries had the function.
Whenever I forget what a join does, I find [this](https://twitter.com/grrrck/status/1029567123029467136?s=19) tweet
Cowplot package.
Cowplot package of plot_grid should do it?
Or better yet, if they are all using the same dataframe and different columns, use tidyr::gather() on all the columns, then facet_wrap() on your key.
Looks awesome! Didn't know about cowplot, but it works like a charm!
I don't export to Word very often. Mostly, I export to HTML. For that application, I have a custom CSS file that I use. I don't know if the CSS settings will work for Word. Either way, I recommend that you review how knitr will use your CSS if it exists its own default CSS otherwise. Also, check out the [kableExtra package](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).
You should create a template, reduce the font size for tables, then load it in knitr during rendering. I am in mobile, now. Google knitr word template for examples.
These books may be *priced* at $944, but I'm not I would say they are *valued* at $944. Thanks for the heads-up though.
Look up flextable package
Take a look at [patchwork](https://github.com/thomasp85/patchwork)
Warning: The above url is SPAM because it contains an affiliate tag. For anyone interested in the bundle, here is the non-affiliate url: https://www.humblebundle.com/books/stem-bookshelf-mercury-books
Why does this product exist lol? Has to be custom
I use R btw
I have a Wrangler. And now I want this tire cover.
I need it!
Lol. I know that guy!
&gt;I use aRch btw
Is it Hadley Wickham? That would be awesome....
Nah. He’s an IBM’er. Plus I think Hadley is based out of Texas?
Yeah R Programming is great!
If you read the error message slowly one more time you'll notice it says "invalid 'ncol' value **(&lt;0)**". The ncol value you're trying to use is less than zero. Which means the length(min_adv_col_name) is zero. You should look at the page again and use your browser's developer tools to make sure you get right path the table.
Do you see this on Linux? When you are fully out of memory the session can and will crash as far as I remember. Swap space could be making it crawl
Thanks for replying. Yes, on Linux. Do you know why the different behavior between Python and R? (I just found an old thread on stackoverflow suggesting to use ulimit..)
This guy needs to do less
I don't experience this, so I don't know what you should do. My R process hangs but my system either continues sharing CPU time or kills my R process.
What OS? What are the error/warning messages?
Remove and reinstall RStudio from scratch — potentially a different version, your installation seems broken (this is unlikely to be related to R proper).
So I'm not sure what the output from pdf\_text() looks like since I haven't used it before, but something like this should generally work with dealing with those two strings: library(tidyverse) way_one &lt;- "Totalsum: 881&amp;209 783&amp;450 43&amp;729 52&amp;280" way_two &lt;- "Totalsum: 881 209 783 450 43 729 52 280" your_df &lt;- tibble(text = c(way_one, way_two)) your_df %&gt;% mutate(number_extract = str_c(str_sub(text, 19, 21), str_sub(text, 23, 25), sep = " ")) Not sure if you need the space or not for "783 450", but the code currently leaves it in there. Hope this helps! Let me know if this doesn't work.
When you use a histogram to plot densities, it has to put the observations into "bins", thus binwidth. So, if your x-axis is from 1-100 and you have a binwidth of 5, each bin will range from 1-5, 6-10, etc. up until 95-100 See this for more: http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/
Check value of
Should be really comparing `map()` with `Map()`, not with `apply()`.
Why?
Obvious. `Map()` is in base and inspired `map()` from tidyverse. `apply()` is a different thing meant for working with arrays.
Before reinstalling, contact your IT department. I suspect they might have some security software (antivirus, firewall, etc.) on you machine that could be interferring with installation. Especially if you have issues installing packages.
Hi All, Thanks for the advice, and I have a working but horribly clumsy workaround. So it turns out I can pipe within a function, and I also needed to provide a temporary name for my vector. So first I use my file list to create the data I need, and then pipe instructions to make a tibble and assign a column name, before binding each set of page data into a single dataframe: fileset1&lt;- lapply(file_list, FUN = function(files) { pdf_text(files) %&gt;% tibble() %&gt;% rename(text = ".") }) %&gt;% bind_rows() I then repeat this process for a second set of files (fileset2). To combine it's simply using mutate command to assign values: text_df &lt;- bind_rows(good_files %&gt;% mutate(name= "Fileset1"), bad_files %&gt;% mutate(name = "Fileset2")) I'm sure there's a far better way, but for the moment this works.
Thanks for the help - I found a rough working solution similar to what you say, see the thread for an additional post with my solution.
Thanks for the help. Unfortunately I wasn't able to get to grips with your concept, but I found a rough working solution similar, see the thread for an additional post with my solution.
Can you use the MP as a num class? As in 22:30 == 22.5? Only other thing I saw that could be goofing you up is that some of the MP entries contain milliseconds, others don't - that could be throwing off your conversion.
For some reason it added milliseconds when I shipped to Sheets, they don’t actually exist in the dataframe in r. And would it work to just as.numeric(df$MP)?
What are you trying to do?
I'd have to be in front of RStudio to figure that out 100%. The other thing I noticed was duplicate (but different) entries for DeShields. That's probably where your verticals are coming from - could that be a different player?
I have a large data set of crimes in a particular city. There are 11 attributes in this data set such as type of crime, location, date, time, etc. I am trying to use Naive Bayes to predict when a certain crime will happen. For example say I want to know the probability of a robbery happening on a Friday afternoon between the hours of 3pm and 5pm.
R has tons of packages and there's more than one valid way to do many (if not most) things. It'd help to know exactly what data you have (a sample of a few rows and/or a description of it would help), what type of processing and cleaning it needs, and the type of analysis you intend to do. &amp;#x200B; Hard to know if I can help without knowing more. Sry :|
 I have a large data set of crimes in a particular city(over 6 million rows of data over a span of 18 years). There are many attributes in this data set such as type of crime, location, date, time, etc. I am trying to use Naive Bayes to predict when a certain crime will happen. For example say I want to know the probability of a robbery happening on a Friday afternoon between the hours of 3pm and 5pm. (I'm new to Reddit so I don't exactly know how these comment threads work. I pretty much replied to you what I replied to another person - just not sure if you can see my original reply to the other person so sorry for the duplication if you can)
Based on your above comment, it sounds like you want to factor the data and create a set of predictive models?
Correct! I'm just not knowledgeable enough about the Naive Bayes command in R to be able to execute this myself.
Well since ur new to R, the [documentation](https://cran.r-project.org/web/packages/naivebayes/naivebayes.pdf) is great so def check that out. Not saying that's all the help u need but always a great start!
didnt mean to delete my earlier comment btw. My computer is acting up atm so i clicked stuff i didnt mean to. XD
Do you have the data ready to model and just need help with the commands or do you need help getting there?
Does something like this point you in the right direction- [https://rstudio-pubs-static.s3.amazonaws.com/157831\_beebd5628ea6460190b5c39e9b413e24.html](https://rstudio-pubs-static.s3.amazonaws.com/157831_beebd5628ea6460190b5c39e9b413e24.html)
If you want to run simple predictive models, check out kaggle. Tons of kernels show you good practices for feature engineering, model training and diagnostics: [https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) [https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda](https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda)
I checked that link out before. It helped me get in the right direction but I need something that goes a little more in depth. So I have the data ready to model (for the most part - I think I need to convert the character attributes to factor or else Naive Bayes won't work and small things like that) I just need help with the commands pretty much. If you're willing to help I can share the data set with you and the work I've done up to this point in R!
Actually yes! That will be helpful for another part of my code when I get to goodness of fit tests. Thank you!!
This is GREAT! I've never heard of Kaggle. This will definitely help out. Thank you!
I'm not really familiar with these models so it's probably best someone else help.
Great idea. Kaggle has tons of code and many ppl explain what they did. C:
Youre probably going to need as.factor() for your factors, and cbind() for binding together your predictor variables as a matrix
Did you already use Swirl? It is a good place to start with understanding R and it is all within R
This is the book I used to get a grip on R. https://r4ds.had.co.nz/introduction.html I'm also planning on using R for the capstone for my Master's degree in data analytics.
Can anyone try this code on their computer to see if it yields similar results to mine? Thank you!
This uses base R. The tricky part is being able to handle daylight savings if you need to work with a data set that spans both standard and daylight time. You could probably use `period`s from the `lubridate` package to avoid mucking with the hour part of the day separately, or you could simply not worry about daylight savings if you were never going to re-use this code for a different time interval than the one you specified. # setup fake data Sys.setenv( TZ = "America/Los_Angeles" ) dtmrange &lt;- as.POSIXct( c( "2018-06-01 06:00" , "2018-09-30 18:00" ) , format="%Y-%m-%d %H:%M" ) onehour &lt;- as.difftime( 1, units = "hours" ) dta &lt;- data.frame( dtm = seq( dtmrange[ 1 ] , dtmrange[ 2 ] , onehour ) ) dta$temp &lt;- runif( nrow( dta ), min = 10, max=35 ) dta$windspd &lt;- runif( nrow( dta ), min = 0.5, max=6 ) # create grouping columns d &lt;- as.POSIXlt( dta$dtm + 6*onehour ) # read ?DateTimeClasses d$hour &lt;- ifelse( d$hour &lt; 12, 0, 12 ) # midnight or midday grps &lt;- data.frame( dtm2 = as.POSIXct( d ) - 6 * onehour ) dta2 &lt;- aggregate( dta[ , -1 ], grps, FUN = mean )
prexisting packages, thats about it in my opinion.
The output of pdf_text (or actually the code I posted up there) is a vector or n lines of text with all the words separated by a single space. I thought about that, for sure it works, but what if (and that happens in my data) those numbers digits change from 6 to 5 or 7? I came up with a very complicated and cumbersome solution that is suited 100% to my data, I'm sure that at the next update I'll struggle on that. I was hoping in someone known how to uniform the output of pdf_text()
I’m a noob too, but I worked in a place that had lots of reports of client sessions that were sitting in a database. I started off by cleaning the data of personal and identifying info, then loading it all into a txt file. I found a tutorial that explained removing stopwords and stemming etc then built a simple word cloud. That was just for fun. But then the people who wrote the reports got really engaged and wanted their own person word clouds, so I sorted the data out by writer. Then I did sentiment analysis, then looked at frequency of certain words. It turned out that you could really see the personality of the writers through this and it became a useful discussion tool. My job was to examine the client journey and it actually became a core part of my end report, just from playing around. I used a bunch of different tutorials, but mainly https://cran.r-project.org/web/views/NaturalLanguageProcessing.html
Ohh thanks..
Just thinking: you could scrape a couple of subreddits and use nlp to see which is more negative.
Ohh actually I dnt know to scrap from subreddit but since it's possible I will try and do it..
Define "more"
A friend told me that there are **more** physicists using R language now, and statisticians are using other tools. Is this true?
I can't speak to hard numbers, but I can shed some light on what your friend was probably referring to: 'Statistician' as a profession has essentially been relabeled as 'data scientist', at least in the private sector. The data science community has drawn a lot of programmers who are attracted to machine learning, and therefore find Python easier to use because it is a more traditional object oriented programming language. A lot of 'data scientists' who are former programmers use Python, while a lot of 'data scientists' who are former statisticians use R. Currently, physicists are still mostly using R because R provides many specialized packages for physics, where Python's strengths lie mainly in its predictive modeling and machine learning libraries.
Wow, this is a really clever solution. Just so to check I completely understand, you create a dataframe with date/time, windspeed, and temperature. You then create a vector of datetimes called d, and get the starting time to be 12:00. You then use the ifelse statement to make the times either 12:00 or 0:00, subtracted the 6 hours you added earlier, then used the aggregate function to average by either 12:00 or 6:00. This problem has been driving me nuts for weeks, so thank you so much for your help.
3 pm ET happens when this comment is 3 hours and 41 minutes old. You can find the live countdown here: https://countle.com/4kymD0j2I --- I'm a bot, if you want to send feedback, please comment below or send a PM.
My partner is a physicist and he uses Python. Im a biologist and I use R. I think it varies depending on what your projects’ goals are, R is better for running computational/predictive models, Python is better if you need a specific type of analysis that a pre-defined function cant do.
My man!
R is a good front-end for C or F90 submodules, maybe that's what he's talking about
Purrr is love, purrr is life
I've been using r for years and have never made a function. I just had to make a loop. How much better could r really be with functions
Like a lot. Don’t want to sound douchey but they are legit amazing and make your code way easier to write.
 myProgressBar &lt;- txtProgressBar() ticks &lt;- 0 while (TRUE) { setTxtProgressBar(myProgressBar, ticks) ticks &lt;- ticks + .01 Sys.sleep(.1) } close(myProgressBar)
Goddammit Hadley Wickham is a handsome man on top of being brilliant. What the actual fuck.
Not just write but also read. Im not very good but I’m currently pulling a lot from someone else’s code who didn’t use functions at all, and it’s amazing to me how much cleaner everything is just moving it into functions. It’s also kind of crazy how much easier it is to see things that can be done better / faster / more consistent or even corrections that need to be made with functions. It’s also crazy so much easier to explain to people. We’ve found stuff that was mislabeled or misrepresented because of moving stuff into functions. I sometimes had no idea what the point of some of the code was until I broke it and realized it needed to be added to a function - where because the function gave it context it made sense.
[R Packages by Hadley Wickham](http://r-pkgs.had.co.nz/intro.html), is a pretty primer to getting started developing packages in R.
I was gonna reply and say exactly this.
Agreed that Hadley's primer is a good comprehensive roadmap, but first check out Hilary Parker's great post (which Hadley references in his introduction): https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/
Yeah he's achieved like peak awesomeness.
I think you are right, or at least almost there. `b &lt;- data$b == 0` gives you a vector of `TRUE`'s and `FALSE`'s where the b column is 0. I'm a little unclear on what your desired result is, but here are two ways I could see you using `b`: With this logical vector you could do `data$b[b]` in order to get the elements of `data$b` that are 0's. This will just be vector of all 0's of length `sum(b)`. Or you could use `b` to select the rows of `data` where `data$b` is 0: `data[b, ]`.
I want a vector of the rows of the b column that are 0 omiting the rows that are 1. I think the second thing you put there is to get the entire column and not only the rows that are 0. I don´t understand the first way
Aside from Hadley's book, you have the CRAN guide "Writing R Extensions" and you can explore other authors packages on GitHub, such as their DESCRIPTION, NAMESPACE, R files, etc to understand syntax, structure, and styles.
Remember to make a README for the package!
To me, that sounds like `data$b[b]`. This omits the 1's. For example: data &lt;- data.frame( a = sample(c(1, 0), 50, TRUE), b = sample(c(1, 0), 50, TRUE, prob = c(0.6, 0.4)) ) b &lt;- data$b == 0 data$b[b] gives the result: [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 If this isn't what you are after I'll need more clarification.
If you are familiar with the `tidyverse` you could do something like this: library(tidyverse) d %&gt;% group_by(DATE, OPP) %&gt;% summarise(GAME_SCORE = sum(PTS)) %&gt;% right_join(d, by = c("DATE", "OPP"))
This quick guide - though just use the regular version of roxygen2 - will get you up and running really quickly. https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/ Basically... you get an R package in 5 minutes. Although it gets even easier when you start using devtools as it's supposed to be. Anyway, one big thing is that you no longer call "library(...)" at all in your library, you let the NAMESPACE and whatever take care of it.
In addition to the good advice already given about package mechanics, try to follow software development best practices to the extent possible. This means creating as few dependencies on other packages as you can, writing unit tests, providing clear examples / documentation, and erring on the side of being more explicit and verbose, rather than overly succinct and clever. For your users' benefit, this should be stored in your corporate repository (if you have one) with instructions in your readme for how to install, possibly with options for using the tar or pulling from e.g. the git command line. Unless of course you are given permission or told to publish to CRAN and / or host in a public repo like gitub. A repository is always ideal because you get issue tracking and the like, without any additional effort. Try to round up a few internal power user / testers to provide feedback and ferret out edge cases. This ensures you develop at the appropriate level of modularity and abstraction, to e.g. decide if an exported function that is really just a convenience wrapper around core methods justifies the maintenance overhead.
Could you please provide a small example of these data frames and the code you currently have? I'm a little confused.
I hope this formats correctly... &amp;#x200B; df1: (mapping file) v1 v2 all all a c b d &amp;#x200B; df2: (data file) v1 v2 a c a d b d &amp;#x200B; if I wanted to subset on v1 = a, v2 = c I would do subset(df2, v1 == a, v2 == c) output would be: v1 v2 a c &amp;#x200B; now, for example, I want to subset on v1 = all, v2 = d &lt;need help with code here&gt; output would be: v1 v2 a d b d &amp;#x200B; all basically means any value
I think you would need to specifically check for "all" and just return a vector of `TRUE`'s. Since it sounds like you're doing this in a loop I came up with this snippet of code: df1 &lt;- data.frame(v1 = c("all", "a", "b"), v2 = c("all", "c", "d")) df2 &lt;- data.frame(v1 = c("a", "a", "b"), v2 = c("c", "d", "d")) mySubset &lt;- function(v1_val, v2_val, df) { n &lt;- nrow(df) v1_flag &lt;- if (v1_val == "all") rep(TRUE, n) else as.character(df$v1) == v1_val v2_flag &lt;- if (v2_val == "all") rep(TRUE, n) else as.character(df$v2) == v2_val df[v1_flag &amp; v2_flag, ] } # not looped - test mySubset("a", "c", df2) mySubset("all", "d", df2) # looped mapply(mySubset, df1$v1, df1$v2, MoreArgs = list(df = df2), SIMPLIFY = FALSE) This gives you a list of data frames.
This is simplified by using mutate instead of summarise and ending the script there.
Nice!
RWeThereYet17 gives thae correct solution for your question *as worded*. However, it sounds to me like you are looking for the *row indices* rather than the *contents of the rows.* If that's the case, you can use `which(!data$b)`
ahhhhh makes sense now. Thank you for writing this out for me. I have multiple columns but now I have a good idea on how to approach the problem.
Your advice is excellent. Regarding unit tests, can you recommend a framework/package for R? I have used some really good test harnesses over the years for C++. However, I have not found any for R that I like.
with that I get the rows (a and b) where B=0?
Thanks. I am a fan of Hadley's testthat package. Takes away a lot of the boilerplate and integrates seamlessly with his other package - creation libs.
Usual software rules apply. Use source control (e.g. Github, Atlassian, etc.). Make small changes and commit frequently. 100x 2-5 line commits &gt; 1x 400 line commit. Track every issue, even if you decide to close it unfinished as “won’t fix”. Test every edge case you can think of. If possible, have someone who isn’t you also test it. Remember you are not documenting for you. You are documenting for the idiot that comes after you. Please note that the idiot that comes after you might just be you one year later.
Not entirely sure what you want as the output, but maybe this helps: library(data.table) # Load raw data ----------------------------------------------------------- # done by readClipboard() --&gt; dput on the game_id column in your google sheet # for reproducibility: game_ids &lt;- c("20190526_LAS_LVA", "20190531_LAS_CON", "20190604_LAS_NYL", "20190606_LAS_CON", "20190608_LAS_MIN", "20190614_LAS_PHO", "20190615_LAS_NYL", "20190618_LAS_WAS", "20190525_SEA_PHO", "20190529_SEA_MIN", "20190531_SEA_ATL", "20190601_SEA_CHI", "20190604_SEA_MIN", "20190609_SEA_CHI", "20190611_SEA_IND", "20190614_SEA_WAS", "20190616_SEA_CON", "20190601_WAS_ATL", "20190605_WAS_CHI", "20190607_WAS_NYL", "20190609_WAS_DAL", "20190611_WAS_CON", "20190614_WAS_SEA", "20190618_WAS_LAS", "20190620_WAS_LVA", "20190525_WAS_CON", "20190525_CHI_MIN", "20190601_CHI_SEA", "20190605_CHI_WAS", "20190609_CHI_SEA", "20190611_CHI_PHO", "20190615_CHI_IND", "20190619_CHI_NYL") # Split out on underscore and coerce to data.table ------------------------ game_ids_split &lt;- strsplit(game_ids, split = "_", fixed = TRUE) anyDuplicated(game_ids) # FALSE # to dt split_dt &lt;- as.data.table(do.call(rbind, game_ids_split)) names(split_dt) &lt;- c("game_date", "home", "opp") # Handle duplicates within atomic splitted vars --------------------------- split_dt[, vapply(.SD, function(f) any(duplicated(f)), logical(1))] # game_date home opp # TRUE TRUE TRUE # assign a row integer ID to uniquely identify after reshaping, since # we have dupes (once we split out). You could also just append the original # concatenated game_id if that will always be unique split_dt[, rowidx := .I] # chr date to actual date ------------------------------------------------- split_dt[, game_date := as.Date(game_date, format = "%Y%m%d")] # Melt -------------------------------------------------------------------- # not sure if this is what you are looking for? melted &lt;- melt( split_dt, id.vars = c("rowidx", "game_date"), measure.vars = c("home", "opp"), value.name = "team", variable.name = "team_type", variable.factor = FALSE ) setorder(melted, rowidx, game_date) knitr::kable(head(melted)) # | rowidx|game_date |team_type |team | # |------:|:----------|:---------|:----| # | 1|2019-05-26 |home |LAS | # | 1|2019-05-26 |opp |LVA | # | 2|2019-05-31 |home |LAS | # | 2|2019-05-31 |opp |CON | # | 3|2019-06-04 |home |LAS | # | 3|2019-06-04 |opp |NYL |
- You might want to start writing this as a package from the beginning. It makes testing and extending easier for you as well. It also means you won't have to clean up those `source` commands later - You shouldn't need an `alamo_cmd` argument. Unless there's reason to believe the users won't have alamo in their PATH's. The common way here is to just instruct users to add the executable to their PATHS so that the code that is written doesn't depend on the user's own configuration - When the common way the user will interact with the binary is to read the output file, use temporary files. If you think user may need the file themselves, you can have an optional argument to place the output. I'd expect to be able to get the output into R immediately after executing the alamo command. No need to complicate things by adding a separate function for that. - You can probably make things run faster if you get rid of the middle man but I am unfamiliar with how fortran interacts with R so I wouldn't bother. Unless the data gets massive no one will care that you're loosing a few secs reading-writing
I really appreciate the time you put into suggesting a solution! The difficulty with this data frame is that I initially do have the teams and dates separated, but the order that occurs in both this data frame and the parent data frame is by the team that's being focused on, not home vs. away. So like **"20190526_LAS_LVA"** just means that the stats in that row are those accrued by LAS. So 20190526_LVA_LAS would be the opposite, it would be LVA's stats against LAS. Know what I mean? The "LOC" column indicates the game's location, so if a team has @, it's at the other team's location and if it has a blank, they are home. I'm not sure how that could be used, besides maybe doing a conditional? IF "LOC" = @, TEAM = AWAY, else HOME?
Thank you, great advice. Can you elaborate on: "one big thing is that you no longer call "library(...)" at all in your library, you let the NAMESPACE and whatever take care of it." I am relatively new to R and I'm not sure what you mean. How do I let the NAMESPACE(?) take care of it instead of reading in other libraries?
Read the tutorials for details. http://r-pkgs.had.co.nz/namespace.html
I would start by grouping by date and product, and then aggregate the selling price. It should then be easy to create a rolling average.
On mobile, sorry for any issues If all you want to do is generate the same game ID for both teams then in tidyverse I’d do df &lt;- mutate(df, ID = ifelse(LOC = “@“, paste(date, team1, team2, sep = “_”), paste(date, team2, team1, sep = “_”))
Lots of possible solutions here: https://stackoverflow.com/questions/743812/calculating-moving-average No Tidyverse ones though. This might be another route? https://cran.rstudio.com/web/packages/tibbletime/vignettes/TT-03-rollify-for-rolling-analysis.html
FYI tibbletime is part of the tidyverse
Ok, this helps to clarify. Let me make sure that I understand fully, though: &amp;#x200B; * Each game is represented by two rows of stats * At the moment, there is no natural identifier in your data to denote the fact that two rows should be related (by virture of the fact that they are the same game, but for each team If I am still on the right track, then my confusion might have been from the fact that I don't see any records in your google sheet that demonstrate this characteristic. I see 20190526\_LAS\_LAV, but not 20190526\_LAV\_LAS. Could you supply some sample rows that illustrate this?
A brute force but simple solution would be to use the filter function from the dplyr package and loop over all possible products and calculate the mean for each product. Then it would look something like this: products &lt;- unique(dataset$product) for(i in products){ dataset %&gt;% dplyr::filter(product == i) %&gt;% calculate mean }
How about df %&gt;% group_by(product) %&gt;% arrange(date) %&gt;% mutate(roll_avg = cumsum(sell_price) / row_number())
Thank you, but unfortunately the date group is moving as well so this may work, but it seems to be memory intensive.
Okay so I think that creating a range with tibble time is the way to go. Not sure how to entirely achieve that, but that is what Saturday is for haha.
[https://stackoverflow.com/questions/46396417/r-cumulative-sum-over-rolling-date-range](https://stackoverflow.com/questions/46396417/r-cumulative-sum-over-rolling-date-range) &amp;#x200B; It took lots of searching, but here is the official answer haha.
`testthat()` is pretty good IMO, check out http://r-pkgs.had.co.nz/tests.html#tests
Yup, you're right. It's the same game, but the IDs differ because the nomenclature is ID then Team then Opponent. So I would need to call the two unique ID's if I wanted to compare them even though they are only representing the same, single game. Row 22 and Row 38 demonstrate this. The IDs are: 20190609_WAS_DAL and 20190609_DAL_WAS
I will take a look at the `testthat` package again. I evaluated it when it was first released, and I did not like it. However, I think my biggest problem was one of expectation. Although I have been writing R code for over a decade now, I was writing C almost three decades ago, and before I started using R, I had been using C++ for nearly a decade. I have always had a love/hate relationship with R. On the one-hand, it has been invaluable to me for statistical analysis and data visualization. On the other hand, for someone coming from C++, R has so many *stupid* conventions (don't get me started on R's half-assed data type enforcement). I had already made peace with R about the time that the `tidyverse` came along, so, initially, I refused to learn it. However, packages like `dplyr` (which I use almost daily now) along with pipes (which are so conceptually similar to Unix/Linux pipes and C++ streams) started showing up in solutions to questions on Stack Overflow so often, I knew that I could not continue to ignore the `tidyverse`. Now that I've gotten acquainted with the `tidyverse` (as well as `data.table`—my god that package is amazing!), cleaning data has gone from something akin to a dentist visit to an activity I truly enjoy. I'll give `testthat` another look because I think I'm in the right frame of mind, now.
You're the second person in this sub-thread to mention it. I'll definitely give it a look. Thank you!
Got it. Have some ideas on how to address this, and will post later today.
I think adding the `alamo` binary to the `PATH` for a package that is ostensibly a wrapper intended to hide the binary isn't as friendly as just letting the user specify where the binary is. Re wrapping the code in a DLL... this would be dramatically cleaner, but it is your itch you are scratching, so it is your call. There are some quirks you have to get used to in calling compiled code, but they mostly have to do with the nature of the compiled language... you may have to study more if you have not used Fortran before. R actually makes it surprisingly easy to call Fortran and C in a package, though some people advocate writing a C++ wrapper with RCpp to make the R side even easier.
Yeah I see my language was ambiguous. I meant no tidyverse in the SO thread.
&gt;You might want to start writing this as a package from the beginning. It makes testing and extending easier for you as well. It also means you won't have to clean up those source commands later Yeah. When I started my other project a few months ago, I thought writing it as a package would slow down development, which was likely true at the moment but lead to poor organization and not being able to leverage autocomplete/documentation to speed up current use. I just copied the alamo related code from a single project file and reorganized it for this post. &gt;You shouldn't need an alamo\_cmd argument. Unless there's reason to believe the users won't have alamo in their PATH's. The common way here is to just instruct users to add the executable to their PATHS so that the code that is written doesn't depend on the user's own configuration Noted. I just threw that in there in case one I wanted to pass the location of one of the development binaries at some point. The software is distributed as a tarball for Linux and an installer for windows, so there may not be any guarantees it and its license are in the path. Maybe I will remove the argument and just make a package variable. &gt;When the common way the user will interact with the binary is to read the output file, use temporary files. If you think user may need the file themselves, you can have an optional argument to place the output. I'd expect to be able to get the output into R immediately after executing the alamo command. No need to complicate things by adding a separate function for that. Got it. That what I figured.
&gt;Re wrapping the code in a DLL... this would be dramatically cleaner, but it is your itch you are scratching, so it is your call. Yeah. The cleanliness is attractive but ultimately I think it would be more of a selfish endeavor and not the best use of my time. I want to avoid the redundancy of having to add new parameters, error codes, etc. as they are added to the program but I guess that is the point of a wrapper. There are already let's say 50 options. Maybe there is some way I can automate the code generation, or allow the user to specify a list of advanced options that aren't hardcoded into the routine. &gt;R actually makes it surprisingly easy to call Fortran and C in a package, though some people advocate writing a C++ wrapper with RCpp to make the R side even easier. The dynamic load and .Fortran are alluringly simple, but I guess you have to explicitly type check variables, etc. I think using RCpp simplifies the type checking a bit. I'll keep it in mind. Thanks!
I think its because of a single column df. You can try my_data_i &lt;- my_data[-i,,drop=F]
I'd really recommend using the tidyverse packages. my_data_i &lt;- my_data %&gt;% fliter(row_number() == i)
Packages are your friend! I highly recommend this.
I see several issues here: 1. Sticking an image into the question communicates what you are seeing, but provides remarkably little information to the person reading your question. If you can provide a complete sequence of statements that leads us to where you became confused, our responses become more accurate and quick. 2. Your `my_data` object looks like a `tibble` rather than a data frame. It does not bother me that you are using a `tibble`, but `tibble`s behave differently than data frames (`tibble`s are designed to be more predictable, which some regard as an advantage while others regard as a disadvantage), so your description of the problem was misleading. Specifically, if `my_data` is a `tibble`, then writing `my_data[-1,]` definitely does not behave the way you describe, but a `data.frame` does. 3. Your `my_data` object appears to contain two rather different kinds of information, and both kinds are encoded into a single factor which is not helpful. Factors are very useful later in the analysis phase, but it is rarely a good idea to start out working with factors in the data munging stage. Below I will show some code that avoids starting out with this factor representation, but it isn't central to your question. 4. Vectors are very useful basic data types in R for performing computations... so useful that somewhen in the history of R they decided that for a `data.frame` called `mydat`, the expression `mydata[ , 1 ]` (or any expression that retrieves a single column) should "intuitively" return the column as a vector rather than as a data frame containing one column to facilitate doing further computations on that column. When you formed an expression that removed one row, the remaining expression was a single column so R dropped the `data.frame` wrapping that column and returned it as a (factor) vector. 5. You appear to be attempting to use a valid date as a column header, which mixes the information you want to process with the labels you are using to refer to that data. If your original data file has no header line then don't just blindly assume it does. You can for check this kind of thing using a text editor before you read in the file. So here is a sort of reconstruction of what I think could have lead to what you showed: library(tibble) # questionable code my_data0 &lt;- read.table( text = "2019-01-19: 1 2019-01-20: 4 2019-01-22: 2 2019-01-26: ", header=TRUE ) str(my_data0) my_data &lt;- as.tbl( my_data0 ) str(my_data) my_data # what you claimed you were indexing my_data0[ -1, ] # you actually indexed a data frame my_data0[ -1, , drop=FALSE ] # necessary to keep data frame my_data[ -1, ] # tibbles behave differently Here is my concept of a (possibly?) more useful approach # use arguments to avoid mixing data into header and no factors my_data1 &lt;- read.table( text = "2019-01-19: 1 2019-01-20: 4 2019-01-22: 2 2019-01-26: 3 ", header=FALSE, stringsAsFactors=FALSE ) str(my_data1) # examine intermediate result my_data1 # examine intermediate result # a matrix is a "folded" vector, not a data frame my_data1m &lt;- matrix( my_data1$V1, ncol=2, byrow = TRUE ) str(my_data1m) # examine intermediate result my_data1m # examine intermediate result my_data2 &lt;- as.data.frame( my_data1m, stringsAsFactors = FALSE ) str(my_data2) # examine intermediate result my_data2 # examine intermediate result #replace colon at ends of strings with an empty string (nothing) # read ?`regular expression` and ?sub my_data2$V1b &lt;- sub( ":$", "", my_data2$V1 ) str(my_data2) # examine intermediate result my_data2$V1c &lt;- as.Date( my_data2$V1b, format = "%Y-%m-%d" ) my_data2$V2c &lt;- as.numeric( my_data2$V2 ) str(my_data2) # examine intermediate result plot( V2c ~ V1c, data = my_data2 ) You can avoid littering your workspace with variables by using `dplyr` pipes: library(dplyr) my_data3 &lt;- ( read.table( text = "2019-01-19: 1 2019-01-20: 4 2019-01-22: 2 2019-01-26: 3 ", header=FALSE , stringsAsFactors=FALSE ) %&gt;% `[[`( 1 ) # pull out column 1 as vector %&gt;% matrix( ncol=2, byrow = TRUE ) # wrap into 2d %&gt;% as.data.frame( stringsAsFactors = FALSE ) %&gt;% mutate( V1 = as.Date( sub( ":$", "", V1 ) , format = "%Y-%m-%d" ) , V2 = as.numeric( V2 ) ) ) plot( V2 ~ V1, data = my_data3 )
Wonderful, thank you! This shows promise. However, when I try to use that function (having installed a bunch of packages), I get an error message saying row_number() should only be called in a data context Do you know why this might be? Stackoverflow talks a lot about mappings in the dplyr source code when I google it but that doesn't really tell me anything
what is the output when you call str(my_data) just before this command? Acutally, I think base R will convert a single column dataframe to a vector which would explain your original problem.
Thank you! I'm really just brute forcing this and even after editing my post a couple of times it doesn't really include any useful information whatsoever. Like I'd be mildly annoyed were anyone to post a question like this in a subreddit dedicated to a language I knew. Anyhoo, this app_opens &lt;- read.csv('app_opens.csv') my_data_1 &lt;- app_opens[1, ] my_data_1 or head(my_data_1) is basically what I did when I just tried to view the row. Your explanation in 4 seems reasonable, and for 5, would messing with the file and adding some sort of header help out in this case?
Here's an updated approach using `data.table`, which you should be able to translate into your data frame manipulation approach of choice. The general idea is to isolate the "home" vs. "away" concepts, then generate IDs on the combination of date played and: * `TEAM` for "home" * `OPP` for "away" Therefore, a lot of the below could be streamlined, but hoping that breaking out my thought process will make it easier for you to adjust to your specific needs: library(data.table) dat &lt;- setDT( read.table( "clipboard", sep = "\t", header = TRUE, stringsAsFactors = FALSE ) ) # extract date part of GAME_ID as explicit date dat[, date_played := as.Date(sub("\\D", "", GAME_ID, perl = TRUE), format = "%Y%m%d")] # create another variable called 'is_home' based on LOC dat[, is_home := LOC == "@"] dat[c(23, 40)] # Split the data into two tables based on whether home or away ------------ # We need a concept of home and away # One of these two tables needs both teams; the other only needs one # first, let's define the measure cols cols_measure &lt;- c( "SCORE", "FG", "FGA", "X3P", "X3PA", "FT", "FTA", "ORB", "DRB", "TRB", "AST", "STL", "BLK", "TOV" ) cols_dim.home &lt;- c( "date_played", "TEAM", "OPP", "is_home" ) cols_dim.away &lt;- c( "date_played", "TEAM", "OPP", "is_home" ) # Make sub-selects for each of home and away ------------------------------ # home first dat_home &lt;- dat[(is_home), c(cols_dim.home, cols_measure), with = FALSE] # set an id on dat_home; this is your unique identifier dat_home[, id := paste(date_played, TEAM, sep = "_")] # now away dat_away &lt;- dat[(!is_home), c(cols_dim.away, cols_measure), with = FALSE] # set the same id on dat_away, but using 'OPP' col AS 'HOME' dat_away[, id := paste(date_played, OPP, sep = "_")] # Now combine ------------------------------------------------------------- out &lt;- rbindlist( list(dat_home, dat_away), use.names = TRUE ) setcolorder(out, union("id", names(out))) setorder(out, id) First six and a few select columns: knitr::kable(head(out[, .(id, date_played, TEAM, OPP, is_home, SCORE, FG)])) # |id |date_played |TEAM |OPP |is_home | SCORE| FG| # |:--------------|:-----------|:----|:---|:-------|-----:|--:| # |2019-05-24_DAL |2019-05-24 |DAL |ATL |TRUE | 72| 26| # |2019-05-24_DAL |2019-05-24 |ATL |DAL |FALSE | 76| 27| # |2019-05-25_CHI |2019-05-25 |CHI |MIN |TRUE | 71| 28| # |2019-05-25_PHO |2019-05-25 |SEA |PHO |FALSE | 77| 28| # |2019-05-25_WAS |2019-05-25 |WAS |CON |TRUE | 69| 27| # |2019-05-26_LAS |2019-05-26 |LAS |LVA |TRUE | 70| 28|
While it can be tempting to edit the original input file, if you find yourself dealing with a new file at some point then you have to remember what to edit and then insert yourself into the process each time. So unless the file is unusable I tend to look for ways to import it as-is. (Which is not to say that you should never edit a file... just that I prefer to avoid it.) You can see my use of the `header=FALSE` argument above to consume all data rows without assuming the presence of a header line. If you don't like the default names you can change them: app_opens &lt;- read.csv( 'app_opens.csv', header=FALSE, stringsAsFactors=FALSE ) names(app_opens) &lt;- "Values" my_data_1 &lt;- app_opens[ -1, , drop=FALSE ] num_rows &lt;- 2*seq.int( nrow( app_opens ) / 2 ) my_data_vals_vec &lt;- as.numeric( app_opens[ num_rows, ] ) my_data_dts_vec &lt;- as.Date( app_opens[ num_rows - 1, ], format='%Y-%m-%d:' ) I don't see any use of `tibble`s or tidyverse functions in your code, so I still don't know how you obtained the screenshot you provided with the "A data.frame: 6x1" annotation.
It doesn't look like it. Applications to MRI and on the sphere interest me enough that making an R wrapper for one of the the C libraries (or a C++ library, that might be easiest given how Rcpp goes) is now on my list of 'this might be worthwhile to do or, if I teach a stat computing class, give as a possible project', but unfortunately I definitely don't have any bandwidth at all at the moment.
it seems you aren't closing your quotation marks in your GNcountryInfo().
Typo.
Did you try to visit that webpage in your browser? Look at the json response: {"status":{"message":"user account not enabled to use the free webservice. Please enable it on your account page: https://www.geonames.org/manageaccount ","value":10}}
Tidyverse solution is trivial unless I'm missing something: library(tidyr) spread(your_data, term, content) Shouldn't need to, but could group beforehand if that behaves oddly: spread(dplyr::group_by(your_data, mycoID), term, content)
Thank You! You saved me!
Its not working. I found a problem with the way R read my document. While I am figuring out how to fix it, let me ask you, Does this solution take into account the grouping by mycoID. (It looks like it does, but I just wanted to check)
When ever you use read_csv always set the cols parameter.
Using read\_tsv instead of read\_csv fixed the problem on my machine. read\_csv failed to parse the txt file. You can see the reason for the failing by running "problems(bir\_origin)".
There’s probably a package that auto does this but you can just do it yourself relatively easily. So a paired t test is really just seeing if the two samples have a different mean. Your null hypothesis is that they are the same. So you just do a t test on the difference between the two samples. What’s special about clustered p values is just that your degrees of freedom under the null hypothesis change to be the number of clusters. In this case, that’s the number of people. So first compute the t statistic * take the difference between the two samples you want to compare. Call this new column diff. * take mean of diff * divide by the standard error (for the mean estimator, that is standard deviations of diff divided by number of entries in diff) Then use the qt function to find the confidence levels of the t distribution using the appropriate degrees of freedom. See if your t statistic is greater in magnitude than the ones outputted by qt (meaning that they have statistically significantly different means) or not, and you’re done!
&gt;xercise to do these tests from first principles before downloading the package so you have better intuition of what is going on under the hood. &gt; &gt;So a paired t test is really just determining if the two samples have a statistically significantly different mean. Your null hypothesis is that they are the same. So you just do a t test on the difference between the two samples. &gt; &gt;What’s special about clustered p values is just that your degrees of freedom under the null hypothesis change to be the number of clusters. In this case, that’s the number of people. &gt; &gt;So first compute the t statistic &gt; &gt;take the difference between the two samples you want to compare. Call this new column diff.take mean of diffdivide by the standard error (for the mean estimator, that is standard deviations of diff divided by number of entries in diff) Thanks for replying. Following your above steps, I get to a vector of t-statistics (one for each price point). See below. &amp;#x200B; Is this what you meant?
The file isn't structured as a csv (comma separated values). It isn't structured as a rectangular format at all. Using read_delim and setting the delim to something that doesn't appear in the data like tab ("/t") will get you what you want, but you're going to have to do a bit of transformation before you have the data in a usable format.
Don't know what type of plot you're thinking of, but typically people do things in the `aes()` argument like color or shape or fill or they use `facet_wrap()` or something to split it out, but without more particulars it's hard to give guidance.
It's a lot like Catch (and integrates with Catch if you have C++ code in your package), if you've used that and like it.
Just a basic bar graph or maybe line graph. I want to have the price variable on the y-axis, and the property type on x-axis grouping it by neighborhoods (or vice versa).
Yes, if your goal is to test for an equal mean for within price point that is right. And it makes sense to look within price points because different prices probably have different mean demands.
If you have a column in your data frame that tells you which group to bin it by, you would include aes(col = columnname) within your geom_line() to have different colors for each group. If you do not have such a column name, you can either reshape the data such that it does (using gather()) or, you can have a geom_line() for each output variable and changing the y value within each geom_line() using aes()
can you talk a little bit about why you want clustered standard errors? If your data are heteroskedastic with respect to the x variable (treatment), then you should just be doing the formula for a paired-sample t-test but with a Welch t-Test in R instead, right?
If you use `geom_bar` then you may need the `position="dodge"` argument to get the classic side-by-side bar chart.
This exactly. What do you mean by clustered standard errors? An assumption of GLM (t -test) is normality. Clustering is a person-centered methodology, t-test is variable centered, are you doing some kind of mixed method?
Could be that the prof wants a linear mixed model but I don’t know why.
Just remove the header from the document?
Thanks for raising this - and to be honest I'm not sure. I was just told to cluster standard errors at the subject level, I don't really know why. I had assumed that clustering was a way to counter the problem that if we just averaged the demand at each price level for each condition, then variability would be lost. Perhaps I should go back and seek further clarification...
Ultimately I was expecting a single summary comparison across all prices, ie. a single t-statistic rather than a vector of t-statistics.
Seems to work for me too.
After looking at the data it is a mixed design. I think. Where there is a between subjects factor (i.e. dosage) and each subject is measured multiple times. Seems like a mixed methods anova. Unless you want to do latent growth curve modeling, or hierarchical linear modeling. Yeah I would go back and clarify what "clustering standard errors" means
&gt;What’s special about clustered p values is just that your degrees of freedom under the null hypothesis change to be the number of clusters. In this case, that’s the number of people. This is not true in general - the cluster-robust sandwich estimator accounts for the within-group error correlation structure by estimating it using the residuals. Does the estimator collapse to a simple t-test with adjusted degrees of freedom in this special case? It's not obvious to me why it would.
When saying this I was assuming some structure on the data collection, namely that each cluster (the demand for each person) was independent from the other clusters, and that the within would be very highly correlated (i.e two measurements of the same ‘true value’) which seems to be the case the way OP described the data. But you are right that you need this in general, in which OP should use the sandwich library.
I see - that makes sense as a rough approximation (given that the panel is balanced).
In that case you would just combine the diff columns for each price into one diff column and do the t test on that. Sorry for the confusion!
In that case you would just combine the diff columns for each price into one diff column and do the t test on that. Sorry for the confusion!
In that case you would just combine the diff columns for each price into one diff column and do the t test on that. Sorry for the confusion!
 rbindlist(offset.class.ss)
This is the right answer. Here's a full script to load all files from your disk drive and rbind them: library(data.table) my_list_of_files &lt;- list.files(path = "c:/folder_with_files",full.names = TRUE) read_my_list_of_files &lt;- lapply(my_list_of_files, fread, sep=",") dt &lt;- rbindlist(read_my_list_of_files, fill= TRUE)
There are multiple explanations for this. (Atomic) vectors in R can only be of some specific type: logical, character, numeric or integer. You probably heard about these terms. Lists on the other hand are *recursive*, meaning they can contain all kinds of data, such as the ones above, s3/s4 classes (e.g. your model) or even other lists. Data.frames can also be seen as some kind of list, as they contain multiple atomic vectors of different type. So what is the difference? When using atomic vectors, all elements of the vector are known to have the same type! This makes the application of different functions much easier, as these functions already "know" what to deal with. For example, try calculating the mean of a list of numbers with *mean()*. You will end up transforming the list into a numeric vector before applying the function.
thank you for your help :)
Can you post your code?
Why do you need a double loop? Your indexes are just one off so you can do this with for(i in 2:9) and index by i and i+1. You're also going to have one fewer columns in your result than your initial data.
[Here it is](https://docs.google.com/document/d/1aq1gWj6kJT5daRXpB9B-8lGHCkE7hkTAiaKOkY99Dgw/edit?usp=sharing) Warning: It can take a while for the "##Pull the 2018 season" function to process.
Whoops, forgot to reply. The data are implicitly grouped because there is one unique value per measurement per mycoID. When spread you'll end up with one row per mycoID as a result.
Does this work for you? I had to filter out NAs first: minnesota_2018_off_pass %&lt;&gt;% filter(!is.na(PassLocation)) ggplot(data=minnesota_2018_off_pass, aes(x=PassLocation)) + geom_bar() + facet_wrap(~GameID, nrow=4)
Yes it does! Thank you!
`rbindlist` requires the package `data.table`. It is however synonymous with `do.call(rbind, offset.class.ss)`. `do.call` takes two arguments, a function and a list, and calls the function with the list's entries as arguement. It "unpacks" the list and applies to function, so to speak.
Unrelated to your actual question, but you've called tidyr twice.
You're right, I am so used to working with data.table that I didn't even notice haha. Good explanation.
!remindme 11days
I will be messaging you on [**2019-07-06 13:05:59 UTC**](http://www.wolframalpha.com/input/?i=2019-07-06 13:05:59 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Rlanguage/comments/c54y8g/shiny_app_as_a_r_package/es074w9/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Rlanguage/comments/c54y8g/shiny_app_as_a_r_package/es074w9/]%0A%0ARemindMe! 11days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
thank you. it worked. But, would you care to explain why this nested loop isn't working?
Look up the {golem} package workflow by ThinkR
Can you provide a minimally reproducible example (aka [reprex](https://community.rstudio.com/t/faq-how-to-do-a-minimal-reproducible-example-reprex-for-beginners/23061))? It would be much easier to provide assistance with that.
I don't really do sentiment analysis, but I am pretty sure you would need to extract the person identifier from the data before you analyze it, and perform analysis separately for each person.
that's what is typically recommended afaik
Are you trying to do this dynamically ( in a for loop/if statement)? If so I'd advise that you just assign the two variables to internal variables and use paste. TBH paste probably would solve this even if your not doing it dynamically.
And correspondingly, I would've replied to use `bind_rows` from `dplyr`...
Just a heads up, if you use `my_pkg::launch()` you might not need to run `library(my_pkg)`.
The issue here is you've used the wrong arguments and data structure for `spread()`. It takes as arguments a long dataframe, a key variable, and a value variable. You've fed it a dataframe which has no keys, a unit index, and a value variable. Don't use your `myco_subset` dataframe at all. Instead, this should work: df_2 &lt;- spread(myco_table %&gt;% select(-internal_variable_num), term, content)
Oh. Wow. Thanks! Been at this for a while. Was fun.
Your approach doesn't work like you want it to because of two reasons First off, there's a difference between a variable name and a value. Secondly, what you're calling `new_variable` is actually a compound *expression*, not a variable. It so happens that R can solve both of these problems (this is not obvious! Many languages can't solve this problem) but even in R a better solution to the underlying problem is to avoid the problem in the first place: Instead, use a nested list. Let's say that list is called `data`. You can then write data[[input$var]][[input$num]]
You're inevitably going to be linked to [this](https://r4ds.had.co.nz/) textbook.
all you need is [R 4 Data Science](https://r4ds.had.co.nz)
I really liked datacamp. Get an understanding of base r and thrn into tidyverse.
Excuse my ignorance but what is the tidyverse? I'm trying to understand ha!
Thank you
Thanks I'll check it out
Swirlstats.com - interactive lessons on R in R Bookdown.org - bunch of ebooks on various topics in R.
Thank you I'll check these out :)
Tidyverse is a collection of useful packages. Like dplyr. Like when your filtering a dataframe you can Base: Dataframe[dataframe$column == "stuff",] Or dplyr Dataframe %&gt;% filter(column == "stuff" It's just an awesome package to make your life easier. See also Tidyverse.com
```unnest_tokens()``` preserves any other columns in the input, make sure to keep your metadata as column(s) alongside your text.
Echo... 404 no page
good resource: https://db.rstudio.com/getting-started/
It's not working because you're looping way too much for what you actually want to do. You're going through (j,i) = (2,3) (2,4) (2,5) ... (2,10) (3,3) (3,4) ... (3,10) (4,3) ... etc. I assume you just want to compare (2,3) (3,4) (4,5) ... (9,10) instead.
Cheers 😊
Unapologetically, I'm not going to answer your question. Instead I'm just going to ask you why do you want this? At this point, you just do your analysis in R... Applying functions to columns of a data frame is dead simple. And reading excel into R isn't difficult either. At this point why bother?
You could run R scripts in excel through VBA macros. This would allow you to pass arguments, such as values from cells in your workbook into the R script and if you get the R script to write out your results to a file, they can be read back in to excel, all in the same macro. A little sloppy perhaps, but it works. Here is how you could execute the R script in VBA: executeR = "Path/To/Rscript.exe Path/To/R/File" &amp; " " &amp; arg1 &amp; " " &amp; arg2 &amp; ... Dim sh as Object Dim err as Integer err = sh.Run(executeR,0,True)
You could run R scripts in excel through VBA macros. This would allow you to pass arguments, such as values from cells in your workbook into the R script and if you get the R script to write out your results to a file, they can be read back in to excel, all in the same macro. A little sloppy perhaps, but it works. Here is how you could execute the R script in VBA: executeR = "Path/To/Rscript.exe Path/To/R/File" &amp; " " &amp; arg1 &amp; " " &amp; arg2 &amp; ... Dim sh as Object Dim err as Integer err = sh.Run(executeR,0,True)
Is your colleague trying to migrate from 'using ' Matlab in Excel to 'using' R in Excel? Does he have experience working directly in Matlab? I am also unclear why he would need such a roundabout workflow; what are the underlying requirements? And what is Excel - -&gt; Matlab lacking that he is looking to R (or is this a paid vs open source matter)? We might be able to offer more useful advice with some more background info.
He uses Excel as a front end to financial risk and valuation models that he's built in Matlab. Tweak exposures or alpha assumptions to certain asset classes in Excel, submit the exposures and a covariance matrix into Matlab and it spits out risk numbers. That's his work flow as he's developing the models. Once he's done we compile the Matlab code into DLLs and call them from web or desktop apps. The firm is moving towards R and he's trying to come along at his own pace. Right now with his models built in Matlab, there are 3 people in the firm that can work with them. If he moves to R there are about 20.
Hey thanks for the help. That's basically how the excel.link package works. I'd like to avoid VBA but it's definitely an option should nothing else work out.
Data entry is easier in Excel than in R and he's in Excel all day for other reasons.
Idk about recommendations, but it's what I do (intuitive if nothing else)
Because excel is how you end up making mistakes for starters
Teach him RMarkdown and Readxl. He'll be in love very soon
Why not use python? I have script that does that in python. Let me know
Thanks for the additional insight. I appreciate the challenges when it comes to adopting a different tool and trying to limit that change as the sole variable in the context of an existing workflow. I have zero experience trying to call R from Excel (only the other way around), so I can't help on that front; the advice above re: VBA sounds it at least provides a fallback. If that ends up being more trouble than it's worth, I think he'll find R quite approachable and well-suited for his work. I don't have experience with risk modeling, but I do quite a bit of multivariate analysis, and it more or less boils down to some inputs, some pre/post-processing, some transformations, and the critical list of factors/parameters. The inputs can easily be pulled in from Excel if need be, and reloaded into R, pretty much instantly. Parameter-lists are perfectly suited to R `list`s. Then it's a matter of writing a function that handles one case, and batching it with e.g. `Map()`. At the risk of assuming too much, I'd strongly encourage your colleague to evaluate the overhead of trying to find a workaround to easing the transition, to just ripping off the band-aid.
I have never used this but seen it come up in a few conversations. https://bert-toolkit.com/
Yeah I get that. Old habits die hard. But eventually you have to graduate to "big boy" data analysis :) Lock down an Excel spreadsheet of data you've entered, then plug it into your R pipeline.
Show me!
Importing spreadsheets is super quick with odbc: library(odbc) #create odbc connection #driver depends on your version of SQL myconn &lt;- dbConnect(odbc(), Driver = "SQL Server", Server = 'server', Database = 'database', uid = 'username', pwd = 'password') #load csv sheet &lt;- read.csv("table.csv") #write to database referenced in dbConnect dbWriteTable(conn = myconn ,name = "database_table_name" ,value = sheet)
Could you explain the select, -internal\_variable\_num a bit?
That is a variable you don't need which, because it takes multiple values for each `mycoID`, would mess up spreading your data into wide format. Every other variable takes a single value for each `mycoID` so we can just `spread()` them out and get one column per variable and one row per `mycoID`. If we `spread()` it out with that variable included we'll get one row for each value of `internal_variable_num` per `mycoID`.
Lol
OP may be able to get rid of the second loop - ```(ex[, 3:10] - ex[, 2:9])/ ex[, 2:9] * 100```. For readability it makes sense to assign a vector like ```first_year &lt;- 2:9``` but not completely needed.
Thanks - I mention at the bottom of the post that I tried that. It works in that I can successfully call R functions, and I can get back charts or single numbers as return values. Where it's failing is when i want to return a vector. It gives me the top left cell of the vector and nothing else.
Sorry I didn't read the whole thread I guess 😅.
The link you posted gives a `404 error`. Did you actually mean to post [this link](https://github.com/knowledgebaum/matchmaker/blob/master/data/csv/vei__.csv)? Also, what is your reason for creating a SQL database from a single table, and why do it with R? With only one table, if you are using R, there really isn't much value to putting a single CSV into a relational database. You can do all of the roll-ups and summaries using either `base R` or something like `data.table` or the `tidyverse` packages, such as `dplyr`, `tidyr`, and `purrr`. Using this approach is far more powerful and reproducible than using SQL. With all of that said, if all you want to do is write SQL queries from R against a single table, the simplest approach is to use [RSQLite](https://cran.r-project.org/web/packages/RSQLite/index.html). `RSLite` is a wonderful relational database implementation written in C. It's lightning fast, and I believe that it supports a fully standard compliant version of SQL.
If the stocks are stored in some kind of table that has the dates of measurements (stock) then use lubricate %within% function to Check which dates fall between that interval. https://lubricate.tidyverse.org
That was the link I meant. This project is about 1/100th the size of what I am actually trying to do. There are about 20 Megs of text files I am trying to convert to SQL from the flat text format. My goal is to store the data in a better container which can be queried and is persistent.
I'd just add a helper column where you start with 1, then multiply by each successive monthly change (+1 if needed). This number can then act as an "index" for calculating returns over longer periods.
&gt; "...20 Megs of text files..." &gt; "...My goal is to store the data in a better container which can be queried and is persistent." OK, this information is very helpful. Although modern relational databases can scale better than they used to, they still don't have the flexibility and performance of NoSQL databases. In my opinion relational databases really shine when they are used for enforcing data integrity constraints with a schema/data model that is well defined and not evolving quickly. ##Should you chose a SQL or NoSQL DB? If you're not going to be linking/joining this table to other tables, do you really need a relational database? If you're simply looking for performance and a robust container for the data which enables flexible queries, is a SQL DB really the best choice for you? ##Which one should you choose? These days, 20 MB is actually a rather small data set. So, even on a low end laptop, I would expect SQLite to perform very well on such a small data set. Also, the RQLite connector works very well. If you absolutely want to use SQL and you only have 20 MB of data, then SQLite might be an excellent option, but only if the following caveats are not an issue for you. ## Even if you go with SQL, do you really need a server or just something that supports an SQL interface? Although SQLite supports a standard SQL syntax and is very (as in *VERY*) fast, it doesn't have a server. So, if you need all of the overhead that comes with a true RDBMS (*e.g.* a server, security roles, logging, multiuser concurrency, etc.), then SQLite won't be your best option. Rather, you should look at a true RDBMS, such as PostgreSQL or MySQL (despite all of the haters and Oracle taking this over, I still like MySQL). Also, if you're not going to be using the RDBMS to enforce data integrity constraints and if you won't be joining this data table to other tables, then, honestly, I would not use a relational database at all. Rather, I'd go with a NoSQL DB such as MongoDB or [Redis](https://redis.io/). I especially like Redis for performance and ease of use. Just note that Redis is an in-memory DB, which is why it is so fast. However, for this reason, you have to manage concurrency caching yourself and write the DB to disk if you make changes to it. There are always so many tradeoffs to consider for choosing a particular technology stack. Without knowing your full usecase, I can't make an ideal recommendation for you. If you were one of my clients, here's the questions I'd ask you in order to help you make that decisions. ## Decision Criteria 1. Do you actually need a server? Specifically, do you need to have data security managed and data integrity enforced at the data persistence layer itself. 1. If the answer is *yes*, then you should consider a server based database. 1. If the answer is *no*, then do not use any server based database. 1. Should you use a relational (i.e. SQL) or NoSQL DB? To answer this question, you should answer these sub-questions. 1. Do you already know SQL very well? 1. Is this data part of an overall relational data model with many different related entities? 1. What type of users will be querying the data, and how will they query it? 1. **Case 1**: End users are *business domain experts* (i.e. not IT experts) with no specialized database skills. They will be accessing the data via a controlled interface that you develop. 1. **Case 2**: End users are database experts that will be accessing the data via a CLI. They will need the ability to write their own complex queries. For question 2, unless you can answer *yes* to both questions, I would likely not use a SQL database. For question 3, if the answer is Case 1, then the choice of database doesn't really matter. However, if it's Case 2, then you need to choose a database technology with which your users are familiar, and since SQL is by far the most established DB language, you'll probably have to go with SQL. Don't worry if all of this stuff sounds complicated. It's really not. If you're working with R regularly, you'll have no problem figuring this stuff out. Let me know if I can help!
I assume you are opening the file in Excel? That is Excel's doing. It assumes they are dates. There's nothing on the R side you can do to fix that except modify them to not look like dates. For example, change "1-5" to "1 to 5" or "’1-5" (apostrophe before number--the apostrophe usually won't show in Excel).
excel is the devil, I recommend removing it as the default application to open anything but xlsx
Thanks guys!
https://cran.r-project.org/mirrors.html
https://github.com/ThinkR-open/golem/blob/master/README.md
Chrome says: ``` This site can’t be reached cran.r-project.org’s server IP address could not be found. DNS_PROBE_FINISHED_NXDOMAIN ``` Confirmed using Google DNS, I cannot resolve cran.r-project.org ``` ⇒ dig cran.r-project.org @8.8.8.8 ; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; cran.r-project.org @8.8.8.8 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: SERVFAIL, id: 14633 ;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;cran.r-project.org. IN A ;; Query time: 3284 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Wed Jun 26 16:42:57 2019 ;; MSG SIZE rcvd: 36 ``` Package: dplyr R version: 3.6.0
Chrome says: ``` This site can’t be reached cran.r-project.org’s server IP address could not be found. DNS_PROBE_FINISHED_NXDOMAIN ``` Confirmed using Google DNS, I cannot resolve cran.r-project.org ``` ⇒ dig cran.r-project.org @8.8.8.8 ; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; cran.r-project.org @8.8.8.8 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: SERVFAIL, id: 14633 ;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;cran.r-project.org. IN A ;; Query time: 3284 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Wed Jun 26 16:42:57 2019 ;; MSG SIZE rcvd: 36 ``` Package: dplyr R version: 3.6.0
No idea because I can access it and can update packages. I went through this in another reddit post with someone and they updated their RStudio or r version or something and it worked but I have no idea why.
Thanks, that narrows it down a bit. Sounds like the servers hosting cran are still up and you can access it because the DNS lookup is cached. Hopefully this will just magically fix itself by tomorrow :)
You might be able to use install_github to get packages from the cran mirror in Github. You will have to install the dependencies yourself though.
I'm having similar issues.
You could look at gene correlation, but why not actually build a model that takes into account gender? One way of looking at your problem is to construct a nested model that shows there is an interaction between gene and gender when predicting diabetic state. By the way, what criterion are you using for diabetic state? Obviously, you could use a clinical criterion (*e.g.* fasting glucose level, A1C level) based on a threshold cutoff so that for each patient, your status would be either 1) **DIABETIC** or 2) **NOT DIABETIC**, and you could use a multi-logistic model with genes and gender as the independent variables to predict diabetic state (*i.e.* the dependent variable). However, I believe you'd get more insight from using a continuous dependent variable, which, of course, assumes that you have the data. Your point about correcting for multiple testing is a very good one. If you're going to test for many genes, then select only a few for the model, you definitely need to be very clear about your process for variable selection. In non-targeted experiments like this, I will often use the full data set in my model. If I reject any predictors, I'll be very clear in my reasoning for doing so. In summary, take a look at nested and fixed effects models, multiple ANOVA, and multivariate logistic regression. You should also consider RandomForest, which is most often thought of as a classification method, but can also be used for regression, too.
I think OP is just looking for an ancova. m1 = lm(gene2 ~ gene1 * gender * diabetic, data = df) The relevant info will be (1) the graphs and (2) the interaction terms. A significant three-way interaction could mean: the relationship between the genes depends on gender and also whether the person is diabetic. If the three-way is not significant, drop that term out and examine the two-ways.
How much RAM are you working with? You can try a couple different things, including deleting other data.frames you don't need any more and running garbage collection gc() (to make sure it cleared up enough memory to work). You can also close other programs running in the background.
I got 8GB RAM.
Yeah it got hung up on 750 kb so you probably just need to close some background processes or delete some objects you don't plan on using anymore in your environment.
Cleaned the global environment and closed everything. Did not work
even then, 750KB is tiny, I suspect OP might also be on 32 bit R.
&gt; sessionInfo() R version 3.6.0 (2019-04-26) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows &gt;= 8 x64 (build 9200)
Have you tried using the link in submit form and the download.file method at the bottom of the tutorial you posted? First argument is the link, second arg I’m guessing what you want to name your file locally.
Wait. I think your code is wrong. You do not need to make the first two data frames. Just create the DF directly in one step without using cbind. Then just write that out. Everything else seems unessessessary.
This is one of those times when an explicit loop can help. You could cbind subsets of your inputs and write them out, appending to the text file. If you take this approach, you may find it easiest to create the file with a header row only, and write to that each time, sans headers.
Maybe look at `slice()`. This can pull data from a group by their position in that group.
Yes, but it just downloads a file not a pdf. I've searched for something like a writePDF instead of writeBin but no luck. Current work around is using an extension called "Tab Save" to just download all the pdfs through chrome
Have you tried making sure the name ends with .pdf and opening in a pdf viewer program?
It doesn't sound like you need any loops. You just want the two rows from each grouping of ppid? Yaw_threshold = 3 data.frame(ppid = rep(letters[1:2], each = 10), Yaw = rnorm(20,2) + 1:20 / 2) %&gt;% group_by(ppid) %&gt;% filter(Yaw == Yaw[Yaw &gt; Yaw_threshold][1] | Yaw == max(Yaw)[1])
out of curiosity - why would a loop help here? less memory at once, since it's going row by row?
Wow you are right, how can I make it so that all of them come with a .pdf in the name? Also, is it not possible to just feed that logged-in version of the site straight to extract\_tables() instead of downloading it and then feeding it?
Solved thanks to /u/ [13ass13as](https://www.reddit.com/user/13ass13ass/). It actually does download the pdf file but not in pdf format! url &lt;- 'https://www.krollbondratings.com/show_report/20265' session &lt;- html_session(url) url &lt;- jump_to(session, "https://www.krollbondratings.com/auth?uri=/show_report/20265") form &lt;- html_form(read_html(url))[[2]] filled_form &lt;- set_values(form, email = "my_email", password = "password") pdf &lt;- submit_form(session, filled_form) download_url &lt;- 'https://www.krollbondratings.com/show_report/20265' writeBin(download$response$content, basename(download_url))
Pretty much. You are dealing with side effects anyway by writing to a file, so in this case, you can move on with your life if you temporarily ignore the best practice of "tell R what you want" by telling R how to do things. Ideally, you address the fundamental upstream inefficiencies. But (for example) there are times where you cannot have R apply a function across all columns in a single lapply call, but calling that function on one column at a time keeps you under mem limits.
Second question I don’t know. First question, maybe you need to do additional string manipulation to the argument that looks like basename(download_url). Try adding a “.pdf” to the end of that string.
Check out the `plotly` package. You can create a normal `ggplot2` plot object and just wrap it in `plotly::ggplotly()` and it gives you the hover feature. Here is a great example that you could probably tweak. https://plot.ly/ggplot2/geom_point/
Try plotly. It's a little more interactive than some of the other charting functions
Can it be exported into a standalone app without having to access the internet for the plotly api? because thats what I'm afraid of
I'm not sure, I'm no expert with deployment. Maybe someone else could answer that.
Ok no problem I'll definitely check it out though!
If you can't get installr to work, I would recommend downloading and installing the latest version manually. And then copy or reinstall your packages to this installation. https://www.r-project.org/
This exactly right. No loops necessary.
Itʼs unlikely to help. `cbind` on data frames is extremely cheap, it merely needs to allocate and copy a couple of pointers. In reality the code OP has shown is almost certainly unrelated to the actual problem, since 200k rows are also not a lot, and should fit easily into the available RAM unless the individual column data is truly huge.
\##### \# Section Name \##### &amp;#x200B; I will do something similar prior to loading libraries with additional lines for my name and dates.
I do like # Title #### and then if you're using RStudio it makes it a section you can jump to in the bottom left of the source editor pane.
Oh i see. I'm aware from my computer to implement this but if my logic is correct, that `filter()` operator is filtering the first yaw value above threshold, and then the first value the maximum yaw? This is good, however I need the entire row. Does the 1 in square brackets represent selecting the entire row from the filter?
Something is causing copies in OP's code. I assume that at least one of the columns is large, since they are phrases. If both of these are true, then I see no reason why avoiding the cbind that causes memory issues by writing out subsets of each input in a loop would be worse. Of course the actual issue should be addressed upstream, and we have no repex to test.
A good trick I learned # Title 1 ---- # * Subtitle 1 ---- # ** Sub subtitle 1 ---- # Title 2 ---- Looks good in Rstudio browser.. like a navigation tree
Use Rmarkdown with it, and it will be all standalone.. usually results in a few Mb Html. All good
I do this: # Section --------- and indent to a visible margin I have set at 80. It's really important to me to put dashes ("-") to the end of the line. In Rstudio, it lets you collapse everything below it until you get to the next set of dashed lines. You can jump to a section using the bottom in the bottom left of the editor.
&gt;Something is causing copies in OP's code. Not in the code they’ve posted^(1): The only copies happening there are shallow. And, as mentioned, that’s a handful of pointer copies, in the order of dozens of bytes. It will *not* result in a ≥ 750 kiB allocation (which, at any rate, is tiny — the problem is caused by something else and the 750 kiB are just the drop that makes the barrel overflow). &gt;I assume that at least one of the columns is large That’s irrelevant since, as I said, *no column is copied in OP’s code*. &gt;I see no reason why avoiding the cbind that causes memory issues by writing out subsets of each input in a loop would be worse. Because, as I mentioned, while `cbind` doesn’t create copies of columns, *subsetting does*. --- ^(1) Well actually `write.csv` *does* create a copy of the whole DF because it’s … badly implemented. But OP’s code crashes way before that.
Recently found the navigation tree by accident (Ctrl + Shift + O I think). It's so helpful! And Similar to you I do # Title 1 #### # -- Subtitle 1 #### # ---- Subtitle 2 ####.
\######################### \#######This is a section####### \#########################
Functions. If the file necessitates sections itʼs probably a good idea to split the file up.
Don’t do this. If I need sections with titles and extended comments like you're saying, I'll use RMarkdown. This is rare. As much as possible I stick to the adage that comments are deodorant for stinky code. Use functions and packages. Watch Hadley Wickham’s talk on writing good functions. https://m.youtube.com/watch?v=Qne86lxjgtg
I usually Justin do the below: # ============================ But after seeing some of the other comments on here I might change my style up a bit
\########################################## We're talking about this \#### this needs to be done #### \#useful component \#useful component &amp;#x200B; \#### this needs to be done #### \#useful component &amp;#x200B; &amp;#x200B; \########################################## now we're working on this instead &amp;#x200B; &amp;#x200B; I like the white space, and it works well with Rs organization
What do you mean the date group is moving? Try grouping by lubridate::year(date)
I just switched to using R notebooks, but I'm having trouble finding useful tutorials about how to set the options for the chunks. The help page that RStudio sends me to, explains some stuff, but I'd like a bit more of a tutorial kind of thing... and not for the super basics. Do you have any tips maybe?
Rstudio has a New Section menu option. The keyboard shortcut is ctrl+shift+r. Alternatively functions and notebooks can help organise your code, as people have said, as can organising your code into multiple files.
Why not subset(df2, v1 == a | v1 == 'all', v2 == c | v2 == 'all')
I'm a big fan of curly braces with a couple of comments and a long line of hyphens to separate sections of code. The three benefits are that I can collapse the sections, I can run a single section in RStudio without having to highlight all the code in the section (pretty minor advantage but I like it), and I just like the look of it. &amp;#x200B; {# Import packages and functions ---------------------------------------------------------------------- library(data.table) source("myfunctions.R") }# End Import packages and functions {# Import data ---------------------------------------------------------------------------------------- DT &lt;- readRDS("importantdata.RDS") setDT(DT) }# End Import data {# Replace all missing values with the word "cat" ----------------------------------------------------- for (k in names(DT)) set(DT, i = which(is.na(DT[[k]])), j = k, value = "cat") }# End Replace all missing values with the word "cat" &amp;#x200B; I have a snippet in RStudio that includes some info I put at the top of every file, and creates the sections I use in most of my files (importing packages and functions, importing data, inputs). When I create one, adding the hyphens all the way to the right is a minor pain, but I find it easier to differentiate sections when they go out that far. &amp;#x200B; To make sure the hyphens stop at the same point, I add a line at x number of characters in the RStudio global options. I forget where it is but it's in an obvious place.
You should test it at your computer before assuming it doesn't do what you want.
Okay will this roll by the past 365 days or just dates within that year? Thank you for your help.
Came here to say this. Follow the style guide https://style.tidyverse.org/index.html
It will extract the year from your dates. So it should work if you have a group with only half the days in a year and another group with all days (for any number of groups with any number of dates). Just make sure the dates are POSIXct (or equivalent) and not character strings
By rolling mean do you mean annual average? Like the output would be: date, product, mean_sell_price 2015 , doz_eggs, 3.40 2016, doz_eggs, 2.00 2016, milk, 3.00.....
\########### \###NEW##### \##SECTION## \###########
The problem is not with the csv, it's with the assignment. The error tells you that you are trying to assign the data to an object called "3kPNGDes". Object names cannot start with number. If you want a number, put it at the end like this: kPNGDes3.
Do you have the number 3 in a line above this? I have encountered this sometimes when trying to execute commands using Shift-Enter in RStudio and there's a typo above that tries to execute it in the same line as the one I'm trying to run.
``` library(tidyverse) library(magrittr) cumulative_attempts.df %&gt;% filter(location %in% c("CRG_Hadley")) %$% tapply(cumulative_attempts, v_grade, summary) ```
This is great
General tips, not really. https://www.rstudio.com/resources/cheatsheets/ maybe? Happy to point you towards resources if you have a more specific question. What options?
If you are only doing 3 operations. It is my personal, but non-expert, opinion that you should write three lines of code that applies each function to each list. Do your best not to complicate your code with more brackets and parenthesis. Doing 3 lines of code vs one will have very little computational difference (please correct me if I am wrong), and the ability to read the code out loud is so valuable. I can see a use for what you are doing in that you seem to have a nested list and removing elements from a nested list can often be brittle, but without more broader context I would heavily suggest not complicating a simple chuck of code. &amp;#x200B; Also, if I am totally off base with my analysis, please let me know. Would love to hear others opinions.
okay so do you just need the 1-D version of non-uniform discrete Fourier transform? if that's it, I might try to do little personal hackathon in the rather near future, and having one thing to aim at would be beneficial. I'd be working on the [NFFT](https://www-user.tu-chemnitz.de/~potts/nfft/) library - it's not a big deal which thing out of the library I try to implement but obviously it'd be better to do one that somebody wants to actually use. No guarantees about anything, of course.
Any R object, including functions, can be made part of a list. So, all you need to do is create yourself a list of functions. Try the code below: set.seed(123) data &lt;- list(rnorm(100), rnorm(100), rnorm(100)) functions &lt;- list(mean, sum, range) # Using purrr purrr::map2(.x = data, .y = functions, ~.y(.x)) # Using base mapply(function(dat, fun) fun(dat), data, functions) For me, both of these return the following list: [[1]] [1] 0.09040591 [[2]] [1] -10.75468 [[3]] [1] -1.756527 2.293079
I think that was just the minimum reproducible example, and that OP wants to know how to apply this principle where n &gt; 3
Bob Rudis wrote some R code to investigate the DNS issue. Check it out: https://rud.is/b/2019/06/28/quick-hit-dig-ging-into-dns-records-with-processx/
I have experience with SQL. The thing that is going on is more convoluted. There are 15 files which contain different parts of the dataset. Each part of the data set represents a different Family of mushrooms. The columns in each family are almost the same but differ slightly from each other (like between 1 and 3 columns out of 14 are different ). At this stage the project is just for me,( though I would like to share it back with [http://s158336089.onlinehome.us/Ian/](http://s158336089.onlinehome.us/Ian/) ) I would like to make one table out of all the dataset items. Can NoSQL handle a table with different number of columns in it? Is there an easy way to import CSV into noSQL?
Wow, didn't know about mapply as an analog to 'zip' in python. Thanks.
What is Python?
RemindMe! 12 hours
I will be messaging you on [**2019-06-30 22:04:38 UTC**](http://www.wolframalpha.com/input/?i=2019-06-30%2022:04:38%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/Rlanguage/comments/c7b8ci/mooc_vs_book_for_statistical_modelling/ese9t73/) [**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FRlanguage%2Fcomments%2Fc7b8ci%2Fmooc_vs_book_for_statistical_modelling%2Fese9t73%2F%5D%0A%0ARemindMe%21%202019-06-30%2022%3A04%3A38) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20c7b8ci) ***** |[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/c5l9ie/remindmebot_info_v20/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=Feedback)| |-|-|-|-|
Really this will depend entirely on your personal "learning style" and discipline. You'll be able to move faster through the book, but I think it's important to get the foundations solid - if the slow pace and constant reminders of a MOOC would help you learn the basics, or you find the videos more engaging than a bunch of text, or you'd like to have access to forums where you can discuss problems with students, go with the MOOC. If you are confident working from a book, give it a go, and you can always switch methods anyway as the financial outlay is low. I'm very happy that I could learn the basics of R in a class with other people, and since then I've moved on to teaching myself more advanced methods from books and blogs etc. Good luck!
Well, I do have R next semester, so I would be able to do it in class. I'm also doing the R programming course on Coursera to get basics. I do feel MOOC are slow and don't always tel the whole thing. For example, the course I mentioned doesn't say that through the c() function, even vectors can be concatenated. I feel the book does a better job of explaining a function completely. Thanks though. I am thinking of going the MOOC route now.
If you want to deploy to a standalone app then why are you sniffing at R?
Great! Thanks. I have to try and understand how mapply works. When you write mapply(function(dat, fun) fun(dat), data, functions), how does it map mean to 1st element of list, sum to second element of list and so on?? I dont understand the fun(data)
From the description of `mapply`: mapply is a multivariate version of sapply. mapply applies FUN to the first elements of each ... argument, the second elements, the third elements, and so on. Arguments are recycled if necessary. So (in most cases), you provide a function with *n* arguments, and then one vector/list for each argument. `mapply` then cascades down the elements of the vectors/list in turn, evaluates the function, and returns a list with all the results. As an example, say we have 2 numeric vectors (each of length 3) and want to know the product of the first, second, and third elements of the vectors: num1 &lt;- c(1, 2, 3) num2 &lt;- c(2, 4, 8) mapply(function(x, y) x*y, num1, num2) This gives: [1] 2 8 24 (note that `mapply` simplifies the output where possible, in this case to a numeric vector) So `mapply` combines the 1st elements of the vectors/lists provided in the way the `FUN` argument prescribes, then the 2nd, then the 3rd, until it runs out of elements. Unlike my simple example above, where the 2 "lists" provided are numeric vectors: in the original example, one of the lists is a list of functions, the other is a list containing numeric vectors. So, rather than multiplying two numbers together, instead we want to apply the 1st function to the first vector, then the 2nd function to the 2nd vector, and so on. Hence the `FUN` argument is defined as `function(dat, fun) fun(dat)` . Note that (if the names of the `...` arguments provided to `mapply` are not specified) the names of the arguments in `FUN` are irrelevant. The `...` arguments will be supplied to `FUN` in the order they are specified: No names: &gt; mapply(function(dat, fun) fun(dat), data, functions) [[1]] [1] 0.09040591 [[2]] [1] -10.75468 [[3]] [1] -1.756527 2.293079 Names, but in same order as `FUN` arguments &gt; mapply(function(dat, fun) fun(dat), dat = data, fun = functions) [[1]] [1] 0.09040591 [[2]] [1] -10.75468 [[3]] [1] -1.756527 2.293079 Names, in wrong order: &gt; mapply(function(dat, fun) fun(dat), fun = data, dat = functions) Error in fun(dat) : could not find function "fun"
Well I did, and it didn't do what I needed, so my assumption was correct.
ggplot theme margins are the key: https://ggplot2.tidyverse.org/reference/element.html
Awesome! That was a lot of information. Thank you for taking the time to explain this. Seems like apply functions and purrr can cut short the code a lot. i am diving deeper into this
Then you did it wrong, because my code grabs all the columns with the matching row filter.
My pleasure! As an aside, I love the `purrr` functions for their more user-friendly documentation and syntax - but consider using `mapply` (etc) where possible - and if appropriate - in order to cut down on your dependencies.
I know nothing. It seemed like if I used rpy2 it'd be able to export without the end users having to open into an R session, but it thats false, I'm just dumb
Yes, I will do that :)Alos, have you worked with Shiny modules before? My shiny app is getting bigger and bigger and need to avoid repetitive statements. I will post more questions here, hope to get some feedback from you :)
I'm a bit of a Shiny novice - but happy to take a look at your problems. I'll likely learn as much as you do.
Thanks! Sorry, I was travelling and didn't have proper internet access for a while. The main problem I have now is that when I knit to pdf the chunks with code (which I do want to show) are larger than the size of the page and I can't seem to figure out how to change that. I have tried this now: ```{r, echo = FALSE, message = FALSE, warning = FALSE} require(knitr) # Set so that long lines in R will be wrapped: opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy=TRUE) ``` But that doesn't seem to do anything? Also, I can't figure out how to set global settings, so that every chunk I add has the same standard settings (which I could change if necessary, but now I have to add everything separately). :o
Is it based on tidyverse?
 mydf[mydf[15]=="1",] I'm not 100% sure about the punctuation, but the idea is there. Or you can use dplyr in which case the syntax is completely different
To answer your first q, &lt;- is the assignment operator normally used in R ( you can also use = just be consistent throughout your code) %&gt;% is the pipe operator that comes with the magrittr or dplyr packages. It's meaning is 'take the left hand side and put it as the first argument to the right hand side'. This means that dogs %&gt;% mutate(avg_height = (height_low_inches + height_high_inches)/2) is equivalent to mutate(data = dogs, avg_height = (height_low_inches + height_high_inches)) In answer to your second question, == is the comparison operator. Type in 1==2 and you will see it always returns a logical value (FALSE in this case). When passing arguments to a function like mutate you use the single equals sign. If in doubt what the correct syntax is you can always write ?mutate into the console, or even vignette('dplyr').
thank you so much for your clear explanation
no problem, glad it helped. A great resource to learn more about the 'tidyverse' is hadley's book 'r for data science'. [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/)
MOOC first I would say. They tend to focus on more practical applications rather than getting too bogged down in details. Then go back and read up on the details after you've had experience applying it - I find this order works best for me anyway.
Just some slight edits to the previous answer. This will output the names of the relevant rows. row.names(mydf[mydf[,15] == "1",]) &amp;nbsp; If your rows aren't actually named and you just want the row numbers for your condition, you could just use the which() function: which(mydf[,15] == "1")
Does "dogs" no longer exist as it's replaced by "new_col_names"?
I was not able to bust all the way through the course, at work on Edge browser so of course nothing pulls up but at first glance I saw base functions being used.
That is what `rpy` is for. The fly in the ointment is that you need an installation of R that `rpy` can call... so your app would not be so standalone after all.
An account that only posts links to their website, and nothing else. Isn’t this spam?
Actually this is one of my many questions with Shiny :). I have multiple output ids.. output$one, output$two.. and so on till output$ten. I am using renderUI to render components. output$one&lt;-renderUI({ render_slider()$a }) output$two&lt;-renderUI({ render_slider()$b }).. and so on till output$ten&lt;-renderUI({ render_slider()$j }) render_slider() is actually a reactive expression which returns elements as a list something like this: render_slider&lt;-reactive({ ... ... return(list("a"=a,"b"=b,... "j"=j)) }) Will i able to use apply functions or purrr to condense these statements?
Ohhh I had no idea, that wouldn't be good at all thank you kind stranger!
Sorry I can't tell what's going on in your post and can't help. But If you want to make your code more readable in general, set it off by triple backticks. Put ``` on the prior and subsequent lines of your block of code.
Have you properly changed the column names of dogs already, or is this creating the list you will use? new_col_names &lt;- colnames(dogs) just saves the column names that are already there.
In your plot argument, add xlim = c(0,max(yourdata$y)). You might be able to use Inf but I doubt it
Use forward slashes instead of back slashes.
yes i know this solution, but do i need to re-write those slashes everytime? Isn't there a permanent solution to this?
You can access the [pre-loaded datasets](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html) by typing \`data()\` in the console.
Thank you so much. Could you also tell me what the argument 'file' is representing here? I don't see it defined anywhere.
R Studio
Have you tried the “here” library? It may help.
Yes, rstudio “projects” help a lot with this. I strongly recommend it.
The shiny examples website shows how to do this using lapply: https://shiny.rstudio.com/gallery/creating-a-ui-from-a-loop.html
\`file\` is the argument for the \`content\` function, which saves the data in the active working directory as \`file\`. So presumably \`file\` is a string ending in the type you want to save as. So something like \`"data.csv"\`.
Get R studios. Create a project. The directory for your project will be your working directory.
Try doing it using the import data set button. It will give you the proper code with path.
So does the file exist, in that location?
ahhhh, makes sense. My last and final issue is that I don't know what variable/function I should be passing on into my write.csv function. The code I listed is just some sample code, would you be willing to check out the code that I'm working on?
``` ?rock ?pressure ?cars ```
Are you sure that's the correct directory? The errors indicate that the file does not exist. You should try looking in to using RStudio Projects. It will make it so that the root working directory is the project directory and then you could have something like a data folder and access files in it just by pointing it at "data/filename.txt".
Did you even try before posting this
Looks like you are running it on Mac OS, which (generally) does not have a C dRive Try defining the correct path to file. /users/username/desktop/..csv
Projects are the greatest creation in the history of creations.^* Maybe ever. \* With the exception of brms.
well the backslash is the escape character. Notice if you double backslash it 'escapes' the escape character and will read in the file normally. As others have said, use Rstudio projects and keep all your files you need to link to in subdirectories of that project- it's neater that way and you won't need to be copying and pasting long file paths all the time.
Select the file you want to read, copy it, and paste the file in RStudio. The result is the path you want. As others have pointed out, you just have bad paths. If I have a filename that isn’t easy to remember or if I’m not working in an RStudio project, I use this feature a lot to save myself some hassle.
I would validate the filepath using: file.exists('filepath')
My first hack at it would be to just place checks into your script based on either the connection or even what your last query returned to keep running a query until results are returned etc.
Both are definitely feasible. I'm more concerned about the fact it takes an hour to run each report...that could be optimized. &amp;#x200B; Use tryCatch and some error checking: `# an example using odbc:: and DBI::` `con &lt;- DBI::dbConnect(odbc::odbc(), "Credentials")` `# test connection` `if(DBI::dbIsValid(con)) {` `res &lt;- DBI::dbGetquery(con, "SELECT * FROM TABLE")` `} else {` `tryCatch(DBI::dbConnect(odbc::odbc(), "Credentials")), error = function(e) { odbc::odbcCloseAll() })` `}`
One has to wonder if your database doesn't need some index tuning, or if you are moving large amounts of data down to your computer because you haven't put as much smarts on the server as you could. But if this is unavoidable then I might consider using a separate R script to produce the data objects you want to present all at once. You could use the `Rcache` package for this, and then clear the cache once you have generated the reports.
Yes, my SQL code is definitely *NOT* optimized, that will be the work of a future project, for now I need to worry about this R script as it is something that I have direct control of. And regardless of if the SQL code is optimized or not if the connection is spotty I need my R code to detect if the connection is still up and running.
I don't think it's the database that needs tuning, it's probably my SQL code that is slow, but anyways I don't have write access to the database so that is neither here or there anyways. I have thought about caching results but the database is too large many terabytes of data and each query of the server is requesting different data so cache wouldn't help.
dbIsValid(), never seen this function before, this maybe exactly what I am looking for.
Conceptually this is very simple. Get a list of all files you want to do the test (so put all files in one folder and list it’s contents). Loop over each file. Read the file Perform the test. Save the results. Repeat for all files. This assumes each file is of the same format. If not you might need a config file that explains the groupings for each file.
Have you considered using RODBC? It has always been reliable for me.
 # create a list of file names file_names &lt;- list.files(&lt;my directory path&gt;, pattern =".csv$", full.names = TRUE) # create a named list of data.frames containing your experimental data. If you only need a portion of the data from these csvsyou should substitute a more complicated function for `read.csv` to perform both reading and subsetting at this stage. all_tables_from_csvs &lt;- sapply(list.files(pattern = ".csv$"), read.csv) # create a list of pairs of individual names name_pairs &lt;- combn(file_names, 2, simplify = FALSE) # assign a name to each pair of file names names(name_pairs) &lt;- lapply(file_combinations, function(pair) sprintf("Wilcoxon test between %s and %s", pair[1], pair[2])) # create a function that will perform your Wilcoxon test, I don't know how you want to do this so # it's up to you. I assume you already know how to do it on a pair of data.frames. doWilcoxonTest &lt;- function(name_pair, tables_list){ # tables_list[[namepair[1]]] is your first table # tables_list[[namepair[2]]] is your second table #&lt;do your test however you like inside this function&gt; } # return a named list of the results results &lt;- sapply(doWilcoxonTest, name_pairs, tables_list = all_tables_from_csvs
Using this guy's guide, I added this functionality to my package in like an hour (including testing by quickly making another package that included the header file). So I felt obligated to share the guide with others in case they may find it useful.
Awesome, thanks!
Great, thanks!
Thanks , it helped a bit .. this is what i did : render_slider&lt;-reactive({ f1&lt;-function(input_id,name,min,max){ ans&lt;-sliderInput((input_id),name, min = min, max =max,value = (min+max)/5) return(ans) } return(f1) }) lapply(list("var1","var2",...), function(x){ output[[x]] &lt;- renderUI({ switch(x, "var1"=render_slider()("var11","variable1",1,500), "var2"= render_slider()("var22", "variable2", 2,1000), ) }) })
I think you need to wrap the text statement in HTML() to identify the text as HTML within the reactive.
[removed]
You cannot use raw HTML in strings like this. See this doc: https://shiny.rstudio.com/articles/html-tags.html You can use the function HTML() but I would argue that it's an extremely bad idea.
If you use RStudio, the hotkey Ctrl+Shift+H allows you to set the working directory.
You could write a little function that splits up all of the columns into separate data frames and then run an inner_join and/or outer_join to figure out the commonalities, then a output vector that iterates the counts. Or, you could c() all the elements of all the columns, then table(duplicated()) he vector will tell you the total number of unique values, and number of overlap. The function you’d write would would do this across subsets of the data, or written agnostically so it would just accept any data frame and variable names.
Aren't the columns in your sample data frame actually sets? Which means there is no positional correspondence within rows? So this should be a list rather than a data frame so the lengths don't have to be equal. length( setdiff( df$TGEClass.known, df$TGEClass.unknown ) ) length( setdiff( df$TGEClass.unknown, df$TGEClass.known ) ) length( union( df$TGEClass.known, df$TGEClass.known ) )
TGC Industries, Inc. (Nasdaq: TGE) Timeframe | TGE | Date and Time ---|---|--- Last Price | $21.69 | as of 11:42 PM EST on Jul 06, 2019 1-wk High | $21.19 | for the week ending on Jul 05, 2019 1-wk Low | $20.05 | 1-mnth High | $24.15 | for the month of June 2019 1-mnth Low | $20.05 | 52-wk High | $26.35 | on Aug 24, 2018 52-wk Low | $20.05 | on Jun 25, 2019 ^^I ^^am ^^a ^^new ^^bot ^^and ^^I'm ^^still ^^improving, ^^you ^^can ^^provide ^^feedback ^^and ^^suggestions ^^by ^^DMing ^^me!
It depends on situation, but for me is better to create plots through r chunks. It's jest way to maintain reproductibility. Also it's easy to change markdown to powerpoint presentation, slide presentation and etc. Markdown is now quite usefull skill. If you don't know plolty i suggest you to her into topic, beacuse you can make markdown output more interactive.
Chunks are definitely the way to go. That way the code you used to make the plot is embedded right in the rmd, and can even be shown in the output if you wish. Also you can do things like set the overall ggplot theme in an early chunk, and all figures in the document will inherit it.
It is impossible to reproduce your table because there are only 5 unique peptides, not 6. ;) You should be careful how you phrase your problem and results. In your results table, the results are not the number of "unique" peptides. (If they were, the values would be 4,4,5) Your results are the count of peptides that do not occur in other columns, which is not the same as uniqueness. Echoing @jdnewmil, I don't know why your data is in a data.frame to begin with. That seems wrong. That said, assuming a data.frame named \`tge\_data\` with an arbitrary number of columns, you might consider an approach like: unmatchedPeptides &lt;- function(col_name, df) { length( unique( setdiff( df[[col_name]], Reduce(union, df[, names(df) != col_name]) ) ) ) } colwiseUnmatchedPeptides &lt;- function(df) { c( Map(unmatchedPeptides, names(df), list(df)), All = length(unique(Reduce(union, df))) ) } results &lt;- colwiseUnmatchedPeptides(tge_data) From there you can do whatever formatting you like. I'm unfamiliar with DNA data so I don't know if the \`unique\` in the unmatchedPeptides function is necessary, or whether there is already an assumption that the peptides in each column are unique within that column.
Thank you! And I still don’t know Plotly but it’s next on my list.
Hadn’t thought about setting the theme in an early chunk. Will try it today. Thank you!
You could use a matched pairs methodology
Is the online version more up2date than the book from 2017?
If you actually have exact matches the numbers are small enough that just about any matching implementation should have a tolerable running time. I’d just use merge. If you don’t actually have exact matches that’s a totally different question. Check out https://cran.r-project.org/web/packages/cem/cem.pdf Then again it this is a stats question and not a software question all of the usual why match and not adjust or weight would apply.
Step 1: Group your control in such a way that you have one column for the name, one column for women %, one column for men %, and as many columns as age buckets you want to use ( maybe break up by 5 years or 10 years at a time) Then use the subset function to pull 80 samples from the test group. Create the same Columns for test as you have for control. Use a method to verify accuracy. I like pct error{ (test-control)/control } for each metric. Wrap this in an lapply container function to return a list of the multiple matches and the scores. Design another function to find the most optimal match. Step 2: ????? Step 3: Profit
I commented on your other post in r/RStudio, but I just now noticed that you want to match using gender and age as covariates. There are a variety of ways to do this, each with pros and cons, but the method is fairly well summed up in: https://imai.fas.harvard.edu/research/files/matchit.pdf In my example below I used the "greedy method" with two matches from the control for each match from the case. Check out the author's R package "MatchIt" for many other possible options. I also used the author's built in dataset called "lalonde" and added a random gender column to make it work. library(tidyverse) library(MatchIt) df &lt;- lalonde %&gt;% group_by(treat, age) %&gt;% transmute(gender = sample(c("F", "M"), n(), replace = T), salary = (re78 * 10) %&gt;% round(2) ) %&gt;% ungroup %&gt;% transmute(salary, case_ind = treat, age, gender) df_case &lt;- df %&gt;% filter(case_ind == 1) %&gt;% sample_n(40) df_control &lt;- df %&gt;% filter(case_ind == 0) %&gt;% anti_join(df_case) final_df &lt;- bind_rows(df_case, df_control) matches_out &lt;- matchit(case_ind ~ age + gender, data = final_df, method = "optimal", ratio = 2 ) matches_out summary(matches_out) plot(matches_out) df_analysis &lt;- match.data(matches_out) t.test(salary ~ case_ind, df_analysis) df_analysis %&gt;% mutate(case_ind = if_else(case_ind == 1, "Case", "Control")) %&gt;% ggplot(aes(case_ind, salary)) + stat_summary(geom = "errorbar", fun.data = mean_cl_normal, color = "red", width = .3) + stat_summary(geom = "point", fun.y = mean)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rstats] [Using mapply to individual elements of list](https://www.reddit.com/r/rstats/comments/cac39n/using_mapply_to_individual_elements_of_list/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Maybe this: mapply(function(x, y){ list(f1(x), f2(y))}, c(a, b), c(i, j)) First time the inner function is called, x=a and y=i. However, I'd probably do it like this: a&lt;-"char1" b&lt;-"char2" i&lt;-"char4" j&lt;-"char5" l1 &lt;- c(a, b) l2 &lt;- c(i, j) m1&lt;- c("char1"=1, "char2"=2, "char3"=3 ) m2 &lt;- c("char4"=1, "char5"=2, "char6"=3 ) result &lt;- list(m1[l1], m2[l2])
Holy crap I have been using R for years and I never knew about the switch function... amazing!! thank you!
A couple of things. &amp;#x200B; Firstly, I don't see the need for you to have 2 separate functions, `f1` and `f2`, since there is no overlap between the input expressions (i.e. you don't need `f1` and `f2` to treat - for example - `char1` differently). So, as you'll see, I have combined them into a single function. &amp;#x200B; Secondly, you are right that the issue is that your functions (as they are defined) can only deal with an atomic (aka `length() = 1`) vector. Therefore, you need to add an `apply()` function inside `f1` in order to deal with instances where the argument given to `f1` is a list or vector of length greater than 1. &amp;#x200B; Code is below: a&lt;-"char1" b&lt;-"char2" i&lt;-"char4" j&lt;-"char5" ls &lt;- list(c(a,b),c(i,j)) f1 &lt;- function(x) { sapply(x, function(y) {switch(y, "char1"=1, "char2"=2, "char3"=3, "char4"=1, "char5"=2, "char6"=3) }, USE.NAMES = FALSE) } lapply(ls, f1) This produces the following output: # [[1]] # [1] 1 2 # # [[2]] # [1] 1 2 Hope this helps!
&gt;https://imai.fas.harvard.edu/research/files/matchit.pdf Thanks for you help! &amp;#x200B; I'm about to start playing with this to apply to my dataset. &amp;#x200B; I'm still new to R and my stats his rusty but I'd like to understand the nuts and bolts of this function. Will it go through my list of 'cases' and for each data point, find 2 exact matches (age and gender) or will it create 2 groups which are, on average, the same ito of age and gender?
Thanks for you response. &amp;#x200B; I'm still fairly new to R so I need to look up even the basic functions you mention. &amp;#x200B; Any suggestions for a good R online course, perhaps specifically targeted to medical statistics?
btw, I was mystified and intrigued by your 'step 3: profit' and looked it up and found the link to the South Park underpants gnome meme. I just had a 10 minute laughing fit and rediscussed my love for South Park!
So I've tried this for a while using this code: &amp;#x200B; m.out &lt;- matchit(Aetiology ~ AGE + SEX, data = haem_all_transplants, method = "genetic", ratio = 2) &amp;#x200B; but I'm stuck with the following error: &amp;#x200B; Error in eval(family$initialize) : y values must be 0 &lt;= y &lt;= 1
You have the option of matching I’m several different ways. Exact marching is one option. Propensity score matching is a powerful and useful tool. I just used “optimal matching” which allows the tool to find inexact but close matches. So age doesn’t have to be exactly the same but should be close.
A &lt;- vector("list", length = length(a)) names(A) &lt;- a ```dplyr::lst``` might also do it in a single step butt I'm not sure.
This is what `setNames` does: A = setNames(list(c(…), c(…), c(…)), a)
Just pass `multiple = TRUE` while using selectInput.
Thanks. This worked!
Thanks. This worked!
Not sure if you can do that explicitly, but check out the [shiny dashboard](https://shiny.rstudio.com/articles/dashboards.html) selections, as well as [shinydashboardplus](https://github.com/RinteRface/shinydashboardPlus)
People here have given you the correct answer but check out the makenames() function. It will cleanup the strings and make them “cleaner” and unique so your code will be a bit more readable when referencing the columns.
I can’t look I to the function eight now, but i suspect you have to dummitixe catch bars first. Try using model.matrix
Great tip!
I hosted different shiny apps using shinyproxy on a redhat server. Data upload worked seamlessly. Why would you think that it doesn't work? Shinyproxy basically allows you to run independent R-instances using docker - therefore every user can use it's own environment.
Im keen on knowing more about Shinyproxy and how to setup. We have an IT dept in house, so they should be knowing a bit more than me, but they have no experience with R. So we are trying and working together. Is it something that is hard to setup? How has your experience been so far and how about the community? Do they have like a paid support?
First Google result for “NFL datasets” seems fine to me: https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016 Let me know if you have troubles using this data.
I think I was googling something a little different then because I've just looked at that and it looks perfect Thank you
Old faithful, chicken weights.
Cheers
[Tidy Tuesday](https://github.com/rfordatascience/tidy Tuesday) has been running for a bit. Flip through and see if any data sets grab your interest. I like these because they are often not perfectly clean, so you get to practice some of those skills which will be absolutely necessary when working on real data
1. Start using LaTeX because Word sucks. 2. Export using the xtable or stargazer packages. 3. Profit
I don't really know as well, but perhaps you can achieve the same result using Rmarkdown? I'm not a expert, but it basically lets you write Word Documents, HTML, PDF, etc using Markdown syntax (something closer to how you write articles in Wikis, like Wikipedia) combined with R. So, my guess is, you could try to output the table in a Word Document using this package and then try to copy it to your original word document! See this [link](https://www.google.com/url?q=https://bookdown.org/yihui/rmarkdown/word-document.html&amp;sa=U&amp;ved=2ahUKEwib1vjTq6vjAhUzH7kGHU-mD4gQFjAAegQIBBAB&amp;usg=AOvVaw2Rperfkse2Oe2N4irph9dO) to know more about what I'm talking about it. If you're using Rstudio, it's even simpler to use than it looks!
It would help if you provided the error
Try shiny.
Use JavaScript and build a simple html website that does that. You could deploy that on any web server you want and it would be very uncomplicated.
This ^ Happy Cake Day!
That looks correct. If youre trying to do a two-way repeated measures M/ANOVA you need to run separate analysis for within and between subjects statistics, but for one way thats exactly how I do it.
Assuming you're using r studio, just write a function and pass your input value as the parameter when you call the function from the console
R will do that using knitr but knitr uses LaTeX so if you are having trouble using LaTeX R isn't going to be of much help. knitr can also use markdown though, have you tried using markdown, if you are pressed for time Markdown is a lot easier to learn than LaTeX the downside is that it's not a powerful also, I don't know how to do what you are asking in markdown. =( sorry. If you want a quick and dirty solution since you said you already have it typed in word you might just want to K.I.S.S. and do a simple word search and replace in word. Then you can copy the text over to what ever typesetting format that you like.
Its hard to try to imagine what these files actually are. It sounds like a tsv but also has a tree in an unspecified format? Please show a snippet of data and the expected output, ideally in an easily copy pastable format. See [this](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example)
Find a line which is shot and see which score goes up and which team made the shot.
Sorry, still a bit of R amateur - dummitixe catch bars?
Misspelled “dummitize columns” .
Rstudio has regex find and replace, this would let you insert markdown syntax around a matched term.
It'd be helpful to know the specific research question(s) you're answering and whether you had a block ordered / balanced design. If your data isn't balanced, one piece of advice is to use the Anova function from the Cars package so that you can do a Type II and III (SPSS default) ANOVA. The lowercase anova function in base R performs a Type I ANOVA. However, if your data is balanced all forms of ANOVA should return the same results.
The following is a dummy example. Sample file example, which contains the 3/10 species in the tree with 'presence' of trait (encoded as 1), but not the 7 species with 'absence' of trait (encoded as 0): Homo_sapiens 1 Frog 1 Alien 1 What I require is this, based on a list of the ten species in the tree. Monkey 0 Homo_sapiens 1 Cactus 0 Cricket 0 Frog 1 Alien 1 Lion 0 Donkey 0 Shrek 0 Slug 0 There are a few dozen sample files each with a unique combo of species with 'presence' of trait. I don't want to go through each one manually and append "species\_name (tab) 0" for all missing species. I am a classic overcomplicator with coding and can't work out a decent way to do it.
Seems like a join from dplyr. See [this](https://stat545.com/bit001_dplyr-cheatsheet.html). If you do a full_join, or left_join with the full species list on the left, you can replace resulting NAs with 0s after the operation. To use this your tree with the list of all species need to be converted into a data frame. I don't know what is the structure of this tree so can't say much about that.
What am I saying.. That will work but it's also not really necessary. All you need to do is ``` data.frame(species=listOfAllSpecies, exist=as.integer(listOfAllSpecies %in% smallDataFrame[, 1])) ``` Here the smallDataFrame is the first one with the 3 species. listOfAllSpecies is the list of species in that tree
That's what I ended up doing just to get this done. Good old search and replace.
I'm sure there are more elegant ways to do this but assuming the dataframe is already sorted in ascending order, this should work: &amp;#x200B; game\_df &lt;- game\_df\[game\_df$points %in% c('1','2','3'),\] &amp;#x200B; game\_df$phome\_score &lt;- append(0,head(game\_df$home\_score,-1)) game\_df$paway\_score &lt;- append(0,head(game\_df$away\_score,-1)) game\_df$who\_scored &lt;- NA game\_df$who\_scored &lt;- ifelse (game\_df$home\_score &gt; game\_df$phome\_score, "home","away")
You can look up if the name of the player in the player column is in either the home or away column
The skeleton for the roxygen can be generated and updated automatically and you don't *have* to fill it all out - for internal use while I'm still prototyping, I'll often just note what the inputs and return are - if I'll forget them. For a function that you don't feel like you need to document, you can just, like, not document it with roxygen. I think you're going to run into problems if you want something that, like, self-updates but is not in the file. Before roxygen (which is like the sort-of-standard doxygen for other programming languages), people were pretty much just writing .Rd files directly. I think a big part of this is just workflow issues. One thing that would be nice is code folding for roxygen blocks - I haven't seen anything in either ESS or RStudio that does that. That would be so nice. Somebody should tweet somebody about it.
The layout that you see in help files that you get in an R package has absolutely nothing to do with `roxygen2`. All it does is copy the _remarkably compact_ inline documentation into standard-but-heavily-marked-up R documentation Rd files, and base-R code then generates various user-readable formats (HTML, text, LaTeX) from that. If you want to provide an alternate output format for converting Rd files to user-readable layout then have at it... but I think it would be a waste of time. If you truly don't like `roxygen2`... you can always write your documentation directly in Rd files. Many experienced developers advocate that practice (not I)... but it certainly doesn't make your documentation effort faster. IDEs like RStudio already convert the Rd info into tooltips and typing auto-completion data. Note that Rd files are rather rigid in order to support automated verification via `R CMD check` that all parameters and usages are addressed. If you try to reduce your documentation typing to less than the minimum R requires then you will be leaving vague holes in describing how the functions work for users. This is already a common problem with packages that barely meet the minimum automated checks... and `roxygen2` makes it all too easy to skate by.
Roxygen can be as light as ``` #' Best function ever #' @export ``` I don't see a way to avoid long roxygen lines if your documentation is.. well, long. But there is a way to avoid inline comments. You can write your functions and documentations in a separate file. file1.R ``` #' @export bestFunction = function(){ print('yay') } ``` file2.R ``` #' Best function ever #' @name bestFunction NULL ``` The NULL at the end is important since otherwise roxygen will ignore it. The usage isn't exactly non-standard either. People routinely have .R files that only has documentation for data and stuff
Without seeing what the error you're getting is, is assume your problem is that grepl is looking for a regular expression for the pattern and your passing it a variable name.
My bad, the error given is : Warning message: In grepl(player, home_lineup) : argument 'pattern' has length &gt; 1 and only the first element will be used
So it's basically an open source implementation (or hosting?) of RStudio Server?
What's the difference between these and shinyproxy?
You want to a single reference for all your functions. Build a package. And use Roxygen2. Packages can be as small as containing a single function. There is no lower limit and it usually pays of *immediately*. It also has a framework of *compiling a single reference for all functions* for you. And it gets updated every time you rerun the documentation and compiling process. Roxygen2 is as lightweight as they come. It requires very little markup and the advantages of in-code documentation are far greater than any separate documentation system. Even if you opt out making a package, if you want to copy the function to another project, you can copy-paste *a single block of code*. Remember, files have near infinite space for contents. So including any amount of text still leaves you near infinite amount of space for code. And most editors allows you split screen views and multiple entities of same file, so you can literally view the bottom of your function alongside your documentation text. Which are in the same file. So when you update your function's behaviour, you can immediately and in the same file update the description. So no surprises for later-you. Alternatively, if you want to document a *workflow*, consider using Rmarkdown documents. It allows you to interweave a narrative together with code blocks and their output. Although it lacks the ability to provide a single reference for functions.
If you don't intend to share the functions/packages for other people to use who won't look into the source code, "self documentation" is the lightest to go. That is, make the code explain itself using meaningful names for parameters, variable and functions. That is a good best practice anyway. Other than that, roxygen is as light as it can get for an automated documentation generation, as others have pointed out.
Like the error says, grepl expects _one_ string as a regular expression, and you want to use a separate one for each row. This is one of the benefits you get from moving to the more convenient string manipulation library `stringr` (part of the tidyverse). In there is the `str_detect` function which will do what you want. data &lt;- data %&gt;% mutate(player_team = ifelse(str_detect(home_lineup, player), home.y, away.y)) Be careful though, that this (like your own attempt) will interpret the player names as regular expressions. So if any of them have a `.`, that will count for anything. `Al H.` for example would match both `Al Horford` and `Al Harward` etc. If you want to avoid that, and you know your names are all written exacly like they are in the lineup list, you can tell it not to interpret it as a regex with fixed: `str_detect(home_lineup, fixed(player))`
It's dockerised for easier setup. Also adds in a VS Code and RStudio environment. Makes development and deployment a lot easier. &amp;#x200B; I use it, and love it. I especially like the fact that when I need to run some spark jobs using SparklyR i can simply increase the image size in GCP and let it run.
don't forget multi user too.
This project is wonderfull!
I find it difficult to explain this project since I developed *nothing*; I'm just using Docker to combine the benefits of other products/projects. The [Rocker project](https://hub.docker.com/r/rocker/verse) offers RStudio+Shiny in a Docker image. I extended the image to include Python, PowerShell, database drivers, and most importantly, ShinyProxy. With ShinyProxy in the mix, the Rocker image can be (1) secured behind a password and (2) multi-user. I then add NGINX for easy SSL/TLS setup and InfluxDB for logging, which makes the solution more enterprise-ready. Every component is Dockerized, which makes for an easily reproducible development environment.
Thanks for the feedback /u/engti. As I said in another comment, this project is driven by a Dockerized instance of ShinyProxy. &amp;#x200B; To simplify it down to one sentence: this project is a curated, out-of-the-box instance of ShinyProxy that includes RStudio, VS Code, NGINX, InfluxDB to offer a full-featured, easily reproducible, multi-user development environment.
Thanks; I've been iteratively tweaking it for months now! I'm really proud of this version; I think it is the most full-featured offering yet.
Yes! With tiered access, too. I love that I can create something and present it to a non-developer who only has access to view Shiny apps or RMarkdown docs.
Do you have more detailed instrucion for configuration, exposing in on VPS etc?
Can one item have different status codes in one df?
Ok, so I did add the column 'DF' to both DF1 and DF2, setting it equal to 1 for all the rows in DF1 and setting it equal to 2 for all the rows in DF2. I then did an rbind() to combine them. &amp;#x200B; Now, working in the dplyr package, I have the following code to get all the part numbers and their 'Status.Code' values: df %&gt;% group_by(item, DF, Status.Code, From.Year, To.Year) %&gt;% select(item, DF, From.Year, To.Year, Make, Model, Status.Code) %&gt;% filter(Status.Code &gt; 41) %&gt;% print(n=40) I just filtered 'Status.Code' to make it manageable to start. We should (and do) still have differences 'Status.Code.' Here is the results copied and pasted in (I guess you can't combine text and image uploads on Reddit?) &amp;#x200B; # A tibble: 7,885 x 7 # Groups: item, DF, Status.Code, From.Year, To.Year [4,745] item DF From.Year To.Year Make Model Status.Code &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; 1 WF-64080SGB-1 1 1984 2019 KENWORTH W900L 72 2 WF-64080OBL-1 1 1984 2019 KENWORTH W900L 72 3 WF-64080MAX5-1 1 1984 2019 KENWORTH W900L 72 4 WF-64080EDGE-1 1 1984 2019 KENWORTH W900L 72 5 WF-64080BUC-1 1 1984 2019 KENWORTH W900L 72 6 WF-3042SGB-1 1 2018 2019 FREIGHTLINER CASCADIA 72 7 WF-3042OBL-1 1 2018 2019 FREIGHTLINER CASCADIA 72 8 WF-3042MAX5-1 1 2018 2019 FREIGHTLINER CASCADIA 72 9 WF-3042EDGE-1 1 2018 2019 FREIGHTLINER CASCADIA 72 10 WF-3034SGB-1 1 2007 2019 FREIGHTLINER CASCADIA 71 11 WF-3034OBL-1 1 2007 2019 FREIGHTLINER CASCADIA 71 12 WF-3034MAX5-1 1 2007 2019 FREIGHTLINER CASCADIA 71 13 WF-3034EDGE-1 1 2007 2019 FREIGHTLINER CASCADIA 71 14 WF-3034BUC-1 1 2007 2019 FREIGHTLINER CASCADIA 71 15 WF-3026SGB-1 1 2012 2019 KENWORTH T680 72 16 WF-3026OBL-1 1 2012 2019 KENWORTH T680 72 17 WF-3026MAX5-1 1 2012 2019 KENWORTH T680 72 18 WF-3026EDGE-1 1 2012 2019 KENWORTH T680 72 19 WF-3026BUC-1 1 2012 2019 KENWORTH T680 71 20 WF-2226SGB-1 1 2007 2019 FREIGHTLINER CASCADIA 71 21 WF-2226OBL-1 1 2007 2019 FREIGHTLINER CASCADIA 71 22 WF-2226MAX5-1 1 2007 2019 FREIGHTLINER CASCADIA 71 23 WF-2226EDGE-1 1 2007 2019 FREIGHTLINER CASCADIA 72 24 WF-2226BUC-1 1 2007 2019 FREIGHTLINER CASCADIA 71 25 WF-2219SGB-1 1 2005 2016 INTERNATIONAL PROSTAR 71 26 WF-2219SGB-1 1 2017 2019 INTERNATIONAL RH 71 27 WF-2219OBL-1 1 2005 2016 INTERNATIONAL PROSTAR 71 28 WF-2219OBL-1 1 2017 2019 INTERNATIONAL RH 71 29 WF-2219MAX5-1 1 2005 2016 INTERNATIONAL PROSTAR 71 30 WF-2219MAX5-1 1 2017 2019 INTERNATIONAL RH 71 31 WF-2219EDGE-1 1 2005 2016 INTERNATIONAL PROSTAR 71 32 WF-2219EDGE-1 1 2017 2019 INTERNATIONAL RH 71 33 WF-2219BUC-1 1 2005 2016 INTERNATIONAL PROSTAR 71 34 WF-2219BUC-1 1 2017 2019 INTERNATIONAL RH 71 35 WF-1952SGB-1 1 1987 2019 KENWORTH T800 72 36 WF-1952SGB-1 1 1987 2019 KENWORTH T802 72 37 WF-1952SGB-1 1 1987 2019 KENWORTH T803 72 38 WF-1952OBL-1 1 1987 2019 KENWORTH T800 72 39 WF-1952OBL-1 1 1987 2019 KENWORTH T802 72 40 WF-1952OBL-1 1 1987 2019 KENWORTH T803 72 # … with 7,845 more rows &amp;#x200B; Does anyone know how I can now somehow see all the part numbers that have changed? I believe they're unique because I grouped by item, DF (thus which data frame they were originally from), Status.Code, and From.Year and To.Year. &amp;#x200B; I assume I will be doing something with mutate, but am not quite sure what.
I understand your question but am not *100%* sure how to check for the answer (and unfortunately I do not have the background knowledge on the dataset to be able to answer the question intuitively). I ran the following code: nrow(distinct(x)) nrow(distinct(x, item)) nrow(distinct(x, item, Status.Code)) nrow(distinct(x, Status.Code)) and the first 3 lines all return 11,090 rows; while the 4th line returns 9 distinct rows. I believe that that means that one item **cannot** have different status codes, but I am not 100% sure. Thanks for taking the time to respond by the way, I really appreciate it.
Here are some expressions you might find useful to learn about the data you have: dupes1 &lt;- unique( DF1$item[ duplicated( DF1$item ) ] ) DF2$item[ duplicated( DF2$item ) ] xtra1 &lt;- setdiff( unique( DF1 ), unique( DF2 ) ) xtra2 &lt;- setdiff( unique( DF2 ), unique( DF1 ) ) DF1[ xtra1 %in% DF1$item, ] DF2[ xtra2 %in% DF2$item, ]
Oddly enough, when I used the code (changed the home.y and away.y to just home and away, made sure it still matched), it [returned numeric values](https://docs.google.com/spreadsheets/d/1Z1njK5FxCZgIUaxxoRUVaK8DFNWP4OGthT6kuFHcC7I/edit?usp=sharing). Do you know why that'd happen?
Are home and away factors or character vectors? (can check using something like class(data$home) ) If they're factors (categorical) it may be returning the numeric representation in which case you could either convert home/away to characters (add something like "home = as.character(home), away = as.character(away)", to the top of your mutate function) or coerce them in the actual value assignment for player team (by wrapping them with as.character(), when you reference them at the end of your ifelse logic).
Never heard anyone express this concern. I suppose it's possible. If you're really worried, you can check the package in a text editor. Many packages have accompanying publications too, so you could paste in the package functions as text from the pub supplements/appendices. That said, I am not worried about it. It's pretty common to dig into a packages, so packages with large user bases would have likely have had someone notice if there was malicious code.
While CRAN people do briefly review any submitted package there is no guarantee that something won't sneak in. Historically this never happened but similar repositories did suffer from [such attacks](https://blog.npmjs.org/post/180565383195/details-about-the-event-stream-incident) Note that in most cases an R session will not have administrative privileges in your device so harm that can be done is limited. Also most large packages where something like that could be relatively easy to hide, have multiple maintainers many of which are public figures which means more eyes on the code and more personal stakes.
Many thanks for your reply. I am talking to RStudio folks they seem to have a product to isolate R script environments using RStudio Server Pro but the feeling I am getting is we will need to scan the libraries and store in a location for the users to pull from. We are still early in the whole process regarding security and R overall so not sure where this will end.
Thanks for the reply. I just started looking into security and R and CRAN and I found out security for some places block access to using the install.packages using the firewall and maybe even git. Some developers have commonly asked RStudio folks to add virus scanning to Package Manager which sounds great to me as a solution even though it will cost money. I am not sure where my efforts will lead to if anywhere though.
There were also ways to host in company limited versions of cran where you can approve which packages gets in. So you can vet packages and approve them for the whole company to install easily. You'll have to do your own vetting and practically be only marginally safer but it could provide the veneer of safety that the company wants to have.
Check out the microsoft version of cran.
They were factors, and after converting to characters, the returning value was the value in "away" for everything, indicating that there must've been some sort of issue with matching the player value to one of the values contained in "home_lineup". Do you know what this'd be? I did try the fixed() function too as well.
I think you could spin up a Docker container to test out without harm
If you're using dplyr, you are probably looking for \`dplyr::anti\_join()\` or \`dplyr::setdiff()\`
We can't say the R packages are completely virus free. However, people in R community are working on making R safer. See the website shown below for detail. [https://community.rstudio.com/t/will-rstudio-package-manager-include-a-virus-scanner-for-r-packages/12999](https://community.rstudio.com/t/will-rstudio-package-manager-include-a-virus-scanner-for-r-packages/12999)
If you are doing pdf documents, instead of this svg business you can truly typeset your plots by setting in the Rmd chunk options `dev = "tikz"` and have `library(tikzDevice)`, for plots that are actually integrated into your document's style, instead of hand formatting the tables you could use knitr::kable/kableExtra, xtable, or stargazer to automate that bit. Cool article though, I think you can make your life a bit easier with more r packages and more rmarkdown magic
example tikz results: https://imgur.com/a/sSPJVrQ
Thank you for the feedback. &gt; If you are doing pdf documents By separating content from presentation, it is possible to produce a document for multiple formats: PDF, HTML, Word, EPUB, etc. (While the series so far has focused on PDF, there has been interest in generating EPUBs.) &gt; instead of hand formatting the tables kable, xtable, and stargazer are extremely useful; however, using them means that presentation logic is now in two places: ConTeXt and R. The ease of formatting such rich tables with kable is tempting. Linking the styles defined in ConTeXt to those used by kable would take some effort so as to avoid maintaining multiple instances of colours, for example. &gt; typeset your plots by setting in the Rmd chunk options How would this work in practice? Do you have an example?
you realize rmarkdown exists for exactly this purpose right? https://bookdown.org/yihui/rmarkdown/ Theres like no need to touch context or anything :) https://gist.github.com/josephsdavid/b9f0da76a5d8868fc164c6954400ddd4 if you knit to pdf it will automatically force your latex style onto the document, here are some example results https://imgur.com/a/sSPJVrQ
Thank you for putting together that example! &gt; Theres like no need to touch context or anything :) ConTeXt and LaTeX are different typesetting engines, each having their own strengths and weaknesses. I prefer ConTeXt's monolithic architecture to LaTeX's micro-package approach. Using LaTeX, even installing it, is a deal-breaker for my purposes. (LaTeX and ConTeXt take effort to work on the same machine, VMs notwithstanding.) It appears `tikzDevice` absolutely requires LaTeX: &gt; tikzDevice: No appropriate LaTeX compiler could be found. Access to LaTeX is required in order for the TikZ device to produce output. At that 10,000 foot view of converting Markdown documents into PDFs, I completely agree with you: RMarkdown and these blog posts accomplish similar goals. Digging into the details, there are substantial differences: creating EPUBs, using ConTeXt, and complete separation of content from presentation. (LaTeX can do the latter in theory, but in practice is often falls short, needing to tweak the output by embedding macros here and there.) It sounds like you have something that works for you and meets your needs! Always fantastic to have more tools in the toolbox.
I will be following to the next edition if your blog because I am learning a lot :) I see what you mean, you want to do it with complete separation, which is a really noble effort. It may turn out I use your work on my masters thesis, so please keep going!!
I highly recommend making a separate file to hold the child document template. I found this useful: https://stackoverflow.com/questions/21729415/generate-dynamic-r-markdown-blocks/21730473
I've not set this up on a VPS myself, but the setup shouldn't differ. Are you having a specific issue I can help with?
Selenium is for browser automation. Interacting with desktop apps is a completely different task. What are you trying to do?
Well I need to have an app take a screenshot of something on my screen every 5 minutes, identify the hex code, and feed the code back into the Selenium.
The screenshot step really complicates your task. You didn't say what Operating System you are using, but if it is Windows there is AutoIt (and various other similar programs).
I am on a Mac. Firefox actually has an "Eyedropper" feature that identifies the hex code. I wonder if it's possible to control and automate this feature via Selenium.
Try using rMouse package instead. You can also use rjava directly if your use case isn't covered. For screenshots use your OSs command line utility through system
then use the firefox driver?
Is the column a factor or character vector? You can also try filter Table %&gt;% filter(condition1 &amp; condition2)
you probably need to post your data for us to debug that
apply will convert the data frame first to a matrix. All columns will be transformed to the same type (character). &amp;#x200B; To avoid that kind of surprises, you can try the libraries of the tidyverse. &amp;#x200B; `purrr::map_df(data, function_try)` &amp;#x200B; Once yo get the gist of todyverse, you will prefer to write like this: &amp;#x200B; `library(tidyverse)` `data %&gt;% map_df(function_try)`
Thank you! &amp;#x200B; It works as expected. I'm very new to writing functions and apply, so I got stuck in the next step again. &amp;#x200B; When it comes to the checking argument, I could just use IF like this &amp;#x200B; if(data[1,1] == data[2,1]) { data[1,1] } else { paste(data[1,1] , "WRONG", data[2,1], sep = " ") } Then I get this result: [1] "001 WRONG 002" So, I put that into the function function_try &lt;- function(x) { if(class(x) == 'numeric') { print(class(x)) sum(x) } else if(class(x) == 'character') { print(class(x)) print('This is character') if(x[1,1] == x[2,1]) { x[1,1] } else { paste(x[1,1] , "WRONG", x[2,1], sep = " ") } } else { print('something else') } } Run this line `map_df(data, function_try)` But I got an error: &gt; map_df(data, function_try) [1] "character" [1] "This is character" Error in x[1, 1] : incorrect number of dimensions &amp;#x200B; I tried wrapping `paste(x[1,1] , "WRONG", x[2,1], sep = " ")` with `assign()` then `print()` and finally both but I still got the same error. &amp;#x200B; Do I misunderstand how the data is passed inside a function? How can I solve this issue? Please advise.
I'd say it depends on whether you're recoding something through heuristics, a lookup table or numeric ranges. My personal preference: * heuristics: usually ```dplyr::case_when()``` with a bunch of regular expressions. * lookup table: a join * ranges: ```base::cut()``` works well, but you have to pay attention where you want your intervals to be open/closed.
Thank you. I am going to try those and see.
Your are working with columns. So do not use matriz/data.frame notation. Use this modification. &amp;#x200B; `function_try &lt;- function(x)` `{` `if(class(x) == 'numeric')` `{` `print(class(x))` `sum(x)` `}` `else if(class(x) == 'character')` `{` `print(class(x))` `print('This is character')` `if(x[[1]] == x[[2]])` `{` `x[[1]]` `} else` `{` `paste(x[[1]] , "WRONG", x[[2]], sep = " ")` `}` `}` `else` `{` `print('something else')` `}` `}` &amp;#x200B; &amp;#x200B; `purrr::map_df(data, function_try)`
the columns are numerics. they were originally factors, I think, but I converted them to numerics earlier in the code (tbl$FP\_latitude &lt;- as.numeric(as.character(tbl$FP\_latitude)), etc). I assume you meant the filter function in dplR. I gave that a go and: 1. tbl %&gt;% filter(tbl$FP\_longitude &gt;= -124 &amp; tbl$FP\_longitude &lt;= -114) gave me an error because it couldn't figure out with %&gt;% is (this is probably stupid, but I run into these problems sometimes and it's usually resolved when I install a certain package, but I'm not sure what to do in this case). 2. filter(tbl, tbl$FP\_longitude &gt;= -124 &amp; tbl$FP\_longitude &lt;= -114) gave me a whole data frame full of NAs. Am I doing it completely wrong?
What are you doing differently if the `player` is found in `home_lineup` vs `away_lineup`? Do you not add `play_length` to `mp` if the `player` is found in `home_lineup`? You could look into `dplyr::case_when()`, this might work for you.
The only difference between home_lineup and away_lineup is that some of the “player” values are in one and some are in the other. Either way, I want the sum of the play_length when player is found in one of the lineup columns (they’ll only appear in one column per game_id). I’ll check out that function!
It sounds like you don't need to conditionally sum based on which lineup the player is in. Either way you want to add `play_length`.
The only issue is that the “player” value corresponds with who is the primary person making the play. So player == “Ben Simmons” just means when he’s the one making a play. I ran into this problem myself. When I sum play_length by only player, the value that comes back is seconds per player per game. Which makes sense, in a 48 minute game, no player is going to be the primary ball handler for very long. When I did a dplyr function to filter for a specific player in a specific lineup, summing play length gave the correct minutes played value. It’s just when trying to automate that it gives me trouble.
`case_when()` might work for you then. Or you could try creating logical flags for the 2 possibilities and using those to create the conditional sums. d %&gt;% select(game_id, away_lineup, home_lineup, player, play_length_s) %&gt;% mutate(in_home = str_detect(home_lineup, player), in_away = str_detect(away_lineup, player) ) %&gt;% group_by(game_id, player) %&gt;% summarise(n = n(), mp = sum(play_length_s), mp_home = sum(in_home * play_length_s), mp_away = sum(in_away * play_length_s), mp_either = sum((in_home | in_away) * play_length_s), mp_neither = sum((!in_home &amp; !in_away) * play_length_s)) Which gives: # A tibble: 22 x 8 # Groups: game_id [1] game_id player n mp mp_home mp_away mp_either mp_neither &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21800001 0 41 NA NA NA NA NA 2 21800001 Al Horford 20 152 152 0 152 0 3 21800001 Amir Johnson 12 27 0 27 27 0 4 21800001 Aron Baynes 18 158 158 0 158 0 5 21800001 Ben Simmons 50 233 0 221 221 12 6 21800001 Brad Wanamaker 2 11 11 0 11 0 7 21800001 Daniel Theis 3 2 2 0 2 0 8 21800001 Dario Saric 27 129 0 129 129 0 9 21800001 Gordon Hayward 23 157 157 0 157 0 10 21800001 Jaylen Brown 30 159 159 0 159 0 # ... with 12 more rows I think I understand the problem more clearly after doing this. If I'm understanding correctly, I think you are after the `mp_either` result, which for the most part is the same as home and away except for some event types.
Thanks! I can not find any info about it being virus scanned. It would make sense since Microsoft provides services that use R inside of their public cloud to do so but no indications so far but I am still looking.
Thanks I will pass that on! I am not sure how much resources we have to do that though.
i agree, `case_when()` for quick &amp; dirty, data cleaning and complicated conditions, lookup table and join for production quality recodes (assuming most of your stuff incl the lookup table is in a database)
Gotcha. It might be worth asking microsoft directly if they perform virus scans of it? I'm sure they do (ms is surely careful enough not to host unscanned files) but I gather you need something in writing that says as much.
Yep, security folks will require we get something in writing and I feel better about that. Also, Anaconda looks good but they just dropped using MRAN as a default and have their own base libraries and environment. I am hoping they do malware scanning for everything they build or distribute. I am also unsure of what will happen when we go to say a public cloud computing or CSP. As an Enterprise Solution Architect I try to think ahead.
I'd consider asking this on r/stats To ensure I'm understanding you correctly, is it that you want a way to identify a certain region based on certain common parameters, and then examine how that regions profile may change over time? If so my first thought was [archetypal analysis](https://cran.r-project.org/web/packages/archetypes/vignettes/archetypes.pdf) ... the idea being you identify archetypes based on your metrics and monitor change over time. Perhaps examine specific hypotheses using SEM. I am sure others would have a better approach though
Thank you! It works. :)
I'm not sure exactly what I need to do, which makes it difficult to describe, but bear with me. It's essentially a customer level entity that has ships from origin to destination zip codes semi regular i.e. their stable customers and most common destinations all create their "normal" shipping behavior from our perspective (the actual transp company)0. What I want to understand how to do, is to quantifyt some kind of scoring that can capture any deviations from the "normal" pattern. And the main parameters here are origin and destination zipcode and time. Not sure if this helps make it clear. Thanks anyway!@
I am pretty sure not. Prepare the data frame before fitting the models, so your newdata argument can be independent of your original data frame.
Yeah. Now that I'm thinking about it, maybe I can create a vector of strings of column names to use as my loop index? eg: lagcol &lt;- c("lag1", "lag2", "lag3") for(i in lagcol){ myformula &lt;- update.formula(myformula, as.formula(paste0(" ~ . + lagcol$"i" ) } instead of lagcol &lt;- c("lag1", "lag2", "lag3") for(i in 1:ncol(lagcol)){ myformula &lt;- update.formula(myformula, as.formula(paste0(" ~ . + lagcol[,i] ) } with the former it doesnt require all my data to be ordered the same way, in one dataframe, the whole way through
OP (Endtest) is spamming up tech subs, every day with multiple accounts [1,](https://www.reddit.com/user/boss_scarbos) [2,](https://www.reddit.com/user/dragnea_presedinte) [3,](https://www.reddit.com/user/llupei) [4](https://www.reddit.com/user/wernerklaus), [5](https://www.reddit.com/user/jos_cu_klaus), [6](https://www.reddit.com/user/sa_vina_werner), [7](https://www.reddit.com/user/ihavelepower), [8](https://www.reddit.com/user/viorica_presedinte), [9](https://www.reddit.com/user/werner_sclavul), [10](https://www.reddit.com/user/basist_infect), [11](https://www.reddit.com/user/felix_presedinte), [12](https://www.reddit.com/user/werner_la_puscarie) ultimately in an attempt to make you pay money for the service he runs (endtest). [This is the kind of person you're dealing with here](https://imgur.com/xyfZ59P) Still want to give endtest money? **Vote and report accordingly.**
try running install.packages(c('blob','RSQLite','RQDA')) Or even just one at a time while chasing down any installation errors that pop up.
Install tidyverse for piping or just use the filter without piping
Are you on Windows or a Mac? I can't get RQDA to work on a Mac, there's an issue with the GTK package.
Yeah, even before seeing your comment, I was kinda disgusted how the whole "article" was totally "unbiased" with "this and this" bad, here's a fucking advertisement for my alternative.
Thanks for the suggestion. I tried it got this error message: Error in install.packages : missing value where TRUE/FALSE needed
I have tried on both, but am trying to get it work on Windows at the moment.
I got it to work, that is, RQDA to load on Windows by installing the following patch: R-3.6.1 Patched build for Windows (32/64 bit) [https://cran.r-project.org/bin/windows/base/rtest.html](https://cran.r-project.org/bin/windows/base/rtest.html)
Try rolling back a version of R. I’ve had some trouble with the latest version.
Sorry for the slow reply, had a surgery yesterday but I'm back home. I wonder why those numbers are returned? I know what the numbers are supposed to be and those numbers aren't it nor are divisible by 60 (if they were seconds and converted to minutes). I did try something new: instead of having the lineups alternate depending on whether they're the home or away team, I split every team into its own data frame and then differentiated by whether the lineup was from the team or the opponent. Here's a sample of it: [Sample](https://docs.google.com/spreadsheets/d/1R-iv9jGSj9ZUE7dTpp1cWGNPzMw1mIlMoASVxLOJWRo/edit?usp=sharing) Does this make it easier to manipulate? For reference, the play_length is in HMS format and the min_played column should be as well.
You can try using the stargazer package to export in LaTex format or HTML. If exported into HTML, word should be able to read the HTML table if I am not mistaken.
No problem, I hope you're feeling well! Those times are in seconds (I didn't make the connection that `mp` is minutes played). Most of the values of `play_length` are in seconds except some weird ones that are `-12M 0S`. Knowing they are in seconds are things lining up? If not, I guess I'm still confused on your goal.
Interested in answers to the first question. For two, I suggest looking at tidyverse (unless I'm not understanding the end goal - R is great for avoiding loops). For three, there's a bunch of ways but since I mentioned it (apologies for the formatting) - df %&gt;% mutate(theVariable = ifelse(theVariable == whateveryouwantchanged, thingyouwanttoreplace, theVariable)
Thanks for the heads up! omg just looked at what tidyverse can do (such as pipe), totally never used it before and died from bracketing shit. Thanks a lot!
Oh yeah, the -12M is the start of the period. Which may be throwing everything off. That's omit-able though, it's not an actual play.
1. yes I’d save in .rda or similar. Also I’ve found the rio package to be good for import/export generally 2. There’s probably a way to do this using one of the apply functions and possibly lists, I’m not that great with them and can’t remember exactly how at the moment 3. mutate with ifelse works fine for a few values. If you need to recode a lot of values I’d consider using car::recode or dplyr::recode. If it’s *really* a lot I’d create a separate spreadsheet or similar with the desired old and new values, merge with the old data, then fill NA values in the new value column with the old value
1) Check out the data.table package, specifically the fast read (fread) function 2) That's one way of doing it, however you can also use functional programming to accomplish the same result without loops. Specifically, take a look at the last third or so of this link for an example on liner models: [https://cran.r-project.org/web/packages/broom/vignettes/broom\_and\_dplyr.html](https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html) 3) Yes, the question is whether you're just looking to apply this to specific columns or your entire data set. If the former you can use something like: `df$whatever_var &lt;- ifelse(`[`is.na`](https://is.na)`(df$whatever_var) == TRUE, -0.30, whatever_var)` If the later you can just do something like: `df[`[`is.na`](https://is.na)`(df)] &lt;- -0.30`
This might help for #2 https://drsimonj.svbtle.com/running-a-model-on-separate-groups
No, you can't merge with an OR. However, you can create a new column that contains unique ID numbers for all records (assuming your addresses are unique). There are several ways to convert a long character string (address) into an ID number, but which one you should pick would depend on what your ID numbers look like. You might be able to paste the ID number with the address as a kludge, but only if your ID numbers and addresses are 1 to 1. Then with two tables that you have done this to you can merge them with the unified id column.
So origin and destination zip codes are location data. They'll tell you where stuff is. You could also use these to say, calculate things like distance traveled or bring in things like external data (zip code X is in State Y, thus I can look at State level indicators). The issues here are zip codes are not that specific (they don't even overlay onto districts) and so there is likely going to be some "noise" (unsystematic variance; things that could possibly make it more difficult to find trends) in calculating distance, and State level indicators may be too big (trends may be more meaningful at a granular level). The other issue is in regard to how the time variable is stored. If it's anything other than a count (number of minutes in a single cell), you'll probably have to transform it. For instance, if the data was something like, "18-APR-19_00:01:02" you'd have to make decisions about what interval was most meaningful for time to look at (clearly, you'd want to go to the lowest level possible [seconds], but this may require you to break out days, hours, minutes, and convert them all to seconds for a total count). Ideally you would separate on delimiters (like "-" and "_") convert the text "APR" to numeric, and then recode the month, and year to number of days (how far into the year you are). You'd then do something similar for time (converting everything to seconds). You can then convert your days to seconds and have a single seconds variable to equally compare everything (you can always convert this total to minutes, hours, etc; whatever you found meaningful). From here you'd have one continuous variable (time), and a location data point (zip) which is arguably only good for importing town, district, region, state, country level data. For instance, let's say you use zip code to identify an area like a city. You could then import things like traffic data from that city. This could allow you to look at congestion trends and try to determine if time were extended (a delay). You could import data on a variety of things (crime, weather, population change, regional income, etc). What model is "best" is going to involve a lot of testing, and even then may not be incredible accurate (zip code could be too imprecise). How exactly you'd test such a model is also likely going to vary on the exact questions posed. For instance, modeling city level indicators with travel time could be done with a hierarchical linear model (route nested within time; growth curves), but you could also look at developing archetypes (what I shared) in an attempt to identify specific regions based on variables (it's highly interpretive, and may not really result in a way where one can easily discern patterns), you could model variables using SEM and make specific comparisons. Again, I'm sure there could be a better analysis that directly gets at the heart of the question you want, but I wanted to illustrate a few possibilities to highlight how it can be anchored on your research question.
I wanted to answer OP's question, but I think this user did a better job.
It's possible to left\_join twice in the data, then mutate -&gt; coalesce(value.x, value.y) Someone else gave me the answer elsewhere and just to lyk Thank you!
If you're using base R: `df$x &lt;- as.numeric(gsub("[$, ]+", "", df$x))` If you want to use tidyverse: `df &lt;- df %&gt;% mutate(x = as.numeric(gsub("[$, ]+", "", x)))` &amp;#x200B; `gsub` is substituting spaces and $ signs with nothing. The `+` indicates that it can match either of those things multiple times within the same string so it'll get rid of all spaces, even when there are multiple within the string.
you'd better use tidyverse family packages, it is quite fast though not so fast as data.table, but it has a unified data analysis framework with a tidy data format. pls check [https://www.tidyverse.org/](https://www.tidyverse.org/) for more detail.
For a more general solution, take a look at parse_number in readr https://www.rdocumentation.org/packages/readr/versions/1.3.1/topics/parse_number
Hiya, how might I do this for if it meets certain conditions within two columns? E.g. for "1" in column 10 and "2" in column 12, I tried: row.names(mydf[mydf[,10] == "1",] &amp; [mydf[,12] == "2",]) But have had no luck and I am stuck because I am a pleb with R, and don't use it enough to learn properly...
In case you have other characters, the below will extract only numeric values from your vector. It does keep the dots if you have demicals. &amp;#x200B; \# create data.frame df &lt;- data.frame( x = c('$2100', '$2200.00', '$2300.99') ) \# base R df$x &lt;- as.numeric(gsub('\[\^0-9.-\]', '', df$x)) \# tidyverse df &lt;- df %&gt;% mutate(x = str\_replace(x, '\[\^0-9.-\]', ''))
Thanks for the great response - I definitely have some further thinking and planning to do before trying anything. This is making me realize I need to clarify the objective a little better. I'm going to start thinking through what archetypes I could/would use and go from there. Thanks
If you're gonna use tidyverse, i'd recommend str_remove instead of gsub
Thanks a lot!!!
I would read the book "R for Data Science" which is available for free online.
I'm looking for something more interactive. I'm more of a hands-on learner and have a hard time sitting down and reading straight up.
Very close! Keep both of your conditions for the two columns within your first indexing of mydf: row.names(mydf[mydf[,10] == 1 &amp; mydf[,12] == 2,]) I can definitely recommend learning R if you will be doing any sort of stats or data science! We all started out as plebs, don't worry. The more time you spend with it, the more powerful you will realize it is.
I would think of it like this: | Value | Count | Prob. | Cuml. Prob. | | ----- | ----- | ----- | ------------ | | 1 | 1 | 1/5 | 1/5 = 0.2 | | 2 | 2 | 2/5 | 3/5 = 0.6 | | 3 | 1 | 1/5 | 4/5 = 0.8 | | 4 | 1 | 1/5 | 5/5 = 1.0 | | NA | 1 | NA | NA |
So far the best ones have been the DataCamp courses, but you'll have to augment their courses with your own practice (you can use Kaggle, or you can use Tidy Tuesday (https://github.com/rfordatascience/tidytuesday) which has a nicely curated set of raw and cleaned datasets to work with.
so, the cumulative distribution function (CDF) is a little complex—more so than simply dividing the rank by the number of observations. the CDF (in oversimplified terms) is really the probability a random variable being of a value less than the next variable. the help page of the cume_dist function also says “Proportion of all values less than or equal to the current rank”. the number 2 in your vector has a value of 0.6 because, in the sequence of `1, 2, 2, 4, 5`, `2` is at both the second and third position of the ranked vector. obviously, at the second index, the CDF would give us a value of 0.4. however, since 2 is *also at the third position*, the CDF gives us a value of 0.6. this is because the CDF calculates the proportion of all values *less than or equal to the current rank*. you may say, “that makes sense, but the first 2 is the second item in my ranked vector, shouldn’t it be 0.4?” kind of, but not really. remember, the CDF is a probability function. technically, we can’t rank items when they have the same value in a non-arbitrary way, especially if we’re ranking the items by the value. what we do know, however, is that once we get to the second 2, the proportional sum of all prior values equal or less to 2 is 0.6
No problem! One clarification, the archetypes comment was meant in regard to the link for that specific analytic method. In this instance the computer would be used to help identify them. The issue here is often the computer can provide you with information about how many plausible groups there are, but the issue is its inferential (then you would see which regions are grouped with which, and consider what archtypes they are). This may sound easy but sometimes computers can bin things where they fit, even if their relative fit isn't good enough (i.e., it's not a meaningful category, but the computer didn't know where else to classify them). You can, of course, think of your own models and your own profiles that should exist prior to doing this analysis, or even just take a more qualitative approach (you / your team as the content experts decide what the profiles / types are). Which way to go likely depends on your skill set, timeframe, and the primary research question(s).
DataCamp costs money, but it is worth it. It will adapt to your needs and offer you a learning path.
Thank you!
Thank you!
You can apply the cumsum() function across rows and take its transpose to orient it correctly. Try: cs &lt;- data.frame(t(apply(df, 1, cumsum)))
thank you so much! This worked, I think I tried this originally but forgot that I needed to transpose it and only registered that it gave the wrong numbers! Thanks so much again
I completely understand and appreciate the added detail. We have some time before a finalized product is expected. So even if this isnt the path I think it will be good learning opportunity. I understand a lot of this conceptually but dont have a lot of direct experience. Some folks around here do though. Thanks again!
R for data science is interactive. It’s in the form of a book rather than videos, but it includes a pretty thorough list of exercises throughout. If you mean that you like lectures and video examples, then there are other sources. But if you mean you want a learning resource that provides guided examples, [R for Data Science](https://r4ds.had.co.nz/) is very good. I actually recommend this article often. (R for Data Science is step 5, but some of the steps are very short): [New to R? Kickstart your learning and career with these 6 steps! – paulvanderlaken.com](https://paulvanderlaken.com/2017/10/18/learn-r/amp/)
You should check out [`mice::mice`](https://www.rdocumentation.org/packages/mice/versions/3.6.0/topics/mice). You can specify linear regression in the `method` argument. It also has functions to fit models to each imputation and then aggregate them according to Ruben's rules.
You can check it out with a two-months free trial via [Microsoft dev essentials]([https://www.visualstudio.com/dev-essentials/](https://www.visualstudio.com/dev-essentials/)) too.
Gotta argue [against giving DataCamp money](https://www.computerworld.com/article/3389684/r-community-blasts-datacamp-response-to-execs-inappropriate-behavior.html)
I'm not sure if regressing the data is what I want?
What would be your alternative?
They still do at good job at teaching R, SQL hell even Unix (Mac Terminal Commands).
Ah that makes sense. Thanks
Wow this makes it so much obvious. Thanks
I'm assuming your PS. is true and the example you have shown is what you actually want. What you are describing in your example is linear interpolation and linear extrapolation. Interpolation is what happens between known years within a group (country) and extrapolation happens for years outside an interval of known values (such as 2011 in your example). Extrapolation is trickier to get "right". Imputation on the other hand usually refers to methods that build some multivariate model of your dataset to predict the missing values. Examples here are Amelia and MICE. There are simpler methods called imputation too like mean imputation. The interpolation part is very simple as it just draws a line between two points of observed data to fill the missing. This can be done easily with zoo::na.approx() as described here: [https://stackoverflow.com/questions/33696795/r-interpolation-of-nas-by-group](https://stackoverflow.com/questions/33696795/r-interpolation-of-nas-by-group) This however only does interpolation. This won't fill your ends, 2011 will stay missing. Passing rule=2 to na.approx as you might try will simply repeat the last seen value, not extrapolate the trend. This is because there isn't really a sane default for extrapolating trends. What you might do for the missing values at the end is to fit a linear model for each country and use that to predict the missing values. Here is a really nice SO post describing how to do that using lm and data.table: [https://stackoverflow.com/questions/15605772/how-to-efficiently-extrapolate-missing-data-for-multiple-variables](https://stackoverflow.com/questions/15605772/how-to-efficiently-extrapolate-missing-data-for-multiple-variables) So: * Interpolate the missing "in between" values using na.approx. * After doing that, fill the remaining missing using a linear model. A potential problem is that the linear model might produce a jump at the end of your data if the data isn't showing a perfectly time-linear change. If that becomes a problem you'll have to compute the rate of change between the last N years of data and apply that rate of change to the future years. If you want to get fancy you could use a spline.
From a statistics perspective, linear interpolation and linear extrapolation both have **severe** problems and the validity of any results would probably be questionable. [Here's a good article on the various approaches to dealing with missing data](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4) and the drawbacks of each one.
 It is not an option to simply add the stars to the printed output from a t.test. But, you can extract the p-value and use a function from the gtools package: ttest &lt;- t.test(1:10~rep(1:2, each = 5)) pval &lt;- ttest$p.value stars &lt;- gtools::stars.pval(pval) # then put the bits together: paste0("p=", format.pval(pval), stars)
I actually went and started reading the book online &amp; also using the datacamp resource! Looks to be a good start!
It looks like, on the documentation, you need a period before 'fun'. Try: ggplot(iris, aes(x = fct\_reorder(Species, Sepal.Width, .fun = median, .desc =TRUE), y = Sepal.Width)) + geom\_boxplot() Does that work?
I just ran \&gt;attach(iris) \&gt;library(ggplot2) \&gt;library(forcats) \&gt;ggplot(iris, aes(x = fct\_reorder(Species, Sepal.Width, .fun = median, .desc =TRUE), y = Sepal.Width)) + geom\_boxplot() and got what I think you're looking for, [descending boxplots.](https://imgur.com/a/fanLHCX)
Your comment (and a single period) just saved a problem I've spent *entirely* too long on. Thank you!
Of course! If you have other questions, feel free to shoot me a message!
Are you looking to count all instances of each type of interactions? Or instances for specific sets of interactions? I.e. all 3-way interactions vs all instances of (Gene 1, Gene 2, Gene 3)
All 3-way interactions really (and 4- and 5-way). There are 800 genes, and 150,000 edges, so I've no idea how many 3-way interactions there are! Thanks for the reply.
&amp;#x200B; I study your post carefully. It is an amazing post. It is very usefull post for all students. your describing style is also better. thanks for help me. students can learn programming on &lt;a href="[https://www.programmingtutorial.org](https://www.programmingtutorial.org)" &gt;programming tutorial &lt;/a&gt;
I haven't done this before in R (I had a bioengineering class and they used Python to simulate gene splicing) but it sounds like you might be able to do it just using the count() function from the plyr package. I hope this is helpful!
Thanks for the reply, I don't know what each of the 3-way interactions are, I need to infer them first and then count them.
Hi thanks for this! it is very useful. I also found this playlist which is really good. Putting it out there to whoever is interested ([https://www.youtube.com/playlist?list=PLLxj8fULvXwGOf8uHlL4Tr62oXSB5k\_in](https://www.youtube.com/playlist?list=PLLxj8fULvXwGOf8uHlL4Tr62oXSB5k_in))
Could you rename col2 then left_join the dataset to the original?
I am not familiar with your domain. Is a 3-way interaction a statement that there is a group of 3 genes where each of these pairs have an edge? If so, this looks like a problem of [finding cliques in graphs](https://en.wikipedia.org/wiki/Clique_(graph_theory\)) and you may try using the `igraph` package to find them.
Super super helpful. I'll check in with the bosses and see what they have to say. I think if it was only a year of extrapolation, it could be OK but with 8 years of extrapolation it seems like it will be a problem.
Perhaps a graph-specific package like `igraph` would have the easiest way to do this.
Are you trying to infer instances like 1,2 2,3 3,4 Or 1,2 1,2,3
I might consider moving to long format for your data instead of wide format. Gather() can do the trick
 mutate_at(df, vars(starts_with("murder")), ~count = if_else(. &gt; 7, 1, 0))
IMO: First (tidy](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) your data by doing something like ``` murder_data &lt;- df # Always better to have descriptive variable names tidy_murder_data &lt;- murder_data %&gt;% tidyr::gather( key = "year", value = "murder", tidyselect::starts_with("murder") ) %&gt;% dplyr::mutate( year = as.integer(stringr::str_sub(year, 7, -1)) ) ``` Then you can use a grouped aggregation to calculate, for example, the sum of the murders each year ``` tidy_murder_data %&gt;% dplyr::group_by(year) %&gt;% dplyr::summarise( total_murder = sum(murder) ) ``` Or the state with the most murders for each year ``` tidy_murder_data %&gt;% dplyr::group_by(year) %&gt;% dplyr::filter( murder == max(murder) ) ```
What error messages?
This is the error message: https://i.imgur.com/D72umUy.png
PhantomJS is dead and has like 4000 open issues and the single maintainer closed the project the day Chrome Headless was announced. Switch to CH if possible. For your error message: the port seems to be wrong
Thank you for your response. I am not sure if PhantomJS is the issue though, because chromdriver and geckodriver are automatically loaded too. Let me share the whole message in the console: https://i.imgur.com/sE9ER5j.png https://i.imgur.com/HnAQtZU.png
An interaction (in a statistical sense) is when the relationship between two or more variables explains variance above and beyond the variable on its own. As such, "counting" an interaction (how many potential interactions you may have total) is a function of how many variables you have. Assessing whether or not those interactions are statistically significant is another question, and whether those interactions are meaningful is yet another series of questions. Which are you looking to do?
The port number should be the one that is choosen as the port = XXX parameter in the function declaration. I never used RSDriver so I can't help with that problem outside of reading the error mesage, sorry. I use Puppeteer in Nodejs for my crawling because I need some APIs (checking every request) that is not available in Selenium or Puppeteer implementations in other languages (Nodejs package is the official one from Chrome).
Thank you. The links seem quite useful.
a &lt;-c (1,2,3) b &lt;- c('a','b') df &lt;- expand.grid(a,b) df$combined &lt;- paste(df$Var1,df$Var2) df Var1 Var2 combined 1 1 a 1 a 2 2 a 2 a 3 3 a 3 a 4 1 b 1 b 5 2 b 2 b 6 3 b 3 b
Thank you! obviously still learning but I don't know why I spent so much time looking for this answer.
You are welcome! I'm 3 years into R and this was the first time I was able to help someone online. expand.grid() is a nice trick.
You’re calling `document()` inside the source code that you’re running `document` on in the first place. Since that function *executes* the code as part of its work, it’s executing your `document()` call. And again. And again. Etc. The solution is to remove the *two (!)* last lines from your file, they do not belong into the package source.
I'm being a bit pedantic (and possibly redundant since you may already know this in which case I apologize) but outside of a function else should follow the bracket and not be on a new line. The code works as but as this user points out ([https://stackoverflow.com/a/25885505](https://stackoverflow.com/a/25885505)) it won't work outside a function.
Johns Hopkins on coursera has some courses. &amp;#x200B; [https://www.coursera.org/specializations/r](https://www.coursera.org/specializations/r) &amp;#x200B; [https://www.coursera.org/specializations/jhu-data-science](https://www.coursera.org/specializations/jhu-data-science)
1,2 and 1,2,3 more than 1,2 2,3 3,4.
&gt;mutate\_at(df, vars(starts\_with("murder")), \~count = if\_else(. &gt; 7, 1, 0)) While seemingly the simplest way, I've been unable to get this method to work. It yields an error: `Error: unexpected '=' in "mutate_at(df, vars(starts_with("murder")), ~count ="`
Sorry I don't usually use named arguments this way, it's probably mutate_at(df, vars(starts_with("murder")), ~list(count = if_else(. &gt; 7, 1, 0)))
can only add if you eventually adopt the tidyverse this is achieved with `tidyr::crossing(a, b)`
I did not believe you, seeing how any other language would behave normally with the else statement on another line, but it actually does not work when not in a function. I don't know how to feel about that. This language will never cease to amaze me, whether positively or not.
I don’t believe this is the case anymore, in my R package I have a ton of if else statements like this and they all work, you can try for yourself: https://github.com/josephsdavid/tswgewrapped/blob/cb7960f84f97b3f9ed0bdeaa7c5fe2003c00e099/R/playground.R#L27
You may want to add \`stringsAsFactors = FALSE\` unless you're happy with Var2 as a factor.
It would be more helpful if you post the section of the code that seals with the pruning.
optimalcp &lt;- CART$cptable\[which.min(CART$cptable\[,"xerror"\])\] m2 &lt;- prune(CART, cp = optimalcp)
This is a consequence of the syntactic simplicity of "statement ends when syntax is complete at the end of the line" approach taken by R. Since `else` is optional and R does not look ahead, we cannot lay out the code with complete freedom of choice. Other languages have their own syntactic idiosyncrasies... though once we are used to them we sometimes forget that they are unintuitive to the beginner.
You want a join of some type. Look into dplyr: inner_join keeps data that has a match in both data frames left_join keeps all data from the first (left) data frame and provides NAs for missing values from the right data frame. There is also right_join which is just the opposite. full_join gets matches from either table and doesn't toss out any data. anti_join keeps data from the left table where there are no matches in the right.
Look into any() and all(). They will return a TRUE if any or all conditions within match, respectively. Also, I use %in% to test against multiple matches quickly. You can use an ! For not to reverse the logical
Ah thanks! I've tried dplyr functions, can't get my head around how to use ranges though e.g. if between 1-100 then "green", between 2-200 then "red".
Use between() to return a logical
thanks! :)
I'm not exactly sure what your use case is, but if you're just trying to do a red/green category you can do this: Data %&gt;% mutate(CategoryName = if_else(OthCategory &lt;= 100, "Red", "Green")
Ah no, the range in column 1 of dataframe1 extends to 2000, with 20 bins. And I am trying to add a new column in dataframe1 of the colour the cell in column 1, with ranges of colour bins specified in dataframe2.
Okay no problem. Since you've already done it in two data frames, you can stick them together by their key if one is sorted funny with full_join, or just use cbind if they are in the same order. Could you give me some example data if I'm still not understanding you correctly?
Just tried full\_join, which just returns a new dataframe as if it pasted the two together rather than considering ranges. &amp;#x200B; This is dataframe1: 3 6 90 1897 46 267 174 98 This is daraframe2: 1-100 green 101-200 red 201-300 blue 301-400 orange etc What I am aiming for is: 3 green 6 green 90 green 1897 indigo 46 green 267 blue 174 blue 98 green
Try [fuzzyjoin](http://varianceexplained.org/fuzzyjoin/) or a non-equi join from data.table. Alternatively make a helper column in both dataframes with ```cut()```and join on that.
Adding a quick thought that may work for you. Assuming your ranges are all 100 (and they look to be), create a new column in your first data frame using ceiling() and divide by 100. So, ceiling(100/100) gives you a 1, and ceiling(101/100) results in a 2 (or anything up to 199/100). Then, your second data frame doesn't need ranges, it just needs the numbers from 1 to 20, or scale up appropriately. Once you have that, you can use dplyr's left\_join(). `left_join(dataframe1, dataframe2, by = "yournumericcolumn")`
You can also repeat and use the &amp; sign. Like this. ....filter( Date &gt;”2019-02-03” &amp; Date &lt; “2019-05-14”)
Ah thanks, but sadly they aren't I just thought I'd simplify my problem!
Thanks! Still clueless how to implement it though.
You need to bucket df1 values according to df2 ranges. The cut function can do this, or try findinterval.
dplyr::case_when could be a good method here
In the code below, I've demonstrated three different options for you to consider, along with some additional commentary for each one. Note that I needed nearly an hour to write *Option 1*. Although the code isn't very long, debugging it was a pain, because I always forget the subtleties of the `match()` function and the `%in%` operator. I wrote *Option 2* in about 5 minutes. I actually had it working in about 1 minute, but I had to go back and review the documentation for `merge()` to verify that I was using it correctly. For *Option 3*, I needed a minute or so to write the code, and I didn't even bother checking the documentation. The reason I like *Option 3* the best is because it adheres to a well-established abstraction and convention for data manipulation, namely *relational database joins*. With over 40 years of use, relational databases are one of the most enduring computer science concepts. By using an interface that is very well-established, the `dplyr` `*join` functions are very intuitive and easy to use, so you're less likely to make an error. ## Source Code library(dplyr) CheckForEquality &lt;- function( df1, df2, df1_name = deparse(substitute(df1)), df2_name = deparse(substitute(df2))) { test_symbol_rslt = ifelse(identical(df1, df2), "==", "!=") return(paste(df1_name, test_symbol_rslt, df2_name)) } NUM_ROWS_DF1 &lt;- 20 NUM_ROWS_DF2 &lt;- 4 COLORS &lt;- c("RED", "GREEN", "BLUE", "YELLOW", "ORANGE", "BROWN", "BLACK") THINGS &lt;- c("HOUSE", "OFFICE", "CAR", "WINDOW", "CHAIR") # First, generate two sample data frames set.seed(2^3) SAMPLE_VECTOR_DF1 &lt;- sample(x = seq.int(NUM_ROWS_DF1 * 2), size = NUM_ROWS_DF1, replace = FALSE) set.seed(2^4) SAMPLE_VECTOR_DF2 &lt;- sample(SAMPLE_VECTOR_DF1, NUM_ROWS_DF2, replace = FALSE) set.seed(2^5) df_1 &lt;- data.frame( key = SAMPLE_VECTOR_DF1, colors = sample( x = rep.int(COLORS, ceiling(NUM_ROWS_DF1 / length(colors))), size = NUM_ROWS_DF1, replace = FALSE ), stringsAsFactors = FALSE ) set.seed(2^6) df_2 &lt;- data.frame( key = SAMPLE_VECTOR_DF2, things = sample( x = rep.int(THINGS, ceiling(NUM_ROWS_DF2 / length(THINGS))), size = NUM_ROWS_DF2, replace = FALSE ), stringsAsFactors = FALSE ) # Note that all of the values of df_2$key are contained in df_1$key, but the converse is not true. df_1 df_2 # Option 1: Use base R's standard indexing functionality. df2_12 &lt;- df_1[df_1$key %in% df_2$key, ] df2_12 &lt;- df2_12[order(df2_12$key), ] df2_12$things &lt;- df_2$things[match(df_2$key, df2_12$key)] rownames(df2_12) &lt;- NULL # Reset rownames to match df1_12 and df1_21 df2_12 CheckForEquality(df1_12, df2_12) # Option 2: Use base R's merge() command. # Note: In this example, merge() doesn't need to be told which column to use for the join because, by default, # merge() will check both data.frames for all common columns and try to merge them. In this case, because # both data.frames have a column called "key", the only parameters that need to be passed to merge() # are the names of the data.frames to be merged, df_1 and df_2. df1_12 &lt;- merge(df_1, df_2) df1_21 &lt;- merge(df_2, df_1) df1_21 &lt;- df1_21[, c("key", "colors", "things")] # Reorder column names to match df1_12 # Using merge()'s default parameters, df1_2 and df2_1 are the same size, and no values are NULL because # df_2$key is a subset of df_1$key and all values of "key" are unique within a given data.frame. # Excel's vlookup() command let's you specify # whether or not it should perform exact matching and whether or not its data ranges or sorted. # However, with R's merge() function, matches will always be exact, and the length of the resulting # merged data.frame can be longer than, equal to, or shorter than the length of the shortest data.frame # used for merging. With Excel's vlookup capability, the returned values will always be equal in length # to the number of times vlookup() is called, or, in other words, vlookup will always return 1 value # per function call. An exception occurs if you use an array-formula. df1_12 df1_21 CheckForEquality(df1_12, df1_21) # Option 3: Use the Tidyverse's dplyr join functions. df3_12_left_join &lt;- dplyr::left_join(df_1, df_2) df3_12_left_join &lt;- df3_12_left_join %&gt;% arrange(key) # Sort by "key" df3_12_inner_join &lt;- dplyr::inner_join(df_1, df_2) df3_12_inner_join &lt;- df3_12_inner_join %&gt;% arrange(key) # Sort by "key" df3_12_right_join &lt;- dplyr::right_join(df_1, df_2) df3_12_right_join &lt;- df3_12_right_join %&gt;% arrange(key) # Sort by "key" df3_12_left_join df3_12_inner_join df3_12_right_join CheckForEquality(df3_12_inner_join, df3_12_left_join) CheckForEquality(df3_12_inner_join, df3_12_right_join) CheckForEquality(df3_12_inner_join, df1_12) --- ## Side Note: A better alternative to vlookup() and hlookup() in Excel Although I work with R extensively, I still use Excel and Google Sheets extensively. With Excel and Google Sheets, there is a much better alternative to `vlookup()` and `hlookup()'`: using `index(DATA_ARRAY, ROW_INDEX, COLUMN_INDEX)' with `match(VALUE_TO_MATCH, DATA_ARRAY, MATCH_TYPE)`. The syntax for both functions is identical for Excel and Google Sheets. You can review the documentation for the Google Sheets version of [match() using this link](https://support.google.com/docs/answer/3093378?hl=en). There are many articles that explain why `index()`/`match()` is superior to `vlookup()`/`hlookup()`, [such as this one](http://www.mbaexcel.com/excel/why-index-match-is-better-than-vlookup/), but in a nutshell, the `index()`/`match()` combination is much more flexible and robust.
I think you would want to create a column representing the beginning of the month corresponding to that date, and then `group_by( Date, Month )` and then use `mutate` to create a logical column indicating whether to keep the rows in that month and then filter based on that column. If you wanted a code example, perhaps you should include data that could actually be used to confirm that the code works.
 library(dplyr) DF1 &lt;- read.table( text = "Values 3 6 90 1897 46 267 174 98 ", header=TRUE ) DF2 &lt;- read.table( text = "Range Color 1-100 green 101-200 red 201-300 blue 301-400 orange 401-2000 indigo ", header = TRUE, as.is=TRUE ) DF2b &lt;- ( DF2 %&gt;% mutate( RangeMax = as.numeric( sub( "^\\d+-", "", Range ) ) ) ) DF1b &lt;- ( DF1 %&gt;% mutate( Color = cut( Values , breaks = c( 0, DF2b$RangeMax ) , labels = DF2b$Color ) ) ) DF2 is not really an ideal way to represent the ranges. If you are not constrained you can eliminate the need to extract out `RangeMax` by changing your representation in the original lookup data frame. Result in DF1b is a factor... you can use `as.character` if you prefer.
I'd be great if you could provide the sample data in the form that we can easier assist such as data &lt;- data.frame(Date = as.Date('2019-01-01') ,Stock = c('AAPL') ,Return = (0.01)) With that type of sample, it is easier for those that want to help. We just copy the sample code and work on it. It's easier and reproducible too. &amp;#x200B; That being said, I have question 1. Is 'beginning of the month' date always 1? Or does the data use beginning of the trading day which can any number?
Yes you are right, it is trading days and it may not always be 1. &amp;#x200B; As for your comment regarding the sample data, I am happy to provide it. But I am not sure how I can attach it? Will you know how I do it? &amp;#x200B; Thanks a lot!
Hi, interesting, didnt thought of that. Thanks a lot! I wouldve included the data but I dont know how to add an attachment to the post :/
Have you tried using stargazer package? It will present your regression results in a nicely formatted table along with all the stars you are looking for. Note however that if you change the way standard errors are calculated, perhaps you used white standard errors instead of the default, then you may need to make some manual changes to the stars.
You can upload the file in Google Drive, Microsoft OneDrive, and the like. Then share and share the link with us. :)
You can't, but you don't need to. a) You can actually fill out the table you made enough that it would serve as a small test. b) You can create the sample table in R and use the `dput()` function to paste it into the body of your post with four spaces before the first line. c) you could make a github gist and link to it.
I think the cut function would do what you need here? Or dplyr with mutate and case_when
Plans to implement these in Stan?
Currently working on another package for probabilistic supervised learning, a subset of this will be inference and ML for distributions, which will interface with BUGS and maybe JAGS and Stan if people find it useful.
Wow, thanks for all this. That's awesome mate I really appreciate it
Is it similar to this? [https://github.com/alexpghayes/distributions](https://github.com/alexpghayes/distributions)
Stan has probably supplanted BUGS and JAGS these days, so it’s worth including it.
[https://www.rstudio.com/online-learning/](https://www.rstudio.com/online-learning/) R studio website has some helpful resources! *Learn the basics* *Take a free R tutorial by* [*Code School*](https://www.codeschool.com/) *at* [*Try R*](http://tryr.codeschool.com/)*, who provide interactive lessons that will get you writing real code in minutes. It is a great place to make mistakes and test out new skills. You are told immediately when you go wrong and given a chance to fix your code.* *Or try* [*Leada*](https://www.teamleada.com/courses/r-bootcamp)*. Leada gets you programming in your own environment with videos and exercises. The first few courses are free and cover how to install R and RStudio.*
Do you recommend them, and what do they cover in particular?
 Yes, I recommend them for someone who already knows the basics of R and has done a bit of analysis in it as they are really "how-to" and "back-end" books that may show you tips and tricks you didn't know and give a better understanding of the back-end of R. The cookbook is a problem/solution style book where it lists a problem like "You want to use a function in a library, but the library isn't loaded", it then proceeds to tell you how to load the library. It starts really basic with stuff like that, but moves up to statistical analysis, regressions, anovas, graphs, etc. Advanced R is really a study on how R works on the back-end. It's not teaching you how to program in R or do statistical analysis, it dives deep into the details of how R works when you preform certain operations, for example the first "fundamentals" section starts with the difference between objects and their names. For example: a &lt;- c(1, 2, 3) b &lt;- a At this point, both a &amp; b are just names pointing to the same object (the vector (1,2,3)) which is at a given address in memory. When you modify the object, say with b[[3]] &lt;- 9 (assigns the third element of the vector(3) the value of 9, this creates a new object. So now a still points to an object with the values (1,2,3), but b points to a new object with values (1,2,9) and they now have different addresses in memory. That's basically how the book if flowing. The second edition is color which really helps illuminate some of the concepts.
Can you provide a sample of your data? &amp;#x200B; Also, have you tried `filter()` from dplyr package?
Thx for the tip, had no idea Adv. R had a second ed recently released. Just ordered!
Hi thanks for making me aware of that package, I spent months researching what was available and never came across that. Short answer is yes and no. Yes because they also implement an OO style distributions interface and use distributions.jl as inspiration but for some reason that package actually downgrades the \`distr\` package from S4 to S3, which means it lacks inheritance and other design patterns, whereas ours upgrades to R6 and includes inheritance and class hierarchy, decorators and wrappers. Additionally we introduce much more functionality including parameterisations of distributions, composite distributions and more implementations. &amp;#x200B; Are you the maintainer/know the maintainer of that package? It's probably worth a chat!
Thanks that's good to know :)
&gt;How do I take away all rows that dont have a specified value in a certain column in a data table. I tried using the subset function and it told me it wasn't logical What expression you used in subset? You can also use [] to subset
I dont really "know" him but I did make some pull requests on another of his packages. I opened an issue on that one with a link to your package. Thanks for sharing and the package looks nice.
Maybe look at /r/Rlanguage
No worries. Essentially BUGS and JAGS use Gibbs sampling, whereas Stan uses adaptive Hamiltonian Monte Carlo (using NUTS - no U turn sampler) which in a wide variety of cases is more efficient than a Gibbs sampler. Although there’s people who will be explain that in better detail than I can.
This question seems jumbled... 1. What is this word "best"? Other than possibly having some "C:\" stuff (that wouldn't be there anyway normally), what porting issues do you foresee? 2. What do you think of when _you_ say "API"? I think of "Application Programming Interface", which is not a graphical *user* interface like Shiny. Which in turn suggests approaches like the `plumber` package. 3. Are you wanting to secure access to it for restricted use on the open internet? Or would you be satisfied with access on your LAN, perhaps via VPN? AFAIK the open source Shiny server doesn't support that directly, but you may be able to manage it through an Apache server. 4. This really has nothing at all to do with the R language... wouldn't r/rstudio be more appropriate place to post?
Generally, it can be more or less moved from a Windows machine to an Ubuntu. You have to be specific where you have difficulties. Shiny Apps do not expose an API. They are not built for that. If you want a service where something can request some data or the like, see plumber.
Yeah didn't know the best sub for it. I would be hosting an app and using Shiny server to host it and using the plumbr package to create a rest API for it. &amp;#x200B; Just needs to be done internal and I have no clue about networking.
Yea probably need to use plumbr. &amp;#x200B; We want a shiny server to host apps and markdown, but also want to do some adhoc calculations that can be accessed by API and was hoping to use Shiny Server to host a plumbr app or something. &amp;#x200B; If not, I guess I'll have to learn how to dockerize things.
Shiny and plumber are two widely different things. Think graphical browser vs. telnet. Shiny Server does not host plumber.
So I'd use the shiny server for the apps and then dockerize the plumbr? Outside of my job description, but I have to learn it.
Shinyproxy can itself be dockerized, bit otherwise live side-by-side with docker, using it do launch docker images with your apps. Plumber would be a third component with it's own components, packed onto a docker image, oblivious to shinyproxy.
Guess I got a lot to learn.
I'm assuming you want to select all observations that have the specified value (xxx) in column (foo) of the data frame (df): library(dplyr) df &lt;- filter(df, foo == "xxx")
The first thing that comes to mind is the kind of random seed that is in use - set.seed(1991, kind = "Wichmann-Hill") r1 &lt;- rnorm(10) set.seed(1991, kind = "Super-Duper") r2 &lt;- rnorm(10) In this case, r1 and r2 will be different, even though they had the same seed. Another possible cause could be a difference in the default options on each machine: optList &lt;- options() If neither of these is fruitful, it may take a manual run through the calling function [here](https://github.com/cran/nnet/blob/master/R/nnet.R) to see where the results start to diverge.
Generally a subset condition can be anything that returns a logical vector or row indices, so often they're comparisons. Using bracket notation: dt[x == 5] dt[x != "abc"] dt[grep("a", x)] Using subset: subset(dt, x == 5) All of these return a new data.table, so you'd have to assign it to a new variable if you want to continue using it.
Gotcha, we'll make sure all are available then :)
Also, even if the seed is the same, the randomness will differ between OS if they are different machines. I’m not super familiar with software containers, but it might be worth trying something like singularity or docker if you want the exact same behavior on multiple machines.
Same seed should result in same random numbers as long as the pseudo-random number generator is the same. And, AFAIK, in R it is the same across many versions and operating systems.
Machine precision options, things like 32 vs 64 bit, if using compiled code different compiler options yielding slightly different choices, things like that. By significantly different, do you mean "we can tell these are not bit exact", or do you mean, "it actually makes a difference when it comes to decisions"?
Thanks!
Thanks, very useful. I did use dplyr in the end.
This was just posted not too long ago: https://www.reddit.com/r/MachineLearning/comments/cgbwk2/r_a_repository_of_community_detection_graph/
I hear you on the query tip haha, I find that just saying query + R is enough. As for virtual environments, R doesn't have them built in, but there are tools you can use that will work like [packrat](https://rstudio.github.io/packrat/). I have never used it because there are extremely strict backwards compatibility requirements when updating packages on CRAN, and syntax-breaking changes do not happen that often. I've been using R for \~5 years now and the only versioning problem I've run into is when my R version was out of date and I needed to update it to use a new package. This will obviously be different depending how you are using R, if you are frequently sharing your code with coworkers or using R in a production environment, it's probably a good thing to have.
This may be unpopular because we are on an R subreddit, but Python has much more robust libraries for creating web apps, especially REST APIs. Support for things like Flask and Django are ubiquitous on the internet. I'm sure you can get a web app up and running with plumber, but if you are talking about using this in a production environment then I would go with Python. R has many strengths over python, web apps and REST APIs are not one of them.
Honestly, this is just a PoC right now so I don't exactly have to get it ready for a production environment but maybe one day we would so I wouldn't want to go too far in the wrong direction. Neither of us are too great with python though, but are familiar with R so there's just a tiny bit of hesitancy to rewrite everything in python. It's really just a weighing of options - is it more effort to setup the app with plumber, than it is to tear everything down and learn/rebuild with python (with the assumption that integration into the web app is much easier)? That's kind of the question we're wrangling. Honestly I was just hoping there would be some holy grail R maven dependency I haven't heard of that I could add to my java project that would let me interact with the model as though it were an API or java class :p
If you are using Keras/Tensorflow in R then you can skip the R script and just use java directly once you get your h5 file saved: [https://towardsdatascience.com/deploying-keras-deep-learning-models-with-java-62d80464f34a](https://towardsdatascience.com/deploying-keras-deep-learning-models-with-java-62d80464f34a)
Oh interesting. We'll definitely keep this in our back pockets! Thank you, not-so-suspicious gardener
Awesome, thanks!
Sounds like you have 2 questions: 1) Environments in R run similar to python. If you just install packages, they’ll be installed in the system environment. You’re probably familiar with anaconda. You can install R in a condo environment in exactly the same way you do for python. You then have the same dependency issues that you do in python: you environments don’t bleed over, but you still have package version incompatibilities to manage (packrat, etc). 2) google-fu for R: I find that R by itself works, especially once you search enough that google starts associating the character R with the language for you. Beyond that, I find that “tidyverse”, “ggplot”, and “dplyr” are the most common tags I add to my search queries to help google find me the right answers.
I’ve been playing with my intro to R recommendations here: https://revgizmo.github.io/My_path_to_R_mastery/my-intro-to-r-recommendations.html I don’t get into environments per-se, but I think bullets 2-4 will probably be helpful for you as you think about your workflow. My intro to R recommendations (summarized): 0. Read through the links below before starting (at least glance through them) 1. [New to R? Kickstart your learning and career with these 6 steps! – paulvanderlaken.com](https://paulvanderlaken.com/2017/10/18/learn-r/amp/) (learn R) * Especially [R for Data Science](https://r4ds.had.co.nz/) (tidyverse) 2. Read this: [Happy Git and GitHub for the useR](https://happygitwithr.com/) (workflow) 3. Read this: [Project-oriented workflow - Tidyverse](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/) (workflow) 4. Read this: [A perfect RStudio layout (Ilya Kashnitsky, 2018)](https://www.r-bloggers.com/a-perfect-rstudio-layout/) (workflow)
R doesn't support virtual environments. Have you thought about using docker?
RStudio has built-in environment, project, and package management tools that should allow you to maintain version control. See here for a reasonable overview: https://environments.rstudio.com/
There is an experimental package r-env that seperates packages from that folder from other folders, just as virtualenv in python
i'd vote for plumber. i have used it for similar low volume use cases. works well and is fairly straightforward.
You don't need selenium for that. You need R to see what's in your clipboard and pass it to selenium. The way to do that seems OS dependent. I don't have a way to verify this but a quick google search lead me to `data &lt;- readLines(pipe("pbpaste"))`
Perfect. Thank you very much for your response.
Example DF: &gt; (df &lt;- data.frame(a=c("a", "", "b", "", "c"))) a 1 a 2 3 b 4 5 c Reorder based on a condition: &gt; cond &lt;- df$a=="" # could be is.na(), etc. &gt; df[c(which(!cond), which(cond)),] # reorder [1] "a" "b" "c" "" ""
For a system agnostic solution, use `clipr::read_clip()`
Thanks alot, works perfectly.
Hey, Veinie, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Fantastic.
If you're using tidyverse, I recommend dplyr:: arrange... arrange (data, colname == "xyz") This arranges the false rows first, then the true rows
So, there is an R package just for reading the clipboard. Isn't R fantastic? Being a total noob, I am just curious. Did you find the package via a normal Google search?
Cool, thank you for the info!
I vaguely remembered looking into this before and finding a system agnostic method. I remembered the `datapasta` package definitely had access to the clipboard regardless of platform. I looked into its code to see how it does it. It was using clipr.
Cool, good to know. Yeah I saw that every package downloaded checks compatibility internally with a dependent packages and since it seems everything is checked by CRAN, but I also don’t like placing my trust solely in one entity. Thank you for the info
I install R, R studio, my data, and the packages I need in a singularity container for each project. This has the advantage that I also control the whole OS. They're also portable, in contrast to docker. Another advantage is that they're writable - you don't have to specify the whole environment at the start of your project.
Singularity works better in this case.
Now that's reproducible research!
Ah, thank you. Yeah I’ve been using the environment set up in RStudio and noticed that, I’ve just had issues with Python in the past with leaking packages. I’ll check into doing that with condo, thank you. Thank you for the SEO advice, been spoiled from probably too broadly used languages
 Very cool, thank you. Been mostly just going through Derek Banas and RStudio, so that’s a big help
Yeah I was using that, but just didn’t know if it would leak due to nightmares from previous environments
I haven’t, so just having multiple docker containers on my local environment? Kinda like the idea for specific projects for sure
there’s rstudio’s `renv`, which is nice, but has to be ran from within R. some CLI options i’ve used are `jetpack`, which is an r package but can be set up to be used as a CLI by running `jetpack::init()` i like JP b/c of its ability to install packages globally, or locally. another one is `pkgr` by metrum research group, a project focused CLI written in go. it’s an early WIP, but looks promising. it seems to be a declarative approach to r environments, where your “install plan” is defined with a YAML file links: [pkgr]( https://github.com/metrumresearchgroup/pkgr) [jetpack]( https://github.com/ankane/jetpack)
Super awesome, thank you. Because yeah, RStudio is cool, but not my favorite IDE and would rather be in the terminal
You want a network analysis with either in-degree/out-degree/in+out/ego network parameters per node.
i hear you lol. my only gripe with rstudio is it's lack of syntax highlighting for function parameters and known/assigned/unknown variables. if you like working in the terminal, take a look at [radian](https://github.com/randy3k/radian). it's an ipython/julia-repl like console for R
Just checked and I get the same thing, also when subsetting other single columns. That's very weird and I'm also interested in hearing the justification for this.
If you don’t want to drop the dimensionality, ie you want to extract the first row as a matrix rather than a vector. Use M[1, , drop = F]
If you run \`?matrix\` you can see the data argument is a vector. So when you subset the matrix to a single row (or column) you may be ultimately just be pulling the vector back.
&gt;drop = F I didn't know about this. So thank you, and yes that solves it but.... Why isn't this the default? This is the normal thing that I would expect. The other thing is goddamn weird, totally unexpected, and very frustrating to debug.
Cursing is not productive. You say "dim should be able to handle this", but dim has no opportunity to do anything about it in your example... it is a perfect mirror of what you did to the object you gave to it. You also say you understand about the difference between matrices and vectors, but you really are not demonstrating such understanding. Use the `attributes()` function to gain a better understanding of various S3 objects as you encounter them. It looks like the missing link for you is the `drop=FALSE` option in the `[` indexing operator. The original developers of R believed that automatically converting 1xN and Mx1 matrices into vectors would be more useful for interactive use than requiring explicit conversion in the vast majority of cases. I am not sure I agree with this, and Hadley Wickham created the `tibble` special data frame to address this asymmetry for data frames, but experienced R programmers sprinkle their slicing with this argument to restore this symmetry. attributes( M[1,] ) attributes( M[1,,drop=FALSE] )
See my edit. Added some context.
 A &lt;- 1 B &lt;- 1 C &lt;- 2 D &lt;- 3 E &lt;- 5 F &lt;- 8 str( M[ 1, , drop=F ] ) Good practices can prevent such errors. See 8.1.32 in the R Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/
Yes, dropping down from an N-dimensional to an (N-1)-dimensional array when one of the dimensions is 1 is annoying. Though also sometimes desired. It's not a problem with dim, it's a problem with the automatic dropping of dimensions on slices. Which, again, is sometimes desired. They made dropping the default behavior rather than keeping. Sometimes not even the "drop" argument is the right answer to this: I'm doing a fair bit of stuff with random matrices right now, and the natural way to store them is a 3D array and if you want one you pull it out. So, like, when you pull it out, you want to drop that third index and treat it as matrix. But what if it's a `1 x 5` matrix? D'oh! If you *really* need to make sure things are what you want them to be, the unfortunate answer is to always be extremely specific about your types. Usually you don't, but sometimes you do.
Better solution: don’t name variables F. Actually, unless it’s within a very limited function implementing some formula, don’t give variables single letter names, it makes your program hard to grep.... No actually that’s a good tip I’m just trying to protect my ego as a habitually lazy typer.