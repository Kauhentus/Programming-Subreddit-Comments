Well that's a thing
Yes, I too took intro stats and had homework. Did you?
It looks like you've just misspelled `filled.contour`, so it's looking for the object `filled.countour`, which isn't in the graphics namespace. 
My thoughts are that pie charts are awful, 3d pie charts are double awful, and you should do something else instead!
How about a violin plot with price on the y and category on the x? (Or even just a jittered scatter plot?) That way you'd get info on both price and relative frequency across categories while avoiding the double awfulness of 3d pie charts. Sorry, I don't know pie3d or I'd try to actually address your question too instead of just suggesting an alternative .
First... forget about using a 3D pie chart - seems entirely unassociate to what you want to solve (and 3D pie charts are bad mkay). Second, I’ve thought of doing something like this in ggplot2. Without having explored it much, you could build polygons up from the origin (looking like a single stacked bar) and vary the width of each polygon to the metric you want to show in that dimension; then use coord_polar(theta=‘y’) (I think) to get your pie chart, and the heights of each slice would vary like you want. I never went down this rabbit hole bc it seemed a little much for my use case.
How to make pie charts: 1. Raise right hand 2. Make a fist 3. Punch yourself in face 4. Still want to make pie charts? A. Yes, goto 3 B. No, make any other kind of chart How to make 3D pie chart: 1. Don't 
Would a two axis bar chart work? Especially since the point is, obstensively, to point out some relationship between your category and the associated metrics. If you post a sample of your data I'm sure people will be more constructive, but pie charts are like the Nickelback of this world. Not as bad as everyone says, but still pretty awful. Another viz would be more appropriate.
Oh, that's embarrassing! Thanks so much for spotting my mistake. I've spent a lot of time trying to figure out what the problem was!
To answer this question, we first need to know what data type x is. Type the following: str(x) This will tell you the data structure of this variable. Also, if you wanted to convert it into matrix format, you can do as follows: x=as.matrix(x) I would need to see more detail to know for sure, but if it's just a matter of converting x into matrix format, that's how you would do it.
Unfortunately I'm on vacation and I can't look up how exactly to solve this, but the basic problem will be either that lda is not expanding x (you can look at its code to see what it does), or with the fact that your string is not a formula. You'll note that `Y~.` in the first call has no " around it, so putting them in strings won't work. So, in short, if lda doesn't accept a string rather than a formula as its first argument, you need to convert your strings to formulas. This might be tricky since formulas must have an evaluation context to make sense (otherwise `Y` doesn't mean anything). Then you might need to force lda to expand that variable, which will probably involve substitute or quote, or both, or a part of the new tidyeval framework might be able to do it. That's as much as I have right now, sorry!
We've all done it - it's only a stupid mistake if you make it twice! 
as.formula(x) in place of x 
You mean like writeLines()?
Needs white text with a black outline to be readable.
Interesting. A quick GitHub search gives me another R package which was designed for meme generation as well. But I guess they never actually submitted to CRAN https://github.com/leeper/meme. It even highlights text with a black outline as /u/MagisterHerodotus suggested. The only downside is that it hasn't been updated in over 2 years.
seems like a high cost/benefit ratio
It would be a lot easier to help you if you provided: (a) The purpose or goal of your issue (b) Example data (c) Example code (with all arguments used) What function are you using, merge()? Have you tried the argument all.x=TRUE? Why are you extracting LocalDate from "x" and then joining back on "x", to add the variables year and month to the data frame? If you need yearly and monthly breakdowns of the ArithmeticMean, why not directly add two new columns to the "x" data frame rather than attempting a join e.g. if "LocalDate" is in the format YYYY-MM-DD ("2017-10-31"), using the stringr package: ``` x$year &lt;- stringr::str_sub(x$LocalDate, 1,4) # returns "2017" ``` and ``` x$month &lt;- stringr::str_sub(x$LocalDate, 6, 7) # returns "10" ``` Then you can do some math per month by grouping (probably using dplyr or plyr). 
write("blah string", file = "text_file.txt")
There's a million ways to do it, which can be daunting when you're starting out! Many people like the sf (simple features) format for handling Spatial data... I am not one of them. I like sp... its much more like typical GIS data structures so it's more familiar to me. Try this sp way: require(rgdal) dat &lt;- readOGR("~/path/to", "states.shp") my_colors &lt;- ifelse(dat$name == "Oregon", "red", "white") plot(dat, cols = my_colors, main = "Oregon is the Best State") That'll get you started. The code should work, but I'm on mobile so... fingers crossed.
That. And also uses the wrong font. Should be impact
It’s hard to believe that you actually tried researching this. If I google your question title, plus “R”, the first few hits all contain the answer to your question. At any rate, all of the usual output functions (`cat`, `writeLines`, `write`, etc.) contain a `file` or `con` (for “connection”) argument which you can set to a filename or a `file` object.
no
Just gonna mark this as an exact duplicate of https://stackoverflow.com/questions/47034146/geom-smoothmethod-loess-is-not-working-argument-trace-hat-is-missing
Yup it is, thought I would be more likely to reach someone who knows what is the matter by posting to the two places.
look at sorted_data[c(1340,1342),]
There are a couple of things. For one, the Helper vector has 10 elements, while the other 2 have 9. Data frames need to be rectangular. If I eliminate the last element of the Helper vector and do the following: Date &lt;-c('3/13/2017 6:21', '3/20/2017 6:28','3/13/2017 6:22','3/20/2017 6:28',' 3/13/2017 6:23','3/20/2017 6:28','3/13/2017 6:24',' 3/20/2017 6:28', ' 3/24/2017 6:28') Enabled_value&lt;-c(0,1,0,1,0,1,0,1,0) Helper&lt;-c('39RTU1','39RTU1','39RTU2','39RTU2','39RTU2','39RTU3','39RTU3','39RTU4','39RTU4') df &lt;- tibble(Date, Enabled_value, Helper) You'll need to add an id variable to the mix to avoid the issue of duplicate rows: df &lt;- mutate(df, id = row_number()) spread(df, Enabled_value, Helper) # A tibble: 9 x 4 Date id `0` `1` * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 3/13/2017 6:23 5 39RTU2 &lt;NA&gt; 2 3/20/2017 6:28 8 &lt;NA&gt; 39RTU4 3 3/24/2017 6:28 9 39RTU4 &lt;NA&gt; 4 3/13/2017 6:21 1 39RTU1 &lt;NA&gt; 5 3/13/2017 6:22 3 39RTU2 &lt;NA&gt; 6 3/13/2017 6:24 7 39RTU3 &lt;NA&gt; 7 3/20/2017 6:28 2 &lt;NA&gt; 39RTU1 8 3/20/2017 6:28 4 &lt;NA&gt; 39RTU2 9 3/20/2017 6:28 6 &lt;NA&gt; 39RTU3
Query part of the database at a time, aka write a more specific query
Use a database cursor to fetch your query results in batches so it isn't all in memory at the same time. Look at the `fetch-methods` example in the RPostgreSQL docs.
Only subset what you need from SQL. You might not need every record, and SQL can do all sorts of group by aggregation. Not sure what your use case is, but something to consider.
Well I am using data from 2 different servers. I need to combine in some way. I was not able to query two different servers databases, so I decided to get them in r data frame and then use sqldf to do SQL like operation
Can you be a bit more concrete about what the data from the two servers is like and how you're trying to use/combine them?
As far as I can tell, you need more RAM on your machine, or you need to split the csv file up a bit. I think there are some libraries to handle large data sets but can't think of the names off the top of my head. 
This doesn't quite do what OP is asking, but it's close. library(dplyr) library(tibble) library(tidyr) df &lt;- tibble(Helper = Helper[-3], Date, Enabled_value) # Handle duplicates df &lt;- df %&gt;% group_by(Helper, Enabled_value) %&gt;% arrange(Date) %&gt;% mutate(id = 1:n()) %&gt;% ungroup() %&gt;% spread(key = Enabled_value, value = Date) df looks like this: &gt; df # A tibble: 5 x 4 Helper id `0` `1` * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 39RTU1 1 3/13/2017 6:21 3/20/2017 6:28 2 39RTU2 1 3/13/2017 6:22 3/20/2017 6:28 3 39RTU3 1 3/13/2017 6:23 3/20/2017 6:28 4 39RTU4 1 3/24/2017 6:28 3/20/2017 6:28 5 39RTU4 2 3/13/2017 6:24 &lt;NA&gt;
thanks. how could I split the csv file up? and would I still be able to graph it all on one plot?
I really doubt that a 48,650 x 2 numeric matrix is actually taking up 17.6 GB of RAM. This looks like a system issue. If your csv is truly that large then tell me otherwise. I can't check your system, so I'm going to have you fix your file. I suggest opening `temp_data.csv` in excel, copy-pasting the data in cells A1:B48651 into a new excel document, and saving it as a separate csv file. Try loading the new file and tell us what happens. 
Something is not right with your description or your data. The following code with simulated data runs fine on a 12 year laptop with 2 Gb RAM: N &lt;- 48650 temp_data &lt;- data.frame(tempc = runif(N, -30, 25), yr = sample(1990:2010, N, replace = TRUE)) write.table(temp_data, "temp_data.csv", sep=",", row.names=FALSE) data &lt;- read.csv("temp_data.csv", header = TRUE) library(dplyr) mutate(data, tempc = ifelse(abs(lag(tempc)-tempc)&gt;18, NA, tempc)) plot(data$tempc ~ data$yr, type = "l", main = "McMurdo Temperature", xlab = "Year", ylab = "Temperature (C)") Try running str(data) to verify the size and columns types of your data after you read it in. Also note that your mutate() code does nothing since you do not save the results back to data i.e. you should do data &lt;- mutate(data, tempc = ifelse(abs(lag(tempc)-tempc)&gt;18, NA, tempc))
just tried that- got the same error. earlier I also tried saving it as a txt file and using read.table to see if that would work, but it didn't either. It really does seem strange that it's giving me an error of 17.6Gb when there's no way it's that big. Using object.size(data) gives me a little under 40,000,000
THANK YOU! I figured out my problem by running str(data). Turns out the entire yr column was being read as factors. Turned them back to numeric and it's working now! Thanks so much! (And hah, thanks, I probably would have noticed that the mutate wasn't doing anything if I could get it to make a plot. silly of me though.)
Use fread from data.table. no factors, faster, smaller memory, etc. Please try.
You probably want to save the parsed data as. Rda in the /data directory of your package, or is there any reason why they should be text files? 
&gt; You probably want to save the parsed data as. Rda in the /data directory of your package Wouldn't I have to merge all of the text files? 
No. I'm on my phone so I can't provide any links but there is a good book on package development by hadley (available online for free). I'm sure your questions are answered there. 
I don't understand then. Each text file needs to be separately saved as an .Rda? Sorry, I'm still a bit confused. 
Should be. You can also save all in one rda, but don't do that, it's bad practice for packages. Really, read the package data chapter of Headleys book. If you still have questions then I can answer them. 
the `DBI` package has a fetch method as well. There are several tutorials including the package readme: [](https://cran.r-project.org/web/packages/DBI/README.html)
If you don't want to make .Rda files like /u/Hoelk said, you can put whatever you want to `inst/extdata` - when a package is installed to the library, everything from `inst/` is moved one level up, so instead of referring to `inst/extdata/`, you can access your files/directories with `system.file("extdata", "myFile.txt", package = "myPackage)`.
I usually put stuff in the 'inst' directory - I don't think accessing stuff as Rda files is super necessary for small personal packages or releases that aren't going to get a ton of exposure, and it's easier to organize and modify small files as csvs. The correct way to do it, as others have stated, is follow Hadley's book guide. TBH I've run into a lot of bugs doing it this way, though.
I have also tried using mle2, but i obtain the same error ('vmmin' is not finite). I'm working with the income distributions of a set of country, but the fitting does not produce results neither with Gamma or Weibull distributions (same error as before). I have also tried to use the method of moments to obtain the estimation of the parameters to use as starting value of mle and mle2, but nothing changed. What can I do??
&gt; I don't think accessing stuff as Rda files is super necessary for small personal packages or releases that aren't going to get a ton of exposure, and it's easier to organize and modify small files as csvs. What is the motivation for using Rda files instead of csv files? Is there a reason?
`devtools::use_data(my_dataframe)` This will create an .Rda along with the appropriate folder structure. You will also need to document the data set. See the link. http://r-pkgs.had.co.nz/data.html
What is the motivation for using Rda files instead of csv files? Is there a reason?
They're compressed. CRAN only allows a certain amount of in-package data, so it just became best-practice to compress everything. I don't think there's a performance increase.
I see. I might as well used gzipped files then I suppose, or rds files.
Others can expand, but R is a [language and environment for statistical computing and graphics](https://www.r-project.org/about.html). It is not in it self going to be able to make PDF files for you without some serious tooling on your part. You could perhaps use R to auto-generate questions and answers, but **ultimately you will need to use LaTeX to template your document**. [Here](http://eosrei.net/articles/2015/11/latex-templates-python-and-jinja2-generate-pdfs) is one solution that uses Python and the Jinga2 template engine to generate PDFs with a pre-defined LaTeX template. There might be similar options with R, but again you're still using LaTeX.
When is the appropriate time to load the data? If this is in a particular function, I may be re-loading this data each time, which is inefficient. 
When you say "template your document" do you mean deciding where on the page information should go to create the PDF?
Also, when is the appropriate time to load the data, i.e. `devtools::use_data(my_dataframe)`? If this is in a particular function, I may be re-loading this data each time, which is inefficient. 
Also, could R generate random numbers and simply place them on an exercise plot and a separate solution plot to create images I could directly print as worksheets?
Yeah you make your template in LaTeX but also specify locations in which you will use Python to fill in the information. Maybe R as well, I've personally only done this in Python and that is the example I linked to. 
Yes theoretically. I'm not sure how to do it in practice with R. But crucially you still have to solve your original problem of setting up your latex document so that it is formatted the way you wish.
No, the `devtools::use_data()` function is only used to create the dataset inside your package. After you do that one time, your data are part of your package and will be loaded whenever your package is loaded. After that, you can call the data by `data(my_dataset)` or alternately `my_package::my_dataset`. Read the link I posted, it's all in there.
It's faster, and harder to modify/screw with as an end-user. It's also a more formal system, which makes it more reliable and less idiosyncratic. Basically, I don't like to use it sometimes because I want to do the things the system is designed to avoid.
Couldn't I just make the plot (image) the entire page?
You're right you don't have to use LaTeX. If you can generate your entire graphic in R you can simply save it as a PDF. It just might be more difficult to get your text and shapes drawn and aligned just the way you wish in R.
I might not be thinking of something because it's 5 o clock here, but wouldn't you just: ````dtable &lt;- data.table(read.table("path/filename.txt",header=TRUE,sep="\t",check.names=FALSE)) ```` 
Sorry, there was a typo. It should be `empty_vector &lt;- append(empty_vector,strsplit(dtable[i,6],":")[[1]][2]`
Check out knitr, sweave, and/or R markdown. [for example](http://www.stat.cmu.edu/~cshalizi/rmarkdown/). You should be able to get everything you need using some combination of those. If these are for students and you're at a university, try asking around the stats department for help. I've had several professors that used R to make reports like this. Using R with Latex/R markdown is also nice because R can generate the numbers, do the math for the solutions, and output it.
R is vectorized, so there's no need to iterate over rows at all. Strsplit will work on the whole vector. Data.table also has tstrsplit, which transposes the result, which can be useful. Not 100% on the syntax here, but this should point you in the right direction. vec &lt;- unlist(tstrsplit(dtable$variable, ":")[2]) Or if you wanted multiple values assigned back into the table dtable[, c("new1", "new2") := tstrsplit(variable, ":")[1:2]]
It's completely up to you. Data stored in inst/ don't get automatically loaded into memory upon loading the package, so they don't clutter memory unnecessarily. However, if you want to load them at start and keep them in the global scope (which is a terrible idea), you can do it in a function named '.onLoad()', and put it somewhere in the R/ subdirectory, e.g., in R/zzz.R (but be sure to load them to the global environment!).
You have the right idea with creating categories. That should have been something basic enough for you to google on your own. But here's an example: library(ggplot2) df &lt;- data.frame(student = 1:10, score = 1:10, rank = 10:1) df$category &lt;- ifelse(test = df$rank &lt;= 3, yes = "top", no = "bottom") ggplot(data = df, aes(x = category, y = score, fill = category)) + geom_boxplot() Another way to make categories is to create a column and fill it with whatever: `df$example &lt;- NA` Then fill that column based on conditions: `df[df$rank &lt;= 3, "example"] &lt;- "top"` `df[df$rank &gt; 3, "example"] &lt;- "bottom"` Here `df$rank &lt;= 3` returns a vector of TRUE/FALSE values. You're telling R: "I want the rows in the `"example"` column where the condition `df$rank &lt;= 3` is true to change value to `"top"`. 
`?knitr`
Yes
Keep all your data in one data frame, make a column with the fontface you want to use for each value (in your example, depending on the species groups) and map fontface to an aesthetic: library(tidyverse) library(ggrepel) set.seed(1) dat &lt;- data_frame( x = runif(length(letters)), y = runif(length(letters)), letter = letters, typeface = sample(c("plain", "italic", "bold"), size = length(letters), replace = TRUE) ) ggplot(dat, aes(x = x, y = y)) + geom_point() + geom_text_repel(aes(label = letter, fontface = typeface)) [ Result.](https://i.imgur.com/yMreb2Z.png)
Thank you so much, this works flawlessly and it is such a simple solution! My graph is now all nice and neat.
I'd start by trimming the data to just what's relevant - is it just one freeze date per year? Or every time there's ice on the surface?
just first freeze date
I'd run some LM's with freeze date as response and just start throwing in predictors. Mean temp of 30 days before freeze, mean minimum, % days with max below freezing, etc.
A couple years ago, I saw an article on using Elixir to call a Python script to scrape websites concurrently. Elixir handled the parallelization and passing the arguments. Python handled all the work. I'm sure something like that would work. I can't find the article now. I'll keep looking.
I just googled this yesterday. Thanks for posting!
[Something like this](https://stackoverflow.com/questions/14290364/heatmap-with-values-ggplot2) -- See selected answer.
Look at the doparallel and foreach packages. You could put your rvest scrape function in a loop. I'm on my phone, so I can't link to an example, but you can just do a web search on those packages. I've used this for xml web scraping with great success.
Some questions: 1 - you don't seem to be using the lda function - did I miss it in the code you provide? 2 - given you say "status" has "output of 0 or 1", doesn't this mean it would be numeric and hence part of your X?
Some simple LDA code: resid.de is a matrix of gene expression data with rows being genes and columns being distinct samples. anno$Short.Diag is a vector of labels of diagnoses. l.de = lda(t(resid.de), anno$Short.Diag) p.de = predict(l.de, t(resid.de)) Where l.de is the LDA model and p.de is the predicted diagnoses LDA can also be run with leave-one-out cross validation (don't test on your training data, which I did above) cv.de = lda(t(resid.de), anno$Short.Diag, CV=TRUE) Really in the first example it should be more like l.de = lda(train.de, anno$Short.Diag) p.de = predict(l.de, test.de) With separate data for training and testing
You *should* be able to see the output from testthat if your functions fails, it sounds like something else if causing your test functions to fail rather than not passing your `expect_` functions. What is the output if you run `devtools::test()` locally within the top level of your package directory?
Good point. Here is the end I see: ══ Results ═════════════════════════════════════════════════════════════════════ Duration: 10.7 s OK: 896 Failed: 0 Warnings: 5 Skipped: 2 Warning message: `encoding` is deprecated; all files now assumed to be UTF-8 
Ok, I've been looking at the Warnings. One of the Warnings was thrown at `expect_error()`. That's a bit confusing, as there should be an error thrown... How do I tackle the "warning message" while allowing the error?
One nice library that you should check out is lubridate. It has functions ymd, mdy, etc. for simple parsing of these dates. In your case, you could apply the function mdy_hm to your format, and it works. E.g. &gt; mdy_hm("10/31/2017 9:15") [1] "2017-10-31 09:15:00 UTC"
Ok, I've fixed all of the warnings and empty tests from `devtools::test()` I still have this problem from Travis-CI: OK: 894 SKIPPED: 0 FAILED: 9 .... Error: testthat unit tests failed Execution halted * checking PDF version of manual ... WARNING LaTeX errors when creating PDF version. This typically indicates Rd problems. * checking PDF version of manual without hyperrefs or index ... ERROR Re-running with no redirection of stdout/stderr. * DONE Status: 3 ERRORs, 4 WARNINGs, 3 NOTEs
The base package answer is strptime(). That'll get you to a POSIXlt object, but plotting works better with POSIXct - 'lt' is a list and 'ct' is at heart a numeric that represents seconds since an origin, usually Jan 1 1970.
An update: using this as a guide, https://github.com/craigcitro/r-travis/wiki, I included after_failure: - ./travis-tool.sh dump_logs in order to investigate the log file. It appears that each of errors is due to unused argument (force = TRUE) unfortunately, it doesn't appear that `suppressWarnings()` takes care of this issue.
Ah, I remember this prompt. I think this was done at an R workshop I went to years ago. It's a fairly common practice problem for practicing with logistic models in R, so you could find complete solutions by googling, but if you want to work your way there, here's what I'd suggest: use the glm() function to make a generalized linear model of: (damage occurring) as a function of (temperature) use the predict() function on that model you fit to the data, at the temperature requested, to see what it predicts the damage index would be. If you do find yourself enjoying modelling and want to get more into depth (confidence intervals on your prediction, or if you wanted to predict damage as a function of multiple factors with mixed effects, there's plenty more fun to be had) 
Take a look at this article I wrote to help us help you: http://www.bioinformaticscareerguide.com/2017/09/how-to-ask-good-programming-question_20.html
My first thought would be to use lapply over A then call a custom function to check the condition. Something like myFunc&lt;-function(a){...} If nobody has a better solution I'm happy to work with you to solve this. Just PM me
You want to learn? Go to Max Kuhn's site for the caret package. Read it all, download the code, run the examples. http://topepo.github.io/caret/index.html 
 df1 &lt;- df1 %&gt;% filter(species %in% df2$species) should do it
Okay, this is not very elegant, but should do the trick. Write a for loop and pass your relevantlist[i] to whatever way of searching the list is adequate, e.g. grep. If the titles are exactly the same, you can use the == operator.
Look into a intersect, or a left/right merge/join. How to implement depends on how your data is stored but there should be many examples if you google the above terms.
https://shiny.rstudio.com/reference/shiny/latest/dateInput.html
My understanding was that is just another method of allowing the user to control the date input? And changing the date here will not effect the date according to the slider or date range input. Am I incorrect? 
Perfect, that was the magic word I was looking for! Thanks a lot.
So this is the base-R style. Your data frame can be subsetted by passing row-arguments and col-arguments. Since you care about rows, you need to only pass that. You can pass a Boolean/logical/TRUE-FALSE vector to the row argument to get what you want. So if what you want is df$var == abc Then it's Df[df$var == abc,] That expression was passed to the row argument. That's it.
can you give an example of what the output should be?
http://journals.sagepub.com/doi/full/10.3102/1076998616680587 start here. If you need access, I can try to help
Yeah, below is the relevant part of a script I use daily to pull SF data and then dump into a MSSQL DB (I use RODBC to dump the "results" data frame into a Workspace table, if you care about the details). Replace the YOUR_X_HERE parts with what's applicable to you, and customize the soqlQuery object to your liking. The part that tripped me up a bit was figuring out what the SF API calls certain custom fields, and the syntax surrounding that (e.g. Custom_Field_1__c), but you can use these links to point you in the right direction: [API Reference](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm) | [Standard Objects](https://developer.salesforce.com/docs/atlas.en-us.api.meta/api/sforce_api_objects_list.htm). Hope this helps! library(RForcecom) library(sqldf) username &lt;- "YOUR_LOGIN_EMAIL" password &lt;- "YOUR_PASSWORD!YOUR_API_KEY" instanceURL &lt;- "YOUR_SALESFORCE_INSTANCE_URL" apiVersion &lt;- "YOUR_API_VERSION" invisible(session &lt;- rforcecom.login(username, password, instanceURL, apiVersion)) # Initialize connection using parameters above, and hide output from console # Load results of a SOQL query into a data frame: soqlQuery &lt;- " SELECT x, y, z FROM SalesforceTable WHERE DateColumn = LAST_N_DAYS:45 AND DateColumn &lt;&gt; TODAY AND CustomData = FALSE " results &lt;- rforcecom.query(session, soqlQuery, queryAll=TRUE) # Execute SOQL query and dump results to a variable invisible(rforcecom.logout(session)) # Close connection, and hide output from console results &lt;- sqldf(' SELECT x, y, z FROM results GROUP BY Stage_1_Datestamp__c ORDER BY Stage_1_Datestamp__c ') # Do the grouping and ordering in R instead of SOQL for better performance 
Are you on Windows? It should work fine if you map the share point folder as a network drive - [here’s how](https://support.microsoft.com/en-us/help/2616712/how-to-configure-and-to-troubleshoot-mapped-network-drives-that-connec) 
I guess I'd run through a list of them in character form, throw some if-else into the loop to check the rules, then if it passes all the rules paste the digits together and convert to numeric. Were you thinking there might be something more elegant than that?
https://play.google.com/store/apps/details?id=xyz.dataqueen.app The app will help with simple data frame manipulations
I write to temporary files, see `tempfile()`. It ensures you are not cluttering your of your users directories, the OS ensures you have write permissions, and the files magically disappears.
Short of generating all combinations and removing the unwanted, how would you do it by hand? 
You can just put files to be read in folders. You should use temp files for files to be written, because tests shouldn't change anything. Or, you could use mocks. Set up your mocks to return the right results, and don't use the disk at all.
&gt; Or, you could use mocks. Set up your mocks to return the right results, and don't use the disk at all. Do you have an example how this is done in R?
https://rdrr.io/a/cran/testthat/man/with_mock.html
Ah, thanks for this!
Unless the rule sets are themselves structured and not general arbitrary rules like the kind you mentioned, you are stuck with generate and remove. Otherwise your generation rules we probably be more complex than your exclusion rules. Also, I hope you realise that with 25 items to choose from, even just looking at the worst case of 12 or 13 length sets, you are examining over 5 million combinations in each of those cases. Not out of the realm of feasibility but depending on how you set up you exclusions [i.e. no shortcircut logic, etc] it could be painfully slow.
You can download the whole package from github as a .tar.gz or .zip file, and then use install.packages() to install it from source. Then you can specify the libpath using the standard argument.
Awesome, I'll try that.
This is the correct answer - I'd probably also use which() instead of filter().
Thanks I will try this out
There was no warning message?
there was no warning message. Only message that came with the output was this: Correlation matrix not shown by default, as p = 13 &gt; 12. Use print(x, correlation=TRUE) or vcov(x) if you need it
So it turns out the biggest issue I was having was the salesforce account I was given. Apparently it can't generate an authenticity token... So I have to go and bitch at our partner company for giving me a neutered Salesforce account :P I'm going to take a closer look at the code in your response once my account has the right privileges. Thank you very much :) Also -- how come you don't just download your data directly into the R environment? Is it for sharing/distributing to other people?
I use them for any function that makes changes to resources. For example, I have a function db_query that does a bunch of configuration stuff that's required to connect to some internal databases via JDBC. But I don't want to affect the actual DB, and there's no guarantee that every test environment can even connect to it. So when I'm testing a function that queries the DB to grab some data relating to an email campaign and do some stuff to it, I mock the db_query function to just read a CSV file I pulled from the server myself and return that instead. Sorry I can't share the code, but I'm not the copyright holder :)
Ah, yes, the dreaded permissions vortex... my sympathies! As for why I've set it up this way - it's actually just part of a script intended to populate a MSSQL table, nothing too exciting or complicated. R runs against SF, collates data the way I want it (sales by day kinda thing), then dumps the cleaned and finalized output back to the table after nuking any existing rows that are in the date range SF returns. This way it can live alongside other data in the same DB in the format we want, which makes dashboarding, etc. much easier. I'm sure there's probably a better way to accomplish that, but it just so happened that RStudio was open and we were working on some other 3rd party integration stuff in R, so... :)
Missing data? Try `summary(complete.cases(dataset))`
I couldn't quite work it out-- I think I'll have to just buckle down and learn Python for this one. Thank you for the advice, though!
Nailed it. We tried finding NAs in the data other ways but somehow missed it. And the model ran with all rows in JMP. That was frustrating. But thank you for your help!
I tried that... but somehow broke R instead :( I downloaded the zip file and saved it in a folder. Then I used: &gt; install.packages("C://blahblah", repos = NULL, type = "source", lib = "M://blahblah") And it somehow broke R and my session had to be restarted.
Just a question: why do you use the `&lt;&lt;-` assignment-operator when creating yr function?
Hopefully you've learned about Logistic Regression in your class, else this will be a hard problem to setup. 1. You need to build a logistic model (hint: `glm(response ~ predictor, data = your_data, family = "binomial"`) 2. Logistic models require a binary response variable (i.e. there should only be two values in your vector, not 4 like in the data set) 3. Any model can be used to "predict" a value. In logistic regression, you literally get an equation with intercept and coefficients which means you can "plug-in" the numbers. In R this is almost universally using the `predict(model, newdata = stuff_i_want_to_predict_dataframe, type = "response")` function. newdata = the stuff you want to predict. `type = "response"` will return the probability of the response (much easier to interpret), rather than the link value. FYI I calculated a VERY HIGH probability of O ring damage at 31 degrees. 
By no means am I great with strings, I've only worked with them on one of my personal projects. That being said I attempted to help you out - Anything to help us out here on the reddit, less we be ripped apart by the wolves at stack overflow, haha. Here's a regex I tried testing, with puncuation, capital letters, lowercase letters, and digits. grep("[[:punct:]][A-Za-z1-9]", x, value = TRUE) You basically put them all together like that in the "", which is really easy if you aren't worried about their actual order in the string. Lower and Upper Case vector Lu &lt;- replicate(4, paste0(c(sample(Uppercase, 5, replace = TRUE), sample(Lowercase, 3, replace = TRUE)), collapse = "")) Lower, Upper, and Punctuation Lup &lt;- replicate(4, paste0(c(sample(letters, 2, replace = TRUE), sample(digits, 4, replace = TRUE), sample(punctuation, 2, replace = FALSE)), collapse = "")) Lower, Upper, Punctuation, Digits Lupd &lt;- replicate(4, (paste0(c(sample(letters, 2, replace = TRUE), sample(digits, 2, replace = TRUE), sample(punctuation, 2, replace = FALSE), sample(Uppercase, 2, replace = TRUE)), collapse = "")) Make a data.frame for good comparison string_data &lt;- data.frame(Lu, Lup, Lupd, stringsAsFactors = FALSE) Let's apply the regex search and see what pops up. matching &lt;- unlist(sapply(string_data, function(x) grep("[[:punct:]][A-Za-z1-9]", x, value = TRUE))) finally, you only want a certain length of characters. My example here isn't so great, my examples all have the same length of characters. but all you have to do is use *nchar()* matching[nchar(matching) &gt; 3 &amp; nchar(matching) &lt; 9] 
R isn't what you want for this. Use matlab, maple, or any other CAS.
wait a second. What if this is part of the bigger problem he is trying to solve? Does he have to go back to another language to solve it every time? I don't know how R would solve this but I would be surprised if it can't. 
[same problem here](https://www.reddit.com/r/Rlanguage/comments/7bkj35/inverse_of_a_function/?st=J9SCFY27&amp;sh=89e03607https://www.reddit.com/r/Rlanguage/comments/7bkj35/inverse_of_a_function/?st=J9SCFY27&amp;sh=89e03607) 
Also - wow, this is my first time dipped into Rshiny, far trickier than I expected!
This is great. An alternative I have found useful sometimes is the REBUS package. 
Then ya, use R, but he really didn't give any indication that's what it is.
Use the package lubridate. You need two functions because you're doing two things. Step one is to turn your strings into a date data type. The easiest way to do that in this case is with `lubridate::mdy()` which will parse your strings in month day year format. Then you need to get the year out of the new date objects that you have. For that, you use `lubridate::year`. &gt; lubridate::year(lubridate::mdy('9/15/2005')) [1] 2005
`typeof` is the internal storage type, it might be more informative to use `class` on your dates to find out exactly what they are. If they're Dates then you should be able to use `format(your_dates, "%Y")` and add this as a new column to your dataframe.
ah ok... ill try class in future. Though in this case the class is a factor, so I don't think I can use what you gave for the as.integer. Re plotting - I wanted to save these to a new variable in the data frame so that i could do things based on the year etc. thanks
with stringr (package) you can split the string by "/" and pull out the last entry with an apply function, but I will +1 the lubridate solution in case you'd like to do more with the date (which tends to be the case when you look at the data longer)
OK thanks I'll try that. I thought about writing a for loop, casting to string, grabbing the last 4 chars, setting up a new array, but it felt like a bit of a mess. 
it would be: (say your vector is called "dates") sapply(strsplit(dates,"/"), "[[",3) str_split is stringr, strsplit is Base R
oh ok i'll try that, thanks 
this is the best way
Excel
I do have administrative problems with rattle, so limits me a bit. There have to be others though. 
So you want people do do your homework for you? At least show us your attempt that isn't working, or explain which bit you are stuck on.
Why you being mean? I showed the whole first part, creating the table. I don't know what to do with the second part. Which is why I asked the question. 
I'm not being mean, I just don't want to supply an answer to what definitely looks like some sort of assignment without you learning anything. Which bit don't you understand? Creating a function? Applying a function to each element of a vector?
So this is what I did: one &lt;- function() { qt((1 - alphavec), nuvec) } I can call the function, but I'd have to start a new line. And if I try to continue it and use a return function, nothing gets returned. The reason I didn't post any of this is that I'm not even a little bit sure if I'm starting off the right way. 
So you can pass the entire `alphavec` function to the `qt()` function, but it's not doing what you think, it's recycling the 7-element long `alphavec` and returning a single 39-element long vector. So there are a few ways to do `qt(1 - i, nuvec)` for each `i` in `alphavec`. So to avoid telling you the answer, in a completely unrelated problem. I have a vector of 4 values: my_means &lt;- c(1, 3, 10, 30) And I want to create a 4 normal distributions, each one having a mean from values in `my_means`. I could do: lapply(my_means, function(x) rnorm(100, x)) Which creates a list, which is easy enough to convert to a dataframe with a bit of googling.
How about one &lt;- (function(x, y) {qt(1 - x, y)})(alphavec, nuvec) 
That's what I've been doing, it returns only 39 elements. 
Ah, now I understand your problem! one &lt;- sapply(alphavec, function(x) {qt(1 - x, nuvec)}) 
ok, so why did you put 1 - x instead of 1-alphavec? Because I've been doing 1 - alphavec and obviously it's wrong; I guess my issue is with writing a function. 
Because you want to be able to use `sapply` (I recommend reading the documentation). In short, `sapply(x, f)`does about the following (in pseudocode): for i in size(x): y[i] &lt;- f(x[I]) return y Plus "simplifies" the result, ie instead of returning a list, returns a matrix when possible. 
Ohh it makes a lot of sense when it's written like that. I will definitely read over the documentation. Thank you! 
I get that you're trying to just not give out answers. Thank you for your help! 
[removed]
scale_x_continuous(breaks=c([insert desired tick marks on x axis]). Or use the scale_x_discrete version for your data, I think both will work. Breaks will store the vector of tick mark values you want to display on the graph.
When looking at the data sets structure, is the x axis being properly recognized?
This is what I would do. If you don't want to hard code the ticks, i would do something like scale_x_continuous(breaks=c(df$x %&gt;% cut(num_breaks))
 1999 1999 1999 2000 2000 2000 2001 2001 2001 and don't want repetition so these are 1999 2000 2001 basically... The graph is representing the number of a different variable for each year 1999 2 1999 5 1999 8 2000 4 2000 2 2000 8 2001 7 2001 0 2001 4 From that I would have the sum of blah in 1999 is 15, and that's what I've wanted to plot. And as you can see the plot() did this automatically, but the ggplot didn't i'll try your suggestion, but as I have about 103 years ( over 5500 ish entries ) it might be quite a lot of typing
var &lt;- null fix(var) or var &lt;- matrix(dim1, dim2) fix(var) 
If you want to plot the sum, you'll have to calculate the sum for each year and plot it. On the other hand, if you're saying that ggplot is plotting 5500 entries even though there are only 103 years, then you want to convert the year column into a numeric or factor data type.
The problem is none of the information you posted just now is included in your original post. I see in your post history that you made a thread asking people to help you create a reproducible code example. It's good practice to include a minimal reproducible example in your question as a courtesy to the people helping you out. Because it can quickly become frustrating trying to help people who do not include the adequate and necessary information to solve their question (they keep tacking on conditions and problems even once their original question has been answered). In the case of your question (and most questions really) it would be helpful if you 1. described your data and posted a couple of rows of it to illustrate how it looks. 2. Included the type of the relevant columns (are they factors, POSIX (time), numeric?). It could be relevant in helping us help you figure out why ggplot includes every single year as a tick mark on the x-axis. 3. Include your call to create the plot. 4. Ideally you create a minimally reproducible example, using a small subset of your data. In that you include the information from the previous 3 steps. But even if you do not create a minimally reproducible example, at least do the first three. People here are trying to help. The least you can do is value the time spent by those trying to help and eliminate the chances for a misunderstanding. &gt;From that I would have the sum of blah in 1999 is 15, and that's what I've wanted to plot. &gt; And as you can see the plot() did this automatically, but the ggplot didn't &gt; i'll try your suggestion, but as I have about 103 years ( over 5500 ish entries ) it might be quite a lot of typing We cannot se that plot() did this automatically because you didn't describe your data in your original post. Also as far as we can see the plots are identical on the y-scale. It gives us no reason to believe ggplot plotted 5500 entries. Give complete information when asking a question!
Ah, I see what you are saying now. The issue is that the date labels on the x axis are too wide and the date breaks too narrow so the labels are printing on top of each other and overlapping. The solution is scale_x_date() instead of scale_x_continuous(). Try something like scale_x_date(date_breaks = “10 years”, date_minor_breaks = “1 year”, labels = “‘%y”). Good luck!
I should note that this is only true if your x-axis is a date format. If it is an integer, you can mutate it into a date, or use one of the other solutions mentioned above like scale_x_continuous(labels = seq(1900, 2010, by = 10))
&gt; If you want to plot the sum, you'll have to calculate the sum for each year and &gt; plot it. Yes - I have created a data frame which has two vectors. * 1) the sum of Apples for each each year * 2) the corresponding year &gt; On the other hand, if you're saying that ggplot is plotting 5500 entries even &gt; though there are only 103 years, then you want to convert the year column into &gt; a numeric or factor data type. I don't think that is the problem (I'm on &gt; mobile, so can't really tell so well). You're correct, ggplot is just plotting every value from the years vector, it's not plotting 5500, but 103. This is still enough for it to be much too dense though. &gt; Also, you don't have to type it, the vector stored in breaks will be of type &gt; BreaksVector = seq(from= StartYear, to=EndYear, by=10). Look up seq for details I'm OK with seq, I'll try this. Thanks 
 &gt; The problem is none of the information you posted just now is included in your original post. Sorry about that - I'm still a bit unsure of the vocab and erred on the side of making the post more brief rather than a rambling mess as I thought it might be easier `&gt;.&lt;` &gt; I see in your post history that you made a thread asking people to help you create a reproducible code example. It's good practice to include a minimal reproducible example in your question as a courtesy to the people helping you out. Because it can quickly become frustrating trying to help people who do not include the adequate and necessary information to solve their question (they keep tacking on conditions and problems even once their original question has been answered). Yes I agree - I honestly thought that this would be answerable with what I gave (like, "*oh set ticks=sensible*", or something), but making a minimal working example is something I should just do out of habit and manners! &gt; In the case of your question (and most questions really)... I completely agree with all of these points. Sorry if it came across as lazy or asking too much, it wasn't my intention. 
To me it looks like your date variable is a character vector (~ a list of strings) which makes ggplot interpret it as a factor. What you want is to 1. convert the date variable to either numeric or a proper date time object (the package `lubridate` could help), and then 2. you can format the x-axis as you'd like with scale_x_continuous or scale_x_datetime
thanks - i'm going to make a proper example that has reproducible code, i underestimate how vague i was being
Thanks - [I've made a post with some code here](https://www.reddit.com/r/Rlanguage/comments/7c8f1v/adjusting_the_tick_values_in_ggplot_sensibly/?st=j9vdm14q&amp;sh=9e9fa3f9), hopefully that's more clear... I didn't mean to be so vague with this one. 
thanks - i had a play and I've made a post with mwe / code etc [here](https://www.reddit.com/r/Rlanguage/comments/7c8f1v/adjusting_the_tick_values_in_ggplot_sensibly/?st=j9vdm14q&amp;sh=9e9fa3f9). . hopefully that's clearer !
i tried to use the seq and such, and got some results... but couldn't seem to get the ticks to behave sensibly. i made a post [here](https://www.reddit.com/r/Rlanguage/comments/7c8f1v/adjusting_the_tick_values_in_ggplot_sensibly/?st=j9vdm14q&amp;sh=9e9fa3f9). with some code though... i hope that's more clear than this one was
i'm not familiar with this percentage syntax :S i created a new post with code [here](https://www.reddit.com/r/Rlanguage/comments/7c8f1v/adjusting_the_tick_values_in_ggplot_sensibly/?st=j9vdm14q&amp;sh=9e9fa3f9). though, hopefully that's better explained
You just want scale_x_continuous rather than scale_x_discrete and p &lt;- ggplot(aboardYears, aes(yearLevels, aboardYearTotal)) breakV &lt;- seq(1910,1990,by=20) p + geom_point(aes(size = aboardYearTotal))+ scale_x_discrete(breaks=breakV) Will work.
 library(tidyverse) library(lubridate) #base plot, the biggest concern is that this plot gives the impression that each tick is an equal-sized time window plot(aboardYears) # Replication of base plot, still not recommended due to unequal time windows aboardYears %&gt;% ggplot(aes(yearLevels, aboardYearTotal)) + # use same point style as base plot geom_point(shape = 95, size = 10) + scale_y_continuous(breaks = c(50,100,150)) + # feed vector of values for breaks, use every other value as base plot does scale_x_discrete(breaks = (aboardYears$yearLevels[seq(1, length(aboardYears$yearLevels), 2)])) + # remove panel elements theme_bw() + theme(panel.grid = element_blank()) # If trying to show change and time is important, convert to date as such aboardYears %&gt;% # convert factor of year levels into a date mutate(yearLevels = ymd(paste0(yearLevels, "-01-01"))) %&gt;% ggplot(aes(yearLevels, aboardYearTotal)) + geom_line(linetype = "dotted") + geom_point(shape = 95, size = 10) # if each year is really meant to be a factor, use a barplot like this aboardYears %&gt;% ggplot(aes(x = yearLevels, y = aboardYearTotal)) + #arranged by year geom_bar(stat = "identity") # if using a barplot, consider rearranging so its even more obvious that each year is discontinuous aboardYears %&gt;% ggplot(aes(x =reorder(yearLevels, -aboardYearTotal), y = aboardYearTotal)) + #arranged by total for year geom_bar(stat = "identity") + scale_x_discrete(name = "Levels in each year, arranged by total") 
Just a note that the behavior of the base plot function is concerning in this situation. The distance between year levels is not equal, so when plot() prints every other label (or hides any label to keep the axis clean) it becomes very unclear that each point is not an observation of equal distance from the next). Since tukeysbinges factored the yearLevels, scale_x_continuous will not work unless s/he converts the year levels back to numeric (which will then plot, as expected with large gaps where there are gaps between observations). If it is very important to keep the years as factors, then breaking the order by arranging by totals or something like that will make it more clear.
https://www.reddit.com/r/Rlanguage/comments/7c8f1v/adjusting_the_tick_values_in_ggplot_sensibly/dpnye8m/
For example, or(one_or_more(PUNCT), one_or_more(ALNUM)) has a regex output of &lt;regex&gt; (?:[[:punct:]]+|[[:alnum:]]+) 
As others have pointed out: If the values on your x-axis are not evenly spaced, it could be misleading to use `scale_x_discrete`. It will make it look like the time passed between 1943 and 1955 is the same as the time passed between 1989 and 1990. So change your yearLevels into either a numeric or a date format as suggested by /u/infrequentaccismus . Another tip: There's a function called `aggregate` in base R that helps you sum a variable grouped by a different variable. df_sum &lt;- aggregate(Aboard ~ Years, FUN = sum, data = df) sapply(df_sum, class) # Years is not factor, it is numeric ggplot(df_sum, aes(Years, Aboard)) + geom_point(aes(size = Aboard)) + scale_x_continuous(breaks=breakV), labels=as.character(breakV)) So `Aboard ~ Years` tells the function I want some function applied on the variable `Aboard`, and it should be applied grouped by the variable `Years`. `FUN` specifies the function I want applied, i.e. I want the `Aboard` values to be summed up grouped by `Years`. There's also a package called `dplyr` that's commonly used. For example by /u/infrequentaccismus . So when you see the pipe operator `%&gt;%`, it usually has to do with that family of packages. The pipe means that the result of whatever is in front of the pipe gets passed as the first argument to the function in front. df %&gt;% ggplot(aes(x = Years)) is the same thing as `ggplot(df, aes(x = Years))`. I know from personal experience it can be difficult to understand what people who are helping you are doing when they use a package you're not familiar with. So don't hesitate to ask for clarification.
 library(rebus) library(stringr) dt &lt;- data.frame(string_of_interest = c("HelloWorld!", "hi", "HelloWorld!HelloWorld!HelloWorld!", "HelloWorld", "helloworld!", "HELLOWORLD!")) lower_limit &lt;- 8 upper_limit &lt;- 20 dt %&gt;% filter(str_length(string_of_interest) &gt;= lower_limit &amp; str_length(string_of_interest) &lt;= upper_limit &amp; str_detect(string_of_interest, one_or_more(PUNCT)) &amp; str_detect(string_of_interest, one_or_more(char_class(ASCII_UPPER))) &amp; str_detect(string_of_interest, one_or_more(char_class(ASCII_LOWER))) )
Excellent points! Thanks! To add to your comment, and since I didnt comment at all about how OP aggregated the totals, here is a simple dplyr way to build the aggregated data frame: df %&gt;% group_by(Years) %&gt;% summarise(Aboard_sum = sum(Aboard)) You can use a dplyr pipeline to immediately plot the aggregation without saving an intermediate table: df %&gt;% group_by(Years) %&gt;% summarise(Aboard_sum = sum(Aboard)) %&gt;% mutate(yearLevels = ymd(paste0(yearLevels, "-01-01"))) %&gt;% ggplot(aes(yearLevels, aboardYearTotal)) + geom_line(linetype = "dotted") + geom_point(shape = 95, size = 10) 
thanks a lot - I'm struggling with all this `%` stuff at the moment tbh though :/ I'll have a look at other approaches and see if i can avoid this for now. Going on the other comment it just seems like a bit of syntax sugar or something? thanks though
Thanks, but this doesn't seem to work So going through that df_sum &lt;- aggregate(Aboard ~ Years, FUN = sum, data = df) sapply(df_sum, class) # Years is not factor, it is numeric For me this previous line (with sapply) doesn't return numeric, it returns factor. Here's the output : &gt; sapply(df_sum, class) Years Aboard "factor" "integer" Continuing with the code ... breakV &lt;- seq(from = 1910, to = 1990, by = 20) ggplot(df_sum, aes(Years, Aboard)) + geom_point(aes(size = Aboard)) + scale_x_continuous(breaks=breakV), labels=as.character(breakV)) I'm not sure what's going on with this section - but it's returning an error as is: Error: unexpected ')' in " labels = as.character(breakV))" I changed the code to the following : From : scale_x_continuous(breaks=breakV), labels=as.character(breakV)) to (removed the parenthesis next to breakV) : scale_x_continuous(breaks=breakV, labels=as.character(breakV)) Which returns an error about the discrete / continuous Error: Discrete value supplied to continuous scale Which feels like the original problem again :/ I'm getting no output 
You've used discrete and not continuous though? 
I'll just note that this isn't any simpler at all to someone who has never met dplyr 
Your years are factored so they are discrete. It is treating 1980, 1981, 1982 the same as yellow, purple, blue. There is no value or inherent ordering. As a result, you cannot give this discrete, factored year value to scale_x_continuous(). You must either coerce your years back to numeric, or use scale_x_discrete()
well I guess I'm asking people here what's best as I'm currently learning, I don't know what I *should* do in this situation. I tried to run the code provided by the OP and it didn't work for reasons I outlined. I don't know what the comment # Years is not factor, it is numeric Means in the context - as It seems that ( as you're saying ) years *is* a factor? cheers
I use train() function from the *caret* package. *caret* contains plenty of functions related to machine learning. 
Sorry I hadn't updated your code. If you change your year to a numeric it'll work. The other comments may give you better advice though. p &lt;- ggplot(aboardYears, aes(as.numeric(yearLevels), aboardYearTotal)) breakV &lt;- seq(1910,1990,by=20) p + geom_point(aes(size = aboardYearTotal)) + scale_x_continuous(breaks=breakV)
I don’t see that comment in the op’s code. Which user posted that comment in their code? In the op’s code, yearLevels was explicitly coded as a factor, which makes it I possible to do math the year (for example, to see whether there is a different distance between each year level)
&gt; it is numeric https://www.reddit.com/r/Rlanguage/comments/7c8f1v/adjusting_the_tick_values_in_ggplot_sensibly/dpnzowu/ 
That's my mistake. I apologize. I didn't run ## change years to factor variable, so that I have levels to work with. df$Years &lt;- factor(df$Years) the above line in your code before I aggregated. I rushed through copying the data without checking to see that you made Years into a factor later. I should have been clearer in showing this. It was my turn to cause a misunderstanding now by not properly explaining how I copied your data :) So to clarify: I created the same `df` as you but I stopped at ` df &lt;- data.frame(Aboard, Years)`. Didn't make the Years column into a factor afterwards. That's why my Years column was numeric. 
 This is what the op wrote in the code: ## change years to factor variable, so that I have levels to work with. df$Years &lt;- factor(df$Years) As a result, it is a factor, which is discrete and not continuous. as.numeric(df$Years) would convert it back to continuous. 
did not know you could import data from excel, you could have just said that instead .... but whatever
An ugly byt effective way would subset the dataframe obtained lapply(..., function(x) { y-&gt; read.xlsx(x, skip=1) y-&gt; y[c(1:(nrow(y) -3), ] y})
Haha sorry dude, didn't mean to hurt your feelings. If you have a matrix of data I find it easiest to just input everything into excel and then read the file in R using read.csv, then if you need to make changes just adjust the excel file and then reimport the data. 
I will try it sooon :) Haven’t played with this in a while 
Just wondering: what is wrong with the solution in the stackoverflow discussion?
lul, i was half joking.... you can do it directly with readxl package
Geocoding with the Google maps api or data science toolkit is probably the easiest way. I would follow the advice in that stack overflow. 
Look up the gls function in nlme
yeah, looked up, didn't help.
how so? You fit your model with glm, your Omega is specified in the correlation argument to gls
It has been ages since I saw this but: if you want to do it a bit more manually but not fully manually (as in dealing with the matrix algebra yourself), I think you can decompose the Omega matrix (Cholesky?) and do a linear transformation X and Y and then use regular lm facilities. 
Hi. I don't know if this answers your question, but you may use the xPath library. Here is a sample code .. web scraping the list of p (paragraphs) with class "title" from the page "https://www.reddit.com/r/Rlanguage/" In order to use xPath you need to be familiar with HTML and CSS. You can run the code in RStudio or Terminal/CommandLine. library(XML) library(curl) library(data.table) RedditR = getURL("https://www.reddit.com/r/Rlanguage/", encoding = "UTF-8") RedditR.doc = htmlParse(RedditR , asText=TRUE, encoding = "UTF-8") RedditR &lt;- xpathSApply(RedditR.doc, '//p[@class="title"]', xmlValue, encoding="utf-8") RedditR &lt;- data.frame(RedditR) RedditR 
Thanks for your response! Originally I put up all the code, but I took it down when I realized the website doesn't allow for scraping. 
something is causing it to get converted to a dense format it seems. try: indcs = which(rownames(sparse.matrix.full) %in% important.terms) sparse.matrix.filtered &lt;- sparse.matrix.full[indcs, , drop=F] 
That worked! You've managed to cut my runtime of hours to seconds with this! I know it's an entirely different question, but would you also know how to calculate the row and column sums of a sparse matrix? I feel as though there should be a simple way that doesn't take a multi-hour for loop to do.
Have you tried apply () ?
Depending on how you are generating your decision trees you might be able to use [`rpart.plot`](https://cran.r-project.org/web/packages/rpart.plot/) (i.e. if you've used [`rpart`](https://cran.r-project.org/web/packages/rpart.plot/) to generate them).
&gt; Sorry I can't share the code, but I'm not the copyright holder :) That's understandable. Do you have any github examples (of open-source code!) of others doing this? 
Solution is in the edit.
Sorry if this is a stupid question but why is this way considered ugly. and what do the "..." mean in the first part of the lapply function? Thanks for the help.
btw this might not be true for sparse matricies (haven't used them), but: matrix.full[important.terms,] generally works, so I would assume it also works for sparse - no need to use %in% (the order of the rows will end up being different) PS the reason why your for loop didn't work is because `i` count's up to your original matrix size, while the matrix you are working with gets shorter, each time you remove a row you skip the next. A while loop works better in such a situation, but in general it is bad to do memory operations in a loop as you found out ^^^^
The ... means the same than ur code : list.files(pattern=« xlsx ») It s ugly because it s not optimal : it loads the whole content and it subsets a lot of data. We could - check if it s possible while loading the file (in the read.xlsx function or another similar one) - subset by removing the few unwanted lines (instead of selecting a lot). If you have very big files, (or if you are a purist) it could be interesting to test different approaches and benchmark the time it takes to run. You may save a fraction of seconds by excel file. But for 99% of cases, it won t make a difference
I wasn't able to get your code to work for some reason but I was able to get it to work with 2 lines of code: headcount.data.files &lt;- list.files(pattern = ".xlsx") headcount.df &lt;- lapply(headcount.data.files, function(x){y &lt;- read_xlsx(x,skip = 1)}) headcount.df &lt;- lapply(headcount.df, function(x) x[c(1:(nrow(x)-3)),])
Another question for you: I have a blank column in the report called date. is there a way I could use a part of the file name to fill in that column? the file names look like abcdef(date).xlsx and I would need to pull the date into that column
I think it's 'ts(data, $12)'
What is stored in oriUnique? A single value or a list? Also it would help to include the exact error message
Just spent the afternoon coding these with VBA. Thanks for the link.
I can't see any picture? 
Also can't see the picture. 
The problem is that omega is a covarianvce matrix, not a correlation matrix. gls - as far as I could see - only allowed me to input correlation matrix and not covariance matrix. I need to input covariance to correct for heteroskedasticity,
Library(tidyverse) rough_example &lt;- function(col) { df %&gt;% Select (col) %&gt;% Group_by (col) %&gt;% Summarize (freq = n()) } list_of_results &lt;- map(df, rough_example) This will return you a list of the counts in each column. Might be a couple things that need to be changed specific to your question because can't see your picture.
I won't do your homework for you and I don't know exactly what format your professor wants but I'll give you some very simple code to plot a quadratic that might point you in the right direction as you could do a similar method with more complex equations. #Creates a vector that stores values x &lt;- c(-2,-1,0,1,2) #Squaring vector x and calling it y y &lt;- x^2 #Plotting solution (dependent variable goes first) plot(y~x)
So are you just trying to solve those with R? R isn't really a "solver's" package like a programming language or a math language (like mathematica)
That's what I thought too. Is there any way to just represent this equations graphically with R and then I could analyze them once they're graphed? I really don't know what else I could do.
Do you have the scripts that your professor gave you? 
Yeah, I do. Can I pm them to you? :)
Oh please, keep the conversation here, I'm interested in graphical solutions in R too
Are they long? You can PM, but it'd be better if you posted them here... so that anyone looking later can benefit from them 
Ok, this is an example script he sent us for a parabole script: # parabole plot(c(-10,10),c(-10,10),type="n",xlab="x",ylab="y") abline(h=0,v=0) # curve(x^2-2*x-1,-10,10,lwd=2,add=TRUE) curve(x^2/24+x/12+1,-10,10,lwd=2,col="red",add=TRUE) curve(x^2*32/9+x*25/3-4,-10,10,lwd=2,col="blue",add=TRUE) # legend(1,-5,c("x^2-2*x-1","x^2/24+x/12+1","x^2*32/9+x*25/3-4"),lwd=2,col=c("black","red","blue")) # plot(c(-10,10),c(-10,10),type="n",xlab="x",ylab="y") abline(h=0,v=0) # curve(-(x^2)-8*x-18,-10,10,lwd=2,add=TRUE) curve(-(x^2)/24+x/12+1,-10,10,lwd=2,col="red",add=TRUE) curve(-(x^2)*32/9+x*160/3-192,-10,10,lwd=2,col="blue",add=TRUE) # legend(-10,10,c("-(x^2)-8*x-18","-(x^2)/24+x/12+1","-(x^2)*32/9+x*160/3-192"),lwd=2,col=c("black","red","blue")) # vertice plot(c(-20,20),c(-20,20),type="n",xlab="x",ylab="y") abline(h=0,v=0) # curve(x^2+8*x+18,-20,20,lwd=2,add=TRUE) curve(x^2-8*x+18,-20,20,lwd=2,col="green",add=TRUE) curve(x^2+5*x+18,-20,20,lwd=2,col="red",add=TRUE) curve(x^2-12*x+18,-20,20,lwd=2,col="blue",add=TRUE) # legend(-20,-2,c("x^2+8*x+18","x^2-8*x+18","x^2+5*x+18","x^2-12*x+18"),lwd=2,col=c("black","green","red","blue")) title("CAMBIA IL VERTICE") # cambia "c" plot(c(-4,4),c(-4,4),type="n",xlab="x",ylab="y") abline(h=0,v=0) # curve(x^2-3*x+2,-20,20,lwd=2,add=TRUE) curve(x^2-3*x,-20,20,lwd=2,col="red",add=TRUE) curve(x^2-3*x-2,-20,20,lwd=2,col="blue",add=TRUE) # legend(-4,4,c("x^2-3*x+2","x^2-3*x","x^2-3*x-2"),lwd=2,col=c("black","red","blue")) title("VARIA c: IL GRAFICO DELLA PARABOLA E' TRASLATO") # intersezioni con gli assi plot(c(-4,12),c(-2,8),type="n",xlab="x",ylab="y") abline(h=0,v=0) # curve(0.25*x^2-2*x+3,-20,20,lwd=2,add=TRUE) curve(0.25*x^2-2*x+4,-20,20,lwd=2,col="red",add=TRUE) curve(0.25*x^2-2*x+6,-20,20,lwd=2,col="blue",add=TRUE) # legend(1,8,c("0.25*x^2-2*x+3","0.25*x^2-2*x+4","0.25*x^2-2*x+6"),lwd=2,col=c("black","red","blue")) title("INTERSEZIONI CON GLI ASSI") # disequazioni curve(x^2-5*x+4,-1,5) abline(h=0,v=0) # x &lt;- seq(1,4,0.01) y &lt;- x^2 - 5*x + 4 lines(x,y,col="red",lwd=3) # text(1,0.5,"1") text(4,0.5,"4") 
I don't know if it's this easy, but I used: curve(1/x/(x+3),lwd=2,add=TRUE) and if you think of your x as your n, then we can see that as x -&gt; infinity, the y -&gt; 0, so it should converge? for the sum, we could use integrals, and some other tricks, but I also don't know where you are in terms of "math level", so I want to be sure you understand the next step. This graph is only to show the convergence of terms to 0
Oh, it actually is this easy. I don't know the correspondent math course in the rest of the world but it's analysis 1 course. I understand the concepts when I'm doing excercises and stuff. It's just that our professor doesn't have the time to teach us a programming language, so he just literally sent us those scripts and told us to get the solution by ourselves. 
That's one way to do it too... but that's not graphical...
I don't know what to say then... Is there any way I could solve graphically these functions then? https://gyazo.com/228ccde94a1b3deb65d9cfc1e6220094
Hi, I'm a bot that links Gyazo images directly to save bandwidth. Direct link: https://i.gyazo.com/228ccde94a1b3deb65d9cfc1e6220094.png Imgur mirror: https://i.imgur.com/KEy97YV.png ^^[Sourcev2](https://github.com/Ptomerty/GyazoBot) ^^| ^^[Why?](https://github.com/Ptomerty/GyazoBot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/u/derpherp128) ^^| ^^[leavemealone](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=ignoreme&amp;message=ignoreme)
Those are easier, you move everything to one side, and then look for when the graph crosses the x axis. ( when it's equal to 0 )
So, if you're still not using it, I'd recommend you to install RStudio. Some people don't like it but it was good for me when I started with R. You can "CTRL+ENTER" for running each line of your code, which make things a lot easier for beginners. Another point, are you familiar with any programming language? I mean, could you understand what's going on with your professors examples? As far as I can see it pretty much solves almost all the equations you post. If it's the problem I could give a short explanation for each line of the code.
Nope, I'm not familiar at all with any programming language. I don't understand what three quarters of these lines of codes do. I don't know what every single word means too (in the code I mean). Professor isn't even bothered to explain them in class which is just meh... If you can be really bothered to do that I'd be pretty grateful to you! For example right now I'm having an issue with this: &gt; plot(c(-10,10),c(-10,10),type="n",xlab="x",ylab="y") &gt; abline(h=0,v=0) &gt; curve (x^2 -2x-1+2x/x-1,lwd=2, add= TRUE) Errore: unexpected symbol in "curve (x^2-2x" &gt; How come? 
 plot(c(-10,10),c(-10,10),type="n",xlab="x",ylab="y") #creates a blank plot abline(h=0,v=0) # adds one or more straight lines through the current plot curve (x2 -2x-1+2x/x-1,lwd=2, add= TRUE) # Draws a curve corresponding to a function, in this case the function is not written properly, or like some may say not in the computer-way. In this particular case I think the problem is the way you wrote the formula, for a power function you use ^, * for multiplication, so the formula had to be writen like this: x^2 - 2*x - 1+ 2*x/x-1. If you're using RStudio you might find the "help" tab in the lower right side of the screen, or you may type ?command (i.e. ?plot) in the R terminal to check the help for a specific command.
See for outliers and then check out if certainly they are correct or not 
But how do I establish a pattern for when A or B are correct/incorrect?
How many records do you have?
Records having both system A and B ~ 65K Records having only system A ~ 1K Records having only system B ~ 9K All records ~ 75K 
First you have to set a range of values for a and then for b, try to make an scatter plot to see if you can classify somehow the observations and then use the boxplot to see which ones behave badly or weird. 
I think you are asking a statistical question instead of an R question. Visualizing seems to be a good thing to do - why not plot this data with a scatterplot (like suggested below) and identify the outliers? That is a good first step. Establishing an acceptable range for your expected values is another way to do it - are you aware of what the acceptable paramaters are for the car weights? If so then it's just a matter of simple subsetting to figure out which ones exceed that range. 
Is this not a basic statistics question? If there's existing population data on this question then you could perform the standard deviations and the T Tests, see which cars are outside of your acceptable range of error, no? First though, you can always visualize. With that data I'd see if there's a way to convert between A and B so you can process the information, using *mutate* and then I'd *gather* the relevant measurements so that you can easily plot them in a scatterplot with the *ggplot* command. Prety nice to get an initial look. Next I'd run regressions though. 
Do they have a common column/field? 
no
not sure I really get what you mean but rbind.fill from the plyr package may help? Alternatively you could put in a dummy column in each df with the same name and use merge(...., all=TRUE) 
In my particular case I would have 2 dataframes and merge them to look like [this](https://imgur.com/a/zw51P) This is just a small example, in reality it would be 48 intervals per day for YTD and YTD totals. But I'm interested in the generic case for data that might not be related, but need to be on the same sheet.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ZdxEHIZ.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Ah I get you. That's how I would do it anyway, with the dummy columns, if you figure out something better let me know!
It might just be easier to do that in excel itself
So.. you might want to look at the XLSX library. I don't know ALL of it's functions but worst case scenario is that you can put a dataset on separate Excel sheets. Best of luck! 
if it was just a one time thing then sure, but this report should be updated daily which is why I wanted to automate it. I can force it to do what I want, but was hoping there was a very simple function I was just not aware of. 
Ahhhh that makes sense. I'd look into the dplyr package. https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html It has great tools for this type of thing and there are a lot of resources on how to best use it. It's also quite computationally efficient
Why not just keep them separate but include both next to each other in the presentation, just like you'd do if you had two images?
Try this: dataFrame1 &lt;- data.frame(Date=c(1:10), Interval= rep(c(1,2), each=5), Calls=sample(1:100, 10)) dataFrame2 &lt;- data.frame(Date=c(1:3), Calls=sample(1:100, 3)) combineDataFrame &lt;- function(dataFrame1, dataFrame2, outputfile){ row_n1 &lt;- nrow(dataFrame1) row_n2 &lt;- nrow(dataFrame2) # create blank rows addLine &lt;- dataFrame2[1,] for(i in 1:ncol(addLine)){ addLine[,i] &lt;- "" } # add blank rows to second dataframe addLine &lt;- addLine[rep(1, (row_n1 - row_n2)),] dataFrame2 &lt;- rbind(dataFrame2, addLine) # combine dataframes output &lt;- cbind(dataFrame1, blank="", dataFrame2) colnames(output) &lt;- gsub("blank", "", colnames(output)) write.csv(output, file=outputfile, row.names = F) } combineDataFrame(dataFrame1, dataFrame2, "out.csv")
I don't have an answer, but it's an interesting question - can you give more details? 
if it loads results as you scroll then youll need to use Selenium. Make sure your scraping is in accordance to terms and conditions first.
No, it requires you to click to the next page, it’s just that the URL doesn’t change. 
[removed]
Depending on where you installed Cygwin and where your script is something [along the lines of](https://stackoverflow.com/questions/15736898/running-a-shell-script-through-cygwin-on-windows) system("c:\cygwin\bin/bash path/to/your/script.sh") [This answer](https://stackoverflow.com/questions/5068264/cant-call-cygwin-commands-from-r) might also be useful as it shows a way to start R from within Cygwin and call commands directly without having to explicitly include the path. StackOverflow is an invaluable resource for R questions/answers restrict your searches to those tagged with `R` by including `[r]` in your search term.
If that's happening it means that the 'new page' isn't actually a new page, but instead JavaScript is being used to change the content of the current page. Often these kinds of sites can actually often be quite nice to scrape. What you need to do is open developer tools in your browser and use it to identify the request that the JS is using to retrieve the next results. Pretty often you'll find it requests from a URL that returns nice JASON results.
Yo totally can relate. Here's an example of my code. If you have more questions feel free to reach out. # SERVER SIDE # This will let your UI know what dataframe to convert to a csv file and what to name it output$downloadbuttonforcsv = downloadHandler('name_you_want_to_give_the_file.csv', content = function(file) { ## INSERT DATAFRAME HERE ## I will use testdataframe as the example testdataframe write.table(testdataframe, file ,sep=",",row.names = F) }) ## UI SIDE # The first argumetn of downloadButton looks for an ouput to generate from Server # The second argument will be the text ON the actual download button fluidRow( downloadButton('downloadbuttonforcsv', 'Download your file by clicking here') )
See this post about 'splitting a variable': - https://stackoverflow.com/questions/6104836/splitting-a-continuous-variable-into-equal-sized-groups What you want to do is split the data in groups for X, and then split it again into groups for Z.
You can use substr or strsplit to split these after XX. Then count the number of characters in the second part, and then create a string with many zeros as 4 minus the #characters in the second part, then glue all. Something like this with dplyr - did not test it, not sure if it works. spat.dat %&lt;&gt;% mutate(latter = substr(5, nchar(ID), n.zeros = 4-nchar(latter), zeros.to.insert = paste0(rep("0", n.zeros), collapse=''), new.ID = paste0('42XX', zeros.to.insert, latter) ) Of course you can combine all to a single line not to get all those extra columns. 
you could compile ur package and call it with like .Call or whatever it's called. or you could pipe the commands to console but the best would probably be rcpp. It's not too much overhead and makes data conversions like much much much easier.
Ggplot can make binning and plotting really simple and easy. # create a dataframe with random values, normally distributed data.frame(x = rnorm(1000, 0, (17/12)/2), y = rnorm(1000, 2.5, 1)) %&gt;% ggplot(aes(x,y)) + #Build heatmap stat_bin2d(aes(fill=..count..), bins = 20) + #Draw rectangle for strike zone geom_rect(aes(xmin=-(17/12)/2,xmax=(17/12)/2,ymin=1.7,ymax=3.3), color = "red", fill = NA) + scale_y_continuous(name = element_blank(), limits = c(0,4), expand = c(0,0)) + scale_x_continuous(name = element_blank(), limits = c(-2,2), expand = c(0,0)) + coord_equal() + theme_minimal()
You can also use the information above in combination with this information: http://rpubs.com/himanshu004/baseball_pitch_charts install.packages('pitchRx') library(pitchRx) bdata &lt;- scrape(game.ids = "gid_2015_06_20_pitmlb_wasmlb_1") bdata$pitch %&gt;% as_tibble %&gt;% mutate(strike = str_detect(des, "Called Strike")) %&gt;% select(strike, x, y) %&gt;% ggplot(aes(x,y)) + #Build heatmap stat_bin2d(aes(fill=..count..), bins = 20) + #Draw rectangle for strike zone geom_rect(aes(xmin=100,xmax=150,ymin=150,ymax=200), color = "red", fill = NA) + coord_equal() + theme_minimal()
I see what this is trying to do, but when I try it, R does not recognize %&lt;&gt;% even after loading dplyr. I also need to figure out how to do this.
Try loading magrittr as well. If that does not work for some reason, you can use regular R: latter = substr(spat.dat$ID, 5, nchar(spat.dat$ID)), n.zeros = 4-nchar(latter), zeros.to.insert = unlist(Map(function(x) paste0(rep("0", x), collapse = ''), n.zeros)) spatdat$new.ID = paste0('42XX', zeros.to.insert, latter) Oh, also this, and previous answer with dplyr (fixed an error there too) assumes you added stringsAsFactors = FALSE to your dataframe call above. Otherwise, nchar does not work. 
This worked perfect. Just one last question. What if I have 42XX and 42XY. How can I get it to the first 4 characters from spatdat$ID? Instead of specifying as stringsAsFactors, I used latter = substr(spatdat$ID, 5, nchar(as.character(spatdat$ID))) 
Answered my own question by looking at substr more. latter = substr(spatdat$ID, 5, nchar(as.character(spatdat$ID))) first= substr(spatdat$ID,1,4) n.zeros = 4-nchar(latter) zeros.to.insert = unlist(Map(function(x) paste0(rep("0", x), collapse = ''), n.zeros)) spatdat$new.ID = paste0(first, zeros.to.insert, latter) Thanks a bunch!
There is an r package -&gt; V8, that's what you want. Will be easier than selenium/phantom JS. Good article recently on R-bloggers about it. 
I'd probably just use `sprintf` spatdat$ID %&gt;% sprintf("%0.4s%04s", ., substring(., 5))
Yeah this is a more elegant solution for sure. This one did not work on my ubuntu machine, and apparently does not work on windows machines either, here is the small modification to make it work: spatdat$new.ID &lt;- spatdat$ID %&gt;% sprintf("%0.4s%04d", ., as.numeric(substring(., 5))) (needs magrittr) 
Going to throw my hat in here: Assuming none of your entries are over 8 characters already: library(stringr) paste0(subtr(spatdat$ID,1,4),str_pad(substr(spatdat$ID,5,8),width=4,side = "left", pad = "0") 
Thanks! So that code gives me the distribution of all pitches - how would I get it where I get percentage of the times that bin is called a strike?
I got it to this: https://imgur.com/a/ryH6n But do you know how I would get that black line he has around the 50% squares?
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/2Y9fK1v.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dq1fxnt) 
If you want every value to be included... it isn't really random.no comments (yet) sorted by: best 
&gt;Not OK (there is no 2 in the following list) : You mean that there's no 0, right? &gt;Seems to work, but I'm not sure how to ensure that I get every value within the range If you want every value in the range, then the list isn't really random. I *think* what you want is the following. 'vec.1 &lt;- 0, 1, 1 ,2 , 3` `vec.1u &lt;- unique(vec.1)` `vec.2 &lt;- c( vec1u, sample(vec.1, 5 - length(vec.1u), replace=T)` 'vec.3 &lt;- unlist(vec.2)'
 sample(c(0,1,2,3), 5, replace=TRUE)
That’s what geom_rect() does. Just give it appropriate xmin, xmax, ymin, and ymax values. 
 sample(seq(0,3)) seq(0,3) creates a sequence from zero to three. sample() permutes the sequence randomly.
The length of this list isn't going to be 5.
I read this question as looking for all the permutations of that list that include each of the numbers 0:3 at least once. Here's how I would do that. library(gtools) al.perm &lt;- permutations(n=4,r=5,v=0:3,repeats.allowed=T) lut &lt;- function(x) length(unique(x)) perm.test &lt;- apply(al.perm,1,lut) perm.fit &lt;- al.perm[which(perm.test==4),] 
But it it's more of an ellipse than a rectangle.
Try `sample(x=0:3, size=5, replace=TRUE)` Look up `?sample` to understand how it works and what each argument is doing.
Oh, you want every value, and this won’t guarantee it. Never mind.
Ah, I see what you are referring to now. I believe that is a countour. I don’t have time now, but experiment with contours = TRUE and set a single contour at the threshold that you want. That should work. 
Thanks - I've tried to look for the contours = TRUE, but can't seem to figure out how to use the threshold - if you can find it - it would greatly appreciated!
I'm pretty sure this is what you're asking for: weird_list = function(maxval, n){ stopifnot(n&gt;=maxval+1) n_unq = maxval+1 counts = rep(1,n_unq) missing = n - sum(counts) if(missing&gt;0){ counts = counts + rmultinom(1, missing, rep(1/missing, n_unq)) } unlist(sapply(1:n_unq, function(i) rep(i-1, counts[i]) )) } 
I would look at this differently from a mathematical point of view. You want each number there at least once. That's known so start with a list of each number and use sample only for the left over spaces you want to fill. In a really basic step by step for your five digit problem: Start with a variable that already has 0,1,2,3 Sample a number from 0-3 Combine the two together Sort You can get that down to one line easy enough.
c(0:3, sample(0:3, size = 1))[order(runif(5))] I think this is what you need?
This is basically what my code is doing, but more efficiently than what you are suggesting because by sampling counts rather than values, we can skip sorting the output.
I'm on mobile too. U/Charudatta has a one line version of what I'm talking about above. I'd be very surprised if yours is faster.
You may have meant u/Charudatta instead of U/Charudatta. --------------------------------------- ^^^Remember, *^^OP ^^^may ^^^have ^^ninja-edited.* ^^I ^^correct ^^subreddit ^^^and ^^user ^^links ^^with ^^^a ^^capital ^^R ^^^or ^^U, ^^^which ^^are ^^^usually ^^unusable. ^^**-Srikar**
Its one line because it's for fixed parameters. It gives the user to figure out how many missing samples there are and provide that value explicitly. Additionally, they're randomly sorting the output, when the output was supposed to be sorted (by my understanding). Anyway, I'll play with this in a bit.
You are right, here is my suggestion. Other solutions in this thread seem to be more complicated. @disagreeabledinosaur has it right. sample( c( 0:3, sample(0:3, size=1) ) ) Since OP requires that each integer between 0 and 3 be included at least once, I include these explicitly with 0:3. Since the sequence must have 5 elements, one more is sampled randomly with sample(0:3, size=1). The outer sample() permutes the whole thing randomly.
test&lt;-function(minval, maxval, length) {sort(c(minval:maxval, sample(minval:maxval, size=(length+minval-maxval-1),replace=TRUE)))}
You need to provide more information... the data and a minimal example of the problem you are facing and what you did towards solving it.
I literally did nothing because I don't get it. I'm at the point of crying, lol. I can't really provide the dataset since its 300 subjects, but I can provide the questions. 
Dr. Black asked 300 students to choose between four characteristics of a partner that they find most important. The characteristics that they had to choose among were (dataset codes in parentheses): 1) physical attractiveness (attractive); 2) personality (personality); 3) intelligence (intelligence); or 4) socioeconomic status (socstat). The dataset (available on Moodle) is entitled ‘ass2_q2.csv’. Use α = .10 for the questions below. a) Use null hypothesis testing (all steps) to determine if physical attractiveness, personality, intelligence, and socioeconomic status are not equally important in selecting a romantic partner. Be sure to interpret the effect size (again, even if it is purely subjective) and include a general summary regarding the results. b) What Dr. Black was really interested in was whether physical attractiveness was more important than personality as a characteristic that students value in a partner. Using only students who chose one of these two options (attractiveness or personality), use R to generate a table of frequencies for each of the two options. Then, BY HAND, use null hypothesis testing (all steps) to determine if students were more likely to choose physical attractiveness over personality as their most important characteristic. Calculate the p-value and/or critical value using R. Be sure to (subjectively) interpret the effect size and include a general summary statement regarding your results. 
Ok, now edit your text on top to reflect the above, and provide a dropbox link to the csv... so others can help you..
do I have to make the dropbox link public? Or does it do that itself?
I think I got it? https://www.dropbox.com/s/053bnrwf31cgh64/The%20data.csv?dl=0
Ok, the first thing is that your file has repeated headings, which are not allowed to happen when you run read.csv. The command to first load the dataset should be: read.csv("https://www.dropbox.com/s/053bnrwf31cgh64/The%20data.csv?dl=0", header=T,fill=T) Run that in your RStudio and check the error that it spits out...
The solution to this problem can be found here: https://stackoverflow.com/questions/4066607/reading-a-csv-file-with-repeated-row-names-in-r#4066708
&gt; read.csv("https://www.dropbox.com/s/053bnrwf31cgh64/The%20data.csv?dl=0", header=T,fill=T) It says "Error in read.table(file = file, header = header, sep = sep, quote = quote, : duplicate 'row.names' are not allowed" 
Yep, that means two rows have the same name, you should open the csv in excel (or whatever) and change one of the repeated row.names... or give them a exclusive name as per the link.
I think I got it....I have no idea...
https://www.dropbox.com/s/qgu1v3ovkrxseox/The%20data%20-%20The%20data.csv?dl=0
nope, I didn't. just checked on R
This works for the specific example, but won't work on arbitrary vectors
It's something related to Dropbox, nevermind, do: mydata &lt;- read.csv("The data.csv") And it should work... Now inspect the data with `head(mydata)`, `summary(mydata)` You will see that the data looks ok.
Lol dude just relax and take the F. It's just one assignment, it won't kill your grade. Next time start earlier. 
wow...yes it will...it is 20% of my final grade. -.- I did start early. -.- there are more questions above man. I did those a week ago. I tried to get help on these second set of questions for a few days now and no one has come to help. The professor isn't giving anyone help. 
I got it!!! I had to add the document first to my files that R goes to first. I got in the head and summary so far.
So, for step one, I have to do the null hypothesis and the alternative. What would that be? 
I got Ho: There is no relationship between attractiveness, intelligence, personality, and socioexconomics to selecting a romantic partner H1: There is a relationship between attractiveness, intelligence, personality, and socioexconomics to selecting a romantic partner
The question is not clear, actually. To test hypothesis, one should first say which test statistic to run. He must have given you some clue during the classes...
well...we did normal hypothesis testing about a week and a half ago, and then we started last week goodness of fit and chi square 
Oh! yeah its 1. State your statistical hypotheses 2. Define how extreme your results must be to be statistically significant (i.e., define the critical region and decision rules) 3. Calculate the test statistic 4. Make a decision regarding statistical significance 5. Calculate and interpret the confidence interval (not sure about confidence interval if its chi square/goodness of fit...because you don't do confidence interval when you want to figure out that stuff right?) 6. Calculate and interpret an effect size
The following will help you with question (b): # 2-Way Cross Tabulation install.packages("gmodels") library(gmodels) CrossTable(mydata$preference) 
thank you! 
How am I to figure out the pvalue with R? 
In general it is super easy, but you first have to decide which test you will use.... so for example, for a simple regression you can use `lm()`. In case you need to find out how it works, run `?lm` or `??lm`... as soon as you pass the info, the function outputs the p-value for the test...
Oh! It says I can either calculate the p-value OR critical value? Do you know how I would do that through R? 
I don't really know what critical value is, I would totally go for a calculation of a p-value.
Name the list using the file names in headcount.data.files (passed through `basenames` if they have a path on the front), then use the `.id` argument of bind_rows to make those names a column in the data, then pull the date element out of the new column, using substr (or similar) if the character position is consistent, or regex if it isn't. Example: names(headcount.list) &lt;- headcount.data.files headcount.df &lt;- bind_rows(headcount.list, .id = "filename") headcount.df &lt;- mutate(headcount.df, date = substr(filename, 10, 15) I don't know the actual positions you need, and my tidyverse is a bit rusty since I would normally use `data.table::rbindlist` for this, but this should get you going.
how about acast and melt from package reshape2?
Have you accounted for what happens at the beginning/end of your loop? On your last goes i+1 won't exist. Surely you should also look backwards with the rolls rather then forwards.
Could you please elaborate more, I am confused. 
When i=n, die [i+3] doesn't exist.
How can I correct this, by that logic i+2 would not exist either correct? 
Correct. It's your homework though. Work it out yourself.
I have been attempting that for several hours. Could you can please provide more guidance toward the answer or at least provide some semblance of hope as to whether or not I am close. 
 repeat { x &lt;- sample(3, 5, replace = TRUE) if (all(sort(unique(x)) == 1:3)) break } This obviously works for other values of 3 and 5 :-)
system("foo.sh") But I hardly ever need to do that anymore. R has so much stuff built in that you can usually use R as your scripting language and do not need to call out to shell or other unix utilities. See for example, ?grep, ?files, and ?files2.
How do I do question 2 b?
your counting your i up from 1 to n, right? so do the vectors contain enough elements for n+? you are indexing? To debug it, honestly strip away all chest and jail stuff, run it and try to understand the error message if here is one, when that is gone and the results make sense, add in jail and then add in chests retesting every time.
Can't you just create a weight vector with weight 1 if it's not in the last 5 entries and 1.1 if it is in the last five entries? Ex with 15 total entries: weight &lt;- c(rep(1,10),rep(1.1,5)) 
That would work. Was thinking this person could also create 10x his dataset, and then add in one more entry for the 5 he wants to overweight. Just in case the weights part is confusing.
Then the standard errors would be wrong if you're duplicating data like that
Thank you for this. I have never worked with weights before so I was a little confused and just wanted to make sure I did t right. Thanks again 
Thanks for this. I’m going to try to use weights but I wouldn’t have thought about this. Thank you. 
Good catch
Check out the Fluid Grid System https://shiny.rstudio.com/articles/layout-guide.html
Thanks! will check it out
You can export/print shiny dashboards to HTML and PDF formats. And while I haven’t done any E/Access work myself, a quick Google search leads me to believe that it’s definitely possible https://stackoverflow.com/questions/13070706/how-to-connect-r-with-access-database-in-64-bit-window
I would really recommend trying to get a MySql database. Personally I prefer Excel over Access, but a real database would be even better. For reporting, check out PowerBI, it's free/cheap and will be better than Shiny. Shiny isn't really meant for what it sounds like you're trying to do. 
Yeah, those are all great suggestions, there are just cost/beaurocratic barriers. Even if I could set up my own MySQL database, I work w mental health data so I'm not confident in my ability to keep that info secure and within hippa regulations
What was in `my_text` to begin with?
It's a huge file. About 900,000 lines of blog posts.
Oh, I get it now. That's because you're misreading the table. Your copy-paste of the output doesn't seem to align with what `inspect()` is outputting. I've created a toy example: newline_text = "a bunch of words that I am using a bunch of times in a single line\nbut then sometimes there's new lines\nand i don't know what to do about them when i see a new line" cleaned_text = gsub("\n", " ", newline_text) newline_tdm = TermDocumentMatrix(VCorpus(VectorSource(newline_text))) cleaned_tdm = TermDocumentMatrix(VCorpus(VectorSource(cleaned_text))) And let's take a look at `inspect()`: &gt; inspect(cleaned_tdm) &lt;&lt;TermDocumentMatrix (terms: 21, documents: 1)&gt;&gt; Non-/sparse entries: 21/0 Sparsity : 0% Maximal term length: 9 Weighting : term frequency (tf) Sample : Docs Terms 1 about 1 and 1 bunch 2 but 1 don't 1 know 1 line 2 lines 1 new 2 see 1 Do you notice the difference between your's and mine? There's a `1` next to the word "Terms", and under the word "Docs". That's your clue. The number next to each term isn't *the number of documents it appeared in*, but rather **the number of times it appeared in a given document D**. The '1' at the top indicates it's Document #1. Let's use an example with more documents: &gt; data('crude') &gt; bigger = TermDocumentMatrix(VCorpus(VectorSource(crude))) &gt; bigger &lt;&lt;TermDocumentMatrix (terms: 1266, documents: 20)&gt;&gt; Non-/sparse entries: 2255/23065 Sparsity : 91% Maximal term length: 17 Weighting : term frequency (tf) &gt; inspect(bigger) &lt;&lt;TermDocumentMatrix (terms: 1266, documents: 20)&gt;&gt; Non-/sparse entries: 2255/23065 Sparsity : 91% Maximal term length: 17 Weighting : term frequency (tf) Sample : Docs Terms 10 11 16 17 19 2 6 7 8 9 and 6 5 5 6 5 9 7 11 3 9 for 2 4 4 5 3 5 4 3 1 6 its 2 0 2 2 1 6 8 3 0 3 mln 3 9 2 2 0 4 4 1 0 0 oil 9 5 4 4 3 11 7 3 3 4 opec 6 5 0 0 0 10 6 1 2 1 prices 7 4 2 2 2 3 2 0 1 0 said 5 5 2 2 3 9 6 0 3 4 that 2 0 1 1 3 10 4 1 0 2 the 27 21 8 13 21 17 15 30 6 18
&gt; The number next to each term isn't the number of documents it appeared in, but rather the number of times it appeared in a given document D. The '1' at the top indicates it's Document #1. Ahh, for some reason I thought it was supposed to be the number of the document that it appears in. I don't know if you've seen my update, but I've re-ran my code, this time also inspecting the entire matrix rather than the first ten rows. I now get the following: ## &lt;&lt;TermDocumentMatrix (terms: 179357, documents: 1)&gt;&gt; ## Non-/sparse entries: 179357/0 ## Sparsity : 0% ## Maximal term length: 572 ## Weighting : term frequency (tf) ## Sample : ## Docs ## Terms 1 ## and 57359 ## are 10320 ## for 19220 ## have 11936 ## that 24040 ## the 98106 ## this 12354 ## was 15170 ## with 15452 ## you 14680
&gt; Ahh, for some reason I thought it was supposed to be the number of the document that it appears in. That's probably because 1. R has *atrocious* documentation and 2. the name of the column is 'Docs', as if to say "# of Docs" If `?inspect` was actually helpful, or if the column was named `Doc ID` you'd probably have been fine.
All sounds true. Although I always dread having to dive into the documentation for anything, R or otherwise.
Makes sense. I just always recommend staying far away from Access. It's not a good database and it's hard to migrate to another better system later. In my opinion, you are probably better of using Excel until you can do it right.
Yeah, I don't like access either but we have such large data set on under powered computers that it's getting to the point where I literally can't do my job. They are buying me more RAM but I don't think that alone will make much of a difference. 
swirlstats.com
Sounds like you're over thinking the problem? Are you trying to find the mean of a data set?
Consider an introduction to computer science like CS50 (edx.org) It won t help you in R, but it will teach teach you how to think your program and how o structure your data
I’m not sure if that’s specifically OP’s problem (though it sure sounds like it), but as a very very novice coder I’ve found that one of my biggest problems is overthinking things. I once spent forty-five minutes writing some arcane solution I’d settled upon when the sensible answer was just inner_join. 
Well, to be fair, that question is worded strangely... But, just look and understand the data set first. To me, this means to find the average vote count between 2004 and 2008 for eachcl county in Georgie. It should occur to you that you want to filter out all observations from Georgia, then group by county, and then summarize to find the mean! Looking into learning the dplyr package.
Think step for step what you need to reach the thing yout want to code. Start with a fresh R sheet and write down the steps in comments (e.g. #IMPORT DATASET etc.), then fill up the lines between them with code. This helped me much in the beginning. 
You're overthinkimg it. Try producing the answer in Excel, using a pivot table. Once you know what the answer looks like, and what operations to get there, you can write the code.
Just be patient with yourself. Those things that were once difficult will become easy and you’ll find new difficulties but you’ll be able to do a lot more than you could previously. Do lots of tutorials, writing other people’s code, and you’ll slowly pick up better habits.
It sounds like you want to put the cart ahead of the horse. Learning the syntax and flow controls of a language is extremely important to be able answer the actual problems you intend to solve with a language. Practice with small questions and build concepts you do not fully understand. For example, right now I'm practicing making vectorized functions using the apply family. But I can't do that until I understand the flow of the apply functions. Don't let yourself get too far ahead of your knowledge. Answering a question like you posed takes time to develop the skills to answer correctly. 
You need to be able to put the code element into a larger context - what are you trying to use it for? A lot of people are unable to understand the code because they don't see the big picture of how to apply it. In this regard, understand the purpose behind why you are implementing a particular block of code, and the actual coding becomes a lot easier. Learn it in chunks by topic, as opposed to line by line.
I agree. This is a very straightforward dplyr problem.
I agree with your approach: try to write the problem in pseudo code first. I would write this is as: Use a dataframe of vote counts with a columns for year of the vote and county then Filter to keep rows where year &gt;= 2004 &amp; year &lt;= 2008 then Group the date by each county in the county column then Calculate the mean of the vote count column for each group I hope this helps! It’s helpful to think about each verb and what you want that verb to do to each column. 
Familiarity breeds confidence, you just have to understand what each 'thing' does and practice, and google similar examples when things get tough. I hated it to begin with but grew to love it once it stopped being such a battle.
yea using R is kinda like this: https://www.youtube.com/watch?v=rH48caFgZcI but you'll get the hang of it.. in this case, i would use which() to subset your data to get only counties that are in GA and that voted democratic and that voted in 2004 (i'm assuming your dataset has columns where these conditions are stated)....then take this and get the mean using mean()
Learning R is hard because it is a 30 something year old language, if we taking the S language into account. Do not try to use R as actual programming language, consider it as a highly scriptable tool for data. If you want something simplier in the same enviroment, take a look at R for Data Science book. This book tells you how to perform basic tasks and teaches you how to use one of the most popular packages for data manipulation - Tidyverse. I am using this almost every day at work.
Why? Genuinely curious.
I do not know the differences between the assigment operators in R, so the choise was based on what the open source code I was working with had, so I went with it. To follow your question with another question, what are the differences between the operators?
I appreciate this is not exactly what you have asked for, but if you read and go through the examples in this guide, I think you’ll be in a great position for your classes next smemester. https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf Good luck! :) 
check out code school or data camp
You're gonna make it. Slow and steady progress. I have no prior experience in programming languages but I've been steadily learning R for a year. Started out with the typical intro on data types and subsetting, and common functions. Dplyr was the most helpful thing I learned after that, then Tidyr, then ggplot2. If you can get those 3 packages down then most of your data analysis problems will disappear. You'll feel like you're flying. I couldn't code a for loop if my life depended on it until up to a month ago. Oddly enough I was a lot more comfortable wth the apply functions. So learn those packages first, for practicality sake. If you really wanna learn the loops go back and learn them later? 
I can't really write a proper explanation right now, but briefly: `&lt;&lt;-` is used to assign variables in the global environment. That means that if you assign a variable inside a function using the `&lt;&lt;-` operator, you will be able to access that variable outside of the function. In contrast, if you do the same using the regular assignment operator `&lt;-` (same as `=` in your code. don't ask.) the variable will only be accessible within the scope of your function. To illustrate, try running the following code: test_function_1 &lt;- function() { my_var &lt;- "Hello world" } test_function_1() print(my_var) test_function_2 &lt;- function() { my_global_var &lt;&lt;- "Hello world" } test_function_2() print(my_global_var) If you want to know more, read up on scoping in R.
http://swirlstats.com/ is nice
It more consistent for small problems. Might just be me, though!
As another interested party in learning R, thanks for the link!
Thank you for the detailed reply! It is clear to me why this was used then, the code is structured as a "big" function containing function/variable definition sets, which means you call the "big" function to initialise/instantiate eveything. I'm not aware if this is the correct idiom in R for organising your functions/code but personally I did not like it much because some variables were set in places away from where they were used, which made it a bit tricky to trace everything. In the defence of the original author it was a small piece of code so it wasn't that hard to take in the entire script and follow it, but I doupt if this style of writting scales up well. 
My answer would depend a bit on how you have your missing responses coded, either as NA (the R missing data symbol) or as a string (like "NA"). This finds the number of columns in each row that are NA or "NA" and compares it to the total number of columns. If they are all missing, it gets removed. m &lt;- data.frame(x=c(1, NA, 3, 4, 5), y=c(6, NA, 8, NA, 10), z=c(11, NA, 13, NA, NA)) m &lt;- m[rowSums(is.na(m))!=ncol(m),] Or: n &lt;- data.frame(x=c(1, "NA", 3, 4, 5), y=c(6, "NA", 8, "NA", 10), z=c(11, "NA", 13, "NA", "NA")) n &lt;- n[rowSums(n=="NA")!=ncol(n),] To indicate the correct response, you can create another column for each question (or a separate data frame) of boolean indicators (TRUE/FALSE or 1/0). d &lt;- data.frame(q1=c("a","a","c"),q2=c("a","c","b"),q3=c("a","c","c")) d$q1.correct &lt;- ifelse(d$q1=="a",1,0) d$q2.correct &lt;- ifelse(d$q2=="c",1,0) d$q3.correct &lt;- ifelse(d$q3=="c",1,0)
The csv data file had missing responses as NA, and when importing the data file I set the missing values option to NA. I am going to try this out now, thank you for your time and for the response. 
This post is a great write-up for helping R developers get a sense of data processing with Python.
added column Z table$Z &lt;- ifelse(table$X=="C", "C", "Not C") now grouped on col Z
I too would love to find such. Hopefully someone will be kind enough to share a good lead. 
https://www.r-bloggers.com/building-shiny-apps-an-interactive-tutorial/
Shiny is definitely the way to go here. https://rstudio.github.io/shinydashboard/
what is the difference between shiny and shiny dashboard?
I am an amateur in R. I find shiny difficult. Instead I save my "dashboard" ever hour to html file in drobox folder.
Shiny is difficult for me too. I am curious about what you mean by “dashboard” then. Could you explain?
https://www.dropbox.com/s/4i5v33mxfohufl0/rRagnar_2017-10-01.html?dl=0 This "dashboard" is made one's a day and it is done with RMardown but it is not interactive.
Check out flexdashboard.
I might be wrong but it seems you can’t do interactive chart in RMarkdown...
Are you still available? I'm in the same boat. That guy's class is so messed up. It's not just his language that's difficult, it that there's quiz questions and labs that go over things he never even discussed. Now I'm sitting at the final scared to start it because it's all jumbled in my head.
Sure. What are you looping over? If it's the rows of a data frame, then you can do something like this: example_data &lt;- data.frame(a = c(1,2,3,4,5), b = c(6,7,8,9,10), c = c(11,12,13,14,15)) for(row_index in 1:nrow(example_data)) { current_row &lt;- example_data[row_index,] print(current_row) } 
Do you mean like a career? Yeah dude! Salaried programmers get paid well. Also there’s countless companies that hire by the project, if that’s what you’re talking about. Check out upwork.com and freelancer.com, and there’s like 4 or 5 other websites (their names are beyond me right now, just google it) that host thousands of jobs a day. A good portion of them are crap, but if you do the job as specified you still get paid. You may also land return customers if you can create good relationships with them. 
This will definitely encourage me to start from the 0 I’m at at the moment. Thank you kind stranger!
What's the incentive for the person you want to pay you to choose you over someone who already knows how to code?
No problem. Keep learning, and once you’re comfortable, try and find a project! I started with simple data collection and it helped me build my confidence to do harder projects.
I’ll be significantly cheaper ?
Source?
It’s a cheap energy drink sold by Morrisons supermarket in the UK.
And I'd like a gold plated toilet as incentive to take a dump, but for some strange reason nobody wants to give me one.
Can’t tell if you’re just trolling or genuinely think it’s a bad idea. Surely if someone has a job which isn’t time sensitive, or on a very limited budget, they’d rather pay a nominal fee to get something done than pay a larger amount of money to an expert? I get that an expert would be able to make changes more easily and would understand it better, but not everyone / every organisation has the finances for that.
I'm saying unless you've got some evidence to the contrary, my experience is that in the field of coding there are plenty of people with solid experience who will work for peanuts. So to get this to work put yourself in the person with coding requirement's shoes: there are people with experience I can outsource this work to for pennies to get this work done in a reasonable timeframe, what incentive do I have to pay this person who needs to be taught how to code to get the work done for (possibly) a lot more money and (probably) a lot longer time?
But it won’t be a lot more money - I’ll do it for free / peanut dust.
[removed]
So you'll need to show me that 1) Your rates are cheaper than the random guy on o-desk or whatever outsourcing platform I decide to use for the same amount of work done. This doesn't sound like what you were describing in your original post. And that 2) You can demonstrate to me that you can get the work done in a language you say you still need to learn in the timeframe I require the work to be done in. I don't see how somebody can make such a guarantee personally.
I would also recommend avoiding loops. The dplyr package would be good for this
You could use a for loop. #table is your table you’re referencing X &lt;- 3 #number of rows in the rolling window Output &lt;- matrix(nrow = nrow(table)-X+1, ncol = #number of colunns in your summary table line) for(i in x:nrow(table)){ selecteddata &lt;- table[(i-X+1):i,] #continue calculations here Output[i,] &lt;- #insert summary row here } Or you can use 
Are you someone who does hire people to do this stuff?
I've applied for and worked in relevant roles, I'm more than happy to give you some feedback on what the people in charge of hiring I've spoken to are looking for if you'd like.
I have not tried. Maybe with formattable you can add little bit interactivity to tables. https://cran.r-project.org/web/packages/formattable/vignettes/formattable-data-frame.html
I think I understand how to implement this. I will give it a try.
I'm not 100% surre why it's not working, but I just used the function from the second anwser (Jayden) and it worked fine: lm_eqn = function(m) { l &lt;- list(a = format(coef(m)[1], digits = 2), b = format(abs(coef(m)[2]), digits = 2), r2 = format(summary(m)$r.squared, digits = 3)); if (coef(m)[2] &gt;= 0) { eq &lt;- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,l) } else { eq &lt;- substitute(italic(y) == a - b %.% italic(x)*","~~italic(r)^2~"="~r2,l) } as.character(as.expression(eq)); } I think you can just use this instead and should be fixed 
Shiny dashboard is another package which adds some extra UI features to the base shiny UI. I believe it’s based on bootstrap 3 and the SB admin templates, which allow for great fluid dashboard designs. For example it has extra functions to call sidebars which will collapse on mobile. It’s great because you don’t need to know css or HTML to get a great looking dashboard.
Okay, I'm trying the example from get. neighborhood and I can get it to run in the Rgui, but not in Rstudio - have i broken Rstudio somehow? I tried reinstalling it, but it isn't better. 
Thank you!! This works. My sleep-detrivated brain didn't realise I could try this instead.
I would simply use the dplyr R pacakge to create a new table with only the IATA codes and country names and merge it with the routes table by the IATA codes. Here's an example: library(dplyr) # Create dummy airports data frame airports &lt;- data.frame( country = c("country1", "country2", "country3"), airport_iata = c("AAA", "ABX", "ABA"), some_other_column = c("A", "B", "C"), stringsAsFactors = FALSE) # Create some routes. routes &lt;- data.frame( id = c(1,2,3,4,5,6), airport_iata = c("ABX", "ABX", "ABA", "AAA", "AAA", "ABA"), stringsAsFactors = FALSE) # Create a new data frame from the airports df with only IATA codes # and country names and save it in 'source_countries' source_countries &lt;- airports %&gt;% select(airport_iata, country) # Merge the previously created df with the routes df by IATA code, and rename # the country column to source_country routes_with_countries &lt;- left_join(routes, source_countries, by = "airport_iata") %&gt;% rename(source_country = country) 
Got any NA-values in yr data?
&gt; PS, sorry for the "shitpost" type formatting, I'm kinda in a rush but can't figure out how to do it. Indent code by four characters at the start of the line so that it comes out like code Check the dimensions of your data set, how many observations fulfil your filtering criteria and what is the data completeness of those meeting it (try `summary(smallmonths`) and as /u/episyrphus suggests whether there are `NA` in any variables (although I'd be surprised if that were the cause since such observations are normally silently dropped from the analysis, but if all observations for `lsize2` are missing in this subset it might throw up this error).
You want a merge (in base) or join (in dplyr) the two columns from the airports data.frame to the routes data.frame. Alternatively set the IATA as rownames in airports and then access the rows via these names. ie: routes$country &lt;- airports[routes$IATA, "country"]
It's the [problem of floating points](https://stackoverflow.com/questions/2100490/floating-point-inaccuracy-examples), common to all (?) programming languages. R just hides the issue I guess: &gt; a = 1 &gt; b = 5 &gt; c = 1 &gt; d = 10 &gt; e = 7 &gt; f = 10 &gt; a/b * 1 + c/d * 2 + e/f * 3 [1] 2.5 It's easier to see what's happening in python: &gt;&gt;&gt; from __future__ import division &gt;&gt;&gt; a = 1 &gt;&gt;&gt; b = 5 &gt;&gt;&gt; c = 1 &gt;&gt;&gt; d = 10 &gt;&gt;&gt; e = 7 &gt;&gt;&gt; f = 10 &gt;&gt;&gt; a/b * 1 + c/d * 2 + e/f * 3 2.4999999999999996 The solution in many programming languages is a new data type "Decimal" which is slower to number-crunch with, but which doesn't have the accuracy issues. R doesn't have one built in, and I don't know a package off the top of my head.
Just add a parenthesis on the left side of the comparison: e.g. if( (a+b+c)==1). Otherwise R will evaluate only if c == 1.
Ha, welcome to R. The first chapter of [R Inferno \[PDF\]](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf) might be of interest to you. In short: **always** assume there will be error when operating on floating point numbers. To compare floating point numbers you can use the `all.equal()` function rather than `==` Example: a &lt;- 1 b &lt;- 5 c &lt;- 1 d &lt;- 10 e &lt;- 7 f &lt;- 10 result &lt;- a/b * 1 + c/d * 2 + e/f * 3 == 2.5 all.equal(result, 2.5) # [1] TRUE all.equal(result, 2.51) # [1] "Mean relative difference: 0.004" 
You're falling into the floating point trap. See Circle 1 of The R Inferno by Patrick Burns. Comparing numeric (floats) using `a == b` is not reliable. I compare their difference at some tolerance level instead: `(a-b) &lt; (0.0001)`
&gt; common to all (?) programming languages Just to clear up your tentative question mark: it’s not a “programming language problem”. It’s a mathematics problem. It simply fundamentally cannot be done. When representing fractions with a finite number of floating point digits, there are some fractions that you cannot represent accurately. Decimal has the same problem: try writing 1/3 or 1/7 precisely as a decimal fraction. Typical floating point numbers are a finite representation of binary fractions, but the same principle applies.
Thanks! That makes a lot of sense actually. This must cause so many problems in programming!
As other people have said, this is a floating point problem. The function `all.equal` has tolerance built in for this: &gt; a = 1 &gt; b = 5 &gt; c = 1 &gt; d = 10 &gt; e = 7 &gt; f = 10 &gt; identical(a/b * 1 + c/d * 2 + e/f * 3, 2.5) [1] FALSE &gt; all.equal(a/b * 1 + c/d * 2 + e/f * 3, 2.5) [1] TRUE So you can leverage it to resolve these problems sometimes.
Thanks for your tip! I fixed it by converting the result and the target to strings and then comparing them. Bit of an ugly fix.
This is not true.
I'd use a smaller tolerance, like 1e-6, and take the absolute value of the difference, but otherwise yeah this is the way to do it. 
Or all.equals
I started on Monday. I'm using http://www.google.co.nz/url?q=https://cran.r-project.org/doc/contrib/usingR.pdf&amp;sa=U&amp;ved=0ahUKEwj9nMvv4enXAhXFjpQKHb4vBfcQFggLMAA&amp;usg=AOvVaw3lrMbbPpbZDxgLtNfuxMUK Been good so far, once I got the basics I have been practicing with data from work. My first challenge was using the package ggplot2 to make a boxplot with a jitter overlay. Took me a few hours but that is my recommendation. 
[This free textbook](http://r4ds.had.co.nz) by the chief data scientists at RStudio is the best place to start.
One thing you will find useful in all walks of life is to be specific when you ask questions. The verb "to group" can mean a lot of different things, even inside an apparently small field. So it's really difficult to answer your question, because it's not at all obvious which of the many meanings of "grouping" you might mean. Further to that, your description of your file is quite vague. What are the contents of your file? What types of information are in it? And so on.
Agreeing with the above comment that we have no idea what you are talking about, I suggest the dplyr packages. 
PM me your email - ive got a pdf copy of R for dummies and a couple of other purportedly useful resources (I recently attended an introduction to R class, but haven’t followed up on any of this yet!)
mateuszdadej@gmail.com thanks !!!
I second the Hadley Whickam books and the R Studio forums. I’d also plug Data Camp. Robinson and Gromlund were excellent instructors.
I like how the choices give you a false illusion 
When you say diagnostics do you mean console output? 
No like if there is an issue with the code, there will be red squiggly lines under the text etc.
I don't understand the illusion or trick here? Am I really dumb...? The answer is A. Granted it doesn't print it on one line or with commas, is that the catch?
This is homework, isn't it?
Check the Project options, since those can override global options.
Not o.p. but I would be interestes in those pdfs. My emial is Markwrng@gmail.com. thanks in advance good sir/madame. 
It's not available in the menu for some reason. 
Generally don't include the kitchen sink in a model. Use AIC to select the best model. Usually the r2 goes up with a better model.
Not sure if I understood correctly, I'd suggest you to try plotting the data and see if you get some correlation. R2 is a measure of how "far" your data falls out of your model. Of course you could hand pick some points and your R2 would go up to .99 but you would end with a useless model.Maybe some non-parametric approach could fit better
So generaly what columns are well suited for a model. I realize things like a zip code arent. But what about time, latitude, longitude, etc?
Try running a stepAIC on the data in order to get AIC data.
Will you forward the pdf to others?
If the Project menu is inactive, you're probably not in a project, so that's probably not the problem. Can you check all your global options. Here's what should be set: https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics
`j2012_two_removed &lt;- j2012Full[-c(1420, 1611),]` I'm surprised row 1420 still shows up, but 1611 isn't 1611 after removing rows above it. You can also try subsetting to just the columns you need, and then use `complete.cases()`.
OK that makes sense, thanks so much!! It’s finally working properly 
When you remove row 1420, doesn't row 1611 become 1610?
You'd edit the function readdata... Think of it this way: filenames = list.files(pattern="*.csv") filename=filenames[1] Now write a function that takes filename and does all the transforms you want. Then do the lapply/rbind to apply the function to a list of files, then bind the vectors into a matrix. You may want to make the data you return into a dataframe instead of a vector, but that's up to you (I prefer to rbind dataframes, maybe that's just a personal quirk.) 
Will it? I'd rather pay someone that knows what they are doing to give me the right result than have to debug a "cheaper" solution afterwards. I have no doubt (as a coder with &gt;20 years of experience) that what you can do in a week I can probably do in the order of 5 to 30 minutes. Source: I train incredibly smart non-coder grad students to code regularly. If you want to learn to code and you need projects, do some for free for the experience. Once you're developed some skills (ie: you can actually produce results and have a sense of what you can/can't accomplish), then you can start charging someone for your efforts. Or, you can get a job that leverages some of your existing skills, and take on some coding work as part of that job where your strength is your domain-level experience.
There's always a risk to outsourcing work to someone else. When that someone has no actual experience doing something that requires a significant degree of skill, that risk far higher than most people would accept, given a minimal difference in price.
Thanks for the genuine feedback unlike the other guy who just seemed to have a hated for people willing to try new things. 
Most likely your working directory is not the same as where your file is. do getwd() to see whether it is the same as the location of your file. you can always add full path in you read.csv
it gives me the same error. And the directory is the same that where the csv is
[removed]
Which OS are you using? It’s important to save the excel file as CSV relevant to the OS. When you save it as CSV, uou get 3 options, csv dos, mac os csv, or simple comma separated... pls use this option, I assume you’ve set the working directory correctly 
Make sure you are using forward slashes. Windows uses \, but R likes /.
Im using windows 10. The working directory is the same and i saved it in simple comma separated as in the video. But when i try to read it, the error pop up
I second this as the probable cause use getwd() To see what the file directory is. Compare it to your file locaiton. See if specifying arguments helps any. You may have something the import isn't reading by default? read.csv(file, header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "", …)
Hi again. I changed some things and now it gives me an other error Error in read.table(file = file, header = header, sep = sep, quote = quote, : more columns than column names It has 2 columns and are called as x and y. Dont know what to do
All solved. I had to change in excel the separator into a full stop. Then R read the csv perfect. Thank you all
If it makes you feel any better, it's how I got into coding as a professional at the start of my career. I have a biology background, and started writing simple analysis scripts as my first code...but I wasn't a "coder". I was still paid to do bench work just like guy next to me. I wrote code for the experience/learning, got better, then landed a gig in a startup. I did 2 years of classes at night while learning, and basically spent every second I wasn't at work learning to code. I think that path is much harder now - there are more people who know how to code than 20 years ago, so you need to really put some effort into changing careers (or building a career at all.) Nobody's gonna hand it to you.
if you find an answer please let me know, I'd like to know how this is done myself.
In countries where commas are used as decimal separators, excel will output semicolon separated csv-files by default. In those cases you can use `read.csv2(file)` or you can add the separator manually to `read.csv(file, sep = ";")`. 
Have you tried; read.csv(file.choose()) and selecting it directly from the directory. 
Thanks
I think it's: read.csv( choose.files( ) )
Prepare/clean your dataset and you're almost done. For example: df &lt;- data.frame(x = c("a", "b", "c"), y = 1:3, z = c("a", 7, 7)) df$z &lt;- as.double(as.character(df$z)) rownames(df) &lt;- df$x df$x = NULL cov(df, use="complete.obs") 
Is your data a vector? Do you just want it split to two columns and keep the 'x-' and 'y-'? If yes you can try: coords &lt;- c('x-1', 'y-1', 'x-2', 'y-2', 'x-3', 'y-3') matrix(coords, ncol = 2, byrow = TRUE)
fool-proof* ;)
This thread is rather motivating. I’ve been playing with Shiny and Flexdashboards a couple of months ago but couldn’t get buy-in at work. I’ve tried everything from self-service dashboards like Google Data Studio to screens in Geckoboard. But it’s so amazingly frustrating when people cabt explain what the hell they’re looking for.. no solution seems to ever be good enough. Realistically, they could just look that stuff up in Google Analytics and our databases but somehow everyone claims to want dashboards but then can’t make their mind up on what data they want to see and how. But I’ll give Shiny a new try.
Check out this shiny gallery and see if there's anything you like. Code is available, so you can reproduce it: https://shiny.rstudio.com/gallery/ I am not sure what metric (what is your data? 10,000 subreddits - what information from these subreddits?) you used to perform tSNE dimensionality reduction on, but you can also use clustering algorithms (e.g. "cluster" package) to define your clusters. Or you can also try more advanced clustering methods like HDBSCAN. Again I don't know what your data is, so these clustering algorithms may not be relevant.
Thanks for the input! Sorry I forgot to elaborate on the actual data. I extracted Reddit comment data from Google's BigQuery for comments made in January of 2017. With these comments I created a sparse sparse matrix with rows and columns each being unique subreddit names. At the indeces of the matrix intergers represented the number of unique commentor who had posted in each of the subreddits for that month. Effectively I'm linking subreddits through commentors. I then narrowed the subreddits represented to only the 10,000 most popular ones by number of commenters and performed a t-SNE. I'm actually not familiar with HDBSCAN, will my current matrix work as input for the algorithm?
Have you checked out the ggmap library yet? You can use Google's API with it up to a query limit - you can check the Google's API for the different limits like geocoding. FYI: you have different options for how you get the geocode back. Check out the documentation. https://cran.r-project.org/web/packages/ggmap/ggmap.pdf 
 stream_in(url(paste0('https://maps.googleapis.com/maps/api/geocode/json?address=', address, '&amp;key=', key))) https://cran.r-project.org/web/packages/jsonlite/jsonlite.pdf
You might want to do some normalization so that the results don't get confounded by the number of comments in a subreddit. Something like X[subredditA, subredditB] = (# of users who posted in both A and B ) divided by (# of users who posted in either A or B). 
Clustering algorithms will take any matrix you throw at them. I also agree with u/Deto that you should normalize your matrix. Try to colorize your tSNE dimensionality reduction plot by total number of users in a subreddit (you can do this easily in ggplot2) and you can get an idea of how much of you tSNE projection depends on size differences.
So the height of each bar is the count of objects in each status? Approximately how many observations are in each day?
I’ll be back at my computer in a few hours and can send something to you. If I understand your question, this is fairly trivial to make. 
No. The height of each bar is the same, just the color inside each bar changes with time. 
Well for missing data, you can use the arguments is.na to find the number of missing cells, na.omit to remove them from the dataset, and na.rm to ignore them when doing calculations (like mean or something). Hopefully that's what you're looking for. 
It's not data missing in the frame but rather an entire missing row (i.e. no data was observed/no row generated). There aren't any NAs.
I built a dataframe with three columns: date, object, and type. I made 60 unique objects and assigned them a type at random with a ratio of 6:2:1. Then I plotted them using ggplot2. Since it sounds like your data is daily data and I made a range from beginning of the year to now, I made the width of each column 100% to eliminate some weird white space. Then I specified position = "fill" as it sounds like you want the fill of each bar to be the proportion of the type for all 60 objects for that day. Code is below: #install.packages(c("tidyverse", "scales")) library(tidyverse) expand.grid(list(date = seq(as.Date("2017-01-01"), as.Date("2017-12-05"), by = 1), object = 1:60)) %&gt;% as_tibble %&gt;% bind_cols(type = tibble(type = c(rep("type_a", 6), rep("type_b", 2), "type_c")) %&gt;% sample_n(20340, replace = T) %&gt;% mutate(type = factor(type)) ) %&gt;% ggplot(aes(date, fill = type)) + geom_bar(position = "fill", width = 1) + scale_y_continuous(labels = scales::percent, expand = c(0,0)) 
I didn't realize that the netcat command was so simple. Kinda renders [this little repo I made](https://github.com/MattSandy/wspr) worthless. Why are you avoiding using the system function? 
Take the difference between consequtive rows. I think the function is diff() or difftime(), you may also need to use strptime.. not sure.
Good point about normalization, I completely forgot to implement that, so I'm glad you reminded me! When you say to divide the number of shared commenters between two subreddits by the number of commenters of either A or B, do you mean that I should sum the number of commenters of A and B and divide by that value, or do you mean it's my choice whether to use the value of A or B? I can see how both methods could normalize the data, I'm just unsure which method would work better.
use shift() I've used this and it's very powerful... if you can give a sample vector, I can show you how it works 
The sum would work. If it's feasible, it's best to take the count of the people who commented in A or B. For example, if {John, Jane, Joe} commended in A, and {Joe, Bob} commended in B, then the numerator would be the number of people in the intersection {Joe} = 1 over the number of people in the union {John, Jane, Joe, Bob} = 4. 
[Here's an example](https://pastebin.com/raw/JHFrmWWT)
I am also new, but is it possible you are working with floating values. Might make a difference if you change the values to an integer
thanks for your reply, how do i do that? just tried to figure it out myself, with no luck. i have programming experience but for some reason, R is extremely difficult for me so far!!
Well assuming all your values dont contain any decimal points, you could try as.integer(). So if I am not wrong the it should be new.var = as.integer(old.var) Try it, see what happens, only harm is you have a new variable
You can get rid of the x axis scales by using an input of xaxt='n' in the hist() formula itself, then follow it using the axis() command. Using ?axis can help you see the usage (it should be on the at= section). Also, to answer your question about coercing numbers to integers, you can use as.integer to coerce an entire vector of numbers to integers. The one thing to remember if you're a beginner in R is that R is a vectorized language, meaning you can pass vectors as arguments to help you do many tasks more efficiently that would otherwise require a loop.
Use ggplot2 
Since A is not invertible, the system Ax = b could have zero or infinitely many solutions. The latter happens to be the case. Using the pracma package, we can row reduce the augmented matrix Ab and find the solution by hand: library(pracma) A &lt;- rbind(c(1, 2, 3, -1, 0, 0), c(2, 1, -1, 0, -1, 0), c(0, 0, 1, 0, 0, -1)) b &lt;- c(10, 20, 4) Ab &lt;- cbind(A, b) rref(Ab) x1 &lt;- c(11.5, 1, 4, 15.5, 0, 0) A %*% x1 # works But this solution isn't strictly positive. Since any linear combination of two solutions s1 and s2 to the system Ax = 0 is also a solution to Ax = 0, any linear combination x1 + c1 * s1 + c2 * s2 is a solution to Ax = b: zv &lt;- c(0, 0, 0) rref(cbind(A, zv)) s1 &lt;- c(2, -2, 1, 1, 1, 1) s2 &lt;- c(7/3, -5/3, 1, 2, 2, 1) A %*% s1 # works A %*% s2 # works c1 &lt;- 0.1 c2 &lt;- 0.1 soln &lt;- x1 + c1 * s1 + c2*s2 soln A %*% soln c1 and c2 can both be any real number.
How did you get the vectors x1,s1 and s2?
I solved them all by hand. For x1, I saw that the second row of rref(Ab) is c(0, 1, 0, -0.67, 0.33, 2.33, -9.33). You said you wanted a positive solution, so I calculated what the 4th coefficient whould have to be if I wanted the second one to be 1: (-10.33/-0.67). I then calculated what the first coefficient would have to yield the desired solution under these conditions. The third row is straightforward. For s1: I didn't want to get messy with decimals, so I only calculated what the first three coefficients would have to be if the last three were 1. For s2: had to get messy with the decimals, and seeked a nonzero vector. So, I picked coefficients 4 and 5 to be 2 (since they are both 1 in s1) and added 2 times the fourth and fifth terms to the 6th term. Then I knew what the first and second coefficients had to be. There may be an easier solution than doing it by hand, but its been awhile since I've done LA. 
Your function hist() is assuming that the possible values (1,2,3,4) are continuous. Instead, you actually want to count the occurrences of each distinct value, you want a factor. Try converting Data$column into a factor like this: factor(Data$column)
Use a tryCatch() function for error handling. I haven't tried this out but something like: ConnectToDb &lt;- function(a,b,c,d) { con &lt;- tryCatch({ dbConnect(MySQL(), user = a, password = b, dbname = c, host = d) print("Connection made") }, error = function(err) { print("Please confirm your login details") return(NA) }
It looks like even though you have a 'numeric' vector you're trying to plot, you actually want to treat it as categorical and create a barplot instead. 
If you really just have values that are integers between 1 to 4, I might just do something like barplot(table(Data$YearsBought)), instead of a histogram.
Ok, I'll let on I'm pretty ignorant about all this... would websockets be the what the Python approach above is doing? I saw in the docs for `socket` that it uses the BSD socket interface and assumed that was different. Are they the same or maybe provide some of the same functionality? Basically, I'm not against the `system` function, per se, but the reason I am looking for an R equivalent of the Python `socket` module is because: &gt; This module provides access to the BSD socket interface. It is available on all modern Unix systems, Windows, Mac OS X, BeOS, OS/2, and probably additional platforms. https://docs.python.org/2/library/socket.html Whereas, I don't know how to do something with the `system` function that would work across systems. On Linux I could do the netcat thing inside `system` or even something like: `echo -n "foo:1|c" &gt;/dev/udp/localhost/8125` but it'd be nice to be able to do something that works across systems. Since you can apparently use Python code from R, maybe that's the best route if there's no native R way to do it. It'd be nice not to rely on Python, but that may be the quickest way to get something that works in multiple places.
I personally think that this is very valid and probably what I would do, but I think his teacher is probably adhering to a set teaching schedule that involves just histograms at the moment, so I assume that is what is desired.
Not sure if it is different, but I think websockets can negotiate things like services. It might be different, and still work. Try finding a websockets chrome extension to see if you can connect over that protocol. You could also make a proxy server, so that you could convert all http requests to a socket message. Have python just listening.
* When you type `View(getResult)` are you seeing the same function as you expect? * Have you tried calling the function outside of sapply and adding a print for the Refused case? * Have you tried doing `if(trimws(as.character(answer))=="Refused")` Side note, this could be programmed a little better if(answer %in% c("Don't Know","Refused")) { return(answer) } else if(answer==correctAnswer) { return("correct") } else { return("Incorrect") }
Ok, so I just did the View(getResult) and it shows up as if there was no case existed for the “Refused” case. I’ll look into the other stuff now 
Yep, that can sneak up on you. Just redefine the function and you should be good to go.
&gt; You could also make a proxy server, so that you could convert all http requests to a socket message. That sounds more difficult than calling Python code from R, though admittedly, I don't know what it would take to do something like that. I come from the land of statistics, and that sounds like voodoo, but if you can share some code or examples, that'd be lovely and very helpful. Assuming I could figure out how to do this (no clue how long that would take), is there a reason one might chose this over calling Python from R? 
The only reason to make that proxy is if you plan on doing similar stuff in the future. 
A factor is a class wrapped around a vector; it's a vector of ints with some metadata added. A data frame is really a list of vectors, each column being a vector. So different columns aren't the same factor because they aren't part of the same vector. You can't make them the same factor, but you can make it so the same words are coded to the same integer by remaking the factors. First you need to find the levels of the factor (if you don't already know them: (d = data.frame(V1=c("one", "one","two"),V2=c("two","one","three"),V3=c("three","two","three"))) (l = unique(c(levels(d$V1),levels(d$V2),levels(d$V3)))) This gives you a character vector with one element for each level found in any of the input factors. Then you remake the old factors into new ones based on those levels: (d2 = as.data.frame(lapply(d, function (x) factor(x, levels=l)))) 
Factors are decided on a column-by-column basis, not within rows! In this case R probably takes all unique strings within a column, sorts them alphabetically and assigns a sequence of integers based on that ordering. So in column `V1` `one three` becomes `1 2`; in column `V2` `one three two` becomes `1 2 3`, and in column `V3` `three two` becomes `1 2`.
You might want to consider the [`case_when()`](https://www.rdocumentation.org/packages/dplyr/versions/0.7.3/topics/case_when) function from &gt;dplyr-0.7.
The other comments touch on what is actually going on. I'd like to focus on why you want to do what you want to do. If you explain your logic and raining we can either tell you a better way to do it or explain why is not a great idea. Either way you learn. 
I'm not very good with regex, but this should work: library(stringr) test_str &lt;- "smalldog123.isverycute_morewords98765" find &lt;- "smalldog" extract_digits_following &lt;- function(input_str, find){ regex &lt;- str_c(find, "[0-9]+") digits &lt;- str_extract(str_extract(input_str, regex), "[0-9]+") return (digits) } Someone else probably has a more elegant solution :)
Would something like expand.grid() help? if he just wants all combinations of those variables, then that might help. 
that's not R code... but yes in R you can define a function: nested_loop &lt;- function( expression_function ) { for(i in 1:10) { for(j in 1:10) { expression_function(i,j) }} } I don't think you can get around defining the inside as a function without making it unsightly R code...
This is awesome! Thanks so much! 
Thanks for offering to help! Basically I'm trying to work with a large CSV file of three columns, the first two columns contain character vectors and the third contains numeric vectors.I'd like to treat values of the first and second columns as indexes in a matrix, and where the two intersect, I'd like to place the numeric value in the third column. I'm doing this as a sparse matrix, so I can easily build it by initializing the i, j, and x variables of the Matrix function with the three columns of the imported CSV file's data frame. There are identical names within the first two columns, and to get the names to work as indeces, I've just been importing them as factors, and changing the columns to numeric with as.numeric when loading them into the Matrix function, however, now I realize that doing this could lead to mislabeled names. So here's how I've attempted to fix this issue: # Importing CSV with stringsAsFactors set to dalse subreddit.overlaps.df &lt;- read.csv("subreddit_overlaps_2017_01.csv", stringsAsFactors = FALSE) # Getting a vector of all unique names in the first two columns of the data frame. subreddits.unique &lt;- unique(c(subreddit.overlaps[, 1], subreddit.overlaps[, 2])) # Sorting vector of unique names subreddits.unique.sort &lt;- sort(subreddits.unique) # Getting vector of name indeces. # This isn't useful now, but it might be later. subreddits.index &lt;- 1:length(subreddits.unique.sort) # Converting names in the first column of imported CSV file data frame to numeric vector. subreddit.1.numeric &lt;- match(subreddit.overlaps[, 1], subreddits.unique.sort) # Converting names in the first column of imported CSV file data frame to numeric vector. subreddit.2.numeric &lt;- match(subreddit.overlaps[, 2], subreddits.unique.sort) # Creating numeric data frame using numeric vectors. subreddit.overlaps.numeric.df &lt;- data.frame(subreddit.1 = subreddit.1.numeric, subreddit.2 = subreddit.2.numeric, overlap = subreddit.overlaps[, 3]) I think that logically it works, but I'm unsure whether there is an easier way. I never used match before either, but hopefully I'm using it correctly!
Stuff you think is cool or that was difficult. Fun visualisations, interesting models, novel approaches to parameterisation... Anything you've done that you were proud of.
Shiny applications are a great addition to a portfolio as they're tangible to anyone regardless of their ability to code.
What does 'names(anesRmOversample)' return? Some operations add an extra column. 
Whatever you do write them as [packages](http://r-pkgs.had.co.nz/) and host them on GitHub. If you write Shiny applications you can [host them for free too](https://www.shinyapps.io/).
If you work in digital performance marketing there's probably a lot you can do at work. 
*worked in digital, currently unemployed, job just did not work out.
Oh, sorry. Good luck! I often recommend to our marketing team to learn SQL too. They think "analytics" and R is pretty sexy, so that's what they gravitate towards. However, 90% of the time, when they want "analysis", they just want data in their hands, structured in a way that's meaningful. Excel to play around with it is enough, and being able to query from whatever data warehouse they want gets them there. Depends on permissions, company culture, and all that of course.
Thanks! I really appreciate it, working my ass off on SQL too. Need a brush up but my Oracle is solid. Hopefully I get it down well enough someone will let me be a junior something. 
I couldn't get to your pastebin (I don't know if my organization prevents me, but check the following in your R session: df &lt;- data.frame(A = runif(100),B = rnorm(100)) shift(df$A) shift(df$A,5) shift(df$A,5,type = "lead") shift(df$A,5,fill = 0) That's how it works. for days, and what you're trying to do, lets do the following: Since df$A is a vector, lets work with vectors for now. date_vector (dv) will be 90 days. dv &lt;- as.Date("2017-01-01")+0:89 We'll knock out a few dates: dv &lt;- dv[-c(6,34,89)] ok, I know what days we knocked out because it's a sequence, but say I didn't know when it started (the case with your data), well, we can use shift: dv - shift(dv) and to see it side by side, we can make a data.frame: data.frame(dv, dv - shift(dv)) dv dv...shift.dv. 1 2017-01-01 NA days 2 2017-01-02 1 days 3 2017-01-03 1 days 4 2017-01-04 1 days 5 2017-01-05 1 days 6 2017-01-07 2 days &lt;------- to get a better picture, you could select rows from that data frame where the second column is greater than 1 
I think this should do the trick: myColCI &lt;- function(df, dep.var, indep.vars) { summary.lst &lt;- list() confint.lst &lt;- list() for (i in 1:length(indep.vars)) { form &lt;- as.formula(paste(dep.var, "~", indep.vars[i])) fit &lt;- glm(form, data=df) summary.lst[[i]] &lt;- summary(fit) confint.lst[[i]] &lt;- confint(fit) } return(list(summary.lst, confint.lst)) } This function will take the data frame (nepalData), the dependent variable (death180, as a string), and a vector of all the column names you want to fit this model to (all as strings as well), and will output a list containing all of the fit summaries and the confidence intervals. ...This assumes you want to do a univariate analysis for each independent variable. If you want to do a model that contains all of the variables at once (and you have a lot of variables that you don't want to type out by hand), then it's actually much simpler: form &lt;- as.formula(paste("death180 ~", paste(indep.vars, collapse="+"))) fit &lt;- glm(form, data=nepalData, family="binomial") summary(fit) confint(fit) where all you'd need to change here would be substituting the "indep.vars" with a vector of all the column names you'd want to include in the analysis. This will automatically take all of the column names and turn them into a formula. If, however, you only have a few columns you want to use in the analysis, you could just do this: fit &lt;- glm(death180 ~ Column1 + Column2 + Column3, data=nepalData, family="binomial") summary(fit) confint(fit) and replace "Column"s with the actual column names, and keep adding them for however many you want.
Here is a nice pipelined option for you: library(tidyverse) library(broom) mtcars %&gt;% select(-vs) %&gt;% #remove dependent variable select(mpg:hp,wt,gear:carb) %&gt;% #select independent vars that you want to build models for names %&gt;% #extracts names of columns paste('vs ~', .) %&gt;% #creates formulas for models map(~glm(as.formula(.x), data = mtcars, family = binomial ) %&gt;% # map a binomial glm to each independent var tidy %&gt;% mutate(term = ifelse(term == "(Intercept)", paste0(.x, term), term)) ) %&gt;% bind_rows #bring all the dataframes back into a single dataframe
Hm, sounds like a path I don't want to go down. I'm looking for a dead simple approach that could ideally be done from any R session without a need to set up a server. Unless there is an Eli5 of what you've suggested, it sounds over my head. I appreciate the suggestions, but if you have any thoughts that stay entirely within R, that would be would I'm looking for. I found the svSocket package, but not sure that's designed for my use case. More playing around needed. I see there's an httpuv package (websockets) that could be relevant, but will need to read further to see if it can help. It's interesting that this is somewhat difficult to accomplish from R but there's statsd support from lots of other languages. Sounds like wrapping a C package that does this may be one way to go if there's nothing already available.
Same way you plot any graph. 
If youre trying to plot it, using ggplot2 you can either use stat_function and specify a function(x), or pre-generate a dataframe of x values and corresponding y values then use g &lt;- ggplot(dataframe, aes(x=xdata,y=ydata))+geom_line()
Which package has the shift() function?
This is really nice. I still consider myself a novice though I always have the voice saying, "y'know there's a better way to do this." I really should look at R's source code more often.
base
Great to see a guide for using R outside of RStudio! I set up lintr with Ale recently and it was super helpful. Couldn't find a good autocompletion setup but looking forward to trying out your recommendations. Vim 4 life!
but also ggplot2 of course: https://plot.ly/ggplot2/
There are quite a few packages for tables... * [desctable](https://cran.r-project.org/web/packages/desctable/index.html) * [formattable](https://cran.r-project.org/web/packages/formattable/index.html) * [huxtable](https://cran.r-project.org/web/packages/huxtable/index.html) * [pixiedust](https://cran.r-project.org/web/packages/pixiedust/index.html) * [tangram](https://cran.r-project.org/web/packages/tangram/index.html) * [tables](https://cran.r-project.org/web/packages/tables/index.html) * [xtable](https://cran.r-project.org/web/packages/xtable/index.html) 
The [Box-Cox transformation](https://en.wikipedia.org/wiki/Power_transform#Box–Cox_transformation) is a method that tries a number of different normalizations (including log, sqrt, etc.) and a lot of statistical software can run it to find the transformation (of the set examined) that produces the best normality. Before that, however, you should look at your data and figure out why it isn't normal: Is it outliers? Is it skew or kurtosis? Is it bumping against a zero lower bound? Use plots and summary statistics to explore the data. What type of data is it? measurement, proportion, count. 
Pretty cool, thanks!
here is the full R code just to get an idea of what i am doing ********* library(tm) library(stringr) library(wordcloud) #folder containing the files we want to cluster folder&lt;- "" list.files(path=folder) list.files(path=folder, pattern="*.txt") filelist&lt;- list.files(path=folder, pattern="*.txt") filelist paste(folder, "\\", filelist) paste(folder, "\\", filelist, sep="") filelist&lt;- paste(folder, "\\", filelist, sep="") typeof(filelist) lapply(filelist, FUN=readLines) a&lt;- lapply(filelist, FUN=readLines) lapply(a, FUN=paste, collapse=" ") corpus&lt;- lapply(a, FUN=paste, collapse=" ") corpus gsub(pattern="\\W", replace=" ", corpus) corpus2&lt;- gsub(pattern="\\W", replace=" ", corpus) gsub(pattern="\\d", replace=" ", corpus2) corpus2&lt;- gsub(pattern="\\d", replace=" ", corpus2) tolower(corpus2) corpus2&lt;- tolower(corpus2) removeWords(corpus2, stopwords("english")) corpus2&lt;- removeWords(corpus2, stopwords("english")) gsub(pattern="\\b[A-z]\\b{1}", replace=" ", corpus2) corpus2&lt;- gsub(pattern="\\b[A-z]\\b{1}", replace=" ", corpus2) stripWhitespace(corpus2) corpus2&lt;- stripWhitespace(corpus2) corpus3&lt;- Corpus(VectorSource(corpus2)) corpus3 dtm&lt;- TermDocumentMatrix(corpus3,control = list(minWordLength =1)) dtm_tfxidf&lt;- weightTfIdf(dtm) m1&lt;- as.matrix(dtm_tfxidf) m&lt;- t(m1) rownames(m)&lt;- 1:nrow(m) norm_eucl&lt;- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5) m_norm&lt;- norm_eucl(m) num_cluster&lt;- 4 cl&lt;- kmeans(m_norm, num_cluster) round(cl$centers, digits = 1) for (i in 1:num_cluster){ cat(paste("cluster ", i, ":",sep = "")) s&lt;- sort(cl$centers[i, ], decreasing = T) cat(names(s)[1:20], "\n") } ********* The above code gives me something like this: *********** cluster 1:mom air piano unique shopping wemo country kit kitchen harmony gary amazing lazy wrap everywhere strip blind enjoying learns outage cluster 2:ihome nog frustration cognac shopping update exact addressed payment alexia calendar dollar disappointing warranty mac weak searches streaming difficulty score cluster 3:lexa junk crappy expired reconnects trial waste customers circling deffective desktop hole paperweight ********* How can i save Cluster 1, cluster 2 and cluster 3 outputs as a file? help please
It would be helpful if you posted a sample of code that has an example of the data and the analysis you've run, and what you've tried. With that, it's easier to propose a method to output the data in some format. Maybe this link will help, but it's hard to know without knowing something of your code: https://www.r-bloggers.com/export-r-output-to-a-file/
thanks man, you saved me so much trouble. I was able to save it somehow with stargazer. i tried the R blogger's code already but no luck. it's solved anyway. I appreciate
yes, as a new user I was hoping to see a non r-studio workflow, but hadn't seen much
Why are you using cat if you want to save and not print? "cat" just prints to console. Do something like this: # outside the loop, initialize an empty object results &lt;- NULL # loop for (i in 1:num_cluster) { iter &lt;- paste0("cluster", i) s &lt;- sort(cl$centers[i, ], decreasing = TRUE) # save your results to a vector x &lt;- names(s)[1:20] # add the cluster id x &lt;- c(iter, c) # store in results results &lt;- rbind(results, x) } Then you can write results out to cvs file with "write.csv" Also, please (please please please) read and follow this for your code: https://google.github.io/styleguide/Rguide.xml
I wouldn’t recommend ‹stargazer›, [it’s a terrible package](https://www.reddit.com/r/rstats/comments/6o9v9h/whats_your_favorite_relatively_obscure_r_package/dkgw9q1/). Literally anything else is better (pander, xtable, …).
It’s worth noting that this is a *bad* table that violates several rules for well-formatted tables (to name just the most glaring flaws: the column alignments are wrong, there are too many significant digits, the table borders are bad). I’ve found that the easiest description of good table design is in the [“booktabs” LaTeX package description](http://anorien.csc.warwick.ac.uk/mirrors/CTAN/macros/latex/contrib/booktabs/booktabs.pdf). Unfortunately no R package directly generates such tables well in my experience. My recommendation is to use the ‹pander› package to generate Markdown tables, and then convert those to LaTeX using pandoc.
Thanks - I haven't actually used it myself. It just came up on a search of things that might help the OP.
You should look up documentation for dplyr. I think the filter command is what you want. Then, you could use summarize function for summary stats of particular variables/columns. 
Thank you! I'll take a look at it
You can also do this without dplyr, by specifying the column you want data from: dataone &lt;- people$weight[people$height==170]
Wow thanks! It worked perfectly!
Man, I thought I was doing bad when my code took like 6 minutes to compile... Somewhat off topic, but I've been playing with the idea of using AWS for a few different projects, but I find their pricing to be confusing. There are like 100+ different services and personally I cannot tell what's free and what's not. And some of the 'free' stuff is only free for the first year, so I don't want to sign up until I know I have something to play with.
Thank you, i'm sorry for the jumbled code. Bad coding habit, but sadly hard to change. I'll try.
the thing is the free tier option only last one year and is a single-core machine. But you can setup all you stuff in it, and then if when you want to run a heavy computation, start a new instance from it with a spot request on a 32 cores one (xlarge). The price should be around 0,30$/hour. so you would pay only ~1$ for your computing. Fair enough for me
&gt; I want to extract the values from the third row as long as they meet a criterion from my second row I think you mean column?
Base R can do it in one line as well: mean(people$weight[people$height == 170]) 
My Alma Mater! I doubt I can give you much help as I am learning the language myself but it would be easier to ask something specific than to post an open-ended question here on reddit. Also note I doubt anyone is going to do your project for you if that is what you are hinting at. But if you ask for direction someone might point you to where you can start.
Appreciate the response and feedback! My project essentially is trying to relate the National Anthem protests to either rise and fall in NFL attendance. I created my own data set from scratch in Excel and exported that into R. I am just having some trouble with my ggplot functions I don't know whether or not I need to tidy my data in order for them to properly show or not. It's been stressing me out so much lately. 
What's your exact question? What do you mean "tidy" your data? How is it not tidy now? Surely if you created it from scratch in excel, it should be coming in a, at least somewhat usable form. Show your code and what you are confused about with ggplot.
Im gonna bet you have all sorts of encoding issues first off. Secondly i suggest you hit stackoverflow and do your homework.
Wow, what an interesting assignment. As far as the ggplot2 commands are concerned, its quite simple. The bigger focus is probably going to be on getting a robust data collection pertaining to the attendence, the tv ratings, and the kneeling. and possibly even the actual kneeling tactics (are they doing it on field, are certain teams keeping kneelers in the locker room) and then also keep in mind to annotate on the graph the point in time when the networks stopped showing the kneeling on tv. 
Are your X, Y, and ID variables all in a single column? Say, Date Team Record Kneelers Seats_Filled 12-15-2017 New Orleans 5/5 4 .75 12-07-2017 New Orleans 4/5 5 .80 12-07-2017 Minnesota 3/6 2 .70 12-01-2017 Minnesota 2/5 3 .75 Not sure what kind of columns or data you have but if it is organized in a "tidy" format like above, then the ggplot2 commands would be very simple, ggplot(my_footbawl_data, aes(x = Data, y = Seats_Filled, color = Team)) + geom_line() That'd give you a huge messy graph with a lot of teams, but you could either filter the teams first by another variable (say, win/loss record) or maybe do it regionally (by conference). Better yet facet them by those variables. 
That makes a lot of sense I'll keep it in mind when subsetting. I'll also take a look at data.table, thanks a lot for your time! 
Thanks for your reply. Best of luck. 
You have five people commenting here trying to help you and probably dozens more reading, but you haven't yet posted * any code * even a partial set of your data * a **single question**. /u/Wusuowhey even takes a guess about what you have, and you respond with 'yeah, kinda like that, but sorta different'. Ask a question, post some data (it can even be fake data that looks like yours), or tell us what you want to end up with. Otherwise, you're on your own. 
You are right. To be honest, I am not sure how to format it correctly to post it
How would I rescale the players involved? 
Also with rescale().
Okay. Do you have a suggestion on what I should scale it to? I have never used the scale package before 
Thank you for the response. So instead of listing the weeks by rows I should combine them to fit under 1 row? 
I'm not advanced by any means but yes that's what I would do - you would have 32 rows for each week (32 observations with 4 variables)
I'm not advanced by any means but yes that's what I would do - you would have 32 rows for each week (32 observations with 4 variables)
Np I can understand frustration when starting I'm still early on but yeah simple things often aren't simple ha - those titles are Column headers not rows btw 
Np I can understand frustration when starting I'm still early on but yeah simple things often aren't simple ha - those titles are Column headers not rows btw As I often work with sports data lmk if you run into further issues 
data.table is much less confusing than R and more elegant than dplyr... I wouldn't use base when there's tools to manage more than just this one instance.
[removed]
You need to use [[.
`$name syntax` is not a variable placeholder. When you write `data$input_column_name` it literally searches for a column named `input_column_name`. There are a few solutions. One is to use `[` function for selecting a column like so: data[,input_column_name] Another is use `getElement`: getElement(data, input_column_name)
I don't understand what you are asking.
I am trying to convert a dataframe to XTS. I have my dataframe with the table above and get the following when trying to convert it... ColClasses = c("character", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric") Data &lt;- read.table("c:/users/user/desktop/NQ.csv", sep=",", header=TRUE, colClasses=ColClasses) as.xts(Data) The first two commands go through fine but the 3rd produces the error "Error during wrapup: character string is not in a standard unambiguous format" 
Try this: https://stackoverflow.com/questions/9335917/r-stock-market-data-from-csv-to-xts
so "Data" imports without issue?
Actually, looks like I just got it to work. I imported the data using the following commands: --------------------------------------------------------------------------------------------- ColClasses = c("character", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric") nq &lt;- read.zoo("c:/users/user/desktop/NQ.csv", index.column = 1, sep = ",", header = TRUE, FUN = as.POSIXct, colClasses = ColClasses) as.xts(nq) --------------------------------------------------------------------------------------------- I'll leave the post up in case someone else stumbles upon it. 
glad you got it to work
Use the help function and read about the functions. You will find the answer in less time than it takes to get a response on a reddit post.
idk how to use it
`?t.test`
Your code is almost working as intended, only it's plotting the lengths and rulers over near where the UK would be, which is outside of the frame. I've fiddled a bit with your code to get the text etc visible (from line 97 in your Iceland script): x_label = -48689.6 y_label = 1925876.9 par(mfrow=c(2,3), mai=c(.2,.2,.2,0)) for (i in 1:length(y)) { # i = 1 plot(b, col='lightgray', lwd=2) p &lt;- y[[i]] lines(p, col='red', lwd=3) points(p, pch=20, col='blue', cex=2) bar &lt;- rbind(cbind(x_label, y_label), cbind(x_label, y_label-rulers[i]*1000)) lines(bar, lwd=2) points(bar, pch=20, cex=1.5) text(x_label, mean(bar[,2]), paste(rulers[i], ' km'), cex=1.5) text(x_label, bar[2,2]-50000, paste0('(', nrow(p), ')'), cex=1.25) }
I ended up having to change the x_label value to -260000 and the y_label value to 2225876.9 before I actually saw the text. But I did not know that I needed to adjust those values to show the text, thank you so much for taking the time to go through the code!
[Here is a tutorial on how to find out more about R and it is a lot faster than reddit](http://lmgtfy.com/?q=R+help+function)
Fantastic project! Welcome to the wonderful world of R. ;)
Thank you! 
 dat &lt;- data.frame( a = c("a", "b", "c", NA, "d", "e"), b = c("1", "2", "3", "4", "5", "6")) dat$a[is.na(dat$a)] &lt;- dat$b[is.na(dat$a)] If you're dealing with actual NULL values, please provide a small example dataset so I can see what you're working with.
Yes, actually I have it in csv file, basically I want to do it for the whole specific column and check if the value is empty I want to insert the value from the same row number in other column (like loops)
please provide a small example dataset so I can see what you're working with
First, I think that empty values in a dataframe or matrix are always NA (or an empty space depending on data), so I'm going to assume you have NA values. Let's say your main column (with NAs) is called: "Main" and your second column (with values to be fetched) is "Secondary", in a dataframe called "Df" library(dplyr) Df &lt;- Df %&gt;% mutate(Main = ifelse(is.na(Main), Secondary, Main)) Just a heads up on something that took me way longer than it should to understand: if your columns have different data types this might be problematic, so you might wanna convert them to the same type before (or inside the mutate function, both should be fine). I also recomend converting all factor columns to character columns if you wanna do this to them. 
[data screenshot](https://drive.google.com/open?id=1gtelqXGUnM0-ON4f9XKlp-ozmG1kxhG_) Here’s an example of the data I want to insert text in the empty rows in “endaddress” fill it with other column similar like “startaddress” but diffirent values. 
When you read the data into R, what do those values become? If they become NA, then the code I posted will work. If they are blank strings, then this will work (just replace `a` and `b` with the actual column names): dat$a[dat$a == ""] &lt;- dat$b[dat$a == ""]
Become blanks like this [head data](https://drive.google.com/open?id=1b9Mwx533b8A6Idw1-AEMNRoPiXSdjxOy) 
This is the case when you use vectorised form of if statement - see an example below and help page for ifelse() df &lt;- data.frame(first=c(1,2,3),second=c(4,NA,5)) df$second &lt;- ifelse(is.na(df$second),df$first,df$second) 
ok then try the code I just wrote
I don’t know why the code is not working [wrong code](https://drive.google.com/open?id=1jnwq2J6-Vp_MjEJVFq0OW_q5uGEVPHGI) 
What if it’s blank not NA? Thanks
Thank you I will try it. 
 uberD$endaddress[uberD$endaddress == ""] &lt;- uberD$startaddress[uberD$endaddress == ""] or, to make it clearer: missingIndices &lt;- uberD$startaddress == "" uberD$startaddress[missingIndices] &lt;- uberD$endaddress[missingIndices]
Your columns are coded as factors, might wanna try converting them to character first (like I explained in my anwser, sometimes factors would give me issues): uberD &lt;- apply(uberD, 2, as.character) after they are character,s run the code u/goodygood23 gave you. It looks like it should work
I tried it both doesn’t work. Thank you
what happens when you print `missingIndices`?
False and some of them NA
Sounds like those values are actually NA? What happens when you do: missingIndices &lt;- is.na(uberD$startaddress) uberD$startaddress[missingIndices] &lt;- uberD$endaddress[missingIndices] As others have mentioned, if you get an error about factor levels, try reading in the data with the additional argument `stringsAsFactors = TRUE` if you're using read.table or read.csv.
First one works Thanks
The first argument of ifelse() is any expression that evaluates to TRUE/FALSE. So you can put there for example df$b == "", or df$b == 0 or pretty much any test.
[Kaggle kernels](https://www.kaggle.com/kernels) can have some great advanced analysis. Real data is used to solve real problems by some very smart people. &amp;nbsp; [Github Awesome R](https://github.com/qinwf/awesome-R) has a nice collections of cool things R can do. &amp;nbsp; [Rpubs](http://rpubs.com/) from RStudio has a good collection of R scripts/codes/analysis. 
so just to clarify you expected to input: A,B,C and get 1,2,3? If so just use df &lt;- data.frame(c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j")) lapply(df,function(x){match(x, letters[1:26])})
It's not the numbers specifically I'm looking for, it's any sort of substitution of this type. So it could A,B,C, and spit out Alpha, Bravo, Charlie.
Try this: df &lt;- data.frame(c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j"), c("test", "test2", "3", "4", "5", "6", "7", "8", "9", "10")) res&lt;-data.frame(unlist(lapply(df[,1],function(x){df[match(x,df[,1]),2]}))) 
Yeah so the first part of your df should be: flights_dt &lt;- flights_dt %&gt;% The way I think about it is the pipe has to take the original df, transform it through all your commands after the pipe, then store the result in the object flights_dt. 
My underscores were removed due to formatting. The underscore is not the issue though. Please be advised. 
Won't this modify the original data of flights_df though? 
Yup. What were you looking to do? 
You could call ggplot like this: ggplot(data = flights_dt, aes(x =minutes, y = mean_delay)) + ggline() That will give you a quick plot of the mean delay over each minute. 
Thanks!
I think I must not have explained it well. So in the data frame (or matrix, or whatever is best), there are correlating values. I want to take a string (say, "b a e e b b c") and where the parts of the string match value 1 ("a", "b", etc.), I want it to give value 2 ("alpha", "bravo", etc.). So I want to turn that string to "alpha bravo echo echo bravo bravo charlie", for example. The data frame/matrix would read column 1: "a", "b", "c", "d", "e"; and column 2: "alpha", "bravo", "charlie", "delta", "echo". I hope that makes sense. Sorry if it didn't earlier!
I think I get it now. Try this: mystring&lt;-"a a b" mystringsplit&lt;-strsplit(mystring," ") df &lt;- data.frame(c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j"), c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")) toString(unlist(lapply(mystringsplit,function(x){ df[match(x,df[,1]),2] })))
This works! Thanks! Is the mystringsplit important for separating out the letters? I was thinking of applying this to a larger text where the letters aren't necessarily separated by spaces, but also punctuation. Trying with lapply(mystring) just gives me NA. I'm not entirely sure why. I can follow the logic of the rest of the last line, though, so I really appreciate it! Every day a new function.
When you summarize, you are left with only the columns you grouped by and the columns you create in the summarise call. Try running your first option with everything except the last pipeline and ggplot Call and you will see that they dataframe you pass into the ggplot Call does not have a column called av_delay. 
mystringsplit is just a variable that stores the string as a list (which is what you have to feed into lapply()). You dont necessarily need it but I was trying to avoid doing everything in one line. you can totally get fancier with the strsplit() using things like regex. Let me know if you want more examples. 
I'm always happy to learn more. Re: mystringsplit - I meant is it necessary to split before using lapply(), but your reply now answers that question--indeed it does. I think that was my big issue, I didn't realize you needed to split the string. I had before done this manually with sub(), where instead of the values stored in a matrix or data frame, they were individual variables, something like this: &gt; &gt; alpha &lt;- "alpha" &gt; &gt; beta &lt;- "beta" &gt; &gt; charlie &lt;- "charlie" &gt; &gt; one &lt;- "a" &gt; &gt; two &lt;- "b" &gt; &gt; three &lt;- "c" &gt; &gt; abc &lt;- "a a b" &gt; &gt; abc2 &lt;- sub(one, alpha, abc) &gt; &gt; abc2 &lt;- sub(two, beta, abc2) "abc2" would then output "alpha alpha bravo". I just figured I could automate that process (which, of course, yours does, so again, I really appreciate it!).
glad to help!
Use Anaconda and Spyder (python) with the selenium package to scrape then feed it into R as a data frame.
You can use selenium from within R as well: https://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-basics.html But really OP, you have two options, depending on how tumblr is made: * Is it just html where you load the next page by pressing "next"? If so, the `rvest` package is good I think (or beautifulsoup in python). * If it's more complicated stuff where the content is being loaded by javascript after the page load, you'll want something like selenium
nice...I didn't know that...good info
Can you preproces csv into smaller files (or better some database) before making reports? 
I believe you can install the windows subsystem for Linux if you have windows 10. The R console may be easier as a terminal console, but there are options such as xming for rendering guis from the Linux side. This is super late I know. 
I'm actually trying to strengthen my data science bona fides myself. A friend of mine is pretty active in social justice orgs and does a lot of DS work for them. I asked him just the other day about how to work on my portfolio by helping out nonprofits and he had this to say: &gt; &gt; My suggestion for getting involved in datascience stuff for social causes is to just find a group whose work you like, and do something for them without asking that you think would be useful for them, and then present it to them via email and then put it up somewhere for others to see that you are a "data scientist". Worst case they don't like it/don't use it, but you'll have something to show and you'll learn something. Make them some nice looking plots in ggplot/tableau, etc. Thats a broad answer, but thats honestly how it works out usually. The thing with what it means to be a data scientist is that its soo broad that you can just start doing it. If you don't have an org in mind, you can always look for interesting datasets in a domain you are interested in instead. The trick is that your first project is not going to be your best and it won't pigeonhole you into one "area", its just something to start on and you get better by doing it. &gt; &gt; Contributing to an open source project is also doable ( the main thing is getting comfortable with git ), but its probably not totally necessary ( although getting functional with git is very helpful for any team project ) &gt; Not exactly revolutionary advice, but thought it would apply here. Good luck!
I have ran into similar issues in the past, and have started doing foo %&gt; mutate(...) %&gt;% { gplot(., aes(...)) + geom_bar(...) } to remedy that. 
A SQLite database 
Hmm, I'm not sure. I'm not trying to scrape individual blogs, rather I want to scrape the search function for tumblr. Like for example, let's say I want to collect stuff that's been tagged #goals. With Twitter, I can use R to scrape for all those tweets that have that as a hashtag or a key word, then it makes a data frame in which I can see the text content of all the tweets with that key word in the table. I'd like to do the same thing with everything a tumblr search would return for that same tag. Example [here] (https://www.tumblr.com/search/goals). Ideally I'd like all of those posts to be in a table so I could run some analyses on that. I'm actually a social scientist, not a programmer, so I'm struggling to figure out which method I need :/ Do you know how I would find that out?
[removed]
I think you can use the [`doParallel`](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf) package to do work on windows machines; it would use the `snow` package behind the scenes. I would do something like library(doParallel) library(tidyverse) big_df &lt;- read_csv('2.5big_csv_file.csv') list_of_dfs &lt;- big_df %&gt;% split(.$a_column_to_split_with) registerDoParallel(cores = 4) # use most of your machines cores foreach(i in seq_along(list_of_dfs)) %dopar% { df &lt;- list_of_dfs[[i]] params &lt;- list(param_one = df, other_param = ...) rmarkdown::render('my_templated.Rmd') } I'm sure some of the syntax isn't perfect, but that should be close. Also, just a heads up that parallel commands will help speed but just throwing x cores at a problem won't necessarily make something x times faster.
Total shot in the dark here, but is the library `Hmisc` installed?
Yes it is, at this location: C:\Program Files\R-3.4.3\library 
Can someone help me!
Yeah; I'm having the same problem just now. Seems to be a recent issue.
The scale() function should normalize your discounts
Just to confirm you are running: install.packages("Rcmdr") then getting the error output correct?
I've read your post twice, but I still don't understand what wish to accomplish. Are you trying to understand which factor is the most important to a given customer who purchased multiple items, such as *ProductCategory*, *RawDiscount* (raw difference between list and purchase prices), or *NormalizedDiscount*, which is (*PurchasePrice* - *ListPrice*)/*ListPrice*? I don't understand why you need a composite variable. 
Your example code is neither minimal nor reproducible. Including your code in your question is important, and well done for providing it, but if your code isn't reproducible, as in I can type on my command line exactly what you provided and see your error, then I can't help you. If your code isn't minimal, in the sense that it's the smallest possible code that reproduces the problem, that means you're not done debugging yet, because you could have removed some parts of the code that are dead ends. Until you provide a minimal reprex, it's going to be tricky for anyone to help you. You're relying on the kindness of strangers, so you need to make their lives as easy as possible when they come to help you.
I want to create an index to evaluate a customers overall discount rate in comparision to other customers. For example, if one customer was a bad business partner and just haggled and beat us up non stop over price. This is a customer we would consider dropping 
I think I understand now. When you said "index," originally, I thought you meant it as in a "database index." Now, I assume you actually mean it as some sort of numerical value you can use to rank order customers. Is my assumption correct? Just based on the information you told us you have available, I do not believe you can achieve your goal. The reason being that most businesses reserve the biggest discounts for their highest volume customers. Therefore, if you were looking solely at discount as a proxy for "good/bad" customer, you might penalize your best customers. Without understanding the characteristics of your data, I can't make specific recommendations. For example, 1. Do you have hundreds or millions of customers? 1. What fraction of your customers make multiple purchases, less than 10% or greater than 90%? 1. What is the range/variance in total sales per customer as well as price per item? 1. For example could you say that 90% of customers purchase between $50 and $100 per year, or is the number more like between $50 and $500,000 per year? 1. What is the range/variance in price per item? 1. Similarly, do 90% or more of your items cost between $5 - $25 per item, or is the range more like $5 - $500 per item. 1. Is discount percentage determined more by product type, customer, or interaction of these two? For example, does one customer purchase primarily ketchup while another purchases primarily hotdogs? In this case, if hotdogs are considered a commodity item but ketchup is a premium item, then you're probably not discounting ketchup much, whereas hotdogs could be heavily discounted. In this case, the customer purchasing hotdogs will be getting a higher discount, but not because he's a pain to deal with. If I knew the specific composition of your data, I could give you very tailored recommendations. Without this information, here are some of the general analyses I recommend. 1. **Descriptive Statistics** 1. Max, min, mean, median for 1. total sales per 1. customer 1. per item 1. customer+item combination 1. Total discount per customer: Find the value of SUM(ItemQuantity(ItemListPrice - ItemPurchasePrice)) Once you have the data above, you can design more specific analyses. For example, do some customers get (or not get) a discount on every item that is different from other customers with a specific level of statistical confidence? As an example, imagine a single customer who receives and anomalously high discount on hot dogs and relish. Or, is the discount more of a function between customer and item? For example, does customer A get a higher than average discount on hotdogs while customer B gets a higher than average discount on relish than other customers? Depending upon the composition of your data, you might be limited to simple statistics such as t-tests and ANOVAs. Alternatively, you might have data that is suitable for more sophisticated analyses such as a generalized linear model with fixed and random effects, including interactions.
If I am correct, yes. I installed Rcmdr via the cloud, but it needed Rcmdr misc and Hmisc too (not in version short summer 3.4.2 though)
I guess so, probably it's still in the debug stage, since the newest version is only like.. 20-21 days old. Hopefully for you too the problem will be resolved.
Ill respond more later, since you did such a big write up , i want to respond quickly too. but on my way to bed. A few things: 1. My main goal is to use this compare discount rates of our "Major Accounts". So, our best, highest volume customers. So, its comparing apples to apples here more or less. And i want to see if we have given one customer more of a discount 2. Discounts are done on a product by product basis. So, everytime he buys a product. He negotiates that price. So, a customer can have highly discounted products and paying list for others regardless of his relationship with us 3. All of these customers have high purchase volumes with us. This is our top 3% of customers. 4. Discounts are done based on the interaction at that point of time.... thats why im investigating this... there has been 0 cosnistency in discounting. One customer could get a massive discount on hotdogs, while another customer could get 0. just depending on the sales person who worked that deal/interactions involved. 0 consistency. However, some customers are kown to bad business partners and will negogiate and manipulate to get discounts on a more frequent basis. this is what im investigating 6. Since im comparing discounts for customers with a similar purchase volume, i think creating this index is fair as im not penalizing anyone. Agree? 7. To compensate for the commodity for necessity good issue. I was going to create varying levels of the index. I was going to create a Necessity Good index I was going to create a commodity Good index and then i was going to create an overall. but i still need to scale my products.
'ff' package can be used for big data if I remember correctly.
Getting more RAM is an option, if you can't install any more in your local machine then you could consider cloud computing services such as Amazon, or using a VPS. Alternatively you might be interested in [Revolution R Enterprise](https://www.microsoft.com/en-us/download/details.aspx?id=51204) which is capable of working with data outside of RAM I believe (I've not affiliation to them).
 package:caret And also using streaming io connections or database lookups
Thanks, I'll check out the ff package. 
Thanks, I'll check out Revolution's enterprise setup. RAM is so cheap I'll probably just get more of that in the meantime. 
This is /r/Rlanguage and not engineering heavy/statistically light /r/datascience or /r/MachineLearning. So we're in a space safe enough to suggest sampling. Unless your classes are super imbalanced, you'll likely be fine with sampling. Also chunking data: Like sampling, but more thorough! To elaborate on the "getting more RAM" idea... yes. The new best practice appears to be Docker... basically you package your analyses in Docker and then quickly port it to a bigger instance in the cloud. The concept works well with sampling. Get your analysis to work, move to a bigger machine, and change your sampling ratio to 1. It also helps for reproducibility. This is something I'm still working on integrating into my workflow, but the concept is solid. A good overview is [here](https://towardsdatascience.com/how-docker-can-help-you-become-a-more-effective-data-scientist-7fc048ef91d5). Also Spark on a cluster. It's still in memory, but on many machines... You'll have to rewrite your code though... so my least favorite option unless you've got the hardware regularly handy.
Haha, funny how sampling has become something that gets down voted. I agree that sampling is a good approach but a lot of my analyses require the entire dataset so chunking has been my method of choice so far. I like the Docker idea so I'll definitely look into that as well. As for Spark, it would be nice but my organization doesn't have the money for that kind of thing so I'll put that on my wish list for later. Thanks for the reply. 
Spark.
I don't know if it gets downvoted here, but it definitely does IRL! It's sometimes hard to hear that maybe your data isn't special or that your hard-fought engineering skills aren't necessary. Regarding Docker, I forgot this: https://github.com/rocker-org/rocker
by improve do you mean improve the analysis or improve the aesthetics of the visualized graph? for the latter, check out `ggraph`
How about `read_csv` from `readr`?
Like these? *https://briatte.github.io/ggnet/ *https://www.r-graph-gallery.com/network/ *http://kateto.net/network-visualization
Mostly just the aesthetics. Thanks for the suggestion 
Thanks! 
That isn't actually an error you need to worry about - see here: https://twitter.com/daattali/status/933360836088758273 I see this message a lot, but if the app is working, you can safely ignore them. If the app isn't working properly, are there any other messages in the console? 
If you need help with any of the tutorials or your work PM me, I love helping with this kind of stuff. (Yes I know I'm weird)
Thanks for the idea. I don't see how read_csv will allow me to reach into a single tar.gz file and extract one csv file? foreach with %dopar% allows simultaneous operations. In my case, I can process my 90k files 10-40 at a time. Of course, any CPU or memory operation I do becomes 10-40x as burdensome using this method. If your workload doesn't get broken up that well, it's not going to speed up. 
First, try to just use `read.csv` (and `read_csv`, as u/nobadchainsmokers noted) because it will accept and automatically decompress .gz files. You didn't mention it, but I have to mention: try using `list.files` in combination with `lapply` (or `map` from `purrr`/the `tidyverse`, or `mclapply` from `parallel`). Something like `mclapply(list.files("path/to/files", full.names = TRUE, pattern = "this_one"), function(x) read.csv(x))`. I would be careful using `mclapply` on a high-performance cluster, though, I've found it drops iterations if there are as many threads as cores.
I'd recommend this book: http://r4ds.had.co.nz/
thanks!
There are many great books out there, but if you want to just play around and lern the absolute basics i recommend the swirl-package. just type this in your r-console: &gt; install.packages("swirl") &gt; library(swirl) &gt;swirl()
If you're looking for courses, both DataCamp and Coursera offer tons of stuff for R.
Here's the best way to make your graphs pretty: 1. Do your analysis in R 2. Export to a flat file 3. Import into gephi to tweak the layout and stuff 4. Export to a flat file 5. Read into d3 to actually make a pretty picture (svg)
Here's a quick 6-step guide to starting in R: https://paulvanderlaken.com/2017/10/18/learn-r/ Here's a comprehensive list of books, courses and other learning materials: https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/ All free, of course.
I'm still not seeing the syntax for extracting an archive of multiple csv files within a tar.gz, or how to specify which file if I know the filename . Can you spell it out for me? I totally get a single csv file gzipped, but that's not what I have. Thanks!
datacamp is really good for applied learning
Do one of the projects from kaggle
I couldn't find any R projects on there. edit: never mind, found em. thanks!
Any reason why you can't just unpack it with tar first, then list.files() on the unpacked directory, then read them in?
I found Udemy to be an invaluable tool to learn R. There are a lot of paid and free courses.
If you're a sports fan like me, you can try developing a custom projection system for player statistics using 3 year data samples. This requires data cleaning and joining of separate tables, statistical regression, sorting, creation of new variables, etc.
That's a really good idea actually. Thanks so much!
I've read this 4 times and cannot understand what you are asking. Are the data in different tables and you're looking to combine them? If so look into dplyr and left_join. 
Check the version of your Hmisc package. If it is 4.1-0, roll back to 4.0-3. I had same problem and this fixed it (temporarily). I was told by the author of the Hmisc package that the author of Rcmdr had provided a new version of R commander package to CRAN to address this issue, but I can't find it.
I would like to take the top table (the data I receive) and turn it into the bottom table.
You can recode the two columns using mutate: data.frame(PartNumber = c("Server", 0040204, 0040204,"Cust1", "Cust1", "Cust1"), SC = c("M", "X", "X", "M", "M", "M"), ComponentNumber = c(0040204, 988019802,"CS9881", "M8857", "M8151", "Server"), SC2 = c("X", "B", "B", "B", "B", "M"), QtyPer = c(1, 2, 3, 4, 5, 6)) %&gt;% mutate (PartNumber = dplyr::recode (PartNumber, "40204" = "Server"), SC = dplyr::recode (SC, "X" = "M"))
Hello &amp; good day, how is it possible to install the older version of Hmisc (4.0.3) without "touching" the actual packages related to Rcmdr? Kind Regards
Ohhhh I misunderstood! My bad. I would have to suggest a change to your workflow so that each file is zipped individually, which I imagine is out of scope......good luck! 
Terribly sorry but I am too noob to execute that. Could try it if you could give me directions though...
I used R for my masters thesis, took multiple classes on it, have done multiple independent projects and have applied to over 30 data analyst jobs and haven't gotten anywhere. Good luck to you though!
This works for this small example, but I don't think this will work when scaled up for thousands of parts with tens of thousands of lines of data similar to this. Ideally, I would like the code to automatically go through the first data frame and give me something similar to the second data frame in my example. Any ideas?
Follow the instruction in the link. https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages Just replace the "ggplot2" and the source address with your own: packageurl &lt;- "https://cran.r-project.org/src/contrib/Hmisc_4.0-3.tar.gz" install.packages(packageurl, repos=NULL, type="source")
Not sure if this is helpful, but check out bioinformatics jobs. Super understaffed field right now where R + Linux are the two key skills. Many research groups will be cool with training you on the biology aspect if you're a solid coder.
Rcmdr depends on Hmisc, however they are different packages. You can uninstall Hmisc by simply run "remove packages("Hmisc")" in R console, then install the old version.
Have you thought about taking any MOOCs? I did the Data Science Specialization on Coursera through Johns Hopkins. It was worth the money I paid for it. 
Hi, Thegratercheese Thanks for your advice. I will take a look on it. 
Here is a big list of free courses, books and other stuff: https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/
I owe my R learning to data science with R book, the one written by Headley Wickham.
www.r-bloggers.com
This is the link to the free bookdown version: it is excellent. http://r4ds.had.co.nz Everything by Wickham is great. The free and paid Datacamp R courses are also good. I’d also recommend accessing R-based Kaggle competition solutions, and playing around with the data and code yourself. I find I learn faster when I have to piece together solutions myself that aren’t available as pre-built code. So grabbing a dataset that intrigues you and working through some base R or tidyverse is a great way to spend your time. 
I am currently reading *Discovering Statistics using R*. That's a great book that allows you to speed through content if you're a beginner both in statistics and R (better if you have a very basic grasp of both).
I guess another question is whether or not what I am trying to accomplish is possible?
Assigning it to an object, rather than printing it to a file, will end up with it in RAM and not in your working directory.
[Advanced R](http://adv-r.had.co.nz/) by Hadley Wickham
I don't know how to do that in a function. Although I have tried the following but I believe I have made a logical error : for(i in 1:length(r_list)){ assign(paste("r", r_list[[i]], sep = ""), i) return(r[i]) }
Try seq(10, -10, -1)
10, 11, 12, ..., 998, 999, 1000, ..., 123454, 123455, 123456, ... ..., ..., ..., ..., 519283745, 519283746, ..., ..., ..., ..., ..., ..., ..., -12... -11... -10. Do you see the problem? The '1' means increment the first number (10), by 1 until you get to the second number, -10. Use a negative increment (a 'decrement') instead.
I see, i didn't realize that the argument was increment, i thought it generally implied intervals. Thank you very much! 
Here's the default S3 definition of `seq`: seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)), length.out = NULL, along.with = NULL, ...)
Also note that either of the following options also works: &gt; seq(10, -10, 1) Error in seq.default(10, -10, 1) : wrong sign in 'by' argument &gt; seq(10, -10) [1] 10 9 8 7 6 5 4 3 2 1 0 -1 -2 -3 -4 -5 [17] -6 -7 -8 -9 -10 &gt; seq(10, -10, length.out = 5) [1] 10 5 0 -5 -10 If you leave out the '1', then `seq` will determine an appropriate `by` argument, being `((to - from)/(length.out - 1))`. You can also force the value of length.out to be how many *items* you want, and it will determine the interval for you.
Thank you! This is very useful info 
Quick question, I consider myself a strong R programmer but I guess it could be considered surface level since it's basically just tidyverse and a handful of modeling packages. Is it worth diving into the nitty gritty of R (S3 generics, etc)? Does it help you in your day to day? Fwiw my background is entry level 'data scientist' and all the places I'm interviewing at are using python.
&gt; Fwiw my background is entry level 'data scientist' Probably the most important thing is learning *how* to be a programmer. Not *how* to program, but how to be a **programmer**. Get comfortable looking at manuals, help docs, man pages, and so on. That type of skill will help you in R as much as it will in Python.
I got it! I did the following: a=SplitRas(raster=r,ppside=3,save=F,plot=F) for (i in (1:length(a))){ assign(paste("SplitRas",i,sep=""),a[[i]]) } 
Don't bother learning that stuff until you need it. You will need eventually if you keep working in R, if only because you'll be working with a package that uses S3 or complex environments and something won't work the way you want it to. Then you'll need to learn it. But prior to that, there isn't much point.
I'm not sure I understand the question fully but you might want to look into r markdown. Allows you to embed R code (with/without console) with regular text 
In general, after running the scripts, the console will display each script statement line-by-line and also the script result line-by-line. I hope to get rid of the script statement in display, leaving only result to be shown.
Hi. I removed new version of 'Hmisc' and then installed old version of it. However, during installation of the old 'Hmisc' package, I had error message as follows. I would appreciate it if you could tell us how to handle this error. installation of package ‘/var/folders/xh/2pn7vjcn4717q9b0vyntd3x00000gn/T//RtmpbNm7kq/downloaded_packages/Hmisc_4.0-3.tar.gz’ had non-zero exit status
Like /u/niklz, I suspect R markdown/notebooks/knitr is really what you want, but if you actually need to print in the console for some reason you could create your file then source it. There is a source button in RStudio that will run all of your open file, or you can use the source function. Sourcing the file will only output anything within a print function. So for sourcing to output a data frame, you have to use print(df) rather than just df. But... As far as I know there's no option to just disable the appearance of statements in the console.
I’m not sure that it can if it’s randomly placing values in different columns. Sounds like it’s an issue with how it’s extracted. Where is the data coming from? 
Just run your script through `RScript` in the terminal instead of inside RStudio. RStudio is for *development*, not for running code in production (i.e. running the final code repeatedly).
The data is coming from a report from a legacy mrp system. It organizes all parts in the report in alphabetical order. Although the 00402 number (and what goes into it) does go into the server, the mrp system sees them as 2 completely different parts so it would alphabetize accordingly. With that being said, the company's IT department does a good job at organizing the data from the file that is actually produced from the report that is ran. They run the file through a program called monarch to put the data you see above into columns.
Without an example, you can try echo=FALSE in the code block.
My guess is that you don't have the lib installed which is required to compile the package from source. Have you tried the first method in the instruction (use devtools)? You may need to install the "devtools" package first with "install packages (devtools)" command in the R console. You also need to replace the package name "ggpot2" and version number "0.9.2" with "Hmisc" and "4.0-3".
I think I need selenium, but I'm pretty new to this. At least, I haven't been able to extract what I need with SelectorGadget using RVest. Could you point me to any good tutorials to get started with Selenium? I'm a social scientist so I really didn't understand what that link was trying to have me do.
Isn’t that only in RMarkdown tho?
What is your process of collecting the output? If you're just going to grab the output and save it somewhere, couldn't you just use a post process to edit the resulting text file? Iirc, all command lines will have a leading `&gt;`, and you could just delete those lines. Edit: I mean, using a program to delete lines starting with `&gt;`.
[removed]
The package `lubridate` makes working with dates quite easy. You should simply be able to use the `month`function to extract the month of each of your dates, and use that information to create groups of data. Then summarise each group. Something like this should do: library(dplyr) library(tidyr) #Imaginary dataset df &lt;- data.frame("date" = c(...your dates....), "temperature" = c(...your temperatures...)) df.summary &lt;- df %&gt;% group_by(month(date)) %&gt;% #Group data by months of date summarise("Mean_temp" = mean(temperature)) #Summarise per month df.summary should then be easy to plot EDIT: just to ensure there are no mistakes, you might want to specify how your dates are set up. Lubridate does this by wrapping the date in a function with 3 letters in the name: y = year, m = month, d = day. So, if your dates are as year-month-day, just do `month(ymd(date))`, if they are month-day-year do `month(mdy(date))`, etc. This ensures that the dates are parsed correctly.
I’m on mobile, but look into converting the months into POSIX time. The code will be different depending on how your current month is, but a quick google search should be able to help you. If not, show me a bit of your data and I’ll see if I can help out. I’m currently on mobile right now so can’t do much. Edit: now that I think about it that won’t work if you have time before 1970. I think POSIX time starts then. If your data is separated by months exactly then The easiest option is to create a new column of ints 1 - (how many months). The x-axis values sill be numbers, but I doubt you’d plan on having that many months on the graph anyway. It will still show you the trend and you can still code for max and mins separately.
Do you have any examples of how this has been done?
Do you have any good tutorials to follow especially for a bipartite network graph? Thanks 
xts First create your xts object. Make sure your date index is of class “date” x &lt;- xts(temperatures, date) months &lt;- endpoints(x, on=“months”) agg &lt;- periodapply(x, months, FUN=sum) 
I'm doing something similar but with team statistics. I'm trying to predict which team would win in a matchup, given the teams' performance statistics. I'm trying to turn it into an application where users can log in and use it. :)
If you are planning on calculating a bipartite projection, igraph is great. If you're looking for a bipartite layout algorithm, I bet there's a gephi plugin for that
Here's one I did: http://dmarx.github.io/map-of-reddit-by-active-users/
Not sure about MySQL, but Postgres actually lets you "install" R in the DB and create your own functions using R that run within queries. 
Yes. Load up the dplyr package, use group_by() on the variables you want, and then use summarise(total = sum()). Check out this for learning R http://r4ds.had.co.nz
I haven't used SAS before, but it sounds like you might be interested in the "group_by" and "summarise" functions in R's dplyr library. [This post](https://www.r-bloggers.com/using-r-quickly-calculating-summary-statistics-with-dplyr/) by R-Bloggers explains it pretty well. If you're more visual, [this one](https://www3.nd.edu/~steve/computing_with_data/24_dplyr/dplyr.html) has some sample outputs as well. Once you've got summary statistics, you could just take the sum() of each row, or you could use colsum() to get grand totals: colSums(Filter(is.numeric, dataframeName),na.rm = TRUE) which will give you an output with grand totals for each column.
This package very useful to make various tables. https://cran.r-project.org/web/packages/tables/vignettes/tables.pdf
I’m not sure I understand your question. You want to calculate the subtotal of what for each combination of a factors?
Let's say I have 14 variables, and I want to perform a "group by" 2 variables, for every combinations of 2 variables possibles (var1, var2) (var1, var3) (var2, var3) etc.... (var13, var14)
Either look up dplyr or data.table
I just heard about rApache. It allows you to create web services using R. The only other thing I ran across while doing a quick search was this article. https://www.r-bloggers.com/integrating-php-and-r/amp/
From [Link](https://www.statmethods.net/stats/descriptives.html) Summary Statistics by Group A simple way of generating summary statistics by grouping variable is available in the psych package. library(psych) describe.by(mydata, group,...) The doBy package provides much of the functionality of SAS PROC SUMMARY. It defines the desired table using a model formula and a function. Here is a simple example. library(doBy) summaryBy(mpg + wt ~ cyl + vs, data = mtcars, FUN = function(x) { c(m = mean(x), s = sd(x)) } ) # produces mpg.m wt.m mpg.s wt.s for each # combination of the levels of cyl and vs
Thank you. However I'm looking for a solution that generates automatically all the combinations of "group by".
[removed]
So, for example, let’s use the classic “mtcars” dataset: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html. You might want to find every possible combination of cylinders and gears (ie 4,6, and 8 cylinders in 5-speed and 6-speed configurations) and then calculate an aggregation like the total number of cars that fit each description or the average mpg?
Here is an example of how to summarise the average horsepower and number of observations in every combination of cylinders and gears from the mtcars dataset. library(tidyverse) mtcars %&gt;% split(list(.$cyl, .$gear)) %&gt;% map(summarise, cyl = min(cyl), gear = min(gear), av_mpg = mean(mpg) %&gt;% round(1), num_models = n() ) %&gt;% bind_rows %&gt;% filter(num_models != 0) %&gt;% arrange(-av_mpg)
Here is a more concise version using u/eigenlad solution. library(doBy) summaryBy(mpg ~ cyl + gear, data = mtcars, FUN = c(mean, NROW) ) %&gt;% arrange(-mpg.mean)
Installation of Rcmdr within R 3.4.3 on WinOS seems to work without the Hmisc issue mentioned above. I tested a clean install with the actual versions of R 3.4.3 and Rcmdr from scratch on Win10 and everything was working out-of-the-box however if you are working with MacOSX the following steps might be helpful for you: 1. make sure you have installed the appropriate gfortran-version for your MacOSX: https://gcc.gnu.org/wiki/GFortranBinaries#MacOS 2. install the devtools package install.packages("devtools",dependencies=T,repos='https://cloud.r-project.org/') require(devtools) 3. remove the installed Hmisc package remove.packages("Hmisc") 4. Install the "old" version of Hmisc from scratch packageurl &lt;- "https://cran.r-project.org/src/contrib/Hmisc_4.0-3.tar.gz" install.packages(packageurl, repos=NULL, type="source") 5. Restart R and start R Commander: library(Rcmdr) Hope this helps...
beautiful. This worked well, thanks
thanks for that
Thank you
 library(dplyr) df &lt;- df %&gt;% filter(column_youre_talking_about != 0)
Because R is a functional programming language, you don't usually delete rows. Rather, you write code that will return a copy of the data with the rows removed. You can assign this copy to the same name as the old data, effectively overwriting it. This might seem like a semantic difference now but it will matter later. So, with that said, the way you delete something is like this: data &lt;- fun(data) Where the function `fun` returns a copy with the rows you don't want removed. So, how to define that function? You can do it in base R like this: data &lt;- data[data$undesirable == 1,] But this syntax is super ugly, too much repetition and the dangling comma is horrible. So, some people prefer to use the package dplyr: library(dplyr) data &lt;- filter(data, undesirable == 0) Which they say is easier to read. Btw, this question has been asked literally a million times before. If you couldn't find, say, [this Stack answer](https://stackoverflow.com/questions/1686569/filter-data-frame-rows-by-a-logical-condition), you should work on your googling. Google is a core skill for any programmer. That way you don't have to wait for a reply. If you found it but didn't understand it, that's different.
Am I the only one who likes base R's way of doing this? It's just as long code-wise to do as with dplyr and I can detach one more package.
Just a tip, the condition he set up gives you rows with 0, you want to use != for "not zero" instead
Totally agree!
This super basic use case doesn't really show the strength of the dplyr method. It's mainly useful when you have a long chain of complex operations, perhaps iterating over a list-column to compute new objects based on existing complex values, because that swiftly becomes very unreadable.
You're not alone. I find base R easier for filtering most the time, especially when my column names are dynamic or the number of columns is dynamic. I still haven't figured out how to refer to columns by number in dplyr.
Also, the base `subset` function will give the same results as `filter` here without the need for outside packages.
No. Not at all. I try to do the vast majority of my work in base R. Not as pretty but I strongly prefer and recommend finding a solution in base R whenever possible over adding dependencies/loading packages for one or two small things here and there. It’s all too easy to end up loading 20+ libraries (probably closer to 40+ based on the decencies of the called libraries). 
Could you do this if it wasn't a function?
I'm struggling to (i'm a beginner), I am unfamiliar with combining filtering/dplyr with ggplot.
If you need to plot the proportion of a certain given name, this should do it. I'm assuming proportion is the same as percentage, and that you can load dplyr and ggplot2 beforehand. myfun &lt;- function(x = 'name'){ ukbabynames %&gt;% filter(name == x) %&gt;% ggplot() + geom_line(aes(x = year, y = percentage)) Call the function like this: mufun('Emma') And get a graph like [this](https://imgur.com/a/ofkUh).
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/zF0pBJy.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20drwmbc7) 
Can you post a sample of your dataset
I keep getting an error in filter_impl saying object x is not found. I have tried both writing "ukbabynames" and trying "df". [picture](https://imgur.com/a/DPYY5) 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/KPxEF3g.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
This should be close: df2 &lt;- df %&gt;% group_by(week) %&gt;% summarise(n_holidays = sum(`Type of Day` == 'Holiday')) left_join(df, df2, by = week) %&gt;% mutate(holiday_week = ifelse(n_holidays &gt; 0, TRUE, FALSE))
[script](https://imgur.com/a/DPYY5) and [sample of dataset](https://imgur.com/QVBG6rL) I've started again so I am aware I haven't yet added the column regarding the % of babies having that given name in the given year. Thank you.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/KPxEF3g.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Close the bracket on the function. }
Can you post how you calculated the percentage to begin with for each name for each year?
[removed]
I did the below. Maybe someone can make it cleaner, but it works fine. library(dplyr) library(ggplot2) library(ukbabynames) babynames &lt;- ukbabynames name_proportion &lt;- function(NAME) { babynames %&gt;% group_by(year) %&gt;% summarise(count = sum(n)) %&gt;% left_join(babynames, by = 'year') %&gt;% mutate(proportion = n/count) %&gt;% filter(name == NAME) %&gt;% ggplot(aes(x = year, y = proportion)) + geom_line() + ggtitle(paste("Proportion for the name", NAME, "by year")) } name_proportion("Emma")
Nearly the same way you did, I just multiplied by 100 for percentage. dplyr, group_by, summarise, left_join, and mutate. I mutated the dataset prior to the function since the original poster said he'd already done that part. 
eval(parse(text = paste("x.",v1,sep = ""))) works I think
This is gratuitously complex and error-prone.
get(paste0("x.",v1))
Some points to clarify before answering: `"x.v1"` is **not** a character variable. It’s a character *literal*. A variable in R is a *name* that you can assign a value to. `x.v1` can be a variable. `"x.v1"` never is^(1). What you actually want to achieve is to read a variable’s *value*, given its name as a *character string*. R has a function to do just that: [`get`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/get.html). So you can write: x.v1 = 'test' v1 = 'v1' get(paste0('x.', v1)) … and this will print `[1] "test"`. That said, most of the time you don’t want to write such code — you’d want to use a named vector or list instead: x = c(v1 = 'test', v2 = 'kitty') Now evaluating `x` will print v1 v2 "test" "kitty" And to access a single element you can use subsetting with the name: index = 'v1' x[index] --- ^(1) But R allows a workaround, so that `` `"x.v1"` `` (note the backticks!) *is* a valid variable name.
Your response is indeed much better, I litereally just copy pasted the first anwser from google :). Thank you for your explanation
&gt; `ifelse(n_holidays &gt; 0, TRUE, FALSE)` This can/should be simplified to just `n_holidays &gt; 0`.
Thank you very much, I got it to work very [nicely](https://i.imglnx.com/XljWYp.png). Is there a quick way to make it so I can choose multiple names? Apparently with the use of %in% but I'm not sure how. I'll leave my code so far. myfun &lt;- function(x = 'name'){ d_p %&gt;% filter(name == x) %&gt;% ggplot(aes(x = year, y = p, colour = sex, group = sex)) + labs(x = "Year", y = "Proportion of babies with the given name") + geom_line(color='steelblue', size = 1.5) + geom_point(color = 'steelblue', size = 3) + geom_point(colour = 'black', size = 2) + ggtitle("UK babies' names from 1996 to 2015") + geom_smooth(method = lm, se = FALSE) + scale_x_continuous(breaks = c(1996, 2000, 2005, 2010, 2015), minor_breaks = NULL, limits = NULL) + scale_y_continuous(label = percent) + theme(legend.position = "bottom") } myfun('Adam') 
Thank you very much, I have pasted my code so far on another comment on this thread, but the ggtitle name/pasting code you've done is very much what I was trying to figure out next. Cheers.
Agreed. I don't put much effort into questions from single post accounts.
 library(dplyr) library(ggplot2) library(ukbabynames) babynames &lt;- ukbabynames name_proportion &lt;- function(NAME) { babynames %&gt;% group_by(year) %&gt;% summarise(count = sum(n)) %&gt;% left_join(babynames, by = 'year') %&gt;% mutate(proportion = n/count) %&gt;% filter(name %in% NAME) %&gt;% ggplot(aes(x = year, y = proportion, group = name, color = name)) + geom_line() + ggtitle("Proportion of names by year") } name_proportion(c("James", "William", "Justin"))
I never used xts, I just learned to use the tidyverse `lubridate`. I will assume that your dates are recorded in a column called `date`, in a data.frame called `df`. You can tell `dplyr`to create groups per month and then create a summary of the data, per group (if you have more than one year in `df`you have to group by month and year): library(dplyr) library(lubridate) df_summary &lt;- df %&gt;% group_by(month(date)) %&gt;% summarise("Mean_X" = mean(X)) 
Use `apply.monthly()'. There is an `apply.` function for daily, weekly, monthly, quarterly, and yearly. There's also the more general `period.apply()` in the xts package.
Thank you so much! That is exactly the function I need. Have a happy new year!
Thank you so much! That is exactly the function I need. Have a happy new year!
You are my hero :D!!!
There's not a way to make a (worthwhile) graph out of a single number. 
That's perfect, thank you so much. I don't suppose I could ask for your brief help on the same dataset but a slightly different problem could I? I need to make something similar to [this](https://i.imglnx.com/gsbcCb.png) with the same [dataset](https://imgur.com/QVBG6rL). Whenever I try to count how many unique observations there are, I seem to get a homogeneous number across all years. Thanks in advance.
Sorry, I should've stated that it is a graph to depict how the frequency changes over some years.
can you show your data frame? with a head(data) if you just want to see how the number of case change over years yearTable &lt;- table(data$year)) plot(yearTable)
I'm making some big assumptions here, namely in the format of your data frame: variables: Year, First, Second values: 2014, A, B values: 2014, A, A values: 2014, A, B values: 2015, C, A values: 2015, B, B So, you've got 2 unique observations in 2014 and 2 unique observations in 2015. We'll call your data frame df: df &lt;- distinct(df) #For keeping only unique observations df &lt;- df %&gt;% group_by(year) %&gt;% tally() So, unique observations first, then a tally of remaining observations by year (you know they're all unique by the second line, so you just need to count the number of times each year appears). Is that what you're asking? The resulting data set will have 2 variables, year and n. You can then use ggplot with bars or lines if that's what you're going for. ggplot(df, aes(y=n, x=factor(year))) + geom_bar(stat = "identity") 
Thanks for the time put into your reply. My dataset looks like [this](https://i.imglnx.com/Aj4BZ5.png) and so male/female is another variable in this, I need it to looks something like [this](https://i.imglnx.com/gsbcCb.png). I tried this below, &gt; df &lt;- distinct(df) #For keeping only unique observations and it seemed to yield what looks like the correct amount of observations, but the grouping and tally didn't work. 
Come on man! Just tell us you're using the ukbabynames dataset. That would make this so much easier. `df %&gt;% group_by(year, sex) %&gt;% summarise(distinct = n_distinct(name)` Then use ggplot2 from there. So is this for a class, or are you asking from a different account now? There have been a few questions about this dataset in the past couple of days.
Sorry i get mixed up with having two accounts. That post is more specifically for the entropy help which i am struggling with currently. It's for a class but it isn't a summative assessment.
How are you collecting the data? Can you organize it to make male/female a category column? Then you could do a number of plots with an easy way to separate them. I would think it would be easy to transpose them.
Yeah, you need to group by multiple variables so the tally won't work the same way. If you just grouped by year it definitely should have worked. One of the most common little combinations I use on a daily basis. What you can do, edited now that I've seen your data: df &lt;- df %&gt;% group_by(year, sex) %&gt;% count() ggplot(df, aes(y=nn, x=year, color=sex)) + geom_line() + geom_point() I'll let you pretty it up and reposition the legend and change the line types and such, but that is the base graphic you're looking for.
 babynames_count &lt;- babynames %&gt;% group_by(year, sex) %&gt;% summarise(total_names = n_distinct(name)) ggplot(data = babynames_count, aes(x = year, y = total_names, color = sex)) + geom_line(aes(linetype = sex, color = sex)) + geom_point(aes(color = sex)) + ylab("Number of Different Names") + labs(title = "Number of different babies' names in the UK, 1996-2015", subtitle = "Data from the R package UK babynames") + theme(legend.position = "bottom") + scale_x_continuous(limits = c(1995,2015))
You are welcome :)
Bless your kind soul. Thank you. And yes I will keep that in mind, also be sure to let me know if you can code in C# using Visual Studio.
Try: table(your$data)
Never heard of Statgraphics before, could it be down to differences in [floating point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic) and lack of precision? [This](https://stat.ethz.ch/R-manual/R-devel/library/base/html/double.html) has some information on precision types in R (search around for more, and also check what Statgraphics does/uses). 
Installed R in June so not that old... not at work so I can't tell you what exact version Windows 10 Unsure of kernel/dist
I’m interested. Can you share how the output of a single variable should look? I’m picturing that the reason you want to do this is to get a sense of range, distribution, na’s etc for each variable?
Do you mean every column in a data frame, or multiple data frames?
Thank you very much for the tutorial. I am a experienced R analyst who knows nothing about databases, but will be trained soon in the future. A question, why postgree? Is it open source? What is the difference to regular sql?
Guessing you're on Windows, and the package requires compilation. Download RTools and retry (these are the tools like gcc required to compile packages).
Yes, postgres is open source. But there are plenty different open source databases (e.g. Mysql, sqlite, clickhouse). Sql is not a database, it's a standardised query language. But most databases support it. All others are known as "nosql databases".
as /u/kazi1 said try installing rtools
Try `lapply(mtcars, table)`. Internally, a data frame is a list of columns, so most of the *apply type functions will loop through the columns for you.
The biggest reason I prefer postgresql is postgis extension [\(https://postgis.net/\)](https://postgis.net/) for spatial data. The extension makes it really easy to manage spatial data. For example, we can store a table of cities with a spatial geometry column and do spatial queries with sql like selecting all cities within a state, all cities within a distance from a location etc.
Thanks, good to know.
I'll give that a try although it's a huge pain because I have to get IT to approve those types of downloads. I think I used tidyverse like... less than 2 months ago and my version of R is pretty new. 
That's what ive been using but normally formatting like as.matrix does nothing to improve readability and I have 300 bars and .5 mill obsv so I need it to be readable lol
I'm guessing Hour variable (and others) are part of a data frame? In that case, try my_dataframe[my_dataframe$Hour==10,] If the location is the name of the variable in the dataframe, just add $location to that
If you have that many, perhaps you should consider a plot instead, or selecting a few key statistics to skim through. Here's an example of a bunch of histograms with the number of NAs in the titles library(tidyverse); library(glue) gather(mtcars) %&gt;% # calculate NA group_by(key) %&gt;% mutate(n_NA = sum(is.na(value))) %&gt;% ungroup() %&gt;% mutate(key = glue("{key}: {n_NA} NA")) %&gt;% # plot ggplot(aes(value)) + geom_histogram(bins = 20) + facet_wrap(~key, scales = "free") 
I mean there are prob 60 variables I will need to look at on a routine basis as I clean, but I really do need tabs, not just NAs. 
Is each variable a factor?
Most of them. A few are numeric. The factors all have different response options
I just had this problem today. I uninstalled R &amp; RStudio, deleted all the folders, and reinstalled everything and it worked flawlessly. If you do this, backup your data!
I think you are looking for summary(your_dataframe). If you want different metrics to summarise, try: df %&gt;% summarise_all(median) (or whatever function you want to use for your metric) If you want to get real fancy and apply a bunch of customer summarising functions, try this: # Build list of functions and addition arguments list( NA.n = function(x) sum(is.na(x)), NA.percent = function(x) sum(is.na(x))/length(x), unique.n = function(x) n_distinct(x) ) %&gt;% #apply each function to each variable in your dataset to sumamrise it sapply(function(fn) your_dataframe %&gt;% summarise_all(fn))
I'm not sure if a function like the one you requested exists. I think the easiest path would be to simply correct the data so that capitalization is consistent. If you think that lower case would not look good, why not make all first letters uppercase? You say you have many columns but you can use an apply function so you don't have to go over the columns one by one. ---The method below will not work if your columns are in factor format! Let's start by making a function that capitalizes the first leter of a string: capitalize &lt;- function(string){ #Take a string and capitalize first letter paste0(toupper(substring(string,1,1)), substring(string,2)) } Now, let's say your data.frame is called df. We want to apply this function to all your columns: capital.df &lt;- as.data.frame( apply(array = df, margin = 2, FUN = capitalize), stringsAsFactors = FALSE) The apply function returns a list after applying a function to your array (in this case a data frame) over the determined margin (in this case, 2 means apply to columns). That list was then reconverted into a data frame (with character vectors rather than factors, which can sometimes cause issues). 
I know the pain, trust me, I know your pain.
&gt; This is why I'm simply looking for a way to create a frequency chart that ignores letter casing. Think about it: what would the labels in the chart be? Since you have multiple spellings, you need to pick one as the canonical one. `tolower` accomplishes this, albeit maybe not in the best way (it’s also [fairly nontrivial once you consider Unicode](https://blog.codinghorror.com/whats-wrong-with-turkey/), but that’s just an aside). /u/Cronormo’s answer is an alternative, slightly more sophisticated way of picking the canonical spelling. An even more sophisticated solution might, for instance, use a dictionary of preferred spellings, and a set of rules to derive alternatives. But whichever way you choose, you need to perform the transformation of your labels to the canonical form; a case-insensitive compare isn’t sufficient.
&gt;Think about it: what would the labels in the chart be? Since you have multiple spellings, you need to pick one as the canonical one. I understand. I thought pivot tables in Excel were case insensitive which is what I was looking for (wasn't sure how it chose the label); however, now I think I tried that on a table that I had already formatted. I thought about changing them to lower, then making the first capital again which the other poster mentioned. I may try this.
Thanks, something like this was more of a backup idea that I had, but I may give it a shot.
This is pretty far over my head,so I'm going to ask some stupid questions. You're saying the files are in the computer? /S But seriously, what do you mean by headless machines? Like a VM? Also was is vim?
It's interesting to see it working in vanilla vim but why not use https://github.com/jalvesaq/Nvim-R ? It works with the built in terminal in tmux and can open a new terminal if you use it locally in Linux/Mac while having much more advance features 
Yes this is vim (text editor) running under screen (terminal window manager). I edit text files with vim and with this setup I can press Ctrl-L in vim to send the line from my text file to a R console which is running under the same screen session. The mappings (the two lines of code) needs to be added to a configuration file which vim uses - ".vimrc" which is usually located in the home folder (~/). Once this is added and vim is restarted the shortcuts will work. Note that the text is sent to a R which is running in the same screen session. So we need to start screen first, open an R instance, then open another window within the same screen session with vim to be able to pass commands between them. By headless machines I mean the servers and high performance computers at university which I can access via network only through a terminal (ssh). These are really powerful machines (some have ram of 1TB!!!) made for specific purposes (database, storage of data, high memory, high processing etc.), run on linux and usually dont have any GUI (xserver) on them so the need for terminal based solution.
Thanks! nvim looks awesome! will definitely check it out 
The author also has a nice R package called colorout to colorize the outputs of the R shell. Goes well with it
Maybe the newish [`skimr`](https://cran.r-project.org/web/packages/skimr/vignettes/Using_skimr.html) would be useful
Thanks i sussed it out, it seems that it was not to dissimilar to Matlab after all! Logical indexing i believe is the term....
Can someone please tell me the advantages of programming on a pain text editor such as VIM as oppsed to a IDE like RStudio.
Some large companies have bylaws that essentially disallow the use of free/unsupported software. So if such a company wanted to use R, they would purchase the commercial edition.
RStudio's commercial license comes with support that the Open Source Edition doesn't, so companies that want that would go for it. It's the same for techs like MySQL, where the companies that need support subsidize the rest of us to use the technology for free. Also, if a company doesn't want to share its modified source code (RStudio free is licensed under AGPL), then they would want to pay the premium.
My immediate thought is that your word variable is converted to a factor. That happens for example when you load CSV files without stringAsFactors=FALSE, same every time you convert to data.frame.
Sounds good, thanks!
you can use ":" to include interaction terms in your linear regression. for example, X1:X2 for interaction terms between X1 and X2. use "*" to include both noninteraction terms and interaction terms. for example, X1*X2 includes terms for X1 , X2 , and interaction terms between X1 and X2
StringsAsFactors is my arch nemesis. 
But good for the development of RStudio
Read up on factors in R. You can re-set the levels as they may be assigned alphabetically. I tend to read them in as characters and then assign the levels myself. var &lt;- as.factor (var, levels = c("Level 1", "Level 2", etc))
Just to go behind the scenes; read.csv, read.table, data.frame, etc. by default converts characters to factors (unless stringsAsFactors is set to FALSE explicitly or via options). A factor vector in R is actually an integer vector, where each unique value is associated a label. When viewing the vector, you see the labels and think it's just a character vector. But when you start to perform operations on it, integer operations and comparisons work with the integer part, and character operations and comparisons work with the character part. It mostly works okay, untill you have integers encoded as characters,and then nothing works as you would expect.
I’d agree except I think the goal should be to add those columns together to produce the result.
rnorm and runif and probably what you’re looking for.
There is a bookdown R package which leverages markdown syntax for fast writing, allows R code to be included in the source (similar to sweave). The resultant output is html which means you can use any htmlwidgets in your source. ( see r htmlwidgets gallery if you aren't familiar). And in theory you should be able to leverage shiny input output binding directly too. 
Another vote for bookdown. Check out Hadley wickhams R for Data Science online e book to see a result. 
You can create a vector or 10,000 observations with the rnornm() and runif() functions. The default arguments for runif() are 0 and 1, so it already meets the conditions of [0,1]. Here, I assign the results of each call to the variables x and y as described in the problem. I also wrap them in parentheses to print the result of the call to the console. (x &lt;- rnorm(10000)) (y &lt;- runif(10000)) When you add two vectors to each other in R, it "vectorizes" the function, meaning that the elements at each index are added to the elements at the matching index of each vector (as long as the vectors are the same length). This produces a vector of 10,000 observations that are the result of a random number between 0 and 1 being added to a random number from a normal distribution with mean 0 and sd of 1. I assign the result to a variable called "solution" and wrap it in parentheses to print. (solution &lt;- x + y) Finally, I call x, y, and the solution vectors in the hist() function to view a histogram of the distribution to see what happened. hist(x) hist(y) hist(solution)
You need to install X11 from XQuartz. That should solve your issues. Can download it here: https://www.xquartz.org/index.html
Since you're knew, I'd recommend starting with a package called data.table (for data manipulation), it'll make things elegant and simple once you get the syntax down (it's easier than it sounds). To answer your question, I'd do this. I first saved the spreadsheet as a csv, and got to the directory I needed it etc.... Then: (using "poldat" as a shorthand for "Primary2016") library(data.table) library(ggplot) poldat &lt;- fread("Dem2016Results - Sheet1.csv") poldat[,Sanders := as.numeric(gsub(",","",Sanders))] poldat[,Clinton := as.numeric(gsub(",","",Clinton))] poldat[,SandersPercent := Sanders * 100 / (Sanders + Clinton)] ggplot() + geom_col(data = poldat[,.(SandersPercent =mean(SandersPercent)),Type],aes(x = Type,y = SandersPercent)) Please note that I had to gsub and as.numeric because doing the csv thing brought in the commas that are in the spreadsheet (the numbers were saved in the file as character strings, very bad, but not insurmountable). Is this kind of what you were looking for? 
I'm pretty geom_col is the same as geom_bar(stat = "identity") so it wants single numbers and you need to average your percents first: library(tidyverse) Primary2016 %&gt;% mutate(SandersPercent = Sanders/(Sanders + Clinton) * 100) %&gt;% group_by(Type) %&gt;% summarize(meanSP = mean(SandersPercent)) %&gt;% ggplot(aes(x = Type, y = meanSP)) + geom_col() &amp;nbsp; Try that, I'm relatively new to R and may have made a mistake in there. Also, your data is already well organized for ggplot and depending on what this is for, it may be more useful to show all the points so you can easily see the variance as well: Primary2016 %&gt;% mutate(SandersPercent = Sanders/(Sanders + Clinton) * 100) %&gt;% ggplot(aes(x = Type, y = SandersPercent)) + geom_point()
That's part of the data.table package, you need to: install.packages("data.table") then: library(data.table)
Thanks so much, this works very well! I don't know tidyverse yet; is there any chance you know how to achieve the same result without using its notation? 
[This is what I get with that code](https://imgur.com/a/hUKJz).
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/khv3Fcp.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dscjepc) 
Your original attempt was very close. You had already calculated the proportions for each and put them into separate dataframes. Generally, a ggplot2 bar plot by default is going to show counts of rows for each factor level, not averages. You need to calculate the averages yourself, put the averages and types in a dataframe, and tell ggplot to use these averages for your y-axis (stat = "identity" in the geom call). To piggyback off your original code and not use tidyverse functions you could do something like this: Primary2016BarPlotDF &lt;- data.frame(Type = factor(c("Primary", "Caucus"), levels = c("Primary", "Caucus")), MeanSP = c(mean(Primaries$SandersPercent), mean(Caucuses$SandersPercent))) ggplot(Primary2016BarPlotDF, aes(x = Type, y = MeanSP)) + geom_col(stat = "identity") Giving ggplot a single dataframe can also make it easier for adding extras like color. This site has some similar examples if you want to see what else you can do to make it look nice and was useful for me when I was starting out: http://www.cookbook-r.com/Graphs/Bar_and_line_graphs_(ggplot2)/ 
thanks!
thanks!
Bookdown seems awesome, I will look into it. Thank you!
No worries
Thank you 👌🏻
Thank you so much. My "Rcmdr" works now!! 
Can you give a sample of what you're looking at? If you have date in the data, it would be really easy, just not intuitive.
depends on how your data is structured, as /u/RShinra said, but here's a basic approach: library(dplyr) rankCalls &lt;- calls %&gt;% # assumes you have a data frame called `calls` group_by(callID) %&gt;% # e.g. phone number, or pair of phone numbers mutate(order = rank(date)) # assumes you have a date column in a format that can be processed by rank() If you post what your data actually looks like, we could help more specifically. 
This is straightforward to do using dplyr and tidyr, so I made up a reproducible example for you: dec17 &lt;- runif(10, 1, 10) nov17 &lt;- runif(10, 1, 10) oct17 &lt;- runif(10, 1, 10) sept17 &lt;- runif(10, 1, 10) VPN &lt;- c(rep("00AY240-AO", 5), rep("00AY764-AO", 5)) example_df &lt;- data_frame(VPN,dec17,nov17,oct17,sept17) &gt; print(example_df) # A tibble: 10 x 5 VPN dec17 nov17 oct17 sept17 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 00AY240-AO 2.313332 9.792752 1.903567 3.491741 2 00AY240-AO 7.020411 3.073641 9.584933 7.769014 3 00AY240-AO 4.448911 6.208491 4.266804 8.050817 4 00AY240-AO 5.569162 7.089413 8.824844 9.889802 5 00AY240-AO 7.198577 3.622552 8.441254 2.573960 6 00AY764-AO 8.702897 3.462453 7.134576 3.048879 7 00AY764-AO 6.582543 5.290662 2.718703 1.690875 8 00AY764-AO 5.815277 9.658812 5.895519 4.923937 9 00AY764-AO 7.649297 1.705913 8.023125 7.247472 10 00AY764-AO 8.833116 6.520910 7.201441 9.500218 You want to gather up the data, then group the data appropriately, and then summarize it by sum(). sum_df &lt;- example_df %&gt;% gather(month, "amount", 2:5) %&gt;% # Gather the data into three columns, VPN, month, and amount group_by(VPN, month) %&gt;% # Group the data by VPN and month summarize(total = sum(amount)) # Find the sum of "amount" for every combination of VPN and month The first gather() command transforms the dataframe to tidy data, like so: # A tibble: 40 x 3 VPN month amount &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 00AY240-AO dec17 6.530109 2 00AY240-AO dec17 2.032592 3 00AY240-AO dec17 5.067576 4 00AY240-AO dec17 3.486587 5 00AY240-AO dec17 1.374431 6 00AY764-AO dec17 4.167567 7 00AY764-AO dec17 8.642220 8 00AY764-AO dec17 9.378445 Then the group_by() and the summarize() functions perform actions on each combination of VPN and month, yielding a dataframe like this: &gt; print(sum_df) # A tibble: 8 x 3 # Groups: VPN [?] VPN month total &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 00AY240-AO dec17 18.49130 2 00AY240-AO nov17 28.77193 3 00AY240-AO oct17 32.94592 4 00AY240-AO sept17 24.74616 5 00AY764-AO dec17 27.14787 6 00AY764-AO nov17 19.91436 7 00AY764-AO oct17 28.14408 8 00AY764-AO sept17 26.88951 Where "total" is the sum of all the rows with the same values of VPN and month.
I have date and phone #. 
This should give you a decent start, though you can play around with the theme elements to make it look more like that chart: library(ggplot2) df &lt;- data.frame( x = c(18.76,21.31,20.60,21.79,19.36,22.31,20.08,23.53, 19.39,20.65,18.89,20.38,22.95,22.97,19.25,21.76, 22.06,19.87,18.06,21.19)) ggplot(df, aes(x = x, y = 0)) + geom_point(shape = 1, color = "darkblue", size = 4) + scale_x_continuous(limits = c(17,25), breaks = seq(17, 25, 2)) + theme(panel.background = element_blank(), axis.line.x.bottom = element_line(color = "black"), axis.text.y = element_blank(), axis.title = element_blank(), axis.ticks.y = element_blank() )
Maybe this will get you started: n = 20 v0 1:n sixes = n %/% 6 leftovers = n %% 6 flipper = c(1,1,1,-1,-1,-1) sign_change = rep(x = flipper, times = sixes) if(leftovers != 0){ sign_change = c(sign_change, flipper[1:leftovers]) } v1 = v0*sign_change I'm sure there's a much slicker way to do this but I'm a noob too :)
Wow, thanks. I'll have to mess around with this a bit! It looks good, I just wish there were a simple way to remove the excessive padding (between the number line and the points).
You can do this in base R also. It could be simpler but it isn't that hard. It involves setting the y axis to internal ("i") interval calculation under par(), setting the y limit of the plot to go from 0 to any number &gt; 0, letting the points extend past the graphing region (xpd), and turning off both axes. The last command turns only the x axis back on. x &lt;- c(.10,.5,.45,.62,.72,.31) y &lt;- c(0,0,0,0,0,0) par(yaxs='i') plot(x,y,xlim=c(0,1),ylim = c(0,1),xpd=NA,axes=F,ann=F) axis(1)
I'm not sure if this helps because I can't use the dataset (I'm also no expert by any means;) however, does using the aesthetic in geom_point() work? So: geom_point(aes(size = votes_cast_in_thousands)) I'm uncertain if this would help. If it does, you should probably put all modifying aesthetics within the geom_point().
&gt;Secondly, I would like to more definition on the plots, as it has mushed into a mess. With overlapping points it often helps to add transparency, e.g.: `geom_point(alpha = 0.4)` http://ggplot2.tidyverse.org/reference/geom_point.html
The boxes for size are not boxes, but lines whose size are determined by number of voters. Instead of `guide = 'legend'`, use `guide=guide_legend(override.aes=list(pch=19))` (or some variation thereof). Also, your linear regression does not take the number of voters into account. A district with 5,000 is waited equally with a district with 500,000 voters (if these exists). This does introduce some bias or skrewness into your results. At least compare with a model than uses the number of votes as a weight, see https://stats.stackexchange.com/questions/7513/how-to-use-weights-in-function-lm-in-r
Thanks for your answer. &gt; geom_point(aes(size = votes_cast_in_thousands)) When i added this in, caeteris paribus, the graph didn't change unfortunately.
&gt; geom_point(alpha = 0.4) That definitely somewhat improved it, thanks. Would you suggest changing the lm line colour? I tried but it wouldn't work. I think it's okay now though with the transparency points.
Sorry about this throwaway comment. I'll look into this more after I work for a bit. I'll upvote for visibility.
Okay thanks. And yeah i agree regarding Lm actually, but I was required to show the linear relationship. Would this be a quick fix to weight it? Or would I have to mess around with making new columns for example?
I had to run sudo R to get it to run on my Linux machine at home. My work computer (windows) I had to point the install location to a folder that was in my user folder, not in C:\Program Files. 
For weights in ggplot2, see https://stackoverflow.com/questions/21996403/how-to-plot-weighted-loess-smoothing-in-ggplot2 
Would you be able to go a little more into detail on this or show your code? I honestly don't know how to work with what you're saying, although I can tell you that it is installing into my User folder under C://Users/User/Documents/R/R-3.2.3/library
This is a quality response. Thank you because I learned something too.
So you're not running Linux, you can ignore the first part. The second part is the advice that I gave you, but it seems like that doesn't help. I did a quick bit of google-fu and found [this](https://stackoverflow.com/questions/31249980/error-in-r-package-which-is-only-available-in-source-form-and-may-need-compil#33565333) on stack overflow. Basically, a user suggests that there are issues with the CRAN mirror that is being used. If you would be so kind as to run the R gui (Instead of RStudio) and send the command &gt; install.packages("tidyverse") into the console window, select the mirror closest to you, and tell me if it either worked or what the output was, I would be absolutely ecstatic. 
Do you just get the generic 'there is no package called tidyverse' error message when you try library(tidyverse) after your install attempt, or is it something more informative? In any case, I'm not sure what the issue might be there, but you don't need the full tidyverse metapackage to use ggplot. Just run install.packages('ggplot2') and you should be good to go. If there are other tidyverse packages you need, you can also install them individually. 
Yeah I get the "no package" thing. But I may be able to work with what you said, assuming the data packages the tutorial site I was planning on using works, otherwise I guess I can just find a different tutorial. Thanks!
I am kind of confused on what you are trying to accomplish. The transpose function is very simple. Is there a reason you do not want to use it? Does it not work with something you are trying to do? Alternately, you could change your original function to: data2 &lt;- replicate(10, rnorm(20, 10, 1)) The averages should still be the same with this function even though they are calculated based on rows. If it's data you're importing it wouldn't work, though.
Try this: install.packages("tidyverse", dependencies = TRUE)
I want to generate 20 populations of 10 values that are to be placed in matrix rows, i.e.: `data &lt;- t(replicate(20, rnorm(10,10,1))`Using t() is fine in this specific example, but should I decide to make a big matrix instead (e.g. 1,000,000x1,000), I suspect transposing will make things unnecessarily slower. `data2 &lt;- replicate(10, rnorm(20, 10, 1))` works but I'm wondering if there is an alternative way to do this, e.g. with rbind(). Similarly, in the second example, I'd like to sample rows and place the samples into rows rather than columns all while avoiding additional transposing. It seems that with matrix producing functions, R tends to treat vectors as column vectors rather than row vectors. Maybe it has something to do with promotion of vectors to matrices. In that case I'm curious if it's possible to change over which index this promotion occurs. PS The reason I prefer rows instead of columns is because I'm used to dataframes.
Okay that makes sense. I am NOT an R expert. I am in the process of learning the language as well. Are you familiar with tidyverse? There are two functions that may be exactly what you're looking for. gather() spread() I think these functions will do what you want them to do, but it will take some tweaking on your dataframe you use. Tidyverse is much quicker than base R, so it might be worth it to transform your data into a workable tibble. Here are some examples of how you can make it work. If you can organize your dataframe into a tibble you should be able to rearrange your data however you want with transposing, adding/removing columns, in multiple ways. data &lt;- replicate(1, rnorm(10,10,1)) colnames(data) &lt;- c("Var1") data &lt;- as.tibble(data) data %&gt;% mutate(Population = c("Austin", "Houston", "Dallas", "San Antonio", "New York", "San Francisco", "Atlanta", "Boston", "Phoenix", "Denver")) %&gt;% spread(key = Population, value = Var1) I learned this in R For Data Science, which is available online if you want to read up on it. You will need the library(tidyverse) in order to use it.
An even faster data type sounds really interesting! I'll take a look, thanks!
I'll give it an attempt
Check out [rasterize](https://www.rdocumentation.org/packages/raster/versions/2.6-7/topics/rasterize) 
Wow, thanks i'am looking for something like this for ages. 
There is no *one* sort function. Depending on the data and use-case, there are a variety that get called internally via `base::sort.int`. In the end, they are all C functions: `do_qsort`, `do_psort`, `do_sort` and `do_order` (found in `src/main/[q]sort.c`). Comparing the raw performance with Haskell is tricky since Haskell’s default data structure is the (non-contiguous) linked list, whereas R uses contiguous vectors. They are bound to be faster in this particular scenario.
I used the fastest sort functions available in Haskell that operate on unboxed MVectors.
Ah, fair enough. In that case you can [browse R’s implementation on GitHub](https://github.com/wch/r-source) to check for details. The implementation of sorting is unfortunately fairly messy and appears to include lots of tuning. It doesn’t make for easy reading.
your explanation is really useful even if it isn't exactly what i was looking to do. I did end up figuring out the aggregate function! I may not have explained what i wanted as precisely as i should have, i'll be filing this away though because it'll definitely help me later! And i'm upvoting you for both the sheer amount of pedagogical expertise and time you must have put in. thank you and keep being awesome!
https://www.reddit.com/r/rstats/comments/7pq61u/1_help_i_have_a_line_graph_and_want_to_have_the/
They're both public. I can get them downloaded into R itself but not R Studio
you can use a package called lubridate. A very handy package when it comes to dates. 
It seems like this might work, but for the rolling statistics I'm using 336 rows of data, so I would still need the loop to use the last 335 rows of previous data, then 334, etc. as additional new data files are found.
Maybe check out the packages that do spline fits, much more general than polynomials. Another idea is to try rational interpolation. Supposed to be very robust.
I'm always forgetting this "feature". Check out tibbles, a more sensible version of data.frames. Using them, this will never happen again. 
Hell, I'm impressed you got R talking to Oracle at all. The few times I tried I failed miserably. 
Really? First link description has answer. http://lmgtfy.com/?q=r+subset+dataframe+by+column+value 
I've read that netlogo or python even Julia are best suitable for agent based modeling 
IME, it works fine but not great. You start to run into issues with the lack of software engineering tools available, performance and very weak graphical capabilities if you try to turn it into something serious. For experimenting / prototyping, netlogo is really simple and fast. If you actually want to do large simulations / use the simulation in production, it's better to switch to a language with better tooling. Python works well, but almost any mainstream programming language will do fine. Most of the work will be the same across languages; it really comes down to thinking scientifically and following sane development practices within a good architecture. For resources: I'd consider picking up an introductory game development book for your language of choice. There's a ton of overlap and it'll accelerate things nicely.
This is a pretty cool idea and something I've been thinking about doing for a while now. Popular wisdom seems to be "spin up an AWS instance" if you need more computing power but I could never find something that would do everything from spinning up the instance, installing the packages, loading the dataset, and computing the analysis all in one go. Docker may be what I have been looking for.
a) all combinations is probably unrealistic to handle, alone for the last case you have 9x10^13 combinations... b) how far is "not very far", do you have a solution if there was only one colour? If yes, split the problem into: selecting the positions of all tickets and then which ticket has which colour.
Try installing the plogr package
Throwaway account and a sub par description of what you want, hmmm, good luck getting answers! You might have better luck if you improve your ability to describe the problem, or do something much better - ie, *showing* us what you have, and also *showing* us what you want. 
People here will have an easier time helping you out if you provide some additional details about what your data is and what you're trying to do. It will be even easier for us to help out if you provide some example data so others can reproduce your error. To start with: what is `m` and what is `dataset` in your code? If I were you I'd check what sort of output you get from `predict(m, dataset[, -14])` when you run that command on its own. Does the output from that command have the same length as `dataset[,14]`? If it does not, then that is why `table()` throws the error you get: `table()` expects that the output of `predict(m, dataset[, -14])` and `dataset[,14]` have the same length.
here "m" classify navieBayes algorithm m &lt;- naiveBayes(dataset[,-14], dataset[,14]) table(predict(m, dataset[,-14]), dataset[,14]) 
here "m" classify navieBayes algorithm m &lt;- naiveBayes(dataset[,-14], dataset[,14]) table(predict(m, dataset[,-14]), dataset[,14]) 
m &lt;- naiveBayes(dataset[,-14], dataset[,14]) m class(m) #pred &lt;-levels( pred&lt;- predict(m,dataset[,-14],type="raw") pred table(pred,dataset$class)
I don't have the answer here, since this is a tough problem. I imagine iteratively setting up contrasts for regression to be a primary problem here given the "explicit" way that they are set up in R (ie using the column names, outside of quotation marks). This particular issue might be solved in Stackexchange. Good luck
not sure if this is what you're asking -- specifically I'm not sure what you mean by 'Attribute' -- but if you have a column `x` in a data frame `df`, `mean(is.na(df$x))` will give you the proportion of NAs or NaNs in the column. In case it's unclear, that's because is.na() returns a vector of TRUE and FALSE. In R, TRUE == 1 and FALSE == 0, so taking the mean of a bunch of 1s and 0s gets you the proportion of 1s. Neat trick once you get used to it. Sorry if this is not at all what you meant. 
it's price column and some values are blanks I want R code to calculate the percentage of the missing values with the whole values in the column.
Have you read the data into R? You say it's price data (so numeric type I assume), but also that it has blanks that aren't NA. This doesn't usually happen in R. A numeric vector represents missing data as NA, while a character vector might use '' (the empty string) or simply NA. In either case, the same principle applies -- test whether each element of the relevant column is equal to [however blanks are represented in your data]. How about `mean(df$x == '')`? But yeah, posting sample data would help if you're still confused! Copy and paste the output of `str(df)` where 'df' is the name of your data frame. 
is the column text (likely, because a numeric column won't leave a blank). If so, your condition for a true false would be something like DataFrame$Price == "" and using unclog's answer: mean(DataFrame$Price == "") would give you a percentage. If your blanks are different (they're not truly blanks, they're spaces) you'll have to change them 
is that a space between "CSV" and "file"? if it is, you might want to change it to underscore
Piling on, it looks like you open the file path with: " and end it with: ”
Spaces in path names are totally fine unless you’re using Windows 95.
didn't know that...i guess spaces are discouraged because they wouldn't work when assigning a variable?
&gt; i guess spaces are discouraged because they wouldn't work when assigning a variable? No, assignment to variables has nothing to do with it. I don’t know why some people discourage the use of spaces in file names. They probably worry about some legacy software breaking. But that’s a poor reason: spaces in filenames are simply *valid*, and software has no good excuse for not supporting them, especially since file system functions have supported them in all major operating systems for two decades. In practice, using spaces in filenames nowadays is completely unproblematic.
i'm an old dog and i can't learn new tricks, but i appreciate this reply...it's good to know i can use spaces but i'll probably continue using underscores..i justify the underscores as being just a little less ambiguous (it's difficult to clearly express a space)
Opps didn't mean to delete my first answer. There is a somwhat clunky way of doing this with purrr. library(magrittr) library(purrr) x = t(combn(1:4, 2)) %&gt;% as.data.frame() y = t(combn(1:4, 3)) %&gt;% as.data.frame() z = cross2(transpose(x), transpose(y)) d = map_df(z, ~ flatten(.x) %&gt;% set_names(., seq_along(.))) 
Ok, one more approach using dplyr instead that seems a bit less hacky. library(dplyr) x = data.frame(id=1, t(combn(1:4, 2))) y = data.frame(id=1, t(combn(1:4, 3))) full_join(x, y, by="id") %&gt;% select(-id)
Currently on mobile, but this might be missing the replicate portion of the problem, where I need each row of one column to be replicated the height of the other column (and merged) for the whole column. Hopefully that makes sense, but I'll try to put a short example later. 
Yes, I also tend to avoid spaces in paths, for much the same reason.
For example, you've a dataframe as below: df &lt;- data.frame(x= c(1,2,3,4, NA), y = c(2,3,4,5,6)) Then you can calculate the percentage of missing values as follows: sapply(df, function(x) length(which(is.na(x)))/nrow(df)) This will give you the missing values for all, but if you want to calculate the missing percentage of just one column, then you can use this: length(which(is.na(df$x)))/nrow(df) Hope, this is what you were looking for.
library(caret) has a function - confusionMatrix()
&gt; length(which(is.na(df$x)))/nrow(df) I tried the last one because I want to calculate the percentage of the missing values just for one column and I got this result: &gt; length(which(is.na(uberD$price)))/nrow(uberD) [1] 0 so, I think doesn't work because the missing values are blanks not NAs.
I've chased enough unbalanced quotes and brackets until I let rstudio handle it
&gt; length(which(is.na(uberD$price)))/nrow(uberD) Instead of above code, use this: &gt; length(which((uberD$price = "")))/nrow(uberD)
I tried it and here the result that I got: &gt; length(which((uberD$price="")))/nrow(uberD) Error in which((uberD$price = "")) : argument to 'which' is not logical &gt; length(which(uberD$price=""))/nrow(uberD) Error: unexpected '=' in "length(which(uberD$price="
benchmarks???
Really? You help me a lot thanks. 
Mean(df$variable)
I don't have any experience with Django so can't help on that front. We have started a blog at work using blogdown and it is a very pleasant experience, and I think given the amount of effort involved it does look nice. With regards to your plolty/shiny questions. You should find that any htmlwidgets like shiny, leaflet etc should be fine as they Dont rely on a running R session. They are just html and javascript. To have anything shiny will require a shiny server running somewhere. Whether your host has a shiny server running that you can leverage or its hosted somewhere else and you link to content. Either way though, no shiny server, no shiny functionality. as a side note it doesn't mean you can't use functions from the shiny package. A lot of functions in shiny just generate html. You can still use these. There isn't great benefit to doing so most of the time but the layouts and input widgets don't actually need the R session in the background so can be used in static pages. It's the dynamic content and message passing to and from R that will require shiny server. 
Thanks for the input on Shiny's dependencies. I have given it more thought and I may move away from requiring interactivity. At least at these early stages. I will look more into what aspects of Shiny require the server functionality vs simply rendering HTML.
Sorry there is a typo in my post that perhaps makes something unclear. You should be able to have interactivity via htmlwidgets packages like plotly and leaflet (in my original reply I wrote shiny so may be confusing) If I remember rightly you can even use crosstalk for communication between widgets in a non shiny context. Then you can use things like DataTables for manipulating plots, or linked brushing. there is lots of neat stuff you can do without shiny
The installer in rstudio makes package management alot easier. If you aren't already using that I would suggest it. 
I think I may be able to help you (at least with the curl error). Are you using Linux? If so, I always have trouble when installing tidyverse on top of base R. To get rid of that error, you will need to install the 'libcurl4-openssl-dev' package. I always forget what packages I need to fix the errors every time I do a reinstall, and have to do a bit of detective work. I think I usually install the 'libssl-dev' and 'libxml2-dev' packages, too. This is of course AFTER installing base R, when I go to install tidyverse (which contains ggplot2). And as dash_44 said, R Studio is highly recommended if you're not already planning to do so.
RedditExtractorR is the only thing I am aware of. 
Thanks
Does this do something like what youre looking for? expand.grid(1, 2:4, 1, 2, 3:5)
what do you mean by "variable" ?
I am sort of new to shiny myself so this is probably wrong but the parameters sent to shiny server should be input, output and session not input, input2, output and output2. The input and output variables hold a list of variables so all of your inputs can be accessed with input$ and likewise for the output variables. What I think it happening is shiny is sending output as the second parameter of shinyserver() which you have named input2. I could totally be wrong about this but here is my suggestion: change shinyServer(function(input, input2, output, output2){ into this shinyServer(input, output, session){ again I'm a newb so I might be wrong.
I got that, but that is on the server side and mainly on the function, should I be more specific with my outputs?
I did that on the server, and I have already posted that there is an output$shapefile variable, no?
also did you remove the word "function from your call to shinyserver you do not need to define a new function when calling shinyserver() shinyserver(~~function(~~(input, ~~input2,~~ output, ~~output2,~~ session)~~)~~{
 output$shapefile renderPlot({ I don't see assignment operator here between the variable and the function.
did it, still the same problem
Rewrite your shiny code to be in one file so that is easy to copy and paste for reproducing. Not sure if I will have time to debug it for you. But I would go the way of simplifying what you are trying to do and then adding pieces one by one to pinpoint the issue.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rprogramming] [Learning R. Having problems with Logarithmic axes](https://www.reddit.com/r/rprogramming/comments/7s1ff5/learning_r_having_problems_with_logarithmic_axes/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
The column that you are trying to derive average from. 
Good call. 
that's what I got &gt; mean(uberD$price) [1] NA Warning message: In mean.default(uberD$price) : argument is not numeric or logical: returning NA
that's what I got &gt; mean(uberD$price) [1] NA Warning message: In mean.default(uberD$price) : argument is not numeric or logical: returning NA
I’m relatively new to the language and was just browsing this subreddit, but try ‘mean’ as opposed to mean.default. Also check the structure of your data to make sure that the column is numeric. Str(uberD) #if not numeric uberD$price -&gt; as.numeric(uberD$price) mean(uberD$price, narm = TRUE) narm = TRUE ignores records containing NAs
Thank you for trying. Actually some of the variables is blanks. May be because that it doesn’t work. 
Have you tried logging the vars before plotting?
Yes I tried logging the vars before plotting, and it still doesn't look like the graphs in the file. I'm beginning to wonder if the data file I have for the exercise is supposed to be different as it doesn't match for the headers. It's the only one I got for the book, so I'm not sure what is going on. I may just try reaching out to the author to find out more info.
"Is there something wrong with ..., my data...?" Yes, the plotlines come form the data in the file "curvedata.txt" not from the file plotdata.txt