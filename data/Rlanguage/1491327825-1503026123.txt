Follow-up: The HTML code comes clean in Python via their version of Selenium and PhantomJS. I'm not sure how to clear the oddly encoded single backslashes in R strings, so here is an alternative. If you find a better way in R let me know! #Pre-rec: Install BeautifulSoup for Python on command line. Place phantomjs.exe in python default directory. #Import the packages import platform from bs4 import BeautifulSoup from selenium import webdriver #Version Windows check if platform.system() == 'Windows': PHANTOMJS_PATH = './phantomjs.exe' else: PHANTOMJS_PATH = './phantomjs' #Boot up the phantom browser browser = webdriver.PhantomJS(PHANTOMJS_PATH) #Browse to the website browser.get('https://www.poloniex.com/trollbox') #Scrape that post-javascript HTML source = BeautifulSoup(browser.page_source, "html.parser") #Identifies the table with chat content, ids, usernames source.findAll('table')[0].tbody.findAll('tr')
Also, try... RSiteSearch("type your question here")
To get help, you need to show code, what you've tried, and specifically where you're struggling. 
Sounds good, I'll be sure to look it up. Thanks for the insight. Really appreciate it. Mind me asking what you do career wise?
I'm working as an analyst in a hospital but most of the data I work with are mainly operational. 
i don't think they exist yet. Aerospace is more C++ based. So in a way it could be combined and integrated into R, but I don't see any packages just yet.
Thanks for the input!
This question comes up every so often (e.g. [here](https://www.reddit.com/r/rstats/comments/5okk9w/anyone_use_microsoft_r_in_industry/)) and the answer is: # We don’t know! The whole thing is unfortunately a legal minefield: * The **Free Software Foundation** (FSF), which authors the GNU GPL, [is pretty explicit (second paragraph)](https://www.gnu.org/licenses/gpl-faq.html#IfInterpreterIsGPL): according to them, all code that is running on R, and is thus using the R base libraries (that are distributed under GPL), need to be themselves GPL compatible. What this means [is also explained](https://www.gnu.org/licenses/gpl-faq.html#GPLModuleLicense). * By contrast, **various R interest groups** have their own interpretation on the matter, and they disagree with the FSF. [Hadley has made the quite strong claim](https://twitter.com/hadleywickham/status/821762007980527618) that the above “is a minority belief [in the R community]”, that [R code doesn’t need to be GPL compatible](https://twitter.com/hadleywickham/status/821735951894454273), and that lawyers agree. The thing is, the FSF have *also* asked lawyers, and I’d rather trust the license authors, who are legal professionals, on this one. * The official statement by the **R foundation** on the matter [is completely useless](https://stat.ethz.ch/pipermail/r-devel/2009-May/053248.html) because it doesn’t answer the question at all. So which interpretation is correct? *I* don’t know, and as a consequence I’m *extremely* unhappy that the R project is distributed under a license where **the license holders disagree with the license authors on its interpretation** (and, as mentioned, I trust the license authors’ expertise more). It’s a bad situation, and the best way forward would be to change the R license (this would require rewriting all existing code though, or getting the consent of all existing authors to change the license). That said, a small clarification to answer your question precisely: &gt; Does that mean that anything created with, or incorporating an aspect created with, is subject to the GPL? 5 No. See the GNU FAQ items linked above. But if you want to *release* your code, it needs to be released under a GPL-compatible license (according to the FSF).
Wow, I definitely need to read up on some stuff. Thanks for the input! 
You're welcome. Much, if not all, data wrangling (pre-processing) that you'll need to do can be achieved with the dplyr package with relative ease. Also, learn pipes (%&gt;%). They make life much easier.
&gt; The rest of your comment is confusing, since “installing” ≠ “linking”, so whether something is dynamically or statically installed (what does that even mean?) is irrelevant. No doubt, since you have confused me by pointing to the GPL FAQ which covered interpreted code and then went on to deal with "linking" which is where I've started to misunderstand the difference. As far as I'm aware R packages are "installed" along with dependencies. Could please enlighten me as to how they might be "linked" and/or what "linking" is in the context of R because if I've understood you correctly this is why you pointed to that FAQ and wrote... &gt; Note, if your script uses package/library that is licenced under the GPL, and you want to distribute your script, you may need to license your script under the GPL as well. That way I might learn something about R, licensing and linking of libraries under such systems rather than just being down voted for not understanding. As it is I've read the FAQ you linked, and my understanding of how R packages are installed doesn't fit with linking of libraries described there and why if I or anyone else writes a script (which is interpreted) that uses a GPL licensed library might itself need licensing. To be clear on what my understanding is neither of these are linking library(x) ...nor is writing the following in a package `DESCRIPTION`... Depends: R (&gt;= 3.3.3), dplyr(&gt;= 0.5.0) They're dependencies, the first requires the user to install it otherwise the script they've been furnished with won't run, the second installs `dplyr` as a dependency and doesn't link to it, but does make the functions the package contain available because I have written functions which use those in the `dplyr` package. Thanks in advance.
In the context of R, “dynamic linking” would indeed be a call to `library`, `require` or similar. In essence, any operation that makes the code of a package X available at runtime to another package is called “linking to X” for the purpose of the GPL.
My undergrad was in biochem and I am finishing up my masters in business analytics so I'll see if I can help out a little. First, the good news, your prof seemed to have given you nearly all of the required code so you should only need to copy/paste and maybe substitute a few variables here and there. Secondly, if you haven't figured it out, anything starting with ## is just a comment and doesn't affect the code. Thirdly, if you haven't already, I highly recommend downloading RStudio to write the code in. It'll highlight where you make errors and give you fairly clear error codes if something goes wrong. You can highlight code and run just parts of it as you go so double check your work every couple of steps and you'll be done in no time. 
I'm not an expert at Weka but I feel like Weka is more for predictive analytics and not so much for finding correlations. I'd have to see a bigger piece of the dataset to give you a real answer but if guess you would want to either do market basket analysis or make some dummy variables depending on how many types of food/illnesses we are talking about.
Cool, sounds good. Best of luck. 
Something must have gone wrong during data scrubbing. The worst-performing country according to the original data is Indonesia, but it’s missing in the chart along with a lot of other countries. Here’s what the chart looks like for me: http://i.imgur.com/XjhMSQt.png
This is not an answer to your question, but I have had much more success skipping all java based packages. I used readxl and openxlsx successfully on opening all excel files I had.
You’re confusing variable *names* and their *values*: The `$` operator requires a *name* afterwards, not a character string variable. Your code will return the column called “samplename”, which does not exist. You need to use `[[` instead: print(df[[samplename]]) `x[["y"]]` (note the quotation marks!) is roughly equivalent to `x$y`.
Interesting. Do you know what backends they use?
Just to add to the other comments, here's why that doesn't work. df$b # Works just fine df$"b" # Doesn't work thisname &lt;- "b" df$thisname # Also doesn't work When you go through the names. the names get converted to strings. You can't use the "$" operator with strings. As for vectorizing columns, check out the "apply" family of functions.
Thanks solved
Thanks solved
There are a couple ways but here is one rm(list=ls()) # recreate the original data frame N &lt;- 20 DF &lt;- data.frame(var1=sample(c("a", "b"), 20, T)) DF$var2 &lt;- NA # create the translation vector translate &lt;- c(a="cat", b="dog") # apply DF$var2 &lt;- translate[DF$var1] head(DF)
Appreciate it, thanks!
There are many ways to do this, but if it is just two variables and you want to use the base packages then (assuming your dataframe is named df): df$var2[df$var1 == 'a'] &lt;- 'cat' df$var2[df$var1 == 'b'] &lt;- 'dog' 
You can write a completly independent C++ program with all its features and provide a small R-API via RCPP. Therefore I would say, there's theoretically nothing to skip while learning C++ in this context. Nevertheless you probably won't need everything in C++ for your project. 
No, I've just kept the countries I needed from the list getting rid of some I didn't need for my presentation to make the chart more readable and relevant to what I was talking about. But thanks for sharing the chart with the full list of countries!
So, firstly I assume you mean Microsoft R server. There is quite a small subset of the community that uses it so you may struggle to get good answers. You'd be better off reading Microsoft's docs. What I do know is that they rewrote parts of R to use more C, so that will have something to do with it. You have a few options depending on exactly what you're trying to do: 1. Get more RAM. By far the easiest option since RAM is so cheap. I have 32gb on my work desktop and we have servers with plenty more. 300k rows really isn't much data. I played Overwatch with 2.4 million rows loaded the other day. 1. Don't load so much data; there aren't many problems where you really need all that data. Load only the parts that you need for your task and/or batch your problem. As a corollary to this, you might find keeping your data in a database and being smart about how you query it is a better choice. 1. Use disk caching; since you're using Microsoft R already, it has some functions that begin with `rx` that are versions of base R functions that will swap onto disk as necessary. This will be slow as fuck though. 1. Use distributed computing - if your problem really is intractably massive, this is more or less your only option. Very few problems truly need this solution, but it's an option. 
All parts of C++ available, as long as you stick to C++11 (initial C++14 support is coming with R 3.5) But a lot of C++ isn't particularly relevant for the type of problems you typically want to solve in R. I'd recommend ignoring OO programming and focussing on more functional techniques using the STL. 
There are numerous packages dedicated to this issue. See `ff` or `bigmemory`
I was just last night wishing I could do this and then blundered in here by chance. Are you married? Would you like to be? I'm no catch but I'm devoted. 
If you assign the results at the end of a long pipe chain it makes some sense to use -&gt; at the end instead of &lt;- at the beginning.
Ah gotcha. Thanks!
​Here are some other resources to learn from. Best of luck. [Statmethods - quick cheatsheet of R code, doesn't explain much - Descriptives, Frequencies, Correlations, t-test, non-parametric, multiple regression, regression diagnostics, ANOVA, power​](http://www.statmethods.net/) [Boston University Public Health - concisely explains code and interpreting results - R descriptive, graphs, ANOVA, correlation, regression, categorical, logistic, survival](http://sphweb.bumc.bu.edu/otlt/MPH-Modules/Menu/index.html) 
That's sad :( doing analytics work on a laptop is like trying to hammer in a nail with a screwdriver. I have both and I'd rather RDP into my desktop from my laptop than try to do analysis on it! 
Good lord this is an ugly inforgraphic. That tiled background is terrible. Why do SAS, python and julia make appearances at all? Why the picture of congress? This is basically unreadable because of the background alone, and I don't think it would be especially useful even with a better (i.e. plainer) background.
RStudio helped me (and many others) with learning R a great deal.
Been there! I'm admittedly a noob with less than a year of R under my belt but Lubridate simplified things greatly for me. It might be worth going over the documentation again.
 &gt; dat &lt;- paste0("string", 1:22) &gt; library(gsubfn) &gt; # the %02d tells how wide you want it to be. This gives 2 digits so 1 becomes 01. &gt; gsubfn("[[:digit:]]+", function(x){sprintf("%02d", as.numeric(x))}, dat) [1] "string01" "string02" "string03" "string04" "string05" "string06" "string07" "string08" "string09" "string10" [11] "string11" "string12" "string13" "string14" "string15" "string16" "string17" "string18" "string19" "string20" [21] "string21" "string22"
Short, to the point, and probably faster than a for loop. But I doubt a new R user could figure something like this out. The apply functions are hard enough, and do.call is even stranger. I know loops are frowned upon, but they're easy to read and write.
Not with gsub, but you can match with grepl on the two columns and join the logic vectors. Or create a new vector by pasting the two columns together.
C mostly openxlsx is c++ (Rcpp) readxl as well see (https://cran.r-project.org/web/packages/readxl/index.html)
Though I was exposed to them later, I found learning how apply functions work was no more complicated than loops. After successfully using them a couple times, it stuck and it's ultimately more powerful so worth learning.
Something about `[&lt;-` is slow. library(microbenchmark) f_c_method = function(x){ df = data.frame() for (i in 1:100) { df = rbind(df, rnorm(1000)) } return(df) } f_define_method = function(x){ df = matrix(1, nrow = 100, ncol = 1000) for (i in 1:100) { df[i,] = rnorm(1000) } return(df) } f_lapply_docall = function(x) { df = lapply(1:100, function(x) rnorm(1000)) return(do.call(rbind, df)) } microbenchmark(f_define_method(), f_c_method(), f_lapply_docall(), times = 10L) Unit: milliseconds expr min lq mean median uq max neval cld f_define_method() 9.103375 9.217467 9.391015 9.358406 9.558563 9.662391 10 a f_c_method() 1369.374150 1405.344695 1452.000560 1425.548862 1487.721954 1588.485984 10 b f_lapply_docall() 8.854265 8.952172 10.498983 9.215099 9.695553 16.968698 10 a 
Does it make a difference that the slowest (f_c_method()) is the only one that is a data frame. The other two look like matrices here. I thought matricies were always faster that data frames, but more restrictive. I ran your code and used df = as.data.frame(matrix(1, nrow = 100, ncol = 1000)) for the f_define_method and df = as.data.frame(lapply(1:100, function(x), rnorm(1000))) for the f_lapply_docall method. The lapply was about 10 times faster than f_define_method. lapply wins again. Thanks for demonstrating that.
lapply is not inherently faster than for loop. It is basically a for loop wrapper. The difference is in the way data is stored. It seems that: 1. matrices [&lt;- is faster than data.frame [&lt;-. 2. Storing as list, with either loop or *apply, and binding them is marginally faster than method 1. 3. The slowest way by far, as expected, is iteratively growing the output with either rbind() or c(). f_lapply_docall = function(x) { df = lapply(1:100, function(x) rnorm(1000)) return(do.call(rbind, df)) } f_list_docall = function(x) { df = list() for (i in 1:100) df[[i]] = rnorm(1000) return(do.call(rbind, df)) } microbenchmark(f_list_docall(), f_lapply_docall(), times = 10L) Unit: milliseconds expr min lq mean median uq max neval cld f_list_docall() 8.869266 9.166935 9.329982 9.188845 9.603173 10.04731 10 a f_lapply_docall() 8.810838 9.035866 9.371434 9.083043 9.238786 11.79819 10 a
&gt; I also don't like rstudio though I’m curious: why? Personally I don’t use RStudio much because I prefer powerful text editors (Vim etc) but even I have to admit that RStudio in recent versions is pretty darn impressive. It’s almost unanimously praised.
You could give it a temporary column df$new &lt;- paste(df$one,df$two,sep="-")
That's why I switched the arrow operator to C-; . I also have %&gt;% mapped to C-' .
When I started I found the list.files argument handy when trying to read in multiple .csv (or whatever) files without having to specify the individual names. filenames &lt;- as.list(list.files(path = ".", pattern = ".csv", all.files = FALSE, full.names = TRUE, recursive = TRUE) You'll want to check the options of that argument to ensure that you get the information that you want. I would save that information as a list (as I do above), and then use one of the plyr/dplyr functions to read the data in. For an example to suit your present purposes, I'd write a function to call in the data based on the file name, get the appropriate summary statistics, and then return these summary statistics. I'd apply that self-written function with ldply, which takes in a List (the names of your data files, reading them in) and returns a Dataframe, each line of which corresponds to the information returned in the function. So: getsums &lt;- function(csv_filename){ data &lt;- read.csv(csv_filename) this_mean &lt;- mean(data[,1]) that_mean &lt;- mean(data[,2]) meandf &lt;- data.frame(this_mean, that_mean) return(meandf) } Then: results_df &lt;- ldply(filenames, getsums) There are far more efficient solutions, but I feel this can help you learn some of the basic parts of more advanced R programming. Happy learning!
Simple, tidy, and takes care of the problem of reading only the .csv files in the path specified. And afterwards, you still have a function to do it all over again. I like it.
Aw. I appreciate the effort! I'm realizing my effort using `ifelse` is quite silly because that only looks at rows in which `signal1==signal2==signal3` is true. You had a very clever trick with `if(sum(df[i,1:3]) == 3)` 
if I give a temporary column, what do I do next?
you should look into `dplyr`, specifically `dplyr::mutate()`. i normally am annoyed when people recommend some package to do a simple thing, but dplyr isn't some obscure onetime use package. it's very popular and a game changer
Yeah, good points. I agree with the flawed UI design. [And we’re certainly not the only ones to think that.](https://www.reddit.com/r/programming/comments/64oqaq/electron_is_flash_for_the_desktop/)
You should look at using the [readr](https://cran.r-project.org/web/packages/readr/vignettes/readr.html) package which is designed for reading flat-files into internal R structures called [tibbles](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html) (a special form of data frame). From there you will be able to plot your data with for example [ggplot2](http://ggplot2.tidyverse.org/reference/).
Yeah. The same think that OP complains about. RKWard is just doing that with Firefox instead of Chrome. I love and use Firefox, but holy fuck, doing that so you can write your UI with XUL? Exactly mu thoughts.
This is the first time I've seen rle(). Nice solution. I think it's an improvement on my if-elseif-else with a hackey counter. Thanks.
There are two kinds of solution to this problem: technical, and process. /u/enilkcals has suggested what I think is the correct technical solution so I won't dwell on it. The correct process solution is to beat your upstream data source with a hosepipe until they agree to provide the data in a proper interchange format rather than this bizarre wtf file. There is a reason why (several) standards exist for interchange - but, no, they had to make up something new. The non-hyperbolic version of that is that if you can influence the upstream dependency to change their formatting, you should do so, because they're making everyone's life incredibly difficult. Once they're using a proper format, you can use packages like rJSON, RProtoBuf, readr::read_csv, etc to parse the files.
Thank you :). This really means a lot to me. I tend to ask a lot on StackOverflow and I've always been told my questions were terrible. 
nice, didn't know about that one! 
Which variables are you trying to rename/change to T and NT?
This *MIGHT* work. / 23| 0[0-3]/ (everything between forward slashes)
That doesn't sound like something you *can't* change, it sounds like something that's inconvenient to change. If the software is FOSS you can make a patch to change the output (and contribute it back upstream). If it's not, you can use your relationship with them to get the changes on their roadmap and get them prioritised. 
You can use the xts library, and as.xts(). Then: http://stackoverflow.com/questions/13912282/subset-xts-object-by-time-of-day
this might work grep("([01]\\d|2[0-3]):([0-5]\\d:){2}") Or more less specific: grep("(\\d\\d:){2}\\d\\d") I hope this helps. I always end up working it out by trial and error. You might need to use the capture group. Also strsplt on the [space] may reformat the data in a way that's easier to work with. 
If you just want to specific for the labelz object then you can just use labelz&lt;- c("T", "T", "T", "T", "T", "NT", "NT", "NT", "NT", "NT")
R won't know how many places are significant, you can however tell the round function how many decimal places you want
Can you give an example of how you'd want it to work? 
Basically how you were taught to round in elementary school.
Ohhhhhh, Now I know what you're looking for. Why do you want to do this?
Thx
Part of the reason why round *doesn't* do that by default is because that will gradually cause your computations to shift upwards over time. Here's some more info: ([source](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)) &gt; Rounding is straightforward, with the exception of how to round halfway cases; for example, should 12.5 round to 12 or 13? One school of thought divides the 10 digits in half, letting {0, 1, 2, 3, 4} round down, and {5, 6, 7, 8, 9} round up; thus 12.5 would round to 13. This is how rounding works on Digital Equipment Corporation's VAX computers. Another school of thought says that since numbers ending in 5 are halfway between two possible roundings, they should round down half the time and round up the other half. One way of obtaining this 50% behavior to require that the rounded result have its least significant digit be even. Thus 12.5 rounds to 12 rather than 13 because 2 is even. Which of these methods is best, round up or round to even? You also have the problem of determining how negative numbers should round. What is "up" in that case? Further negative, or back towards the positive numbers? Furthermore, you should also be aware that there are numbers that can't be represented in base-2, such as &gt; 0 == (0.3 - 3*0.1) [1] FALSE ___ All of that being said, here's a solution for your problem: lastDigit = function(decimal) { charstring = as.character(decimal) finalchar = substr(charstring, nchar(charstring), nchar(charstring)) return(as.integer(finalchar)) } roundUp = function(decimal) { charstring = as.character(decimal) parts = unlist(strsplit(charstring, "\\.")) whole = parts[1] fractional = parts[2] removed = substr(fractional, 0, nchar(fractional) - 1) if (nchar(removed) == 0) { # round the whole part up, ignore fractional whole = as.integer(whole) + 1 fractional = 0 } else { # round the fractional part up, keep whole the same fractional = as.integer(removed) + 1 } back_together = paste(whole, ".", fractional, sep = "") return(as.numeric(back_together)) } truncateDown = function(decimal) { charstring = as.character(decimal) removed = substr(charstring, 0, nchar(charstring) - 1) return(as.numeric(removed)) } roundElementarySchool = function(num) { if (num %% 1 == 0) { # integer, already maximally rounded return(num) } else if (lastDigit(num) &gt;= 5) { return(roundUp(num)) } else { return(truncateDown(num)) } } &gt; roundElementarySchool(0.4) [1] 0 &gt; roundElementarySchool(0.5) [1] 1 &gt; roundElementarySchool(1.5) [1] 2 It only chunks one decimal at a time, so no fast-forwarding and picking a particular decimal place to round at. &gt; roundElementarySchool(0.12345) [1] 0.1235 &gt; roundElementarySchool(0.98765) [1] 0.9877 &gt; roundElementarySchool(0.11111) [1] 0.1111 &gt; roundElementarySchool(0.55555) [1] 0.5556 EDIT: major unit-testing caveat. I went to test it on the default pi constant (3.141593) and it worked, but threw an error for a longer version (3.141592653589). The reason is because I cast the numbers to integer and add 1 in the `roundUp` function. This means that my function only supports up to `INT_MAX`, 2^(31) - 1, or about 10 decimal places, plus or minus one or two. Feel free to refactor that, because I won't.
Yeah dude, I'm sure you totally wrote that on the toilet and didn't [plagiarize it word-for-word from this StackOverflow](http://stackoverflow.com/a/12688836/7381700)
lmfao
True true, but that's not actually what OP wants
thank you
I need to implement calculations in a way that I have been instructed to do so.
I am in no position to challenge the SOP.
Think of it as a dataframe (table) where each row will draw a segment, rather than individual vectors of values. The first element of each of those vectors comprise the first row of the dataframe, so the first segment starts at 0, ends at 60, for the left hind leg. So yes, the order of the leg array matters. Type View(df) to get a better idea of how the data is structured. I've only entered the data using data.frame for reproducibility, I'd suggest you enter your data into a spreadsheet and import as a csv, it will make more sense that way. No problem, let me know if anything is unclear. If you want to tweak any aspects of the graph, ggplot2 has a fantastic documentation [here](http://ggplot2.tidyverse.org/reference/).
&gt; How do I write a grep to select only the elements between 11 PM and 3:59 AM? You don’t. `grep` is for pattern matching based on [*regular expressions*](https://en.wikipedia.org/wiki/Regular_expression)(give or take). What you want to select is patently *not* a regular text pattern; it’s a time in a specific representation. — So parse the time out of the string and check its range.
Try this full_hist &lt;- hist_table %&gt;% full_join(all_weeks_df, by = "week_ending") %&gt;% group_by(week_ending) %&gt;% complete(product, dept) %&gt;% filter(!(is.na(product) | is.na(dept))) %&gt;% arrange(product, dept) print(full_hist, n = 32) How do you want the final_date data to be repeated/filled? 
I could hardly understand what you were trying to do...but this step: data.lgr &lt;- data.frame(IsB, expr.data) is causing recycling of expr.data which originally had only 2 rows (thanks to line 3).. In the end, the way your data is set up is all even numbered rows have one type of data and all odd numbered rows have another. The output of predict is 0.7+ for both types of data, which you are then setting as TRUE for both. 
&gt; rm(list = ls()) Don’t do that. Start a new R session. Better yet, have the code in a script/RMarkdown/… that’s launched with a new R session rather than `source`’d.
Wow this works perfectly! Thank you so much for the help man. 
Now the errors are &gt; install.packages("magrittr") &gt; &gt; &gt; install.packages("magrittr") Error: unexpected '&gt;' in "&gt;" &gt; install.packages("magrittr") Installing package into ‘C:/Users/Mohsin/Documents/R/win-library/3.2’ (as ‘lib’ is unspecified) trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/magrittr_1.5.zip' Content type 'application/zip' length 149741 bytes (146 KB) downloaded 146 KB package ‘magrittr’ successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\Users\Mohsin\AppData\Local\Temp\Rtmp2loVfQ\downloaded_packages &gt; &gt; library(magrittr) Warning message: package ‘magrittr’ was built under R version 3.2.5 &gt; library(tibble) Error in library(tibble) : there is no package called ‘tibble’ &gt; library(dplyr) Error in library(dplyr) : there is no package called ‘dplyr’ &gt; library(tidyr) Error in library(tidyr) : there is no package called ‘tidyr’ &gt; library(stringi) Error in library(stringi) : there is no package called ‘stringi’ &gt; &gt; readLines("file.txt") %&gt;% + .[grepl("(Time|WatBalR)", .)] %&gt;% + stri_split_charclass("\\p{Z}", simplify = TRUE) %&gt;% + as_tibble %&gt;% + select(Variable = V1, Value = V3) %&gt;% + mutate_at("Value", as.numeric) %&gt;% + mutate(Observation = sort(rep(1:(n()/2), 2))) %&gt;% + spread(Variable, Value) Error in file(con, "r") : cannot open the connection In addition: Warning message: In file(con, "r") : cannot open file 'file.txt': No such file or directory &gt; install.packages("tibble") Installing package into ‘C:/Users/Mohsin/Documents/R/win-library/3.2’ (as ‘lib’ is unspecified) also installing the dependencies ‘lazyeval’, ‘Rcpp’ trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/lazyeval_0.2.0.zip' Content type 'application/zip' length 138280 bytes (135 KB) downloaded 135 KB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/Rcpp_0.12.10.zip' Content type 'application/zip' length 3261598 bytes (3.1 MB) downloaded 3.1 MB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/tibble_1.3.0.zip' Content type 'application/zip' length 566811 bytes (553 KB) downloaded 553 KB package ‘lazyeval’ successfully unpacked and MD5 sums checked package ‘Rcpp’ successfully unpacked and MD5 sums checked package ‘tibble’ successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\Users\Mohsin\AppData\Local\Temp\Rtmp2loVfQ\downloaded_packages &gt; install.packages("dplyr") Installing package into ‘C:/Users/Mohsin/Documents/R/win-library/3.2’ (as ‘lib’ is unspecified) also installing the dependencies ‘assertthat’, ‘R6’, ‘DBI’, ‘BH’ trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/assertthat_0.2.0.zip' Content type 'application/zip' length 43651 bytes (42 KB) downloaded 42 KB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/R6_2.2.0.zip' Content type 'application/zip' length 197111 bytes (192 KB) downloaded 192 KB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/DBI_0.6-1.zip' Content type 'application/zip' length 743268 bytes (725 KB) downloaded 725 KB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/BH_1.62.0-1.zip' Content type 'application/zip' length 16150077 bytes (15.4 MB) downloaded 15.4 MB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/dplyr_0.5.0.zip' Content type 'application/zip' length 2520548 bytes (2.4 MB) downloaded 2.4 MB package ‘assertthat’ successfully unpacked and MD5 sums checked package ‘R6’ successfully unpacked and MD5 sums checked package ‘DBI’ successfully unpacked and MD5 sums checked &gt; install.packages("tidyr") Installing package into ‘C:/Users/Mohsin/Documents/R/win-library/3.2’ (as ‘lib’ is unspecified) also installing the dependencies ‘BH’, ‘dplyr’, ‘stringi’ trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/BH_1.62.0-1.zip' Content type 'application/zip' length 16150077 bytes (15.4 MB) downloaded 15.4 MB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/dplyr_0.5.0.zip' Content type 'application/zip' length 2520548 bytes (2.4 MB) downloaded 2.4 MB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/stringi_1.1.5.zip' Content type 'application/zip' length 14220135 bytes (13.6 MB) downloaded 13.6 MB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/tidyr_0.6.1.zip' Content type 'application/zip' length 802348 bytes (783 KB) downloaded 783 KB &gt; install.packages("stringi") Installing package into ‘C:/Users/Mohsin/Documents/R/win-library/3.2’ (as ‘lib’ is unspecified) trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/stringi_1.1.5.zip' Content type 'application/zip' length 14220135 bytes (13.6 MB) downloaded 13.6 MB package ‘stringi’ successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\Users\Mohsin\AppData\Local\Temp\Rtmp2loVfQ\downloaded_packages &gt; library(magrittr) &gt; library(tibble) Warning message: package ‘tibble’ was built under R version 3.2.5 &gt; library(dplyr) Error in library(dplyr) : there is no package called ‘dplyr’ &gt; library(tidyr) Error in library(tidyr) : there is no package called ‘tidyr’ &gt; library(stringi) Warning message: package ‘stringi’ was built under R version 3.2.5 &gt; &gt; readLines("file.txt") %&gt;% + .[grepl("(Time|WatBalR)", .)] %&gt;% + stri_split_charclass("\\p{Z}", simplify = TRUE) %&gt;% + as_tibble %&gt;% + select(Variable = V1, Value = V3) %&gt;% + mutate_at("Value", as.numeric) %&gt;% + mutate(Observation = sort(rep(1:(n()/2), 2))) %&gt;% + spread(Variable, Value) Error in file(con, "r") : cannot open the connection In addition: Warning message: In file(con, "r") : cannot open file 'file.txt': No such file or directory
Could you elaborate on this? I'm following an example given in class, and I'm juts following along for the most part. Here is the example given. It works correctly: #The first several lines are same as before, get the data set for logistic regression library("ALL"); data(ALL); allB123 &lt;- ALL[ , which(ALL$BT %in% c("B1","B2","B3"))] expr.data &lt;-exprs(allB123)[ "1389_at", ] IsB1 &lt;- (allB123$BT=="B1") data.lgr &lt;- data.frame( IsB1,expr.data) #The whole data set in data.lgr ## 10 fold cross-validation require(caret) n&lt;-dim(data.lgr)[1] #size of the whole sample index&lt;-1:n #index for data points K&lt;-10 #number of folds flds &lt;- createFolds(index, k=K) #create K folds mcr.cv.raw&lt;-rep(NA, K) ## A vector to save raw mcr for (i in 1:K) { testID&lt;-flds[[i]] #the i-th fold as validation set data.tr&lt;-data.lgr[-testID,] #remove the test cases from training data data.test&lt;-data.lgr[testID,] #validation (test) data fit.lgr &lt;- glm(IsB1~., family=binomial(link='logit'), data=data.tr) #train model pred.prob &lt;- predict(fit.lgr, newdata=data.test, type="response") #prediction probability pred.B1&lt;- (pred.prob&gt; 0.5) #prediction class mcr.cv.raw[i]&lt;- sum(pred.B1!=data.test$IsB1)/length(pred.B1)#mcr on testing case } mcr.cv&lt;-mean(mcr.cv.raw) #average the mcr over K folds. He does the same thing in the beginning, so I'm not sure why mine is causing problems.
The difference arises due to third line of your original code, where unlike your class code, you are selecting two rows instead of one. Nevertheless, following code transposes the matrix before making a dataframe. I am not sure what you want: data.lgr &lt;- data.frame(IsB, t(expr.data))
Thanks for the answer! My comment on variable names was actually preemptive, as every time I post in here (or in SO) there is some comment over how long my variable names are! This is the first time someone actually suggested the contrary, and I agree with you. 
This sounds like what you're going for http://stackoverflow.com/questions/17721126/simplest-way-to-do-grouped-barplot
I think Python is widely preferred for this.
They are. The data definitely has variation though (between 1 and 0)
Is this true for all the variables?
No https://drive.google.com/file/d/0B6Eez8YTMBg1WmstZjNGNG5HLVk/view?usp=sharing
Check out the R function ifelse(). Basically works just like the Excel IF() function.
Thanks, but one of the issues I'm having is that I haven't generated column `C`. Column `C` depends on `A`, `B` and lagged values of itself. I'm not quite sure how `ifelse()` will handle something like `x$C &lt;- ifelse(x$A=-1, -1, ifelse((x$B=-1)&amp;(lag(x$C)=-1), -1, 0)` if `x$C` has not been generated.
It's a period not a star, I'm on mobile too... lm(D12~., data) The star means interaction terms. 
Mxnet in R can do deep learning. Better to use python but this is an R thread so, what do you want to know specifically? :)
That's how the statistician in our department said we should do it. She made that point as if it were gospel. There are a few different measures of disability we use as they assess different things. We are trying to see which ones are affected by which treatments &amp; baseline factors (insurance status, etc).
Set up a dummy variable (1/0) and cbind that to the other two columns. Turn it into a matrix and go.
I just tried out the R library "imager" for image processing, and it's really good. The documentation is great, too. 
Starting with three columns (date, A, B), you could do it in 2 steps: * 1. Copy the contents of A over to a new column C. * 2. Check on each of your values in your new column C that are 0, replacing those that match your criteria with -1 Copy A to new column C df$C &lt;- df$A Using /u/Wannabe_Genius's ifelse approach, Look at each of the rows (except the last one) and if they match a criteria give it a -1 value df$C[1:(nrow(df) - 1)] &lt;- sapply(1:(nrow(df)-1), function(x) ifelse(df$C[x] == -1 | (df$B[x] == -1 &amp; df$C[x+1] == -1), -1, 0)) Alternatively if you want to avoid ifs you can pop a -1 in the subset of data that matches your criteria. dfrows &lt;- 1:(nrow(df) -1) df$C[which(df$C[dfrows] == 0 &amp; df$B[dfrows] == -1 &amp; df$C[dfrows + 1] == -1)] &lt;- -1 A B C 1 0 -1 -1 2 -1 -1 -1 3 0 -1 0 4 0 -1 -1 5 -1 -1 -1 6 -1 -1 -1 7 -1 -1 -1 8 -1 -1 -1 9 -1 -1 -1 10 -1 -1 -1 11 0 -1 0 12 0 -1 0 Not sure where the 3rd -1 is coming from in your example. If it is being put in on multiple iterations of the script then you could run it a number of times, but then you'd eventually come across the problem where your C column autofills with -1s until it finds a 0 in the B column or hits the very bottom -1 row. 
I'm new to R as well, but when learning any new programming language my general approach is: read a little bit from a well-organized textbook, do some of the exercises/tutorials to get familiar with some basic functions. eventually I see a function that looks like it could solve some little problem I've had in my head for a while; so I set out on a little project to try and solve it. usually this forces me to go look up and learn a few new things because it's never as simple as it seems. once I complete that project, or realize it's too complex for my current level of knowledge, I either pick a smaller scope project to finish up quickly and cement my recently learned skills, or I go back and read a few more chapters, do a few more canned exercises/problems/tutorials and apply them to a new mini-project. rinse and repeat. I personally learn much more by trying to solve problems that I make up; because I'm interested in their solutions. And when you try and fail you learn about the real world limitations and advantages of the functions you were trying to use to solve it.
https://learnxinyminutes.com/docs/r/ That site helped me a ton transition from other languages when I just needed to get the basics down. http://www.statmethods.net/index.html That site also helped me.
Also, I'm wondering if a `for()` could work in this case. For instance, for the first condition we could just say `df$C &lt;- df$A` as you mentioned. Then for the second part, I'm wondering if something like this would work: `for (i in 2:length(dat[,1]){ dat[i-1,3] &lt;- ifelse(dat[i-1,2]==-1&amp;dat[i,3]==-1, dat[i,3])`. In this case, row 3 will get filled since it will look at row 2, will already have a -1 on account of -1 in `A`. And then row 4 will get filled since row 3 is filled. The above code doesn't work beause it removes the -1 from 2, but I'm wondering if a variant of it might do the trick. 
The Kaggle Titanic challenge. A well known dataset, loads of scripts ('kernels') to look at and learn from. Plenty of walk throughs to read.
When you read the CSV into the data frame, you're essentially editing cells in an array, editing strings. Your loop should iterate thru the cells looking for whatever you've told it to look for and make changes as you go. As for difficulty, it's not something I'd want to do when using R for the very first time, but as programming challenges go its pretty easy and not markedly different from doing it in Python, for example. 
Does this also cover basics like matrices, vectors, etc?
When you install the mxnet library, it has a few examples with codes that comes with it. In that example they apply a CNN on the mnist dataset. This is a good start to understand how to apply deep learning to recognise images and predict new ones
'Data Wrangling With R' is a more noob friendly book, and it's much more recent.
This is a great suggestion. While much of that book is inaccessible to R noobs (unless you're proficient in another language), this chapter is pretty straightforward. You might have to read it with the chapter on environments too, though.
Yes. all libraries are installed. and everything renders fine when not in the loop. Unfortunately the HTML is needed inside the loop, because each loop is it's own row and div. So otherwise it wouldn't render the HTML properly. The number of rows/divs is also variable depending on how many loops. I just showed one output loop here, there were actually a dozen more, but didn't want to clutter things up. It seems to loop through and include the HTML tags without issue. The challenge (I think) is that the whole thing is output like a single character value inside quotes, instead of properly rendered.
Microsoft didn't buy R, they bought Revolution R which was one of many companies based on making products using the R language.
I've only used some of this but one thing MS has done is integrate R into TSQL to allow for our daily ETL processes to smoothly integrate R code without having to pass a bunch of data through an ODBC connection. Example: One table we have is basically a 10m-row edgelist for a graph. We want to label all the connected components of the graph, and it's easier and faster (as far as I'm aware) to do this via igraph than trying to code some sort of graph procedure into SQL natively. R Server allows for this to be done as part of our daily ETL. Beyond that, I know MS has forked R and maintains its own version that ostensibly has a variety of optimizations. There are also some libraries that have been developed that are supposed to allow for setting a remote SQL server as a compute context for development, among other things.
Yes, I think so.
I like to hope that they bought up Revolution R just to get their name out there, so maybe people might think of other Microsoft products when they think of their next R project (Windows, MSSQL, Azure, Power BI, etc). I use Tableau sometimes and pretty much always format the data to work with Tableau using R (creating subsets mostly, but sometimes collapsing data too) instead of within Tableau. Maybe some integrated options in their IDE to connect to their storage solutions or something could sway users. What I would love is the ability to locally connect to the current session the IDE is using over websockets so I can create monitoring applications and whatnot. I never found a way to create a websocket server for the R session without keeping the script running unfortunately. 
There is a batch mode for the CLI version that might work for you purposes. 
I'm confused by the many command line options: Rscript.exe, R.exe, batch, littler, etc. And how to pipe data into an R script and get the output in stdout without numbered outputs "[1]" . Basically I'm trying to wrap my script into a [docker image](https://hub.docker.com/_/r-base/) with its installed dependencies and call it from the command line or another program's subprocess capabilities.
Oh you want to handle piping in a similar manner to unix command line right? I saw a blog post about this a while back, I'll link back if I can find it. In the meantime here is a [S/O post](http://stackoverflow.com/questions/9370609/piping-stdin-to-r) that could be helpful. Personally I have had trouble handling the formatting while piping in R also, so you are not alone. EDIT: here is the blog post I was thinking of: [blog](https://datafireball.com/2013/10/10/putting-your-r-code-into-pipeline/). It is not as extensive as I remembered, but it may help you.
Thank you, that would be super helpful! Yes, getting data in and out using pipes is proving tricky so far. I'm already reading your suggested S/O post =)
Thanks, I'll definitely post back here if I find a more general approach. This has already made accessing R models much easier for me =)
Oh man, that would have been terrible. No, Microsoft did not buy the language... Stay away from their products... R is an excellent tool to get statistics done without having to depend in closed source tools, it liberates you. 
Have you looked into Rserve, the server version of R? I'm going to set it up soon myself. I think in general when building modern cloud based workflows windows is not what comes first to mind. Personally I do all my cloud based developments on Linux servers today. The last couple windows based projects I was working on were really not great experiences and I promised my self, never again. 
&gt;Have you looked into Rserve, the server version of R? I'm going to set it up soon myself. Looked into it a while back, but it wasn't what I was looking for. Still like working inside of RStudio. For creating socket servers I just use httpuv and jsonlite in a variant of [this function](http://rlang.io/use-an-r-session-as-a-websocket-server/). &gt;I think in general when building modern cloud based workflows windows is not what comes first to mind. Probably why they are trying to change that. &gt;Personally I do all my cloud based developments on Linux servers today. I don't do any cloud work, but I have some spare machines running RStudio Server on various Linux distros. 
The XY problem?
Never worked with rstudio server. What are its main uses? 
SQLite is a solid idea, I have a ~4GB file to process in my RScript, but the command line arguments are just the path to a tsv and 2-3 flags. The tsv is read into R as a dataframe and has all the metadata needed to process the larger file. It feels a little hacky currently... storing all of the tsv data in central db might help smooth over the process...
Nice one!
The issue was with Nginx, not Shiny server Here's where I found the solution: https://github.com/rstudio/shiny-server/issues/118
why do you want to do this operation? I have a feeling there may be a better way to attack the problem you are trying to solve. I'm sure there is a way to uniquely label each grouping of &lt;1000, but my feeling is that it will be messy.
Since I was unable to locate your instructor and you've managed to spam this question to multiple sites instead of figuring out, I decline to answer your question in a positive manner. Consider yourself rebuked.
Since when is it a violation to ask for help on homework? I did not ask for a solution guide or the answer. I was only asking for guidance on how R would perform this operation. I provided the question as context and the code is 100% mine. 
Again, I was not asking for a solution. I was asking for guidance as to what function or code or whatever, would do this. I was in no way asking for somebody to complete the problem. If that is how it came off then my bad, but the intent was to find out if I use arima() or forecast() or whatever. 
I'm trying to understand what you mean by number groupings of 1. From the example, it looks like then number corresponds to the number in the second column?
Rows 1-3, and rows 5-6. These are the "groups" of values under 1000
In retrospect I think your suggestion is more appropriate than what I linked. R for Data Science has a good introduction to functions as well http://r4ds.had.co.nz/functions.html
Wait... Since when did Azure functions start supporting R? I guess I should read and I might find out.
(Like others said, the 'unique groupings' are only unique if the sort in your example makes any sense. In your example it looks like second_column might be used to group the labels on?) One possible, though somewhat hacky, way of doing it would be the following: * add a column '&lt; 1000' like you did ('new') * make a function that operates on a logical vector: ** start a counter at 1 ** while value == 1, assign current_counter_value ** while value != 1, increment counter by one and assign 0 instead until you hit another 1 (make sure to only increment the counter once for each group of 0's, obviously) * apply this function to the 'new' vector, and output into the 'label' vector
Then you should ask for clarification. I don't know what an "identifying trend" is, the use of "on" is incorrect grammar, and the phrase "code regarding confidence intervals" doesn't mean anything because confidence intervals are a statistics question, not a programming question. The only part of the question that makes sense is "What are".
trust me I know. You seem like an asshole though 
Google Maps already includes building shapes at a close enough zoom level, right?
Can you get a few .R files from her with the code she uses to accomplish these tricks? Surely that is allowed. Apropos, if your workplace is going to fire you for asking a colleague for some tips and tricks with data.table you should probably think about finding a new job.
I hardly use it, but when I do I find [this](https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf) to be a helpful reference.
I am getting "error in apply (1:nrow(df), 1, function(x) combn(df[x,], 2)) : dim(x) must have a positive length" error. 
Can you explain to me what a PCorpus is? I am curious?
A corpus is a collection of text documents for text mining. A pcorpus is a permanent corpus that is not loaded into memory, instead it keeps a reference to the files and loads them as needed.
Not sure without seeing your function or data. Try going through and manually putting in a subset of rows, starting with the one that worked in your example. Do you have any missing data, or invalid data? sapply(1, function(x) combn(df[x,],2)) sapply(1:5, function(x) combn(df[x,],2)) sapply(1:10, function(x) combn(df[x,],2))
Any idea what the tricks do? I don't use DT, but I use dplyr relentlessly, and I find it more functional/intuitive than DT. I'd love to know if DT is superior in any way or if it's a general preference thing. 
R studio all the way. Makes life a lot easier!!! 
delet this
I use sublime text,it is not the smoothest ride.
not that great. rstudio or nvim-r are really just the two alright ones.
Anyone vscode?
It all depends on if the underlying C code checks for user interrupts, which allows the code to gracefully terminate. 
That, and other non-C code that might be being called. Some R functions include, for example, Fortran, and lots of packages include code in other languages. I have a custom-written internal patch for RJDBC to get it to work with some JDBC drivers we have - that uses rJava to call a JVM and run Java code. At the moment if there is a network problem and you don't want to wait for the connection to time out, you have to halt R :(
Yeah unfortunately all you can do is ask the maintainer of the package to add that code. (All the packages in the tidyverse should have it in the appropriate place but it's easy to miss)
Rstudio is such a lovely IDE that I see no reason to switch.
From my understanding data.table handles large amounts of data better. Not sure why, think I read it has something to I do with the internal structure of dataframes vs data tables.There is the dtplyr package that allows you to use dplyr commands on DTs.
Did you ever consider he could just be taking a mooc for extra knowledge and not in a real program or class where he might have a more readily available instructor to assist him in understanding ​the question better? 
Sure! Yes, I'm using piping (%&gt;%). It seems like you know what this does, but basically it 'pipes' the left side of the pipe into the function on the right side, as its first argument (the data the function should act on). If you didn't group by time, the summarize command would collapse over all time points. The result would basically be 4 points, one for each diet. group_by() basically states what structure you want to preserve (in this context, anyway). This is needed because of the way summarize() works: it creates new 'summary' variables, using whatever functions you tell it to, collapsing over all data except what you've explicitly told it to preserve (with group_by()). So an English interpretation might be: "hang on to this Time and Diet info, because those are differences I want to keep in my summary. Next, for each combination of Time and Diet (but disregarding chick ID), calculate the mean and standard error of chick weights, and spit back a data frame with that info." Your ggplot approach works fine too. The way I did it allows you to save a 'template' plot, which you can add layers to later. You could do the same after adding some layers, too. For example you could save the whole plot I made (chickPlot + ... + theme_bw()) as a variable (say, chickPlotLine) and then add a title by saying chickPlotLine + ggtitle('Chick Weights over Time by Diet') (or something). Honestly I usually do it your way too, since I'm often creating single-use plots that I don't later need to customize. 
No problem. As for the aggregation that I wrote: my.data &lt;- aggregate(weight ~ Time + Diet, data = ChickWeight, mean) The weight ~ Time + Diet Is a formula (if you have used any of the stats or modeling functions in R you will be familiar with formulae). Basically you're telling R to aggregate weights by time-point and by diet (so for every time point, take the mean weight of each diet). data = ChickWeight Just tells R that I want this data frame mean should actually say: FUN = mean I'm just telling R to aggregate the weights by their mean. You could also aggregate them by counts (ex., how many observations are available for each time point and diet combination) or any other summary statistic. The reason why it works without including 'FUN = mean' is because the aggregate() function recognizes that the third argument is 'FUN', so you don't actually need to write it in. There are a lot of aggregating functions in R, so the choice is yours. In base R, there's aggregate(), tapply(), and by(). There are slight differences between these in terms of how you write the input and the format of the output (for example, aggregate() spits out a data frame while tapply() gives you a table). The _apply() family of functions also tends to have shorter run times (but this only tends to matter if you're processing &gt;1 million observations). There are also a bunch of packages with similar functions - dplyr is one of the really popular ones which uses group_by() in conjunction with summarise() to achieve the same effect, as pointed out by u/unclognition. dplyr is generally redundant to base R (as in everything you can do in dplyr you can do in base R), but just because it achieves the same effect doesn't mean it's not very useful. It was designed to be more similar to English and the function inputs are better standardized compared to base R (ex., the first argument is always a data frame, and the output is always a data frame). One advantage of summarise() over aggregate() is that summarise() can aggregate data across multiple functions at once (ex., if you want the means, medians, sd, counts, etc. all in one go). 
Thank you very much for explaining this to me! I appreciate it a lot and it really helps!
ok gotcha! I only have used formula for regressions, didn't realize it had other uses to like this. Thank you very much for explaining this to me and taking your time to code this out.
You want to delete the values that are -99 in every column...then what? What happens to the values in those observations – do you set them to some other value? NA? Let's say you want to take a dataframe and change all -99 values to NA. You can instead define a replacement function, then mutate all columns and apply that replacement function. Something like: library(dplyr) data &lt;- read_csv("path/to/data.csv") replace_func &lt;- function(val) {ifelse(val == -99, yes = NA, no = val)} data &lt;- data %&gt;% mutate_each(funs(replace_func)) Note: I haven't run this in R, but it seems more or less okay. Let me know how it goes!
Thanks. I work on analytics occasionally and my new employer has Atom on their approved software list, but not R studio. I was wanted to get people's opinion before pushing in IT to use it. 
The kable() function in the knitr package is an easy way to display objects as a decent looking table. Be sure to check the function details to use it as you want. Better tables can be made using LaTeX coding, but that takes a while and kable() may be sufficient for your needs.
Can you draw it by hand? Just sketch it for us. How many intervals do you have? I can see they are overlapping, should they be overlapping on your plot?
Good. That helps. So could this graph be described as trying to graph each interval separately and then combine all of the interval graphs into a single graph for the final product?
I'm not sure how great of a format this is – after all, can students have variable course enrollments? This is why a long data format (what you have above) is generally preferred. Otherwise, yes, go checkout tidyr (tidyr::spread() may be your best bet) and dplyr.
library(tidyverse) library(stringr) df &lt;- data_frame(id = c(12,12,12,14,14), term = c(201610, 201710, 201610, 201610, 201620), course = c("ENGL209", "MATH209", "PSHY220", "ENGL219", "CMPS209"), grade = c(67, 77, 70, 90, 67) ) %&gt;% tbl_df df2 &lt;- df %&gt;% group_by(id) %&gt;% mutate(obs = row_number(id)) %&gt;% gather(variables, value, -obs, -id) %&gt;% arrange(obs) %&gt;% unite(observation, c(variables, obs), sep = "_") %&gt;% spread(observation, value) %&gt;% select(id, term_1, course_1, grade_1, term_2, course_2, grade_2, term_3, course_3, grade_3) df df2
Sorry about the formatting, not sure how to get it to accept the line return. You could find a programmatic way to sort your columns, if needed, but this should do the trick otherwise
Google "long to wide format R" 
Look into using rbind or rbind_rows to add the new row, and then dplyr::arrange to sort it as you prefer. 
What in the world Why not have the course as the column, grade as the value, and id and term as the first columns?
You can also use the 'spin' function on base R files. In either case, basically plot commands in the code just produce figures in the html or pdf output files, the .rmd files only contain the code like a .R file (but in code blocks)
Have you check out the browser console to see if any errors are thrown?
 summarise(group_by(ecls, catholic), n_students = n(), mean_math = mean(c5r2mtsc_std), std_error = sd(c5r2mtsc_std) / sqrt(n_students)) Think of it as "and then". So take ecls then group it by catholic, and then summarise those particular results. It avoids the issue of nesting steps in that the first step is the furthest into the middle of the code, and functions that are applied in the later end of the nesting have the function name and it's arguments at opposite ends of the code. It makes it far more readable while allowing for complex tasks. 
Easiest way to think about it. `x %&gt;% f(y)` is the same as `f(x,y)`.
They didnt but that may have been coincidental
You know what, you're completely right. The only thing is, teaching something technical with the top down method via a static medium can get bloated really quick. My two statistics textbooks would go from 700 pages to 2x or 3x their page count if it went that direction. In these scenarios, it's probably better off labeling itself as a theoretical tutorial.
My guess is that Shiny's menu elements only interact with the ui and do not have a connection to the server. 
The menu elements do interact with server, which is why observe({input$whatever}) works. I did eventually figure it out if your interested. The output was being suppressed because the browser considered the button to be in a collapsed menu and thus not active. Quick fix: outputOptions(output, 'buttonname', suspendWhenHidden=FALSE)
Here is the code in case the r-fiddle doesn't work: library(stats4) x &lt;- c(63.93, 58.48, 50.50, 48.00, 46.00, 42.80, 36.25, 53.90, 53.47, 59.11, 65.80, 64.80, 64.59, 62.72, 54.36, 55.50, 45.30) m.grd &lt;- seq(45,65,length=50) s2.grd &lt;- seq(40,250,length=50) ll &lt;- matrix(0,ncol=50,nrow=50) for ( i in 1:50){ for (j in 1:50){ ll[i,j] &lt;- sum(dnorm(x,m.grd[i],sd=sqrt(s2.grd[j]),log=T)) } } minus.ll &lt;- function(mu,s2){ -sum(dnorm(x,mean=mu,sd=sqrt(s2),log=T)) } # How do I make the following function work in place of dnorm() # minus.ll &lt;- function(mu,s2){ # log(prod(1/(sqrt(2*pi*s2))*exp(-(((x-mu)^2)/(2*s2))))) # } output &lt;- summary(mle(minus.ll,start=list(mu=0,s2=1))) estimates &lt;- c(coef(output)[1],coef(output)[2]) contour(m.grd,s2.grd,ll,xlab="mu",ylab="sigma^2",col="red") points(estimates[1], estimates[2], col="black",pch=3,cex=5) persp(m.grd,s2.grd,ll,xlab="mu",ylab="sigma^2",theta=45,col=360) 
And ctrl + shift + c to comment/uncomment one or more lines.
Here is a comment from the blog-post you linked to: &gt; In emacs comment a selected region with ALT+; But I can't make guarantees if that works or now.
&gt; Any other computer programming language I can think of allow block comments Then you don't know very many languages. There are lots that don't have block comments. In fact, some languages that technically support block comments discourage their usage (e.g. C++) because its block comments can't be nested, creating the risk of accidentally uncommenting something by having a nested block comment. That, together with the fact that block comments are really strictly unnecessary, is the likely reason R doesn't have them.
I select the region and then hit Ctrl-1 to comment, Ctrl-2 to uncomment.
Understood. Thanks!
Ah okay thank you! Swirl just suddenly had me writing in the text editor after using the console for 7ish lessons and I was confused by the sudden change. 
So I reformatted your prime_list function so I could read it in multiple lines ---&gt; prime_list &lt;- function(x, y){ for(i in x:y){ if(prime(i)){ next } print(i)} } So let's trace the loop. What we have here is... **For** the numbers **i from x to y**, **if** the number we are currently on **is prime**, go to the **next** number. If the number isn't prime, so we don't move on to the next number, **print** the number. This means that whenever the number is prime, we will go to the next number and do nothing. Otherwise, we will print the number. This means the code will print composite numbers. What the "next" function does is stop the current iteration of the loop and move on. This means that whenever you got to a prime number, you would say "next" and not print it, while the composite numbers would not receive a next, and therefore would print. The loop you want is... **For** the numbers **i from x to y**, **if** the number we are currently on **is prime**, **print** the number. Otherwise, do **nothing**. We could achieve this with ##init function prime_list &lt;- function(x, y){ ## start loop for(i in x:y){ ## check if i is prime if(prime(i)){ ## if i is prime, print i print(i) } ## if i is not prime, the print function will not run ## Nothing is here so nothing will execute, as you want ## then we will reach the bottom of loop and iterate ## loop iterates here } ## end function } We could do the otherwise statement in many ways. I did it such that since we basically want no output when number is composite, I can just have it run nothing after checking if the number is prime. It will check if number is prime and print or not, then run nothing, therefore achieving the functionality we want. A more transparent way to do it could be to have an if-else statement, having a "next" or a empty else statement for when the number is not prime, but that's more code and more effort. I think that explains it? I'm going to sleep but if you have more questions maybe reply and someone else can help, or i can answer when i wake up 
Also, putting my "interview hat" on, your `prime()` function requires O(n) linear time because you check every number between 2 and x/2. Can you think of a way to optimize it so that you don't need all these numbers? (There are two correct answers).
&gt; I only need to check the **prime** numbers While *technically* correct, what is one property that (almost) all prime numbers share? Hint 1: A property shared by all primes except a specific prime. &gt; I only need to check the prime numbers from 2 to **x/2** Hint 2: Bold is incorrect Let A, B be the divisors of a composite number, such that A &lt; B, WLOG. What is the upper bound of A, the smaller divisor? 
But im sure swirl explained it to you, didn't it? 
Does it take a while to run because there are a lot of cases you are running through the model? How long would it take to just run through 1000 cases? 100 cases? Also: no, I don't think your proposed method makes any sense. 
&gt; All prime numbers except 2 are odd Correct. &gt; I only need to go from 2 to sqrt(x) Correct. In R, you can describe that as the sequence `seq(3, ceiling(sqrt(x)), 2)` which means "from 3 to the ceiling of square root of x, by 2". Then concatenate 2 to the front to check if the number is even. Either keep doing what you're doing (check 2 separately) or concatenate vectors like this: c(2, seq(3, ceiling(sqrt(x)), 2))
It's just a matter of specificity. Ceiling turns decimals into integers and if you're not sure how seq handles its bounds, it's best to be conservative. In this case, seq is inclusive. Other languages (such as python range and xrange) are exclusive. Mathematically, you're right. 
That's what I do, but it doesn't answer the block comment question. 
Are you mainly looking for R focussed books? Or more theory behind quantitative trading and different strategies and approaches? 
Mainly R focused books with an emphasis on financial applications. But if you've got recommendations for theory books I'd love to hear those as well!
Not as well as featherfooted
True, I don't know why the language developers chose to not include a way to do block comments. It's not unique to R, but I don't know why some languages such as R don't have it.
Take a look at the %in% command as well as dplyr::contains to get started. Depending on how new you are, you might need to look into bracketed sub-setting and $ sub-setting.
Ill look into it. thanks! I've learn both type of subsetting but i prefer using $&lt;name of col&gt; as i find it easier. 
Thanks! Looked into the grepl() function. It is very useful thanks!
There is nothing wrong with reduce but reduce is a binary function, so it merges two dataframes at a time, which makes it slower. 
Agreed. If there are lots of job titles and they will need to be updated it makes sense to store that externally in a CSV or even a MySQL database.
Oh so that failed experiment is still alive
Hate that thing because of bad experience: I created one topic in quite some detail. Then a few high-rep users commented among themselves and decided to split the topic into 3 distinct parts. So they split it into parts and got all the "credit" for creating it, while my post was deleted. Maybe it's a good idea, but for now I don't think it's working. I have several other topics there, but they never had any new activity. I think the users who ask questions on SO are mainly people who are lazy and don't read original documentation. So they will not suddenly start reading SO documentation. And this is specially true for the `R` tag.
What is dead can never compile. 
It's interesting seeing what people think of it from different programming communities, and I want the feature to take off beyond just being a little experiment. 
One of the terms for the predict() function is newdata, which takes a data frame of values to predict. Just make sure it has a column with the same names as the x variable you used to build the model ( height in your example). Check the help text for predict or predict.lm to see all the details. 
Suppose we have a data frame with height and weight and we want a model for weight ~ height: df &lt;- data.frame(height=c(70,66,69,74,71), weight=c(223,121,190,340,210)) m &lt;- lm(weight ~ height, data=df) To predict on new data, create a new data frame with the values you want: df.new &lt;- data.frame(height=c(60,70,80,90)) weight.pred &lt;- predict(m, newdata=df.new)
But you can't ever be sure past your data on the y or x axis the model doesn't change from x to x^2. 
You can make out of sample estimates, but not out of range. Like if you have 3 data points of x = c(35,76,12), youc an use the model to predict x = 54, but not x = 11 or x = 77. Sometimes people go out of range anyway, because "its the best of what we can do given what we have," but as a practitioner, I would not really pay much attention to such values; it'd just be a neat curiosity, but not something to build a product or build an academic theory out of. 
Yes, which is why you generally want some theoretical foundation for your model (this is econometrics). You can never be *sure* of anything when working with samples. That is statistics. But if you have a well-specified model it *should* be able to make out of sample predictions. Again, that's often the point.
We definitely went to different graduate schools
So what's your question?
I think you might be in the wrong subreddit. Is this R?
I usually create the vector before hand e.g. output &lt;- matrix(nrow=nrow(vector_1), ncol=1) output &lt;- as.data.frame(output) colnames(output) &lt;- c("name") My for loops are different to yours though. I'd have, for(i in 1:nrow(vector_1)) Then in the loop you could have something like, if(vector_1[i] &gt;= 28) { output$name[i] &lt;- "A" } 
You should try to avoid loops in R, because they are slow and you will save mental overhead. If you want to use if-else constructs, you can write a function and then use sapply. letterGrade &lt;- function(x) if (x &gt; 28) { "A" } .... else { "F" } numeric_grades &lt;- c(27, 29, 15) letter_grades &lt;- sapply(numeric_grades, letterGrade)
 set_attributes &lt;- function (obj, att, val) lapply (obj, function (obj, att, val) {attr(obj, att) &lt;- val; return (obj)}, att, val) mtcars$carb &lt;- set_attributes(mtcars$carb, "gear", mtcars$gear) attr(mtcars$carb[[1]], "gear") What I get is [1] 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4 But What I want is [1] 4 Thanks.
If you are not sure what the size of the vector will be when you are done, use rbind(), or the ugly but easy trick x &lt;- c(x,y).
If you can't set the attributes on each member of an atomic vector then why does this work? attr(mtcars$carb[[1]], "foo") &lt;- "bar" attr(mtcars$carb[[1]], "foo") My problem is that the whole vector is assigned to the attribute for each member. I need to figure out how to just assign the corresponding value.
It doesn't. R version 3.2.5 (2016-04-14) -- "Very, Very Secure Dishes" Copyright (C) 2016 The R Foundation for Statistical Computing Platform: x86_64-w64-mingw32/x64 (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. &gt; attr(mtcars$carb[[1]], "foo") &lt;- "bar" &gt; attr(mtcars$carb[[1]], "foo") NULL &gt; The question is then, what have you done to `mtcars` prior to it?
What is the separator between values? Space, comma, tabular....?
Try running your file in the R interpreter outside of RStudio. Just to eliminate that it is anything related with your current session. It really sounds like a par issue though. Try par(mfrow=c(1,1)) again as well
R^2 will always approach 1 as the degrees of freedom tends to 0. This is why we have adjusted R^2, to penalize for the number of coefficients we include in a regression model. A model can have a R^2 of 1 and be complete garbage. All of this has nothing to do with making out of sample inferences. In many cases, the goal of inference is to observe statistics from a sample so that we can understand more about the parameters of the entire population. If you are repeatedly observing statistics from samples that look nothing like your population, then what exactly are you doing?
I understand that. You don't have to use the command line.
dev.off() until you get a warning that their is no device to close, then retry. Your par() should then be reset to the basic settings. The issue arises somewhere before the line of code. regarding the subQ: no idea, I actually can't recall a function that requires a user input that can't be provided as a parameter of the top of my head
Rmarkdown is the way to go.
Are you sure a dialog menu doesn't open up somewhere but wasn't brought to the foreground?
It doesn't open anywhere as far as I can see...
yes
There is a difference of using multiple threads on one computer and distributing them in a cluster. Assuming the first (I have no experience with the second) parallel, foreach or multicore should do the job. However a while loop might/wont work directly, instead you have to add a second internal loop that is parallelized which computes 32 or 50 models at once and before the while loop checks again if there were enough good models (alternatively you set it up that you have 500 parallel calls and each is a while loop that exists when it found a good model)
I second foreach, and also the advice /u/murgs gave you. Here's how you'd setup a foreach on 2 cores: cl = makeCluster(2) registerDoParallel(cl) results &lt;- foreach(n = 1:500, .combine = rbind) %dopar% { output_good = FALSE while(!output_good){ output = runsim() output_good = is_output_good(output) } output # assumes a dataframe } In this code, you will get 500 answers, that each get bound together into a dataframe called results. On each cluster, the while loop will run runsim() until its results are good. The final output line is what gets rbinded by foreach back into results.
Thank you both! Will try your suggestions tomorrow :) great idea to put the while loop inside the function, I was trying to have it on the outside which wasn't working. 
How would you do it by hand? Literally, with the data frame on a sheet of paper (or just on screen), write down in hand how you would do your calculation, step by step.
Well, I would look at a row, and see which cell is the leftmost empty cell, and count the "cubes" that way. The main problem is how to stop the counting between the "Walking" and "Not_Walking".
I'm having trouble understanding your data structure, is it possible to reformat it into a data.frame? https://www.datacamp.com/community/tutorials/15-easy-solutions-data-frame-problems-r#gs.tKLXafs
Here is a start if you want to do a loop... for(i in 1:nrow(myDataFrame)) { row &lt;- myDataFrame[i,] # Your logic here } Basically I would set a variable, keep adding to until I saw the not_walking and then add it to a vector or whatever you wanted to do with it in the first place. Normally loops in R are almost always better using apply/sapply/mapply. I wish I would have forced myself to do those earlier.
Yes. Use as.Date() with the ` format =` argument. Look up the details for the format argument with: help(strptime) 
Perfect! Thank you! One quick question: is "%M:%S" not a valid format? I didn't see it anywhere and my code runs fine when I have "%H:%M:%S" but not "%M:%S
How are you importing your data? You may need to use the infamous stringsAsFactors = FALSE argument first.
Ah okay. I realized that the problem was that some bits of my data were actually in HH:MM:SS format to start and for some reason I can't get R to accept both that format and the MM:SS format at the same time. For example when I did %M:%S it changed 01:05:42 to 00:01:05. If I use %H:%M%S it makes all times under an hour turn to NA.
This is totally doable with RSelenium combined with rvest. Getting Selenium setup with Windows is a pain but once you get it going, scraping a site like this is dead simple. 
Look into the documentation for *apply functions (or use map from purr). Sometimes loops are fine, sometimes they aren't. The pitfalls and benefits can be googled. My favorite part of lapply is how stupid easy it is to parallelize (using parLapply). Mostly *apply functions save time in typing and are a more elegant solution than for loops, but not always faster.
Are you talking about the first link or the second link? The first link is more other stuff like mactex and cool fonts. The second link seems like overkill but I'm curious why he has it set up that way. 
It would work for neural networks?, because that's my goal 
It'll work for applying any function to a list. 
With tidyr package, you could do: df %&gt;% select(time, id, type) %&gt;% spread(type, time) %&gt;% mutate(diff=c - a) You might have to swap type and time in the spread function.
There are so many great libraries :D I tried to use `dplyr` before but this seems more understandable. Thank you very much! 
Thanks so much for your help! I'll test it out tonight 
Thanks
I know the R package ggmap has pretty useful functionality. I am a student working in Institutional Research and Analysis for a Canadian University. Last week I used a function called geocode() in which you can input a postal code and it spits out a set of longitude/latitude coordinates (pulled from Google database). From this you can plot the coordinate points on a map using get_map() and geom_point(). From there why not try cross referencing those points with known coordinates of the service locations, performing a task such as deducing to which point exists the shortest distance. Just an idea! 
&gt; If I just do a normal homebrew setup and decide I need those compilers later, would it be a huge issue to install them? No, none. That’s why I recommend this incremental setup: you *can’t* a priori know all the tools you’re going to need at some point. And while it makes sense to install the most common ones in one go, I believe that my post covers all the essentials.
That's not bad! But I really need just the data, not visualization. I'll need to eventually export a csv with the 'nearest' city name. 
I'm a recent graduate and am interested in this as well! 
Start with the [zipcode package](https://cran.r-project.org/web/packages/zipcode/zipcode.pdf) for a list of all the mailing zipcodes in the US and their lat/long pair. You can then maintain a list of your facilities and write a function to compare and calculate distance. I believe the zipcode package has 2012 data from the USPS.
Kaggle
I am trying to do a similar thing but for some reason when I use the spread function within tidyr I get the error Error: Duplicate identifiers for rows (2637, 2638), (2805, 2806), (2854, 2855), (2913, 2914) My dataframe looks like OPs but with different column names. But the idea is the same. I have multiple rows, one for each combination of sampleID and toxin measured. I want to make one row for each sampleID and columns for each toxin measured. My code looks like tidyr::spread(df, toxin, result) Any tips would be greatly appreciated
I don't have any guided projects but I'd definitely look into stuff you want to automate in your life. Like for me, I have to send out the same e-mail everyday based on some data I get everyday. Although that's boring you can apply it to other stuff! Something that can be cool and current, right now, would be scraping some NBA playoff stats and having R send those in e-mail to your friends. There'd be a lot of documentation for that and you can piece together stuff you actually want. :) or not idk what's cool 
Have tried sapply(mycolumn,FUN=paste,baseurl), or some such?
That's not bad actually. Do you have any documentation for this? I can google it also
**Here's a sneak peek of [/r/rstats](https://np.reddit.com/r/rstats) using the [top posts](https://np.reddit.com/r/rstats/top/?sort=top&amp;t=year) of the year!** \#1: [The xkcd package for R](http://xkcd.r-forge.r-project.org/) | [15 comments](https://np.reddit.com/r/rstats/comments/5v6z2l/the_xkcd_package_for_r/) \#2: [Top 50 ggplot2 Visualizations - The Master List (With Full R Code)](http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html) | [13 comments](https://np.reddit.com/r/rstats/comments/5np36c/top_50_ggplot2_visualizations_the_master_list/) \#3: [Announcing RStudio v1.0!](https://blog.rstudio.org/2016/11/01/announcing-rstudio-v1-0/) | [10 comments](https://np.reddit.com/r/rstats/comments/5ak26h/announcing_rstudio_v10/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
The geocode() function also returns the data so you could make your own database if you wanted.
Input 3 needs to be in the server section of your app. You can use the renderUI function to make this input dynamic output$input3 &lt;- renderUI({ use inputs 1 and 2 to filter the data table to make a vector of choices called my_choices selectInput('input3', choices = my_choices) }) You can reference the dynamic UI in the UI portion of the code as follows uiOutput('input3')
This sounds like a similar issue I had in a shiny app I wrote about a month ago. I ended up using `observe()` and `reactive()` functions to monitor any changes in my select input and then limit choices based on those selections. Below is my code used my server function: limitIndustry &lt;- reactive({ sem &lt;- input$semesters hd &lt;- subset(hd, hd$Semester==sem) return(c("Choose One" = "", unique(as.character(hd$Type)))) }) observe({ updateSelectInput(session,"industry",choices = limitIndustry()) }) So I had the user choose what semesters they wanted to look at and then had a function watching if those inputs changed and then subsetting some data based on the semesters they chose. You can also incorporate if statements and supply different 'choices='. 
Just use `paste0`: paste0(base_url, df$URL)
I haven't worked with distance functions at all. How would I calculate the distance between two zips or two lat/long pairs? 
They aren't all that guided, but I took a really good class in R a couple years ago and the class site is still up with all the assignments: http://eeyore.ucdavis.edu/stat141/Homeworks.html
DataCamp is the best way for learn R code within the Big Data world 😎
We'll need a bit more information. I just copied the text verbatim, and it all works as expected.
Totally understand. I made the leap assumption that if they are into cryptography they probably already leverage these languages. 
You are an angel. 
Thank you - this solved my problem 
oh wow you're right. thanks!
Please let me know if you find an easy implementation of a cryptologically secure RNG. I have a VERY amateur text encryption function i've been playing with: https://github.com/Adamishere/RNG-Encryption/blob/master/Encyption%20Program.R I would like to switch out the Mersenne Twister with something more secure. 
Because names that short aren't allowed
awwww I was afraid that was the answer.
Your post contains no text. I assume this is a mistake. When you ask for programming help, the most important thing is that you provide a minimal reproducible example - I should be able to take your example and paste it into an R session and see your error. The code you provide should be the smallest example you can create that still makes the error happen.
That's true and a subtle but serious gotcha. You need to use a list aware apply function, either lapply or sapply. 
If is not vectorised. Most of the time you want ifelse.
Sounds like you still might not be using if/else correctly, there shouldn't be any commas, so to be honest I'm not sure how you rewrote that code. Take a look at [this resource](https://www.programiz.com/r-programming/if-else-statement). Your syntax should look like this: if (condition A){ execute 1 } else if (condition B){ execute 2 } else if (condition C){ execute 3 } else execute 4 Make sense?
Have you looked at the table() function? You can summarize by a list of variables (in this case site and animal type). There are plenty of other ways to go about this but this is one of the easiest (and is part of base R). 
There is a website that searches Google for R related things only, I'll post the link later.
I am interested in more of this. :) What packages did you use? 
Lots of ggplot2 for the control charts and most of the plots in the reports. knitr for generating the reports (and some rmarkdown, but only when I have to output to a word doc). rvest, XML, scrapeR for getting data from the web into a database. scheduleR to schedule scripts to run automatically.
* Grab data from Web APIs, crunch it, turn it into HTML reports, email it to people as a CRON job daily during a big product launch * Construct SQL queries to grab data relating to lots of experiments, grab the associated metrics (with various definitions), run statistical analyses, produce powerpoints of the results * Grab pre-defined audience segments from a DB, do some modelling to find high-value audiences, push them to APIs such as Facebook for targeting through paid media and to eCRM systems for targeting with emails and push notifications. * Perform paid media attribution modelling, running lots of different models with massive data on a regular basis to benchmark different models against each other.
I haven't worked with databases before. Would you mind running through a quick explanation of how it would look like? I'm imagining MySQL but I wouldn't know how a zip code database would look like and how I can 'query' it to view the closest operating 'tract'. Thank you! 
I have automated a generation of a 3 days excel work of a coworker that pre select clients. Now take an hour because some databases still are in excel... A daily report of status of sales.
What packages are you using to get R to talk to the APIs?
Why not just set up a shiny dashboard?
Many APIs already have packages (eg RGA). Those that don't, there are guides to using httr as a base to make your own package
So... I have been trying but my issue is that I can't get the data in a centralized place for the app to reference. Right now I'm using Google Sheets and I could write to the sheet with R but my data is not too long and it times out. If you have any idea, I'm desperate to hear haha Do you work with dashboards? I'd really love to check out your GitHub page 
I'd love to hear more about attribution modeling or any helpful links as I am doing the same thing
Besides all the jazzes you can automate with spreadsheets and datbase, charts, etc. Here's something interesting I've used R to automate on: So in my office there's a co-worker that, out of courtesy, would take lunch orders from everyone at the office and call for take-out. I used to request her everyday what kind of food I want to have on that day and she'd gladly order them. One day due to high amount of work I forgot to make a request, as a result I was starving for that afternoon. So I decide to write an R script that randomly pick out a dish from a pre-selected menu (mind you, it knows what dish I favor, so not truly random) and email her every working day at 10:30am. It can even keep track of what I had in the past to make sure I don't have Egg roll for the whole week. So far my lunch-lady doesn't trust it...
Please tell me it's called Goog-R
Just view the raw readme.md to see what they did to get those
How many files of this "structure" is there? Only this one file? Copy-paste the relevant lines; you can parse it through read.table by not specifying a filename but giving a string to the `text=` argument. More than one file? The file is mostly unstructured. You would have to deduct a general structure from looking through multiple files.
`mean(df$column, na.rm = T)` `colMeans(df, na.rm = T)` `apply(df, 2, mean, na.rm = T)` 
Dude, you're awesome. So the first one is one column. Easy enough. What is the main difference between the second and third one, though? How would I know which one is the best to use? The second one seems easier... 
`help(apply)` `help(colMeans)` https://www.google.com/search?q=r+column+mean
Thank you very much!
You can also just host it on a dedicated server to a specific port on a network and have that port direct to a web URL. You'd just need an SSL cert. just set the R script to auto run on start-up. Unless you're going to have 10+ active users constantly.
Here are the instructions: https://www.quandl.com/tools/r Try the github option: install.packages("devtools") library(devtools) install_github("quandl/quandl-r") library(Quandl)
Best I can figure is there's a problem with the dependent variable in your formula? https://www.rdocumentation.org/packages/Matching/versions/0.48/topics/MatchBalance
[Here is what happens when I initially run just the first line.](http://imgur.com/a/J5v9W) 
An excellent book. 
To just get the dataframe of the 4 columns and all rows you can do this. for(columns in seq(1,800,4)) { tmp.df &lt;- df[1:nrow(df),columns:(columns+3)] } Then do with that tmp.df what you will. 
Write more in functional programming style than in imperative style. Advanced R by Hadley Wickham is a great book.
you can use dropbox as a csv file repository and read/write using the rdrop2 package.
A far easier way to replace zero values with NA is: d$volume[d$volume == 0] = NA Rather than looping through values you can use vectorisation. It's worth seeing what `d$volume == 0` returns, and how you're subsetting your data on a Boolean vector.
No need for a function to change 0's to NA's. df$target2 &lt;- ifelse(df$target == 0, NA, df$target) This will create a new column named target2. target2 will be the same as target, but with 0 replaced by NA. If you really want to use the function you've created, the $ operator doesn't work with variable names. You'll have better luck using bracketted subsetting. When you find yourself needing to manipulate data.frames in several ways often, you'll probably want to look into the dplyr package. mutate(), filter(), and select() are the ones I find myself using over and over again. 
are you dealing with large dataframes? or dataframes that may or may not have missing values? how many values are you using for your join keys? here we have the following: &gt; library(dplyr) &gt; &gt; left &lt;- data.frame(x = c(1, 2), y = c(3, 4)) &gt; right &lt;- data.frame(x = c(1, 3), z = c(5, 6)) &gt; &gt; df &lt;- left %&gt;% + left_join(right) Joining, by = "x" &gt; df x y z 1 1 3 5 2 2 4 NA if you have no missing values in your right dataframe, then you know whether or not a value would be included in an inner join if there are null values after your join. you could do something like: &gt; df %&gt;% mutate(inner = ifelse(!is.na(z), 'in right', 'not in right')) x y z inner 1 1 3 5 in right 2 2 4 NA not in right or in a single chain &gt; left %&gt;% + left_join(right) %&gt;% + mutate(inner = ifelse(!is.na(z), 'in right', 'not in right')) Joining, by = "x" x y z inner 1 1 3 5 in right 2 2 4 NA not in right if you may have missing values in your right dataframe, so you can't assume that just checking for nas will be sufficient, you can just do a `%in%` mutate. left %&gt;% left_join(right) %&gt;% mutate(inner = ifelse(x %in% right$x, 'in right', 'not in right')) where x is your join key. if you have multiple join keys, it would get more complicated, but follow the same basic idea. another way you could do it would be to duplicate your join columns in the right dataframe, and then you know if the combo of them are null, then they would not be in the inner join. &gt; right$x2 = right$x &gt; left %&gt;% + left_join(right) %&gt;% + mutate(inner = ifelse(!is.na(x2), 'in right', 'not in right')) Joining, by = "x" x y z x2 inner 1 1 3 5 1 in right 2 2 4 NA NA not in right
Thank you so much! I didn't realize the behavior of variables was as stated above. I've used dplyr before for grouping data frames, so I'll check it out!
Ah, I didn't realize using the symbol = could be used for calling indices. Is there a good indexing guide for R? I'm used to some pretty formal python syntax. My teachers in college stressed formalism over functionalism.
Terms ending in 30, meaning terms worse remainder after division by 100 is 30: ending_in_30 = df$term %% 100 == 30 Replace those: df[ending_in_30, 'term'] = df[ending_in_30, 'term'] + 50
I see, thank you! I'm new at R and understand some of what you said (maybe not all but I'm getting there), and class() reveals they are not SAS files. So what would you change exactly in my code to remove the .sas7bdat extensions from the table names (I want to keep what comes before the .sad7bdat)? I imported hundreds of tables and don't want to type that extension every time I want to analyze a table (or manually change the names one by one if possible). Also, if I wanted to automatically save all of them into a different folder as .csv for future purposes after importing them (basically convert all of them automatically so someone else can use them in Excel or whatever), would that be possible? 
If you *only* want to write them out as CSV files, and don't intend to use them for anything in R, then don't even bother using assign() and just write them out: lapply(list.files(pattern="*.sas7bdat"), function(x) { readr::write_csv(read_sas(x), path = paste(tools::file_path_sans_ext(x),".csv")) }) This code is very simple; `lapply` is just a more "R-like" way of doing a for loop that applies the function to each element of the list. `readr::write_csv` is a function from the readr package that writes its first argument to the location in the `path` argument. The first argument (that will be written) is your `read_sas` call. The path uses the function `file_path_sans_ext` from the `tools` package to drop the extension .sas7bdat from the file name, and uses `paste` to add .csv to the end. Done.
Yeah, I'm using shinnyapps.io Would you know if I can automate updating the data with R? Right now I have a python script that fetches the data for me but I couldn't work with the Google Sheets API and I still manually have to upload the data. 
Not particularly a office task, but at the end of every month, I generate a report with spending and investments summary/analysis.
list_of_char_vecs &lt;- strsplit(as.character(list[,2], "") gives you a list of char vectors so you can then loop over the list and do your vector comparison (e.g. lapply(list_of_char_vecs, '==', "Y")) the important part is that you don't want to unlist the results, because then you lose the information which chars belonged to which row alternatively you can also use grep() to find occurrences of 'Y' in your strings
Here is an analogy: wood is free, but the violin that you make out of it is not. you do not sell "scripts" you sell your ability to write them (and i suspect you undercharge too). In terms of making a product for a client unless writing documentation IS the part of you deal make sure your script is extensively commented with call examples embedded into 
Yeah, I have no qualms selling prepackaged solutions based on R. Time is time.
I'm in academia too, just about to defend :).... :/ Many of the scripts I created have already been cited in various papers I've written but this particular implementation is not. I agree about the time/expertise as the product being sold. It's just good to hear it from the outside to make sure I it's inline with common practice.
My thoughts too. Just wondering if this is something people do on the side or for contract jobs (ie it's acceptable practice).
You should have access to the entire site from FTP. Try adding the directory you want after the final / in your path. Right now you are dumping the files in the root of your server
What was the point of this link? 
The short answer to this question is yes, just use tools::file_path_sans_ext to remove it from the name. The long answer though is that you shouldn't really be doing this at all. It would be much more R-like to assign each data frame as an element in a list. It might also be worthwhile creating a separate environment to keep them in, but a list will allow you much more flexibility if you want to be able to do stuff to each of those data frames in turn later on, which is presumably the point of the exercise. 
I thought this was a very interesting resource for learning to do text analytics in R. It's sort of a repost, but this particular link has the content formatted in a very user-friendly way and I thought it was worth sharing.
How did you get the R script to interact with Salesforce?
Are you trying to import the excel file? You need to save the data as txt or csv before you can import it using read.table(). And always take a look at the imported data with str() to see if it worked the way you intended.
Other helpful tips Asking questions is a great way to deepen your understanding but to get the most out of it: - you will get backlash from people like me who read stuff before their morning coffee, cracky or annoyed, try to ignore the bitterness and see of there is any advise in their answer - identify the smallest possible example where stuff goes wrong, is it the reading in part, or since actions further down the road, does it happen all the time, with all the files? (This also helps you in searching for an answer. - try searching for an answer first, you'd be surprised what others have asked. Include in your search: name of the package, r, and the problem. So f.i. "r open excel file" or "r readxl how to open file" 
Try df &lt;- read.csv('the file path') plot(df$TIME, df$MASS)
I apologize if you already basically answered this, but is there any way to remove ".csv" *after* they have been imported? I just want them removed from [here](http://imgur.com/a/W1vIh) so I don't have to type ".csv" at the end of every table every time I want to pull one up. I'm very new to R and can't seem to get the exact coding to work. Once this is done I can begin the actual project hah. Any way you could provide the exact code that may work? 
I don't know why I'm having such a hard time with this. Here is my original code: #Reformat the SAS files as Excel files and store it in the same location. lapply(list.files(pattern="*.sas7bdat"), function(x) { readr::write_csv(read_sas(x), path = paste(tools::file_path_sans_ext(x),".csv")) }) #Delete the redundant SAS files. listoffiles&lt;-list.files(pattern="*.sas7bdat") file.remove(listoffiles) #Import the Excel files. newfiles&lt;-list.files() for(i in 1:length(newfiles)){ assign(newfiles[i],read.csv(newfiles[i]))} Any way you could alter the code so I can see exactly what needs to be changed? All of my changes do not work :/
Agree, this is a great intro to basic text analysis and love the package. There are quite a few text packages by this is one of my faves for sure. 
Not data$TIME ?
Well, I thought I had it figured out: Able to call spread without error and then... Rstudio crashed every time I ran it. Back to reading more!
how do you decide which one to use? i'm familiar with `tm` but it seems like my team uses `tidytext` more often. it still is up to me what i use, just wondering what the pros and cons are btw the two. i also do subscribe to the tidy philosophy so i'm pretty sure i'll be going with tidytext anyways, but i'd like to hear your thoughts.
I'm not sure how different they are in the end. One may be faster, but that's just a guess. I think with a lot of R packages it comes down to what you are most comfortable with, with caveats for large data and special features. My approach generally is to do as much as I can in the packages I am most familiar with before bringing in other packages. Between tidytext, stringr and text2vec I don't find myself using tm often. 
This worked for me: library(tidyr) ID&lt;-c(123,123,234,234,234,345) Day&lt;-c(1,2,1,3,2,1) Score&lt;-c(5,4,30,34,25,12) df&lt;-data.frame(ID,Day,Score) spread(df, Day, Score) Is tidyverse installed? Are you selecting "Day" and not "Day " or "day"
This question is quite vague. The answer depends of the fidelity you need. I don't think it will be possible to show all 120 numbers in a way that users will be able to accurately eyeball all 120 figures. So, what are the important facts that your users should be to discern after seeing your visualisation? 
It took a while of tinkering but finally got it after several days--you think you would be able to simply change the names in the global environment with a loop and sub or something, but apparently that's not the case. Quick question btw, as an aspiring R programmer, how did you learn about most of these tools? Did you read about most of them before hand, or did you learn them as you went? Trying to figure out how much time I should spend reading about R and different things before jumping into instant immersion. 
Heatmap? y-axis the sites, x-axis the year, fill the miles? Or you could have fill the deviation from the average for that site (or year). 
fread into data.table is the fastest way to read csv files
Use data.table
That'll be pretty boring, as it'll be just one point. But, if you really want to: plot(mean(dataframe$column1)~mean(dataframe$column2))
http://imgur.com/a/aLqKv Something similar to this
Ah OK. Seems so obvious now that you've said it! Thanks!
The difference is that with `sapply`, a new random number is drawn for every number in `v`, and added to it. Without `sapply`, the same random number is added to every element in `v`. You can see the difference easily when `v` is a vector of identical numbers: &gt; addrand(rep(1, 10)) [1] 28 28 28 28 28 28 28 28 28 28 &gt; sapply(rep(1, 10), addrand) [1] 28 39 59 92 22 91 96 68 64 8 That said, the example is indeed badly chosen because it can be rewritten easily to work without `sapply`: addrand = function(x){ ran = sample(1 : 100, length(x)) x + ran } addrand(v)
Careful — this is wrong. See my answer below.
OK that makes sense. I hadn't considered it looking at each element in v individually. Thanks! 
?cbind, round(mean(the_df[,1:2])) and sum(the_df[,3]) If I'm understanding the question correctly.
[R faq 7.31](https://cran.rstudio.com/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f)
There are probably other ways but. 1. subset the relevant variables and create an indicator collumn with a different key for each data frame. 2. joint the 3 df with a "rbind" or similar 1. use the "aes(group = key)" statement in ggplot. 
I see what you are doing here, but it is not quite what I need. You are essentially summing and taking the average of Orders and Regular orders, that would give me the average of those two but not the average of the whole data set. I already have those numbers. I would like to find a way to make a new row in the first data.frame so that the grand average sits below them with the total count (which could indeed be the sum of the two above) as well as label the row "Grand Average." Is that clearer? Thanks,
The part about constants doesn’t apply to R, nor to other functional languages; and I would strongly advise against following the convention of putting variables in `UPPER_CASES` because it’s simply not done in R (with a few weird exceptions in base R), and it’s plain unnecessary: The whole point of R is that *all* values are immutable. True, names *can* be reassigned but usually shouldn’t. That is, once you’ve assigned a value to a name, don’t change that name in the same scope (the exception is subset assignment and similar things, but with the existence of dplyr this too will become uncommon). Put differently, what information does it convey that `IRIS_TARGET` in the example is in all caps? By contrast, why is `target_variable`not in all caps? Neither is ever reassigned, nor should it. The same goes for *every other variable* in that example. The use of all caps here is utterly arbitrary and meaningless. Since we’re already critiquing this piece of code: [the `if` statements in that code are also highly problematic](https://www.captechconsulting.com/blogs/eliminate-branching-if-statements-to-produce-better-code), and are in fact a warning sign for code smell. Don’t do dispatch by hard-coding `if`…`else if` cases. Use the appropriate dynamic dispatch instead. Take the first case: if (DATASET == DATASET_IRIS){ data(iris) df &lt;- iris target_variable &lt;- IRIS_TARGET } … This can (and in this simple case, *should*) be rewritten as: df = get(dataset_name) target_variable = target[dataset_name] Where `target` is a lookup table for the variable names (and there’s no need for `data` anyway). Similarly for the choice of model. Rather than encoding the choice in a string ([seriously; this is a classic antipattern](http://wiki.c2.com/?StringlyTyped)^(1)), store a *function*, and call it directly: my_model = modelling_method(data, modelling_formula) etc. There’s a programming philosophy that **every** `if` statement should be regarded as suspect, and be replaced by dynamic dispatch. This is certainly extreme, and there are valid cases where an `if` makes the code simpler. But not here. --- ^(1) Though admittedly even established R libraries abuse it constantly; case in point: caret. Don’t emulate their bad behaviour, though.
Sorry I mixed it up a bit, you need rbind you just have to make a new data.frame out of the values to add it. Btw at the moment you are using an extra label column, but you could also use rownames (i believe then you could also just assign a new rowname the data to add a row.
So the read.csv doesn't work? try a basic read.table, but it's pretty weird. you're sure you don't have a typo somewhere? read.table("chicago-nmmaps.csv", header = T, sep = ",")
I will check those functions out when I get back to work, thanks.
Literally the first result if you Google your error message: https://stackoverflow.com/questions/24246982/read-sas7bdat-unable-to-read-compressed-file
&gt; I've tried Googling and haven't found a good answer. What *exactly* did you Google? We can help you understand why you didn't find any results.
Have you checked that the subsetted data frame is in fact populated? I.e. do `summary(Anotreze)`, as it is this data frame you are passing on to the regression.
Also, your second call to `lm` doesn't make sense as you are supplying both the two vectors from your global environment, as well as supplying an additional data frame.
I was wondering if you could just round a column after it had been created (I couldn't get that to work), but doing it beforehand worked. Thanks.
It is. The data I want to plot is in it.
*Which* values of the regression? The actual input, the residuals, predicted values, or the regression itself?
Ya can't. The object returned from ya `cbind` operation be a matrix. First column comprises a character vector, so whole matrix be a character matrix! `round` is not defined for characters.
So not even the regression. You want to plot the values of the data frame. What have you tried?
I've erased my older code plotting the input, now I've tryied replotting it and the y-axis limits were very inadequate. Instead of starting at 0 and going to 1, it started at -1 and went up until 5. I'm trying this now but there must be some syntax error because it isn't working. No to mention that it isn't plotting the data from the Anotreze subset. plot.default(pwt90$csh_i ~ pwt90$poupançapeloPIB, type = "p", xlim = (0, 1), ylim = (0, 1)) EDIT: does this make sense? plot.default(Anotreze$csh_i ~ Anotreze$poupançapeloPIB, type = "p", xlim = (0, 1), ylim = (0, 1))
Don't use `plot.default`. Just use `plot`. `plot(x ~y, data=Anotreze)` should do the trick. The x- and y-limits are calculated based on the data; double check the input data to ensure which range of values it contains. Se e.g. `range(Anotreze$csh_i)`. As you already have discovered, you can use the `xlim` and `ylim` arguments to plot to set the range explicitly.
Declare it as data.frame. names &lt;- c("NameA", "NameB") percents &lt;- c(0.4454, 0.5532) table1 &lt;- data.frame(names = names, percents = percents) then round table1$percents &lt;- round(table1$percents, digit = 2) 
Did you ever find a solution?
I’d recommend avoiding Google style guides completely. They are *highly* idiosyncratic due to the special requirements at Google (and then, as in the case of dot in names, not even that can justify some of the recommendations). The R style guide isn’t *as problematic* but the Google C++ style guide, for instance, [is infamous for being particularly bad](https://www.linkedin.com/pulse/20140503193653-3046051-why-google-style-guide-for-c-is-a-deal-breaker): &gt; In short, the [Google C++ style guide] bans much of what makes C++ a useful programming language, sometimes backing it up with myths and ignorance, sometimes with emotions, sometimes with the requirement to support already broken codebase […], but most of all, it appears to be targeted at beginners who don't plan on learning C++ but have some Java and C experience. Hadley’s style guide is generally great (although I strongly disagree with using `&lt;-` as an assignment operator but that’s a different can of worms that I won’t open here). If you follow that, you won’t have problems in the R world.
Yeah, my bad. I download the image for google chrome. Run the image on docker but when I run the code in RStudio it's saying there's an error because the OS is Linux and it's referencing FireFox too.. Have you done stuff with Selenium before? 
Yup, used it to do mass download of articles from site that wasn't easy to do through rvest so went with Selenium. I remember having run into some problems but managed to figure it out with documentation and stuff I found on various forums. I remember there was some configuring in docker to be done properly. I got it running using both Chrome and Firefox so it can be done. I recommend posting exact error messages that you are getting - those are much better starting point to helping out :)
But hey maybe I get better at it by the time you get around to it and you can PM me.
lubridate:: has some great methods for date transformations and tidyverse:: (which has magrittr for the piping, data_frame, and stringr for string transformations) is great for a variety of fast and simple transformations. 1) Added a "day" to each date 2) Used dmy() to parse the date 3) Removed the percent symbol 4) Converted from character to numeric and divided by 100
[use the yearmon function in the zoo library](https://www.rdocumentation.org/packages/zoo/versions/1.8-0/topics/yearmon)
I had the same issue, I used strptime and it did exactly that. If you wanna see how I used it specifically I can give you more detail
It takes .2 seconds (give or take a few tenths) to run this transformation on 3,000,000 observations. &gt; microbenchmark(transform_dataframe(frame)) Unit: milliseconds min lq mean median uq max neval 259.8789 279.1542 374.4388 289.9834 538.3485 746.0892 100 The vectorized functions in R, in particular through the dplyr package, are ridiculously fast. I don't think there much reason to be worried about how slow/expensive these operations are. ;) . Thoughts? library(tidyverse) library(stringr) library(microbenchmark) month &lt;- rep(c("June, 2015", "July, 2015", "August, 2015"), 1000000) col1 &lt;- rep(c("90.85%", "90.65%", "90.84%") , 1000000) frame &lt;- data_frame(Month = month, Column = col1) transform_dataframe &lt;- function(x) { x &lt;- x %&gt;% mutate(Month = paste0("1 ",Month), Month = dmy(Month), Column = str_replace_all(.$Column, "%", ""), Column = as.numeric(Column)/100 ) } microbenchmark(transform_dataframe(frame), times = 100L) 
It sounds like you want a way to add marginal totals to the table output. Try the following: addmargins(table(dt)) You can also calculate a table of proportions using prop.table() Hope this helps!
Thanks so much for the advice. I'm following a course at the moment but I'm still finding I get lead down the wrong path for a lot of things. 
What's the actual error message? What was the actual line of code you used? For rmarkdown, you most likely have to install pandoc too. It however comes with RStudio, so bug your IT until they install it for you.
Friendly neighborhood code-review. 1. Break up your major chunks into sections with an extra line of whitespace. Rather than one continuous stream of ideas, I should be able to make out the gist of your algorithm just by scanning the "code blocks" of contiguous lines of code. What you've written here is the equivalent of a 500-word essay with no paragraph breaks. 2. Your initial comment block is very good and detailed. It tells me what the function takes as input and what should be expected to happen. This is sometimes called a "docstring" in other languages and you should keep doing that. 3. Your in-line comments, however, are too descriptive. You don't need to comment that tolower() makes text lowercase. I could figure that out from the function name! Here's an example of how to make that block better: change r&lt;-tolower(x) #converts ciphertext to lower case s&lt;-gsub(pattern="\\W", replace="", r) # removes punctuation t&lt;-gsub(pattern="\\d", replace="", s) # removes numerals to # strip punctuation and numerals from input text cleaned = gsub(pattern = "\\W|\\d", replace = "", tolower(x)) I've shortened the number of lines of code (by combining two regexes with a pipe bar '|') and folded the tolower call into the gsub. Then I replaced all three unnecessary comments with a single comment describing what I'm doing in general. Lastly, I gave it a descriptive variable name. Now excuse the fact that Markdown will read the next "4." as being the start of a new number list at "1." 4. I don't understand what the `substring(..., 1:nchar, 1:nchar)` is doing. I think that's some sort of hack to have substring read the nth char of the string n times. That seems horrifically inefficient. I suggest you use `letters &lt;- strsplit(cleaned, split = "")` instead, which cuts the string into every individual character using an empty regex. It is also more recognizable to other programmers (though admittedly it's just as opaque). This would be a prime candidate of making a helper function to call inside your `freq.sort` function. 5. `# I couldn't figure out how to make a string of the column names.` Since you index'd at the 2nd spot, I'm guessing you did the following: &gt; attributes(tab) $dim [1] 7 $dimnames $dimnames$letter.frequencies [1] "a" "b" "c" "d" "e" "f" "g" $class [1] "table" And index'd at #2 to get dimnames. The dollar sign $ means a named field of the object in R. You can think of these as public vars described in other object-oriented programming languages. To get the variable, you can index it directly using its name. In this case, dimnames() is also the name of a function designed to do this exact job. `dimnames(tab)` and `attributes(tab)$dimnames` are equivalent. Then you use $-indexing to get the nested variable `$letter.frequencies` underneath. So the entire block: tabstring&lt;-attributes(tab) # culls information about that table. # previous line and following lines are an ad hoc solution because # I couldn't figure out how to make a string of the column names. tbstring2&lt;-paste(tabstring[2]) #pulls only the info we want tbstring3&lt;-gsub(pattern="\\W", replace="", tbstring2) tbstring4&lt;-gsub(pattern="\\d", replace="", tbstring3) # these serve to clean up the noise generated by R tbstring5&lt;-unlist(strsplit(tbstring4, split='ciesc', fixed=TRUE))[2] # removes everything but the column names from the prior table should be replaced with: # stitch characters together into a translation string for chartr() ordered_characters = dimnames(tab)$letter.frequencies ordered_in_a_string = paste(ordered_characters, collapse = "") Note the usage of `collapse` to turn a character vector into a single value, using the `collapse` param's value as a delimiter. Now onto point #6. 6. What's a better way to count the remainder until 26, than running that while loop? Easy: take 26 and subtract the current tbstring size. If you have 10 chars in your input, then append 16 underscores. In order to repeat a bunch of the same value in a row, use `rep` like `rep("_", 16)`. Paste together using paste and the collapse param (so paste the 16 underscores into one string, then paste your old tbstring and the underscores string together). ___ Otherwise, looks good to me!
Yes, also had pandoc downloaded and installed. I will show the error messsages when I get into work, but basically it says render() doesn't exist and that rmarkdown version is not matching my R version. Not using RStudio. 
What is the exact error message for the version not matching that of R? And does it occur?
Thank you sir or ma'am. You were spot on in both cases
&gt; Your initial comment block is very good and detailed. It tells me what the function takes as input and what should be expected to happen. This is sometimes called a "docstring" in other languages and you should keep doing that. \begin{shameless_self_promotion} Speaking of docstings... Not too long ago I added a package which actually gives the ability to write a "docstring" in R and view it via the built in help viewers. If that sounds interesting check out my [github page for the package](https://github.com/dasonk/docstring). It's available on CRAN but the readme is easier to view on github. \end{shameless_shame_promotion}
I'm using R 3.2.5 Thanks! I tried running again this morning and I got a different error message when I used *render()*: *Error in packageVersion("yaml") : package 'yaml' not found* So I'll just download that package and hopefully it will work. I have another question. I'm getting a weird warning message when I open R: *Warning message: In normalizedPath(path.expand(path), winslash, mustWork) :path[1]"&lt;lists ~R/win-library/3.2 location&gt;": Access is denied* I'm not sure how I would resolve that. 
Thanks. I have looked at it with pairs, see some with heavier correlation but trying to get more complicated. The inputs are continuous, various KPIs (reliability, speed, ...) and the output is a score 0-100. 
 require("ggplot2") require("dplyr") df &lt;- df %&gt;% group_by(allocation, date) %&gt;% summarize(total_hours = sum(hours)) ggplot(df, aes(x = date, y = total_hours, color=allocation)) + geom_smooth() for more info, look up Hadley Wickham's books on dplyr and ggplot2 which are free online also you might need to transform the date into something workable, like a date format -- the lubridate package might be helpful there. 
Hello mate, hope I'm not too late to the party, use the neuralnet package. With a 3 layer neural network, you'll train it with your data set and you will be fine.
Definitely on my radar. My next step is to generate a plot of the frequencies of letters in the sample with the frequencies of letters in standard English to allow for the person using the function to make decisions for themselves about character replacement. EDIT: hit enter too soon. After that, I want to try the same thing but with an if(){} else{} section where it tests if the most common 3 letter word matches "the" and then changes the decryption rule so that it matches this (and so on) but that's obviously much more complex than I'm ready for right now. 
I ran into the same problem. Gave up on docker and actually got RSelenium to work. So here's a solution if you ever need to go back on this path. * Install firefox * Download geckodriver and unzip the exe here c:\bin\geckodriver * Run the following in cmd &gt;set PATH=%PATH%;C:\bin\geckodriver * start Selenium server by opening a command prompt and then type * cd ~YOUR_R_PATH~\library\RSelenium\bin * Download selenium server into the above location http://www.seleniumhq.org/download/ * java -jar selenium-server-standalone-x.xx.x.jar &amp;nbsp; After that run the following in R library(RSelenium) remDr &lt;- remoteDriver() remDr$open() remDr$navigate("http://yourwebsite.com")
Am I able to do this in a way that extracts anything inside the first set of parentheses so I can extract from multiple strings in the same format, whether it's "!Next?" or not?
Thanks, this works perfectly! As a side question, if I wanted to extract the date in the parentheses instead of the name in the quotes, how can I differentiate the parenthesis in the string from the parenthesis I use for extraction? For instance, this just returns a blank string: gsub("^\".*?\" ((.*?)).*", "\\1", line)
You have to escape them with slashes like the quotation marks.
That's what I thought too, but it seems to be throwing an error when I do it like that: &gt; gsub("^\".*?\" (\(.\)).*", "\\1", line) Error: '\(' is an unrecognized escape in character string starting ""^\".*?\" (\(" EDIT: Apparently you need to escape parentheses with double slashes. It works now.
What makes you think you need a non linear model for this? You might, but it seems like there are some core concepts you're missing. You see, you have a bunch of input variables and a single output, this is practically a foundational modeling exercise. I would highly recommend you use this opportunity to learn about the basic regression and machine learning techniques and how those may be implemented in R. Kaggle has some great practice data sets and examples to get you started. Once you understand the nature of what you are trying to solve, then just try a bunch of different models with caret and see what has the best fit. Also 'earth' is probably the easiest non linear modeling library :)
Yep, but it depends on what you want. by default geom_smooth() adds a loess line through the points, which is just a locally-weighted regression line. you can try something like: ggplot(df, aes(x = date, y = total_hours, color=allocation)) + geom_smooth(formula = y ~ poly(x, 3)) which plots a cubic polynomial line. You can change the 3 in this to higher numbers depending on how it looks. You could also try something with splines: ggplot(df, aes(x = date, y = total_hours, color=allocation)) + geom_smooth(method="glm", formula=y~ns(x,3)) I've never tried this but I think it's how it works 
Not with stringr - it just recognizes a single string. The other solution is what you're looking for. 
I have the same question. I'd love to hear someone's answer on this. 
I know you can. But I never figured out how. I only needed my script to run every 3 months haha
thanks. I do hope they would give the same support and up-to-date version to R as with python.
https://www.reddit.com/r/Rlanguage/comments/6c71r5/why_isnt_there_an_rr/dimwloi/
danke!
i haven't had a chance to verify this, but if i were you, i'd put all the dataframes into a list (if you have them in a single directory, you can get the paths from a list.files() call, and then do a lapply to read the csvs into a list. then you can use the reduce function. see the top answer here: https://stackoverflow.com/questions/22644780/merging-multiple-csv-files-in-r-using-do-call
Good idea. Post your solution when you figure it out. I might need it one day :D
I'll probably DM it to you when I get it. Hopefully by the weekend's end! I'm looking at the TaskscheduleR package right now but I haven't used it fully. 
It's just whitespace delimited text: df &lt;- read.delim("http://www.stat.purdue.edu/%7Echong/stat520/bjr-data/chem-temp", sep = " ", dec = ".") df &lt;- df[,-1] df &lt;- as.vector(unlist(t(df)))
I can use any delimiter in Open Office or Libre Office spreadsheet software.
I would imagine so, sorry I'm just used to Excel.
What I do is use regular expressions to clean things up and format as .csv. If your text editor doesn't support regex, get a new one.
Also explaining why you chose to use x over y. People can be embarrassed to ask questions sometimes, so explaining why you are using the techniques and tech stack you used can go a long way. Why you chose the normalization method you chose, why you dropped observations, etc. 
1. As often as possible. You can't predict when you'll need code again in advance, so do the best job you can in the time you have. 2. Always. The incremental time investment is small and the returns are huge over every project you'll work on 3. Your stakeholders are the best ones to answer this, but they have to be at a level of sophistication where they don't mind seeing a little bit how the sausage is made. People like to imagine that data is very clinical and scientific, but asking them for their feedback early will disabuse them of that notion. Provided that they can handle that, you can get them involved in the process much earlier and let them guide you towards what's most interesting. You can't be a subject matter expert in every field, so let them guide you. You'll never be making the One True Product, they'll always have more questions, so getting the product into their hands faster is always good. 4. See answer to #3.
Aside from technical advice I would strongly advise you to always study your industry to better understand why you are even there. What is your purpose and role within your organization? When are less data-driven decisions justifiable and when aren't they? If your boss asks you to do something and you have a better understanding of the situation, can you instead tell him or her why you think another project is more important? Regarding your point #4, in a fast paced environment it's just as important to know what not to do as it is to know what to do. Knowing when and how to say no comes with time, but I find it extremely important, especially when I'm interviewing candidates. I don't want cowards on my team. Analytical discipline is only as valuable as the decisions it influences. So understanding the social and organizational mechanisms in which that occurs will help you stand out from the flood of "heads down" quants that need to be told what to do. 
Machine learning algorithms, especially those that can solve classification problems. Start with logistic regression and move up to more sofisticated models (randomForest, nNet,...) after. Get to know what a ROC curve is and how to utilize it, comes useful in churn problems, since usually target variable has non-even distribution.
You need to understand the scope of work before you begin. Usually churn models are used to predict those customers most likely to churn before they churn, so they can be given special treatment. It doesn't sound like this is your use case. So, why do people need to know which variables impact churn? It's significant because you can make a decent predictor of churn without even having to have useful variables in the model, for example if there is high covariance. Perhaps there is a "whale targeting" programme already in place that triggers several actions targeted at whales, but your model doesn't include data on them all. That probably wouldn't matter much to its ability to make a prediction, but it would matter a great deal to its ability to tell you which of those actions is most effective. 
I use `readxl` for reading excel packages and `WriteXLS` to write excel files. You could write an entire book on all of the packages for manipulating data, but I'd recommend starting with the `tidyverse` package (it's really a meta-package, but it will encourage you to learn good habits in R from the start). 
https://www.pinnacle.com/en/betting-articles/betting-strategy/r-betting-analysis
To simplify it, say I have 1000 rows of data of all churned customers. (Customers who are are inactive for 30+ days). From these 1000 records, I would like to find patterns. Say a lot of customers churning are between the age of 15 and 30. So maybe 5 parameters are identified. After I identify the 5 parameters, I would like to plot a graph to show. As I said, I am quite new to this field, but was wondering if this is possible. 
I was thinking more about Kelly betting calculation or any other betting approaches that might exist and their implementation in R.
Yes, and the point I'm trying to make is that it's relatively easy to do this analysis. The tools are pretty good now. The more important question is *why* you need to do this; what will the business impact be? What decisions will people make? In your example, perhaps that would lead to some specific actions targeted at 15-30 year olds to try to reduce their rate of churn. But it turns out that in reality, the 15-30 variable is highly covariant with several other variables that aren't included in your model that are also important for churn, and your action targeting 15-30 year olds will be much less effective than the model would indicate. If you have a good understanding of how your model will be used in the wild, you can anticipate this kind of problem and preemptively avoid it.
Also, why run the colour like that? Set it as an aesthetic with x and yv ggplot(data1, aes(LF, UCS, colour=Z_CONTRY, size=LOF)) + geom_point() and then set the colour scale with `scale_colour_manual`. 
That is; lookuptable &lt;- list( serbia = ”orange4", russia = "red" ) etc
Hi there! I am not very familiar with ggplot2. It seems there are about 57 ways to do the same exact thing and this confuses me to some extent. Anyways, i got a hint to use setNames for my colors along with what you guys said here. And it worked, no wonder here.
And if you must use nested ifelse's don't and use [`case_when()`](https://www.rdocumentation.org/packages/dplyr/versions/0.5.0/topics/case_when) instead.
Yes I agree with that. But I just want to go ahead with the data we have. If anything can make sense of it. What do we use for this? 
You can simply use dontrun to stop the problematic examples being run. https://stackoverflow.com/a/12038225
[strptime](https://stat.ethz.ch/R-manual/R-devel/library/base/html/strptime.html)? Works for me.
Have you tried using the [Chron](https://cran.r-project.org/web/packages/chron/chron.pdf) package? The time has to start as a character vector, but then you convert it to a chron object, which stores it as a numeric (decimal between 0 and 1). For my data, I do: &gt;data[,3]&lt;-as.character(data[,3]) #time formatted hh:mm:ss &gt;data[,3]&lt;-times(data[,3], format=c(times="h:m:s")) #Convert to chron object If you want to see it as the decimal, you'd just do as.numeric(data[,3])
This is awesome. Thanks Hadley if you are reading this.
Thank you! I revisited strptime again, works perfectly for trying for plot functions. Data$StartTime &lt;- strptime( Data$StartTime, %T) But it won't leave out the date in the output. The result gives me the current system date in front of it: "2017-06-13 00:07:58 EDT" I ultimately only want the time "00:07:58" and I tried the format() or the sub() but it turns the field into a character which again can't be plotted. &gt; format(Data$StartTime, %T) &gt; sub(".*\\s+","", Data$StartTime) Am I missing something? Thanks for your help!!
Sadly i need 30 colors :-( Set1 is really good though!
Yeap. I picked that up. Thanks!
1. Definitely worth it. Most data science roles are as exactly as you described. 2. Since you're applying for a junior role and you'll have to learn a lot, I would focus on communicating how you have quickly learned things in the past. I had a lot of interviews coming out of grad school and there was a ton of variation in the technical questions I was asked. 3. You should by the R for Data Science book by Hadley Wickham. Data camp is also a good resource. 4. Where I work, there is ~15,000 difference in salary for those with a masters degree compared to those with an undergraduate. Once you start your new role, you should have some of the data science veterans on the team look at the curriculum to see if it would be worth your time and money.
It's something else. It's a mix of accounting and forecasting the daily cash needs of the company. Then based on the forecast, we execute trades to raise the cash needed and often lend it to other groups within the company. Those other groups leverage the cash to make deals with external companies. Also, I'm responsible for tracking and executing enterprise-wide debt (from bonds, options, notes, etc). Overall, it's very company-specific accounting systems, trading systems, and excel that I work heavily with. Sometimes I'll leverage VBA to automate rote tasks. 
Thanks so much for your reply!! This is extremely helpful and gives me hope. I thought I might be making a foolish decision in switching careers but this is all really encouraging. Thanks again!
Right. The job I had before I moved into data science was like the one you describe. Data science is everything you do right now, just more in every dimension. Definitely more computer science and more math. From my experience, the computer science (R, python) just needs hours spent on it. I'm yet to come across someone who is definitive that university is necessary to learn cs. It was the math that baked my noodle. I ended up needing to get a Masters degree to properly learn the maths. As a reference point, the Black Scholes equation is at the level of difficulty you get in data science. 
Nice! Awesome to see that you made a transition similar to what I'm going for. I minored in statistics and used some SAS, R, STATA, and GRETL software but it was all at a basic level, and nothing that I remember to this day. Definitely going to brush up this summer. How was experience transitioning fields? Was it pretty rough at first?
I don't understand can you give me an example?
Rough? Well, the thing is stakeholders ask you questions that they expect you to be able to answer. The pathway to answering them is through some very complicated math, and then back to actionable insights. If you can think of a way to answer the types of questions you get without using advanced maths, then no, it's easy. I couldn't though, so yeah it was rough. But I'm passionate about this stuff so I felt it was worth it. A few months ago someone posted that data science industry standard is at the level of the Stanford Machine Learning on Coursera and at least the first few chapters of Elements of Statistical Learning. My experience backs up that that is the level you should be working at.
##Stored procedure A stored procedure (also termed proc, storp, sproc, StoPro, StoredProc, StoreProc, sp, or SP) is a subroutine available to applications that access a relational database management system (RDBMS). Such procedures are stored in the database data dictionary. Typical uses for stored procedures include data-validation (integrated into the database) or access-control mechanisms. Furthermore, stored procedures can consolidate and centralize logic that was originally implemented in applications. To save time and memory, extensive or complex processing that requires execution of several SQL statements can be saved into stored procedures, and all applications call the procedures. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.2
Thank you but I can't change anything on the SQL server I retrieve data from it.
Isn't the tidyverse able to query databases? Think the only limitation is that it can't handle ms sql hence I never really looked into it myself.
All you need is to initially create a vector of the subject 'names' (data_names &lt;- c("sub1", "sub2") , which you can also do with dir() and some gsubing automatically) and then you run a for loop on this vector and have the name you can paste into all the output file names. If you don't want the forloop around your analysis, write a script that takes a input file name (via commandArgs or what its called) does the analysis and uses the input file name in the output names. Then either run it by hand for all subjects or write a second script with the for loop that calls the other script. EDIT if it wasn't clear this works: name &lt;- "sub1" write.table(data, filename=paste0("table1_",name,".csv") )
Looked it up; see [dbplyr](http://dbplyr.tidyverse.org/articles/dbplyr.html) &gt; "As well as working with local in-memory data stored in data frames, dplyr also works with remote on-disk data stored in databases. This is particularly useful in two scenarios: Your data is already in a database. You have so much data that it does not all fit into memory simultaneously and you need to use some external storage engine." ---- # Create index of table flights flights_db &lt;- tbl(con, "flights") # set up query using tidyverse verbs, like: flights_db %&gt;% select(year:day, dep_delay, arr_delay) # adding collect() in the pipe actually will start calling the data flights_db %&gt;% group_by(dest) %&gt;% summarise(delay = mean(dep_time)) %&gt;% collect() ---- There is an advantage here that might be very useful when using interactive shiny dashboards with user inputs whilst lowering demand from the server and data kept in memory in R u/phatboye
Yes this is how I originally tried to tackle this issue but I couldn't get it to work. I don't have the code in front of me now but I will post it tomorrow.
Look up squr package on GitHub it makes this easy.
Cool, thanks for your input. I started using ReadXL today and seems straight forward. I just need to decide which library I'm going to use to write new excel files now.
Have you tried using paste? I have found that it is helpful in passing parameters to a query. Here is an example: data &lt;- sqlQuery(channel, paste("Select * FROM TABLE WHERE VALUE1 &lt;",Variable1," and DATE &lt; '",today,"00:00:00';"))
You can install devtools from inside RStudio; either using the menu ‹Tools› → ‹Install Packages…› or by just typing `install.packages('devtools')` into the console. *But.* I’m gonna be frank with you: If you “don’t want to muck around in terminal” you’re gonna have a miserable time as a programmer. The terminal is an indispensable tool. Take time to learn and love it. You *can* work without it, but it’ll hold you back.
I have tried that but couldn't get it to work.
Hmmm. Can you send your example of when paste failed?
Your code works fine if you change the way you're subsetting to using the $: `sum(df$ordtyp == 'international' &amp; df$shipdate &gt;= today-7 &amp; df$shipdate &lt;= today-1)` You really should check out the lubridate package too, though. Makes dates/times/days much easier.
I'm all for making my life easier, but I only need to manipulate dates in this piece of code I am working on
The problem was with the structure of data the subset returns. I don't know enough about it to explain it well. Nearly all the R code I've written and read uses either the $notation for subsetting based on the column name, or bracketted subsetting using the numeric position of the column or row. (df[rownumbers, columnnumbers]. I've very rarely seen df['nameofrow']. A lot of this can be simplified with the dplyr package and the filter(), select(), mutate() etc... functions.
You've basically been given a stack of building materials and asked to build a house, despite not being trained in construction whatsoever. Oh, and you have to use this hammR. To take your questions in reverse, yes you can repeat an analysis for 350+ stores every so often. That's why we automate. This is however the last step. Start from the beginning. 1. Subset your data for a single store. 1. Visualise your data for debugging. 1. Analyse your data using what ever model is required. This is a distinct step from visualising with ggplot2! 1. Summarise the models' performance and compare them. 1. Visualise the expected demand *including the uncertainty*. How this is done depends on your model. 1. Repeat ad libitum for all stores...
You've not explained whether you actually need your file to be csv or if you're just doing that as a convenient intermediate step. You could use googlesheets (https://github.com/jennybc/googlesheets) to write the R object directly as a new Google sheet e.g. iris_ss &lt;- gs_new("iris", input = head(iris, 3), trim = TRUE) You could then directly import from Googlesheets to R by title (in this case iris_ss). You could also export from Googlesheets to csv and then import the local version of the csv (if for some reason you needed that). 
Will try to go about it like this and get back :) Thanks mate!
I made the following change to the server portion, and it seems to work fine: server &lt;- function(input, output) { #I created a data object that has the filters applied so that it can be used in write.csv() calls below. data &lt;- reactive({ data &lt;- DS if (input$product != "ALL"){ data &lt;- data[data$PRODUCT %in% input$product,] } data }) output$ds &lt;- DT::renderDataTable({ data() #Now referring this object. }, rownames = T, server = F) # download filtered rows output$downloadFiltered &lt;- downloadHandler( filename = "filteredData.csv", content = function(file){ s = input$ds_rows_all write.csv(data(), file, row.names = T) #Changed to the created object. }) # download selected rows output$downloadSelected &lt;- downloadHandler( filename = "selectedData.csv", content = function(file){ s = input$ds_rows_selected write.csv(data()[s, , drop = F], file, row.names = T) #Changed to the created object. } ) }
Is your data daily or monthly? Try using prophet package - it's quick and gives good results If your data is monthly, try using holtwinter model, again it's relatively easy Use dplyr nest function to split up the data and purrr package map function to vectorise your operations
Hey sorry no solution to this yet. I'm still interested in finishing this project but for now it's on hold. I thought about trying to include a portable version of the browser in my config folder, but I haven't had a chance to try it out. 
It's my favorite part of stats. 
I posted the code it's not formatted though. 
A data.table solution library(data.table) # make the example data df = data.table("C" = c(1, 0, 1, 1, 2, 2), "month" = rep(c(5, 6), each = 3)) df[, D := cumsum(C), by = month] 
You need to use `1i` as the notation, not `i` on its own. `i` on its own refers to the variable named `i`, whereas the notation `1i` is similar to the notation `1L` in that they both refer to a specific kind of number. `1L` is the integer 1 and `1i` is imaginary. You could also read the detailed help for `complex`, especially looking at the examples, and try them yourself. &gt; complex(real = 2, imaginary = 3) [1] 2+3i &gt; 2+3i [1] 2+3i
No idea. Maybe try `install.packages("tidyverse")` That will give you the packages you want, if it works.
You nearly never need a for loop in R, and this is a good example of when you don't need one. If you just add the bounds to the data frame as columns it's incredibly easy: &gt; set.seed(50) &gt; data_frame &lt;- data.frame(value = rnorm(5), upper_bound = rnorm(5), lower_bound = rnorm(5)) &gt; data_frame$check &lt;- (data_frame$value &lt; data_frame$upper_bound &amp; data_frame$value &gt; data_frame$upper_bound) &gt; data_frame value upper_bound lower_bound check 1 0.54966989 -0.2778645 0.2952068 FALSE 2 -0.84160374 0.3608284 0.5547522 FALSE 3 0.03299794 -0.5909124 -0.4986355 FALSE 4 0.52414971 0.9755906 0.1957338 FALSE 5 -1.72760411 -1.4457499 -0.4555405 FALSE Note that I didn't insist that upper_bound exceeds lower_bound in this example for sake of time, but hopefully it illustrates how simple this would be to implement with the right values.
yes, my data is aggregated monthly sales! I have finally been successful at: 1. Subsetting the data for a single store (one with complete data over all the periods) 2. Visualizing it (simple line graph, Histogram) -Histogram looks like a bell curve -&gt; probably some elements of normal distribution in my data? -Line Graph doesn´t look like there is seasonality or trends, it´s pretty random 3. How do I figure out which model to use for forecasting?
Ah, never mind, I got it to work. I changed the sprintf function to just be the original http link you provided, along with x, such as: url &lt;- sprintf("https://find-mba.com/schools/usa?page=%s&amp;keyword=&amp;rank=false&amp;accredition=true&amp;cities=&amp;specs=14&amp;sort=popularity&amp;numberperpage=8", x)
How large is large? You basically run Shiny apps the same way on the desktop as server, with a change to the IP address you are accessing and allow the port to be visible via port forwarding or firewall settings. As far as scaling with R, Spark is probably your best bet. You can also handle dataframes in a file instead of memory, and cluster servers together for parallel processing. 
Try using holtwinter 
I agree that you should read the manual, but I know that as a beginner it can be hard to make heads or tails of the technical documentation. mutate() is used to create new columns (or modify existing columns) in a data frame. Here's an example where I create a data frame, fill a column with random IQ scores, then z-score them in a new column. I'll leave you to figure out how to do the rescaling from 0 to 1. library(dplyr) df &lt;- data.frame(iq = round(rnorm(100,100,15))) head(df) df &lt;- df %&gt;% mutate(z.iq = (iq - mean(iq))/sd(iq)) head(df) (it's weird to actually do this with IQ scores since IQ is defined to have mean = 100 and sd = 15, so really the z-scoring should use those population parameters instead of the sample mean and sd, but it's just an example)
Can you ask this question in stackoverflo. I can give you my code to use
I think R-shiny is a R HTTP server. Did you mean this http://ampcamp.berkeley.edu/5/exercises/sparkr.html ? I already use rJava which runs in the same JVM as other Java code with which interacts. Data is pushed by R-shiny HTTP server using web sockets to the browser. My question was about a pure R server. Is there such a thing ?
I've never had any issues with xlsx or openxlsx, you might try those libraries. 
What is table? I'd bet there are some Inf, NaN, or NA values in your data.
Your problem is that `table$x` is looking for a column called `x`, not one called whatever you're storing in the x variable the function takes as input. To get the expected behavior, try `table[,x]` Also it looks like you want to pass `speed` and `distance` as strings when calling the function: `plotting("speed", "distance", cars)`
This works: plot2 &lt;- function(dat, x, y) { x &lt;- dat[, x] y &lt;- dat[, y] plot(x, y) } 
I tried that approach as well, but I get the following: table&lt;-cars x&lt;-"distance" y&lt;-"speed" table[,x] Error in `[.data.frame`(table, , x) : undefined columns selected
That's because there is no column "distance" in the cars dataset. Try "dist" instead.
Great, thanks! The first time I tried to store a column with table[,x], I failed to put quotation marks around the names when using the function. The second time, apparently I was using the wrong name altogether. It definitely works now, however.
You answered your own question. Create a new project. When you load the project it will set your working directory to the dir of that project. As far as importing data, go to "project options" and set it to save/load your data on startup and exit.
You can also save everything in the environment to a specific file, and can use this to checkpoint during longer tasks if you want. save.image(file = 'step1.RData') save.image(file = 'step2.RData') etc...
Wow, this sounds like a ballache. One possible solution is to store the 58 data frames as elements of a list and then use `purrr::map` to map a function to each data frame. 
Subtracting the mean and dividing by the standard deviation produces standardized results, but OP mentions 'normalized'. Is it possible that OP meant 'unitized', which is accomplished by dividing each element by the maximum value?
this. dplyr is my go-to
Ya it´s terrible. I need this stuff for my bachelor thesis and the clock is ticking so I did it manually now but that meant creating 100 seperate dataframes from the main dataframe and then manually filling in each dataframe name in my formulas. I´m halfway there now and it took me hours :s
Yeah, don't do that &gt;_&lt; Do you know anything about the way lists work in programming languages? They're basically just a collection of objects. The way a data frame is structured internally is that it's a list, where each element of the list is a column of the data frame. You can see this yourself like this: &gt; str(data.frame(col1 = 1:5, col2 = letters[1:5])) 'data.frame': 5 obs. of 2 variables: $ col1: int 1 2 3 4 5 $ col2: Factor w/ 5 levels "a","b","c","d",..: 1 2 3 4 5 &gt; str(list(col1 = 1:5, col2 = letters[1:5])) List of 2 $ col1: int [1:5] 1 2 3 4 5 $ col2: chr [1:5] "a" "b" "c" "d" ... The output is nearly exactly the same, because internally they're nearly exactly the same thing. But each element of the list doesn't have to be a column - it can be whatever you want. Even a dataframe. Check this shit out: &gt; str(list(one = data.frame(one_column1 = 1:5, one_column2 = letters[1:5]), two = data.frame(two_column1 = 5:1, two_column2 = letters[5:1]))) List of 2 $ one:'data.frame': 5 obs. of 2 variables: ..$ one_column1: int [1:5] 1 2 3 4 5 ..$ one_column2: Factor w/ 5 levels "a","b","c","d",..: 1 2 3 4 5 $ two:'data.frame': 5 obs. of 2 variables: ..$ two_column1: int [1:5] 5 4 3 2 1 ..$ two_column2: Factor w/ 5 levels "a","b","c","d",..: 5 4 3 2 1 Hopefully it's clear how this can be extended to any number of data frames, even 58 of them! Now you can use `purrr::map` to do things one at a time to each member of a list, like another language would iterate through them in a for loop: str( purrr::map( list(one = data.frame(column1 = 1:5, column2 = letters[1:5]), two = data.frame(column1 = 5:1, column2 = letters[5:1])), function(x) {sum(x$column1)} ) ) List of 2 $ one: int 15 $ two: int 15 So now the two elements of our list have been transformed from data frames into the sum of their "column1" columns, as the function we defined does. Pretty cool, huh?
yeah I intentionally gave a different example to illustrate how mutate works without answering the math part, since this seemed like it was a homework question. I interpreted the rescaling as remapping the lowest value to 0 and the highest to 1, which would be done by mutate(rescaled.iq = (iq - min(iq)) / (max(iq) - min(iq))) (whereas just dividing by the max would get the max value equal to 1, but not the min equal to 0 unless 0 was already the min)
My guess is that it's ordering the factors alphabetically (Sophomore comes after Senior). How is R supposed to know that Senior comes after Junior which comes after Sophomore which comes after Freshman? Run a google search on "ordering factors in R"
This link has a really great explanation of the difference between the assignment operators: https://stackoverflow.com/questions/1741820/assignment-operators-in-r-and Generally speaking you can use the equals operator, in my experience I've gradually come to prefer the arrow assignment for readability. It helps when you have multi row function calls. There are only a couple of times when you must have arrow assignments, such as suppressWarnings and also system.time. 
I'm trying to get it to work now, but I get the following error: Error in model.frame.default(formula = x$Freq ~ (x$Var1) + (x$Var2), data = x) : invalid type (NULL) for variable 'x$Freq' with this code: purrr::map( split_dataframe &lt;- split(input,r), #print(list(split_dataframe)), function(x) { distance_offspring_frequencies = table(x$distance &gt; median(x$distance), x$offspring == 0) distance_contingency_table = xtabs(x$Freq ~ (x$Var1)+(x$Var2), data=x) } ) It seems that I'm incorrectly pointing towards the values created in "distance_offspring_frequencies". When I run the code with just the frequency table code it returns the frequency tables perfectly, but it does not have the "Var1", "Var2" column names in the console where it's printed. So I'm not sure how to reference them now. The return of just the frequency table is as follows, but then 58 times: $`1` FALSE TRUE FALSE 309 191 TRUE 362 138 $`2` FALSE TRUE FALSE 328 172 TRUE 382 118 $`3` FALSE TRUE FALSE 360 140 TRUE 398 102 **Edit: I just realized that this output seems a lot like it's a contingency table already?? How is this possible? If I run that "distance_offspring_frequencies" line on my main dataframe it returns a table with a Freq in it. Do you think I can just use the "distance_offspring_frequencies" line as a contingency table and do a Fisher test on that?** **Edit2: Fisher test seems to work properly on that frequency table line so I guess it works. Now I just need to append that p-value to a vector in my global environment so I can use that vector for graphs. It looks like all variables I create in the function stay within the function, how do I append a value created in the function into a global vector?** **Edit3: Assign to the rescue! I got it to work :) Thank you so much for the advice!**
&gt; There are only a couple of times when you must have arrow assignments This is a common misconception. As explained above, you can use equals assign everywhere. Just put parentheses around the assignment.
I don't really understand enough about your data and your use case right now to understand in detail why this isn't working, but the punchline is that the error is telling you that the variable x doesn't contain a variable Freq, or that Freq is NULL when it shouldn't be. Think about what the value of x will be when your anonymous function is called, whether it will have an element called Freq in it, and whether that value can ever be NULL - because R isn't able to find one right now, or if it can, it's finding NULL. Your code is pretty weird, though. There are two main things that are weird: purrr::map( split_dataframe &lt;- split(input,r), Why are you doing assignment inside a function call? This is nearly always a really bad idea. In my example, I just pass a value into `map` and you should do the same thing: the_list_of_data_frames &lt;- list(...some code to generate a list of data frames...) purrr::map(the_list_of_data_frames, the_function_to_map_to_the_list) The second thing is that your anonymous function is a bit confused about what it should be returning. function(x) { distance_offspring_frequencies = table(x$distance &gt; median(x$distance), x$offspring == 0) distance_contingency_table = xtabs(x$Freq ~ (x$Var1)+(x$Var2), data=x) } What do you think the return value from this function will look like? You haven't explicitly called the function `return`, so R just returns the value of the last expression it parsed. That expression is the `distance_contingency_table = xtabs(x$Freq ~ (x$Var1)+(x$Var2), data=x)`, which will transparently return the value it just assigned, so it's just returning `xtabs(x$Freq ~ (x$Var1)+(x$Var2), data=x)`. The value of `distance_offspring_frequencies` is never used for anything, so it doesn't appear.
Which editors supports this keystroke?
I use R studio, I guess I'm not sure if others do. Sorry if they don't!
Ok fine. There are times when you must use the arrow assignment unless you take the extra step of using curly braces. Also, I'm not addressing special packages like data.table that use := for assignment within the data.table object, and possibly some Hadleyverse packages that follow their own rules. (Hadley recommends arrow assignment actually). I'm pretty sure I've needed to use the arrow in some other special cases when building packages, but I can't remember exactly if / when that's true. In short, I use arrows mostly for readability but also for peace of mind knowing that I won't have a complex problem (like needing to add curly braces in some odd spot to avoid a strange error). Also, alt dash is fast and easy in the R Studio IDE. 
heh, TIL Thanks
Maybe I'm the odd one out, but I actually really like being able to assign to the right with -&gt; as well as &lt;-. Can be very convenient when working through stuff interactively.
 &gt; grade &lt;- factor(levels=c("Freshman","Sophmore","Junior","Senior"),ordered=TRUE) &gt; grade ordered(0) Levels: Freshman &lt; Sophmore &lt; Junior &lt; Senior Use levels to order your list. If you don't this is what you get. &gt; grade &lt;-factor(c("Freshman","Sophmore","Junior","Senior"),ordered=TRUE) &gt; grade [1] Freshman Sophmore Junior Senior Levels: Freshman &lt; Junior &lt; Senior &lt; Sophmore 
what do you mean "it's comma separated" but "strings are delimited with tab"?
There is a tab around each value on the input file?
 rowtotals &lt;- function(df){ totalcolumn &lt;- rowSums(df) newdf &lt;- cbind(df, totalcolumn) return(newdf) } cars &lt;- rowtotals(cars) return() specifies the output of the function and you just assign that to your desired name. 
Do you mean you want to change an object in the global environment as a step in a larger function? If so, assign() might work where you specify the variable, what to assign to it, and in which environment to assign it. I've never tried this before so just guessing.
This should do it: rowtotals &lt;- function(df) { totalcolumn &lt;- rowSums(df) dfname &lt;- deparse(substitute(df)) newdf &lt;- cbind(df, totalcolumn) assign(dfname, newdf, pos = globalenv()) } edit: I highly suggest reading Hadley Wickham's book "Advanced R". It's wonderful.
You don't have to go to reference classes to get this. It's easy enough to do but I don't think OP should be taking this approach so won't provide an example on how to do it. 
&gt; You don't have to go to reference classes to get this. No &gt; but I don't think OP should be taking this approach tend to agree &gt; It's easy enough Hmm, getting the object name dynamically, possibly without knowing the level of nesting beforehand? Doable but could be a bit convoluted without any real benefit. I would love to see your example.
The idea is to show the no. of customers across various region. The no. of customers are actually churners, so it would give us an idea from where there are more number of churners. Are there any other alternatives to display data like this? 
Yes, I am doing a table on my data. Right now I have tried to limit the no. of occurences by selecting values greater than 10000. 
Yeah I just sorted the data and did a top 30. Less cluttered now. Thanks :)
I'm currently struggling with an API in R that has a timeout issue. Can anyone help with a way to repeat the call until all rows are populated?
Well sorry to say this, but that text file is insane using tabs this way, and as a sane person I refuse to assist in propagating it any further. Therefore, not gonna have any advice for "rewriting that output format"
Just Google "r 3d graph" and you'll be presented with a wealth of options. 
\t is the typical encoding for a TAB, e.g. if you look at read.table and reading in a TAB separated file you have: sep="\t". Therefore what you have to do is add the \t to the text and then save it without quotes. If the program reading the file doesn't auto-convert it to the white space you were hoping for, you are out of luck. But again, I personally would prefer to just not quote and not tab-surround the text.
I used to use the "=" when I first started to teach myself how to code, but after about a month or two I found out that the "&lt;-" could be made by hitting "alt -" It is much more satisfying.
Try using sink() to dump all console output into a text file.... That was the first thing I came up with, there are probably better methods
`odbc` is apparently faster
Well... AUC - area under the curve, or area under the ROC curve (they are the same) basically tells you how well the model describes your data - the higher it is the better. In terms of your goal - it is not very clear to me. How do you define properties(it is some sort of distribution, or correlations between the variables?)? What exactly do you want to explain with the model?
Data is in a data frame in all numerical. I've tried the basic scale(data, center=T and F). This returns some negative values. I've also tried rescale data from scales package between 0-10, but forcing all of my data to this both amplifies some noise and makes large changes insignificant. Can I scale it like the first function but force it to positive?
RODBC isn't using ODBC?
ROC, AUC, gini, precision, and recall are all evaluation metrics for classification models. This is a poison regression. OP is modeling counts. I don't see how any of those metrics apply.
Ooooh, sh*t... Well, yea. My bad :-D I would be happy to hear the answer as well, in this case. P.S. Sorry, OP!
But in this case wouldn't actual vs predicted counts work for him? Maybe i am misunderstanding the point of a poison regression...
You can absolutely still use cross validation. The purpose of cross-validation is to evaluate how your model performs on held-out data to make sure you aren't overfitting. My issue was using evluation methods specifically tailored for understanding the performance of classification models when you are clearly performing a regression. You can totally use cross validation on regression models though. 
Try: oldw &lt;- getOption("warn") options(warn = -1) at the beginning of your script, and then options(warn = oldw) at the end. (Or where you want warnings to stop being suppressed) This is how I always suppress warnings.
This dataset and problem is just an example; I'm trying to figure out how to solve it using a loop so it works for any dataset. I figured out my actual problem by first creating a data frame to plug the columns into it, but by first giving it enough rows equal to the table which is to go through the loop. Given that, I'm not sure if there are other ways this can be solved, or if you always have to first give the empty data frame the same number of rows.
You can make a copy of your original dataframe and overwrite it to converse the column names, or just create a new dataframe of appropriate size. I generally use x &lt;- as.data.frame(matrix("", nrow(df), ncol(df))) and then names(x) &lt;- names(df) But yes, you have to give the loop an appropriate dataframe to put the output in to.
Ok, so I guess this sums it up: mean(df[,string]) mean(df[[string]]) mean(df$object) mean(cars[,object]) mean(df[object]) I guess it just takes some memorization.
I was wondering about this the other day as I recall reading the main reason there isn't an R app for Android is because it hasn't (or can't) be compiled for the ARM chip architecture. But I've a Raspberry Pi2 with ArchLinux installed and R is available for the the distribution and the chip is an ARMv7 processor so am unsure about it. Practically though I'll always do my computing on a PC so whilst a nice feature to have R on my phone I'm not bothered enough to investigate further (can always SSH into my computer, my VPS or even a cloud service and do stuff there if I ever need to do stuff on the go). I actually find typing on a phone/tablet really laborious and would never really consider doing significant work from one, keyboards still have a place.
I mean we are in age essentially where Win/Linux/Mac desktop distributions are not enough. I would not do heavy calculations but its always good to have atleast basic interpreter like QPython. Its fun and saves time to do thing on the go in bus etc. 
I don't have experience with this library, and you'd probably have to check it's documentation to get an answer, but this is my guess. Unless the desired R function got compiled in (either statically or dynamically with dynamic library), what would be its advantages over simple `system()` function calling the R binary?
Use the various "apply" functions. lapply is particularly useful in conjunction with do.call. df1 &lt;- data.frame(lapply(iris[,1:4], "*", 2)) df2 &lt;- do.call(cbind, lapply(iris[,1:4], "*", 2)) df3 &lt;- as.data.frame(apply(iris[,1:4], 2, "*", 2)) 
If you just wanna play around a bit: https://play.google.com/store/apps/details?id=com.freeit.java Don't think this is a proper or complete R-console
(snark) losing the respect of your peeRs
I looked at Dash, but the only hosting option seems to be with plotly, and plotly either forces you to pay to make the dashboard/app/whatever private, or forces you to make it public. Neither are good options for me, particularly at the subscription rates Plotly wants.
The result of apply on tablea is TRUE FALSE, so you're only selecting one column. R automatically converts this single columnto a (unnamed) vector. Try tablea[,1] and see what you get. There shouldn't be a column name there, hence why you're getting NULL. If you wanted to fix this to work on both examples try: colnames(tablea)[apply(tablea,2,function(species) any(species=="setosa"))] or names(which(apply(tablea,2,function(species) any(species=="setosa")))) I'm sure there's other solutions out there too. 
I see, thank you for your answer. I'll try your fix as well.
unique will pull the strings in the order they appear. Then you can manually assign the levels.... But I agree that this seems like a problem that shouldn't be a problem. In other words you should change something else upstream. Factors in R are important and useful, but can also be terrible and sink your work. 
Y'all need 'library(forcats)'
If if you backticks \` then the text prints as `in-line code` (doesn't show good on mobile) 
maybe try order() 
It appear rmaps will be a good option. Does anyone have a good alternative? (I plan to show events on a map with a timeline).
Check out [htmlwidgets](http://www.htmlwidgets.org) 
Not gonna lie, tableau if you have it
Leaflet
There totally is and I know a way that involves dplyr. I'm away from my PC for a couple days though so I can't pull you an example. I think it involves "mutate" and some if/ifelse statements. Sorry I can't remember more but some google searches with those terms should find it. 
Yes, just create a new vector using two apply statements. First write something like v = apply(X = rdn_dat[,c(all the columns you're checking)], MARGIN = 2, FUN = is.na) and then apply max/min (depending on whether you want all or at least one) like apply(X = v, MARGIN = 1, FUN = max) filter on that.
It would be useful to have a short sample of your data and the intended output. This is polite, if nothing else, because it saves me a lot of time. The first thing I have to do is usually produce a data frame to do some testing with. Your specification is also incomplete - your code does much more than "check if there is a reassignment date and calculate the days since". It seems like your intention is to check: 1. If there is a Released date, then calculate the date diff based on that, else 2. If there is a Reassigned date, then calculate the date diff based on that, else 3. return NA If that's the intention, it's dead easy to set this up in dplyr: dplyr::mutate(rdn_dat, Days.Since = as.numeric(difftime(today(), dplyr::coalesce(Released, Reassigned), units = "days"))) (not tested - if it doesn't work, provide some sample data)
?ifelse ??dplyr::case_when
Thank you! 
How's this? n &lt;- 6 z &lt;- lapply(1:n, function(i) 0:1) unname(as.matrix(expand.grid(z))) Transpose if needed. 
Nice.
So... it took me lots of time, but now i am getting somewhere Visualization has shown: - strong seasonality (once a year demand drops) - no linear trend (there is a general linear upwards trend in demand for all stores, but it´s not very big) - demand seems to have a normal distribution (shaped exactly like bell curve) I can´t fit a model. Any recommendations as on where to go about finding a suiting model and testing it?
Yes. Use prophet library. It's super easy to use. Results are pretty good. I use it a lot.
I can't offer much help as I am a newbie myself, but it might help others if you gave more info like how much time will you need per week, how much you are willing to pay and a what city/state you are located.
Hello. I am a math and statistics professor who is on break for the summer. I have three years of experience working with R and two years of experience with online tutoring. I would be happy to go through the course with you and provide guidance when you need it. If you are interested, PM me and I will send you a link to my tutoring webpage. 
I've moved away from R into Python, Java and more software engineering type of programming. But still, I may be able to help if I have spare time. Whatever question you have throughout the course, send me a PM. I'll help if I actually can and if I have time. Sort of like a back-up plan.
Thanks very much, I'll PM you if I need help. :) 
I just sent you a PM, thanks :) 
I'm a PhD student who uses R for all of my analysis. I can assist you if needed as well! Just pm me.
You're a saint! Wouldn't have found the package myself for sure. My first model is fitted and looks quite great :) I will have to read into what and how the package is actually fitting in the background - I have to explain results :) Thanks mate! I have now a great, working model and forecast for one of the stores. How can I apply it to all the other stores (400+) easily?
The model uses Stan library. Supply holidays and look up the Git page....in the issues tab under closed issues, there is a thread on how to improve accuracy. On daily forecast the model allocation for sat/sun is a bit skewed. Re. Applying to 400 stores, search stack overflow for "how to create a list of list and then perform a vectorised function" Nest and map are your helpers. There is also a function with prophet to help you with the tasks. Well done for sticking it out.
This is how I was taught, and whilst it does work, it leaves you fairly blind to quite a few programming fundamentals which makes it much harder later on. 
R has you covered: scan("path/to/file", what = "character") [Source](https://stat.ethz.ch/R-manual/R-devel/library/base/html/scan.html)
I agree with so much of this. One of the most challenging aspects of learning R initially was that there are so many ways to do the same thing. I know for myself, it's easy to get hung up on the details of accomplishing a task, which for beginners can be a distraction from the more important details about the task itself (validity and related threats, inferences and interpretation, etc.)
This is interesting. Not to doubt your experience, but programming fundamentals are challenging no matter when they're learned. Why do you believe they'd have been less challenging if you'd learned them first?
Yes, because I would have had a teacher for the fundamentals then, rather than trying to puzzle them out by myself as I'm doing now. 
That is a perfect name 
Hey, I've been following your blog for a while now, I totally agree with your advice on using ggplot2 / dplyr instead of the base package. It helped me get past R's quirky syntax and become productive, which I feel is a big stumbling block for beginners. What package do you recommend for interactive graphics? I've tried ggvis, but it doesn't seem to support a lot of common things, and it doesn't seem to be in active development. What other options are there?
I tried to make the case for teaching the tidyverse in undergrad statistical computing classes [here on reddit a couple months ago.](https://www.reddit.com/r/statistics/comments/6awvyg/r_how_to_selfteach/dhi7cm9/) I don't think the old-school statistics prof I was responding to was convinced but obviously I completely agree with this blogpost. I think many statisticians in academia don't deal with raw unprocessed data often enough to have an appreciation for how efficient tidyverse functions are at making all kinds of data ready for analysis. If all you do is run simulations, or you usually get something already cleaned by your domain collaborator's research assistants, or you work on one kind of problem and the data always come in basically the same, then sure, I can see why you might be unimpressed by someone telling you to use mutate instead of modifying data frame columns directly. So of course these stats professors, unconvinced by the gospel of Hadley (and even maybe a bit put off by the cult-like aspects and reflexively scoffing), will go on teaching R the same ineffective ways they always have. My anecdotal experience is that the grad students and junior faculty at my university in areas like political science are much keener on adopting and evangelizing the tidyverse than counterparts in statistics. For them, building a dataset that combines measures from a bunch of sources, recoding survey responses, etc. takes a lot of work and tidyverse packages ease the pain.
I love it already
Clever
Plot.ly, you can even add interactivity to a ggplot with one function call.
27 thousand GB?
The lsa::cosine function returns a matrix, comparing all n column vectors against each other. That could be the source of the problem; how many columns does your dataset have?
If you expect people to help you, you really need to be more explicit in your problem description.
for each entry in the small data frame compute all distances to the large data frame, use order(...) and which( ... &lt;= how_many_matches_you_want)
sorry for not replying I fixed the problem with help from a coworker, the issue was that the SQL parameters didn't have single quotes around them. Thank you for your help.
sorry for not replying I fixed the problem with help from a coworker, the issue was that the SQL parameters didn't have single quotes around them. Thank you for your help.
No worries … glad you were able to get it to work.
highcharter is pretty slick
No.
It's a useful tool (sometimes) but not at all essential for learning R. Reasons I use Excel: 1) Data entry / simple manipulation 2) Loading a CSV file I generated myself to make sure all the columns line up and I didn't do anything daft with quotes and commas. 
&gt; do I ever need to learn how to use Excel? Like in a job if I'm using simple data could I just always use R since that's what I would be most familiar with? Yes, you will be able to do almost anything you might want to do in Excel in R, so there is little value in learning Excel. You'll likely find that lots of people you work/collaborate with provide you with data in Excel though so it would be worth learning how to get data from Excel into R. There are various ways, exporting individual worksheets to CSV then using `read.csv()` to import them into R, or there are dedicated packages such as [`readxl`](http://readxl.tidyverse.org/) or [`xlsx`](https://cran.r-project.org/package=xlsx‎). You might also encounter people using Google Sheets in which case you can leverage the [`googlesheets`](https://cran.r-project.org/package=googlesheets) package to your advantage.
Yes. Excel is without a doubt faster for some tasks. Also, if your manager doesn't know R but knows Excel, good luck trying to convince him to take your R code and not something ported into Excel. People need to understand what you're doing. If you can do it in Excel, do it in Excel. Nobody wants to learn how to review your code for some simple task that R is completely unnecessary for.
I found for econometrics that if you are compiling your own data from various sources, it can be much much easier to do in excel than in R. As many others have mentioned it's definitely not a must and if you're working with precompiled data sets, it will be unnecessary.
In theory, no. In practice, probably yes. When we are hiring data analysts we look for a language for statistical programming or scripting, like R or Python. We assume they know Excel. If we find out they don't then we would be seriously confused and certainly not hire them. 
Neither of this is necessarily a reason to learn Excel. OP can just import/export data as needed. I’m not saying that learning Excel wouldn’t *help*, but it’s definitely not required, and may be a very bad use of time that could be spent more productively learning something else.
So even if I could take the data from R and convert it to an Excel file that would still not be ok? 
How about "Excel is the industry standard for finance/Econ and if you can't use it then people will see you as incompetent"
I would learn the basics, but not go in-depth regarding complex functions etc. Knowing your way around all office programs is useful, even if you end up not using them frequently.
Any introduction tutorial will explain most of the stuff. For checking the uniqueness, use the negation of %in% (works for vector against vector). (if it should also be unique in File1 you'll also need unique(...) )
Everyone in finance uses excel, even for things they shouldn't be. You HAVE to be able to use it to get by in the industry.
thank, work very nice
 setwd("C:/working_directory") require(dplyr) file1 &lt;- read.csv("file1.csv") file2 &lt;- read.csv("file2.csv") keep &lt;- anti_join(file1, file2, by = "ThisColumn") that should do it. my recommendation for learning how this works though would be to read some base R tutorials then dive into dplyr, which will be all you need for 95% of data cleaning tasks
Yes, learn Excel, but try very hard to only use it for lower level data work. Force yourself to R as much as you can. Because Excel is endlessly deep, it starts to look like the solution for everything. Granted, this is my story, and anecdote ain't data. But I'm fairly good at Excel, and even as my work moves toward a more R reality, I stick with my Excel because I can. I feel like the R folks around me are whipping my ass, but I'm an old dog, and this is a new (to me) trick.
In that case I guess I'll get Excel for Dummies then, thanks!
It's kind of like asking, "If I'm learning French, do I really need to learn Spanish?" They are both languages, sure, but depending on where you are, one is going to be a whole lot more useful than the other. God forbid you end up somewhere they use SAS. The simple answer is, you are in college. Take every opportunity to learn as many skills as you can. The more complex answer is, if you learn both, you will end up using them both for different things. R is great for: * processing large data sets (at least until they get so big using Python would make a noticeable difference.) * making really nice graphics, especially with ggplot2 and other packages. ----------------------- Excel is great for: * when you have small tables, or when you actually want to see the data. * when you don't care what your barplot looks like, and just need to whip something out with default colors and legend. * making presentations because you can manipulate the plots inside powerpoint. * for sharing whatever programs you write because everyone has it. (Especially true when working with lay people.)
Fair enough, but this is completely independent of learning R, and at best marginally related to OP’s question. My reply was specifically to the points you mentioned, and I maintain that these are *not* a compelling argument to learn R. Your second comment, now *that’s* an argument.
You may have to add some code of what you mean, at least I am totally lost. But ?'*' and ?'%*%' might help if you mean multiplying two vectors.
Oh right! So, for example, logistic regression, where the formula you're trying to model is for dependent variable y and independent variables k and l: y ~ k + l + k:l AKA y ~ k*l
Yes. That is what an interaction term is.
That assumes you are in control of the situation. What would you do when you receive an XLXS including pivot tables and charts, and let's say it's from a third party vendor or person in another department of your company that has their own internal tools, and you're troubleshooting something together? "Sorry I only know R". R can't read all that functionality in Excel and it shouldn't have to. They are different tools for different purposes. Everyone with any technical job sitting behind a computer should have at least basic knowledge of Excel. Even people in our HR department use Excel.
I recommend daisy(matrix, method = "euclidean"). Dissimilarities are computed between the rows of the matrix. 
I think(?) you're asking to filter out observations based on the number of missing columns. Here's one solution, but I'm sure there are more elegant solutions (i.e., don't use loops) if you put thought into it. # Make some random data and create NAs x &lt;- replicate(5, rnorm(4)) x[c(1, 3), c(1, 2, 3, 5)] &lt;- NA df &lt;- data.frame(name = c("Alex", "Carol", "Lewis", "Peter"), x) # Mask that is TRUE if less than 3 NA columns mask &lt;- logical(4) for(i in 1:4) { mask[i] &lt;- sum(is.na(df[i, ])) &lt; 3 } df &lt;- df[mask, ] 
 data&lt;-iris[c(1:5,51:52,101:110),] #select some data new.data&lt;-as.data.frame(matrix(nrow=0, ncol=5)) colnames(new.data)&lt;-colnames(data) for (i in 1:length(unique(data$Species))) { x&lt;-subset(data, data$Species==unique(data$Species)[i]) if (dim(x)[1]&gt;3) { new.data&lt;-rbind(new.data, data[which(data$Species==unique(data$Species)[i]),]) } } I don't know of a way to remove lines from a data frame that won't mess up the loop counting, so I create a new data frame and add rows to it. I'm not a great programmer, but this is functional. And if someone knows a better way to create a data.frame with no rows, I'd love to learn it. Some edits: data&lt;-iris[c(1:5,51:52,101:110),] for (i in length(unique(data$Species)):1) { if (dim(subset(data, data$Species==unique(data$Species) [i]))[1]&lt;3) { data&lt;-data[-which(data$Species==unique(data$Species)[i]),] } } If the loop counts backwards, you can remove rows just fine.
I'm curious about why you need a "fuzzy" random forest to classify objects? Have you tried just using a regular random forest?
As /u/Alex_Pan mentioned, I think you mean fewer than x columns. If so, you can use the apply function: remove.these.rows &lt;- sum(apply(dataframe, 1, is.na)) &lt; x dataframe.subset &lt;- dataframe[!remove.these.rows, ]
Careful, `apply` transforms your data frame argument into a matrix. This is inefficient, and results in silent data coercion of your columns into a common type. It's almost never what you want to do. Instead, `lapply` over the row indices, even though that's unfortunately a much less nice syntax. Or use dplyr etc.
I've been attempting to use fuzzy random forests because the data I'm working with is highly correlational, and I've heard that fuzzy random forests deal with that issue more effectively. I've probably set up the forest incorrectly, but when I did actually make a random forest, I was getting a 100% error at one point. I'm not sure what I've done wrong.
Is there any particular reason you want to use R? You can actually directly combine the mp3 files together using "cat" from the command line and it will create a valid mp3 if they are all the same bitrate and format (e.g. the ffmpeg step from this link isn't strictly necessary) https://superuser.com/questions/202809/join-multiple-mp3-files-lossless
Thanks, I mainly use R for my job and I'm always looking for ways to improve so further exposure to R's sound packages could prove useful. 
for sure, experimenting is great. command line stuff can be quite job-valuable as well (it sort of gets you out of the point-and-click mindset even more than just programming by itself does)
with dplyr: df %&gt;% group_by(name) %&gt;% filter(n() &gt;= 3) %&gt;% ungroup()
for 'df', I should use the df's name, right? The same goes for the column name -&gt; name, yeah? But what's 'n'?
Oh, no, there is not a column for every individual. There's one column designated for people's names. Each row has somebody's name under that column, and there are usually multiple rows with a particular name (several data points of a person's performance). What I'd like to do is remove all names (actually, rows that have those names) with fewer than x rows.
n() is a function from the dplyr package that returns the number of rows in each group.
This is the best solution. And yeah, df would be the name of your dataframe, so to save the results in a new dataframe: (and acquire the dplyr package) require(dplyr) new.df &lt;- old.df %&gt;% group_by(name_column) %&gt;% filter(n() &gt;= 3) %&gt;% ungroup() what this does is it takes the old dataframe, groups it by unique names (if they're not unique you need to make a unique identifier for the names), filters out unique names that have fewer than three observations per name, and then removes the grouping structure from the data. here's a good overview of dplyr: https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html
The n() function counts the number of rows in the sub-data-set. library(dplyr) # Create a data frame with varying numbers of rows per "name" df &lt;- data.frame(name = rep(letters[1:10], sample(1:10, 10))) df %&gt;% # Group into sub-sets, each with the same name group_by(name) %&gt;% # Count the number of rows in each subset with n() # Then filter out any subsets which have n() &lt; 3 filter(n() &gt;= 3) So group_by() is splitting the data-frame into sub-data-frames, each with a single unique value of name. n() counts how many rows are in that sub-data-frame filter() removes groups with fewer than 3 rows. To translate this to your code, you'd substitute whatever your data frame is called for df, and you'd substitute the column with the person's name for name.
You need i to increment from 1 through the length of your vector. And the conditions need to check each element y[i], not each index i: pH = c(7.50,7.00,8.00,8.00,7.25,7.50,7.50,8.00,8.00,6.60,8.00,8.20,7.00,7.00,8.00,8.00,8.00,7.00,7.50,7.50,7.50,7.50,NA,7.50,7.00,7.00,7.00,7.00,7.50,7.50,7.00,7.25,7.00,7.50,7.00,7.00,7.00,7.00,7.00,7.00,7.25,7.50,7.25,7.50,7.25,8.00) pH.mode &lt;- function(x) { y &lt;- as.vector(ifelse(is.na(x), -1, x)) result &lt;- rep(NA, length(y)) for (i in 1:length(y)) { if (i&lt;=-1) {result[i] &lt;- 0} else if (8.3&lt;=y[i] &amp; y[i]&lt;=8.6 | 5.4&lt;=y[i] &amp; y[i]&lt;=5.6) {result[i] &lt;- 4} else if (8.1&lt;=y[i] &amp; y[i]&lt;=8.2 | 5.7&lt;=y[i] &amp; y[i]&lt;=6.0) {result[i] &lt;- 3} else if (7.9&lt;=y[i] &amp; y[i]&lt;=8.0 | 6.1&lt;=y[i] &amp; y[i]&lt;=6.5) {result[i] &lt;- 2} else if (6.6&lt;=y[i] &amp; y[i]&lt;=7.8) {result[i] &lt;- 1} else {result[i] &lt;- 5} } return(as.vector(result)) } pH.mode(pH)
Thank you very much, it makes sense! The only thing is that I didn't want to start as "i in 1:y" or "i in 1:length(y)" because I had assigned "-1" to my NA values? Or it does not matter and the sequence start as "1:length(y)" only means for R "from start to finish"? I tried creating a case for NA but it kept failing so I transformed the NAs in "-1" in a new vector.
For R the loop operates on your index. So anytime you want to loop through the elements of a vector x, regardless of what the elements are, for(i in 1:length(x)) is what you want. Shouldn't have to reassign the NAs to anything special as long as you have an if statement to cover them.
Thank you again!
I like using R for everything. When Python fanboys tell me that something isn't possible in R, I like to point out that it's possible, and have examples if possible. Even if it's something as weird as appending mp3 files. You do need command line too though, no matter what. Actually at the 2012 UseR conference there was a "give your data a listen" talk where someone turned data into audio files. It was well received actually, though I wasn't interested.
dataframe$Species &lt;- factor(dataframe$Species, levels = c("virginica", "versicolor", "setosa"))
The answer from /u/SabotageTheWrit shows what the problem with your code is, but the solution they give isn’t very good for several reasons. One of them is that R is being daft: **Never** use `1 : length(something)`. Here’s what happens when `something` is empty: &gt; something = c() &gt; 1 : length(something) [1] 1 0 &gt; for (i in 1 : length(something)) print(i) [1] 1 [1] 0 I can’t think of a single situation where this result is what you wanted. Luckily, there’s an easy fix: Use `seq_along(something)` instead. It always works. --- However, this is still not the best solution to your problem. First off, your code contains lots of unnecessary function calls. Let’s inspect them one by one: &gt; y &lt;- as.vector(ifelse(is.na(x), -1, x)) The result of `ifelse` *is already a vector*. No need to call `as.vector`. Furthermore, the whole statement is actually superfluous: there’s absolutely no need to transform your `NA` values into -1 (which are subsequently transformed into 0 anyway). Remove the whole line, and handle `NA`s late on, instead of handling -1. &gt; return(as.vector(result)) Same thing here: `result` is already a vector, so `as.vector` does nothing useful. Moreover, `return` is *also* unnecessary here. If you want your function’s return value to be `result`, just write `result` (but put it on its own line, don’t hide it behind the closing brace of the `for` loop!). Some people argue that using `return` is clearer — but [they are wrong](https://www.reddit.com/r/rprogramming/comments/5x83ap/explanation_of_code_please/denq9qd/). Lastly, your loop. I’d estimate that 90% of the loop in your code is duplication. This makes it quite hard to see what’s actually going on, namely that you want to assign different modes based on the pH range. This is how typos and other errors creep in, and stay undetected. Whenever you write code like this you should stop and look for a better solution. R has several. One of them is [`case_when` from the ‹dplyr› package](https://www.rdocumentation.org/packages/dplyr/versions/0.7.1/topics/case_when). This is a general solution for writing such otherwise unreadable `if` statements: pH_mode = function (x) { case_when( between(x, 5.4, 5.6) ~ 4, between(x, 5.7, 6.0) ~ 3, between(x, 6.1, 6.5) ~ 2, between(x, 6.6, 7.8) ~ 1, between(x, 7.9, 8.0) ~ 2, between(x, 8.1, 8.2) ~ 3, between(x, 8.3, 8.6) ~ 4, is.na(x) ~ 0, TRUE ~ 5 ) } Still some repetition — but much better! You’ll notice that there is no more `for` loop; many functions in R, including `case_when`, work on vectors and don’t need loops. You should take advantage of this as much as you can. But there’s still some repetition. Every line in the `case_when` (except for the last two) is mapping a continuous interval to a value. Surely there must be something to do this, built in? There is! It’s called `cut`, and it splits a vector into continuous, adjacent (!) intervals. Speaking of “adjacent”: what happens for a pH such as 7.85? Neither your specification nor your code handle this. Don’t assume such values won’t occur, because floating point values aren’t exact; a value that *looks like* `7.8` might in reality be stored as something like 7.8000000001, and your code breaks.^(1) So make your intervals adjacent. And then use `cat`: pH_mode = function (x) { modes = c(5, 4, 3, 2, 1, 2, 3, 4, 5) mode_index = cut(x, breaks = c(0, 5.4, 5.7, 6.1, 6.6, 7.9, 8.1, 8.3, 8.7, Inf), right = FALSE, labels = FALSE) ifelse(is.na(mode_index), 0, modes[mode_index]) } ([Check the documentation](https://www.rdocumentation.org/packages/base/versions/3.4.1/topics/cut) for what the parameters mean.) This code is the holy grail: it has **no repetitions**. Everything in it actually encodes something meaningful. *This*, finally, is a proper way to implement such a function in R. ---- ^(1) Stop: this is *not* merely a theoretical problem. In fact, I stumbled across it when trying to debug your code. I generated some sample pH values as follows: test_pH = c(seq(5.1, 9, by = 0.1), NA) The 7th value of `test_pH` is 5.7: &gt; test_pH[7] [1] 5.7 &gt; pH_mode(test_pH[7]) [1] 4 &gt; pH_mode(5.7) [1] 3 Wait, *what*!?!?? The problem is that `test_pH[7]` isn’t *exactly* 5.7, it’s just approximate due to how computers compute numbers. To display a more exact value, we can use the `format` function: &gt; format(test_pH[7], digits = 20) [1] "5.6999999999999992895" &gt; format(5.7, digits = 20) [1] "5.7000000000000001776" … I hope you see the problem.
WOW. Thank you very much for being so generous with your time and knowledge. I just received a free lesson/lecture in coding. I am in awe (and somewhat moved). I had actually stumbled upon dplyr and case_when while reading *stackoverflow* questions and replies trying to figure out how to fix my code. I read the documentation but there were so many functionalities and I wasn't sure I should try a new package. The point about making the intervals continuous is interesting. I didn't think I wouldhave problems with it because the data being input in the database only has two digits after the point (e.g. 7.50) due to the instrument used to measure the parameter. I understand you explained why continuous intervals is the **best practice**, but wonder if in this case it would not be a problem because of the way the data is input? All the best, 
&gt; understand you explained why continuous intervals is the best practice, but wonder if in this case it would not be a problem because of the way the data is input? Take another look at my footnote: even if your measurements only have two digits, depending on how they are read in R, they will still be inexact, because some decimal fractions simply cannot be represented by exact binary fractions (= floating point numbers). So even if the machine produces the value 5.7, it will be encoded in R either as a number that’s slightly larger or slightly smaller. In fact, out of all the single-decimal digits between 0 and 1, only 0.5 can be represented exactly as a floating point number (because it’s a power of 2: 0.5 = 2^(-1)). Every other number is inexact.
Do you know how categorical variables actually get coded in a model?
Well interaction terms literally are just taking the variables involved and multiplying them together. So for continuous variables you're just looking at the product of the two variables to get the interaction term. If a categorical variable is involved then you actually have multiple columns of data involved due to the way that the categorical variables are coded. I'd suggest looking up how categorical variable coding can be done (there are multiple ways) and get an understanding of that before trying to understand how interactions with categorical variables happens under the hood. If you want to see what's really going on you can use the model.matrix function to see what the input columns actually are. Here is a toy example using the mtcars data dat &lt;- mtcars[,c("mpg", "cyl", "am", "disp", "hp")] dat$cyl &lt;- factor(dat$cyl) dat$am &lt;- factor(dat$am) model.matrix(mpg ~ cyl + am + cyl:am + disp + am:disp + hp + disp:hp , data = dat) That gives you two categorical variables (one with 3 levels and one with 2) then their interaction, then a continous variable, it's interaction with one of the categorical variables, another continuous variable, and the interaction of the continuous variables. That code will show how the data actually gets entered into the model. Once you've learned more about categorical coding then this might make more sense and the interaction terms will make more sense too (since they're just the products of the resulting columns).
http://r-statistics.co/ggplot2-Tutorial-With-R.html
&gt; In fact, out of all the single-decimal digits between 0 and 1, only 0.5 can be represented exactly as a floating point number (because it’s a power of 2: 0.5 = 2-1). Every other number is inexact. Computer science 101: computers process in binaries. Thank you for the patience!
&gt; library(ggplot2) library(dplyr) &gt; data_mean &lt;- group_by(PlantGrowth, group) %&gt;% summarise( mean_group = mean(weight)) &gt; bp &lt;- ggplot(data=PlantGrowth, aes(x=group, y=weight, fill=group ,)) + geom_violin( ) + geom_dotplot(binaxis='y', stackdir='center', dotsize=0.5, binwidth = 0.01) + &gt;scale_fill_discrete(labels= paste(data_mean$group ,data_mean$mean_group , sep = " " ) ) &gt;bp Let me know if this is what you need the "scale_fill_discrete" bit adds in the labels next to the "fill" legend. the "paste" bit adds in the summarized means which was created earlier. paste simply concatenates it. [link of results](http://imgur.com/a/1Iun8) i did not round the means as im lazy
Anyone should feel free to correct me if I'm wrong, but I think the Facebook Graph API has changed a ton since 2012 (the year the original blog post was made) and it's far more difficult to use for crawling/scraping now. But as far as I know, you can't extend out into friends of friends, and their friends, and so on.
Here's a free online version of Hadley Wickham's R for Data Science: http://r4ds.had.co.nz A great resource for plotting, modeling and much more.
why not assign the top and bottom 5 to their own variables and filter that way? I think in your example you are first filtering for the top 5 and then using that result to filter out the bottom 5.
Thanks for the quick response :) I avoided using this technique because I have 55 unique IDs, so I think I'd have to assign 110 variables just for one data frame filter. I plan on doing this type of exclusion for 2% through 5% as well to observe changes in regresssion correlation as outliers are removed. As for your other point, I didnt consider that using the &amp; operator was sequential so that could definitely be a contributing factor. However, I should point out that the number of rows excluded when running the upper and lower exclusions separately add up to the same amount of rows excluded when running them together using the &amp; operator. I'm really wondering why the reversing of the greater-than/less-than operators result in vastly different row exclusions, when (in my mind) they should be complementary to each other.
Can you use the ! Operator instead of reversing the formula
I confirm this
Absolutely! Doing this for the lower exclusions (second code example) removed all but 3460 rows rather than just the 42. So it seems to be doing the same thing as inverting the less-than sign. But that still doesn't help me figure out why the results are not complementary. For example, is had a sample size of 70,000 and excluding 42 rows when using the exclusion script, then shouldn't using the ! Operator or inversing the less than operator output 69,958 rows?
I know dplyr is all the rage and it *is great*. But when talking to a db wouldn't it be better to just use sql directly? Two reasons why I would recommend this: 1) You learn something much more broadly applicable, 2) It's orders of magnitude easier to find good and relevant documentation. And SQL is not more complicated than trying to use dplyr's pseudo-sql.
I think you have your parentheses wrong: inner_join(tbl(mydb, "flight"), by = "flight_id") Dito for `condition`. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/datascience] [XPost my question from r\/RLanguage: twitteR package returning short-form rather than long form URLs. Any thoughts?](https://np.reddit.com/r/datascience/comments/6n518m/xpost_my_question_from_rrlanguage_twitter_package/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
ugh where is my feedback... i want my fake internet points...
I don't use Photoshop so correct me if I'm mistaken, but if what you're trying to do is convert images to black and white then you do it by linearly combining each channel into a new one and then saving it. There are formulas out there that describe the weighting each color channel should have. I do not know of any R packages that split the image into the channels you described but most if not all do RGB. So you would take each channel and do something like xR+yG+zB=blackAndWhite, and then save the image to disk. 
You're right, I had it wrong. It's still not working correctly though if anyone had any additional ideas.
I agree that using SQL would be ideal, and I actually already know many of the commands so it wouldn't be difficult at all to do. But I need to learn dplyr for a class, so I can't default to SQL.
I stumbled upon some packages but the thing is that I get the desired output changing channels red, yellow and green, so changing RGB wont do the trick (or maybe it could but I'm no color expert)
Is this a binary classification problem (two classes)? If so, 100% error is just 100% accuracy backwards. Care to post some R code? I could take a look.
Here’s something that might help get you part of the way - https://stackoverflow.com/questions/22352568/cmy-colour-function-in-r
Agreed. Either dump the relevant tables into data frames to manipulate, or pull the proper report from SQL. 
Remove the quotes around `conditions_precipitation`. It will never be true that the string `"conditions_precipitation"` is in that vector of weather conditions.
The problem is basically that your `date` column isn't a date data type: &gt; class(x$date) [1] "factor" So the first thing you need to do is to convert it to a date data type. This is easiest as a two-step process. The first step is to learn about the wonderful `stringsAsFactors` argument of base R. This will turn your factor into a string when it's imported: &gt; x&lt;-read.table(text="date,value + 2017-01-01,4 + 2017-01-02,1 + 2017-01-03,1 + 2017-01-07,5 + 2017-01-13,22 + 2017-01-22,33 + 2017-02-10,41 + 2017-02-11,31 + 2017-02-12,2 + 2017-03-13,3", sep=",", header=TRUE, stringsAsFactors = FALSE) &gt; class(x$date) [1] "character" You should usually do this as standard (or, swap to using the `readr` package which is faster and has better defaults than the base R functions). Then the second step is to make `date` into a date data type, which is easy: &gt; x$date &lt;- as.Date(x$date) &gt; class(x$date) [1] "Date" And now `plot` can understand what its defaults should be a bit more easily. [New output from `plot(x)`](https://i.imgur.com/RMjMebk.png). Also, once you start getting into advanced kinds of plots I'd recommend learning the package ggplot2 over base R plotting. PS, props for a very well-asked question. You provided a small dataset and the minimum amount of code to demonstrate your problem and a good description of what happened and what you want to happen. That made it very easy to help you, so good job :) 
Makes sense. Thanks!
This is it.
If you are forecasting from a time series, the typical steps are as follows: 1) separate data into training and test sets (where test set is a sequence at the end) 2) fit competing models on training set, and then test prediction accuracy by forecasting test data 3) fit "best" model to whole data set and use it to make real predictions (aka "model validation") All that said, within each of these steps there are many choices you make that have no real "best practice". For example, how much data should be used for the test set? Well, it depends on the time tends in the data you have. What criteria will you use to select the "best" model? There are many metrics one could choose, and actually using some ensemble of all your models will likely give you better predictions than any given model. 
Haven't tested, but something like this? newlist = lapply(list, function(x) as.data.table(x)[(nrow (x)+1), `:=` (col1 = NA, col2 =NA)]) unlist(newlist) EDIT: changed '=:' to `:=` (was on mobile and couldn't find the backticks, also got it backwards) 
I'll take a look at tomorrow. I seem to have bludgeoned it using data.table but your code looks way better.
hey, that version didn't work, but I was intrigued so I finagled a different way. The way below is tested to work. list = list(cbind(seq(1,10,1),seq(1,10,1)),cbind(seq(1,10,1),seq(1,10,1)),cbind(seq(1,10,1),seq(1,10,1))) newlist = list() for (i in 1:length(list)) { l = list[[i]] ll = rbind(l, c(NA,NA)) newlist[[i]] = ll } final = rbindlist(lapply(newlist, function(x) as.data.table(x))) 
`purrr` is a great package for lists. In particular `purrr::map` will let you apply a function to each element of a list similarly to `apply`.
Thanks for the answer it cleared the topic a bit. What I am still wondering is that once the training data model is good fit and performs well should I just use the same model to forecast both the testing part of the data as well as future? It seems quite redundant to use a model to "forecast" the time period that we already have data on. Is this part where I should "refit" the model with same parameters to the training+testing set? I will try to focus next week on learning the different methods and specifics of different models :) Also here is a good read for anyone else getting started with forecasting time series https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials
This was the issue. Thanks for the help!
Hey, for some reason, using your code (with appropriate substitutions) didn't work. I can't count out the possibility I did something wrong. But what got it done was: df2 &lt;- group_by(df, name) df2 &lt;- filter(df2, n() &gt; 2) Many thanks for pointing me to the right direction and getting me 99% there.
I like openxlsx
[WriteXLS](https://cran.r-project.org/web/packages/WriteXLS/index.html) is also nice. It uses Perl, which is installed by default on most Linux machines.
`tokenizer::unnest_tokens(... token = "character")` should get you pointed in the right direction. `library(tidytext)` is great and uses tokenizer. recommended if you subscribe to tidyverse philosophy, hadley's stuff (PBUH - Praise Be Upon Hadley) https://www.rdocumentation.org/packages/tidytext/versions/0.1.3/topics/unnest_tokens
Group them to lessen the entries then do a boxplot. Careful with the groups though, I think there is a rule to maintain representativity (idk if that is the word, translating from other language) 
Seconding this. Also check out http://tidytextmining.com/ for more info on how to use the package/various examples and tutorials. 
Can you please give some more information about your data, is this already cleaned/treated in any way? If it's raw data you should start with a PCA to emphasize variation and bring out strong patterns in a dataset
Might not be the best way, but... &gt; str &lt;- "puppies" &gt; v &lt;- paste(substring(str, seq(1,nchar(str),1), seq(1,nchar(str),1)),collapse = " ") &gt; v [1] "p u p p i e s"
To answer your question–no, you should use the model that fit best on test data, and then re-fit it to the whole data set (training + test). The reason is that in time series models, the further you are forecasting into the future, the more uncertain your estimate becomes. So, it you forecast using the model only trained on the training set, then you are throwing away valuable information that could provide better/more certain future estimates. Of course, this assumes that the time-series is stationary to some extent. All that said, you are correct in your thinking: &gt; It seems quite redundant to use a model to "forecast" the time period that we already have data on. Happy modeling! 
How does it look in a histogram?
Hi, I'm trying to apply R for by business and have recently stumbled upon the exact same thing too. Turns out that R doesn't have that capability. Here's an article explaining some details and giving the answer how this can be done in R https://stackoverflow.com/questions/20689650/how-to-append-rows-to-an-r-data-frame
I think your confusion stems from trying to implement the cross join as part of the j argument, CJ is usually used in i argument. This code works fine: DT2 &lt;- DT[CJ(unique_point = unique_point, biased = biased, type = c("A", "B", "C"), unique = T), on = c("unique_point", "biased")] Read up on the X[Y] join syntax. I find it kind of confusing personally and try to use merge() where I can. Another way of doing this but broken down into two steps, imo more clear about what exactly is happening: DT_CJ &lt;- CJ(unique_point = DT$unique_point, biased = DT$biased, type = c("A", "B", "C"), unique = T) DT2 &lt;- merge(DT, DT_CJ, by = c("unique_point", "biased"), allow.cartesian = T)
I think this would be a lot easier if you use 'expand.grid' and then convert the data.frame back to a data table later. Maybe you could use expand.grid within a data.table, but why kill yourself. After expand grid, join/merge back in the data columns. 
Yes it does, it's just that data tables and data frames are different in this case. This is a more relevant discussion https://stackoverflow.com/questions/16652533/insert-a-row-in-a-data-table Data tables are not very insert friendly and you need to combine similar data structures instead. You can insert at the end of a data frame though. 
Yes it does and doing it 1000 times slower than otherwise is "no" for me. (Although, technically, yes)
add the xlim = c(1910, 2016) argument inside of plot() 
Might have been an issue with the pipe operator, if that worked and the other didn't, since they're basically the same functional composition (unless I just misunderstood the numerical criterion, in which case... my bad). 
Seconded. Facebook limited the API's functionality a ton. 
There's also options to choose exactly where you want your markers.
&gt; my coding language is somewhat basic No dice. In order to harness the GPU for custom computations you’ll need fairly advanced knowledge of C or C++. R itself, let alone a basic knowledge of it, is insufficient.
Okay, pretend that I have advanced knowledge of it, my bosses do and if I find anything to help them then they can understand it.
One way to do this is to use RSelenium to actually run a browser and then select elements. That's the way I usually go. You can also use the httr package and GET() to load them iirc.
1) consider different algorithms like xgboost 2) https://devblogs.nvidia.com/parallelforall/accelerate-r-applications-cuda/ Give that a read to get started. I'm not an expert but it's something. 
I'm not sure how one can even say that. If you had even the most basic Google skills you would have already Googled the high performance computing page on CRAN with a dedicated section to GPU computing which would have listed at least a few packages that would have served as starting points. And if you can't understand the above paragraph, then maybe you shouldn't use reddit to do your job.
Ahh thanks for your help, well actually snarky comment. I found that website and from it and jogging my bosses memory I learned I needed to use our linux machine to use the package I wanted to. 
There is probably a way better answer for this on stack overflow, but whatever here we go...I'm on mobile, so I'm not going to try and format this response, so this will basically be pseudo code. I would use dplyr for this, so to start I would load that with the commands library(dplyr) or library(tidyverse) Second you want to group your data frame based on your ID column which looks like group_by(ID) Third, you use mutate to create each of the column means, which looks like mutate(v1mean = mean(var1), v2 mean = mean(var2), etc So putting it all together: library(dplyr) newdf &lt;- dh %&gt;% group_by(ID) %&gt;% mutate (v1mean = mean(var1), v2 mean = mean(var2)) newdf 
Thanks for the response, shall try this in the morning when I'm at my work pc. Just one more question. If my Id is a string like "identification value". How do I use that when theres a space split in the name? 
I don't think Dplyr will take a quoted column name (which is the whole reason I use it), but I think you can use the back-ticks (usually the very top left key on a full keyboard) instead. I'm not at my computer so I can't test that, sorry. 
This should get you started (NodeJS) https://gist.github.com/MattSandy/025735db0fe7d8994ea957b44b2b2928 As far as the the internals of each page, they do make a seperate request with post information (sent as JSON maybe?) for each school. Might need to use PhantomJS if you can't hook into post request. 
You are correct. Enclose the column name in backticks in dplyr and it will parse correctly. 
Yes it’s possible but since you didn’t give any details on what approximation you’re performing, it’s impossible to provide you with more details.
Sorry, something like this for about 30 companies with about 10 values in both x,y per company. Currently I am subsetting each companies data, but this requires me to read the datatable to know which companies exist. firstcalc &lt;- approx(companyA$value.x, CompanyB$value.y , xout&lt;- 1:100 , method="linear")
First issue: your code isn’t specifying an `xout` argument, it’s assigning to a *variable* called `xout`. Use `=`, not `&lt;-`. Anyway, you can avoid hard-coding the company names by putting the values into a common list or data.frame rather than having separate variables for each company. That’s the proper way to encode this data.
RSelenium is also quite easy to learn. This is probably your best solution, depending on how many pages you actually need to scrape
I'm still not clear what the "approximations" are you're talking about. But whatever it is, you'll want use dplyr and group_by each company. I can give you some sample code if you clarify what the approximations are
I've never used R, but trying to learn as a complement to my uni studies. The approximation function ( *approx()* ) as I've understood it can be used to interpolate values. In this case I want to interpolate values x and y from each company. Maybe this is wrong, should I use approxfun instead? 
so your desired output is a linear interpolation of values disaggregated by companies A and B? maybe you can give a quick example of what an idea output woudl look like
Try: Table1 &lt;- cbind(Table1, Title) Table1 &lt;- cbind(Table1, Review) And check to ensure new columns are accurate: View(Table1)
Use lapply and data.frame: library(dplyr) tmp &lt;- lapply(1:N_pages, function(j) { website &lt;- read_html(paste0(url, j)) Title &lt;- website %&gt;% html_nodes(".a-color-base") %&gt;% html_text() Review &lt;- website %&gt;% html_nodes(".review-text") %&gt;% html_text() Stars &lt;- website %&gt;% html_nodes("#cm_cr-review_list .review-rating") %&gt;% html_text() %&gt;% str_extract("\\d") %&gt;% as.numeric() data.frame(Title = Title, Review = Review, Stars = Stars) }) reviews &lt;- bind_rows(tmp) This has the advantage of not requiring you to update an object every time (rbind operations can get kinda slow) - you get a list of 15 data frames, and then you stick them all together in a single step. You could get fancier and use purrr::map_df(), but that tends to be more complicated than you'd need for something like this. If you wanted to eventually use a vector of URLs, though, I'd probably go the map_df() approach. 
Mmm this is almost there, but it is listing each new set of data laterally instead of the same column: For example, it goes: Title1,Stars1,Review1 Title2,Stars2,Title2 Hope that makes sense, tough to format. 
Weird, I'm getting the following error: In bind_rows_(x, .id) : Unequal factor levels: coercing to character Is it because stars is numeric while title and review are text?
You can look at the [package archive](https://cran.r-project.org/src/contrib/Archive/openxlsx/) to see if any of the older versions of openxlsx work. Maybe try to match the version installed on your computer with the one your coworker is trying to install?
You probably won't get that error if you use data_frame instead of data.frame, or add stringsAsFactors = F to the data.frame() call. 
Look into the "broom" package. It has a function, tidy(), that converts model coefficients to a dataframe. Hope that helps! https://cran.r-project.org/web/packages/broom/vignettes/broom.html
Thank you! I've been searching for over an hour!
Also have a look at xtable and sjPlot. 
You could try the development version install.packages(c("Rcpp", "devtools"), dependencies=TRUE) require(devtools) install_github("awalker89/openxlsx")
Clone the repo from github, open up the project file, then use `devtools::build()` with `binary = TRUE`. If you don't have Windows at home then use `devtools::build_win()` but read the documentation carefully to make sure the zip file gets emailed to you and not the package creator.
It would be helpful if you could explain what your expected end result is. You've explained your situation pretty well, but you didn't leave me with a good understanding of why it's a problem or how it doesn't match your expectations. Also, is there a reason you're using reshape2 instead of dplyr/tidyr? 
What's your end goal? It's possible to make a list of data frames, but I'm guessing this is just a part of a bigger solution you're working on. If you really want to split your my.data into a list of dataframes for each name in bolag, you can use `plyr::dlply`. Otherwise try to describe what you're trying to do after you got this list. install.packages("plyr") # need to do this only once library(plyr) list_of_data_frames &lt;- dlply(my.data, "Bolag", data.frame) More general advice: read [this](http://r4ds.had.co.nz/transform.html) and keep referencing it while you're learning. 
I want to interpolate the values from mty.years and z.spread.mid for each name in bolag (each name can have up to 10 unique values of mty.years and z.spread.mid) and then exporting them to excel. I figured the best way to do this was first to split up my.data and get the values of mty.years and z.spread.mid for each name in separate dataframes which are named after the name in bolag. Thanks for the help and link!
So you want to create some new values of z.spread.mid. Is this information "independent" or do you need information from the other Bolags? If you don't, you can do something like this (the link above should explain how the data wranglng works): install.packages("tidyverse") library(tidyverse) # Start manipulating mt.data ... mt.data &lt;- mt.data %&gt;% group_by(Bolag) %&gt;% # Add a new column with the interpolated values. # Here's an example with cubic splines mutate(i.z.spread.mid = splinefun(mty.years, z.spread.mid)(mty.years)) 
yeah exactly. The information is independant from the other names in Bolag, but one name has multiple mty.years/z.spread.mid values.
So, my code above (or maybe you want a different interpolation function) gives you the results you seem to be lookng for here, but I guess this is still not the final goal. What do you want to do with this interpolated data in excel? 
by "source" do you mean read in the data? you need to tell R where to find the files. Depending on what function you are using to bring in those files, you either include the path with the function, or you can use setwd() to provide the path to the files. 
The file was given to us with their sourcing: source("C:/R/ModelCode2015/DayLen.R") I changed it to where I saved the file on my computer: source("C:/Users/My Name/Desktop/EagMorMod/DayLen.R") But when I try to run the program it tells me it can't find DayLen. Is there something obvious I'm missing?
try this: setwd("C:/Users/My Name/Desktop/EagMorMod"); source("DayLen.R") if that doesnt work try this: setwd("C:/Users/My Name/Desktop/EagMorMod"); file.names&lt;-dir() that should return a list of all the files in that path and then check if "DayLen" is listed
I tried doing that and the error I got says "cannot change working directory".
Ok then that probably means you have something wrong in your path. Step through it manually to make sure the document is exactly where you are saying it is. Also make sure your spelling is correct. R is case sensitive. 
I think I figured out the problem. The instructions I got didn't say that you have to run the sourcing code. I did that first and the rest of the program worked fine. Thanks so much for the help.
Homework!
If you write out in words the strategy you want to use to get the answer and the problems you're having translating that strategy into R code, I can help. 
?table
.
Does it work when you run the code chunks manually in e.g. RStudio? Have you disabled caching?
The script works fine when not running it in Knitr, and I'm using RStudio. In one code chunk, a function stores an object to the global environment using &lt;&lt;- within the function. In another code chunk, it is supposed to use that stored object for a function. It says the object is not found. I haven't disabled caching, not sure what that does or how it is done.
They are not the same thing. Nan, *not a number* are the result of some calculation such as 0/0 , and should be unrelated to NA which are missing value. There is sth happenning in your data, removing the NaN is not a solution. https://stats.stackexchange.com/questions/5686/what-is-the-difference-between-nan-and-na#5687
Yes, I recognize that they are different, but the NaN result did not appear to be there before grouping all values by 'col3'. There were some rows without data that were removed by na.omit, but then for some reason the sort() function still reports it as a category (though one that is unnamed, as before, and with zero values.) I believe the NaN error came up after I tried to perform an operation on the sorted categories. This makes sense as it's an empty category, so if any part of that operation involved a division, it would obviously return NaN. My question is why sort() would still keep this category that has no name nor any values counted. 
OpenMP is a C/C++ library for parallel threading on one computer. So my best guess would be that there is a problem with your compiler set up. (or if you aren't using gcc/g++ the flag might also be called differently - only have experience with them)
Guessing you're on a Mac and are attempting to install from source. You'll need gcc... Which is not the clang in disguise that claims to be gcc.
can you explain what you mean. I am a novice.
I'm definitively not a specialist on the matter, i just encountered a NaN problem the other day and chipped in. Maybe the issues is on the data side, or not, your example is not that explicit, nor reproducible. Inspect your dataframe with is.nan(). Also &gt; There are a handful of rows (4, it seems) that either have NA values or no data you should code no data as NA. 
Install GCC https://solarianprogrammer.com/2017/05/21/compiling-gcc-macos/
You can use R as a general purpose scripting language. R is also good for manipulating rectangular data and making plots. 
[Rscript](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/Rscript.html)
Can I call an R script inside my shell script?
u/ReimannOne is right. If your file is foo.R, then your shell command will be `Rscript foo.R`
Not if you can't read a help file.
I see, my problem was when I had to pass a string from the shell commands to my Rscript (i.e I wanted my Rscript to inherit $s=5) but I can try what you said
Rscript can pass arguments see ?commandArgs
Thanks.
If you want to call it in a more convoluted way you can also do this &gt;Terminal echo "test1 test2 test2" | Rscript run.R &gt;run.R line &lt;- unlist(strsplit(readLines(file('stdin', 'r'), n=1), ' ')) print(line) #[1] "test1" "test2" "test2" Words for piping an output from something else like &gt;Terminal pwd | Rscript run.R \^To pass the current terminal directory to the R script.
Also argparse package
Since you are looking to automate and are already using Photoshop, maybe you could also use the batch processing function photoshop (used to?) have. I am aware this is not an R solution but maybe it bypasses your R problem.
not sure but I think NaN may be like NULL in SQL where it's unknown
Can't read Polish but looks great! 
See 40 other bands on https://www.facebook.com/media/set/?set=a.1782337698723389.1073741830.1775448726078953&amp;type=3
How does this DVC differ from a good ol' fashioned Make file? Seems to be the same based on the examples. Remind me to put a makefile of some sort in my analysis folders...
It uses AWS S3 and GCP cloud storages for data.
You probably don't need any package for that part. What you want to do depends a bit on the formatting of the text file. Assuming it is a txt file and the character name are on a separate line, before there text comes: read the text in with readLines, run over the lines with a for loop where you first compare the whole line against a vector of character names, if it matches start saving the next lines in a list (or something) for that character until you get a line that matches a different character name. etc. You'll have to also handle stage directions etc. I doubt there is a package that somehow does more of the work for you.
Makefile keeps only current version of your experimental data. DVC keeps all of them. So, you can return to a data file that was generated two commits ago without rebuilding the file. It is useful for data projects and ML when one step takes hours. And there are a few more nice features like integration to Git and cloud storages.
How does it work on databases (say, SQLite) that are populated over a range of commits?
You need to write a script that pulls data subset from DB and stores it as a file for a feature analytical steps. DVC can control the imported files, but not DB.
This would be fantastic and I would use it dozens of times a day. Also, I would love to be able to assign Ctrl+r to run lines of code... I learned R on Windows but on Linux the only option is to hit Ctrl+Enter!
Look into autohotkey. I think it could handle this just fine. 
I have some apps on [my website](https://daattali.com/shiny/) that customize the ui. The most extreme one is [this one for football game replays](https://daattali.com/shiny/cfl/) or my [masters project](https://daattali.com/shiny/ddpcr/)
**How** you can use it in your functions is a very different question to **why** you should use it in your functions, which is a different question again to **what** use cases you might already have for tidyeval. I think part of the problem people are having understanding tidyeval is that the explanations are trying to do too many things at once. They're trying to introduce new concepts and new terminology - a lot of which is trying too hard to be too cute, like "quosure" - and their functional purpose, while also explaining the historical backgrounds and the necessity of their existence. It's natural that that would be the case I suppose, given that a lot of people are confused about the latter as well as the former, but it doesn't make it easy to understand! Ultimately though **why** you should use tidyeval is very simple. It's basically only got one specific use case, which is the following: you have a function, into which your user might pass an expression: my_specialised_filter_function(b == 3) And you're worried about what value `b` is supposed to be the name of. Should it be the name of a column in a data frame? Should it be in the calling environment (R's default)? Should it be some other environment following some rules you've laid out? The whole purpose of tidyeval is to: 1. Delay the evaluation of `b == 3` so that it's not evaluated before you've had a chance to redefine where it will be evaluated. 2. Redefine where it will be evaluated. 3. Evaluate it at the right time in that new context. That's really it. **What** uses you might put this to really depends on your domain, but I would highly recommend finding a specific case where you need to know this before you dive in. It's far simpler to just use the features of `tidyeval` that are exposed, for example by the newer version of `dplyr`, than it is to understand how they work. You need the extra context of having a problem to solve to really understand it. As to **how** you can use it - well, that will be dictated by the exact use case you want to put it to. I've hinted at one very simplistic example above - you might have already established some rules about how the expression will be used to filter existing data but you want your user to be able to pass whatever expression they like that will be used to filter the data in some specific ways. For that, you could benefit from `tidyeval`.
Thanks, those look really good !
When RStudio enters the right parenthesis for you, after you typed the contents of the parenthesis and type ) it simple ... overwrites the previous one. There is an option to disable RStudio from inserting the paired parenthesis. Find it in the editor's setting.
No offense but this is a bad solution. I am trying to make use of a good feature and you're suggesting I just kill the feature? What if I have double parentheses or different types of brackets? I'd end up doing more work than necessary.
&gt; It's difficult to work with the file as it is very large at 624.7 MB, and it crashes my browser whenever I try and look at it's structure, but if I could see the tags, I would know the exact values I wanted to extract. If you're on Mac or Linux, you could use [head](https://www.linux.com/blog/14-tail-and-head-commands-linuxunix) -100 to get the first 100 lines of the file and you could redirect those lines to another file that won't crash. It won't be valid XML since you'll have tags that aren't closed, but it should give you a better idea of what you're working with. It appears that someone else got [the same error](https://github.com/hiratake55/RForcecom/issues/15) and thought it might be because the XML was too deeply nested. You might want to write some xpath queries to create your data frame instead. If you use xpath, you can also see the structure of your file - 1. start out with the root node 2. look at its children 3. choose a child 4. look at its children and so on. Then you can determine what tags correspond to each column you want to keep and hopefully traverse your tree that way. 
see caret maybe
Wut? I suggested that you literally do not have to do anything. Keep the feature, when you get to the end of your parenthesis, key in ). It will *not* add a second ). If you have multiple nested sets (parentheses, square, or curly brackets), RStudio keeps track of them for you.
Machine learning? Why not just a periodogram or DFT first to see what you can learn??
Did it already.
I've had success using this [package](http://code.markedmondson.me/googleAnalyticsR/v4.html). There is an example in the documentation on that page that will hopefully help you out. 
??? You told me: 1. Rewrite the parenthesis over R's automatic one 2. disable the paid parenthesis feature both of these undo the auto parentheses and don't solve the problem. I'm not trying to be a jerk, I'm just saying these aren't solutions to my problem.
Fantastic. The examples on batches are exactly what I was looking for!
u/a_statistician's answer is good, but there are also packages for parsing XML, with apply like functions for traversing the nodes of the XML. The XML package is the first hit on Google and there are tutorials too. https://www.google.com/search?q=r+xml
http://blogs.neuwirth.priv.at/software/2017/01/03/rstudio-on-raspberrypi-3/
Would you mind explaining your use case? Or, some other use cases? I have no experience with Raspberry pi and find it interesting. I know people who use it for homebrewing beer... nothing about R though.
you can use R directly from teh command line on raspberry Pi (something like sudo get apt R) , but that's less desirable than having the full IDE. I think it's just for fun; there's nothing that a Pi can do that you'd need R for that another computer wouldn't be better for, especially for big data that requires speed. That having been said, I think it'd be fun to do my stat homework on my Pi Zero W so ... when the semester starts up again!
Thanks so much. I think part of the issue is my attempt to wrap what is a bunch of tidyverse code into a function, and backing out from there made it confusing. The delay evaluation part helped it click for me.
Haha. Makes sense. I'm still just trying to think of something practical to do with it! 
Digital guestbook with captive portal under its own wifi hotspot, home automation, million things with art or data driven physical notifications (motors, lights, displays, etc). 
I usually think of R for research, model building, prediction, etc. Can't imagine those applications for a digital guest book.
Oh, I thought you meant a Pi in general. 
Check out map_df from the purrr package 
Try reading this: http://www.statmethods.net/interface/customizing.html
You mean the function called `c` that creates a vector or list? I wasn't aware that this was a debate. [The help file](https://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html) has the word "combine" in the title.
Hey, not sure if the previous method you mention (using ggplot2) is implicitly alongside R Markdown?
Rscript name_of_your_script.R
You can use a for loop for a list of websites. Get it working for one website then build a for loop around it. You can store the results separately or if its similar data use cbind/rbind to make a growing dataframe
Look into doMC and their foreach operator. If you set up your problem correctly, I have a feeling it would work.
http://www.lmgtfy.com/?q=execute+r+script+from+terminal
https://play.google.com/store/books/details?id=EsWeBAAAQBAJ
Feed your list of sites into parLapply from the snow/parallel library. Below I [1] make a cluster with 8 nodes, [2] loading rvest across all nodes with clusterEvalQ(). Next [3] I run all of my urls (in a list) through my desired function. When it is done, I close the cluster [4]. cl&lt;-makeCluster(8, "SOCK") clusterEvalQ(cl, {require(rvest)}) dat&lt;-parLappy(cl, list_of_urls, function) stopCluster(cl) Note that 'function' above can be something you create also. For example: dat&lt;-parLappy(cl, list_of_urls, function(x){ do(x) %&gt;% something() %&gt;% cool() %&gt;% here() }) This won't be exactly 8x faster, but it'll be close as most scraping processes are quite resource light. I often run dozens more than I have CPU cores for with great results...
If you are running [ArchLinux ARM](https://archlinuxarm.org/) (or in fact almost any Raspberry Pi distro) then you can use [Emacs](https://www.gnu.org/software/emacs/) + [ESS](http://ess.r-project.org/). Something along the lines of the following should get you up and running and ArchLinux ARM... sudo pacman -Syu emacs git clone https://aur.archlinux.org/emacs-ess.git cd emacs-ess makepkg -sri cd .. 
While running XGBoost what does your performance monitor show? In my case I could see it load up pretty much all of threads without any special calibration. m1 &lt;- xgb.train(data = dtrain, max.depth = 7, eta = .3, nthread = 15, nround = 400, print_every_n = 100, gamma = .001, watchlist = watchlist)
For instance: library(rvest) page &lt;- list() for(i in 1:5) { url &lt;- paste0("http://www.address.com/some/page=", i) page[i] &lt;- read_html(url) }
I never actually ran it because I kept seeing when I tried installing it that it doesn't support OpenMP.
the inverse FFT maybe ? Just glancing at the documentation it looks something like fft(FourierComponents , inverse=TRUE)
Tried that, but doesn't look like it works: originalWave = fft(fft(wave), inverse = TRUE) plot (originalWave) http://imgur.com/a/2v54j
What happens if you divide the results by N=1000 and kill the very small complex component?
Check to see if it's actually an issue first. I personally found it to be pretty much run and go when it came to code. Set up your sparse matrix, set up your test/training data sets, have fun. Heck, if this is a one-off on a relatively small dataset, even if multi-threading doesn't work you might only be out a few seconds - if that's the case, don't worry too much. XGBoost runs REALLY fast. 
I did try running it a while back and noticed it was taking hours (I started doing hyperparameter tuning with a grid search so that's why it takes a while.) My multithreaded python version of the script runs fine.
That's odd then. I definitely understand grid search making things take a lot longer though. Even then what did task manager say? It sounds like you have a decent feel for what you're doing, sorry I can't be of more help. 
I re-edit my post so it can be clearer what I want
Haha no worries, to be fair I was trying to see the functionality of the code to translate it to Python for a company I'm interning at. But I eventually just skipped that step and translated it to python. Thank you for your help anyways!
Thanks, that was it! Turns out there was nothing in my .Rprofile but a rouge url. 
No, I was using standard R code with ReporteRs to output to powerpoint. While R Markdown is a viable alternative I have some end users who demand powerpoint output.
The point about performance is a bit misleading. Python uses C under the hood too and both R and numpy can link to accelerated numerical libraries if you install them correctly.
Yes but it is hard to know exactly how to help without knowing more about the csv. Are you trying to merge datasets? 
What exactly is the analysis, why would it return iphone count of 50. Would it find that in one of the table values above? Is what I saying making sense? Try to be as explicit as possible like I don't know anything about the project 
Is most of the numerical stuff even C? I thought BLAS/LAPACK were fortran? Or does it depend on the library? Too lazy to look it up right now.
Yeah not sure, actually. But C or Fortran, the distinction isn't so important here.
Awesome! Signed up! Thanks for hosting this! I'm excited for the real life company examples.
Thanks for tuning it! We hope that it will be helpful for you.
bargraph(1:10) or what are you looking for? have you googled "R bargraph" and looked at examples?
What does the data you are using to plot your chart look like? What have you tried already? What was the result?
I have mainly looked at ggplot2 and tidyr packages which has led me getting so confused. To answer your question ... yes I've looked at examples but those examples have more than one variable they use to graph. I only have one variable I'm trying to graph. Edit: To clarify. Grammar. 
The data I am looking at are tweets, @POTUS. I've been using Introduction to tidytext ( https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) and just getting more confused. I've filtered the data to which Twitter followers @POTUS reweets, I've got the top ten (want more). 
Not sure what that data would look like (list of strings with usernames returning multiple times?). Have you tried a simple hist () function with your data container as argument? 
It's like pulling teeth in here sometimes. What do your data look like? What is the head of your data frame? Is it already summarized or do you need to take means? What would your bar chart look like?
You're still not being clear what your variables are. Tweets? Followers? Number of retweets? What's your y? What's your x?
Please post your code so far. 
Variable: Reweets Accounted being looked at: @POTUS X: Those accounts @POTUS have retweeted (not followers or tweets) Y: The number of times those accounts have been retweeted 
[removed]
Geom_histogram is what you want.
Maybe this sub needs some info in the sidebar on how to ask a somewhat decent question... Questions like these are very annoying indeed.
How is your data structured? From your example it sounds like you're doing a single calculation referencing a 2x2 area in a spreadsheet. If you're only doing a single calculation it might not be worth it to do this in R. If you're doing multiple calculations, you need to restructure your data into a more consistent format: the most popular and easiest is to have a single observation/record to on a single row. 
http://www.burns-stat.com/spreadsheet-r-vector/ Near the middle. 
This isn't what you're looking for but [this](http://r4ds.had.co.nz/) is an excellent guide to get started with. Feel free to skip to portions of the text that are more appealing
Honestly, I found the best thing to do is just attempt an Excel function and then if it doesn't work just quickly google "r equivalent of excel ____() function". Between R, Excel, Python, SAS, Tableau, SQL, etc. it gets too hard to remember what is what. Good luck!
If you're fast you can cheaply buy a udemy course that explains that and more. the course is called " R programming A-Z™ " It's a great course that I can warmly recommend
How much are we talking, causing making a video and learning how to do a new task takes quite a bit of time.
Hey Boom, umm, the amount can be negotiated... and an actual video is probably unnecessary. I just would like to animate this data set I have in excel but using R. The data is heart BPM vs time, so I would like for the animation to look a certain way that makes sense. It's over a period of 10 hours i think, so it needs to be sped up/time-lapsed. How could I send you excel file to talk further? 
you mean like df[[1,1]] &lt;- 1 ? or df[[1,2]] &lt;- 0? I don't understand what you are asking. 
I'm assuming you're used to subsetting like this: my.data.frame$my.column Instead, you can do this: my.data.frame[,1] 1 is the index of the the column you are interested in. From here you can do a loop, but you also can do this more programmatically. Identify all yes and no values and change them to 1 and 0. You can then cast every column to what you need by identifying which ones are 1s and 0s. 
Stackoverflow has a detailed guide on writing a good question.
Well, here's some hacky ways. This solution would give you yes = 2 and no = 1. df &lt;- matrix(sample(c("yes", "no"), size = 24, replace =TRUE), nrow = 6) df &lt;- data.frame(df, stringsAsFactors = TRUE) sapply(df, as.numeric) And this is ugly but works: df &lt;- matrix(sample(c("yes", "no"), size = 24, replace =TRUE), nrow = 6) df &lt;- data.frame(df, stringsAsFactors = FALSE) df[which(df == "yes", arr.ind = TRUE)] &lt;- 1 df[which(df == "no", arr.ind = TRUE)] &lt;- 0 df 
There are some good answers here, but if this is a one-off edit you might be better off just using excel or some other text editor. Using find and replace would be quick and easy.
So if I understand your question correctly: 1) You want to change "Yes" to 1 and "No" to 0 2) You want to do this for multiple columns 3) You want to do this by utilizing the column index and not the column name We can break down each requirement into individual code pieces. 1) change Yes to 1 and No to 0 for 1 specified column: df$someCol &lt;- ifelse(df$someCol == "Yes", 1, 0) 2) Instead of using the column name (someCol), we want to use the column index (say, 5) df[ , 5] &lt;- ifelse(df[[5]] == "Yes", 1, 0) 3) Carry out the operation for multiple indexes: 1 way is to use a loop... Indexes on which to apply the logic desiredIndexes &lt;- c(1, 3, 5, 6, 12) for(i in desiredIndexes) { df[, i] &lt;- ifelse(df[[i]] == "Yes", 1, 0) } Another way is to use lapply: df[ , desiredIndexes] &lt;- lapply(df[ , desiredIndexes] , function(x) ifelse(x == "Yes", 1, 0))
That's what I was looking for thanks 
no need to use ifelse, just use booleans (or go via booleans) df[ , desiredIndexes] &lt;- as.integer(df[ , desiredIndexes] == "Yes") Since booleans are interpreted as 1 and 0 in match contexts, you can probably also leave the as.integer away.
I think you can skip the 'which' part and use the logical vector directly, but you will end up with "1" and "0" (i.e. characters not integers) the way you do it.
It's literally 1 or 2 lines of code, why should find and replace be much easier/quicker. (Ignoring the terrible side affects it can have, like changing "not" in other columns to "0t" ...)
True. This is a much simpler solution. :)
Right you are about the which()! Although if there are NAs anywhere, there might be problem. So I guess finish with as.integer(df). Huh. Whole thing is clunky anyways. 
What about using dplyr mutate_all with an ifelse?
It seems most of the answers here solve your problem, but if you wanted to just globally change all your Yes and No's to 1's and 0's respectively, you can just lapply and gsub. Something like what is below just repeat for No and 0. df &lt;- as.data.frame(lapply(df, function(x) gsub(pattern = "Yes","1",x)))
 library(stringr) has_yes &lt;- str_detect(list_of_files, "YES") batch_1 &lt;- list_of_files[has_yes] batch_2 &lt;- list_of_files[!has_yes]
&gt; library(stringr) &gt; has_yes &lt;- str_detect(list_of_files, "YES") &gt; batch_1 &lt;- list_of_files[has_yes] &gt; batch_2 &lt;- list_of_files[!has_yes] This applies to the file name, right?
Yes, sorry I didn't explain it more. I assume you have your filenames in some variable (ie `list_of_files`). If you don't have your filenames yet you can get them with `list_of_files &lt;- list.files("/path/to/folder")`. stringr is a library for manipulating strings and using regular expressions (install with `install.packages()`). Read up on regular expressions on google if needed.
Thanks!
PS: Regular expressions can turn ugly and complicated quickly. Piece of advice: Don't spend more than a few hours (for now) trying to understand them.
Apologies from my end. You have made a good point about have the sidebar having a place to ask a good question. After all the comments, I could have explained my question in much better detail ... without confusion. 
https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example 
https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example No one can help you if you don't show any code and people won't really want to help. Can you either show us your file or create a fake dataset with the same problem? What does calling `sessionInfo()` say your versions of R and readxl are?
u/Negotiator1226 Thank you for posting this. 
And, I can read code if you were to write it. I just have no idea what R is capable of and need to know if this is a viable solution for animating the data I have (and potentially expounding on that animation). 
A few hours?! That'll only get you to figure out if you want Grep grepl grex etc. god they're awful 😭
I'll email you
I can't say if this is exactly what you are looking for, but assuming accounts_retweeted to be the names of the retweeted Twitter accounts, and times_retweeted to be number of retweets (a random sample in the below example), you could use something like this: accounts_retweeted&lt;-c("A","B","C","D","E") times_retweeted&lt;-sample(1:20, 5) barplot(times_retweeted, main="Twitter Accounts", xlab="Accounts Retweeted", names.arg=accounts_retweeted)
 if(sum(is.na(df[1,]))==length(df[1,]))
Thanks! 
A little more readable: all(is.na(df[1,]))
Thanks for writing a reproducible example with code and data. Just want to check, is this how you're calling `affinity`? `affinity(orderuniverse, item_brand_code, order_number)` Your problem is not in `filter` but I just want to check this is how you're calling it first.
So I figured it out this afternoon. The problem was the unique(odnumber). I switched this to unique(OrderNumber) based on some stack exchange advice and it worked. The call is brandcode_affinity(data_line,data_line$ItemBrandCode,data_line$OrderNumber) I could not get it to work without the data_lines$ but I am not sure why. 
It seems like you have read the [dplyr programming vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html). However, it gets into pretty advanced concepts and it seems like you are a relative beginner to R. So, first of all, note that in your function you use `unique(odnumber)`. But `odnumber` is not a quosure like in the dplyr programming vignette. Instead it is the object itself that you passed to the function. So it is the object `order_number` that is getting passed to `unique`. Your function would give you the exact same result if you called it with `affinity(orderuniverse, item_brand_code, c("1","1","1","2","2","2","3","3","3","4","4","4"))` since that vector is just the original object. So, when you select the unique order numbers you are getting 1 through 4, not 1 through 3. Also note that what you want is the first argument to be the data frame and the second argument and third arguments to be bare column names. However, your function is partly treating them as the object itself. This causes a problem because, for example, if you ran `rm(item_brand_code, order_number)` before calling `affinity` then you would get an error because those two objects are missing. If you made sure to quote and unquote everything in the function then you would not get an error. Another thing to note is that you can clean up your original code with: orderuniverse %&gt;% group_by(order_number) %&gt;% summarize(item_brand_code = list(item_brand_code)) %&gt;% filter(map_lgl(item_brand_code, ~ "ab" %in% .)) %&gt;% unnest() %&gt;% group_by(item_brand_code) %&gt;% summarize(brandcode_count = n()) %&gt;% arrange(desc(brandcode_count)) Then your function would become: affinity &lt;- function(dt, bcode, odnumber) { quo_bcode &lt;- enquo(bcode) quo_odnumber &lt;- enquo(odnumber) name_bcode &lt;- quo_name(quo_bcode) orderuniverse %&gt;% group_by(!!quo_odnumber) %&gt;% summarize(!!name_bcode := list(!!quo_bcode)) %&gt;% filter(map_lgl(!!quo_bcode, ~ "ab" %in% .)) %&gt;% unnest() %&gt;% group_by(!!quo_bcode) %&gt;% summarize(brandcode_count = n()) %&gt;% arrange(desc(brandcode_count)) } Also, make sure to use clear coding style with spaces and tabs like in my example.
Just want to note this isn't a very robust solution. Let's say you changed the name of OrderNumber in your data frame with data_line &lt;- rename(data_line, order_number = OrderNumber) Then calling brandcode_affinity(data_line,data_line$ItemBrandCode,data_line$order_number) I am guessing this will cause an error unless you jumped into the function and changed the code, which is never a good idea.
Thanks for sharing. I will put this into practice 
If you're only using R then maybe. Otherwise I'd say no, I'm too attached to vim-slime with tmux.
If you're saving dataframes in a binary format, then have a look at [feather](https://blog.rstudio.com/2016/03/29/feather/). Same idea, only they can be used with R, python and Julia.
I work with large multidimensional arrays a lot, and RDS format is perfect for that. It's quick and the file size is reasonable. 
Hadley strikes again. I'll take a look at this, thanks for posting. This will be very useful for my Python setup at least.
Just note that RStudio is not the only R IDE out there. I personally prefer RKWard.
Off the top of your head, would you know if the speed for reading and writing using readRDS() and loadRDS() is faster than the save() and load() functions - if it is, by how much?
I also use R with vim. Any good R autoformat or linters out there?
If you run ?saveRDS or help(saveRDS) it has this in the help file: &gt; These functions provide the means to save a single R object to a connection (typically a file) and to restore the object, quite possibly under a different name. This differs from save and load, which save and restore one or more named objects into an environment. They are widely used by R itself, for example to store metadata for a package and to store the help.search databases: the ".rds" file extension is most often used. save/load don't do the same thing as saveRDS and readRDS. save/load save an entire environment while readRDS and saveRDS save a single file to be loaded into an environment.
I use Sublime and R, can take some tweaking to get it working. I really can't use RStudio
First time hearing of this! How does sf compare to sp in general? 
What do you use to inspect/view datasets? That's the feature I really like RStudio for, otherwise keen on switching to vim myself...
You might as well try it. It's a really nice IDE, and much simpler than stuff like Eclipse, so there isn't much to learn.
Not op. But I save the data in CSV and open it in libreoffice.
Sublime works well.. The Repl has some quirks though...
You're not wrong it's odd to setup
What about seeing data in your global environment? That's one thing I feel like I would miss because I tend to lose track of that.
Idk if this is possible with vim... It would be nice, tho
Nvim-R allows this although I personally don't use it. Running `ls()`, and inspecting the data in the terminal, has always been enough for me (you should never have too many symbols defined at any given time during execution anyway).
Yeah must have forgotten about that, got so used to the view in RStudio. Would you mind expanding on why it's not good to have many symbols defined? Especially when working on a script over and over I often end up having quite a large number defined.
Just head() for column structure and summary statistics or unique() for other things. Don't find myself needing to be able to scroll through it all.
Assuming you have the filename as "test.csv", I would use the "dplyr" package and the "zoo" package. "dplyr" helps you piping the operation, though not necessary for a short code like this. "zoo" package contains the **na.locf()** function, which replaces NA values with previous [ fromLast = FALSE ], or next value [ fromLast = TRUE ]. &gt; require(dplyr) &gt; require(zoo) &gt; &gt; df1 &lt;- read.csv(file = "./test.csv", header = T, na.strings = "") &gt; df1$x2 &lt;- na.locf(df1$x2, fromLast = FALSE) &gt; colnames(df1) &lt;- c("Subject", "Room", "Start Time", "Duration") &gt; df1 &lt;- filter(df1, Subject != "Room Name" &amp; Subject != "Subject") &gt; df1 &lt;- df1 %&gt;% select("Room", "Subject", "Start Time", "Duration") &gt; df1 Room Subject Start Time Duration 1 ConfRmA Dept A 7/10/2017 07:30 02:30 2 ConfRmA Dept B 7/10/2017 12:30 01:30 3 ConfRmB Dept B 7/11/2017 08:00 01:00 4 ConfRmB Dept C 7/11/2017 09:30 03:30
Thank you! Looks like zoo was my missing piece.
It’s just a consequence of fundamental software engineering principles such as [reducing moving parts](https://en.wikipedia.org/wiki/Moving_parts), [loose coupling](https://en.wikipedia.org/wiki/Loose_coupling) (= [small interfaces](https://en.wikipedia.org/wiki/Interface_segregation_principle); global variables are the opposite of that) and [minimising variable scope](http://wiki.c2.com/?ReduceScopeOfVariable). A complex system is harder to reason about; systems with fewer parts are less complex; thus they are more robust and easier to understand.
In Nvim-R if you have the CSV plugin it'll open a buffer with the data quasi-formatted if you hit &lt;localleader&gt;rv
na.locf is probably better, but here is a truly grotesque data.table solution as well. It basically creates a row numbering, then joins on that row number and rolls the conference room forward during the join. Then cleans it up. library(data.table) dt &lt;- data.table(x1,x2,x3,x4) setnames(dt, c("Subject", "Room", "Start Time", "Duration")) dt &lt;- dt[, ID := .I][, Room := dt[Room != "", .(ID, Room)][dt, Room, roll = T, on = "ID"]] dt &lt;- dt[!(Subject %in% c("Room Name", "Subject"))][, ID := NULL] 
Yeah RDS is great. I use it to bridge data pull/model fitting/results scripts, so at each point you have a tidy rds file with all the relevant data/metadata for the next step. For example, when I am model fitting I will form the output into a list something like this: list(dt = data, mod = model, iv = indep_var, dv = dep_var, type = class(model), version = "1.5 (final)", createtime = Sys.time(), note = "Final Revision of GLM") Then I write the "results" script in a generic way to accept lists structured like this. It makes working with different iterations of the same model at the same time much easier. 
I appreciate having options. I already got na.locf working, but I had to install the zoo package. Your solution would have worked with my existing packages.
Why not just a for loop? Result would be more readable than the other suggestions here IMO.
That's what I came for assistance with. In TSQL or PL/SQL I'd have done a for loop and called it a day. However, I'm still new to R and the syntax was biting me. All the docs I read have an overly simple example then explained in detail why something from the apply family was better. 
Functional approaches like 'apply' make sense when each row can be operated on independently. However, in your case, the processing for each row is dependent on the header row that came before it. So this is really a case where a state machine (via a for loop, and some variables like `current.room`) makes the most sense. I've read recently that the performance differences between `apply` and `for` are really overstated (maybe it used to be worse?) and so people tend to over-prescribe `apply` by default.
There is also the lag() function in dplyr that uses the last row. 
`apply` is just a loop. If you write your loops in a sensible way (not appending on each iteration), then the performance between the two should be pretty much the same.
Specify a mirror in your options. options(repos = "https://cran.rstudio.org") Changing it to whatever mirror you want. 
I've wondered what the real performance difference is. When I read blog post about, "for loops bad, use apply," they tend to come off a little bit smug. "Look, instead of 10 lines of easy to read code, I have a one liner that makes absolutely no sense to whomever didn't write it."
That's the repository, not the mirror... Or am i wrong? With R Studio i choose repository (cran or github or whatever) and a mirror (country where the mirror is stored).
Yeah - often people who are new to programming don't understand that the best code is code that is easy to read and that you don't get any bonus points for being overly clever/complicated.
I don't have time to review your code right now and so I'm not sure what exactly the speed bottleneck is, but you may be interested in the [data.table::foverlaps](https://www.rdocumentation.org/packages/data.table/versions/1.10.4/topics/foverlaps) function, which seems to do the same thing as your findOverlaps function. [This article](https://www.r-bloggers.com/comparing-the-execution-time-between-foverlaps-and-findoverlaps-data-table-vs-genomicranges/) suggests it's only more efficient for small datasets, but that was posted 2 years ago and judging by the discussion in the comments [here](http://zvfak.blogspot.com/2015/04/comparing-execution-time-between.html) the authors have done some optimizations since then. [Another comparison](https://github.com/tdhock/datatable-foverlaps).
I did realize that, apparently data.table::foverlaps is based on the IRanges:findOverlaps implementation. I'm sure the IRanges is plenty fast but the data.table authors also have a pretty good reputation for speed, so figured it might be worth it to just swap it out and see if it's any faster. Have you tried line profiling the code to see what the largest time sink is?
From ?options repos: URLs of the repositories for use by update.packages. Defaults to c(CRAN="@CRAN@"), a value that causes some utilities to prompt for a CRAN mirror. To avoid this do set the CRAN mirror, by something like local({r &lt;- getOption("repos"); r["CRAN"] &lt;- "http://my.local.cran"; options(repos = r)}). Note that you can add more repositories (Bioconductor, R-Forge, Rforge.net ...) using setRepositories. You can set your options for different repositories but by default you're just setting the mirror for CRAN. If it makes you feel better you can be more explicit and use `options(repos = c(CRAN = "https://cran.rstudio.org"))` (or whichever mirror you want... Dig into the documentation to learn more about it but this should get you what you need. Give it a try.
Interested to see this run vim inside r studio
This is great news for my research, as I frequently run calculations on our distributed grid computation nodes. Starting a RStudio on them and have an X11 window forwarded to my local workstation is at best inefficient. So I tend to have a script in some general editor and an interactive R session in terminal. **But not any longer!** (Once the script is developed and debugged, they run as batch jobs.)
Are you using base graphics or some other package (like ggplot2)?
Been waiting for [themes](http://imgur.com/a/CM4mb) for so long!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/aqj3KJf.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dliacs5) 
I actually got help elsewhere and got it done, at least in part, with the library "lattice" and function parallelplot(). However, this was just intended to be the first step in a big multiple regression analysis which has utterly fallen through since I can't manage to get r to do linear regression on each line individually (trying to compare each individually to a sample line of slope -1). Thanks anyway!
&gt; However, this was just intended to be the first step in a big multiple regression analysis which has utterly fallen through since I can't manage to get r to do linear regression on each line individually (trying to compare each individually to a sample line of slope -1). That's doable. Using the iris example dataset, if we wanted to regress sepal.length onto each column individually, we could do so using individual_results &lt;- lapply(iris[,2:4], function(data_column){ column_result &lt;- lm(iris$Sepal.Length ~ data_column) return(column_result) }) where individual_results is a list containing the list output by the lm object. You can then extract whatever you need from there. This is not the only way of doing it, but this is a simple way of it.
Can you import from SQL and do the aggregation in dplyr?
So, firstly I would strongly recommend you use `readr::read_csv` over `read.csv` - it's better in practically every way. Secondly, I'd just like to understand what the "Excel file" you mentioned is - in this case you're talking about a CSV file, right? Just so you're aware, CSV isn't typically called an "Excel file" because it's just a text file (open it in Notepad and take a look) with [a format suggested by the Internet Society](https://tools.ietf.org/html/rfc4180). If you use "Excel file" to describe a CSV file without explaining it, you'll find that people will be likely to get confused. Third, your code is a bit confusing. You don't need the `c()` around `'ETHICS.CODE.URL'` - in fact in the case of selecting one column, you might prefer the simpler syntax `CSV1$ETHICS.CODE.URL`. Secondly, I don't really understand the point of creating CSV2 - if you wanted a data frame returned, you could have used the syntax `['ETHICS.CODE.URL']` instead of `[,'ETHICS.CODE.URL']`. I will admit that this is confusing! It's a reason to prefer `readr::read_csv`, because that function returns a "tibble", which is a special kind of data frame that, among other things, makes that syntax more consistent. Finally, to your question about downloading things. There are three basic methods that I use for downloading from the internet - the `utils::download.file`, `RCurl::getBinaryURL` (which is a wrapper for `RCurl::getURL` with good defaults for downloading binary files, which PDFs are), and `httr::GET`. Of these, honestly `utils::download.file` is the easiest to get started with, but I wanted you to be aware of the other options in case you start doing more things with, for example, APIs. So to run through your list of PDFs and download them, you just need a function that does that, and to apply that function to the list of URLs. Hopefully this sort of construct is familiar to you: download_a_file &lt;- function(url) {...} lapply(CSV1$ETHICS.CODE.URL, download_a_file) That code loops through the list of URLs that's returned by `CSV1$ETHICS.CODE.URL`, running `download_a_file` on each list element. Now we just need a function to download a file. This function is very simple because we just need to call `download.file` and give it the URL and a name to save to. There is a handy function in R called `base::basename` that returns a file name when given a full file path. So the final code can be: download_a_file &lt;- function(url) download.file(url = url, destfile = basename(url)) lapply(CSV1$ETHICS.CODE.URL, download_a_file) And that should be it.
I'm not the OP but thanks so much for the clear explanation. What about the last step to parse PDFs into text or some other readable format?
If they already have text data embedded, you want the [pdftools](https://github.com/ropensci/pdftools#readme) package. If not, you need to OCR them first with something like [tesseract](https://github.com/ropensci/tesseract#readme).
Their "pause membership" option is nice. So I don't have that "unused gym membership" feeling going on. I'll probably reactivate my membership for a few classes soon, and I won't have any resentment stored up. 
Ha! It does look that way. I'm glad I looked at this post if just for /u/Campezi's teaching me about the pause membership option. I'm about 5 weeks in and have blown through most of the Data Scientist career track, but I had some experience with R. What is it about the course structure you don't like? I've been a bit annoyed by different syntaxes causing my answers to be incorrect, but that hasn't been a huge problem. My main issue with Datacamp (and Codecademy) is that there's a bit too much handholding. It's probably a positive feature for anyone totally new. Having said that - as someone who was already familiar with the R basics, I've learned a lot of tricks going through the courses that are easy to apply because I already have the background. I feel like I'm getting my money's worth.
Okay :-) Thanks, i will give it a try.
It's been the opposite for me - I'm a total newbie, and there's often a piece of information missed that means I can't answer without seeing it first. Also there's not so much context - you can use this to do this - rather, here's this function you can perform, when you use it, who knows? I'd wish for more context. I think what sums it up is every time you finish an exercise, you can click 'submit' or use the shortcut (cmd + shift + enter). If you choose to click, it always shows a condescending message saying 'use the shortcut' - sometimes I don't have two hands because I'm writing in a book!
Your question is quite difficult to answer when it's missing so much information, which makes it very hard to help you. A well-asked question consists of three parts: 1. The smallest possible piece of example code which, when I run it in my console - that is to say, in a fresh R session - will produce exactly the behaviour that you are experiencing. 1. An explanation of what you expected to happen when you ran that code and why that behaviour is important to you. 1. An explanation of what actually happened and why it doesn't work for your purposes. Your question doesn't have any of this. Much of your code is missing (eg the getPage function is from the Rfacebook package but the package is never loaded, and the dates data structure and the variable n are never defined) so I can't see precisely what is happening. Your question mentions slides, printing, and an "R presentation file" but none of that is in the code you gave. My assumption is that the only thing you're asking about is how to drop a column from the df_daily data structure, but without understanding your problem and your use case, I can't give you good advice about what to do.
I took your advice and updated my post, i hope that makes things a bit more clear and if not then it looks like I won't be able to get help, if you intended on running the code you will need to get an auth code from here and put it in the token variable (hopefully you have a Facebook account to do that with) https://developers.facebook.com/tools/explorer/ 
Taking a look now - you should remove your tokens from your script, they're supposed to be private information.
I answered your question on Stack, hope that helps. It was still quite tricky to answer your question because your example needs to be minimal. Here is how I would have asked this question minimally: &gt; When I run `Rfacebook::getPage()`, a message is printed to the console: &gt; data &lt;- Rfacebook::getPage("BuzzFeed", "SECRET TOKEN GOES HERE") 25 posts &gt; How do I stop this?
Okay thanks for your help and that token is expired so its no use to anyone, with Facebook the token times out, that's the one i used yesterday but either way I'll remove it just in case.
I don't suppose you could help with this as well? similar situation. when i run TwitteR::setup_twitter_oauth(), a message gets printed to the console. setup_twitter_oauth(key,secret,accessToken,accessSecret) [1] "Using direct authentication" 
Yep, this package is another guilty party of using the `print` function in package functions (you can download the package source from CRAN and take a look to see if this is what it's doing - it's line 19 of oauth.R in the package that's the problem). It's really bad practice for packages to use `print` for this exact reason. There's no way to turn it off without completely disabling all printing by the function. It's the reason why the condition system exists, which is what `message`, `warning` and `stop` use, and it's extensible. We were lucky that the rFacebook package includes a switch to turn it off, but this package doesn't seem to. The only thing you can do is wrap the function in `invisible(capture.output(...))` and then it will stop printing. Also, the twitteR package is really old - last updated in 2015 - and has no github. So it doesn't seem worth it to tell the author to fix their shit, as it was with rFacebook. There is a newer package called rTweet, which I'm not familiar with but you might find works better because it's being actively worked on.
From a user perspective, I find it easier to work with. Simply print(spObject) returns a bunch of nested lists which are unintuitive, but print(sfObject) returns a data.frame which even a novice can understand. Everything about sf objects is more intuitive. There are internal benefits as well (e.g. increased speed and native support via ggplot2's geom_sf() for plotting.)
That seems pretty useful actually, especially since I'm switching between visio-spatial and geospatial analysis quite frequently (using data frames).
Not sure if this would work but you could try using locator( ) to store the location of a mouse click and then apply shading to the wells that are in the grid location where a click has been performed.
/u/deanat78 made a shiny app that does very nearly exactly what you're asking. He posted a demo, and it has a github repo. https://daattali.com/shiny/lightsout/
that would be perfect, but I already have a working GUI using gwidgets and it would be better if I didn't have to make it web based as the analysis done by the program is quite involved. Currently I just use checkbox groups, but it doesn't look great and takes up way to much space
I don't think I can get the result I want using this method, but thanks for the reply
If working template of exactly what you asked for isn't enough, you're probably on your own from here.
I guess my reddit anonymity is gone, that's scary :) I do have something closer to what OP wants -- a GUI for selecting wells from a 96 well plate -- though it's less pretty. Go to https://daattali.com/shiny/ddpcr/ go to the "Use sample dataset" tab, load one, then in the next tab go to "subset plate" 
I think (with a bit of bias, but still) it's a good idea to learn shiny and migrate to it
You said you already have a working solution with checkbox groups but you don't like it/takes up too much space. Using CSS might solve that problem by reducing the spacing
yes, this is perfect I suppose it is best to switch to shiny if there is no other way to accomplish the same thing in gwidgets thx
Make a data frame with the name and other variables/observations you'd like to join. Then use merge() or one of the join functions from dplyr (e.g. left_join()) to join by name. Then reorder the columns and you're good to go. 
You need to be a little more clear about what you're asking and formatting of the data. From what I can guess: x &lt;- c (0,10,20,30,40,50) y &lt;- c(49.8657, 27.0679, 43.2998, 58.5062, 55.5389, 59.064) df &lt;- data.frame (x,y) plot (df) or library (ggplot) df %&gt;% ggplot (aes (x = x, y = y)) + geom_line() + geom_point() Depending on what you want the final output to be. Read up on ggplot to customise the chart. 
Or just cbind() it... no need to get fancy if you don't have to.
It’s a shame that the `tryCatch` documentation fails to include a salient example for this use-case. Here it is: tryCatch( command1, error = function (e) command2 ) Inside the error handler, `e` contains an object that describes the error. If the commands are made up of several commands you need to group them into braced groups as usual (`{ … }`).
Thanks! It worked
Ex: Length: 45, 23, 64.2, 9, 47.29 Sp. Present: "not present", "present", "present", "not present", "present 
That is definitely slow, you want something like this: aa_props &lt;- c("A"=1.8, "Y"=500, "C"=1.2) NumeratedPeptides &lt;- lapply(LetteredPeptides, function (x) aa_props[x]) That does all 20 at once uses a map instead of string comparisons and lapply is faster then sapply (you can merge the list into a matrix if you need to with do.call and rbind or cbind).
Use library stringr where you split each element to a vector of 9 1 charater elements. This could even be extended as a matrix, `m`. Your replacements is a named vector, i.e `lookup &lt;- c(A=2, B=5, C=1, ...)`. You can then simply use it as a lookup: `lookup[m]`.
How have you loaded the data? How many measurements do you have per animal present, or per animal not present? Are there missing data? What have you tried? Are using `plot`?
Thank you, I did not know lapply was faster. Sapply it's the only one I was taught so it's the one I use. I will try your way and rerun it.
No need for stringr, the builtin `strsplit` can do that. It returns a list rather than a matrix but that's easily transformed via `do.call(cbind, …)`.
wow, great stuff! I love building shiny apps. 
I would combine them into a dataframe and then make sure your "present" vector is a factor. From there are multiple ways to average and plot
lapply and sapply are practically the same (look up the documentation .. they are both wrappers to `apply`). The reason that this solution is faster is that instead of doing regex on each element, it just looks up which number matches each letter. This can be vectorized, and in fact you don't even need any apply function: # define the translation aa_props &lt;- c("A"=1.8, "Y"=500, "C"=1.2) # for each element in LetteredPeptides .. # select the element from aa_props that matches the name NumeratedPeptides &lt;- aa_props[LetteredPeptides]
You want to look into summarizing your data into a 2x1 data frame where row 1 is present and its average and row two is absent and it's average. Then use geom bar in ggplot. What have you tried?
This should get you [pretty close.](http://imgur.com/a/vcTjz) The legends would be a bit more complicated, but some googling will probably get you there. library(ggplot2) library(ggmap) library(maps) library(mapdata) library(ggthemes) library(ggrepel) world &lt;- map_data('world') cities &lt;- data.frame(city = c("Calgary", "Seattle", "San Francisco", "Detroit", "Houston", "Mexico City", "Caracas"), stringsAsFactors = F) cities$value &lt;- rnorm(nrow(cities)) * 20 cities$value[1] &lt;- 0 cities[,3:4] &lt;- geocode(cities$city) cities$change &lt;- sapply(as.character(sign(cities$value)), switch, "1" = "Increase", "0" = "No change", "-1" = "Decrease") cities$change &lt;- factor(cities$change, levels = c("Increase", "No change", "Decrease")) p &lt;- ggplot() + geom_polygon(data = world, aes(x = long, y = lat, group = group), fill = "light grey") + geom_point(data = cities, aes(x = lon, y = lat, color = change, size = abs(value)), alpha = .4) + geom_text_repel(data = cities, aes(x = lon, y = lat, label = city, color = change), show.legend = F) + theme_map() + scale_color_manual(values = c("dodgerblue3", "black", "brown2")) + coord_fixed(1.3) + theme(legend.position = 'top') + guides(color = guide_legend(title = element_blank(), order = 1), size = guide_legend(title = 'Change in liveability score, 2007-2017', title.position = 'top', order = 2, label.position = 'bottom', label.hjust = .5, override.aes = list(color = 'lightblue4', alpha = 1))) + ggtitle(label = "Change in liveability score", subtitle = "2007-2017") Edit: I did the wrong plot from your link. Oh well.
Without a reproducible example for the data I can only give you the general direction: ggplot(x) + geom_point + geom_line + facet_wrap(~"column with up down classifiers") The middle would have 2 points (before/after) as well. If you really want only one dot you have to graph it separately AFAIK and then combine it later.
Wrong plot or not, that's a great one. Good work, thanks.
This is as close as I feel like getting right now. It's ugly, terrible code. It would be a pain in the ass to do some of this programmatically (although whether it's less of a pain in the ass to do it manually for a large dataset is debatable), but this at least approaches the appearance. [The picture.](http://imgur.com/a/SHPGB) Edit: fixed a couple things. library(tidyr) library(dplyr) library(ggplot2) library(ggrepel) library(cowplot) library(grid) library(extrafont) # if you want extra fonts - could take a while # if not, change the FONT variable to a family you do have font_import() loadfonts() FONT &lt;- "Arial Narrow" # Define a probable data set cities &lt;- data.frame(city = c("Toronto,\nCanada", "Auckland,\nNew Zealand", "Taipei,\nTaiwan", "Tripoli,\nLibya", "New Delhi,\nIndia", "Pretoria,\nSouth Africa", "Calgary,\nCanada", "Washington\nDC, US", "Hanoi,\nVietnam", "Manila,\nPhilippines", "Detroit,\nUS", "Seattle,\nUS", "Vancouver,\nCanada", "New York,\nUS", "Moscow,\nRussia", "Hong Kong", "Oslo,\nNorway", "Paris,\nFrance"), start = c(98, 93, 82, 40, 56, 70, 95, 76, 80, 55, 45, 89, 98, 93, 82, 40, 56, 70), end = c(99, 95, 90, 45, 67, 74, 95, 76, 80, 55, 45, 89, 90, 85, 81, 34, 50, 59), stringsAsFactors = F) # Calculate difference between start and end, and use that to assign the "status" cities &lt;- mutate(cities, difference = end - start) cities$status &lt;- sapply(as.character(sign(cities$difference)), switch, "1" = "More liveable", "0" = "No change", "-1" = "Less liveable") # Randomly select approximately 50% of cities in each # section to show bold and with the labels present. Also choose randomly which # side of the column the labels will appear. fractionToFocus &lt;- 0.5 cities &lt;- cities %&gt;% group_by(status) %&gt;% sample_frac(fractionToFocus) %&gt;% ungroup() %&gt;% mutate(focus = T, side = sample(c("left", "right"), size = nrow(.), replace = T), label = city) %&gt;% full_join(cities) %&gt;% gather(key = Year, value = Score, start, end) cities$status &lt;- factor(cities$status, levels = c("More liveable", "No change", "Less liveable")) cities$focus &lt;- ifelse(is.na(cities$focus), F, T) cities$focus &lt;- factor(cities$focus, levels = c(T, F), labels = c(T, F)) cities$Year &lt;- ifelse(cities$Year == 'start', 2007, 2017) FONTSIZE = 11 LINESIZE = 1 POINTSIZE = 2 ALPHA = .4 p1.dat &lt;- filter(cities, status == "More liveable") COLOR &lt;- "dodgerblue3" p1 &lt;- ggplot(p1.dat, aes(x = Year, y = Score, alpha = focus)) + theme_gray(base_size = FONTSIZE, base_family = FONT) + geom_rect(ymin = -Inf, ymax = Inf, xmin = 2007, xmax = 2017, fill = 'white') + geom_rect(ymin = 100, ymax = Inf, xmin = -Inf, xmax = Inf, fill = 'white') + geom_point(color = COLOR, size = POINTSIZE) + geom_line(aes(group = city), color = COLOR, size = LINESIZE) + facet_grid(.~status, scales = "free_x") + scale_alpha_manual(values = c(1, ALPHA) ) + geom_text(data = filter(p1.dat, side == 'left' &amp; Year == '2007'), aes(label = label), color = "black", nudge_x = -5, fontface = "bold", size = 3, family = FONT) + geom_text(data = filter(p1.dat, side == 'right' &amp; Year == '2017'), aes(label = label), color = "black", nudge_x = 5, fontface = "bold", size = 3, family = FONT) + theme(legend.position = 'none', strip.background =element_rect(fill = COLOR), strip.text = element_text(color = 'white', face = 'bold', family = FONT), strip.placement = 'outside', panel.background = element_rect(color = NA, fill = alpha('aliceblue', .6)), panel.grid.minor = element_blank(), panel.grid.major = element_line(colour = 'grey'), axis.title = element_blank(), axis.text = element_text(face = 'bold', family = FONT)) + scale_y_continuous(limits = c(30, 100), breaks = seq(30, 100, 10), expand = c(.01,.01)) + scale_x_continuous(limits = c(1997, 2027), breaks = c(2007, 2017)) p2.dat &lt;- filter(cities, status == "No change" &amp; Year == 2017) COLOR &lt;- "black" p2 &lt;- ggplot(p2.dat, aes(x = Year, y = Score, alpha = focus)) + theme_gray(base_size = FONTSIZE, base_family = FONT) + geom_rect(ymin = 100, ymax = Inf, xmin = -Inf, xmax = Inf, fill = 'white') + geom_point(color = COLOR, size = POINTSIZE) + geom_line(aes(group = city), color = COLOR, size = LINESIZE) + facet_grid(.~status, scales = "free_x") + scale_alpha_manual(values = c(1, ALPHA) ) + geom_text(data = filter(p2.dat, side == 'left'), aes(label = label), color = "black", nudge_x = -5, fontface = "bold", size = 3, family = FONT) + geom_text(data = filter(p2.dat, side == 'right'), aes(label = label), color = "black", nudge_x = 5, fontface = "bold", size = 3, family = FONT) + theme(legend.position = 'none', strip.background =element_rect(fill = alpha(COLOR, .6)), strip.text = element_text(color = 'white', face = 'bold', family = FONT), strip.placement = 'outside', panel.background = element_rect(color = NA, fill = alpha('aliceblue', .6)), panel.grid.minor = element_blank(), panel.grid.major = element_line(colour = 'grey'), axis.title = element_blank(), axis.text.y = element_blank(), axis.text.x = element_text(face = 'bold', family = FONT), axis.ticks.y = element_blank()) + scale_y_continuous(limits = c(30, 100), breaks = seq(30, 100, 10), expand = c(.01,.01)) + scale_x_continuous(limits = c(2007, 2027), breaks = 2017) p3.dat &lt;- filter(cities, status == "Less liveable") COLOR &lt;- "brown2" p3 &lt;- ggplot(p3.dat, aes(x = Year, y = Score, alpha = focus)) + theme_gray(base_size = FONTSIZE, base_family = FONT) + geom_rect(ymin = -Inf, ymax = Inf, xmin = 2007, xmax = 2017, fill = 'white') + geom_rect(ymin = 100, ymax = Inf, xmin = -Inf, xmax = Inf, fill = 'white') + geom_point(color = COLOR, size = POINTSIZE) + geom_line(aes(group = city), color = COLOR, size = LINESIZE) + facet_grid(.~status, scales = "free_x") + scale_alpha_manual(values = c(1, ALPHA) ) + geom_text(data = filter(p3.dat, side == 'left' &amp; Year == 2007), aes(label = label), color = "black", nudge_x = -5, fontface = "bold", size = 3, family = FONT) + geom_text(data = filter(p3.dat, side == 'right' &amp; Year == 2017), aes(label = label), color = "black", nudge_x = 5, fontface = "bold", size = 3, family = FONT) + theme(legend.position = 'none', strip.background =element_rect(fill = COLOR), strip.text = element_text(color = 'white', face = 'bold', family = FONT), strip.placement = 'outside', panel.background = element_rect(color = NA, fill = alpha('aliceblue', .6)), panel.grid.minor = element_blank(), panel.grid.major = element_line(colour = 'grey'), axis.title = element_blank(), axis.text = element_text(face = 'bold', family = FONT)) + scale_y_continuous(limits = c(30, 100), breaks = seq(30, 100, 10), position = 'right', expand = c(.01,.01)) + scale_x_continuous(limits = c(1997, 2027), breaks = c(2007, 2017)) p &lt;- ggdraw() + draw_plot(p1, 0, 0, .35, 1) + draw_plot(p2, .4, 0, .2, 1) + draw_plot(p3, .65, 0, .35, 1) 
Seems like a useful tool for R users:)
Nice idea. I made a much more simple version a while ago in bash. No fancy checking for package updates etc. $PACKAGE=$1 R -e "install.packages('$PACKAGE', repos='http://cran.us.r-project.org')"
You can have the scales vary between facets with the `scales` argument, so adding `scales = "free_x"` would allow the middle facet to only have one point. I started doing it that way before realizing it would be difficult to change the color of the facet label strip at the top (it would require altering the plot's grob, which feels slightly more hacky than what I ended up doing, but only very slightly). 