Hmmm. Guess this is a little late. Seems you should think about some statistical analyses for readily available data in your subset of biology.
Let's see I have used it for QTL analyses, GLMMs, ML parameter estimation, Mixture Models, every multivariate method out there, microarray analyses, and ALL graphing. Ggplot is the shit!
It's no longer there, did you quit or something?
Blog not found. 
What is your field of research. Personally, I'd suggest perl over python.. But I'm biased. I use perl, bash, and R pretty much everyday. I'm in plant evolutionary genetics and do a lot of bioinformatics. 
I'm running 64 bit win7 enterprise. I set the memory for R, using this in the target field of the shortcut --max-mem-size=8183M I really don't understand how it can struggle with such a small dataset!
Thanks a lot for trying to run that in your own time. I really appreciate it. I don't think I'm going to be able to get my hands on some more RAM anytime soon so I've started looking at some other methods.
No prob! Good luck!
I'm new to R also :), welcome. What exactly are you looking to get out of R, maybe I could point you in the right direction. 
Also subscribe to /r/rstats 
That link goes nowhere for me, I think it is local to your computer.
Keep in mind, == is not necessarily checking if they are identical, use identical for that. &gt; identical(TRUE,"TRUE") [1] FALSE You'd probably be best off asking the mailing list because there is no way of knowing without checking the R internals to see what == in fact actually does. It would be far faster to ask them. 
Very nice! Thanks!
sqrt(x)
It would be a little easier to understand if you explained what you are trying to accomplish, and provide something that's reproducible directly (as it stands, I'm not really sure what x$newcol x$first, etc actually are). Here it looks like you are trying to create a new string every ith iteration for each row of x, and are storing that to some object newcol in x....but I'm not really to what end. 
For loops in R is an awful place to be. Its slow and frustrating. The real power is in the apply families and things like the plyr package. 
Thank you!
I can only think of two possibilities what you are trying to do: either you simply want to wrap the for loop in a function: function_name &lt;- function(x) { for (i in 1:(nrow(x))){ x$newcol[i] &lt;- paste(x$first[i], x$lastl[i+1],sep="_") } return(x) } or you want to rewrite your for loop with the apply family which would be something like: x$newcol &lt;- sapply(1:nrow(x), function (i) paste(x$first[i], x$lastl[i+1],sep="_")) or with mapply (not really nicer because of the different indexes) x$newcol[-nrow(x)] &lt;- mapply(paste, x$first[-nrow(x)], x$lastl[-1],MoreArgs=list(sep="_")) I tested non of the code above, so I'm not sure it does the same as your for loop, and by the way x$lastl[i+1] runs out of limits and should return an error EDIT I just realized something important you seem to be missing, most functions in R can also take vectors and operate on them (this is always the fastest way), so the mapply is not needed: x$newcol[-nrow(x)] &lt;- paste( x$first[-nrow(x)], x$lastl[-1], sep="_") 
&gt; Its slow and frustrating. Not necessarily, if you predefine your result array and only fill it, for loops can be faster than sapply (lapply is generally slightly faster than for, but not very much) The important thing to learn/know is that memory operations are slow and each extension operation ( c(), cbind(),rbind(),...) results in a memory assignment. That and using functions on vectors instead of iterating over them ...
You're right. I'm wondering how for loops are supported in r parallel? I know that the apply family has some parallel counterparts but what about for? 
they aren't and there is a good reason, with for loops you can create side effects, which could lead to race conditions etc with the apply family, by definition each iteration step is computed independent of the others (this is also the main reason you should use them, you make a clearer statement of what is happening) EDIT there is the foreach package, but the name is slightly misleading, it is basically an apply function
Yeah I though I heard of that for each package. Ah yes immutability. One of the reasons I love Scala so much. :) 
With SciPy and NumPy you can actually do a lot of the same things in R that you can do in Python. R is nice because you don't have to do programming if you don't want to. Since you already know how to program, there may not be a lot of incentive to switch to R for what you are doing now. However, there are a lot of great R libraries, so you may want to check and see what's out there.
I just recently dove into Python and R in parallel. I may be wrong, but I don't think you need to "jump" into R. It's very easy and intuitive. So far I've really enjoyed using it as a complement to Python. as mr_kitty pointed out, it has incredibly easy and useful packages. ggplot2 is great and you can also find some easy-to-implement machine learning packages.
So this looks like the start of a FAQ; I just got here, but is there one for this subreddit?
As (now) noted in the stackoverflow post, the reason for this oddity has to do with R's rules for coercion of types. Basically, the boolean value TRUE gets coerced into a character vector ("TRUE") and since the two strings are identical, the comparison yields a boolean value of true.
 I think part of the reason is what you hinted at: there are many people (scientists, economists, mathematicians) that just need a powerful library focused on math-related stuff with nice scripting-like interface. They usually don't care much about general purpose libraries. So it is clean, lean and easy to understand and work with for people who just want to do stat analysis on their data. Right now making some plots or doing certain type of analysis needs much more work to do in Python than in R. However, I think in the long run as Python libraries such as Pandas become more widely-used and more mature, they will become dominant among current R users. I should also mention that there is http://rpy.sourceforge.net , which seems to allow Python and R integration. 
As R has grown people have added additional packages for even more statistics and plotting (e.g. the whole bioconductor library group). So while languages like python are starting to have good libraries with statistics, R is one step ahead and has even more statistics available through libraries. In the long run the user bases could be merged, but that isn't how it works typically, R users will keep using R and users of other languages will stick with them. While R is designed as a special-purpose language it has full general purpose capabilities. Syntactic niceties is the only thing different languages (generally) bring you. You can write a for loop with goto's (to which it is always converted in the machine code), but it is ugly and hides the important parts of your code, so (nearly all) modern languages give you the simple syntax of a for loop. In the end all languages are trade-off's between what can you do, how and in what amount of time.
I don't personally use rpy, but there is a version for python 3 under development and seems active: https://bitbucket.org/lgautier/rpy2/src?at=version_2.1.x_python3
I'm not an expert in this, but I would look at the means and standard deviations of each of the clusters for each of the variables. Then for each variable, you can look at which clusters have a greater mean difference than (roughly) pooled standard deviation. Hopefully someone has a more robust method.
Although I have not used python, I hear it is very similar to R since both are object oriented languages. R's strength is that it has many different statistical functions that are either built in or can by easily downloaded. For example, I use the "MCMCglmm" package to run Bayesian models (it's one of many Bayesian packages). If you are looking to do more graphing, I highly suggest using the "ggplot2" package. 
Thank you!
http://vis.supstat.com/2012/11/contour-plots-with-d3-and-r/ using knitr to create the HTML page http://rcharts.io/ a package which uses different javascript frameworks to create plots, I don't know how difficult it is to customize the figures if you want something specific [Basics of JavaScript and D3 for R Users](http://civilstat.com/?p=536)
there are built in functions in R for debugging, namely the debug()/browser() functions are very good. just add "if(condition) browser()" to a line in your code and it will sort of be like conditional breakpoint and it will open up a debug console. then you can eval statements in the browser. i'm not sure what "moving/evaling in calling (parent?) stack frame" is but it may be helped with these features Also, the debugging functionality in rstudio is very new so i might expect it to improve. 
Though I appreciate your response, having to alter your code to debug isn't quite the same in particular as within a debug you can't really turn them on and off (well, not easily) &gt; i'm not sure what "moving/evaling in calling (parent?) stack frame" Loosely, the stack frame means the state (variables and their values; aka environment) as it exists within some function (or base environment); a behind your back thing. When you enter a function all the parameters, in some fashion, and other info, are pushed on the internal program execution stack as well as the local variables etc. This is the functions "stack frame". This is what defines what variables you see (the scope....forgetting global variables etc). When you leave that function its stack frame goes away (popped) and the program returns to the caller and its stack frame. So, let's say function A calls function B and you're stopped in the debugger at B. There's a stack frame for A and on top of that, the top stack frame, is the one for B. Let's say after examining/analyzing etc B's variables etc that your of a mind that perhaps the variables passed into B (from A) were "funky". A logical course of action would be to "pop up the stack" (from B's stack frame), putting you in As environment, as it existed (with some caveats) right before it called B, and check it out . Note, the stack frame for B still exists but internally R has been "fooled" into thinking that A is the top stack frame Now that you've popped up the stack into A's environment, you can dink around in there (as if you had stopped the debugger right before the call to B) to see if its state (variable values etc) was really funky before you called B. Perhaps, after dinking around in A's environment (looking at variables, doing plots of them etc), you might conclude that things really did go awry in A or perhaps it wasn't and stop debugging or continue accordingly. You might conclude that things were messed up in whatever called A and pop up to its stack. Note, whenever you "continue" the debugger goes back to the real top frame (e.g. B) and executes just as normal. Honestly, I haven't seen a language without these debugger features in 20 years. I don't know how people develop without it. 
There are some ways to do that (or: to do similar things to that), but they do involve modifying code. Personally, I tend to use tryCatch, where the catch expression is save.image() - it takes the current environment and all the objects and their states, and throws it into an .RData file. I partner that with options(error=traceback), to print the stack trace whenever something goes screwy. You could also look at ?Sys.parent for ways to access the call stack. Generally-speaking, though, these are all either hacks or things that don't quiiite get at what I think you're asking about. I spend a disturbing amount of my time seeing errors and having to laboriously run each individual child function, piece by piece, with many calls to str(). If you find a direct solution, please do let me know.
You might want to investigate the work Hadley (PBUH) has been doing with integrating the two directly - https://github.com/hadley/r2d3 (It's worth it just for the name)
I simply don't understand how a language could exist as long as R has without anyone making a reasonable IDE/debugger. I can find such for Python and I believe R predates it.
The funny thing is is that R really is a general programming language. In fact, it's language is more sophisticated than matlab (which is much older). R's completely OOP, everything's an object, it has closures, default parameters, lazy evaluation etc. Point being, that as a language it's sophisticated but it environment is practically antediluvian even a decade after its release, even though the debug features I've mentioned have been standard for a long time, and even though people write large programs in it (Note, I think a lot of people just write 50 line functions to run some specific analysis but others develop serious algorithms in it. All I can say is that they must have balls of steel). I can only imagine that regardless of its general power, and in line with what you wrote, that it wasn't considered a general programming language and so the people who would normally have written a IDE debugger just didn't think about it. On the other hand, I'm surprised the same people that developed R didn't get around to this (didn't the lack of a debugger drive them nuts?! Isn't this what grad students are for?)
I have no contention with anything you've written. Can I have my full featured modern IDE/Debugger now? (-; 
info@rstudio.com? ;p
?
If you're looking for an IDE, in my opinion nothing beats [RStudio](http://www.rstudio.com/). It's great for developing/debugging and perfect when you're just trying to hack something up quickly.
Probably because there are more R Google+ users than R Reddit users. Another thing is how hard it is to find this subreddit, I had to search for "R language reddit" on Google and it didn't point straight to the subreddit, but a article that was posted here.
In my opinion don't use R as a programming language, use it as a command-line statistical interface.
Just to let you know, putting 4 spaces before a line will make it show up as code. Do that from now on. The problem is that you are trying to tell the histogram to use your whole data frame for colors. You need to give it just one vector of data. Try changing it to say col=bs$education
OK, just remembered, you can't do a multi-group histogram in one line with the hist command. First off, R doesn't 'freak out.' You may be, but R doesn't. If something unexpected happened, it is because of something you did, not R. It does not have a mind. Anyways, here is some code to do a multi-group histogram with base graphics. Let me know if anything is unclear: # make 2 group histogram x1 = rnorm(10,1,1) x2 = rnorm(10,4,1) hist(x1,col='red',main='TEST',ylab='y',xlab='x',xlim=c(min(c(x1,x2)),max(c(x1,x2)))) hist(x2,col='blue',add=T) legend("topleft",c("A","B"),fill=c('red','blue'))
Additional to what forever_erratic said: yes R sessions memorize everything while they are open and running; and they can also save information (like assigned variables) for the next time you open R. A simple way to check this is the ls() function after you just opened a new session, it should return nothing (character(0)) in a new clean session. Depending on if you are using windows or linux and the console or an IDE there are different possibilities to deactivate this behaviour if you don't like it. Regarding your problem, it also helps if you explain what your intention was. For example I think you actually want a barplot(...) and probably the parameter 'fill' and not 'color', but that is just a guess. 
Yes, you are right, my mistake, if then you want barplot(table(edu)), the problem/reason I wouldn't use hist is because it believes the numeric representations (0,1,2...) are meaningful and not the factor labels (high school, bachelor, ...). That means that you can also create a histogram with less bars than education levels. Also with barplot you can color the different bars, with hist you cannot color the different bars, which is what you are trying to do, if I understood that part correctly.
Ooooh. See I didn't know that about the colors. Okay. So we're on to something now. I think I can fix this. Just deep breathing and one step at a time!
Thank you very much!
So you want us to do your homework? I don't mind helping if your questions are somewhat specific, but it just looks like you copied the homework assignment. 1) google "R plot gallery functions" or some combination 2) well what do you see? and people like to draw lines showing trends (I don't know if that is meant) 3) google what those types of plots are 4) google the words you don't understand and think about the last one a bit
^ what he said
Moving to 3 is pretty safe. The only problems I've really encountered are with packages that made the jump from 2 straight to 3.0.2; ubuntu's default is 3.0.1, so it's kiiind of a pain not to have roxygen2 support, for example, without a backported version that interacts weirdly with other, backported-to-different-points packages.
Appreciate the reply. I got an output!! And it doesn't look absolutely absurd! (hopefully) last question: so when you have Results1 &lt;- c(5:24) that sets Results 1 to the 5th column and reads in the 24 data points? Because...say this is an excel spread Sheet with X corresponding to the A column, Results1 to the B, etc...then say they each have 1000 rows. How do I assign the right Results to the right columns etc to get an accurate graph? you're the best by the way
Oh ok I see I see. So, Since I have 1252 rows I would do x &lt;- c(1:1252) Results1 &lt;- c(2:1250) Results2 &lt;- c(3:1251) Results3 &lt;- c(4:1252) Or would it have to be 1252*4 = 5008? Because I did something like that based off of what you said and I just got 4 linear diagonal lines whereas it should come out as three sporadic lines because the Y axis ranges from [0, 1] and each result has 1252 different values corresponding with it. Like [THIS](http://i.imgur.com/SBh2Ugx.png) is what I want the graph to output. Having a range of [0,1] on the y-axis. If that makes sense EDIT: Also, just in general...having X being 1 -&gt; n would be fine too. Just having a constant numerical value works. The important part is the y axis and the multiple lines
I really can't explain how much I appreciate you helping me out with this. So I did all of that and [this](http://i.imgur.com/TforJ02.png) is my result. So my Results1 etc are named differently, I just used Results1 to make it easier to describe my problem. But that shows my graph and the code I entered
head(partial_data) and summary(Site.377) both look accurate, including all of the ranges. when I put in plot(Site.377$X, Site.377$Results1) it kicked back "Error in if (equidist) seq.int(1/(2 * ny), 1 - 1/(2 * ny), by = 1/ny) else (yat[-1L] + : missing value where TRUE/FALSE needed"
I don't use ggplot2, so I can't help you their, but with the base plotting functions, you can use lines() to add further lines something like this should work: (col changes the color) plot(Site.377$X, Site.377$Result1, typ="line") lines(Site.377$X, Site.377$Result2, col=2)
are both numeric? with lapply(Site.377, class) R will output the data type of every column in the data.frame, if they aren't integer/numeric it probably wont plot what you want.
ok, what that means is R thinks that each value belong to a "group" with the names of these groups being the numbers you really want. This can happen when reading in the data. Try Site.377 &lt;- lapply(Site.377, function (x) as.numeric(as.string(x))) and then retry the stuff (head(Site.377) should return more or less the same, I think quotation marks may be removed). The lapply is needed to conserve the data.frame structure, I think. Basically you are first replacing the information to which "group" the value belongs with the names of the groups and then reinterpreting the name strings as numbers. If you don't go via the string representation, you get wrong integers, which are used for the "factor" storage but have nothing to do with the values you want. The other possibility is to change the code where you load the data, so it recognizes the values as numerical values.
Using any libraries in the process? 
sorry it should have been as.character (not as.string), and in the plot functions type instead of typ
Writing this from my phone, so I can't verify it but maybe try the axis function: plot(x_values, y_values) axis(1, at = x_values)
Xlim? 
This looks good, I'll just add that you need to suppress the default x-axis: plot(x_values, y_values, xaxt="n") axis(1, at = x_values)
Also, in your example you have more y data than x data which I think will cause problems. 
* Terminal + debug functions * StatET for Eclipse is a great IDE, I haven't checked out how it runs debug functions though. They could be well integrated into eclipse's debugging tools or not.
R is more difficult than Python is some areas (e.g. apply() functions family replace for loops and are not very intuitive in the beginning and if you want more examples, google "R inferno"). R is easier than Python in some areas, especially plotting. I use plot() to check my data as I'm writing a script and that becomes very helpful. But it all depends on you: are you willing to learn another language and eventually master the basics in a couple of months and most likely by yourself? For myself, I needed the libraries that parsed bioinformatics sequencing data so I made the plunge rather quickly and I am still using R for a another non-sequencing project. However, my co-workers haven't taken the plunge as Python, SQL and JavaScript work just fine for *what they are doing.*
would this run under 8gb? also, how would i go about using this lm() to score new data with so many models
there are not my videos, i post them because there isn't a ton of activity here. 
I don't understand the question I guess? HMMs are implemented in R packages all over the place. Of course for speed I'm sure they are mostly written in C/Fortran. So the answer is yes. 
"The Church–Turing thesis states that this is a law of mathematics—that a universal Turing machine can, in principle, perform any calculation that any other programmable computer can." - [Turing Completeness](http://en.wikipedia.org/wiki/Turing_completeness)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Turing completeness**](http://en.wikipedia.org/wiki/Turing%20completeness): [](#sfw) --- &gt; &gt;In [computability theory](http://en.wikipedia.org/wiki/Computability_theory), a system of data-manipulation rules (such as a computer's [instruction set](http://en.wikipedia.org/wiki/Instruction_set), a [programming language](http://en.wikipedia.org/wiki/Programming_language), or a [cellular automaton](http://en.wikipedia.org/wiki/Cellular_automaton)) is said to be __Turing complete__ or __computationally universal__ if it can be used to simulate any single-taped [Turing machine](http://en.wikipedia.org/wiki/Turing_machine). The concept is named after [Alan Turing](http://en.wikipedia.org/wiki/Alan_Turing). A classic example is [lambda calculus](http://en.wikipedia.org/wiki/Lambda_calculus). &gt;[Computability theory](http://en.wikipedia.org/wiki/Computability_theory) includes the closely related concept of [Turing equivalence](http://en.wikipedia.org/wiki/Turing_equivalence). Two computers P and Q are called Turing equivalent if P can simulate Q and Q can simulate P. Thus, a Turing-complete system is one that can simulate a Turing machine; and, per the [Church–Turing thesis](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis), that any real-world computer can be simulated by a Turing machine, it is Turing equivalent to a Turing machine. &gt;In colloquial usage, the terms "Turing complete" or "Turing equivalent" are used to mean that any real-world general-purpose computer or computer language can approximately simulate any other real-world general-purpose computer or computer language. The reason this is only approximate is that within the bounds of finite memory, they are only [linear bounded automaton](http://en.wikipedia.org/wiki/Linear_bounded_automaton) complete. Also, any physical computing device has a finite lifespan. In contrast, a [universal computer](http://en.wikipedia.org/wiki/Universal_computer) is defined as a device with a Turing complete instruction set, infinite memory, and an infinite lifespan. &gt; --- ^Interesting: [^Turing ^reduction](http://en.wikipedia.org/wiki/Turing_reduction) ^| [^Prolog](http://en.wikipedia.org/wiki/Prolog) ^| [^Alan ^Turing](http://en.wikipedia.org/wiki/Alan_Turing) ^| [^Universal ^Turing ^machine](http://en.wikipedia.org/wiki/Universal_Turing_machine) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cg96g32) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cg96g32)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Here are a couple of resources that I've gotten lots of use out of. Thought these might help spark a few ideas for cool/interesting projects you could work on: UC Irvine Machine Learning Repository! 273 data sets as a service to the machine learning community http://archive.ics.uci.edu/ml/ ML tutorials includes a bunch of R examples http://blog.yhathq.com/ vik paruchuri's blog - data scientist in boston with great posts outlining some of his R projects. http://vikparuchuri.com/
Of course. R can (almost) be used as a general purpose language. 
If you can learn to program with any other language, you can also learn to program with R. The reason it is used a lot by math and statistics guys is because it gives additional easy access to functionality they need, e.g. ploting, statistical distributions, statistical methods etc.
I think you want to use the factor data type in that instance. 
Assuming your table is called my_table: my_table$V58 = factor(my_table$V58)
Oh, I stumbled across this function earlier but I guess I was using it wrong. Anyway, thank you!
Yeah, I actually saw this function but I was using it wrong.
Coursera has a data-science track that teaches R. I just started today. There is a capstone at the end that incorporates R and other tools with a public open data project. 
I'm unsure about your question, if this is your code: fit &lt;- lm(y ~ x1 + x2 + x3, data=mydata) then the y column is being modelled using the x1, x2, x3 columns + a constant (constant can be removed with adding + 0) And I don't know what you want with the intercept ... mhh if you mean the ordering of the categories, that's probably based on the ordering of the factor levels, which are internally represented as numbers.
It looks like this works. Thanks!
Damn that is some horrible formatting. What is the error you get from R? Did you try running each line one at a time? If so where did the error occur?
In the line with the error, you actually don't have a hyphen/minus sign. You actually have a dash. These are different. If you replace that with `-` instead of the dash that is there now, it runs just fine. Regarding the formatting, it is really hard to read. There is white space in weird places and having the line end with white space and the next line open with a comma is very odd. /u/hadley has a very good style guide for R that will help make your code more readable on his site at [http://adv-r.had.co.nz/Style.html](http://adv-r.had.co.nz/Style.html). There is also [Google's R Style Guide](https://google-styleguide.googlecode.com/svn/trunk/Rguide.xml). Both are very similar and worth reviewing. You would be surprised how much more readable consistently following a style guide can make code. 
Thanks! I didn't realize that there was a difference between a hyphen and a dash. Thanks for the links too. Like I said in my reply to the other poster, on my end, it looks fine, or at least it looks like the solutions manual (which is my only reference point for what is fine looking really since I am new to this), but I'll take a look at these links.
with getwd() you get the working directory you are in, and with dir() it should list all files in your working directory. The combination should solve your problem. With setwd("path_to_new_dir") you set your directory, but I guess you knew that. If that doesn't help, what system are you on, what interface are you using etc.
running under windows? try slashes in stead of backslashes, that one caught me off guard (novice too here)
Ah, I see now on my computer it's fine. I was on my phone and it was just a cluster fuck haha.. glad the other user provided help
If you are having trouble the other users gave good advice but one thing you can do to get a feel for how to deal with paths to files is use file.choose() I'm not a fan of when people do something like read.csv(file.choose()) but I do recommend just using file.choose() so you can get a file browser, choose the file you want, and then use the path to the file that it gives you and put it in your code.
thanks for the advice; I didnt know about those commands. Sure enough, it recognized that the directory was in the right place, and even that the file was in the directory. But for reasons unknown the read.csv( command couldnt find it. I was/am having this problem on my mac book, but the problem does not seem to be present on my PC. I was able to use a solution elsewhere in this thread, though, so I'll be able to use that! Thanks for the help!
Hurray for learning new things. I was actually having this main problem on my MacBook, but as I also use a PC, this is good info to have. Thanks, and good luck learning this stuff!
Thanks! This ended up working for me. read.csv() still cant find the file, but it opened just fine after using file.choose() 
You can either use slashes instead of blackslashes *or* use two backslashes. The reason is that in a string the backslash is reserved as the escape character so that you can do something like \n to specify a new line - so you need two backslashes to get an actual single backslash in the string so to tell R that you're not actually escaping any other character and you just want a backslash.
I'm on this course too, it's a bit daunting but I really think its a lot of fun
cheers!
In your last image you never closed your quotes
Yeah I figured that out just now, I fixed that and get NA as result. I updated the picture. 
After checking out your code, I am unsure what you want to do. You transpose your matrix before going over the columns, therefore you effectively go over rows of your data.frame, is this correct? Then you are computing thousands of correlations, no wonder it takes long. If this is really what you want to do, I would write down the mathematical formula, and see if you can rearrange it to make it simpler/quicker to compute. But some pointers: for (proptype1 in property.types) { for (proptype2 in property.types) You are filing a complete matrix, but you only need the upper triangle, this will reduce the time by half (and a bit, because you also don't need the diagonal) &gt; then substitute out 1 and -1 for "NA", Why? It is relative arbitrary. look at ?cor, specifically: If x and y are matrices then the covariances (or correlations) between the columns of x and the columns of y are computed. 
Thanks for your input. I transpose because my observations go vertically (think a regression data frame). On second thought I guess the correlation function can take vectors that are horizontal or vertical since that's irrelevant to the function itself. Yes I am aware of filling out a complete matrix when I only need half - I'm currently doing each property type one at a time so and then deleting it from the property.types array so that it doesn't do redundant work. I could code it in but all I need is the final product and I am going to avoid running this that many times. &gt;Why? It is relative arbitrary. Because the 1s are the correlation with a single observation with the same observation in the second subset? (And I'm guessing the -1s are full NA series vs. anything else). So I guess in your last paragraph what you are saying is that I isolate a single property type or geographic location in subset1, do the same for subset2, and then use cor on the subsets (not individual columns), and then take the mean of the resulting correlation matrix? Is there anything in that matrix that should be filtered out because it would be redundant/overcounting? So my code would be: subset1 &lt;- subset(data, data$Property.Type==proptype1) subset2 &lt;- subset(data, data$Property.Type==proptype2) average_correlation &lt;- mean(cor(subset1, subset2, use="pairwise.complete.obs"), na.rm=TRUE) And the mean I would get would be the average correlation of property type 1 and property type 2 across all possible observation pairs? 
Regarding the transpose, I don't mind it, I wanted to make sure that I understood correctly that you are computing correlations between thousands of vectors that are relatively short (~50). Regarding the complete vs half, and doing it by hand ... all you need is: for(i in 1:length(property.types)) { subset1 &lt;- subset(loan.data, loan.data$Property.Type==property.types[i]) for(j in (i+1):length(property.types)) subset2 &lt;- subset(loan.data, loan.data$Property.Type==property.types[j]) &gt; Because the 1s are the correlation with a single observation with the same observation in the second subset? (And I'm guessing the -1s are full NA series vs. anything else). -1 are not NA against anything, that results in NAs. My guess is that you are getting 1s and -1s, because you have different observations that scaled match/mismatch perfectly, that are not necessaries the same observations. This is what I meant with it being arbitrary. (If the same observations can be in both sets, it might indeed make sense to exclude them, but I would try and do that directly by getting the information from the selection.) Regarding cor on a matrix, in that case you do need the transposition, because it specifically compares the columns. And while this is surely quicker, I can't tell you how much it would be. (But yes, as I understand it, it should exactly produce your corr.list, you can always test it on a small set.)
So I looked into the 1s and -1s, and I have many time series where there are only 2 common years of available data. When it is 2 years, it typically returns 1 or -1. So I guess they are all erroneous values - wish there was a way to have use="pairwise.complete.obs.at.least.3" in cor() or something like that. Thanks again. Code runs substantially faster. 
Ahh of course that makes sense, in that case you do want to NA them out. You can create a filter for 2 or less common data points with sum(!is.na( ... )) &lt;= 2, but I don't know if it is worth the bother or if just replacing the 1/-1 with NA is good enough. No Problem, happy to help. R is one of the languages where it is easy to quickly implement slow code that does a lot, but you have to know your way around to make fast code for non trivial problems.
R by default isn't going to run on multiple threads. There are packages that will run a job in parallel (snow, parallel, foreach) come to mind but they are decidedly non-trivial to set up for most users and are also going to require you to work on either Linux or a Unix based OS (like Mac OSX). Not Windows. Beyond that, multithreaded calculations are complicated in a lot of cases. Suppose I want to match a subject with T = 1 to a similar subject with T = 0 but I don't want to match multiple T = 1 people to the same T = 0 people. Then each iteration of the loop depends on what the previous iteration was. If there is dependence like that, you are looking at a massive undertaking for a typical user to get it set up right and to work using more than 1 job. Are you using someone else's functions or your own code? Is that code vectorized or can it be vectorized to run faster? If it can't be vectorized and you need more performance still, find out what is slowing it down and learn enough C++ (or C) to rewrite that bit using Rcpp. 
Thanks for explaining the complexity that multicore processing brings. I will try to vectorize the slower parts of the code, if performance is still an issue i will do some more research on how to apply multicores in an effective manner.
Did you try the install.packages approach? It worked for me. install.packages('RGoogleTrends', repos = "http://www.omegahat.org/R", type = "source") 
The question has been answered for cor.test . If you want to do a correlation table, you can do this: a = data.frame(rnorm(4),rnorm(4)) &gt; a rnorm.4. rnorm.4..1 1 0.42992676 -0.4572966 2 -0.05857476 -0.2714395 3 1.59366111 -0.2117290 4 0.61301796 2.5508591 &gt; cor(a) rnorm.4. rnorm.4..1 rnorm.4. 1.000000000 0.002571904 rnorm.4..1 0.002571904 1.000000000 tl;dr the "cor" function takes a dataframe and spits out a correlation table (still need cor.test for significance though)
assuming the edges match perfectly: take a polygon for every other polygon for every edge (pair of (x,y) paris) of either polygon (all vs all) compare, if identical create a new polygon by merging the x and y vectors remove the two polygon's you merged repeat until you only have 1 polygon left, or tried out all and couldn't merge the vector merging might be slighly tricky, but basically you need first_polygone_before_matched_edge,second_polygone_after,second_before and then first_after, that way the direction should stay correct (and of course you want to include the ends of the matched edge once; You might also have to watch out when connecting the second after and before, to make sure the point is only there once, I'm not sure how it is implemented in sp)
Would the gUnion function of the rgeos library do what you're describing? http://cran.r-project.org/web/packages/rgeos/rgeos.pdf 
&gt; print for example, matrix[1][1] In R you subset matrices like this: matrix[1,1] Generally matrices are simply long vectors which have an additional information about their dimensions. The row dimension is the "lowest level" dimension, i.e.: 1 4 7 2 5 8 3 6 9 is represented as 1 2 3 4 5 6 7 8 9 I'm not exactly sure how the interface to C works, but I could imagine that you filled in the vector thinking the primary dimension were the columns. double res[5][*n_points - 1] also suggests this (you have row amount of vectors of length column, while R uses column amount of vectors of length row, in either case appended to a long vector).
You could put the credentials in the environment and then retrieve them from R, or read them from somewhere else as /u/murgs suggests. To use the environment wouldn't be totally secure, and you'll need to be careful to ensure credentials don't end up in your shell's history file. That said, it will allow you to share the scripts without issue (although beware sharing workspaces), and is how Rails developers [solve the problem](http://railsapps.github.io/rails-environment-variables.html). An example, run from a shell: SUPER_SECRET=secret R And then get the value from the interactive R session: R version 3.0.1 (2013-05-16) -- "Good Sport" Copyright (C) 2013 The R Foundation for Statistical Computing Platform: x86_64-apple-darwin10.8.0 (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. [Previously saved workspace restored] &gt; Sys.getenv("SUPER_SECRET") [1] "secret" 
it always help to see a small sample of how the data.frame actually looks, but just guessing: If you want to extract a specific subset of the data, you can basically do this with a logical statement: my.data.frame[my.data.frame$user == "bob",] would only give you the rows where the "user" column matches bob. If you want to do this by all the different user's at the same time, there is by,aggregate and tapply. They have slightly different applications and syntax, and I can't remember which one you want but basically you give it the data.frame/column of the data and the column(name) by which to subgroup the data and then finally the functions which it should use. If you didn't know, summary(x) gives you a summary of x, mean(x) gives you the mean of x, sd(x) gives you the standard deviation and hist(x) plots a histogram. PS with ?function.name you get to an integrated help system
Thanks for the information, but I'm still unable to subsort each column. the data looks like the following: USER ITEM RATING USER1 ITEM1 numeric value from .5 to 5 USER1 ITEM2 "" USER1 ITEM3 "" USER2 ITEM1 "" USER2 ITEM2 "" USER2 ITEM3 "" etc, but for thousands of both users and items and not every user rated every item. How would I get a new dataset of every mean rating for each user and again for each item? I don't think it's tapply, from what I could tell from a Youtube instruction video. I don't see how to do it in aggregate. Thanks. edit: maybe if I could rewrite to data.frame to have Users be rows and Items be columns with empty spaces where there was no rating it would be easier. The tutorials seemed to be geared that way. Do you know how I could do this?
if your data.frame is called tmp, then tapply(tmp$RATING, list(tmp$USER), mean) should give you the mean rating of every user and tapply(tmp$RATING, list(tmp$ITEM), mean) the mean rating for every item, maybe I am also misunderstanding what you want. Regarding your edit, there is a reshape function in R and a reshape package (which is apparently faster). I have used neither, so I can't help you much further in that regard.
My best advice is probably to start thinking in vector operations as opposed to loops. Everything in R is a vector so even something like `x &lt;- 5` creates a vector `x` with a length of one whose only element is 5. Loops are slow in R and you really only need them if you explicitly want to do something a given number of times. For almost all of your operations you simply call a function on a vector. &gt; x &lt;- c(1,3,5,7,9) &gt; x [1] 1 3 5 7 9 &gt; x &lt;- x + 1 &gt; x [1] 2 4 6 8 10 I love R, but I actually try to not do too much programming in it. I find R is most useful for interacting with data. 
Please see my edit above: I really appreciate the advice, but I am looking for more advanced topics.
Thanks! I know what you mean - but it seems that R is mostly about dataframes - which are very similar to what pandas has to offer (or maybe it's the other way around :)). I will definitely have a look at the OOP part - I never touched that with R before.
God bless your soul :) Thanks for the great answer.
Your answer saved my work project. I'll give you reddit gold!
Glad I could help and thank you for the gold!
It worked perfectly. I love you. 
So I quickly realized that I need to remove users who had voted fewer than 20 times from the original data.frame because they were skewing the data. How would I go about this? Thanks again for the advice. 
with votes &lt;- table(tmp$USER) you can get how often every user voted (you can also abuse tapply but the result will be differently formated: tapply(rep(1,lengt(tmp$USER)), list(tmp$USER), sum)) votes &lt;- votes[votes &gt; 20] to filter out those that are fewer and tmp[tmp$USER %in% names(votes), ] gives you a data.frame where only the users are left that have voted more frequently but this is probably slow, if you only have few low count voters you can also go the reverse way, of checking against that list, and leaving them out If you are only interested in removing their means for the case of looking at the user-means, and not the item-means, you can of course filter out that result data.frame in a similar fashion and it should be much faster, because your table is much smaller.
Looks like an ideal job for `plyr`: install.packages("plyr") library("plyr") dat &lt;- read.table("test.tab", header=T) ddply(dat, .(USER), transform, UserMean=mean(Rating)) USER ITEM Rating UserMean 1 USER1 ITEM1 3 3.000000 2 USER1 ITEM2 2 3.000000 3 USER1 ITEM3 4 3.000000 4 USER2 ITEM1 1 2.666667 5 USER2 ITEM2 2 2.666667 6 USER2 ITEM3 5 2.666667 That said, I'm not sure why the approach you say you use doesn't work. Does it print an error message? What output do you get instead?
I'm having trouble on the third step. I'm not sure what "test.tab" is supposed to be. I tried the big data.frame, the array of user means and a table of user means. But each time I got the same error: Error in file(file, "rt") : cannot open the connection In addition: Warning message: In file(file, "rt") : cannot open file 'UserMeanTable': No such file or directory It says there's no such file, but I made it. when I type class(UserMeanTable), it says table and I can see the whole table when just typing UserMeanTable. What am I missing?
The `read.table` stuff is just me loading the table in the session, you don't have to do that. In your case you'll want to do `ddply(Ratings, .(USER), transform, UserMean=mean(Rating))`. Does that help?
you probably want to merge the two together using the USER column of both: merge(Ratings, UserMean, by="User") you may have to check if it should or shouldn't be resorted, and it can keep/remove entries if the user lists don't match (of course both data.frames have to have a User column in the above example, but you can also use, by.x and by.y to select the specific columns (or rownames with 0) for the two matrices/data.frames) EDIT what you tried, assumes that UserMean has as many entries as Ratings, which it doesn't since it only has one entry per user, you could also extend the entries by how frequent each user appears (and/or when they appear) but merge does exactly that
If I follow you correctly, you want something like... d1$dummy[x1&lt;0] &lt;- "e1" d1$dummy[x1&gt;=0 &amp; x1&lt;=10] &lt;- "e2" d1$dummy[x1&gt;10] &lt;- "e3" tapply(d1$x1, d1$dummy, summary, na.rm=T) 
c(e1,e2,e3) concatinates the three vectors, creating one that is three times as long as x1, therefore the error what you could do is use list(e1,e2,e3), but that also isn't what you want (if I remember correctly how tapply works). So you either want what /u/owls_with_towels said, or if the groups can overlap, something like: lapply(list(e1,e2,e3), function(e) summary(x1[e]))
The first argument to tapply is what you want the function to operate on, the second argument is how to do the indexing. You'll want to flip those to get what you want. tapply(Ratings$AdjustedRating, Ratings$Item, mean) with that said have you considered using something like data.table or dplyr? They make this kind of thing easier, more flexible, and are blazing fast.
Keep going - you're getting there and it'll only get easier. One thing I wished I'd learned earlier is: RSiteSearch("your problem goes here") searches the R website, library manuals and user groups for answers
You could use the rbind.fill function in the plyr package. You'll need to do a little extra work too. &gt; v1 a b c 1 2 3 &gt; v2 c d 1 2 &gt; library(plyr) &gt; out &lt;- rbind.fill(as.data.frame(t(v1)), as.data.frame(t(v2))) &gt; out a b c d 1 1 2 3 NA 2 NA NA 1 2 &gt; out[is.na(out)] &lt;- 0 &gt; out a b c d 1 1 2 3 0 2 0 0 1 2
thanks. Maybe I'm going about this the wrong way? Seems like this should be something I should be able to do out-of-the-box. I'm a software engineer by trade and my statistics and linear algebra are a tad rusty :) I think I actually have my data inverted. I'm using each vector as an observation. Is that correct? Or are vectors supposed to contain all observations of a single variable? Anyway, there still will be data missing from one or the other. In plain (as possible) english: From my database, I can calculate that 'v1' has 1 'a', 2 'b's, and 3 'c's, and 'v2' has 1 'c' and 'v2' has 1 'c' and 2 'd's I guess my question really is: what's the best way for me to load this into R so v1 and v2 are each records in a dataframe if i don't have a datapoint for every variable? I see something relevant with creating a table from factors - but that would require me to list out all of the values, right? &gt;v1 &lt;- c("a", "b", "b", "c", "c", "c") 
in a data.frame the columns are generally the different values while the rows are your observations, because each column can be a different data type, but the rows have to be identical types. Can you calculate from your database the occurrences with 0's? That would probably be the most elegant solution. Yes, using table from factors (or similar functions with telling him all possible elements) would return a vector you want. And yes, you would somehow have to load your observations into R, but if they are for example comma separated you can use split(x,sep=',') to get the vector after loading it in as string. A last possibility is to first create a empty data.frame with 0 everywhere and then iteratively fill up the positions with your sparse counts.
str is very helpful when looking at lists. I don't understand what you mean by "Is there a library or a set of functions that can make this process simpler? Is the only way to write nested apply functions?"
Basically is there a tutorial that can explain how to find out what elements are there in a list and how to reach each element. The list I am working on is so large that the str(x) basically scrolls off the top of the screen. How can I dig one step at a time? Eg. if x is my list and I do sapply(x,"[") [,1] [,2] control_row 0 1 RP_Data List,122 List,122 Non_RP_Data List,700 List,273 factors List,1 List,1 id "18140" "18139" category "Var" "Var" Now how do I navigate from here. If I only want to dig into say "RP_Data". Another scenario I face is querying the list. If I want a certain element matching some criteria how do I do that. I am not able to wrap my head around navigating lists.
If the elements in the list have names you can use names(x) to just get those. If you want to inspect one of those then subset it o &lt;- lm(mpg ~ wt, data = mtcars) # o is a list str(o) names(o) # Let's see more about qr o$qr # it is also a list so we can apply str to it str(o$qr) # or str(o[["qr"]]) names(o$qr) # Pull out one of those elements o$qr$rank
list's are one dimensional arrays/vectors, where each element can be another R object including further lists. The elements can also be assigned names, by which you can reference them, i.e. my_list[["name_of_element"]] [[ returns the element specified by the position or the name (the $ operator is an alternative for this) [ returns a list of either a single element, or several elements, so while [[ removes the outer most list layer [ does not Getting to see what is inside a list, is not always trivial, but since the list usually comes from somewhere e.g. a function, checking the documentary (of the function) usually helps.
I don't know how you did the subset or what the original df looked like but you can do: rownames(df) &lt;- NULL
It would be best if you showed a small snippet of code. It's hard to say what you're doing with no details.
You can remove the column pretty simply after the dataset is created with * dataset$row.names&lt;-NULL or since it's the first column * dataset&lt;-dataset[,2:ncol(dataset)] or * dataset&lt;-dataset[,-1] not knowing how you're going about the subsetting makes it more difficult to know how to stop it creating the column in the first place. I think what's happening is that you're writing the subset and then reading it again somewhere. When it writes the subset it writes the rownames as a column, but has one fewer column headings then number of columns. When it reads it again, the rownames get assigned a column name row.names. If that's the case, whenever you're using the write.XXX function, try adding row.names=FALSE, or play with row.names in the read function.
In your case I would usually use merge. I'd load each v up as a dataframe with the first column as "label" and the second column as "observation" so v1: &gt;label v1 &gt;a3 &gt;b1 &gt;c2 etc. then I'd merge: &gt;out&lt;-merge(v1,v2,by="label",all.x=TRUE,all.y=TRUE) you can repeat that over and over for each v3 . . . v20000 and at the end &gt;out[is.na(out)] &lt;- 0 and transpose it if you really want it in the other format. or to use them as you have now: &gt;merge(as.data.frame(v1),as.data.frame(v2),by="row.names",all.x=TRUE,all.y=TRUE) 
It would be helpful if you could elaborate on what you plan to use these languages for. What are your long term professional goals?
I do not have detailed objectives at this time, because I am not very advanced with programming. I would to give myself the skills to implement any ideas from statistics and numerical methods that I might find applicable in the future. I might want to write a program that would calculate numbers about stock markets, for example. I would want to have the most potential for using theories from statistics and numerical analysis, for calculating numbers from a data set which could be very large.
How about getting familiar with some big data technologies like hadoop, map reduce, hive, pig, etc... And data visualization solutions like qlikview, tableau or spotfire? If you're planning on a career in Analytics these would be a smart thing to learn. Also learn about how to use AWS. They are offering a free year....
At this time I am not well informed about any of the big data technologies that you listed. I will read up on them so that I at least know what they are. I would rather be advanced on a small number of programming languages and programs than be beginner or intermediate on a large number of programming languages and programs.
A couple of people have suggested Scala and Julia. Does anyone here agree with the suggestion to learn Scala and Julia?
Cool stuff. Thanks for the link. I'm just starting R soon, and I'll check this out! (I found something on this website: https://www.codeschool.com/courses/try-r ) It may be helpful as well! 
Ok, give me a minute
Wait a minute, trying to post the actual error, I solved the problem by commenting all the customizations I made to my /usr/lib/R/etc/Rprofile.site file (like loading personal R libraries for default). If it was not for your very logical suggestion, I would not have solved it yet. Thank you very much!
It didn't work. &gt; cat [1] "M" "F" "F" "M" "M" &gt; factor = factor(cat, ordered = T) &gt; factor [1] M F F M M Levels: F &lt; M &gt; levels(factor) = c("Male", "Female") &gt; factor [1] Female Male Male Female Female Levels: Male &lt; Female
From ?factor: &gt; The levels of a factor are by default sorted, but the sort order may well depend on the locale at the time of creation, and should not be assumed to be ASCII. They are sorted by alphabet/numeric not by occurrence in the factor, this also makes sense because your levels replacement call now works independent of if your factor_survey_vector has an "M" or "F" first. And R generally copies data as soon as you use a function on it, so after factor_survey_vector = factor(survey_vector) there is no further connection between the two. So if you change one, you only change that one.
It almost looks like that bin is exactly 19 units tall (hint hint). Edit: Didn't see your edit before. But yeah... you figured it out.
So just to be clear, it assigns the first label "Male" to F because F is earlier in alphabetic order than M?
well rbind will combine the dataframes, I'd add a column to each with the individuals name first. i.e: JAY$Person&lt;-"Jay" etc. then ALL&lt;-rbind(JAY,BILL,TED) for specific combinations you can subset out the data which I usually do using the logical operators. For instance, assuming AGE is numeric ALL[ALL$AGE==14,] will give you all the records with the age of 14. You can combine multiple criteria with | for or and &amp; for and e.g. ALL[ALL$Person=="Bill" | ALL$Person=="Ted",] would give you only records for Bill or Ted. You can just get a specific column(s) by putting it in quotes in the column part of the ALL specification (after the comma. e.g. ALL[ALL$AGE&gt;14,"SNIX"] You can then do sum(), mean() etc. on that result. Or you can use aggregate which takes various functions. aggregate(ALL$SNIC,by=list(ALL$AGE),FUN=mean) will get you the mean for each age group for snickers, and you can sub in sum for mean as required. If you want specific subsets of ALL you can either put them into aggregate, say just Bill for instance: aggregate(ALL[ALL$Person=="Bill","SNIC"],by=list(ALL[ALL$Person=="Bill","AGE"]),FUN=mean) or you could separate them first Special&lt;-ALL[ALL$Person=="Bill",] aggregate(Special$SNIC,by=list(Special$AGE),FUN=mean) There are probably two dozen other ways to approach it, but that's what I'd start with.
i looked up subsetting like you said and it helped a lot too i'm just really new to using R (and analyzing imported data on the whole) and am trying to learn it better by doing a project type thing. thanks for your help!
You should use dplyr for this. Absolutely phenomenal grouping and summarizing courtesy of Hadley. 
^Z doesn't close R, it suspends it (use `fg` to bring it back). To actually close an interactive session you can use ^D (CTRL + D) on an empty line, or use `q()`. As for these being "shitty", they might be, but they're age-old defaults. You quit an R session the same way you quit most shells.
Gotta watch out when the code itself has strings in it though.
Multi-line comments are indeed best handled by a good editor/IDE. Emacs+ESS, or RStudio come to mind. Probably worth the effort to learn good tools if you use R a lot. 
An easy way to maintain formatting and quickly "comment out" a large block is to use an if (FALSE) control statement around the offending code. if (FALSE) { Type whatever you want here. }
If you want a specific ordering, just list the values in order using the "levels" argument. Assign labels via the labels argument to factor (see ?factor). factor_survey_vector = factor(survey_vector) &gt; summary(factor_survey_vector) F M 2 3 factor_survey_vector = factor(survey_vector, levels=c("M","F")) summary(factor_survey_vector) M F 3 2 factor_survey_vector = factor(survey_vector, levels=c("M","F"), labels=c("Male","Female")) summary(factor_survey_vector) Male Female 3 2 
or vim.
I covered vim with "or any other decent editor" :)
If you are new to R, get oriented with http://tryr.codeschool.com For specific tasks, http://www.statmethods.net has short, on-point demos. While I am not sure I understand your task completely, it looks like you want to get the unique values contained in a single column. This is a straightforward operation. If you meant something else, provide more details and perhaps a sample data set with an example of the output you hope to generate. Assume that the variable is called plant_type and the dataset is called plant_data_set. Try the following: is.factor(plant_data_set$plant_type) if TRUE, then simply use levels(plant_data_set$plant_type) for a list of unique values and summary(plant_data_set$plant_type) for a count of the number of occurrences. If plant_data_set$plant_type is not a factor, then you can use unique() to get a list of the unique values in the column. 
Hi! Thanks for the advice and for the suggestion for where to get help. I have already tried CodeSchool and it's a bit too rudimentary given how much I already know. And believe me, StatMethods is already bookmarked. Sorry for the vague description of my problem. I was a bit frantic when writing this and looking back at it, I realize it's just a senseless rambling. So let me be more specific. I have three columns I am interested in: (1) plant, (2) fungal, (3) invasive. All three are factors. plant is a list of every plant surveyed over 6 years in 12 plots, about 150 levels over 200k+ observations. fungal is a factor that denotes what type of fungal association those plants might have and invasive denotes whether or not the plant is invasive or native to our experimental plots. Because data$plant contains such a long list of plants, that change every year and vary between plots, I can't easily look up the fungal and invasive assignment for each plant species. My current year's data doesn't have those assignments and so I need to add them. What I'm ultimately looking for is a data.frame that contains three columns: (1) a list of the unique levels in data$plant (the plant species), (2) a list of the data$fungal assignments to each plant, and (3) the data$invasive assignment. So in my case, the unique() function doesn't do me much good because I need to know what data$fungal and data$invasive reads for each unique plant species. I thought about running grep() on each unique level in data$plant, but then I would have to manually search for data[i, "fungal"] where i is the first unique occurrence of each plant species. Is there any function that would make this kind of summary easily executed? Thanks again for the help!
It's not clear to me what you're looking to do.
 arr1 = c(1,2,3) arr2 = c(4,5,6) arr3 = c(7,8,9) arr4 = c(10,11,12) t.test(arr1, arr2)["p.value"] t.test(arr1, arr3)["p.value"] t.test(arr1, arr4)["p.value"] t.test(arr2, arr3)["p.value"] t.test(arr2, arr4)["p.value"] t.test(arr3, arr4)["p.value"] But I want it in the following format: [,1] [,2] [,3] [,4] [1,] 1.0000000000 0.021311640 0.001826261 0.0003850677 [2,] 0.0213116400 1.000000000 0.021311640 0.0018262610 [3,] 0.0018262610 0.021311640 1.000000000 0.0213116400 [4,] 0.0003850677 0.001826261 0.021311640 1.0000000000 Outer almost does what I want, but I can't figure out how to feed it arrays appropriately. &gt; outer(c(1,2,3,4),c(1,2,3,4),FUN="*") [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 2 4 6 8 [3,] 3 6 9 12 [4,] 4 8 12 16 Basically they above but with a multidimensional array as arguments to outer.
Not sure if you've solved this already but I found a way to do it by creating a list of your arrays. However, *outer* doesn't seem to like being passed lists as arguments so I found the easiest way is to just use a for loop. Since that's not very efficient in R you could write it using Rcpp to avoid all the horrible copying: library(Rcpp) cppFunction( " GenericMatrix outerCpp(const List x, const List y, Function f) { int xLen = x.size(); int yLen = y.size(); GenericMatrix out(xLen, yLen); for (int i = 0; i != xLen; i++) { for (int j = 0; j != yLen; j++) { out(i, j) = f(x[i], y[j]); } } return(out); } ") outerCpp(list(arr1, arr2, arr3, arr4), list(arr1, arr2, arr3, arr4), function(x, y) t.test(x, y)$p.value) [,1] [,2] [,3] [,4] [1,] 1 0.02131164 0.001826261 0.0003850677 [2,] 0.02131164 1 0.02131164 0.001826261 [3,] 0.001826261 0.02131164 1 0.02131164 [4,] 0.0003850677 0.001826261 0.02131164 1 Edit: once again, premature optimisation is the root of all evil: a regular for loop in R is just as fast and there's no need for the Rcpp part - you can just use: outerR &lt;- function(x, y, funct) { out &lt;- matrix(nrow = length(x), ncol = length(y)) for (i in 1:length(x)) { for(j in 1:length(y)) { out[i, j] &lt;- funct(x[[i]], x[[j]]) } } } outerR(list(arr1, arr2, arr3, arr4), list(arr1, arr2, arr3, arr4), function(x, y) t.test(x, y)$p.value) Still, that was fun, and I learned that you can pass R functions to Rcpp ones! 
I like this: http://bitesizebio.com/10718/an-easy-way-to-start-using-r-in-your-research-basic-tutorial/ disclaimer, I wrote it. There are two follow-ups: http://bitesizebio.com/19666/an-easy-way-to-start-using-r-in-your-research-exploratory-data-analysis/ http://bitesizebio.com/20517/riding-the-r-train-using-r-for-statistical-tests/ and soon to be a third on ggplot.
I like that you use RStudio, since that is what I use. I'll definitely take a closer look at this later; off to work with me for now!
Python is the closest as they're both scripting languages. Python is also widely regarded as the easiest language to start with. Java, C++ and Visual Basic are substantially different (used for writing compiled programs rather than scripts which is generally more complicated and time-consuming), and while learning any language would be helpful for learning R, these are not good choices if R is your goal. Honestly, the amount of programming you need to get into R is pretty low. Many R users AREN'T programmers at all, as you can do a lot just by learning simple one-line commands (ex. "lm(y ~ x.1 + x.2, na.action=na.rm)" gives you a regression model). You should easily be able to jump right in. R for non-programmers: http://java.dzone.com/articles/how-get-non-programmer-started http://alyssafrazee.com/introducing-R.html If you insist on learning another language first, you can accomplish a LOT on your own much more quickly than in a classroom. Python links: https://wiki.python.org/moin/BeginnersGuide/NonProgrammers http://www.pythonforbeginners.com/basics/python-websites-tutorials "How to think like a computer scientist (python edition)" is a good intro. http://interactivepython.org/courselib/static/thinkcspy/toc.html# Best of luck! (and if you decide to take a class, go with python unless you have some other reason for learning one of the other languages)
Free R course starting Sept. 9: https://www.edx.org/course/kix/kix-kiexplorx-explore-statistics-r-1524
You might have to unlearn a lot of concepts common to other programming languages when you start R. If you have worked with data before via SQL or even Excel, you will find jumping into R easier. Have some computing background like functions, conditional statements, variables and parameters and just a logical approach will help a lot.
R is a complete programming language and can be used for any programming task. I am not sure what you mean by 'meaningful projects'. Are you trying to learn R or find the meaning of life? If you don't start because the project is not meaningful enough then when the time comes to do something meaningful, then you won't know enough. I would say start small but start now. Pick what kind of project you want to do. * If you like algorithms try solving problems at projecteuler.net * If you want a bigger challenge try the google code jam problems * Look at r-bloggers.com and try some of the fun projects they talk about (like escaping zombies http://www.r-bloggers.com/r-programming-challenge-escape-the-zombie-horde/) * Build a game (tic-tac-toe, sudoku etc.) * If you like building UIs look into shiny (this is fun but can be tricky) * R's power comes from its vast set of libraries, play around with them to become expert in them (http://blog.yhathq.com/posts/10-R-packages-I-wish-I-knew-about-earlier.html) The above don't even get started on the statistical powers of R, but that's ok because R is so much more. I have used R to do all sorts of stuff - Twitter wordcloud (https://lc0.github.io/blog/2013/08/25/twitter-word-cloud-with-r/), checking if my local train is running on time by parsing their real time API (http://api.bart.gov/docs/overview/index.aspx), monitor stuff and send me alerts, write a SQL generator that also connects and pulls daat from a database, scrape and crawl websites to figure out which links are not working to even organize my photos on my hard drive. 
I was thinking that I should start off in a classroom environment because I really have no programming knowledge whatsoever. I'm taking a CourseRA course that is a very basic intro to R, and even with that there are sometimes things that I don't understand. When I google an issue I'm having, I have so little background knowledge that I can't even understand the answers. The questions I have are so basic that they seem obvious to everyone but me. I often don't know the vocabulary to google either. So I was thinking that starting off someplace where there will be someone to help in person would be helpful.
Thanks for the links! Those do look helpful.
I think you're very right, that the issue is going to be learning to think like a programmer. I really don't know even very basic things right now so that's why I think a class would help. Thanks for your reply!
Great, thank you!
By any chance it's that john hopkins course?
Just in case anyone stumbles across this, it seems that loadhistory() is more or less what I wanted.
Check out www.coursera.com. They've got a good free online R programming course. Jump right in!
Coursera has a good R class that kicks off once a month and lasts 4 weeks.
I started learning to code in R earlier this summer using (of all things) "R for Dummies". It really breaks it down into simple-to-understand terms and is extremely thorough.
**Recommended Resources:** Can download R here: http://www.r-project.org/ Can download R Studio here (a nicer program for running R, highly recommended): http://www.rstudio.com/ A gentler introduction to R, here are 10 very short videos that give you a "tour" of R (about 30 minutes, total): http://www.youtube.com/watch?v=Tbbvw_LDks4&amp;list=PL0cNPtWZWKMRmB6D9QhUyR-gOHoD3qRxt A short course introducing basic R functions and syntax (about 2 hours, total): http://tryr.codeschool.com/ Link to the videos for one of the Coursera classes (in case you don’t end up following along with the Coursera schedule): https://www.youtube.com/user/rdpeng/playlists **Other Good Resources:** A new R training website that looks really good, but I haven’t used yet. I’ll probably test out the finance in R course at some point: https://www.datacamp.com/ R guides on probably, statistics, econometrics, times series, etc.: http://blog.revolutionanalytics.com/2014/03/an-r-meta-book.html The official “Introduction to R” manual: http://cran.r-project.org/doc/manuals/R-intro.pdf
I'm not totally following what you're looking for, but are basically just wanting to simulate a random walk? x = seq(from=1,by=1,length.out=1000) t = 1 # temperature, at zero, there will be no walking y = cumsum(rnorm(length(x),0,t)) plot(x,y) #but probably it can't go under the tub, so have to do a loop y[1] = 0 for (i in 2:length(x)){ y[i] = y[i-1] + rnorm(1,0,t) if (y[i] &lt; 0){ y[i] = 0 } } plot(x,y)
Hmm, interesting problem. Some sort of fitness landscape experiment? Unfortunately I can't help you with a closed-form solution. Could you just model the shape of the tub and use that for slopes? 
I don't know if this is helpful, but what you want to do roughly matches simulated annealing without its part of reducing the temperature over time. How it practically works is that you randomly chose a neighbouring position and based on the energy differential between the two and the temperature of the system you compute a probability to accept/reject the new position. This is a numerical solution, if an analytical solution is possible depends on the specific details of the functions, and is more of a statistics problem and less an R problem.
this is perfect; i'm starting a new project where i'll be getting into funnels to a greater extent than i have before. does anyone have any further reading they can recommend on similar topics?
I'm enrolled in the 3rd part of the Coursera data science specialization currently and I can say it's great so far. I would caution someone inexperienced in programming from taking multiple courses at once, but so far the instruction has been great.
It's a language written mainly by statisticians for statisticians. Indexing starting at 1 makes sense to mathematicians/statisticians and it occurs in other languages as well. You can use = for assignment if you want but that's not the common convention. The choices might be frustrating to you but I assure you that there is a reason for the decisions that were made.
Yes, its quite unique as far as languages go, but its not particularly difficult to pick up. Indexing starts at 1 - this is pretty easy for non-programmers to pick up. Assignment operator doesn't get confused with equality, which I like. Some parts are strange (values are vectors), but it works well for statistics. Beats always having to write loops. 
Revolution Analytics has a good [article](http://blog.revolutionanalytics.com/2008/12/use-equals-or-arrow-for-assignment.html) on this and explains that the use of "&lt;-" was originally the only option for assignment in R (and prior languages that R borrowed from). R added the ability to use "=" as an assignment operator in 2001 but I suppose the use of "&lt;-" had become so ingrained by this point that many R programmers stuck with it. One slight difference should be noted is that ["&lt;- can be used anywhere, whereas the operator = is only allowed at the top level"](http://stat.ethz.ch/R-manual/R-patched/library/base/html/assignOps.html). For example, if you run the following: sum(x &lt;- 1:10) sum(y = 1:10) x will be assigned to the series 1:10 but y will be undefined. I personally use "&lt;-" mostly to feel like a real R programmer...
Funny enough I am quite happy about the divergent syntax choices, because when they are clearly different, you wont try and use the wrong syntax when switching between languages, once you learned it. And honestly nearly every language has some abnormal (initially frustrating) syntax, you just get used to it: python has indents instead of brackets, perl has $,@,% in front of variable names, in C/C++ * b and *b mean vastly different things ...
I've been using R for 5 years, everyday, and about three years ago switched to using just '='. I've never encountered a single problem for it (although I never do strange assignments like thekcchief exampled). I find it reduces my chance for errors -- I only have to get one character right, not two.
We don't! It is a throwback a time when keyboards had a ← key. R has had many assignment operators in the past but has moved towards embracing the equality sign for assignment (as have other C-family languages). [History of R assignments operators](http://developer.r-project.org/equalAssign.html) Though some people make a virtue of upholding traditional operators, there are plenty of R programmers who simply use the equality sign. See [this manual entry for the five currently supported assignment operators](http://stat.ethz.ch/R-manual/R-devel/library/base/html/assignOps.html). Note: the arrow forms can have some unexpected side effects when used inside a function. 
The language that became R was developed starting in 1975, a time when computer keyboards were not even standardized yet and C was a new language. Computer language design was still in its infancy. As a result, there is a great deal of interesting baggage. R is developed by people who want to get things done and who don't necessarily spend a great deal of time choosing the most elegant design implementations. Python is a very modern language in contrast to R, developed almost 20 years later and incorporating many lessons learned since the time R was new. Python has less backwards compatibility baggage. I use both languages and actually find them to be pretty similar to each other. It is their subtle differences that actually cause me the most trouble: len() vs length(), looping through collections, etc. Nice overview of R language history: https://www.stat.auckland.ac.nz/%7Estat782/downloads/01-Basics.pdf R / Python traslation table: http://mathesaurus.sourceforge.net/matlab-python-xref.pdf
yourtable[8,]
/u/niafall7 explains how you get the 8th row out of a matrix you have in R. If you want to read it out of a file, use read.table(...) (or the appropriate wrapper) and the parameters skip=7 (to skip the leading rows) and nrows=1 to only read in one row.
I already loaded the table into the variable "data." So as per niafall7's suggestion, is the code "data(8,)"?
Note that when mr_kitty is talking about R they are for the most part talking about S. R was derived from and essentially was a clone of S but R didn't actually show up until the late 90s.
Close it is data[8,]. 
Hmm I don't really understand what you're asking. But if you have several text files in a directory each containing data with the same header, why don't you just use something from like lapply to read in all the files and then use the ol do.call() to make the dataframe?
I'm doing the coursera course on it and one assignment doesn't call for that. I'd post the question entirely, but I had to accept some promise not to cheat or plagarize (in case anyone cares, I'm just doing this to learn). The situation is a file contains a bunch of csv files that I need to analyze. I have to create a function, but I don't even know how to start. Like I have zero clue. 
The correct answer was posted. And, yes, the airpollution problem does call for that solution.
Where is it posted? Thanks
In the loop that you wrote, you are storing the sum in the same variable for each iteration of the loop. If you want the information stored in a vector, you need to do this: x&lt;-NULL for (i in 1:12) {x[i] &lt;- sum(data$Month == i)} R should be able to run this code in a heartbeat, but when you have a lot of data, it can be much faster to use the 'apply' family of vectorized functions, rather than loops. If you're going to be doing a lot of this kind of stuff, it'll probably be worth your time to become familiar with them if you aren't already. Here's a helpful start: http://www.r-bloggers.com/using-apply-sapply-lapply-in-r/ Good luck!
Also worth noting, if you have your month names stored as factors rather than character strings, you can just do x&lt;-summary(data$Month) and that will give you the counts.
Thanks. What's the point of setting x as NULL? (I'm new to R)
It's basically just a statement that establishes that x is something before you start trying to store information in x. If you try running the loop as written, but without x previously defined, then you'll get an error message when the loop tries to store something in x[1]. Basically, you can create new variables within loops, but if you are trying to store something in an indexed object within a loop, the object needs to be defined first.
An alternative to a for-loop is using the apply family of functions: x &lt;- sapply(1:12, function (i) sum(data$Month==i)) you store the result of running the function with the different i-values in the vector x (There are also more complex variations (tapply, mapply, ....) where you could, for example calculate the mean for each month separately)
The any function returns you TRUE if any of the vector elements are TRUE and otherwise FALSE, that's generally not what you want (only if you would call any for every row after comparing it with A/B/C, which is over complicated). I can't think of a nice way how to do it without handling this as a special case, in which case you can also just skip the sub setting step altogether, since you want everything anyway.
I want the option for ALL or the ability to do any one of three subsets. I'll keep hammering away at this as time permits. Thank you for your reply.
Found a solution to my problem. instead of subsetting, I'm using variable comparisons the trick is to just set your variable to equal what you're trying to match on. pseudo code below if (value == "All" then input ==parameter...) input plot(var1[parameter == input], var2[parameter == input]) 
Two other useful tools are: A Tiny Handbook of R An Introduction to R
Am I correct in thinking your data is monthly observations over multiple years with one row for each month? When you use cbind it's repeating 1 to 12 all the way down the column, so you need a little extra to get just the ones you're looking for. Try adding a start and end month to the variables you define in your function, and something like [month %in% c(start.month:end.month),] after you bind the data to the months to select just the ones in the range you are interested in. 
I don't even know how this function is doing anything, or even not erroring out. In your script is 'directory' actually 'data'? Howbout this: temps = rnorm(144) months = rep(seq(1,12),12) mydata = data.frame(temps,months) months_to_get = c(2,4)# feb april ff = function(mydata,months_to_get){ thosetemps = mydata[mydata$months %in% months_to_get,'temps'] thosemonths = mydata[mydata$months %in% months_to_get,'months'] result = cbind(thosetemps,thosemonths) result } ff(mydata,months_to_get) 
In order to use R safely, you need to understand the recycling rule. Have a look at a resource like https://www.inkling.com/read/r-cookbook-paul-teetor-1st/chapter-5/recipe-5-3. If that information is too advanced, spend some time completing TryR at http://tryr.codeschool.com. Comments: It would be better practice to give your function a descriptive name that is not similar to the built-in method function(). month_range_data = function( directory, months = 1:12) {....} What is directory and how does your function use it? Where does "data" come from if it is not created inside the function? As far as solving the actual task, we need some more information about the desired infup and output. Try this out: data = as.data.frame(matrix(c(1,1,1,2,2,3,3,3,3, 60, 55, 54, 53, 48, 56, 49, 44, 47), ncol=2)) names(data) = c("month", "temperature") Is "data" similar to your starting data format? Please add some detail showing what the results of your function should be. 
I assume you're talking about ggplot? That's a difficult question. You might be better off not using faceting, and instead using grid.Extra to arrange the plots, a la par(mfrow()) for base graphics.
Thanks for this. Yes, I am talking about ggplot. I wonder if you might have an answer to a related question. I have used facet_grid() to produce three bar charts in a row, each with the same y-axis which is on the far left of the plot. However, only the plot on the left has the axis line. I would like all of the plots to have the y-axis line (though not the ticks or labels). Any ideas how to do this? 
In fact, I think using geom_hline(yintercept=0) does the jog
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 2. [**Origin**](https://en.wikipedia.org/wiki/Zero-based_numbering#Origin) of article [**Zero-based numbering**](https://en.wikipedia.org/wiki/Zero-based%20numbering): [](#sfw) --- &gt;[Martin Richards](https://en.wikipedia.org/wiki/Martin_Richards_(computer_scientist\)), creator of [BCPL](https://en.wikipedia.org/wiki/BCPL) language (precursor of [C](https://en.wikipedia.org/wiki/C_(programming_language\))), designed arrays initiating at 0 as the natural position to start accessing the array contents in the language, since the value of a [pointer](https://en.wikipedia.org/wiki/Pointer_(computer_programming\)) *p* used as an address accesses the position p+0 in memory. Canadian systems analyst Mike Hoye asked Richards the reasons for choosing that convention. BCPL was first compiled for the [IBM 7094](https://en.wikipedia.org/wiki/IBM_7094); the language introduced no [indirection lookups](https://en.wikipedia.org/wiki/Indirection_lookup) at [run time](https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase\)), so the indirection optimization provided by these arrays was used at compile time. The optimization was nevertheless important, as batch processes in the system could be interrupted at any time to calculate [yacht handicapping](https://en.wikipedia.org/wiki/Velocity_prediction_program#Sailing_Yacht_Handicapping) for the president of [IBM](https://en.wikipedia.org/wiki/IBM)'s racing yacht. &gt; --- ^Interesting: [^Longest ^increasing ^subsequence](https://en.wikipedia.org/wiki/Longest_increasing_subsequence) ^| [^Telephone ^numbers ^in ^the ^United ^Kingdom](https://en.wikipedia.org/wiki/Telephone_numbers_in_the_United_Kingdom) ^| [^Natural ^number](https://en.wikipedia.org/wiki/Natural_number) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ck5jv3o) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ck5jv3o)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
 ?print brings up the help for print which mentions cat(), which could be what you want. Otherwise there is also message() which is more likely what you want.
This should work, I totally missed the cat() mention... I didn't get a "red" message, but that wasn't really important. Thanks again! `&gt; x &lt;- function(x = 0){cat(x)}` `&gt; x("WRITE SOME KIND OF MESSAGE")` `WRITE SOME KIND OF MESSAGE`
as long as you aren't making a package cat is great (it's also what I mostly use). They (cat and message) write to different devices (therefore the different colours), which is important if you want to catch the messages and hide them etc.
Why do pirates take so long to learn the alphabet? Because they spend so much time at C. (A pirate joke that has nothing to do with R!)
You didn't mention where you're coming from, but check out [swirl](http://swirlstats.com/) and the [John Hopkins course on coursera](https://class.coursera.org/rprog-007).
I have 0 knowledge in R. Mainly would like to focus on an actuarial approach. Thanks for the links!
Check out all the task views that interest you. 
[Codeschool](https://www.codeschool.com/courses/try-r) is a quite good resource for a beginner. The R course is free.
As the author mentions at the end of the article most of the "issues" that they have are that R doesn't behave exactly like {insert other software package here}. I read the article and after a majority of the points they bring up I said to myself "why would I want R to do it that other way?".
Since almost everyone comes to R from different stats packages I think many have the experience of not understanding why R works the way it does. I mostly agree with you though, I like how it works, even if it is a bit odd at times. 
Having a REPL is amazing. Having open files you can update while testing in the REPL? Double amazing. I'm not sure why but I couldn't quite get the hang of the emacs modes that did similar things (for other languages) that Rstudio does but I am ready to give them another try.
Emacs/Vim based environments are still very basic compared to R Studio. The only things I've seen that come close are SmallTalk environments (Pharo especially, probably some commercial ones too) and old Lisp environments. R Studio gives you the REPL, but also IDE features like code completion, documentation, version control, package management, and the server package is just over the top. So much win... 
Just wondering, i only recently started working with R but come from an object-oriented programming background... What advantages does R have as a general programming language over compiled languages like Java/C#, or even python as a scripting language? 
I have been developing apps with shiny for the most part of 2014. Its pretty amazing once you get the hang reactivity and event handling.
I'm intrigued; what do you need to be able to work with bytes for?
&lt;-
-&gt;
Yes, definitions are =, assignments are &lt;-
Clearly for assignments you should use `assign` assign("x", 2) or `&lt;-`("x", 2) Haha - no I'm in the "&lt;-" camp.
mostly &lt;- but sometimes -&gt;
= every single time. Been doing it for years and it's never failed me. 
I don't see any advantages as a general-purpose language. I wouldn't recommend R as a general-purpose language. But for statistical analysis, especially in the data-exploration phase, the IDE and quick analysis methods make it a great tool. I only learned R because I took a class that uses it. However, it has quickly become my go-to tool for data analysis. 
Good post. R isn't perfect. One of the key advantages of R over commercial packages is the open-source community behind it. That can also be a weakness as R isn't as focused and well planned as other tools. In the end, it all comes down to using the right tool for the job. I wouldn't use R to code back-end services. I probably wouldn't use R to create a website, though Shiny looks promising and the ease of layout might make me change my mind at least for a site who's main purpose is data presentation. On the other hand, I wouldn't use Ruby for data analysis, and even though one can do anything in Python that they can do in R, I like using R for data analysis. I would like to add allowing dots in variable names to the list. I absolutely hate that. 
For medium-to-large projects, I prefers Sublime Text in Vim mode. Rather than viewing plots and console output, I'd rely on tests to see how I'm doing. For data analysis, exploration, the IDE is fantastic. Rstudio replaced Excel and largely replaced SQL when learning about my data. 
It is awesome, but there two downsides I noticed: - using large intermediate datasets or long calculations, save the results and only load them as RData, while knitr can cache results of code chunks in theory, there are many things that lead to a recalculation - the report is good to read, the markdown/code not that much, the fact that the code is split up into chunks can make it more difficult to follow That being said, the new integration in Rstudio together with shiny so you can make/run documents with interactive plots without hassle is really great. (I probably have to learn one of the javascript plotting "languages" at some point. But they seems unintuitive and the interface over the R packages seems so clunky until now.)
Knitr and shiny my friend. 
Some men just want to watch the world burn...
Wouldn't be too bad to do with base plot but using ggplot2 would make this easier. Would you be open to that?
Sure. I've been using the beeswarm package, similar to stripchart. Its a standard style in my field and I like it aesthetically.
A simple (but not really elegant) solution might be to just paste the two vectors together and then do the plots of value ~ sample_and_treatment EDIT you also need to transform the character vector paste produces back to a factor vector EDIT2 ok the nice way to do it is probably along this line: layout(matrix(1:4,1));tapply(value, list(sample,treatment), boxplot)
find the picture you want to make and follow the sample code: http://docs.ggplot2.org/current/
Awesome! Thanks so much...looks like I'm gonna have to give ggplot2 a go.
To give this a name, `g` is now a [functional](http://en.wikipedia.org/wiki/Higher-order_function), as it returns a function. 
It works fine for me. Are you using read.csv or read.xlsx? When I download, it comes as a CSV. Also, are you sure that what your seeing isn't the way that R displays data.frames with many columns? Do a dim() on it. It should be 23x1000. IF 23x1000 it read in correctly, and you're just seeing the way R displays large data.frames. 
Ya, that's what I meant when I asked if R cuts columns. Thanks for actually doing this out to help me!
Could you tell me the code for what you are describing (I'm new to R)? The one on how to change dimensions. thanks I did this, but got an error dim(cat) &lt;- c(23, 1000) Error in dim(cat) &lt;- c(23, 1000) : dims [product 23000] do not match the length of object [23] 
Is there a way to fix this, or is it automatic in R's console output?
you can make your console wider otherwise it is automatic and makes sense, consoles typically can't scroll sideways, so you would otherwise have to just not display that information. GUI's like Rstudio can give you the possibility of looking at matrices in a style similar to excel. But R's primary goal isn't to look at tables, but to work on/with them, so you should quickly notice that this isn't a big problem.
If your data.frame is cat, then just type dim(cat) to see the dimensions. 
Thanks. I guess it just takes a bit of getting used to
This should cover it http://www.ats.ucla.edu/stat/r/faq/subset_R.htm
dog &lt;- subset(pizza, CAT &gt; 5 &amp; CAT &lt; 15) dog &lt;- subset(pizza, CAT %in% 6:14) 
Thanks!. I was incorrectly doing it as subset(pizza, CAT&gt;5, CAT&lt;15). 
Who needs subset? dog &lt;- pizza[pizza$CAT &gt; 5 &amp; pizza$CAT &lt; 15, ]
Why did you put a comma at the end?
Subsetting with bracket notation expects two values [row(s), column(s)]. If you want the first 3 rows and columns 8 &amp; 9, you can do the following: my_data[1:3, 8:9] If you want the first 3 rows but you want all of the columns, you can simply leave the column empty. But you still need the comma: my_data[1:3, ] You can do the same for rows if you only want certain columns: my_data[, 1:3] In the extreme, you can do the following to get everything: my_data[ , ] Of course, you wouldn't do the last one... you'd do simply "my_data" with no brackets
I suppose I could precompute the width vector, thanks. I was wanting to pass a function which varies the width based on the timestamp.
I use the kable() function from the knittr package in markdown when I'm looking to do nice tables. I've never tired formatting like that but there may be a way to do it. 
so what the others are pointing out are functions to generate a nice looking table in text/latex/html format, but they are terrible to export into excel, for that you can just use write.table, which makes me unsure if we are understanding your question correctly. (You can't generate the nice looking table and then copy it into excel, still looking nice, as far as I know - if that was your question)
As far as I know there is no package that can export or generate the formatting for excel/calc.
I always use &gt; install.packages("package") &gt; library("package") Good luck. 
The package name is quoted in install packages, however it's not quoted when calling library Eg install.packages("plyr") library(plyr) 
Just the class called "R Programming" or their recommended sequence of classes? I have taken the first 3 courses in the recommended Data Science track. The "Data Scientist's Toolbox" (first in the recommended sequence) is easy and, if you don't already know the things it covers, informative. The next two classes, "R Programming" and "Getting and Cleaning Data" are nothing like the first class. The lectures don't touch on a lot of the things you will need to know to complete the assignments. In fact, many times the lectures are totally irrelevant to the assignments so after you watch the videos you will still need to Google everything you need to know to do the assignment. There is considerable talk on the forums with people saying things like, "I took this class last year and I came back again now hoping they had gotten it together but it hasn't changed at all." From what I gather, if you have a programming background, you'll do OK because you will know what terms to search to find "how to do &lt;whatever&gt; in R" but if you don't have much programming experience you can waste a huge amount of time trying to figure out what term to search to find out how to do whatever action you need, since it was never mentioned at all in lecture. Basically, I found the classes a huge disappointment. It seems like minimal tweaking would make them good classes--they could just add a short lecture to each section which briefly describes the terms you need to look up to do the homework and students would have a foundation to tackle the assignment. If the classes were marketed as being primarily for people with programming experience, the lectures might seem reasonable, but they are supposedly for anyone regardless of experience and the class forums are full of people who are frustrated by these courses. 
You can quote during the call to library too...
&gt; RStudio's "install packages" is equivalent to R's "load packages." No, they both should have both, install onto your machine, while load means load into your running R session. &gt; In R, "install packages" gives you a list of countries. How do I do that in RStudio? RStudio likely has a set default of which repository it uses to install the packages, when you call install.packages it than gets this with getOption("repos") automatically. If you want the selection list use chooseCRANmirror() and pass the one you want to install.package() as repos
I'm starting Statistical Inference this week. The classes could be a lot better. I've found them very helpful because I do statistical analysis and regression modeling in R for my job. So everything I've learned I've been able to go out and apply the next day. Example: I had never used ggplot2 before taking R Programming, now it's the only graphing packing I use. Of all the classes so far, R Programming was probably the best and most useful for me. It definitely has the most practical hands-on knowledge. It won't teach you everything, but the very basics about R's data structures and how to manipulate and plot them as well as write functions. The best thing to do, and this goes for all programming, is to have a project that you are building while taking these courses.
That sounds relatively straightforward. Yes, you'll need to turn each sample into a data.frame or data.table (or just a vector if it's just a single variable). If you need to randomly sample from each sample (maintaining that you're 95% within your control limit), use the sample() command and save the values you get. Then use those values to compute whether you're within your limit. If you don't need to sample, just take the standard deviation and compare it to your limit and you're done. Check the R documentation for how to wrap all this in a function. If you're just reading in the data and checking the SD and then throwing errors if it's outside your range, then this is pretty basic (i.e. exercise your google-fu).
Thanks so much. I'm very new to R so it's all still a bit strange and intimidating, and I sometimes feel confused with my grasp of statistics. I'll try what you suggested and google the hell out of the rest. Really appreciate it!
You can also change the default repository through RStudio's Settings. Just click the "Packages" tab and there should be a place to set the default.
In my personal, humble opinion, the lecture quality is quite good, and the projects and homework are very well designed. I'm also using this stuff for my day job, doing research on transportation design and planning (bicycles in Tempe). I've thoroughly enjoyed the first three of the sequence (so far), Data Scientist's Toolbox (easy but good background if you're new), Getting and Cleaning Data, and R Programming.
Didn't know that. Good to know :-) 
Did you try explicitly installing grid?
Perhaps you should use order() instead? http://www.statmethods.net/management/sorting.html
I did not, thanks for the suggestion. I played around with it for a while and got R to cooperate. I think the ggplot package wasn't able to load completely because the version of R I was using was incompatible? not sure, but it's working now. 
Check out the qcc and SixSigma packages for examples of SPC implementations in R. Also make sure to use the right estimate of sigma for your control charts!
proper code block: abc = match(sort(ACT),ACT) gpa.clim.se = predict(gpaline, se.fit=TRUE) se.gpa = gpa.clim.se$se.fit fit.gpa = gpa.clim.se$fit w = sqrt(2*qf(.95,2,20)) low = fit.gpa-w*se.gpa up = fit.gpa+w*se.gpa joint = cbind(low,up) matplot(ACT[abc],cbind(gpa.clim.se[abc,],joint[abc,]),type="l",col=c(1,2,2,3,3),lty=c(1,2,2,3,3), + xlab="Alcohol-to-weight Ratio",ylab="BAC", main="Point-wise and joint confidence bands") can you say which of the '[' selections returns the error, or if it is matplot?
If you're only using the basic R environment, I'd highly recommend checking out RStudio which would have made this super-obvious. The right-hand "Environment" panel would have lit up with your new objects and you would have been good to go. As for the explanation you wanted: basically, R keeps objects in memory (your variables, data frames, etc) and you can store these in-between sessions of R using the save() function. The canonical example given in the manual is: xx = pi save(xx, "pi.Rdata") From there, you can share this RData file with other people and they can download your copy and do exactly what you did: load("pi.Rdata") ls() # xx xx # 3.14159... I think your misunderstanding is that you tried to do (using my example) head("pi.Rdata") which obviously did nothing for you. The reason (as you found out) is that when you run `load()`, it *restores* any of the saved objects when `save()` was called, *into* your current environment. You don't have to explicitly specify what objects to load, it just does them all. This is actually how people basically create permanent R sessions by saving their entire environment when they exit R and loading the old session when the start R.
I hope it helped!
In my dataframe, "CAT" is a column. Why do I have to put the condition in the row part of the brackets (i.e. before the comma)? Aren't I selection for column CAT that is &gt; 0?
I think this is what you want: NewFile &lt;- YourFile NewFile[YourFile[,"CAT"]==1,"CAT"] &lt;- 2 #Puts a two in the column CAT of NewFile where there is a one in column CAT of YourFile You can do the same thing for 2) by converting al 5s into 4s en not changing the 4s You can change your column names as follows: colnames(File)[,"CAT"] &lt;- "New Name" #changes column name CAT into New Name Hope this helps! Edit: Changed notation 
CAT is a column, but in your use case you're concerned with finding which *rows* of the CAT column are &gt;5 and &lt;15. If you evaluate only "pizza$CAT &gt; 5 &amp; pizza$CAT &lt; 15" it will return a vector of TRUE FALSE values, one for each row in CAT. By then plugging that vector into the row area of the bracket (before the comma), you're saying "give me only those rows of *each* column wherein CAT was &gt;5 and &lt;15." You could do this in two steps to see it more clearly: \#\# Grab row indexes of the column CAT that meet our requirements idx = pizza$CAT &gt; 5 &amp; pizza$CAT &lt; 15 \#\# Subset the *rows* that are TRUE in idx, for every column pizza[idx, ]
If I were doing this, I'd use the shell, similar to /u/collared_dropout. for file in *; do grep '\(Beginning\|End\)' $file | tr -d '[]' &gt; $file.filtered done Then in R, with `plyr` to load and combine each filtered file: files = list.files(pattern=".*\\.filtered") ldply(files, function(file){ df &lt;- read.table(file); #load the file #parse date: df$start &lt;- strptime(paste(df[,c(1,2,3)]), format="%Y-%m-%d %H:%M:%S -0500") #add filename df$filename &lt;- file; #lag so first row's end column is second row's start col. df$end &lt;- c(df$start[2:length(df$start), NA) #get rid of the ends now we've got rows with start &amp; stop: df &lt;- df[df$type == "Beginning"] df}) You can then get durations by subtracting the end and start columns, You might need to tweak the date format string.
R isn't great for big files, but if your log files aren't to large, it shouldn't be a problem to do it straight in R. readLines() reads a file into a vector of char strings you can than grep() on it to get all hits at once. Do that for both start and end assuming you haven't got nesting in the tasks, you can just match them, strsplit() or substr() the relevant part out of the strings and look at the resulting list of times.
Thought about that. Not ideal but nothing else appears to be working. 
I have never seen one larger than 10MB, and even then, that's only been once. Thanks for the info. I do like how robust R can be for these things, and am glad this isn't unattainable. The language seems really nice so far.
I've compiled R from source before on Linux (I have no idea why, but I know that I did). It wasn't too complicated. You can probably find a few tutorials. Since you are on OSX, it won't be as much of a pain as if you were on Windows. 
yeah, it shouldn't be too bad, but I am worried that the SPSS R integration package might expect something specific that compelling from source won't automatically do. 
Sorry, OldFile should have been NewFile, those are just the names I gave. And copying your original file is necessary because you need the original numbers. If you would use the same file you would change for example 1 -&gt; 2 and if you would want to change 2-&gt;3 next, all new 2s would change along. Which would result in your 1s and 2s changed in 3s. Also it's always nice to have a backup in case something goes wrong.
We use R to do statistical analysis and subsequent plotting with data from sql server. The overlap in functionality is that they can both be used to perform data manipulation and aggregation. However, given we have a large amount of data, it's easier to prep the data in sql server with views, procedures, etc., and then use R only for analysis, regression, plotting, etc. - This lifts a heavy burden for the analysts to perform that data cleansing within the confines of their PC RAM, and offloads that work ahead of time to the more efficient server. library(RODBC) localdb &lt;- odbcDriverConnect("Driver={SQL Server};Server=InstanceName;Database=DatabASeName;Trusted_Connection=Yes;") data &lt;- sqlQuery(localdb, paste("select somefield from sometable where something &gt; 0"))
Edited my reply. Thanks for the tips.
Edited my reply. Thanks for the tips.
It works rather well with dplyr. It can abstract tables for you and render queries lazily. The [database vignette](http://cran.rstudio.com/web/packages/dplyr/vignettes/databases.html) for the package has more details of how this works. 
Those won't install on Yosemite. It appears to think that i am running something older than 10.5. 
You might be able to force it with [Pacifist](https://www.charlessoft.com/). I don't have Yosemite, so I can't try it.
as far as the help says, crosstab() returns a CrossTable object, which has an entry t which is the actual table. So you just have to implement the function based on the elements of that matrix: ct &lt;- crosstab( your_data ) the_table &lt;- ct$t phi &lt;- function( x ) {x[1,1]*x[2,2] - ....} phi(the_table) 
We use R for developing the models, and then one of the engineers codes it up in Ruby using our specs. There are R gems in Ruby so we could write it into production, but it is a lot slower and we would have yet another language in the stack.
I haven't used it but I've heard good things about Shiny. I'll probably wind up using it as I'm working through Coursera's Data Scientist Specialization track. As far as how R fits into my software stack, it's one of the things we use for the Tempe Bike Count data analysis. GIS software, Excel, Perl, and other things fit in, but with a lot of people involved in the project in short term contributor roles (eg, interns from ASU), we're hoping to move towards R as a standard. It can be checked into github and shared, and run on different data without pasting the data in which results in a proliferation of versions of Excel files, which is what we started with and are still largely on for a lot of the analyses.
I have made a few simple shiny apps for some real time data displays. For example I have a few temperature and humidity sensors around the factory that send data to a mysql db using a python script. The shiny app then queries the database. But as others have mentioned it seems that if you were looking for a really robust web application you might want to implement it in a more general purpose language. That being said there is shiny server pro but I haven't really looked to hard at it yet.
My experience is similar to IR's--we house large datasets in PostgreSQL, run queries via rPostgreSQL, and run stats, generate plots, etc. in R. Also wrote a couple of R scripts that scraped web data and rearranged the information in DFs which were then written/appended to existing tables in SQL. In my experience, rPostgreSQL can be quite finicky--for example, it doesn't seem to like capitalized table names. Certainly not a perfect solution, but makes the back/forth less cluttered.
I think you might be mistaking R for K perhaps? R's notation isn't remotely as exotic. Although it supports Unicode, it's rare to use it. To answer your question, LaTeX probably fits the standard for typesetting things. Agda's emacs mode allows you to type weird symbols through LaTeX-style macros too. I don't think anyone here is likely to answer the second part of the question in your post either, although I could offer guesses that's because it's a rarely used feature; would probably piss maintainers off; and can be solved more easily with macros than convincing people to change their keyboard layout.
It's not a data set. It's a time series. The data() command simply loads the dataset into your environment. View is probably what you're thinking should have happened. ?mtcars ## format: data.frame data(mtcars) View(mtcars) ?Nile ## format: time series data(Nile) View(Nile) data &lt;- data.frame(Nile) ## coerced into data frame View(data) ## view as data frame For the phones data, you're probably looking for this; library(MASS) data(phones) phones ## show phones list in console. View(phones) ## or show phones in viewer.
with ls() you can see all defined 'variables' in the environment which the loaded datasets also are (as /u/IcredibleMouse pointed out what data() does). ?data should also give some further info
I just want to point out that // isn't a comment character in R. You need to use # to get comments.
Oh.. OK.. I was just tying that in Reddit.. didn't do so in R itself. But thanks anyways.. Didn't knew this before :)
I'm not familiar with subset function. But as a far as I understand your question. Would it be CAT!=3 the operator you're looking for?. 
I don't know. what does ! do? Subset() is a function that lets you create a new dataframe from information in an old dataframe. Does CAT!=3 remove all the rows with 3 in my dataframe?
The data-science track has been hugely beneficial to me. I do have a programming background, so that helped a lot. I'd say the benefit of using Coursera is the training mimics the real world where you have to do research on your own. They aren't teaching R, they are teaching how to work with R in the real world. If you have a specific need for R and don't really need all the statistics and "data science," you could probably just grab a book and learn it quicker. I found that having deadlines and specific projects helped. I also had real work to do with R. Like /u/scrottie I'm using R at my job. In fact, I got a new job from the work I did while taking the Coursera track. I highly recommend Coursera as long as you don't mind having to research on your own. If you don't have a programming background, that might be difficult, as the track advances quickly. 
Revolution R in my experience: - Incredibly brittle modeling functions. When something goes wrong, you get internal messages from deep inside the guts of the function, leaving you little hope of debugging on your own. Even dropping variables with variance = 0 (or close to 0) didn't happen. - Some functions had options that were documented but just DIDNT do anything at all. - Up until now, Revolution R had been based on 2.14, which was about a year or so behind current R versions - Any correspondence with them seemed like they had a very thinly stretched team of engineers implementing what the highest paying clients asked for as fast as possible; an incredibly short sighted way to run a business let along build stable software. I'd test it out and see for yourself, since it's free. Just don't hold your breath, that company heavily exaggerates what their software does. Just my experience though.
Good feedback. Thanks. 
Right. M-x set-input-method RET TeX and you can write \lambda that emacs convert automatically. 
EDIT: It seems like I can only subset a variable once? What is this? &gt;datarace0 = subset(interaction, DECSORGS&gt;0 &amp; DECSORGS&lt;8 AGE&lt;98 &amp; EDUC&lt;98 &amp; DEGREE&lt;8 &amp; CLASS&gt;0 &amp; CLASS&lt;5 &amp; MARITAL&lt;9 &amp; RELIG&lt;98 &amp; SRCBELT&gt;0 &amp; BORN&gt;0 &amp; BORN&lt;8 &amp; PARTYID&lt;8 &amp; RACE&gt;1) &gt; freq(datarace0$RACE) &gt;datarace0$RACE &gt; Frequency Percent &gt;2 148 68.84 &gt;3 67 31.16 &gt;Total 215 100.00 &gt;datarace0 = subset(interaction, DECSORGS&gt;0 &amp; DECSORGS&lt;8 AGE&lt;98 &amp; EDUC&lt;98 &amp; DEGREE&lt;8 &amp; CLASS&gt;0 &amp; CLASS&lt;5 &amp; MARITAL&lt;9 &amp; RELIG&lt;98 &amp; SRCBELT&gt;0 &amp; BORN&gt;0 &amp; BORN&lt;8 &amp; PARTYID&lt;8 &amp; RACE&lt;1) #I FLIPPED THE SIGNS OF "RACE" &gt; freq(datarace0$RACE) &gt;Error in plot.window(xlim, ylim, log = log, ...) : need finite 'xlim' values In addition: Warning messages: 1: In min(w.l) : no non-missing arguments to min; returning Inf 2: In max(w.r) : no non-missing arguments to max; returning -Inf 3: In min(x) : no non-missing arguments to min; returning Inf 4: In max(x) : no non-missing arguments to max; returning -Inf
I also need some help with this assignment, I also didn't really understand during the classes, forgot to ask for help along the way, and now it's ending - can someone do the work for me so I don't fail, because I don't really have the time to read any one of the thousands of website tutorials out there, and I'd really like to be on equal footing with my peers who actually did the work? I don't really have a specific question, since I'm not sure what to even ask, I just need the completed project. Bonus points if you can provide the testing logic as well. Thanks all.
After you do your recoding take a look at clean_int. Has it been recoded? In your subset command you're assuming that it has...
I would guess the confusion is arising where you attach and detach sets. I am not an expert by any means, but it is possible that the problem is that some objects are masked by others, and what you are recoding is not actually the race that is in clean_int.
in clean_int the column is RACE not race, therefore in datarace0 it is also RACE colnames(datarave0) should show you the names, or maybe also head(datarace0) to see if it really is empty (If I remember correctly you also don't need to attach/detach with subset)
do you actually have cases where RACE is 0 ? table(interactions$RACE) can show you how often what occurs. The recoding probably failed, you have to reassign it to the data.frame column for it to change there
how do u post code in reddit like that with that bar on the side ?
takes the possibility of a being 0.what do u think?
You want to use == and not &lt;- when checking for equality in an if statement.
rgui? u mean the software from sourceforge?i reopen rstudio and seems better. what else can i fix in the code?
Sourceforge? No. RGui is the default console that comes when you install R. At least on Windows and your paths seem to indicate that you're on windows. 
i found it. whats the difference
R Gui is the official console for R on windows. RStudio is nicer to use and adds more features (not to the language but to interacting with R) but can cause issues sometimes. Notice that the errors you have say "rstudio" somewhere in them. That means rstudio was what was causing your issue. You can look up more information yourself.
i understand. Can u see any other problems woth the code? should i put in the beginning . a=as.numeric(readline()) b=as.numeric(readline()) c=as.numeric(readline()) or is it right as it is without the as.numeric 
You absolutely need the as.numeric in there. readline will give characters otherwise which you can't use with mathematical operations. There is a simpler way to answer your question though and it is just to try it without the as.numeric in there and see if it works. Playing around with this stuff isn't going to hurt you.
i have try it but always get errors like NA's introduced by coercion and alot more others
which tells you that you can't just treat the input from readline as numeric automatically and you do need the call to as.numeric in there... 
no i mean when i put as.numeric(readline()) in there i have the problem
okay.if i source the first line only i get source('~/.active-rstudio-document') if i source again i get this Warning message: In eval(expr, envir, enclos) : NAs introduced by coercion how do i know if its right what im doing ? 
Do you have an instructor you can go to? It seems like you need a lot of help with just interacting with R and RStudio in general. This is much easier to talk about in person.
ok i will try to talk with the instructor but until then can i do anything else? seems an easy thing but cant see results, i have made an algorithm and a flowchart in order to make it easier but when it comes to making the code i dont know whats up , how i test it and what is right or wrong and where
i'm not sure what a spectral power diagram is, but this link might help: http://rstudio-pubs-static.s3.amazonaws.com/9428_1197bd003ebd43c49b429f22ea4f36e5.html
newDF &lt;- oldDF[CAT!=3,] should work. please let me know 
it looks like this has already been answered, but i would also mention that when you bring data into your workspace, it's a good idea to apply the str() and summary() functions to the data - as a way of checking it's structure and properties
This is what I do as well, find it very effective.
Like backgrammon_no said, can't really do this without more info on what you want. But if, for example, you just want names whose occurrance is less than the mean occurrance + 2 SDs (this would certainly be a tail in a normal distribution), use: df$human_names[df$occurance &lt; (mean(df$occurance) - 2*sd(df$occurance))] 
not sure how helpful this is, but the basic summary() function will give you the different quartiles. you could also use the quantile() function. for example: quantile(data$Column2, 0.95) 300.2 #gives the 95th percentile 
I think /u/R_fan covers it all. http://swirlstats.com/ is also good, if you want to look into a different tutorial (it's interactive!)
subset(x, CAT!=3) will return all rows except those where CAT=3.
use skip=7, i.e. read.table("file_name.txt", skip=7) you can enter ?read.table into the console and it gives you a help page, which explains additional parameters such as skip (this works with any function name for that matter)
Use a subsetting selection of the data. Like sum(dataset[dataset$ConditionCol=='Yes',]$ColToSum). So, dataset$ConditionCol=='Yes' gets the vector for the correct rows. Then, dataset[dataset$ConditionCol=='Yes',] actually gets the rows. Then, dataset[dataset$ConditionCol=='Yes',]$ColToSum gets the rows for that one column to sum. Finally, sum(dataset[dataset$ConditionCol=='Yes',]$ColToSum) adds it all together. Don't forget to use quotes and that comma in the brackets. Examples: sum(iris[iris$Species=='setosa',]$Petal.Width) sum(iris[which(iris$Species=='setosa'),]$Petal.Width) 
You can use the `duplicated` function to determine what is a duplicate within a vector. x = your.df$names x[duplicated(x)] Duplicated returns a boolean True/False vector, with one value for each value in your input. Then we subset into the original input vector, pulling out the values which were duplicates. A more "fun" way to do this would be to compute it ourselves: require(reshape2) occurrences = dcast(your.df, names ~ ., value.var = "names", function(x) length(x)) duplicates = occurences[which(occurences[, 2] &gt; 1), ]
this is probably slightly pedantic on my side, but I prefere iris[iris$Species=='setosa','Petal.Width'] or iris$Petal.Width[iris$Species=='setosa'] they are just slightly more intuitive for me, than getting all columns before picking one 
 num_cases &lt;- nrow(mydataframe) all_row_nums &lt;- seq(1: num_cases) rand_rows &lt;- sample(x = all_row_nums, size = 40, replace = F) df_1 &lt;- mydataframe[rand_rows, ] df_2 &lt;- mydataframe[!rand_rows, ] Does that make sense?
How does df_1 and df_2 differ in case number? I need one to be 60% and one to be 40%. Also, does size=40 refer to 40 cases or 40%? My 100 was just an example. I need 40%, not 40. Thanks for helping
is "sample" the name of the dataframe?
http://stat.ethz.ch/R-manual/R-patched/library/base/html/sample.html Edit: read link or see below. Sorry!
no sample is a function, which samples for the first vector (1:100) as many as the second nr (40) cases and replace says if you should put the cases back (can be choosen twice) or not.
The first argument is a vector of row indicies, with which you then subset the data.frame. Otherwise you subset the columns.
the function is sample() not rand()
Thanks. How do i put the remaining 60% into another vector so there are no repeats? 
And above you said its sample(), but now youre using rand()?
Shit! Edited
Yeah, sorry, it's sample()
&gt; How do i put the remaining 60% into another vector I think you mean a data frame? If so, the top code does that, with the line df_2 &lt;- ... The "!" symbol means "not". If you want to get rid of repeats, df_3 &lt;- unique(df_2) 
I have no idea about speed, to be honest I'm unsure to what degree '[' and $ are merged/preprocessed/transformed before being applied. It could be that their execution is virtually the same.
Thanks, that made sense until the last line of your comment lol. I wuld think the ! would adequately address repeats since everything that is not in rand_rows is presumably not in rand_rows (and hence no repeats)? 
I'd say it's not best to think of this as an R problem, but to think of it as a math problem. Write out the algorithm, then people who know R can help. Give us something to go on. 
I also probably should have said that I am not great with algorithms. Sorry, I just signed up for reddit. rosettacode seems to have the script for every language but R. I've never even heard of most of those though, and wouldn't know how to convert it to R. Are any of these languages very similar to R? http://rosettacode.org/wiki/Knapsack_problem/Bounded Here is some more info on the problem. www.or.deis.unibo.it/kp/Chapter3.pdf 
Instead of the aggregate function, try the colMeans(df) function. Will return the average of each column. 
Sorry, I just got around to starting cleaning my data. What does "all_row_nums" refer to? My number of rows? Like if I have 4000 rows, I would put x=4000?
df_1 worked out - I got a variable with 40% of the original df. However, df_2 with ! did not work out. I got no rows. Why?
Unfortunately, I need to find the aggregate of sections of the first column. I did figure it out though, thank you. 
No. Personal project to learn R
I know this is a bit late, but you can also use filter() from the dplyr package. filter(pizza, CAT &gt; 5, CAT &lt; 15) filter(pizza, CAT %in% 6:14) 
I wonder if all the MOOC's (on line classes with many 1000's students) in R are adding to these numbers. 
I was just about to start learning, now seems like the correct time. 
The general solution to subset a data frame *df* according to a matching pattern requires neither a for loop or any of the apply functions. The use of "which" is also unnecessary in this case (although wrapping the bracket expression in which() wouldn't change the result): df[df$position == 3, ] If you want to match a range, such as 3, 4, and 5, you can use %in%: df[df$position %in% 3:5, ] Of course, this also works for arbitrary vectors of unordered elements: df[df$position %in% c(1, 5, 3, 10), ] You can leverage a function on one column contingent upon this matching. Assuming you want the mean of a column called *response* you can do the following (notice the lack of trailing comma in the bracketed expression this time): mean(df$response[df$position %in% 3:5]) I hope this helps!
That was a mistake I made as well when learning R - have to think of it as a functional language, less in loops and more in group functions. It's rewarding though! 
Im already doing that one, thanks!
You're looking for *which*. If the data frame is called df, then: which(df$name == "") This returns the index numbers where df$name == "" is TRUE
Wow, thank you You've saved me I feel stupid for asking but Im very unfamiliar with R, the basics are a struggle
Not to ask another stupid question but is there a way I can exclude specific columns from that search? Out of the 20 columns 2 of them will regularly have missing data which is acceptable, but every other row can not have missing data
I may not understand you correctly, but... In the example I gave, the *only* column that is searched is the column "name". All other columns have no effect on the result of that expression. If you're hoping to handle missing values in the statistical sense---that is, you want to "remove" missing values from the analysis of two columns, while keeping those rows for other columns, then your problem is a not a problem with R syntax, but rather becomes one of how to handle missing values from a statistical standpoint. Or am I confused by what you're asking?
Not available for 3.1.1. Is there any other package similar to it that work in 3.1.1？
Nvm! overlooked column formatting did not correspond with colors &amp; number of dataframes (n=16, not 12)
Needs less jpeg.
Haha I get you. But how do I begin to "learn it" are there any "problems/projects" online?
Well why do you want to learn R?
Grab a data set and perform some simple analysis on it. T-tests, ANOVA, etc. It's the only real way to learn the language.
Where do you suggest looking for data sets? I'd love to learn and play around with it! Where may we find data?
I can't really speak to its legitimacy, but I've heard good things about https://www.quandl.com/
Thanks, I check that.
So there are a lot of ways to do this and copying and pasting will never be one of them... For example you could output the t.test info you want to a dataframe and iterate through your tests. Here is just a loop of 100 ttests on some random data. I am grabbing the T-statistic, df, pvalue, and the CI # Initialize dataframe t_test_df &lt;-data.frame("t_score" = numeric(), "df" = numeric(), "p.value" = numeric(), "lower95CI" = numeric(), "upper95CI" = numeric(), row.names = NULL) # loop through t-tests for(i in 1:100){ sample1 &lt;- rnorm(10,0,1) sample2 &lt;- rnorm(10,sqrt(i),1) t_results&lt;-t.test(sample1,sample2) t_test_df[i,] &lt;- c(t_results$statistic, t_results$parameter, t_results$p.value, t_results$conf.int[1], t_results$conf.int[2]) } # write to csv write.csv(t_test_df,"ttest.csv") But I must say you need to be careful for multiple comparisons when doing that many t-tests. Here is the obligatory [relevant xkcd.](http://xkcd.com/882/)
[Image](http://imgs.xkcd.com/comics/significant.png) **Title:** Significant **Title-text:** 'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!' [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=882#Explanation) **Stats:** This comic has been referenced 139 times, representing 0.3304% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cmfhwvj)
Take some or all of [Johns Hopkins Data Science Specialization on Coursera.](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage) Its a sequence of classes in Data Science with R as the featured software. Its a great way to get your hands dirty in a structured way. 
http://www.datacamp.com/ and http://swirlstats.com Both are sources for interactive lessons. 
I did https://www.datacamp.com lessons and loved them. Really cool working in a browser. 
First, separate all your "scripts" into their own functions and place all these functions into one or many R files. Then, you can use source ("MyFunctions.R") visible from other scripts. Your functions may take data as parameters and/or return some other data, which can then be used in other scripts. 
source() should do it, I tend to use it for loading functions defined in 'helper' script that only have function definitions, but it should runs all code in the file. (I don't know about passing parameters, in that case you may need to do a system call of Rscript ...) Even if you write the main calls all into one script, you can use save/load to create 'partial' applications. (and can check if the result files already exist etc.) Which brings me to my last point, if you want to do something similar more often and have scripts automatically rerun only if something has changed, you want a make system. I have started using Scons and while it took a little to set up, it now works like a charm.
Neat, although RStudio doesn't support CommandArgs so I had to wing it on that one before submitting. Definitely will be accepting some challenges. 
Personally I have been moving all of my code to use underscores, including PHP/jQuery stuff.
In R I tend to use underscore. But I'm probably not 100% consistent, I don't mind switching it up, as long as it is easily readable. (In C I use camel case, partially to more clearly distinguish in my mind in which language I am programming at the moment, so there is a natural mixture now and again if I aren't careful when switching.)
Are there more dates associated with those? Do you want the actual dates or just the length of measurement? 
thanks - i'll look into that :)
those are the only type of dates, so it wouldn't be mixing something like 2008/01/09 and 2008/2009. 
using them to make a graph with the dates on the x-axis. i would not want to have the year listed with half the value.
thanks for the replies to my question. what i ended up doing is keeping the date as a factor, and ordering the factor so it's chronological. a bit of a hack, but couldn't think of anything better.
alternative to dragonnards solution (which may lead to problems with big datasets due to %in% taking some time) another solution is to sample the positions/data rows: sample(1:length(Countries),45,replace=F) or sample(1:nrow(Countries),45,replace=F) (depending on if it's a vector or a data.frame) and then the '-' solution works. To get your sample you naturally still have to do data[test.index]
that might work - thanks. your solution is a little beyond my knowledge, but i'll work on it - thanks :)
I did something similar by randomly reordering the way whole dataset. (Basically just take as many samples without replacement as there are data entries) Then I took the first x number of entries as my sample and everything else was the second set.
This is an example of how to extract data from the CPS MORG (Merged Outgoing Rotation Groups) into R. However, I was wondering if anyone knows the best way and/or has examples of how to extract IPUMS (Integrated Public Use Microdata Series) into R. I've posted links to all of my programs for extracting and processing the CPS MORG data at the bottom of http://econdataus.com/amjobs2.htm but I'm new to R and I'm sure that they could be improved. 
[What have you tried?](http://mattgemmell.com/what-have-you-tried/)
This might be of use to you. http://gastonsanchez.com/blog/how-to/2012/10/03/Dendrograms.html
http://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/
So, this [page](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/lm.ridge.html) says to use the [coef method](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/coef.html) to get the coefficients. Both of these site, [1](http://stats.stackexchange.com/questions/23548/poor-predictions-from-lm-ridge) and [2](http://people.stat.sfu.ca/~cao/Teaching/852/Ridge.R), look like they have example code to get predictions. (ridge/lm does not have an automatic function to do that.) You should be able to get RMSE from that. I haven't actually run any code, but I hope that works.
Good point on the large dataset issue. A note that when sampling from 1 to n you don't have to use the 1:n notation. Just sample(n) is sufficient.
 this also does not work... dataframe_2&lt;-dataframe$pa[, 2:ncol(dataframe$pa)]/1000 Error in 2:ncol(dataframe$pa) : argument of length 0
In the above you're trying to divide every column for column 7 - column 997617 by 100? dataframe_2&lt;-dataframe[,7]/100 
I'm sure there is a better way to do this and I am quite the noob as well but I think this would work. dataframe2=dataframe dataframe2$pa=dataframe$pa/100
I think that with the attention R has received, that there will probably be an optimized release coming eventually. R is slow, but still amazing.
I think you should not copy the dataframe and better save the result to a list or vector because if the dataframe is too big you could have memory problems
This sounds like more of a database/design question.. I think it really depends on what you want to do with the data, and also on the memory constraints. 100,000 doesn't sound like *too* much data, are you trying to represent it in a different format in order to save space, or to facilitate further analysis? The first thing that comes to my mind, which you probably though of as well, is using a pseudo relational database. Assuming there are many duplicated terms in both genre and lyric themes, you can assign each one a unique id and have a data.frame storing the id -&gt; string mapping. Then have each row of your original data.frame store a list of ids for genres and a list of ids for themes. Hard to know the best solution without knowing the specific needs though. Hope this helps!
My aim is solely to analize the data, the structure in which I'm going to store it has the sole purpose of facilitating that task. I guess memory'll not be an issue, as the whole dataset is just a few MB. I don't know much about non relational databases, so I really don't know what to say in that regard. I'll look into it anyway, thank you! I was trying to find and efficient structure because there are so many subgenres and lyric themes that the idea of creating a dummy for each one is not feasible, I'd end up having hundreds of columns (probably) so I was trying to find a different paradigm.
for the Genre and Lyric column, you probably want a list of vectors each. Saving the individual terms either as characters, or factors, in the case of factors you have to make sure when creating them, that every vector has the full list of possible terms as levels. Then you can get occurrences by unlisting the lists and counting the frequencies and can subset your data set using an indexing like this: lapply(genre_vector, function (x) 'Rock' %in% x) Hope that helps, this might not be optimal, but it is what I would probably do (given the information you gave). Naturally you can save the lists in a combined list so you only have one variable, but you can't make it a data.frame, because they can't contain lists as columns.
Are you just using read.table? na.strings controls what gets read as NA. If it's not already at na.strings="", you could do that. You might need fill=TRUE depending on how your data is set up. Generally, look at ?read.table.
Similar to what's been said before, if you export your spreadsheet as a csv then use read.csv ("file") na's should be put in the missing places. read.csv is a sub function of read.table 
Cool, I had no idea about the I() function, that seems useful. strsplit() should do the trick for splitting if they always have exactly the same unique character combination between them, e.g. ', ' (and like most functions you can call it on a vector)
Assuming you don't really know what the rating of the different movies should be (i.e. you haven't got a table you can compare against): you can use tapply (or similar functions) to get a list of ratings per movie, then check which cases have multiple ratings, and in those cases which rating is the most frequent and therefore probably correct one. If you have a table to compare against, you can simply compare them: which(d$Rating != rating_look_up_table_with_movie_as_names[d$Movie])
&gt;Almost. I think. More so, I'd like to, given this data, determine which state has a higher number of dog/cat people. That may be equivalent to what you said, but, like I said, my stats/prob background is somewhat weak. If by higher number you mean absolute number, as in 1million people, you can do that too by multiplying the probability you get for the number of people in the state, obviously. However, given the vast difference in population among states in the us, a probability estimate could be more appropriate. I don't know the first thing about US demographics, but it could be there are more cat lovers in new york state than the whole of rhode island, even if people in rhode island loathed dogs, just because there are so many people in the ny area. by 'aggregation' i mean simply that if you feel the number are not big enough to draw conclusions on the single state, you might consider creating artificial regions (e.g. the "Midwest", I don't know) so that you have an acceptable sample size. I suggest you get the data first and then think about what you can do with it. Procedures such as aggregation are done routinely but they depend on the scope and problems of the analysis, they're seldom planned beforehand. To leave you with some pointers, look into 'logistic regression'. It might be similar to what you're trying to do.
I definitely like the idea of splitting it into regions. I'll give that a shot, too. Thanks for the great responses! If anything, I've come to realize that my basic understanding of stats is *way* low!
Since the analysis you're doing is at the state-level, having more data from some states than others doesn't "skew" your results, because states don't compete for influence on some other variable. They each have their own independent proportion. It simply makes your estimate more reliable for states that provided more data. 
If each case has their state listed, you can just do an interaction effect ANOVA for example. Or if you do a regression, simply adding the region category will control for it.
there's plenty of books that try to teach R and basic statistics at the same time; maybe you could kill two birds with one stone and look into that :-)
I'm thinking that's the best route! Any suggestions? Like you said, there are plenty out there!
Yes, there is only one point for each, because that's the total area of each land type/site. My code is: &amp;nbsp; MV2 &lt;-manova(cbind(Crop, Human.use, Natural, Pasture, Trees, Water)~Site2,data=landuse) MV2 #provides residuals summary(MV2) #this is when the error message comes up &amp;nbsp; Do you think the error message is because I don't have enough data points for each of the responses? I'm beginning to think that a MANOVA may not be the best method for analyzing this, but it was what my supervisor (I'm a graduate student) had recommended to use ...
The problem is that you only have one value per site. A MANOVA, like an ANOVA, does its inference by comparing *variance*. A dataset with only one estimate per treatment (here, site) has no variance. Therefore, you can't use MANOVA, or really any other stat that would compare the different sites. You could do a regression that would look for relationships between your numerical variables, but that would be looking at entirely different questions.
Hm, ok, not sure what my supervisor was trying to suggest. I'll probably just try a PCA or something. Thanks for answering though!
You mentioned that your single data point is a total area of each site.. are you getting that total from a large data set that you possess or is that number coming from a measurement you made in the field divided by the area? The former allows for analysis of variance etc. the latter does not. 
I have a bit of experience using GIS and I think that could be your problem. Edit: Of course I have a limited amount of info about what you are trying to accomplish. Another thing, you changed your sites to numbers but they are still factors. Changing them to integers is not the way to fix the problem that you ran into. Once again though I am not 100% sure of the assignment or what you are trying to accomplish.
there is no package called randoForest.
Assuming the typo isn't the problem and you can install them, but not load them with library, my best guess would be that during installation you selected a path to install it in that is not part of the regular paths R checks. But I think typo or installation failing are more likely.
There is one called pROC, so I doubt that is the issue
Yeah but if you notice the error message they post at the end... there is no package called randoForest
I've run into memory problems when using dget to retrieve large dput-ed objects which were solved by using dump and source, but I'm not technical enough to go beyond that.
I'm a little confused by the question. Take a look at the help for the commands you listed. If this doesn't help let me know and I'll try to help. 
Ok thanks I will try to have a look. I am quite new to R and on the online course I am following they put all these commands in the same lecture without explaining it very well..
After multiple years of using R I have never touched parse/deparse (and baresly deput/dget). So I don't think the functions are highly important. But the quick rundown in both cases is, transform x to a string or a string to x, but dput/dget works on variables (the d probably stands for data), while deparse/parse works on expressions (i.e. code). So as example if you call dput() on a vector you get a string with the "c(3,4,2)" notation. While deparse() would probably just give you a string with the variable name. (I would have to test this, since I never used it as mentioned above.)
I'm working through it right now and really enjoying it. Like tbone said, the best part is that it makes you enter the commands yourself which really helps. With previous programming experience, it shouldn't take you more than an afternoon or so to get through the introduction module, so I'd say give it a shot!
Its a great intro. take the Coursera Class , you need it for the class projects and quizes
r&gt; ?dput This is *not* a good way to transfer objects between R sessions. ‘dump’ is better, but the function ‘save’ is designed to be used for transporting R data, and will work with R objects that ‘dput’ does not handle correctly as well as being much faster. 
Right, so it says that `dump` is better (and `save` is superior), but I just wanted to know WHY dump is better than dput
http://www.r-statistics.com/2012/08/how-to-load-the-rjava-package-after-the-error-java_home-cannot-be-determined-from-the-registry/
Ha! How do you use it? 
Well, the short answer is 'yes you can probably do it.' The longer answer is more complicated. As I'm sure you already know, you're kind of using the wrong tools for the job, but I'm assuming you don't really have a choice in the matter. (If you do have a choice, now is the time to explore a more traditional database and front end solution...) Without knowing much more about the job, I feel safe saying you'll need to write some in between code to glue it together. Python is your best choice for this. So assuming you must use access and excel, you have two obvious paths: 1.) Write a python script that dumps data from access and then runs the R script on that data before importing it back to access. There are python packages that support interfacing with R. Rpy comes to mind. 2.) Write a python script that connects to access or dumps data and then run the analytical code within python using pandas and scikit-learn. I lean toward choice two because it's less complicated (though for the record it'd still be duct taped together...). In terms of maintainability, they're both bad choices, but I understand sometimes you have to play the hand you're dealt. Good luck and feel free to reach out with questions. 
thanks. And you are right, my hands are completely tied. I only wanted to stick with rather than python for analysis due to my familiarity. Rather than hand code the connection, would you know of any proprietary software/company that has the available tools? There is some money to be had possibly, and especially more for proven and supported systems rather than duct taped. But seriously Captian, thanks for your input.
I'm not aware of any off the shelf software that would solve the problem end to end, but you may be able to find an R package that connects to access to read/write. If you can find something that lets R connect to the data in Access, you could run the R script as a cron job and build in some logic that says "is this access record new since the last run? If so perform analysis else skip it." Then you could run the script every five minutes or something on the server side. So it would be completely invisible to end users. If there's no access connector package, you could write one, but to be honest, that part's over my head. So my choice would still be python. Though based on what you're saying, it seems like you've already done the hard work in R, so you're best off with option one from my last comment. Basically just writing a short script that acts as a wrapper to the R script. Then you can do the same cron approach. Hope this is helpful. Edit: found this: sandymuspratt.blogspot.com/2013/01/getting-access-data-into-r.html?m=1 Still pretty complicated but just might work using RODBC. 
Thank you for your help. when I add scale_x_discrete(breaks = PTID, labels = paste(PTID, "\n", Group)) I get the error Error in is.vector(breaks) : object 'PTID' not found I have been using both stack exchange and rcookbook they are great resources!
Look up the rep function. It repeats stuff. e.g. c(rep("Yes", times=100), rep("No", times=200)) or better yet rep(c("Yes", "No"), times=c(100, 200))
Further, you can use `sample()` if it's important that the order is randomized.
what do you want
I'm sorry I wasn't being clear. I want to have a random sketch like : https://web.archive.org/web/20130414031131/http://gallery.r-enthusiasts.com/graph/Smily_and_Grumpy_faces_174 or https://web.archive.org/web/20130620090453/http://gallery.r-enthusiasts.com/allgraph.php?page=2 I'm trying to understand how to do an "artistical" drawing in R so I must have so example with source code. I hope now it's all clear
I just signed up for fortnightly data challenge from teamleada.com, where a data set is provided with problems to solve 
There's a decent article on r-bloggers about web scraping articles. I'm on way to work, I'll try to post the link later today. 
the binwidth default is lumping them together. hist(space, breaks=c(0,1,2,3,4,5,6,7,8,9,10))
That would be great! Thanks!
This was the article that I initially looked at - the problem I ran into was that it just scrapes the first paragraph, but I want to get all of the text published by a particular author on the Times' website. Thank you very much though!
Haha I guess I'll have to learn some Python! Thanks!
No problem, I am 100% sure it works, keep on coding.
Why are you using attach and could you post the full code (ideally completely reproducible, so including a dummy datatable, you can get that with dput), because if you call dotplot(datatable$column) you shouldn't need the attach anyway. Also if you are using the dotplot from ggplot2, I think it takes a data.frame and not just a single column of the data.frame (i,e. a vector).
for this example you can just do two individual calculations by hand using (assuming the column has 'male'/'female', otherwise use 1/2 or what you use) male_data &lt;- dater[dater$sex == 'male',] abline(...) female_data &lt;- dater[dater$sex == 'female',] abline(...) or you could use a for loop: for(sex_type in c('male','female')) { data_subset &lt;- dater[dater$sex == sex_type,] abline(...) } EDIT I used '=' instead of '==' which is of course stupid
`dotplot` is coming from the epicalc package. Here's some sample data (`data.txt`): gender major siblings hair F CS 2 2 F MathGer 1 24 F Chem 1 12 F IA 1 14 M CS 2 1 M BCMB 0 .5 M CS 5 10 M BCMB 0 1.5 M CS 1 0.4 M Math 3 1 F CSMath 1 12 F Math 3 24 F Math 1 18 F ENVS 1 15 F BCMB 1 20 F CSMath 1 9 Here's the code, as it was provided to me: require(epicalc) datatable = read.table("data.txt", header=T) attach(datatable) counts = table(major) dotplot(counts) That produces the wonderful error I'm concerned with: `Error in dotplot(counts) : object '.data' not found` In fact, it seems to mess up the current session's environment quite badly, e.g.: &gt; attach(datatable) &gt; dotplot(table(major)) Error in dotplot(table(major)) : object '.data' not found &gt; dotplot(table(datatable$major)) Error in dotplot(table(datatable$major)) : object '.data' not found And of course, doing it without `attach()` has no problems whatsoever: require(epicalc) datatable = read.table("~/Desktop/School/College/2014-2015/MATH-255 - Statistics/HW02_Math255.txt", header=T) # attach(datatable) counts = table(datatable$major) dotplot(counts) So, what's the deal? Some bad assumptions in `dotplot()` about the current scope/environment? Or something in `attach()` messing that up?
Hi, thanks for the reply. I'm trying to use the first bit of code you supplied but am getting an *unexpected '='* error. The only thing I changed was 'male' to 'M' because that's how I denote sex in my dataset. Please excuse me in advance if I'm doing something stupid-- I'm still learning with R.
stupid me, sorry it naturally should be == as a comparison, not = as an assignment, see the EDITed version
great job!
Ah - now I get it. You could do this with plot(), if you play with settings and functions. Or is there a special package for this application? EDIT: Found some aRt: http://accidental-art.tumblr.com/
Yes something like that! But I can't see any source code :(
I think that [tidyr](https://github.com/hadley/tidyr) (written by Hadley of course) is the new way of reshaping data. `reshape2` might have a few more functions and be more powerful, but for the common reshaping operations that are used 90% of the time, `tidyr` is simpler and is the new cool way as far as I understand
If you are only opening a few datasets, the gui in Rstudio makes importing files really easy. There are also ways to import directories of files. As for the slash issue, Notepad++ or other text editor and replace may be the best way to go if its repetitive.
there are several ways to transform R tables to markdown http://stackoverflow.com/questions/15488350/programmatically-creating-markdown-tables-in-r-with-knitr
Ahh, good idea. I figured it was some poor assumption on the part of the author but I don't know enough about R yet to determine exactly what. Thanks.
thanks. i'll see if it works.
Makes sense. I'll try it out. I was more curious about generating that exact plot though. What mathematical functions produce those data point patterns? Anyways, I *think* I found what I was looking for. Might need a little tweaking, but it's close enough. Thanks for your help!
you have to set the block outbut to something specific (asis?) instead of the standard, so it doesn't get wrapped in a code block or is completely silenced.
I use the kable function from knittr all the time and it usually works great. Looks great in markdown. 
Thank you for sharing!
Can you explain what you mean by "but there is no option for authentication in opensource"?
Is it good idea that I start to make open source R web framework like shiny include some shinyapp.io features?
Call png() before you call plot(), do all your plotting, and then dev.off()
Worked perfectly, thanks mate. Can you tell me what it is that dev.off() does? What is meant by the 'device'? 
I don't think that's possible. To help with your googling you should know that in R you should use the term "object" instead of "variable". 
This is what I have come up with and it works. Open to other (more elegant) suggestions. my Solution amounts to a brute force method and not very transportable. #Make a subset, assign 1 factor variable, combine back together with rbind. sporty&lt;-subset(cardat,sporty==1) pickup&lt;-subset(cardat,pickup==1) suv&lt;-subset(cardat,suv==1) minivan&lt;-subset(cardat,minivan==1) AllWD&lt;-subset(cardat,sporty==0&amp;pickup==0&amp;suv==0&amp;wagon==0&amp;minivan==0&amp;AllWD==1) wagon&lt;-subset(cardat,wagon==1) sedan&lt;-subset(cardat,sporty==0&amp;pickup==0&amp;suv==0&amp;wagon==0&amp;minivan==0&amp;AllWD==0) sporty[,"type"]&lt;-"sporty" pickup[,"type"]&lt;-"pickup" minivan[,"type"]&lt;-"minivan" suv[,"type"]&lt;-"suv" sedan[,"type"]&lt;-"sedan" wagon[,"type"]&lt;-"wagon" AllWD[,"type"]&lt;-"AllWD" cardat&lt;-rbind(sporty,pickup,minivan,suv,wagon,AllWD,sedan)
Thanks I will look into this later, since I was in a hurry for this project I have done a somewhat brutish method :(
with Sys.sleep() you can sleep for x seconds and with Sys.time you can get the current time, so you can plug together several variations (wait x min after you finished the last one, check every ~min if it is time again etc.) you can also add rows with rbind(), but generally repeatedly extending a data.frame (or a matrix,vector,...) is a bad idea in R, because the whole thing is copied into a new memory location ever so often. So ideally you either pre-create one and then fill it up or only ever look at the new stuff. Given your time frame, this probably is less of a problem in this case, as long as your data.frame doesn't consist of millions of rows (and/or hundreds of columns). ps if you are on linux you can also set a cron job to run your Rscript every half hour
The lm has no 'knowledge' of time, it just tries to predict one value given several others. So in the book example, y given u,v,w. The numbers for u,v,w are just made up for the example. In your case you would use the numbers that your forcast tells you. I.e. for 2015: predict(m, newdata=data.frame(worldRealGDP=121.19, USCrudeProd=9.53, BBL=71))
Thank you! I have a virtual machine on my Mac that runs linux, but I haven't had my Mac - it's in my desk and I'm lazy haha! I took your suggestion and built the Sys.sleep() command into an infinite for loop and it's working beautifully :) Thank you!
What do you mean? Can you prove some more detail and add a little context for what you're actually trying to do?
if I'm getting this right I would probably create a matrix to save the results: results &lt;- matrix(0, nrow=length(race), ncol=length(horse)) for i in 1:length(race) for j in 1:length(horse) results[i,j] &lt;- MonteCarlo and than you can use which.max() on every row to get a vector of the winners on which you can then use table() to get the win frequencies of the different horses
 Error in UseMethod("predict") : no applicable method for 'predict' applied to an object of class "formula"
I'm not sure how your model is an object of class "formula". My model is of type "lm". Here's exactly what I did. 1) Copy your table into a text file, w/o the headers. 2) Run the following code: data = read.csv("path_to_text_file", header=FALSE, sep="\t") names(data) = c("Revenue", "WorldRealGDP", "USCrudeProd", "BBL") model = lm(Revenue ~ WorldRealGDP*USCrudeProd*BBL, data=data) predict(model, data.frame(WorldRealGDP= 115, USCrudeProd = 7.5, BBL=95 )) Which returned a value of 3558.896.
What about this: I could make unique race identifiers by concatenating Track, Race Number, and Race Date. Could you walk me through the syntax of going from a dataframe with unique races, numbered horses with no repeats per race, and a predicted value, to a list of races, where each race contains all of the numbered horses, and each horse has it's predicted value as a property. Then I could say: for each race sim = matrix(NA, nrow = 100,000, ncol = length(horses)) for each horse sim[, j] = monteCarlo(predicted value) for each row in sim Results = which.max() I don't think I used which.max() correctly, but the pseudocode seems to work. Does that make sense to you? Can you help with defining the objects that I need?
Row 415 looks like this: sporty suv wagon minivan pickup AllWD 415 0 0 0 0 1 1 So they are not true dummy variables. If they were [this link here has some elegant solutions] (http://r.789695.n4.nabble.com/R-how-to-convert-multiple-dummy-variables-to-1-factor-variable-td810654.html).
&gt; The problem I see here is that MonteCarlo isn't a value, it would be a vector of maybe 100,000 values. That's why I originally had each race in it's own matrix, and the which.max() argument would have to be in the loop. Then I read your initial description wrong, so you will simulate the performance of every horse in every race 100,000 times? In that case I would only save the accumulated outcomes of the simulations and though away the individual values as soon as possible. I.e. for every race: Montecarlo for every participating horse, and then something like table(apply(sim_matirx, 1, which.max)) and save that in some matrix/data.frame &gt; Also can you explain how the for loop would work exactly? I thought the horses where the same in every race, and with length(race) / length(horse) I meant the length of unique identifiers, not the length of the column in the data.frame you described.
You can drop columns from your data.frame when reading it with read.table or later (if you resave the data.frame without them e.g data_frame[,-1]) or you can use factors (which R automatically does most of the time) which makes handling vectors with only few possible strings very fast, because they are internally encoded as a set of integers with names.
In R you run the iteration variable over a vector of numbers, so for(i in 1:10) runs over the numbers 1 until 10, but for(i in 10) only runs for 10, so you want to add '1:' in your for loops for (j in sum(raceID == raceID[i])) { that's not what you want, for starters you should index unique(raceID) and if you want specific monteCarlo simulations per horse, you might need to get the horse id instead of 1:amount_of_horses for (k in length(sim)) { nrow, ncol or dim for dimensions of a matrix winners = c(1:sum(raceID == raceID[i]), which.max(sim[i,])) you want k not i at the end and I don't know for what you need the start, but my guess is that it is not ideal. If you added it for the minimum one win, you can give table a vector of possible outcomes and that should then also lead to 0 entries, if I aren't mistaken results might not be to useful the way you save it, a list of vectors might be more convenient, but that depends what you want to do later.
&gt; Why should I index raceID? As a sidenote, they are ordered in my dataframe, if that matters. if the first two entries in your DB have the same raceID, raceID[1] and raceID[2] will be the same ID, even though you want the second unique race id in the second case or? &gt; sum(ifelse(sum(raceID == x + horseID == y) == 2, 1, 0)) sum( (raceID == x) &amp;&amp; (horseID==y) ) does the same, but as far as I can tell, sum(raceID == x) should give you the amount of horses in the race, and ye could pre-compute all of them beforehand with table if you wanted to. But my specific point was, if you need the horseID for horse specific MonteCarlo simulations, you will have to do it differently (if the MonteCarlo simulations aren't horse specific I'm unsure why you need all the raceID/horseID at all) I told you an alternative (how I would do it) to your solution of the 'zero win' problem, take it or leave it. 
Yeah, I didn't know what you meant by "index", but I realized that eventually. I went with making a temp dataframe that's a subset of my full df where rID = rID[i]. The horse matters, but like I said, as long as everything stays in order, I can link back to my data so it shouldn't matter. "results" isn't really the end product. It has to go back to the original dataframe, otherwise I'd need to keep track of a lot more. I'm still not sure if my code actually runs yet, but my original question is more than answered. I have a much better understanding of what I'm doing. Thanks!
/r/homeworkhelp
Are you trying to avoid reading in the whole file because it's gigantic? If that's the case, there are solutions for fast filereaders - look at `data.table`'s `fread()` or Hadley's `fastread` package
additional to what deanat78 said, read.table also has a some parameters that might help, in the colClasses vector you can set "NULL" to skip the columns and with skip and nrow you can skip part of the start and then read nrow rows. But generally the idea in R is that you load all the data into your R session, and then run the stuff on the data in R. (Just realized: if the problem is subsetting a data.frame look at ?'[' basically you just comma separate the dimensions or use data_frame$column_name to get the column vector)
Assuming your matrix or data frame is called "data" you could: apply(data, 2, mean) Or you could achieve the same result with: colMeans(data)
Hey bud. Interesting question you got there. Unfortunately, you haven't given me enough information for me to help you. How much dough you willing to pony up cowboy? What kind of cattle you be wranglin'? Answer them questions and I can help you out. 
RAM is crucial in my experience. My i5 (8GB, win8.1) can run R analyses that my coworker's i7 (4GB, centOS linux) can't. We both have hybrid SSD-HDD.
I'm actually using an alpha of 0.1 for both. kruskalmc(proportion,order,probs=0.1) The results of this test show that there is no difference in the distributions. condition&lt;-kruskal(proportion,order,alpha=0.1,p.adj="bonferroni") The results of this test suggest that one distribution is different from the rest. Does kruskalmc() adjust the p-value for multiple comparisons and could this be why there is the difference?
I'm not sure. Have you read this? You might be onto something with the p adjust. http://r.789695.n4.nabble.com/Multiple-Comparisons-Kruskal-Wallis-Test-kruskal-agricolae-and-kruskalmc-pgirmess-don-t-yield-the-sa-td4639004.html
Have you checked out "R Programming" from coursera?
Look into `ggvis`, not sure if that's what you mean
there is a way to do this with rCharts: http://ramnathv.github.io/rChartsShiny/
An hour is an incredibly short amount of time when you're teaching - you'll be amazed at how little you'll feel like you can cover. You can't teach somebody how to program in an hour. What you have there seems like it would take more than an hour if you wanted to actually teach it. Instead I would suggest that you focus on two things: 1) Show them how to do something they want to do in the language. Something that is cool. Actually get them to do it too. Answer questions along the way but don't worry about *all* of the details because you don't have the time to worry about all of the details. If you show them how R can help them then they'll take the initiative to learn the rest - which brings me to my next suggestion 2) Teach them how to teach themselves. What I mean is that we all get stuck. Make sure you show them how to find help and how to solve problems themselves. Google is great but it's hard to google "R" - teach them the best ways to search for the answers so when they are super excited about how awesome R is and want to learn it more they have the tools to do so. Don't drone. Be excited. One of the best ways to do that is to teach exciting stuff. One day they might care about the intricacies of apply or how logical indexing or vector recycling works but trust me - when they're first being introduced to the language isn't the day they care about that. Vectorization is a very important topic for R programmers but it isn't something that they need to know in the first hour. Or at least it shouldn't be if they aren't programmers.
While you may not have time I think data frames are one of the best features. This seems like an ancillary feature until you consider how much time everyone spends preening data (data scientists, general analyst, or statistician)
The run button only runs the current line where you wrote `num`, (or the lines currently selected) rather than the entire script. Pressing Source will run all of the lines.
basically you either have to see source(code_file.R) or all the code copied into the console, otherwise you are only running the part which is echoed in the console
Ok, thank you. Thanks to your keywords I found exactly what I meant: [link](http://www.improving-visualisation.org/vis/id=301). I'll go from here, but now that you or anyone who reads this know what I mean, the question still stands. I'll start looking through ggplot to see if I can accomplish this, but I'd love to hear about any methods or packages that others would recommend. Thanks for the help!
If you get stuck check out Show Me the Numbers: Designing Tables and Graphs to Enlighten by few. 
I agree with the top response. Its hard to do something in an hour. Perhaps show them a cool demo which would be in their field and then show how to get help in rstudio followed by some places where they could learn more on their own. Tryr.codeschool. com and datacamp.com comes to mind.
Can u tell me which one? I found ones that go to latex but this doesn't fit my needs. The medicine world doesn't use latex. 
I read this in the morning from my device which isn't logged into reddit. So I get to see the frontpage that anyone can see. I just realized today. That the subreddit data is beautiful is chalk full of statisticians, analytic nerds, and data scientists. http://www.reddit.com/r/dataisbeautiful/comments/2y1n3q/users_of_stata_and_r_give_more_snarky_comments_on/ and this was from yesterday before I left work. I'm really interested in combining the powers of R programming and excel, then eventually pick up python. http://www.r-bloggers.com/export-r-results-tables-to-excel-please-dont-kick-me-out-of-your-club/ Here's a real interesting spinet I sent to a coworker whose not familiar w/ R, we work in supply chain and often analyze the results of forecasting based on time series analyses. http://web.warwick.ac.uk/statsdept/useR-2011/abstracts/040411-millo_ortolani.pdf *snipet
I don't know how much this would help with job prospects, but I've got a CompSci background (including lots of work with SQL like you), and I'm taking and thoroughly enjoying Coursera's _Data Science Specialization_: https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop 
I took the whole sequence including the capstone and whole-heartedly agree that it was very informative and made me much more comfortable working in R. Depending on your background you may need to do a fair amount of Google/stackoverflow and read alot of the suggested readings. However I am not sure how widely recognized it is at this point. That being said even if you don't want to spend the $500 on the verified certificate it is definitely a great resource for an introduction into a lot of different aspects of R and data science and highly recommend it. 
and head() prints the first 10 rows by default... you have to specify your result (or data) more specifically if that isn't what you want
If I have a 6gb csv, how can I Import the first 25 rows without using read.csv and loading the entire csv? It's way too memory intensive to load the entire csv And I DONT know the number of rows in the csv
yup figured it out. i was angry i was like wtf why is this taking so long. little did i realize it was loadign that damn csv into memory
I'd also ask...is there a way to randomize that nrow=25?
Randomize in what sense? Just put a function there instead of a number. nrow=sample(30,1) would make in a random integer in [1,30]
&gt; Read is by design sequential, but there is skip parameter(listed just after nrow) to "move" to selected row(e.g. skip=100,nrow=5 would read rows 101-105). Have you looked into the documentation? To get totally random subset of rows without loading the file you'd need to read them by one and rbind, but it sounds like a bad idea. 
in pseudo code you would have something like: add_combination &lt;- function( elements_left ) { new_possibilities &lt;- get_possibilities( elements_left ) for(possibility in new_possibilities) { results += possibility * add_combination( elements_left[ - elements_in_possibility ] ) } return( results ) } So you only have one instruction, but you call it recursively, at some point new_possibilities will be empty and the function returns nothing and then it back tracks creating all combinations. get_possibilities basically computes all possible one sandwich solutions given a list of ingredients (ie. uses combn() ). Now recursion tends to not scale very well, because you are creating lots of function calls and (if you aren't handling it cleverly) lots of duplicate computations. One way to remove duplicate computations is to create an order for your sandwiches and only allow adding of sandwiches that are 'below' the sandwiches you already have (you then also have to pass them to the function) PS in the pseudocode += means add and * basically all combinations of the two. How to implement them depends strongly on the information you need and how big the data is (i.e. how much you have to care about memory intensive operations)
Can you reword what you want to do? It's not clear to me. Maybe provide more sample input and expected output. I have a hunch that what you want is just `your_func &lt;- function(x){max(rle(diff(x))$lengths)+1}` and then you can use it like `your_func(A)`
Not a very useful post - people aren't likely to do this with 0 explanation/rationale
You are looking for nchar() You get the vector you want with: x[nchar(x) &lt;= 3]
Try using the nchar function. &gt; x[nchar(x)&lt;=3] [1] "aaa" "bbb"
Sorry, my initial question was pretty poorly written. I wrote it on my phone before I went to bed, aha. I'll give you more context: basically I have a data set with 2 groups, profitable loans and unprofitable loans, and I have certain variables one of them being for example home ownership. I want to do a type of histogram for profitable and unprofitable loans with the bins basically being the levels of my home ownership variable. But I have so many more observations in one group than the other so its difficult to compare the two groups right now. This is the code I have right now if it makes it easier to see: phome=ggplot(loandata,aes(x=home_ownership,fill=as.factor(profitable)))+geom_bar(aes(y = (..count..)/sum(..count..)),position="dodge") http://i.imgur.com/nS7mudM.jpg This is for a different variable but notice how overall frequency for both groups sums to 1 but not for individual groups which is what I desire.
If you want it to sum to one, maybe refactor your data set so that they are percentages instead of counts. 
I can send a sample assignment at request.
http://learnxinyminutes.com/docs/r/
Ohhh... So we're interested in returning the set of all permutations of size m of the elements of the list 0:n. Let me take a shot at that instead. 
I really *really* **really** implore you to see if you can solve your problem using combinations instead of permutations. I think these two benchmarks really say it all: &gt; system.time(combs(100, 100)) user system elapsed 0.01 0.00 0.01 &gt; system.time(perms(5, 5)) user system elapsed 0 0 0 &gt; system.time(perms(6, 6)) user system elapsed 0.02 0.00 0.02 &gt; system.time(perms(7, 7)) user system elapsed 0.49 0.00 0.48 &gt; system.time(perms(8, 8)) user system elapsed 3.33 1.28 4.89 &gt; system.time(perms(9, 9)) Show Traceback Rerun with Debug Error: cannot allocate vector of size 3.0 Gb Timing stopped at: 5.18 1.78 7.16 &gt; system.time(perms(10, 10)) Show Traceback Rerun with Debug Error: cannot allocate vector of size 5.6 Gb Timing stopped at: 2 1.1 3.11 If you can, combinations are **significantly** easier to work with than permutations.
assuming by data set you mean data frame; try this data[which(data$score != 'Zero'),] 
This did it! Thanks!
Lol thanks for the suggestion =) Amusingly, that's the course I just finished that made me realize how much stats math I've forgotten...in any case, a GREAT recommendation!
I tend to keep a copy of Zar's closeby; if you can find a used copy, that might be a good way to get back into stats. I took side-by-side courses in R and Stats, using fake data to get through some questions.
I get most my R news from http://www.r-bloggers.com In chatting with developer friends of mine they usually just find R an odd language to use. It doesn't really address the needs of most developers who are not data scientists and even then other tools will often make more sense. In academia there are still tons of people using SPSS and SAS and they usually find R interesting but not worth the time to learn. As far as this sub goes, not sure why it isn't more active.
I agree that it's odd that R isn't used more often. At my university at least, R is the basic program used for all stat classes, so many people are familiar with it. I think the problem is that non-programming people don't want to go through the trouble, and programming people get more utility from other languages with packages (e.g. python with Numby, which was written to mimick R in python) since they will know both data science and non data science programming. Then there are people like me, who sucks at programming, but for some reason finds R easier to use than STATA or SPSS. As for academia, this is the sense I get from talking to academics and seeing what researchers use. It seems like the harder scientists are moving to python and C+++, since it helps them model better (I think other languages are better at modelling simulations). The more mathematical social scientists (e.g. econ) are moving from STATA to R (I was advised to learn R, since "All the new people are learning R"). And the softer social scientists are switching from SPSS to STATA. R is fairly popular in the finance industries, with major quants and even the Federal Reserve using R
If the organization/group already uses matlab or SciPy it might be better to stick with those tools instead of using R
I work in healthcare reimbursement and R/shiny is like a godsend for me. I deal with thousands of files from dozens of different data management systems, each one unique, and we have a very real need for our tools to be web-enabled. Python worked great, mainly because of Pandas, but even R/shiny makes Python/flask look like a maintenance nightmare from the PoV of how quickly we need to turn data around. Imagine being a data analyst and having to write javascript/css/html as part of your workflow. It's fun but it's the most wasteful, counterproductive use of time in the world. And pandas can be an absolute nightmare for importing raw data into an RDBMS, mainly because of how it insists on type inference. The hacky type inference part of Pandas is so bad. Not everything that has numbers and a decimal point is a float, and I don't have time to sit there and hold its hand and define hundreds of unique columns to it. dtype doesn't even work with fwf in pandas, have to run zfill transform application on every row that doesn't want to cooperate. I thought part of the Python philosophy was that "explicit is better than implicit." The job I inherited was done in hand-rolled PHP. Imagine that. After turning literally thousands of lines of PHP into hundreds of lines of Python into dozens of lines of R I'm about to go get a freaking R tattoo. It's like bringing a panzer to a snowball fight, every other firm in the region is still using Excel.
I use a lot of Python and R at work, in my opinion R's syntax can be a little off-putting to the uninitiated. That and R's object-orientedness is a little disoriented, with S3 vs. S4, no easy way to determine methods for an object (like Python's dir() function), etc.
It sounds a bit silly but I think one thing that really doesn't help it is the name. It's very difficult to search for "R". Python, C++, visual basic etc. are all very easy to type into google and find basic results for. It's always just that bit more awkward to find stuff on R simply due to the awkward name. If I type C++ into the search box on reddit, I get a dozen relevant subforums. Typing in R, or R Programming or R Language doesn't get me anything relevant to R.
I come from object oriented software dev. R is just a lot different (syntax and paradigm) wise compared to all the languages I have used in the past. I gravitate towards python because it feels more natural and in tune with how I think using other languages I like R for traditional statistics, producing diagnostic plots, etc. But would prefer to use python (or java, c#, etc) in general as a language. Shiny looks awesome, but I haven't had a chance to use it yet. 
"rlang" sometimes helps
If you've ever gotten deep into web development with frameworks like AngularJS, d3js, and writing RESTful endpoints, then Shiny would probably completely change the way you look at R. No other language has the kind of web framework that R does.
Fair enough. I guess it depends on how popular it becomes.
I think [this is a good post](http://www.johnmyleswhite.com/notebook/2013/12/22/the-relationship-between-vectorized-and-devectorized-code/) talking about R performance considerations. The R design patterns of using vectors and dataframes makes a lot of sense for data science. It can be an easier way to think about data and doing work when you are performing analysis on a somewhat specific problems. However, if the problems you are solving are less ad hoc and more generalizable than what a data scientist is typically doing, then the tradeoffs that R makes in terms of performance for the sake of making it easier to think about the data probably are not worth it compared to a general purpose language. I think I actually answered the opposite of what you were asking. However, think of it this way—while Fortran is used for scientific computing and was likely used to write whatever BLAS you are using, that doesn't mean that you won't conserve a lot of effort and brainpower by simply loading the library then using a higher level language instead to address the analysis task you are working on.
Sure, but you might want to include, say, the name of the package chisq.power lives in ;)
Why? I even thought about writing a front/backend system (in PHP, or maybe Node for sockets) that communicates with R for outside of R. I have written some Shiny apps, but the more I work with them the more I wonder.
I know very little about programming and all the jobs I'm looking at (data related ones) mention R, so to me it seems pretty popular and a language that will hopefully earn me some extra $$$$
I would recommend at the very least using the Google Charts package for working with Shiny. When you said you were working with MVC, it sounded like you were actually doing a lot more than some dynamically generated static outputs... like building an interactive dashboard with NVD3 or something. That is something I have currently on the back burner for a project, and from what I worked out, it just seems a heck of a lot easier to connect PHP to R to build the interface and dump the JSON. As far as having to type more code, it doesn't take very long. You can build your own microframework if you work on similar projects. 
Uh... that guy's "vectorized" benchmark looks a little bit wonky to me. vectorized &lt;- function() { a &lt;- c(1, 1) b &lt;- c(2, 2) x &lt;- c(NaN, NaN) for (i in 1:1000000) { x &lt;- a + b } return() } That is not vectorized in any sense of the word. It uses a fucking loop. And he complains that it takes 0.49 seconds per iteration? Maybe that's a sign that one should use a **vector** instead of a loop. actually_vectorized &lt;- function() { a &lt;- rep(1, times = 1e6) b &lt;- rep(2, times = 1e6) x = a + b return() } time &lt;- function(n) { timings = replicate(n, system.time(actually_vectorized())) } mean(time(10)[3, ]) # average elapsed time is row #3 Mine takes 0.1 seconds on average.
Yeah, making a dashboard that will allow me to interact with a rules engine for parsing, row/col validating, and db imports of client files. I consider it a microservice since it's just going to do one thing very well: file imports of messy data. The part where shiny really shines is with creating the rule sets (what defines a "good file", fwf column definitions, table header definitions). All reactive so you can basically sample a file, craft a rule around it and get instant feedback that your rule is correct, then apply that rule to however many hundreds of files are in the same layout. Just picture the JavaScript alone that's behind an MVC application like that, I don't have to write a single line of it. ^(^I ^also ^suck ^at ^css ^layout ^and ^design.)
No experience with RDBMS analytics, just whatever MySQL offers and a little bit of Postgres (scripted stored procedures, json, arrays, all seems so nice). When work slows down we're going to experiment with migrating to Postgres. &gt; preferences between doing stuff in SQL vs R? It's complicated and it depends. My background is in software engineering so I see everything in terms of trade-offs. I love them both, though. From a query perspective I have no preference per se, as I see them both as different but equal. Nothing compares to SQL when talking about tabular data. It's easy to read, write, maintain, it's powerful, expressive, and it's the de facto standard bar none. If I had to lay it out without writing a 10 page essay I'd say I want to do **as much as is possible in R**, taking complete advantage of rstudio, ggplot2, rmarkdown, shiny, data.table, etc. while still maintaining our high level yearly mission-critical state law business logic type stuff (the pulse of our firm) in pure SQL for ethical reasons (ie: replacing a programmer analyst is a lot harder and more expensive than finding someone who knows SQL). I think a lesser, more selfish person, would do everything in their pet language and use that as leverage to extort salary and retain their position. I know for a fact that R developers do not exist in this area for even twice what I'm being paid, so I'll be grateful for what opportunities I'm being given to grow while still keeping the best-interests of my colleagues in mind. Plus, while I *have* found a **dramatic** reduction in lines of code by switching from other languages to R, I do not foresee the same thing happening by switching bulk SQL to data.table without obfuscating the pure business logic into jumbled nonsense. SQL logic is already as pure as it gets for tabular data, this is the same reason why I am vehemently against ORMs.
Really excellent. I don't find much use for these sorts of plots at work, as they're a bit much for the average business user, but as chart porn they're great. Have you reached out to Hadley and co about incorporating this into ggplot as standard?
Thanks! I haven't, I'm fairly new to R and kind of intimidated by Hadley and the other great R wizards.. thanks for the encouragement though
Anytime, thanks for contributing to R. I'd imagine a more thorough solution would involve treating this as a special case of faceting, in which case you'd be able to piggyback on the existing treatment of titles and axes. If you're serious about pursuing this, you might consider inspecting the ggplot code itself and see if you can get something like a ggplot::facet_margins call working in a branched version of the ggplot2 codebase.
That's a good idea I haven't though about, I might look into that when I finish a few other packages I want to get out
Should the line &gt; "dist.prim.quad=(cts.new$dist.prim)^2" be &gt; "cts.new$dist.prim.quad=(cts.new$dist.prim)^2" else it doesn't look like you're saving the quadratic term to the dataset so "zeroinfl(...data=cts.new...)" doesn't tell R where this variable is. 
You would do something like this. Sum((x-y)^2) Just using the caret. There's no exponent
whoops. I mean sum((x_i - y)^2) for i = 1....n
Yes you can use sum((x-y)^2) to get the sum of the squared differences between x and y. You can also always just give it a try and see if it works.
I presume you're out of memory. Have you tried using something like [R-ODBC](http://cran.r-project.org/web/packages/RODBC/index.html) to access your data? It takes all the "heavy lifting" outside R and makes the whole process a lot quicker and more efficient.
I actually used fread and it worked fine!
Have you tried using dplyr yet? That should give data transformation a similar feel to SQL and make things a lot easier. The base R syntax is kind of a nightmare... but you get used to it. I guess I would normally use SQL to join and filter larger tables until I had the data I wanted and then switch to R. I think that if you are joining large tables in R you might be pulling the data in too early. I really don't join much at all by the time I'm in R. Once you get used to R you can transform the data very quickly... the greatest strength of R, for me, is how much you can do with very little code. Definitely took a few weeks to get used to. You could do everything in SQL and pick the data up in R at the end for visualization... I know some people who do that. You'll find the balance that works for you the more you use them together. I wouldn't sweat it too much.
SQL is used to manipulate data, while R is used to actually do stuff with the data (analysis). They aren't interchangeable but can be used together. &gt;Honestly, I'm a bit disappointed. It's kinda slow Usually run time will not be a huge issue with R, since it's for backend analysis. .05 sec vs. 5 sec is not a big deal when you're doing it 10 times per day. Even then, there's more efficient packages than what's included when you download R. &gt;whole "vectorization" stuff is not my thing Have you taken linear algebra? That might be a good course to take in conjunction with learning R. Vectorization is much quicker/efficient than a loop. &gt;Anyway, I enrolled into coursera data scientist course, mostly due to machine learning part, and am currently learning R. This is why you learn R, since you can do machine learning algorithms with 1 line of code.
I am pretty new to r but also come from an SQL background. At the start I did all my data munging with sqldf. It was great with WITH support etc (SQLite syntax). I am slowly starting to use some R syntax (esp all the apply/melt/plyr/dplyr stuff). I went into r to use it for some data visualisation stuff (ggplot2 shit you can't do in excel) and getting into exporting PDF inforgraphics into illustrator to look pretty lol but the fact I can go back to sql (using sqldf) when something becomes too complex/hard/no time to google and fuck around is a godsend.
you can use grep() and related regex functions in conjunction with directory and file navigation functions like list.files() For example: VectorOfFiles = list.files(directory/path/) VectorOfFilesIWant = VectorOfFiles[grep("pattern")] https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html http://rfunction.com/archives/1042 
Im probably rated a novice at both, but I find R is better when I need to clean unstructured data, and when I need to analyze the data. (i.e. "let me send you 10 100k line excel spreadsheets and then you can make a business recommendation on what we should do")
Using the dplyr and lubridate packages it would look something like: library(dplyr) library(lubridate) data$checkout_datetime &lt;- ymd_hms(data$checkout_datetime) data %&gt;% filter(month(ymd_hms(checkout_datetime)) == 5) %&gt;% group_by(year(ymd_hms(checkout_datetime))) %&gt;% summarise(total_sales = sum(sales)) Alternatively using data.table and lubridate: library(data.table) library(lubridate) data2 &lt;- data.table(data) data2[, checkout_datetime := ymd_hms(checkout_datetime)] data2[ month(checkout_datetime) == 5, .(total_sales = sum(sales)), by = year(checkout_datetime) ] Sorry for any mistakes - on mobile!
This is absolutely wonderful, thank you so much!
You can do it with the cast function in plyr I think. That said, the first way is usually the desirable format, for stats and plotting.
Totally agree with the intro tutorial linked here. Learning the long/wide data distinction has been really helpful for me.
Also consider using [tidyr](http://blog.rstudio.org/2014/07/22/introducing-tidyr/)
That worked!! Thank you! Now, what if that file is not in my working directory? I tried moving my WD up a level and fread didn't like it So newestfile lives at WD/child1/child2/newestFile.
I promise you, those are real R color names. It doesn't work, no matter what colors you use. 
Everything with Hadley Wickham. Also, there are lot of other ways to work on making better use of R from online training (like [here](https://www.datacamp.com/)) and blogs that discuss different developments, new packages, tricks, conferences, etc. (like [here](http://blog.revolutionanalytics.com/) and [here](http://www.r-bloggers.com/))
+1 for all things Hadley Wickham. 
Practical Extraction &amp; Reporting Language would make for a good comparison to R string functions. 
While I agree with what you're saying in general, I was looking for something that actually indicted a particular language (or a language's libraries) as the current "king of string manipulation" or some such...
I'll toss in my opinion, if that's what you're looking for. Regular expressions, while not a language per se, are string manip kings. I'm also extremely partial to python, due to its slicing, something I've not seen (or been able to implement correctly) in other languages. So... Python+Regex is what I use for all my parsing. Once I have an algorithm perfect, I port things to C#. I must point out, though, that these are only the ones I use because they are the ones I've always used.
It probably strongly depends on what you want to do and how large your string vectors are. perl and python (what I tend to use for such tasks) can load files by line, which can give you quite a speed bonus. With this, you can also split the lines individually instead of creating a whole second copy of your data with the split (as R does). That said, if you aren't hitting limits I prefer the solution that saves programming time and creates the simplest pipeline (i.e. only one language) PS I just realized stringr might be better than vanilla R regarding reading files by line and string split, I never came around to look into it further.
 f = factor(c(12,34,56)) as.numeric(levels(f)) [1] 12 34 56 not the whole answer but may help
Regex is my goto as well, and it is pretty much universal which is nice.
it might help to sketch out what you want visually. you said: &gt; ('Time' on x-axis and 'RiseOrFall' on Y-axis) So I'm not sure what TRPM is in your ggplot... Is this what you want? Time &lt;- c(12,34,56,78,90,123,567) RiseOrFall &lt;- c(32,0,0,25,29,0,50) df &lt;- data.frame(Time,RiseOrFall) ggplot(data=df, aes(x=Time, y=RiseOrFall)) + geom_line() 
FYI, by using a while-loop and `readLines`, `R` can read files in line-by-line... See [here](http://r.789695.n4.nabble.com/reading-a-text-file-one-line-at-a-time-td2326046.html).
thanks, in hindsight it seems completely obvious, will have to try how fast it works
I thought I would add more detail. This is an example of what some of properties of the question would be. *Difficulty:{ Easy, Medium, Hard, } *Subject:{ Calculus, Algebra, *Type:{ Numeric Entry Multiple Choice - Multiple Answer Questions Multiple Choice - Select One } Thanks again for the help.
First, given your strategy at the moment, only replace a player if he is really cheaper, if he costs the same, but isn't as good, you are just making the team worse. Brute forcing might be possible, given that you have four subsets, but the different amount of players in def/med/for might be a problem. You definitely want to preprocess your player list: order them by score and then remove all players that have a lower score than a cheaper player (i.e. going from high scoring to low, remove any player that costs more then the previous one). Ok, I just rethought that, and you can only do that for the goalkeeper, for the other players you have to create all possible subsets of a size (say 3 defenders) and then do the filtering based on their combined scores and costs, but that might be to computationally expensive, depends on the amount of players. Additionally, if you weren't already planing to do it for the depth first approach, you want to stop going down, as soon as your teams score is worse then the best you already found. If you go with a heuristic approach, pre-analysing the score to cost curves for each defender/midfielders and forwards might make sense.
Ahh I understand now. Yeah, I could implement that. I think with these methods combined, the dfs is a possibility. I can probably get to something like 15 goalies, 30-100 of the rest of the positions. Depending on the week.
100% what you want is Shiny. http://shiny.rstudio.com/
If you RTFM you must have stumbled over: &gt; isTRUE(x) is an abbreviation of identical(TRUE, x), and so is true if and only if x is a length-one logical vector with no attributes (not even names). as.logical(5) on the other hand returns TRUE, which is what happens in the first case (automatic type conversion) BONUS try identical(1.0, as.integer(1)) vs 1.0 == as.integer(1)
So with the compiler package you can compile your code, but I don't think you can easily exchange it with other people except by making a package out of it, which you can pack into a 'binary' installer. But you can't create an executable that runs without R, at least not that I know of.
The two code snippets for x does something very similar. factor is generally used to save categorical data, it uses 'levels' that are internally represented as integers (for space and speed) but have names assigned. So in your case the names are just slightly different (1,2,3,4 vs 3,6,7,10), but otherwise everything would be the same. I think the main question is what do the three elements represent? Are they 3 dimensions (i.e. you aren't allowed to switch the order), in that case you want a matrix or three different y vectors. Are they 3 replica/separate measurements, in that case you would just need x to be a vector with the matching values x&lt;-c(3,3,3,6,6,6,7,7,7,10,10,10) Are they something else I can't think of at the moment?
It is possible that Sys.glob("example\_folder\_*/") will do what you want.
There's no built in way to hide your R code. I remember reading something by one of the guys behind it stating that it was intentional that you are unable to hide your code, as he felt all R source code should be able to be read (can't find it now though) EDIT: Check out this email thread on the mailing list: https://stat.ethz.ch/pipermail/r-help/2011-July/thread.html#282705
I asked my professor today (taking a stats course at college). The factors were fine, but he wanted me to compare an anova on the linear model (y~xfactors) vs another anova on the model (y~xvector) [this is what you were suggesting with the "vector with the matching values]. So you're correct across the board. Thank you!! I'll definitely utilize this sub more as I learn more R. 
The wikipedia explanation of the ROC curve is good, the short version is: how good your method performs for different cutoffs of X highest score predictions. I'm not 100% sure about the variable contributions table, but my guess is that it shows how important (and in what way) the variables (you use as input for the predictor) are in the prediction model.
Thank you for providing the link! It was an interesting read
1. Sounds like a good idea, everywhere I look tells me to use vectoring. 2. I'm actually not doing 100 database requests, just 1 request/query per function. Then running calculations (100 rows of data) from each of the acquired dataframes post-query. 3. Any useful links on AJAX? Basically what I'm doing is having a dropdown menu to select "N" (N = 1:20), submit button, action="same_page.php", $_GET["N"], and sending "N" to R to export .png images to the folder. Then when "same_page.php" is reloaded, the graphics are displayed through HTML. 4. Haha couldn't be more correct... I'm currently junior in college and this is my first SQL related course. I'm incredibly deficient when it comes to applying and benefiting from database connections. 5. Course requirement :) Thank you, I'll definitely be looking in to all of these!
Oh wow. Yeah I've been using javascript for this project, that sound like it would be exactly what I want. Gonna have to look into that.
Note that shiny does the same thing with dynamically loading plots but it uses web sockets instead of ajax. That allows it to be more interactive essentially. Also don't cross post to both rlanguage and rstats in the future
 df[sample(nrow(df), the_number_of_rows_you_want), ] [From here](http://stackoverflow.com/questions/8273313/random-rows-in-dataframe-in-r)
Thing is that with 4 million rows, I don't have the memory to run that much, I need to run it in 'chunks' of say every 10,000 rows I load one row into a set variable Instead of a 6gb csv think of something like a 100gb csv on an average joe's pc
How many columns is this? 4 million rows shouldn't be that large of a CSV. Even 1 million x 500 column data frame I am working on right now isn't a gig. 
I lied 40 mill 24 col
Should still be able to read it all in eventually. Might take a few minutes though. Alternatively, import pandas import random n = 1000000 #number of records in file s = 10000 #desired sample size filename = "data.txt" skip = sorted(random.sample(xrange(n),n-s)) df = pandas.read_csv(filename, skiprows=skip) [Source](http://stackoverflow.com/questions/22258491/read-a-small-random-sample-from-a-big-csv-file-into-a-python-data-frame) 
Does MySQL not do parameter binding? Then you wouldn't have to use sprintf at all. 
Maybe that's the problem. Just using for loops. I probably have to use an apply function 
Sounds good, R revolution open boasts about automatic multicore. So did they just implement the parallel package into default R?
 data &lt;- data.frame(a = rnorm(100, 10, 0.15), b = rnorm(100, 20, 0.5)) data$c &lt;- data$a + data$b plot(hist(data$c))
I'll try this tomorrow, thanks!
I've read the help, although I've never found R documentation very helpful... After I import and export through Access, I can load the data nicely. The import settings are separated by "|" and no quotes, and the export settings are exactly the same... The command I use to load the access-processed data is read.table(..., header = TRUE, sep = "|", quote = "") What I would like to do is site = getURL(...) read.table(site, header = TRUE, sep = "|", quote = "") The problem is just that is crashes RStudio. I tried your code and it crashed. I can't access the object that I referred to here as "site". I don't get an error message other than "R has encountered a fatal error." 
open the saved file in notepad++ and go to the line with the error. See if it is missing some of the fields. You can try encoding='UTF-8'. If the url is a text data file format, you can read it directly using read.table('http://...', sep='|', quote="", header=TRUE, as.is=T, comment.char="")
Is there any way to incorporate a username and password into this? The file is on an ftp server that is password protected, but this might work otherwise.
I am not sure about that. You can go back to Rcurl to open the url and try passing the result of getURL to read.table(). 
If you want to learn the basics, I suggest swirl. Swirl is a package that allows you to learn R interactively in the R console itself. You can find more info [here](http://swirlstats.com/). 
I've been doing the Data Science track from Johns Hopkins on Coursera. Overall I think it is pretty good. 
Cheers, I've heard a lot about coursera; I'll sign up today! Thanks!
Try this introduction by swirl and datacamp: https://www.datacamp.com/swirl-r-tutorial 
Those specific courses are on a timeframe, but you can take multiple courses at once. Some of them build on each other, but there are quite a few that you can take at the same time.
This is typically the error you get when it's trying to evaluate something that doesn't exist. I think your problem is that at some point either b[j] or a[i] are being called but either a or b is a shorter length than that value i or j respectively. A practical example of this would be asking for the 27th letter in the alphabet. To check this, rerun with a print statement after you declare your for loop but before you evaluate (b[j] &gt;= a[i]), something like "print(c(i,j))" to give you the index that j and i are on, and compare that value to "length(b)" and "length(a)" 
as /u/justastupidkid said, the problem occures because you are addressing a vector position that doesn't exists and the problem is: for (j in 2:length(b)){ for the first iteration ( where b=c(1) ) evaluates to (j in 2:1) so in the loop it does b[2], which doesn't exist. You are also using append wrong, see ?append Otherwise, when posting use 4 spaces before each code line to format it as code, then it is more readable. Oh and you can just do: a &lt;- sample(1:1000000, x, replace=TRUE)
Looks good! After some tweaking, a similar process gave me my histogram. Thanks
is there a way to maybe use a loop or something for all of the left joins? 
Also, I would suggest rearranging your data like this: data =data.frame(year=rep(c(1999:2002),4),gpa=c(3.3,3.2,2.9,3.2,3.7,3.8,3.9,3.65,2.8,2.5,3.1,2.8,1.9,2.1,2.3,1.4),person = rep(c("John","Mohammad","Ben","Tyrian"),each=4))
A function might make more sense: new_join &lt;- function(tbl_in, var_in){ test%% left_join(tbl_in, by = var_in)%&gt;% select(-var_in)} test &lt;- new_join(dischargestatuscode, "DischargeStatusID") test &lt;- new_join(agecode, "AgeID") ... I guess you could use this function to loop through the tables using a table of table names and join variables. Seems like a lot of work for only 10 joins... its going to make your code harder to read imo. 
Thanks for the matrix(1:16, 4) suggestion! Really good idea.
I'm not 100% what you are trying to do, but I believe to make it loopable you either have to throw all your tables into a named list, or use get() together with a ID to table_name named vector. I'm also not sure if it might not be a better solution to cbind the appropriate data.frame parts e.g. cbind(test, dischargestatuscode[ test$DischargeStatusID , ], ... assuming you make/have DischargeStatusID the rownames of dischargestatuscode
Environmental science. I use R to analyze soil and water chemistry data. 
Astrophysics, right now we are using R to analyze cluster data.
I work in the private sector at the moment. I have not been able to find anything from the EPA in the way of R resources, but they do have a lot on statistical methods. R has a lot of packages written for geostatistics and other environmental work too. I'm really just still getting my feet wet, but am super excited to be doing this stuff. 
Interesting that you mention that...I've been working on that specialization as time allows. I'm about 50% done now, and should finish this summer. In the project I mentioned in the original question, I'm hoping to find industries with emergent use of R. I'm curious to look at their general technology adoption rates and how that compares to the rate of adoption of R. 
Fisheries science. R has been hugely important as it's the common language that biologists and economists can speak when we're evaluating huge bioeconomic models to evaluate management actions.
I use it for spatial statistics in GIS with packages like spatstat,spatial,rgdal. There are also some nice packages that make plotting maps easy like ggmap and rMaps.
Online Traffic Analytics is my field of work. We import traffic data, scrape social media using R packages. And then build dashboards using tools like RMarkdown, Shiny etc.
Perhaps clear the environment afterwards?
Well excuse me for trying to be of help. Here's a slightly less blind guess. Typing ?par gives this under the examples heading: &gt;op &lt;- par(mfrow = c(2, 2), # 2 x 2 pictures on one plot pty = "s") # square plotting region, # independent of device size .# At end of plotting, reset to previous settings: par(op) See if that works. 
You actually would have this issue in R as well. If you're closing the plotting window then you're actually destroying the graphic device. If you don't close the window you'll have the same issue in Rgui as you're experiencing now. You can't really tell it to only apply once but you can reset the options after you do your plotting. If you're doing your plotting from within a function you could make use of on.exit to make sure the cleanup happens after your code is run. If you remove the current plot (using the document with the red x on it which appears above the plotting region) that will reset the graphical options. Alternatively you could just reset them manually using one of the other suggestions in the thread. I typically just use layout(1) to reset things after modifying mfrows.
Being shitty to people who are trying to help you is not a good strategy. 
What /u/portoguy mentions about rearranging your data will be important to you in the future, if not obvious at the moment. Essentially, his code rearranges your data so that you are not losing data relationship information and conforms to [tidy data guidelines](http://vita.had.co.nz/papers/tidy-data.pdf). As a VERY brief overview, tidy data starts with these traits: * Each variable forms a column. * Each observation forms a row. * Each type of observational unit forms a table. In short, the way your original data is arrange creates 4 columns of the same type of data, GPA. While it is useful to summarize it that way, you lose information about the explicit column relationships that are potentially important by themselves. For example, the next user of the data set must know from an external source that John, Mohammad, Ben and Tyrian are "students" and that 1999, 2000, 2001, 2002 are "year" of study. By organizing the data by "student", "year" and "gpa", those relationships are communicated explicitly. Also, if you choose to expand the data set to express other relationships, like "Cumulative GPA" for example, then you only have to add an additional column, rather than adding a whole new dimension to your table. Because most R commands are structured to work optimally with tidy data sets, organizing your data in a tidy format makes your actual programs simpler to write, easier to read and debug. and easier to plot with the various graphics packages. What you will find as you write more R code is that the workflow is generally: * Get raw data * Tidy it up as appropriate * Analyze * Present 
I was just wondering about the fact that I don't remember R resetting it when I got to your post :) I'm not sure, but calling dev.new() should also reset the plotting device in Rstudio (can't test atm),
Your post is somewhat hard to read. I've formatted it for you: &gt; I am currently working on a problem in R Studio and I am stuck. I am stuck on part 5 which asks: **5) Make a function that iterates the Collatz function and stops when the trajectory reaches 1. It should then print out the trajectory. Try this for several numbers and plot the trajectory using plot(iterateCollatz(x)).** I got the collatz function working. how I can solve the problem above? below is how far I reached, I am stuck. How can I make the function stop when trajectory reaches 1?? Task 3: Make a function that implements the Collatz function. If x is odd then C(x) = 3x + 1 If x is even then C(x)=x/2 f3 &lt;- function (x) { if (x%%2 == 0){x/2} else {3*x + 1 } } &gt; and next Task 4:Make a function that iterates the Collatz function a user specified number of times, and returns the trajectory. f4 &lt;- function (x,n){ orbit &lt;- (x) for (i in 1:n) { orbit[i+1] &lt;- f3(orbit[i]) } return(orbit) } &gt; thank you
Anyway, you have a few options, most of which center around how you want to control the flow of your program. Now, in problems like this, the first division is how you want to get from the beginning of the path to the end. You could "search" for the end of the trajectory by testing large sections all at once and then checking to see if you made it to the end yet (this is more heuristic), or you could literally follow the directions and "iterate" until you find the end of the trajectory. I've provided one example of the first and two of the second, down below. Your biggest problem is that I don't think it's practical to use your `f4(x, i)` function to solve `f5`. My "search" solution uses `f4` but both of my "iterate" solutions use `f3`. "Search": f5.search = function(x, i = 1) { while(TRUE) { orbit = f4(x, i) if (1 %in% orbit) { first_instance_of_one = which(orbit == 1)[1] return(orbit[1:first_instance_of_one]) } i = i * 2 # make bigger and bigger orbits until we find 1 } } "Iterate": f5.iterate = function(x) { if (x == 1) { return(x) } path = c(x) # yes a singleton is already a vector but I'm trying to be obvious x_next = f3(x) while (x_next != 1) { path = c(path, x_next) x_next = f3(x_next) } path = c(path, x_next) # x_next == 1 at this point return(path) } f5.recurse = function(x, path = NULL) { if (x == 1) { path = c(path, 1) return(path) } else { return(f5.recurse(f3(x), c(path, x))) } } Results: &gt; f5.search(35) [1] 35 106 53 160 80 40 20 10 5 16 8 4 2 1 &gt; f5.iterate(35) [1] 35 106 53 160 80 40 20 10 5 16 8 4 2 1 &gt; f5.recurse(35) [1] 35 106 53 160 80 40 20 10 5 16 8 4 2 1
`!` is the universal sign of negation. Try it in your R environment. &gt; !TRUE [1] FALSE So `!=` is the negation of `==` and `!=` is pronounced "not equal to". Thus... &gt; 1 == 1 [1] TRUE &gt; 1 == 2 [1] FALSE &gt; 1 != 2 [1] TRUE so `x_next != 1` is saying "tell me, is x_next not equal to 1 right now?", which in context of that line is saying "Until I see the x_next become 1, I want to repeat applying f3 as long as x_next is not 1". `path` is not an R function, it is the name of the variable I have given to collect the elements of the "list". At this point do note that R lists are very different than, say, Python lists or Java ArrayLists so I say the word "list" in the Lisp sense - it's just a collection of things. In this case, my "list of x_next" is in reality a vector of integers, whose length is the number of steps from the beginning number `x` until it degenerates to 1. I chose "path" because "orbit" sounds too cyclical. In my opinion, the Collatz sequence is a *path* from the first number to the last number, which is always 1. Assuming the Collatz conjecture is true, you will never hit the same number twice until you get to one, in which case you cycle 4-2-1 infinitely. Since every element is unique, then it is quite aptly defined/named as a "path". I'm glad to see that you're dissecting my code and asking questions about it. I challenge you to figure out how each one works, and *why* it is guaranteed to find the answer. Also consider how efficient these are in terms of time spent (note: you could test this using the `system.time()` function) as well as space (a wee bit harder to test in real-time but with experience and a computer systems textbook you could debate with me which one is most space-efficient).
&gt; pretty cool line charts by using the "both" (or "b") option in plot() this I learned today. Thank you for the help. I will start working on this later this week. Will keep you posted.
It seems a little odd to see this posted here. What alternatives are people using?
I'm in psycholinguistics and I'd say a majority of people in the field use R.
Yes I agree, everyone who uses R pretty much knows that RStudio is what you use. Unless you have a substantial CS background, in which case you might be using vim/emacs/sublime. But RStudio is pretty much the standard... And this article seems to really focus on features that only came in the last couple of months. Which are AMAZING features, but it seems like this post focuses a lot more on the new stuff rather than the IDE itself
I didn't know it included "light VIM support". But why would anyone who uses VIM use an IDE?
Nice question pt2091 First of all, I LOVE VIM so much. I use VIM for 2 years and I still use VIM for coding and text editing. Why I use an IDE? This is all because some killer features I can easily get with an IDE (and difficult with VIM): debugging (I got many troubles when I debug without an IDE), package management, code completion, code highlights and code diagnostics. If you use R, RStudio take step furthers integrated RMarkdown, shiny app and something like that. So why don't we combine some nice features from vim: navigation without mouse and arrow keys, command mode, visual mode with some good ones from an IDE like studio?
Just use dev.off() or dev.new() to restart the graphical device, no restart needed :)
Well depending on how many of these samples you need and/or whether you know the number in advance the two obvious answers are to make a list of the samples and to generate an environment that contains them as variables. Lists can be slow in some situations, but I would try that first. If performance is an issue, then you can try an environment approach [maybe like the one discussed here](http://stackoverflow.com/questions/17046336/here-we-go-again-append-an-element-to-a-list-in-r). 
This isn't directly related to the error as that's pretty self explanatory, some of your variables are not numeric. But look at the corr.test function from the psych package- it's much more flexible and gives you n, p, etc. 
Yes, and that while loop looks exactly like my `f5.iterate` solution, and it doesn't use `f4()`. The difference between my while loop and that while loop is that I stored the next iteration in a variable and used the concatenation function `c()` whereas this one puts it directly into the vector/array and then increments a counter. Two ways to do the exact same thing.
They are simply visual aids for users to look at vectors and other sequential output. [1] means that the element to right of itself is the 1st element of the printed output, likewise [19] for the 19th printed element, and so on.
There's too many numbers (elements) to display in one line so it wraps and gives you a quick index to see which "row" you're on. Of course a vector isn't a matrix so it's not a literal row, but it's useful. 
it pastes together the character strings you pass it with the platform dependent path separator. file.path("folder","file") produces "folder/file" on unix, while "folder\file" on windows
To answer a question that wasn't asked- if you need to see the index number for each element, like if you're selecting column names or something by index, you can ask for them as a matrix: as.matrix(names(foo)) so you don't have to count over across the row to get the index value. 
thanks ! will definitely try! 
I honestly do not know how to complete the rest of the problem, the remaining questions are as follows: 6: Make a function which takes a number x as an argument and returns a list that contains the orbit of x and the number of iterations of the Collatz function it takes to reach the number. 7: Make a function which takes a vectorx as an argument and returns a data frame consisting of a column for the numbers inx and a column for how many iterations each takes to reach 1. 8: Use your function from Task 5 to create a function which takes a number x as an argument and returns the largest number in the trajectory of x (we will define the end of a trajectory as the first time that trajectory reaches 1). can you help? thank you 
No, I'm not going to. You need to learn how to do this on your own, otherwise you'll never be able to write your own programs. These are simple questions and you should be able to do them. I can try to re-word them to give you hints but I'm not going to supply any more solutions unless you have technical difficulty (why is this giving an error, what function will accomplish a certain task, etc). One thing that helps me put together solutions is to think of the *inputs* and *outputs* of each problem, then focus on how to turn the input into the output. If you learn more about [functional programming](http://en.wikipedia.org/wiki/Functional_programming) (and surprise! R is a functional language), then you will permit me to describe these functions by their *type*. I will write type as [Problem Number]: function_name = function(input) -&gt; output ___ 3: f3 = function(int) -&gt; int, given one integer, return its Collatz successor 4: f4 = function(int, int) -&gt; int list, given a starting point P and a number of steps N, create a list of N Collatz successors starting from P 5: f5 = function(int) -&gt; int list, given a starting point P, create a list of indeterminate length repeatedly applying the Collatz successor function until the trajectory is equal to 1 ___ Now that we've gotten those out of the way, let's look at the ones you have left. 6: f6 = function(int) -&gt; (int list, int), this one takes one input and returns **two** inputs, something you haven't had to do before. If you scour my other posts, you'll notice I once mentioned the difference between an "R list" and a list in literally any other language - you are about to learn that difference. You will want to look at the `list()` function in the help files to see how to build one, but basically think of a `list` in R as a `Object` in languages such as JavaScript, Python, Java, whatever other language you have experience with. The "collection" type of lists in R is what we use for vectors. So in this problem, you have to take a starting point P (like we did in f5 and f4), run it until we reach the end of the trajectory (like we did in f5), then return both the *collection of successors* as well as *the length of that collection*. You will find `f5()` and the `length()` function very useful here. 7: f7 = function(int list) -&gt; int list, this is your first application of *mapping* (a functional idea) where you take a collection of items and run a function over each one of them in parallel. It's much more efficient than using a loop, which has to do one then the next then the next then the next and so on. You will have to take each element of the list coming in, apply f6 to them, then from the **list of f6 outputs** you will need to grab just the length result (the 2nd thing returned). Then you will have a "list of lengths". 8: f8 = function(int) -&gt; int, given a starting point P, return the largest element of the Collatz sequence starting from P. When you get to this point, look back at how you did #6. Now replace the `length()` function with the `max()` function. ___ I think number 7 is the trickiest one, the other two I did with one-liners.
You need to bind the ggvis plot to shiny. Add: layer_points() %&gt;% bind_shiny("inv_data") If you post more questions, it helps to provide a self contained reproducible example. 
&gt; I have no idea how to put it down because I don't know it. Which is why I said that if you have *technical* question, then I'd be willing to help you deal with the syntax and semantics of each code block. However, I don't want to feed you the remainder of the problems without making you "earn them" first. So you did 6 and you did 7 - how do you think you can approach #8 given how you solved #6?
I am currently studying the course, it seems very good. Does lecturer using the book with the course should I find the course's book.
Thanks, will give it a shot today!
That is basically the most elegant variant (I prefer the apply family over such for loops, but that is mostly taste). Speed wise you can optimize it by just grouping the sampling and then indexing subsets from that (for example over a matrix) random_samples &lt;- list() for (i in 1:15) { random_samples[[i]] &lt;- matrix( rnorm( i * 1000, 0,1), nrow=1000 ) } and then e.g. apply(random_samples[[10]], 1, sd) but 15000 rnorm calls probably isn't to big of a problem. on a side note, the dangerous thing with for loops is that you might do something like x &lt;- c(x, new_x_element) which is slow because R has to copy all of x around all the time
So let's say I want to increment a range (25,175) by intervals of 25. How would I do this? I just tried scale_y_continuous(breaks = c(25,175)) and only the 25 displays on the axis now. that is the only y axis tick.
No, the rule is use the correct tool for the job. For many complex jobs, R is the correct tool. Don't confuse user-friendly with simple and dumbed down. R is not for "programmers", it's mostly for people who need to do complex analyses as part of scientific work that isn't creating websites or databases. That you still *can* do that very robustly in R is a plus. 
I was just teasing you guys. You're entirely welcome to use it if you can't use a real programming language - no shame in that! 
with ggplot2, try something like this: scale_x_continuous(breaks = seq(25, 175, by=25), limits = c(25,175))
No need to make the solution, just a hint would be nice too! :)
You can try [plotly](http://blog.revolutionanalytics.com/2014/05/ropensci-and-plotly-make-ggplots-shareable-interactive-and-with-d3.html), I think it comes closest to have it as interactive. But you will need to sign up and open it in a browser. I'm not sure how exactly the maps are in ggmap. If they are vector graphics, then they will maybe scale nicely, otherwise they might become distorted. If you want to add slider to make it dynamic by changing a parameter, you should look into shiny. I would also recommend asking on stackoverflow or crossvalidated, you usually get better answers there, this subreddit is rather dead. Still doing your MSc thesis or something else? What is this data?
Shiny is awsome! Brings R to the web in an easy syntax http://shiny.rstudio.com/gallery/
As far as I can tell (haven't used it before either) and understand the help, you are correct. The way I tested it: enum(5) + enum(6) it returns a enum 11, so the '+' definition of integer was applied to the enum objects. If you use the 'representation'/Slots you end up with your second case, a class that contains different classes, not a class that is a child of another class. EDIT you can specifically test it with inherits(enum(5), "numeric")
I knew I'd find a discrepancy eventually if I looked/tested hard enough. My formal class is lost in the following example: &gt; mtcars$cyl = enum(mtcars$cyl) &gt; str(mtcars) 'data.frame': 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl :Formal class 'enum' [package ".GlobalEnv"] with 1 slots .. ..@ .Data: num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... &gt; cyl_gear = dcast(mtcars, cyl ~ gear) Using carb as value column: use value.var to override. Aggregation function missing: defaulting to length &gt; str(cyl_gear) 'data.frame': 3 obs. of 4 variables: $ cyl: num 4 6 8 $ 3 : int 1 2 12 $ 4 : int 8 4 0 $ 5 : int 2 1 2 Crap. Any ideas?
Well as far as I can tell it is a problem/error of dcast, not of the class inheritance. (Hadley's work is generally great, but at some point it must call some function that does the equivalent of as.numeric(...) on it. You may want to go ahead and just ask him or post a question on the reshape package github page or so.) This is also a nice tutorial I just found: http://adv-r.had.co.nz/S4.html (by Hadley)
&gt; cran.r-project.org/doc/contrib/Short-refcard.pdf thank you checkin it out now :D
maybe I'm missing something, but how about: Adj &lt;- R_CA4_DG - (xsum$coefficients[2])*(eTIV[1]-mean(eTIV[1:8])) I suspect this will work as a vectorized operation
i have no experience with a windows install, but i would assume you'd just install it like a normal web server. and your users would need your machine's address to access the shiny apps you've put into it. also if they can't one of the first things to check is your port configurations.
might also help to use Tidyr since this has kind of replaced reshape2. my projects are still on reshape2 though.
while a growing vector isn't a problem for 8 elements, it can quickly lead to memory inefficiencies problems and I therefore suggest not using it, instead: results &lt;- rep(NULL, 8) for (i in 1:8){ results[i] &lt;- R_CA4_DG[i] - (xsum$coefficients[2])*(eTIV[i]-mean(eTIV[1:8])) }
it should work with: Adj &lt;- R_CA4_DG - (xsum$coefficients[2])*(eTIV-mean(eTIV)) assuming that the vectors (R_CA4_DG and eTIV) are 8 long, and you always want the matching element of both. If this doesn't work, try parts of the expression and let us know what is being returned.
you're welcome. though the steps seemed a little more involved than for my ubuntu setup. i just need to drag my folder and it works. good luck with this though.
hello, this did not work. the vectors are 16 long with pre and post 8 each. when doing this even for all 16 i got the first number then 15 NA
cool thanks! so I was able to obtain a new vector at the end of my sheet with all 16 data point. awesome :D This was what I used, recommendations to shorten or clean? reg &lt;- lm(R_CA4_DG[1:8] ~ eTIV[1:8]) regsum &lt;- summary(reg) aRCA4DG &lt;- rep(NULL, 8) for (i in 1:8){ aRCA4DG[i] &lt;- R_CA4_DG[i] - (regsum$coefficients[2])*(eTIV[i]-mean(eTIV[1:8])) } volumes$RCA4DGadj[1:8] &lt;- aRCA4DG reg2 &lt;- lm(R_CA4_DG[9:16] ~ eTIV[9:16]) regsum2 &lt;- summary(reg2) aaRCA4DG &lt;- rep(NULL, 8) for (i in 9:16){ aaRCA4DG[i] &lt;- R_CA4_DG[i] - (regsum2$coefficients[2])*(eTIV[i]-mean(eTIV[9:16])) } volumes$RCA4DGadj[9:16] &lt;- aaRCA4DG[9:16] So, now feeling cool, I just tried to do two for loops to try and run that procedure on every variable and failed miserably lol. Very bad misuse of "[]"?? for (i in volumes$[i]){ reg[i] &lt;- lm([i[1:8]]) ~ eTIV[1:8]) reg[i]sum &lt;- summary[a[i]] a[i] &lt;- rep(NULL,8) for (j in 1:8){ a[i] &lt;- [i[j]] - reg[i]sum$coefficients[2])*(eTIV[j]-mean(eTIV[1:8])) } volumes$[i[1:8]] &lt;- a[i]] } 
&gt; what would be a better way for calling variables from a dataset? One that is correct syntax :D $ only works if you actually write the element/column name, you can't use a variable. As posted in my last comment [[ ]] is the equivalent that does take variables (and numbers) &gt; First row are headers You generally put the header in colnames(volumes) (for matrix or data.frame) or names(volumes) (for list or data.frames). read.table can do this automatically with the header=TRUE option.
hmmm I see. Thank you for all your help! I will keep working on this :D
Thanks, will look into it. I've been trying to break it down and try to for loop or lapply to obtain only the coefficient of regression using lm(). I've been trying to do something like, regr &lt;- list() for (i in volumes$[[i]]){ regr[[i]] &lt;- lm(i~eTIV) } but that gives me: Error: unexpected '[[' in "for (i in volumes$[[" regr[i] &lt;- lm(i[1:8]~eTIV[1:8]) Warning message: In regr[i] &lt;- lm(i[1:8] ~ eTIV[1:8]) : number of items to replace is not a multiple of replacement length } Error: unexpected '}' in "}" 
Thank you, got answers with summary(regr[[1]])$coefficient[2]
Yes, the remote API takes like 10 seconds to return a simple summary table data. I've looked up some articles on Postgres, and it looks promising. Thanks for the advice. 
Just spitballing, but it seems like a Poisson process except that there is not a lambda that is both the mean and variance. If the variance ends up being the mean squared in this mechanism, couldn't you approximate this with a Gamma distribution? library(knitr) rand_seed(123) m_hat &lt;- NULL; v_hat &lt;- NULL Means &lt;- c(63.63, 51.96, 78.11, 2.58, 4.66) Variances &lt;- c(2397.85, 3173.28, 6443.70, 9.95, 12.13) for (x_bar in Means) { rvs &lt;- rgamma(n = 1e6, shape = 1, scale = x_bar) m_hat &lt;- append(m_hat, mean(rvs)) v_hat &lt;- append(v_hat, var(rvs)) } kable(cbind(Means, m_hat, v_hat, Variances)) Which produces: | Means| m_hat| v_hat| Variances| |-----:|---------:|-----------:|---------:| | 63.63| 63.683140| 4065.185070| 2397.85| | 51.96| 51.929746| 2700.587467| 3173.28| | 78.11| 78.209222| 6117.636291| 6443.70| | 2.58| 2.580922| 6.661436| 9.95| | 4.66| 4.659595| 21.716097| 12.13| While that seems to get a little closer, it's just me playing with R without a justifying reason. Your data may be multimodal depending on something else affecting the birds' decisions.
Thanks for your help! My data aren't multimodal so that shouldn't cause a problem. It looks like the Gamma distribution fits well which is good to know. My problem though is that I don' t know how to statistically test individual repeatability with this data. For Gaussian data R has a really nice package which is based on anova and essentially compares within and between individual variation. Where there is significantly more variation between individuals than within repeated measures of the same individuals that shows shows that individuals of the same population are consistently different from one another. I hope that makes sense. With normal data this is straightforward but with my data it seems much much complicated. I'm not bad at stats and I can use R quite well but I'm struggling with this one...
The dependent variable goes on the left of the tilde and the independent variable goes on the right (in terms of interpretation).
If you're using R I guess you're into statistics? If so, say yes, and I'll let you know. 
I am yes, more data science though.
Well if you're specifically talking about why the order matters in R, it matters because you can store that relationship into an object and use it to do a number of things that rely on specifying one of the variables as the predictor and the other as the predicted. For instance, you can give that model a certain x and have it spit out a predicted y.
Indent your code 4 spaces. 
and now?
because they use lapply to apply the function on each element of the list split_low, not on the list itself. split_low is a list of vectors that contain the string elements split_low[1] is a list containing only the first vector i.e. list(c("gauss","1777")) split_low[[1]] is the first vector c("gauss","1777") split_low[[1]][1] is the first element of the first vector "gauss" so the lapply does something equivalent to tmp &lt;- list() for(i in 1:length(split_low)) { tmp[[i]] &lt;- select_el( split_low[[i]], y ) } return(tmp) 
thank you! that makes sense
Now the code has become unindented for some reason. So again, please indent all the code (including the comments) by four spaces, except where you did it already (in which case keep them indented). When editing, at the bottom right of the textbox there's a "formatting help" link which may help with basic formatting while on reddit. reddit uses a slightly-customized version of Markdown to control formatting; see [here](http://daringfireball.net/projects/markdown/syntax) for more details on Markdown. If you use `knitr` and related functionality in R, or you use any StackExchange sites, you'll likely have seen Markdown already. 
This could be more involved than you want, but doesn't have to be. Build a faux stock trading program. You can scrape Yahoo Finance for the data automatically, and use as simple or as complex of an algorithm you'd like to return a buy sell or hold order for a particular security. Track your progress over a few days (or longer), refine, repeat. I'm on mobile so I won't link to lots of resources but the steps to do this are all easily Google-able, like reading from Yahoo. 
Why do you not just use the viewer? It shows the entire dataset nowadays.
Swirl is great and I absolutely love it, but does it have any lessons for the topics he mentioned?
How far have you gotten in the specialization? (I have 2 to go + capstone)
I use the nnet package for this frequently, and really like the breadth and depth of customization it offers. I'm on mobile, but google it. There's a great annotated example from the UCLA analytic group. Edit: here is the resource http://www.ats.ucla.edu/stat/r/dae/mlogit.htm
Throw it on github and put a link to it. That's one of the standard ways for showing off code on your resume.
Depends on what you use to write that resume. If it's Tex: use listings for example Other than that I'd put it online (as it was already mentioned)
Also, note that it is very easy to use Rmarkdown and knitr as built into RStudio. There are plenty of examples. In addition to preserving R code along with syntax highlighting, you can format your text using markdown, and any math using Tex with blocks marked off using $$.
I tried to do this code #library library(jsonlite) library(rvest) library(RJSONIO) library(ggplot2) library(dplyr) library(data.table) library(rvest) library(rmarkdown) #Create a function that will download movie info rottenrate &lt;- function(movie){ require(RJSONIO) link &lt;- paste("http://www.omdbapi.com/?t=", movie, "&amp;y=&amp;plot=short&amp;r=json&amp;tomatoes=true", sep = "") jsonData &lt;- fromJSON(link) return(jsonData) } vrottenrate &lt;- Vectorize(rottenrate, "movie", SIMPLIFY = F) #Puts in the data into a table mikebay_movies &lt;- html("http://en.wikipedia.org/wiki/Michael_Bay") %&gt;% html_nodes ('#mw-content-text &gt; table:nth-child(42)') %&gt;% html_table (fill = T) %&gt;% as.data.frame #Grab all of the info and put it into a dataframe mikebay_movies &lt;- lapply(vrottenrate(mikebay_movies$Film), function(x) as.data.frame(t(x), stringsAsFactors = FALSE)) mikebay_movies_dt &lt;- rbindlist(mikebay_movies,fill=TRUE) #Clean up misreads and N/A's mikebay_movies_dt &lt;- mikebay_movies_dt [-c(1, 13), ] mikebay_movies_dt$BoxOffice[1] &lt;- '$141.1M' mikebay_movies_dt$BoxOffice[2] &lt;- '$335.1M' mikebay_movies_dt$BoxOffice[7] &lt;- '$319.2M' mikebay_movies_dt$tomatoMeter [7] &lt;- '57' mikebay_movies_dt$tomatoImage[7] &lt;- 'rotten' mikebay_movies_dt$Title[8] &lt;- 'Transformers:RotF' mikebay_movies_dt$Title[9] &lt;- 'Transformers:DoM' mikebay_movies_dt$Title[11] &lt;- 'Transformers:AoE' mikebay_movies_dt$Released &lt;- as.Date(mikebay_movies_dt$Released, '%d %b %Y') mikebay_movies_dt$BoxOffice &lt;- substr(mikebay_movies_dt$BoxOffice, 1, nchar(mikebay_movies_dt$BoxOffice)-1) mikebay_movies_dt$BoxOffice &lt;- substring(mikebay_movies_dt$BoxOffice, 2) mikebay_movies_dt$BoxOffice &lt;- as.numeric(mikebay_movies_dt$BoxOffice) mikebay_movies_dt$BoxOffice &lt;- mikebay_movies_dt$BoxOffice*1000000 #Create graph mikebay_usergraph &lt;- ggplot(mikebay_movies_dt, aes(y = tomatoUserMeter, x = Released, label = Title)) + geom_point(aes(size = BoxOffice)) + (aes(color = tomatoImage)) + geom_text(hjust = .45, vjust = -.63) + labs(title = "The Fall of Bayhem") + scale_colour_manual(values=c('#3a9425', '#ed1c24')) #show the final product But got #Errors Quitting from lines 13-58 (Preview-147028fd1b57.Rmd) Error in file(con, "r") : cannot open the connection Calls: &lt;Anonymous&gt; ... &lt;Anonymous&gt; -&gt; fromJSON -&gt; fromJSON -&gt; I -&gt; structure -&gt; unique Execution halted In return
To be honest I wouldn't bother. This code isn't readable enough or interesting enough to parse are the resume stage. 
... Ah yes... Seems to be working now. Thanks for the eyeballs!
I guess Vim takes time to properly setup to a specific environment. I particularly find Vim navigation shortcuts really slick, but I lack experience and patience to properly set it up whenever I start working with a new language or toolset. That's why a lot of times I prefer to use the standard IDE (in this case RStudio) but with Vim mode when it's available. It's almost always available. 
I don't really have a solution to your question. However, I have done some network visualizations using [Gephi](http://gephi.github.io/). I feel like it can do a lot of what you are trying to achieve (adjusting node sizes, edge weights, layouts etc.) very easily and may be worth looking at if you don't absolutely have to use R.
This looks very interesting. I am using R mostly because I was interested in learning it and it seems like it should be possible, I just don't know enough about R yet to understand how to do it. That all said, I would like the data represented nicely, so if I can't figure it out in R I will definitely check out Gephi - I had a quick look and it does seem like it would produce what I want. Thanks!
I'm using R, Shiny, ggplot2 and PostgresQL to build my dashboard and I love them. Correct me if I misunderstood, why we need to replace RDBMs with data.table? Using database driver to connect and query is much easier 
&gt; Using database driver to connect and query is much easier I agree. Cleaner code too.
make a 'toy' dataset that can be used for a minimal reproducible example and post your question to stackoverflow. I (and many other people) will then look at it. I'm not sure what the rules are for RLanguage (I don't come here nearly as much as /r/rstats), but in /r/rstats it's recommended to post to stackoverflow and the post the link to your question as your reddit post. But you'll get yelled at for not presenting a full reproducible example (as you should be). 
Thanks! I will. Is there a preferred method of presenting the reproducible example? (like for javascript there is jsfiddle)?
Ha. Well, I mean, I *knew* what javascript was, technically speaking. jsfiddle sounds nice, it would be nice if R had something like that. I think the issue with it would be that (this is where my unfamiliarity what javascript will show) javascript is all-encompassed within the base language. Whereas with R, many problems people have are with implementing some function z from package x. An *Rfiddle*, if you may, would have to have every package pre-installed. Even so, some issues are that people are using old packages and trying to do new-package things, or vice versa. 
I don't think it would be an issue - the packages could be installed dynamically and the version of R chosen. jsfiddle has the option to pull in external packages for javascript :) In fact, I thought I'd just check something ... http://www.r-fiddle.org
Well sticking with that theme, have you ever used [merge](https://stat.ethz.ch/R-manual/R-devel/library/base/html/merge.html)? It's kind of like cbind except it lets you merge the two dataframes by a certain identifier field.
Anything in the apply family of functions is invaluable. I'd say 99% of the time I need to do some sort of aggregating or data wrangling I can do it with an apply function.
I don't know the answer but I guarantee it's in here, this is a great resource for MCMCglmm. http://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf Usually model components have slots and can be examined with str() 
ddply is my favorite thing ever!! I can't think of the last day where I didn't use ddply at least once. Hadley Wickham is my hero
best trick: install.packages(data.table)
It's magrittr and Hadley didn't create that package. It could be argued that he brought the piping craze to the forefront but I'm not a huge fan.
**Old man yells at cloud** There's nothing below that I use that will blow your mind, but it seems like people get excited about new packages that rewrite older functionality; e.g. aggregate vs. plyr. The hype machine makes knowing newer things seem like you are on the cutting edge... but reinventing base R is not cutting edge. Fancy cat says: &gt; I should go through the base manuals again. aggregate() # Base function for pivot tables... covers my ddply needs, especially when paired with: apply() # If you don't understand vectorization your code is likely suboptimal and you don't disagree when people say R is slow; which is a mistake. unique() # Pulls out unique values or vectors merge() # SQL joins in R. table() # 'crosstabs' are important The glmnet package is a workhorse smoothScatter() # It's a 2D density plot; great for dense/'big' data visualization Intro to CS # Taking a course on programming in the CS dept really helped this statistician write better code.
&gt; apply() # If you don't understand vectorization your code is likely suboptimal and you don't disagree when people say R is slow; which is a mistake. `apply()` isn't *really* vectorisation, it's essentially a for loop underneath. Anyway I agree with you to some extent—it's good to have a good grasp of base functions. But if you are using things like `aggregate()` instead of something like data table, your code is generally going to be more verbose and slower (for non-trivial things).
does it work on the commandline, using pgsql for example? just to rule out that it's a security group issue. (http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html)
Thanks, the course notes are helpful, I am working my way through them now!
Thanks! The gather function worked well for this purpose.
thanks for the link. turned out it works from home, but the office firewall is blocking it at work. now, the internal IT team is saying they can allow access but they need an IP address to whitelist in there rules. but my rds instance doesn't seem to have any IP address info attached. any ideas?
I'm unsure if this will help, b/c I am not the one setting Spark up at my office. My co-worker, who is setting it up, is having an issue integrating Rstudio and Spark. I get that error message when I try to set up the context in RStudio, but it's fine when I set up the context via the command line. I'll double check tomorrow, but doesn't hurt for you to try to set it up that way. 
Thanks so much for the help! The tutorials make it look so easy -- real life is not always the same. &gt; print(Company[Zips_df$ZXTA5CE10]) data frame with 0 columns and 23 rows 
would have preferred examples in standard R rather than dplyr.
I got *Error establishing a database connection*
Thanks. This is what I'm looking for, but was hoping for step by step how to get there. How do I create the "matching R matrices" for models, year, color. Let's say total inventory at dealerships is known as Number of cars in inventory on lot ------------------------------------------ Dealer #1 350 Dealer #2 420 Dealer #3 280 
If you are using Rstudio to compose your .Rmd file you can click on the settings and select to keep the .md file in the advanced tab. This will add a line into the YAML at the top of the .Rmd so that when you knit the file it will not clean up the .md file. I believe it's something like keep-md: true You should be able to find it with a bit of googling/reading the docs 
Thanks. I know how to place values in a vector and assign it to an R variable. I should have been more specific. What I'm really looking more at how the Prob library would assist mitigate the effort in solving the problem using R. Specifically how it implements fact tables and the operations though the Prob library to assist in solving this problem. There are plenty of tutorials on the core R language but not too many on the Probability, library Library(Prob) 
Awesome, Thanks. That's exactly what I needed.
Do they actually have vectors of values or strings with several values separated by spaces in them? (with class() you can get the class of a value or just by returning it's value you can see if it is a vector or a string, if it's a string you probably want to use strsplit two separate the values and then as.numeric to convert them to numbers) R in general can handle vectors of values fine, it can have problems/undesired behaviour when using two vectors at once in a function like '&gt;', since it recycles values etc.
May have spoken a bit early, I'm a bit out of practise with R. Melting/casting is for rearranging a data frame to long/wide format, without really touching the values of each cell. You can either split them, and place one value in its own column, using the space as a delimiter, but that is only viable if the values sync up with the values in the other column. Assuming the picture you've shown is a real sample, it looks like DistanceToRadar only contains one value, and TimeToEnd is where the values are changing. We can split the one column into however many required columns and then work from that.
This stack overflow question appears to be asking the same question that you are: http://stackoverflow.com/questions/16668699/breaking-up-melting-text-data-in-a-column-in-r
This looks like the right path to me. Create a function that takes in one row. This should pull apart TimeToEnd and DistanceToRadar using strsplit. Then pull together Id, TimeToEnd, DistanceToRadar, and maybe a trial number. The function should output what you want one row to end up being. Put this function into a by, apply, or do.call. Reassemble the dataset.
I have successfully (and easily) built packages for windows using devtools and roxygen2. [This tutorial]( http://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/ ) walks through a simple example.
&gt; Why can't I simply bring those packages over to the 64-bit Windows R? Because the authors of the R language made the rather poor decision to restrict binary compatibility between OS products. It's understood such a restriction would be in place when native C/C++ code must be referenced by the methods in the package but my package contains 100% R code and nothing else. So a) you are saying should handle the special case where only R code is used separately even if it is only a minor problem and b) R code can be compiled and I believe is compiled by default when building packages since a few versions, so it is no different than C/C++ code &gt; Yet all documentation suggest it still must be built for each environment (can anyone confirm this incomprehensible bad design is actually true?). Well you can also provide the package as source code, which should make everything relatively trivial To solve your problem, did you following all the steps, especially setting PATH as dasonk pointed out: https://www.biostat.wisc.edu/~kbroman/Rintro/Rwinpack.html Alternatively you could also try out: http://win-builder.r-project.org/ PS what 500+MB? as far as I see Rtools.exe is &lt;50MB and that should be all you need additional to R
No. Other documentation on-line implies Rcmd.exe has been retired. Thanks for the reply.
R Tools are certainly NOT all you need. At bare minimum if you want to provide any level of help, there are additional tools for building help documents required. I see examples of setting RTools path that includes a reference to .../pearl/bin and pearl isn't even part of the current Rtools.exe 64-bit distribution. Neither is \MinGW. Yet the Environemnt Path examples would imply at one point both of these sub-directories existed inside RTools. It's a fucking train wreck trying to decipher where to fill in the gaps. Don't believe me? Go download the 64-bit distribution of RTools. Here is where I got it. https://cran.r-project.org/bin/windows/Rtools/ Also by inspecting other libraries installed it appears when no native code (C/C++, etc...) is linked the code in the library is just uncompressed plan-text R. Yet still there is documentation claiming separate package builds are required for Mac OS X, Linux and Windows. That makes no god damn sense whatsoever and is a huge burden on both end-users and developers. I guess this is what you get when you have Statisticians creating software instead of real Computer Science engineers. It speaks to the failure of the R community that this sort of thing isn't better supported. All tutorials for Windows are conflicting and offer incomplete advice. I don't think highly of the R community. 
&gt; RTools ok, after checking the website that does really look like a train wreck &gt; Also by inspecting other libraries installed it appears when no native code (C/C++, etc...) is linked the code in the library is just uncompressed plan-text R. I don't know which version you have (and in windows it could be different), but for me on linux I have to tell the package compiler if I want the source code included or not, if not it only packages compiled bytecode. And as I said, you can create zipped packages from which you can install (after briefly reading the docu, this should work on windows if you have no C/C++ code, if you do you need Rtools...). &gt; That makes no god damn sense whatsoever and is a huge burden on both end-users and developers. Given that 99% of the packages are provided either via CRAN or a source repository, I have never noticed this huge burden before. &gt; It speaks to the failure of the R community that this sort of thing isn't better supported. Or it reflects the fact that the R community is largely a unix community.
/u/murgs: it looks like your solution should work. Thank you. /u/dasonk: I will hopefully be using the function I'm building to make graphs in bulk so I'll be using loops only as a last resort. Sorry, I should have made that more clear. Thanks for your help
Oh we love avoiding loops whenever possible. With that said if you tell me you can't do something and I think it's entirely possible I will definitely let you know... even if it's not a good idea to do that thing. 
I love this response so much. Thanks for keeping this subreddit so classy. 
To play devils advocate, I would ask that you consider that the world of corporate programming is very different from Scientific Programming and it always has been since the days of FORTRAN. The contemporary programming languages available today for Professional Developers provide you with luxuries that have not trickled down yet to more niche languages such as R. Cross platform binary compatibility is such a feature. Will R every get there? Does it really need to? At this stage the R language is well established and not likely to go through any major overall just so it can better fit modern paradigms. If a commercial R distribution does rise to that challenge, i expect it will go over about as well as Microsoft's FORTRAN.NET has. It will probably find a home with certain customers, but I don't see open source R and commercial applications like R Studio going though major architecture revisions any time soon. Could the packaging paradigm for R be improved? I agree by any standard the way it was designed is quite bad, but honestly the pain is primary felt when trying to build packages on Windows. There is little question that the R community is predominately on non-Windows platforms. Instead of building proprietary tools for things such a documentation generation, R was designed to rely on open source third-party support. I suppose the "duct-taped together" analogy is as good as any. Yes, that means you're depending on third-party tools in your build pipeline that may also have lengthy documentation that you must study to utilize properly. Yes, it sucks. But again this is the nature of the open source community. It's part of how this community provides the quality of tools that it does for FREE. If there is an answer to helping R that involves commerical software vendors, it has to do with better tools being brought to market to abstract the existing R design paradigms. R Studio is an example of this. It is what it is. For what it's worth I can relate to some of your frustration. Now to try and provide some help regarding Windows packages 1.) Download R x64 3.2.1 for Windows. Avoid RRO RGui 64 3.2.1. They appear to be the same but RRO version is missing a few important files you'll need for building packages. 2.) Download RTools.exe. Yes, it's changed a bit and some of the paths you'll see referenced are now incorrect. Just make sure you get the latest version and that you get the Environment path references in the correct order. Remove the ones that are no longer valid. 3.) "R CMD build" for Unix platforms translates to "RCMD build" on Windows (no space between R and CMD). 4.) I have never been able to get "R CMD INSTALL package" to work on Windows. Instead from inside the R environment try setwd("C:/FolderWherePackageFileIs/"); packages&lt;-dir(); install.packages("yourpackagehere.tar.gz", repos=NULL) Good luck. 
you can split each string by number of characters with substr: http://www.inside-r.org/r-doc/base/substr
You can use the groupby method in the dplyr package to group the data based on different parameters and then just plot it using whichever package you're comfortable with. 
Try top_n() function from dplyr package.
Thanks for the reply! I will try this. :)
A loop is a super inefficient solution to this question.
you can select from a table or dataframe like so: dudes&lt;-dataframe[dataframe$Sex==1, ] whitedudes&lt;-dudes[dudes$Race==1,] then plot(whitedudes, whitechicks) 
it sounds like you many want to sort the data by a certain variable before selecting out cases using nrow (e.g., the cases with the highest values for a certain variable). see my response teigue below... 
You're right. Double quotes don't have to be attached. Problem solved. For some reason when I was attaching double quotes earlier it wasn't working, but for a different reason. Now I get the results I wanted. For the record, it's: &gt; nodename&lt;-as.character(prodlist$Node[prodlistcounter]) &gt; price&lt;-html(page_source[1]) %&gt;% html_nodes(nodename) %&gt;% html_text()
Note that the output given looks to be exactly what you want. It's a string that contains the literal text " .sixteen " (although maybe you wanted paste0 to avoid the spaces at the beginning and end). The slashes aren't actually a part of the string (you can check this by using `cat` on the output) but are there to show you that the quotes are escaped.
Thanks I will play around with cat(). 
R is a data analytics language, not a general purpose programming language. It really doesn't make any sense to compare R with scala. R can be compared with the significant sector in the python world that focuses on data analytics, though. 
Why does it return a valid value when I use the literals: &gt; -0.007777519 ^ 0.09980119 [1] -0.6158904 
Order of operations. Put parenthesis around the first number.
From this it looks like it is interpreting it as -(0.007777519 ^ 0.09980119). Try like this and then with opening parentheses before negative sign and closing before caret. 
Indent your code 4 spaces. When you edit your post, click "formatting help" at the bottom right. 
That worked. Thanks for the tip
&gt; -0.007777519 ^ 0.09980119 [1] -0.6158904 is doing -(0.007777519 ^ 0.09980119), while a ^ b is doing (-0.007777519) ^ 0.09980119
I don't have R in front of me right now, but this sounds like a job for the cast function from the reshape package to get the data in a more usable form.
&gt; You're taking a root of a negative number. I'm very much an R n00b so forgive the question, but does R not support complex numbers?
Aaaaah. That explains it. Thanks!
Unfortunately I was explicitly told to use apply, but I was able to figure it out. His book barely mentioned the x~variable syntax but i was able to find other examples on it online.
you can use complex numbers (see ?complex) but when working with typical numerical numbers (integers or doubles), functions do not suddenly return a complex number (this is how most programming languages handle the situation)
What is the OS for the remote folder? Local folder? Is there SSH? Samba? Dropbox? * [Samba guide](https://www.linux.com/learn/tutorials/296391-easy-samba-setup) * [Dropbox](https://github.com/karthik/rdrop2)
I used dropbox for a similar problem. Installed dropbox on the remote computer and saved the file there, installed dropbox on my own computer and read the file from there. I had R Running on the remote computer saving simulation results into csv files in the drop box regularly. I then sat at my own computer in my office analysing the results.
Just a note on your speed claim. For the benchmarks I've seen, dplyr was comparing favorably to pandas for most operations, on version 0.2 or some such. Not sure about today, but it seems like dplyr has gotten some pretty significant speed increases in the versions since. For instance, base functions like setdiff and intersect have been implemented in C++ and masking the base functions when you load dplyr.
I can't comprehensively compare the two languages, but a few thoughts: 1. Check out Hadley Wickham's free book [Advanced R](http://adv-r.had.co.nz/), which may be more of an intermediate text but is a great reference for good practices in R. 2. You could also look at the [Google R Style Guide](https://google-styleguide.googlecode.com/svn/trunk/Rguide.xml), for more best practices. 3. YMMV but I've found the combination of Python and R to be really powerful. R is great for statistics, and has a critical mass of support and research behind it...Python is catching up with some of R's capabilities, but can't hold a candle to the vast number of packages for any type of analysis. Writing code in R will make you better at using it, but I still find that it's sometimes easier to use Python to clean data before using it in R. I see R as the statistical analysis software and Python as the glue that binds everything together and makes it work.
"Check out Hadley Wickham's free book Advanced R, which may be more of an intermediate text" Do you know of a book more advanced than Advanced R ? 
What caught me out going the other way: R will happily copy data at every opportunity, whereas python has to be explicitly told to. e.g in R: a &lt;- seq(1:100) b &lt;- a b[1:10] &lt;- 0 Any changes to 'b', will not effect 'a'. Whereas in python 'b' is a view of 'a', and 'a' would be altered as well (unless you explicitly tell python to copy).
Ha! I was caught by that too, the other way round.
The link /u/mppp posted looks great...also being sure you fully understand everything in Advanced R will get you up to advanced, plus reading Wickham's book on R packages, and poking around the code of basic R functions.
Did you try a different starting value?
Wow. That is too much for me but thank you for the link.
Thank you.
Matrix representation can be column major or row major, when interfacing code this leads to many bugs hard to spot if you are not aware of this 
This is weird. I do all my analysis in Python. In python if I say, A = somedataframe B = A B = B.drop("PATIENT_ID") A will still have the column "PATIENT_ID" but B will not... 
Interesting question. I'm not that adept at R and had trouble following your code, but I don't think your code is determining probability, it's running a simulation 1,000,000 times and then taking that proportion as a probability. Probably close enough for this example but bad practice in determining probability because you are extrapolating from a simulation instead of calculating the probability itself. Following your approach (roughly), I wrote a short Python program and iterated it over 10 million runs of 6 rolls and came up with a percentage of 1.54% My pgm (Python 3.4) import random roll_number = range(1,7) rolls = 10**7 all6 = 0 dupes = 0 while rolls &gt; 0: idx = 6 results = [] while idx &gt; 0: x = random.sample(roll_number,1) results.append(x[0]) idx -= 1 if len(set(results)) == 6: all6 += 1 else: dupes += 1 rolls -= 1 print(all6, dupes) print(all6 + dupes) Still, the result surprised me because I would have thought the probability would be (1/6)^6 which would work out to 0.00214%, obviously much smaller than 1.5%. Thinking further I realize that the .00214% number is the probability of any **ONE** sequence of 6 numbers being rolled. Because your problem doesn't require a specific sequence then the answer is not the .00214% number, but rather the combined probabilities of every possible sequence where all six rolls are unique i.e. {3,2,6,1,5,4}, {2,3,1,5,6,4}, {6,2,1,5,3,4}, etc. . At any rate I don't think the question is asking you to run a simulation, but rather to compute the probability. To do this, you need to divide (total possible combinations of sequences where each of the six throws are unique) by (total number of possible sequences). Read up on [combinations and factorials](https://www.mathsisfun.com/combinatorics/combinations-permutations.html) for more detailed explanation. In Python 3.4 from math import factorial # total possible combinations of six rolls of the dice where # each roll is unique unique_six = factorial(6) # total all possible combinations of six rolls of the dice all_possible_rolls = 6.0 ** 6.0 # probability of Six rolls of die coming up with 6 unique numbers probability = unique_six / all_possible_rolls as_percent = 100 * probability print("Probability of six rolls of die coming up with 6 unique numbers is " + str(as_percent) + " %") which returns 1.54320987654 % matching the results of my simulation, but independent of any simulation.
Absolutely. Mine is not a hard claim. I am just assuming that having explicit indexes helps achieve better performance at the cost of easy of use.
Perhaps the many different ways to do OOP in R. S3 is basically like a IF function, while Reference classes are more like python - with mutable objects etc. But the packages that use RC are few and far between, so to me, as someone who only know R, it seems counterintuitive that I can mutate an object instead of explicitly assigning the results of a function to the same name.
I didn't (even try to) read your code. It's easier on you and us if you break it up into separate lines. Anyways, here's how I would do it. Use the sample() function. It picks a number of samples from a set that you give it. sample(x = 1:6, size = 6, replace = TRUE) "replace" means that numbers are "put back" after they get chosen. So the above statement is the same as rolling a dice 6 times. Did each number get chosen exactly once? Ask like this: rolls &lt;- sample(x = 1:6, size = 6, replace = TRUE) setequal(rolls, 1:6) Now do that a million times and keep track of how often 6 numbers come up once. count = 0 for(i in 1:1000000){ if(setequal(sample(1:6, 6, replace = T), 1:6)) count = count + 1 } count / 1000000 Edit: this assumes that you want to do it via simulation instead of calculating the real probability. 
Apologies for the ignorance, but the Rscript command is downloaded with R? Rscript apparently works by "Rscript [options] [-e expression] file [args]"
Nevermind. It appears "Rscript" is a command in utils. 
Without more details: if your for loop doesn't have any side effects (i.e. there is only one assignment at the end which is the information conserved) and the lines are all processed independently, rewriting the for loop as an sapply function is possible. But it might look more complicated and therefore less understandable, why don't you just pack your code into a function called read_odd_file_format() then it doesn't really matter how ugly the internal implementation is EDIT ok you might want something like: blocks &lt;- lapply(1:(length(Sublines)-1), function (i) blah[SubLines[i]:SubLines[i+1]-1] ) clean_blocks &lt;- lapply(blocks, block_cleaning_function) the_one_table &lt;- do.call(rbind, clean_blocks)
Thank you for this, I will look into implement this method. Any suggestions on how to handle concatening data with different columns? All the data shares 10 column names but sometimes only 1 or 2 are actually present on groups. Should I assume they always have 10 but fill the unused items with NA's?
I'm going to tomorrow morning. Thank you!
And that did not work. 
Hmm, I don't have access to the actual database, just the dump files. FWIW, I'm trying to play around with the Ashley Madison files. I don't have nefarious intentions, just thought it'd be an interesting way to work on my R/data science chops. 
/u/backgammon_no had a good point; you can do it via simulation, but even then, he used a for loop. If I were to do it, I'd create a function that picked the numbers (using sample) and checked to see if it was equal to the set 1:6. Then I'd use the replicate function on it, and check to see how many trues were returned. booleanlist = replicate(1000000, checkIfEqual()) booleantable = as.data.frame(table(booleanlist)) booleantable = subset(booleantable, booleantable$booleanlist == TRUE) print(booleantable$Freq[1]/1000000) Using this method, I got approximately .01548 on a simulation of one million, which isn't far from the actual probability, which is given by (6!)/6^6 
Is replicate faster than loops?
It should be faster than for loops written in R, because replicate runs the for loop in C. 
I find that having a dataset and a problem in mind is really useful, otherwise it's all a bit abstract. You can sit and learn about mapply vs tapply all day, but it's going to make little sense unless you apply it.
I like the udacity course to get you going and using rstudio, which is an awesome IDE
It is definitely the best out there right now. I have no idea how they keep the session like they do. I may have been looking into writing my own IDE :)
I wanted to emphasise how much I agree with this. One thing I'd add is that it's worth forcing yourself to start using R when you run into a problem for which you might naturally reach for another tool. I had been interested in R for some time, but always ended up using Python or other tools because, when a problem came along, I wanted it solved right then. Eventually, I forced myself to use R for a small, particular case; after that I found more and more cases where it became the natural choice.
It takes some time, but I had a great experience with O'Reilly's *Learning R*. It takes you step-by-step from the basics up to the programming and graphics skill you'd need to explore your own data. 
This is a good solution. For some context as to why this works: `(x &gt; 152.5)` will return a vector of length `length(x)`, with each element being a TRUE or FALSE value indicating if the corresponding element in `x` is greater than 152.5. If we have `x &lt;- c(100, 200, 320, 5)`, for example, `x &gt; 152.5` will return c(FALSE, TRUE, TRUE, FALSE) In R, `TRUE` is equivalent to the integer 1, and `FALSE` is equivalent to 0. So when you sum the last line in the output above, you're really running sum(c(0, 1, 1, 0)) This evaluate to 2, showing the number of elements in `x` that are greater than 152.5. ~~(Sorry for the poor formatting. On my phone at the moment)~~ EDIT: fixed formatting
I never thought of ooglag's solution, the way I usually do it, replace one with the sequence and the second with an sapply: r &lt;- 1:25 sum(sapply(0:23, function (j) sum((p^25 * choose(j, r) ) + (q * j))) thinking about it, I'm unsure choose can handle vectors, in which case you would have to extract it as a second sapply (just inside the inner sum)
Disclaimer: I haven't done this. It looks like you want to [superimpose a function](http://docs.ggplot2.org/current/stat_function.html) though, and have that function be the [cauchy function](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Cauchy.html). 
nope, I ended up finding the solution by using a dataframe and subsetting.
Might want to post the solution in case others ever have this problem and are searching for it.
[removed]
hist(case0301$Rainfall [case0301$Treatment == 'Unseeded',] , freq=FALSE,br=75) Have a read on some methods for subsetting your data: http://www.statmethods.net/management/subset.html https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html
Thank you! I will bookmark these.
first start each code line with 4 spaces for proper formating paste creates a string (or a list of strings), so you are passing t.test two strings, but you really want to pass him the vectors of means or? how about: groupmin &lt;- c(group1mean,group2mean,group3mean,group4mean,group5mean,group6mean)[min] etc. PS you can simplify your initial code by using something similar to: group_means &lt;- tapply(data1$response, rep(1:6, each=8), FUN=mean) It applies the function mean to each subset of the first vector defined by the second vector. Similarly you can use split() to create the individual group vectors, or only create them for groupmin/max by subsetting based on the index e.g. groupmin &lt;- data1[(min-1)*8+1:8,"response"]
Abandon plot(), ye who enter here These are some good resources on ggplot2: http://docs.ggplot2.org/current/ http://rpubs.com/hadley/ggplot2-layers http://rpubs.com/hadley/ggplot2-layers 
data &lt;- data.frame(list("s" = s, "w' = W)) ggplot(data) + geom_scatter(aes(x = s, y = W)) + geom_smooth() + scale_x_log10() + scale_y_log10() (ps, those are terrible variable names)
Tried a few but I haven't had lucky guesses maybe 
As the others mentioned lwd is just line width. TRUE is not a function but a logical/boolean value, and in R generally TRUE and FALSE are used (T and F is also possible). All constant variables (which they effectively are) are all caps in R. Basically it's just a syntax definition.
Nice that worked perfectly!! Thank you :)
Why not use python? (not trolling!) BeautifulSoup etc
Like u/battez, I'm not entirely sure why you want to use R for this purpose, but here's something to help you get started. First off, the [rvest package](http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/) conveniently allows you to use either CSS and/or XPath selectors for parsing your HTML. It even allows you to use selectors on individual nodes, as you'll see in the code sample below. But first, [here's a terrible reference for CSS selectors.](http://www.w3schools.com/cssref/css_selectors.asp) [Here's a tutorial for XPath.](http://www.tizag.com/xmlTutorial/xpathtutorial.php) I've personally worked with CSS more and am more familiar with it. They're less powerful for navigating a DOM tree compared to XPath, but for the most part, they'll do the trick. Basic idea with CSS selectors is that they are cascading (i.e., they go from the top down). For example, in the HTML file you have, `.thread` would select all the tags that say `&lt;x class='thread'&gt;`, where x is an HTML tag (e.g., `div`). Alternatively, `div.thread` would select all `&lt;div class='thread'&gt;` in a document. Then, `div.thread .user` would select all the tags that say `&lt;x class='user'&gt;` *within* tags that say `&lt;div class='thread'&gt;`. And so on. There are a list of other selectors from the [terrible reference linked above](http://www.w3schools.com/cssref/css_selectors.asp). Anyway. I'm rambling. Here's the help I promised: [See this gist for the HTML and R files](https://gist.github.com/anonymous/0b6778f1b99f2f700309). I'm leaving out the HTML here because it follows the format you posted and is not essential to the answer. And this R: library(rvest) library(stringr) # test.html is the name of the relevant HTML file on your computer. Replace with # the relevant filename. Can also be a URL. test_html_file &lt;- html('test.html') # Loop through the threads. The "css = " is not strictly necessary. It's # included for clarity. threads &lt;- html_nodes(test_html_file, css = ".thread") # Make a matrix for storing the data. data &lt;- matrix(NA, 0, 4) colnames(data) &lt;- c("participants", "user", "timestamp", "content") for ( i in 1:length(threads) ) { thread &lt;- threads[[i]] names &lt;- str_trim(html_text(html_children(thread)[[1]])) # Find all messages in the given thread messages &lt;- html_nodes(thread, css = ".message") # Go through all the messages for a thread for ( m in 1:length(messages) ) { message &lt;- messages[[m]] # Find all the elements with class = "user" (in this case, they are # conveniently all message sender names. message_user &lt;- html_text(html_node(message, css = ".user")) # Same thing for timestamps message_timestamp &lt;- html_text(html_node(message, css = ".meta")) # Find the message (p element). message_text &lt;- html_text(html_node(message, xpath="following-sibling::p[1]")) data &lt;- rbind(data, c(names, message_user, message_timestamp, message_text)) } } print(data) You get: participants user timestamp content [1,] "Alpha, Bravo, Charlie, Delta" "Alpha" "Thursday, August 18, 2011 at \n 11:53pm PDT" "Yo Bravo" [2,] "Alpha, Bravo, Charlie, Delta" "Bravo" "Thursday, August 18, 2011 at \n 11:54pm PDT" "What?" [3,] "Alpha, Bravo, Charlie, Delta" "Delta" "Thursday, August 19, 2011 at \n 12:05am PDT" "Who are you two and what are you doing in my thread?" [4,] "Echo, Foxtrot, Golf, Hotel" "Foxtrot" "Thursday, August 20, 2011 at \n 09:53pm PDT" "What kind of a name is Echo?" [5,] "Echo, Foxtrot, Golf, Hotel" "Echo" "Thursday, August 20, 2011 at \n 10:55pm PDT" "Yeah? Well, you're named Foxtrot." I should add that I used XPath on one of the lines because the CSS alternative wouldn't have been as nice. This line: # Find all messages in the given thread messages &lt;- html_nodes(thread, css = ".message") Could be replaced with this instead: # Find all messages in the given thread messages &lt;- html_nodes(thread, css = ".message") # Find all message contents (finds the p elements following each message). + is the sibling selector. messages_contents &lt;- html_nodes(thread, css = ".message+p") And this line: # Find the message (p element). message_text &lt;- html_text(html_node(message, xpath="following-sibling::p[1]")) Would've been this: # Find the message (p element). message_text &lt;- html_text(messages_contents[[m]])
... isn't that a copy and paste from ?aaindex ? I already looked through that but it doesn't give an explanation for how the numbers were obtained or what they signify (how to interpret) 
Thanks for this. I had up to getting the list of nodes that are threads, but couldn't get the loop part to work (Yours works, I mean I couldn't get it before). With the XML library, the equivalent of "html(test.htm)" is "htmlParse(test.htm)". The equivalent of "html_nodes" is 'getNodeSet'. And the equivalent of "html_text" is "xmlValue". I got to the list of threads before with: x = htmlParse(test) y = getNodeSet(x, "//div[@class = 'thread']") But then I had problems when I was testing anything else, for instance: z = getNodeSet(threads[[1]], "//div[@class = 'message']") This line causes R to crash. I think it should return the node set of messages which I could then apply "xmlValue()" to. But a) I didn't have a small version of my dataset, and I didn't know how to subset something that I couldn't parse in the first place and b) R crashing sucks and I thought this must be such an easy solution so I asked here. The reason I'm not using python is that I don't know how. So basically I'd be in the same boat except I wouldn't have an idea what even the most rudimentary functions are, i.e. coercing vectors to a matrix, etc... Obviously I could learn it, but I actually need to get this done for a visualization assignment, so I don't have all the time in the world to mess around learning python, when I know R could do it. Thanks again. This really helps me out.
Yes and well: &gt; Description Data were imported from release 9.1 (AUG 2006) of the aaindex1 database. So the data was imported from a database &gt; Source http://www.genome.jp/aaindex That's probably it. And if the website doesn't explain it, one of the papers regarding the database should... So as I said, it is a question regarding the amino acid index database, so you will find your answer there, not with R users (of whom &gt;99.99% have never heard of the data in question, let alone used it) As an analogy, you are effectively asking Microsoft why the Polish language has a specific spelling rule, because Word uses a Polish dictionary.
In addition, what you might want to do is take the mean of a larger number of samples. For example, if I asked for 100,000 random numbers between 1 and 10 I would likely have an even distribution of all of them, but the mean would be 5. If I ran that same simulation 1000 times, the mean would deviate somewhat, but would typically be very close to 5. See how the mean gets closer to .5 the higher number of samples: &gt; x = runif(10); mean(x) &gt;[1] 0.3734271 &gt; x = runif(100); mean(x) &gt;[1] 0.487949 &gt; x = runif(10000); mean(x) &gt;[1] 0.5012995 &gt; x = runif(1000000); mean(x) &gt;[1] 0.5001245 
you should be able to skip the *isTRUE* val&lt;-ifelse(gamesDF$Dupe[gdf_pos]==FALSE,do something,do something else))
I had the same problem you had. I emailed Quandl about it, and they told me just to run &gt;install.packages("Quandl") And that should install 2.7.0. Worked fine for me.
what kind of stuff do you want summarized? do you want mean scores? Or do you want a new data frame for each club? tapply() can give you averages (or other function) with a grouping variable; you could also look into aggregate(). dplyr would also be an option doing groupby(HomeTeam). tapply(goals$HomeScores, goals$HomeTeam, mean) should give you means of home scores grouped by HomeTeam. 
i'm guessing at this b/c don't have data or R in front of me and it'll be a little more involved. i'd try something like: (aside: if you haven't seen it, the %&gt;% is a pipe operator in dplyr and other package that links statements.) library(dplyr) home_team_sums&lt;-goals%&gt;%select(HomeTeam, HomeScores)%&gt;%groupby(HomeTeam)%&gt;%summarize(sumHomeGoals = sum(HomeScores)) repeat for away_team_sum then you'd need to join them and sum to get total goals byteam&lt;-merge(x= home_team_sums, y = away_team_sums, by.x=HomeTeam, by.y = VisitingTeam) there is another argument for all.x, all.y or all depending on the type of join. I'd guess all = FALSE would work if all the teams are in both columns. then this should work byteam$AllGoals&lt;-byteam$sumHomeGoals+byteam$sumVisitGoals 
Thanks. I solved it a few hours after I made the post by reinstalling R in a root folder. Apparently there is a glitch for some package codes where it only works if R is in a root (e.g. C:\\R instead of C:\\programs\blahblah\R)
Thanks, you're awesome! I think I have enough to get going with this :)
I think using dplyr package is easier to code and maintain and that's what I do in practice. Hint dplyr Group_by Summarise From my exp. This approach is a lot easier than base R
Use dplyr There are good video tutorials and Bing search for the dplyr cheat sheet
hey, we found the Microsoft employee! 
try this: # a list of names var_names &lt;- letters() # iterating through the names for(i in var_names){ # assign a random value to the selected variable name assign(x=i, value=rnorm(n=1)) } # list the variables in the environment ls() # accessing the value of the 7th variable get(var_names[7])
No, there is actually a column in the data frame that says 'ID'. What I pass into the function should go here: alldata = do.call("rbind", lapply(temp[id], read.csv, header = TRUE))[complete.cases(do.call("rbind", lapply(temp[id], read.csv, header = TRUE))),]
K let me look at the dplyr instead of plyr.
a few pointers: setwd(directory) without setting it back is dangerous since multiple calls of the function will not do the same thing. It is better to read the files from the directory, without changing the working directory. do.call("rbind", lapply(temp[id], read.csv, header = TRUE))[complete.cases(do.call("rbind", lapply(temp[id], read.csv, header = TRUE))),] You are reading in the file(s) twice, don't do that, make the subsetting a separate line lapply(temp[id], read.csv, header = TRUE) so id is actually several id's and not just one, otherwise you don't need lapply etc. I'm sure summaryBy has advantages, but in this case a tapply call would do just as well as far as I can see. monitorobs &lt;- tapply(alldata$sulfate, alldata$ID, FUN =length) I haven't used summaryBy, however from the examples you might need to leave away the data.frame name e.g: monitorobs = summaryBy(sulfate ~ ID, data = alldata, FUN = c(length)) Possibly the same with rename. EDIT just realized for the specific case table(alldata$ID) should return the same, since length only returns the amount, independent of the values, so alldata$sulfate should basically be ignored 
Thanks. sulfate ~ ID was right. Plus the "alldata$sulfate.length" = "nobs" should have been "sulfate.length" = "nobs" when I renamed the column headers. Then the function worked. 
Reading through the documentation on it. Seems like it works great but can't find many examples online yet. I'll keep looking.
You can use the argument `skip` in read.table to skip a certain number of lines. Sometimes you can just skip a certain number of lines and then just read a file like it is a normal tab or comma separated file. Do you think that would work here? The argument `comment.char` also allows you to specify what lines that are comments start with. You may find that preferable here.
 read.table("http://pages.stat.wisc.edu/~jgillett/327-1/4/beef.txt", comment.char = "%", header = TRUE) This does the trick. I didn't know about this, thanks for posting.
&gt; read.table("http://pages.stat.wisc.edu/~jgillett/327-1/4/beef.txt", comment.char = "%", header = TRUE) omg thank you SO much! I was struggling with this for so long. 
Thank you, edited post
are you sure about that? http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/ the sample data conversion here leads me to assume that long data actually has only a single data point per row. Otherwise you would have multiple conditions in the same row. http://seananderson.ca/2013/10/19/reshape.html similarly here, the long format only has a single datapoint per row. it may very well impossible to do what im asking in pure long data, definitions aside. Which is what i was trying to figure out. 
I'm not sure I understand your question. But I know that RStudio has some server tools. I think you have to pay for them ...
FastRWeb might do what you need, I've used it with success.
I done some work on using R through a jvm restservice using rosuda java and rserve. Using a clojure wrapper around rosuda. https://github.com/skm-ice/rincanter. Currently using the rincanter in a rest service with a simple connection pool with fixed numbers of RConnections. 
it should be. i'd assume turning the 4 variables into 1 factor variable w/ 4 levels and use that variable in the facet_grid() would work. i'm unsure what you are using to distinguish between the 4 csvs, but it'd be a simple subset if you have a variable that distinguishes them. mydata$CSVDistinguisher&lt;-'Error' mydata$CSVDistinguisher[mydata$CSV1== 1]&lt;-'CSV1' ..repeat for CSV2,3,4 may need to do as.factor() after 
Really cool
You are very close. Im sure the StackOverflow answer will help. But have you looked at the 'subset' function? It can subset your data based on a criteria; for eg Arsenal and only data with Arsenal will be selected. 
To make code look like code, put four spaces (or more for further indentation) at the start of the line - all of your code is currently one one line and very hard to read. I can tell now that you've got a loop where i is going from 1 to 100. If you reformat it so the code is readable I could say a bit more.
Thanks for the advise. I edited it.
Right, but as you go further along, what does it mean? So for the next part with the creation of the coh variable. You're creating a dataframe where cohort=i? and date equals a vector which i:100? Forgive me, but I'm still a little lost.
[removed]
First, that code won't do anything - there's no closing brace } to end the for loop. Adding an ending brace to the end wouldn't really make any sense either - all the for loop would do is make a data.frame with some weird columns. And I don't know why someone would use replicate(1, ...) - you are telling it to repeat the bit in parentheses once, which means you could've left out the replicate and the results would be the same. And while the code will create a data.frame, it doesn't do anything with it so it will just be overwritten the next iteration of the loop. In the end the total effect would be the same as: coh = data.frame(cohort=100, date=100, week.lt=1, num=sample(c(1:40), 1, rep=TRUE), av=sample(c(5:10), 1)) print(coh) Which just gives: cohort date week.lt num av 1 100 100 1 39 10 Where the 39 is a random number from 1 to 40 and the 10 is a random number from 5 to 10. But each iteration of the loop makes a weird data.frame with however many rows of 'i' for the cohort column, numbers 'i' through 100 for the date, week 1 through 'i', and random numbers in those ranges for the last two columns. It will then do absolutely nothing with that data.frame and overwrite it on the next loop. It really seems like this is only the first part of a code snippet.
Well, they are doing some data manipulation that would really only make sense if you know what the the data is. It isn't a generic analysis, it is an analysis that is specialized. A line like: ifelse(max(coh$date)&gt;1, coh$num[coh$week.lt==2] &lt;- sample(c(75:90), 1, rep=TRUE), NA) Checks if the maximum of the date column of the data.frame that was created is greater than 1, however that will always be true, so checking is redundant. Given that the condition is always true, all rows where the week.lt column is 2, will have their num set to a random number between 75 and 90, instead of between 1 and 40 (I have no idea why they would do want that). Overall, if you are trying to learn R, I wouldn't recommend using that code as a source. It looks almost like it is deliberately obfuscated, although it could just be written by someone who doesn't know R that well. The line I quoted above could be rewritten more clearly as: coh$num[coh$week.lt == 2] = sample(c(75:90), 1) And it has the exact same effect. Same with all the other lines checking if max(coh$date) &gt; something) - every single time the loop runs, max(coh$date) == 100, so every single one of those ifelse statements could be removed and just replaced with the bit that happens when it is true. Other issues: like I said before, there is no reason to replicate something 1 time. Also, if you are only sampling 1 time, then it doesn't matter whether you are sampling with or without replacement. The vast bulk of that code snippet is just useless.
Feeding mapply with 2 copies might be a solution: X&lt;-c(1, 3, 6, 4, 2, 9, 10, 1) direction&lt;- mapply(function(x,y) sign(y-x) ,X[-length(X)],X[-1]) dir.change.pos&lt;- which( direction[-1] != direction[-length(direction)] ) length(dir.change.pos) 
simplest case sapply: y &lt;- sum( sapply(2:(length(x) - 1), function (i) { ((x[i] &lt; x[i + 1] &amp; x[i] &lt; x[i - 1]) | (x[i] &gt; x[i + 1] &amp; x[i] &gt; x[i - 1])) } ) sapply returns a vector (or more complex thing) (in this case a vector of TRUE/FALSE's) and sum adds them up to a single value. Now you could go further with mapply as erizon describes or look into ifelse, since it can also be applied to vectors, but thinking of vectors, you can also use them more directly: smaller &lt;- (x[-1] &lt; x[-length(x)]) y &lt;- sum( smaller[-1] == smaller[-length(x)] ) depending on how you want to handle cases with equal values you might have to play around. For example to perfectly match your behaviour, you could also do: smaller &lt;- (x[-1] &lt; x[-length(x)]) bigger &lt;- (x[-1] &gt; x[-length(x)]) y &lt;- sum( (smaller[-1] &amp; smaller[-length(smaller)]) | (bigger[-1] &amp; bigger[-length(bigger)]) )
This is change point detection. Forget apply functions. First, take the diff of the original vector. Then, use sign to find which are increasing and decreasing. Then take the diff again, which will give you zeros, 2s, and -2s. Take the absolute value of that, then count the twos. I'm on mobile but I think it's like this, if your original vector is x. x = diff(x) x=sign(x) x=diff(x) x=abs(x) sum(x==2) 
 sum(diff(sign(diff(x))) != 0) 
Appreciate it. So like, could you explain using i with another example? Perhaps contextualized around baseball? That's generally how I learn most things in R.
 str(points) gives you the structure of the 'object' In this case it will be 2-dimensional since **points[,1]** means take the first column of the object points. (specifically [, take all rows ,1] take the first columns). Just look up matrices and/or data.frames it will be one or the other.
Ah, I finally understand. Thanks!
Thank you. This is very helpful. Just curious, so if the original code was c(1:50), it would do it 50 times?
 for (i in c("a","b","c")) { print(i) } Sorry, I missed the second close parentheses.
If you don't mind going to other packages there is `stri_count_regex` in the stringi package or `str_count` in stringr. Both should do what you want.
These questions seem to be mostly syntaxtual or not knowing the tools available to you. for 2), where df is the data frame colnames(df) &lt;- c("ID", "etc", "etc") for 7) there are countless ways to do this. If it's already a dataframe then subset works nicely. use ?subset to read more. 9) I'm not really sure what is being asked here, but I think it's because I don't understand the science. Maybe you can explain more if you still need help, or give a MWE without all the jargon. to replace all values in a column of a dataframe with NA use df$col &lt;- NA Let me know if you need more clarification. 
thanks for suggestion. will make another post about this package.
Cool, though I'm not sure how necessary that would be, there are so many good tidyr articles already
Run as Administrator? 
Really cool, thanks for the link!
Thanks for the reply. Will check it out. 
Sorry, unfortunately i haven't ventured on to big data. I'm in a similar situation as you, trying to teach myself R for applying to data scientist jobs. Just thought I'd pass some good advice i was given on to you. 
Build a package. It doesn't have to be complex, but you'll learn lots, and it represents a bit of a commitment to learning.
I've never built one, but it's on my to-do list. One of the complaints at my job is that since we primarily use scripts (that can be edited), it's hard to ensure that the results I get today are directly comparable to those you got last year. Packages are essentially a good way to version control your functions.
Thank you.
assuming they are time ordered, you can skip the inner for loop and just do vector operations (there was a nearly identical question here or in Rstats today...), given the long format you will have to do a for loop and subsetting for it, it will definitely be quicker to save the boolean selection vector, it might be quicker to save the selection instead of computing it thrice.
You're just full of tricks.
Here a few reasons why I use R over matlab: * R is free (as in beer or something like that) * R is easier to install (I use Linux, its like one line and then its installed and works) * Matlab had problems installing and doesn't work with my windowmanager and I had to spend time to get it to work, don't know if that changed * I had to rerun a few scripts on linux that worked on windows, they didn't work because excel and some other stuff * matlab relies heavy on modules that can do cool stuff, most universities get them for free but they are damn expensive. I'm not saying matlab is bad, wrong, worse than R or something like that. But I will not buy matlab after university and why should I learn a tool that I will not use? If everybody in your workgroup is using matlab maybe you should use it too. 
Do you understand the problem? Can you do it by hand on a set of 10 numbers with 2 groups? 3 8 5 6 2 9 3 4 7 1
If you look in ?sort you will notice it also mentions ?order, that function might be useful Also it usually helps thinking anout how you can automatise the code, so if the question changes from 10 groups to 5, you just have to change one value (generally, how difficult would it be to change the code if any of the values in the questions change)
Here's your answer: http://stackoverflow.com/questions/3318333/split-a-vector-into-chunks-in-r
11:15, restate my assumptions: 1. Mathematics is the language of nature. 2. Everything around us can be represented and understood through numbers. 3. If you graph these numbers, patterns emerge. Therefore: There are patterns everywhere in nature.
https://xkcd.com/1570/
You have to add 4 spaces in front of every code line (including a empty line before the block) to make it a code block, see the formatting help on the bottom right of the comment box. I forgot most I ever learned about ANOVA again, so I wont try and help (because I would most likely be spreading misinformation), but there are several tutorials for ANOVA analysis in R like: http://www.statmethods.net/stats/anova.html and http://www.personality-project.org/r/r.anova.html If not helping you in what to do specifically, it might help in better formalizing where your stuck. 
http://imgur.com/etXrH8k
Read the question carefully. It says, in any case, print out the vector element's value. You are only printing it if it is not &gt;10 or &lt;10. This means that it will only print linkedin[i] if it is 10, which is not what the question is asking. Also, you might want to change the last print statement.
Thank you! That makes more sense. I was a little impatient so I had to press show me the solution. That resulted with something like for (li in 1:length(linkedin)), I'm not entirely sure why it used li instead of i. Is that because the vector is treated as a list ? Understanding all of this syntax is definitely slightly difficult. I'm getting there mini lego brick by mini lego brick.
It doesn't matter what is used there. It is just a variable. It can be li, i, x, abc, or anything else for that matter, as long as it is a valid variable name, and you will get the same result.
This is the nerdiest and most wonderful thing I've seen on the internet this month. Well done.
Moments like this are why I love programming.
As you said, you want to do webscraping in R. So I typed "R web scraping" into Google and 3 of the first 4 results directed me to the [rvest package](https://github.com/hadley/rvest). And the first line in that webpage reads "Simple web scraping for R". So that seems like a good start
I second this! I had a similar problem earlier and changing it to factor worked fixed it.
Wow, the second to last paragraph regarding other tech Verizon uses is interesting: &gt; Also used: SurveyGizmo, Room.co for secure video chats;Google Hangouts was a non-starter because Google records those sessions, he said, GPG Suite for encrypting communications and RStudio for working in R.
library(quantmod) getSymbols('UPS') returns = diff(log(UPS$UPS.Adjusted)) UPS_Adjusted = returns[returns&lt;=-.03,] print(UPS_Adjusted) dates = index(UPS_Adjusted) days_of_year = strftime(index(UPS_Adjusted), format = "%j") print(days_of_year) Watch out the formatting changed between Reddit and RStudio 
Hi Paddy! Is there a way to make the days the actual day in sequence that it is? For example, if the first day is January 1st and UPS has a 3% loss on January 3rd (but not the 1st or 2nd.) The day number that shows is a '3' instead of a 1? EDIT: I found it, it's the *which()* command. Thanks!
well head(training.set[,"Status"]) would be interesting to see, possibly also class(...) You might just have to re-assing the column with as.factor(...) I see two possibilities, your trainingdata gets scrambled (at least header to columns) or you set stringsAsFactors=FALSE somewhere globally and now are getting the consequences of the vector being strings and not factors.
If you do is.factor(training.set$Status) that will tell you if it's. Factor. You could also try class(). If it's not you should be able to cast it using as.factor()
trainingdata &lt;- read.csv("C:\\.....filepath.csv", header=true") It should be " header=TRUE ".
It means that there is a level in your test set that is not present in your training set. For example, if your training set contains : blue, green, blue, yellow. R will tell you if it finds red in the test set. 
To elaborate: say I had a question on a survey asking people how they feel about umbrellas. I have a variable umbrellaFeels. Say I get 4 responses to this question: "I really like them a lot!" "They keep the rain off my head, so they're ok I guess." "They keep the rain off my head, so they are ok I guess." "Umbrellas suck. I much prefer ponchos." If you try to read that in with read.csv, it will cast it to factor with 4 levels by default (unless you specify stringsAsFactors=FALSE). 1 will be assigned to "I like them a lot!" 2 will be assigned to the next and so on until the fourth response gets assigned a four. Now suppose I collect another two responses and save a new .csv with 6 responses to this question. The first four levels will be assigned as above (assuming they are still the first four lines in the dataset). Perhaps the 5th response was "I really like them a lot!" Because this exact string has already occurred, it doesn't represent a new level, it is assigned to the level 1 (Which was the first occurrence of that exact string). If then the sixth variable was "I don't like this question." would be assigned a 5. And so on. So a factor has levels equal to the number of unique values for a variable contained in the dataset. 
Can you please explain your question with an example? Have you tried the for loop ? 
?seq if it should be an exponential growth use an exponential series with 1:nrow(your_data_frame)
https://github.com/arunsrinivasan/flights/wiki/NYC-Flights-2014-data Not so good for regression, but good for everything else The `MASS` package has a lot of sample data sets that would work.
If you have a week to wait, gain access to the IPUMS USA, but that may be a little more complicated than you are ready for.
/r/datasets 
Great tip, I actually stumbled into this sub today in my search
The package 'metrics' allows you to calculate RMSE, etc. 
not possible to do it my way?
RStudio has a "Source" button in the top-right corner of the editor. Beside the button there's a little arrow. Click on the arrow and you'll see a dropdown with "Source" and "Source with Echo". You want "Source".
I'd use dplyr for this. If you want to use aggregate then ignore this. Something like master%&gt;%select(name, team, opp, score, week, year, recavg, rushavg, qbrat)%&gt;%group_by(name, team, opp, score, week, year)%&gt;%summarise(RecDay = sum(recavg), RushDay = sum(rushavg), qbrat = sum(qbrat)... whatever other stats you want) If data is set up the way I am picturing (I didn't look at your google drive, just read your paragraphs) max instead of sum should work too. 
Thanks again for your time by the way. 
I would create a function using strsplit() to split apart each string, use length() to get the character from the last split index (M, K, etc). Then use paste() to put all of the string except for the last index back together. Wrap that last bit in as.numeric(), and then a set of if statements to multiply it by whatever the multiplier should be for your conversion, for example if it is equal to K, multiply it by 1000.
As far as I know none exist. There are paid certs that come from online courses which include having to pass projects, quizzes, and tests. But these certs aren't guaranteed to mean anything to hiring bodies. Personally, I'd just do some pet projects and host them in shiny or github and reference them on your resume. Further including a link to your stackoverflow account if you are active there would be good. 
Thanks! I appreciate the response, I'll make some pretty documents and host then online then
The company where I work hires a lot of R engineers. A certificate would mean nothing compared to a proven experience and/or examples of developed apps.
Where do you work? (please PM if you'd rather be discrete.)
I've done pet projects on things like projecting future fantasy football scores, analyzing the cost benefits of reaching on players at different times in the draft, made a shiny app that computes the euclidean distances between user inputs and all players in the NFL, a shiny app for visualizing everywhere I've ever lived using leaflet, etc. I've also been fairly active on stack.overflow for things like dplyr, data.table, ggplot2. I'm also in the middle of taking the 80 or so functions I've written over the past two years into 8 distinct packages. They're not anything other people would probably use, and I have no intent to publish them. The point of all of these things is to have nice tidy products to show your programming chops, regardless of what you're using them for. 
Don't suppose you would mention your employer? Maybe atleast the field that hires these r engineers?
PMd
You could edit the function definition in the package itself. This approach could be risky, though, since other function in the package might depend on the output (and the class of the output) from the function of interest. The safest approach is to write a wrapper function as you suggested. If you don't want the function definition in your script, you can always define it in another file and call that file using `source()`. 
have a look at Rstudio it is a solid gui for programming R Just look for a basic R tutorial or online course and do it, there is also an R package (a collection of code from others) you can install that then runs an interactive introduction session: http://swirlstats.com/ There is no need/advantage in learning another programming language before R, each language has its own syntax and creates 'bad habits'. Reading/understanding the principle ideas of imperative and functional programming can help, but the main parts you will learn just by following a tutorial. side note: have a project in mind, that always help to dig a bit deeper, especially since you then have a direction to dig
My opinion on learning R is a personal one so people feel free to disagree. R is an infinite tool box. You can use R to do a lot of things. Someone turned to me the other day and said "Do you know you can make videos in R?" I have no idea what that is or what it would look like or why you would.... but maybe someone got R to do it, I wouldn’t be surprised. You can't learn how to R, because you never stop learning how to do things with R, you learn the "aRt of R" - as I was told. Learning to use R is different for everyone as every useR has a different need. There is only one truth about learning R (apart from learning the very basics that you will pick up in any good R course). In my opinion, that truth is that becoming an adept useR is about learning how to learn things in R. Learning how to take the problem you currently have and search for an answer. Whatever problem you have, someone has had it before. And they wrote about it on a random web page, or a blog, or a git hub page or something. If it’s a relatively common problem, maybe they decided to be a hero and write a function so that you can just plug in what you have, press run and get out what you want. Learning R is learning that to solve nearly every problem you have, you need to search for an answer. Search online, search the help pages, read a lot, try it out and fix it so it works. When all else fails, if you’re lucky enough to have a new problem, then you turn to the forums, you ask other useRs (your colleagues, friends and online useRs) and they, out of the goodness of their heaRts will fix your 6 hours of stress in two lines of code. That is why it’s so hard to find a course that works for you (in my experience) because what your learning may be completely irrelevant to your interest. If it’s a good course then it will still teach you about the art of R, but if it’s not obvious what you are supposed to be learning from the staRt you may lose motivation. That's what happened to me. tl;dr the art of R is learning how to humble yourself, and search for Help. 
Make videos in R? I'm guessing they just mean they can use the animate package to basically record plots? Nobody would do actual video editing in R though. I mean I'm sure it would be possible but it wouldn't be worth it...
Could you also PM me?
That's my point though, you could do it. I have no idea why you would but resourceful people, or bored people have made R do all kinds of things. It is an infinite toolbox.
Also I feel that I failed to really answer the question properly, I would just stick with R. It's really not that difficult or scary once you gather momentum. 
Sorry this is so late. library(stringr) library(magrittr) myVect &lt;- c("400.00K", "1.00K", "0.50K", "1.50M") conversions &lt;- list(K = 1000, M = 1000000) myVect %&gt;% str_extract("[A-Z]") %&gt;% lapply(function(x) conversions[[x]]) %&gt;% unlist %&gt;% `*`(as.numeric(str_extract(myVect, ".*(?=[A-Z])"))) The beauty of this one is that if a new suffix shows up, all you have to do is put it in the conversions list and it will process.
I don't think you need to learn any other languages before giving R a go. When I first started learning R, I went through a bunch of books. The one that I found to be the most helpful is this one: http://www.amazon.com/Beginners-Guide-Use-Alain-Zuur-ebook/dp/B008BB7PL2/ref=sr_1_36?s=books&amp;ie=UTF8&amp;qid=1448732237&amp;sr=1-36&amp;keywords=Introduction+to+R Good luck!
I'd suggest checking out OpenCPU. It's an R Javascript API framework, but claims to be language agnostic.
@zamaterian - do you have an example of how to use rserve with pooled rconnections ? I never knew about this! But I'm interested to know - did you guys try out Renjin at all ? I mean if it works, then it would be great for deployment. If not, any particular reason why ?
I'm very, very interested!! We use jruby, so it will be a little different, but will appreciate any code that you can show. Especially the configuration settings of R/JVM. How are you running Rserve ? something like supervisord or something... that is the part, that I'm also fairly confused about.
Read the csv data into R. Don't let strings be treated as factors or the following tricks won't work d = read.csv("data.csv", stringsAsFactors=F) The following one-liner should get the job done, but returns a needless vector of "Blue Winner"s. You can ignore the output. This trick relies on side-effects (using the &lt;&lt;- operator, which allows you to mutate a variable outside of the scope of the function). Once you've run this, your data frame "d" will be formatted as requested. sapply(1:dim(d)[1], function(x) d[x, grep("Blue", d[x,])[1]] &lt;&lt;- "Blue Winner") While I was tinkering, I tried to first solve this problem using anonymous functions. The lisper in me wants this to work: An anonymous function works when calling rows one at a time... Replace the (1) at the end with any row you want to have replaced. Wrap this in a for loop and you will get your result also. (function(x) d[x, grep("Blue", d[x,])[1]] &lt;&lt;- "Blue Winner")(1) **The following does not work**, but this was what I was originally going for: convert everything at once using a "vectorized" approach. This shorthand leads to every row being treated as the first row. I would love to see someone else make this solution work as intended... (function(x) d[x, grep("Blue", d[x,])[1]] &lt;&lt;- "Blue Winner")(1:dim(d)[1]) 
This is interactive, takes a few hours to do and is a very soft landing into R. http://tryr.codeschool.com/ after that, I would suggest installing R and R studio https://cran.rstudio.com/ https://www.rstudio.com/ After that, alternate between useful projects (work/hobby) and coursera courses http://www.r-bloggers.com/complete-list-of-coursera-courses-using-r-ranked-by-popularity/ Some packages to look into learning (some personal recommendations) plyr, dplyr, ggplot2, lubridate, stringr, data.table 
interesting... I should look into this. I actually got a week of training from them(redundant given my experience/autodidaction) through my employer.
https://gist.github.com/MattSandy/90fc0b9dd45f69683588 here's a gist about [someone](https://www.reddit.com/r/Rlanguage/comments/3mmhhh/r_web_server/cvgg8el) using nodejs and libuv to run R code. I wonder what you think of that ? Also, I'm a bit confused about your statement *" what I do is create a bunch of connection and possible load initialize them with some R code or rds. If is a lot of data, thats needs to be shared across Rconnection (to avoid having the same data multiple time in memory) i'm loading at R startup, just before starting Rserv"* I'm confused on how you are doing this in production ? are you using a R script that loads data or something else.
&gt; before when starting R -e "source(foo.R); bar() ;library(Rserve); run.Rserve(args='--no-save --slave', port=6311" This will start R and load the foo source file and execute the bar function, and last start the RServ on port 6311. Currently we are loading most of our static data via a rds at R startup, (add the rds at the deployment time in the docker image.) and then all the data/source will be available for all the RConnection started from java. But you could easily load your data either via the RConnection or calling a R function that access your database. When we starts new Rconnection, its is initialized with newly updated data or source files - before being added to the pool. When the initialized resources changes, the pooled Rconnections are closed. And reopend with the new data. In the future we will probably drop using R directly from java, and start using sparkR since our datastorage is cassandra. 
For hierarchical clustering I will use 1-correlation to get distances. Assuming `theData` is your microarray data with 134 columns and 48701 rows. distMat &lt;- as.dist(1-cor(theData)) # 1-similarity to get dist. tree &lt;- hclust(distMat) plot(tree) groups &lt;- cutree(tree, k=5) # to get 5 groups
There are lots of tutorials out there for microarray data analysis with R, just google a bit or look at relevant bioconductor packages.
so this is probably really basic, but how do we change the sample names? Our plots are really big and unreadable so we really need to change the names or place the samples in groups. Thank you so much for your help!!!
Several ways to do that. Easiest would be plot(tree, labels=newLabels) Where `newLabels` is a vector with new names. So for example `plot(tree, labels=1:134)` would put numbers 1-134 as labels. Old names can be obtained with: tree$labels
ah thank you! My plots look much cleaner now :)
Try ddply from the plyr package. summarised_data &lt;- ddply(data, .(name, week, year), summarise, pass_stats_sum = sum(pass_stats, na.rm = T), rush_stats_sum = sum(rush_stats, na.rm = T), rec_stats_sum = sum(rec_stats, na.rm = T)) 
Hadley Wickham writes some kickass libraries. I've never used lubridate, but I'm sure it's awesome. You should be able to this in base quite easily though. Date| :--| 25.10.07| 03.08.08| 11.01.14| as.Date(test$Date, "%d.%m.%y") Date| :--| 2007-10-25| 2008-08-03| 2014-01-11| Casting shit in R is something I've found to be one of the most frustrating parts of the language. Idk what the consensus is among the best users, but I use a lot of the as.Something() functions, so I'd say they're a good thing to get used to.
You need to be a privileged user to write to C:\. RStudio runs it's R process as an unprivileged user (for fairly obvious reasons). You need to create a user library somewhere (as opposed to the system library, which is the one R is trying to install to). I believe if you open R outside of RStudio and try to install a package, it will prompt you and create such a user library for you if one does not exist. 