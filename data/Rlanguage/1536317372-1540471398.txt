You can do: dataframe$column[is.na(dataframe$column)] &lt;- "&lt;NA&gt;" this replaces all elements of the column (I'm assuming you are working with a data.frame) which are NA with the characters &lt;NA&gt; 
There sure is, say YourPackage has a function called "generic.function" which you would like to modify, you can see the default function by just running the function name: ``` generic.function ``` Copy the output to clipboard excluding the final line: ```&lt;environment: namespace:YourPackage&gt;``` Paste it into your script, give it a new name, and modify to do whatever you need it to do: ``` custom.function &lt;- function(args){ perform your modified function }``` You might run into trouble if the function is calling other functions from the same package, but there is nothing stopping you from loading a function from your script, rather than loading the package itself. 
Is your column really a factor or a character column? I thought if it was a factor it would already be doing what you say.
Yeah. I thought there may be some 'recommended way'. But at the end, I just copied the R scripts in R directory and then sourced them so the functions in them are loaded in dependency order. Then some slight path adjustments. It seems to somewhat work, but it feels dirty.
This seems to work for vectors, but not for a simple dataframe I tried: &gt; x &lt;- c(1,2,NA,3) &gt; print(x) [1] 1 2 NA 3 &gt; print(x, na.print="&lt;NA&gt;") [1] 1 2 &lt;NA&gt; 3 
`print` is entirely redundant in your code but fundamentally `print` isn’t for general printing, despite its name — instead it’s simply a generic function that shows a representation of your data on the R command line. For *printing* you should use other facilities, depending on the task at hand: * `message` and `warning` prints, as the name suggests, messages and warnings * `cat` and `writeLines` are general text writing functions Hence the suggested answer by /u/Laerphon. Alternatively you could also create a single string and print that, via cat(paste(rep('-', 30), collapse = '')) This is longer than the other solution and does the same in this case, but it might come in handy since it creates a single character string rather than a vector of length &gt; 1, and thus prints correctly when passed e.g. to `message` and `writeLines`. Furthermore, don’t forget to write a newline character when using `cat`, otherwise `cat` won’t add it automatically.
Bizarre, the simplest, most straightforward solution for combining all these data.frames into a single one is still missing: library(dplyr) result = bind_rows(unholy.list)
You can’t `unnest` a list.
You're right, I actually made a mistake and it was an integer, sorry rookie here... That bring the case, do you know a way to make the &lt;&gt; appear around integer NAs?
I thought you said the state names were two letters. I'm officially confused:)
If you use RStudio and are familiar with the tidyverse, tibbles will display them in a different color.
Ah just so it'd stand out a bit more. But perhaps I can find other workarounds. 
Do you mean NAs appear in a different colour when printed in the console? I tried to google that but didn't find anything. That'd be amazing though if that's what you mean.
This guy gets it 
Maybe something like shinyFiles might be useful? If not it might be some code that you can mess with that will be easier than what you're starting with here. 
Just try it and see. tibble::tibble(x = c(1:5, NA, 7:12), y = c(rep(NA, 12)))
Be careful with this solution. You will lose the representation of these as NA. You'd have to try to detect them by matching the character string "&lt;NA&gt;" I believe.
This problem comes up in the often-used Lahman baseball dataset. One of the leagues (LgId) from 1871-1875 was called the "National Association" which is abbreviated "NA". So you have to adjust your CSV readers to read "NA" as a string.
http://db.rstudio.com/ provides about as good of an overview of this as you’re going to find. It should be enough to get you on the right path. Already having the odbc driver and an existing connection outside of R is definitely half the battle!
It's not actually a 'dark and thick line'. It's a series of individual plot points. The whiskers of a boxplot don't, by default, show the range. If you do ?boxplot, have a look at the 'range' parameter - it states: this determines how far the plot whiskers extend out from the box. If range is positive, the whiskers extend to the most extreme data point which is no more than range times the interquartile range from the box. A value of zero causes the whiskers to extend to the data extremes. (Also if you check wikipedia for boxplot, you can read about the various definitions of the whiskers - R's boxplot default produces the Tukey plot.) And in addition, points outside the whisker range are individually plotted, as they're considered to be outliers (for some definition of 'outlier'). I'll be honest - while I know this is true, I'm struggling to find that explained in the help file for boxplot, which doesn't help. What other clues do we have? Well firstly we could do plot(density(smulher)) which'll show you how skew the distribution is. Also length(smulher) is another clue - now we know it's both skew and a big data set - hence there's likely to be a lot of outliers. And that's what you're seeing - a lot of points higher than the top whisker, so plotted individually. But lets go back to the range parameter in ?boxplot. Note it says "A value of zero causes the whiskers to extend to the data extremes." So if you did boxplot(smulher, range = 0) you'd get a boxplot where the whiskers go to the range of the data (which is my personal preference, no disrespect intended to Tukey!). Hope that helps! 
Thank you so much!!!!!!!! I'll try it! 
i do this: connect_to_sql_server &lt;- function(db, driver_path, host, port, user, pw) { drv &lt;- RJDBC::JDBC('com.microsoft.sqlserver.jdbc.SQLServerDriver', driver_path, identifier.quote="'") conn_string &lt;- paste0('jdbc:sqlserver://', host,':',port,';database=', db) DBI::dbConnect(drv, conn_string, user, pw) } 
"SQLOLEDB" is the OLEDB Provider (for use in ADO, for example), not the ODBC driver. Check the ODBC Data Source Administrator -&gt; Drivers tab on the machine to find the correct driver name. You'll want something like: "Driver=SQL Server;Server=my_server;Database=my_database;Trusted_Connection='True'" I'm also going to upvote u/UnhappySquirrel's post below. The page you're probably looking for is: [http://db.rstudio.com/databases/microsoft-sql-server/](http://db.rstudio.com/databases/microsoft-sql-server/) 
Why not just use cov()? I’m honestly not sure how you’re going about calculating a true covariance matrix that isn’t symmetric, they’re symmetric by definition 
Hi, I realised my post is pretty inaccurate. What I need is to estimate the correlation matrix of a multivariate distribution as part of the multi-stage maximum likelihood procedure for estimating copula parameters. I've got my likelihood function, but it's not giving me the result I expect. I assume the symmetry of the correlation matrix has to be imposed as a constraint, but I'm not sure how to accomplish that.
Cool thanks I learned something new!
Use inner_join(csvone, csvtwo, by="matchingcolumn"). Here is an example http://stat545.com/bit001_dplyr-cheatsheet.html#inner_joinsuperheroes-publishers Here is dplyr cheatsheet for reference https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf 
So on 1 csv file I have the name of the product, the size, and the color &amp;#x200B; On a different csv file I have name, size, color, AND the UPC. I want to be able to find the product's UPC from the larger csv file and just add it to the smaller one based on the name, size, and color of the product.
Sure, you need to do a merge (join). If your first data frame (CSV read into R) is called `x`, and the one with UPC is called `y`: z &lt;- merge(x, y) [Documentation](https://www.math.ucla.edu/~anderson/rw1001/library/base/html/merge.html).
Yeah you wanna use a join function. Probably left_join() or right_join(). Can't remember which package but it comes from SQL
Dplyr
A data frame is a list of vectors. It probably successfully allocated memory for a vector or two before the error. 
What version of R
The answer lies in data.table. Use that to merge
I believe this may be related actually. Upon double-checking, I subsetted the second table that was to be merged incorrectly which only included a couple numeric columns, many rows of which had the same values within itself and those of the first table, and had all=TRUE. Not sure, but after I corrected what I was trying to merge, it worked instantly. Given that, I'm still not sure why it wouldn't merge since I supposedly had the RAM to do it, but either ways it's not an issue anymore.
After a couple hours, I discovered the second table I was trying to merge to the first was incorrectly subsetted from the original and only included a couple numeric columns, so I was trying to merge an incomplete table. Still not sure why it wouldn't merge given that I had more than enough free RAM after checking computer performance processes, but either ways not an issue anymore.
The choice of kernel function generally makes very little difference - bandwidth is much more important. For kernel regression (local linear regression), the Epanechnikov kernel is optimal - except for when you're estimating at the edge of your data, in which case a trianglular kernel is optimal. For kernel density estimation, I'm not sure what the optimal kernel is - but as I said, it doesn't really matter.
Start here, great source https://topepo.github.io/caret/recursive-feature-elimination.html
Thanks for the response. This was actually the first thing I looked at in fact my code is pretty much copied from what that author had except I changed the variable names and changed removed certain parts. 
You can use `system.file` for this
Some combination of filter, left join, and rbind? 
How many unique item types do you have?
Thanks for replying, though I'm not sure I completely understand. mean.table is a table of length 3. It contains the mean Item_Visibility of each Item_Type. That is to say: mean visibility of 'snack' type items, mean visibility of 'seafood' type items and mean visibility of 'meat' type items. I want to replace each NA in the Item_Visibility column with the relevant mean.table value. I know I can do a hard insert or with an ifelse statement nested inside an ifelse statement. However, I want to use a method which would also be feasible if I had 20 Item_Types with 800 NA values in Item_Visibility. 
I don’t know if this is directed at me or him, but it’s okay, it was only a mix-up.
`full &lt;- full %&gt;%` `group_by(Item_Type) %&gt;%` `mutate(Item_Visibility = if_else(`[`is.na`](https://is.na)`(Item_Visibility), mean(Item_Visibility, na.rm = TRUE), Item_Visibility)`
This approach will solve OP's problem. However, it requires `library(tidyverse)`. I point it out because OP is likely a beginner and was using all `base` functions.
Totally second that. Unless you have a strong reason for using base R only, I would learn the tidyverse directly. Everything is much more elegant and consistent with purrr and dplyr compared to their base counterparts. Some resources to get started: [https://www.datacamp.com/courses/introduction-to-the-tidyverse](https://www.datacamp.com/courses/introduction-to-the-tidyverse) [http://r4ds.had.co.nz/](http://r4ds.had.co.nz/)
&gt; No, you probably misunderstood it. You don'tneed to calculate the variance of the predictors, you need to calculate how much of the variance of the dependent variable is explained by the predictor(s) Thank you, so I need an F test to determinate how much of the variance in the Y is explained by the predictors. Tecnically how can I do it? F test on R? Which is the command, just an example? 
I use this: library(tidyverse) df %&gt;% mutate_if(is.logical, as.numeric) This code will convert all your logical columns to numeric ones. Let me know if it works for you
Can you cowplot it with seven individual graphs? Just as a quick messy fix.
Check out Jose Portilla on udemy. He has around 15 classes with one specifically for your needs called “Data science and ml bootcamp with r”. 
Sorry maybe vetted wasn't the best choice of word. The point I meant to get across was that I won't need to be updating the model in this process. That will be done separately, and not hourly, more likely closer to monthly. &amp;#x200B; I went through and simulated the output the model would have created every hour over the past 2 years (always with non-trained data), and the margins of error were always well within my needs. That's why I considered it "vetted".
Have you had a look at plumber? 
This sounds like something I would use Shiny Dashboard for. https://rstudio.github.io/shinydashboard/
I have not, I will look into it.
This looks like it may be a great fit! Is this something you use? Any idea of how steep of a learning curve there is?
Short version is its a package for exposing R functions as REST API endpoints. The rplumber docs whilst incomplete are pretty good for getting up and running. you could also check out opencpu although I don't have sufficient experience using it to really offer any advice or comparison 
I second this. Shiny is super convenient. I made a nice visualization using shiny by learning it from scratch and the whole thing took me only about 30 hours. My data isn't dynamic but that won't matter at all. PM me if you need more help or want to see what I was able to make.
Shiny &amp; Shiny Dashboard. Makes interactive web-base interface that will pull the data every time someone opens it up - multiple users accessing it simultaneously with no problem. [https://shiny.rstudio.com/](https://shiny.rstudio.com/) [https://rstudio.github.io/shinydashboard/](https://rstudio.github.io/shinydashboard/)
You can set up your own Shiny server for it. I didn't do this but Shiny does offer software to do that - look at their website.
If I set up my own server that would work. I also saw something on stackoverflow about basically making it a compiled application. It could live on a network drive, that would be fine. Will look into this tomorrow. &gt;EDIT: Also, if the people all register accounts on shinyapps.io you can restrict your app to be only available to people by their username, if that would solve some of the security concerns. Unfortunately for my case this still wouldn't be possible. We can't have any cloud hosted data or anything.
Any experience with hosting it locally? It just needs to be internal on our network, but no one but me has R installed.
Depending on what you mean by "present the output", shiny could be a good option. However there are other things to consider. Some other considerations (not necessarily in order) 1. Maintain dedicated servers for general purpose analytics or for specific applications, these will probably be Linux servers, and they will be accessible on your network. 2. Provision access as needed, and control firewalls at the server level. Also, you can upgrade to shiny pro and control access with LDAP. Still the firewalls are not a bad idea depending on what you're doing and size of organization. 3. Maintain at least two tiers for development and production, possibly three tiers for dev, prod, and QA. Each tier is on a separate server. Also each database might be mirrored in the same environment. Develop locally or on dev. 4. Use source control (git) and maintain a system of branching for development and production (dev and master branches). Ideally you only pull to the production server from master, and dev from dev. Depending on how rapidly you want to react to this API, this might be too much overhead. We use issue branches for dev candidates, and don't usually develop directly to dev. 5. You may want to force a review process for pull requests, you can do this with a policy / rule within gitlab or github 6. On the server use crontab to manage the model's update frequency 7. If you go with a Shiny app use the @reboot option in crontab to restart your app on a reboot. 8. Within Shiny make sure that you're reloading the data (either from a file or a SQL pull) within the server. Otherwise your data will only load when you restart the shiny process. Also, you might want to force it to reload with a timer within the server if the user will have the app open for longer than the update frequency. These notes are off the top of my head, may not be everything. 
I host my Shiny server in a Google cloud instance, but you should be able to hold it on any linux server you have sitting around on your network. This is my preferred method - no one needs to know anything about R - they just go to a URL you point them to and they get what they want. Here are the install instructions: [https://www.rstudio.com/products/shiny/download-server/](https://www.rstudio.com/products/shiny/download-server/)
You don't even need to do a compiled application, I run my apps through my company's network web server and just host it to a local port and run and refresh the script through Windows Task Scheduler. 
Heatmaps. Use geom_raster in the ggplot package 
Yes https://www.littlemissdata.com/blog/heatmaps
Cheers!
I’ll look into that one, thanks!
First you need to think about a function which you want to fit. E.g. a monod funciton, then you can use nls() to fit that function: `dat=data.frame(PAR=p,O2=o)` `opt &lt;- nls(O2~v*PAR/(PAR+K),data = dat,start = list(K=40,v=0.00015))` `plot(p, o, xlim=c(0,130), ylim=c(-0.00005,0.0002), xlab="PAR", ylab="O2")` `lines(0:120,predict(opt,newdata=list(PAR=0:120)),col=2)` &amp;#x200B;
Was your question plotting or tracking?
Don’t use `R CMD BATCH`, it’s obsolete and only supported for backwards compatibility. `RScript` is the replacement.
I appreciate the thoughtful reply, but it's been working for 3 years without fail. So there are a lot of other things I'm going to do before fixing this. Maybe I don't have issues with ls because I'm not handling many files or something. It's weird how many basic Linux commands have weird issues. For example I've also read about horror stories with `mv` failing when moving across mounts. Who would think that you need to write a program to move files? 
If it was me, I’d figure out the boundaries using a few linear models... mx+b and then create a categorical variable to use when sampling groups. For the linear equations, it might be easiest to eyeball it given that it appears arbitrary anyway. Once you have the equations, look at your values and use inequalities to determine if the values fall below or above the lines. My best guess as I’m waking up.
 &gt; Also, you can upgrade to shiny pro and control access with LDAP. I can recommend running the free shinyproxy-application. It's based on docker and a free alternative to the costly shiny servers. I run it on a redhat server and it works just fine. Also supports LDAP. 
That's interesting, but I'd actually like to support the wonderful thing that is RStudio! Still, good to know
Coursera doea have a few courses that use R. You can also see if you can find any books or ebooks on R. I have a few at home. If I remember I'll post the titles when I get home.
That would be awesome, thanks!
&gt;I will be using this for large excel spreadsheets, tableau and R How large is large? What sort of datasets/extracts for Tableau? What computations will you be running? Will you be plotting data? Overall they are solid laptops and the SSD will serve you well reading in data/extracts. If you are doing something with 10 million or so cells you shouldn't have issues, but if you are building up many features for mlr it could take a while to build the model and tune parameters. If you run out of RAM look into feather.
See mlr (https://github.com/mlr-org/mlr/tree/master/R), they test warnings, errors, messages, numeric results, everything.
I prefer to put the final dataframe at the start instead of end of the dplyr chain but this is what you want OP.
Yep, that's the one!
As someone who is starting to use Python more than R, I miss this so much it hurts. "dfply" just isn't the same.
note, none of the functions in dplyr modify existing data structures they only create, if you want to modify the existing data structure you need to reassign the result back to the original data structure.
Gpu seems to be a little undersized if you have to compute heavy parallelized tasks. Also the storage capacity is really low. It might work if your tasks involve only database analysys.
In base: df$Ratio &lt;- df$Volume / df$Height
A triplet is just a set of three, looks like. He happens to have three numbers in each of his vectors so he calls them triplets. 
Books I have: R in Action: Data analysis and graphics with R by Robert I. Kabacoff and Practical Data Science with R by Nina Zumel and John Mount.
I suppose tracking, just some add on feature / library or something so it does it automatically. If it’s plotting them I’m actually unsure where I’d get the data from in the first place...
That’s what i thought, and no one really responded to your actual question. The best idea I have is I use a word tracking app to track all my apps (chrome, rstudio, word, excel, etc) and it tracks by the hour. But it’s not time spent but words typed (spaces).
Yes ideally 8th gen is the way to go, if OP can afford it
Ah ok, I’ll have a look into those. But is there a particular app you know of that works for R?
can you use %&lt;&gt;% in a similar chain way as %&gt;%? so stack multiple operations but use %&lt;&gt;% between them?
You can simply download he code (.rmd file) and open it on RStudio. There is no in between steps. The data sets referred on lines 44 and 45 are available in the "Data" section of the challenge.
When I try to knit it I get errors? 
You open it as R Markdown. However: output file: Markdown.txt [1] "Markdown.txt" Warning messages: 1: In file(file, "rt") : cannot open file '../input/student-mat.csv': No such file or directory 2: In file(file, "rt") : cannot open file '../input/student-por.csv': No such file or directory Error in cat(o, file = "C:/Users/Risin/AppData/Local/Temp/Rtmp6vC5OF/knitr-output4b18efc6df5.out") : object 'o' not found
Nah I think it only works for the noun not the verbs. 
I hear that it annoys some people because it makes the difference between a permanent and temporary mutation less obvious. I love it but never remember to call magrittr before I try to use it so it tends to be more of a hassle. 
I use dataframes almost exclusively for my GGplot and plotly plots.
I'm not even sure I understand. Are you talking about contour plots? I'm pretty sure the answer is never. The nice thing about R is data.frame. The nicer thing is data.table, which is an enhanced data.frame (but still inherits the class). 
or just change the path for the files for where you have placed the files
You can assign to any environment using the `assign()` function. `assign("name",value,envir=.GlobalEnv)` But I would add that you should probably try to avoid this. Much better would be have your function return the modified data frame and assign it onto the same object you pass in. ``` F =function(x){ ... return(x) } original_table = F(original_table) ``` This avoids writing functions which have side effects. 
Good R style dictates that if you want that behavior, you should use explicit assignment: tablecolSums &lt;- function(x){ x$SpeedPlusDistance &lt;- rowSums(x) return(x) } cars &lt;- tablecolSums(cars) If you want to do this to a large number of dataframes or matrices, those objects should be elements in a list. Using the superassigment operator will cause unfortunate and horrible things to occur if you feed your function arbitrary inputs. You should not use the global environment as an object dumping ground. Superassignment is great with lexical scoping inside of nested functions, however, where an interior function behavior modifies the environment contents in the level above it.
I've heard this before, but I still don't know what the issue would be. Why should I assign it to the global environment afterwards rather than in the function itself?
From a software engineering perspective, this assignment behavior leads to problems with [action at a distance](https://en.wikipedia.org/wiki/Action_at_a_distance_%28computer_programming%29). R is a functional, object-oriented programming language that discourages doing this sort of thing. Many good practices in R (apply functions for instance) are much harder to use if you assign everything to the global environment rather than contain them in their own objects (or environments).
Agreed. Plus it makes it much more difficult to reason about your code. In 5 lines no big deal. In 1000 lines much harder to follow what's going on. If someone else looks at your code it's harder for them to figure out too. Plus it makes your function harder to write such that it's reusable. In this case you could have `assign(”cars", ...)` but now your function always assigns to cars regardless of what you pass in. That's bad right, non re-usable function that has a side effect of always creating an object called cars with the input + rowSums you provided in the global environment regardless of whether one was there before or not. 
you might wanna have a look at `help(nls)` the argument start, which defines the starting variables vor the calibration (this is what you are doing here) and must either be a named list or a named vector. the argument data is giving "an optional data frame in which to evaluate the variables in formula and weights. Can also be a list or an environment, but not a matrix" So if the data.frame containing your data is called `mydata` it should state `data=mydata` and then in the formular expression (the first argument of nls) you dont need to state the name of the data.frame but can just refere to the column names (as I did in my example). Hope this helps.. &amp;#x200B;
Yeah I get that error also how do I get rid of that
I get this error after doing all of that &gt;Error in .f(.x\[\[i\]\], ...) : object 'Walc.x' not found Calls: &lt;Anonymous&gt; ... &lt;Anonymous&gt; -&gt; vars\_select\_eval -&gt; map\_if -&gt; map -&gt; .f &amp;#x200B;
Seems to be the consensus as nobody’s gone against what you said yet. Think I’ll scrap those matrix plotting flash cards then. Cheers!
&gt;First you need to think about a function which you want to fit. E.g. a monod funciton, then you can use nls() to fit that function: &gt; &gt;dat=data.frame(PAR=p,O2=o) &gt; &gt;opt &lt;- nls(O2\~v\*PAR/(PAR+K),data = dat,start = list(K=40,v=0.00015)) &gt; &gt;plot(p, o, xlim=c(0,130), ylim=c(-0.00005,0.0002), xlab="PAR", ylab="O2") &gt; &gt;lines(0:120,predict(opt,newdata=list(PAR=0:120)),col=2) &amp;#x200B; Sorry may I ask you if the monod function is used also for other things, beside the bacteria life growning? Could you make any example, please?
Sorry didn't mean to be harsh. Your question makes sense, it's a very "non R" sort of paradigm! There are not many times in R when you need an actual matrix, but every once in a while you do need an actual matrix. You'll probably find factors to be confusing. By default string columns within data frames are represented as factors. You'll see the text, but they're encoded to integers internally. It's great for regression models (and many types of plots), but also factors can be super confusing and annoying if you're not expecting it. Data tables are amazing for a lot of reasons, but performance and the ability to aggregate are some of the biggest features. By default strings are not factors in data tables, but they are coerced to strings as needed within model functions. 
So the error is coming from this then alcohol.1 &lt;- alcohol%&gt;% select(as.numeric(Walc.x), as.numeric(Dalc.x), as.numeric(G1.x), as.numeric(G1.y), as.numeric(G2.x), as.numeric(G2.y), as.numeric(G3.x), as.numeric(G3.y)) The error message indicates that the walc.x variable doesn't exist. I don't have access to r at the moment so I can't properly check but I would guess you either need to get rid of the .X or that it's either all lower or upper case. If either of these are true though I'd worry about the rest of the code especially how early this error message crops up. 
For your particular case, defining helper functions, the official way is to define them in files inside `tests/testthat` whose names start with `helper`. {testthat} 2.0 also adds [setup and teardown helpers](http://testthat.r-lib.org/news/index.html#setup-and-teardown).
That's very useful. +1 star
Yeah I tried but no avail :(
Try to call Walc.X in the console directly, to check if exist
It doesn't, I'm really not sure why it's not working I mean it's a highly rated code...
Legend absolute legend mate.
That X1 looks an awful lot like upper case :)
oh my gosh! I didn't notice because it was the third one -- i thought it had problems with the first one! Thank you so much for your help, I thought I was going insane
Lol, don’t know if I’d go that far, but I appreciate it.
To add on to /u/coolerheads, "which()" returns a logical vector. Something like c("True", "False", "False","True",...). If you want the rownames of the dataframe, you need to subset the dataframe and take rownames of that.
All good :) Strings as Factors makes sense for the most part, so things can be grouped into a tidy boxplot or something. Will definitely check out these data table things in time.
Set.seed is the random number generator, which (someone correct me if I’m wrong), should determine which rows of the data set are selected that make up the test and training set. Switching around these rows will eventually pick a combination such that the predicted values when using the test set are higher. 
How do you define your usage? Do you want to see times when you edited files? Create new ones? Times you made commits to a repo (as pictired)? 
It's cheating and it's definitely frowned upon. That being said, I don't think anybody would have a problem with you using the 92% seed. The difference is trivial and it basically happened by accident. It's not systematic cheating from your side. Running the simulation 100 times and picking an outlier data set to achieve a freakishly high accuracy would be very dishonest. I can imagine that an academic would lose all credibility if they got caught doing that. However, the incentive for cheating is strong as well. Obviously. You're literally making your model look better than it is, which translates directly into improved chances for getting published and people wanting to listen to what you have to say. I'm the wrong person to ask about the risk-return trade off in this case. I've only had to work extensively with three research papers (I'm a student). I certainly had the feeling that in those three studies, their model worked a lot better when the authors tested it than when I tested it. I don't know how this literature is typically peer reviewed, but in my cases, replicating the studies was certainly very time consuming, and I didn't know their exact method and data, so it would be hard for me to say whether they cheated and how. I guess I could easily see academics cheating and getting away with it for a long time. **tl;dr** Yes it's frowned upon. In your case, it's not bad. I don't recommend it because you're not improving your model, you're just drawing a bullseye around your arrow after the fact. If you make a bad model look good this way, people will probably be very impressed with it as long as they don't realize how you made it happen.
Oh I see! It's the seed to select the training set! Then, yes, it's a bad idea. Your are fooling yourself doing this. If you do this systematically, you are artificially creating two equivalent dataset to increase your accuracy, which is basically overfitting.
Honestly I’m thinking that an app that tracks time on the computer at all once I start the timer would work better, as I’m in and out of the R program a lot. So short answer, I’m honestly not sure... But my flash card program gives me number of cards reviewed, so maybe I’ll just use that. 
Cheers for the feedback!
I’m a student as well but aiming more for consulting type work over research. But I’m sure the same rules apply. Bulls eye comment was a great point, the model is already pretty damn good (because it’s from an online course where they chose highly correlated variables), and the gain is fairly marginal. Still, think I’ll just stick with (123) or something. 
Don't try to find the best seed value. That can dramatically hurt the generalizability of your model. Here's a bit of an extreme example to prove the point. It picks the best seed for cross-validating the linear regression output. In my example, the best seed would be 10231. &amp;#x200B; `library(tidyverse)` `library(modelr)` `dta &lt;- tibble(x = rnorm(1000), y = rnorm(1000))` `find_seed &lt;- function(seed) {` `set.seed(seed)` `train_index &lt;- sample(c(TRUE, FALSE), nrow(dta), replace = TRUE)` `train &lt;- dta[train_index, ]` `test &lt;- dta[!train_index, ]` `regression_model &lt;- lm(y ~ x, data = train)` `model_rsme &lt;- rmse(regression_model, test)` `tibble(seed = seed, rsme = model_rsme)` `}` &amp;#x200B; `seed_hack &lt;- map_dfr(1:1:100000, find_seed)` `seed_hack %&gt;%` `dplyr::filter(rsme == min(rsme))` `# A tibble: 1 x 2` `# seed rsme` \# &lt;int&gt; &lt;dbl&gt; \# 10231 0.900
Happy to say I just got this book in the mail! I use R Markdown for a lot of stuff at work and it'll be nice to have a more complete understanding of it.
Doesn't cross validation exist, in part, to smooth those cases?
Not useful.
First, have you done shape and dispersion tests on the cell data? Are you sure Tukey should be your post hoc? With unequal variance you would use either Bonferoni contrasts or Games and Howell as post hocs (t-tests done strategically to spare a loss of alpha) 
Haven’t quite learned how to do that just yet to be totally honest... It’s up there as a priority though. 
It’s hard to overstate how Hadley wickham’s resources have been for people with your goals. Please look into R for data science working the next few weeks. You’ll also want a good understanding of base R. R isn’t hard, actual analysis is though, so I’d encourage you to focus most of your time on your quantitative and analytical skills. The actual programming is just one of many tools in your belt 
Fair enough, then you need to provide more specific detail about your problem. Given what you've said, the only thing I am confident about is that what you want is the wrong approach.
Your article is worth reading.I after long time read some online article in so peaceful atmosphere.That's because of your quality [pay for essay.net review](https://www.analyzedu.com/writing-services-reviews/payforessay-net-review.html) style which inspired me a lot. 
I started with nothing, so the educational package "swirl" helped me get started. It's basically a course developed in R for newbies. This is how you run it from the R console: - &gt;install.packages("swirl") - &gt;library(swirl) After that, the console will tell you what to do. You should complete course 1: R Programming: The basics of programming in R. For me, I wasn't able to really learn about R before I did the "Getting and Cleaning Data" course. You can add this to your list of available courses (after loading swirl) by typing &gt;install_course("Getting and Cleaning Data") There is, of course, a bunch of other interesting courses there about regression models, statistical inference, exploratory data analysis, etc. Cheers
Look into swirl. It teaches you R, in R, by guiding you through practical tasks. You'll learn the basics and not-so-basics as well as several statistics and analytics skills.
I kinda see. It's a weird thing to ask it to do it in R if any app would do. If I wanted to do it in R, I'd just write something that writes the current time to a file when you run it, at set intervals if you don't want to turn it off manually. Then another script to plot the thing reading the file. You can even make it execute on startup if need be 
corresponding [link](http://r4ds.had.co.nz/)
[Here is a great example.](https://stackoverflow.com/questions/5720508/sql-server-rodbc-connection)
Data science consultant here. It sounds like you're doing accuracy measures on the training set, which is a good gut check on how your model is doing but performance on validation data is WAY more important. The simplest thing to do this is to train your model on most of the data (say 80%) and based on that model use those coefficients and features to score the remaining data (the validation set). The accuracy measures on the validation set is way more important, it's an unbiased way to test if your model overfits your data. There is still value in the accuracy scores on the training set. If your model doesn't overfit the data you should have roughly similar scores. If you're not happy with that accuracy you can try different combinations of features, alternative algorithms, or develop more model features. 
No, as you said I wasnt sure which test would be more fit. How could I then implement Bonderone/Games/Howel tests in rStudio?
install.packages("devtools") require(devtools) install_github("burrm/lolcat") 1. Check for normality in cells. 2. If normal, use ADA dispersion test, if not use ADMn-1 dispersion test. 3. If variance is equal, use Tukey as post hoc (all possible pairwise comparisons), if not use Games and Howell contrasts. ***Be sure to visualize interaction plot to determine which means should be contrasted in the post hoc. 
reddit will probably delete these links, but more advanced: https://www.tidytextmining.com/ http://vita.had.co.nz/papers/model-vis.html http://www.feat.engineering/index.html http://dwoll.de/rexrepos/index.html this last one is very useful
Use DBI and the driver from rMySql or whatever that package is called?
Yeah the accuracy I provided was on the test set, which made up 20% of the original set. But I haven’t learned more sophisticated things like cross validation just yet, so I’m sure there’s plenty of room to improve. 
&gt;Just an additional note -- a well-fit model should not have performance that drastically changes by changing the seed, depending on certain factors (e.g., with small sample size, this may be the case). One way to ensure you get a "good" estimate of the true error rate is to use &gt; &gt;caret &gt; &gt; to &gt; &gt;train() &gt; &gt; your model using &gt; &gt;k &gt; &gt;\-fold cross validation. In general, this is the preferred method anyhow, where &gt; &gt;k &gt; &gt; depends on how many data points you have. Repeated cross validation helps train the model on many different sets of data. &amp;#x200B; Sorry I don't understand. I always use standardized R-squared and AIC to evaluate the goodness of a model fit, plus Erros versus fitted data. What is this ''seed'' thing? &amp;#x200B;
Take a look at the other seeds. Do results vary largely?
&gt;Yeah I definitely need to learn how to do this. Still pretty early days for me but I’ll get around to it soon. For reference the entire data set was only 50 rows for the sake of simplicity, so perhaps that’d explain the variance by changing the seed I use standardized R-squared and AIC to evaluate the goodness of a model fit, plus Erros versus fitted data. What is this ''seed'' thing? Why to implement it in regression? &amp;#x200B; &amp;#x200B;
&gt;A triplet is just a set of three, looks like. He happens to have three numbers in each of his vectors so he calls them triplets. &amp;#x200B; THank you, but why he would that output?
Randomises the way that the test and training set are determined. Thanks I’ll check those out. 
Who knows? People need all sorts of outputs for whatever specific problems they're working on. Maybe that's a coefficient matrix he needs to generate for whatever reason. Generally, Stack Overflow doesn't concern itself with WHY someone needs something, just with HOW to get what they need. 
In the equation without interaction terms the effects of the independent variable on the dependent variable do not affect each other. In your example this means that how weather affects gasoline consumption does not depend on road conditions. With interaction terms the interpretation changes. However it is good practice to write all lower order terms in the regression equation, i.e. instead of Y ~ atmospherical weather * the state of the road + speed you should write Y ~ weather + state of the road + speed + weather * state of the road Now, how weather affects gasoline consumption depends on road conditions. For example, if the weather changes from sunny to rainy the change in gasoline consumption would depend on the state of the road (e.g. good or bad) without interaction terms from sunny to rainy would always have the same changes in gasoline consumption regardless of the state of the road.
Is this an experiment or do you just want to know the significance of your variables? If the latter is the case, you could just use a T Test. Pretty new to R myself, so apologies if I misunderstood.
&amp;#x200B; Thank you, I am at the beginning. Basically I have understood that th kernel regression is applied when the distribution has no mean, no variance and so on. I don't understand what is the bandwith ... I can't understand which value to give it in the function. In this function: [https://wikimedia.org/api/rest\_v1/media/math/render/svg/cdb037bb2ca874299f458754f4b0930dc73b75af](https://wikimedia.org/api/rest_v1/media/math/render/svg/cdb037bb2ca874299f458754f4b0930dc73b75af) where K is a kernel with a bandwidth h. The denominator is a weighting term with sum 1. &amp;#x200B; What is a bandwith ''h'' ? ANd what are they the weighting terms, what re they used for? Why their sum is ''1'' ? &amp;#x200B;
&gt; Why not assume I had the missing piece of the empty vector puzzle? Because I know R well enough to see that your approach just isn’t very good. You may *think* you have the missing piece of the puzzle but I *know* that you’re making your life a lot more complicated than necessary. To use your analogy: &gt; When I was a TA my students would just answer the question I asked. Exactly: except in the above conversation I would be the TA and you’d be the student, don’t you agree? I don’t mean to be condescending but it’s simply a fact that your question shows that you’re missing a piece of knowledge to judge which approach works well here. For somebody who’s an experienced R programmer, your request simply doesn’t make sense.
Hey, yes the cells are normal and variances are equal, but when I want to do 5 times the mean of one cell minus 3 times the mean of another cell, does that not necesarelly change the variances then? (var of 5x &lt;&gt; var of 3x) how would I do this opperation then? Thank you a lot again!
&gt;Basically I have understood that th kernel regression is applied when the distribution has no mean, no variance and so on. No, that's not the case. Kernel regression is used when you don't want to specify a functional form for the relationship between variables. &gt;What is a bandwith ''h'' ? A common form of kernel regression is called local linear regression. The name is very descriptive - it works by estimating a number of linear regressions (standard OLS in the simplest case), but for each one you only use data close around the point you're interested in estimating (hence "local"). The bandwidth decides how far around each point you look when estimating your local regression.
As somebody who just started, this is the way I should have done it. If your goal is getting a job, SQL and a bit of dashboarding are a must have. R is what rounds it out against other candidates. If you already can do this, learn basic R and some simple Machine Learning and your good to go!
Good point. I just added a summary to my answer.
I'm a big fan of feather
Can you explain why you want to do that?
Trying to parse https://www.reddit.com/r/buildapcsales/new.rss
my go-to example of the significance of interations is this. School students listen to a list of words at three different environmental noise levels: no nose, low noise, loud noise. We record how many words they recall correctly. We know the noise level, the number of words recalled correctly and the gender of the students. I turns out that recall at no noselevel is best for alls students. All students recall rather less well at the low nose level with no difference by gender. At the loud noise level the male students recall better than the female students: the level of one variable interacts with another.
Datacamp, stack overflow, Reddit. Lots and lots of google searches. You'll find open source software can be frustrating if you've never used it before. Different versions will cooperate differently with packages and libraries (fancy-speak for features). I'm in the early stages of my R career and it's like playing bumper cars. 
Hi, I'll try to answer your questions. set.seed() is a function of R to fix the *seed* of the pseudo-random number generator. The *seed* determines the initial state of the pseudo-random function. Why? because, as you can see, most calculations in programming don't really need a truly random number generator, and they use a pseudo-random number generator instead, which is actually a completely deterministic function. If you fix the *seed*, you will always obtain the same results from this kind of functions, allowing **replicability**, which is very important in research. I like to think the *seed* as the page number in those old [books of random numbers](https://en.wikipedia.org/wiki/A_Million_Random_Digits_with_100,000_Normal_Deviates): if you pick always the same page, you will always get the same set of random numbers. If you don't set the *seed,* a seemingly random one is chosen based in some internal process of the system, like the system clock. Now, in the context of this thread, OP used a pseudo-random function to split a dataset in two: a training set and a test set. Why OP did this? because both datasets had to be equivalent, so OP chose, pseudo-randomly, cases for each new dataset. Why OP fixed the seed? To always have the same combination of pseudo-randomly picked cases every time the script was run. Now, What is this thing of training and testing sets, and accuracy, and RMSE, and cross-validation, and features and stuff? Well, first, you need to take into account that OP is doing some Machine Learning stuff, and this field have a slightly different jargon than traditional statistics. So, *features* are indeed -as you suggested- the *predictors.* Machine Learning focus way more in the *accuracy* of the predictions. That's why there are always a *training dataset*, used to create the estimator/model, and a *test dataset,* used to test and validate the estimator/model. In Machine Learning, absolute measures of fit are preferred, such as the *RMSE* (root mean squared error) which is something like the average deviation of the estimated values from the observed ones. The *R-squared* (or coefficient of determination) is a standardized measured of fit, which is something like the ratio of the total sum of squares that is explained by the model, and hence goes from 0 to 1. Want to make sure sure your model generalize to other data, you used *Cross-validation,* which comes in many different flavors. One idea you may have is to create more than one combination of training and test datasets, drew from the original dataset. Then test the accuracy of your model in all of those options, and finally create an average of the accuracy obtained across all the iterations. Well, that is *k-fold cross-validation* (in a very simplistic explanation). Hope this answer your questions! Best!!!
You're doing a polynomial fit, basically saying that if PAR is x and O2 is y, then y = A + Bx + Cx\^2 where A, B, and C are the model coefficients. If you have these coefficients, then you can plug in any number for x (PAR) and get the predicted y (O2). &amp;#x200B; Now, I'm not sure how you get this out of the ggplot call, but if you run the lm (Linear Model) call separately, you can do this to get your model coefficients (A, B, and C above) result &lt;- lm(O2 &lt;- PAR + I(PAR\^2)) coef(result) &amp;#x200B;
I would read the documentation first. Most of your questions are answered very early in RStudio's pages on shiny. In a Shiny app you will have two functions, a server logic function and a UI function. The UI function is basically markdown and the server function handles all the inputs/outputs. Any other functions in your .R script can be called from inside the server function. Keep in mind that You can use reactive inputs to have the kind of dynamic/interactive content you're talking about. File uploads are possible as well. Shiny apps need to be hosted on a compatible platform. Shinyapps.io is the most accessible. The free tier has limited use per month. You can host your own shiny server but it needs to be on a server where you have access to install software yourself. I wouldn't go this route with your limited familiarity.
I would read the documentation first. Most of your questions are answered very early in RStudio's pages on shiny. In a Shiny app you will have two functions, a server logic function and a UI function, as in your snippet. Any other functions in your .R script can be called from inside the server function. You can also source them from external scripts. You can use reactive inputs to have the kind of dynamic/interactive content you're talking about. File uploads are possible as well. Shiny apps need to be hosted on a compatible platform. Shinyapps.io is the most accessible. The free tier has limited use per month. You can host your own shiny server but it needs to be on a server where you have access to install software yourself. I wouldn't go this route with your limited familiarity.
That's awesome, thank you so much! It looks like the lm call is the same as the one in ggplot, and that makes perfect sense that it's a polynomial fit (I guess I've had one too many cups of coffee, or not enough). Thanks for your help! 
I could be misunderstanding your application, but it sounds like you should use a database
Check out ShinyWidgets and ShinyDashboard. Also check out conditionalPanel() for making certain things appear given conditions.
&gt; Randomises the way that the test and training set are determined. &gt; &gt; Thanks I’ll check those out. Thank you. And from the results that seed gives to you, which one to take? Which result to discard? Do you put 95% CI? (confindence intervals) and you take the mean?
Check out the Shiny courses on DataCamp.
I mean, sorta yea, but think of it as a marriage to someone with a big house, a pool, and a car that turns into a bed. What IDE are you currently using?
What's an IDE? /s Just the good ol' terminal for me.
Interactive[?] Development Environment. RStudio is an IDE and IMHO the best one for R. Code completion is a God send. If you're working in R I would HIGHLY recommend doing it through an IDE just because you can edit code, run it, fix it, run it again all without having to copy and paste a bunch of stuff or type it all by hand.
Yeah, I was kinda bummed when I learned colab didn't support R yet. They have great collaborative features.
If you already know R, use RNotebook. On the other hand, if you already know Python, use Jupyter notebooks. I explain my rationale in this [RPubs post](http://rpubs.com/thaufas/420078).
I think it's *Integrated* Development Environment. I was just making a joke about being a certain type of dev. :) Never been super big on IDEs, for whatever reason.
What is your rationale for "if you already know Python, use Jupyter"
The assumption is that someone already knows Python but has no experience with R. I guess I should have added a case where a user knows both R and Python but has no experience with any of the Markdown/Notebook technologies. In that case, I'd still recommend RNotebook, but only because I have no experience with Jupyter and my Python knowledge is very limited.
I think all of them can be useful, but I greatly prefer that R Markdown and R Notebooks are fundamentally a plain-text file, rather than Jupyter's .ipynb. If you use git or other version control systems, plain text is so much easier to work with. &amp;#x200B; Here's a presentation that Joel Grus did at JupyterCon this year about why he doesn't like Jupyter notebooks. I think he makes some very good arguments. It's worth a look, even if you ultimately decide that the drawbacks of Notebooks are still acceptable for your use case: [https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057\_0\_1](https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057_0_1)
The features you've described are available on pretty much any text editor.
*Technically*, .ipynb is a JSON file which is plain-text... though not pleasant to read in a text editor. Joel Grus basically says that Notebooks are not a good development environment, which I think misses the point that Jupyter notebooks and R notebooks were conceived as and are effective as presentation software.
I still use emacs + ESS but one thing I like RStudio for as an IDE is the data viewer.
O I see what you mean. I haven't used it yet but the documentation for reticulate says it runs all python chunks in the same session 
Jupyter notebooks are garbage. Want to run code out of order in a file that can't be version controlled, in a non-ide environment that will teach you poor coding habits and keyboard shortcuts that are irrelevant? JUPYTER NOTEBOOKS! Otherwise, markdown/Rmarkdown and Rstudio is best in class. I edit my python scripts and run my shell scripts from Rstudio. Rmarkdown can run all of the above in a doc. If you're triggered by Rstudio try Spyder
I use emacs' org-mode, which allows me to include any programming language and export to pdf/html/word etc., but also integrates with my todo-system.
Joel's complaint is directed at the (ab)use of notebooks for exploratory data analysis, not at their legitimate use as a presentation software.
Well. Except that influential people in the data science and machine learning community are telling everybody to buy "washing machines" from Elon Musk, to use your metaphor.
But then the title should have been why you shouldn't be washing your clothes with machines purchased from Elon Musk, rather than "I don't like [washing machine that is not actually a washing machine]."
Renamed them and that worked! Thank you! 
Have you tried to just do install.packages("stringi")? Also https://stackoverflow.com/q/47377196/6108905
Thank you very much for this, it did the trick. 
Thanks for the clarification. You're right, I am a beginner but I'm kind of familiar with tidyverse as I've been teaching myself using http://r4ds.had.co.nz/ but it still confuses me a bit, especially the pipeline approach. Just a question, when using the pipeline, can I mix base R commands with tidyverse commands? For example: full &lt;- full %&gt;% group_by(Item_Type) %&gt;% full$Item_Visibility &lt;- ifelse(is.na(full$Item_Visibility), mean(full$Item_Visibility), full$Item_Visibility))
Thanks, I'm actually teaching myself using the r4ds online book which primarily uses tidyverse so I'm familiarising myself with it. The fact that it's so powerful and flexible makes it a bit confusing but I'm getting there. Just one question, can I use base R commands in the pipe?
Power regression is nonlinear and therefore doesn't work with "lm" which can only estimate linear models. You could use the logarithm, then you estimate log(y) ~ log(a) + b * log(x)
I don't think the syntax you've written will work as intended. Although you probably could get a mixed paradigm to work, I strongly discourage it because it will be more confusing than sticking with one paradigm or the other. Many base functions will work with a pipeline with no modification at all. However, if you find yourself needing a base function in a pipeline, there's likely already a Tidyverse implementation. In cases where a Tidyverse implementation doesn't exist for a base function, here's one simple tip that might help you. If you need to pass a data frame to a base function, use the `.` operator as a synonym for the data frame name. In most base functions, the data frame will be the first function parameter. In these cases, you don't need to do anything because the `%&gt;%` operator takes care of passing the `.` operator for you. Also, if you need a particular column as a function parameter, in most cases, you simply type its name. For example, if `df` is a data frame with two columns, `temperature` and `pressure`, and you wanted to get both the mean temperatures and pressures, this code should work. df %&gt;% summarize( mean_temp = mean(temperature), mean_press = mean(pressure), n = n() ) I hope that helps.
I don't think any of the other answers will do what you want but they are correct you want to fit a log-log regression. You want to plot a fitted curve y = a*x^b with the data untransformed. I think the other answer will only work if you plot the data log-transformed, which you don't really want. You can do what you want with ``` ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth( formula = 'y ~ log(x)' , method = 'glm' , method.args = list(family = gaussian(link = 'log')) ) ``` This fits a regression which is effectively the same as `log(y) ~ log(x)` but uses `glm` instead so you can have a transformed `y`. You could also do roughly the same thing (without error bars) by fitting a regression and then using the `predict` function and `stat_function`. The following shows these two methods roughly coincide. ``` ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth( formula = 'y ~ log(x)' , method = 'glm' , method.args = list(family = gaussian(link = 'log')) , color = 'red' ) + stat_function( fun = function(x) exp(predict(fit, newdata = data.frame(displ = x))) , color = 'dark green' ) ```
thank you kind stranger! I copied your code as follows: geom_smooth(method = "glm", formula = y ~ log(x), method.args = list(family = gaussian(link = log))) But i'm getting "Computation failed in `stat_smooth()`: cannot find valid starting values: please specify some" How do I specify the starting values? That would actually be ideal as the previous formula (y ~ x + I(x^2)) arbitrarily plotted the initial y-intercept above the lowest value. If i could manually add in the starting intercept that would be awesome!
`starting values` doesn't mean the y-intercept. It's a technical value for the `glm` fitting algorithm. I am guessing your error is because you have negative values or zero values in the `y` variable. Unfortunately this just means that a power regression isn't a good model for your data. One simple fix would be to assume those negative values are outliers and filter them out.
aaaaaah shoot. Unfortunately they're not outliers. I'm having the hardest time trying to fight the best model for my data. I'm looking at how algae respond to increasing light levels, starting at zero light (darkness). Oxygen produced is on the y, light is on the x. For at least the first value of x (x=0) y will *always* be negative because the algae are consuming oxygen. However, the algae never saturate, so I can't use fitwebb(), any of the michelis-menton curves, or any other model that assumes saturation. As an ecologist I was told I wouldn't need math!
Thank you for your answer, I'm actually using books you linked as my reference books, it's pretty good. The motive of my codes was to figure out how I can estimate values using a *for* loop instead of estimating those for individual countries. For example, I can rewrite the codes country_analysis_name&lt;-"Denmark" # create a sub set country_analysis&lt;-time_series_transform[,country_analysis_name] country_analysis&lt;-na.omit(country_analysis) as: country_names &lt;- colnames(ts_transform)[2:length(colnames(ts_transform))] i_total &lt;- length(country_names) for (i in 1:1_total) { country_data &lt;- ts_transform[, country_names[i]] # Here comes all the codes that iterate through log_returns of each country # Fit the ARIMA model on those log_returns and forecast until forecast_period # Combine the results in a table } There might be some nice way to make those codes sipmpler but I couldn't really figure it out. 
As a first step, you can get rid of i_total, by doing this: for (i in 1:length(country_names) { But even better, you can do this: for (country_name in country_names) { country_data &lt;- ts_transform[, country_name] But that's not going to make a huge difference over what you have... it's just a bit tidier. 
Actually that ARIMA String function made the codes pretty uglier but I didn't have any other solution to extract the best forecast model as `'ARIMA(3,0,0) with zero mean'`and save these lines for each country in a table. &amp;#x200B;
Hi, I created a *for* loop and changed my codes. Much appreciated if you can have a look and give me a feedback.
Thanks for the feedback, will go make it. 
Just opened it up and gave it a spin. It's a nice app. Just a few small points here. When I zoomed out (accidentally) I noticed that the map appears to continue infinitely. In my browser, the global map was repeated 4 times in a row, but the Japan data was only plotted on the first map. So that did seem a bit weird. Also for some reason there was a blue "pin" in Singapore, which I'm not sure what that was. Is it a Japanese branded hotel, or is it my location? (I'm not in Singapore.) Anyway, maybe you can set a limit to how far people are allowed to zoom out, like choose a resolution that just keeps Japan in the viewing frame. Also When I zoomed in very far, I noticed that there weren't any labels on the individual flags. I think a lot of people would expect to see a name or label pop up when you zoom in at a certain resolution. 
You could add an address pop up so traveling folks could zero in on the location? addMarkers(data = DATA, lat = ~ Y, lng = ~ X, icon = , popup = DATA$Address, clusterOptions = markerClusterOptions() 
It really depends on what you are doing. I've run R on a decade old potato computer for intermediate stuff
Being picky here, but I would suggest making your boxplots with plotly. It will give you nice interactivity over standard ggplot with mouse-over labels, etc. library(plotly) p &lt;- plot\_ly(data = df, y = thing\_to\_boxplot, color = thing\_to\_color\_by, type = "box") p
One option is to go with the cheaper laptop and develop code there. Anything that needs more *oomph* can be run on a bigger computer (may at school, work, PC at home, etc.) or set up in "the cloud" (e.g. http://www.louisaslett.com/RStudio_AMI/). Macs seem to keep their value pretty well, so if your friend finds the less powerful computer won't work out afterall, they can probably sell it for a decent amount and upgrade to a more powerful machine. Also, do they HAVE to use Mac? A few-years old business-class laptop like a /r/thinkpad will be more powerful and significantly cheaper.
Thanks for the quick responses! My friend is in Wildlife Sciences and planning on learning R for Grad School. I don't think she **has** to learn R, she just wants to. She's super smart but also never learned a programming language before, so I'm assuming her code shouldn't be too terribly difficult. At least for her first couple years teaching it to herself.
You could always set up rstudio on a cloud machine. Rstudio themselves offer a docker image for this. Its accessible via a browser so anything with Internet access is fine as a dev machine. Get a cheap Bluetooth keyboard for your phone and you are ready to go. The price difference between a Bluetooth keyboard and a macbook pro pays for a whole lot of cloud service and it's always scalable to whatever power you need. Turn it off when your not using it and it costs you nothing during that time. 
I'm a PhD in Wildlife Sciences. Any ideas what type of analyses she'll be running or field of research?
Not talking about flaws in your code but I would say that this approach is flawed...especially using auto.arima. From my experience, I don't find auto\_arima reliable at all...I found it to be extremely unreliable in many cases and some of it's forecasts are just way off the actuals. I would instead suggest you to use arima approach where you first need to investigate the ggacf, ggpacf plots to find the right parameters (p,d,q) and then input them in the arima() function to make the forecasts. It takes time but it's lot lot more reliable than auto arima
I'm a fisheries biologist and i run the crap out of R on an old ThinkPad T530. Love my ThinkPad 
Don't think there is a way to do that. You can try editing code in google drive but there wont be a session attached. Might be able to hack something that reads that file and executes on a session in a shared machine it but can't think of a way to do it that'll be remotely useful and doesn't require too more effort than it's probably worth. Unless you are working on the exact same functionality, you're probably better of modularizing and just using version control 
RStudio Cloud.
Thank you for the feedback, will try to figure out how to limit the zoom. I used this code to hide the error message tags$style(type = "text/css", ".shiny-output-error { visibility: hidden; }", ".shiny-output-error:before { visibility: hidden; }" )
Thank you. Make it interactive is my next goal for my future shiny app.
I have an Air. For most of what I do, it is very fast (and would be on either). There are certain situations where that is not the case (e.g. recently when I was handling a dataset of 20,000 individuals with ~20-ish values recorded for each and I needed to perform a bootstrap on it, it took about 20 minutes which has happened to me on one occasion) but those are rarer and they depend on your needs. Do they expect to use that much data?
It depends entirely on what she plans to do. Super simple linear regression modeling on small datasets? A Raspberry Pi would suffice. Machine learning on massive raster data? You could crash a million dollar cluster computer by maxing out the RAM. I agree with the Thinkpad route... cheaper and more powerful. I’ve never felt very comfortable using R and RStudio on a Mac, for some reason it just seems better on a PC (and best on Linux)... but that’s probably just my own preferences coming through. If she is planning on doing lots of graphic work (Adobe) I’d recommend a Mac. Also all of my Macs have lasted forever (I still use my 2006 MBP from time to time). 
You’d have to make me learn R to prove it. Good luck! 🐍 🐍 🐍 
is this a deep fried meme in /r/R sub. A surprise to be sure
I love it
He’s not the messiah he’s a very naughty boy
*boi
Just bought a 13” MBP. Runs R like a dream. Graphics are beautiful 
“The Pandas API sucks” “stone him!l
This would not give OP what they are looking for (although RStudio Cloud is great). To get this kind of interactive editing, it might work to install and run RStudio Server.
Uhm, this is r/Rlanguage? Are you lost good sir? 🤔😆
Yes you are right. I'm not entirely using auto.arima for my final version, just for a comparison. I checked different combinations based on lower AIC, acf or pacf. Auto.arima didn't produce the best fitted model most of the time, even though I enfored stepwise=FALSE, approximation=FALSE criteria to limit its approximations. I needed a comparative overview which is why this code, I'm definitely gonna validate individually before I put these results into implementation. 
/r/R ?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rstats] [We need to unify the R subreddits!](https://www.reddit.com/r/rstats/comments/9h3jro/we_need_to_unify_the_r_subreddits/) - [/r/rstudio] [We need to unify the R subreddits!](https://www.reddit.com/r/RStudio/comments/9h3t84/we_need_to_unify_the_r_subreddits/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
We need the moderators on board for that. A user migration to a unified subreddit will only happen if the other subreddits close down. Choose a subreddit or make a new one, invite moderators from every old subreddit to participate. Also you might need a tagging and filtering system for specific subgroups of content.
This is a great idea. Happy to help out wherever I can. 
Agreed Make the moderators of the old subreddits the moderators and subject matter experts for their sections of the new one?
Like the tidyverse of R subs
/r/eddit?
I'm on board and will sub to the winner after the other R subs fight to the death
The true answer. 
Relevant xkcd - https://xkcd.com/927/
One sub to rule them all! 
You use it to select named elements usually from a data frame but I think can also be used for named elements from a list. So if you had a data frame called data with 2 columns x and y you could access the columns by doing data$x or data$y.
I’ll only agree to this is we can use r/R. 
I was really disappointed when I found you r/R wasn't the R subreddit
Indeed. It would be great if the moderators could figure something out. Maybe lock the old subreddits and point users to the unified one? I'm not sure, I don't have any experience in moderating.
Yeah, that's what I also thought about before posting. Lets hope it doesn't come to this.
Great idea! 
/r/Rrrr
R Moderators, assemble! Just kidding, I'll let OP do the mass summoning ritual if they want to take their idea further, but they have my vote of confidence to try. Surprisingly, not much mod-overlap between subs (they may not even be aware of all the R subreddits), but there is cross-linking in the sidebars of the larger ones, suggesting that the userbase could probably get along if they were merged. Logistically, I would suggest merging into one of the larger existing subreddits, purely on principle to prevent needless sub proliferation (as per the warning of xkcd #927). Personally in favour of using /r/rstats simply due to size of userbase, and because I learned R from the statistics side of things, but would accept /r/Rlanguage as the general-purpose subreddit. **Listed mods by subreddit:** rstats - vsbuffalo - hadley - ffualo - deanat78 - hrbrmstr - krossvalidate - keepitsalty RLanguage - infracanis - hadley - VallenderLabs - IamGrabear RStudio - BooRadleyBoo - BillCarney - pyl_time - stevenfazzio - VallenderLabs - IamGrabear rprogramming - /u/1608 (hasn't posted/commented in over a year?) - shaggorama R_Programming - fnord123 - Darwinmate rshiny - wouldeye - jerburt RStatsProgram (literally one post, zero comments, created by the sole mod; this post may cause their largest traffic spike ever) - In_Sillico
since /r/rstats is the largest one, just unify others with it
 The People's Front of Judea?
Well, I would just be happy, if people realizsd that R foes not equal rstudio, which is just an IDE, one of many out there. This monopolistic tendebcies worries me.
Wouldeye also moderates r/learnrstats. 
Thanks, I seem to be getting the following error message: Error in grouped_df_impl(data, unname(vars), drop) : Column `ID` is unknown &amp;#x200B;
Great idea, I hope the mods will agree to this.
Can flair it as such
Any idea of the usage rates of different IDEs?
We could appeal to the admins for an exception...
Yes
I knew what this was without opening it! One of my favorites. 
Damn I had no idea there are so many R subs, knew only Rlanguage and rstats. Even though I asked questions only on Rlanguage. Great idea to unify R in one subreddit. You have my support.
Irony is when Hadley Wickham is not a moderator of /r/RStudio.
yes, there are definitely times I wish I had something in between rstudio and vim, but I think anyone considering making an IDE has a hard time when rstudio is so deeply paired with R
I'm not familiar with all those subs, but as a mod of rstats I do agree. I may not be 100% unbiased, but I think rstats should be the unifying sub as it's the largest and is also the stackoverflow/Twitter tag people use most commonly. Someone needs to reach out to other subs mods and ask them if they agree to make the necessary changes to their subs
From a user engagement stand point, this is the best answer.
Seconding this: ESS is a very usable alternative to RStudio.
Splitters!
As moderator of /r/learnR I concur that consolidation makes sense.
https://www.reddit.com/r/ggggg/
I'm using RKWard and I'm really happy with it.
I completely agree
Mods seem open to discussion. A couple even pointed out their other subs in response to my post. You can add /r/learnR and /r/learnprogramming to your list of R subs, which brings us to a total of nine subs diluting any searches for *the* R subreddit. As for tags/flair brainstorming, I would leave it to experienced mods for what works and what doesn't, but there seem to be some common themes suggested based on how the subreddits proliferated. (Alternatively, someone smarter than myself could run an unsupervised k-means clustering algorithm to suggest flair groupings based on the aggregate posting activity ...) **Post flairs/tags that a general R subreddit may need:** *Software/Tools* - new R releases, IDEs such as RStudio (and hopefully more in the future), and package releases/updates, and specifics such as working with rshiny, ggplot2, tidyverse, and "hey I made a package and uploaded it to GitHub" etc *Tutorials/Learning programming/Beginner questions* - People looking for or offering help with the basics. Also, a bot should probably always respond with "here's where to download R, here's RStudio which is helpful, and here is Stack Overflow for R so you can google whether someone has already asked your exact question. Now that we covered the basics, you can ask people for help." *Discussion/Theory/Advanced techniques or Optimizations* - Discussions along the lines of "what package does *this* the fastest/most CPU-efficient/with the least memory/multi-threaded or distributed across multiple machines &amp; servers," when to use R vs other languages (or along with other languages), limitations of certain functions or data structures, or discussion of professional/academic works using R *Troubleshooting/Feedback/Testing/Project Help* - Posts should be required to include some concrete code (functioning or otherwise) or data set (or some dummy data to clarify the format/types), or a specific goal in mind to get specific answers. Or something semi-complete that seems to work, and they just want feedback whether their approach/implementation is appropriate given their specific context. *Statistics* - The section I would prefer to browse separately, and relevant to the purpose of the language, where posters can ask for/propose project ideas, and discuss how to use R in the context of specific statistical analyses. *Miscellaneous/Fluff/Humour/Fanart/Meta* - somewhere to put all the discussion that isn't exactly within the scope of the subreddit, but where this is probably the place it will be most appreciated, and tagged such that the rest of the sub isn't accidentally tainted by /r/ProgrammerHumor and "Rule 34: R x Python fanart" posts. I think the shitposting will be inevitable if the combined subreddit grows to a certain critical mass, so best to pre-emptively sandbox it . *News/Releases/Articles* - Honestly, most of these topics could probably be sorted within the above divisions, unless people start posting their blog posts, or every time some entity ranks "the top N languages for Y field", or just want to share/discuss good articles from academic/applied/R-blogger-type sources. Still on the fence about this one, just brainstorming.
Well, if C didn't get one...
We should not confuse a programming language and one of many IDEs for this language. Thus, r/rstudio should be out of discussion
r/letterR
[nvim-R](https://github.com/jalvesaq/Nvim-R)? Though I'm pretty happy with tmux and vim-slime.
r/rstatsmemes
Don't act like r is successfully used in marketing analytics. It's all smoke and mirrors!!!
It’s free
I wholeheartedly agree, especially with the point about other sites using rstats. Is rstats interested in new mods? I'd love to be a part of an initiative to get more activity going on rstats to help convince the members of other subs to migrate over. 
I've never looked into tmux or screen much, since my i3wm situation is perfect for my needs and it seems overkill, though I just spent some time trying to get tmux and vim-slime working, but it's just janky enough that I decided eh skip it. :) I'm not really interested in debugging it, but for instance: C-c C-c was very unreliable, a few times I got it to prompt me for socket/pane, but mostly it seemed to ignore it, and sometimes vim would try to interpret (telling me I need to !qa or whatever) I'm not sure where in the chain things went wrong, but it's not worth the bother for me. Rkward wants a massive paragraph of dependencies pulled in, which feels quite silly to me and I suppose I'll check out nvim-R, but at a first glance it's looking like a lot of hassle. Recently I was looking at Ruby, and someone has made a REPL called "Pry", an improvement on the default irb. It is really great! A lot of very intriguing features, ways to write functions and save them or open your default editor to edit a multiline function you've written, color syntax, other neat features (even works as a debugger, you can add a call to pry in a ruby script and it will open at that point, for quick debugging) Something like this would be fantastic for R, basically an upgraded command-line interpreter. But in the end, I actually love Rstudio so I am happy. The best would be if they came up with an easy "minimal mode" that toggled all the dressings and toolbars down into completely unlabeled planes: maybe script, REPL, files, plot. I mean, it's already pretty minimal but those couple inches can really make a difference, especially for someone like me who uses tiling WM and would like to comfortably have a PDF open next to the IDE. 
What is the need for shiny server pro, shinyapps.io r studio connect etc? Does the free version have password protection and SSL certificates?
Ha! I got to give a talk at NIST a few years ago and I couldn't resist putting that xkcd as my final slide. The reception was... mixed. :-)
Rstudio server doesn't using the same session on the same file and doesn't support collaborative editing. just like normal rstudio
Ah, the [pro version](https://support.rstudio.com/hc/en-us/articles/211659737-Sharing-Projects-in-RStudio-Server-Pro) does seem to support this indeed
Sweet! I just tried atom with hydrogen and it seems to work well. Is there also a more traditional REPL plug-in that works with R? 
new to all subs and new to R itself. But would love if there was one main sub
I didn't find the atom-R and atom-ide packages in the atom package manager. I don't know how the atom ecosystem works, but what is the recommend setup for R, and where do I find these packages? I'm a bit scared to just install every package possible for R. 
I was joking. I actually use r for market research. But I do hear the smoke and mirrors accusations thrown around by people from time to time
&gt; (a&lt;-100,b&lt;100) &gt; b&lt;100) That's a comparison, not an assignment.
I am not sure, ifelse takes vector as input and gives out vector as output
 ifelse(1==1||2==2||3==3,(a&lt;- b&lt;-100),(a&lt;-b&lt;-1000)) Look into case_when() as an alternative. https://dplyr.tidyverse.org/reference/case_when.html
Yeah but that's like this: v = c(1,2,1,2,1,2) ifelse(v == 1, 10, 5) outputs: 10 5 10 5 10 5
You will want to look at axis() specifically the at and labels parametets.
Ifelse(any(1 == 1, 2 == 2, 3 == 3), eval(x), eval(y)) ??
You are going to want to use an IDE like RStudio for development rather than from the command line. If you want to clear everything from memory you can use a command like "rm(list=ls())" 
I would start a session, then exit and choose the option to not save the session, then start another session. Is there a more elegant way?
`ifelse()` and `if () {...} else {...}` structures in R are intended to work in very different circumstances. The former, which you are trying here, is best designed for operating element-wise on vectors. For example, if I wanted to make a new variable `x` in the `iris` dataset, which is "small" if `Sepal.Length` is less than 4.5, and "large" if not: iris$x &lt;- ifelse(Sepal.Length &lt; 4.5, "small", "large") Now, I'd have a column that gives me the intended results. However, `if () {...} else {...}` is much more designed for control flow. So, if you're assigning values to objects in an environment (which it seems you are, so I'll use your example): if (1==1 || 2==2 || 3==3) { a &lt;- 100 b &lt;- 100 } else { a &lt;- 1000 b &lt;- 1000 } Also, note that you're trying to use commas (`,`) for statement separation, but R uses semicolons (`;`) for multi-command single-line statements; but please also note that both are bad practice.
Rstudio's [Shinyapps](https://www.shinyapps.io/) is probably a best use case for you here since their enterprise level is $3600/yr and you might even be able to get away with the standard with is only 1200/yr. Standard does include Authentication which will work for your use case but only if you don't need to scale this as users have to be added manually. 
When I do this, it still saves the commands.
This still leaves previous commands that I have typed. Every time I hit the up arrow, a previous command from a previous session comes up.
Ctrl+L?
What commands are you running to save your history?
Ha.
just delete your .Rhistory file
grep/gsub - i do a lot of string manipulation and search..
Here's one way to tackle it. Read the y/n column into a dataframe as character values. Convert to lowercase, create a new column with 1/0 numeric encoding library(tidyverse) your\_dataframe &lt;- read.csv(path, stringsAsFactors = FALSE) clean\_df &lt;- your\_dataframe %&gt;% mutate( yn\_clean = toLower(yn\_column) %&gt;% mutate( yn\_numeric = if\_else(yn\_clean = "y", 1, 0)
%&gt;%
R Studio has the broom icon? 
Maybe something like this: #create a dataframe dat &lt;- data.frame(as.factor(c("yes", "YES", "Yes", "no", "NO", "No"))) colnames(dat) &lt;- c("Var1") # look at it: str(dat) &gt; 'data.frame': 6 obs. of 1 variable: &gt; $ Var1: Factor w/ 6 levels "no","No","NO",..: 4 6 5 1 3 2 # convert values to consistent value and re-factor dat$Var1 &lt;- as.factor(tolower(dat$Var1)) droplevels(dat) str(dat) &gt; 'data.frame': 6 obs. of 1 variable: &gt; $ Var1: Factor w/ 2 levels "no","yes": 2 2 2 1 1 1
? &amp;#x200B; No joke, probably the most useful bit of R code out there
Thank you! This worked.
This is a non-generic way to do it : #make a vector of all yes/no columns response = c('b','c','d','e') #convert to character format data[,response] = lapply(data[,response], as.character) #convert to numeric 1 0 data[,response] = sapply(data[,response], nchar) - 2 # convert back to factor if you want data[,respose] = lapply(data[,response], as.factor) Note that numeric operations are possible on logicals, its treats yes or true as 1 and no or false as 0
dat&lt;- data.frame(t=seq(0, 2*pi, by=0.1) ) xhrt &lt;- function(t) 16*sin(t)^3 yhrt &lt;- function(t) 13*cos(t)-5*cos(2*t)-2*cos(3*t)-cos(4*t) dat$y=yhrt(dat$t) dat$x=xhrt(dat$t) plot(y ~ x, data=dat, type="l", bty="n", xaxt="n", yaxt="n", ann=FALSE) with(dat, polygon(x,y, col="hotpink")) points(c(10,-10, -15, 15), c(-10, -10, 10, 10), pch=169, font=5)
 dat&lt;- data.frame(t=seq(0, 2*pi, by=0.1) ) xhrt &lt;- function(t) 16*sin(t)^3 yhrt &lt;- function(t) 13*cos(t)-5*cos(2*t)-2*cos(3*t)-cos(4*t) dat$y = yhrt(dat$t) dat$x = xhrt(dat$t) plot(y ~ x, data=dat, type="l", bty="n", xaxt="n", yaxt="n", ann=FALSE) with(dat, polygon(x,y, col="hotpink")) points(c(10,-10, -15, 15), c(-10, -10, 10, 10), pch=169, font=5) &amp;#x200B;
Adding grepl into the mix.
grepl, interaction, fread, lapply, sapply, which, and logical filtering show up in my work almost all the time.
Frankly, I think a case_when inside of a mutate is super helpful. Before I learned that I was using if_else statements but that ended up having millions of commas. Now I just ~ it away.
Ok, somebody tell me what they are drawing. I’m on mobile. 
There is no downside to using fread() to load csvs unless you count the fact it’s not a base function. That said, I find myself loading data from databases or data warehouses which is Infinitely preferable if you have that infrastructure. If you are saving r data structures and loading them later, loadrds() and saverds() are much faster than csvs. 
Have a whole section on apply type functions. Programmers coming from other languages are so tempted to do for loops and these are really slow in R.
If saving r dataframes and loading them later, I prefer fst or feather packages.
Same. 
One of the first things I teach my junior data scientists when they start
Feather has the additional advantage of being language agnostic. 
 AskingForHomeworkAnswers &lt;- T if(AskingForHomeworkAnswers){ cat("Do your own fucking work") }
Listen man, my collective experience with R is less than 24 hours. Equating it to a language, if I asked someone what their favorite phrases were in a foreign language, you telling me to go look up some in a book is not only going to be less helpful, but also less conducive to actually learning how real people speak that language. You don't have to be a dick.
Pipes are nice, but I feel like they're only useful if you don't want a giant string of code.
Some bits of R that always make me smile: - `parallel::mclapply()` is just so seamless (but I don’t think it works on Windows) - `rapply()` is a recursive apply. Think nested lists, or those times where a column in a data frame is actually two vectors (happens more often in web scrapping that I’ve noticed). - `-&gt;` is the same as `&lt;-` but the other direction. When I’m writing code I know I’ll be sharing I like to sneak in one or two of these. - `%in%`, to return `TRUE` for: `”a” %in% c("a”, “b”, “c”)`. - `substitute`, for when you have a variable called `df` but you want that name as a character: `substitute(df)` will return `”df”`
Loops in R are exactly as fast or slow as using apply functions. (There was a performance bug in past versions of R which made loops slower, but that was a long time ago.) Apply functions are better than loops because they abstract the looping away and thus increase expressiveness, readability and robustness. Not because of performance.
Actually every pirate’s first love is the C.
`$` is used to index into a list using a name rather than a numeric index: test = list(foo = 1, bar = 2) test$foo # prints 1 This also works with data.frame columns, since data.frames are lists: mtcars$cyl Note that you’re indexing using a *name*, not a string. So the following doesn’t do what you might think: var = 'cyl' mtcars$var Instead, you’ll need to use the conventional indexing syntax, which works for string names and numeric indexes: mtcars[[var]] To recap, `list$name` is roughly equivalent to `list[['name']]`.
Thanks for this. I tried to get it to work with readr before and kept getting bugs I couldn’t figure out. 
You got me there
I think that might be more useful than you suspect. I love the pipes because they make the code more *readable.* I huge portion of the mistakes I make are simply because the code gets too dense and I can't follow the flow of it. The utility of being able to determine what a snippet does in a few seconds is hard to underestimate
Used this recently until I realized the data set already had lat and long coords in it. Felt simultaneously smart and dumb. 
Or just paste it into data.table::fread
Good tip for snippets, but if you are a package maintainer or want to write a library, avoid `::` and use the @import or @importFrom roxygen2 tags like recommended in Hadley's book.
No, that’s not actually [what Hadley’s book recommends](http://r-pkgs.had.co.nz/namespace.html#import-r). To be perfectly clear: **Using `::` inside packages is recommended.** `@importFrom` is *only* recommended if you keep using the same functions over and over again.
I was under the impression that there is a limit of like 200 records when you do this? Is thattrue?
Chains using dplyr.
I remembered that part of his book wrong it seems, but I stand by my point. The additional task of remembering to write the dependency in the Description file is a possible error source, as I have seen many times in my own code, that of colleagues or packages on GitHub. It's also unnecessary to use a second method in package code, as @importFrom keeps the dependencies neatly on top of your code to be seen at a quick glance and is even minimally more efficient (which definitely matters for functions within deeply nested loops). I use `::` often, but never in packages, only in interactive scripts.
It’s not really an error source since `R CMD CHECK` will tell you if you forgot it.
2,500 a day, limited by google api. You can use the code geocodeQueryCheck() to see how many you have left for each day.
Interesting. I spent a lot of time transitioning away from for loops because of performance - perhaps it was on the older version. Thanks for the insight.
Are you using quotes? setwd('/home/me/desktop')
&gt;Error: 'clipboard' is not an exported object from 'namespace:readr' :( &amp;#x200B;
[Relevant.](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/)
Thanks for sharing that I had seen it before but forgot about it. It is a bit strongly opinionated but i think it describes generally good practices.
Update your readr version from Github, the version on CRAN is ridiculously outdated I think.
Ok, tried something new. Continuing after the filter, I tried count(column 2 &gt; 0). The result was a tibble with FALSE, TRUE, NA and integers for each. Using the integers I would just like to do (TRUE / TRUE + FALSE +NA). 
In dplyr.. your_dataframe %&gt;% filter(is.na(column_name))
Can this handle multiple columns?
This will return observations that have at least one NA/blank in any of the columns, including the row with the missing Date. I think you need something a little more specific. Let me switch to my laptop and I’ll see what I can do.
Thanks! I think this is the best I'll get (and it works fine), but I was hoping to avoid having to use an 'or' statement for each of my 16 result columns. It's not a big deal though, thanks again!
Thanks!
Oh I didn't realize that you had that many columns to filter on. There's probably a more elegant solution then. Let me keep messing around.
If you can turn the vals to be TRUE or FALSE, you can get the % of positives by using mean() on that column. 
RDS provides better compression than Feather if that is important to you.
One thing to consider is the size of the datasets they need to work with. R is a notorious memory hog, and if they are working with "big data" (just threw up a little on my keyboard typing that) then the amount of RAM might be important. That being said, I can get most of my work done with a 4GB laptop. Only after I started working with spatial data did I start to encounter memory allocation errors. Unless they need to read several GB of data into memory at a time, I doubt they'll notice any major limitations with the Mackbook Air versus the Pro. Another thing to consider is the number of cores that each machine has. A more experienced R programmer finds was to parallelize their code to improve performance, and the number of cores then becomes a bottleneck. 
?complete.cases()
Good idea! data[!complete.cases(data[,c("Result1", "Result2")]),] 
For a base with dplyr fairly easy solution: `mt[complete.cases(mt %&gt;% select(-mpg)),]` You could make this completely in base by selecting columns in base rather than with select... but if you've got more than 1 or 2 columns to exclude, I vote dplyr. complete.cases returns a logical vector with TRUE if there are no missing values in the row. By selecting out the columns we don't want to check, it only checks complete.cases on the 16 (in your case) columns you do care about so you get only the rows you want. Sounds like you're comfortable with the dplyr or base methods. I'll add a data.table solution (with the data called mt, which is mtcars in data.table form if you feel like popping in a few NAs and testing this out) `mt[, missingcount := rowSums(`[`is.na`](https://is.na)`(.SD)), .SD = !"mpg"]` `mt[missingcount == 0]` The first line checks all the columns in a row for NA except mpg (the .SD = !"mpg") and sums the result. If the sum is &gt; 0, then an NA was found, naturally, so we can just filter the missingcount column to values equal to 0. This thread is a great example of how we can do the same thing so many different ways in R. &amp;#x200B;
&gt;Ok, tried something new. Continuing after the filter, I tried count(column 2 &gt; 0). The result was a tibble with FALSE, TRUE, NA and integers for each. Using the integers I would just like to do (TRUE / TRUE + FALSE +NA). What is the OP trying to do? Is him trying to calculate the percentage of what?
&gt;If you can turn the vals to be TRUE or FALSE, you can get the % of positives by using mean() on that column. &gt; &gt;Something like this: data %&gt;% select(column1) %&gt;% mutate(positive = column1 &gt; 0) %&gt;% summarize(positives = mean(positive)) &gt; &gt; &gt; &gt;Is the OP trying to get the % of the frequency? Is he trying to transform frequencyto percentages? I don't understand &amp;#x200B;
So I’m on mobile but this is what you’re looking for: data %&gt;% select(column1, column2) %&gt;% filter(column1 == x) %&gt;% summarize(frequency = sum(column2 &gt; 0)/n())
It’s creating a 100x100 matrix and adjusting the values of the matrix based on certain conditions in the loop (for .. in ...). What is your end goal in learning R?
Im trying to understand A* and im looking at this example: https://github.com/CodingTrain/website/pull/404/files When he runs the demo im guessing the function "huller" is adding some random blocks, but I cant understand what this code does below it? Could you explain that for me?
The percent of rows in column 2 that contained a positive integer; I meant (TRUE / (TRUE + FALSE +NA)). I ended up doing two sets of select(), filter(), etc. to get the numerator and the denominator, each a assigned to a variable, and then did the division with the variables I assigned.
Thanks, I ended up doing it a different way, but I will try this too.
Thanks, I haven't used the mutate function yet, so I'll give it a try.
FWIW my router identifies this as a malware site and blocks it
Paste isn’t a vectorized function IIRC. Try mutate. 
In the same way as paste? I'm new to R.
newFrame &lt;- cbind(Date1 = frame1$dateCol, Date2 = frame2$dateCol, Date3 = frame3$dateCol)
Easy Lots of great frameworks for EDA Connects to all databases visualisation Tidyverse 
Do you have an alternative? R is great for data mining, analysis and visualisation, but why choose R over some of the alternatives comes down to context of who you are, who you are with,and what you want to do.
Large selection of packages for web scraping, converting from various formats, connecting to databases, querying APIs. Flexible and diverse built-in plotting capabilities. Powerful third party plotting packages like ggplot2. 
depends on what kind of data you're mining. Image data? Semantic analysis? Deep machine learning? Python has much better libraries. R wins out in ease of use, availability of libraries, and traditional statistical methods. 
Consider your use case. R is a versatile option, but you may need to turn to other programs or languages if your task has very specific needs.
Possibly, because you're using R for data analysis, and it's just slightly easier to only use one language in a project?
I wouldn't use R for mining, especially if the data is dirty. I'd use Python. And then use R on the cleaned dataset.
It worked for me in Rstudio. Not sure what the problem might be.
It's because you're comparing integers to doubles. Run again with this line right under the "dirs = ..." line that's there already. dirs = lapply(dirs, as.integer)
Thank you! So weird that it works for some and not for others tho
There's usually a cleaner way to do this kind of thing in R than using loops. Maybe the method below will work for you (result is the same, but in a slightly different format) all_nodes &lt;- expand.grid(1:10, 1:10) neighbors &lt;- function(val, comp){ comp[apply(comp, 1, function(x) sum(abs(x - val)) &lt;= 1),] } neighbors(all_nodes[1,], all_nodes)
Correct. 
When I get stuck I resort to debugging. The simplest form of debugging is printing out variable information. If you `print(head(car_variable))` you'll see what it is, which will make reading the code a lot easier. Also, learning self study techniques is a butterfly effect to excellence. If you manually go out of your way to learn: - what each operator does - what each type does - each function does for each type - as well as functions not tied to a specific type, one at a time, you will become amazing at R. This can take months of note taking to years depending on your study style. A trick is to do self-study is while you're working on something else. It's a sort of piecemeal learning style. Note: If you find yourself having to learn the same thing multiple times, something in your learning routine can be optimized, similar to the same way you can replace writing the same lines of code over and over again into a function or a loop. So, for example: cbind(pH, N, Dens, P, Ca, Mg, K, Na, Conduc)~Block+Contour+Depth+Contour*Depth -1 Do you know what cbind does? Do you know what ~ does? Do you know what + does in that context, as well as * and -1? Do you know what the variables are pH, N, Dens, P, Ca, Mg, K, Na, Conduc ? If not, you can print them to see. Once you know all of those pieces you will be able to read that line of code. Pace yourself! It's okay if it takes a while to interpret a single line of code, because once you have, you'll know how to read new code from what you learned.
It appears that there is a Manova() function included with the car package that you may utilize. I believe you can include type = “III” as an argument to run a type III Manova. Should look at ?Manova since you’ve installed the car package
bare with me I have the cognition of a gold fish on my good days. So my prof said to just run a manova test like in her example. (not the one from the car\_package I believe) Var1=factor(DataSet\[,1\] #So in the original problem I'm not sure that I need to specify column one - \[,1\] Var2=(DataSet\[,3:4\]) #I'm not even sure what the other mean's or columns I'd want to test against Var3=manova(Var2\~Var1) #I cannot reach this step as I don't get what I'm suppose to be comparing.
IMO, tabular data manipulation in R is so much easier than Python. Our office will use R to do most of the manipulation and then Python via reticulate or other scripts for the heavy lifting. 
&gt;a&lt;-with(Soils, car::Manova(manova(cbind(pH, N, Dens, P, Ca, Mg, K, Na, Conduc) \~Block+Contour+Depth+Contour\*Depth -1), type = c('III'), test.statistic = c('Wilks')) ) Thank you, you damn code wizard, can you break down what your code is doing so I a mire moron can use it at a later date? I'm pretty sure I get it but I don't trust myself for anything.
Python.
I have no idea what tilde does in this parameter nor do I understand how the addition and star operators work in this context or what -1 is for. Nothing is ever explained, its just here copy this code and read me back its p-value. So I get really irritated when the class is set up to where a toddler can do it but it's so content light that if anything goes wrong what am I to do? it's not like I'm learning - other than what ctrl c or ctrl p does...
Notebooks, by their nature, don’t continually print to the viewer screen. What you type is fairly close to the final output, by design. To see the output, you can click “preview” which will open a window so you can see the output as it is *right now*. If you chose the drop down arrow next to preview, there is an option to see it in the lower right hand corner instead of a separate window. I believe you can also set this as the standard option under Tools &gt; Global options. I also suggest clearing everything and rerunning all your code before finishing. If you change code, but don’t run the chunk, the output doesn’t change when you open the .html file, which can cause confusion. 
Once you know what you don't know, you've got to google it. Figure out what the operators in R mean before, then come back to it. The code will be more readable then.
Really excellent answers and responses. I like your style. 
Thank you. \^_\^
&gt; Notebooks, by their nature, don’t continually print to the viewer screen. They do once you’ve opened the preview window for the first time: every time you save your document, the preview updates. Whether this happens on saving or on every keystroke is in principle an arbitrary decision. In unrelated matters, this live update of Notebooks is still fairly buggy.
Fix your router configuration. Ideone is a fairly large code sharing website, it’s not a malware site and not more prone than other sites to accidentally hosting malware via ads.
Yes. I was referring to not updating with every keystroke. Saving re-renders the html output. While this behavior is technically arbitrary and could be changed in the future, I suspect it is unlikely. The RStudio team seems to be philosophically dedicated to treating the notebook pane as the single necessary focus (plotting figures, minimizing the console, etc.) and, from a personal standpoint, I’d rather the computing power be spent doing analyses over continuous rendering. Maybe a “live viewer” rather than a preview might appear in the future? I’m not sure what else is in the pipeline. 
&gt; Yes. I was referring to not updating with every keystroke. But this is *not* a fundamental feature of Notebooks, it’s a (probably intentional) limitation of the current implementation. There are plenty of equivalent pieces of software that perform continuous update, from online editors to plain old Vim + LaTeX preview. I agree that the focus for Notebooks appears to be on the edit pane but given how fundamentally different the rendering inside the edit pane and the Preview window is, there will either need to be a convergence of the two or (more likely) an improved live update of the Preview pane.
Sure, I guess as a concept. Notebooks, I think, are still based upon Knitr/pandoc, which requires the rendering step. Switching to a new technology would be possible, but would probably be difficult to keep back compatible. I’m not sure I’d agree there are major differences in the view between the edit pane and the preview. Markdown is meant to be a slightly simplified “what you see is what you get” (wysiwg) format, but I guess you’re seeing a stronger need than I. 
I just want to say this is a great discussion. I haven’t had a chance to speak with anybody but baby R users in over a year. 
For the most part I agree. The differences come in when using more advanced pandoc features (to the extent that R Notebooks support them) and MathJax.
Oh, MathJax. Everyone’s favorite thing that never works quite right. 
You're right, you are pretty close to a solution. I'd like to suggest that you don't manipulate the data that you have, but add to it instead. Make a new column that has what you want for 'A', while keeping the original 'A' column. Here's an example using dplyr's mutate: mydf %&gt;% mutate(A_new = str_replace(A, C, "") #str_remove() might be better: mydf %&gt;% mutate(A_new = str_remove(A, C)) Those ought to work, but I haven't tried them on example data.
If you're already using `stringr`, you might want to fold in some other [`tidyverse`](https://tidyverse.org) libraries, as they're designed to work together neatly and consistently--you'll also get to avoid using the *apply family (which personally, I have a hard time keeping straight). We'll load [`tibble`](https://tibble.tidyverse.org) ("a modern reimagining of the data.frame"), [`dplyr`](https://dplyr.tidyverse.org) ("a grammar of data manipulation"), and [`glue`](https://glue.tidyverse.org)("dependency free interpreted string literals"). If you haven't already, you'll need to install the tidyverse (`install.packages("tidyverse")`) to access these libraries. library(stringr) library(tibble) library(dplyr) library(glue) # reconstructing your base dataframe with tribble() df &lt;- tribble( ~A, ~B, ~C, "Apple Lime", 4, "Lime", "Lime Pear", 6, "Pear", "Grape Berry",6, "Berry" ) df # # A tibble: 3 x 3 # A B C # &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; # 1 Apple Lime 4 Lime # 2 Lime Pear 6 Pear # 3 Grape Berry 6 Berry # mutate() alters tibble columns, # glue() evaluates data as string literals, with {obj} template syntax df_2 &lt;- mutate(df, A = str_remove_all(A, glue("\\s*{C}\\s*"))) df_2 # # A tibble: 3 x 3 # A B C # &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; # 1 Apple 4 Lime # 2 Lime 6 Pear # 3 Grape 6 Berry Hope this is helpful! If you're new to R, I can't recommend [R For Data Science](http://r4ds.had.co.nz/) enough. 
Man, that syntax is super simple, thank you. How would you recommend incorpporating this into the s/r/m/l/etc.. apply to loop over the dataframe. I'm still a bit mixed up by when to use which version of apply and whats required syntactically. Thanks!
In this case I wouldn't recommend using an *apply, but only because there's a solution that doesn't need it. I've had a hard time using the *apply functions when there's a tidyverse solution that doesn't need them. You can try reading [this](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family) to help understand which apply to use when, but trial and error along with looking at lots of examples is the best way to learn them. It can take a while. I still need to refer to the help pages when using some of the apply() functions in many cases. TL;DR: tidyverse is fast and easy. Try to learn base R too as you go along.
&gt;If you chose the drop down arrow next to preview, there is an option to see it in the lower right hand corner instead of a seperate window This fixed it, thank you! 
Hey, SkrrCan420, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
Hey, just following up - this was a perfect solution for my specific problem! Thanks for the additonal hint on the purrr library as well! You rock. 
Anytime! Glad to hear it worked out--enjoy getting to know R better as you read on :).
You may be able to do it with select_if
Not sure how to do this via dplyr. I will look into that but these are the sort of things that base R is quite good at: table8 &lt;- flights[,seq(1,ncol(flights),by = 2)]
A few answers here: https://stackoverflow.com/questions/24440258/selecting-multiple-odd-or-even-columns-rows-for-dataframe Easiest way doesn't use dplyr: flights[,c(TRUE, FALSE)]
dplyr::select() can work with the index number of the column or the names. For example, select(day, month, year) select(-vars(dep_time:lastcolumn)) select(1,2,3) select(-4:length(names(df))) All do the same thing. Since you need to come up with an index, you could do something like: select(seq(2:length(names(df)), by = 2)) Or you could use square bracket subsetting and do something like: df[,1:length(names(df)) %% 2] I typed this on my phone so there may be some minor mistakes, but good luck! 
Apparently its very simple and a quick google search showed me that dplyr select() can take numeric positions table8 &lt;- flights %&gt;% select(seq(1,ncol(flights),by = 2))
Why use base R when you could load dozens of packages with thousands of functions with redundant functionality that matches the syntax that happens to make sense to you at that moment? That way when you hand off your code people practically *have* to ask you about it, and you get the chance to look smart! 
Show me on the doll where Hadley hurt you.
Yeah. I just need to mutate some tibbles and I'll be back in my happy place. 
The normal way you do it in data take would be to use names eg `flights[ ,.(YEAR, DAY, DEPARTURE_TIME, ... etc)]`, although the question does seem to be asking for indexing by number. It's a weird question in my opinion. 
\*points at base R\*
The easiest way to drop a column in base R: df$col &lt;- NULL
Yeah man using implicit recycling of vectors is just so much clearer and better! /s
Better: format code properly (by prefixing it with four spaces). Don't use hard line breaks (unless you post poems), use code formatting and paragraphs instead.
You're misunderstanding the the mode function. In the future, try using help() or question mark function ?() to read documentation. Basically, mode() in R is the equivalent of the python type() function. It just returns the internal storage mode. To get the statistical mode, you will need to use something like table() to count the number of occurences of each value, sort that table, then return the most frequent one. This link has an example: https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode 
The function `mode` tells you the storage mode of an object. That is not what you want. You will have to write your own function to compute the mode of a variable. There are a couple options in here: https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode 
Datacamp has a time series course. $30 a month for datacamp but it’s been useful for me. Also rob j hyndman has a free online text called forecasting principles and practice which outlines forecasting in R in an applied methodology. Very good resource and it’s free I’ve been siding it a lot lately. Highly recommend this book. 
Thanks!
I used DataCamp with zero R knowledge. Did the finance track. If you have experience with R skip the initial courses. I personally liked it. 
I second Forecasting: Principles and Practice online text, it's one of my favourite for time series analysis which I do extensively at my job. Also try some courses at edX and Datacamp, both are quite good for R. I find datacamp's R courses more comprehensive though. They're having 50% discount for one year subscription. Highly recommended, well I subscribed today haha! 
Thank you I just downloaded the pdf and gonna check out that deal as well
Plus one for datacamp 
This is one of the best resources I know of today. 
with() will call any code in the 2nd argument in context of the first, ie. use the data from soils for any object referenced in 2nd call like the pH, N, Dens, etc. The rest just uses the Manova function from the car library ( car::Manova) to something on the object generated from the call to manova(). The output from Manova() called via 'with' is stored in 'a'. 'a' is printed.
On the other hand, some combination of table() and square bracket subsetting is much more obscure and difficult to read for a beginner than what I wrote. 
Ah, a sneaky edit to add the `library()` call to prove your point! :) But of course, I agree, just wanted to point out the lack of context.
If your data is in a data frame and you want to see what records are in your data you can use table() which will list all the different records in your data set. I hope this helps. 
shouldn't this just be a data frame? then you can use the standard operators to assert your conditions? &amp;#x200B; &amp;#x200B; |Breed|Speed|Weight| |:-|:-|:-| |Doberman|8|5| |Daschund|6|3| |Bulldog|5|9| &amp;#x200B; And then if you're using dplyr, you could simply do something like below to find the fastest dog &amp;#x200B; `data %&gt;% top_n(1,Speed)` &amp;#x200B; [https://dplyr.tidyverse.org/reference/top\_n.html](https://dplyr.tidyverse.org/reference/top_n.html) &amp;#x200B; &amp;#x200B;
R doesn't have any native hashmap/dictionary data structure, there is the [hashmap](https://cran.r-project.org/web/packages/hashmap/README.html) package which should be far more efficient than any dataframe lookups people will recommend.
I have tried, but the speed is still slow. Since I knew how to make something like a dict in python, tried and with a dict, the speed was really fast for big datasets. I only gave an example with dog breed, when I have a df being like Dog Breed|Fish Type|Horse Breed|... :--|:--|:--|:-- Dog1|Fish1|Horse 1 Dog2|Fish2|Horse 2 Dog3|Fish2|Horse 3 Dog1|Fish1|Horse 2 ...|...|... where I wish to know, which index in my df, I have Dog 1, Dog 2, Dog 3 ... for dogs. Fish 1, Fish 2, Fish 3, for fishes and Horse 1, Horse 2, Horse 3 ... for fishes.
Thanks will look into it
Can you be more specific? :)
Sure thing. Try executing the command: table(dog_list) in the console. If it doesn’t work have a look at what class the data is in using class(dog_list). If the data are not numeric then convert the data using as.numeric. I would first convert the vector to a data frame: dog_list &lt;- as.data.frame(dog_list) colnames(dog_list) &lt;- c(“dog_types”) #This will then create a data frame called dog_list; while the column name for the types of dogs is dog_types. # Convert column values to numeric: dog_list$dog_types &lt;- as.numeric(dog_list$dog_types) Now you should be able to use the function table(): Dog_Summary &lt;- table(dog_list$dog_types)
Your dataframe example doesn't make sense to me. What are you trying to represent with each row?
index|Dog Breed|Fish Type|Horse Breed|... :--|:--|:--|:--|:-- 1|Dog1|Fish1|Horse 1 2|Dog2|Fish2|Horse 2 3|Dog3|Fish2|Horse 3 4|Dog1|Fish1|Horse 2 5|...|...|... So I want to be able to lookup, at which index' Dog1 is. In this small example it would be Dog1_index = [1, 4], Dog2_index = [2], Dog3_index = [3] Same for fish and for horses. If I do a which(Dog1 == Dog_breed_column) then it is really slow for large datasets, so that is why I was wondering if I could make a dictionary with the indexes for each column
I exactly sure of what you want , but I believe that you arriver looking for the which function. 
It kind of does: environments by default use hash lookup for names, and several functions use it when performing data frame merges (data.table? Can't remember…).
Named lists are the same as dictionaries. But the question is why do you need the indexes? Sounds like a messy way to do things. You need to use a dataframe &amp;#x200B; &amp;#x200B;
Is there any relationship between "Dog1", "Fish1", and "Horse1" in the first row? If not, why wouldn't you just store these in separate vectors? You can even name the elements of a vector in R. dogs &lt;- c("first" = "Dog1", "second" = "Dog2")
I wouldn't use a dataframe if all you have is a group of separate lists. It implies that there is a relationship between the elements of each row. If all of the animals in row one are in the same cage or owned by the same person or something then it makes sense.
Well maybe it was stupid of me to have Dog, fish and horse as an example. What if I have Location instead, i.e. and the index is where in location 1, 2 and 3 they are located or something else. Horse 1 can be at location 2 and 3 index|Location 1|Location 2|Location 3|... :--|:--|:--|:--|:-- 1|Dog1|Horse1|Fish1 2|Horse3|Fish2|Horse1 3|Fish2|Horse2|Horse3 4|Dog1|Fish1|Horse2 5|...|...|...|... Then i want to quickly look up my data.frame and find all the index values. I wish to build a dictionary of location 1, location 2 and location 3. Then I can just type locatio1['dog1'] and get all the index values of dog1 in location 1. Does it make more sense now? :)
If you're storing your data in a data frame, then the best way to find the index number (row number) is to grep the column you're interested in. grep('Golden_retriever', df$dogs) grep('pattern', dataframe$column) It would also work for a vector. grep('Golden_retriever, dogs) grep('pattern', vector) But I'm not sure if you're looking for a single match, multiple matches? Are you trying to feed it a list of patterns to return?
So instead of using animals, lets use locations i.e. Well maybe it was stupid of me to have Dog, fish and horse as an example. What if I have Location instead. Horse 1 can be at location 2 and 3, this is if Horse 1 have been both places. I wish to then lookup which index values Horse 1 have in location 2. I have to use the index values of each location index|Location 1|Location 2|Location 3|... :--|:--|:--|:--|:-- 1|Dog1|Horse1|Fish1 2|Horse3|Fish2|Horse1 3|Fish2|Horse2|Horse3 4|Dog1|Fish1|Horse2 5|...|...|...|... Then i want to quickly look up my data.frame and find all the index values. I wish to build a dictionary of location 1, location 2 and location 3. Then I can just type location1['dog1'] and get all the index values of dog1 in location 1. Does it make more sense now? :)
Well maybe it was stupid of me to have Dog, fish and horse as an example. What if I have Location instead, i.e. and the index is where in location 1, 2 and 3 they are located or something else. Horse 1 can be at location 2 and 3 index|Location 1|Location 2|Location 3|... :--|:--|:--|:--|:-- 1|Dog1|Horse1|Fish1 2|Horse3|Fish2|Horse1 3|Fish2|Horse2|Horse3 4|Dog1|Fish1|Horse2 5|...|...|...|... Then i want to quickly look up my data.frame and find all the index values. I wish to build a dictionary of location 1, location 2 and location 3. Then I can just type locatio1['dog1'] and get all the index values of dog1 in location 1. Does it make more sense now? :)
This is the best answer I think. R uses environments in the way op is asking. 
Thank you all!
If you want to keep that same basic structure, you can use lists, which are essentially dictionaries in Python.
*doc\_dic* ( ͡° ͜ʖ ͡°)
They *might* be appropriate but it’s likely that a better way of solving OP’s problem is via operations on data.frames, as shown in other answers.
The two things that are most similar to a Python dictionary in R (that i'm aware of), are a list or a vector. A vector is the 'simplest' and contains a group of strings with the same class/type. Whereas a List can contain multiple vectors or data frames, each containing their own sets of strings. A data frame is a list of vectors, kept in a column like format. I don't know off the top of my head what is similar to a data frame in Python, but it's like an spreadsheet or csv. When you're saying 'dictionary', I'm interpreting that as you are wanting a vector. So you're wanting a single list of all strings found in each location. To start, we're going to have your table as a data frame. loc1&lt;-c('dog1','horse3','dog1','fish2') loc2&lt;-c('fist1','fish2','horse1','horse2') loc3&lt;-c('horse2','horse1','fish1','horse3') df&lt;-data.frame(location_1=loc1,location_2=loc2,location_3=loc3) ## check that it's a data frame is.data.frame(data) To get a vector from a data frame, you want to subset your columns out as an individual object. locat1&lt;-df$location_1 grep('dog1', locat1) In this scenario, your 'dictionary' is locat1. But locat1 is the same as df$location_1, so you're just adding an extra line of code to save your data frame's column as a separate 'dictionary' object. The above code is identical to: grep('dog1', df$location_1) The only reason you'd want to create the 'dictionary' is if you want to use it downstream for something else and it's shorter than typing out the data frame subsetting (i.e. df$column_name). IF for whatever reason you're looking for subsetting a list, they'll behave a little differently. To get the vector or column, you'll have to use the following: list$df$column OR list$vector Does that kind of get more at what you're asking? 
From reading through this thread, I feel like the [XY Problem](http://xyproblem.info/) might be relevant here. Are you able to give us a snippet of the actual data that you're working with, and a high-level explanation of what you want?
Don't grep over searches, use vectorisation e.g. Isn't this just a list? dog_list = list("Greyhound" = c(1,2,3,4), "Labrador" = c(2,3,4,5,6), etc..) # get the values of one dog dog_list[["Greyhound"]] # get the indices names(dog_list) 
I’m not sure if this will help you. https://appsilon.com/fast-data-lookups-in-r-dplyr-vs-data-table/
I can't share the actual data, but I can make something similar as the data I have. Give me a day. Should I upload it here or link to a new thread? :)
You can use them in a similar way syntactically, but named lists don't use a hash map and lookups are O(N) instead of O(1)
That's be great! I think in this thread would be best.
Don't buy the year pass. Do it for a couple months until you run out of material
I’m doing it now and enjoying it. I’m already using what I’m learning at work, so that’s neat. But I would also echo what was said above and recommend just going month to month until you are done. Assuming you have regular, dedicated time to do the work, you should finish the R material in less than a year.
I suggest this as well.
Hey I am new to using R, how exactly may I connect or utilize this API?
Try the data table classes!
Yes. It's extremely inexpensive for what they offer
Totally worth it but they offer 50% off for an annual subscription 2 or 3 times a year including last Black Friday so I'd keep your eyes open for that.
Use this: &amp;#x200B; `r_object &lt;- readShapePoly("file.shp")` or `r_object &lt;- readShapePoints("file.shp")` It depends of the shape of the map. &amp;#x200B;
could not find function. Is it in Shapefiles package too?
Got my subscription last year for half off and it has definitely been worth. Lots of exposure to various applied concepts not offered in the free lessons. 
Sign up for Microsoft Visual Studio (free). The subscription has a link for 2 free months of datacamp.
Thank you. I'll be on the hunt for thise juicy deals 
Thank you. Would you mind being a little bit more specific about those "various applied concepts not offered in the free lessons"?
Sorry, mis the package. &amp;#x200B; library(maptools)
I saw a post a while back about trouble grabbing a table from Wikipedia. Decided to write up some info on start to finish for scraping the climate data tables. 
Wait until black Friday when the year long subscription is 50% off. 
Also I can’t read 
You can have $100,000 in virtual R coins you code from your first slot machine program. With the right mindset, you'll be rich!
Sorry, I don’t know anything about the slot machine since that would require reading a book or something before just. Asking a. Question in a poorly moderated subreddit 
I have a free premium version thanks to my job. I don't know how much it costs but I can say I've found several of the courses helpful, and it's great to have a resource you can dip-into easily when you encounter a new problem.
Yesterday ended up a 67% discount on annual membership. I guess black Friday will be your best bet. 
Yes
In the dplyr package, you can use the select() function and indicate which columns you want to keep. Suppose I have a dataframe named Df and I want to keep only two variables called column1 and column2. Df.new &lt;- Df %&gt;% Select(column1, column2) You can also use the minus sign in front of a variable to indicate that you want to drop that/those variables and keep the rest.
The dplyr package is grant for data cleanup, wrangling. Say you have a data set with 6 columns. You want to keep columns 1, 2, 3 or alternatively drop 4, 5, 6. You would use the select function in both cases. ``` data_set %&gt;% selcect(col1, col2, col3) ``` Or dropping: ``` data_set %&gt;% select(-col4, -col5, -col6) ``` You can be more systematic too. If your columns have patterns in the name you can use a couple of functions including starts_with, ends_with, contains. For instance if the columns you don't want start with the characters "abc" you could use: ``` data_set %&gt;% select(-starts_with("abc")) ``` Plenty of other things you can do with dplyr. But that should give you a sense that it can be useful for what you're trying to do.
Thanks for this guys. &amp;#x200B; Could I also refer to column by name? &amp;#x200B; So &amp;#x200B; data_set %&gt;% select("per607", "per608", "603") 
Sure! The dplyr solutions below are good ones. In case you haven't come across it yet, I will add that the %&gt;% is to pass what is on the left to the right. So, c(1,2,3) %&gt;% sum() will pass c(1,2,3) as the first argument to the sum function. It's really useful as part of the dplyr workflow. &amp;#x200B; I'd like to add the base R, since I think it's important to know even if you get right into dplyr and enjoy its ease-of-use. I'll use the mtcars dataset as my example because it's available to everyone, and I use it in like 95% of examples. `mtcars` `keepvars &lt;- c("mpg","cyl","wt") #variables we want to keep` `cutvars &lt;- c("disp","hp","drat","qsec","vs","am","gear","carb") #variables we want to cut` &amp;#x200B; `mtcars[, keepvars] # Passing a character vector of names to keep - works great` `mtcars[, -cutvars] # This doesn't work, unfortunately. You can see why if you just print -cutvars (or try)` `mtcars[, paste0("-",cutvars)] # Seems like it might work!... but still no. You can print the paste0() bit and see it's not quite right` &amp;#x200B; Generally to de-select variables in base R, we use a logical or numeric vector instead &amp;#x200B; [`logical.select`](https://logical.select) `&lt;- !names(mtcars) %in% cutvars # Here, we return a logical vector indicating column names that are NOT in cutvars (so, we keep the TRUE)` [`numeric.select`](https://numeric.select) `&lt;- which(!names(mtcars) %in% cutvars) # Here, a numeric vector indicating the column names that are NOT in cutvars` &amp;#x200B; `mtcars[,`[`logical.select`](https://logical.select)`] #Same result as mtcars[, keepvars]` `mtcars[,`[`numeric.select`](https://numeric.select)`] #Same result as mtcars[, keepvars]` &amp;#x200B; If you have more than a few variables, the best you can do in base R is a regular expression (or multiple regular expressions) to create a character, numeric, or logical vector of the columns you want to keep. In fact, that's similar to what I do now because I mostly use data.table, but I heartily recommend the dplyr solutions already posted - just wanted to add the base bit to help with understanding of how it works.
I just subset it out - you could remove the columns you want: CMP &lt;- subset(CMP, select -c(oecdmember, another_column, even_another_column) or you can just select the columns you want: CMP &lt;- subset(CMP, select = c(not_oecdmember, another_column, even_another_column) 
You absolutely can... the col1, col2, col3 above are just the sample names being used. In dplyr select (and in all the main dplyr "verb functions"), you don't actually need the quotes at all. &amp;#x200B; However, there's a "watch out" in the code you provided. `data_set %&gt;% select("per607","per608","603")` is fine... but for CHARACTER names, you don't need the quotes. `data_set %&gt;% select(per607, per608, 603)` Would work for the first 2, but try to return the 603rd column. If it doesn't exist, you'll get an error. If it does exist, it'll return your 603rd column, which may not be your column named "603". `data_set %&gt;% select(per607, per608, \`603\`) #back ticks` works... but I'd go ahead and add a letter or something to any column with a number as the name
that's what I would do!
When I try and open Reduced\_CMP it shows: &amp;#x200B; Error: unexpected symbol in: "Reduced\_CMP &lt;- subset(CMP1, select=c(countryname, edate, partyname, progtype, per601, per602, per603, per604, per607, per608) Reduced\_CMP"
&gt; Reduced_CMP &lt;- subset(CMP1, select=c(countryname, edate, partyname, progtype, per601, per602, per603, per604, per607, per608)) 
Beautiful!! &amp;#x200B; Presumably rows would be the similar Reduced\_CMP &lt;- subset(CMP1, select=r(1, 2, 3, 4, 5, ..., )) &amp;#x200B;
Hey guys, &amp;#x200B; Thank you all for your help so far. So I've managed to remove all the columns I don't need, which instantly made a huge difference. To achieve this I used the subset method suggested by drbaellow91 - this proved the simplest for me to do. Now I need to remove some rows. I don't have names for rows but rather row numbers. I have ranges that I want to delete: 614 - 714 1050 - 1124 1408 - 1551 1636 - 1729 2112 - 4282 How would I go about this? I note that chirsbot5000 speaks about filters in the dplyr package. Would it be possible to reference multiple filters and remove all those not mentioned? i.e. tell it to include all rows that have the following countries: Austria, Belgium, Denmark, Finland, France, Germany, Greece, Italy, Netherlands, Norway, Sweden, Switzerland, and the United Kingdom while deleting all rows that do not have these countries listed? &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
To remove the that range of columns, try something like this: your\_dataset &lt;- your\_dataset\[-c(614:714,1050:1124,1408:1151,1636:1729,2112:4282), \] With dplyr, you can filter our rows not meeting those conditions. Let's assume the column name containing the countries is called "country." excluded\_countries &lt;- c("Austria","Belgium","Denmark") dplyr::filter(your\_dataset, !country %in% excluded\_vars) &amp;#x200B;
Just wanted to correct your syntax as it may save you a lot of trouble down the line. You wrote: select=r(1,2,3,...) I think you may be under the impression that c() means "columns" and r() means "rows." I can see how you would think this, but this is not the case. "r" is not a function in base R. c() is actually the vector data type and does not by itself denote columns (but can be used to store column names, of course). I am still rather novice with R (been learning it for about 8 months now), but what helps me most when thinking about manipulating data is learning how vectors and incidences work. If you are well-versed in how to access vectorized data, that's half the battle.
Yeah, looking at your post history and it all makes sense now. Good luck! 
Thank you for this reply - this is what I was asking for when I said 'presumably rows would be similar'. Yes, I thought the c represented columns. &amp;#x200B; So how would one apply this to rows?
The countries are under column one "countryname". Would this filter still work? 
A simple example would be a more in-depth understanding of the ggplot2 package and the Grammar of Graphics. The lessons build on one another starting with the first free lesson. Start with a simple scatter plot, and then learn how to add in layers, colors, and the thought process behind building a complete plot that is self-evident and clean. Other applications involve things like Sentiment analysis, Regression Trees, Clustering, etc. One free lesson is not enough to even scratch the surface, but after completing the paid-for lessons, I feel that I have just scratched the surface and also know a bit better how to dive deeper when an appropriate problem arises. 
Do you prefere english literature? Otherwise there is a [german book](https://www.amazon.de/Angewandte-Zeitreihenanalyse-Lehr-Handb%C3%BCcher-Statistik/dp/3486712144) by Rainer Schlittgen that is meant for people with some knowledge in R.
I speak no German, unfortunately.
library(dplyr) if your data is named 'dta' dta&lt;-as.tibble(dta) gather(dta,Bug,Name,-c(Date,ID))
This book is a nice intro to basic time series. https://otexts.org/fpp2/
Good bot! (Thank you) If one were to delve deeper into the subject (out of curiosity now), what would you recommend? I'm particularly curious because I thought the topics covered in this book were already quite advanced.
It sounds like you're looking for time series material, but [Applied Predictive Modeling](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=sr_1_2?ie=UTF8&amp;qid=1538077827&amp;sr=8-2&amp;keywords=applied+predictive+modeling&amp;dpID=41N0DMuzrHL&amp;preST=_SY291_BO1,204,203,200_QL40_&amp;dpSrc=srch) may be of interest to you. For time series and R specifically, [this text](https://www.amazon.com/Time-Analysis-Its-Applications-Statistics/dp/3319524518/ref=sr_1_1?ie=UTF8&amp;qid=1538078047&amp;sr=8-1&amp;keywords=time+series+r&amp;dpID=41v6C004Q0L&amp;preST=_SY291_BO1,204,203,200_QL40_&amp;dpSrc=srch) seems well-reviewed.
You can subset with logical vectors. index &lt;- c(T , T ,F, ...) my.data[index, ] will keep the rows where index is true. my.data[, index] will keep the columns. You can also subset by names. good.names &lt;- c( "Fred", "Barney", "Wilma") my.data[, good.names] 
Forecasting has a lot of little fiddle bits—it’s notoriously hard to do right. We’ve had a lot of luck with Facebook’s Prophet which simplifies the process but still produces good forecasts https://research.fb.com/prophet-forecasting-at-scale/
I use that too
I use that too
Just pick any R project that you'd be interested in and do it. 
Was going to recommend that too
I'm going to disagree with most people here. Around half of my job is forecasting and I think the best way to learn it is to just jump into it. Specifically, learn examples of Rob Hyndman's packages (forecast and forecastHybrid) which will allow you to run arima(auto.arima is awesome), exponential smoothing models, and seasonal models. Then I would go to Kaggle and look up some good examples of XGBoost, which is incredibly powerful if you are patient and play around with the calibrations. Finally, I would look at prophet and simple Bayesian methods, as these methods, along with neural networks often do a great job at overfitting certain events which does a great job of counteracting underfitting common in other methods. Biggest advice to you though would be to experiment and test as many models as you can though as most of them run fairly quickly and don't actually require that much code. Don't over complicate it
I would check out the forecast package and Hyndman's blog, start here: http://pkg.robjhyndman.com/forecast/ I agree with u/Trappist1, but wanted a tldr version. The auto.arima is insanely powerful and simple. It's like a PhD in a box. 
I'm currently waiting on documentation to be able to get back to work, I was working as a oil &amp; gas consultant. In my workplace a lot of the forecasts were done with finger to the wind with just what "seemed like" reasonable figures. I want to bring a more consistent and robust approach to our forecasts, but I forgot everything I learned in econometrics and I am hoping that now that the reason to learn this stuff is a lot more tangible, I'll pick it all back up again.
That's a fantastic resource! Will definitely look into it. You worked on it?
I plugged in auto.arima in a relatively small data set (33 points with a visually upward trend) and it gave me a less-than-credible (0,1,1) parameter set. I can absolutely believe that the problem is with me or the data I chose and not the function, but then ACF and PACF sort of didn't read the data to have zero autocorrelation. All this to say, I think some times fitting ARIMA requires some leg work.
Nope, just used it.
Based on its performance on M3 data, it’s univariate automatic forecasting is pretty poor relative to ETS, ARIMA and Theta. Can’t speak to its ability to forecast at the daily frequency (which seems to be what it was primarily made for) though, or with expert intervention.
What is the reason for using list wrapped around a vector?
all_nodes is a list of list, and without list with vectors it seems to be searching for 1 first and then 2/3. But since there is no 1 in all_nodes it returns NA. Im all new to R so im just trying to get something to work. 
Seems to work for other ones just not 1, 2. Very weird. I'm not sure why, but I'll try to keep thinking about it.
That is so weird.... Why cant I call get the index out c(1, 2) when I try then?
 you.df &lt;- subset(cuckold.df, select = c(whorewife,tyronebangsher,whenyourplayingolf, etc))
I don't know. It doesnt make sense about 1, 2. I tried specifically using integers to see. Like c(1L, 2L)
My work pays for me to do it, but honestly i'd pay out of my own pocket. I think it is a very valuable tool.
Wow. This is really helpful, thank you a lot. I'll be looking for discounts on Black Friday to keep on my andventure. Once again, thank you.
Thank you!
Phonepost, sorry if code doesn't work. The sprintf function takes as input a format and a vector and outputs a formatted character vector. Read the help pages for a full description/good examples. In your case, check whether sprintf('%.1f', forms$clinicaldx1) works.
It didn't work...I even tried "%3.1f" hoping that maybe it'd keep my first 3 numbers and move the decimal to give me 1 decimal point and nothing happened
Going to need some context to answer that one. RMSE (root mean squared error) is a great estimating tool for a lot of reasons, but there are also a lot of problems it fails very badly at
嗯，这篇文章太冗长了，但是很有意思。 [rushessay](https://essaypinglun.wordpress.com/rushessay-com-%E8%AF%84%E8%AE%BA/) 现在我回到写作服务工作，因为我必须尽快完成我的工作。谢谢。 
First, let me just say that "in general" is going a little bit too far. No one number is the be-all or end-all in statistics! Consider the set of residuals in your model. (That is, the so-called "errors" in the phrase "root mean squared error": the differences between each measured value and the model-predicted value). Typically, our model is designed such that the mean of this set is 0. Then if you write down the formula for the *variance* of this set, you'll see it is just the MSE. And so the standard deviation is the square root of that: rMSE. Now it should be clear that we want the residuals to be clustered tightly around 0. Big residuals mean that our predictions were very far off. As you're probably aware, variance and standard deviation are very common metrics for describing how widely dispersed a set of numbers are. They have mathematical properties that make them good to work with, and those properties are useful in this context as well. For example the variance (or SD if you prefer) appears directly in important formulae like the normal (Gaussian) distribution. Sometimes we might assume the residuals are normally distributed: then we would want to know the variance of that normal distribution, and we can estimate it with the MSE.
Cool, good catch. Any idea about the string argument though?
I don't think that rmse is better than other measures, it's just super convenient and easy to calculate. 
Happy to help :) Have fun coding
15.0 feet ≈ 4.6 metres ^(1 foot ≈ 0.3m) ^(I'm a bot. Downvote to remove.) _____ ^| ^[Info](https://www.reddit.com/user/Bot_Metric/comments/8lt7af/i_am_a_bot/) ^| ^[PM](https://www.reddit.com/message/compose?to=Ttime5) ^| ^[Stats](https://www.reddit.com/message/compose?to=Bot_Metric&amp;subject=stats&amp;message=Send%20this%20message%20to%20get%20some%20stats!) ^| ^[Opt-out](https://www.reddit.com/message/compose?to=Bot_Metric&amp;subject=Don't%20reply%20to%20me&amp;message=If%20you%20send%20this%20message,%20I%20will%20no%20longer%20reply%20to%20your%20comments%20and%20posts.) ^| ^[Patreon](https://www.patreon.com/MetricBot) ^| ^[v.4.4.5](https://www.reddit.com/user/Bot_Metric/comments/8o9vgz/updates/) ^|
There might be an easier way, but you could always use gsub to remove the text and then convert to numeric?
Unfortunately I can't because I'm actually going to do this to an entire column, and other entries includes different sentences with numbers, so doing this would return things I'm also not interested in.
How about this? https://stackoverflow.com/questions/14543627/extracting-numbers-from-vectors-of-strings
The simple way in my opinion would be to use stringr to create a function that extracts the number after the phrase you are looking for, converts the string to a numeric and checks: library(stringr) rawr &lt;- function(str){ ft &lt;- str_replace(str_extract(str, "\\bThe tree is\\s+[0-9]*"),"The tree is ","") return(ifelse(is.na(ft),FALSE,ft&gt;=10)) } rawr("The tree is 10 feet") rawr("There is no tree") rawr("The car is 4 feet")
The simple way in my opinion would be to use stringr to create a function that extracts the number after the phrase you are looking for, converts the string to a numeric and checks: library(stringr) rawr &lt;- function(str){ ft &lt;- str_replace(str_extract(str, "\\bThe tree is\\s+[0-9]*"),"The tree is ","") return(ifelse(is.na(ft),FALSE,as.numeric(ft)&gt;=10)) } rawr("The tree is 10 feet") rawr("The tree is 5 feet") rawr("There is no tree") rawr("The car is 100 feet")
I would add using stringr makes the code a lot more readable. x &lt;- "The tree is 15 feet tall." str_extract(x,'[0-9]+') &gt; 10 From there you can make a function to make this easier. narrative &lt;- "The tree is 15 feet tall and the building is 20 feet tall" check.min.height &lt;- function(narrative,subject,height) { extract &lt;- str_extract(narrative,paste0(subject,'.*?[0-9]+')) as.numeric(gsub("[^0-9]",'',extract)) &gt;= height } check.min.height(narrative,"tree",20) check.min.height(narrative,"tree",10) check.min.height(narrative,"building",25) check.min.height(narrative,"building",20) 
Check out this: https://stats.stackexchange.com/questions/127598/square-things-in-statistics-generalized-rationale
You need to parse this again in your head carefully: ```FumRep5[seq(1, nrow(FumRep5), n), 5]``` You are telling R to take the column means of rows 1, 6, 11, 16... from row 5 of your data frame. R is telling you “rows 1,6,11, column 5 doesn’t have columns!” Because it doesn’t. A single column in a data frame is just a vector. Either use something like ```colMeans(FumRep5[seq(1, nrow(FumRep5), n), c(5,7,9)])``` and actually select multiple columns, or just use ```mean(FumRep5[seq(1, nrow(FumRep5), n), 5])``` like you would with any other vector.
Just to elaborate, `FumRep5[seq(1, nrow(FumRep5), n), 5]` is one dimensional because if you select a single column, R returns a vector, not a Matrix. However, you can add the `drop = False` argument to prevent this (and instead you get a matrix type with 1 column). So the code would be `FumRep5[seq(1, nrow(FumRep5), n), 5, drop = FALSE]`. &amp;#x200B; However, as /u/OsbertParsely indicated, you should really just use 'mean' here. 
Pro tip: check the 'type' argument with match.arg. f &lt;- function(type = c("gaussian", "cauchy")) { type &lt;- match.arg(type) if (type == "gaussian") { ### } } You get both partial matching and error checking. 
I'm reading the documentation now, thanks. Man, R can do a lot of stuff other than the usual math-related functionalities.
Once you run your regression, you can use the predict interval, for all x values or just one x value: x=rnorm(200) y=rnorm(200) fit = lm(y~x) predict(fit, data.frame(x=96), interval="predict") &amp;#x200B;
I think that's the confidence interval and not the prediction interval. 
The preface will explain where this text falls and where to find additional detail. Always read the intro and the table of contents. It's boring but almost always useful. 
Yep, my bad
Here's what I found on stack overflow: https://stackoverflow.com/questions/37806387/r-calculate-standard-deviation-in-cols-in-a-data-frame-despite-of-na-values
It looks like there aren’t spaces between your “$x” and “(sd). It needs to look like $x(sd). Also to call a column by indexing with brackets it needs to be: dataframe$variable(sd[,1]). Indexing works with brackets works by and columns inside brackets separated by a comma. So to call only a column you use [,column].
I'm not quite understanding, do i form it like this [Correlation.Data](https://Correlation.Data)(sd($x))?? i'm very new to python. 
this took the standard deviation of each row. i was the entire deviation of the column &amp;#x200B;
If Correlation.Data is your data frame, it's Correlation.Data$x To get the standard deviation, you would want SD(Correlation.Data$x)
oh man.... that worked..thank you!
Can you transpose your df?
Use the sd() function. There's plenty of documentation elsewhere on this function. Essentially, this will print a single value for that column. By the way, here's where your syntax error is: &gt; sd(Correlation.Data,$y) Correction: sd(Correlation.Data$y). I assume you want the SD of column of *x*, so just substitute *y* for whatever variable it is you intend to analyze.
I would recommend reading *An Introduction to R* published by the R Core Team, the current developers of the R language, and thoroughly reading the documentation provided with the packages you depend on. If you can't figure it out on your own, I always refer to Stack Exchange. These things combined are more than enough in my experience.
I think you are right, I was working of this post r/https://stackoverflow.com/questions/29554796/meaning-of-band-width-in-ggplot-geom-smooth-lm "By default, it is the 95% confidence level interval for predictions from a linear model ("lm")." &amp;#x200B;
I've heard very good things about Hadley Wickham's [R for Data Science](http://r4ds.had.co.nz/). Even if you wouldn't consider yourself a "data scientist" it will certainly give you a good overview of how R works and the basic syntax that would help anyone trying to do data analysis with R.
What's wrong with `gsub`? 
c(rep(0,45),rep (1,15))
Thank you :D
I would probably use dplyr if it were my data set. use dplyr::group_by to group by product, then use the dplyr::summarize function to get the sum of the other rows you are summarizing. 
This screams for dplyr. Best thing since sliced bread. &amp;#x200B; Check the free (!) ebook by the grandmaster Hadley Wickham himself, he explains the tidyverse very well. Hands down, the best place to start your journey in R / datascience! [http://r4ds.had.co.nz/](http://r4ds.had.co.nz/) &amp;#x200B; &amp;#x200B;
https://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html
I am using PostgreSQL to get the data
Would that keep the relationship between the days and the products? Im asking before trying it. As soon as get to work I'll try it, thanks!
Remove print from your pipe, add %&gt;% filter(age == max(age)) %&gt;% pull() Note that arrange is useless in this pipe. Also, you can do the same for the youngest using min instead of max. 
 df %&gt;% group_by(Product = if_else(Product == “A”, “A”, “Other”) %&gt;% summarise(Total = sum(count, na.rm = TRUE))
[https://stat.ethz.ch/R-manual/R-devel/library/base/html/row.names.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/row.names.html)
If you're just wanting to change your row names, you can use the rownames function: `rownames&lt;-`(mytable, c('Y1','Y2')) However, the names of columns and rows shouldn't have any effect on what you're trying to do, which is find the covariance. For covariance, you don't have to specify X and Y. Just do cov(mytable) 
Your table already has the joint probability? For which numbers are you attempting to compute the co-variance? It would seem as though you have a Y\_1 and a Y\_2 variable. The transpose function within R may help you reformat your table if that is what you are after: Transpose the table into a single column, concatenating the two rows: &gt; rbind(t(mytable[1,]), t(mytable[2,])) Out: 1 X1 0.226 X2 0.125 X3 0.084 X4 0.121 X5 0.077 X1 0.043 X2 0.095 X3 0.066 X4 0.088 X5 0.075 Transpose the table into two columns, concatenating across the columns: &gt; cbind(t(mytable[1,]), t(mytable[2,])) Out: 1 2 X1 0.226 0.043 X2 0.125 0.095 X3 0.084 0.066 X4 0.121 0.088 X5 0.077 0.075
Never seen pull before, could you just do pull(citizenship)?
Shouldn't need filters, I've had good luck with functions that rely on names() and which.max(). So in your case: A[which.max(A[['age']]),'citizenship'] should return exactly what you're looking for!
Or just %&gt;% top_n(1, age) %&gt;% pull(citizenship) 
Yes that's probably more concise than what I suggested. There are a lot of ways to find the same result.
Ok as for the type of analysis, she says: "I don't really know yet, since I don't have the specifics of the project. I know I'll be building mathematical models most likely with several covariates to project population vital rates (survival, probability of occupancy, fecundity) in response to environmental/climate or land use conditions. I'll probably be running Bayesian analysis and maybe some Tukeys or ANOVA tests? But I haven't learned a lot about the actual statistical analysis behind the project." As far as her field of research: "Population dynamics of amphibians." She'll be a great herpetologist one day.
&gt; Once you run your regression, you can use the predict interval, for all x values or just one x value: &gt; &gt; x=rnorm(200) &gt; y=rnorm(200) &gt; fit = lm(y~x) &gt; predict(fit, data.frame(x=96), interval="predict") Thank you. What is ''200'' ? and data.frame (x=96)? 
&gt; ggplot2 does this by default for linear regressions. It's the 'se' bit that does it. I typed it out here, but it defaults to TRUE. &gt; &gt; ggplot(your.data.frame, aes(your.data.frame$x,your.data.frame$y)) + &gt; geom_smooth(method='lm', &gt; formula =y~x, &gt; se =TRUE) Thank you... I am still at the beginning, can I ask you what is the difference between prediction intervals and a standard deviation? Basically are the prediction intervals the standard deviation of the errors? 
&gt; oldest &lt;- citizenship[which.max(age)]
Don’t worry about the 200, I was just making up number to use in my example. Once you have a fitted model, use the predict function. Here’s a good explanation: http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/
If the size of your set is not too large, you can just get the unique elements in the list using `unique()` in base: library(combinat) a_c &lt;- c(1,2,2) unique(permn(a_c)) #&gt; [[1]] #&gt; [1] 1 2 2 #&gt; #&gt; [[2]] #&gt; [1] 2 1 2 #&gt; #&gt; [[3]] #&gt; [1] 2 2 1
Thank you!
Thank you!
Kaggle challanges are fun. 
Wow, that's really a Pandora's box. Hearing it for the first time
What do A, B and C look like?
A is about 10000 obs of 12 variables, B is 28000 of 7 variables and C is 75000 of 17 variables. They're big data sets :( 
Would an inner join work?
and, presumably, there is a variable "customer" in all three dataframes?
Are you just looking for a list of the common data? You can try combining them all into a vector then searching for which one is duplicated Ex. Temp &lt;- c(Dataframe1$names, dataframe2$names, dataframe3$names) Temp &lt;- temp [duplicated(temp)] You'll need to use the unique function as you combine them into a vector if there's multiple instances of the same name in the same list. 
Yeah so what those of us who are employed a common path before data science was a buzzword, was to get an analyst position and price you can take technology to leverage actionable Intel. Kaggle is cool and all but it's not work experience. Sometimes you have to start somewhere and put in the work and the hours
There's many ways in R. The simplest way I can think of is to use base R's `intersect` function. Here's an example with vectors. You should be able to adapt it to data frames by realizing that a single column of a data frame is a vector. a &lt;- 1:6 b &lt;- 4:10 c &lt;- 5:10 a b c rslt &lt;- intersect(a, intersect(b, c)) rslt The answer is `5` and `6`.
I would imagine scoring well on a few competitions would be noteworthy on a résumé. Heck, I do poorly (like top 75%) and there are some impressive people ranked below. 
No expert, but this seems like the simplest way of doing it! To add more to your point about multiple ways to do this: I've actually found myself with a similar-ish problem very recently, but my solution was a tad more convoluted using the function "%in%": a &lt;- 1:6 b &lt;- 4:10 c &lt;- 5:10 shared_a_b &lt;- a[a %in% b] shared_a_b_c &lt;- c[c %in% shared_a_b] shared_a_b_c Which also returns `5` and `6`. Also if it helps OP, "a$customer" should give you the customer column as a vector for dataset "a".
Could you transpose the data.frame? 
I have the data frame transposed as well, sorry for the stupid question but what is the benefit to transposing it? It also changes the array from a data frame to an atomic vector.
Sorry, wrong terminology from me. I should have suggested the 'melt' function within the reshape2 package. You should be able to create a new date variable and then filter. 
No worries, thanks for the help! I will try using the melt function on the data. If I understand what it does, it creates a new column of dates that I can then filter and melts the other data to the corresponding date?
Check this out: https://www.statmethods.net/management/reshape.html
Check this out: https://www.statmethods.net/management/reshape.html
Not sure if there is a right answer. Numerics should almost always (if not always) be numerics (double, float, int, etc), but characters you might want to have as characters OR factors, depending on what you are doing. SQL also uses TINYINT(1) for booleens pretty frequently. You can test what gets mapped. &gt; db_sql=dbConnect(MySQL(),user="root", + host="127.0.0.1", + dbname="test", + password="", + port=3306) &gt; q &lt;- "SELECT * FROM MAIN" &gt; reports &lt;- dbGetQuery(db_sql, q) Warning messages: 1: In .local(conn, statement, ...) : Unsigned INTEGER in col 0 imported as numeric 2: In .local(conn, statement, ...) : unrecognized MySQL field type 7 in column 5 imported as character &gt; &gt; for(column in names(reports)) { + print(paste0(column,": ",typeof(reports[[column]]))) + } [1] "id: double" [1] "char_test: character" [1] "float_test: double" [1] "bool_test: integer" [1] "double_test: double" [1] "timestamp_test: character" [1] "datetime_test: character" [1] "date_test: character" &gt; q &lt;- "SELECT DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'main'" &gt; schema &lt;- dbGetQuery(db_sql, q) &gt; print(data.frame(SQL = schema$DATA_TYPE, R = as.vector(sapply(reports,typeof)))) SQL R 1 int double 2 char character 3 float double 4 tinyint integer 5 double double 6 timestamp character 7 datetime character 8 date character &gt; dbDisconnect(db_sql) [1] TRUE
Thanks for including your last line, which might help OP see the relevance of adapting my code to work with a data frame. I probably take this kind of knowledge for granted, and I remember struggling with these concepts and feeling like people replying to my post were not simplifying the answers enough. Regarding the way you solved the problem, it does look a little more complex than my approach, but it's actually more flexible, too. In your approach, you're returning a vector of logicals (ie TRUE/FALSE) where values in vector `a` that matched values in vector `b` are marked `TRUE`, and unmatched positions are `FALSE`. That's a nice approach when you need to keep a vector of values that is of a specific length. The help page for this function and the closely related `match` function can be viewed with `?'%in%'`. Of course, like most R help pages, it's minimally useful. A related function that I also use from time-time is `which()`. With any programming language, I construct simple little code examples to help me understand very fundamental behavior. Listed below are some of my code samples for understanding the various ways of matching vector elements in R. They are trivial, but you might find them helpful. LETTERS # [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S" "T" "U" "V" "W" # [24] "X" "Y" "Z" --- ## Example 1A LETTERS == "A" # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # [16] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE --- ## Example 1B LETTERS == c("A", "R") # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # [16] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE --- ## Example 1C LETTERS == c("A", "R", "S") # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # [16] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # Warning message: # In LETTERS == c("A", "R", "S") : # longer object length is not a multiple of shorter object length --- ## Example 2A LETTERS %in% "A" # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # [16] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE --- ## Example 2B LETTERS %in% c("A", "R") # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # [16] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE --- ## Example 2C LETTERS %in% c("A", "R", "S") # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE # [16] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE --- ## Example 3 which(LETTERS %in% c("A", "R", "S")) # [1] 1 18 19 --- ## Example 4A match(LETTERS, c("A", "R", "S")) # [1] 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 2 3 NA NA NA NA NA NA NA --- ## Example 4B match(c("A", "R", "S"), LETTERS) # [1] 1 18 19 --- ## Example 4C is.na(match(LETTERS, c("A", "R", "S"))) # [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # [16] TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE --- ## Example 5A intersect(LETTERS, c("A", "R", "S")) # [1] "A" "R" "S" --- ## Example 5B intersect(c("A", "R", "S"), LETTERS) # [1] "A" "R" "S" --- --- ## Example 6A t1 &lt;- rep(letters[1:4], 5) t1 # [1] "a" "b" "c" "d" "a" "b" "c" "d" "a" "b" "c" "d" "a" "b" "c" "d" "a" "b" "c" "d" match("a", t1) # [1] 1 --- ## Example 6B match(t1, "a") # [1] 1 NA NA NA 1 NA NA NA 1 NA NA NA 1 NA NA NA 1 NA NA NA # Notes 1. Examples 1A, 1B, and 1C give identical results to Examples 2A, 2B, and 2C, respectively, but with one key difference. Example 1C throws a warning, whereas 2C does not. The code in Examples 1A-C is a bit more computationally efficient, but because of vector recycling, it is not as robust as the code in Examples 2A-C. 1. Examples 3 and 4B give identical results. 1. Examples 2C and 4C are logical negations of each other. 1. When using the `%in%` function, the output is always a logical vector whose length is equal to the first argument. 1. The `which` function requires a logical vector as input. 1. Be careful with the `match()` (and the related `%in%`) function, because it only returns the position of the first matches of the first argument in the second argument. 1. See Example 6A, where "a" is present 5 times in vector `t1`, but only the first match is returned. 1. In Example 6B, all of the `"a"` values in `t1` were located because the function tests each value of the first argument against the second argument. 1. All of the values in the first argument get tested, but once a match is found in the second argument, no further testing for that value occurs. Rather, testing for the next element in the first argument begins. This behavior has bitten me a few times in the past. 
Have a hobby? See if you can make something for it, or if you can find related data. If you do make something for it you can use that something to collect related data if you can convince other people to use it. You can grab one of those cheap digitalocean machines to serve shiny apps 
Might be bad practise, but assuming your variables are already ordered: For the year 1960: from &lt;- which(names(dat) == "X1960.07.01") to &lt;- which(names(dat) == "X1960.09.30") dat.sub &lt;- dat[,from:to] Then for all other years: for(i in 1961:2005){ from &lt;- which(names(dat) == paste("X", i, ".07.01", sep="")) to &lt;- which(names(dat) == paste("X", i, ".09.30", sep="")) dat.sub &lt;- rbind(dat.sub, dat[,from:to]) } 
I would strongly suggest using a long format for this rather than a wide format. It's probably easiest to address this at the point where the data is produced rather than in R, but it can be done there as well. By long, I mean that instead of having something shaped like job X1960.10.04 X1960.10.05 job1 22 37 job2 42 11 Then instead try to produce data that looks like this: job date observation job1 1960-10-04 22 job1 1960-10-05 37 job2 1960-10-04 42 job2 1960-10-05 11 Once in this format the data you want can be easily extracted. Something like subset(longform, strftime(longform$date, "%m") %in% c("7", "8", "9", "10")) would do it. Inside R, you could do this transformation as well, but it would require some string processing to get the column names into an acceptable format for a Date or POSIXct. If this is the only option, I would recommend the *tidyr* package, and the *dplyr* package for the actual manipulation: require(tidyr) foo &lt;- data.frame( job=c('job1', 'job2'), X1960.10.04=c(22, 42), X1960.10.05=c(37, 11)) foo %&gt;% gather(date, observation, -job) job date observation 1 job1 1960-10-04 22 2 job2 1960-10-04 42 3 job1 1960-10-05 37 4 job2 1960-10-05 11 The %&gt;% notation and all that can be challenging, but well worth learning.
The tidyverse approach would be to group_by followed by summarize
why does every always ignore us third party voters =(
Suppose your data is called `df`, the variable is `party`, and the values are "Republican" and "Democrat". In base R (without additional packages like the tidyverse) you can subset the data with brackets: republicans &lt;- df[df$party == "Republican"] # data with only Republicans democrats &lt;- df[df$party == "Democrat"] # data with only Democrats
Or List &lt;- split(df, df$party) List$Replubican List$Democrat
Fantastic, thank you!
Look into Packrat which integrates easily into RStudio and sets up a private package library for your different projects similar to how you would set up a virtual environment in Python: http://rstudio.github.io/packrat/
Have you looked at `expand.grid`? Might be easy too much for that function, but I'm not sure. 
 require(devtools) install_version("ggplot2", version = "0.9.1", repos = "http://cran.us.r-project.org") https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages You can even go farther by creating a [docker container](https://o2r.info/2017/05/30/containerit-package/).
This is correct. Also beware that depending on your SQL admin, they may have written your entire database as strings, in which case you'll need to cast them to their proper variable types. For example, our BigQuery database where I work, the data is all stored as strings/characters.
&gt;You should use double equal signsif (type == "gaussian"){ code }the single equals sign is used to assign values, whereas double denotes logical expression. I knew about conditional probability, but how can a string be considered as a value that gives ''probability''? Sorry for da question &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Here's the gist of it: create a vector consonants, that contains all the consonants. Also create a vector vowels, that contains all the vowels. Then you can use the sample command to sample two letters from consonants, and another sample command to get two from vowels. Store this in vectors, for example c_letters and v_letters. If you are allowed to use the same letter more than once, be sure to set replace=TRUE in the sample function. This allows the function to grab letters with replacement. Lastly, you can paste the letters together to make your word using the paste0 command - you can get the first sampled consonant by getting c_letters[1], the first vowel by using v_letters[1], and then the second ones with c_letters[2] and v_letters[2]. Stick all that into paste0 and boom. If you want the full code, let me know. 
 consonants &lt;- c("b","c","d",...) vowels &lt;- c("a","e","i","o","u") numwords &lt;- 10000000 sample1 &lt;- runif(numwords, 1, 21) sample2 &lt;- runif(numwords, 1, 5) sample3 &lt;- runif(numwords, 1, 21) sample4 &lt;- runif(numwords, 1, 5) seqindex &lt;- seq(1, numwords, by=1) words &lt;- sapply(seqindex, function(x) paste0( consonants[sample1[x]], vowels[sample2[x]], consonants[sample3[x]], vowels[sample4[x]])) words &lt;- unique(words) Adjust `numwords` higher if you aren't getting enough unique results. 
wow thank you so much. would you be able to provide me with the full code? the consonants and vowels are from another language (not english) so i'd need to import a data file with the list of consonants and vowels (I use the international phonetic alphabet (1-to-1 correspondence to a symbol and a sound).
Here's the code for English. Just change the content of consonants and vowels. https://imgur.com/qrdlo0Y
There’s Microsoft Open R which is designed around making R fast and reproducible. It is a version of R that defaults to optimised BLAS libraries, has static snapshot repositories, a package called checkpoint (like packrat but doesn’t store a separate version of the whole package in a new location, only the version info). It’s basically designed for exactly what you want. But if you don’t want to be in the MS ecosystem then you can still use the checkpoint package with normal R. 
Writing is an art and everyone has not this artistic skill but many [wizessays](https://www.wizessay.com/) staff members have started a campaign for writers like you.Now you can promote your work. 
 When it doubt, brute force it. data$region &lt;- "NULL" data$region[data$north==1] &lt;- "North" data$region[data$south==1] &lt;- "South" data$region[data$west==1] &lt;- "West" data$region[data$east==1] &lt;- "East"
If your data are in a data frame with four columns for north, south, east and west, you could do something like the following: # toy data north &lt;- c(1,0,0,0) south &lt;- c(0,1,0,0) east &lt;- c(0,0,1,0) west &lt;- c(0,0,0,1) dat &lt;- data.frame(north, south, east, west) # vector of region names region &lt;- c("north", "south", "east", "west") # extract region name corresponding to column number that has a 1 dat$region &lt;- apply(dat,1,function(x)region[which(x==1)]) &amp;#x200B;
Nice clean solution!
You could also use “unite” if you have the tidy package installed. 
Something like this? # Some test data test = c( "The box is 3 miles away and is 4 feet tall.", "The box is 3 miles away and is 14 feet tall.", "The box is 24 feet tall and 3 miles away.", "No idea where the box is." ) # Do some regex results = gsub('.* ([[:digit:]]+) feet .*', '\\1', test) # Tidy up to remove missed extractions, because I'm bad at regexes ifelse(results == test, NA, results) 
Sure. So at a basic level we're using a 'regular expression', or regex. It's a pattern matching syntax that's used all over the place in computer science for exactly this purpose, but the syntax is pretty durn arcane. ?regex gives you the R implementation of the syntax; ?grep gives you the various regex tools to use with that syntax. We use gsub (or sub I believe works here as well) because we want to extract part of the text. The particular pattern we're interested in is (based on your original question) a space, followed by a sequence of numbers, followed by a space, followed by the word 'feet'. In regex speak this becomes the pattern " [[:digit:]]+ feet "; [[:digit:]] represents a single digit; the following + means that the preceding pattern occurs 1 or more times (so we can catch more than single digit characters); the spaces represent spaces; and the word feet literally matches the word feet. So we have the pattern we want; the next step is pulling out the specific part of the pattern we want. The help for regex tells us that to pull out a specific part of a pattern we can use brackets, and then reference them in the second part of gsub. So now we have the pattern " ([[:digit:]]+) feet" - we're matching the pattern we've already described, but the part we're interested in is only the "[[:digit:]]+" pattern, captured within the brackets. To complete this, we need to capture the rest of the phrase. This is where the ".*" bits come in. The character "." represents any character, and the "*" means 0 or more times. So now we have our full pattern: Any characters, zero or more times space, followed by 1 or more digits, followed by space, followed by the word 'feet', followed by a space (and we capture only the digits part of this expression) Any characters, zero or more times Now hopefully it should be obvious there are some things that can get missed here - for example, the phrase "1.2 feet" wouldn't be captured, because it doesn't match the pattern "space-1 or more digits-space-feet", because of the decimal point. In that case you'd need to expand the pattern using the ? character. Similarly, the word 'Feet' wouldn't match. Honestly though, those are probably best left as exercises for the reader, because I've always found the best way to get used to writing regexes is to write regexes - they take some practice :) Hope that helps! 
Once I have this "region" variable, how would I use it in operations? (hist, mean, etc)
Thank you! I really appreciate it!
Lookaheads and look behinds are useful to know about. Check the strings cheatsheet from RStudio 
I highly recommend the rebus package. It’s made regular expressions so easy for me. You won’t learn regular expressions directly but it allows you to create them with very plain language. An example: START %R% one_or_more(DGT) %R% SPC %R% or(“Main”, “Maple”) %R% END So, starts with one or more digits, followed by a space, then the words Main or Maple to end the string. Kind of a silly example to showcase some of the more commonly used features. If you type the above into R, it will output the regular expression, so I am learning more about some more complicated patterns that way.
This would be a good place to learn about the switch function 
If you're having the tidyr package, try using the gather() function. data %&gt;% gather(key=region,val,-id) %&gt;% filter(val==1) Here, the columns that need not have to be gathered could be preceded with '-' as in '-id'. For more info: https://stackoverflow.com/questions/29227111/convert-multiple-binary-columns-to-single-categorical-column 
Unfortunately `switch` is less than useful in this circumstance. `dplyr::case_when` would be marginally better but using binary encoding as shown by /u/goctlr is cleaner still.
Sadly, that's not it. One-line definitions/conditions don't need braces, and even if I added the braces, the functions still don't work. I believe the answer can be found [here](http://adv-r.had.co.nz/Computing-on-the-language.html), perhaps in the "Calling from another function" section, but it sounds like the author's proposal is to just create a copy of fun1 that uses a quoted string rather than a column. That's not an elegant solution, but I suppose it beats nothing.
The definition of `fun1()` says to evaluate `cname` to be whatever the literal R calling code was. For example, `fun1()` evaluates `cname` to be `cname2`, `substitute(cname2))`, and `deparse(substitute(cname2))` in each of your versions of `fun2()`. I think that no matter what you do *statically* in your definition of `fun2()`, you cannot change this baked-in behavior of `fun1()`. If you are determined to not copy/modify `fun1()`, then you will need to define `fun2()` *dynamically*: fun2 &lt;- function(df, cname2) { eval(parse(text = paste0("fun1(", deparse(substitute(df)), ", ", deparse(substitute(cname2)), ")"))) }
What is your thought process for your if statements? Generally I would want to input manual coefficients to get the best prediction in my validation set. I am making a predictive analytics model for insurance so we have lots of variables we need to predict but after we get our initial coefficients we typically adjust them due to business needs/ in order to get the best prediction to our test set. &amp;#x200B; How would you use ifelse statements to do something like this?
what happens if you get rid of the row.names argument. 
So that fixes the first error. Second: https://stackoverflow.com/questions/23209464/get-embedded-nuls-found-in-input-when-reading-a-csv-using-read-csv
Perfect. I put ,fileEncoding="UTF-16LE" and i don´t have more errors. Thanks a lot
Hi Thank you so much for the suggestion, and sorry for the late reply. I have been trying to implement your idea, and have some success with it :) I am able to pull out the data for a specific year like 1960, but when I nest that within a for loop I get an unexpected result. I also used c instead of paste, was there a reason you prefer paste to c? X1961.07.01 X1961.07.02 X1961.07.03 6967 7.067000e+03 7.091000e+03 6967 7.067000e+03 7.091000e+03 6967 7.067000e+03 7.091000e+03 I think, I have an idea of what's going on, but am not too sure. The first column in the dat array (data frame) is being repeated over in the loop rather than going to the next column. X 6967 7067 7091 7098 7118 
&gt; https://www.statmethods.net/management/reshape.html Thanks for the link, I have been playing with the reshape and reshape2 packages. I am trying to figure out the best way to convert the dates to a column instead of them being the columns. 
Hi Thank you for the suggestion, a couple other people have commented on this transformation as well. I have several packages that will help convert the data from wide to long, so that it's easier to subset and manipulate.
Incredible! Your dynamic fun2 does the trick perfectly! You have saved my day. 
I don't have time to help right at this moment, but if you've got a general question I/we can help address and post it here, I can get to it later today
Nah, post questions here, get answers here, leave your questions up so people can use the search function and learn from you. 
Everyone starts somewhere. There are very basic questions all across stackexchange as well. 
I have been able to reformat the data so that now it looks closer to the format that you recommended. Gauge X.1 Date value 6967 9386900 X1960.10.03 0.000098500 7067 9480000 X1960.10.03 0.064181250 7091 9492400 X1960.10.03 0.015694835 NowI'm working on subsetting the data based on two variables. My apologies, but my first post was not quite complete. First I am going to work on identifying the the three largest values at each gauge per year. Then I am trying to identify which events occurred between July 1st and Sept 30th of each year. Would you recommend still using extracting the data with the subset function, and also sorry but what is longform is it just a variable or a function as well? 
Don't worry about that, we all start as noobs.
* alright this is the start. Let's say I got a RData File. It's bassicly a big excel sheet, first row are names it looks somewhat like this: Country Income Age Germany 10 5 England 4 7 &amp;#x200B; &amp;#x200B; I load it in with getwd() setwd("path") data &lt;- load(file = "path") &amp;#x200B; Is this correct so far? How do I actually view my "Data" now? I got a file named Data in my enviroment, did I do it like this? 
First, you should download rStudio and install that. Then, run everything in that. It'll be loads easier for you. (Sounds like maybe you have this set up, but want to confirm). You can type data, and see what happens. You can type head(data) and see what happens. Or, View(data). Or, you can even click the file Data in your environment.
I got R-Studio, well if I write "data" it says &amp;#x200B; \&gt; data \[1\] "d" &amp;#x200B; \&gt; head(data) \[1\] "d" &amp;#x200B; \&gt; View(data) basicly a table with 1 and d &amp;#x200B; so thats wrong already? &amp;#x200B;
Hey, xDreal, just a quick heads-up: **basicly** is actually spelled **basically**. You can remember it by **ends with -ally**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Yeah, that would mean you didn't load it correctly. :) So, let's work backwards! You know you didn't load it correctly. Let's diagnose the rest of your code. setwd("~/Desktop") for instance. Now, if I want to load rData, (who uses rData, try to keep things in .csv so it's easier to share) - load("data.RData") Do you see the difference there? I'm not loading the path -- that wouldn't really make sense. I can't load my Desktop.
Yeah I'm sorry but I actually did that right setwd("path") (Used the right path I think replaced the \\ with / after R-Studio complained) Than I used dat1 &lt;- load(file = "filename.RData")
It's fine, just be clear about what you're doing. We can't see what you're doing. You said above : &gt;data &lt;- load(file = "path") That would not be correct. Furthermore, it's getting even more confusing, because now you're introducing dat1 when originally you said it was data. Are you trying the above with dat1? 
getwd() is giving me the right folder! &amp;#x200B; \&gt; getwd() \[1\] "Right folder!" myDat &lt;- load("filename.Rdata") head(myDat) \[1\] "d" &amp;#x200B; &amp;#x200B; &amp;#x200B; myDat &lt;-Load("filename.Rdata") is not giving me the right data head(myDat) is giving me 
But I just found out head(d) is giving me the right output! What I dont understand I never created a file called d, I did even clear the enviroment before starting again.
just to verify: you know for sure that the file with your data is located in the exact folder that getwd() filepath points to? you are using the exact filename of that folder, extension included, inside the load() function?
Yes I'm sure! 
If I'm not mistaken then, I think that when you load an Rdata file into your environment, that it is assigned a variable name that is determined by the Rdata file. It would seem that "d" may be what is automatically associated with that file. if you run: head(d) that gives you the first few rows of your data?
Yes it does, everything works if I use "d" as my filename.
Alright seems like the File was the problem :D Thank you very much! And sorry for wasting your time.
Read “R for Data Science”
May I ask you one more thing? Let's say I have like 20 Countrys in my table and each country occures multipli times, and I want the average income for each country, Country Income Germany 5 Germany 10 England 5 Spain 5 ... How can I do this? &amp;#x200B;
Go away.
Use this http://swirlstats.com/
[R for Data Science](www.r4ds.had.co.nz)
Are you using different coefficients based on a variable in another column? If so you can do something like: Library (tidyverse) Df %&gt;% Mutate(var = ifelse(categoricalVar == "example1", var*log(1.02), Ifelse(repeat until all of your conditions are logically met)
Not the best environment for typing code but hopefully you get the idea
The basic idea of a nested ifelse Loop is to have a condition an outcome and then I knew ifelse until you run out of logical possibilities. 
AH I see. So you're making new columns that contain manually entered variable coefficient times the variable? If the variable is categorical I assume you would have to code it to be multiplied by a 1? &amp;#x200B;
I’m not trying to test anything logically though. I am creating a tweedie GLM for prediction and I just want to edit coefficients to plot my predicted results to compare with my actual results. I’m in insurance so my goal is to predict losses using various customer attributes (some are continuous some are categorical with multiple binnings). Some examples are: customer credit data, prior year claims, geographical data based on location of customer. I use a GLM to predict losses and then plot the results to compare to the actual losses that occurred (this is all past year data over the span of 10 years or so). I sometimes need to manually edit coefficients to make my predictions get closer to what the actual results are so I can validate the algorithm ( the idea is that if my GLM algorithm can predict actual past experience then we can use it to predict future losses as well)
Ok. I was thinking that the coefficients would change. But if they are constant then you could store them outside the data set. 
Not a waste of time at all, glad things are working
I would personally utilize a library called "dplyr" for a task like that. I would highly recommend getting familiar with its use if you continue learning R. After running install.packages("dplyr") in your console, I would run something like the following code: library(dplyr) d %&gt;% group_by(Country) %&gt;% summarize(Average_Income = mean(Income))
Alternatively, you could utilize the aggregate() function: aggregate(Income ~ Country, data = d, FUN = mean)
Perfect. Do you know how the predict function works? I was storing new values in coefficient levels manually using : GLM$coefficients[“variablenamelevel”] &lt;- manual value here After doing so I would then re predict . Predloss &lt;- predict(GLM, data= data1, type=“response”) However I noticed after changing coefficients the predict function kept giving me the same values for all 1k+ of my observations. I hope this makes sense. I’m having a hell of a time trying to get the predict function to work consistently. 
You should read up on the GLM function. Don't quote me on this I'm pretty sure it will always recalculate the coefficients whenever you run the model. That is more or less what the model is. If you want to run it with your own coefficients then I think you will have to manually calculate... I haven't done a lot of work with modeling since grade school so I could be wrong. If I were you I would go poke around on stack overflow. Sorry I couldn't be more help
It shouldn't be that hard as a general linear model is just straight multiplication
Right I understand how it looks manually but when I have a function with potentially 81 variables at the moment, each with possibly 2-3 levels it becomes cumbersome to manually code out the entire equation.. I just want an easy way to manually edit a level or two here or there without using excel.. Thanks for all your help regardless !! :)
Even if it does exist, I don't imagine R is terribly well suited to this. You'd probably want a tool that could take better advantage of a GPU. What are you trying to index in the images? Color averages? Some kind of text? 
Just use load without assignment. It will put the 'd' object in your environment and you can work with it there. 'Load' and 'data' are the two big exceptions to assignment rules 
R can do all of that, of course. Check out [Keras for R](https://tensorflow.rstudio.com/keras/).
I don't excacly understand what You want to achive, but mayby this would be better: library(tidyverse) library(googledrive) # download file drive_get(as_id("1GRx69h41zh4_6V2n-qtwstiy3H4b588f")) %&gt;% drive_download(path = "file_from_web.RData") load("~/RProjects/GoogleDrive/file_from_web.RData") my_dates %&gt;% ggplot() + geom_point(aes(Patient_Paa_Opstue, Operations_Stue), color = "green") + # start geom_point(aes(Patient_Forlader_Stuen, Operations_Stue), color = "red") + # end geom_segment(aes(x = Patient_Paa_Opstue, xend = Patient_Forlader_Stuen, y = Operations_Stue, yend = Operations_Stue)) # durration 
if()
You've forgotten a whole bunch of parentheses "()" around return and if statements. Also you can format your code n reddit posts to be more readable. The following seems to do what you want it to do. Don't just copy it but try to see where it's different. mean = function(x){ sum = 0 count = 0 if(length(x) &lt; 5){ return(FALSE) }else{ for(i in 1:length(x)){ if(i %% 5 == 0){ sum = sum + x[i] count = count + 1 } } } return(sum / count) }
geom\_raster() would work for this a well I think
I can't tell for sure but my guess is that RTools will work with Microsoft's R distribution. Also on the MRAN site has info on using R tools inside visual studio. I've never used it myself as I use the base R distribution but it's worth giving it a try if you have VS installed.
Add another geom_segment() with other column names 
Thanks for the suggestion! &amp;#x200B; I was hoping that I wouldn't have to bind two datasets together and simply just "add" the calculated columns onto the existing dataset, using the column names of the given formula.
There are essentially three ways of doing this. The first way uses `eval`; a variant of this was already shown but **I strongly advise against this implementation** because it needlessly relies on `parse`. R allows you to manipulate unevaluated expressions, so do that instead. It’s more efficient and requires less indirection: fun2 = function (df, cname2) { cname = substitute(cname2) eval(bquote(fun1(df, .(cname)))) } `bquote` creates a quoted expression. Inside it, everything that’s surrounded by `.(…)` is substituted with its value. The second way indirectly invokes a function via `do.call`: fun3 = function (df, cname3) { do.call(fun2, list(df, substitute(cname3))) } In this particular case it results in shorter code but it’s less flexible. I used to use this a lot but I can’t remember using it recently at all. The third way is the “modern” way, using [tidy eval and *quosures*](https://rlang.r-lib.org/index.html). This is basically modern, clean implementation of non-standard evaluation in R that permits systematic construction of calls. It’s a new framework to learn but, oh boy, is it worth it. The code looks superficially similar to the first way (using `eval)`: fun4 = function (df, cname4) { eval_tidy(quo(fun1(df, !! ensym(cname4)))) } Here `eval_tidy`, `quo` and `ensym` are from {tidyr}. In this order they correspond roughly to `eval`, `bquote` and `substitute`. But they are much more powerful than the latter. To understand how, I invite you to read the vignette linked above, [*Programming with dplyr*](https://dplyr.tidyverse.org/articles/programming.html).
I strongly recommend against manipulating strings of expressions when you can easily manipulate unevaluated expressions directly.
Braces around function bodies are not necessary (though recommended for readability) in R. In R, braces perform purely one single purpose: they group multiple expressions into one. That’s it. In fact, `{a ; b}` in R is *exactly* equivalent to `` `{`(a, b)``. That is, it’s a function call to a function named `` `{` ``.
Some comments: * it's a really bad idea to name your function 'mean', as it will overwrite the built-in function * when possible, try to take advantage of R's vectorization -- often, it will be faster than `for` loops * it's possible, but not a great idea, to have your function sometimes return a Boolean (FALSE) and sometimes return a numeric. Perhaps using NA would be better than FALSE. * learn about indexing into vectors with Boolean masks So all that being said, I think the following does what you want, in a perhaps somewhat cleaner way: mean_every_fifth &lt;- function (x) { len = length(x) if (len &lt; 5) { return(NA) } else { mask = rep(c(F, F, F, F, T), length.out = len) return(mean(x[mask])) } }
As some others have pointed out, use if() to test length criteria. Conceptually, I would try to avoid iteration and more advantage of vectorization and built-in functions. One solution might be to create an index of 1:lenght(x) and chunk it up into groups of 5. index&lt;- 1:length(x) %% 5 From here, there are a lot of built in functions to help. You could use mean() and subset your vector by index: mean(x[index==0) or to get all groups: aggregate(a, by=list(index), mean) Or if you put the index and your vector x into dataframe df: aggregate(x ~ index, df, mean) and then select which group you want to return. 
Just want to add, Keras is great, I've used it for a couple things and I love it. 
Respectfully, trying to avoid tidyverse, but thank you for providing this diverse array of solutions. I think your second, with do.call, was a real forehead-slapper of a revelation, and I have now implemented that over the far messier eval-parse setup previous recommended. : )
Try the following code: m1 &lt;- lm(mortality_rate ~ ownership_rate, data = gun_violence_us) plot(mortality_rate ~ ownership_rate, data = gun_violence_us) abline(m1)
&gt; m1 &lt;- lm(mortality_rate ~ ownership_rate, data = gun_violence_us) &gt; &gt; plot(mortality_rate ~ ownership_rate, data = gun_violence_us) &gt; &gt; abline(m1) Still getting the same error code. I had already run the m1 &lt;- lm(mortality_rate ~ ownership_rate, data = gun_violence_us) bit earlier in the markdown. Is there a chance that there is something wrong with my R settings or something? Even my friends who are stat majors kind figure this one out.
`apply(matrix, 2, function(x) any(x &gt; 0))` will test whether any element of a column is positive. It uses any, but is a lot more straightforward than the above. If you want to retrieve the column names, just subset with it: colnames(test_mat)[apply(test_mat, 2, function(x) any(x &gt; 0))]
Did you pull the data from a particular dataset I could get a link to? I’m curious if I can replicate the problem.
Just to check, bc it happens to me more frequently than I'd like to admit: did you run all previous code chunks in your markdown file before running this particular code?
Sweet. And yeah, the green play button is your friend when running code chunks. Though if you want to use ctrl + enter still, you can highlight a section of code first before using the command.
Is there a way to apply that to a single column at a time? My intention is to find the first column that has a negative value in row 1 and at least one entry below it is positive then break the loop. 
If you need to loop, you can work like so, repeating infinitely until you run out of columns: i &lt;- as.integer(1) repeat{ if(any(test_mat[,i] &gt; 0)){ print(i) break } else { i &lt;- i+1 } } This is mostly equivalent to this vectorized operation, which just returns the index of the first column with a positive value: min(which(apply(test_mat, 2, function(x) any(x &gt; 0)))) 
Predict well levels based on rainfall and prior well levels. The question is, how do you know when you're at risk of over using a watershed? I can upload rain data. Well levels are at bntgroundwater.org
There's a lot of analytics to do with these data sets and I only have so much volunteer time to work on them
Thanks :) If I for instance, wish to show, that a OP_stue i open from 8-16 with a geom_tile, do I have to make a sequence of time from 8-16 or is there an easier way? :)
#the garbage code below will #tell you which columns have at least #1 (-) in the 1s in the final vec1 #1= at least 1 (-) #0= no (-) vec&lt;-vector() vec1&lt;-vector() for (j in 1:ncol(df)){ for (i in 1:nrow(df)){ Vec[i]&lt;-ifelse(df[i,j]&lt;0, 1, 0) } Vec1[j]&lt;-ifelse(sum(vec)&gt;0, 1, 0) } 
Nice! I didn't know that reddit supported code formatting. Thanks!
https://jstaf.github.io/2018/03/25/atom-ide.html
Yes, this is easily done: [https://www.datacamp.com/community/blog/jupyter-notebook-r](https://www.datacamp.com/community/blog/jupyter-notebook-r) Shortly: $ R &gt; install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest')) &gt; devtools::install_github('IRkernel/IRkernel') # Install IRKernel for the current user &gt; IRkernel::installspec() # Or install IRKernel system-wide &gt; IRkernel::installspec(user = FALSE) Make sure not to install R in your anaconda environment and it will use your system version.
conda install -c r ipython-notebook r-irkernel
Well I tried running the IRkernel commands but I'm getting the following error. Any idea why? `Error in IRkernel::installspec() : jupyter-client has to be installed but "jupyter kernelspec --version" exited with code 127. In addition: Warning message: In system2("jupyter", c("kernelspec", "--version"), FALSE, FALSE) : error in running command`
Isn't this also installing another instance of R inside anaconda tho?
Did you source your virtual environment before running R?
Oh okay. I thought you meant R. Do I need to do that in the terminal before running R? I’m not super experienced with python and anaconda tbh. 
Yes, you will need to source the environment first in the terminal before running R. And then anytime you want to run Jupyter notebooks in the future you will also have to source the same environment. It's the small price you pay for using virtual environments like anaconda.
Subset the vector Beta1 &lt;- beta[2] or Beta1 &lt;- beta[‘x1’] 
I've been using https://notebooks.azure.com/ for my R work (great for portability). 
IMO, your peers and professors are correct.
Try binding y into the dataset and have variable names as columns. Then add data = dataframe to the model. 
I tried modifying the code to fix, but am now getting the following error: "Error in as.data.frame.default(x\[\[i\]\], optional = TRUE, stringsAsFactors = stringsAsFactors) : cannot coerce class ‘"family"’ to a data.frame" Code: set.seed(12345) n=1000 x=cbind(1,runif(n)) y=poisson(link = "log") b=c(.2,.25) mu &lt;- 10\^(X%\*%b) df &lt;- data.frame(x,y) fit=glm(y\~x-1, family=poisson(link = log), data = df) summary(fit)
 y=poisson(link = "log") That's your problem. Are you sure you have that right? I think you want `y` to be some vector of values. That's what your formula `y ~ X -1` is expecting.
Exactly; I think he is needing to use one of these instead. https://www.rdocumentation.org/packages/stats/versions/3.3/topics/Poisson
The poisson function doesn't generate random poisson values if that's what you're trying to do. Use rpois instead.
I tried adding rpois to the data creation step and R is looking for a definition of lambda, but I don't know how to define lambda for this context. Error is "object lambda not found". set.seed(12345) n=1000 X=cbind(1,runif(n)) b=c(.2,.25) mu &lt;- 10\^(X%\*%b) y=rpois(n, **lambda**) df &lt;- data.frame(x,y) fit=glm(y\~x-1, family=poisson(link = log), data = df) summary(fit) 
you want to say `mu` there. `lambda` is either a constant or a vector, it's the parameter the poisson r.v. uses to draw from.
Check out the dplyr package’s case_when function
Probably the easiest way to do this since it's already a factor: `stringr::str_replace_na(LatePaymentBin, replacement = "Missing")` 
Perfect. What if I have a column that has a weird character value like "#N/A" as it was imported from an excel document? Would I have to change the "#N/A"s into NAs first then do this?
Set up the output file for writing as a binary file, and then, whether you are on Windows or Unix/Linux, the output will always be written in a Unix/Linux style. This [StackOverflow post](https://stackoverflow.com/questions/36933590/how-to-write-files-with-unix-end-of-lines-on-r-for-windows) has a great explanation. Let me know if you still have questions.
Also, if you have a bunch of files that you've already written to disk, and you don't want to have to export them from R again, use the Linux `tr` command \([tr man page](https://linux.die.net/man/1/tr)\) to get rid of the carriage return that Windows irritatingly inserts into text files.
You gave an excellent code example!
Thank you!
Thanks! I'll try this out. 
Great point. I can definitely do it at the start of my code. Appreciate the code example!
~~R random number generation isn't random, it's pseudorandom. Setting the seed to a different value will result in a different output, but using the same seed over and over ensures the same pseudorandom output. This is actually a benefit as it enables reproducible research--if I set the seed to 111 in my code and pass it along to you, your random number generation will be identical to mine, which removes any difference in results due to varying random input~~...oh, hell, you mean each value inside Xlat is identical, not that each time you generate `Xlat` it's identical to the time you generated it previously, right? Well, to start off, `survdata()` isn't returning anything. It looks like you're trying to assign a value inside it to `Xlat`, but that won't work because of lexical scoping. In order for something inside a function to affect the global environment, you need to explicitly return something or use the global assignment operator `&lt;&lt;-` which is bad practice because you might end up creating non-obvious dependencies between functions. So suppose you add `return(Xlat)` to the bottom of your `survdata()` function definition. Your next problem is that what's being returned is a vector of 100 values. Look here: survdata &lt;- function(N, lambda, alpha, beta2) { Z &lt;- sample(x=c(0,1), size=N, replace=TRUE, prob=c(0.5, 0.5)) u &lt;- runif(N) Xlat &lt;- (1/alpha)*(log((((-log(u))/(exp(Z*beta2)))*alpha)/lambda)+1) return(Xlat) } dat &lt;- survdata(N=100, lambda=1, alpha=1, beta2=0.6) Since you're calling `survdata()` with `N = 100`, `Z` is going to be a vector of length 100 due to the size argument in sample equaling `N`. You're also generating 100 `runif` values and assigning that to `u`. So the output of the calculation that you're assigning to `Xlat` is going to be 100 values. Calling that function and assigning it to `dat` means `dat` is now a vector of 100 values. This means that `dat` is not a model; you can't try to pull any coefficients (like you try to do with `dat$coef`) from it because it's simply a vector. Furthermore, those 100 values may be what you're looking to generate in the first place. That all make sense?
Make a new column. Copy values you have. Then geometric interpolation, then backfill the end. Not clean but works. I can make example tomorrow. 
This should work geom_interp &lt;- function(x){ ind &lt;- which(complete.cases(x))[1:2] root &lt;- diff(ind) ratio &lt;- (x[ind[2]]/x[ind[1]])^(1/root) t0 &lt;- x[ind[1]]/(ratio^(ind[1] - 1)) out &lt;- t0*ratio^(seq_along(x) - 1) out } library(tidyverse) df %&gt;% group_by(AK) %&gt;% mutate(new_Var2 = geom_interp(Var2))
x$new_variable = x$a + x$b Not sure the math you're trying use for the variable combination, but this will work. 
Depends on how you want to deal with the ends of the array x[-1,"a"] Will give you row+1 for column "a", while x[,"a"] Gives you row. But you can't add them together because of different lengths. x[,"a"] + c(x[-1,"a"],0) Is one solution, throwing in a 0 as the last value for row+1.
You can use the lead/lag functions in dplyr to do what you are describing.
I figured it out. My variable SMI was apparently giving me some issues. Now, however, I struggle with how to predict. It seems I cannot use the "predict" function as with SVM. Anyone got suggestions?
My only concern with doing this is I'm not super comfortable with data types in R, so with something like time, where you cant always do simple addition (i.e. 100 miliseconds in a second but 60 seconds in a minute) that when I go to create new time scales, they're not going to be acting as time should. I'd feel more comfortable using a built in package and using the functions that have already been developed if that makes sense. Eventually, I'm going to be sorting a list of times into these time segments, so I need them to act as times. 
i highly recommend it... also maybe worthwhile to do the free courses on [datacamp.com](https://datacamp.com) to brush up on these topics. &amp;#x200B; in any case something like this should probably work: &amp;#x200B; `df %&gt;%` `mutate(` `new_time = hms(originalTime) + seconds(30)` `)` &amp;#x200B; This assumes that: * \- *df* is your dataframe * \- there is a column named *originalTime* with the time stored in some Posix format * if in doubt do *str(df)* to see your datatype * it simply creates a new column called *new\_time* and saves the value from *originalTime* with 30s added in you could even do the whole thing in one pipeline. &amp;#x200B; `df %&gt;%` `mutate(` `originalTime = (whatever substring function you used)` `) %&gt;%` `mutate(` `new_time = hms(originalTime) + seconds(30)` `)` &amp;#x200B; &amp;#x200B;
1) how about posting what you’re running and show your code 2) why do you “need” it in one line of code
 x %&gt;% mutate(new_col = (x - lead(x)) / 100)
&gt; ab=scheffe.test(y, f, DFerror=12, MSerror=16.9, Fc=4.34, alpha = 0.05, group=TRUE, main = NULL,console=FALSE) &gt; ab $`statistics` MSerror Df F Mean CV Scheffe CriticalDifference 16.9 12 3.885294 13.8 29.78957 2.787577 7.247701 $parameters test name.t ntr alpha Scheffe f 3 0.05 $means y std r Min Max Q25 Q50 Q75 circuit 1 10.8 2.774887 5 8 15 9 10 12 circuit 2 22.2 4.868265 5 17 30 20 21 23 circuit 3 8.4 4.393177 5 5 16 6 7 8 $comparison NULL $groups y groups circuit 2 22.2 a circuit 1 10.8 b circuit 3 8.4 b attr(,"class") [1] "group" &gt; &gt; ab=scheffe.test(y, f, DFerror=12, MSerror=16.9, Fc=31.31, alpha = 0.05, group=TRUE, main = NULL,console=FALSE) &gt; ab $`statistics` MSerror Df F Mean CV Scheffe CriticalDifference 16.9 12 3.885294 13.8 29.78957 2.787577 7.247701 $parameters test name.t ntr alpha Scheffe f 3 0.05 $means y std r Min Max Q25 Q50 Q75 circuit 1 10.8 2.774887 5 8 15 9 10 12 circuit 2 22.2 4.868265 5 17 30 20 21 23 circuit 3 8.4 4.393177 5 5 16 6 7 8 $comparison NULL $groups y groups circuit 2 22.2 a circuit 1 10.8 b circuit 3 8.4 b attr(,"class") [1] "group" &gt; contrasts(x, contrasts = TRUE, sparse = FALSE) Error in contrasts(x, contrasts = TRUE, sparse = FALSE) : object 'x' not found &gt; contrasts(x, how.many) &lt;- value Error: object 'value' not found &gt; contrasts(y, contrasts = TRUE, sparse = FALSE) Error in contrasts(y, contrasts = TRUE, sparse = FALSE) : contrasts apply only to factors &gt; contrasts(model1, contrasts = TRUE, sparse = FALSE) Error in contrasts(model1, contrasts = TRUE, sparse = FALSE) : contrasts apply only to factors &gt; contrast=cbind(c(1,-2,1),c(1,-2,1)) &gt; contrast [,1] [,2] [1,] 1 1 [2,] -2 -2 [3,] 1 1 &gt; contrasts(f)=contrast &gt; summary.lm(aov(y~f)) Call: aov(formula = y ~ f) Residuals: Min 1Q Median 3Q Max -5.2 -2.4 -1.2 1.6 7.8 Coefficients: (1 not defined because of singularities) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 13.8000 1.0554 13.076 7.41e-09 *** f1 -4.2000 0.7463 -5.628 8.23e-05 *** f2 NA NA NA NA --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 4.088 on 13 degrees of freedom Multiple R-squared: 0.709, Adjusted R-squared: 0.6866 F-statistic: 31.67 on 1 and 13 DF, p-value: 8.229e-05 &gt; anova(model1) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) f 2 543.6 271.8 16.083 0.0004023 *** Residuals 12 202.8 16.9 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &gt; anova(C1) Error in UseMethod("anova") : no applicable method for 'anova' applied to an object of class "c('double', 'numeric')" &gt; s=factor(rep(c("Circuit 1),each=5)) + ) + u=lm(C1~s) + anova(u) + ) + + ) + a + summary(model1 + ) + ))anova(C1) + + anova(C1) + anova(C1) + anova(C1)&gt; summary(model1) Call: lm(formula = y ~ f) Residuals: Min 1Q Median 3Q Max -5.2 -2.3 -1.2 1.0 7.8 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 8.400 1.838 4.569 0.000645 *** fcurit 1 2.400 2.600 0.923 0.374155 fcurit 2 13.800 2.600 5.308 0.000186 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 4.111 on 12 degrees of freedom Multiple R-squared: 0.7283, Adjusted R-squared: 0.683 F-statistic: 16.08 on 2 and 12 DF, p-value: 0.0004023 &gt; summary(y) Min. 1st Qu. Median Mean 3rd Qu. Max. 5.0 8.0 12.0 13.8 18.5 30.0 &gt; anova(model1) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) f 2 543.6 271.8 16.083 0.0004023 *** Residuals 12 202.8 16.9 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 I try tons of method for MORE than two hours 
If you have some values `vals` and you plot them like this: plot(vals) it uses 'vals' as the y coordinates and substitutes 1:length(vals) as the x coordinates. You can specify x like this: x = seq(length(vals)) plot(x, vals) Now, if you want the x positions to start at zero, just subtract 1: plot(x-1, vals)
Maersk\_A\_R$APO\_SMA &lt;- SMA(Maersk\_A\_R$P\_CLOSE, n=20) - SMA(Maersk\_A\_R$P\_CLOSE, n=26)
Honestly if you are in R studio the find and replace feature you pull up through control-f jsnt a bad option.
Although I have done some map-work using R, I had never even heard of *kriging* prior to your post. You might find this [RPubs document](https://rpubs.com/nabilabd/118172) useful. It even specifically uses the Meuse data set, too. I think that the `sp` package will be very useful to you. It's mentioned in the pub I cited above, and I've used it previously to shape data sets for plotting heat-map choropleths using boundaries for countries, states, counties, or other administrative geographic boundary areas. The problem is very similar to what you want to do.
Some dude on GitHub who goes by "geneorama" posted some examples: https://github.com/geneorama/wnv_map_demo?files=1 I would start with mv thefts (motor vehicle). WNV demo is using GAM models to smooth out the predictions. Sorry there's no package. 
Thanks! I’ve tried this but it doesn’t seem to be working :P 
Apologies yeah I do realise my question didn’t give enough detail, putting up another question that hopefully makes more sense!
 Eg Time | stock | price 1 a 54 2 a 58 3 a 53 4 a 60 5 a 61 1 b 23 2 b 24 3 b 21 4 b 26 1 c 80 2 c 87 3 c 90 4 c 85 5 c 86 6 c 87 7 c 88 
Assuming your function has arguments time and price I suggest using the data.table packages for this. `install.package("data.table")` `library(data.table)` `yourData &lt;- as.data.table(yourData)` `yourData[ , yourFunction(time , price) , by = stock]` This would basically apply your function for each stock in your data and return the results in a new data.table
If you're going to post code, surround the block with backticks. Like this: library(jsonlite) rm(list = ls()) # set default information city &lt;- ""Philadelphia,US"" timezone &lt;- -4 # function utilities nullQ &lt;- function (x) { ifelse(is.null(x) || is.na(x), 0, x) } naQ &lt;- function (x) { ifelse(is.null(x) || is.na(x), NA, x) } # API for 5 days forcast weather &lt;- fromJSON( paste( ""http://api.openweathermap.org/data/2.5/forecast?q="", city, ""&amp;appid=42e84301e9b39293899f1359c02ba514"", sep = """" ), flatten = TRUE ) test &lt;- weather$list time &lt;- c() for (i in 1:dim(test)[[1]]) { time &lt;- append(time, as.POSIXct(test$dt[[i]], origin = ""1970-01-01"", tz = ""UTC"")) } temperature &lt;- test$main.temp - 273.15 temperature_min &lt;- test$main.temp_min - 273.15 temperature_max &lt;- test$main.temp_max - 273.15 pressure &lt;- test$main.pressure sea_level &lt;- test$main.sea_level ground_level &lt;- test$main.grnd_level humidity &lt;- test$main.humidity temp_kf &lt;- test$main.temp_kf clouds &lt;- test$clouds.all wind_speed &lt;- test$wind.speed wind_degree &lt;- (test$wind.deg + 180) %% 360 - 180 cond &lt;- c() for (i in 1:dim(test)[[1]]) { cond &lt;- append(cond, test$weather[[i]]$main) } condition &lt;- c() for (i in 1:dim(test)[[1]]) { condition &lt;- append(condition, test$weather[[i]]$description) } time_zone &lt;- c() for (i in 1:dim(test)[[1]]) { time_zone &lt;- append(time_zone, timezone) } rain &lt;- c() for (i in 1:dim(test)[[1]]) { rain &lt;- append(rain, nullQ(test$rain.3h[[i]])) } snow &lt;- c() for (i in 1:dim(test)[[1]]) { snow &lt;- append(snow, nullQ(test$snow.3h[[i]])) } weatherData &lt;- data.frame( time, time_zone, temperature, temperature_min, temperature_max, pressure, sea_level, ground_level, humidity, temp_kf, clouds, wind_speed, wind_degree, cond, condition, rain, snow ) # API for weather now weather &lt;- fromJSON( paste( ""http://api.openweathermap.org/data/2.5/weather?q="", city, ""&amp;appid=b9fac368f396b03c9b54d47986e40256&amp;cnt=16"", sep = """" ), flatten = TRUE ) time &lt;- as.POSIXct(weather$dt, origin = ""1970-01-01"", tz = ""UTC"") time_zone &lt;- timezone longitude &lt;- naQ(weather$coord$lon) latitude &lt;- naQ(weather$coord$lat) cond &lt;- naQ(weather$weather$main) condition &lt;- naQ(weather$weather$description) pressure &lt;- naQ(weather$main$pressure) humidity &lt;- naQ(weather$main$humidity) temperature &lt;- naQ(weather$main$temp - 273.15) temperature_min &lt;- naQ(weather$main$temp_min - 273.15) temperature_max &lt;- naQ(weather$main$temp_max - 273.15) visibility &lt;- naQ(weather$visibility) wind_speed &lt;- naQ(weather$wind$speed) wind_degree &lt;- naQ((weather$wind$deg + 180) %% 360 - 180) country &lt;- weather$sys$country city &lt;- weather$name sunrise &lt;- as.POSIXct(weather$sys$sunrise, origin = ""1970-01-01"", tz = ""UTC"") sunset &lt;- as.POSIXct(weather$sys$sunset, origin = ""1970-01-01"", tz = ""UTC"") cloud &lt;- naQ(weather$clouds$all) weatherNow &lt;- data.frame( time, time_zone, longitude, latitude, cond, condition, pressure, humidity, temperature, temperature_min, temperature_max, visibility, wind_speed, wind_degree, city, country, sunset, sunrise, cloud )`
I am not 100% clear on what you want to do. Can you include more details about your function? If you want to calculate an average for each group and have the output be a df, here is a tidyverse solution: library(tidyverse) new_df &lt;- df %&gt;% group_by(stock) %&gt;% summarise(avg = mean(price, na.rm = TRUE)) 
Ahh alright thanks for your advice (: hopefully this makes things clearer? My function consists of the TTR package functions. Tech_indicators &lt;- function(x) { Df &lt;- all_data %&gt;% filter(x) Df$Sma100 &lt;- TTR::SMA(Df$price, n=100) } Etc.... How do I change my function such that it will run this code for each different stock? 
I'll be honest, I have not used the TTR package before, and I do not do anything finance related for work. But, it looks like your function is not working because the filter does not know what to filter and on which dataframe. You would need something like this for it to filter library(tidyverse) library(TTR) #My function consists of the TTR package functions. Tech_indicators &lt;- function(x, dataframe) { new_df &lt;- df %&gt;% filter(Stock == x) new_df$Sma100 &lt;- TTR::SMA(new_df$Price, nrow(new_df)) return(new_df) } Tech_indicators("a", df) I do not know what you want your N to be, but this sets it as the total number of cases that were filtered (because in the example data frame there were less than 100). Is that what you're looking for?
If none of the original character values have NA, this should suffice: 
library(tidyverse) df %&gt;% mutate_if(is_numeric, as.factor) This line of code will see if your column is numeric, if it is, it will convert it to factor else will leave it unchanged.
I am on mobile so I can use this to do your function later on my computer. If you just want 1 number would the below code work? It is simple. If not, I will give it a go last, or @Thaufus’s solution looks like it works. new_df &lt;- df %&gt;% group_by(Stock) %&gt;% top_n(3, Eg_Time) %&gt;% summarise(avg = mean(Price, na.rm = False)
Yes you can learn it. It is good first choice. 
Well, technically you can learn any programming language without prior programming knowledge. I learned SAS first and started learning R with DataCamp. Going from SAS to R was not exactly easy because R is object oriented and functions are very different. It probably would have been easier to learn R without any prior knowledge of SAS. I would suggest using DataCamp if you are trying to learn R
Yes. And R is a good starting point not only because it is fairly easy to pick up, but also as in the case you'd like to deepen your programming knowledge to Python, R is very helpful in that
Thanks for your answer sir
I'm going to be a dissenting opinion here: If you want to learn a computer language, don't choose R as your first/primary language. You will be MUCH better served selecting something like Python as your first language. If you need R's functionality for whatever you're doing, then you *can* learn R without having any previous coding knowledge... but it can be difficult. Concepts in coding -- like indexing and assignment -- can be hard to learn, and it's made harder by the fact that R has many equivalent ways to perform the same operation (e.g., dataframe[['ColumnName']] is the same as dataframe$ColumnName, x=1 is the same as x&lt;-1, etc.) I strongly recommend picking up a primary language like Python and then using that foundation of knowledge to learn unique scripting languages like R.
github - it loads EVERYWHERE but at work on a mac - but i have loaded the app before at work when hosted on shinyapps.io
So I went through the suggestions in that section and since I do not really know what this is supposed to look like, I'm not sure it helps. I googled the R formatting and it seems like all this code is a hot mess. If you could point me to a resource that you recommend, I'll see if I can revise.
I’m not saying you’re wrong in general, but you also chose a “problem” that exists in Python. dataframe[[‘ColumnName’]] and dataframe.ColumnName are the same
I'll second DataCamp. Well worth it.
Downsides : 1. You will be everyone's point of contact in your office when they need some statistical work to get done. 2. You might attend meetups on weekends instead of haying around. 3. In no time, you will get addicted to data analysis. 4. You will scratch your head when working with web scrapping and big data (for now). That's all I can think of right now. 
Plenty of people use R as a first language because they need to do some stats in their fields and they're not programmers. If you get R and RStudio installed on your computer, a great way to get started is with SwirlStats... you interactively learn R using R: https://swirlstats.com/
I'll third DataCamp it's a great tool. They'll often have 50% for a year around Black Friday so I wouldn't pay for more than a month at a time until that though.
You probably mean `as.numeric`, not `as.factor`.
I am guessing your work is messing with the websockets. If you can, I would recommend throwing it behind nginx, or alternatively disabling websockets on the server. https://community.rstudio.com/t/is-firewall-blocking-shiny-websockets-wss/3040
R is good for helping you learn Pandas in Python. There are many Pythonic concepts that will feel very foreign to you if you start with R.
R is the first language I picked up and the one I know best and use in work on a daily basis. I truly love working in R. I worked with python quite a bit in the past but it's pretty rusty. I do see that most other people in my organization use Python and I've reached the conclusion that I need to re-learn Python - set it as my goal for the next month. I've already started reworking my R code in Python and found that's a good way to learn it. With the new reticulate package it's easier to work with both languages. R is a great language, but I think you'd be better off starting with python. 
&gt;If you see this page, the nginx web server is successfully installed and working. it is behind nginx
I meant securing it behind nginx. Shiny Server (non-pro) isn't secure, but you can use nginx to make it secure. 
I am learning R for a class right now. I have no previous experience, but want to advance my career. So...here I am! It's quite possible to start here.
I would say R is easier to pick up than a lot of programming languages since it's not made for programmers as much as for researchers. It is still very much a full programming language though and will take time to learn.
&gt; What I am wondering is what downsides are there when starting programming with R? • You likely won't learn about optimisation for a while. As a result you will be going back and changing how you code. That's ok though, you may never even need to do that. • You can't do as much from scratch with R unless you learn C++. If you are looking to do data analysis, R is an excellent place to start. If you're looking to go into dev, you should probably start somewhere else. Python would be a good start as it is quite user friendly. And learn from a good book with exercises. A lot of the newer interactive courses like datacamp give you instant gratification that helps initially, but will seriously mess you up once you encounter difficult problems. You need to learn how to think about code, not any specific language itself.
Yes you can learn R as a first programming language. Despite what others are saying - Python is not necessarily better in that regard. Just keep in mind that R is not OO like Python but leans towards the functional side. If you want to get more out of R in general programming trends look for resources on functional programming instead of OO.
Yes
Yes. Amazing gateway drug of a language.
This gives me a good start. I appreciate the input and research points. I just wish BI gave some better scripting options but I’ll just plug through. 
It's the first one I learned. Let me pick up python quicker. Issue is I get confused between the two and use R code in python sometimes, which doesn't work. I think R is better structured than python, but I'm probably biased because I learned it first. 
R4DS.HAD.CO.NZ
For small scale stuff, shinyapps.io is less hassle. 
$40 a month tho for 500 hours
https://bookdown.org/yihui/rmarkdown/custom-css-1.html#slide-ids-and-classes Bootstrap has build in class names for alerts such as "alert alert-info" https://v4-alpha.getbootstrap.com/components/alerts/
It seems to me most people use R as a collection of tools (which are absolutely awesome) rather than a programming language. I think you'll be fine, but I highly recommend enrolling in some MOOC with online assignments if you want to learn efficiently. However, if your goal is to learn a programming language specifically, I advise against R because it's just not a great one. Python is many times better as a language, and it also happens to offer a lot of nice data analysis tools. The tools maybe somewhat worse than in R on average, but their diversity is much greater. 
You can provide custom stylesheets for the R Markdown document. It’s [documented in the R Markdown guide](https://bookdown.org/yihui/rmarkdown/html-document.html#appearance-and-style).
I started by using this website/package: [https://swirlstats.com/](https://swirlstats.com/)
Swirl or Datacamp are going to be your best bets imo.
I apologize for the late response (Have had a hectic week), but thanks for your help! It was very insightful!
Depends on what are you trying to learn R for. DataCamp/Coursera are good starting points, as well as Swirl. I started by simply googling for instructions for techniques I was working with, and watched a plenty of Youtube videos.
Thanks for the reply! I have used custom stylesheets some. Do you know if the exact format seen in this notebook can be created with css? In particular, the format for the questions, where there is a darker bar along the top of the box, and an overlaid box in the upper left corner that states the question number. I feel like that must be obtained using some sort of package. 
Thanks for the reply! Do you know if the exact format seen in this notebook can be created with css? In particular, the format for the questions, where there is a darker bar along the top of the box, and an overlaid box in the upper left corner that states the question number. 
It really isn't that bad. There are some very helpful guides http://adv-r.had.co.nz/Rcpp.html http://dirk.eddelbuettel.com/papers/rcpp_workshop_introduction_user2012.pdf I'd start off writing some test code, and once you get the hang of how best to google for what you want, i'd say jump in and start writing what you need. Just start writing code, best way to learn. 
https://paulvanderlaken.com/2017/10/18/learn-r/
Anything can be created with CSS. You can also add inline styles and modify existing CSS in rmarkdown. https://bookdown.org/yihui/rmarkdown/language-engines.html#javascript-and-css For this, they used an ordered list. You can inspect styles yourself [following this guide](https://developers.google.com/web/tools/chrome-devtools/inspect-styles/). ol &gt; li:before { content: "Question " counter(li); counter-increment: li; position: absolute; top: -2px; left: -2em; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; box-sizing: border-box; width: 7em; margin-right: 8px; padding: 4px; border-top: 2px solid #317EAC; color: #fff; background: #317EAC; font-weight: bold; font-family: "Helvetica Neue", Arial, sans-serif; text-align: center; }
“If”? Yes, since it’s done that way. How — not exactly sure but you can of course inspect the CSS of the page. At a glance, it’s fairly hacky since it styles `&lt;ol&gt;` elements to be displayed as questions. The rule in question is via the `:before` pseudo-class: ol &gt; li:before { content: "Question " counter(li); counter-increment: li; position: absolute; top: -2px; left: -2em; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; box-sizing: border-box; width: 7em; margin-right: 8px; padding: 4px; border-top: 2px solid #317EAC; color: #fff; background: #317EAC; font-weight: bold; font-family: "Helvetica Neue", Arial, sans-serif; text-align: center; } 
The Analytics Edge on Coursera. Can't recommend this course enough - it's what got me started in R and has really changed my career for the better. 
Ah. Okay. Thank you very much! It should have occurred to me that I could see the css by looking at the sort code for the page. Thanks again! 
Ah. Okay. Thank you very much! It should have occurred to me that I could see the css by looking at the sort code for the page. Thanks again!
Same boat as you. I have packages and tutorials on how to use them for sample data but I don't know how to apply it to my own or even understand the functions that are being carried out. 
R is the first language I learned as well.
Sorry you meant edX?
I use it. Basic Rcpp is pretty straight forward with a low learning curve if you know C++.
If you use easy syntax packages, particularly dplyr there are none. R has many point and click packages that will write R code you can copy or edit. you will not learn important CS concepts but for munging viz and Eda it probably will not matter.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/statistics] [Exists plot like mosaic plot of ChiSqr residuals but for continuous variables?](https://www.reddit.com/r/statistics/comments/9nx9gm/exists_plot_like_mosaic_plot_of_chisqr_residuals/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
How many users and how many ratings does each user have? You could plot means per person with confidence intervals around them which would account for two of those. I guess it depends on what you’re doing as well. I’d be inclined to just produce a table with all of that but I don’t know what you’re doing. Something like library(dplyr) library(skimr) df %&gt;% group_by(individual) %&gt;% skim() You can also use skim just to look at an individual variable too. I love skim. Again though, it depends on what you want to do. As an aside, a scatter plot would be inappropriate for this anyway because you don’t have two variables you desire to examine the interaction between. 
How many users and how many ratings does each user have? You could plot means per person with confidence intervals around them which would account for two of those. I guess it depends on what you’re doing as well. I’d be inclined to just produce a table with all of that but I don’t know what you’re doing. Something like library(dplyr) library(skimr) df %&gt;% group_by(individual) %&gt;% skim() You can also use skim just to look at an individual variable too. I love skim. Again though, it depends on what you want to do. As an aside, a scatter plot would be inappropriate for this anyway because you don’t have two variables you desire to examine the interaction between. 
This is a really good resource to learn R: [Hadley Wichkam's R for data science](http://r4ds.had.co.nz/) It will guide you through visualizations, data wrangling, project workflow, and more.
Ah - I think It was `dput()` that I was searching for! Thanks though 
The rio package is also great - it has an import() and an export() function, you specify the file type in the file name and it does the rest! For example, export(your_df, “your file name.csv”) will give you a csv like you are looking for without having to deal with all of that write.table() bs.
Try wrapping Contribution inside order(-) like y=order(-Contribution).
You should probably stick unique onto your levels argument. You can also use the reorder function on x and y arguments in ggplot .
Thanks for your comment, adding y=order(-Contribution) solved a problem I hadn't seen before (the x-axis was out of order), so that's cool. But, it didn't help with the order of Species, and it totally threw off my y-axis; now the axis is in some weird range (up to 600), even though all of the variables are decimals (out of 1)
Oh man, adding mydata$Species&lt;- factor(mydata$Species, levels=unique(mydata$Species)) mydata$Time&lt;- factor(mydata$Time, levels=unique(mydata$Time)) mydata$Contribution&lt;- factor(mydata$Contribution, levels=unique(mydata$Contribution)) Did some crazy things, but none of them helped maintain the order of my variables (Species) in each stack. Thanks for your comment though!
you probably need to explicitly define the factor levels of that variable. https://stackoverflow.com/questions/32345923/how-to-control-ordering-of-stacked-bar-chart-using-identity-on-ggplot2
&gt; y=reorder(-Contribution, Contribution) Ah, thanks for the edit. Still though, I get some crazy shenanigans when adding that line in! I defined the explicitly defined the order, so that solved the previously unrealized problem. This is head(mydata) if that helps. Species Contribution Time Site 1 Pterygophora 0.6063492 Summer 2018 (June) Stillwater Cove 2 D. californicum 0.2317460 Summer 2018 (June) Stillwater Cove 3 Stephanocystis 0.1619048 Summer 2018 (June) Stillwater Cove 4 Pterygophora 0.5814815 Summer 2018 (August) Stillwater Cove 5 D. californicum 0.2592593 Summer 2018 (August) Stillwater Cove 6 C. ruprechtiana 0.1296296 Summer 2018 (August) Stillwater Cove
Yeah I was just trying to give a quick answer, I'm sure someone has already given you a better one. It does sounds like you wanted an ordered factor though, which you can do with the ordered function. 
So every cell is colored according to the value in it? Like a heatmap?
Use forcats::fct_relevel() or one of the other family of functions for reordering factors in forcats 
So your goal is to create different facets and order each facet differently? Maybe this https://stackoverflow.com/questions/48179726/ordering-factors-in-each-facet-of-ggplot-by-y-axis-value ?
&gt;I've also tried Reticulate package and it manages to load the data, but it is saved in numpy.ndarray and I can't convert it to dataframe. Reticulate converts numpy arrays to R matrices. You should be able to convert this simply with “as.data.frame(obj)”. Can you share the relevant parts of your code so we can see what’s going wrong? 
You might be overthinking this. Lower the alpha of your scatter plot and maybe lower point size until it becomes readable.
great to see these forums of website comment topics, its very very informative blog,gonna add it in my [best british essay](https://www.essayontime.co.uk/) , thank you so much for posting with this, keep it up. 
Instead of just stating the homework problem, tell us what you've tried and what isn't working. Do you know the formulas behind each of these functions (e.g. can you write the equation you are trying to implement?)
That is a good idea but it only helps with the first step / problem and that was well solved by heatmaps 2d histograms and density contours. 
There will be many scatterplots. Soon I will add even more statistics like if users prefer unpopular books and the like. Skimmer is nice as an improvement over summary but that is all. 
What is this post? How does this engage the community? It looks like you're trying to scream into the void. This community isn't meant for that.
I'd be happy to help if you have a specific question. 
All I’m saying is that with the statistics you’re describing, a scatter plot is inappropriate. 
Thank you so much!! I checked it now and this is what I need! You helped me a lot! Cheers!
Thank you!
Your welcome. I have personally evaluated all the table formatting packages and my opinion the DT package with formattable package integration is the best for Shiny apps. So my suggestion is to still use DT to render the tables but use the formattable integration to format. There is a page in the DT documentation that talks about that. 
Will check it. Thx again!
The code you posted has many problems. For starters, you never close the parentheses after the first `ggplot()` call. Also, you are telling ggplot to look for variables that are not present in the dataframes you supply. There's no `TCCHDPE_ng` column in the `errTCCWater_ng` object and there's no `TCCWater_ng` in the `errTCCHDPE_ng` object. That said, it is hard to understand what are the results you want. Your labels say that the x axis is "Time (hours)", but in the aes calls, x is `TCCwater_ng`. Finally, you mention that you want to plot both horizontal and vertical error bars, but you only call `geom_errorbar` (only plots vertical errorbars, for horizontal errorbars, you should look into `geom_errorbarh`. I played with your code a bit, trying to figure out what you wanted. These two snippets produce plots that seem to make sense, maybe this is what you wanted all along. ggplot(errTCCWater_ng, aes(Hour, TCCWater_ng)) + geom_errorbar(aes( ymin = TCCWater_ng - se, ymax = TCCWater_ng + se ), colour = "black", width=.1) + geom_line(size = 1.25) + geom_point(size = 3) + xlab("Time (hours)") + ylab("Triclocarban in Water (ng TCC)") + ggtitle("Equilibrium Kinetics of Triclocarban Sorption onto HDPE Microplastics") + theme_bw() + theme(legend.position=c(0.75,0.05),legend.justification=c(1,0)) Another call, now with HDPE ggplot(errTCCHDPE_ng, aes(Hour, TCCHDPE_ng)) + geom_errorbar(aes( ymin = TCCHDPE_ng - se, ymax = TCCHDPE_ng + se ), colour = "black", width = .1) + geom_line(size = 1.25) + geom_point(size = 3) + xlab("Time (hours)") + ylab("Something different from the first plot") + ggtitle("Also different from the first plot") + theme_bw() + theme(legend.position = c(0.75, 0.05), legend.justification = c(1,0)) 
Ty for all of the advice! This code works when I'm just doing horizontal error bars when I plot time vs. concentration. But when I want to do concentration in water vs concentration in hope, and have BOTH error bars, that's when it doesn't work. I will try this today! 
Datacamp.com Spend thirty dollars and follow along with the simulated console. Or use swirl and follow along in your own environment. Or.... And I know this sounds stupid. Kaggle.com - download data - clean it - analysis - markdown.
That doesn't work for me, it just throws errors for some reason. Thanks though. What did work was F150.F250 &lt;- subset (dat, Models == "F350" | Models== "Raptor") 
 data &lt;- do.call(rbind,replicate(2000,setNames(sample(1:10,3,replace=FALSE),LETTERS[1:3]),simplify=FALSE)) **Breakdown:** sample(1:10,3,replace=FALSE) generates three random numbers between 1 and 10, with no repetitions allowed, setNames( ... , LETTERS[1:3]) assigns the names A, B and C to the sample result, replicate(2000, .... , simplify=FALSE) repeats that procedure 2000 times and returns the result as a list, and finally do.call(rbind, ... ) takes the elements of that list and concatenates them as rows in a matrix.
Well, it seems I haven't got how this community works. Sorry for saying that I was in a bad mood after all the struggling. Even though, I don't really think that any place shouldn't be allowed for a different voice.
Datacamp + Kaggle. That’s the plan of action. Thank you! Appreciate your help. 
Honestly. Don't buy the "annual" datacamp membership, shits a joke. Do a month here and a month there. It's nice to have as a backup for when you really want to THROW yourself into something new, but I wouldn't recommend it for long-term learning. Use Kaggle for data and stack exchange/overflow when you can. Follow blogs/github and just reproduce what other people have done.
Okay, that’s good advice. Thank you. One more question, sorry if it seems stupid but what should be a good timeline for me to do this? I am really tired of my current job and I want to get out ASAP which means I am willing to do anything and everything. But at the same time, I don’t want to think that I become an expert in 1 month. Thoughts? 
I'm not entirely sure what your experience with programming and statistical analysis is. I've been doing this for about four years and I have only got independent contractor jobs. I don't think you'll become an expert in one month, realistically, a few months of everyday coding would get you "pretty damn" good. Though, I suspect it takes more time than that.
Makes perfect sense. I really appreciate your response. Thank you! 
i see nothing. are you saying youve written nothing?
it did not show up i upload again thank you for your replying and answering 
it did not show up so I upload it again thank you for replying me and answering me!
Is the aim to end up with a data.frame? Then you want expand.grid: expand.grid(c('foo', 'bar'), c('baz', 'cur')) 
Well, then I might as well use `data.frame(a=c("foo", "foo", "bar", "bar", b=c("baz", "cur", "baz", "cur"))` This is just a toy example I hoped would get he point across. I need those applys and function. How do I transform my `result` to get the `data.frame`?
Should be able to perfectly fine. Store your passwords in an encrypted file, call the file for that information and then treat it similar to webscraping. 
You're looking for a Cartesian product: &gt; expand.grid(c("baz", "cur"), c("foo", "bar"))[2:1] Var2 Var1 1 foo baz 2 foo cur 3 bar baz 4 bar cur or &gt; expand.grid(c("baz", "cur"), c("foo", "bar"), stringsAsFactors=FALSE)[2:1] Var2 Var1 1 foo baz 2 foo cur 3 bar baz 4 bar cur stackoverflow is better for these types of questions
This worked! :) Thank you so so much!
Are you open to using purrr::map_df() instead of lapply()?
I think something like this would do it: indexLast &lt;- function(dat, indexes) { ndims &lt;- length(dim(dat)) otherDims &lt;- seq(ndims - 1) result &lt;- apply(dat, otherDims, `[`, indexes) return(result) } w &lt;- array(1:72, dim = c(6,3,4)) w[,,2] indexLast(w, 2) I didn't test this at all though
Not helpful to your question but can rhandsontable be used as an app that a user can edit and then save (so another user can then visit the app and see someone's changes/comments)?
Don't think it is the best way to do that, but it can be done with some good coding. 
Your V1 probably doesn't have enough columns or rows to subset by the row names, which I'm presuming are numbered? Your input is probably the wrong shape, but that error message isn't very informative. I'd just dig straight into the source code for that function at this point, if it was me 
Yeah, I did a few courses and I am on the same :(
**Newick format** In mathematics, Newick tree format (or Newick notation or New Hampshire tree format) is a way of representing graph-theoretical trees with edge lengths using parentheses and commas. It was adopted by James Archie, William H. E. Day, Joseph Felsenstein, Wayne Maddison, Christopher Meacham, F. James Rohlf, and David Swofford, at two meetings in 1986, the second of which was at Newick's restaurant in Dover, New Hampshire, US. The adopted format is a generalization of the format developed by Meacham in 1984 for the first tree-drawing programs in Felsenstein's PHYLIP package. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Thanks, actually that's the way I usually go, but for reasons that I'm too lazy to explain, I'll need to stick with arrays for this problem. For anyone else that's reading this, this is the smart way to go 
This works! Thanks! 
FWIW, I was able to get `pblm()` to output a valid result with the following code. library(ape) library(picante) tree1 = read.tree(text = "(Bovine:0.69395,(Gibbon:0.36079,(Orang:0.33636,(Gorilla:0.17147,(Chimp:0.19268, Human:0.11927):0.08386):0.06124):0.15057):0.54939,Mouse:1.21460):0.10;") tree2 = read.tree(text = "(Bovine:0.69395,(Hylobates:0.36079,(Pongo:0.33636,(G._Gorilla:0.17147, (P._paniscus:0.19268,H._sapiens:0.11927):0.08386):0.06124):0.15057):0.54939, Rodent:1.21460);") plot(tree1, main = "tree1") plot(tree2, main = "tree2") set.seed(2^8) matvals &lt;- ifelse(runif(49, 0, 1) &gt; 0.5, 1, 0) assoc_matrix = matrix( matvals, nrow=7, ncol=7, byrow = TRUE, dimnames = list(tree1$tip.label, tree2$tip.label)) pblm(assocs = assoc_matrix, tree1 = tree1, tree2 = tree2) I found the code for these two trees on [the website for the Newick format](http://evolution.genetics.washington.edu/phylip/newicktree.html). As the two plots in my code show, the trees are exactly the same in terms of distance, but they have different leaf names, with the second one being more descriptive. I created a fake *association matrix* with random values using the `ifelse()` statement followed by the `matrix()` function. My matrix values are binary, but according to the `pblm()` documentation, the association matrix can either be binary or continuous. In my case, both `tree1` and `tree2` each contain 7 items, so I created an *association matrix* with 7 rows x 7 columns. If `tree1` and `tree2` contain `n` and `m` values, respectively, I assume that your association matrix will need to contain `n` x `m` values, but, truthfully, I don't know for sure. As another commenter here stated, the error message you were receiving was not very descriptive, so, without input from the package author or someone knowledgeable, the only way to figure out what is happening would be to review the source code for the package. Judging by the large number of parameters the `pblm()` function takes, I'm guessing that the code is going to be hard to figure out. Good luck.
Can you describe the data? Better yet, do you have a sample?
I'd also love to see a sample of the data if it would be available to us
I've also been interested in processing functional images in R I used matlab for my PhD, but it would be great to re-visit my data in R
Do you know that R was designed with vectorised code in mind? 
Doesn’t make sense to me. If you want the front end to utilise r output you might as well rely on plumber.
That's what I suggested (and what will be used).
My apologies. I'm so bad at formatting and everything haha You absolutely understood what I wanted and are correct. Just tried it. I get the below which is almost perfect: year CloseType N 1: 2000 204 2: 2000 HIGH 24 3: 2000 LOW 32 4: 2001 231 5: 2001 HIGH 16 6: 2001 LOW 14 There should be a way where I get the following: year N CloseHigh CloseLow 1: 2000 260 24 32 2: 2001 261 16 14 Do you think, you can teach me this as well ? :D thanks!!!
Apologies if I’m missing the point, but have you tried “Tools -- Global Options —Appearance”?
Add a year column to your dataframe. Then do table(MyData$year, MyData$CloseType). If that is not close enough for you, try: Newdf &lt;- table(MyData$year, MyData$CloseType)
I think OP wants particular words in the documentation pane highlighted more prominently. e.g. “see more” My suspicion is that this might be possible because ultimately its an HTML doc. Just not sure how though.
This is great!! From here on I can continue by moving my columns around. Thaaaanks!
10% missing data is frankly quite a bit of missing data, especially if you have alot of variance. You can "trust" the estimates but i'd be wary. Have you considered more modern methods of imputation? Not mean or regression, but a maximum likelihood model or MICE?
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day!
In reality I have about 1500 observations and 108 missing from one data source and 162 from another. These are external variables we are attaching to our internal data the best we can. We are using a few variables from each data source. We are looking for lift not fit and we understand there will be a good amount of variance but this is as good as we can get with the way data is at the moment. Isn't a GLM creating a maximum likelihood model to begin with?
Yup, GLM is a maximum likelihood model. I guess, I just prefer to do some version of imputation more often then not. 
Thanks! I'll look into these.
I don't have a sample to share unfortunately. I will revisit this thread and update if I figure anything out AND/OR have images to share one we publish/etc. &amp;#x200B; &amp;#x200B;
As it stands, we have all of our image processing code completed. I am fairly new to this level of analysis (using R, MatLab, etc)--Since I've used R the most, I am just trying to find a familiar way in which to process these data. For context: I do Traumatic Brain injury research. Currently, we are using Matlab to create our connectivity maps (from an adjacency matrix) in the brain. Ultimately, we create a graphical representation of the strength of connectivity between nodes within the brain. I'd like to be able to do this in R, which I assume is possible. 
Not an expert but I'd get a machine that no one's using, set up a while loop or some websocket/Github api magick that checks for repo updates (the simplest way to do this would involve running git2r::pull at set intervals) and does devtools::test() whenever there is an update. You probably can set up something that'll be fine unless you have too many huge repos fairly easily in an afternoon if you are really short on money. I have set ups like this relying on Cron jobs and they work fine but I am no company
This sounds like a perfect use of `purrr::pmap()` my_fun &lt;- function(x, y, z) { glue::glue(x, y, z) } my_fun("a", "b", "c") my_tbl &lt;- readr::read_csv(file = "x,y,z\nA,B,C\nD,E,F\nH,I,J") my_tbl purrr::pmap(.l = my_tbl, .f = my_fun)
After re-reading your post, I see your function updates an existing data frame, in that case you will want to use `purrr:pwalk()` instead so as to not return anything.
There are alternatives to Travis that you can host as part of your development environment, eg [Jenkins](https://jenkins.io/solutions/github/). 
Awesome, thank you! Exactly what I was looking for.
How good is your knowledge on block chain tech...I want to dive into this area and am looking for resources. I sure can help with analysis of data.
Lol thanks. Glad to help. Something like dcast(N~year+close_type, data={previous result goes here}) Data table takes a while to learn but it's worth it. Super fast and powerful. It's the only package I use. 
Thanks. I didn't really consider Jenkins because I had (incorrectly) assumed Jenkins was like Travis where you basically had to do it off-premises, so I'll definitely look into this.
Exactly.
Leanpub.com 
Haaaa!
For igraph specifically, this is a good resource. [http://kateto.net/netscix2016](http://kateto.net/netscix2016) &amp;#x200B; However, I think that there's a chance you'll like [tidygraph](https://www.data-imaginist.com/2017/introducing-tidygraph/) a lot more, which provides a more tidy style of doing network analysis. It offers all of the power available in the igraph package too if I am not mistaken. &amp;#x200B;
This article just appears to be just a list of links to paid courses
Wait, you don't think OP is just trying to sell classes to an open source software here and on other R subs, do you?
"What will you learn: &lt;a list of the courses&gt;" It seems like a very minimal effort to get a credit for affiliate links. Some of these courses really are good, but the author gives no opinion about them, and seems to have no idea that they will really overlap each other significantly. I don't think anyone who couldn't find as much information in ten minutes from a few searches should be trying to studying data science anyway.
The function should accept a vector input. Then, loop over the length of the vector (minus one) and use brackets to call the appropriate values at each stage.
Why not use all vector operations: lag(x) - x / x
 f &lt;- function(x) diff(x)/x[-length(x)]
Thank you very much! It worked.
Was R possibly upgraded to a new version? For example in my "R" folder, I have a folder for "3.0" and "3.4" and there are some differences in which libraries are in each. That would be my first guess, anyway.
Could someone please explain how x[-n] works?
Yeah, since OP mentioned `n` I thought I would include it explicitly, but even so it would have been better if I had written out as multiple lines: `f &lt;- function(x) { n &lt;- length(x); diff(x)/x[-n] }`.
Well, I’ve just discovered this feature that you can give coins (and with it a week of gold) to people, and I’ve tried to solve this task for so long without success, that I thought I’m gonna give you a little bit more than just a “Thank you”. But again, Thank you.
Well, great - that was a nice surprise for a change.
P.S. you might have better luck with these kind of 'how to code this?' kind of questions on http://stackoverflow.com. They are a little more brutal in requiring you to set up the problem and giving a minimal example, but the community for this is much larger and much quicker.
I’ve submitted this to there as well yesterday, but they still haven’t answered me. I am usually able to find the solution to the problems that I have (most of the time on stackoverflow), but I was so stuck with this, that I had to ask it. 
&gt; For one reason or another, they cannot be included in one arrayed variable. My guess is this assumption is wrong. In general, in R you want to be working with things on lists (or some other similar data structure), and either using functions that are inherently vectorized (will run once for each input element if you provide it a list or a vector), or use something like apply() or purrr::map to loop over your list. For more of this general advice, check out [this amazing resource on R](http://r4ds.had.co.nz/). If you're just doing this twice, why not create a function? my_function(input_variable) { # do something } my_function(var_hello) my_function(var_goodbye) I know you said you can't put it into a list or array, but really?? Why wouldn't this work: variables = list(var_hello, var_goodbye) result = purrr::map(variables, my_function) If you want more specific advice, asking a more specific question. You're asking about macros because you're used to macros. Try taking some steps back: your data now is in these two variables, but how did they get there? What are you trying to do? If you post some example code with some toy data, I'm sure someone will literally come along and show you how to do it elegantly in R :)
&gt; Could someone please explain how x[-n] works? You might find this code helpful. x &lt;- 1:10 x # Return the full array # [1] 1 2 3 4 5 6 7 8 9 10 x[c(2, 4, 9)] # Return the 2nd, 4th, and 9th elements # [1] 2 4 9 x[c(-7:-10)] # Return all elements except the 7th, 8th, 9th, and 10th elements. # [1] 1 2 3 4 5 6
I'm shocked - maybe the rate of incoming questions have finally overwhelmed the number of responders.
Since you're attempting to be condescending, why not point me to any that discusses macros? Not functions, to be clear.
I’m curious as to why you don’t want to use a function. It’ll accomplish a similar result as to what you described, unless I’m just missing something. 
&gt; Or even if you're doing it a lot more times. I know you said you can't put it into a list or array, but really?? Hey. First, thank you very much for the help. As you point out, the issue is based on how I learned coding (lists didn't occur to me until you mentioned it right now). Are you saying that I create a list as you did, I can run something similar to the macros I bring up? &gt; If you post some example code with some toy data, I'm sure someone will literally come along and show you how to do it elegantly in R :) While I'm about to give lists a try, here's a piece of a code I'm working with. Had it been a language that supports macros, I would have only one block of code instead of the two (in my actual case: 12), and that's what I'm looking for in R or RStudio. bmp("g_valued.bmp", width = 1800, height = 1800) L = layout_with_fr(g_valued) cluster = cluster_walktrap(g_valued) membership(cluster) # affiliation list length(sizes(cluster)) # number of clusters sizes(cluster) plot(cluster, g_valued, col = V(g_valued)$color, layout = L, vertex.label=NA, vertex.size=6) dev.off() bmp("g_mean.bmp", width = 1800, height = 1800) L = layout_with_fr(g_mean) cluster = cluster_walktrap(g_mean) membership(cluster) # affiliation list length(sizes(cluster)) # number of clusters sizes(cluster) plot(cluster, g_mean, col = V(g_mean)$color, layout = L, vertex.label=NA, vertex.size=6) dev.off()
Thanks! I can definitely use functions (I'm not new to R); but I've always been curious about an equivalent of macros in R. I'm asking more in an attempt to learn about whether they exist or not since Google didn't yield any useful results.
&gt; I’m curious as to why you don’t want to use a function. I've worked with languages that support macros, and since Google didn't yield any useful results, I figured I'll inquire here about it. Functions are totally OK to use, but I was trying to learn about whether or not R supports macros - and if yes, how.
Macros are for languages that don't have real functions (closures)
I’m not aware of a macro system, but I feel like functions in R are essentially what you’re looking for, especially since it’s an interpreted language. That said, there is a dfmacro (define macro) function in the gtools package. That may be a place to start. 
It's not being condescending. You just haven't put in the most basic of effort. Literally any intro tutorial will get you started, and there isn't a conversation that can be had until you've done at least that. "does sas import data?" "does sas have functions?" 
Thank you very much. Both your comments were very useful. I'll check the book.
Sounds really cool. Thanks for the reference. I will read it in the coming for the next few days.
Yes but functions and macros are not the same; I've worked with languages that have both.
You have seen them in non-compiled languages that have both?
I’m curious? What language have you used that uses both macros and functions? I’d love to see the different contexts that these words are used. I do agree with the consensus though that functions will do what you are describing. 
You'd think
I'd have to look at your data to help you with the mean problem. The cor command does not use na.rm=TRUE, it uses use='complete.obs' for complete observations. 
Also, for the mean issue, I added the 'na.rm=TRUE' &amp; 'trim' command at 100, since it's a percent and am now getting a number. Does this seem like it would be correct?
Again - couldn't tell you. trim = 100 shouldn't do anything according to the documentation for the mean function. It looks like na.rm=TRUE is the hero here. The documentation says that trim cuts that fraction of the vector out from each end of it before computing the mean.
It's accurate for the cases that are complete, yes. If you have a lot of missing data then you should probably try to impute some, if possible, before computing the correlation.
Very strange, because when I use na.rm only, it doesn't work, but when I add trim it does. Thanks for your help though! Very much appreciated!!
You should probably look at your data more closely to see what may be causing this. Is there a data point that has an Inf value? Check things like that out. try unique() on that vector and it will display some values. Another good way to figure out if there's an Inf value is to attempt something like which( vector &gt; 100000000) - that will return the position of Inf, if any. 
Glad to help. Good luck!
Luckily there's not a lot of missing data (only ~1%).
Think you should still try and impute the data, to learn it for future references
It would help if you explained what you actually mean by “macro”. The example you’ve shown isn’t clear, and would simply be done by functions in R (and other languages). You mention elsewhere that you’ve “worked with languages that have both [functions and macros]“. Well then, which languages are those? Most modern languages don’t have macros, although a few (C, C++ …) have a so-called “preprocessor” which performs simple textual replacements using something called “macros”. The most famous example of a language with macros is [Lisp](http://wiki.c2.com/?LispMacro). But Lisp macros and Lisp functions only differ by whether they evaluate their arguments. In R, functions can do both these things. This means that R functions can do everything Lisp macros can do.
Yes, I think you're correct. We cover that later this semester so I very likely will end up doing that. Thanks!
Try `mean(variable[!is.infinite(variable)])` 
Typically, I use [DiagrammeR](http://rich-iannone.github.io/DiagrammeR/graphs.html) for graph analysis. I've looked at igraph, but I didn't find it as easy to use and powerful as DiagrammeR. I was not aware of tidygraph until now, so I'll take a look at it, too.
Imputing artificially reduces the variance. There's no reason to do it with that little missing.
Data.table has the merge function which operates like base merge. You can use it's arguments all.x and all.y to perform operations like that of inner join, left join, etc. You can use these to compare data in two data frames. 
[This is a great resource on merging dataframes.](https://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html) As for finding rows in/not in the other matched &lt;- which(df.A$column %in% df.B$column) Now you have the rows and can select them, or select all except for them. 
How would I select all except for them?
semi join [https://stackoverflow.com/questions/18969420/perform-a-semi-join-with-data-table](https://stackoverflow.com/questions/18969420/perform-a-semi-join-with-data-table)
In my R environment (R 3.5.1) no extra packages loaded &gt; surveyanswers &lt;- c(1:7) gives &gt; [1] 1 2 3 4 5 6 7 The problem may be related to your R environment.
That makes sense, thanks. So it's just a vector because its a number ? 
No, it‘s a vector, because you define it as a vector starting from 1 to 7 incremented by 1. that‘s what the operator „:“ does.
thank you
Try ggplot(fval, aes(x=dt, y=fdv, col = fund)) + geom_line()
Yes, like this. This "problem" is basically the whole reason we use long-format datasets and ggplot.
Yes. 
That was much easier than I was trying to make it... Thanks!
I think /r/rstats is more active 
I mean, with rnorm() you need to specify a mean and standard deviation. If, let's say you set a mean of 4.5 and a standard deviation of 1.75, then about 95% of the numbers should be between 1 and 8. But there's nothing preventing you from getting numbers in the extreme tails of the distribution -- it's just unlikely. Even with those parameters, though, you'd get numbers outside 1 and 8 about 5% of the time. To echo what /u/endaemon asked, are you sure you want a normal distribution here? If you're absolutely sure, then you will need some process to discard numbers outside the range you are wanting. There is also the "truncdist" package that lets you specify lower and upper bounds on various distributions.
Depending on what you’re doing with the data, I would recommend using lubridate to handle the date time stamps and then check out tidyquant, tibbletime, and anomalize for some methods for handling time series data in a tidy fashion. 
If you wish to copy and paste rows in some simple clicks, then neither R nor Python is for you. You will have to learn to code and I dunno if it is only me, but a true data analyst will need to learn to manipulate the data in either R or Python or a 3rd coding language. Maybe it is time for you to sit down and learn one of these. But maybe Power BI could be something for you I do not know if it fits your needs or if it is not complex enough. I know that Power BI can do the same stuff as excel and is really fast + is really good at plotting the data. What we do at our workplace is we use R to write the code to manipulate stuff where we end up with a final .csv file, that we use in Power BI to plot. The reason we use Power BI is that you can give the dynamical presentation to a client where they can click on stuff without actually getting the data.
R skills are less intuitive and user friendly than excel for many reasons, but R is nothing to fear. Invest some time in it and it will make many parts of you work easier. You can’t point and click, but you can type a few lines out and do everything you want to do. And there’s no more hunting and pecking around the ribbon. I prefer R code to clumbsy excel formulas for sure. And you’re able to work with a lot of data without messing the origins files up. Make the switch
If your budget constraint is not that tight, SAS is fully integrated into Excel afaik. So you have features of a statistical programming language AND the easy-to-use approach you have in Excel (which is handy at times...). Nevertheless: You gotta learn how to code stuff if you want to automate and replicate stuff. Or do more than just basic stuff you can do in Excel.
There is a joy in cracking a coding language that you don't get from copy-pasting. Maybe there are integrated software solutions that fit your needs, like SAS that the other poster mentioned - I personally don't even know what that is. I had one class of R in one semester of my Master's degree and became hooked. It makes you a more flexible and skilled data analyst as well, if you have some coding knowledge - higher variety of graphics to display, broad skills that can be applied elsewhere (working as a consultant, I do some data analysis, but my boss asked me to produce a specific map for a specific project, which I can now do with R, and I'm planning on picking up Python to use with Excel), ability to handle larger data sets (as you alluded to), etc. I'm approaching it as a life-long journey that is already providing tangible benefits. I won't win any hacking competitions tomorrow, but I'll be having fun, adding to my skill set and... well mostly these two things. I also have the good fortune of having a supportive boss, and an office that is somewhat coding illiterate, which makes my value proposition easier to communicate.
You won't go wrong with this as your 'core' guide: [R for Data Science](https://r4ds.had.co.nz/) Plus, don't see it as a substitution for Excel (i.e. as a 'replacement'), R can do many many more things Excel can't do, and these additional things would come in as a 'bonus' for pursuing R. 
Like many have said, R has a learning curve. But the things you can do in two lines of code will make it worth it. Definitely use RStudio. R will also let you make amazing web apps with Shiny that are perfect for making interactive displays of your analyses/models/whatever it is you make. You can also handle much larger datasets in R than in Excel if I'm not mistaken. Excel poops out for me around 100k rows.
Yep, same here. Picked it up in a semester in college, been using it for years. Even if you only learn the basics, R is very powerful compared to most visual tools.
24 \* 30 \* 12 = 8,640 &amp;#x200B; 24 \* 365 = 8,760 &amp;#x200B; The difference in the hours are because the months have differing dates in them. I imagine the diff between 8,766 and 8,760 is because they assumed 365.25 days in a year.
See if there’s any meetup groups of r enthusiasts in your area. Having someone who can help you will make your life a lot easier. 
Check out clipper, clipped, and wtf (write temp file) https://github.com/geneorama/geneorama Those functions let you write to the clipboard , read from it, and open csv files in excel. 
Thank you for your answer! I did not know those other packages. I'll certainly check 'em out. ;)
I about 4 weeks into making the transition. Basically I needed more than what Excel could give me to help communicate ideas to non-data ppl. I'm starting from zero coding experience. You really have to follow a course instead of self-teaching to make the most of your time. Things are coming quickly. Already the things I'm making are being seen and appreciated by my supervisor 
I would encourage you to learn R. However, it won't be any easier than Python. So you might choose to just double down on Python since you've got a head start there.