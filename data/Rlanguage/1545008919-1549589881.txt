R can be thought of as a collection of domains. Each field has its specialties and best practices. While there is overlap, its important to understand what your domain is focusing on. What are you studying and what field to you want to pursue?
I study Maths with Stats, don't know if you're familiar with how the UK system works but it's all mathematical modules, with "a minor" in statistics. No general ed reqs like in the US. My dissertation topic is data analysis and I'm applying for Data analysis roles. I think I know what you're talking about but if you could tell me those specialities it would be helpful. Although I am hoping to be more well-rounded. 
I don't think you want "sum" of errors, what if the actual is BEFORE vs what if it is AFTER? You will then be summing some negatives and some positves. In stats they get around this by squaring each difference, that way negatives and positives count for the same as they're still the same diff from the mean. As the other comment said, regression is just a formal approach to finding the functional relationship between your expectations and the real data you have. Check out more here: &amp;#x200B; [http://r-statistics.co/Linear-Regression.html](http://r-statistics.co/Linear-Regression.html)
Ok I see...well that's fairly general. These days, Data Analysis roles span so many industries and fields, that there are of course generalities, but often times key differences in approaches and coding styles exist between them all. In that regard absolutely learn data.table ([https://github.com/Rdatatable/data.table/wiki](https://github.com/Rdatatable/data.table/wiki)) for intuitive and clean data handling which is among the fastest and most RAM efficient of all R packages. It will generalize anywhere. I'd consider doubling-down on your area of expertise and focus on as much statistics as you can. Learn how to use the MLR package ( [https://mlr.mlr-org.com/](https://mlr.mlr-org.com/) ) for machine learning. For Bayesian statistics check out the [http://mc-stan.org/rstan/](http://mc-stan.org/rstan/). These are both difficult at first, but are valuable. Once you can transform data into the formats required by the function arguments of each package, much work will be done according to the package syntax. So getting familiar with these will probably increase your skills more than spending too much time achieving diminishing returns on the most elegant looking data wrangling approach. Both MLR and rstan have a lot of functionality to learn, reflecting the ambitious scope of each. CRAN has Task Views, which are good way to learn about high-quality packages in each field and identify their authors. Task Views are curated by humans with a lot of experience in their domain so you might find good examples in the work, writings, blogs, etc of those who manage Views you find most relevant. [https://cran.r-project.org/web/views/](https://cran.r-project.org/web/views/) Do you have a local R user group on [meetup.com](https://meetup.com)? If so, I'd go to one and see what people are doing. They may help you get a sense of what approaches are used in various data analysis roles in the area. Also, I'd look up mango-solutions in London...If you call/write them maybe they'll tell you what they find useful. Good luck! &amp;#x200B;
1. I posted the sollution on Stackoverflow. fyi: [Sollution for translateR and Microsoft API](https://stackoverflow.com/questions/53803383/r-package-translater-and-microsoft-api/53814715#53814715)
I gonna check up this course, in order to see if I can enrich in some way my original post. 
I will check the ‚Äúhere‚Äù package. Thank you for sharing! 
Thank you for your comment. Definitely a lot to check and understand. 
What is that title? 
Week, I‚Äôm trying to looking for a relationship between salary of the teachers and SAT scores? Do you think that the word ‚Äúconnection‚Äù is not good? 
&gt;Week, I‚Äôm trying to looking for a relationship between salary of the teachers and SAT scores? Do you think that the word ‚Äúconnection‚Äù is not good? Instead of connection, relationship is typically used in this context of examining two variables. The "teacher's salary teacher" in the title seems like a typo. Is there an extra word that shouldn't be there? Also, when you say SAT scores I would be explicit and say student SAT scores. So overall, I'd consider renaming it to "Is there a relationship between teacher salary and their student's SAT scores". 
Thank you, I will rename the post, your title is pretty clear. 
You should take a look at tidyverse here - [https://tidyr.tidyverse.org/reference/spread.html](https://tidyr.tidyverse.org/reference/spread.html). You can use SPREAD to take data from long format to wide format.
This matrix cannot exist, because the number of columns don't match the number of observations in each column. You can try to use "spread" and "gather" functions from the "dplyr" package in the tidyverse. 
I have been looking for this, you just saved me. Got 24 excel spreadsheets with monthly sales where I need the product number to be the header but not as a column variable. This is the best, thanks again
What are you hoping for here? 1 histogram that contains 2 categories? 1 plot with two histograms? 1 pair of plots side by side? To filter with two groups ``` Df[group == ‚Äùa‚Äù | group == ‚Äúb", ] ``` To get 2 histograms one plot ``` hist(group1) hist(group2, add =TRUE) ``` Two plots side by side ``` op = par(mfrow = c(1,2)) hist(group1) hist(group2) par(op) ``` personally I would highly recommend ggplot2 package for this sort of thing. There is loads of documentation online. Please excuse any poor formatting, I am on my phone right now. 
If you need to summarize() then mutate() you should first ungroup()
/r/titlegore
üòê
the condition to subset should use"&amp;", not ","
Thank you!!
try the [data.table library](https://cran.r-project.org/web/packages/data.table/index.html) 
Interesting. Is this an alternative to tibbles?
Swap out the &amp; for the | and this would work with dplyr. 
== is to do a match, he wants to find not matches which is !=
Simply replace `&amp;` by `|`. This is by distributivity of boolean operations: Check out [de Morgan's law](https://en.m.wikipedia.org/wiki/De_Morgan%27s_laws) for more information.
This doesn't answer the question at all.
For considering visualization possibilities [ggplot cheat sheet](https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf) may be useful, I suppose.
Not a or not b will return true for everything unless a == b.
 network = igraph::graph_from_adjacency_matrix(yourMatrix) plot(network) 
No, it‚Äôs the opposite. In my text, OP‚Äôs ‚ÄúA‚Äù is `Name == 'Smith'` and ‚ÄúB‚Äù is `Brand == 'Honda'`. *That‚Äôs* the conditions OP wants to invert (according to their text, not their code). So we need to invert `! (Name == 'Smith' &amp; Brand == 'Honda')`. OP‚Äôs code attempted this but failed to distribute the negation properly according to de Morgan‚Äôs law.
You said replace and with or. OP had (!= &amp; !=) so I understood your comment to mean (!= | !=).
Yes, and that‚Äôs what OP needs to do. The correct code is `filter(data, Name != 'Smith' | Brand != 'Honda')` (which is equivalent to the conditional in my previous comment).
Yeah, maybe I should have used "data visualization" insted of "graph". Sorry for that. Thanks for the reply!
Do you know what the columns represent?
Convert to dataframe, Melt/gather them to integers, then plot as points or tiles in ggplot. 
You still need to explain your end goal. One can probably create literally any kind of plot based on the data but without knowing what the purpose of it all, it's likely to be unhelpful. It is also courtesy to provide a reproducible data sample. A screenshot is not a reproducible data sample
The gold standards for viz are ggplot2 and shiny. There's some up front learning needed to use both. R tends to get shoved into the stats/ML niche but it works decently as a scripting language in it's own right. You can web scrape and generate simple XML. You can read/write to database. You can parse out large files and render a tkinter gui. Look up the tidyverse and anything written by Hadley Wickham. At the sacrifice of sheer performance, I think data munging can be written extremely elegantly/expressively in R.
Thanks I use ggplot2 a lot and get a lot of compliments on my nifty graphs. Shiny is something I just hear about but don't really know what it does - I will look into it! &amp;#x200B; I did not understand a couple of those other things, but I'll start squirreling :)
A little more exploring and it looks like Shiny will be a great start for me! I'll see what I can cram in over the summer break :) Thanks!
This seems like exactly what I need!!! Now to figure out how to apply it, i kinda suck at reading documentation. Thank you very much!!! 
Wouldn't NAs just fill any missing holes? &amp;#x200B;
https://cran.r-project.org/web/packages/vcd/vignettes/strucplot.pdf
looks good, thank you
R4ds.had.co.nz is the best resource I know of to get started. I read it in little pieces but I'm pretty sure I've read the whole thing
Thank you I have bookmarked it!
My b
It worked perfectly THANK YOU!!!! SOLVED
Just did a quick read of the docs and it looks like centre should be a point which is an x,y coord rather than a single number. ellipse(x, scale = c(1, 1), centre = c(0, 0), level = 0.95, t = sqrt(qchisq(level, 2)), which = c(1, 2), npoints = 100, ...) centre The centre of the ellipse will be at this position.
Is it possible that one or both of your mean() are returning NULL?
Maybe. So this would be because LD1 or LD2 for one Group isn't being computed right, and so vector just has one value? Good thought I'll look into it thanks.
You're a genius? Thanks a lot. Glad we've got logic.
Some suggestions in \[Is there a way to run R on an Android phone? : rstats\]([https://www.reddit.com/r/rstats/comments/3oydnx/is\_there\_a\_way\_to\_run\_r\_on\_an\_android\_phone/](https://www.reddit.com/r/rstats/comments/3oydnx/is_there_a_way_to_run_r_on_an_android_phone/)) They're a bit dated though, these days you can install a full-blown \[GNU/Linux distribution under Android using GNURoot\]([https://www.techrepublic.com/article/use-gnuroot-to-install-a-gnulinux-distribution-on-your-android-device/](https://www.techrepublic.com/article/use-gnuroot-to-install-a-gnulinux-distribution-on-your-android-device/)) and then install R under that to run it. How you install will depend on what distribution you plump for, under ArchLinux it would be along the lines of... &amp;#x200B; pacman -Syu r &amp;#x200B; Under Debian you can follow \[these instructions\]([https://cran.r-project.org/bin/linux/debian/](https://cran.r-project.org/bin/linux/debian/)).
Ok, I'm sorry for that. I'm going to explain myself while I provide a reproducible data sample. I have a corpus data (15 M words) about a political debate and I want to find the co-ocurrence of two terms within, say, 10k words. I create two vectors of positions of two terms: "false" and "law". false.v &lt;- c(133844, 133880, 145106, 150995, 152516, 152557, 153697, 155507) law.v &lt;- c(48064, 155644, 251315, 297303, 323417, 349576, 368052, 543487) Then I want to gather them on a matrix to see the co-ocurrence using the 'outer' function. The positions are taken from the same matrix, so I'm creating a matrix of differences: distances &lt;- outer(false.v, law.v, "-") To make this easier to read let's name them: rownames(distances) &lt;- paste0("False", false.v) colnames(distances) &lt;- paste0("Law", law.v) Okay, so we have the matrix ready. To find which pairs of positions were within 10000 words of each other I just run: abs(distances) &lt;= 10000 I convert this to a dataframe: dataframe1 &lt;- as.data.frame(abs(difference) &lt;= 10000) That's how I created the dataframe you can see in the picture I uploaded yesterday. I converted it to integers with the help of a redditor in this post (thanks /u/jackbrux) and it looks like more useful. Now, I want to create something like a heatmap in order to visualize in a better way the data I'm working with. I hope I have been helpful this time. Sorry for the inconveniences. Thanks for everything! &amp;#x200B; &amp;#x200B;
You can use Termux ([https://termux.com/](https://termux.com/)). A full guide can be found here: [https://conr.ca/post/installing-r-on-android-via-termux/](https://conr.ca/post/installing-r-on-android-via-termux/)
Ok then step 2?
This is an amazing resource. Will be going through it over the holidays for sure. Thanks for posting!
Another thing to consider is you can install R and RStudio on a a free AWS server and access your own R session online from anywhere or any platform
There are a few way to suppress the library startup messages. You could wrap them around a `suppressPackageStartupMessages()` function, but you could also just add the `include = FALSE` chunk option. 
omg, I'm such a dumbass. Thank you
Try using [rstudio.cloud](https://rstudio.cloud) to access a free online environment. 
This is much simpler than my suggestion! 
 my_function &lt;- function(x){ result &lt;- do_stuff(x) return(result) } Check out my data science youtube, y'all
messages = FALSE also works if you want to keep showing the code
Thanks!! Already had termux installed so was easy continue from there!
Use as.Date()
Thank you for your comment.
Use as.numeric() on the point forecast to get rid of the index in your data frame: library(forecast) library(xts) nnfit &lt;- nnetar(y2, lambda = 0) fnnfit &lt;- forecast(nnfit) fcast\_df &lt;- data.frame( Date = [as.Date](https://as.Date)(index(as.xts(fnnfit$mean))), Forecast = as.numeric(fnnfit$mean) )
if the date won't convert you can try as.Date(index(fnnfit$mean))
Thanks for responding. Your first formula failed, and the second one was successful, but it converted everything into a date of 1975-07-13. &amp;#x200B; Any other solutions? Many thanks Oneiricer
You're welcome. I think you should cover the &lt;&lt;- operator and it's use in functions. 
Looks like you forgot to add a aes element and a geom to me.
This is the best tl;dr I could make, [original](https://soco.ps/2SXWMgt) reduced by 84%. (I'm a bot) ***** &gt; With data sizes in TBs, we couldn&amp;#039;t keep a local copy of our data for analysis, so we needed to find a way to directly interact with data stored in the cloud for our scripts without disrupting the current workflow. &gt; We are pleased to share that we are announcing flyio, an open source R package as an interface to interact with data in the cloud or local storage! You can check out the documentation and follow the development on GitHub. &gt; Flyio provides a common interface to interact with data from cloud storage providers or local storage directly from R. It currently supports AWS S3 and Google Cloud Storage. ***** [**Extended Summary**](http://np.reddit.com/r/autotldr/comments/a7x8x8/announcing_flyio_an_r_package_to_interact_with/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 2.02, ~372711 tl;drs so far.") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PM's and comments are monitored, constructive feedback is welcome.") | *Top* *keywords*: **data**^#1 **flyio**^#2 **cloud**^#3 **File**^#4 **storage**^#5
As mkuehn said, you are missing the "Aesthetic" argument aes(), where you set your variables. You also did not provide a geometry. Try: ggplot(data = pf, aes(x = dob_day))+ geom_histogram()
If you look at the documentation, strsplit() returns a list the same length as your input and each element of that list is a character vector of the splits. So, if you're input is `c("a b c", "d e f")` and you're splitting on " ", you get a list of two character vectors: strsplit(c("a b c","d e f"), " ") [[1]] [1] "a" "b" "c" [[2]] [1] "d" "e" "f" If you'd like your result to be a single vector, you can call unlist(): unlist(strsplit(c("a b c","d e f"), " ")) [1] "a" "b" "c" "d" "e" "f" This works even if you're result is a list of one: strsplit(c("a b c"), " ") [[1]] [1] "a" "b" "c" unlist(strsplit(c("a b c"), " ")) [1] "a" "b" "c" &amp;#x200B;
Thank you for the response! How exactly does \[\[\]\] come into play? 
Yeah I did too but I'm not sure if what's the difference between R Cookbook and R Graphics Cookbook 
I bought the graphic cookbook.. as implied it‚Äôs all about making plots and graphics, not on data analysis or anything
Is there a reason this cannot be a function? If you are always doing pairwise searching, your function should take 2 input arguments, worda and wordb. Run this which command on both worda and then wordb. Then run the next few lines to find the occurrences within some distance. 
I can't give you the exact code as I am on mobile now but have a look at the tidyr::gather function https://tidyr.tidyverse.org
What if instead you have a function go through the corpus looking for any of the first words, once it gets there it starts from that word on looking for any of the matches or until it reaches 10k. Save those to a vector and resume. This way you don't search back, only forward, and it goes faster as you go. Hope I explained it well. I'll try to write up a mock I'm just on my phone.
I'm sorry, but I'm not sure if I understand what you're proposing :P. Wouldn't I have to run those commands for every word? In any case I don't really know what commands do I have to write in order to do that :(. Thank you very much for your response!!
I almost always use unlist(strsplit(()) because I have yet to need my split string returned as a list
I was thinking something like this: ``` first_word &lt;- c(‚Äúfalse‚Äù, ‚Äúlie‚Äù, ‚Äúwhatever‚Äù) second_word &lt;- c(‚Äúlaw‚Äù, ‚Äúmoney‚Äù, ‚Äúwhatever‚Äù) matches1 &lt;- integer() matches2 &lt;- integer() for(i in 1:length(corpus)): if(corpus[i] %in% first_word): for (j in i:i+10000): if(corpus[j] %in% second_word): matches1 &lt;- c(matches1, i) matches2 &lt;- c(matches2, j) ``` Note: it's very not optimal but you can make it faster by changing the last part (I just can't remember what you have to do to it). Does this help?
If you like visualizing things, it sounds like it may be easy to identify a few just by plotting the vector densities together using the density function. plot(density(false));lines(density(law)) words.df = data.frame(False = false.v, Law = law.v, ...) plot(words.df[,1]) for(i in 2:length(words.df[1,])){lines(density(words[,i]), col = i)}
Edit: I forgot to add `cbind(matches1, matches2)` To get a nice two column output
Check out R's language definitions [here](https://cran.r-project.org/doc/manuals/R-lang.html#Indexing). The difference between \[\] and \[\[\]\] for lists is that \[\] returns a list of the selected elements and \[\[\]\] returns the element. So, strsplit(stuff)\[1\] returns a list of the vector of characters while strsplit(stuff)\[\[1\]\] returns the vector of characters itself.
I've been trying for a while to make this work with map(). It doesn't seem to work. I do this: temp$AccountName %&gt;% map(QuarterSum(dat,.,1)) As I understand, this should take the list of Account names from my tibble, pipe it into map, which in turn takes each element of that list and applies it as an argument in my QuarterSum function. The result should be a list of numbers for each account. I tried a minimal example in which I created a list with only one item in it and all I get is an empty result: NULL. If I just type QuarterSum(dat, "Company A", 1), it works perfectly. If I pipe it like this: "Company A" %&gt;% QuarterSum(dat,.,1) it works as well. But as soon as I try to pipe a list in combination with map it falls apart.
When you post data, you should post the output of \`print(data)\` or \`str(data)\` or both, so it's clear what kind of data we're dealing with. In your reply you mention using \`purrr::map\` on a list, but I would have assumed that your first example was a data frame. So it's important to be completely clear. Second, you can simplify your code to do one call to mutate: temp &lt;- mutate(temp, Q1 = QuarterSum(dat,AccountName,1), Q2 = QuarterSum(dat,AccountName,2), Q3 = QuarterSum(dat,AccountName,3), Q4 = QuarterSum(dat,AccountName,4) ) Third, you'll need to explain how \`QuarterSum\` works. What is it doing with the values that are passed (don't worry about the arithmetic, just the logic). What does it return?
Could you create a vector like ( 1 2 2 3 3 3 ... 8 8 8 8 8 8 8 8) and then for the function you choose a random element of that vector? 
Is this a homework question? Anyway, look into the ```sample``` function.
This may not be the most efficient method, but it returns my understanding of what you have asked. Note, that I have defined my bias as no bias, but that can be easily changed: roll8 &lt;- function(n) { bias &lt;- rep(x = 1/8, times = 8) Nk &lt;- sample(x = 8, size = n, replace = TRUE, prob = bias) for(k in 1:n) { R &lt;- k if(Nk[k] &lt;= k) break } Nk[(R+1):n] &lt;- NA list("Nk" = Nk, "R" = R) }
That seems pretty straightforward. (It also looks like homework) [Make a dice-roller](https://blog.revolutionanalytics.com/2009/02/how-to-choose-a-random-number-in-r.html), and have it record rolls. Then have it stop when it hits a certain number or below.
looks like you need to: 1) create a function: roll8() &lt;- function() {} 2) define "bias". I did something similar when playing with expected results of die tosses recently. In my case, I chose to define "bias" as an increase in "1s" and corresponding decrease in the max value ("8s"). I implemented this by creating a numeric vector called "probability" where the first item was 1/8+bias, the middle items were 1/8, and the last item was 1/8-bias. 3) since you want your function to decide whether to end itself based on the results of the function, this seems like a while loop to me. 4) The input vs output is a little unclear to me. are they asking for: roll8(trails) which returns "trails" number of R results? If trials = 3, then, "it took 3 rolls to roll a 1", "it took 8 rolls to roll a 1", "it took 1 roll to roll a 1" so the function returns R = 3, 8, 1. Or is it looking for the actual outcomes? When I first read this, I assumed that you needed to create a function roll8() that took no inputs.
Phrasing! I really was thinking something different when I clicked that link.Glad it wasn't what I was thinking when I first hit the link.
I'm really sorry. If you don't mind, in which way do you think that could be better? In any case, thank you. 
What is the proportion of women in national parliaments? ‚ÄúProportion*s*‚Äù refers to a woman‚Äôs body. 
Thank you, I didn‚Äôt know that. Now I feel completely embarrassed... I‚Äôm gonna check every title before to post with a dictionary. 
No worries. We understood what you meant. It probably would not be in a dictionary anyway. 
Thank you! 
Thank you! 
Thanks for this. Here's the whole story: str(dat): Classes 'tbl_df, 'tbl' and 'data.frame: ... AccountName: chr "CompanyA" CompanyB"... City: chr "Boston" "NYC" ... State: chr "MA" "NY" Year: int 2018 2018 ... Month: int 12 11 ... Week: int 50 46 Price: num 50 29.7 ... Quarter: num 4 4 ... &amp;#x200B; QuarterSum looks like this: QuarterSum &lt;- function(Account, Q, Y, dat) { temp &lt;- dat %&gt;% filter(dat,AccountName == Account) %&gt;% filter(dat, Quarter == Q) %&gt;% filter(dat, Year == 2018) %&gt;% summarize(sum = sum(Price)) return(temp$sum) } This function just by itself works correctly, if I type in all the arguments myself for an example. &amp;#x200B; Now I want to create an overview with the end result like this: AccountName Total Q1 Q2 Q3 Q4 CompanyA 10 2 5 2 1 CompanyB 5 1 3 0 1 ... &amp;#x200B; I'm able to create a data frame that contains the first two columns just fine like this: TopAccounts &lt;- function(dat, Y) { temp &lt;- dat %&gt;% filter(dat, Year == Y) %&gt;% group_by(AccountName) %&gt;% summarize(Total = sum(Price)) %&gt;% arrange(desc(Total)) # Until here, it works well. Now I want to add the quarter columns and fill those in. I'll write it out only for one quarter to save space: temp = add_column(temp,Q1 = NA) temp$Q1 &lt;- list(temp$AccountName) %&gt;% map(QuarterSum(., 1, Y, dat)) return(temp) } Since my first post I found that the main problem is passing on the list to my QuarterSum function through map. Even if I pass on a list with one single item, I don't get a result, whereas if I pass on just the string ("CompanyA) without map, then it works. For testing purposes, I rewrote the QuarterSum function to take only one argument (Account) and hard coded year and Quarter into the function. Used like this, I can actually use the `list %&gt;% map()` function. While I may just end up doing this, I'd rather just make it work in this simpler way with multiple arguments, if at all possible. It seems like as soon as QuarterSum() receives more than one argument, the operation fails and map or pipe doesn't know what to do. How do I reliably pass on the list through the pipe as well as the additional arguments without it failing? 
[Have a look at this video from useR](https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/Interacting-with-databases-from-Shiny). Even still, I think Flask or Django would be better suited to what you want. 
Ok, that's a lot clearer now. It seems like your challenge is that you want to create a sum of the Price variable for each quarter and for the entire year for each company. This is actually extremely easy to do, so you're in luck :) But there are a few concepts that will allow your code to be more idiomatically R-ish and tidyverse-ish. Note that that doesn't mean strictly speaking that this is better, just that it's more what was intended by Hadley and the people who designed those systems. The first concept to understand in the tidyverse is the concept of "long format" data. This means that the same metric (Price, in your case) should only appear in one column. Your Total and Q1-Q4 columns are all different transformations of Price, so you have to change things around a bit to get it into long format. I would do something like: |Company|Quarter|Price\_Sum| |:-|:-|:-| |CompanyA|1|2| |CompanyA|2|5| |CompanyA|3|2| |CompanyA|4|1| |CompanyB|1|1| |CompanyB|2|3| |CompanyB|3|0| |CompanyB|4|1| Getting your data into this format is extremely easy: &gt; testing_data &lt;- tibble::tribble(~Company, ~Quarter, ~Price, + "CompanyA", 1, 35, + "CompanyA", 1, 25, + "CompanyA", 1, 37, + "CompanyA", 2, 12, + "CompanyA", 2, 42, + "CompanyA", 2, 2, + "CompanyB", 1, 15, + "CompanyB", 2, 12) &gt; testing_data # A tibble: 8 x 3 Company Quarter Price &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 CompanyA 1 35 2 CompanyA 1 25 3 CompanyA 1 37 4 CompanyA 2 12 5 CompanyA 2 42 6 CompanyA 2 2 7 CompanyB 1 15 8 CompanyB 2 12 &gt; group_by(testing_data, Company, Quarter) %&gt;% summarise(Price = sum(Price)) # A tibble: 4 x 3 # Groups: Company [?] Company Quarter Price &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 CompanyA 1 97 2 CompanyA 2 56 3 CompanyB 1 15 4 CompanyB 2 12 Hopefully none of that is particularly shocking to you. Now if you want to restructure the data, you can use tidyr::spread - &gt; dplyr::group_by(testing_data, Company, Quarter) %&gt;% summarise(Price = sum(Price)) %&gt;% tidyr::spread(Quarter, Price) # A tibble: 2 x 3 # Groups: Company [2] Company `1` `2` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 CompanyA 97 56 2 CompanyB 15 12 And you're basically all the way home, no loops or mapping required. The last part is the Total column. This is going to be really pesky, but this is another tidyverse rule, which is that you should never have multiple aggregations of the same data in the same data frame. It doesn't make sense, in the tidyverse, to have both the Total and the Q1-Q4 variables in the same table, because you can't sum all those columns together. Then there would be double-counting that would screw up anyone else trying to do more analysis on that data. The exception to this rule is that sometimes humans want to read it, and they want to see these things together, so the solution is to keep your aggregations in separate data frames and then combine them in a temporary variable immediately before you make them human-readable. So calculating the Total is the same process, without the Quarter group: &gt; dplyr::group_by(testing_data, Company) %&gt;% summarise(Price = sum(Price)) # A tibble: 2 x 2 Company Price &lt;chr&gt; &lt;dbl&gt; 1 CompanyA 153 2 CompanyB 27 If you simply must get everyone together in one table to show it to a human, then do that right at the end using a join: &gt; company_quarter_agg &lt;- dplyr::group_by(testing_data, Company, Quarter) %&gt;% summarise(Price = sum(Price)) %&gt;% tidyr::spread(Quarter, Price) &gt; company_agg &lt;- dplyr::group_by(testing_data, Company) %&gt;% summarise(Price = sum(Price)) &gt; company_agg %&gt;% left_join(company_quarter_agg) Joining, by = "Company" # A tibble: 2 x 4 Company Price `1` `2` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 CompanyA 153 97 56 2 CompanyB 27 15 12 And you're all the way home, with no need for loops, and you still have your three data frames testing\_data, company\_agg and company\_quarter\_agg, that can be reused safely in further analysis steps. Now I want to reiterate that this isn't an unequivocally better way to do this, but I certainly think it's simpler, having many fewer lines of code, and it's closer to what the R and tidyverse design expects you to do.
You don't need to use a function for this actually. The easier way would be to do I this way: summed_dat &lt;- dat %&gt;% group_by(AccountName) %&gt;% summarize(Total = sum(Price), Q1 = sum(Price[Quarter == 1]), Q2 = sum(Price[Quarter == 2]), Q3 = sum(Price[Quarter == 3]), Q4 = sum(Price[Quarter == 4])) %&gt;% arrange(desc(Total)) &amp;#x200B;
Here's a much cleaner way using the example data you made: summed_dat &lt;- testing_data %&gt;% group_by(Company) %&gt;% summarize(Total = sum(Price), Q1 = sum(Price[Quarter == 1]), Q2 = sum(Price[Quarter == 2])) %&gt;% arrange(desc(Total)) &amp;#x200B;
In my opinion, that's not cleaner. The main reason is because you have to specify the number of Quarter columns. This approach isn't scalable to other types of data, because if this was a column with a large number of values (like the City column in the real data, say) it's unwieldy to specify all the values manually. My approach doesn't require anything to be hardcoded except the column names.
"I'm trying to focus my learning on one language right now" is the start of many follies, and I can't help but think this is one of them. Django is so simple and easy to use, and Python is actually intended for that kind of use. ORM is very much not in R's wheelhouse, so when you started with "I want to learn more about R" and moved on to "I want to have an ORM", you already strayed very far from the pack.
&amp;#x200B;
thank you!
thank you for answering me
I didn't know you could do that! Does the referencing of one column in another have a name? I'm refering to: Price[Quarter ==1]
Hey, TeachUsPlz, just a quick heads-up: **refering** is actually spelled **referring**. You can remember it by **two rs**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
hEy, TeAcHuSpLz, JuSt a qUiCk hEaDs-uP: **rEfErInG** Is aCtUaLlY SpElLeD **ReFeRrInG**. yOu cAn rEmEmBeR It bY **TwO Rs**. HaVe a nIcE DaY! ^^^^ThE ^^^^PaReNt ^^^^cOmMeNtEr ^^^^cAn ^^^^rEpLy ^^^^wItH ^^^^'dElEtE' ^^^^tO ^^^^DeLeTe ^^^^tHiS ^^^^CoMmEnT.
Don't even think about it.
dOn't eVeN ThInK AbOuT It.
Wow, that's very helpful to see. Definitely makes sense and is simple enough. I haven't tried spread before, so that's good to learn. I also appreciate the tip to keep the data separate and join it only when display . It's a good habit to put in place early on. Thanks a ton!
Yep it‚Äôs pretty handy. It‚Äôs called row indexing or row subsetting. It‚Äôs something that dplyr/tidyverse uses from base R that is pretty useful when using the *summarize* function
let me talk it in detail, by the way thank you for answering me if we type assume roll8(3), the result would be: (1) 12345678 R 1 1 (2) 12345678 R 21 2 (3) 12345678 R 234565 6
let me talk it in detail, by the way thank you for answering me if we type assume roll8(3), the result would be: (1) 12345678 \-&gt; 1 R =1 \-------------------------------------- (2) 12345678 \-&gt; 21 R =2 \-------------------------------------- (3) 12345678 \-&gt; 234565 R=6
Yup, looks like a while loop
i just cannot figure it out in the while loop now i finish the random variable parts
Start with Nk = 8. 
Do not fall into the pitfall of using one language for everything. Specifically using R for web apps will get you into trouble because Shiny is somewhat proprietary. The community shiny server quite honestly sucks, and solutions like ShinyProxy are incredibly limited. Spend the time to learn a second language or two. It will expand your ability to think about problems and make you a better developer. Period. R is a very unusual language in a lot of ways and you will benefit from learning a more conventional language. I love shiny apps for prototyping but you _will_ run into serious performance bottlenecks with even a modest number of simultaneous users. If you're new to web development I highly recommend Laravel for PHP (great ORM and templating in an MVC pattern, great way to learn OOP, great documentation and lots of YouTube tutorials). Otherwise you could try Node.js and something like Express + Jade, but in my experience Laravel is a much cleaner and more opinionated ecosystem.
may i ask another question? yes i successfully got the R next if I want to plot PMF, it means that I have to know the numbers of R in different value how can I do?
PMF?
What I found works for me is to google the phrase sometimes in quotes to get the exact match. Generally if it's grammatically correct it'll get a lot more exact hits. For instance 'before to post' does not get a lot of hits for that exact phrase, usually before and post being located in different parts of the sentence. Whereas 'before posting' gets a lot of exact hits for that phrase in context. Hope this made sense.
Thank you! That is an excellent idea to look for the most common sentence. I will start to use your strategy. 
Good luck!
I don‚Äôt really agree, shiny can be scaled (https://www.rstudio.com/resources/videos/scaling-shiny/) . And if I‚Äôm not wrong, it does have an open source option (limited, but workarounds exist). It just takes some time (including for ORM) It really depends on your skills, budget and time. If you are lucky enough to know every language, sure, take the best out of each. If not, play with the cards you have in your hand. I don‚Äôt think there is a good or bad solution. It‚Äôs a tradeoff between learning a new framework and find workaround in your comfort zone. 
That video is talking about scaling to 10,000. I'm not saying it can't be done, I'm saying it's time consuming and expensive. Scaling a PHP or Node.js app to 10,000 users is almost not even a consideration, the code doesn't really need to be changed and you can host it dirt cheap. I'm not suggesting anyone learn every language, just that OP already knows Ruby on Rails and will probably have a much more performant app if they just use that. Use the best tool for the job especially if you already know that tool.
It's the piping of `gpa` into `save` that's making this not work. If you use `load("data/gpa.rdata", verbose = TRUE)`, you'll see that it loads the `gpa` into the environment as `.`. I'm not totally clear on how `%&gt;%` works under the hood to cause this behavior, but I think it's considered bad practice to pipe into `save` in any case.
This is happening because `save` does some special stuff under the hood to learn about the names objects have and some other bits. It can't get the right name for `gpa` because `%&gt;%` masks that name from it. You can see this yourself by looking at the source code of `save` and checking out this line: as.character(substitute(list(...)))[-1L] You can do: &gt; gpa %&gt;% (function(...){as.character(substitute(list(...)))[-1L]})() [1] "." This is the part that is "incorrect" from your perspective. Piping with base functions isn't usually a fantastic idea in general.
&gt; Piping with base functions isn't usually a fantastic idea in general. Nonsense, there‚Äôs generally nothing wrong with that. You just need to be aware of, and be careful with, non-standard evaluation (as in this example).
The `%&gt;%` literally passes the object on as `.`.
 parent &lt;- lag(object) The lag() function does just what you‚Äôre asking. I‚Äôm on mobile so I can‚Äôt link the docs at the moment. 
 data &lt;- tibble( object = LETTERS[1:5], depth = c(1, 2, 3, 2, 3) ) data %&gt;% mutate(row = row_number(), key = depth - 1) %&gt;% left_join(data %&gt;% rename(parent = object), by = c("key" = "depth")) %&gt;% group_by(object) %&gt;% filter(row == min(row)) %&gt;% select(object, parent)
Hey mate, thanks for your reply. I think I understand most of it. The output isn't quite right though. The join is returning multiple records per row because it's joining on all possible depth -1 instead of the nearest one above it. Your example outputs this: # A tibble: 7 x 2 # Groups: object [5] object parent &lt;chr&gt; &lt;chr&gt; 1 A NA 2 B A 3 C B 4 C D 5 D A 6 E B 7 E D Which works in the case of B, C, and D if we're grabbing the first result, but not in the case of E because the correct parent is D. When I tried to run this over my data I had to kill it `Error: cannot allocate vector of size 1.3 Gb`. &amp;#x200B; Things I'm not sure about are: * How have the rows duplicated if it's a left join? * Why are we filtering on min(row)? Should be max row where not greater than object row (to get last above). &amp;#x200B; &amp;#x200B;
I think I get what you mean. Lag the depth column so that it can't match on rows below. So in the apply we're lagging by `row - nrows - 1` so that the first row lags the entire vector and each row down lags progressively less. This would mean that row 24 could match on 1:23. Is that what you had in mind?
I think (or hope) my most recent edit fixes this - try running it again, I'd made a minor error. Rows can duplicate with a left join if there are multiple matches - so here I filter to only keep the relevant row. I'd suggest running the code bit by bit, including an extra pipe each time to get a feel for what's going on. data %&gt;% mutate(key = depth - 1) %&gt;% left_join(data %&gt;% mutate(row = row_number()) %&gt;% rename(parent = object), by = c("key" = "depth")) Returns the following: # A tibble: 7 x 5 object depth key parent row &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 A 1 0 NA NA 2 B 2 1 A 1 3 C 3 2 B 2 4 C 3 2 D 4 5 D 2 1 A 1 6 E 3 2 B 2 7 E 3 2 D 4 For each object, I think the correct match is the one with minimal row. So grouping by each object, and keeping data where row is minimal does the job (I think)!
Okay, yep. So I think the last thing to do is get this filter right. In the case of E down the bottom, if we filter on min then it is telling me the parent is B, but B is further up in a different branch of the hierarchy. The correct parent for E is D. To complicate this, if we add another object child of F at depth 2 then E will have three matches (rows 2, 4, and 6). Since E's row is 5, the filter should get rid of anything greater than 5 and return the max of that. &amp;#x200B; # A tibble: 10 x 5 object depth key parent row &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 A 1 0 NA NA 2 B 2 1 A 1 &lt;- Parent 3 C 3 2 B 2 &lt;- Parent 4 C 3 2 D 4 5 C 3 2 F 6 6 D 2 1 A 1 &lt;- Parent 7 E 3 2 B 2 8 E 3 2 D 4 &lt;- Parent 9 E 3 2 F 6 10 F 2 1 A 1 &lt;- Parent &amp;#x200B;
Ah I see. I should check my output more thoroughly - I thought that rogue B was a D. Does this work? data &lt;- tibble( object = LETTERS[1:6], depth = c(1, 2, 3, 2, 3, 2) ) data %&gt;% mutate(row = row_number(), key = depth - 1) %&gt;% left_join(data %&gt;% mutate(row2 = row_number()) %&gt;% rename(parent = object), by = c("key" = "depth")) %&gt;% group_by(object) %&gt;% filter(row2 &lt; row &amp; (row == min(row) | is.na(row))) %&gt;% select(object, parent) &amp;#x200B;
I know this isn't really related to your question, but you should really look into using vectorized operations in R versus your for loop solution. While it may not make a difference on this small of a data set, it will for larger data. Here is what I mean: ``` library(plyr) library(datasets) library(microbenchmark) data("iris") iris$Sepal.Length size &lt;- c() for (s in iris$Sepal.Length){ if (s &lt; 5.8){ size &lt;- c(size, "SMALL") } else if(s &gt;= 5.8){ size &lt;- c(size, "LARGE") } } iris$Size &lt;- size microbenchmark::microbenchmark( for_loop = {size &lt;- c() for (s in iris$Sepal.Length){ if (s &lt; 5.8){ size &lt;- c(size, "SMALL") } else if(s &gt;= 5.8){ size &lt;- c(size, "LARGE") } } iris$Size &lt;- size }, vectorized = { iris &lt;- iris %&gt;% mutate(size = ifelse(size &lt; 5.8, "SMALL", "LARGE")) }) ``` ``` Unit: microseconds expr min lq mean median uq max neval cld for_loop 4550.910 4868.673 7638.7170 5290.715 7008.237 157554.243 100 b vectorized 228.265 270.140 362.4392 342.396 399.873 998.449 100 a ```
 (*just for readability* ) I know this isn't really related to your question, but you should really look into using vectorized operations in R versus your for loop solution. While it may not make a difference on this small of a data set, it will for larger data (it's also much more concise code). Here is what I mean: Also, you may want to check out the vcd package, but I don't think it is necessarily "ggplot" style. library(plyr) library(datasets) library(microbenchmark) data("iris") microbenchmark::microbenchmark( for_loop = {size &lt;- c() for (s in iris$Sepal.Length){ if (s &lt; 5.8){ size &lt;- c(size, "SMALL") } else if(s &gt;= 5.8){ size &lt;- c(size, "LARGE") } } iris$Size &lt;- size }, vectorized = { iris$Size &lt;- ifelse(iris$Sepal.Length &lt; 5.8, "SMALL", "LARGE") }, times = 100) Unit: microseconds expr min lq mean median uq max neval cld for_loop 4559.532 4749.203 5277.18311 4894.9475 5230.9805 12969.989 100 b vectorized 58.709 64.251 80.38145 79.0305 82.9305 240.171 100 a 
thanks, seems reasonable. 
This might be of interest too: https://cran.r-project.org/web/packages/ggmosaic/vignettes/ggmosaic.html
thanks, can I pass a table object to that and generate plots within a loop? 
I have no idea. 
with the `ifelse`, what's the approach if there are more than two categories there? Say I wanted to split into small/medium/large/huge , or something. 
Rather than nesting ifelse() statements, it‚Äôs best to use case_when()
Never knew about case_when... How does it differ from switch?
No I think you will likely need to do a ggplot2 solution BUT you can make a ggplot2 as an output of a function and iterate over that function. 
It's not a nested ifelse. 
This.
Check out the formatCurrency function for the datatable library. This will maintain numeric values and the formatting will not alter the underlying column datatype (right now you‚Äôre sorting strings).
data.table::formatCurrency is probably the better option but FYI you can use [column rendering](https://rstudio.github.io/DT/options.html) to format the column in Shiny instead of formatting the values by replacing the column in the actual data frame.
crplot in car package would be helpful for partial residual plots. 
Any question like this is almost always solved by doing `str(data)`. You should provide the output from this with every such question, and 90% of the time you'll find the solution yourself. In this case, you'll find that your currencies are formatted as strings, because they have to be to include the $ character.
Great job and effort! Thanks!!! I was planning to use it for geological simulation What do you think?
Maybe [https://bert-toolkit.com/](https://bert-toolkit.com/) but I've never used it. I saw someone mention it once somewhere, so it came to mind when I read your question.
It depends on your data and your objective. This package will take any (expensive) function that: 1) Takes an input and 2) produces an output which you would like to maximize The typical use case is hyperparameter optimization, but this is a flexible package that allows you to specify the function being optimized. I don't know exactly what your simulation would look like, but if it fits the criteria above, the package would probably be very useful. As a side note, the bayesian optimization performed by this package is called "[Kriging](https://en.wikipedia.org/wiki/Kriging)", and it's first use case was to determine the most likely locations for high concentrations of gold based on geological surveys :)
**Kriging** In statistics, originally in geostatistics, kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process governed by prior covariances. Under suitable assumptions on the priors, kriging gives the best linear unbiased prediction of the intermediate values. Interpolating methods based on other criteria such as smoothness (e.g., smoothing spline) need not yield the most likely intermediate values. The method is widely used in the domain of spatial analysis and computer experiments. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
In statistics, originally in geostatistics, kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process governed by prior covariances. Under suitable assumptions on the priors, kriging gives the best linear unbiased prediction of the intermediate values. Interpolating methods based on other criteria such as smoothness (e.g., smoothing spline) need not yield the most likely intermediate values. The method is widely used in the domain of spatial analysis and computer experiments. The technique is also known as Wiener‚ÄìKolmogorov prediction, after Norbert Wiener and Andrey Kolmogorov.
Have you tried walk in purrr?
[https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk\_test#Interpretation](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test#Interpretation)
have u used this with h2o before? how does it compare to their hyperparameter optimization?
Read Excel with readxl package, write with writexl package. Run R scripts from Excel by Rscript started from VBA. 
h2o has grid search and random search functionality, but it does not perform Bayesian optimization, [which has been shown to be much quicker](https://arxiv.org/pdf/1206.2944.pdf). I have never used h2o, however I *have* used caret in the past (and I believe they're pretty similar, correct me if I'm wrong), so I can speak to the differences there. Caret has a common API for a pretty wide range of model types, so if your goal is to compare many different model types to determine the best one, then you would want to use these packages. However, condensing all of these models into a common API creates considerable limitations. Caret and h20 cannot perform Bayesian optimization, and they do not always have the parameters you want to tune (see xgboost). They also may not have the package appropriate for the project (Keras/TensorFlow). Finally, the models themselves are limited inside the package API. The beauty of xgboost is its wide array or objective functions - only 2 or 3 of these are available through the caret API. I'm not trying to say that Caret is a bad package - I have used it multiple times at work, it's very slick for certain use cases. I'm simply naming some of the limitations I have run into which have forced me to change my approach and go with another solution. ParBayesianOptimization is a general optimizer of expensive functions. It will work with whatever function you feed it, which means that you can use any of the functionality that would normally be available to you in the package you are using. The downside to this is that there is no slick API, you need to define the scoring function (and multiple other things) yourself. Also, caret and h2o have pretty cool model diagnostic and comparison/plotting tools available to them. At the end of the day, it depends on what you want out of your project. If you have any other questions, let me know!
&gt;The typical use case is hyperparameter optimization, but this is a flexible package that allows you to specify the function being optimized. I don't know exactly what your simulation would look like, but if it fits the criteria above, the package would probably be very useful. sorry noob here. Can you please give me an example of ''hyperparameter'' ? I know the ''parameter'' which are, for example, the Mean, the median, the standard deviation, the slope of a regression line, and so non.... but what are the hyper-parameters? If you would make a ''visual'' example (even with one pic) how would you explain them to me?
What is the purpose of the plot? Always start there. The point of a plot is to convey a message or understand some data that is too complex or large to digest with a table or some text. Without knowing your purpose, there's not really any way to know if changes to your plot are better or worse.
thanks, what about the method of plotting? As in the actual code , does that look messy? I wasn't sure if I was taking a bit of a long way around a few things, but I wasn't sure of anything better. The categories are nominal, yes. Perhaps I'm misleading the interpretation by having a gradient like I have. Thanks. 
The main issue with this sort of stacked chart is that if the category in which the groups differ is near the bottom, it makes the two groups appear to be different in *every* category because an eye shifting horizontally from group to group will see mismatched boundaries. Stacked bar charts are good if you want to emphasize the difference between two groups whose composition varies in one or two aspects. In that case, make the top components the ones that differ between the two groups and the bottom ones the ones that stay the same. This presentation makes it easy to point out the areas in which two groups are similar (portions of the bar where an eye scanning horizontal will see little difference between groups) and where they are different (where a horizontal scan detects group differences). As an alternative, try a grouped bar chart, with a group A and group B bar for component 1, a group A bar &amp; group B bar for component 2, etc. This makes visual group comparisons using basic perception faculties less likely to mislead. 
Your impetus to make a data visualization should start with a question. A good question can help you determine whether or not your plot is getting the job done or if you would be better served with a different approach. For the code, you might look into the ggsci package, which has nice color themes that are academic journal publication quality. You might also look into cowplot, which offers a number of features to again help with making publication quality plots. 
&gt; Your impetus to make a data visualization should start with a question Thanks, I agree. I've taken that away to try and focus on the visualisation, but now realise it's a pretty inseparable part (hence yourself remarking on it). Yourself and others have put that question at the forefront a bit more for me though. Thank you for the library recommendations. 
ggplot is generally for exploratory analysis. If your code outputs the plot correctly, I wouldn't stress out about it being "messy"
&gt; The main issue with this sort of stacked chart is that if the category in which the groups differ is near the bottom, it makes the two groups appear to be different in every category because an eye shifting horizontally from group to group will see mismatched boundaries. Yeah, this is something that I'm wondering actually. If I have 2 categories ( so it's like a two way table ) then it seems to *feel* better. But when there are three it feels like I don't really have such a grasp at a glance, because the middle section is floating I guess. &gt; As an alternative, try a grouped bar chart, with a group A and group B bar for component 1, a group A bar &amp; group B bar for component 2, etc. This makes visual group comparisons using basic perception faculties less likely to mislead. Do you have a link to something that you're thinking of there please? Anything of google images or whatever will do, I just want to make sure that I'm interpreting what you're saying correctly. I think that you're suggesting that I would have the same setup as there is here, but with 3 bars in each of Yes / No. [Like this](https://i.stack.imgur.com/ZkvI1.png) My concern with this is that It feels like I lose the ability to compare between Yes / No there. Comparing the different levels of the factor (A,B,C) within Yes / No is easier like that, but perhaps it's harder comparing between Yes / No. Or... perhaps I'm just overthinking it a bit too much. 
My advice would be to keep it as simple as possible. I don't know what the purpose of your plot is but you are probably overthinking it. Generally, the purpose of a graphic is to convey information clearly. Usually, the best plot settings to achieve that are the default ones because they were designed by professional designers and people looking at your plot are likely to be familiar with that. Don't reinvent the wheel. &amp;#x200B; Also, your y axis is incorrectly labelled. Those aren't percentages.
OK all good, if there's nothing glaring then that's fine. Just wondered whether I was missing out on some gg-jazz everyone else was using. Thanks
 Re the y axis, can't i state from this graph that just over 50% of yes were C? I'm missing something obvious here... Having it labelled as it is (with the numbers just going all the way up the axis ) feels a bit wonky, the label should be proportion though I think. thanks 
A percentage would be 50%, not 0.5
That linked plot is a great example. The best presentation depends critically on the message you want the plot to convey. Decide if group or category comparisons are the more important part of the presentation and adjust accordingly. 
Ok cool ( you replied in the thread rather than to the comment, not sure if that's worth knowing or not ). What you say makes sense though, thanks
I think a question, like you were asked above (what is the purpose of the plot), is most important. For instance, stacked bar charts are generally used to look at differences in distributions (kind of like a pie chart; meaning that it's easy to tell when groups are highly different it relatively even, but it's hard to see differences that are more marginal). Bar charts where the bars are side by side are usually used to look at differences between groups, and with some axis adjustments, they are usually better at illustrating the direction (high v. low). Again, either plot is technically right, it's just a matter of what you want the take-home message of the plot to be.
Your choice of three shades of blue would be better for a variable that represents a scale ( such as low , medium, high), rather than categorical (strawberry, banana)
My question is usually two fold: what % of the sample are A, B, and C? And what is the "Yes Rate" (% of all As that answered yes) for A, B, and C? Usually comparing it the way you did hides differences in Yes Rate, since the samples for each category across yes and no are so similar. And by keeping sample size per letter in mind it prevents you from over analyzing outlier performances with small sample. All you see here is that C is most popular, but you could tell that with a univariate analysis. 
That‚Äôs Reddit mobile for you! 
Can you try using paste? Also, you should look in to ggplot and facet_wrap or facet_grid. That's what I prefer to use, but if this is how you want to do it then that's fine too. ``` datafile &lt;- list.files(pattern="bam.quality20.bam.coverage_2.txt") par(mfrow=c(3,3)) for (i in datafile) { data &lt;- read.table(i) plot(x=data$V5, data$V6, xlim=c(162285,162598), ylim=c(0,100), main=paste("US ", i), type = "l", ylab="Coverage", xlab="Base pairs" ) } ```
I don't have much experience with this but perhaps parameters in RMarkdown or RStudio Connect? https://rmarkdown.rstudio.com/lesson-6.html https://www.rstudio.com/products/connect/
Was able to build a functionality as that with rmarkdown and LaTeX. Great results. 
https://yihui.name/knitr/
(FinanceChannel == Channel | is.null(Channel))
I'm any other language I would set a default argument like Channel=NULL And then test for this: FinanceChannel == Channel | is.null(Channel) BUT I'm pretty sure that if Channel is null, R evaluates this as null | False -&gt; null, which probably breaks the rest of the statement. So instead use NA instead of null and test using is.na()
I'm not an expert, but I would just duplicate it with an if-else statement using is.null() on Channel, or have a default for channel that is something along the lines of "unknown" or "no entry"
It is not a good practice to call an object from global environment (premLossData) directly from within the function instead of passing it as arguments. &amp;#x200B; But for now, you can set default value for your function argument, which will be used if no values are passed for that particular argument while calling the function: CountSale &lt;- function(beginDate, EndDate, Channel = premLossData$FinanceChannel, rateFactors, grain){ salestat &lt;- premLossData\[PolicyTermStartEffDate &gt;= beginDate &amp; PolicyTermStartEffDate &lt; EndDate &amp; TransactionType == 'New Business' &amp; FinanceChannel == Channel , lapply(.SD,whichCountFunction) , by = rateFactors, .SDcols = wp\_cols\] } Also consider using 'with'. See ?with in Rstudio.
yeah, it gets a bit into graph for the sake of graph territory I suppose. Thanks
Don't leave me hanging! Tell me those six numbers at least! Let's see if there is anything interesting in this data! % of total that is A, % of total that is B, % of total that is C, % of As that answered Yes, % of Bs that answered Yes, and % of Cs that answered Yes. 
RMarkdown and YAML headers is your easiest answer I think. This is all possible Thanks to knitr
WACK! But I will take a look at it sometime in the future. Thanks for the heads up though
Looks very nice for a first release. A couple of questions: 1. What's the backend for the parallel execution dependency? The following requirements usually don't apply when one's using `parallel` with the `fork` type of cluster: "FUN must be executable using only packages specified in packages (and base packages)", "FUN must be executable using only the the objects specified in export" 1. Sometimes it's undesirable to have random files being written by an analysis. Could you make an option to return the intermediary results instead of the current `saveIntermediate` option? Or is it just the same as the `ScoreDT` element of the returned list? 1. If the `leftOff` parameter provides enough evaluations, are there new random evaluations still performed? In other words, is `initPoints` inclusive of the `leftOff`-provided evaluations? 1. I usually like to just run hyperparameter optimization as long as I can. I don't want to pre-specify the number of evaluations, and instead stop the search process manually. For example, sometimes I have a `file.exists` check for a file after every iteration, and `touch` the file when I want the process to stop. Is it possible within your package? It could be as simple as providing a callback parameter after each evaluation, returning a boolean whether to stop the loop. 1. Are custom kernels possible? Any predefined choice that works well with a mixed set of continuous and discrete variables would be nice too. Thank you!
Thank you for the suggestions /u/Liorithiel! 1) Not sure what you mean by this, but the process works by setting up a foreach %dopar% - instead of sending the entire global environment to each R front end, you can specify just the things you need. Also, as far as I know, packages must be specified in the .export option in order to be loaded into each front end. The export parameter is passed to the foreach .export parameter. 2) You are right, it is not the most elegant solution to read to an RDS. This is not written anywhere in the documentation, ScoreDT can actually be used as a leftOff set. I will update the documentation. 3) The process will not repeat parameter sets, if you provide a leftOff table, a GP will be fit to it and the process will sample the new points. If it cannot find any points that haven't already been sampled after 100 tries, it will quit. This usually only happens if you have all integer parameters. 4) Fascinating, I was playing around with different early stopping ideas, but this never occurred to me. I do think this could cause problems - stopping a multicore process manually is never a good idea, what would you think about setting a time limit? If the process has been running more than x seconds, quit gracefully? 5) This package uses GauPro to fit the Gaussian process, which does not support custom kernels. Currently, kernels specifications are passed as strings so that GauPro does not have to be loaded to use the package. 6) If you would like, I can send you an Rmd Kernel I wrote for the kaggle Housing Prices competition which takes \~20 minutes to run on my Ryzen 5 1600. Efficient is difficult to answer - however I have found that running (for example) 12 xgboost models in 4 parallel threads (nthread = 1) with %dopar% is much, much faster than running 12 sequential models in 4 threads (nthread = 4) using normal xgboost parallelization. I have a personal theory that this is because xgboost has to constantly schedule the split searches to the cores, however I haven't gotten around to testing it. &amp;#x200B; One other problem I have run into is reproducibility. I plan on adding a seed parameter on the next update. As of now, you can get reproduceable results by using an initGrid parameter, and setting a seed in the scoring function. Thank you for your suggestions, I look forward to your response!
Please post a reprodeceable example
Try the [strsplit](https://www.rdocumentation.org/packages/base/versions/3.5.2/topics/strsplit) function in R. You may have to do several iterations where you break text apart by various characters (space, comma, colon, period, etc). I'd also advise, in future surveys, creating a form where participants enter each word individually (line by line) instead of one text box.
could you be more specific
Easy way : use excel -&gt; Data -&gt; select column -&gt; click split column -&gt; select the type of delimiter you want to divide with . ( tab / space / ; ) . Whola. You are done 
strsplit can take regular expression commands, so you can set 'sep' equal to something like: strsplit( dat, '[, ]') such that it will split by anything in the brackets. [source](https://www.rdocumentation.org/packages/base/versions/3.5.2/topics/regex) 
I've done that in excel but I want to do it in R to make my data cleaning/analysis more reproducible.
Ok ,. Then try . Strsplit . / gsub 
&gt; I'd also advise, in future surveys, creating a form where participants enter each word individually (line by line) instead of one text box. I had the exact same thought when I opened the data - I should've thought of this beforehand!
If we don't know what your 'data' is, we can't reproduce the error or troubleshoot it.
If you are familiar with dplyr, this should be what you are looking for I think: https://tidyr.tidyverse.org/reference/separate.html
There's a tidyr function for this.
Since there may not be the same number of words in each cell, you may need separate_rows() instead. It‚Äôs an extremely useful function! 
Ooh, this looks like a useful function! Thanks, it'll probably simplify some of my work. For some reason OP specified splitting the entries across columns, not rows. But rows probably make more sense, depending on how the data is structured.
Yeah exactly. I have seen some people ask to split across columns, but only because they were going to gather this columns into rows right after haha. Splitting into columns doesn‚Äôt actually work very well (at least it has unpredictable behavior) when you have different number of separators in each cell. 
What does your input file look like? If you have your "problem column" as a single text file, with each line something like cat, dog, ham mouse gerbil horse pig then why not: Data=read.table(file="myfile.txt", sep=" ", header=F, stringsAsFactors=F, fill=T) Data=gsub(",", "", Data) If you need to do prior data processing, I personally am not averse to pulling in a file with readLines(), isolating the field I want to split, and using write.table() so I can read.table() the data back in.
Sorry about that, the 'data' is representing a dataframe of 2575 observations to 2082 variables &amp;#x200B; ||Predictor (1 or 0)|v1 |v2|v3|v4....v2081| |:-|:-|:-|:-|:-|:-| |1|1|0|0|3|...| |2|0|0|8|0|...| |3 (follows sequentially to 2575)|1|1|0|2|...| &amp;#x200B;
I think your suggestion is the best one. Regular expressions were made precisely for solving problems like this.
&gt; 1) Not sure what you mean by this, but the process works by setting up a foreach %dopar% - instead of sending the entire global environment to each R front end, you can specify just the things you need. Also, as far as I know, packages must be specified in the .export option in order to be loaded into each front end. The export parameter is passed to the foreach .export parameter. Thank you, that answers my question. The `fork` cluster has the unique feature that the `.export` parameter is usually not necessary. Specifying in your documentation what you wrote above would be useful for practical uses. &gt; 3) The process will not repeat parameter sets, if you provide a leftOff table, a GP will be fit to it and the process will sample the new points. If it cannot find any points that haven't already been sampled after 100 tries, it will quit. This usually only happens if you have all integer parameters. That's very reasonable. &gt; 4) Fascinating, I was playing around with different early stopping ideas, but this never occurred to me. I do think this could cause problems - stopping a multicore process manually is never a good idea, what would you think about setting a time limit? If the process has been running more than x seconds, quit gracefully? I usually just let the last iterations to finish (a "graceful" stop) instead of killing them, in my case they're usually short enough that waiting, let say, additional half an hour is not a problem. Strict time limit would still be not what I mean‚Äîmy personal workload is a bit unpredictable. &gt; 5) This package uses GauPro to fit the Gaussian process, which does not support custom kernels. Currently, kernels specifications are passed as strings so that GauPro does not have to be loaded to use the package. I see. So any potential new kernel would need to be implemented in GauPro. Not a bad idea, as it would then be useful for other applications‚Ä¶ &gt; 6) [‚Ä¶] Efficient is difficult to answer [‚Ä¶] TBH, I was just asking for the time to fit the GP model itself. I have some bad experience with an old library which took more than a minute to fit a GP model to 150 data points‚Ä¶ more than an evaluation of the optimized function.
This isn't an exact answer, but tm package, text mining should have a function doing it. 
Some form of splitting them in to tokens. See https://www.tidytextmining.com https://www.rdocumentation.org/packages/tidytext/versions/0.2.0/topics/unnest_tokens
This is a great answer. To add a few details, since you didn‚Äôt validate on input (eg force the user to is wonky letters, spaces and commas) you will have to a bit more work. I recommend tokenizing the words (splitting them) base on any character that is not a letter or an apostrophe. You will need a little regex to do this. 
The other answers aren't bad, but overthink it a little. Regular expressions have built-in functionality for identifying words. library(stringr) str_count(input_strings, '\\w+')
I think that function does some of that "out of the box". I'd recommend trying it with the default function call first and then seeing if anything else needs to be tweaked. My experience is that it will take care of what you are saying automatically. 
True, it catches ‚Äúmost‚Äù characters that would define a word. 
Single string: myString &lt;- ‚Äúhello world‚Äù length(strsplit(myString)[[1]]) &gt;&gt;&gt; 2 Character vector: myString &lt;- c(‚Äúhello world‚Äù, ‚Äúhello‚Äù, ‚Äúfoo bar world‚Äù) sapply(myString, function(i) strsplit(i)[[1]]) &gt;&gt;&gt; [1] 2 1 3
It really depends on the function and how you intend it to be used. Personally I would say that writing a function to perform the same operation on multiple fields is quite a bad idea. You should write a function that performs the operation for one field, and then use an apply function (or dplyr's mutate, or many other options) to go through them all. Your proposed function then becomes a very thin wrapper over apply or mutate and isn't really worth the effort to create. Your users are better placed to decide how they want to map your function to their data. If you simply must do it, is your intention that users will use it more in interactive mode, in simple scripts, or as part of other functions in a complex program? Do you want them to have to stringify the names or to be able to use bare names?
What does \\\\w+ mean?
How would this handle a list with commas? As in "chair, book, key"
I was assuming there would be a space either way. I suppose you could do a `gsub` to replace commas with white space first, then run it. Double whitespaces may return as zero-length characters, so you could add a nested sapply to remove all results from `strsplit` that are `nchar` == 0. 
Regular expressions are a mini-language for parsing strings. In this case ```\``` is an escape, because otherwise R doesn't pass on backslashes inside strings, the next ```\``` indicates that we want to match on a special value. ```w``` tells the parser that we're interested in words and ```+``` makes it count any occurrence of one or more. You can find more about regexes and stringr in [the docs](https://stringr.tidyverse.org/articles/regular-expressions.html).
what button?
Which R IDE are you using? Are you talking about the import dataset button on rstudio?
Yes! 
I'm in rstudio! 
don't use that button write code to import data, it's a one line command.
It's much faster for me to use the button because it let's me import a file by browsing my folders and selecting the ffil. My data files are scattered throughout different folders. If I type the code I'd have to type a long file address. 
Also, if we're insisting on reading in Excel files, I tend to prefer the "readxl" package from tidyverse, which uses the command `read_excel` to read in data from .xls or .xlsx files.
What's the benefit of using xls xlxs files over csv files (besides the benefits of the tidyverse)?
You can write code to get it to ask you to select a file https://www.r-bloggers.com/r-choose-file-dialog-box/
There is none. In my experience, R (and really most programming languages) handle .csv files much, much better. It's really not good practice to keep data in .xls or .xlsx files - if it's a single sheet of data, save it as a .csv. Now, having said that, sometimes you are just given .xls or .xlsx files with data to be analyzed. If it's only 1 or 2 sheets, I will sometimes go in and manually convert them to .csv. However, sometimes you'll get dozens to hundreds of sheets - in this case, I prefer to just deal with the Excel format using `read_excel()`. TL;DR - always use .csv when possible. Sometimes it's not practical to convert dozens of sheets to .csv, so use `read_excel()` in those cases.
On my version of Rstudio, the Import Dataset button gives me a drop-down menu that lets me pick between importing from text, excel, or from other statistical languages. Same with File -&gt; Import Dataset. Does it look like that on your end?
If you know the path to the file, I think typing is nearly as fast. RStudio uses tab-complete, so you could effectively get to the file by just starting with `C:path/to/top/directory` then using tab to pull up all the options, navigate with arrows, enter to select the next subdirectory, tab again, etc. until you get to the file. Or, if this is something you'll be doing routinely, I'd suggests reorganizing your directories with that in mind. You could even throw all the files into the same directory if that makes it easier.
then you should select From Text (readr) instead of From Excel. &amp;#x200B; Btw, each of those options uses read\_excel and read\_csv. 
Copy-pasting a file into Rstudio will actually paste the file path! (just have to delete a prefix)
I *think* the utility here may vary with OS? I want to say Windows defaults to the C:\ format, but R wants C:/
Actually it works fine for me on both Windows 7 and 10, backslashes are converted automatically.
 if(data$wmorder=="abcd"){(data$WMIndex &lt;-data$BetaScore - data$AlphaScore)} else if(data$wmorder == "cdab"){(data$WMIndex &lt;-data$AlphaScore - data$BetaScore)} 
What does this statement return? What happens when you try with one &amp; instead of two &amp;&amp;?
try using `mutate` and `ifelse` or `case_when`
Iirc you can write ''C:\...'' and Rstudio accepts the path. 
It returns this.. &amp;#x200B; &gt;Warning message: &gt; &gt;In if (data$wmorder == "abcd") { : &gt; &gt; the condition has length &gt; 1 and only the first element will be used It also doesn't run even if I only run the first half of it... &gt;data$WMIndex &lt;- if(data$wmorder=="abcd"){(data$BetaScore - data$AlphaScore)} &amp;#x200B;
Use only one &amp; to inspect all elements 
&gt;You are misunderstanding &amp;&amp; I'm sure you're right, but it also doesn't work even if I just run data$WMIndex &lt;- if(data$wmorder=="abcd"){(data$BetaScore - data$AlphaScore)}
What is str(data$wmorder) ? It's likely you have stored abcd as a vector of four elements instead of a string
This returns &gt;Error: unexpected '}' in "if(data$wmorder=="abcd"){(data$WMIndex &lt;- (data$BetaScore - data$AlphaScore))} else if(data$wmorder == "cdab"){(data$WMIndex &lt;-(data$AlphaScore - data$BetaScore)}" I'm pretty new to R so I never know how to make sense of these errors. To me this sounds like "there is an extra {", but that doesn't appear to be the case. &amp;#x200B;
Oh interesting, how can I check/fix that? Would I do this in r or in excel?
Would you mind explaining the difference between &amp;&amp; and &amp;? I've read about the distinction on help-websites but I still don't get it. 
Best explained here :) https://stackoverflow.com/questions/6558921/boolean-operators-and
Str( ) is an R command. Short for structure. 
Here is what str(data$wmorder) returns! &gt;Factor w/ 2 levels "abcd","cdab": 1 2 2 2 1 2 1 1 1 1 ... &amp;#x200B;
Got it. Your problem is that you are trying to use if( ) to test a vector with length greater than one. Normally, you would use a for loop to iterate through each value of data$wmorder. With r, try this... Ifelse ( data$wmorder, 1, 198) Substitute your true value for "1' Substitute your else value for "198' Make sense? 
&gt; (this is ignoring personal spreadsheets that will never be used for analysis - I keep my cell line database in a .xlsx file). Why do you use .xlsx?
It's just one huge sheet with things color coded. Its only purpose is to track which cell lines I have, how many vials I have remaining, and which box they're in. The choice for .xlsx wasn't really a "choice," per se. I have Excel 2013, and it defaults to .xlsx.
By the way, one of my sessions is on switch with factors. The video hasn't been released yet, but here's some sample code. https://github.com/mnr/R-for-Data-Science-Lunchbreak-Lessons/blob/master/0x_65_switchOnFactor.R
I have never known one mainly bc I see .csv as the most versatile file format. You can feed it into excel, R, SAS, etc...
Perhaps I'm not understanding your question correctly, but those row numbers you see in R studio aren't really in your dataframe. Also IMO openxlsx is much better for excel functions than readxl 
Looks like data$wmorder is longer than one element, whilst "abcd" is one element. It's checking a vector against a single condition. In order to do this the way I think you want, you should use ifelse(), which works in the same way as excel. Ifelse(data$wmorder == "abcd", data$betascore - data$alphascore, ifelse(data$wmorder == "cdab", data$alphascore - data$betascore, "Error")) should work. 
Try to name your empty cell, "observation\_index", for example. After import you'll have observation\_index column, and then you coud use this column's values as rownames: rownames(your_data) &lt;- your_data$observation_index &amp;#x200B;
They are row names as the other user said. You can just use : rownames(your\_dataframe) &lt;- NULL or something like it
Once you get used to manually typing (especially with tab complete) it‚Äôs hardly ever faster to manually select a file - you‚Äôre just more used to that method. However, if you must click then assign a variable to the path string using file.choose then feed this to read_csv. 
What is tab complete?
Users will be using it interactively. The function will be used for on both tbl\_df and spark parquet data. My users are most familiar with dplyr syntax and my understanding is that I can't call user-defined R functions within mutate() for spark tables (although maybe I'm wrong?)
No way? I could have sworn my R threw a tantrum when I last tried that... I must have been messing something else up I guess!
Start typing a directory/file name, then hit tab. RStudio will list all options in the current directory that begin with whatever you have typed. Example: You type `C:/c` and hit tab. RStudio will show you `cats.jpg`, `code.R`, and `customer data` (a subdirectory). You then use the arrows keys to select which option you want, then hit enter. RStudio then completes the typing for you. More generally, you can also just hit tab at the top of a directory, and RStudio will show you everything in the directory.
do ppl just not format code on here?
If you want to browse, then just use read.csv(file.choose()) 
Have you tried using ggrepel as you mentioned? Did it not work?
&gt; symbols displaced and lines pointing to the labels That's what ggrepel does for you. [Here's an example](https://seethedatablog.wordpress.com/2016/12/26/mapping-cities-some-tweaks-with-ggplot-and-ggrepel/)
No, because I rather have different shapes than codes and, I have not figured out how (or if it is possible) to label with [shapes](https://ggplot2.tidyverse.org/reference/scale_shape-6.png) rather than text.
So what have you tried so far then? I think ggrepel will work.
Sorry for being imprecise. That is actually horribly written for what I wanted to mean. I meant Is it possible to have different [shapes](https://ggplot2.tidyverse.org/reference/scale_shape-6.png) and lines pointing to a specific point.
Ok really sorry if this is obvious but, how can I make labels with shapes.
Have you tried anything yet? What error messages are happening?
Sounds better to simply make 2 different maps than plot overlapping coordinates for different things. That or look into html widgets / leaflet https://www.htmlwidgets.org Another (wacky) idea might be to lower the opacity of the points and if you have 2-3 different "types" of points then use primary colors that make it obvious 2 things are overlapping. IE yellow + blue would make overlapping points appear green, but green wouldn't be in the legend, so its obvious its an overlapping yellow and blue?
So why won't jitter work for you again? Points are usually going to be related to coordinates, the point of geom_jitter is to tweak their locations slightly so you can see more points, while maintaining their approximate positions. You can tweak how far the points are jittered with the "width" and "height" arguments. You might be able to make vectors using the original data and a modified version that you add/subtract to move the symbols off, but that's gonna be a lot of work as you mentioned.
Thank you!
Thank you!
Last time I posted this, the wordpress site was doing crazy overlay ads. So I made my own simple blog and posting again.
+1 for this. I hate doing it, but, sometimes it makes sense for the use case. it's handy for those rare occasions.
The first pattern is "unconventional" in R because it's useless. The `local` function accomplishes the same thing cleanly. The second pattern is indeed useful, and actively encouraged by the Unix philosophy (but beware of potential loss of information and of the performance overhead associated with reading and writing lots of data redundantly).
I didn't know about `local` will give it a look.
uh do you maybe have a use case for `local()`? sounds interesting
Not entirely related but you might be interested in [R6 classes](https://adv-r.hadley.nz/r6.html). rhcess package is the only one I saw in the wild that uses it but you can encapsulate your functions to a single object that also holds your data
For the other case, you can store file names in a vector and process them in parallel using one of the many options in R. Here's mclapply, from the core package parallel: https://stat.ethz.ch/R-manual/R-devel/library/parallel/html/mclapply.html The cool thing these days is to use futures. The furrr package is an option. https://davisvaughan.github.io/furrr/
you need to update R to the latest version of R. There is no way around that. Older versions of R are probably not going to be supported for long. One suggestion is to install Linux on your Mac and run R from there if you really need this package.
The choice depends - in part - on how long you want to use the data, whether you want it to easily transfer from OS to OS, and how much you care about the formatting within the file. The .xls/.xlxs file type is proprietary - if Excel somehow loses support from Microsoft and goes the way of the dodo, you may not be able to access your files in the future. Sure, this isn't a likely scenario, but I've heard it touted quite a bit. The bigger issue is transferability - other people may not use Excel at all, and depending on if you want to share these files, they may not be able to access the data within the file. 
If you look at https://cran.r-project.org/web/packages/slam/index.html it specifically states slam needs at least version 3.4.0.
1. Just use unicode: œÄ 2. I am not sure what you intend. `breaks=-32:32` is making breakpoints at -6, -5, -4, -3 etc.. Then you're labelling them (wrongly) -6pi/16, -5pi/16 etc. Did you really want breaks at -6pi/16, -5pi/16 etc? If so, you could just use sinx$x. But that gets messy... Maybe you instead wanted instead just a few (correct) labels? Something like this should do: to_xaxis = function(x) paste0(x, "œÄ/16", sep="") scale_x_continuous(breaks=seq(from=-2*pi, to=2*pi, by=pi/2), labels=to_xaxis(seq(-32, 32, by = 8))) 3. You could make a function if you're often plotting some random small bits. Or you could just `plot(x, sin(x), 'l')`. Both would be fast, but not have your customized labels. plot_function = function(f, x = seq(-4, 4, length.out = 20)) { d = data.frame(x) d$y = f(d$x) ggplot(d, aes(x,y)) + geom_line() } plot_function(sin) 
Your best bet is [Rstudio](https://www.rstudio.com/). It's a much better way of working and pretty much the standard.
I think what you are describing are reactive elements. https://towardsdatascience.com/get-started-with-examples-of-reactivity-in-in-shiny-apps-db409079dd11
Hi thanks for your reply, Maybe I wasn't clear but I was wondering if the implementation of what i'm describing exist, in an automated way. Else, I know the basic of shiny 
Well, not that I know of. If you don't want to work at the level of making the reactive stuff on your own, maybe you should look at something like https://www.red-gate.com/simple-talk/sql/bi/power-bi-introduction-working-with-r-scripts-in-power-bi-desktop-part-3/ https://bert-toolkit.com/ I don't really have experience with these, but maybe they can help manage your components better for you.
You can write your output to a file
Thank you! 
I was able to update. Thank you!
Okay great. I was hoping you didn't take my comment the wrong way -- was just trying to make sure you knew for a fact that you were having issues because the package specifically required a higher R version!
I see that you lifted the head/tail example from the linked Stack Overflow post. Note that head/tail in a mathematical sense (e.g. x/xs for anyone whose done functional programming pattern matching) is not the same as head/tail in R. Head() in R defaults to n=6 and returns the first *couple* of elements in a list, rather than just the first one. And tail() in R returns the last *couple* of elements. So head(tail(x)) does not return the second element, but rather just the tail. You can see that if length(x) == n, then head(x, n) is a no-op. And since both head and tail default to n=6, well, you get the idea. 
The best way is to realize that pipe is basically just overloaded function: lhs %&gt;% rhs is the same as: `%&gt;%`(lhs, rhs) Also the function takes it's right side function and shoves at the first place in arguments its left side. So: a %&gt;% foo(b) becomes: foo(a, b) You might want to study the function itself (for example taken from the package `purr`) &gt; purrr::`%&gt;%` function (lhs, rhs) { parent &lt;- parent.frame() env &lt;- new.env(parent = parent) chain_parts &lt;- split_chain(match.call(), env = env) pipes &lt;- chain_parts[["pipes"]] rhss &lt;- chain_parts[["rhss"]] lhs &lt;- chain_parts[["lhs"]] env[["_function_list"]] &lt;- lapply(1:length(rhss), function(i) wrap_function(rhss[[i]], pipes[[i]], parent)) env[["_fseq"]] &lt;- `class&lt;-`(eval(quote(function(value) freduce(value, `_function_list`)), env, env), c("fseq", "function")) env[["freduce"]] &lt;- freduce if (is_placeholder(lhs)) { env[["_fseq"]] } else { env[["_lhs"]] &lt;- eval(lhs, parent, parent) result &lt;- withVisible(eval(quote(`_fseq`(`_lhs`)), env, env)) if (is_compound_pipe(pipes[[1L]])) { eval(call("&lt;-", lhs, result[["value"]]), parent, parent) } else { if (result[["visible"]]) result[["value"]] else invisible(result[["value"]]) } } } &lt;bytecode: 0x564fff806140&gt; &lt;environment: 0x564fff805180&gt; 
1. how do I do that? 2. I think I see what I am doing wrong here thanks 3. thank you
I'm sorry but would you mind explaining that again in lamens terms? I haven't formally learned R so a lot of the jargon is flying over my head. &amp;#x200B; For instance, in this context, I'm not even exactly sure what you mean by "expressions" in the first sentence. Are the '\\', 'w', and '+' examples of expressions? or is the whole \\\\w+ an expression? &amp;#x200B; What does an "escape" mean? &amp;#x200B; &gt; the next \\ indicates that we want to match on a special value Does this mean that only after the '\\\\' we are specifying the types of things we want to count? &amp;#x200B; &amp;#x200B; The questions aside, your answer was by far the most helpful and helped me make a ton of progress on my project!
Hello /u/mniemannross ! Sorry for replying late, lots of new years things going on. (Also, Happy New Year!) &amp;#x200B; I don't think I understand what you're saying. What do you mean by my "true value"? In this context I'm also not sure what would take the place of 198 at all. 
So if I understand this correctly it's overwriting my wmorder instances (i.e., abcd &amp; cdab) such that if a participants wmorder = abcd, then "abcd" becomes the betascore-alphascore. Correct?
Check out the documentation for the function mean and the na.rm parameter. It will not ignore them by default. 
ok, I was able to make a better looking plot library(tidyverse) xaxisnew &lt;- c("-2\U03C0","-3/2\U03C0","-\U03C0","-\U03C0/2","0","1/2\U03C0", "\U03C0", "3/2\U03C0","2\U03C0") sinx &lt;- tibble(num = -32:32, x = seq(from=-2*pi, to=2*pi, by=pi/16), #xaxis = paste0(as.character(-32:32), "\U03C0", "/", as.character(16), sep=""), xaxis = as.character(NA), y = sin(x) ) sinx$xaxis[c(seq.int(from=1, to=65, by=8 ))] &lt;- xaxisnew ggplot(data=sinx) + geom_path(mapping=aes(x=x, y=y)) + scale_x_continuous(breaks=-4:4*pi/2, labels=xaxisnew) + labs(title="sin(x)", subtitle = "[x|-2\U03C0, 2\U03C0]", x="x", y="sin(x)") + coord_cartesian(xlim = c(-2*pi, 2*pi)) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) still that took a lot of effort to write this
THANK YOU!
So you want a new value with the items summed / 4 for each row?
lapply(data, mean, na.rm = TRUE)
&gt;The best way is to realize that pipe is basically just overloaded function: &gt; &gt; lhs %&gt;% rhs &gt; &gt;is the same as: &gt; &gt; `%&gt;%`(lhs, rhs) That's how all binary operators in R are, no? Although, &gt;One thing, you can do the exact same overloading with the plus operator, which is also basically just a function \`+\`(). That is the way ggplot handles its layers. That's a special `+` method defined in ggplot. Not the same as the base R `+` function; and even in the context of ggplot it's not synonymous with %&gt;%, as discussed [here](https://stackoverflow.com/q/35332861/9195557). It's also worth understanding that the pipe operator uses [nonstandard evaluation](http://adv-r.had.co.nz/Computing-on-the-language.html). That is, when you write a %&gt;% f(b) or equivalently `%&gt;%`(a, f(b)), the pipe can actually see the strings "a" and "f(b)", not merely the values they yield: that's how it pulls this trick. Speaking for myself, at least, I was very uncomfortable using this operator until I understood what kind of black magic was at work. 
For each row/participant, I want their Score to be the mean of Item1, Item2, Item3, and Item4. 
``` set.seed(4) df &lt;- data.frame( Item1 = runif(5), Item2 = runif(5), Item3 = runif(5), Item4 = runif(5) ) df df$score &lt;- rowSums(df) / ncol(df) df ```
Because I have 4 variables, would I place c(Item1, Item 2, Item 3, Item4) where data is?
I'm sorry but that's really confusing to me
It's taking the sum of the row and dividing by the number of columns. 
what is set.seed(4) for?
You're focused on the wrong aspects of my example. That is just to make it reproducible. The important part is ``` df$score &lt;- rowSums(df) / ncol(df) ``` I don't know what your data looks like, so I just made some up to demonstrate. 
If you give me a small sample of what your data frame looks like I can make an example for your exact data.
Sorry I'm just very R-illiterate and I want to know what's going on. When you put Item1 = runif(5), what does the "runif(5)" mean?
Again, that is just random data. That has nothing to do with your question. 
Please share what your actual data looks like and I can tailor a response to your specific situation.
Here are two more ways to do it ``` df$score &lt;- apply(df, 1, mean) df$score &lt;- (df$Item1 + df$Item2 + df$Item3 + df$Item4) / 4 ```
All right, well I added comments to the code above. Hopefully it can help you. Good luck.
I recommend reading the documentation for the apply function. It's pretty powerful and you will use it frequently. the arguments are essentially: data, operate on rows or columns (1 or 2), and the function you want performed.
Sorry if I frustrated you, but thanks!
Haha no I just had to revise my strategy. I hope at least something clicked for you. R is kind of weird but if you keep at it it will make sense. I think... :)
Thanks! I'm reading it now!
Well I appreciate your help!!!! I did what you said, and I think I did it correctly, but it's still not working. It's returning means less than 1 for all of the rows, but the lowest answer choice on my items were all 1 so that doesn't seem possible. 
rowMeans should work: data$Score &lt;- rowMeans(data[,1:4]) This assumes that Item1, Item2, Item3, and Item4 are the first four columns of the ‚Äúdata‚Äù data.frame. If you‚Äôre expecting a dataset with a varying number of columns, you‚Äôll obviously need to make the subsetting more flexible than 1:4, but the general idea is the same: rowMeans should work. 
If you want a "tidy" type of answer: library(dplyr) data = data %&gt;% mutate(Score = (Item1 + Item2 + Item3 + Item4)/4) Not the most elegant, but for a small number of columns, I might do that. Another option is, if those columns are consecutive in the data frame: data$Score = data %&gt;% select(Item1:Item4) %&gt;% rowMeans You can de-pipe this if you want or make it more "tidy".
Like I said, if you could provide what your data actually looks like then we can help better. Even more detailed description. How many rows and columns etc... otherwise I have no idea how you are implementing suggested solutions 
Just to add some more explanation in their example, df$score &lt;- rowSums(df) / ncol(df) This statement can basically be read as: Create a variable named ("score") in the data frame (ds). This variable will be the sums of the row in your data frame (the example provided assumes you only have those variables you want to take a mean of in your data), and divides by the number of observations within a column (number of participants). So the end part (after the arrow) is you take an average of the entire row, and the first part (before the arrow) is saying to perform the operation described (an average) in your data and place the value in a new column called score. My data$My New Variable &lt;- Sum of entire Row (My Data) / number of people in columns (of My Data) In R you'll usually see data frames expressed as "df" in a lot of examples, and the $ is usually a separator telling where to look or what to create within that data. If I had to guess, you're a psych person so let's consider this same example in Excel. Let's say you have a dataset with a bunch of variables and (given psych) messy and missing data. The method described above would sum cells A1, B1, C1, and D1 and then divide by the number of people (A1 to A20, if you had 20 participants). Your (likely) problem is that you'll have people who skipped items. So columns A-D may have different numbers of N. What if someone answered 5 to item 1 but skipped 2-4? Further, in practicality, you likely have other variables in your data and don't want to sum the entire row. Conventionally, you probably want to do something like tell R to take a mean of column J to Column M, perhaps even using a rule like "only count the person if they answered 3 of the 4 items" (if that's the case, let me know). I'd recommend something like, My Data $ New Variable Called Mean (or whatever) &lt; is created by - taking the row means, within my data, selecting columns J, K, accounting for missing. In R, you could do this like: *Data$Variable &lt;- rowMeans(subset(Data, select =(J, K)), na.rm=TRUE) 
Won‚Äôt pmean work here?
No, if is not vectorised, meaning that it can't evaluate a bunch of stuff against one condition. It can only evaluate one thing at a time, but you're giving it a whole column of data to compare to the condition. Not knowing what to do, its programmed to just compare the condition to the first element of wmorder. This means if the condition is true in the first row of your data.frame, then it will evaluate as true for the whole lot. To test all of wmorder individually with if you'd need to loop through each element of wmorder individually. As R is poopy at loops, this is best avoided. To get around this, the R developers came up with ifelse(), which CAN evaluate one condition against a whole column in one go. So using ifelse instead of if should solve your problem. There are a couple of other ways to do this if this doesn't work for you, so let us know. 
Check out this StackOverflow answer for a tidy-er version of what you‚Äôre suggesting: https://stackoverflow.com/questions/33401788/dplyr-using-mutate-like-rowmeans/35553114
Not sure I entirely understand your problem yet but can you just merge the two dataframes?
Like I mentioned in my post in data frame **RSX** \- *Partner\_company\_ID* is equal meaning what shows in the second data frame **PSX** \- *ID\_company* (this task I had done) and added values from PSX in RSX data frame `RSX$PS_EURXX &lt;- PSX$Declared_amount_EUR[match(RSX$Partner_company_ID, PSX$ID_company)]` using **RSX -** *Partner\_company\_ID* and **PSX** \- *ID\_company* like a keys. But data frame **RSX** \- *ID\_company* is the equal meaning what shows data frame **PSX** \- *Partner\_company\_ID* (this is the problem and I don‚Äôt know is this function merge or something else how to add right **RSX** \- *ID\_company* to right **PSX** \- *Partner\_company\_ID* in the same time I need some additional function not removing already used `RSX$PS_EURXX &lt;- PSX$Declared_amount_EUR[match(RSX$Partner_company_ID, PSX$ID_company)]` I just started learning R language so I will appreciate your help!
Just change the colnames so they are all the same. Then rbind should work. 
This is an excellent post... why on earth did someone downvote it?
That worked perfectly. Thanks, and sorry for asking such a dumb question!
You already gave mock data input which is good. Maybe can you mock up what your output is supposed to look like given that input? I know that's tedious but it's very hard to read written English describing a coding problem. I'm having trouble seeing what you're saying, dupe keys? Aggregate first? Merge with two keys?
Do you want to join RSX and PSX using Partner_Comapny_ID and ID_company? merge(x=RSX,y=PSX, by.x="Partner_Comapny_ID", by.y="ID_company") This creates a new dataframe with rows in correct correspondence. Similar to what you are doing with match. This is performing an inner join. You can change that with the all parameter. Hope this helps. Still not completely sure what the output you want is. 
&gt;You already gave mock data input which is good. Maybe can you mock up what your output is supposed to look like given that input? &gt; &gt;I know that's tedious but it's very hard to read written English describing a coding problem. I'm having trouble seeing what you're saying, dupe keys? Aggregate first? Merge with two keys? Right now I understand that I have to use merge instead what I have done previously so after setting data frames RSX and PSX I deleted repeated values and I get this result. `RSX_2 &lt;- data.frame(ID_company_RS=c(1,2,3,4,5,6),` `Declared_amount_EUR=c(10,30,40,50,60,70),` `Partner_company_ID_RS=c(1,2,3,5,2,6))` &amp;#x200B; `PSX_2 &lt;- data.frame(ID_company_PS=c(1,2,3,5,2,6),` `Declared_amount_EUR=c(10,35,39,55,61,70),` `Partner_company_ID_PS=c(1,2,3,4,5,6))` &amp;#x200B; `tabula_2 &lt;- merge(x=RSX_2,y=PSX_2, by.x="ID_company_RS", by.y="Partner_company_ID_PS")` &amp;#x200B; |ID\_company\_RS|Declared\_amount\_EUR.x|Partner\_company\_ID\_RS|ID\_company\_PS|Declared\_amount\_EUR.y| |:-|:-|:-|:-|:-| |1|10|1|1|10| |2|30|2|2|35| |3|40|3|3|39| |4|50|5|5|55| |5|60|2|2|61| |6|70|6|6|70| This is what I want, but problem is that company‚Äôs ID in data frames RSX and PSX may be repeated several times and merge function makes incorrect results. So I don't know how manage this problem, maybe the solution is that I merge with two keys at the same time. How it is possible?
Thanks a lot for replay! It was really useful! I tried this and this is what I want, but problem is in that cases when the company‚Äôs ID in data frames RSX and PSX may be repeated several times and merge function makes incorrect results. So I don't know how manage this problem, maybe the solution is that I merge with two keys at the same time. How it is possible? Or is it possible?
You can pass a vector to the all.x and all.y parameters. For example if you wanted to join Partner\_company\_ID too ID\_company and ID\_company to Partner\_company\_ID in RSX and PSX, respectively, you would call: `merge(x=RSX,y=PSX, by.x=c("Partner_company_ID", "ID_company"), by.y=c("ID_company", "Partner_company_ID"))` which is just and inner join using two keys. you can add all.x=T for a left join, all.y=T for a right join and all=T for a full join. &amp;#x200B; I am not sure what you mean by duplicates being a problem. 
Would the datatables package do what you want? see [https://rstudio.github.io/DT/](https://rstudio.github.io/DT/) scroll to the section "2.8 Column Filters" Where they have examples of using different filters per column. 
I know this won't answer your question directly, but the documentation clearly states that "confidence level for the returned confidence interval. Currently only used for the Pearson product moment correlation coefficient if there are at least 4 complete pairs of observations.". So it does not give it for Kendall's tau either. My guess would be that these are non-parametric, but anything after that I don't really know.
How would you R maniacs rate this joke? "My girlfriend broke up with me because she overheard me talking to a colleague about laying pipe later in the lab. If only she'd known about R's incredibly convenient dplyr library!" When I first came up with it I laughed for a solid 10 minutes. What do you guys think?
A 95% CI depends on a Gaussian distribution which presumably isn‚Äôt present if you are doing a Spearman rank correlation. 
Hi there, take a look at unlist() if each list item has the same number of columns. After you have one big dataframe you should be able to use spread() in tidyverse to spread it into more cols. Is that what you're looking for? 
Look into coin:spearman_test(). I believe you can get CIs for there. 
I've tried to figure out what you're trying to describe here but I'm not great at R, is there any way you could walk me through it a bit more? &amp;#x200B;
[https://tidyr.tidyverse.org/reference/unnest.html](https://tidyr.tidyverse.org/reference/unnest.html) You can unnest the lists you want, specify that you only want the certain row's output from the unnested lists so you only get the ninth one. 
What do you mean by fully write the function?
https://en.m.wikipedia.org/wiki/Normal_distribution Entropy for normal distribution: H(x)=1/2 \* log(2 /pi e /sigma ^2)
I can't figure this out. What could cause this behavior? Here's the code: `library(treeio)` `library(ggtree)` `testree &lt;- read.mrbayes("/path/to/my/data.con.nex")` `attributes(testree)$phylo$tip.label=gsub("_"," ", attributes(testree)$phylo$tip.label)` `acNO &lt;- scan("/path/to/tip/labels.txt", character(), quote= "")` `gen &lt;- c(rep(NA,length(acNO)))` `sp &lt;- c(rep(NA,length(acNO)))` `d &lt;- data.frame(label = testree@phylo[["tip.label"]], acNO = acNO, gen = gen, sp = sp)` `ggtree(testree) %&lt;+% d + geom_tiplab(aes(label=paste0('bold(', acNO, ')~', gen, '~', sp, sep=" ")), parse=T, size=3.5) + ggplot2::xlim(0,0.4)`
There are probably a few options when it comes to managing your duplicate problem, but here are a few easy ways you can identify which records are an issue and maybe exclude them for separate analysis. # Logical vector for dupe id's duplicated(RSX$ID_company) # Which rows have dupes? which(duplicated(RSX$ID_company)) # Show only rows with dupe id's RSX[duplicated(RSX$ID_company),] # Are any records duplicated for both keys? any(duplicated(RSX[,c('ID_company', 'Declared_amount_EUR')])) &amp;#x200B;
Is there any way you can provide a reproducible example that doesn't rely on your specific data? The problem likely lies in the geom_tiplab() call but it's hard to troubleshoot without replicating locally.
Here's a reproducible example using the example data from the ggtree package: `library(treeio)` `library(ggtree)` `nwk &lt;- system.file("extdata", "sample.nwk", package="treeio")` `tree &lt;- read.tree(nwk)` `vec &lt;- c(rep("SK15612.",13))` `vec &lt;- paste(vec,seq(length(vec)),sep="")` `v &lt;- data.frame(label = tree[["tip.label"]], vec = vec)` `ggtree(tree) %&lt;+% v + geom_tiplab(aes(label=paste0('bold(', vec, ')' )), parse=T, size=3.5)`
You can always definite your own function: Integrand &lt;- function(x, my, sigma){ } Or definite it anonymously inside the call to integrate.
This kind of thing happens with ggplot when you‚Äôre mistakenly plotting text on top of each other again and again. 
Is there something wrong with my code, or do you think it's an issue with geom_tiplab()?
There are several ways to do it. How are they coded in your data?
It's a single criterion and it is there score on a single variable. If Ps score less than X# correct they are excluded. Thanks for the links!
Yeah that should be a fairly standard thing to do.
Something like this: ``` df &lt;- data.frame(participant_id = 1:5, number_correct = c(5, 15, 25, 3, 4)) subset(df, number_correct &lt; 10) ```
Thank you so much! I used the filter() function in tidyverse and it worked! I'm just a little confused about what I need to do now to ensure that my analyses only include the people that make it through the filter. Do I need to create a new dataframe from the filter (i.e., NewDataFrame &lt;- filter(number_correct &lt; 10), and then run all future analyses on that dataframe?
Yes but the way you have written it will not work. You either need to pipe in the original data frame or provide as the first argument to filter. 
Like this? NewDataFrame &lt;- data.frame(filter(number_correct &lt; 10))
No. Please see my reply.
If you just do filter(number_correct &lt; 10) the filter function has no idea what you want to filter.
Sorry, that was a type on my end. It actually reads filter(data, number_correct &lt; 10) So, what I mean was NewDataFrame &lt;- filter(data, number_correct &lt; 10)
Okay that should be it then. That will store the filtered result in to a new variable.
Something is going wrong. When I run filter(fulldata, fulldata$MathScore &lt; 50) and it gives me the preview table in the console, I eyeball much more than 19 scores above 50, yet when I run data &lt;- filter(fulldata, fulldata$MathScore &lt; 50) it says that there are only 19 observations in data. 
Imo you should just totally embrace the tidyverse for now and read about pipes. It makes stringing together data cleaning steps together very clear and straight forward. The other ways of doing it aren‚Äôt bad, but the tidyverse philosophy really reduces the barrier to entry imo. 
I agree, but I try to provide other options just in case someone is not familiar. Just in case there's a base R fanatic out there... :) I suppose I should learn data.table to provide that too, but I haven't had the urge to learn it.
I meant to reply to the op who asked the question. Your advice was great and comprehensive! Sorry for making it seem like I was being rude. 
No I didnt take it that way I just didnt want to offer d any base R people hahaha.
I'll look into it! My stats prof was all about the tidyverse I just haven't jumped into it yet. I don't even know what you mean by piping in, for instance. 
I wrote a handout I will post tomorrow. But in the meantime https://style.tidyverse.org/pipes.html
%&gt;% is the pipe operator filter(df, ...) is equivalent to df %&gt;% filter(...) It basically takes the left hand side and makes it the first argument. Of course, there is more to it than that but for right now that is what you need to know. It allows you to chain together several functions without having to store in intermediate objects. So you could filter, select, mutate all in the same "line" by just continuing to pipe.
You can also reassign the output of your filter to df using this operator %&lt;&gt;%. df %&lt;&gt;% filter(...)
What do you mean "select the car version of recode()"? As in: car::recode() &amp;#x200B; ?
When I type "recode" and wait a moment, a small box pops up with a list of available recode() function, with the package it belongs to on the right (e.g., {dplyr}). I selected the car one, and it still doesn't work. I'm in rstudio if that makes a difference. 
You need to type it how I typed it.
&gt;car:: It still doesn't work when I typed it how you typed it. How can I remove tidyverse from r's working memory (sorry I don't know the proper term)
You are typing car::recode(...) ? Can you share any of your code? 
When this happened to me it‚Äôs usually after I restart R and there was a variable defined from debugging or messing around and wasn‚Äôt defined in code. Maybe that‚Äôs a good place to start. 
When I restarted r, it worked! You were right! Thank you!
I retarted R and it worked. Sometimes it's just that simple!
omit.na(), or the argument, na.rm = TRUE, only.if that argument exists though
Yup. Depends on what your are trying to do. 
using dplyr from tidyverse: &amp;#x200B; your\_filtered\_dataframe\_name &lt;- unfiltered\_dataframe %&gt;% dplyr::filter(criterion == TRUE) 
In short, handling with missing data appropriately is a huge work. na.omit(mydata) removes missing values. But, this may not be the best way to handle it. I firstly look at the percentages of the missing variables. Then, I remove the ones which has more than 75% missing value. If this is not good enough, I remove variables up to 50% missing. Then, applying na.omit gives a better result. 
Just to nitpick, but you never need to write `== TRUE`. The criterion should evaluate to a boolean value, so just use that directly. 
Give us more information - why does it have NA values? Why is it a problem for you? How do you want to ignore - do you want to delete those vars, report the missingness, or just create a new df without those missing values? Give us more info
Fair point, I haven't written filters that generic example way actually, was just an example that I clearly haven't applied yet. Usually have just used the !is.na or != value or the comparators &gt; &lt; etc
There are a hundred different ways you could plot this data... Often times when working with data the questions is not ‚Äúhow‚Äù can I plot this but ‚Äúwhat‚Äù do I want my plot to show. Are you trying to show a linear relationship between 2 of your 9 measurements? A scatter plot would work. Point being, what is it you are trying to show? For starters, you have 20k observations across 9 variables. What are those 9 measurements?
Well they are data from different accelerometers. I want to plot first three of the columns in a single plot and their magnitude in another one. This is to show visualization of the output from the accelerometers at different times intervals. The graph can be looked at to analyse the spikes in the data to identify and correlate it with activity during which the spike occured.
If ( sum(is.na(mydata$col))/nrow(mydata) &gt; .5 ) { mydata$col &lt;- null }
What visualization package are you using? Faceting with ggplot2 would let you easily stack several different views of your data. Depending on the data, it might also make things easier to format it as time series before graphing. 
Thanks for advice but with merge function I lost values that is in one data frame but isn't in other, so I learned use this function and in that way I gather all information I need. RSX %&gt;% full_join(PSX, by = c(ID_company = "Partner_company_ID", Partner_company_ID = "ID_company"))%&gt;% select(ID_company, Partner_company_ID, everything()) 
Thanks for your suggestions :)
Just a comment, you only need to put backticks around variables that are otherwise not valid R names ‚Äî so you need it around `1`, `2` etc. but it‚Äôs unconventional to put them around normal variables such as `var1` etc.
I think using purrr is probably your best bet. &amp;#x200B; [https://sebastiansauer.github.io/multiple-lm-purrr2/](https://sebastiansauer.github.io/multiple-lm-purrr2/)
Thanks, I'll look into it. The example linked is useful to get to know what it might be able to do, but would need some tweaking for purpose. I guess that's my afternoon sorted...
aaand I fell flat on my face.
I agree. Maybe this will also help you understand: [https://www.youtube.com/watch?v=rz3\_FDVt9eg](https://www.youtube.com/watch?v=rz3_FDVt9eg)
You can create a formula from text using as.formula(). Something like: f &lt;- as.formula(`var1` ~ `var2` + `var3` + `1`) fit &lt;- lm(f, data=rweights) So you can loop (or vectorize) through the numbers you want, create a formula and fit it. (Warning: untested code.) &amp;#x200B; allResults &lt;- lapply(1:1000, function(x) { f &lt;- as.formula(paste0("var1 ~ var2 + var3 + `", x, "`")) fit &lt;- lm(f, data = rweights) return(fit) }) allResults is a list of regression models. &amp;#x200B;
R does math in a matrix element-wise. That means if you have subtract two matrices, it subtracts the values in each spot in the two matrices from each other. So, you can do this very quickly by creating two matrices with the values in the right place for the subtracting you want. 
Yes, this is [a popular recommendation](https://www.google.com/search?q=r+preallocate+vector). Be careful with what references the vector. If there was ever only one reference (e.g. only one variable that references that vector), R is smart enough to do updates in-place. But if not, R will copy the 8 megabytes the vector is taking (assuming doubles) each time you update it. Make sure you have a way to benchmark your code.
\&gt; I'm concatenating each new element onto the vector as they are found don't do that :( (people are right with thinking it is slow) What Liorithiel said + can you use an apply instead of the for loop? even if just for subunits, this could reduce the amount of "whole vector copying" by several orders of magnitude. (or ideally you store the subunits in a list and then only concatenate them in one step at the end). If you have issues with the vector site and your RAM (my genomic ones ended up being gigabytes sometimes...), there are packages to deal with that. PS the 'simple doubling technique' might actually be something similar to what R does automatically (I assume it doesn't continue growing linearly). Another solution might just make it have 100k or something that is 1-2 orders of magnitude below what you expect the maximum roughly to be and then just extend it every 100k entries. Then you don't have a hard limit, don't overshoot completely and reduce the amount of concatenation drastically. What isn't that great about this whole approach is that you will have to be doing a check of your index against the size every iteration, which is also quite a bit of a time waste.
As u/Katdal2 suggests, doing this with matrix math is the solution. Here's an example that does what you're asking. It's not really generalized, but that's a slightly separate task... # Create a matrix containing 1000 rows of 156 columns each, with values ranging from 1 to 7 a&lt;-matrix(round(runif(1000*156,1,7)),1000,156) # Create a second matrix, where Column 1 is 0, the next 38 columns are repeats of Column 1 from matrix a # Repeat to create the blocks to match columns 39,77 and 115... b&lt;-cbind(0, matrix(rep(a[,1],38),1000,38), 0, matrix(rep(a[,39],38),1000,38), 0, matrix(rep(a[,77],38),1000,38), 0, matrix(rep(a[,115],38),1000,38)) # Subtract matrix b from matrix a and store in c... c&lt;-a-b # or... a-cbind(0, matrix(rep(a[,1],38),1000,38), 0, matrix(rep(a[,39],38),1000,38), 0, matrix(rep(a[,77],38),1000,38), 0, matrix(rep(a[,115],38),1000,38)) A little bit more coding could generalize this to do the task of subtract column N from columns N:N+X and repeat for the remaining columns...
Just in case someone's new to the issues that missing data can introduce, here's a fairly short blog post that covers at least sopme of the basics... [https://measuringu.com/missing-data/](https://measuringu.com/missing-data/) As the author of the article states, sometimes ignoring NAs can introduce bias into your analysis. And, in R, like many languages, there's no inherent difference between the several types of "NA" or "missing values"... NA can be "Not Available", "Not Applicable", or "Not Answered". "Not Available" can happen when a sensor fails; "Not Applicable" is like asking a bachelor for his wife's name, "Not Answered" is when someone makes a conscious choice to skip a question (which can be an important measure itself...).
R vectors unfortunately do not reserve memory. Every concatenation leads to a copy of the whole vector being made into new memory.
&gt; PS the 'simple doubling technique' might actually be something similar to what R does automatically (I assume it doesn't continue growing linearly). It is unfortunately not that smart. It does a lot of recopying and adding chunks at a time. Unfortunately, the current version of Wickham's Advanced R doesn't have this information where I was expecting it and I can't find it to link to in just a moment of poking through, so you'll have to google it if you want to see just how awkward it is. It's sooooo awkward.
You can just subtract. Example to try out: exampledf &lt;- data.frame( x1 = 1:1000, x2 = sample(1:10, 1000, replace = TRUE), x3 = 1000:1, x4 = 1 ) head(exampledf) result = exampledf - exampledf$x1 head(result)
There is probably a way to paradigm shift the problem away from this kind of logic, rather than using a vector for it. You talk about "finding elements", so you may find it a lot easier to map a function to each row and then filter the dataset based on the function output, for example.
This is very helpful, thanks.
I've never read a pivot table into R myself but couldn't you just export it as a regular flat file from excel?
The problem is I cannot get it on a flat file format
it's been a long while since I've made a pivot table in excel. Now that I think of it I think I may be getting it confused with a crosstab table.
ok I just googled pivot tables as a refresher. In excel can you make sure the pivot table is in it's own worksheet. If so then you can export the data from excel in a native file format (*.xls or *.xlsx) then import that specific worksheet into R.
Thanks so much. What exactly fo you mean by "in its own worksheet"?
at the bottom left side of every excel file that you open in excel there should be tabs which indicate each worksheet in the excel file by default excel usually has 3 sheets named "sheet 1" "sheet 2" and "sheet 3" unless someone renamed them. Make sure the data in the pivot table are in a worksheet by itself with no other data. If so you can import just that worksheet into excel.
Typically, a pivottable is reading data from a source that's in a flat file format. Perhaps there's another worksheet with the actual data? You can then read this data into R. Also, you might want to check out the rpivottable package if you like PTs and want to use one within R. [https://cran.r-project.org/web/packages/rpivotTable/vignettes/rpivotTableIntroduction.html](https://cran.r-project.org/web/packages/rpivotTable/vignettes/rpivotTableIntroduction.html)
Try using readxl. Parameters are path, sheet, and skip. Skip is the number of rows to skip before you ever to the pivot table. 
Yes, I know. However this is hidden and locked, which is why I want to use R to "reverse" that. 
Unless something changed recently, excel uses reaaaaally weak encryption for passworded sheets.
Yeah you can do classic brute force attacks cycling through ascii symbols as it doesn't time you out if you put in the wrong password. Just make a loop to go through I think it's CHR()? Shouldn't be too hard to write a loop to try every combination starting at 1 character, then 2,3,4,5 and so on. I've seen it done, and shouldn't take too long to run unless the password is massive
Try opening the file in OpenOffice. I haven't tried it in years, but it used to be that you could open a locked Excel file in OpenOffice without issue. Worth a shot ;)
Hopefully you already know that if you are hoping to reverse engineer the raw data file from a Pivot Table, you're basically out of luck. Pivot tables are *summaries* of data... The ***best*** and ***most ethical*** solution is to reach out to the owner of the dataset, explain your needs, and request access. You may find out, though, that they don't have permission to share the raw data, and that's why they only provide summary data. As others have suggested, you can try to use brute-force to gain unauthorized access to the data source. Be careful, though -- you may be violating company policy or even the law by doing so.
Here is what I was looking for (previous edition of the book): http://adv-r.had.co.nz/memory.html 
I remembered the start part, and assumed that it would continue with the 128 blocks (or even slightly bigger ones), but it indeed does non of that. Well maybe in the future...
readxl #1
ggplot2 facets are amazing
To me this sounds like a for loop around a plot function. Or maybe instead of plot() you call lines() or some such which doesnt render from scratch
Join the tables and then mutate a new column. I can send some sample code later. 
There's a package called broom with a function called unnest(): [https://cran.r-project.org/web/packages/broom/vignettes/broom\_and\_dplyr.html](https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html)
Inner.Join() on account, then mutate column 1 - column 2
Would you be interested in a freelance side job? This seems above my head. 
Perhaps, send me more details via message 
Thats too funny! Im going to go open this up just so I can read the end of that comment! I need to know what its supposed to be used with üòÇ
There is an XKCD theme too! What's the opposite of comic sans?
Try this: library(tidyverse) df2018_17 &lt;- df2018 %&gt;% inner_join(df2017, by = "AccountName") %&gt;% mutate("vs LY Total" = (Total.x - Total.y)/Total.y)) %&gt;% mutate("vs LY Q1" = (Q1.x - Q1.y)/Q1.y) %&gt;% select(-Total.x) This is the general format but you should get the idea. Do the same for the other quarters, rearrange the columns in the order you want and then rename them with the colnames(df2018_17) &lt;- c("Total", "LY", ...) command. I'm doing this from memory, so apologies if there are any code errors. 
Are you using vscode? If so are you able to use the variable explorer or is it only available in VS?
Just a dark skin in Rstudio
Tragic serif I'd imagine
Ah! Thank you for your reply!
No worries, happy coding!
With what program have you seen it being done? Something like autohotkey?
But will it read a password locked Excel?
Wingdings.
&gt;This should be combined with With what?
I'm guessing 2 pints of everclear and shame.
Are you familiar with lubridate? It looks like it has some functions that would help, like the ones at the bottom of the cheatsheet: https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf I don't have the time to find a solution to your problem right now, but that might help in getting you in the right direction.
Hey, no I'm pretty sure you can do it (I.e some function to unlock a sheet with an argument for password) directly in vba. If you then loop that command to attempt different passwords you've got a bruteforece attack
Thanks for the response! Yeah I think Lubridate will hold the key, never used it before but it came up a few times when searching for a solution myself. Thanks!
What do you mean by ‚Äúshort?‚Äù
I think the learning curve is steeper with R since its a functional language and the syntax is abstract. But it still has an edge over python in certain areas which is why people still recommend learning R in addition to python.
Base R has always been a bit of a Frankenstein, and users that come from other programming languages note that almost right away. Python was written to be a readable general purpose language from the beginning and it does that very well. S, which became R, was written to be an interactive stats tool that became a programming language. I don't see how that process can work out cleanly. Fortunately, we have the RStudio team that has put out some of the best and most complete libraries I have encountered in any language (not to mention the best bar-none free IDE available by a long shot). ML has been a huge boon for Python for sure, but remember python is also used heavily for web-development, automation, etc. Because of the diverse range of uses the ecosystem developed more quickly and holistically than that of R. All that is to say that comparing the two is probably not appropriate. We don't compare hammers to screwdrivers because they are different tools with different qualities. When I need to do EDA, quick visualization, data cleaning, or straightforward modeling I am quick to pull out R. Also, Shiny allows you to stand-up decent web apps quite quickly which is awesome. Alternatively, if I want to put something into production or do something with ML then I'm using Python. Sometimes these lines blur and that's ok too. Is Python taking over R's turf so to speak? It's absolutely possible. Until a better alternative to Pandas gains momentum then I think R will always have it's place thanks to tidyverse. If someone puts together a tidyverse like series of packages in Python then we may have a different situation. In short, my thinking on the subject is that R will never really go away (at least not due to the popularity of Python) because it has some great qualities that are very useful. There is now a strong user/ developer base producing quality products. More importantly, we should remember the tools come and go, but knowledge remains. At some point there will be something better that replaces both R and Python. If we deeply understand what we're doing then it won't matter.
I try so hard to stick with R for its ease of use. Which sounds crazy at first, but I can do so much in so little time. I‚Äôm trying to learn Keras for R and it‚Äôs going really smoothly. Without Rstudio, I‚Äôd be using python for sure. 
Python is more widely used, that doesn't mean R is going away. It's possible some users leeched from R to Python, but it's also possible that the total number of users is going up, especially for a language like Python which is in many Programming 101 courses. Coding in general is just a lot more common these days anyway, nevermind the fact that you're referencing a programming language index, while R is a data/statistics language. Survey statisticians and you may find a very different result. 
R and Py have some things in common, but I don‚Äôt think they are as directly comparable / in competition as the blogosphere makes them out to be. I use both ambidextrously, and often for very different things. I find them very complementary and there is a useful division of labor in my workflow.
Python seems king with images and deep learning, but a lot of its popularity also is from embedded devices and websites. [Google trends doesn't seem to show it moving too much, but there is a slight decline.](https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=r%20programming,r%20language) If you [add python](https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=r%20programming,r%20language,python%20programming,python%20language) you can judge for yourself. 
Whispers: ‚ÄúJuliaaaaaaaa‚Äù
No. &gt; I'm thinking in particular domains like ML/AI (in particular deep learning), MCMC, as well as general data science tasks. Deep learning, absolutely. MCMC, meh. Not sure what new packages are out there, but both R and Python have interfaces to Stan, which is one of the best libraries out there. For general data science, it depends. Python has always been better than R for general purpose scripting, web scraping, working with JSON (dictionaries make it all so easy), and anything involving text (NLP, string processing, etc.). Meanwhile, R has always been great for visualization and building anything specialized without a Python equivalent. I'd never do a survival analysis in Python, just like I'd probably never build a neural net in R. So I don't think much has changed in that regard. However, since the 2014 advent of the tidyverse, R has been blowing Python out of the water for general data processing (i.e. when your data comes in an n x p table). dplyr is far intuitive to work with than pandas indexing, and there are new packages coming out every day that use the tidy data framework. It's led to packages like `sf`, which lets you easily work with spatial data using the same framework you use for regular data, `broom` for model output in columns, and the concept of nested columns where you can store an entire model in a cell. New package development tools have also led to making significantly easier for people to contribute to the R package ecosystem, and this is *after* R's well-deserved reputation as a language where if you want to do anything, someone has written a package for it. Like, there's a package that lets you 3D print a surface plot. Thomas Lin Pedersen, who writes a ton of R packages, just published the first production version of `gganimate`, which lets you write customized animations to an existing static ggplot2 object. The flexibility is unparalleled, and the tidyverse framework means that there are coding, naming, and argument order standards closer to what you'd expect from Python. &gt; I also predict Python starting to encroach more and more into "pure" statistics in the coming years. Why? What are some current projects, other than StatsModels, that have been doing this? &gt; As for evidence, last year R was ranked #8 on the TIOBE index of programming language popularity, while Python was #4. This year R has dropped to #12 (one of the biggest drops), while Python has bumped up to #3. TIOBE is a general programming language popularity list. R is not a general purpose programming language, has never been, and never will be. So of course Python is going to be ranked above R, because R is mostly used for statistics and visualization, while Python is used for everything from web development to machine learning. I'd be more interested in seeing a DS equivalent programming language list. The fact that a domain-specialized language like R is ranked #12 is impressive. I'm also not sure exactly how major a 4-point bump is because I don't know how far apart the rankings actually are.
Less to learn.
Thanks for the thoughtful response! Regarding Python's encroachment into statistics, StatsModels is definitely the biggest effort that I'm aware of. While there is still a definite gap between Python and R in terms of breadth of statistical ability, claims like 'Python can't (yet) do X statistical analysis' are not very future-proof. I had a longer comment typed up along those lines when I came across a post on R-bloggers, [StatsModels: the first nail in R‚Äôs coffin](https://www.r-bloggers.com/statsmodels-the-first-nail-in-rs-coffin/). I don't agree with that tabloid headline, but the argument is basically that it's easy to copy or re-implement existing packages (this is true for both Python and R). But given Python's bigger user and developer community, and (potentially) broader set of tasks that it excels at, getting Python to catch up with R in terms of stats is easier and faster than getting R to catch up with Python in other areas. Once we reach that tipping point, statisticians could start writing the killer packages for Python first. I am not sure that this will really be the case, but I was curious to hear others' input.
When you're just getting started you probably only deal with the functions from the base package. The base package includes functions for the core functionality of R. Since the base package includes functions for statistical analysis I'd say that R base has a greater functionality than other more general languages. Python for example doesn't give you the functionality for statistical tasks out of the box, you have to import pandas and numpy. So R is not "shorter" than other languages, rather the opposite. Also [there is a package for almost every task you want to work on](https://cran.r-project.org/web/packages/available_packages_by_name.html). Other packages like the ones from the [tidyverse](https://www.tidyverse.org/) improve some of the shortcomings of the base package. I'd really recommend having a look at the tidyverse once you have gotten the grip on the way R works. The best way for you to learn R depends mostly on the type of learner you are. I liked the datacamp courses, but courses like coursera or udacity are offering motivated me for a longer time. There are also some pretty good books ("R for Data Science") and lots of tutorials on youtube. 
No. I was a regular user of R and more than 1 year ago I started to use Python for machine learning stuffs. I‚Äôve not seen any dataviz libraries as good ggplot2. I don‚Äôt mean seaborn or matplotlib are bad but ggplot and lattice are awesome libraries. Also there is no any alternatives for shiny. So I don‚Äôt think R is losing ground. 
Most statisticians do not care about software development. They‚Äôre in the feedback loop of ‚Äústatisticians use R so they use R‚Äù. Saying ‚Äúall that has to happen is for statisticians to get on board!‚Äù is like saying ‚Äúall you need to be the World‚Äôs Strongest Man is to lift weights a lot and eat.‚Äù R‚Äôs biggest strength is its use in the statistics community, and the advent of the tidyverse has made it even more likely to stick around. StatsModels has been around for years and still isn‚Äôt close to R‚Äôs level of support, and it doesn‚Äôt seem like it‚Äôs going to happen anytime soon. It can do basic statistics but not much beyond that. There have been many times when I‚Äôve been doing an analysis and needed a super specialized function that was in someone‚Äôs 2005 R package. That package will probably never make its way to Python. Plus statisticians hate coding a lot. R‚Äôs package ecosystem means you can focus on the statistics and easy, intuitive data manipulation. Python is much better for development, but statisticians don‚Äôt want to have to think like software engineers to get their work done. Like pandas has a lot of unintuitive behavior and requires you to make liberal use of writing functions to do most vectorized operations, and its indexing system makes no sense to someone who thinks of data in terms of columns. If a data operation in R takes 30 seconds and takes an hour in Python, it‚Äôs a no brainer which one I‚Äôd pick.
[Nobody knows](https://github.com/jrnold/ggthemes/issues/102) 
Yep, I'll revisit Julia when it has more libraries.
How can she get more libraries if people don't switch to her? Honestly, I need a Julia tidyverse (queryverse is the project but it's incipient) and for RStudio to support Julia and then I'll completely switch over. 
I hear ya, as soon as I'm gainfully unemployed, I'll see about building Julia versions of the R libraries I use.
Hah okay well clue me in so I can help out.
as the saying goes, "RStudio is the RStudio of Python." 
To add, the one particularity about R is that some of the leaders in the statistical community, such as Tibshirani or Hastie, contribute their own packages in R. It provides me with confidence that the statistical tools I use work as intended under the hood. 
the [sf package[(https://github.com/r-spatial/sf) and maybe the [nngeo package](https://github.com/michaeldorman/nngeo) might be able to do what you want. Specifically `st_buffer()`, `st_distance()`, and `st_nn(x, y, k = 1)`. This is sounds a lot like the question asked by /u/vj4 here: https://www.reddit.com/r/rstats/comments/aebamb/how_do_i_use_all_the_cores_on_a_server_to_match/ vj4 seemed to get a workable solution to the problem, and that might work for you too. You can probably find a quicker one using the above mentioned packages. Post some sample data, and I might give it a shot too. Good luck. 
I read some post on a site, can't check since I'm on my phone, but they were trying to say that R isn't a real data science language and Python was taking over. That type of language and "there can only be one, I am the Highlander" attitude is nonsense and the article was a bunch of clickbait. Their reasoning for this was broader adoption of Python in industrial settings. When they surveyed Kaggle users, R was top if I remember it right. So who is right? It comes down to the user, their problems that need to be solved, and the environment it's being tasked in. Those authors in the article I mentioned never brought up the massive set of bioinformatics tools that are developed in R (Bioconductor). I wonder if people asking this question feel that there is ideally one programming language that should be "the best" for everything; learn this and that is all you need to know... But that mindset is not feasible and I can't really see that being the case in the future for most programming applications.
See this post from a few days ago: https://www.reddit.com/r/Rlanguage/comments/aeorg6/finding_observation_in_dataset_a_within_miles_of/ There is an example of running the matching process in parallel in the comments. It's not the exact thing you were asking, but very close. 
&gt;To add, the one particularity about R is that some of the leaders in the statistical community, such as Gelman, Tibshirani or Hastie, contribute their own packages in R. It provides me with confidence that the statistical tools I use work as intended under the hood. You raise a really good point that I think isn't appreciated as much as it should be. Building a ML model in Python with Scikit-Learn, PyTorch, TensorFlow, etc is so easy that almost anyone can do it with just a few lines of code. I've come across numerous articles that claim some of these Python implementations are not 100% correct under the hood. I can't remember the exact examples, but I was rather surprised. R might have the same problem, but because of the rigorous QC by CRAN and the platform's slightly steeper learning curve, bad implementations are less likely to be as widely propagated.
Just wrote my first julia script yesterday. A real pleasure to have fast for loops (no need to contort myself to vectorize everything), no GIL, and clean syntax. Definitely going to use it more. Of course will continue to use R and python for the great libraries, but also looking forward to trying out the Pycall and Rcall Julia libraries -- hoping that Julia will be a nice environment to glue together R and python code and get the best of all worlds.
Wat
Rstudio is basically the best ide around. People keep asking ‚Äúwhat‚Äôs the Rstudio of Python?‚Äù ie what‚Äôs the best IDE for python? Rstudio now supports some python in its docs so the saying is ‚Äú*Rstudio* is the Rstudio of python.‚Äù
They‚Äôre different tools for different circumstances. So no. 
&amp;#x200B;
I would say the Rstudio of python is \[Dash\]([https://github.com/plotly/dash](https://github.com/plotly/dash))
If you want bands of 25 for example, make the dataset tidy so it only has 2 columns, value and year. Then add a column with something like ifelse(value&lt;=-25,"value&lt;=-25",ifelse(value&lt;=0,"-25&lt;value&lt;=0,ifelse(value&lt;=25,"0&lt;value&lt;=25","value&gt;25"))) so that you have a column of labels to cluster colour by (lats say called "band" . Then, aes(x=year, y=value, colour=band) + geom_point() should do it. Is that what you're looking for? 
The issue has been fixed just now.
Instead of reaching for Julia, just use numba to JIT a python method that you don't want to have to vectorize.
You mean shiny? 
Yeah, I really should try out numba, certainly a much easier way to JIT coming from Python. Still, it doesn't solve the GIL problem -- it was nice to easily use multiple threads in my script. Otherwise would need to drop into Cython, or use multiprocess and incur the cost of passing the data to each subprocess. &amp;#x200B; In any case I'm glad I tried out Julia, seems like a very nice language with a lot of nice features (I'm particular intrigued by some of the lispy aspects of it), and it's fun to learn new programming languages.
I know everyone loves the tidyverse (mentioned in a few comments here), but if you have checked out data.table yet, I recommend you do so. Especially if you're trying to process large datasets. 
No, not at all, I think for analysis the opposite may may actually be happening. I think one reason it may sometimes seem like python is "winning" is that the whole field of data analysis is seeing tremendous growth and Python being bigger is therefore growing more in absolute terms and more non-stats people are entering the field. Python may be better for general purpose programming but for specialized analysis I feel really uncomfortable using Python packages written and maintained be "generalists" when there is an R package written by the actual, peer reviewed author of the method I want to use. Another important difference is that the whole ecosystem of R is far more consistent and rigorously maintained. R packages on CRAN are checked against all current version of R several times a day. Packages that fail are quickly removed. Dependency resolution works really well. More generally I see R evolving at a much higher speed than Python, so I wouldn't fear for R's survival.
The direct answer to your question would probably be the `cut` function. Another option could be to select a more diverse colour scale.
Ah, you are right. Got them confused.
&gt; It provides me with confidence that the statistical tools I use work as intended under the hood. Easily one of the biggest issues with the pydata ecosystem, in particular scikit-learn. [Rant.](https://www.reddit.com/r/statistics/comments/8de54s/is_r_better_than_python_at_anything_i_started/)
But this is true for any language. R and CRAN have shit packages too. Don't just automatically trust a package because it's on CRAN. There are some really really lazy packages or there.
http://cloudyr.github.io/
Yeah, I recall reading this. It's unreal to think that so many people use these tools with complete trust without checking if they're doing what they're supposed to be doing. It concerns me that so many who consider themselves data scientists/machine learners don't at least make sure that the tools they're using are actually working properly.
Apparently [numba has support for parallelization](http://numba.pydata.org/numba-doc/latest/user/parallel.html), though I haven't tried it yet.
No idea, but I‚Äôd recommend learning the keyboard shortcuts - it‚Äôs much faster. 
Have you tried updating your version and your packages? Could have something to do with your graphics card (happend to me when a part of my plotly-output in a shiny app always disappeared), but a quick driver-update fixed that.
I have not, but that sounds like a good idea! I'll try that and see how it goes. Thanks so much
That's some solid advice. I know a few and it's definitely quicker and makes me feel cool... Thanks for the heads up! 
There appears to be one in the h2o package. https://www.rdocumentation.org/packages/h2o/versions/3.20.0.8/topics/h2o.cummin It might be in a smaller package somewhere though.
You can inspect the source code for whatever function you are wondering about.
I'm sorry how does one go about doing that
Try typing the function at your console with no () or google the function name and source code.
cummin is a primitive, so you can't actually inspect the source code by typing the function name into R. Is this a homework problem? The cumulative minimum is the minimum of df\[1:row\]. Does that help lead you in the right direction?
No this is just for my own work I'm doing. So I need it to go through a column of data and find the cumulative min
I didn't actually care much either way, just sounded like a homework problem. Sometimes it's helpful to just open up a spreadsheet and go through it there and it'll just click after playing around a little. In your first column, use RANDBETWEEN() to generate a column of random numbers. In cell B1, use =MIN($A$1:A1) and drag down. The A1 will change to A2,A3,A4,A5 as you go down. So just think through that process and code it. For the first row, of course it's just the min. In the second row, it's the min of the first and second row. In the third row, it's the min of the first, second, and third rows... and so on (assuming we're talking about a single column, of course).
Yes it's just one column. I guess my issue was figuring out how to reference the 2:n column to look through for the min 
Gotcha: set.seed(1) DF &lt;- data.frame(numbers = rnorm(25), mincum = NA) for (i in 1:nrow(DF)) { DF[i,2] &lt;- min(DF[1:i,1]) } DF &amp;#x200B;
Right which is why I also suggested looking for it somewhere else. I was trying to help the OP by expanding their options and seeing if they could find it on their own.
I just hadn't seen anything like this out there. Personally, I've used excel for selection of primary sampling units, and this was made mostly for myself. I'd be interested in any comments / criticisms you have. The code is available on github, if you have any interest.
##r/global_mande --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/9wy10w/ignore_list/)
A general algorithm that is not the best way to do it in R but is illustrative, in C-style pseudo code: # you have a vector x of length n and a result vector y y[0] = x[0]; for(i = 1; i &lt; n; ++i){ y[i] = y[i-1]; if(x[i] &lt; y[i]) y[i] = x[i]; } Turning it into an actual function or doing it better or doing it in R is an exercise for the reader.
One option is to use the data.tree-package and convert from nested list &gt; data.tree &gt; data.frame. Depending on your input: library(data.tree) library(dplyr) tools &lt;- list() tools[["Fasteners"]] &lt;- list() tools[["Fasteners"]][["Screws"]] &lt;- list() tools[["Fasteners"]][["Screws"]][["Machine"]] &lt;- list() tools[["Fasteners"]][["Screws"]][["Tapping"]] &lt;- list() tools[["Fasteners"]][["Screws"]][["Machine"]][["Round Head"]] &lt;- list() tools[["Fasteners"]][["Screws"]][["Machine"]][["Pan Head"]] &lt;- list() tools[["Fasteners"]][["Screws"]][["Machine"]][["Flat Head"]] &lt;- list() tools[["Fasteners"]][["Screws"]][["Tapping"]][["Type A"]] &lt;- list() tools[["Fasteners"]][["Screws"]][["Tapping"]][["Type AB"]] &lt;- list() tools %&gt;% FromListSimple() %&gt;% ToDataFrameTypeCol() %&gt;% select(Parent = level_2, Child = level_3, Type = level_4, Style = level_5) Which returns: |Parent |Child |Type |Style | |:---------|:------|:-------|:----------| |Fasteners |Screws |Machine |Round Head | |Fasteners |Screws |Machine |Pan Head | |Fasteners |Screws |Machine |Flat Head | |Fasteners |Screws |Tapping |Type A | |Fasteners |Screws |Tapping |Type AB |
Agree on all points. Would also add that RStudio offerings like (Pro/Connect) can be very enticing for larger enterprises to buy into, especially those used to SAS. Not sure a Python equivalent to that ecosystem exists currently. With tidyverse basics being relatively easy to pick up, I also have a hard time imagining that educational institutions are going to abandon it anytime soon. I can see a situation where organizations who hire mostly CS type grads gravitate towards Python, those who hire more maths/economics types gravitate towards R, and those who hire a mix try to support both over the near term.
I have never had to put this encoding flag in my packages in order for roxygen2 to work.
 Tried taking it out, but roxygen2 throws a warning about it, same as reported here: https://stackoverflow.com/questions/51694929/warning-about-utf-8-with-roxygen2 It will still build the package, but I'm not sure if any of the documentation will fail to render. The packages are quite large, so it's a pain to search all the help files to check removing the encoding file didn't break anything.
The documentation renders when you run roxygen::document() so you can check it first.
 It is quite technical to read as I have not much time and I even tried to get set up from the [high school assignment help](https://www.essaywritinglab.co.uk/assignment/) online page but I could not get. Thank you so much for sharing this. 
&gt; RStudio is the best ide I‚Äôve ever used. We hear that a lot but, I bet, never from programmers: RStudio, while nice, is incredibly bare-bone compared to mature IDEs for other programming languages. Where RStudio shines is as data exploration toolkit. But for *programming* there are other IDEs, some of which have been around for decades, that are in a completely different league compared to RStudio (such as Visual Studio, IntelliJ, CLion, etc).
That error message suggests that your variable names are enormous as it says that variable names can't be bigger than 10k bytes. I'm guessing that 10000 bytes for a variable name would be 1 variable with 10k characters as its name? That sounds like an error in your csv to me. Check names(df.list[[1]]) to see if anything weird is going on there. As an alternative, can you just use unlist(df.list) and then reshape from there? 
Thank you. Only one of the csv files was messed up, causing all the trouble. 
Awesome glad you figured it out :) 
Yes you are right about programmers but title seems like comparing R and Python. Also I‚Äôve a statistics background and using just R and Python like I said. So I‚Äôm not a programmer and never used any other IDEs for another programming languages. 
Btw, you should use fread from the datatable package. Its is insanely fast. I'm amazed by it everytime
readr::read\_csv is also faster than read.csv, although not as fast as datatable::fread.
Can you give an example of the data? These all seem imminently solvable in either base R or the dplyr package. I'm not sure what Version A and Version B mean, but I'm imagining it would be clear in the data.
why not just use an ordinary data.frame/data.table/tibble?
Not entirely sure from the question what you are after, but if it's versioned data as a project evolves have you seen quilt? https://docs.quiltdata.com which has R bindings. 
Thanks for the link. This looks great!
No worries
It's probably being caused by the fill=NA in the first code stack and the second one probably isn't mapping fill do to the fill=districts@data$index not being wrapped in aes(fill = )
Thanks. If i remove fill=NA in the first code block, then I get the error: "Error: Aesthetics must be either length 1 or the same as the data (1647057): fill"
I would recommend you to use the sf package with geom_sf( )! https://github.com/r-spatial/sf/blob/master/README.md
Have you ever tried RMarkdown? You can easily create a report using HTML features only running R code. For example, you have a process which you get this base, do some changes to it, and then you want to know first observations, etc. You use R to find these informations and put on the report. And stays something processual: you can create a Function that renders the report at the end of the script, so you don't need to do anything manually. See if that's suits you!
The problem sounds like it is literally what the error message says. The best way to make sure the mapping aesthetic is the same length is to have it as a column in the same data frame. 
What you're actually asking for is a data description report. You could build your own using RMarkdown and some R code. [I'd suggest you have a look at this article that will introduce you to a number of tools that might help you](https://dabblingwithdata.wordpress.com/2018/01/02/my-favourite-r-package-for-summarising-data/)
I love R, but why not use a spreadsheet? I think you just need to write documentation. Or are you asking how to sync the fields with your documentation?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rstats] [R Shiny Plotly Help with selecting Multiple dates. Reproducible example](https://www.reddit.com/r/rstats/comments/agg2f9/r_shiny_plotly_help_with_selecting_multiple_dates/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Two things. 1. Remove [`as.Date`](https://as.Date)`()` from your vector of dates. Selectize Input is returning strings for you to filter on. 2. In your filter, change `date == input$start_date` to `date %in% input$start_date`. 
Awesome, thank you. So in my data frame if my dates are formatted as dates should I change them to character? 
I've used R Markdown but I want an excel output so I can sort and filter the file. It will be very large as the dataset has over 400 variables
Thanks, I'll take a look
Because I want an external documentation of the meta data. 
Nice! Also, the chart makes more sense if the `total_time` is a numeric. Hopefully, that's just true in the example and not the actual data.
It's 10 years of data and over 400 variables. This dataset is for a live project so it undergoes occasional change, as such, if I set up a flexible script, it makes my job easier
That or you can cast `input$start_date` to a date before comparing with `%in%`
Incredible! Thanks. My little noob brain was about to explode. 
Don't you need to manually define a data dictionary? Maybe you could automate the categories of responses (like A, B, C versus A, B, C, D), but you'd still need a human to record what it means. I get that you'd like to automate, but a data dictionary would mean sophisticated AI in my thinking. 
=)
One last question. When I‚Äôm ‚Äúcasting‚Äù the select input start_date to be a date, which seems to be the easiest way to go about this, do I do that in the reactive function it‚Äôs self? And do I use cast? Or would I be using some form of as.Date. Sorry, I‚Äôve been trying to work through this and still seem to be running in a circle. If you have a sample of what concerting input$start_date to a date would like, that would help me greatly 
Do you have the ability to change the pivot table? You should be able to make the format tabular with repeating items so it looks like a data table, just make sure you turn row and column totals off. From here you can basically duplicate the table with formulas off to the side. For example, if the pivot table is in columns A-D, in E you can simple put =A2, F would be =B2, etc. Then you can either copy/paste into another sheet or workbook to use. 
If you are set on regression trees, you can use: 1) Ranger for random forrests 2) xgboost for gradient boosting Both are good packages. However, it seems to me like what you *really* want is an interpretable model from which you can extract insights. You are less interested in actual model accuracy. Your best bet for this will be a GLM, from which you can extract the coefficients, create confidence intervals from your coefficients, determine valuable features with lasso regression, and many other things. For this, I would use the [glmnet](https://cran.r-project.org/web/packages/glmnet/glmnet.pdf) package, which allows you to perform cross validation of elastic net regression very quickly.
While I‚Äôm interested in seeing if there is, this type of visualization is wholly for aesthetic and not practical purposes. You lose no insight from removing the human figure while retaining the number.
I'm certain that it is possible, but I'm not sure what library would make it easiest. I'm curious as to how you did it in Excel. Just a column chart with shapes on top?
Yeah i agree, that it doesn't add any extra insight, but I am wanting to make a dashboard and wanting to pretty it up and to try and humanize the data somewhat. I know you could probably just show percentages. 
yeah i just did a column chart. Compliant| Total| label y location| lable x location ---|----|----|---- 53%| 100%| 75% | compliant then you paste a human shape over the top. make gap width 0, series overlap 100%. make it a combo chart with a scatter for the label positioning. turn the transparency of the 100% bar down a bit, delete all the extra bits like axis, gridlines etc 
Make an SVG in illustrator/Inkscape with the shape of the man. Add a layer behind that is masked by the man. Then using R you can change the height of the masked layer. SVG is a text format, so you can just do a find replace. Not a very R based solution, but it will work.
Why is it wearing pants on it‚Äôs hands?
print(‚Äú54%‚Äù) Done
Yeah I know we can do that but still interested in the part on graph
I will have a look at this. 
Anyone use this / have any opinions on it? Recently started using rsample for train/test splits.
I second this response. I would also add to check out the caret package for easy use of glmnet with cross validation, etc. 
Recipes might be my new favorite package. Essentially handles all of the data preprocessing steps. It allows you to do all the preprocessing on your training set, then apply the same actions to your test set very easily. Parsnip is great, but tidymodels doesn‚Äôt have a grid-search cross-validation yet. I‚Äôm at the rstudio conference right now, and Max Kuhn (the writer of caret and many of the tidymodels packages) said it‚Äôll be coming this year. Once it adds that I‚Äôll be all in on tidymodels.
Mittens?
How does it compare to mlr, in general? I‚Äôve been using that for a while now and am really impressed at how simple it makes pretty complex tasks right from pre-processing through to benchmarking different algorithms.
Eagerly waiting for cross validation in parnsip and understanding dails. The tidymodels seems amazing, giving a tidyverse feel for modelling. This is definitely in my bag for a long term. 
It seems so tasty - I would love an O‚ÄôReilly book like ‚ÄòR for Data Science‚Äô that covers tidymodels in 2019
Delicious 
A glm will show which predictors have a strong linear correlation with the target, but there's no apriori reason the predictors need be linearly related. I'd think a tree based method isn't a bad place to start, it will certainly give you a much easier path to dealing with the large number of correlated factors on census data. Rather than trying to interpret the tree structure, however, you should focus on measures such as variable importance to see which predictors are really driving the model.
I think in the end it'll be a bit "tidier" than mlr. For example in recipes when you are performing an action on the data you can do use tidy selection of variables. Here's an example of the workflow. library(tidyverse) library(tidymodels) library(AmesHousing) ames_df &lt;- make_ames() # Generates an Ames dataset split &lt;- initial_split(ames_df) # Creates a split object train_df &lt;- training(split) # Extracts the training set test_df &lt;- testing(split) # Extracts the testing set cleaning_recipe &lt;- recipe(Sale_Price ~ ., data = train_df) %&gt;% step_range(all_numeric(), -Sale_Price) %&gt;% # Scale from 0 to 1 step_dummy(all_nominal()) %&gt;% # Create dummy variables step_knnimpute(all_predictors()) %&gt;% # Impute missing values with knn step_nzv(all_predictors()) %&gt;% # Drop near-zero-variance predictors step_interact(~starts_with("Central_Air"):Year_Built) %&gt;% # Interaction terms prep(train_df) # Declare dataset to be used as the basis for cleaning clean_train_df &lt;- cleaning_recipe %&gt;% bake(train_df) # Apply cleaning to train_df clean_test_df &lt;- cleaning_recipe %&gt;% bake(test_df) # Apply same cleaning to test_df As of right now I think mlr does more than tidymodels can. Everything in tidymodels is built to be extremely malleable at any point in the process, but for a while longer it doesn't have the same functionality that mlr has.
That‚Äôs interesting, thanks. From what I can tell tidymodels seems to take the Unix modular philosophy of do one thing, and do it well. That has the plus side of being very obvious exactly what is happening at every step. On the other hand it appears to make it relatively more verbose and involving to change/modify. My experience so far with mlr is (provides you‚Äôre not building bespoke functions/methods) nearly everything has a relatively quick and easy way to modify what‚Äôs happening. It‚Äôs more holistic, in that sense - which has pros and cons, too. It can be a bit opaque exactly what is/isn‚Äôt happening (although when you read the literature, it‚Äôs always doing the sensible thing, anyway). On the other hand, it takes barely a few lines to set-up test/train split, pre-processing, tuning, prediction, accuracy tests etc etc.
It sounds like your data has been read in with day as a factor, not a number. Try converting day to a number first data$day &lt;- as.numeric(levels(data$day)[data$day)]) Then you code should work. 
If I understood you correctly, you want to continue summing up even though the variable's value has went from 100 to 0. Below code should work, assuming your original variable is called 'vector': Reduce( function(x, y) { if (x / 100 &gt;= 1) { y + 100 * (floor(x/100)) } else { y} }, vector, accumulate = T) What you need to understand is that Reduce function takes each two following observations and in case the first one has already reached 100, the second one should be equal to 100*floored(last_peak/100). Let me know whether it worked :)
Thank you! Sorry I should have edited my post. I actually figure it out like 2 minutes before you replied! Yes, I had to convert to numeric. Here's what I used: &amp;#x200B; Data$day &lt;- as.numeric(as.character(Data$day)) 
Hey thanks for the suggestion, this might be a good lead for me to start with! The one thing I can see now before trying it though, is that I just assumed those peaks to be 100. If you look closely at the plot, it is not consistent and all the peaks are a slightly different y when it resets/purges. This point is influenced by atmospheric pressure.
Check out using select in dplyr https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf
Thanks! it worked
Maybe this? &amp;#x200B; `library(dplyr)` &amp;#x200B; `df &lt;- data.frame(` `period = seq(1,20,1),` `original_value = c(rep(c(2,15,25,34,50,62,71,84,91,99),2))` `)` &amp;#x200B; `df &lt;- df %&gt;%` `mutate(net_value = original_value - lag(original_value),` `net_value = ifelse(net_value &lt; 0 |` [`is.na`](https://is.na)`(net_value), original_value, net_value),` `cum_value = cumsum(net_value))`
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rstats] [How to overlay a color gradient over a image from a distribution?](https://www.reddit.com/r/rstats/comments/ah749n/how_to_overlay_a_color_gradient_over_a_image_from/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
cumsum(x) %% Y
 x &lt;- rep(1:10, 2) cumsum(pmax(0, diff(c(0, x)))) # c(0, x) so initial value is captured if non zero 
I‚Äôm not sure how accurate this is but I generally monitor memory usage from the os though task manager or activity monitor.
I'd look into lubridate package and try to use their date conversion rules. [https://lubridate.tidyverse.org/](https://lubridate.tidyverse.org/)
Generally, I do too when I run scripts on my PC, but I am running this from a shell script through a SLURM job scheduler and I'm not sure if that's possible. But thanks for the reply 
Converting dates works better if you start out with strptime() to get it into POSIX* format. That makes it POSIXlt, which is a list format suitable for sub setting but it doesn‚Äôt order well. You can convert that using as.POSIXct which makes it numeric (epoch seconds), which orders fine. as.Date works fine from POSIXlt or POSIXct, but you gotta watch the time zones or you could end up +/- one day when it converts from UTC You can try the lubridate library but honestly you should learn how time works in base package too, it‚Äôs a common source of issues 
One way I could think to do this would be to use a for loop that iterates through each of the users and runs the `lookup_users()` function from the rtweet package. Parse and store the results in a data frame or a list. A for loop might not be the most efficient way to do it, but it would work.
Why aren't you using the Twitter API? It has a [GET Followers] (203 https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-followers-list.html) method. 
Yeah, maybe I should have phrased my question better. I can get followers by using rtweet as well. My issue is that I have a list of something like a 1000 different users I want to get followers for. Now with rtweet that means separating each user id with a comma. That's going to take forever with a 1000 different user id's and I don't imagine it's any different with Twitter API. I'm mostly wondering if there's some way for me to take this large list of user id's from excel, place it as is without manually adding a 1000 commas, and get followers.
What's your programming experience like? It shouldn't be difficult to write a Python script that takes your ID list and outputs a comma-separated list.
It's virtually non-existent tbh.
I'm still not understanding what you're trying to accomplish. Can you please post some example data? For example, assume that you've stored your data in a `data.frame` called `df_tweets`. In that case, please run the following command and paste the output here. NUM_SAMPLES &lt;- 50 dput(df_tweets[sample(rownames(df_tweets), NUM_SAMPLES), ]) 
It doesn't let me run it. Returns a warning: "Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE'"
That tells me that you don't have 1,000 rows, which assumed was the case. Instead, try this. dput(df_tweets)
I accidentally used the previous command on a different dataset with only 2 rows. this is what it returns when i was dput(df_tweets) on the right dataset: )), row.names = c(NA, -3064L), class = c("tbl_df", "tbl", "data.frame"))
rep(age, freq)
rep(c(19,25,31), times = c(2,5,10))
rep(age, freq)
You can use the rep function for this: `age &lt;- c(19, 25, 31)` `freq &lt;- c(2, 5, 10)` `yourData &lt;- rep(age, freq)`
 plot(data$Longitude, data$Latitude, type = "n", xlab = "longitude", ylab = "latitude") then your command
Not ure if I interprete your question correctly. Do you mean this? age &lt;- c(19, 25, 31) freq &lt;- c(2, 5, 10) rep(age, freq) # 19 19 25 25 25 25 25 31 31 31 31 31 31 31 31 31 31 See `?rep` and the `times` argument
If your having trouble with the other comments, try the argument each= within the rep() function
Its unnecessary. If users, movies in edx are in validation, and Edx is split into train and test so that each have all users and movies in edx; Then all 3 validation, test and train shall have all users and movies. The trick is how do u do such splits for users &amp; movies with one or two ratings?
Can you please help explain further why it's unnecessary? The validation set was specifically created so it wouldn't contain users or movies that the edx set doesn't have (the edx set has some users or movies that validation set doesn't have though). I would think that it's good if the train set, test set, and validation set all have the same users and movies (although not the same combination of user and movie, of course)...
I think you are mistaken in what CRAN checks in a package, you could have a t-test that spits out random numbers and it would be accepted. 
Thank you, that worked
Impossible to be sure without looking at your dataframe but try grouping by them number &lt;- data %&gt;% group_by(State, Longitude, Latitude) %&gt;% summarise(Count = n()) 
You need to use the paste function when specifying the filename, and just pull the value from the column directly. Assuming col3 will *always* be the same value: write.csv(output, file = paste0(output[1,]$col3,"_output.csv")) &amp;#x200B;
Map ```color```, not ```fill```, to the ```result``` column: overall_plot &lt;- ggplot(overall_df, aes(x = width, y = height, color = result)) + geom_jitter(width = jitter_width, height = jitter_height, size = psize, alpha=opac)+ labs(x = NULL, y = NULL) + scale_y_continuous(labels= c("B","M","T"), breaks = c(1,3,5)) + scale_x_continuous(labels= c("R","M","L"), breaks = c(1,3,5))
oh ok, thank you. How do I then specify the colours that will be chosen ? the default here (on my system at least) is to choose a light blue and light grey. I'd prefer red/green for this. cheers
First, if the values are numeric make sure there are converted to a factor (or character): aes(..., color = factor(result)) To modify a color scale: scale_color_manual(values = c("red", "green)) https://ggplot2.tidyverse.org/reference/scale_manual.html 
ah ok, thanks a lot. 
Yes. I know. I have a dictum that all computer advice that has not actually been tried is worthless. Hence all computer books that are not written using literate programming (Rmarkdown or similar) are worthless. I did not follow my own advice in what I originally posted (and completely erased in my edit). I felt guilty and checked, and found that I had spouted nonsense. Then I found what worked and posted that. And the correct is a lot simpler.
You'll first want to "melt" the data frame into a long format instead of a wide one you have here, which will make it much easier to process. The wide format is much more user friendly for collecting data, but not so much for analysis. Because of this, I have a number of scripts that take data I, our students, or our technician generate and convert them into a long format. I use the word "melt", because that is the function name for my preferred way of doing it (i.e., the melt() function in the reshape2 package). There are other methods, but that's the one I like. For what I do I typically have columns formatted this way: ID| Time| Covariate (e.g. exposure status)| Dependent variable type (e.g. gene 1, gene 2, etc.)| Dependent Variable data (e.g. your gene's 'text') ---|---|----|----|---- mouse 1| 1| exposed| gene 1| &lt;gene 1 text&gt; mouse 1| 1| exposed| gene 2| &lt;gene 2 text&gt; mouse 2| 1| control|gene 1|&lt;gene 1 text&gt; Then you can use dplyr functions to summarize as you see fit (just use the group_by function first). Also makes it easier to plot, too. This is also the preferred way to have your data if you want to do any sort of regression analysis. 
That looks really good, thanks for your help! 
Thank you. This works perfectly. 
you might want to check out the book [R for data science](https://r4ds.had.co.nz/) it outlines the justification for why /u/More_Momus suggested that you organize data this way and also a lot of other useful tools for manipulating data in R.
Will do, thanks for pointing me in the right direction!
Are you trying to just print out a list, or do you want to put the results in a vector, etc?
I'd like them added to a column in df. I have a solution in a loop, but was after a more elegant solution. # Add D2 D3 D4 D5 D2 &lt;- NULL D3 &lt;- NULL D4 &lt;- NULL D5 &lt;- NULL H01_col &lt;- which(colnames(r100X) == "HUP01") Max_col &lt;- ncol(r100X) r100X &lt;- r100X[which(!is.na(r100X$HUP01)),] system.time( for (i in 1:nrow(r100X)) { D1.Col &lt;- max(which(!is.na(r100X[i,H01_col:Max_col]))) + H01_col -1 D2 &lt;- c(D2, ifelse(D1.Col -1 &lt; H01_col, NA, r100X[i, D1.Col -1])) D3 &lt;- c(D3, ifelse(D1.Col -2 &lt; H01_col, NA, r100X[i, D1.Col -2])) D4 &lt;- c(D4, ifelse(D1.Col -3 &lt; H01_col, NA, r100X[i, D1.Col -3])) D5 &lt;- c(D5, ifelse(D1.Col -4 &lt; H01_col, NA, r100X[i, D1.Col -4])) } ) r100X$D2 &lt;- D2 r100X$D3 &lt;- D3 r100X$D4 &lt;- D4 r100X$D5 &lt;- D5 rm(D2) rm(D3) rm(D4) rm(D5)
If I understand you correctly, you have a matrix m and a vector v. v has the same length as the matrix has rows. You want to return the vector whose i-th element is the (i, j)-th element of the matrix such that the i-th element of v is j. set.seed(01212019) n_col &lt;- 10 n_row &lt;- 20 df &lt;- matrix(rnorm(n_col*n_row), nrow = n_row) col_selector &lt;- sample(1:n_col, n_row, replace = TRUE) diag(df[,col_selector])
Your code here looks very non-R, so it's good you're looking for another approach. I have a few suggestions, but it's hard to give exact advice without more context. This code has several "code smells" that suggests that there's another approach that would work better. For instance, looping over rows, logic based on column position, and growing objects in a loop. For a quick speed up, we could address the last of these. Initialize your Dn objects to their full size. If they're storing integers, that would look like `D2 &lt;- vector(mode="integer", length=nrow(r100X))`. And then assign into each element in the loop, e.g. `D2[i] &lt;- ifelse(...)`. This would eliminate one of the problems, and should be faster. A better solution would require more fundamental changes. First, I'd recommend learning about either data.table or the tidyverse packages to make it easier to interact with data.frames. Then, I would guess that the information currently encoded in column position would be better represented as a separate variable, meaning this data would make more sense "long" than "wide". For the tools to help you do that, look to data.table's `melt` or tidyr's `gather`. For some motivation on why, read a bit about the concept of "tidy data". Essentially, all the columns like "HUP01" should be collapsed into just 2 columns: one called "HUP" with the values currently in the cells, and another called let's say index, with the values currently in the colnames ("01", ...). If this data were tidied up, my guess is that this same operation could be done with some by-grouping logic, with no loops at all, and very quickly. Essentially the elements that currently define rows would become the elements you group by, and you could do your logic against your new index variable rather than column position. Sorry if that's vague, but it's hard to get more specific without more context.
One way to do this is to create two datasets, one for each season, then inner join them on the individual. This may not be optimal, but it is easy to read/understand and will definitely work
&gt;!observations %&gt;% group_by(individual) %&gt;% filter((any(month &gt; 5) &amp; any(month &lt; 9)) | (any(month &gt; 9) &amp; any(month &lt; 13)))!&lt;
I did this and it worked, took a couple of steps and could've probably been done faster but if it works I'm happy. Thank you!
If you're open to IRC, there are a lot of helpful and knowledgeable people in #r on freenode.
Thanks I'll take a look
See apply
It's two different "data" objects. &amp;#x200B; In the first call you have data = fcsSet, then later you add new data, splitff. &amp;#x200B; You need to create one object that contains both sets of data for it to work this way. 
Props to you for doing flow data in R like this. I can‚Äôt figure out these packages to save my life. Not sure if it‚Äôs because I‚Äôm bad at flow or bad at R. 
But splitff was split off from fcsset, so is a subset of fcsset although they are now two different dataframes. Why is this different from using a subset for geom\_encircle's data argument like this (direct from the ggalt package's example)?: gg &lt;- ggplot(mpg, aes(displ, hwy)) gg + geom\_encircle(data=subset(mpg, hwy&gt;40)) + geom\_point() &amp;#x200B; Also I though you could combine different data objects into single plots, as in the examples here: [https://stackoverflow.com/questions/21192002/how-to-combine-2-plots-ggplot-into-one-plot](https://stackoverflow.com/questions/21192002/how-to-combine-2-plots-ggplot-into-one-plot) and here: [https://stackoverflow.com/questions/40297206/overlaying-two-plots-using-ggplot2-in-r](https://stackoverflow.com/questions/40297206/overlaying-two-plots-using-ggplot2-in-r) where answers demonstrate plotting two different dataframes on the same plot.
This guy flows
The scales are different in both plots. Try moving the geom_encircle to just below the first geom. 
I stand corrected, but I think the poster who just responded to you has it correct. Move that geom to below stat\_density, that SHOULD fix your problem. 
The scales look the same to me, except for the number of significant digits on the x-axis. (But yeah why is that? Shouldn't added geoms just inherit the aesthetics etc from prior?ü§î) Ok, I tried: ggcyto(data = fcsSet\[\[1\]\], aes(x = 55, y = 59)) + geom\_encircle(data = splitff\[\[4\]\],inherit.aes = T)+ geom\_hex(bins=300) + stat\_density\_2d(colour = "black",bins=100,size=.2)+ xlim(-.1,4) + ylim(-.1,4) + scale\_x\_continuous(trans = "log10", limits = c(NA,4)) + scale\_x\_continuous(trans = "log10", limits = c(NA,4)) &amp;#x200B; And got the same result, including the x-axis "difference."
Thx, but didn't help üòû 
trying to anyway...
Nah, ggplot2 can easily handle multiple data.frames. Each geom and stat can have their own data.frame passed through the `data` argument.
I did. One function that makes it in just about every project I use is: `%notin%` &lt;- Negate(%in%) Because otherwise I find myself using this pattern way too much: !x %in% y and I really dislike the way that looks.
Why do you use scale_x_continuous(trans = "log10", limits = c(NA,4)) twice in a row? Also, check the names of the variables. One plot has "V1" and the other "V1 (4)", and it looks like it's faceting (dividing the plot) by that name.
Good catch on the duplicate row. I removed it and no change. It‚Äôs says V1(4) because it is the 4th dataframe in a list of data frames, each of which is a different subset of the same larger dataframe. Even when I rename a new dataframe (rename_splitff &lt;- splitff[[4]]) and then use that as the geom_encircle data, it still plots as V1(4).
Start learning R and then check coursera for some good data science courses.
You were right, it was faceting by name. I had to add this line to get it to not do that: facet_grid(rows = 1,cols = NULL) Seems like a weird necessity, but now I get a single plot, thanks!
Interesting. Do you always load plotmath or have you defined it outside of that library?
Oh, wow, I've never seen the plotmath package before. But `%in%` is in base R and I just stick the code I wrote above wherever I'm keeping my helper functions.
 happy_yes_no &lt;- function(p) runif(1) &gt;= p n = 700 drinks = sample(c("tea", "coffee", "water") happy = sapply(drinks, function(x) switch(x, tea = happy_yes_no(.25), water = happy_yes_no(.1), coffee = happy_yes_no(.9))
%in% is party of base r, but I'm pretty sure %notin% is party of plotmath, not base r . Maybe I'm confused?
&gt;My notes about "Structuring ML projects" Part IV | Coursera Deep Learning Specialization Thank you for your comment! 
Right, what I'm saying is I just make my own function `%notin%` instead of importing plotmath (which I didn't existed an hour ago).
Whoa, surprised that the latter form works. I would have assumed that you'd need parenthesis and the ! operator would have higher priority than the %in% operator
That's precisely why I don't like it. It's not at all obvious what the `!` is negating. Before I learned about the magic of `Negate` I just made a point of using parentheses to make it more readable (i.e. `!(x %in% y)`
&gt; `%notin%` &lt;- Negate(%in%) &gt; Am I the only one that this isn't working for? I had to do this: ``` `%notin%` &lt;- Negate(function(x, y) x %in% y) ```
Sorry, it should be: `Negate(`%in%`)`
&gt; Negate(`%in%`) Yep works now.
Oh, also in your above snippet (which I appreciate you sharing), you need the backticks around the %in% when passing it to the Negate function.
Yeah, I should know better by now than to post code online before running it first :/
OHHH! that explains why I was confused. Why don't you make a package for %notin%
I guess because it feels silly to make a package just to import a single one line function. I don't know if that's justified or not--using a package would always make it explicit where that function was coming from and definitely adheres more to DRY. 
You could try to download your Facebook data and mess with your conversations data. E.g. check how much letters you exchanged with friends. Put it on a graph to get a nice timeline of your social life. I think it should be approachable for a beginner and fun.
thanks, so here the 0,1 value of happy is based off the drink, rather than the drink being based on the value of happy. In order to make this work for more variables it would need to be the other way around wouldn't it? thanks
Nice. I usually do it this way: "%!in%" &lt;- function(x, y) !(x %in% y)
Depending on how deep you want to get, you could look at your Spotify data and do some exploratory analysis. You could even look to recreate some forms/tests in google forms and analyze the responses for relationships. I‚Äôd also recommend playing with RMarkdown. I found that to be a huge help in learning how to add R to my workflow.
100% agree on Rmarkdown with ggplot2. I have been able to automate presentations and powerpoints with it. Has really helped out my professional life. A couple other good packages to look at are stargazer and plotly
If you're a psych student, I'd suggest talking to one of your teachers/professors/lecturers and asking them if they have a dataset that they have been meaning to analyze, clean, model, visualize or something like that but have not been able to either because of technical inability or time. Don't promise them anything, but tell them you are working on you R and want to help out. If you get the data and are able to do what they request, you might be able to write a paper out of it if you find what they were looking for and get co-authored for just having done the work. It'll also teach you good practices in terms of data cleaning, communication, and whatever. "Fun" projects are great for time killers and slow learning, but having a goal in mind really puts the right heat under your feet. 
I started by analyzing my fantasy football team scores with it
This about as excellent a recommendation as you will ever get. +1
You can run government data through a regression with other publicly available data and look to find interesting correllations. For example, you could ask if the W/L record of local sports franchises affects suicide rate or maybe see if average temperature has an influence on violent crime statistics for a given country.
I like figuring out the odds of winning games. For example I'm playing a dice game with my toddler. There's an orchard with 4 kinds of fruit in it, and the goal is to "pick" it all before the crow "arrives". You play by rolling a single 6 sided die. 4 of the sides correspond to one fruit, allowing you to "pick" one of that fruit. One side of the die is a basket, allowing you to pick any two fruits. The last side of the die is the crow - when you roll the crow, you add a puzzle piece to a picture of a crow. If the crow puzzle is completed before you get all the fruit, you lose, but if you get all the fruit, you win. There are 10 of each fruit, and 9 pieces to the crow puzzle. So how often can you expect to win? 
I got into R programming by way of ggplot2, the popular visualization package. For me, this was the most fun and the most different from what I could do in other programs (SPSS and Excel, in my case). I learned a lot just by setting data up to make the visuals I wanted to make. You could download the step-count data from your phone for the last year, and see how your physical activity varied by day, month, year, and hour. That would be some pretty simple coding that lends itself to visuals and might be interesting to you. &amp;#x200B;
Plotmath isn't a package. It's simply the name of the help page that explains math plotting commands supported by base R graphics. It's automatically loaded.
Maybe too long of an answer for here, but how does one download that information? I've never been able to figure out how to
https://duckduckgo.com/?q=how+to+download+facebook+data&amp;t=fpas&amp;ia=web
Just find something you already do that can be improved by programming. I use it for D&amp;D all the time and it even lead to me contributing to an R package.
I did some quick research on %notin% . In base R, it's part of the grDevices package, specifically used by `text`, `mtext`, `axis`, and `legend` for example: thisThing &lt;- c(1,2,3) thatThing &lt;- c(3,4,5) thisThing %in% thatThing # returns FALSE FALSE TRUE thisThing %notin% thatThing # returns Error: could not find function "%notin% # this works... plot( x = thisThing, y = thatThing) text( x = 2, y = 4.5, expression(thisThing %notin% thatThing)) ...but I'm still looking for %notin% to appear in base R without installing a package or defining it as described below. I do see it defined in [mefa](https://cran.r-project.org/web/packages/mefa/index.html) and [hutils](https://cran.r-project.org/web/packages/hutils/index.html) . &amp;#x200B; Your thoughts?
This makes a lot of sense. I added a note above about trying to find it - it does appear in two packages (mefa and hutils) but isn't available as an operator. It is available in plotmath, but as a printable expression. 
Thanks - you are correct and helpful! &amp;#x200B; I did some experimentation on this and slapped together a brief demonstration - included above.
One of those silly name generators but for names and descriptions of la croix flavors 
Wait, do you mean what drink someone uses is meant to depend on whether not they're happy, or the other way around?
How does one go about retrieving their Spotify data?
I‚Äôm on my phone right now, and in a meeting, so I can‚Äôt test this but [this](https://www.makeuseof.com/tag/download-privacy-data-spotify/) has a few notes on how to do it. There is also a [package](https://www.rcharlie.com/spotifyr/) for it. 
Wow awesome, thank you for the info! I didn‚Äôt even think to look for a package, I guess there really is a package for everything! 
For me, it's scraping. Scraping most websites is not that tricky and it's a great way to work with all data formats that you'll end up using. Then you can do visualization, run models, etc
+1 for typing that while in a meeting
This is very much in the wrong sub lol. But the answer to your question is get a VPN. Tunnelbear is a good one
I think this may depend on how you plan to use R. Are you looking to just play with big data, or are you wanting to learn how to use it as a statistical powerhouse?
If you want to learn the basics of all the different machine learning techniques in R, this book has been invaluable to me for that. 
Yes you should read the book you're on as it covers a lot of ground, but /u/vazz's recommendation is also superb if you want to learn more about machine learning specifically.
My goal is to go into clinical research, and I‚Äôve learned that R is an extremely useful tool to have at your disposal. However, being a complete beginner, I‚Äôm not 100% clear on which one would suit me better, but just based off your description, I think, yes, statistical powerhouse. 
If you're an absolute beginner, both in terms of programming and statistics, [this blog post](https://paulvanderlaken.com/2017/10/18/learn-r/) has resources that I'd recommend starting with. They get you into how programming actually works, how to make some quick plots, etc. R for Data Science is basically the R Bible, but I feel it does assume you know a little bit about data science or programming in general.
Wow, cheers mate! Blog post gives some nice tips for beginners like me, and edX has a variety of options for R (Just checked!) 
Why did that standard start? Or why is it still recommended? For the first one: dunno, maybe it‚Äôs in the The New S Language? For the second: = is used for passing function arguments, while == is used to test equivalence, leaving &lt;- to be a non-ambiguous sign for assignment. What about indentation is interesting?
Data camp is a great start. Also if any of the analysis you're performing is on time series data I would recommend Forecasting Principles and Practice https://otexts.com/fpp2/.
That's a really great book to use, although it pushes a very package heavy way of coding. The R Cookbook is a good one for looking up all sorts of common tasks (handy to have as a reference). It's also available as a web-book; [http://www.cookbook-r.com/](http://www.cookbook-r.com/)
I would say indenting works the same as other languages with non-meaningful whitespace (basically, everything but python). You indent to show things that are nested within each other. So, x &lt;- 1 x &lt;- x + 1 becomes x &lt;- 1 if (x &lt; 2){ x &lt;- x + 1 } because the second line is nested in the if statement. As far as I know, this is pretty standard across languages. Regarding the assignment operators, there is a very nice answer here: [https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-in-r](https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-in-r) &amp;#x200B; &amp;#x200B;
[removed]
The book you have will be great. Focus on making plots and pulling / splitting data. From there, check out the definitive RMarkdown guide (also a free ebook). With RMarkdown you can take what you learn from the book you are reading now, and learn how to make reproducible reports. This may not sound like a big deal but get good enough with markdown and it's subsequent packages (like bookdown) and you can do anything to write your dissertation to produce journal articles using R. Again, it can be hard to grasp how easy this will make your life at first, but imagine making a 200 page dissertation and inserting a table on page one. R can auto update all the table names for you. Or imagine you forgot to turn on a filter and your statistical analysis are wrong, or you have to remake tables / plots. Well, with R you can just turn the filter on, re-run your script, and your report (journal article, dissertation, etc) is automatically corrected in seconds.
I second the blog post, but might just be a bit biased... : ) Somewhere further down the line, you might want to have a look at [this big overview of R stuff](https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/) for more focused learning resources. Hope you enjoy your leaRning!
I found [this book](https://books.google.co.nz/books/about/An_Introduction_to_R.html?id=1U0iMwEACAAJ&amp;redir_esc=y) really helpful. Not too wordsy, but detailed and a good reference point. 
Thanks for the offer! There's a PDF online, but after flipping through it, it does seem concise and too the point without being to wordy. 
Ah awesome! Glad there is a PDF version. I think he‚Äôs bringing an updated one out soon as well. Good luck on your R journey!
Swirl is good to get a grip of using R and the interface, but I would really recommend R for Data science (free online) once you have a hang of the basics. 
&gt; [indentation in R] clearly works differently from any language I‚Äôve worked with before. Not really, no. Indentation in R mostly follows the rules of languages with similar syntax structures. And, although its major paradigm is vastly different, R indentation can/should be applied pretty much according to the same guidelines as for languages such as C, Java, or JavaScript.
You seem to be asking whether you should start using a different book rather than *R for Data Science* so let‚Äôs be clear: **no**, don‚Äôt do this. The book you‚Äôre using is excellent. There are other excellent books but as a general introduction to the subject I doubt there‚Äôs one that‚Äôs *better*.
My point is we are not mentioning how many Deviations we are considering.
the mean and standard deviation you pass it in this function determine the features of the distribution (which are the only parameters needed to do so for a normal distribution) play with plotting a normal distribution with different means and standard deviations to visualize this
Aren't all normal distributions infinite, do we consider only the first three deviations since the others are almost negligible?
nvm I've understood thanks &amp;#x200B;
you're just randomly drawing a value from a normal distribution with a set of parameters. as your x value approaches (negative) infinity, the probability of drawing it approaches zero (but is always strictly positive). 
The programming with R by Norm Matloff
If you could do rnorm(&lt;infinity&gt;) the resulting output would be perfectly normally distributed. 
Discover statistics using R by Andy Field &lt;3 one of the best books I read in general
No such thing as as a stats or programming silly question. I'm yet to meet the guy that waltzed his way into either subject.
If you're drawing just one number, then probably not, because it provides little information about the distribution it came from. But let's say you're doing a simulation and you need to draw hundreds or thousands of numbers. In this case, by choosing `rnorm` you're saying you want the collection of samples to come from that specified normal distribution. The normal distribution is defined by a mean and the value for "1 standard deviation". Once you know those two parameters, you can calculate mathematically any other deviations you could want. You could choose other distributions and they have different parameters (e.g. exponential and poisson are defined only by a mean or rate). So if you're drawing a lot of samples, then both the distribution you choose, and the parameters (like standard deviation) matter.
/r/lostredditors 
Second this. No question is stupid
rnorm samples a number (or numbers) randomly *from the normal distribution*. That means it‚Äôll tend to give you more numbers at the peak of the normal distribution than in the rails - in the proportion it should (if you do enough numbers). It does not sample numbers randomly between minus and plus infinity with no weighting (that would be sampling from a flat/uniform distribution). Therefore, unless you want to sample from a normal distribution with zero mean and unit standard deviation (the default) you should give the function the parameters of the normal distribution you‚Äôre interested in. For example, let‚Äôs say I‚Äôve calculated the mean and std dev of heights in a population - and then want to sample randomly from that population (in proportion to the population‚Äôs distribution - assuming it‚Äôs normal) - I‚Äôd use rnorm with the mean and std dev I‚Äôd previously determined. That‚Äôs why there are several other sampling functions - eg rbinorm samples from the binomial distribution. If you want a number from minus to plus infinity from a flat (uniform) distribution - i.e. with no weighting - then you would use *runif*, which of course, doesn‚Äôt need a mean and std dev. Sounds like you‚Äôre confusing the two. 
&gt;If you could do rnorm(&lt;infinity&gt;) the resulting output would be perfectly normally distributed. Well... No. Computers use *pseudo*-random number generators.
No specific answer, but general advice: &amp;#x200B; library(lubridate) is helpful most of the times when struggling with dates in R.
For the actual dataset I have used lubridate to generate the Isoweek, which is what I am using to generate weekly summary data. Usually I can convert it back into something that I can plot using scale_x_date and it works fine, but this end of year error is causing problems.
How about lubridate::floor_date(date, "week"). That should give you the dates of all the mondays (or whatever the first day of the week is in your locale), and no faffing about with week numbers is needed.
Great - that works perfectly - thanks.
shameless self plug: you can also use my package dint (available on cran) for working with isoweeks. it's lighter than lubridate (just depends on base R) and has nice syntax for working with isoweeks, months and quarters
In the past, I've used: https://github.com/tiagomendesdantas/Rspotify Read up on the Spotify API limitations, such as a limit to how many tracks you can request at once. I got around that by using a loops. Not very R-like, but it worked really well. You can do really cool visualizations, great correlation plots, and practice PCA. 
Hmm. Maybe it has to do with what lubridate picks as the starting day of a week? I think it picks Sunday. The Sunday of the first week in January was in December this year. You can change this if you want. Just a thought. 
Check out [r4ds.had.co.nz](https://r4ds.had.co.nz). You might be looking for ?head
If I understand correctly, here is the general solution : lm(variable~., data=mydata) Put your observed variable instead of 'variable'. 
Sorry, not quite sure exactly what you mean by this but I'll offer what I can. If you want to know the differences between effects of strength on popularity across populations you could do something like: lm(popularity ~ strength * puppy + strength * alien) with puppy and alien taking a 1 for individuals possessing that quality. In the output, strength * puppy would represent the effect of increasing strength on puppy popularity, strength * alien the effect of increasing strength on alien popularity, puppy would represent the average difference of being a puppy on popularity from the baseline (in this case elephant), alien would be the difference from the baseline for being an alien, strength the effect of strength given elephant (since it measures when puppy and alien are 0) and the intercept would represent an elephant with a strength value of 0. From here you would perform something like a Wald test as discussed [here](https://andrewpwheeler.wordpress.com/2016/10/19/testing-the-equality-of-two-regression-coefficients/). If instead you assume that strength matters for popularity within groups (ie a stronger puppy/alien/elephant will be more/less popular than a weaker puppy/alien/elephant) with an effect roughly equivalent across groups but with groups having different underlying popularities (people naturally like puppies more than aliens), you would want to estimate an equation of: popularity ~ strength with fixed effects for group membership. A good function for this is felm() in the package lfe
You might need to identify the type of panel data regression required before using felm() directly.
Try printing ‚Äúy‚Äù to see the values making it in the loop. My hunch is that you‚Äôll see the prob. 
I take it back. Maybe rounding. I tried changing code to .9997069 and it still produced 11 iterations. .99970767 makes 11 iterations but .99970767 makes 10
It's probably rounding. Even if the computer displays a number out to 6 decimal places, in most programming languages the number it's storing under the hood has something like 23 decimal places of precision. So I'm assuming you got the number youre using by running the function and looking at the output but the number you were looking at was probably rounded off to a smaller number of decimal places when you printed it. The reason this happens is that it would be annoying to see ALL the decimal places every time. You can probably control how many are shown with some R setting or option.
y &lt;- round( sum(dpois(x=0:i, lambda=3)), 7)
Yes! This is exactly what I needed!
It was rounding. The output only shows some of the digits. Thanks for helping point me in the right direction
That was it. Thanks for helping
```View()``` might be what you're looking for.
thanks, I ended up specifying lower max_count value to get to further pages.
great book, thanks!
I think I understand what you are asking for. If you want to look at how the effect of X on Y can be different across groups within the same model... lm(Y ~ X + as.factor(group) + X*as.factor(group), data=data) Then in the model one group will be excluded, which will be your referent group. As such the slopes for the interactions will be the slope difference between each group and the referent group. or alternatively you can have each one of your groups as a dummy coded (0/1) variable... lm(Y ~ X + Group1 + Group2 + X*Group1 + X*Group2, data=data) In the above example you will have to exclude one group from the model which will serve as the referent group. See above on what the slope of the interaction means. If you want to run a separate regression model for each group then it could be... lm(Y ~ X, data=subset(data,Group==1) lm(Y ~ X, data=subset(data,Group==2) lm(Y ~ X, data=subset(data,Group==3) In the first two approaches, you will only have a test of significance comparing each group vs the referent group, meaning... Group1 vs Group3 Group2 vs Group3 If you want to have a test of significance for comparing the means for each pair of your groups while controlling for the effect of X, then you will have to run an ancova model. I forgot the code for that so you will have to look it up. I hoped that helped. If anyone else sees an error in what I wrote please message me so I can fix it.
Well how about also printing y? There might also be another pois function that makes your life simpler.
I have no doubt there is an easier way lol, I was printing y for troubleshooting but I think I finally muddled my way through. Thanks 
Try ggscatter(data, x = "X", y = "Y", color = "royalblue4", palette = "darkgoldenrod2", add = "reg.line", [conf.int](https://conf.int/) = TRUE, cor.coef = TRUE, cor.method = "spearman", xlab = "X", ylab = "Y") &amp;#x200B;
I added the quotation marks and this is the error that gets returned... `Error in \`[.data.frame\`(data, , x) : undefined columns selected`
I troubleshot some more and fixed it! Thanks!!!
Glad you got it sorted!
What is ggscatter? Why not just use geom_point?
You're assuming I know what that is hahaha 
I just looked up the ggscatter function. Looks like it is part of a package that tries to make ggplot easier for academics who aren‚Äôt accustomed to writing statistical code. I don‚Äôt know anything about this package (sorry!). However it is just built around ggplot ‚Äî which has tons of help. It‚Äôs probably one of the most downloaded r packages. If you can‚Äôt get help with your current strategy, you may want to check out ggplot. Here‚Äôs a walk through on scatter plots. http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization
Posting what you did to fix the problem helps others. ;)
I know this is a few days late, but I was laying in bed browsing reddit when I originally saw this post. I saved it &amp; told myself next time I got on my computer I would link you what I've been using to learn R. [/u/1337HxC](https://www.reddit.com/user/1337HxC) had a fantastic post, especially with a very useful blog post. However, since it hasn't been brought up yet, I'll also throw in that [https://rstudio-education.github.io/hopr/](https://rstudio-education.github.io/hopr/) is a great book to use to get some learning in. I started learning R back in December, and I started with R for Data Science, and just recently went back and did the HOPR due to a teacher's recommendation. It was in the same style as R for Data Science, as the author for HOPR is a co-author of R4DS.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rstats] [Thought I was a pro -- Help me scrape a single page?](https://www.reddit.com/r/rstats/comments/ako39m/thought_i_was_a_pro_help_me_scrape_a_single_page/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Here you go. library(tidyr) library(dplyr) dat1 dat2 &lt;- dat1 %&gt;% tidyr::gather(key = "song_number", value = "song_name", song_1, song_2, song_3, song_4, song_5) dat2 dat3 &lt;- dat2 %&gt;% select(song_number, song_name) %&gt;% group_by(song_number, song_name) %&gt;% summarize(count = n()) dat3 dat4 &lt;- dat3 %&gt;% spread(song_number, count) dat4 **dat1** --- |country|city|song_1|song_2|song_3|song_4|song_5| |--- |--- |--- |--- |--- |--- |--- | |portugal|lisbon|Hardwired|Atlas, Rise!|Seek and Destroy|Harvester of Sorrow|Welcome Home| |spain|madrid|Hardwired|Atlas, Rise!|Seek and Destroy|Leper Messiah|Welcome Home| |spain|barcelona|Hardwired|Atlas, Rise!|Seek and Destroy|Harvester of Sorrow|Fade to Black| **dat4** --- | song_name | song_1 | song_2 | song_3 | song_4 | song_5 | |------------- |-------- |-------- |-------- |--------- |------- | | Atlas, Rise! | NA | 3 | NA | NA | NA | | Fade to Black | NA | NA | NA | NA | 1 | | Hardwired | 3 | NA | NA | NA | NA | | Harvester of Sorrow | NA | NA | NA | 2 | NA | | Leper Messiah | NA | NA | NA | 1 | NA | | Seek and Destroy | NA | NA | 3 | NA | NA | | Welcome Home | NA | NA | NA | NA | 2 | 
No idea if this will work in r - I mostly use python for web scrapping. //\*\[@id="ba-content"\]/table/tbody/tr/td/a/b Returns all of the names for me.
/u/Thaufas has a great answer to your question. You did really well asking the question too. Thanks for including data, what you want to end up with &amp; what you've tried. I'm going to try to persuade you to use a long format for your data. It might make further analysis, graphing, etc a little easier for you. I added a line of data for some variation since all of the songs were in the same place in your sample data. songs_df &lt;- tibble::tribble( ~country, ~city, ~song_1, ~song_2, ~song_3, ~song_4, ~song_5, "portugal", "lisbon", "Hardwired", "Atlas, Rise!", "Seek and Destroy", "Harvester of Sorrow", "Welcome Home", "spain", "madrid", "Hardwired", "Atlas, Rise!", "Seek and Destroy", "Leper Messiah", "Welcome Home", "spain", "barcelona", "Hardwired", "Atlas, Rise!", "Seek and Destroy", "Harvester of Sorrow", "Fade to Black", "germany", "munich", "Seek and Destroy", "Atlas, Rise!", "Seek and Destroy", "Atlas, Rise!", "Fade to Black" ) library(tidyverse) #the whole tidyverse isn't necessary, but I'm lazy songs_df_long &lt;- songs_df %&gt;% # gather the data into a long format gather(key = 'song_number', value = 'song_name', -country, -city) %&gt;% # remove 'song_' so we just have a number indicating when in the set it was played # I'm no good with regex, so this might only catch single digits, # which may or may not work for sets with &gt;9 songs mutate( num_in_set = as.numeric(stringr::str_extract(string = song_number, pattern = '[0-9]'))) %&gt;% # we don't need song_number column anymore, since we have num_in_set now select(-song_number) That should get us here: # A tibble: 20 x 4 country city song_name num_in_set &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 germany munich Seek and Destroy 1 2 germany munich Atlas, Rise! 2 3 germany munich Seek and Destroy 3 4 germany munich Atlas, Rise! 4 5 germany munich Fade to Black 5 6 portugal lisbon Hardwired 1 7 portugal lisbon Atlas, Rise! 2 And from there you can `filter()`, `group_by()`, `count()` etc. Like this: songs_df_long %&gt;% group_by(song_name, num_in_set) %&gt;% count() And see that 'Atlas, Rise!' was played second in the set 4 times, and fourth in the set 1 time: song_name num_in_set n &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 Atlas, Rise! 2 4 2 Atlas, Rise! 4 1 3 Fade to Black 5 2 4 Hardwired 1 3 5 Harvester of Sorrow 4 2 6 Leper Messiah 4 1 7 Seek and Destroy 1 1 8 Seek and Destroy 3 4 9 Welcome Home 5 2 
What happens if you just update all of them? `update.packages(ask=F)`
If you install a package under R 3.4, you can't use the same package with R 3.5. You'll have to re-install the packages. The problem is that R 3.5 seems to still have your R 3.4 on its path. Run the .libPaths() function to see a list of directories on your path. Normally R puts packages in a sub-directory of the version number. For example, my user library is in: &amp;#x200B; \~/R/x86\_64-pc-linux-gnu-library/3.4 &amp;#x200B; If I installed R 3.5, it would look in the /3.5 dir instead, and wouldn't try to load the 3.4 packages. Maybe you overrode the default library directory with an environment variable or through some other method? In this case, R thinks you have packages installed for R 3.5, but then when it loads them they fail. &amp;#x200B; If you don't need to have both R 3.4 and R 3.5 working with the packages, you can just delete all your packages and re-install. If you do need them both, I think \`packrat\` is made to help with this. Or, I believe there is a file in the R install directory that you can use to specify library paths - you could edit this and specify different library paths for each version to keep them separate.
&gt; .libPaths() &gt; .libPaths() [1] "/home/hhh/R/x86_64-pc-linux-gnu-library/3.5" "/usr/local/lib/R/site-library" [3] "/usr/lib/R/site-library" "/usr/lib/R/library" I've deleted the 3.4 directory I've been trying to manually install every library that comes up as an error but I can't get past this kinda thing &gt; install.packages("rvest") Installing package into ‚Äò/home/hhh/R/x86_64-pc-linux-gnu-library/3.5‚Äô (as ‚Äòlib‚Äô is unspecified) Error in install.packages : This version of R is not set up to install source packages If it was installed from an RPM, you may need the R-devel RPM Warning messages: 1: In .rs.normalizePath(libPaths) : path[2]="/usr/local/lib/R/site-library": No such file or directory 2: In .rs.normalizePath(libPaths) : path[3]="/usr/lib/R/site-library": No such file or directory I've really no idea what this is about. I thought I could just update software
 I've deleted the directory that had all the 3.4 stuff in it. I just ran that command and got : &gt; update.packages(ask=F) Warning in install.packages(update[instlib == l, "Package"], l, contriburl = contriburl, : 'lib = "/usr/lib/R/library"' is not writable Would you like to use a personal library instead? (yes/No/cancel) 
I'm an R user so this is what I do: First use the SelectorGadget app to determine the CSS ([https://selectorgadget.com/](https://selectorgadget.com/)). The CSS is "a b". &amp;#x200B; Then I use "xml2" and "rvest" libraries as follows: &amp;#x200B; library(rvest) library(xml2) \# beerAdvocate beerurl = "[https://www.beeradvocate.com/place/list/?start=0&amp;&amp;s\_id=WI&amp;city=milwaukee&amp;brewery=Y&amp;sort=name](https://www.beeradvocate.com/place/list/?start=0&amp;&amp;s_id=WI&amp;city=milwaukee&amp;brewery=Y&amp;sort=name)" read\_html(beerurl) %&gt;% html\_nodes(css = "a b") &amp;#x200B; This yields: {xml\_nodeset (21)} \[1\] &lt;b&gt;Extreme Beer Fest&lt;/b&gt; \[2\] &lt;b&gt;1840 Brewing Company&lt;/b&gt; \[3\] &lt;b&gt;Ale Asylum Riverhouse&lt;/b&gt; \[4\] &lt;b&gt;Black Husky Brewing&lt;/b&gt; \[5\] &lt;b&gt;Bricks \&amp;amp; Barley Brewing Company&lt;/b&gt; \[6\] &lt;b&gt;Broken Bat Brewery&lt;/b&gt; \[7\] &lt;b&gt;Buffalo Water Beer Company&lt;/b&gt; \[8\] &lt;b&gt;City Lights Brewing Co.&lt;/b&gt; \[9\] &lt;b&gt;Company Brewing&lt;/b&gt; &amp;#x200B; &amp;#x200B;
Thanks for your answer! I did that and it worked perfectly!
Thanks for your input, I am still really learning and keeping things simple, but I realized in this project that indeed it was important to spend time to transform your data in order to make it easier to manipulate after. Right now I keep coming back to formatting my data each time I try to do something! But i'll try to improve in that way.
This works: `library(rvest)` `url &lt;- "`[`https://www.beeradvocate.com/place/list/?start=0&amp;&amp;s_id=WI&amp;city=milwaukee&amp;brewery=Y&amp;sort=name`](https://www.beeradvocate.com/place/list/?start=0&amp;&amp;s_id=WI&amp;city=milwaukee&amp;brewery=Y&amp;sort=name)`"` &amp;#x200B; `beer &lt;- read_html(url)` &amp;#x200B; `brewery_links &lt;- beer %&gt;%` `html_nodes("tr:nth-child(2n) a") %&gt;% ##select even children of the rows, which are all the brewery links and the navigation links` `.[4:23] %&gt;% ##select only the brewery links and ignore the navigation links` `html_attr("href") %&gt;%` `xml2::url_absolute(url) ##get absolute url` &amp;#x200B; `brewery_names &lt;- beer %&gt;%` `html_nodes("tr td a b") %&gt;%` `html_text()` &amp;#x200B; `brewery_full &lt;- data_frame(brewery_names = brewery_names,` `brewery_links = brewery_links)`
Out of curiosity, why R? When it comes to GIS the choice is often Python. 
Honestly, because I‚Äôm not familiar with Python and R is my go-to. If that‚Äôs the way to go though, I‚Äôll start putting my energy there. Thanks for your comment. 
Honestly, because I‚Äôm not familiar with Python and R is my go-to. If that‚Äôs the way to go though, I‚Äôll start putting my energy there. Thanks for your comment. 
If you're going to work long-term with GIS you're energy and time is better spent in learning Python and SQL.
Is that true? There is quite a number of GIS/spatial statistics facilities in R. E.g., https://www.jessesadler.com/post/gis-with-r-intro/
I've worked with GIS/spatial stuff with R. I just never found any reason to choose R over Python.
I strongly recommend deploying R and RStudio in docker containers. Rocker is a series of maintained docker images that will make deployment and package installation-management very smooth and managing external dependencies It is very worth the time to learn how to use it. 
thanks, I was reading something the other day where the researchers had used a docker container actually. is that what you're suggesting? A standard docker that sysadmin people use? Or is this different? If you could link to a guide or something of what you mean that'd be great, thanks
Yeah, that's weird that /usr/lib/R/site-library doesn't exist. Seems like maybe something went wrong with the update? Anyways, I see that you wiped and re-installed and its working.
Yeah, I've no idea how it went so wrong but it *seems* to be working now. Thank you :)
I just noticed that you wanted the URLs too. Here's a data.frame of brewery name and URL: &amp;#x200B; &amp;#x200B; `library(rvest)` `library(xml2)` &amp;#x200B; &amp;#x200B; `# beerAdvocate` `beerurl = "`[`https://www.beeradvocate.com/place/list/?start=0&amp;&amp;s_id=WI&amp;city=milwaukee&amp;brewery=Y&amp;sort=name`](https://www.beeradvocate.com/place/list/?start=0&amp;&amp;s_id=WI&amp;city=milwaukee&amp;brewery=Y&amp;sort=name)`"` &amp;#x200B; `res &lt;-` `read_html(beerurl)` &amp;#x200B; `brewery &lt;-` `res %&gt;%` `html_nodes(css = "a b") %&gt;%` `html_text()` `brewery &lt;- brewery[2:21] # remove crud` &amp;#x200B; `beerUrls &lt;-` `res %&gt;%` `html_nodes(css = "tr:nth-child(2n) a") %&gt;%` `html_attr("href")` `beerUrls &lt;- beerUrls[4:23] # remove crud` &amp;#x200B; `data.frame(` `brewery = brewery,` `beerUrls = url_absolute(beerUrls, "`[`https://www.beeradvocate.com`](https://www.beeradvocate.com)`")` `)` &amp;#x200B; Thanks to u/[redditorfor1hr](https://www.reddit.com/user/redditorfor1hr) for showing me url\_absolute() 
Try https://ropenscilabs.github.io/r-docker-tutorial/ and then I would recommend reading the Docker documentation on their website. Their getting started guides are well written. 
What advantages does do her have over a local install? 
`X = c(1,3,45,6,7,7)` `Y = c(1,2,3,1,2,3)` &amp;#x200B; `mean(X[Y==1])` `aggregate(X ~ Y, FUN=mean)` &amp;#x200B;
If I understand your post correctly, you want to use the mean function in your geom_bar() call. geom_bar(stat = "summary", fun.y = "mean")
If you reorganize your data you should be fine: &amp;#x200B; library(ggplot2) library(tidyr) &amp;#x200B; \#make dataset Var\_1 &lt;- c(0.234, 0.523, 0.342, 0.351, 0.351, 0.352) Var\_2 &lt;- c(0.512, 0.656, 0.612, 0.434, 0.052, 0.364) Var\_3 &lt;- c(0.940, 0.806, 0.754, 0.984, 0.944, 0.834) data &lt;- data.frame(Var\_1, Var\_2, Var\_3) &amp;#x200B; \#reorganize data data &lt;- gather(data, key = "Var", value = "value", 1:3) data$Var &lt;- as.factor(data$Var) &amp;#x200B; \#Build plot ggplot(data, aes(Var, value, fill = Var)) + geom\_bar(stat = "summary", fun.y = "mean")
Here's some more ways to conditionally subset since you're new to R x &lt;- c(1,3,45,6,7,7) y &lt;- c(1,2,3,1,2,3) x2 &lt;- x[which(y %in% c(1)] x2 &lt;- x[which(y %in% c(0, 1, NA))] x2 &lt;- x[which(y == 1)] x2 &lt;- x[-which(y != 1)] x2 &lt;- numeric() for (i in 1:length(y)) { if (i == 1) { x2 &lt;- append(x2, y[i]) } } mean(x2) # Will throw error if NAs are present mean(x2, na.rm = T)
Old Post, but 2ondering. If you use the 3mo from msft, are you able to take datacamp discounts post-trial (like 1yr for $150) or are yiu stuck paying with the $25 a month?
I was able to sign up on Black Friday for $150 with no issues.
The looks like a neat little homework problem or exercise. I'll pitch in a solution template for the first one. Basically, it can be split into two smaller problems. First, figure out how many symbols to print per line of the shape. Second, figure out how to pad those symbols so that they're centered. I implemented the printing parts in the solution in the linked gist. I've left figuring out how may symbols per line up to you. My solution used the seq() and c() functions. [https://gist.github.com/grosscol/c44549f99aac390a248a058e8c250d52](https://gist.github.com/grosscol/c44549f99aac390a248a058e8c250d52) &amp;#x200B; Hope it helps.
Nice thanks, will look into that. I finished the second one. Well at least it works with a handful of articles I found online. Here the code for anyone in the future finding this page: str1&lt;-"The team almost didn't win when they kicked the ball down the field ball ball" sentence&lt;-strsplit(str1," ")[[1]] str2&lt;-"Trump met with Congress today to discuss the bill" sentence1&lt;-strsplit(str2," ")[[1]] str3&lt;-"The scientists have a new hypothesis on global warming" sentence3&lt;-strsplit(str3," ")[[1]] str4&lt;-"The suspect got away yesterday after the police chase" sentence4&lt;-strsplit(str4," ")[[1]] str5&lt;-"Sacramento leaders say they are determined to find solutions to the city's homeless crisis." sentence5&lt;-strsplit(str5," ")[[1]] str6&lt;-"Many NASA instruments are keeping tabs on Hurricane Michael from space, including AIRS and MISR." sentence6&lt;-strsplit(str6," ")[[1]] str7&lt;-"The House Democratic Women's Working Group is inviting female members of both parties to wear white to President Donald Trump's State of the Union address next week as a symbol of solidarity." sentence7&lt;-strsplit(str7," ")[[1]] str8&lt;-"Goodell has used his Super Bowl bully pulpit in the past to lobby for rule changes that he wanted to see, and succeeded." sentence8&lt;-strsplit(str8," ")[[1]] str9&lt;-"One snowstorm over the U.S East Coast does not invalidate decades of observations around the entire world showing that yes, the planet is warming," sentence9&lt;-strsplit(str9," ")[[1]] str10&lt;-"A third suspect ran out the backdoor of the home." sentence10&lt;-strsplit(str10," ")[[1]] a&lt;-c("ball","sport","team","player","field","win","lose", "Giants", "Dodgers", "Patriots","Super","Bowl") b&lt;-c("Trump", "Congress" , "Senate" , "bill", "democracy" ,"mandate","Democratic","party","parties" ) c&lt;-c( "experiment" ,"future", "medicine", "observations","hypothesis", "conclusion", "scientist", "scientists","global warming", "space","researchers", "NASA", "Mars","Rover","climate change", "glaciers") d&lt;-c("local","suspect","report", "police", "neighbor","weather","rain","sunny","homeless", "Sacramento") categorize&lt;-function(sentence) { if(length(sentence[sentence %in% a]) &gt; length(sentence[sentence %in% b]) ){ print ("Sports")} else { if(length(sentence[sentence %in% b]) &gt; length(sentence[sentence %in% c])){ print("Politics")} else { if(length(sentence[sentence %in% c]) &gt; length(sentence[sentence %in% d])){ print("Science")} else { print("News") } } } } categorize(sentence) categorize(sentence1) categorize(sentence3) categorize(sentence4) categorize(sentence5) categorize(sentence6) categorize(sentence7) categorize(sentence8) categorize(sentence9) categorize(sentence10) [1] "Sports" [1] "Politics" [1] "Science" [1] "News" [1] "News" [1] "Science" [1] "Politics" [1] "Sports" [1] "Science" [1] "News" 
Depending on the the data your function requires as input you could try to write a shiny app and host it somewhere so that people could use your function from the browser using a GUI 
See if you can get it added to a popular biology package. 
At minimum, if you think it doesn't need independent package, you could add it as a well commented R script together with some example data as a supplementary file with your paper. This way interested users should be able to adapt it to their workflow.
Read Hadley Wickhams stuff on making packages with roxygen. You should make a package, even if it‚Äôs just one function, and then host it on a github. If it‚Äôs a really useful function, write some demos and include a sample dataset and write a small article about it to get the word out. Shouldn‚Äôt take you more than a couple hours to get it into good package format!
If the group are savvy enough to use basic r functions, you could write a library: https://support.rstudio.com/hc/en-us/articles/200486488-Developing-Packages-with-RStudio Or you could write the scripts into a shiny app, where any of the inputs are predefined in am interactive website: https://shiny.rstudio.com/articles/build.html https://deanattali.com/blog/building-shiny-apps-tutorial/ If I was you, I'd do both (i.e write the library, then use that for a shiny app). This is the approach I use for most of my projects involving other technical disciplines. 
Maybe you could add it as supplementary material to your publication?
Here's a hint for the second one: you can do ```sum(xs %in% ys)``` to get a count of how many elements of x appear in y.
I‚Äôve been looking into this a little bit myself. Sounds like you can deploy a shiner app using Shiny Server is you have access to something running Linux. There‚Äôs a tutorial online I found on how to deploy on AWS but I‚Äôve not followed it through yet. Will try and find a link tomorrow when on my work laptop.
update: after doing some reading online I have came across the curve() function which is more inline to what I was originally looking for.
You seem to have found all the good solutions already. You can also do it by hand following instructions [here](https://oddhypothesis.blogspot.com/2014/04/deploying-self-contained-r-apps-to.html) and there seems to be a way to [use electron](https://www.youtube.com/watch?v=ARrbbviGvjc&amp;ab_channel=RConsortium)
Agreed. There are plenty of packages that are one function. 
I‚Äôm actually working through this problem now. Rinno definitely works. Keep in mind that to make it R naive, it has to bundle R and all the required packages up with it, making it fairly bloated for what is delivered. Personally, I‚Äôm trying to figure out a way to locally host the app in a password protected manner without using shiny server pro. 
I know you said no containers, but containers make it pretty dang easy.
See shinyproxy 
I tried once with portable R. It contained Rscript.exe, so I launched shiny app with a .bat command. It worked like a charm
So I haven‚Äôt used containers but if they‚Äôre useful I‚Äôve got to jump the hurdle eventually. How would containers look for sharing the app? I‚Äôm looking to have non R users have a dashboard on their local computers. 
If you can get a really easy ‚Äúclick and install‚Äù app out of one of the solutions you mentioned, I‚Äôd do that for sure. Using docker isn‚Äôt too difficult though. There are ready-to-go Shiny Server images already, so you only have to add your app. Good instructions at https://hub.docker.com/r/rocker/shiny/ Probably the simplest thing to do is just run docker on a server so users can access the app in their browsers via host url. Local install is also workable. Users just have to install docker, and the run your container. The main benefit is separation of dependencies; no need to install R + libs each time. It def helps if you can team up with someone like IT to handle the docker client install.
I've used the first solution before, it's a bit clunky, but yu can even install it on a USB stick. I'm keen to try the electon solution, anyone have any success with it?
Never tried myself. Just found about it when I was googling for the other link for you
Yeah but I'm not gonna pay 15$ for just the R book, only one I care about.
[removed]
So ... I could write a .bat/.exe file that users can just click on to run the Shiny App on their machine? I'm trying to make this as user friendly as possible :) &amp;#x200B;
Solid backup plan. I'm trying to make R more mainstream but was told to try to keep it as resource/maintenance free as possible. Having IT help a department install Docker would be a harder sell but I believe in the benefits of scripting reports and validation checks around here. 
God forbid you branch out and learn how to do anything outside of R...
They had a series of data books a couple of years back that was well worth it. For $15, it‚Äôs not a bad price, even if you only read one or two of them. 
God would probably forbit learning Perl, and very few people would want to learn Bash + Windows PowerShell. That is just a weird combo. 
You don't write your css in bash
There's 4 errors here. Thanks crew. 
I've never used Rcmdr. From what I understand, it's a gui usually used for academic purposes. In my opinion, I'd say it boils down to whether or not you've learned to code. The tasks you've done in Rcmdr, did they involve you coding? Or playing with the gui? Do know how to effectively use base R, and packages related to tasks commonly done in R? (E.g. ggplot2, dplyr, etc). If not, then I wouldn't recommend you claim to know how to use R, but rather how to use Rcmdr. 
Are affiliate links permitted here? This links to a Tweet, which links to a shortened url, which expands to contain an affiliate referral. Referral free link here: https://www.humblebundle.com/books/programming-cookbooks Referral link contains this at the end of the url: hmb_source=navbar&amp;hmb_medium=product_tile&amp;partner=somedeals14&amp;hmb_campaign=tile_index_1 
I hear they even have CSS on the internet now.
Use R Studio, infinitely better than Rcmdr.
Agreed. If only all IDEs were as good as RStudio. 
Spyder is pretty similar for python and R studio felt a lot like a rip off of Matlab. But I do still really like R studio. 
no, stop recommending a monopolistic monstrosity
Try gganimate 
I don‚Äôt use RStudio myself but it‚Äôs actually pretty good. What‚Äôs the problem with it?
The gtools package has a permutations function that might make your life easier. Here's a solution using apply and lapply: https://gist.github.com/grosscol/d6c9c0408391c3e00645dd602e3e3f8a
Can you clarify size=boarder_t ? Not in your example code. 
Gganimate was recently added to cran, it's much more stable than it used to be. Highly recommend.
&gt; Unfortunately that's not possible because the library was originally written in another language. That‚Äôs a non sequitur. You can ship the code in another language, it doesn‚Äôt have to be C. Compilation might be a tad harder but not by much. &gt; Would it be ok to just download the library during installation? No, that‚Äôs fundamentally not supported, unfortunately. However, you *can* download the library during the first loading of your package. &gt; Or should I just put those file into the libs folder and avoid CRAN / Bioconductor ? Sure, that‚Äôs another possibility. However, it might limit the reach of your package. Although installing via GitHub (or, generally, other Git sources) is becoming easier, it‚Äôs still not the norm in R. And having binary libraries hosted with code has its own problems (though you may decide that it‚Äôs acceptable in your case).
Thanks for the great answer!
yes, sorry, I've added that now # thickness of the plot boarders boarder_t = 0.5 
I can't help you with hiding the numbers but I have a tip to make it look nicer: Is it absolutely necessary to do a pie chart? Consider using a bar plot for this, as the differences are much clearer (why do you think there's no geom_pie? It's deliberate)
&gt; Given that alternatives exist it can hardly be called a monopoly. I don't think so. Given the marker share of rstudio, I've came across many instances, where rstudio encourages writing nonstandard R code which works only in rstudio. This encourages bad practice as the project codebase is becoming dependent in IDE, which is abhorrent. This creates a 'positive' feedback loops of dependency of gaining more and more users, not unlike microsoft and google do with their products. So fuck rstudio.
yeah you're probably right ha... https://i.imgur.com/aT0Lm9H.png When sorting the bars it seems to make the colour scheme less clear, is this something that can be adjusted? Usually they go from light to dark, but they're a bit jumbled here because I sorted by size thanks
yes, all of this can be adjusted. you can either reorder the factors (probably not what you want), or you can specify your own colors: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/ also, agree with the top comment here -- stay away from pie charts!
rather than giving my own colours I was happy with the brewer scheme, just that when I ordered the bars it didn't adjust the colours accordingly. I'm happy to leave the pie chart, I thought twice about it and went with it for some reason. The shame.
You can assign colors to them yourself. https://stackoverflow.com/questions/38788357/change-bar-plot-colour-in-geom-bar-with-ggplot2-in-r And you can use this sheet for finding colors you like: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
I didn't want to assign them myself though, I wanted to use the scheme. It's just that when I reorder the bars it doesn't account for that in colouring them 
&gt; I've came across many instances, where rstudio encourages writing nonstandard R code which works only in rstudio. Can you give an example? I‚Äôm having a hard time understanding what you mean by that. There are RStudio extensions but they are pretty useless and nobody in their right mind would develop such an extension instead of a package, where appropriate. &gt; This encourages bad practice as the project codebase is becoming dependent in IDE, which is abhorrent. I‚Äôd agree if this were true but, like I said, I‚Äôd need to see some examples.
One thing that comet to my mind right now is the 'Run app' button in rstudio to run shiny applications, which runs some shiny functions in background. Many of my coworkers use this button to run shiny apps, completely oblivious that a proper way to run the app from shell (without rstudio) is missing.
There‚Äôs some confusion here. The RStudio ‚ÄúRun App‚Äù button for a Shiny app simply runs the default Shiny entry point. The same thing can be achieved by either sourcing the `app.R` file, or by running `Rscript app.R` ‚Äî both outside RStudio. In fact, that is *the* established way of running a Shiny app, it‚Äôs completely unrelated to RStudio. Rstudio simply adds a button which executes this standard workflow, it adds nothing proprietary. &gt; well, the catch is that shiny is from the same developers as rstudio, which gives an unfair advantage to other non-shiny solutions. The two are completely decoupled. There‚Äôs really nothing easier than running `Rscript app.R`.
&gt; The same thing can be achieved by either sourcing the app.R file, or by running Rscript app.R ‚Äî both outside RStudio. I don't have the code right now to doublecheck, but there certainly is not an app.R to source. There used to be global, ui and server entry functions, but nothing to bind them. 
Not at my computer, but I think those are the y axis labels. You can remove them with axis.y.text in theme, or by setting the labels in scale_y_contnuous to nothing. Also don‚Äôt use a pie chart, especially in ggplot2 as it doesn‚Äôt do them very well.
yeah - the pie chart's been scrapped for a bar. Thanks
Check out stargazer! [Here's a vignette](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf)
gt is a pretty exciting new package: https://github.com/rstudio/gt
My personal favorite is kableExtra. Make's much better looking tables than base R and it's relatively easy to implement. Stargazer would be my other recommendation. [Link to Kable](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html)
For changing the color scheme of your filled bars in your bar chart, you only need to make one simple adjust. Currently, your code looks as follows: bp &lt;- ggplot(totals, aes(x = "", y = Count, fill=Stage))... Change the `fill` option like so. bp &lt;- ggplot(totals, aes(x = "", y = Count, fill=reorder(Stage, Count, sort))... 
I also like `formattable` for more interesting tables. 
Actually the default way to write shiny apps do not include that single line that executes said app which is the `runApp` command. But it is absurd that this causes people to write code that doesn't work with anything else but rstudio or that it'd cause them not to know what they are doing. When you press the runapp button Rstudio transparently runs the `runApp` function. If you didn't know how to run an app before, you will know after pressing the button since you see the code that's executed. The only legitimate example I can think of would be rstudio add-ins included in many packages. I am yet to see one I would consider harmful.
Actually the default way to write shiny apps do not include that single line that executes said app which is the `runApp` command. But it is absurd that this causes people to write code that doesn't work with anything else but rstudio or that it'd cause them not to know what they are doing. When you press the runapp button Rstudio transparently runs the `runApp` function. If you didn't know how to run an app before, you will know after pressing the button since you see the code that's executed. The only legitimate example I can think of would be rstudio add-ins included in many packages. I am yet to see one I would consider harmful.
Came here to say this!
It's daylight savings time. Do the comparison without using "as.character" and it gives the time zone.
Sadly, it's not on CRAN yet. On my company, we can only use packages fron CRAN :( It's looks great! If I had it, I would substitute DT for GT right away! :)
Great suggestion. Format is awesome, especially if you're forced to make tables that are part of a Word doc.
Just download the zip file of the repository and install manually? Or this is also a no go?
Assuming your data frame is called `df_0`: df_0$flag &lt;- ifelse(df_0$Celsius &gt;= 12 &amp; df_0$Celsius &lt;= 15, "yes", "no"
I‚Äôve been playing with the ‚Äòexpss‚Äô package from CRAN for a similar purpose and it‚Äôs been pretty good. I believe there‚Äôs ongoing work on it, the developer has been responsive on email and stack overflow when I‚Äôve had questions. 
Here's a simple crack at it. I think I created the same sort of data using some Tidyverse data wrangling functions. I would question the use of a lines in a graph like this, unless the real data shows something easier to parse (less variable) across the years. x &lt;- 1990:2018 d1 &lt;- data.frame(years = sample(x, 30, replace=T), event = rbinom(30, 1, 0.7), category = "d1") d2 &lt;- data.frame(years = sample(x, 80, replace=T), event = rbinom(80, 1, 0.53), category = "d2") d3 &lt;- data.frame(years = sample(x, 60, replace=T), event = rbinom(60, 1, 0.65), category = "d3") d4 &lt;- data.frame(years = sample(x, 90, replace=T), event = rbinom(90, 1, 0.77), category = "d4") library(tidyverse) df &lt;- bind_rows(d1,d2,d3,d4) df_summary &lt;- df %&gt;% filter(category %in% c("d1","d2")) %&gt;% #comment this line if all 4 are desired group_by(years,category) %&gt;% #specify groups for summary statistics summarize(rate = mean(event)) #create a new data.frame with rate as mean of event in each group ggplot(df_summary,aes(x = years, y = rate, color = category)) + geom_point(size = 4) + geom_line(size = 0.5) The main insight is that you can create the data in long form by adding a column that specifies your category (d1, d2, ...), then use that to specify another aesthetic in your ggplot. It also makes it easier to work with other Tidyverse tools for manipulating the data.
I can confidently recommend kableExtra with high praise. It's very flexible and easy to learn. If you're comfortable with HTML, CSS, and JS and want ultimate flexibility, consider using RHTML. 
A lot of the commands you use in R are vectorized. Meaning, the "for loops" are built in for you. In R, try and avoid using loops. Look into sapply, lapply, and apply. Get familiar will base R. You'll thank yourself.
I know a bit about vectorization and the apply family of functions. I just can't wrap my head around how to achieve that in this case where the operation inside the nested for loops includes accessing another data.table and writing to yet another data.table.
So I would approach this by first making a temperature_in_range variable. Which has the value TRUE if an observation was between 12-15 and then FALSE otherwise. /u/ryapric shows how to do this below. You seem to have evenly spaced (hourly) measurements, so the `rle` function ([run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)) will give you the lengths the vector is in the True/False state. You then have to filter this to get only the lengths for runs of True, and then look at whether the longest of those lengths is greater than or equal to 6. temps &lt;- c(1,2,2,8,8,13,13,13,13,13,13,5,5,5,5,5) temp_in_range &lt;- function(x) ifelse( x&gt;=12 &amp; x&lt;=15, T, F) longest_true_run &lt;- function(x) max(x$lengths[x$values]) longest_true_run(rle(temp_in_range(temps))) 
why not try, dplyr -&gt; mutate and case_when? Its faster than nested loops for sure.
I've been playing with e1071::cmeans recently. Gets the job done.
Only works if you have all the dependencies, which if it's from rstudio will probably require about 20 other packages which aren't on CRAN either.
Pretty sure you can easily achieve that a b c bit with ifelse(ifelse(ifelse())) which does the whole vector rather than looping through the elements. Probs same for the others too? On lapply way of doing it, define a function with index i and lay out the function with ifs like you've done already then return the result at the end. 
As others have pointed out, you can usually can avoid for loops in R, unless you really know what you're doing. That being said, you are writing this as you would write in C, and you aren't especially using data.table greatest features. [Try starting with this vignette about how to use it](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html). Then, read [this one about how to update data tables by reference](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reference-semantics.html) and you can not only reduce your function to barely 6 lines, but it will run in less than a second. Good luck! (I would help you, but I can only use my phone for now)
scale_color_discrete(name="THIS")
your "postpre" legend is for color. take your current code and add to it: **+ labs(color = "Whatever you want")** additionally, I'd recommend having this RStudio Add-on, it's very helpful with things like this https://github.com/calligross/ggthemeassist 
oh great, I've installed. Thanks
no problem, it's a life saver, there's so much minutiae with ggplot, very few people know all of it.
Personally, I use e1071::cmeans, I have found both method arguments **"ufcl"** (Unsupervised Fuzzy Competitive Learning) and **"cmeans"** (C-Means Fuzzy Clustering) to be so much better than most other clustering algos.
this also works, thanks is there any reason to prefer this over `labs(color="blah")`? Or visa versa. They both work here anyway, I'm just curious. 
I just took a look at *gt* this morning. It looks very promising. 
sample_n from dplyr for a random sample
First of all, I commend you for your neat code. Second, I see this statement: a &lt;- as.numeric(dt[m, 4]) However dt as shown above only has 3 columns? Also, I don't see a gene column, which is used in the for loops. dt2 is also referenced, but I don't see what that looks like. Honestly, looking at your code, whatever operation you're running looks rather complex. The most efficient way to do this will probably involve several vectorized arguments. Without a reproduceable example, I can't give you a definite answer unfortunately. If your data is sufficiently small, you could run your loops in parallel.
When you get a chance, I‚Äôd love to see your code. 
I'd really like to try and help, but your code references a dt2 and dt3 that you didn't provide and references a 4th column in dt that doesn't seem to exist. It's much more difficult to parse what you are trying to do without this. If you can provide small, reproducible datasets (ie, just build tibbles or dt with ten rows and only the columns you need, and include that code) then I'd love to look at this again and help if I can. Look at this as an example: &amp;#x200B; library(tidyverse) library(data.table) dt &lt;- tribble( \~unigene, \~s.start, \~s.end, 70333442, 25564, 25601, 47901818, 151379, 151340, 22804559, 237716, 237685, 51650325, 370382, 370338, 45665472, 691509, 691553, 67902793, 885586, 885616, 107540914, 909650, 909617, 22777344, 1234774, 1234752, 103835796, 1421236, 1421263, 46208443, 1595024, 1595059 ) %&gt;% data.table
 data.tables : like dataframes but with a third argument ¬´ by ¬ª. It calculates by groups. Runtime is very low
Not if all you're doing is labeling the legend. However, using scale_color_discrete, you can also define the colors used and the names associated with each color.
Using dplyr: ``` library(dplyr) Dataset %&gt;% group_by(SEX) %&gt;% summarize(mean(EARNINGS)) ```
Ah bummer.
You can do this easily in base R. aggregate(EARNWAGE~SEX, data=your_data, mean)
What an amazing little gem of a package! Thanks! is it possible to export tables to LaTeX with the same format? - I have not used it yet but will do when I need to write up my results!
Alright! Thanks guys, I'll give that a shot.
Your question is getting a lot of negative attention (downvoted) because you clearly didn't even try finding the answer yourself. Googling the question gives you the answer almost directly. You're welcome to come back with specific questions concerning your attempts at finding the answer. But don't just post homework here, that's unacceptable.
funct &lt;- function (v){ Zscore = sum(max(v) - mean(v))/sd(v) return (zscore) } You have to pass the input argument inside the function definition. The variable zscore is what the function should return.
Look at the `scale` function.
Thanks! I did look at the scale() function but I guess I am more curious if it‚Äôs possible to use create my own function and use the sapply to only apply it to specific columns. 
Hmm I'm unsure whether there is an easier way to do this within ggplot. If I were you I'd probably plot Monday faceted by it's day in it's own subsetted data frame and plot the single Friday individually. You could then facet Tuesday through Thursday and use something like cowplot / ggrid to arrange the plots in the desired layout (putting the strip of four plots for monday at the top and the one plot for Friday at the bottom).
Thank you!
get rid of the variable declaration and return statement for a simple function like this and make it a one liner. zscore &lt;- function (v){ sum(max(v) - mean(v))/sd(v) } 
Use `facet_grid()` instead of `facet_wrap()`
`?scale`
From a data visualization perspective, even if the grid was properly organized with facet_grid(), you might be able to remove one of the faceting axes to make clearer comparisons. It is not easy to read across barplots in a whole grid, and the data may be too sparse to justify all this ink for so few values. You're using a barplot with stat="identity", but what if you just added a color aesthetic in your aes() function. You could define a new factor "week" which encompasses the five weeks in question. Since user is already defined by x, remove the fill aesthetic. An example, assuming you've created a new variable "week". p &lt;- ggplot(df, aes(y = events, x = user, color = week)) + geom_point() + facet_wrap(~weekday) Alternatively, if date represents something meaningful in your time series data, you might consider making that the x-axis (people are generally used to processing time as an x-axis). This way you could remove all faceting. If you want to maintain the weekdays, you could color them, and use shape for the users, for example. p &lt;- ggplot(df, aes(y = events, x = date, color = weekday, shape = user)) + geom_point() And you could specify the shapes using `scale_shape_manual`.
Sorry, I legitimately didn‚Äôt realize. I did try finding the answer myself and googling but I honestly was just getting very confused so I posted here. Thank you.
Thank you so much! This worked beautifully.
Thank you for the help! Saved me from a serious headache.
You can just use the scale() function!
How different between `kmeans` and `cmeans` besides the latter provides partial memberships? Also, is it a must to apply PCA before clustering or just data scaling is enough for the purpose? Not sure feature elimination is necessary for clustering? If I have too many dimensions, I would be tempted to reduce the number of original variables than PCs. 
I might recommend that GIS programs like ArchGIS might be better suited to do this initial map analysis.
I was initially a bit hesitant to ask the question as I thought something else might be better than R. So I'll definitely give this category a look. Thanks!
Rather than seeing the overlapping points, can you calculate the distances between them and check how many they're within some range?
You might want to check out the lawn library. It's the r implementation of turf.js: https://github.com/ropensci/lawn/blob/master/README.md Alternatively, if you just want to return the average (or another summary stat) for the polygons that underlie some points, you could use the sf and dplyr libraries: library (sf) library(dplyr) pts &lt;- st_read(...your point file...) poly &lt;- st_read(...your polygon file...) joined &lt;- st_join(pts, poly) %&gt;% group_by(...your point unique id...) %&gt;% summarise(polygon_mean = mean(...your polygon column containing numbers to average... , na.rm = TRUE) joined 
This doesn‚Äôt actually compute the *z*\-score of `v`, and it can‚Äôt be used within OP‚Äôs code. If you don‚Äôt want to use `scale`, you can calculate the sample *z*\-score as follows: zscore = function (x) (x - mean(x)) / sd(x) &amp;#x200B;
This should be totally doable in R, but I don‚Äôt understand exactly what you‚Äôre asking for. I think the comment above is trying to refer you to ArcGIS, but you shouldn‚Äôt have to rely on such a poor tool. In terms of spatial analysis and data transformation, R can do everything Arc can do and more! Arc really only has a slight leg up with cartography, but things like QGIS, tmap, and leaflet are making R a strong contender in this regard as well.
[https://developer.twitter.com/en/docs/basics/rate-limiting.html](https://developer.twitter.com/en/docs/basics/rate-limiting.html) You are limited by Twitter API calls. Modify your calls or buy a better API. 
Thanks. What do you mean by "modify your calls"?
Read the link. Simple way to do this if it's one-off situation is to run the calls once every 15 minutes. This would take you 17 hours to get 1000 accounts. 
Yeah I suppose that's the way for me right now as premium API prices are insane. 
Every place I've worked with in GIS has worked in Python and SQL. With most not knowing R exists or don't care for it. You've been warned. 
The answer should be in: http://kateto.net/network-visualization
Thanks for the attempt! &amp;#x200B;
Thanks so much for introducing me to rle! So I guess the return I'm getting from this block is how many true runs I've got in my dataset? &amp;#x200B;
No actually that return must be something else because I definitely don't have years of recording time... Is it perhaps the number of values that fall into true run parameters (in this case, hourly measurements of temperature)? &amp;#x200B;
Thanks a bunch. Tmap seems like the kind of thing I was looking for!
My endusers require visual representation. 
Thanks I'll give this a try. 
Thank you so much for sharing this! we are going to talk about it during our team meeting on wed.!
Only small change I did on your github code is to use length(x) instead of hardcoding the 3's in the name\_grid variable to allow it to deal with different amount of middle names. The permutation function does make my life much easier so thanks for pointing me to it =) &amp;#x200B;
Are you sure your laptop doesn't have 4 thread over 2 core?
Damn i'm thick you're correct, thanks for pointing it out! 
I'm not sure about how thick you are. But I'm glad I could help. It surprised me as well the first time I encountered this.
We'll leave it at dense then, thanks you, have a nice day/evening!
Thanks for the link. I got the above problem solved. Now my next step is to show all nodes that connect to node 1 and node 34 as pie chart shape half red/green. &amp;#x200B; To get all nodes that connect to 1 and 34 I use this: n1 &lt;- neighbors(g.1, 1) n34 &lt;- neighbors(g.1, 34) intersection(n1, n34) for testing in the plot I color these nodes blue g.1&lt;-set.vertex.attribute(g.1, index=intersection(n1,n34),"color",value="blue") the 4 nodes are colored properly in blue. &amp;#x200B; But how do I change the vertex to show as pie chart, and half red/green? I played with: g.1&lt;-set.vertex.attribute(g.1, index=intersection(n1,n34), "shape",value="pie") but get an error Error in set.vertex.attribute(g.1, index = intersection(n1, n34), "shape", : unused argument (index = intersection(n1, n34)) again - any pointers are appreciated.
Thank you and thank to the entire team for the meeting. 
*Hey just noticed..* it's your **6th Cakeday** VictoriousEgret! ^(hug)
You might find this plot useful. The code for creating it is available at [RPubs.com](https://rpubs.com/thaufas/464348). **Figure 1**: [A multi-plot figure created with ggplot2 and gridExtra](https://i.imgur.com/5SPLPlK.png)
Seems like a good modification to account for variable length names. Glad the gist helped.
Good points. One additional advantage of using `scale_color_discrete` (or any custom scale) is the ability to use multiple scales in a single plot.
QGIS + R was what I was looking for. Thanks all! 
Well, it should be the length of the longest run of `True` values, which in this example is the n=6 values of `13` in the middle of the vector. It relies on logical indexing, so `temp_in_range` returns `True` or `False` and then you use the names of the rle vector (True or False) to index the lengths to only get True lengths. The max function gives you the largest run of Trues. Take a look at the output of `rle(temp_in_range(temps))` to get a better feel for it. It needs a bit more code to check if that length is as long as you want and it all needs wrapping in a function so you can run it on each dataset. The use of rle requires you have evenly spaced observations (e.g. hourly).
When passed a vector or a list, `seq()` will create a sequence (with the starting value `from = 1`) the length of the object passed. Example: &gt; a &lt;- list() &gt; seq(a) integer(0) &gt; length(a) &lt;- 10 &gt; seq(a) [1] 1 2 3 4 5 6 7 8 9 10 &gt; length(a) &lt;- 20 &gt; seq(a) [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
Regardless of what you name it, this is probably treating it as `seq(along.with= )` because of its type (identical with `seq_along()` which is preferable to use because there are unintended consequences of `seq` if you get input wrong). What output were you hoping for?
The key for making contingency table is the command `table`. Does this get you going in the right direction? df &lt;- read.table(sep="\t", text=" p_id pact pac pe ets sn r_id rct rut return_date 182 2/4/2019 11:00:00 Emailed 275 2683 21293 NA NA NA 182 2/4/2019 11:00:00 Emailed 276 2333 PAV60 NA NA NA 182 2/4/2019 05:45:00 Called 683 2459 76C2C NA NA NA 183 2/4/2019 11:00:00 Emailed 577 2452 EVY50 NA NA NA 183 2/4/2019 11:00:00 Emailed 578 2459 7CB2C NA NA NA 183 2/4/2019 11:00:00 Emailed 579 2399 MSACG NA NA NA 183 2/4/2019 11:00:00 Emailed 871 2452 EVY50 NA NA NA 183 2/4/2019 11:00:00 Emailed 872 2329 CS001 NA NA NA 183 2/4/2019 11:00:00 Emailed 873 2459 73K2C NA NA NA 183 2/4/2019 11:00:00 Emailed 874 2350 MSACC NA NA NA 183 2/4/2019 11:00:00 Emailed 875 2346 MSACD NA NA NA 183 2/4/2019 11:00:00 Emailed 961 2459 87G2C NA NA NA 183 2/4/2019 11:00:00 Emailed 962 2399 MSACH NA NA NA 183 2/4/2019 11:00:00 Emailed 963 2399 MSACH NA NA NA 183 2/4/2019 11:00:00 Emailed 964 2346 MSACD NA NA NA 184 2/4/2019 11:00:00 Emailed 209 2459 81B2C NA NA NA 184 2/4/2019 11:00:00 Emailed 210 2291 CS025 NA NA NA 184 2/4/2019 11:00:00 Emailed 211 2333 PAV60 NA NA NA 184 2/4/2019 11:00:00 Emailed 445 2459 85F2C NA NA NA 184 2/4/2019 11:00:00 Emailed 446 2492 M1180 NA NA NA 184 2/4/2019 11:00:00 Emailed 604 2459 7CB2C NA NA NA 184 2/4/2019 11:00:00 Emailed 605 2333 PAV60 NA NA NA 184 2/4/2019 11:00:00 Emailed 758 2291 CS031 NA NA NA 184 2/4/2019 11:00:00 Emailed 760 2333 PAV60 NA NA NA 185 2/4/2019 05:45:00 Called 169 2459 77V2T NA NA NA 185 2/4/2019 05:45:00 Called 170 2340 MSACH NA NA NA 185 2/4/2019 05:45:00 Called 171 2472 4KX18 NA NA NA 185 2/4/2019 05:45:00 Called 172 2296 60800 NA NA NA 185 2/4/2019 05:45:00 Called 173 2333 PAV60 NA NA NA 185 2/4/2019 05:45:00 Called 174 2373 PAV70 NA NA NA 185 2/4/2019 05:45:00 Called 175 2373 PAV70 NA NA NA 185 2/4/2019 05:45:00 Called 176 2373 PAV70 NA NA NA 186 2/4/2019 05:45:00 Called 823 2342 H16TR NA NA NA 186 2/4/2019 05:45:00 Called 824 2691 EVY62 NA NA NA 186 2/4/2019 05:45:00 Called 825 2691 EVY63 NA NA NA 189 2/4/2019 05:45:00 Called 600 2312 CS025 NA NA NA 189 2/4/2019 05:45:00 Called 601 2342 G21TF NA NA NA 189 2/4/2019 05:45:00 Called 602 2299 PAV70 NA NA NA 189 2/4/2019 05:45:00 Called 603 2299 PAV70 NA NA NA 189 2/4/2019 05:45:00 Called 604 2683 21293 NA NA NA", header=TRUE, stringsAsFactors=FALSE) # add a fake "returned" column populated with random data df$returned &lt;- sample(x = c("Returned", "NotReturned"), replace = TRUE) # generate a contingency table: table(df$pe, df$returned) NotReturned Returned Called 9 8 Emailed 11 12 This might also be helpful: https://data-flair.training/blogs/r-contengency-tables/ 
&gt;What output were you hoping for? I was hoping to get an error. Because it seemed to me that `"a","b","c"` is not the valid start of sequence (from=). Telling the R to create the sequence that starts with `"a","b","c"` and has some following logical sequences seems non-logical.
`seq` is a generic whose behavior depends on input. The documentation warns to be specific about what you want (using `seq_along` or whatever) if you want to make sure you get what you want. Specifically, it warns that it dispatches based on the class of the first argument even if you name if "from" and expect it to behave accordingly. 
R has two flavors, seq_along() and seq_len(). I think the latter would return an error with char vector input. Using generic seq() lets you be vague.
Does this work if i have multiple rows of p_id because they were either called more than once, emailed more than once, or called and emailed? I want a count of unique p_id's that were only called, only emailed, or both called and emailed?
You'll probably need to make a new dataframe that contains just that information. `dplyr` is a commonly used set of tools that can help you make those transformations.
&gt;it warns that it dispatches based on the class of the first argument even if you name if "from" This is the right answer. Passing \`from\` as a character vector will dispatch to \`seq.default\` and that function is "smart" enough to check the length of "from" and handle it as a special case if its length is more than \`1L\`. In that special case, it returns \`1:from\`. OP could have checked this by looking at the code for seq and seq.default though I will admit that that's pretty obscure.
&gt;it warns that it dispatches based on the class of the first argument even if you name if "from" This is the right answer. Passing \`from\` as a character vector will dispatch to \`seq.default\` and that function is "smart" enough to check the length of "from" and handle it as a special case if its length is more than \`1L\`. In that special case, it returns \`1:from\`. [u/vasili111](https://www.reddit.com/user/vasili111) could have checked this by looking at the code for seq and seq.default though I will admit that that's pretty obscure.
&gt;hmb\_source=navbar&amp;hmb\_medium=product\_tile&amp;partner=somedeals14&amp;hmb\_campaign=tile\_index\_1 I tried combining both. It doesn't work
I am not sure I understand. Isn't that what the jitter does? It doesn't look that strange to me.
I thought jittering simply allowed the data to be spread out horizontally, so as to avoid data points from overlapping with one another (which would look quite messy). Or am I missing something? It doesn't really make sense to me that there are grey dots (non-outliers) being shown beyond the black dots (outliers). If those data points are beyond the outliers, then that would mean that they are also outliers? 
Look into the default behavior of geom_jitter. https://ggplot2.tidyverse.org/reference/geom_jitter.html
It‚Äôs predicting whatever the second (0/1 &lt;‚Äî‚Äî) level is. 
If you're combining `geom_boxplot` and `geom_jitter` you need to exclude the outliers in the `geom_boxplot` layer: ``` geom_boxplot(outlier.shape = NA) ```
 Check out predict.glm() arguments. You can choose to return probabilities and then use your own threshold (eg you may want to use .35 as your optimal threshold). There are other options as well
You're jittering in both directions - on the X and Y axes. as /u/actuaryal states, you should remove outliers when you plot the boxplot and remove the vertical (height) jitter.
This was an interesting enough scenario that I felt compelled to take a stab. My attempt in this public gist: https://gist.github.com/slin30/0c04b16e2100ab96fe20047de6598700 Not complete, and a lot of dependencies on my interpretations and assumptions, but hopefully this gives you some ideas, if nothing else.
contrasts(df$DependentVariable)
In a fresh Docker container (from [this image](https://github.com/rocker-org/rocker-versioned)), it installs, loads, and returns help docs just like normal. What are specific errors you are receiving? What is the output of `Sys.info()`?
Counterpoint: Base R is already friggin huge! [https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html)
I know, I know. But tidyverse does things so much more cleanly.
I don't think it'd be wise to roll it into base R. Just keep base R the same moving forward, IMO, and anything you want to add just do the 2 lines of code to add it. 
Tidyverse is massive. It has some integral parts but they are all rapidly evolving. Being part of base R would hinder that. Think of dplyr and standard evaluation. The entire system got an overhaul in the recent past to use tidyeval. Large changes like that in base R are as far as I know unprecedented. Also tidyverse even has name collisions with base R (filter).
Eh, there's too much and it's very different from base R. Magrittr and %&gt;% should be in base R. So much easier and cleaner than any alternatives.
Can you prove to yourself that it is currently installed? Do you get an error when loading it? Did you update R recently and need to reinstall your libraries? Are you logged in as the same user that installed it? Are you using the installation of R you think you're using? 
Ugh, no. There's enough bloat in the base namespace already, let's keep it at that. Tidyverse could be included in the library by default, though, I concede that, just by the sheer popularity of the package.
I agree at least wait till it stabilizes more then reconsider, but it's far too early to incorporate into base R.
Oftentimes people use tidyverse for dplyr; some people prefer data.table in its place.
`dplyr` and `tidyr` solution using `mtcars` as an example: ``` library(dplyr) library(tidyr) cor_mat &lt;- cor(mtcars) cor_mat[!lower.tri(cor_mat)] &lt;- NA # remove diagonal and redundant values data.frame( cor_mat) %&gt;% rownames_to_column() %&gt;% gather(key="variable", value="correlation", -rowname) %&gt;% filter(abs(correlation) &gt; 0.9) ```
OK, thanks. I do want to be able so see which specific datapoints are outliers though, and using jitter alone shows all of the datapoints as the same colour. Is there a way to distinguish the outliers from non-outliers without doubling up on the datapoints via jittering?
Aah, I was just looking at that article earlier but I didn't think to adjust height because I never considered that there would be a function that shifts datapoints so that they don't correspond to the correct point on the vertical axis!
Thanks for the reply guys, I uninstalled it, started a new workspace, reinstalled and it worked fine. Just one of those things I guess!
Thanks for the reply guys, I uninstalled it, started a new workspace, reinstalled and it worked fine. Just one of those things I guess!
to base r: tidyverse has collisions as others have said tidyverse code with pipes is very different than base r. unpiped tidyverse code is nigh impossible to read tibbles are slow to all hell (and so are the other transformations), and are difficult to read+print once data becomes large enough base r already has so many one off helper functions, adding tidyverse would just build on to that fact 
Making the first table is somewhat straightforward. Here's some R code that doesn't use any fancy external libraries. # Put your data in a dataframe x &lt;- data.frame( id = c(1,1,1,2,2,3,4,4,5,5,6,6), comm = c("Called","Called","Emailed","Called", "Called","Emailed","Called", "Emailed","Called", "Emailed", "Other", "Other"), called = c("Called", "Called", "Not Called", "Called", "Called", "Not Called", "Called","Not Called", "Called", "Not Called", "Not Called", "Not Called"), emailed = c("Not Emailed","Not Emailed","Emailed", "Not Emailed", "Not Emailed", "Emailed","Not Emailed", "Emailed", "Not Emailed","Emailed", "Not Emailed", "Not Emailed"), returned = c("Returned", "Returned", "Returned", "Not Returned", "Not Returned","Returned", "Not Returned", "Not Returned", "Not Returned", "Not Returned", "Not Returned", "Not Returned"), stringsAsFactors=F ) # Split into a list of multiple dataframes, where each dataframe holds the observations for one individual, # do some calculations, and then reconvert results back into a new dataframe y&lt;-setNames(data.frame(t(sapply(split(x, x$id), function(x) { # Sum up the number of calls and emails received for this person calls &lt;- x$comm == "Called" emails &lt;- x$comm == "Emailed" # Sum up the number of calls and emails he/she returned returned_calls &lt;- sum(x$returned[calls] == "Returned", na.rm=T) returned_emails &lt;- sum(x$returned[emails] == "Returned", na.rm=T) c(sum(calls, na.rm=T), sum(emails,na.rm=T), returned_calls, returned_emails) }))), c("calls","emails","returned_calls","returned_emails")) # Store table as dataframe tbl1 &lt;- data.frame( emailed = c(sum(y$calls&gt;0 &amp; y$emails&gt;0), sum(y$calls==0 &amp; y$emails&gt;0)), not_emailed = c(sum(y$calls&gt;0 &amp; y$emails==0), sum(y$calls==0 &amp; y$emails==0)), row.names = c("called", "not_called") ) Okay, so this gives us a table like the one you wanted. Quite involved for such a small thing. For your second table, I tried computing it as described in your post, but it comes out with other numbers than those you give in your example. Could you verify those numbers? I think the description of this second table is a little ambiguous. I mean, what happens if there are two calls for one individual, in which one was returned and the other was not? Does that count as a unique id that returned his phone call or not? Unless I'm missing something, it seems your description breaks down for cases such as these. &amp;#x200B; &amp;#x200B;
Firstly, I've got to say thank you. Thank you so much. I have made more progress with your clear example than with anything I've found online. So from your example, it returned the one correlation which met the criteria (disp and cyl : correlation of 0.9). I plotted the matrix with corrplot and indeed I see that these two variable correlate the tightest (-0.9). However, I want to take the exact same route but apply that to my matrix. An ideal situation (if using this example) would be to have a new matrix generated which only had the criteria-passing variables present........so the new correlation matrix would only have cyl x disp. I could also just make corrplot plot the matrix for only those variables which meet the criteria. Thanks again A TON for this help!
 x &lt;- rep(c(0,3), 100) y &lt;- rep(1:200) data &lt;- data.frame(x,y) data$new &lt;- ifelse(data$x==3, (data$y/2), data$y) data$new 
 df %&gt;% mutate(new_col = if_else(x == 3, y/2, y))
So your interest is keeping any variables which have *at least one correlation stronger than 0.9*, but displaying those variables' correlations with all other retained variables (regardless of strength)? That is even easier: cor_mat &lt;- cor(mtcars) cor_mat &lt;- cor_mat[apply(cor_mat, 1, function(x) any(x &gt; 0.9 &amp; x &lt; 1) ) %in% T, apply(cor_mat, 1, function(x) any(x &gt; 0.9 &amp; x &lt; 1) ) %in% T] cor_mat 
&gt; For your second table, I tried computing it as described in your post, but it comes out with other numbers than those you give in your example. Could you verify those numbers? Ah, your right, my bad, I've edited the post to correct that. &gt; I also think the description of this second table is a little ambiguous. I mean, what happens if there are two calls for one individual, in which one was returned and the other was not? Does that count as a unique id that returned his phone call or not? Again, your right I accidentally left the description a little to vague. In this case if the id returned it for any of calls/emails it counts as returned.
How would i get the second table?
I've worked through your example several times now but for the life of my I cannot seem to get it to work with my data. This made cor_mat to be modified such that now only &gt;0.9 values were returned. I plotted cor_mat and voila, only these variables were plotted. perfect. My data looks **exactly** like mtcars, except there are 8 rows and 300 columns. I make the correlation matrix and it works perfectly. If I plot this, it works perfectly (lots and lots of low correlation junk, of course). Then I use the exact same method and it returns back ...the exact same matrix as I started with! I can't seem to explain it. I do the exact same thing on the same type of data but it won't apply the filter correctly :(
You can look at function arguments by using ?function. Try typing ?plot.default to see what type of parameters you can use, and what makes sense.
add below line inside your plot code: ylim = c(500:900),xlim = c(2003-2013) 
I have to use the code that is shown, I know that works, but I need to fill the gap. Thank you tho
Honestly, the reason you found people recommending you convert it to a data frame then write is because it's probably easiest. You can just write the data frame and have it print the NAs as blank cells. write.csv(&lt;data.frame&gt;,&lt;output.file.name&gt;,na="") If you don't specify na="", it defaults to "NA", which is why you have them in the output. 
 library("data.table") library("magrittr") hello &lt;- list() hello[[1]] &lt;- "My Baby" hello[[2]] &lt;- "My Honey" hello[[3]] &lt;- c("My Ragtime", "Summertime Gal") hello %&lt;&gt;% sapply(function(v) { df &lt;- as.matrix(v) %&gt;% t %&gt;% data.frame names(df) &lt;- paste0("Obvservation_",1:ncol(df)) return(df) }) hello %&lt;&gt;% rbindlist(fill = T) %T&gt;% write.csv(file="hello.csv",na = "",row.names = F) 
Are you opposed to making it a dataframe with NA‚Äôs, and exporting it with the argument na = ‚Äú‚Äù? Or something to that effect. If you are opposed can you provide more info on why you cannot have NA‚Äôs to copy and paste it somewhere? 
I'm thinking about this... But if the number of values is unequal, having something in place of those values is inevitable; I'm not sure how you are going to escape the NAs. Anyways, here is something to try: lapply(X = your_list,FUN = write.table,file = "examplefile.csv",append = T,sep = ",") I didn't test this, but I think it should work? Walking through it: lapply applies a function to each list. The list (or set of lists in your case) in question is used as the X. FUN = sets the function as write.table the additional arguments after are passed to write.table. If you wanted it as a CSV, then the sep = "," is used. The thing that I think will be the critical piece for you is the append = T. Without this, you'll just overwrite the file with each new list; this will add to it successively.
This is awesome! One quick question: this uses rbindlist. I'd like the .csv to be transposed if possible, but there isn't a cbindlist function in those libraries. I'm not familiar with these two libraries either, so I'm not sure where I would use t() to transpose it. 
Is this a homework problem?
Things to check: 1. Is it possible that every column has at least one correlation above 0.9? 2. Is it possible your diagonal elements are not *exactly* 1 and are instead slightly less than 1? This would be a problem. Try this: sum(apply(cor_mat, 1, function(x) any(x &gt; 0.9 &amp; x &lt; 1) )) sum(apply(cor_mat, 2, function(x) any(x &gt; 0.9 &amp; x &lt; 1) )) Do those both return 300 (the number of variables)? In that case they're selecting every case, because every variable has at least one correlation greater than 0.9 and below 1.
That really looks pretty. Thanks for the suggestion.